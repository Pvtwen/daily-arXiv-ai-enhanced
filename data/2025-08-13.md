<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [cs.LG](#cs.LG) [Total: 61]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Where is the Boundary: Multimodal Sensor Fusion Test Bench for Tissue Boundary Delineation](https://arxiv.org/abs/2508.08257)
*Zacharias Chen,Alexa Cristelle Cahilig,Sarah Dias,Prithu Kolar,Ravi Prakash,Patrick J. Codd*

Main category: eess.SP

TL;DR: 论文提出了一种多模态传感测试平台，用于提高手术中组织边界识别的准确性，结合视觉、接触麦克风和力传感器数据。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助神经外科手术缺乏自然感官反馈，尤其在区分健康与肿瘤组织时，现有研究对多模态传感的探索有限。

Method: 开发了一个模块化测试平台，结合视觉引导、接触麦克风和力传感器数据，通过交互式图形界面实时获取和可视化数据。

Result: 实验表明，多模态融合显著提高了材料分类的准确性。

Conclusion: 该平台为手术中的传感器融合提供了可扩展的硬件-软件解决方案，展示了多模态方法在实时组织边界识别中的潜力。

Abstract: Robot-assisted neurological surgery is receiving growing interest due to the
improved dexterity, precision, and control of surgical tools, which results in
better patient outcomes. However, such systems often limit surgeons' natural
sensory feedback, which is crucial in identifying tissues -- particularly in
oncological procedures where distinguishing between healthy and tumorous tissue
is vital. While imaging and force sensing have addressed the lack of sensory
feedback, limited research has explored multimodal sensing options for accurate
tissue boundary delineation. We present a user-friendly, modular test bench
designed to evaluate and integrate complementary multimodal sensors for tissue
identification. Our proposed system first uses vision-based guidance to
estimate boundary locations with visual cues, which are then refined using data
acquired by contact microphones and a force sensor. Real-time data acquisition
and visualization are supported via an interactive graphical interface.
Experimental results demonstrate that multimodal fusion significantly improves
material classification accuracy. The platform provides a scalable
hardware-software solution for exploring sensor fusion in surgical applications
and demonstrates the potential of multimodal approaches in real-time tissue
boundary delineation.

</details>


### [2] [Hardware-friendly IR-HARQ for Polar SCL Decoders](https://arxiv.org/abs/2508.08425)
*Marwan Jalaleddine,Jiajie Li,Warren J. Gross*

Main category: eess.SP

TL;DR: 将极坐标码的增量冗余混合自动重传请求（IR-HARQ）方案中的集合操作转换为二进制向量操作，以减少硬件实现中的内存开销和面积开销。


<details>
  <summary>Details</summary>
Motivation: 为了在下一代无线通信系统中扩展极坐标码的应用，需要支持IR-HARQ方案。但现有方案存在内存访问模式不规则和硬件实现面积开销大的问题。

Method: 提出将集合操作转换为二进制向量操作，并引入新的快速节点集成方法以减少快速节点数量。

Result: 与不支持IR-HARQ的SCL解码相比，内存开销仅为25-27%。

Conclusion: 所提方案显著改善了硬件兼容性，减少了内存和面积开销。

Abstract: To extend the applications of polar codes within next-generation wireless
communication systems, it is essential to incorporate support for Incremental
Redundancy (IR) Hybrid Automatic Repeat Request (HARQ) schemes. The baseline
IR-HARQ scheme's reliance on set-based operations leads to irregular memory
access patterns, posing significant challenges for efficient hardware
implementation. Furthermore, the introduction of new bit types increases the
number of fast nodes that are decoded without traversing the sub-tree,
resulting in a substantial area overhead when implemented in hardware. To
address these issues and improve hardware compatibility, we propose
transforming the set-based operations within the polar IR-HARQ scheme into
binary vector operations. Additionally, we introduce a new fast node
integration approach that avoids increasing the number of fast nodes, thereby
minimizing the associated area overhead. Our proposed scheme results in a
memory overhead of 25-27% compared to successive cancellation list (SCL)
decoding without IR-HARQ support.

</details>


### [3] [An Analytical and Experimental Study of Distributed Uplink Beamforming in the Presence of Carrier Frequency Offsets](https://arxiv.org/abs/2508.08506)
*Mehdi Zafari,Divyanshu Pandey,Rahman Doost-Mohammady*

Main category: eess.SP

TL;DR: 本文分析了分布式多用户波束成形（D-MUBF）在TDD多用户MIMO系统中的挑战，重点关注频率同步误差的影响，并提供了理论和实验评估。


<details>
  <summary>Details</summary>
Motivation: 解决分布式接入点（APs）在频率同步误差下的D-MUBF性能问题，填补实验研究的空白。

Method: 通过理论分析和实验评估（使用RENEW大规模MIMO测试平台），研究D-MUBF在频率同步误差下的性能。

Result: 提供了SINR的闭式表达式，并通过实验数据验证了理论预测。

Conclusion: 研究为未来D-MUBF技术的研究提供了数据集和见解。

Abstract: Realizing distributed multi-user beamforming (D-MUBF) in time division duplex
(TDD)-based multi-user MIMO (MU-MIMO) systems faces significant challenges. One
of the most fundamental challenges is achieving accurate over-the-air (OTA)
timing and frequency synchronization among distributed access points (APs),
particularly due to residual frequency offsets caused by local oscillator (LO)
drifts. Despite decades of research on synchronization for MU-MIMO, there are
only a few experimental studies that evaluate D-MUBF techniques under imperfect
frequency synchronization among distributed antennas. This paper presents an
analytical and experimental assessment of D-MUBF methods in the presence of
frequency synchronization errors. We provide closed-form expressions for
signal-to-interference-plus-noise ratio (SINR) as a function of channel
characteristics and statistical properties of carrier frequency offset (CFO)
among AP antennas. In addition, through experimental evaluations conducted with
the RENEW massive MIMO testbed, we collected comprehensive datasets across
various experimental scenarios. These datasets comprise uplink pilot samples
for channel and CFO estimation, in addition to uplink multi-user data intended
for analyzing D-MUBF techniques. By examining these datasets, we assess the
performance of D-MUBF in the presence of CFO and compare the analytical
predictions with empirical measurements. Furthermore, we make the datasets
publicly available and provide insights on utilizing them for future research
endeavors.

</details>


### [4] [Tensor-Structured Bayesian Channel Prediction for Upper Mid-Band XL-MIMO Systems](https://arxiv.org/abs/2508.08491)
*Hongwei Hou,Yafei Wang,Xinping Yi,Wenjin Wang,Dirk T. M. Slock,Shi Jin*

Main category: eess.SP

TL;DR: 论文提出了一种针对XL-MIMO系统中信道预测的新方法，结合了张量结构建模和贝叶斯推断，解决了移动性导致的信道老化问题。


<details>
  <summary>Details</summary>
Motivation: XL-MIMO系统在移动性下因信道老化和近场传播特性导致性能下降，需要有效的信道预测方法。

Method: 提出了基于张量结构的信道模型（SFT和BDD域），结合贝叶斯推断和EM算法，开发了TS-BLI算法。

Result: 数值模拟表明，该方法在信道预测性能上表现优越。

Conclusion: 所提方法为XL-MIMO系统中的信道预测提供了高效且计算可行的解决方案。

Abstract: The upper mid-band balances coverage and capacity for the future cellular
systems and also embraces XL-MIMO systems, offering enhanced spectral and
energy efficiency. However, these benefits are significantly degraded under
mobility due to channel aging, and further exacerbated by the unique near-field
(NF) and spatial non-stationarity (SnS) propagation in such systems. To address
this challenge, we propose a novel channel prediction approach that
incorporates dedicated channel modeling, probabilistic representations, and
Bayesian inference algorithms for this emerging scenario. Specifically, we
develop tensor-structured channel models in both the spatial-frequency-temporal
(SFT) and beam-delay-Doppler (BDD) domains, which leverage temporal
correlations among multiple pilot symbols for channel prediction. The factor
matrices of multi-linear transformations are parameterized by BDD domain grids
and SnS factors, where beam domain grids are jointly determined by angles and
slopes under spatial-chirp based NF representations. To enable tractable
inference, we replace environment-dependent BDD domain grids with uniformly
sampled ones, and introduce perturbation parameters in each domain to mitigate
grid mismatch. We further propose a hybrid beam domain strategy that integrates
angle-only sampling with slope hyperparameterization to avoid the computational
burden of explicit slope sampling. Based on the probabilistic models, we
develop tensor-structured bi-layer inference (TS-BLI) algorithm under the
expectation-maximization (EM) framework, which reduces computational complexity
via tensor operations by leveraging the bi-layer factor graph for approximate
E-step inference and an alternating strategy with closed-form updates in the
M-step. Numerical simulations based on the near-practical channel simulator
demonstrate the superior channel prediction performance of the proposed
algorithm.

</details>


### [5] [Learning Zero Constellations for Binary MOCZ in Fading Channels](https://arxiv.org/abs/2508.08571)
*Anthony Joseph Perre,Parker Huggins,Alphan Sahin*

Main category: eess.SP

TL;DR: 提出了两种设计零星座的方法，用于二进制调制的共轭倒数零点（BMOCZ）。第一种方法将星座设计视为多标签二分类问题，学习零点位置用于直接零点测试（DiZeT）解码器。第二种方法引入基于神经网络（NN）的解码器，联合学习解码器和零点星座参数。NN解码器能直接推广到平坦衰落信道，尽管训练时仅使用加性高斯白噪声。数值模拟结果表明，学习的零星座优于经典的Huffman BMOCZ星座，NN解码器在计算复杂度增加的情况下实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 设计更高效的零星座和相应的解码器，以提高二进制调制在共轭倒数零点（BMOCZ）上的性能。

Method: 1. 将星座设计视为多标签二分类问题，学习零点位置用于DiZeT解码器。2. 引入NN解码器，联合学习解码器和零点星座参数。

Result: NN解码器能直接推广到平坦衰落信道，学习的零星座性能优于经典Huffman BMOCZ星座，NN解码器在计算复杂度增加的情况下实现了显著性能提升。

Conclusion: 提出的两种方法在性能上优于传统方法，尤其是NN解码器在复杂信道条件下表现优异，但计算复杂度较高。

Abstract: In this work, we propose two methods to design zero constellations for binary
modulation on conjugate-reciprocal zeros (BMOCZ). In the first approach, we
treat constellation design as a multi-label binary classification problem and
learn the zero locations for a direct zero-testing (DiZeT) decoder. In the
second approach, we introduce a neural network (NN)-based decoder and jointly
learn the decoder and zero constellation parameters. We show that the NN-based
decoder can directly generalize to flat-fading channels, despite being trained
under additive white Gaussian noise. Furthermore, the results of numerical
simulations demonstrate that learned zero constellations outperform the
canonical, Huffman BMOCZ constellation, with the proposed NN-based decoder
achieving large performance gain at the expense of increased computational
complexity.

</details>


### [6] [Biomedical Signal Processing: EEG and ECG Classification with Discrete Wavelet Transforms, Energy Distribution, and Convolutional Neural Networks](https://arxiv.org/abs/2508.08602)
*Justin London*

Main category: eess.SP

TL;DR: 提出了一种多模态深度学习模型，利用离散小波变换进行信号预处理，并通过图像融合和特征融合框架提高生物医学信号分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统医生手动分析生理信号容易出错，深度学习可以显著提高分析准确性。

Method: 使用离散小波变换降噪，将信号转换为2D和3D图像，结合多模态图像和特征融合框架。

Result: 实验表明，多模态方法结合小波变换提高了疾病和障碍分类的准确性。

Conclusion: 多模态深度学习模型在生物医学信号分析中具有显著优势。

Abstract: Biomedical signal processing extract meaningful information from
physiological signals like electrocardiograms (ECGs), electroencephalograms
(EEGs), and electromyograms (EMGs) to diagnose, monitor, and treat medical
conditions and diseases such as seizures, cardiomyopathy, and neuromuscular
disorders, respectively. Traditional manual physician analysis of electrical
recordings is prone to human error as subtle anomolies may not be detected.
Recently, advanced deep learning has significantly improved the accuracy of
biomedical signal analysis. A multi-modal deep learning model is proposed that
utilizes discrete wavelet transforms for signal pre-processing to reduce noise.
A multi-modal image fusion and multimodal feature fusion framework is utilized
that converts numeric biomedical signals into 2D and 3D images for image
processing using Gramian angular fields, recurrency plots, and Markov
transition fields. In this paper, deep learning models are applied to ECG, EEG,
and human activity signals using actual medical datasets, brain, and heart
recordings. The results demonstrate that using a multi-modal approach using
wavelet transforms improves the accuracy of disease and disorder
classification.

</details>


### [7] [Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence: A Survey](https://arxiv.org/abs/2508.08620)
*Yang Lu,Shengli Zhang,Chang Liu,Ruichen Zhang,Bo Ai,Dusit Niyato,Wei Ni,Xianbin Wang,Abbas Jamalipour*

Main category: eess.SP

TL;DR: 该论文探讨了图神经网络（GNNs）在无线通信网络中的应用，并提出了基于代理人工智能（AI）的GNN框架，以应对动态环境中多样化的需求。


<details>
  <summary>Details</summary>
Motivation: 通信网络的复杂性和动态性对服务质量提出了更高要求，传统GNN的被动学习框架难以满足需求。

Method: 提出了代理AI驱动的GNN框架，并全面回顾了GNN在无线通信中的应用，包括物理层、MAC层和网络层设计等。

Result: 展示了GNN在多种无线通信技术中的潜力，并提出了基于大语言模型（LLM）的智能问答框架。

Conclusion: 代理AI驱动的GNN框架有望提升无线通信网络的适应性和智能化水平。

Abstract: The rapid advancement of communication technologies has driven the evolution
of communication networks towards both high-dimensional resource utilization
and multifunctional integration. This evolving complexity poses significant
challenges in designing communication networks to satisfy the growing
quality-of-service and time sensitivity of mobile applications in dynamic
environments. Graph neural networks (GNNs) have emerged as fundamental deep
learning (DL) models for complex communication networks. GNNs not only augment
the extraction of features over network topologies but also enhance scalability
and facilitate distributed computation. However, most existing GNNs follow a
traditional passive learning framework, which may fail to meet the needs of
increasingly diverse wireless systems. This survey proposes the employment of
agentic artificial intelligence (AI) to organize and integrate GNNs, enabling
scenario- and task-aware implementation towards edge general intelligence. To
comprehend the full capability of GNNs, we holistically review recent
applications of GNNs in wireless communications and networking. Specifically,
we focus on the alignment between graph representations and network topologies,
and between neural architectures and wireless tasks. We first provide an
overview of GNNs based on prominent neural architectures, followed by the
concept of agentic GNNs. Then, we summarize and compare GNN applications for
conventional systems and emerging technologies, including physical, MAC, and
network layer designs, integrated sensing and communication (ISAC),
reconfigurable intelligent surface (RIS) and cell-free network architecture. We
further propose a large language model (LLM) framework as an intelligent
question-answering agent, leveraging this survey as a local knowledge base to
enable GNN-related responses tailored to wireless communication research.

</details>


### [8] [ReQuestNet: A Foundational Learning model for Channel Estimation](https://arxiv.org/abs/2508.08790)
*Kumar Pratik,Pouriya Sadeghi,Gabriele Cesa,Sanaz Barghi,Joseph B. Soriaga,Yuanning Yu,Supratik Bhattacharjee,Arash Behboodi*

Main category: eess.SP

TL;DR: 提出了一种新型神经网络架构ReQuestNet，用于5G及更高版本的信道估计，显著简化了流程并优于传统线性MMSE方法。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信系统中信道估计的复杂性，如处理可变资源块、动态传输层等问题，并克服传统线性MMSE方法的局限性。

Method: ReQuestNet由CoarseNet和RefinementNet组成，前者进行初步信道估计，后者通过跨预编码PRG和MIMO空间维度相关性进行细化。

Result: 仿真结果显示，ReQuestNet在多种信道条件下显著优于传统MMSE方法，最高可提升10dB增益。

Conclusion: ReQuestNet能有效泛化到未见过的信道配置，动态利用PRG和MIMO相关性，表现出强大的性能。

Abstract: In this paper, we present a novel neural architecture for channel estimation
(CE) in 5G and beyond, the Recurrent Equivariant UERS Estimation Network
(ReQuestNet). It incorporates several practical considerations in wireless
communication systems, such as ability to handle variable number of resource
block (RB), dynamic number of transmit layers, physical resource block groups
(PRGs) bundling size (BS), demodulation reference signal (DMRS) patterns with a
single unified model, thereby, drastically simplifying the CE pipeline. Besides
it addresses several limitations of the legacy linear MMSE solutions, for
example, by being independent of other reference signals and particularly by
jointly processing MIMO layers and differently precoded channels with unknown
precoding at the receiver. ReQuestNet comprises of two sub-units, CoarseNet
followed by RefinementNet. CoarseNet performs per PRG, per transmit-receive
(Tx-Rx) stream channel estimation, while RefinementNet refines the CoarseNet
channel estimate by incorporating correlations across differently precoded
PRGs, and correlation across multiple input multiple output (MIMO) channel
spatial dimensions (cross-MIMO). Simulation results demonstrate that ReQuestNet
significantly outperforms genie minimum mean squared error (MMSE) CE across a
wide range of channel conditions, delay-Doppler profiles, achieving up to 10dB
gain at high SNRs. Notably, ReQuestNet generalizes effectively to unseen
channel profiles, efficiently exploiting inter-PRG and cross-MIMO correlations
under dynamic PRG BS and varying transmit layer allocations.

</details>


### [9] [Sparse Near-Field Channel Estimation for XL-MIMO via Adaptive Filtering](https://arxiv.org/abs/2508.08663)
*Vidya Bhasker Shukla,Italo Atzeni*

Main category: eess.SP

TL;DR: 提出了一种基于自适应滤波的近场稀疏信道估计方法PD-ZALMS，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 满足下一代无线应用对极大规模MIMO系统在近场区域的高效信道估计需求。

Method: 采用子阵列架构，开发了基于自适应滤波的PD-ZALMS框架。

Result: PD-ZALMS在信道估计精度和计算复杂度上优于现有方法，并在低至中等信噪比下优于最小二乘估计。

Conclusion: PD-ZALMS为极大规模MIMO系统的近场信道估计提供了高效解决方案。

Abstract: Extremely large-scale multiple-input multiple-output (XL-MIMO) systems
operating at sub-THz carrier frequencies represent a promising solution to meet
the demands of next-generation wireless applications. This work focuses on
sparse channel estimation for XL-MIMO systems operating in the near-field (NF)
regime. Assuming a practical subarray-based architecture, we develop a NF
channel estimation framework based on adaptive filtering, referred to as
\textit{polar-domain zero-attracting least mean squares (PD-ZALMS)}. The
proposed method achieves significantly superior channel estimation accuracy and
lower computational complexity compared with the well-established polar-domain
orthogonal matching pursuit. In addition, the proposed PD-ZALMS is shown to
outperform the oracle least-squares channel estimator at low-to-moderate
signal-to-noise ratio.

</details>


### [10] [VQ-VAE Based Digital Semantic Communication with Importance-Aware OFDM Transmission](https://arxiv.org/abs/2508.08686)
*Ming Lyu,Hao Chen,Dan Wang,Chen Qiu,Guangyin Feng,Nan Ma,Xiaodong Xu*

Main category: eess.SP

TL;DR: 提出了一种基于VQ-VAE的数字语义通信系统，结合重要性感知OFDM传输，显著提升了语义通信性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的语义通信系统多关注模拟传输，缺乏与数字通信的兼容性。

Method: 使用VQ-VAE生成共享离散码本，结合重要性感知OFDM传输策略，优化语义特征传输。

Result: 实验表明，该方案在低信噪比区域优于传统DeepSC，重建性能更优。

Conclusion: 提出的数字语义通信系统有效提升了传输效率和兼容性。

Abstract: Semantic communication (SemCom) significantly reduces redundant data and
improves transmission efficiency by extracting the latent features of
information. However, most of the conventional deep learning-based SemCom
systems focus on analog transmission and lack in compatibility with practical
digital communications. This paper proposes a vector quantized-variational
autoencoder (VQ-VAE) based digital SemCom system that directly transmits the
semantic features and incorporates the importance-aware orthogonal frequency
division multiplexing (OFDM) transmission to enhance the SemCom performance,
where the VQ-VAE generates a discrete codebook shared between the transmitter
and receiver. At transmitter, the latent semantic features are firstly
extracted by VQ-VAE, and then the shared codebook is adopted to match these
features, which are subsequently transformed into a discrete version to adapt
the digital transmission. To protect the semantic information, an
importance-aware OFDM transmission strategy is proposed to allocate the key
features near the OFDM reference signals, where the feature importance is
derived from the gradient-based method. At the receiver, the features are
rematched with the shared codebook to further correct errors. Finally,
experimental results demonstrate that our proposed scheme outperforms the
conventional DeepSC and achieves better reconstruction performance under low
SNR region.

</details>


### [11] [Evaluating Task Execution Performance Under Energy Measurement Overhead](https://arxiv.org/abs/2508.08757)
*Mateen Ashraf,Shahab Jahanbazi,Onel L. A. López*

Main category: eess.SP

TL;DR: 论文探讨了在能量收集（EH）物联网设备中，能量感知对任务执行性能的影响，并指出能量测量成本可能抵消其潜在优势。通过比较能量盲（EB）和能量感知（EA）任务决策方法，研究发现存在最优的能量测量/任务执行频率以最大化任务完成率。


<details>
  <summary>Details</summary>
Motivation: 能量感知在EH-IoT设备中可能提升性能，但传统上忽略的能量测量成本可能抵消其优势。研究旨在揭示如何通过调整操作参数（如测量频率和任务执行频率）优化性能。

Method: 比较能量盲（EB）和能量感知（EA）任务决策方法，分析不同能量测量/任务执行频率对任务完成率的影响。

Result: 研究发现存在最优的能量测量/任务执行频率，可最大化任务完成率。若参数选择不当，EA调度可能表现不如EB调度。

Conclusion: 合理选择能量测量和任务执行频率对EH-IoT设备的性能至关重要，否则能量测量成本可能导致EA调度效果不佳。

Abstract: Energy-awareness for adapting task execution behavior can bring several
benefits in terms of performance improvement in energy harvesting (EH) Internet
of Things (IoT) devices. However, the energy measurement cost of acquiring
energy information, which is traditionally ignored, can potentially neutralize
or even reverse the potential benefits. This paper highlights operational
parameters, such as energy measurement frequency and task execution frequency,
which can be tuned to improve the task execution performance of an EH-IoT
device. To this end, we consider energy-blind (EB) and energy-aware (EA) task
decision approaches and compare their task completion rate performance. We show
that, for specific hardware design parameters of an EH-IoT device, there exists
an optimal energy measurement/task execution frequency that can maximize the
task completion rate in both approaches. Moreover, if these parameters are not
chosen appropriately, then energy measurement costs can cause EA scheduling to
underperform compared to EB scheduling.

</details>


### [12] [Wideband Coplanar Waveguide MIMO Antenna for 6G Millimeter-Wave Applications with Defected Ground Structure](https://arxiv.org/abs/2508.08771)
*Atta Ullah,Daniyal Munir,Daniel Lindenschmitt,Hans D. Schotten*

Main category: eess.SP

TL;DR: 提出了一种适用于6G毫米波频段的新型宽带小型天线设计，包括单天线和2x2 MIMO天线，性能优异。


<details>
  <summary>Details</summary>
Motivation: 为6G无线网络的高频段（25 GHz至33.5 GHz）提供一种宽带小型天线解决方案。

Method: 采用微带贴片结构，通过共面波导（CPW）和缺陷地结构（DGS）设计单天线和2x2 MIMO天线。

Result: 单天线和MIMO天线在8.5 GHz宽带上表现出优异的回波损耗性能。

Conclusion: 该天线设计适用于6G毫米波技术的多种应用。

Abstract: This research study introduces a novel small antenna with wideband capacity
for the higher frequency range. As a possible contender for 6G wireless
networks, the proposed antenna is designed to target the 6G Millimeter-Wave
(mmWave) operating bands spanning 25 GHz to 33.5 GHz. With a microstrip patch
structure fed by a coplanar waveguide (CPW) with the defected ground structure
(DGS), a single antenna is introduced and then a design of 2 x 2 MIMO antenna
is presented. The single antenna has 2 elements, while the 2 x 2 MIMO antenna
has 8 elements. It achieves remarkably well in terms of return loss of 8.5 GHz
wideband, which is anticipated to be used for several applications in 6G mmWave
technology.

</details>


### [13] [Patient-Adaptive Focused Transmit Beamforming using Cognitive Ultrasound](https://arxiv.org/abs/2508.08782)
*Wessel L. van Nierop,Oisín Nolan,Tristan S. W. Stevens,Ruud J. G. van Sloun*

Main category: eess.SP

TL;DR: 提出了一种患者自适应的聚焦发射方案，通过后验采样和主动感知显著减少发射次数，提高超声图像质量和帧率。


<details>
  <summary>Details</summary>
Motivation: 传统聚焦发射方案帧率低，3D成像更慢；非聚焦发射存在运动去相关和谐波成像限制。

Method: 利用时间扩散模型进行后验采样，主动选择信息量最大的发射信号。

Result: 在2D和3D数据集上优于随机和等距子采样，对比噪声比更高，仅需2%发射即可估计射血分数。

Conclusion: 该方法高效、实时，适用于GPU加速，代码开源。

Abstract: Focused transmit beamforming is the most commonly used acquisition scheme for
echocardiograms, but suffers from relatively low frame rates, and in 3D, even
lower volume rates. Fast imaging based on unfocused transmits has disadvantages
such as motion decorrelation and limited harmonic imaging capabilities. This
work introduces a patient-adaptive focused transmit scheme that has the ability
to drastically reduce the number of transmits needed to produce a high-quality
ultrasound image. The method relies on posterior sampling with a temporal
diffusion model to perceive and reconstruct the anatomy based on partial
observations, while subsequently taking an action to acquire the most
informative transmits. This active perception modality outperforms random and
equispaced subsampling on the 2D EchoNet-Dynamic dataset and a 3D Philips
dataset, where we actively select focused elevation planes. Furthermore, we
show it achieves better performance in terms of generalized contrast-to-noise
ratio when compared to the same number of diverging waves transmits on three
in-house echocardiograms. Additionally, we can estimate ejection fraction using
only 2% of the total transmits and show that the method is robust to outlier
patients. Finally, our method can be run in real-time on GPU accelerators from
2023. The code is publicly available at https://tue-bmd.github.io/ulsa/

</details>


### [14] [Iterative Distortion Cancellation Algorithms for Single-Sideband Systems](https://arxiv.org/abs/2508.08796)
*Jun Dong,Tianwai Bo,Zhuo Wang,Haolei Gao,Zhongwei Tan,Yi Dong*

Main category: eess.SP

TL;DR: 提出了一种迭代失真消除算法，用于数字缓解双边带抖动信号对Kramers-Kronig接收器的影响，无需修改物理层结构。


<details>
  <summary>Details</summary>
Motivation: 解决自动偏置控制模块中抖动信号对接收器性能的影响，提升接收器灵敏度。

Method: 利用KK关系进行初始信号决策，并重建抖动信号引起的失真。

Result: 实验显示，算法将抖动信号容忍度提升至10% Vπ，80公里光纤传输中接收灵敏度提高1 dB以上。

Conclusion: 验证了所提失真消除方法的有效性。

Abstract: We propose an iterative distortion cancellation algorithm to digitally
mitigate the impact of double-sideband dither signal amplitude from the
automatic bias control module on Kramers-Kronig receivers without modifying
physical layer structures. The algorithm utilizes the KK relation for initial
signal decisions and reconstructs the distortion caused by dither signals.
Experimental tests in back-to-back showed it improved tolerance to dither
amplitudes up to 10% V{\pi}. For 80-km fiber transmission, the algorithm
increased the receiver sensitivity by more than 1 dB, confirming the
effectiveness of the proposed distortion cancellation method.

</details>


### [15] [Trajectory-adaptive Beam Shaping: Towards Beam-Management-Free Near-field Communications](https://arxiv.org/abs/2508.08894)
*Sicong Ye,Yulan Gao,Ming Xiao,Peng Wang,Marios Poulakis,Ulrik Imberg*

Main category: eess.SP

TL;DR: 论文提出了一种名为TABS的新方法，通过预定义用户轨迹来优化毫米波和太赫兹频段的波束成形，减少实时波束管理的开销。


<details>
  <summary>Details</summary>
Motivation: 毫米波和太赫兹频段的高频通信虽然能提升数据吞吐量和频谱效率，但面临严重的路径损耗和波束失准问题，传统方法计算和信令开销大。

Method: 提出轨迹自适应波束成形（TABS），利用自加速光束原理，预定义波束路径以匹配用户运动轨迹，无需实时调整。

Result: 仿真结果表明，TABS在链路性能、开销降低和实现复杂度方面优于传统方法。

Conclusion: TABS为高频无线通信提供了一种高效、低开销的波束管理解决方案。

Abstract: The quest for higher wireless carrier frequencies spanning the
millimeter-wave (mmWave) and Terahertz (THz) bands heralds substantial
enhancements in data throughput and spectral efficiency for next-generation
wireless networks. However, these gains come at the cost of severe path loss
and a heightened risk of beam misalignment due to user mobility, especially
pronounced in near-field communication. Traditional solutions rely on extremely
directional beamforming and frequent beam updates via beam management, but such
techniques impose formidable computational and signaling overhead. In response,
we propose a novel approach termed trajectory-adaptive beam shaping (TABS) that
eliminates the need for real-time beam management by shaping the
electromagnetic wavefront to follow the user's predefined trajectory. Drawing
inspiration from self-accelerating beams in optics, TABS concentrates energy
along pre-defined curved paths corresponding to the user's motion without
requiring real-time beam reconfiguration. We further introduce a dedicated
quantitative metric to characterize performance under the TABS framework.
Comprehensive simulations substantiate the superiority of TABS in terms of link
performance, overhead reduction, and implementation complexity.

</details>


### [16] [Scalable RIS-Aided Beamforming Strategies for Near-Field MU-MISO via Multi-Antenna Feeder](https://arxiv.org/abs/2508.08993)
*Giulia Torcolacci,Malte Schellmann,Davide Dardari*

Main category: eess.SP

TL;DR: 论文提出了一种模块化波束成形框架，用于近场区域的可重构智能表面（RIS）辅助多用户通信，采用新型天线架构AT-RIS（结合主动多天线馈源阵列和透射RIS），支持灵活的性能-复杂度权衡。


<details>
  <summary>Details</summary>
Motivation: 研究近场区域RIS辅助多用户通信的灵活性和性能优化，探索不同架构和预编码方案的适用性。

Method: 提出AT-RIS架构，分析对角和非对角T-RIS实现，结合聚焦、最小均方误差和特征模式分解等预编码方案。

Result: 非对角方案在用户少且角度分离度高时性能最优，但对角方案（尤其是聚焦预编码）在用户密度高时表现更稳健、公平且可扩展。

Conclusion: 对角AT-RIS架构在频谱效率、复杂度和公平性之间取得平衡，是近场多用户系统的实用解决方案。

Abstract: This paper investigates a modular beamforming framework for reconfigurable
intelligent surface (RIS)-aided multi-user (MU) communications in the
near-field regime, built upon a novel antenna architecture integrating an
active multi-antenna feeder (AMAF) array with a transmissive RIS (T-RIS),
referred to as AT-RIS. This decoupling enables coordinated yet independently
configurable designs in the AMAF and T-RIS domains, supporting flexible
strategies with diverse complexity-performance trade-offs. Several
implementations are analyzed, including diagonal and non-diagonal T-RIS
architectures, paired with precoding schemes based on focusing, minimum mean
square error, and eigenmode decomposition. Simulation results demonstrate that
while non-diagonal schemes maximize sum rate in scenarios with a limited number
of User Equipments (UEs) and high angular separability, they exhibit fairness
and scalability limitations as UE density increases. Conversely, diagonal T-RIS
configurations, particularly the proposed focusing-based scheme with uniform
feeder-side power allocation, offer robust, fair, and scalable performance with
minimal channel state information. The findings emphasize the critical impact
of UEs' angular separability and reveal inherent trade-offs among spectral
efficiency, complexity, and fairness, positioning diagonal AT-RIS architectures
as practical solutions for scalable near-field MU multiple-input single-output
systems.

</details>


### [17] [Improved SINR Approximation for Downlink SDMA-based Networks with Outdated Channel State Information](https://arxiv.org/abs/2508.09020)
*Maria Cecilia Fernández Montefiore,Gustavo González,F. Javier López-Martínez,Fernando Gregorio*

Main category: eess.SP

TL;DR: 本文提出了一种改进的SINR统计模型，用于下行MU-MIMO系统在CSIT不完美时的性能分析，克服了现有方法低估SINR方差的局限性。


<details>
  <summary>Details</summary>
Motivation: 在下一代无线网络中，理解CSIT不完美时MU-MIMO系统的性能是关键挑战，准确的SINR统计建模对多用户系统性能分析至关重要。

Method: 提出了一种改进的SINR统计近似模型，保留了现有方法（如基于Gamma的近似）的解析简单性，同时克服其局限性。

Result: 在RSMA支持的MIMO下行系统中验证了模型的准确性，适用于不同用户数、天线数和CSIT过时程度。

Conclusion: 改进的SINR模型在多种系统配置下表现出色，为CSIT不完美的MU-MIMO系统提供了更准确的性能分析工具。

Abstract: Understanding the performance of multi-user multiple-input multiple-output
(MU-MIMO) systems under imperfect channel state information at the transmitter
(CSIT) remains a critical challenge in next-generation wireless networks. In
this context, accurate statistical modeling of the
signal-to-interference-plus-noise ratio (SINR) is essential for enabling
tractable performance analysis of multi-user systems. This paper presents an
improved statistical approximation of the SINR for downlink (DL) MU-MIMO
systems with imperfect CSIT. The proposed model retains the analytical
simplicity of existing approaches (e.g., Gamma-based approximations) while
overcoming their limitations, particularly the underestimation of SINR
variance. We evaluate the proposed approximation in the context of
Rate-Splitting Multiple Access (RSMA)-enabled MIMO DL systems with outdated
CSIT. The results demonstrate excellent accuracy across a wide range of system
configurations, including varying numbers of users, antennas, and degrees of
CSIT staleness.

</details>


### [18] [Chartwin: a Case Study on Channel Charting-aided Localization in Dynamic Digital Network Twins](https://arxiv.org/abs/2508.09055)
*Lorenzo Cazzella,Francesco Linsalata,Mahdi Maleki,Damiano Badini,Matteo Matteucci,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 本文提出了一种名为Chartwin的方法，将定位导向的信道图表与动态数字网络孪生（DNT）结合，展示了半监督信道图表在构建空间一致性图表中的显著性能。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统需要空间一致的信道表示以高效完成通信任务，信道图表作为一种无监督学习技术被引入以实现这一目标。

Method: 提出Chartwin方法，结合定位导向的信道图表与动态DNT，通过半监督学习构建空间一致性图表。

Result: 静态DNT的定位误差约为4.5米，动态DNT约为6米，验证了DNT辅助信道图表和定位的有效性。

Conclusion: Chartwin方法在构建空间一致性图表和提升定位性能方面表现出色，为DNT辅助的信道图表和定位提供了有力支持。

Abstract: Wireless communication systems can significantly benefit from the
availability of spatially consistent representations of the wireless channel to
efficiently perform a wide range of communication tasks. Towards this purpose,
channel charting has been introduced as an effective unsupervised learning
technique to achieve both locally and globally consistent radio maps. In this
letter, we propose Chartwin, a case study on the integration of
localization-oriented channel charting with dynamic Digital Network Twins
(DNTs). Numerical results showcase the significant performance of
semi-supervised channel charting in constructing a spatially consistent chart
of the considered extended urban environment. The considered method results in
$\approx$ 4.5 m localization error for the static DNT and $\approx$ 6 m in the
dynamic DNT, fostering DNT-aided channel charting and localization.

</details>


### [19] [Spectral Efficiency Considerations for 6G](https://arxiv.org/abs/2508.09117)
*Joseph Boccuzzi*

Main category: eess.SP

TL;DR: 本文提出了一种新的系统指标——无线电资源利用效率（RUE），用于量化未来6G需求中无线电资源的效率，并通过比较典型蜂窝和无小区大规模MIMO部署来展示其必要性。


<details>
  <summary>Details</summary>
Motivation: 随着无线连接向6G发展，需要更高效地满足高吞吐量、低延迟和高可靠性的需求，而现有指标（如频谱效率和能量效率）不足以全面衡量资源利用效率。

Method: 引入RUE指标，分析5G无线电资源、实际限制和实施损耗对频谱效率的影响，并通过5G MU-MIMO实测数据验证。

Result: 5G的RUE仅为47%，表明6G有显著改进空间；增加传输带宽（从100MHz到1.6GHz）可带来明显收益。

Conclusion: RUE是衡量6G资源效率的重要指标，未来RAN架构需支持6G和AI-RAN以实现更高效率。

Abstract: As wireless connectivity continues to evolve towards 6G, there is an
ever-increasing demand to not only deliver higher throughput, lower latency,
and improved reliability, but also do so as efficiently as possible. To this
point, the term efficiency has been quantified through applications to Spectral
Efficiency (SE) and Energy Efficiency (EE). In this paper we introduce a new
system metric called Radio Resource Utilization Efficiency (RUE). This metric
quantifies the efficiency of the available radio resources (Spectrum, Access
Method, Time Slots, Data Symbols, etc.) used to deliver future 6G demands. We
compare the system performance of Typical Cellular and Cell-Free Massive MIMO
deployments as a vehicle to demonstrate the need for this new metric. We begin
by providing a concise treatment of items impacting SE by introducing three
categories: 5G Radio Resources, Practical Limitations (such as channel matrix
rank deficiency) and Implementation Losses (SINR degradation). For the example
Radio Access Technology configuration analyzed, we show 5G yields an RUE of 47%
(revealing significant room for improvement when defining 6G). Practical
limitation assumptions are compared to 5G Multi-User MIMO (MU-MIMO)
measurements conducted in a commercialized deployment. SE losses are
characterized to offer guidance to advanced algorithms employing Machine
Learning (ML) based techniques. We present the benefits of increasing the
transmission Bandwidth (BW) from 100MHz to 1.6GHz. We describe a Next
Generation RAN architecture that can support 6G and AI-RAN.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants](https://arxiv.org/abs/2508.08266)
*Ryan Mioduski*

Main category: cs.LG

TL;DR: 该研究评估了大型语言模型（LLMs）在将弗吉尼亚17-18世纪土地专利的文本描述转换为地理坐标时的表现，发现某些模型在准确性和成本效益上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 弗吉尼亚土地专利的文本描述限制了空间分析，因此需要一种高效、准确的方法将其转换为地理坐标。

Method: 研究使用了六种OpenAI模型，分为直接转换和工具增强的链式思考两种范式，并与GIS分析师等基线方法进行比较。

Result: 最佳模型（o3-2025-04-16）平均误差为23公里，优于其他模型和基线方法；成本效益最高的模型（gpt-4o-2024-08-06）平均误差为28公里，每1000份专利成本1.09美元。

Conclusion: LLMs在历史地理坐标转换中具有可扩展性、准确性和成本效益的潜力。

Abstract: Virginia's seventeenth- and eighteenth-century land patents survive primarily
as narrative metes-and-bounds descriptions, limiting spatial analysis. This
study systematically evaluates current-generation large language models (LLMs)
in converting these prose abstracts into geographically accurate
latitude/longitude coordinates within a focused evaluation context. A digitized
corpus of 5,471 Virginia patent abstracts (1695-1732) is released, with 43
rigorously verified test cases serving as an initial, geographically focused
benchmark. Six OpenAI models across three architectures (o-series, GPT-4-class,
and GPT-3.5) were tested under two paradigms: direct-to-coordinate and
tool-augmented chain-of-thought invoking external geocoding APIs. Results were
compared with a GIS-analyst baseline, the Stanford NER geoparser, Mordecai-3,
and a county-centroid heuristic.
  The top single-call model, o3-2025-04-16, achieved a mean error of 23 km
(median 14 km), outperforming the median LLM (37.4 km) by 37.5%, the weakest
LLM (50.3 km) by 53.5%, and external baselines by 67% (GIS analyst) and 70%
(Stanford NER). A five-call ensemble further reduced errors to 19 km (median 12
km) at minimal additional cost (approx. USD 0.20 per grant), outperforming the
median LLM by 48.6%. A patentee-name-redaction ablation increased error by
about 9%, indicating reliance on textual landmark and adjacency descriptions
rather than memorization. The cost-efficient gpt-4o-2024-08-06 model maintained
a 28 km mean error at USD 1.09 per 1,000 grants, establishing a strong
cost-accuracy benchmark; external geocoding tools offered no measurable benefit
in this evaluation.
  These findings demonstrate the potential of LLMs for scalable, accurate, and
cost-effective historical georeferencing.

</details>


### [21] [Doctor Sun: A Bilingual Multimodal Large Language Model for Biomedical AI](https://arxiv.org/abs/2508.08270)
*Dong Xue,Ziyao Shao,Zhaoyang Duan,Fangzhou Liu,Bing Li,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 介绍了Doctor Sun，一种专门用于医学的大型多模态生成模型，旨在解决现有医学多模态AI在理解复杂医学概念和文本-图像关系方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于基础LLM的多模态医学AI难以理解复杂医学概念，且LLaVA诱导的医学LMM在捕捉文本与图像关系方面表现不佳。

Method: Doctor Sun整合预训练视觉编码器和医学LLM，通过两阶段训练（特征对齐和指令调优）处理多种医学数据模态。

Result: 发布了SunMed-VL双语医学多模态数据集及所有相关模型、代码和资源，支持生物医学多模态研究发展。

Conclusion: Doctor Sun为医学多模态任务提供了更有效的解决方案，并推动了相关研究的开源共享。

Abstract: Large multimodal models (LMMs) have demonstrated significant potential in
providing innovative solutions for various biomedical tasks, including
pathology analysis, radiology report generation, and biomedical assistance.
However, the existing multimodal biomedical AI is typically based on foundation
LLMs, thus hindering the understanding of intricate medical concepts with
limited medical training data. Moreover, recent LLaVA-induced medical LMMs
struggle to effectively capture the intricate relationship between the texts
and the images. Therefore, we introduce Doctor Sun, a large multimodal
generative model specialized in medicine, developed to encode, integrate, and
interpret diverse biomedical data modalities such as text and images. In
particular, Doctor Sun integrates a pre-trained vision encoder with a medical
LLM and conducts two-stage training on various medical datasets, focusing on
feature alignment and instruction tuning. Moreover, we release SunMed-VL, a
wide-range bilingual medical multimodal dataset, along with all associated
models, code, and resources, to freely support the advancement of biomedical
multimodal research.

</details>


### [22] [Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport](https://arxiv.org/abs/2508.08369)
*Elon Litman*

Main category: cs.LG

TL;DR: 该论文从基本原理出发，为缩放点积注意力机制（SDPA）提供了理论依据，揭示了其前向传递与熵最优传输问题的联系，以及反向传递与强化学习中优势策略梯度的等价性。


<details>
  <summary>Details</summary>
Motivation: SDPA的数学形式通常由启发式方法驱动，缺乏理论基础。本文旨在从优化角度为其提供第一性原理的证明。

Method: 通过证明SDPA的前向传递是退化单边熵最优传输问题的精确解，并分析其反向传递与优势策略梯度的数学等价性。

Result: 揭示了SDPA前向传递的信息几何特性，并证明反向传递的学习梯度形式由该几何特性决定。

Conclusion: SDPA是一个理论完备的机制，前向传递实现最优推断，反向传递执行基于流形的学习更新。

Abstract: The scaled-dot-product attention (SDPA) mechanism is a core component of
modern deep learning, but its mathematical form is often motivated by
heuristics. This work provides a first-principles justification for SDPA. We
first show that the attention forward pass is the exact solution to a
degenerate, one-sided Entropic Optimal Transport (EOT) problem, which seeks a
distribution that maximizes similarity while being maximally entropic. This
optimization perspective has a direct consequence for the backward pass. We
prove that the standard gradient computed via backpropagation is mathematically
identical to an advantage-based policy gradient, a variance-reduced update rule
from reinforcement learning. Crucially, we demonstrate that the EOT formulation
of the forward pass induces a specific information geometry on the space of
attention distributions. It is this geometry, characterized by the Fisher
Information Matrix, that dictates the precise form of the learning gradient,
revealing the advantage-based update as a natural consequence of the
optimization problem being solved. This unified view reveals SDPA as a
principled mechanism where the forward pass performs optimal inference and the
backward pass implements a rational, manifold-aware learning update.

</details>


### [23] [Towards Heterogeneity-Aware and Energy-Efficient Topology Optimization for Decentralized Federated Learning in Edge Environment](https://arxiv.org/abs/2508.08278)
*Yuze Liu,Tiehua Zhang,Zhishu Shen,Libing Wu,Shiping Chen,Jiong Jin*

Main category: cs.LG

TL;DR: Hat-DFed是一个针对边缘计算系统的异构感知和成本高效的分散式联邦学习框架，通过动态优化通信拓扑和重要性感知模型聚合，解决模型性能和能耗问题。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中分散式联邦学习（DFL）面临通信瓶颈、资源异构性和数据异构性等挑战，影响模型性能和能耗效率。

Method: 提出Hat-DFed框架，将拓扑构建建模为双优化问题，设计两阶段算法动态优化拓扑，并引入重要性感知模型聚合机制。

Result: Hat-DFed在复杂边缘环境中最大化模型性能的同时最小化累积能耗，解决了NP-hard问题。

Conclusion: Hat-DFed为边缘计算中的DFL提供了一种高效解决方案，平衡了性能和能耗。

Abstract: Federated learning (FL) has emerged as a promising paradigm within edge
computing (EC) systems, enabling numerous edge devices to collaboratively train
artificial intelligence (AI) models while maintaining data privacy. To overcome
the communication bottlenecks associated with centralized parameter servers,
decentralized federated learning (DFL), which leverages peer-to-peer (P2P)
communication, has been extensively explored in the research community.
Although researchers design a variety of DFL approach to ensure model
convergence, its iterative learning process inevitably incurs considerable cost
along with the growth of model complexity and the number of participants. These
costs are largely influenced by the dynamic changes of topology in each
training round, particularly its sparsity and connectivity conditions.
Furthermore, the inherent resources heterogeneity in the edge environments
affects energy efficiency of learning process, while data heterogeneity
degrades model performance. These factors pose significant challenges to the
design of an effective DFL framework for EC systems. To this end, we propose
Hat-DFed, a heterogeneity-aware and coset-effective decentralized federated
learning (DFL) framework. In Hat-DFed, the topology construction is formulated
as a dual optimization problem, which is then proven to be NP-hard, with the
goal of maximizing model performance while minimizing cumulative energy
consumption in complex edge environments. To solve this problem, we design a
two-phase algorithm that dynamically constructs optimal communication
topologies while unbiasedly estimating their impact on both model performance
and energy cost. Additionally, the algorithm incorporates an importance-aware
model aggregation mechanism to mitigate performance degradation caused by data
heterogeneity.

</details>


### [24] [Regret minimization in Linear Bandits with offline data via extended D-optimal exploration](https://arxiv.org/abs/2508.08420)
*Sushant Vijayan,Arun Suggala,Karthikeyan VS,Soumyabrata Pal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of online regret minimization in linear bandits with
access to prior observations (offline data) from the underlying bandit model.
There are numerous applications where extensive offline data is often
available, such as in recommendation systems, online advertising. Consequently,
this problem has been studied intensively in recent literature. Our algorithm,
Offline-Online Phased Elimination (OOPE), effectively incorporates the offline
data to substantially reduce the online regret compared to prior work. To
leverage offline information prudently, OOPE uses an extended D-optimal design
within each exploration phase. OOPE achieves an online regret is
$\tilde{O}(\sqrt{\deff T \log \left(|\mathcal{A}|T\right)}+d^2)$. $\deff \leq
d)$ is the effective problem dimension which measures the number of poorly
explored directions in offline data and depends on the eigen-spectrum
$(\lambda_k)_{k \in [d]}$ of the Gram matrix of the offline data. The
eigen-spectrum $(\lambda_k)_{k \in [d]}$ is a quantitative measure of the
\emph{quality} of offline data. If the offline data is poorly explored ($\deff
\approx d$), we recover the established regret bounds for purely online setting
while, when offline data is abundant ($\Toff >> T$) and well-explored ($\deff =
o(1) $), the online regret reduces substantially. Additionally, we provide the
first known minimax regret lower bounds in this setting that depend explicitly
on the quality of the offline data. These lower bounds establish the optimality
of our algorithm in regimes where offline data is either well-explored or
poorly explored. Finally, by using a Frank-Wolfe approximation to the extended
optimal design we further improve the $O(d^{2})$ term to
$O\left(\frac{d^{2}}{\deff} \min \{ \deff,1\} \right)$, which can be
substantial in high dimensions with moderate quality of offline data $\deff =
\Omega(1)$.

</details>


### [25] [XFMNet: Decoding Cross-Site and Nonstationary Water Patterns via Stepwise Multimodal Fusion for Long-Term Water Quality Forecasting](https://arxiv.org/abs/2508.08279)
*Ziqi Wang,Hailiang Zhao,Cheng Bao,Wenzhuo Qian,Yuhao Yang,Xueqiang Sun,Shuiguang Deng*

Main category: cs.LG

TL;DR: XFMNet是一种逐步多模态融合网络，用于解决多站点水质预测中的时空动态建模问题，通过整合遥感降水图像和自适应分解技术，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 水质预测因复杂的周期性、非平稳性和生态因素引起的突变波动而具有挑战性，尤其是在多站点场景中需要同时建模时空动态。

Method: XFMNet通过自适应降采样对齐时间分辨率，局部自适应分解分离趋势和周期成分，并利用跨注意力门控融合模块动态整合时空和生态信息。

Result: 在真实数据集上的实验表明，XFMNet显著优于现有基线方法。

Conclusion: XFMNet通过逐步递归融合技术，有效捕捉长期趋势和短期波动，适用于分布式时间序列预测。

Abstract: Long-term time-series forecasting is critical for environmental monitoring,
yet water quality prediction remains challenging due to complex periodicity,
nonstationarity, and abrupt fluctuations induced by ecological factors. These
challenges are further amplified in multi-site scenarios that require
simultaneous modeling of temporal and spatial dynamics. To tackle this, we
introduce XFMNet, a stepwise multimodal fusion network that integrates remote
sensing precipitation imagery to provide spatial and environmental context in
river networks. XFMNet first aligns temporal resolutions between water quality
series and remote sensing inputs via adaptive downsampling, followed by locally
adaptive decomposition to disentangle trend and cycle components. A
cross-attention gated fusion module dynamically integrates temporal patterns
with spatial and ecological cues, enhancing robustness to nonstationarity and
site-specific anomalies. Through progressive and recursive fusion, XFMNet
captures both long-term trends and short-term fluctuations. Extensive
experiments on real-world datasets demonstrate substantial improvements over
state-of-the-art baselines, highlighting the effectiveness of XFMNet for
spatially distributed time series prediction.

</details>


### [26] [Differentiable Cyclic Causal Discovery Under Unmeasured Confounders](https://arxiv.org/abs/2508.08450)
*Muralikrishnna G. Sethuraman,Faramarz Fekri*

Main category: cs.LG

TL;DR: DCCD-CONF是一种新框架，用于在存在未测量混杂因素的情况下，通过干预数据学习非线性循环因果图。


<details>
  <summary>Details</summary>
Motivation: 理解变量间的因果关系是科学研究的核心，但现有方法通常假设所有变量可观测且因果图无环，这在现实中常不成立。

Method: DCCD-CONF通过交替优化图结构和混杂因素分布，最大化数据对数似然。

Result: 在合成数据和真实基因扰动数据集上，DCCD-CONF在因果图恢复和混杂因素识别方面优于现有方法。

Conclusion: DCCD-CONF不仅性能优越，还提供了理论一致性保证。

Abstract: Understanding causal relationships between variables is fundamental across
scientific disciplines. Most causal discovery algorithms rely on two key
assumptions: (i) all variables are observed, and (ii) the underlying causal
graph is acyclic. While these assumptions simplify theoretical analysis, they
are often violated in real-world systems, such as biological networks. Existing
methods that account for confounders either assume linearity or struggle with
scalability. To address these limitations, we propose DCCD-CONF, a novel
framework for differentiable learning of nonlinear cyclic causal graphs in the
presence of unmeasured confounders using interventional data. Our approach
alternates between optimizing the graph structure and estimating the confounder
distribution by maximizing the log-likelihood of the data. Through experiments
on synthetic data and real-world gene perturbation datasets, we show that
DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery
and confounder identification. Additionally, we also provide consistency
guarantees for our framework, reinforcing its theoretical soundness.

</details>


### [27] [MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder](https://arxiv.org/abs/2508.08280)
*Seonyoung Kim,Dongil Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为MoSSDA的新型半监督域自适应框架，用于多变量时间序列分类，通过两步动量编码器和混合增强对比模块实现域不变和类别区分性表示。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在训练和测试数据分布不同（域偏移）时性能下降的问题，特别是在时间序列数据中。

Method: 采用两步动量编码器框架，包括域不变编码器和混合增强对比模块，通过两阶段学习分离梯度流。

Result: 在六个数据集上实现了最先进的性能，并通过消融研究验证了各模块的有效性。

Conclusion: MoSSDA框架在多变量时间序列分类中表现出色，为半监督域自适应提供了有效解决方案。

Abstract: Deep learning has emerged as the most promising approach in various fields;
however, when the distributions of training and test data are different (domain
shift), the performance of deep learning models can degrade. Semi-supervised
domain adaptation (SSDA) is a major approach for addressing this issue,
assuming that a fully labeled training set (source domain) is available, but
the test set (target domain) provides labels only for a small subset. In this
study, we propose a novel two-step momentum encoder-utilized SSDA framework,
MoSSDA, for multivariate time-series classification. Time series data are
highly sensitive to noise, and sequential dependencies cause domain shifts
resulting in critical performance degradation. To obtain a robust,
domain-invariant and class-discriminative representation, MoSSDA employs a
domain-invariant encoder to learn features from both source and target domains.
Subsequently, the learned features are fed to a mixup-enhanced positive
contrastive module consisting of an online momentum encoder. The final
classifier is trained with learned features that exhibit consistency and
discriminability with limited labeled target domain data, without data
augmentation. We applied a two-stage process by separating the gradient flow
between the encoders and the classifier to obtain rich and complex
representations. Through extensive experiments on six diverse datasets, MoSSDA
achieved state-of-the-art performance for three different backbones and various
unlabeled ratios in the target domain data. The Ablation study confirms that
each module, including two-stage learning, is effective in improving the
performance. Our code is available at https://github.com/seonyoungKimm/MoSSDA

</details>


### [28] [Distributed optimization: designed for federated learning](https://arxiv.org/abs/2508.08606)
*Wenyou Guo,Ting Qu,Chunrong Pan,George Q. Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于增广拉格朗日技术的分布式优化算法，适用于联邦学习中的集中式和去中心化通信拓扑，并通过理论保证和实验验证了其高效性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护约束下的跨组织数据协作中受到关注，但现有方法在通信拓扑和计算效率方面存在局限。

Method: 提出基于增广拉格朗日技术的分布式优化算法，结合近端松弛和二次近似，涵盖多种经典无约束优化方法。

Result: 数值实验表明，算法在大规模统计异构场景下表现优异。

Conclusion: 该框架不仅统一了多种优化方法，还提供了收敛性理论支持，适用于实际联邦学习场景。

Abstract: Federated Learning (FL), as a distributed collaborative Machine Learning (ML)
framework under privacy-preserving constraints, has garnered increasing
research attention in cross-organizational data collaboration scenarios. This
paper proposes a class of distributed optimization algorithms based on the
augmented Lagrangian technique, designed to accommodate diverse communication
topologies in both centralized and decentralized FL settings. Furthermore, we
develop multiple termination criteria and parameter update mechanisms to
enhance computational efficiency, accompanied by rigorous theoretical
guarantees of convergence. By generalizing the augmented Lagrangian relaxation
through the incorporation of proximal relaxation and quadratic approximation,
our framework systematically recovers a broad of classical unconstrained
optimization methods, including proximal algorithm, classic gradient descent,
and stochastic gradient descent, among others. Notably, the convergence
properties of these methods can be naturally derived within the proposed
theoretical framework. Numerical experiments demonstrate that the proposed
algorithm exhibits strong performance in large-scale settings with significant
statistical heterogeneity across clients.

</details>


### [29] [Multi-grained spatial-temporal feature complementarity for accurate online cellular traffic prediction](https://arxiv.org/abs/2508.08281)
*Ningning Fu,Shengheng Liu,Weiliang Xie,Yongming Huang*

Main category: cs.LG

TL;DR: 提出了一种基于多粒度时空特征互补（MGSTC）的在线蜂窝流量预测方法，解决了现有研究忽略流量突发性和概念漂移的问题。


<details>
  <summary>Details</summary>
Motivation: 电信数据挖掘有助于优化资源分配，但现有方法依赖人工且忽视流量突发性和概念漂移。

Method: MGSTC通过粗粒度时间注意力和细粒度空间注意力捕捉多粒度特征，并采用在线学习策略适应概念漂移。

Result: 在四个真实数据集上，MGSTC优于11种先进基线方法。

Conclusion: MGSTC在连续预测任务中表现优异，适用于实际电信场景。

Abstract: Knowledge discovered from telecom data can facilitate proactive understanding
of network dynamics and user behaviors, which in turn empowers service
providers to optimize cellular traffic scheduling and resource allocation.
Nevertheless, the telecom industry still heavily relies on manual expert
intervention. Existing studies have been focused on exhaustively explore the
spatial-temporal correlations. However, they often overlook the underlying
characteristics of cellular traffic, which are shaped by the sporadic and
bursty nature of telecom services. Additionally, concept drift creates
substantial obstacles to maintaining satisfactory accuracy in continuous
cellular forecasting tasks. To resolve these problems, we put forward an online
cellular traffic prediction method grounded in Multi-Grained Spatial-Temporal
feature Complementarity (MGSTC). The proposed method is devised to achieve
high-precision predictions in practical continuous forecasting scenarios.
Concretely, MGSTC segments historical data into chunks and employs the
coarse-grained temporal attention to offer a trend reference for the prediction
horizon. Subsequently, fine-grained spatial attention is utilized to capture
detailed correlations among network elements, which enables localized
refinement of the established trend. The complementarity of these multi-grained
spatial-temporal features facilitates the efficient transmission of valuable
information. To accommodate continuous forecasting needs, we implement an
online learning strategy that can detect concept drift in real-time and
promptly switch to the appropriate parameter update stage. Experiments carried
out on four real-world datasets demonstrate that MGSTC outperforms eleven
state-of-the-art baselines consistently.

</details>


### [30] [Understanding Transformers through the Lens of Pavlovian Conditioning](https://arxiv.org/abs/2508.08289)
*Mu Qiao*

Main category: cs.LG

TL;DR: 论文提出了一种将Transformer注意力机制重新解释为巴甫洛夫条件反射的理论框架，揭示了其与线性注意力的数学类比，并分析了注意力操作的动态关联记忆特性。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer注意力机制成功的计算原理，揭示其与生物学中巴甫洛夫条件反射的深层联系。

Method: 通过将注意力的查询、键和值映射为经典条件反射的三个要素（测试刺激、条件刺激和无条件刺激），建立线性注意力模型，分析其动态关联记忆特性。

Result: 提出了容量定理、误差传播分析和生物学启发学习规则的见解，表明注意力头可存储O($\sqrt{d_k}$)关联，并揭示了模型深度、宽度和冗余的权衡。

Conclusion: 现代AI的成功可能源于实现生物学优化的计算原理，而不仅仅是架构创新。

Abstract: Transformer architectures have revolutionized artificial intelligence (AI)
through their attention mechanisms, yet the computational principles underlying
their success remain opaque. We present a novel theoretical framework that
reinterprets the core computation of attention as Pavlovian conditioning. Our
model finds a direct mathematical analogue in linear attention, which
simplifies the analysis of the underlying associative process. We demonstrate
that attention's queries, keys, and values can be mapped to the three elements
of classical conditioning: test stimuli that probe associations, conditional
stimuli (CS) that serve as retrieval cues, and unconditional stimuli (US) that
contain response information. Through this lens, we suggest that each attention
operation constructs a transient associative memory via a Hebbian rule, where
CS-US pairs form dynamic associations that test stimuli can later retrieve. Our
framework yields several theoretical insights grounded in this linearized
model: (1) a capacity theorem showing that attention heads can store
O($\sqrt{d_k}$) associations before interference degrades retrieval; (2) an
error propagation analysis revealing fundamental architectural trade-offs of
balancing model depth, width, and head redundancy to maintain reliability; and
(3) an understanding of how biologically plausible learning rules could enhance
transformer architectures. By establishing this deep connection, we suggest
that the success of modern AI may stem not from architectural novelty alone,
but from implementing computational principles that biology optimized over
millions of years of evolution.

</details>


### [31] [Hi-fi functional priors by learning activations](https://arxiv.org/abs/2508.08880)
*Marcin Sendera,Amin Sorkhei,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: 论文探讨了如何在贝叶斯神经网络（BNNs）中通过可训练激活函数实现函数空间先验，以增强正则化、不确定性量化和风险感知决策。


<details>
  <summary>Details</summary>
Motivation: 函数空间先验能更直观地将信念嵌入模型输出，但将其应用于BNNs具有挑战性。

Method: 通过优化技术探索可训练激活函数（如Pade函数和分段线性函数）如何适应高复杂度先验并匹配目标函数分布。

Result: 实验表明，即使单层宽隐层的BNNs，配备灵活可训练激活函数，也能有效实现目标函数空间先验。

Conclusion: 灵活的可训练激活函数为BNNs实现函数空间先验提供了可行路径。

Abstract: Function-space priors in Bayesian Neural Networks (BNNs) provide a more
intuitive approach to embedding beliefs directly into the model's output,
thereby enhancing regularization, uncertainty quantification, and risk-aware
decision-making. However, imposing function-space priors on BNNs is
challenging. We address this task through optimization techniques that explore
how trainable activations can accommodate higher-complexity priors and match
intricate target function distributions. We investigate flexible activation
models, including Pade functions and piecewise linear functions, and discuss
the learning challenges related to identifiability, loss construction, and
symmetries. Our empirical findings indicate that even BNNs with a single wide
hidden layer when equipped with flexible trainable activation, can effectively
achieve desired function-space priors.

</details>


### [32] [Probabilistic Emissivity Retrieval from Hyperspectral Data via Physics-Guided Variational Inference](https://arxiv.org/abs/2508.08291)
*Joshua R. Tempelman,Kevin Mitchell,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: 提出了一种基于物理条件的生成模型，用于高光谱成像（HSI）目标识别，解决了传统深度学习方法在可解释性和材料预测范围上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在HSI目标识别中仅提供单一材料类别预测，且依赖训练库中的材料，限制了其可解释性和适用性。

Method: 采用概率潜在变量模型学习HSI辐射测量的分布，结合大气和背景估计作为物理条件，通过增强方案和物理损失标准避免训练偏差。

Result: 模型能生成发射率分布，并提供不确定性量化，同时通过基于分布的材料匹配方案返回可能的材料匹配。

Conclusion: 该方法结合了场景上下文信息，捕捉材料光谱变化，并提供可解释的概率度量，提升了HSI目标识别的灵活性和可靠性。

Abstract: Recent research has proven neural networks to be a powerful tool for
performing hyperspectral imaging (HSI) target identification. However, many
deep learning frameworks deliver a single material class prediction and operate
on a per-pixel basis; such approaches are limited in their interpretability and
restricted to predicting materials that are accessible in available training
libraries. In this work, we present an inverse modeling approach in the form of
a physics-conditioned generative model.A probabilistic latent-variable model
learns the underlying distribution of HSI radiance measurements and produces
the conditional distribution of the emissivity spectrum. Moreover, estimates of
the HSI scene's atmosphere and background are used as a physically relevant
conditioning mechanism to contextualize a given radiance measurement during the
encoding and decoding processes. Furthermore, we employ an in-the-loop
augmentation scheme and physics-based loss criteria to avoid bias towards a
predefined training material set and to encourage the model to learn physically
consistent inverse mappings. Monte-Carlo sampling of the model's conditioned
posterior delivers a sought emissivity distribution and allows for
interpretable uncertainty quantification. Moreover, a distribution-based
material matching scheme is presented to return a set of likely material
matches for an inferred emissivity distribution. Hence, we present a strategy
to incorporate contextual information about a given HSI scene, capture the
possible variation of underlying material spectra, and provide interpretable
probability measures of a candidate material accounting for given
remotely-sensed radiance measurement.

</details>


### [33] [Position: Causal Machine Learning Requires Rigorous Synthetic Experiments for Broader Adoption](https://arxiv.org/abs/2508.08883)
*Audrey Poinsot,Panayiotis Panayiotou,Alessandro Leite,Nicolas Chesneau,Özgür Şimşek,Marc Schoenauer*

Main category: cs.LG

TL;DR: 论文主张合成实验对评估因果机器学习方法至关重要，并提出了一套严谨的合成数据评估原则。


<details>
  <summary>Details</summary>
Motivation: 因果机器学习方法因缺乏可靠的评估实践而未被广泛采用，作者认为合成实验是解决这一问题的关键。

Method: 批判性回顾当前评估实践的不足，并提出一套基于合成数据的严谨评估原则。

Result: 通过合成实验可以更精确地评估因果机器学习方法，增强其可靠性和实用性。

Conclusion: 采用提出的评估原则将推动因果机器学习方法的广泛应用和实际影响。

Abstract: Causal machine learning has the potential to revolutionize decision-making by
combining the predictive power of machine learning algorithms with the theory
of causal inference. However, these methods remain underutilized by the broader
machine learning community, in part because current empirical evaluations do
not permit assessment of their reliability and robustness, undermining their
practical utility. Specifically, one of the principal criticisms made by the
community is the extensive use of synthetic experiments. We argue, on the
contrary, that synthetic experiments are essential and necessary to precisely
assess and understand the capabilities of causal machine learning methods. To
substantiate our position, we critically review the current evaluation
practices, spotlight their shortcomings, and propose a set of principles for
conducting rigorous empirical analyses with synthetic data. Adopting the
proposed principles will enable comprehensive evaluations that build trust in
causal machine learning methods, driving their broader adoption and impactful
real-world use.

</details>


### [34] [Channel-Wise MLPs Improve the Generalization of Recurrent Convolutional Networks](https://arxiv.org/abs/2508.08298)
*Nathan Breslow*

Main category: cs.LG

TL;DR: DAMP（带有MLP的通道混合）在泛化能力上显著优于DARC（简单循环卷积结构）。


<details>
  <summary>Details</summary>
Motivation: 研究通道混合通过MLP对循环卷积网络泛化能力的影响。

Method: 比较DARC和DAMP两种架构，使用Re-ARC基准测试。

Result: DAMP在分布内和分布外泛化中表现更优。

Conclusion: MLP的通道混合使网络学习更鲁棒和可泛化的计算模式，DAMP适合超网络方法。

Abstract: We investigate the impact of channel-wise mixing via multi-layer perceptrons
(MLPs) on the generalization capabilities of recurrent convolutional networks.
Specifically, we compare two architectures: DARC (Depth Aware Recurrent
Convolution), which employs a simple recurrent convolutional structure, and
DAMP (Depth Aware Multi-layer Perceptron), which extends DARC with a gated MLP
for channel mixing. Using the Re-ARC benchmark, we find that DAMP significantly
outperforms DARC in both in-distribution and out-of-distribution generalization
under exact-match grading criteria. These results suggest that explicit channel
mixing through MLPs enables recurrent convolutional networks to learn more
robust and generalizable computational patterns. Our findings have implications
for neural program synthesis and highlight the potential of DAMP as a target
architecture for hypernetwork approaches.

</details>


### [35] [Integrating attention into explanation frameworks for language and vision transformers](https://arxiv.org/abs/2508.08966)
*Marte Eggen,Jacob Lysnæs-Larsen,Inga Strümke*

Main category: cs.LG

TL;DR: 论文研究了如何利用注意力权重为Transformer模型提供有意义的解释，提出了两种新方法，分别用于局部和全局解释，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是Transformer架构的核心，但其权重并未直接决定模型输出。研究旨在探索如何利用这些权重信息，补充现有可解释性技术，以提供更丰富的模型解释。

Method: 提出了两种新方法：1) 将注意力权重整合到Shapley值分解中，重新定义特征函数以提供局部解释；2) 将注意力权重与概念激活向量结合，测量概念敏感性以提供全局解释。

Result: 实验表明，注意力权重可以有效地整合到可解释性AI框架中，丰富了Transformer模型的解释能力。

Conclusion: 注意力权重能够为Transformer模型提供有价值的解释信息，补充了现有的可解释性技术。

Abstract: The attention mechanism lies at the core of the transformer architecture,
providing an interpretable model-internal signal that has motivated a growing
interest in attention-based model explanations. Although attention weights do
not directly determine model outputs, they reflect patterns of token influence
that can inform and complement established explainability techniques. This work
studies the potential of utilising the information encoded in attention weights
to provide meaningful model explanations by integrating them into explainable
AI (XAI) frameworks that target fundamentally different aspects of model
behaviour. To this end, we develop two novel explanation methods applicable to
both natural language processing and computer vision tasks. The first
integrates attention weights into the Shapley value decomposition by redefining
the characteristic function in terms of pairwise token interactions via
attention weights, thus adapting this widely used game-theoretic solution
concept to provide attention-driven attributions for local explanations. The
second incorporates attention weights into token-level directional derivatives
defined through concept activation vectors to measure concept sensitivity for
global explanations. Our empirical evaluations on standard benchmarks and in a
comparison study with widely used explanation methods show that attention
weights can be meaningfully incorporated into the studied XAI frameworks,
highlighting their value in enriching transformer explainability.

</details>


### [36] [Comparative study of machine learning and statistical methods for automatic identification and quantification in γ-ray spectrometry](https://arxiv.org/abs/2508.08306)
*Dinh Triem Phan,Jérôme Bobin,Cheick Thiam,Christophe Bobin*

Main category: cs.LG

TL;DR: 论文提出了一个开源的基准测试，用于比较γ射线谱分析中的统计方法和端到端机器学习方法，发现统计方法在已知或建模良好的光谱特征下表现更优，而机器学习在不确定条件下是更好的选择。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏通用的基准测试（包括数据集、代码和比较指标），导致γ射线谱分析方法的评估和比较困难。

Method: 提出了一个包含模拟数据集、不同分析方法和评估指标的开源基准测试，比较了统计方法和机器学习方法在三种场景下的表现。

Result: 统计方法在所有场景和指标中表现优于机器学习方法，但在光谱特征建模不准确时性能显著下降。机器学习在不确定条件下表现更好。

Conclusion: 统计方法适用于已知或建模良好的光谱特征，而机器学习在不确定条件下是更好的选择；统计方法在定量任务中更准确。

Abstract: During the last decade, a large number of different numerical methods have
been proposed to tackle the automatic identification and quantification in
{\gamma}-ray spectrometry. However, the lack of common benchmarks, including
datasets, code and comparison metrics, makes their evaluation and comparison
hard. In that context, we propose an open-source benchmark that comprises
simulated datasets of various {\gamma}-spectrometry settings, codes of
different analysis approaches and evaluation metrics. This allows us to compare
the state-of-the-art end-to-end machine learning with a statistical unmixing
approach using the full spectrum. Three scenarios have been investigated: (1)
spectral signatures are assumed to be known; (2) spectral signatures are
deformed due to physical phenomena such as Compton scattering and attenuation;
and (3) spectral signatures are shifted (e.g., due to temperature variation). A
large dataset of 200000 simulated spectra containing nine radionuclides with an
experimental natural background is used for each scenario with multiple
radionuclides present in the spectrum. Regarding identification performance,
the statistical approach consistently outperforms the machine learning
approaches across all three scenarios for all comparison metrics. However, the
performance of the statistical approach can be significantly impacted when
spectral signatures are not modeled correctly. Consequently, the full-spectrum
statistical approach is most effective with known or well-modeled spectral
signatures, while end-to-end machine learning is a good alternative when
measurement conditions are uncertain for radionuclide identification.
Concerning the quantification task, the statistical approach provides accurate
estimates of radionuclide counting, while the machine learning methods deliver
less satisfactory results.

</details>


### [37] [Scaling Up Active Testing to Large Language Models](https://arxiv.org/abs/2508.09093)
*Gabrielle Berrada,Jannik Kossen,Muhammed Razzak,Freddie Bickford Smith,Yarin Gal,Tom Rainforth*

Main category: cs.LG

TL;DR: 通过主动测试高效评估大语言模型（LLM），利用上下文学习构建低成本代理模型，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决主动测试在大模型评估中计算成本高的问题，提高标签效率。

Method: 使用上下文学习构建代理模型，无需在测试循环中更新，甚至无需目标模型预测。

Result: 方法能更高效地评估LLM性能，减少数据需求。

Conclusion: 主动测试可扩展至大模型评估，显著提升效率。

Abstract: Active testing enables label-efficient evaluation of models through careful
data acquisition. However, its significant computational costs have previously
undermined its use for large models. We show how it can be successfully scaled
up to the evaluation of large language models (LLMs). In particular we show
that the surrogate model used to guide data acquisition can be constructed
cheaply using in-context learning, does not require updating within an
active-testing loop, and can be smaller than the target model. We even find we
can make good data-acquisition decisions without computing predictions with the
target model and further introduce a single-run error estimator to asses how
well active testing is working on the fly. We find that our approach is able to
more effectively evaluate LLM performance with less data than current standard
practices.

</details>


### [38] [Weather-Driven Agricultural Decision-Making Using Digital Twins Under Imperfect Conditions](https://arxiv.org/abs/2508.08326)
*Tamim Ahmed,Monowar Hasan*

Main category: cs.LG

TL;DR: 数字孪生技术通过实时虚拟表示物理系统，提升数字农业中的数据驱动决策。研究开发了模块化框架Cerealia，用于检测农业天气数据不一致性，支持用户决策。


<details>
  <summary>Details</summary>
Motivation: 农业天气数据的不一致性影响决策和自动化任务，需要一种方法来检测和解决这些问题。

Method: 开发模块化框架Cerealia，利用神经网络模型检测异常，并在NVIDIA Jetson Orin平台上测试。

Result: Cerealia在商业果园和公开天气数据集中成功检测到数据不一致性。

Conclusion: 数字孪生技术和Cerealia框架能有效提升农业数据质量和决策支持。

Abstract: By offering a dynamic, real-time virtual representation of physical systems,
digital twin technology can enhance data-driven decision-making in digital
agriculture. Our research shows how digital twins are useful for detecting
inconsistencies in agricultural weather data measurements, which are key
attributes for various agricultural decision-making and automation tasks. We
develop a modular framework named Cerealia that allows end-users to check for
data inconsistencies when perfect weather feeds are unavailable. Cerealia uses
neural network models to check anomalies and aids end-users in informed
decision-making. We develop a prototype of Cerealia using the NVIDIA Jetson
Orin platform and test it with an operational weather network established in a
commercial orchard as well as publicly available weather datasets.

</details>


### [39] [HSA-Net: Hierarchical and Structure-Aware Framework for Efficient and Scalable Molecular Language Modeling](https://arxiv.org/abs/2508.08334)
*Zihang Shao,Wentao Lei,Lei Wang,Wencai Ye,Li Liu*

Main category: cs.LG

TL;DR: 论文提出HSA-Net框架，通过分层特征投影和融合解决GNN中的全局-局部权衡问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GNN在深度层存在过平滑问题，现有方法在深层特征表现不佳，Mamba虽能保留全局信息但忽略浅层细节。

Method: HSA-Net包含分层自适应投影器（HAP）和源感知融合（SAF）模块，动态选择投影方式并智能融合特征。

Result: 实验表明HSA-Net在定量和定性上均优于当前最先进方法。

Conclusion: HSA-Net有效解决了全局-局部权衡问题，为分子表示学习提供了新思路。

Abstract: Molecular representation learning, a cornerstone for downstream tasks like
molecular captioning and molecular property prediction, heavily relies on Graph
Neural Networks (GNN). However, GNN suffers from the over-smoothing problem,
where node-level features collapse in deep GNN layers. While existing feature
projection methods with cross-attention have been introduced to mitigate this
issue, they still perform poorly in deep features. This motivated our
exploration of using Mamba as an alternative projector for its ability to
handle complex sequences. However, we observe that while Mamba excels at
preserving global topological information from deep layers, it neglects
fine-grained details in shallow layers. The capabilities of Mamba and
cross-attention exhibit a global-local trade-off. To resolve this critical
global-local trade-off, we propose Hierarchical and Structure-Aware Network
(HSA-Net), a novel framework with two modules that enables a hierarchical
feature projection and fusion. Firstly, a Hierarchical Adaptive Projector (HAP)
module is introduced to process features from different graph layers. It learns
to dynamically switch between a cross-attention projector for shallow layers
and a structure-aware Graph-Mamba projector for deep layers, producing
high-quality, multi-level features. Secondly, to adaptively merge these
multi-level features, we design a Source-Aware Fusion (SAF) module, which
flexibly selects fusion experts based on the characteristics of the aggregation
features, ensuring a precise and effective final representation fusion.
Extensive experiments demonstrate that our HSA-Net framework quantitatively and
qualitatively outperforms current state-of-the-art (SOTA) methods.

</details>


### [40] [SHeRL-FL: When Representation Learning Meets Split Learning in Hierarchical Federated Learning](https://arxiv.org/abs/2508.08339)
*Dung T. Tran,Nguyen B. Ha,Van-Dinh Nguyen,Kok-Seng Wong*

Main category: cs.LG

TL;DR: SHeRL-FL整合了分层联邦学习和分割学习，通过中间层表示学习减少通信开销和协调复杂性，实验证明其在图像分类和分割任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的计算异构性、通信成本高和模型聚合复杂的问题，同时减轻资源受限设备的训练负担。

Method: 结合分割学习和分层模型聚合，引入中间层表示学习，使客户端和边缘服务器独立于云端计算训练目标。

Result: 在IID和非IID设置下，SHeRL-FL比集中式联邦学习和分层联邦学习减少90%数据传输，比SplitFed减少50%，并提升性能。

Conclusion: SHeRL-FL有效降低了通信开销和协调复杂性，同时提高了训练效率，适用于大规模网络中的联邦学习任务。

Abstract: Federated learning (FL) is a promising approach for addressing scalability
and latency issues in large-scale networks by enabling collaborative model
training without requiring the sharing of raw data. However, existing FL
frameworks often overlook the computational heterogeneity of edge clients and
the growing training burden on resource-limited devices. However, FL suffers
from high communication costs and complex model aggregation, especially with
large models. Previous works combine split learning (SL) and hierarchical FL
(HierFL) to reduce device-side computation and improve scalability, but this
introduces training complexity due to coordination across tiers. To address
these issues, we propose SHeRL-FL, which integrates SL and hierarchical model
aggregation and incorporates representation learning at intermediate layers. By
allowing clients and edge servers to compute training objectives independently
of the cloud, SHeRL-FL significantly reduces both coordination complexity and
communication overhead. To evaluate the effectiveness and efficiency of
SHeRL-FL, we performed experiments on image classification tasks using
CIFAR-10, CIFAR-100, and HAM10000 with AlexNet, ResNet-18, and ResNet-50 in
both IID and non-IID settings. In addition, we evaluate performance on image
segmentation tasks using the ISIC-2018 dataset with a ResNet-50-based U-Net.
Experimental results demonstrate that SHeRL-FL reduces data transmission by
over 90\% compared to centralized FL and HierFL, and by 50\% compared to
SplitFed, which is a hybrid of FL and SL, and further improves hierarchical
split learning methods.

</details>


### [41] [Fuzzy-Pattern Tsetlin Machine](https://arxiv.org/abs/2508.08350)
*Artem Hnilov*

Main category: cs.LG

TL;DR: Fuzzy-Pattern Tsetlin Machine (FPTM) 通过模糊评估子句中的文字，显著减少了所需子句数量、内存占用和训练时间，同时提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 标准 Tsetlin Machine (TM) 的严格子句评估策略导致需要大量子句才能达到竞争性准确性，限制了效率和灵活性。

Method: FPTM 引入模糊评估机制，允许部分文字失败时子句仍能贡献部分投票，从而更灵活地匹配模式。

Result: 在多个数据集上，FPTM 显著减少了子句数量和内存占用，同时提高了准确性和训练速度。例如，在 IMDb 数据集上仅需 1 个子句即可达到 90.15% 的准确率。

Conclusion: FPTM 是一种高效、灵活且鲁棒的 TM 变体，适用于资源受限的环境，如微控制器。

Abstract: The "all-or-nothing" clause evaluation strategy is a core mechanism in the
Tsetlin Machine (TM) family of algorithms. In this approach, each clause - a
logical pattern composed of binary literals mapped to input data - is
disqualified from voting if even a single literal fails. Due to this strict
requirement, standard TMs must employ thousands of clauses to achieve
competitive accuracy. This paper introduces the Fuzzy-Pattern Tsetlin Machine
(FPTM), a novel variant where clause evaluation is fuzzy rather than strict. If
some literals in a clause fail, the remaining ones can still contribute to the
overall vote with a proportionally reduced score. As a result, each clause
effectively consists of sub-patterns that adapt individually to the input,
enabling more flexible, efficient, and robust pattern matching. The proposed
fuzzy mechanism significantly reduces the required number of clauses, memory
footprint, and training time, while simultaneously improving accuracy. On the
IMDb dataset, FPTM achieves 90.15% accuracy with only one clause per class, a
50x reduction in clauses and memory over the Coalesced Tsetlin Machine. FPTM
trains up to 316x faster (45 seconds vs. 4 hours) and fits within 50 KB,
enabling online learning on microcontrollers. Inference throughput reaches 34.5
million predictions/second (51.4 GB/s). On Fashion-MNIST, accuracy reaches
92.18% (2 clauses), 93.19% (20 clauses) and 94.68% (8000 clauses), a ~400x
clause reduction compared to the Composite TM's 93.00% (8000 clauses). On the
Amazon Sales dataset with 20% noise, FPTM achieves 85.22% accuracy,
significantly outperforming the Graph Tsetlin Machine (78.17%) and a Graph
Convolutional Neural Network (66.23%).

</details>


### [42] [Fast weight programming and linear transformers: from machine learning to neurobiology](https://arxiv.org/abs/2508.08435)
*Kazuki Irie,Samuel J. Gershman*

Main category: cs.LG

TL;DR: 本文综述了快速权重编程器（FWPs）的技术基础、计算特性及其与Transformer和状态空间模型的联系，并探讨了FWPs与大脑突触可塑性模型的关联。


<details>
  <summary>Details</summary>
Motivation: 探讨具有二维矩阵形式隐藏状态的RNN（FWPs）的动态权重变化特性及其在机器学习和语言建模中的应用。

Method: 回顾FWPs的技术基础，分析其计算特性，并比较其与Transformer和状态空间模型的异同。

Result: FWPs通过动态调整权重实现短时记忆存储，其特性与大脑突触可塑性模型有相似之处。

Conclusion: FWPs为自然与人工智能的融合提供了新的视角，展示了动态权重调整在模型中的潜力。

Abstract: Recent advances in artificial neural networks for machine learning, and
language modeling in particular, have established a family of recurrent neural
network (RNN) architectures that, unlike conventional RNNs with vector-form
hidden states, use two-dimensional (2D) matrix-form hidden states. Such
2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a
neural network whose synaptic weights (called fast weights) dynamically change
over time as a function of input observations, and serve as short-term memory
storage; corresponding synaptic weight modifications are controlled or
programmed by another network (the programmer) whose parameters are trained
(e.g., by gradient descent). In this Primer, we review the technical
foundations of FWPs, their computational characteristics, and their connections
to transformers and state space models. We also discuss connections between
FWPs and models of synaptic plasticity in the brain, suggesting a convergence
of natural and artificial intelligence.

</details>


### [43] [FetFIDS: A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm](https://arxiv.org/abs/2508.09056)
*Shreya Ghosh,Abu Shafin Mohammad Mahdee Jameel,Aly El Gamal*

Main category: cs.LG

TL;DR: FetFIDS是一种基于特征嵌入而非位置嵌入的Transformer深度学习系统，旨在提升入侵检测性能，特别适合联邦学习环境。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，入侵检测系统（IDS）性能显著提升，但现有方法在联邦学习环境中的隐私和本地化性能改进方面仍有优化空间。

Method: 提出FetFIDS，采用特征嵌入技术替代传统的位置嵌入，设计用于联邦学习环境，通过多轮通信实现隐私保护和性能提升。

Result: FetFIDS在联邦学习环境中表现优于多种最先进的入侵检测系统，并展现出高度适应性。

Conclusion: FetFIDS为入侵检测系统在联邦学习环境中的应用提供了高效且隐私保护的解决方案。

Abstract: Intrusion Detection Systems (IDS) have an increasingly important role in
preventing exploitation of network vulnerabilities by malicious actors. Recent
deep learning based developments have resulted in significant improvements in
the performance of IDS systems. In this paper, we present FetFIDS, where we
explore the employment of feature embedding instead of positional embedding to
improve intrusion detection performance of a transformer based deep learning
system. Our model is developed with the aim of deployments in edge learning
scenarios, where federated learning over multiple communication rounds can
ensure both privacy and localized performance improvements. FetFIDS outperforms
multiple state-of-the-art intrusion detection systems in a federated
environment and demonstrates a high degree of suitability to federated
learning. The code for this work can be found at
https://github.com/ghosh64/fetfids.

</details>


### [44] [Enhanced Liver Tumor Detection in CT Images Using 3D U-Net and Bat Algorithm for Hyperparameter Optimization](https://arxiv.org/abs/2508.08452)
*Nastaran Ghorbani,Bitasadat Jamshidi,Mohsen Rostamy-Malkhalifeh*

Main category: cs.LG

TL;DR: 本文提出了一种结合3D U-Net架构和蝙蝠算法的自动化肝脏肿瘤分割方法，优化了关键参数以提高CT图像分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 肝癌早期检测对治疗至关重要，但现有方法在分割精度和鲁棒性上仍有不足。

Method: 采用3D U-Net架构结合蝙蝠算法优化学习率和批量大小等超参数。

Result: 在公开数据集上表现出高F1分数，平衡了精确率和召回率，尤其适合临床诊断。

Conclusion: 深度学习和元启发式算法的结合为复杂分割任务提供了高效解决方案。

Abstract: Liver cancer is one of the most prevalent and lethal forms of cancer, making
early detection crucial for effective treatment. This paper introduces a novel
approach for automated liver tumor segmentation in computed tomography (CT)
images by integrating a 3D U-Net architecture with the Bat Algorithm for
hyperparameter optimization. The method enhances segmentation accuracy and
robustness by intelligently optimizing key parameters like the learning rate
and batch size. Evaluated on a publicly available dataset, our model
demonstrates a strong ability to balance precision and recall, with a high
F1-score at lower prediction thresholds. This is particularly valuable for
clinical diagnostics, where ensuring no potential tumors are missed is
paramount. Our work contributes to the field of medical image analysis by
demonstrating that the synergy between a robust deep learning architecture and
a metaheuristic optimization algorithm can yield a highly effective solution
for complex segmentation tasks.

</details>


### [45] [Discrete Diffusion-Based Model-Level Explanation of Heterogeneous GNNs with Node Features](https://arxiv.org/abs/2508.08458)
*Pallabee Das,Stefan Heindorf*

Main category: cs.LG

TL;DR: DiGNNExplainer是一种基于离散去噪扩散的模型级解释方法，用于生成具有真实节点特征的异构图解释，解决了现有方法在特征支持性和解释真实性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有异构图神经网络（HGNNs）的解释方法缺乏对实际节点特征的支持，且生成的解释不真实或不忠实。

Method: 提出DiGNNExplainer，利用离散空间中的扩散模型生成真实的离散特征（如词袋特征）。

Result: 在多个数据集上验证，DiGNNExplainer生成的解释更真实且忠实于模型决策，优于现有方法。

Conclusion: DiGNNExplainer填补了异构图解释的空白，为实际应用提供了更可靠的解释工具。

Abstract: Many real-world datasets, such as citation networks, social networks, and
molecular structures, are naturally represented as heterogeneous graphs, where
nodes belong to different types and have additional features. For example, in a
citation network, nodes representing "Paper" or "Author" may include attributes
like keywords or affiliations. A critical machine learning task on these graphs
is node classification, which is useful for applications such as fake news
detection, corporate risk assessment, and molecular property prediction.
Although Heterogeneous Graph Neural Networks (HGNNs) perform well in these
contexts, their predictions remain opaque. Existing post-hoc explanation
methods lack support for actual node features beyond one-hot encoding of node
type and often fail to generate realistic, faithful explanations. To address
these gaps, we propose DiGNNExplainer, a model-level explanation approach that
synthesizes heterogeneous graphs with realistic node features via discrete
denoising diffusion. In particular, we generate realistic discrete features
(e.g., bag-of-words features) using diffusion models within a discrete space,
whereas previous approaches are limited to continuous spaces. We evaluate our
approach on multiple datasets and show that DiGNNExplainer produces
explanations that are realistic and faithful to the model's decision-making,
outperforming state-of-the-art methods.

</details>


### [46] [Sparse Partial Optimal Transport via Quadratic Regularization](https://arxiv.org/abs/2508.08476)
*Khang Tran,Khoa Nguyen,Anh Nguyen,Thong Huynh,Son Pham,Sy-Hoang Nguyen-Dang,Manh Pham,Bang Vo,Mai Ngoc Tran,Mai Ngoc Tran,Dung Luong*

Main category: cs.LG

TL;DR: 本文提出了一种基于二次正则化的部分最优传输（QPOT）方法，解决了传统熵正则化方法导致的传输计划稠密问题，提升了稀疏性，并在多个实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输（OT）要求输入测度质量相等，而部分最优传输（POT）放宽了这一限制，但现有熵正则化方法生成的传输计划稠密，限制了其在稀疏性需求场景中的应用。

Method: 提出了一种二次正则化的POT（QPOT）方法，通过二次正则化诱导稀疏传输计划。

Result: 在合成数据集、CIFAR-10数据集及实际应用（如颜色迁移和领域适应）中，QPOT表现出更高的稀疏性和优越性能。

Conclusion: QPOT为稀疏性需求场景提供了一种有效的替代方案，扩展了POT的应用范围。

Abstract: Partial Optimal Transport (POT) has recently emerged as a central tool in
various Machine Learning (ML) applications. It lifts the stringent assumption
of the conventional Optimal Transport (OT) that input measures are of equal
masses, which is often not guaranteed in real-world datasets, and thus offers
greater flexibility by permitting transport between unbalanced input measures.
Nevertheless, existing major solvers for POT commonly rely on entropic
regularization for acceleration and thus return dense transport plans,
hindering the adoption of POT in various applications that favor sparsity. In
this paper, as an alternative approach to the entropic POT formulation in the
literature, we propose a novel formulation of POT with quadratic
regularization, hence termed quadratic regularized POT (QPOT), which induces
sparsity to the transport plan and consequently facilitates the adoption of POT
in many applications with sparsity requirements. Extensive experiments on
synthetic and CIFAR-10 datasets, as well as real-world applications such as
color transfer and domain adaptations, consistently demonstrate the improved
sparsity and favorable performance of our proposed QPOT formulation.

</details>


### [47] [Biased Local SGD for Efficient Deep Learning on Heterogeneous Systems](https://arxiv.org/abs/2508.08540)
*Jihyun Lim,Junhyuk Jo,Chanhyeok Ko,Young Min Go,Jimin Hwa,Sunwoo Lee*

Main category: cs.LG

TL;DR: 论文提出了一种针对异构计算资源的本地随机梯度下降（local SGD）方法，通过按计算能力分配工作负载和引入有偏数据采样与模型聚合，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统同步SGD在异构计算资源下因同步开销大而效率低下，导致用户仅依赖最快资源（如GPU）。本文旨在探索如何有效利用异构资源进行神经网络训练。

Method: 提出系统感知的本地SGD方法，按计算能力分配工作负载，并通过有偏数据采样和模型聚合优化慢速资源（如CPU）的利用率。

Result: 实验表明，该方法在异构环境中显著加速了本地SGD，且在相同时间预算下达到甚至超过同步SGD的精度。

Conclusion: 该方法为异构环境（如云平台和多节点高性能计算集群）提供了一种高效的并行化策略。

Abstract: Most large-scale neural network training methods assume homogeneous parallel
computing resources. For example, synchronous SGD with data parallelism, the
most widely used parallel training strategy, incurs significant synchronization
overhead when workers process their assigned data at different speeds.
Consequently, in systems with heterogeneous compute resources, users often rely
solely on the fastest components, such as GPUs, for training. In this work, we
explore how to effectively use heterogeneous resources for neural network
training. We propose a system-aware local stochastic gradient descent (local
SGD) method that allocates workloads to each compute resource in proportion to
its compute capacity. To make better use of slower resources such as CPUs, we
intentionally introduce bias into data sampling and model aggregation. Our
study shows that well-controlled bias can significantly accelerate local SGD in
heterogeneous environments, achieving comparable or even higher accuracy than
synchronous SGD with data-parallelism within the same time budget. This
fundamental parallelization strategy can be readily extended to diverse
heterogeneous environments, including cloud platforms and multi-node
high-performance computing clusters.

</details>


### [48] [M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction](https://arxiv.org/abs/2508.08543)
*Guangyin Jin,Sicong Lai,Xiaoshuai Hao,Mingtao Zhang,Jinlei Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于MLP的轻量级交通预测模型M3-Net，解决了现有方法依赖复杂网络结构或模型设计的问题。


<details>
  <summary>Details</summary>
Motivation: 当前交通预测的主流方法依赖复杂的图神经网络或注意力机制，难以高效部署在大规模数据集上。

Method: M3-Net采用时间序列和时空嵌入，结合MLP-Mixer架构和专家混合机制（MoE）。

Result: 在多个真实数据集上的实验表明，M3-Net在预测性能和轻量级部署方面表现优越。

Conclusion: M3-Net为交通预测提供了一种高效且轻量化的解决方案。

Abstract: Achieving accurate traffic prediction is a fundamental but crucial task in
the development of current intelligent transportation systems.Most of the
mainstream methods that have made breakthroughs in traffic prediction rely on
spatio-temporal graph neural networks, spatio-temporal attention mechanisms,
etc. The main challenges of the existing deep learning approaches are that they
either depend on a complete traffic network structure or require intricate
model designs to capture complex spatio-temporal dependencies. These
limitations pose significant challenges for the efficient deployment and
operation of deep learning models on large-scale datasets. To address these
challenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP)
based model M3-Net for traffic prediction. Our proposed model not only employs
time series and spatio-temporal embeddings for efficient feature processing but
also first introduces a novel MLP-Mixer architecture with a mixture of experts
(MoE) mechanism. Extensive experiments conducted on multiple real datasets
demonstrate the superiority of the proposed model in terms of prediction
performance and lightweight deployment.

</details>


### [49] [UQGNN: Uncertainty Quantification of Graph Neural Networks for Multivariate Spatiotemporal Prediction](https://arxiv.org/abs/2508.08551)
*Dahai Yu,Dingyi Zhuang,Lin Jiang,Rongchao Xu,Xinyue Ye,Yuheng Bu,Shenhao Wang,Guang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为UQGNN的图神经网络，用于多变量时空预测，通过量化不确定性和捕捉异构城市现象的相关性，显著提升了预测准确性和不确定性量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数时空预测模型是确定性的，缺乏对不确定性的量化，且通常只关注单一现象，忽略了异构城市现象之间的相关性。

Method: UQGNN包含两个关键创新：1）交互感知的时空嵌入模块，结合多元扩散图卷积网络和交互感知时间卷积网络；2）多元概率预测模块，用于估计期望均值和不确定性。

Result: 在深圳、纽约和芝加哥的四个真实数据集上，UQGNN在预测准确性和不确定性量化方面均优于现有基线模型，例如在深圳数据集上提升了5%。

Conclusion: UQGNN通过量化不确定性和捕捉多变量交互，为时空预测提供了更可靠和准确的解决方案。

Abstract: Spatiotemporal prediction plays a critical role in numerous real-world
applications such as urban planning, transportation optimization, disaster
response, and pandemic control. In recent years, researchers have made
significant progress by developing advanced deep learning models for
spatiotemporal prediction. However, most existing models are deterministic,
i.e., predicting only the expected mean values without quantifying uncertainty,
leading to potentially unreliable and inaccurate outcomes. While recent studies
have introduced probabilistic models to quantify uncertainty, they typically
focus on a single phenomenon (e.g., taxi, bike, crime, or traffic crashes),
thereby neglecting the inherent correlations among heterogeneous urban
phenomena. To address the research gap, we propose a novel Graph Neural Network
with Uncertainty Quantification, termed UQGNN for multivariate spatiotemporal
prediction. UQGNN introduces two key innovations: (i) an Interaction-aware
Spatiotemporal Embedding Module that integrates a multivariate diffusion graph
convolutional network and an interaction-aware temporal convolutional network
to effectively capture complex spatial and temporal interaction patterns, and
(ii) a multivariate probabilistic prediction module designed to estimate both
expected mean values and associated uncertainties. Extensive experiments on
four real-world multivariate spatiotemporal datasets from Shenzhen, New York
City, and Chicago demonstrate that UQGNN consistently outperforms
state-of-the-art baselines in both prediction accuracy and uncertainty
quantification. For example, on the Shenzhen dataset, UQGNN achieves a 5%
improvement in both prediction accuracy and uncertainty quantification.

</details>


### [50] [SHEFL: Resource-Aware Aggregation and Sparsification in Heterogeneous Ensemble Federated Learning](https://arxiv.org/abs/2508.08552)
*Keumseo Ryum,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: SHEFL是一种针对计算能力不同的客户端的全局集成联邦学习框架，通过动态调整资源分配和引入新的聚合方案，解决了计算异构性问题，提升了公平性和性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在现实通信场景中因数据和系统异构性而收敛困难，现有方法常忽略通信效率约束或未能充分捕捉模型预测多样性。

Method: 提出SHEFL框架，根据客户端资源分配不同数量的全局模型，并引入考虑计算能力偏差的聚合方案，动态调整资源比例。

Result: 实验表明，SHEFL有效解决了计算异构性问题，显著提升了公平性和整体性能。

Conclusion: SHEFL为异构计算环境下的联邦学习提供了一种高效解决方案。

Abstract: Federated learning enables distributed training with private data of clients,
but its convergence is hindered by data and system heterogeneity in realistic
communication scenarios. Most existing system heterogeneous FL schemes utilize
global pruning or ensemble distillation, yet they often overlook typical
constraints required for communication efficiency. Meanwhile, deep ensembles
can aggregate predictions from individually trained models to improve
performance, but current ensemble-based FL methods fall short in fully
capturing the diversity of model predictions. In this work, we propose SHEFL, a
global ensemble-based federated learning framework suited for clients with
diverse computational capacities. We allocate different numbers of global
models to clients based on their available resources. We further introduce a
novel aggregation scheme that accounts for bias between clients with different
computational capabilities. To reduce the computational burden of training deep
ensembles and mitigate data bias, we dynamically adjust the resource ratio
across clients - aggressively reducing the influence of underpowered clients in
constrained scenarios, while increasing their weight in the opposite case.
Extensive experiments demonstrate that our method effectively addresses
computational heterogeneity, significantly improving both fairness and overall
performance compared to existing approaches.

</details>


### [51] [Dynamic Rank Adjustment for Accurate and Efficient Neural Network Training](https://arxiv.org/abs/2508.08625)
*Hyuntak Shin,Aecheon Jung,Sunwoo Lee,Sungeun Hong*

Main category: cs.LG

TL;DR: 论文提出了一种动态秩训练框架，通过在低秩训练中穿插全秩训练来恢复权重矩阵的秩，避免秩崩溃，同时保持计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 固定低秩结构限制了权重矩阵的秩，影响模型学习复杂模式的能力，且训练中秩会进一步下降。

Method: 提出动态秩训练框架，交替进行低秩和全秩训练，调整权重矩阵的秩以避免秩崩溃。

Result: 实验表明，该方法在计算成本接近SVD低秩训练的同时，准确性接近全秩训练。

Conclusion: 动态秩训练框架有效解决了低秩训练的局限性，兼具高效性和准确性。

Abstract: Low-rank training methods reduce the number of trainable parameters by
re-parameterizing the weights with matrix decompositions (e.g., singular value
decomposition). However, enforcing a fixed low-rank structure caps the rank of
the weight matrices and can hinder the model's ability to learn complex
patterns. Furthermore, the effective rank of the model's weights tends to
decline during training, and this drop is accelerated when the model is
reparameterized into a low-rank structure. In this study, we argue that
strategically interleaving full-rank training epochs within low-rank training
epochs can effectively restore the rank of the model's weights. Based on our
findings, we propose a general dynamic-rank training framework that is readily
applicable to a wide range of neural-network tasks. We first describe how to
adjust the rank of weight matrix to alleviate the inevitable rank collapse that
arises during training, and then present extensive empirical results that
validate our claims and demonstrate the efficacy of the proposed framework. Our
empirical study shows that the proposed method achieves almost the same
computational cost as SVD-based low-rank training while achieving a comparable
accuracy to full-rank training across various benchmarks.

</details>


### [52] [Classifier Language Models: Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks](https://arxiv.org/abs/2508.08635)
*Adit Krishnan,Chu Wang,Chris Kong*

Main category: cs.LG

TL;DR: 论文提出了一种基于令牌驱动的稀疏微调策略，用于优化小型语言模型在专业分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 工业中的语义分类任务（如客户意图检测）通常高度专业化，需要领域专家标注，且对推理吞吐量要求高，限制了模型规模。因此，需要开发定制化的小型语言模型。

Method: 通过识别并微调模型中对任务敏感的少量参数，利用任务特定的令牌构造，而不引入额外参数。

Result: 该方法在五个语义分类任务中表现优于端到端微调、LoRA等方法，训练成本减半且稳定性更高。

Conclusion: 令牌驱动的稀疏微调策略是一种高效、低成本的小型语言模型优化方案。

Abstract: Semantic text classification requires the understanding of the contextual
significance of specific tokens rather than surface-level patterns or keywords
(as in rule-based or statistical text classification), making large language
models (LLMs) well-suited for this task. However, semantic classification
applications in industry, like customer intent detection or semantic role
labeling, tend to be highly specialized. They require annotation by domain
experts in contrast to general-purpose corpora for pretraining. Further, they
typically require high inference throughputs which limits the model size from
latency and cost perspectives. Thus, for a range of specialized classification
tasks, the preferred solution is to develop customized classifiers by
finetuning smaller language models (e.g., mini-encoders, small language
models).
  In this work, we develop a token-driven sparse finetuning strategy to adapt
small language models to specialized classification tasks. We identify and
finetune a small sensitive subset of model parameters by leveraging
task-specific token constructs in the finetuning dataset, while leaving most of
the pretrained weights unchanged. Unlike adapter approaches such as low rank
adaptation (LoRA), we do not introduce additional parameters to the model. Our
approach identifies highly relevant semantic tokens (case study in the
Appendix) and outperforms end-to-end finetuning, LoRA, layer selection, and
prefix tuning on five diverse semantic classification tasks. We achieve greater
stability and half the training costs vs. end-to-end finetuning.

</details>


### [53] [MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time](https://arxiv.org/abs/2508.08641)
*Peter Phan,Dhruv Agarwal,Kavitha Srinivas,Horst Samulowitz,Pavan Kapanipathi,Andrew McCallum*

Main category: cs.LG

TL;DR: MiGrATe是一种在线测试时间训练方法，结合GRPO搜索算法和混合策略组构建，无需外部训练数据，显著提升黑盒优化任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在探索新解空间与利用高回报解空间之间难以平衡的问题，同时避免依赖手工定制训练数据的局限性。

Method: 采用GRPO搜索算法，结合贪婪采样和邻域采样（NS）的混合策略组构建，通过在线测试时间训练优化LLMs。

Result: 在单词搜索、分子优化和ARC上的假设+程序归纳任务中，MiGrATe均优于仅推理和TTT基线方法。

Conclusion: MiGrATe展示了在线测试时间训练在无需外部监督下解决复杂搜索任务的潜力。

Abstract: Large language models (LLMs) are increasingly being applied to black-box
optimization tasks, from program synthesis to molecule design. Prior work
typically leverages in-context learning to iteratively guide the model towards
better solutions. Such methods, however, often struggle to balance exploration
of new solution spaces with exploitation of high-reward ones. Recently,
test-time training (TTT) with synthetic data has shown promise in improving
solution quality. However, the need for hand-crafted training data tailored to
each task limits feasibility and scalability across domains. To address this
problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a
search algorithm to adapt LLMs at inference without requiring external training
data. MiGrATe operates via a mixed-policy group construction procedure that
combines on-policy sampling with two off-policy data selection techniques:
greedy sampling, which selects top-performing past completions, and
neighborhood sampling (NS), which generates completions structurally similar to
high-reward ones. Together, these components bias the policy gradient towards
exploitation of promising regions in solution space, while preserving
exploration through on-policy sampling. We evaluate MiGrATe on three
challenging domains-word search, molecule optimization, and hypothesis+program
induction on the Abstraction and Reasoning Corpus (ARC)-and find that it
consistently outperforms both inference-only and TTT baselines, demonstrating
the potential of online TTT as a solution for complex search tasks without
external supervision.

</details>


### [54] [$\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models](https://arxiv.org/abs/2508.08657)
*Jiaxin Ju,Yizhen Zheng,Huan Yee Koh,Can Wang,Shirui Pan*

Main category: cs.LG

TL;DR: 论文提出了一种名为M²LLM的多视角框架，通过结合分子结构、任务和规则视角，利用大语言模型（LLM）生成丰富的分子表示，并在多个基准测试中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 分子属性预测在化学、材料科学和药物发现中有广泛应用，但现有方法（如指纹和GNN）往往忽略了语义和上下文知识。LLM在科学领域展现出强大的推理能力，因此作者假设LLM可以生成更丰富的分子表示。

Method: 提出M²LLM框架，整合分子结构、任务和规则三个视角，动态融合以适应任务需求，并利用LLM的编码和推理能力生成分子嵌入和特征。

Result: M²LLM在分类和回归任务中实现了最优性能，LLM生成的表示通过编码和推理功能表现出色。

Conclusion: M²LLM通过多视角融合和LLM的应用，显著提升了分子属性预测的性能，展示了LLM在科学领域的潜力。

Abstract: Accurate molecular property prediction is a critical challenge with
wide-ranging applications in chemistry, materials science, and drug discovery.
Molecular representation methods, including fingerprints and graph neural
networks (GNNs), achieve state-of-the-art results by effectively deriving
features from molecular structures. However, these methods often overlook
decades of accumulated semantic and contextual knowledge. Recent advancements
in large language models (LLMs) demonstrate remarkable reasoning abilities and
prior knowledge across scientific domains, leading us to hypothesize that LLMs
can generate rich molecular representations when guided to reason in multiple
perspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-view
framework that integrates three perspectives: the molecular structure view, the
molecular task view, and the molecular rules view. These views are fused
dynamically to adapt to task requirements, and experiments demonstrate that
$\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarks
across classification and regression tasks. Moreover, we demonstrate that
representation derived from LLM achieves exceptional performance by leveraging
two core functionalities: the generation of molecular embeddings through their
encoding capabilities and the curation of molecular features through advanced
reasoning processes.

</details>


### [55] [Multi-level Collaborative Distillation Meets Global Workspace Model: A Unified Framework for OCIL](https://arxiv.org/abs/2508.08677)
*Shibin Su,Guoqiang Liang,De Cheng,Shizhou Zhang,Lingyan Ran,Yanning Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于全局工作空间模型（GWM）的新方法，通过融合多个学生模型的参数来增强在线类增量学习（OCIL）的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 在线类增量学习（OCIL）在非独立同分布数据流中面临模型稳定性与适应性的挑战，现有方法在严格内存限制下效果不佳。

Method: 通过全局工作空间模型（GWM）融合学生模型参数，并结合多级协作蒸馏机制，实现知识的动态整合与跨任务一致性。

Result: 在三个标准OCIL基准测试中，该方法显著提升了多种OCIL模型的性能。

Conclusion: 提出的方法在严格内存限制下有效平衡了稳定性和适应性，为OCIL提供了更优的解决方案。

Abstract: Online Class-Incremental Learning (OCIL) enables models to learn continuously
from non-i.i.d. data streams and samples of the data streams can be seen only
once, making it more suitable for real-world scenarios compared to offline
learning. However, OCIL faces two key challenges: maintaining model stability
under strict memory constraints and ensuring adaptability to new tasks. Under
stricter memory constraints, current replay-based methods are less effective.
While ensemble methods improve adaptability (plasticity), they often struggle
with stability. To overcome these challenges, we propose a novel approach that
enhances ensemble learning through a Global Workspace Model (GWM)-a shared,
implicit memory that guides the learning of multiple student models. The GWM is
formed by fusing the parameters of all students within each training batch,
capturing the historical learning trajectory and serving as a dynamic anchor
for knowledge consolidation. This fused model is then redistributed
periodically to the students to stabilize learning and promote cross-task
consistency. In addition, we introduce a multi-level collaborative distillation
mechanism. This approach enforces peer-to-peer consistency among students and
preserves historical knowledge by aligning each student with the GWM. As a
result, student models remain adaptable to new tasks while maintaining
previously learned knowledge, striking a better balance between stability and
plasticity. Extensive experiments on three standard OCIL benchmarks show that
our method delivers significant performance improvement for several OCIL models
across various memory budgets.

</details>


### [56] [Expert-Guided Diffusion Planner for Auto-bidding](https://arxiv.org/abs/2508.08687)
*Yunshan Peng,Wenzheng Shu,Jiahao Sun,Yanxiang Zeng,Jinan Pang,Wentao Bai,Yunke Bai,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种基于专家轨迹引导和跳步采样的条件扩散模型方法，用于提升自动出价系统的生成效率和决策质量。


<details>
  <summary>Details</summary>
Motivation: 传统生成式出价仅依赖回报作为最优条件，缺乏个性化结构信息，且扩散模型的逐步生成机制存在时效性风险。

Method: 结合专家轨迹引导和跳步采样策略的条件扩散模型方法。

Result: 离线实验验证了有效性，在线A/B测试中转化率提升11.29%，收入提升12.35%。

Conclusion: 该方法显著提升了自动出价系统的性能，具有实际应用价值。

Abstract: Auto-bidding is extensively applied in advertising systems, serving a
multitude of advertisers. Generative bidding is gradually gaining traction due
to its robust planning capabilities and generalizability. In contrast to
traditional reinforcement learning-based bidding, generative bidding does not
rely on the Markov Decision Process (MDP) exhibiting superior planning
capabilities in long-horizon scenarios. Conditional diffusion modeling
approaches have demonstrated significant potential in the realm of
auto-bidding. However, relying solely on return as the optimality condition is
weak to guarantee the generation of genuinely optimal decision sequences,
lacking personalized structural information. Moreover, diffusion models' t-step
autoregressive generation mechanism inherently carries timeliness risks. To
address these issues, we propose a novel conditional diffusion modeling method
based on expert trajectory guidance combined with a skip-step sampling strategy
to enhance generation efficiency. We have validated the effectiveness of this
approach through extensive offline experiments and achieved statistically
significant results in online A/B testing, achieving an increase of 11.29% in
conversion and a 12.35% in revenue compared with the baseline.

</details>


### [57] [Generative Modeling for Robust Deep Reinforcement Learning on the Traveling Salesman Problem](https://arxiv.org/abs/2508.08718)
*Michael Li,Eric Bae,Christopher Haberland,Natasha Jaques*

Main category: cs.LG

TL;DR: 论文提出了一种名为COGS的方法，通过生成模型采样训练数据，解决了神经网络在TSP问题中泛化能力不足的问题，并在真实数据集TSPLib50上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 经典启发式算法在小规模TSP问题上表现良好，但在大规模问题上计算困难。神经网络虽然推理速度快，但泛化能力差，尤其是在真实分布数据上表现不佳。

Method: 提出了COGS方法，利用生成模型采样训练数据，以覆盖更广泛的TSP分布，并引入了TSPLib50数据集测试真实场景下的泛化能力。

Result: COGS在合成数据集和TSPLib50上均表现出色，尤其在最坏情况下性能提升显著。

Conclusion: COGS通过改进数据覆盖和插值能力，显著提升了神经网络在TSP问题中的分布鲁棒性。

Abstract: The Traveling Salesman Problem (TSP) is a classic NP-hard combinatorial
optimization task with numerous practical applications. Classic heuristic
solvers can attain near-optimal performance for small problem instances, but
become computationally intractable for larger problems. Real-world logistics
problems such as dynamically re-routing last-mile deliveries demand a solver
with fast inference time, which has led researchers to investigate specialized
neural network solvers. However, neural networks struggle to generalize beyond
the synthetic data they were trained on. In particular, we show that there
exist TSP distributions that are realistic in practice, which also consistently
lead to poor worst-case performance for existing neural approaches. To address
this issue of distribution robustness, we present Combinatorial Optimization
with Generative Sampling (COGS), where training data is sampled from a
generative TSP model. We show that COGS provides better data coverage and
interpolation in the space of TSP training distributions. We also present
TSPLib50, a dataset of realistically distributed TSP samples, which tests
real-world generalization ability without conflating this issue with instance
size. We evaluate our method on various synthetic datasets as well as TSPLib50,
and compare to state-of-the-art neural baselines. We demonstrate that COGS
improves distribution robustness, with most performance gains coming from
worst-case scenarios.

</details>


### [58] [Elucidating Rectified Flow with Deterministic Sampler: Polynomial Discretization Complexity for Multi and One-step Models](https://arxiv.org/abs/2508.08735)
*Ruofeng Yang,Zhaoyu Zhu,Bo Jiang,Cheng Chen,Shuai Li*

Main category: cs.LG

TL;DR: 本文证明了在有限支撑假设下，多步和一步RF模型具有多项式离散化复杂度，优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有理论工作对RF模型的离散化复杂度分析不足，且结果依赖指数级参数。本文旨在填补这一空白。

Method: 引入Langevin过程作为校正器，分析RF模型在多步和一步生成中的表现。

Result: RF模型在多步和一步生成中均表现出多项式离散化复杂度，优于VP和VE模型。

Conclusion: 本文首次从理论上解释了RF模型在多步和一步生成中的优越性能。

Abstract: Recently, rectified flow (RF)-based models have achieved state-of-the-art
performance in many areas for both the multi-step and one-step generation.
However, only a few theoretical works analyze the discretization complexity of
RF-based models. Existing works either focus on flow-based models with
stochastic samplers or establish complexity results that exhibit exponential
dependence on problem parameters. In this work, under the realistic bounded
support assumption, we prove the first polynomial discretization complexity for
multi-step and one-step RF-based models with a deterministic sampler
simultaneously. For the multi-step setting, inspired by the predictor-corrector
framework of diffusion models, we introduce a Langevin process as a corrector
and show that RF-based models can achieve better polynomial discretization
complexity than diffusion models. To achieve this result, we conduct a detailed
analysis of the RF-based model and explain why it is better than previous
popular models, such as variance preserving (VP) and variance exploding
(VE)-based models. Based on the observation of multi-step RF-based models, we
further provide the first polynomial discretization complexity result for
one-step RF-based models, improving upon prior results for one-step
diffusion-based models. These findings mark the first step toward theoretically
understanding the impressive empirical performance of RF-based models in both
multi-step and one-step generation.

</details>


### [59] [Interpretable Reward Model via Sparse Autoencoder](https://arxiv.org/abs/2508.08746)
*Shuyi Zhang,Wei Shi,Sihang Li,Jiayi Liao,Tao Liang,Hengxing Cai,Xiang Wang*

Main category: cs.LG

TL;DR: SARM是一种新型奖励模型架构，通过稀疏自编码器提升奖励模型的可解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型缺乏可解释性和灵活性，难以适应用户偏好变化。

Method: SARM将稀疏自编码器集成到奖励模型中，将隐藏激活映射到稀疏且单义的特征空间。

Result: SARM实现了奖励分配的透明化、动态调整偏好变化，并在对齐性能上优于传统模型。

Conclusion: SARM为奖励模型提供了更高的可解释性和适应性，有望推动LLM与人类价值观的对齐。

Abstract: Large language models (LLMs) have been widely deployed across numerous
fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward
models (RMs) as proxies for human preferences to align LLM behaviors with human
values, making the accuracy, reliability, and interpretability of RMs critical
for effective alignment. However, traditional RMs lack interpretability, offer
limited insight into the reasoning behind reward assignments, and are
inflexible toward user preference shifts. While recent multidimensional RMs aim
for improved interpretability, they often fail to provide feature-level
attribution and require costly annotations. To overcome these limitations, we
introduce the Sparse Autoencoder-enhanced Reward Model (\textbf{SARM}), a novel
architecture that integrates a pretrained Sparse Autoencoder (SAE) into a
reward model. SARM maps the hidden activations of LLM-based RM into an
interpretable, sparse, and monosemantic feature space, from which a scalar head
aggregates feature activations to produce transparent and conceptually
meaningful reward scores. Empirical evaluations demonstrate that SARM
facilitates direct feature-level attribution of reward assignments, allows
dynamic adjustment to preference shifts, and achieves superior alignment
performance compared to conventional reward models. Our code is available at
https://github.com/schrieffer-z/sarm.

</details>


### [60] [Differentiated Information Mining: A Semi-supervised Learning Framework for GNNs](https://arxiv.org/abs/2508.08769)
*Long Wang,Kai Liu*

Main category: cs.LG

TL;DR: 提出了一种名为DiFac的半监督学习框架，通过从单一信息源提取差异化因子并强制其一致性，解决了伪标签确认偏差和训练崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 在半监督学习中，引入相互独立的决策因子以增强图神经网络的性能是有效的，但实践中获取这些因子具有挑战性。

Method: DiFac框架通过预训练提取差异化因子，在训练中移除冲突样本并基于最短路径原则选择伪标签，同时可结合额外信息源。

Result: 实验表明，DiFac在低标签情况下显著提升了鲁棒性和泛化能力，优于其他基线方法。

Conclusion: DiFac通过差异化因子一致性和辅助信息源的有效利用，解决了半监督学习中的关键问题。

Abstract: In semi-supervised learning (SSL) for enhancing the performance of graph
neural networks (GNNs) with unlabeled data, introducing mutually independent
decision factors for cross-validation is regarded as an effective strategy to
alleviate pseudo-label confirmation bias and training collapse. However,
obtaining such factors is challenging in practice: additional and valid
information sources are inherently scarce, and even when such sources are
available, their independence from the original source cannot be guaranteed. To
address this challenge, In this paper we propose a Differentiated Factor
Consistency Semi-supervised Framework (DiFac), which derives differentiated
factors from a single information source and enforces their consistency. During
pre-training, the model learns to extract these factors; in training, it
iteratively removes samples with conflicting factors and ranks pseudo-labels
based on the shortest stave principle, selecting the top candidate samples to
reduce overconfidence commonly observed in confidence-based or ensemble-based
methods. Our framework can also incorporate additional information sources. In
this work, we leverage the large multimodal language model to introduce latent
textual knowledge as auxiliary decision factors, and we design a accountability
scoring mechanism to mitigate additional erroneous judgments introduced by
these auxiliary factors. Experiments on multiple benchmark datasets demonstrate
that DiFac consistently improves robustness and generalization in low-label
regimes, outperforming other baseline methods.

</details>


### [61] [TechOps: Technical Documentation Templates for the AI Act](https://arxiv.org/abs/2508.08804)
*Laura Lucaj,Alex Loosley,Hakan Jonsson,Urs Gasser,Patrick van der Smagt*

Main category: cs.LG

TL;DR: 本文提出了开源模板，用于满足欧盟AI法案的技术文档要求，覆盖AI全生命周期，确保透明性、可追溯性和合规性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统文档模板未能完全满足欧盟AI法案的技术文档要求，且未覆盖AI全生命周期。

Method: 引入开源模板，用于数据、模型和应用的文档化，并通过用户反馈和实际场景验证其可用性。

Result: 模板在实际场景中得到验证，包括肤色数据集、神经网络模型和建筑工地安全系统，证明其可行性和实用性。

Conclusion: TechOps模板可作为实用工具，支持监管合规和负责任AI开发的监督。

Abstract: Operationalizing the EU AI Act requires clear technical documentation to
ensure AI systems are transparent, traceable, and accountable. Existing
documentation templates for AI systems do not fully cover the entire AI
lifecycle while meeting the technical documentation requirements of the AI Act.
  This paper addresses those shortcomings by introducing open-source templates
and examples for documenting data, models, and applications to provide
sufficient documentation for certifying compliance with the AI Act. These
templates track the system status over the entire AI lifecycle, ensuring
traceability, reproducibility, and compliance with the AI Act. They also
promote discoverability and collaboration, reduce risks, and align with best
practices in AI documentation and governance.
  The templates are evaluated and refined based on user feedback to enable
insights into their usability and implementability. We then validate the
approach on real-world scenarios, providing examples that further guide their
implementation: the data template is followed to document a skin tones dataset
created to support fairness evaluations of downstream computer vision models
and human-centric applications; the model template is followed to document a
neural network for segmenting human silhouettes in photos. The application
template is tested on a system deployed for construction site safety using
real-time video analytics and sensor data. Our results show that TechOps can
serve as a practical tool to enable oversight for regulatory compliance and
responsible AI development.

</details>


### [62] [TempOpt -- Unsupervised Alarm Relation Learning for Telecommunication Networks](https://arxiv.org/abs/2508.08814)
*Sathiyanaryanan Sampath,Pratyush Uppuluri,Thirumaran Ekambaram*

Main category: cs.LG

TL;DR: 论文提出了一种新型无监督告警关系学习技术TempOpt，用于电信网络中高效识别根告警，克服了现有时间依赖方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 电信网络中告警数量庞大且关系复杂，现有方法难以高效识别根告警，亟需一种更实用的解决方案。

Method: 提出TempOpt技术，通过时间优化学习告警关系，实验验证其优于传统时间依赖方法。

Result: 在真实网络数据集上，TempOpt学习的告警关系质量优于时间依赖方法。

Conclusion: TempOpt为电信网络告警管理提供了一种更高效的无监督学习方案。

Abstract: In a telecommunications network, fault alarms generated by network nodes are
monitored in a Network Operations Centre (NOC) to ensure network availability
and continuous network operations. The monitoring process comprises of tasks
such as active alarms analysis, root alarm identification, and resolution of
the underlying problem. Each network node potentially can generate alarms of
different types, while nodes can be from multiple vendors, a network can have
hundreds of nodes thus resulting in an enormous volume of alarms at any time.
Since network nodes are inter-connected, a single fault in the network would
trigger multiple sequences of alarms across a variety of nodes and from a
monitoring point of view, it is a challenging task for a NOC engineer to be
aware of relations between the various alarms, when trying to identify, for
example, a root alarm on which an action needs to be taken. To effectively
identify root alarms, it is essential to learn relation among the alarms for
accurate and faster resolution. In this work we propose a novel unsupervised
alarm relation learning technique Temporal Optimization (TempOpt) that is
practical and overcomes the limitations of an existing class of alarm
relational learning method-temporal dependency methods. Experiments have been
carried on real-world network datasets, that demonstrate the improved quality
of alarm relations learned by TempOpt as compared to temporal dependency
method.

</details>


### [63] [Wavelet Mixture of Experts for Time Series Forecasting](https://arxiv.org/abs/2508.08825)
*Zheng Zhou,Yu-Jie Xiong,Jia-Chen Zhang,Chun-Ming Xia,Xi-Jiong Xie*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级时间序列预测模型WaveTS系列，结合小波变换和MLP，解决了传统Transformer和MLP模型的局限性，在多通道数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型参数过多且难以捕捉非平稳特征，MLP模型在多通道依赖上表现不佳，因此需要一种更高效的模型。

Method: 结合小波变换和MLP，提出WaveTS-B模型；进一步引入MoE框架和通道聚类策略，提出WaveTS-M模型。

Result: 在八个真实数据集上，WaveTS系列模型以更少参数实现了SOTA性能，WaveTS-M在多通道数据集上表现尤为突出。

Conclusion: WaveTS系列模型在轻量化和多通道依赖处理上具有显著优势，适用于时间序列预测任务。

Abstract: The field of time series forecasting is rapidly advancing, with recent
large-scale Transformers and lightweight Multilayer Perceptron (MLP) models
showing strong predictive performance. However, conventional Transformer models
are often hindered by their large number of parameters and their limited
ability to capture non-stationary features in data through smoothing.
Similarly, MLP models struggle to manage multi-channel dependencies
effectively. To address these limitations, we propose a novel, lightweight time
series prediction model, WaveTS-B. This model combines wavelet transforms with
MLP to capture both periodic and non-stationary characteristics of data in the
wavelet domain. Building on this foundation, we propose a channel clustering
strategy that incorporates a Mixture of Experts (MoE) framework, utilizing a
gating mechanism and expert network to handle multi-channel dependencies
efficiently. We propose WaveTS-M, an advanced model tailored for multi-channel
time series prediction. Empirical evaluation across eight real-world time
series datasets demonstrates that our WaveTS series models achieve
state-of-the-art (SOTA) performance with significantly fewer parameters.
Notably, WaveTS-M shows substantial improvements on multi-channel datasets,
highlighting its effectiveness.

</details>


### [64] [Flow Battery Manifold Design with Heterogeneous Inputs Through Generative Adversarial Neural Networks](https://arxiv.org/abs/2508.08863)
*Eric Seng,Hugh O'Connor,Adam Boyce,Josh J. Bailey,Anton van Beek*

Main category: cs.LG

TL;DR: 提出了一种系统性框架，用于构建生成模型的训练数据集，并通过结合贝叶斯优化增强设计的可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成机器学习在设计表示和探索中具有潜力，但受限于需要大量现有设计数据和缺乏对最优特征的解读。

Method: 开发了系统性框架生成内部同质但相互异质的输入原型，并结合生成模型与贝叶斯优化。

Result: 验证了框架在流电池歧管设计中的有效性，能够捕捉可行设计空间并实现高效探索。

Conclusion: 该工作通过提升质量和可靠性，扩展了生成机器学习在系统设计中的适用性。

Abstract: Generative machine learning has emerged as a powerful tool for design
representation and exploration. However, its application is often constrained
by the need for large datasets of existing designs and the lack of
interpretability about what features drive optimality. To address these
challenges, we introduce a systematic framework for constructing training
datasets tailored to generative models and demonstrate how these models can be
leveraged for interpretable design. The novelty of this work is twofold: (i) we
present a systematic framework for generating archetypes with internally
homogeneous but mutually heterogeneous inputs that can be used to generate a
training dataset, and (ii) we show how integrating generative models with
Bayesian optimization can enhance the interpretability of the latent space of
admissible designs. These findings are validated by using the framework to
design a flow battery manifold, demonstrating that it effectively captures the
space of feasible designs, including novel configurations while enabling
efficient exploration. This work broadens the applicability of generative
machine-learning models in system designs by enhancing quality and reliability.

</details>


### [65] [Oblivionis: A Lightweight Learning and Unlearning Framework for Federated Large Language Models](https://arxiv.org/abs/2508.08875)
*Fuyao Zhang,Xinyu Yan,Tiantong Wu,Wenjie Li,Tianxiang Chen,Yang Cao,Ran Yan,Longtao Huang,Wei Yang Bryan Lim,Qiang Yang*

Main category: cs.LG

TL;DR: Oblivionis框架解决了联邦学习中LLM的遗忘问题，实现了选择性删除私有数据，提升合规性和信任度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的LLM缺乏内置机制满足GDPR等法规的遗忘权需求，且现有框架无法选择性删除客户贡献。

Method: 提出Oblivionis框架，将联邦学习和遗忘作为双重优化目标，整合6种FL和5种遗忘算法进行评估。

Result: 实验表明Oblivionis优于本地训练，在遗忘效果和模型效用间取得平衡。

Conclusion: Oblivionis为联邦LLM遗忘提供了高效解决方案，并为未来LLM发展指明方向。

Abstract: Large Language Models (LLMs) increasingly leverage Federated Learning (FL) to
utilize private, task-specific datasets for fine-tuning while preserving data
privacy. However, while federated LLM frameworks effectively enable
collaborative training without raw data sharing, they critically lack built-in
mechanisms for regulatory compliance like GDPR's right to be forgotten.
Integrating private data heightens concerns over data quality and long-term
governance, yet existing distributed training frameworks offer no principled
way to selectively remove specific client contributions post-training. Due to
distributed data silos, stringent privacy constraints, and the intricacies of
interdependent model aggregation, federated LLM unlearning is significantly
more complex than centralized LLM unlearning. To address this gap, we introduce
Oblivionis, a lightweight learning and unlearning framework that enables
clients to selectively remove specific private data during federated LLM
training, enhancing trustworthiness and regulatory compliance. By unifying FL
and unlearning as a dual optimization objective, we incorporate 6 FL and 5
unlearning algorithms for comprehensive evaluation and comparative analysis,
establishing a robust pipeline for federated LLM unlearning. Extensive
experiments demonstrate that Oblivionis outperforms local training, achieving a
robust balance between forgetting efficacy and model utility, with
cross-algorithm comparisons providing clear directions for future LLM
development.

</details>


### [66] [Towards Scalable Lottery Ticket Networks using Genetic Algorithms](https://arxiv.org/abs/2508.08877)
*Julian Schönberger,Maximilian Zorn,Jonas Nüßlein,Thomas Gabor,Philipp Altmann*

Main category: cs.LG

TL;DR: 论文提出了一种基于遗传算法的高效深度学习模型设计方法，通过识别强彩票假设中的子网络，无需训练即可达到高精度和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型需要大量参数和训练资源，而强彩票假设表明随机初始化的网络中已存在高性能子网络。本文旨在探索无需梯度信息的遗传算法来识别这些子网络。

Method: 使用遗传算法在随机初始化的神经网络中搜索高性能子网络，适用于二元和多类分类任务。

Result: 实验表明，该方法在精度和稀疏性上优于现有技术，且无需梯度信息。

Conclusion: 遗传算法是识别强彩票子网络的有效方法，同时强调了在复杂架构和任务中需采用合适的评估指标。

Abstract: Building modern deep learning systems that are not just effective but also
efficient requires rethinking established paradigms for model training and
neural architecture design. Instead of adapting highly overparameterized
networks and subsequently applying model compression techniques to reduce
resource consumption, a new class of high-performing networks skips the need
for expensive parameter updates, while requiring only a fraction of parameters,
making them highly scalable. The Strong Lottery Ticket Hypothesis posits that
within randomly initialized, sufficiently overparameterized neural networks,
there exist subnetworks that can match the accuracy of the trained original
model-without any training. This work explores the usage of genetic algorithms
for identifying these strong lottery ticket subnetworks. We find that for
instances of binary and multi-class classification tasks, our approach achieves
better accuracies and sparsity levels than the current state-of-the-art without
requiring any gradient information. In addition, we provide justification for
the need for appropriate evaluation metrics when scaling to more complex
network architectures and learning tasks.

</details>


### [67] [Stationarity Exploration for Multivariate Time Series Forecasting](https://arxiv.org/abs/2508.08919)
*Hao Liu,Chun Yang,Zhang xiaoxing,Rui Ma,Xiaobin Zhu*

Main category: cs.LG

TL;DR: APRNet通过解耦振幅和相位，有效捕捉时间序列中的平稳信息，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从复杂的频率分量中提取平稳信息，APRNet旨在解决这一问题。

Method: 提出APRNet，利用振幅和相位的相互关系，通过KLC模块自适应拟合局部函数。

Result: 实验表明APRNet在捕捉时变模式上优于现有方法。

Conclusion: APRNet是一种简单有效的方法，能够显著提升时间序列预测的准确性。

Abstract: Deep learning-based time series forecasting has found widespread
applications. Recently, converting time series data into the frequency domain
for forecasting has become popular for accurately exploring periodic patterns.
However, existing methods often cannot effectively explore stationary
information from complex intertwined frequency components. In this paper, we
propose a simple yet effective Amplitude-Phase Reconstruct Network (APRNet)
that models the inter-relationships of amplitude and phase, which prevents the
amplitude and phase from being constrained by different physical quantities,
thereby decoupling the distinct characteristics of signals for capturing
stationary information. Specifically, we represent the multivariate time series
input across sequence and channel dimensions, highlighting the correlation
between amplitude and phase at multiple interaction frequencies. We propose a
novel Kolmogorov-Arnold-Network-based Local Correlation (KLC) module to
adaptively fit local functions using univariate functions, enabling more
flexible characterization of stationary features across different amplitudes
and phases. This significantly enhances the model's capability to capture
time-varying patterns. Extensive experiments demonstrate the superiority of our
APRNet against the state-of-the-arts (SOTAs).

</details>


### [68] [Exploring Cross-Stage Adversarial Transferability in Class-Incremental Continual Learning](https://arxiv.org/abs/2508.08920)
*Jungwoo Kim,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 论文研究了持续学习模型在阶段转移攻击中的脆弱性，发现现有防御方法效果不足。


<details>
  <summary>Details</summary>
Motivation: 探索持续学习模型在阶段转移攻击中的安全性问题。

Method: 通过模型相似性和鲁棒性退化分析阶段转移攻击的脆弱性。

Result: 持续学习方法对阶段转移攻击高度敏感，现有防御方法效果有限。

Conclusion: 阶段转移攻击对持续学习模型构成严重安全威胁，需进一步研究防御方法。

Abstract: Class-incremental continual learning addresses catastrophic forgetting by
enabling classification models to preserve knowledge of previously learned
classes while acquiring new ones. However, the vulnerability of the models
against adversarial attacks during this process has not been investigated
sufficiently. In this paper, we present the first exploration of vulnerability
to stage-transferred attacks, i.e., an adversarial example generated using the
model in an earlier stage is used to attack the model in a later stage. Our
findings reveal that continual learning methods are highly susceptible to these
attacks, raising a serious security issue. We explain this phenomenon through
model similarity between stages and gradual robustness degradation.
Additionally, we find that existing adversarial training-based defense methods
are not sufficiently effective to stage-transferred attacks. Codes are
available at https://github.com/mcml-official/CSAT.

</details>


### [69] [LNN-PINN: A Unified Physics-Only Training Framework with Liquid Residual Blocks](https://arxiv.org/abs/2508.08935)
*Ze Tao,Hanxuan Wang,Fujun Liu*

Main category: cs.LG

TL;DR: LNN-PINN是一种改进的物理信息神经网络框架，通过引入轻量级门控机制提升复杂问题中的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在复杂问题中预测准确性有限，需要改进。

Method: 提出LNN-PINN框架，保留原有物理建模和优化流程，仅在隐藏层映射中引入轻量级门控机制。

Result: 在四个基准问题中，LNN-PINN显著降低了RMSE和MAE，并展示了强适应性和稳定性。

Conclusion: LNN-PINN为复杂科学和工程问题提供了一种简洁有效的架构改进方案。

Abstract: Physics-informed neural networks (PINNs) have attracted considerable
attention for their ability to integrate partial differential equation priors
into deep learning frameworks; however, they often exhibit limited predictive
accuracy when applied to complex problems. To address this issue, we propose
LNN-PINN, a physics-informed neural network framework that incorporates a
liquid residual gating architecture while preserving the original physics
modeling and optimization pipeline to improve predictive accuracy. The method
introduces a lightweight gating mechanism solely within the hidden-layer
mapping, keeping the sampling strategy, loss composition, and hyperparameter
settings unchanged to ensure that improvements arise purely from architectural
refinement. Across four benchmark problems, LNN-PINN consistently reduced RMSE
and MAE under identical training conditions, with absolute error plots further
confirming its accuracy gains. Moreover, the framework demonstrates strong
adaptability and stability across varying dimensions, boundary conditions, and
operator characteristics. In summary, LNN-PINN offers a concise and effective
architectural enhancement for improving the predictive accuracy of
physics-informed neural networks in complex scientific and engineering
problems.

</details>


### [70] [Generalising Traffic Forecasting to Regions without Traffic Observations](https://arxiv.org/abs/2508.08947)
*Xinyu Su,Majid Sarvi,Feng Liu,Egemen Tanin,Jianzhong Qi*

Main category: cs.LG

TL;DR: 提出GenCast模型，利用外部知识和物理约束提升无传感器区域的交通预测能力。


<details>
  <summary>Details</summary>
Motivation: 由于传感器部署成本高，部分区域缺乏历史交通数据，现有模型泛化能力受限。

Method: 结合物理信息神经网络、外部信号学习模块和空间分组模块，补偿缺失数据并提升泛化性。

Result: 在多组真实数据集上显著降低预测误差。

Conclusion: GenCast通过整合外部知识和物理约束，有效提升了无传感器区域的交通预测能力。

Abstract: Traffic forecasting is essential for intelligent transportation systems.
Accurate forecasting relies on continuous observations collected by traffic
sensors. However, due to high deployment and maintenance costs, not all regions
are equipped with such sensors. This paper aims to forecast for regions without
traffic sensors, where the lack of historical traffic observations challenges
the generalisability of existing models. We propose a model named GenCast, the
core idea of which is to exploit external knowledge to compensate for the
missing observations and to enhance generalisation. We integrate
physics-informed neural networks into GenCast, enabling physical principles to
regularise the learning process. We introduce an external signal learning
module to explore correlations between traffic states and external signals such
as weather conditions, further improving model generalisability. Additionally,
we design a spatial grouping module to filter localised features that hinder
model generalisability. Extensive experiments show that GenCast consistently
reduces forecasting errors on multiple real-world datasets.

</details>


### [71] [GRAVITY: A Controversial Graph Representation Learning for Vertex Classification](https://arxiv.org/abs/2508.08954)
*Etienne Gael Tajeuna,Jean Marie Tshimula*

Main category: cs.LG

TL;DR: GRAVITY是一种基于图的学习框架，通过顶点间的动态交互和潜在势场优化顶点分类。


<details>
  <summary>Details</summary>
Motivation: 传统图学习方法依赖静态邻域信息，无法动态调整顶点间的交互。GRAVITY旨在通过物理启发的动态力场模型改进分类效果。

Method: GRAVITY将顶点建模为通过结构邻近性和属性相似性学习的交互力，形成潜在势场，顶点在势场中移动以优化分类。

Result: 实验表明，GRAVITY在转导和归纳顶点分类任务中均表现优异。

Conclusion: GRAVITY通过动态力场模型显著提升了顶点分类的准确性和语义一致性。

Abstract: In the quest of accurate vertex classification, we introduce GRAVITY
(Graph-based Representation leArning via Vertices Interaction TopologY), a
framework inspired by physical systems where objects self-organize under
attractive forces. GRAVITY models each vertex as exerting influence through
learned interactions shaped by structural proximity and attribute similarity.
These interactions induce a latent potential field in which vertices move
toward energy efficient positions, coalescing around class-consistent
attractors and distancing themselves from unrelated groups. Unlike traditional
message-passing schemes with static neighborhoods, GRAVITY adaptively modulates
the receptive field of each vertex based on a learned force function, enabling
dynamic aggregation driven by context. This field-driven organization sharpens
class boundaries and promotes semantic coherence within latent clusters.
Experiments on real-world benchmarks show that GRAVITY yields competitive
embeddings, excelling in both transductive and inductive vertex classification
tasks.

</details>


### [72] [Fre-CW: Targeted Attack on Time Series Forecasting using Frequency Domain Loss](https://arxiv.org/abs/2508.08955)
*Naifu Feng,Lixing Chen,Junhua Tang,Hua Ding,Jianhua Li,Yang Bai*

Main category: cs.LG

TL;DR: 本文提出了一种基于频域损失的时间序列预测攻击算法，填补了频域信息在对抗攻击中的研究空白，并验证了现有模型的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列预测中易受对抗攻击，而频域特征在此领域的对抗攻击研究不足。

Method: 将分类任务的攻击方法迁移至预测领域，结合时域和频域损失优化对抗样本。

Result: 实验表明现有时间序列预测模型易受攻击，所提方法在主要数据集上表现优异。

Conclusion: 频域信息在时间序列对抗攻击中具有潜力，现有模型需增强鲁棒性。

Abstract: Transformer-based models have made significant progress in time series
forecasting. However, a key limitation of deep learning models is their
susceptibility to adversarial attacks, which has not been studied enough in the
context of time series prediction. In contrast to areas such as computer
vision, where adversarial robustness has been extensively studied, frequency
domain features of time series data play an important role in the prediction
task but have not been sufficiently explored in terms of adversarial attacks.
This paper proposes a time series prediction attack algorithm based on
frequency domain loss. Specifically, we adapt an attack method originally
designed for classification tasks to the prediction field and optimize the
adversarial samples using both time-domain and frequency-domain losses. To the
best of our knowledge, there is no relevant research on using frequency
information for time-series adversarial attacks. Our experimental results show
that these current time series prediction models are vulnerable to adversarial
attacks, and our approach achieves excellent performance on major time series
forecasting datasets.

</details>


### [73] [Low-Regret and Low-Complexity Learning for Hierarchical Inference](https://arxiv.org/abs/2508.08985)
*Sameep Chattopadhyay,Vinay Sutar,Jaya Prakash Champati,Sharayu Moharir*

Main category: cs.LG

TL;DR: 论文提出了一种基于置信度估计的分层推理学习方法（HIL），通过本地和远程模型的协作优化延迟、准确性和带宽使用。


<details>
  <summary>Details</summary>
Motivation: 解决边缘智能系统中分层推理（HI）的动态数据分布和成本变化问题，优化本地推理的准确性估计。

Method: 提出HI-LCB和HI-LCB-lite两种策略，基于UCB框架，利用本地模型置信度建模正确推理概率。

Result: 两种策略实现了$O(\log T)$的遗憾值，优于现有方法的$O(T^{2/3})$，且HI-LCB-lite计算复杂度为$O(1)$。

Conclusion: 新策略在资源受限设备上表现优异，仿真实验验证了其优于现有HIL方法。

Abstract: This work focuses on Hierarchical Inference (HI) in edge intelligence
systems, where a compact Local-ML model on an end-device works in conjunction
with a high-accuracy Remote-ML model on an edge-server. HI aims to reduce
latency, improve accuracy, and lower bandwidth usage by first using the
Local-ML model for inference and offloading to the Remote-ML only when the
local inference is likely incorrect. A critical challenge in HI is estimating
the likelihood of the local inference being incorrect, especially when data
distributions and offloading costs change over time -- a problem we term
Hierarchical Inference Learning (HIL). We introduce a novel approach to HIL by
modeling the probability of correct inference by the Local-ML as an increasing
function of the model's confidence measure, a structure motivated by empirical
observations but previously unexploited. We propose two policies, HI-LCB and
HI-LCB-lite, based on the Upper Confidence Bound (UCB) framework. We
demonstrate that both policies achieve order-optimal regret of $O(\log T)$, a
significant improvement over existing HIL policies with $O(T^{2/3})$ regret
guarantees. Notably, HI-LCB-lite has an $O(1)$ per-sample computational
complexity, making it well-suited for deployment on devices with severe
resource limitations. Simulations using real-world datasets confirm that our
policies outperform existing state-of-the-art HIL methods.

</details>


### [74] [MechaFormer: Sequence Learning for Kinematic Mechanism Design Automation](https://arxiv.org/abs/2508.09005)
*Diana Bolanos,Mohammadmehdi Ataei,Pradeep Kumar Jayaraman*

Main category: cs.LG

TL;DR: MechaFormer是一种基于Transformer的模型，通过将机械设计视为条件序列生成任务，显著提升了路径匹配精度，并生成多样化的新颖设计。


<details>
  <summary>Details</summary>
Motivation: 机械机制设计是一个复杂且搜索空间巨大的工程问题，传统方法难以高效解决。

Method: MechaFormer将目标曲线转化为领域特定语言（DSL）字符串，统一确定拓扑结构和几何参数。

Result: 模型在路径匹配精度上显著优于现有基线，生成的设计多样且有效。

Conclusion: MechaFormer为传统优化器提供了高质量起点，形成了一种高效混合方法。

Abstract: Designing mechanical mechanisms to trace specific paths is a classic yet
notoriously difficult engineering problem, characterized by a vast and complex
search space of discrete topologies and continuous parameters. We introduce
MechaFormer, a Transformer-based model that tackles this challenge by treating
mechanism design as a conditional sequence generation task. Our model learns to
translate a target curve into a domain-specific language (DSL) string,
simultaneously determining the mechanism's topology and geometric parameters in
a single, unified process. MechaFormer significantly outperforms existing
baselines, achieving state-of-the-art path-matching accuracy and generating a
wide diversity of novel and valid designs. We demonstrate a suite of sampling
strategies that can dramatically improve solution quality and offer designers
valuable flexibility. Furthermore, we show that the high-quality outputs from
MechaFormer serve as excellent starting points for traditional optimizers,
creating a hybrid approach that finds superior solutions with remarkable
efficiency.

</details>


### [75] [Causal Machine Learning for Patient-Level Intraoperative Opioid Dose Prediction from Electronic Health Records](https://arxiv.org/abs/2508.09059)
*Jonas Valbjørn Andersena,Anders Peder Højer Karlsen,Markus Harboe Olsen,Nikolaj Krebs Pedersen*

Main category: cs.LG

TL;DR: OPIAID算法是一种基于机器学习的个性化阿片类药物剂量预测与推荐方法，旨在优化疼痛管理并减少不良反应。


<details>
  <summary>Details</summary>
Motivation: 通过个性化剂量推荐，优化疼痛管理并减少阿片类药物相关不良事件（ORADE）。

Method: 利用观察性电子健康记录（EHR）数据训练机器学习模型，采用因果机器学习方法分析剂量、患者特征与结果的关系。

Result: 算法能够根据患者特征和阿片类药物类型提供个性化剂量建议。

Conclusion: OPIAID算法为个性化阿片类药物剂量推荐提供了有效方法，需进一步验证其性能。

Abstract: This paper introduces the OPIAID algorithm, a novel approach for predicting
and recommending personalized opioid dosages for individual patients. The
algorithm optimizes pain management while minimizing opioid related adverse
events (ORADE) by employing machine learning models trained on observational
electronic health records (EHR) data. It leverages a causal machine learning
approach to understand the relationship between opioid dose, case specific
patient and intraoperative characteristics, and pain versus ORADE outcomes. The
OPIAID algorithm considers patient-specific characteristics and the influence
of different opiates, enabling personalized dose recommendations. This paper
outlines the algorithm's methodology and architecture, and discusses key
assumptions, and approaches to evaluating its performance.

</details>


### [76] [Meta-learning optimizes predictions of missing links in real-world networks](https://arxiv.org/abs/2508.09069)
*Bisman Singh,Lucy Van Kleunen,Aaron Clauset*

Main category: cs.LG

TL;DR: 论文系统比较了多种链接预测算法在550个真实网络上的表现，发现模型堆叠（随机森林）在图神经网络中表现最佳，并提出了基于网络特性的元学习算法。


<details>
  <summary>Details</summary>
Motivation: 解决网络中链接预测问题，明确不同算法在不同网络特性下的表现差异。

Method: 比较了四种堆叠算法、42种拓扑链接预测器和两种图神经网络算法，并提出了新的元学习算法。

Result: 模型堆叠（随机森林）在AUC上表现最佳，图神经网络在Top-k上表现竞争性。元学习算法能根据网络特性优化预测。

Conclusion: 算法表现依赖网络特性，元学习算法能显著提升预测性能并扩展到大型网络。

Abstract: Relational data are ubiquitous in real-world data applications, e.g., in
social network analysis or biological modeling, but networks are nearly always
incompletely observed. The state-of-the-art for predicting missing links in the
hard case of a network without node attributes uses model stacking or neural
network techniques. It remains unknown which approach is best, and whether or
how the best choice of algorithm depends on the input network's
characteristics. We answer these questions systematically using a large,
structurally diverse benchmark of 550 real-world networks under two standard
accuracy measures (AUC and Top-k), comparing four stacking algorithms with 42
topological link predictors, two of which we introduce here, and two graph
neural network algorithms. We show that no algorithm is best across all input
networks, all algorithms perform well on most social networks, and few perform
well on economic and biological networks. Overall, model stacking with a random
forest is both highly scalable and surpasses on AUC or is competitive with
graph neural networks on Top-k accuracy. But, algorithm performance depends
strongly on network characteristics like the degree distribution, triangle
density, and degree assortativity. We introduce a meta-learning algorithm that
exploits this variability to optimize link predictions for individual networks
by selecting the best algorithm to apply, which we show outperforms all
state-of-the-art algorithms and scales to large networks.

</details>


### [77] [Chi-Geometry: A Library for Benchmarking Chirality Prediction of GNNs](https://arxiv.org/abs/2508.09097)
*Rylie Weaver,Massamiliano Lupo Pasini*

Main category: cs.LG

TL;DR: Chi-Geometry是一个生成用于测试和基准化GNN预测手性能力的图数据的库，支持用户指定几何和拓扑特征，并生成带有手性中心标记的合成图样本。


<details>
  <summary>Details</summary>
Motivation: 为了更可解释且减少混淆地评估GNN在手性预测任务中的表现，并指导设计性能更好的GNN架构。

Method: Chi-Geometry生成带有随机节点位置和物种的合成图样本，每个图包含一个标记为R或S的手性中心，其余节点标记为N/A。

Result: 通过基准化多种SOTA GNN架构，设计出两种新架构：一种通过全连接提高准确性但计算成本高；另一种引入虚拟节点保持线性计算成本同时保持竞争力。

Conclusion: Chi-Geometry为手性预测任务提供了有效的基准化工具，并指导了两种新GNN架构的设计，分别解决了准确性和计算成本的问题。

Abstract: We introduce Chi-Geometry - a library that generates graph data for testing
and benchmarking GNNs' ability to predict chirality. Chi-Geometry generates
synthetic graph samples with (i) user-specified geometric and topological
traits to isolate certain types of samples and (ii) randomized node positions
and species to minimize extraneous correlations. Each generated graph contains
exactly one chiral center labeled either R or S, while all other nodes are
labeled N/A (non-chiral). The generated samples are then combined into a
cohesive dataset that can be used to assess a GNN's ability to predict
chirality as a node classification task. Chi-Geometry allows more interpretable
and less confounding benchmarking of GNNs for prediction of chirality in the
graph samples which can guide the design of new GNN architectures with improved
predictive performance. We illustrate Chi-Geometry's efficacy by using it to
generate synthetic datasets for benchmarking various state-of-the-art (SOTA)
GNN architectures. The conclusions of these benchmarking results guided our
design of two new GNN architectures. The first GNN architecture established
all-to-all connections in the graph to accurately predict chirality across all
challenging configurations where previously tested SOTA models failed, but at a
computational cost (both for training and inference) that grows quadratically
with the number of graph nodes. The second GNN architecture avoids all-to-all
connections by introducing a virtual node in the original graph structure of
the data, which restores the linear scaling of training and inference
computational cost with respect to the number of nodes in the graph, while
still ensuring competitive accuracy in detecting chirality with respect to SOTA
GNN architectures.

</details>


### [78] [Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving](https://arxiv.org/abs/2508.09099)
*Tianyun Yang,Yunwen Li,Ziniu Li,Zhihang Lin,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: 论文提出了一种结合自然语言推理和形式化语言的几何问题解决方法，通过混合推理轨迹和强化学习优化，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在几何问题解决（GPS）中存在局限性，主要是由于不可靠的图表解释和纯自然语言推理。直接生成形式化程序缺乏中间推理，导致决策过程不透明且易出错。

Method: 提出了一种结合Chain-of-Thought（CoT）和形式化语言的方法，模型交替生成自然语言推理和可执行的代码。通过监督微调和强化学习优化推理轨迹和程序。

Result: 新模型GF-Reasoner在标准GPS基准上实现了高达15%的准确率提升，优于同类7B规模模型和更大的72B模型。生成的推理轨迹更短且更清晰。

Conclusion: 通过结合高阶几何知识和符号计算，该方法显著提升了几何问题解决的性能，并为未来研究提供了设计选择的详细分析。

Abstract: Large vision language models exhibit notable limitations on Geometry Problem
Solving (GPS) because of their unreliable diagram interpretation and pure
natural-language reasoning. A recent line of work mitigates this by using
symbolic solvers: the model directly generates a formal program that a geometry
solver can execute. However, this direct program generation lacks intermediate
reasoning, making the decision process opaque and prone to errors. In this
work, we explore a new approach that integrates Chain-of-Thought (CoT) with
formal language. The model interleaves natural language reasoning with
incremental emission of solver-executable code, producing a hybrid reasoning
trace in which critical derivations are expressed in formal language. To teach
this behavior at scale, we combine (1) supervised fine-tuning on an 11K newly
developed synthetic dataset with interleaved natural language reasoning and
automatic formalization, and (2) solver-in-the-loop reinforcement learning that
jointly optimizes both the CoT narrative and the resulting program through
outcome-based rewards. Built on Qwen2.5-VL-7B, our new model, named
GF-Reasoner, achieves up to 15% accuracy improvements on standard GPS
benchmarks, surpassing both 7B-scale peers and the much larger model
Qwen2.5-VL-72B. By exploiting high-order geometric knowledge and offloading
symbolic computation to the solver, the generated reasoning traces are
noticeably shorter and cleaner. Furthermore, we present a comprehensive
analysis of method design choices (e.g., reasoning paradigms, data synthesis,
training epochs, etc.), providing actionable insights for future research.

</details>


### [79] [Towards Universal Neural Inference](https://arxiv.org/abs/2508.09100)
*Shreyas Bhat Brahmavar,Yang Li,Junier Oliva*

Main category: cs.LG

TL;DR: ASPIRE是一种通用的神经推理模型，用于处理异构结构化数据的语义推理和预测，支持跨数据集的特征依赖学习。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据通常具有多样化和不连贯的形式（如不同的模式、不一致的语义和无固定特征顺序），这使得构建通用模型以跨数据集利用信息变得困难。

Method: ASPIRE结合了基于集合的Transformer（具有排列不变性）和语义基础模块，利用自然语言描述、数据集元数据和上下文示例来学习跨数据集特征依赖。

Result: ASPIRE在多样化基准测试中表现优异，并支持在开放世界设置中进行成本感知的主动特征获取。

Conclusion: ASPIRE为实现真正通用的、语义感知的结构化数据推理迈出了一步。

Abstract: Real-world data often appears in diverse, disjoint forms -- with varying
schemas, inconsistent semantics, and no fixed feature ordering -- making it
challenging to build general-purpose models that can leverage information
across datasets. We introduce ASPIRE, Arbitrary Set-based Permutation-Invariant
Reasoning Engine, a Universal Neural Inference model for semantic reasoning and
prediction over heterogeneous structured data. ASPIRE combines a
permutation-invariant, set-based Transformer with a semantic grounding module
that incorporates natural language descriptions, dataset metadata, and
in-context examples to learn cross-dataset feature dependencies. This
architecture allows ASPIRE to ingest arbitrary sets of feature--value pairs and
support examples, align semantics across disjoint tables, and make predictions
for any specified target. Once trained, ASPIRE generalizes to new inference
tasks without additional tuning. In addition to delivering strong results
across diverse benchmarks, ASPIRE naturally supports cost-aware active feature
acquisition in an open-world setting, selecting informative features under
test-time budget constraints for an arbitrary unseen dataset. These
capabilities position ASPIRE as a step toward truly universal, semantics-aware
inference over structured data.

</details>


### [80] [Deep Neural Network Calibration by Reducing Classifier Shift with Stochastic Masking](https://arxiv.org/abs/2508.09116)
*Jiani Ni,He Zhao,Yibo Yang,Dandan Guo*

Main category: cs.LG

TL;DR: 提出了一种基于掩码的分类器校准方法MaC-Cal，通过随机稀疏性提升置信度与准确性的对齐，解决了DNN校准不足的问题。


<details>
  <summary>Details</summary>
Motivation: DNN在安全关键场景中因校准不足可能导致严重后果，现有方法忽视欠置信问题，需改进。

Method: 采用两阶段训练方案和自适应稀疏性，动态调整掩码保留率以对齐置信度与准确性。

Result: 实验表明MaC-Cal在校准性能和数据损坏下的鲁棒性上表现优异。

Conclusion: MaC-Cal为DNN提供了可靠置信度估计的实用有效解决方案。

Abstract: In recent years, deep neural networks (DNNs) have shown competitive results
in many fields. Despite this success, they often suffer from poor calibration,
especially in safety-critical scenarios such as autonomous driving and
healthcare, where unreliable confidence estimates can lead to serious
consequences. Recent studies have focused on improving calibration by modifying
the classifier, yet such efforts remain limited. Moreover, most existing
approaches overlook calibration errors caused by underconfidence, which can be
equally detrimental. To address these challenges, we propose MaC-Cal, a novel
mask-based classifier calibration method that leverages stochastic sparsity to
enhance the alignment between confidence and accuracy. MaC-Cal adopts a
two-stage training scheme with adaptive sparsity, dynamically adjusting mask
retention rates based on the deviation between confidence and accuracy.
Extensive experiments show that MaC-Cal achieves superior calibration
performance and robustness under data corruption, offering a practical and
effective solution for reliable confidence estimation in DNNs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [81] [On Experiments](https://arxiv.org/abs/2508.08288)
*Brendan van Rooyen*

Main category: stat.ML

TL;DR: 本文提出了一种数学语言来描述科学过程，并贡献了新的数据处理不等式、偏差方差分解、简化证明方法以及线性规划计算缺陷的手段。


<details>
  <summary>Details</summary>
Motivation: 为了自动化科学过程，需要将其用精确的数学语言描述，本文旨在提供这样一种语言。

Method: 基于历史和现代思想，提出新的数学工具，包括数据处理不等式、偏差方差分解等。

Result: 提供了新的理论工具和简化证明方法，支持科学过程的数学建模。

Conclusion: 本文为科学过程的数学描述提供了新工具和方法，推动了自动化研究的发展。

Abstract: The scientific process is a means for turning the results of experiments into
knowledge about the world in which we live. Much research effort has been
directed toward automating this process. To do this, one needs to formulate the
scientific process in a precise mathematical language. This paper outlines one
such language. What is presented here is hardly new. The material leans much on
great thinkers of times past as well as more modern contributions. The novel
contributions of this paper are: A new, general data processing inequality, a
bias variance decomposition for canonical losses, Streamlined proofs of the
Blackwell-Sherman-Stein and Randomization Theorems, and Means to calculate
deficiency via linear programming.

</details>


### [82] [Projection-based multifidelity linear regression for data-scarce applications](https://arxiv.org/abs/2508.08517)
*Vignesh Sella,Julie Pham,Karen Willcox,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: 该论文提出两种基于投影的多保真线性回归方法，用于高维输出系统，通过结合低保真和高保真数据提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 高维系统的代理建模在训练数据有限且成本高昂时面临挑战，多保真方法通过整合低保真和高保真数据来解决这一问题。

Method: 引入两种基于主成分分析的投影方法：(i) 直接数据增强，(ii) 结合线性校正的数据增强，并通过加权最小二乘法训练模型。

Result: 在仅10个高保真样本的低数据情况下，多保真方法比单保真方法的中值精度提高了3%-12%。

Conclusion: 多保真线性回归方法在高维输出系统中显著提升了模型精度，尤其在数据有限的情况下表现优异。

Abstract: Surrogate modeling for systems with high-dimensional quantities of interest
remains challenging, particularly when training data are costly to acquire.
This work develops multifidelity methods for multiple-input multiple-output
linear regression targeting data-limited applications with high-dimensional
outputs. Multifidelity methods integrate many inexpensive low-fidelity model
evaluations with limited, costly high-fidelity evaluations. We introduce two
projection-based multifidelity linear regression approaches that leverage
principal component basis vectors for dimensionality reduction and combine
multifidelity data through: (i) a direct data augmentation using low-fidelity
data, and (ii) a data augmentation incorporating explicit linear corrections
between low-fidelity and high-fidelity data. The data augmentation approaches
combine high-fidelity and low-fidelity data into a unified training set and
train the linear regression model through weighted least squares with
fidelity-specific weights. Various weighting schemes and their impact on
regression accuracy are explored. The proposed multifidelity linear regression
methods are demonstrated on approximating the surface pressure field of a
hypersonic vehicle in flight. In a low-data regime of no more than ten
high-fidelity samples, multifidelity linear regression achieves approximately
3% - 12% improvement in median accuracy compared to single-fidelity methods
with comparable computational cost.

</details>


### [83] [In-Context Learning as Nonparametric Conditional Probability Estimation: Risk Bounds and Optimality](https://arxiv.org/abs/2508.08673)
*Chenrui Liu,Falong Tan,Chuanlong Xie,Yicheng Zeng,Lixing Zhu*

Main category: stat.ML

TL;DR: 本文研究了多类分类中上下文学习（ICL）的预期超额风险，建立了基于KL散度的新oracle不等式，并证明了ICL估计器在条件概率估计中达到最优速率。


<details>
  <summary>Details</summary>
Motivation: 探索ICL在多类分类中的表现，特别是其预期超额风险的界限，以及不同架构（如Transformer和MLP）在ICL中的有效性。

Method: 通过建模任务为标记提示样本和查询输入的序列，使用预训练模型估计查询的条件类概率，并基于KL散度分析风险。

Result: 证明了ICL估计器在条件概率估计中达到最优速率，且MLP在特定假设下也能实现类似效果。

Conclusion: ICL在多类分类中表现优异，且不仅限于Transformer架构，MLP也能实现类似效果。

Abstract: This paper investigates the expected excess risk of In-Context Learning (ICL)
for multiclass classification. We model each task as a sequence of labeled
prompt samples and a query input, where a pre-trained model estimates the
conditional class probabilities of the query. The expected excess risk is
defined as the average truncated Kullback-Leibler (KL) divergence between the
predicted and ground-truth conditional class distributions, averaged over a
specified family of tasks. We establish a new oracle inequality for the
expected excess risk based on KL divergence in multiclass classification. This
allows us to derive tight upper and lower bounds for the expected excess risk
in transformer-based models, demonstrating that the ICL estimator achieves the
minimax optimal rate - up to a logarithmic factor - for conditional probability
estimation. From a technical standpoint, our results introduce a novel method
for controlling generalization error using the uniform empirical covering
entropy of the log-likelihood function class. Furthermore, we show that
multilayer perceptrons (MLPs) can also perform ICL and achieve this optimal
rate under specific assumptions, suggesting that transformers may not be the
exclusive architecture capable of effective ICL.

</details>


### [84] [Hierarchical Variable Importance with Statistical Control for Medical Data-Based Prediction](https://arxiv.org/abs/2508.08724)
*Joseph Paillard,Antoine Collas,Denis A. Engemann,Bertrand Thirion*

Main category: stat.ML

TL;DR: Hierarchical-CPI 是一种模型无关的变量重要性度量方法，通过分层树探索变量组，解决了高相关性数据中的条件重要性消失问题，并在神经影像数据中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂模型在医学影像中可解释性不足的问题，尤其是高相关性数据下现有方法的局限性。

Method: 提出 Hierarchical-CPI，通过分层树结构探索变量组，结合树基重要性分配机制，控制家族错误率。

Result: 在 ADNI 和 TDBRAIN 数据集中验证了方法的有效性，识别出生物学合理的变量。

Conclusion: Hierarchical-CPI 在高相关性数据中表现优越，为医学影像分析提供了更可靠的变量重要性度量。

Abstract: Recent advances in machine learning have greatly expanded the repertoire of
predictive methods for medical imaging. However, the interpretability of
complex models remains a challenge, which limits their utility in medical
applications. Recently, model-agnostic methods have been proposed to measure
conditional variable importance and accommodate complex non-linear models.
However, they often lack power when dealing with highly correlated data, a
common problem in medical imaging. We introduce Hierarchical-CPI, a
model-agnostic variable importance measure that frames the inference problem as
the discovery of groups of variables that are jointly predictive of the
outcome. By exploring subgroups along a hierarchical tree, it remains
computationally tractable, yet also enjoys explicit family-wise error rate
control. Moreover, we address the issue of vanishing conditional importance
under high correlation with a tree-based importance allocation mechanism. We
benchmarked Hierarchical-CPI against state-of-the-art variable importance
methods. Its effectiveness is demonstrated in two neuroimaging datasets:
classifying dementia diagnoses from MRI data (ADNI dataset) and analyzing the
Berger effect on EEG data (TDBRAIN dataset), identifying biologically plausible
variables.

</details>


### [85] [Bio-Inspired Artificial Neural Networks based on Predictive Coding](https://arxiv.org/abs/2508.08762)
*Davide Casnici,Charlotte Frenkel,Justin Dauwels*

Main category: stat.ML

TL;DR: 本文介绍了反向传播（BP）的局限性，并提出预测编码（PC）作为生物启发的替代方案，强调其局部更新和理论优势。


<details>
  <summary>Details</summary>
Motivation: BP依赖全局误差信号，而生物大脑可能使用局部更新机制，如Hebbian学习。PC作为一种生物可行的替代方案，具有局部更新和自动梯度缩放等优势。

Method: PC基于自由能原理，利用局部信息进行权重更新，并与BP和卡尔曼滤波等优化算法建立联系。

Result: PC提供了一种更接近生物学习的训练方法，具有理论优势和实际应用潜力。

Conclusion: PC是BP的有前景替代方案，适合生物启发学习和实际实现，文中还提供了Python实现示例。

Abstract: Backpropagation (BP) of errors is the backbone training algorithm for
artificial neural networks (ANNs). It updates network weights through gradient
descent to minimize a loss function representing the mismatch between
predictions and desired outputs. BP uses the chain rule to propagate the loss
gradient backward through the network hierarchy, allowing efficient weight
updates. However, this process requires weight updates at every layer to rely
on a global error signal generated at the network's output.
  In contrast, the Hebbian model of synaptic plasticity states that weight
updates are local, depending only on the activity of pre- and post-synaptic
neurons. This suggests biological brains likely do not implement BP directly.
Recently, Predictive Coding (PC) has gained interest as a biologically
plausible alternative that updates weights using only local information.
Originating from 1950s work on signal compression, PC was later proposed as a
model of the visual cortex and formalized under the free energy principle,
linking it to Bayesian inference and dynamical systems. PC weight updates rely
solely on local information and provide theoretical advantages such as
automatic scaling of gradients based on uncertainty.
  This lecture notes column offers a novel, tutorial-style introduction to PC,
focusing on its formulation, derivation, and connections to well-known
optimization and signal processing algorithms such as BP and the Kalman Filter
(KF). It aims to support existing literature by guiding readers from the
mathematical foundations of PC to practical implementation, including Python
examples using PyTorch.

</details>
