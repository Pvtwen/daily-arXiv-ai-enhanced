<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.LG](#cs.LG) [Total: 82]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Leap-on-Success Exhaustive Search Method to Find Optimal Robust Minimum Redundancy Arrays (RMRAs): New Array Configurations for Sensor Counts 11 to 20](https://arxiv.org/abs/2507.10706)
*Pradyumna Kunchala,Ashish Patwari*

Main category: eess.SP

TL;DR: 本文提出了针对11至15个传感器的优化RMRA配置，并验证了其鲁棒性，同时展示了适用于更大阵列的近优配置。


<details>
  <summary>Details</summary>
Motivation: 设计能够在单个传感器故障时仍保持准确方向估计的冗余稀疏阵列。

Method: 使用Leap-on-Success穷举搜索算法，高效找到最优RMRA配置，并通过MATLAB模拟验证鲁棒性。

Result: 发现了11至15个传感器的最优RMRA配置，并展示了16至20个传感器的近优配置。

Conclusion: 本研究不仅推进了RMRA设计的先进水平，还提供了一种有效的搜索方法，可用于未来阵列配置优化。

Abstract: Two-fold redundant sparse arrays (TFRAs) are designed to maintain accurate
direction estimation even in the event of a single sensor failure, leveraging
the deliberate coarray redundancy infused into their design. Robust Minimum
Redundancy Arrays (RMRAs), a specialized class of TFRAs, optimize this
redundancy to achieve the maximum possible aperture for a given number of
sensors. However, finding optimal RMRA configurations is an NP-hard problem,
with prior research reporting optimal solutions only for arrays of up to ten
sensors. This paper presents newly discovered optimal RMRA configurations for
array sizes 11 to 15, identified using a novel Leap-on-Success exhaustive
search algorithm that efficiently reduces computational effort by terminating
the search upon locating optimal solutions. The robustness of these arrays was
validated under all single-element failure scenarios using MATLAB simulations,
confirming their superior resilience compared to some existing TFRAs vulnerable
to failures at specific sensor positions. Furthermore, near-optimal
configurations for array sizes 16 to 20 are also reported, highlighting the
potential applicability of the proposed method for larger array designs given
sufficient computational resources. This work not only advances the
state-of-the-art in RMRA design but also introduces an effective search
methodology that can be leveraged for future explorations in array
configuration optimization.

</details>


### [2] [Waterfilling at the Edge: Optimal Percentile Resource Allocation via Risk-Averse Reduction](https://arxiv.org/abs/2507.10838)
*Gokberk Yaylali,Ahmad Ali Khan,Dionysios S. Kalogerias*

Main category: eess.SP

TL;DR: 论文提出了一种基于条件风险价值（CVaR）的资源分配方法，用于优化多终端AWGN信道中的边缘终端传输速率，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于效用的方法（如最小速率、总速率和比例公平）在边缘终端速率优化中表现不佳，缺乏严格和可解释的基础。

Method: 采用CVaR方法，并将其与SLαQ效用等价性结合，通过拉格朗日对偶性提供闭式解，并开发了一种高效的双次梯度下降算法。

Result: 提出的边缘水填充算法能高效分配资源并确保边缘终端速率公平性，数值实验验证了其有效性。

Conclusion: 该方法为边缘终端速率优化提供了严格、可解释且高效的解决方案。

Abstract: We address deterministic resource allocation in point-to-point multi-terminal
AWGN channels without inter-terminal interference, with particular focus on
optimizing quantile transmission rates for cell-edge terminal service.
Classical utility-based approaches -- such as minimum rate, sumrate, and
proportional fairness -- are either overconservative, or inappropriate, or do
not provide a rigorous and/or interpretable foundation for fair rate
optimization at the edge. To overcome these challenges, we employ Conditional
Value-at-Risk (CVaR), a popular coherent risk measure, and establish its
equivalence with the sum-least-$\alpha$th-quantile (SL$\alpha$Q) utility. This
connection enables an exact convex reformulation of the SL$\alpha$Q
maximization problem, facilitating analytical tractability and precise and
interpretable control over cell-edge terminal performance. Utilizing Lagrangian
duality, we provide (for the first time) parameterized closed-form solutions
for the optimal resource policy -- which is of waterfilling-type -- as well as
the associated (auxiliary) Value-at-Risk variable. We further develop a novel
inexact dual subgradient descent algorithm of minimal complexity to determine
globally optimal resource policies, and we rigorously establish its
convergence. The resulting edge waterfilling algorithm iteratively and
efficiently allocates resources while explicitly ensuring transmission rate
fairness across (cell-edge) terminals. Several (even large-scale) numerical
experiments validate the effectiveness of the proposed method for enabling
robust quantile rate optimization at the edge.

</details>


### [3] [Dual RIS-Assisted Monostatic L-Band Radar Target Detection in NLoS Scenarios](https://arxiv.org/abs/2507.11036)
*Salman Liaquat,Ijaz Haider Naqvi,Nor Muzlifah Mahyuddin*

Main category: eess.SP

TL;DR: 论文研究了双RIS辅助雷达在NLoS场景中的性能，表明其SNR优于单RIS系统，并提出通过控制RIS数量和位置优化目标检测。


<details>
  <summary>Details</summary>
Motivation: 在单RIS辅助雷达无法覆盖所有障碍场景时，探索双RIS配置如何提升目标检测的SNR。

Method: 推导了双RIS辅助雷达的SNR表达式，计算接收功率，并分析RIS数量、单元数量和位置对性能的影响。

Result: 双RIS系统在适当配置下可超越单RIS性能，且能通过优化RIS参数实现更精确的目标定位。

Conclusion: 双RIS辅助雷达在NLoS场景中具有显著优势，其性能可通过合理设计进一步提升。

Abstract: The use of a single Reconfigurable Intelligent Surface (RIS) to boost the
signal-to-noise ratio (SNR) at the radar offers significant improvement in
detecting targets, especially in non-line-of-sight (NLoS) scenarios. However,
there are scenarios where no path exists between the radar and the target, even
with a single RIS-assisted radar, due to other present obstacles. This paper
derives an expression for SNR in target detection scenarios where dual RISs
assist a monostatic radar in NLoS situations. We calculate the power received
at the radar through a dual RIS configuration. We show that the SNR performance
of RIS-assisted radars can improve with known locations of the radar and RISs.
Our results demonstrate that the required accuracy in target localization can
be achieved by controlling the number of RISs, the number of unit cells in each
RIS, and properly selecting the locations of RISs to cover the desired region.
The performance of dual RIS-assisted radar systems can surpass that of single
RIS-assisted radar systems under favourable alignment and sufficiently large
RIS sizes.

</details>


### [4] [Optimizing Fluid Antenna Configurations for Constructive Interference Precoding](https://arxiv.org/abs/2507.11093)
*Wenxuan Sun,Mingjie Shao,Luteng Zhu,Yao Ge,Tong Zhang,Zhi Liu*

Main category: eess.SP

TL;DR: 论文提出了一种基于流体天线系统（FAS）的多用户MIMO通信方法，通过优化天线位置和CI预编码，最小化符号错误概率（SEP），并展示了低计算复杂度的优势。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线阵列在多用户MIMO通信中性能受限，FAS通过动态调整天线位置改善传播条件，但如何联合优化SEP和CI预编码尚未解决。

Method: 将SEP最小化问题转化为安全裕度最大化问题，采用平滑技术和块坐标下降（BCD）算法，降低计算复杂度。

Result: 仿真结果表明，该方法在降低误码率（BER）和计算复杂度方面优于固定天线阵列和现有PSO设计的FAS。

Conclusion: FAS结合CI预编码和BCD算法在多用户MIMO通信中具有显著性能提升和低计算复杂度优势。

Abstract: The fluid antenna system (FAS) has emerged as a new physical-layer concept to
provide enhanced propagation conditions for multiuser multiple-input
multiple-output (MIMO) communications over conventional fixed arrays. This work
focuses on minimizing the maximum symbol error probability (SEP) under $M$-ary
phase shift keying (MPSK) signaling in a multiuser downlink equipped with FAS,
where each antenna moves within nonoverlapping intervals. This specific problem
of joint SEP minimization with FAS and constructive interference (CI) precoding
has not been previously addressed. The resulting problem turns out to be a
nonconvex and nonsmooth optimization challenge. We transform the SEP
minimization problem into a safety margin maximization problem in constructive
interference precoding. Then, we customize a smoothing technique and a block
coordinate descent (BCD) algorithm, with emphasis on low computational
complexity. Simulation results show that our approach can reduce bit error rate
(BER) compared to both the fixed arrays and FAS designed by existing particle
swarm optimization (PSO). Also, our approach shows attractively low
computational complexity compared to PSO benchmarks.

</details>


### [5] [Fairness-Aware Secure Integrated Sensing and Communications with Fractional Programming](https://arxiv.org/abs/2507.11224)
*Ali Khandan Boroujeni,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Ghazal Bagheri,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一种新型安全集成感知与通信（ISAC）系统，通过优化问题最大化保密率，并引入熵正则化公平性指标，采用加速二次变换方法解决子问题，验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 设计一个同时服务多通信用户和目标的ISAC系统，确保安全性和公平性。

Method: 提出优化问题最大化保密率，引入熵正则化公平性指标，使用加速二次变换方法迭代求解子问题。

Result: 仿真结果显示在平均保密率、平均数据速率和波束增益方面有性能提升。

Conclusion: 该方法有效优化了ISAC系统的资源分配，确保了安全性和公平性。

Abstract: We propose a novel secure integrated sensing and communications (ISAC) system
designed to serve multiple communication users (CUs) and targets. To that end,
we formulate an optimization problem that maximizes the secrecy rate under
constraints balancing both communication and sensing requirements. To enhance
fairness among users, an entropy-regularized fairness metric is introduced
within the problem framework. We then propose a solution employing an
accelerated quadratic transform (QT) with a non-homogeneous bound to
iteratively solve two subproblems, thereby effectively optimizing the overall
objective. This approach ensures robust security and fairness in resource
allocation for ISAC systems. Finally, simulation results verify the performance
gains in terms of average secrecy rate, average data rate, and beam gain.

</details>


### [6] [Fast and Efficient Implementation of the Maximum Likelihood Estimation for the Linear Regression with Gaussian Model Uncertainty](https://arxiv.org/abs/2507.11249)
*Ruohai Guo,Jiang Zhu,Xing Jiang,Fengzhong Qu*

Main category: eess.SP

TL;DR: 论文扩展了随机变量测量矩阵的线性回归模型分析，证明了MLE问题的凸性和强对偶性，并提出了一种快速统一的GRV-ML算法。


<details>
  <summary>Details</summary>
Motivation: 研究随机测量矩阵均值秩不足时的线性回归模型，填补现有研究的空白。

Method: 扩展分析至超定和欠定系统，证明MLE问题的凸性，提出GRV-ML算法。

Result: 数值模拟验证了理论结果，显示随机性在欠定情况下可能有益。

Conclusion: 论文为更广泛的线性回归问题提供了理论支持和高效算法。

Abstract: The linear regression model with a random variable (RV) measurement matrix,
where the mean of the random measurement matrix has full column rank, has been
extensively studied. In particular, the quasiconvexity of the maximum
likelihood estimation (MLE) problem was established, and the corresponding
Cramer-Rao bound (CRB) was derived, leading to the development of an efficient
bisection-based algorithm known as RV-ML. In contrast, this work extends the
analysis to both overdetermined and underdetermined cases, allowing the mean of
the random measurement matrix to be rank-deficient. A remarkable contribution
is the proof that the equivalent MLE problem is convex and satisfies strong
duality, strengthening previous quasiconvexity results. Moreover, it is shown
that in underdetermined scenarios, the randomness in the measurement matrix can
be beneficial for estimation under certain conditions. In addition, a fast and
unified implementation of the MLE solution, referred to as generalized RV-ML
(GRV-ML), is proposed, which handles a more general case including both
underdetermined and overdetermined systems. Extensive numerical simulations are
provided to validate the theoretical findings.

</details>


### [7] [Sensing Accuracy Optimization for Multi-UAV SAR Interferometry with Data Offloading](https://arxiv.org/abs/2507.11284)
*Mohamed-Amine Lahmeri,Pouya Fakharizadeh,Víctor Mustieles-Pérez,Martin Vossiek,Gerhard Krieger,Robert Schober*

Main category: eess.SP

TL;DR: 论文研究了无人机群在多基线干涉合成孔径雷达（InSAR）感知中的优化，通过进化算法联合优化无人机编队、速度和通信功率分配，显著提高了数字高程模型（DEM）的垂直精度。


<details>
  <summary>Details</summary>
Motivation: 无人机与雷达成像传感器的结合为动态和局部地表过程监测提供了高分辨率、低成本的遥感方案，但多基线InSAR的复杂计算和实时数据传输需求提出了优化挑战。

Method: 使用进化算法（EAs）联合优化无人机编队、速度和通信功率分配，以最小化DEM高度误差并确保感知和通信的服务质量（QoS）。

Result: 数值结果表明，该方法优于遗传算法（GAs）、模拟退火（SA）和深度强化学习（DRL），在多个场景中实现了亚分米级的垂直精度。

Conclusion: 协调无人机群通过雷达干涉测量可实现高精度、实时的地球观测，展示了其巨大潜力。

Abstract: The integration of unmanned aerial vehicles (UAVs) with radar imaging sensors
has revolutionized the monitoring of dynamic and local Earth surface processes
by enabling high-resolution and cost-effective remote sensing. This paper
investigates the optimization of the sensing accuracy of a UAV swarm deployed
to perform multi-baseline interferometric synthetic aperture radar (InSAR)
sensing. In conventional single-baseline InSAR systems, only one synthetic
aperture radar (SAR) antenna pair acquires two SAR images from two distinct
angles to generate a digital elevation model (DEM) of the target area. However,
multi-baseline InSAR extends this concept by aggregating multiple acquisitions
from different angles, thus, significantly enhancing the vertical accuracy of
the DEM. The heavy computations required for this process are performed on the
ground and, therefore, the radar data is transmitted in real time to a ground
station (GS) via a frequency-division multiple access (FDMA) air-to-ground
backhaul link. This work focuses on improving the sensing precision by
minimizing the height error of the averaged DEM while simultaneously ensuring
sensing and communication quality-of-service (QoS). To this end, the UAV
formation, velocity, and communication power allocation are jointly optimized
using evolutionary algorithms (EAs). Our approach is benchmarked against
established optimization methods, including genetic algorithms (GAs), simulated
annealing (SA), and deep reinforcement learning (DRL) techniques. Numerical
results show that the proposed solution outperforms these baseline schemes and
achieves sub-decimeter vertical accuracy in several scenarios. These findings
underline the potential of coordinated UAV swarms for delivering high-precision
and real-time Earth observations through radar interferometry.

</details>


### [8] [Sparse Regression Codes exploit Multi-User Diversity without CSI](https://arxiv.org/abs/2507.11383)
*V S V Sandeep,Sai Dinesh Kancharana,Arun Pachai Kannu*

Main category: eess.SP

TL;DR: 研究了多接收天线非相干平坦衰落信道中的稀疏回归码（SPARC），提出了一种新的实用解码器MLMP，通过部分最大似然度量贪婪地找到用户码字支持。


<details>
  <summary>Details</summary>
Motivation: 探索在多用户多接收天线场景中，如何在没有信道状态信息的情况下实现更好的错误性能和多用户分集。

Method: 提出MLMP解码器，作为逐次组合能量检测器，并改进高码率下的性能。

Result: 在短块长度下，SPARC与MLMP解码器在某些场景中实现了多用户分集，性能优于传统稀疏恢复算法和基于导频的极坐标传输。

Conclusion: SPARC与MLMP解码器在多用户场景中表现出色，优于传统方法，展示了其潜力。

Abstract: We study sparse regression codes (SPARC) for multiple access channels with
multiple receive antennas, in non-coherent flat fading channels. We propose a
novel practical decoder, referred to as maximum likelihood matching pursuit
(MLMP), which greedily finds the support of the codewords of users with partial
maximum likelihood metrics. As opposed to the conventional
successive-cancellation based greedy algorithms, MLMP works as a
successive-combining energy detector. We also propose MLMP modifications to
improve the performance at high code rates. Our studies in short block lengths
show that, even without any channel state information, SPARC with MLMP decoder
achieves multi-user diversity in some scenarios, giving better error
performance with multiple users than that of the corresponding single-user
case. We also show that SPARC with MLMP performs better than conventional
sparse recovery algorithms and pilot-aided transmissions with polar codes.

</details>


### [9] [Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty](https://arxiv.org/abs/2507.11413)
*Christos N. Efrem,Ioannis Krikidis*

Main category: eess.SP

TL;DR: 论文研究了在智能反射面（IRS）辅助的通信系统中，考虑信道状态信息（CSI）不完美的情况下，联合功率分配和反射单元（RE）激活以最大化能量效率（EE）的问题。提出了两种算法：交替优化（AO）和分支定界（B&B）。


<details>
  <summary>Details</summary>
Motivation: 在IRS辅助的通信系统中，如何高效地分配功率和激活反射单元以最大化能量效率，同时考虑CSI的不完美性，是一个具有挑战性的问题。

Method: 提出了两种算法：1）基于Lambert W函数和动态规划（DP）的交替优化（AO）方法；2）以AO为子程序的分支定界（B&B）方法。

Result: 数值结果表明，所提算法优于基线方案，AO在大多数情况下接近最优，B&B平均计算复杂度较低。

Conclusion: 论文提出的AO和B&B算法能有效解决联合功率分配和RE激活问题，AO适用于低复杂度需求，B&B保证全局最优。

Abstract: We study the joint power allocation and reflecting element (RE) activation to
maximize the energy efficiency (EE) in communication systems assisted by an
intelligent reflecting surface (IRS), taking into account imperfections in
channel state information (CSI). The robust optimization problem is mixed
integer, i.e., the optimization variables are continuous (transmit power) and
discrete (binary states of REs). In order to solve this challenging problem we
develop two algorithms. The first one is an alternating optimization (AO)
method that attains a suboptimal solution with low complexity, based on the
Lambert W function and a dynamic programming (DP) algorithm. The second one is
a branch-and-bound (B&B) method that uses AO as its subroutine and is formally
guaranteed to achieve a globally optimal solution. Both algorithms do not
require any external optimization solver for their implementation. Furthermore,
numerical results show that the proposed algorithms outperform the baseline
schemes, AO achieves near-optimal performance in most cases, and B&B has low
computational complexity on average.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: 该研究提出了一种两阶段预测模型架构，用于预测气溶胶光学深度（AOD）及其对海水淡化性能的影响，并开发了基于AOD的智能控制逻辑，以提高系统效率。


<details>
  <summary>Details</summary>
Motivation: 阿联酋依赖海水淡化获取饮用水，但该过程能耗高且受气候变化影响，尤其是AOD对太阳能淡化系统的负面影响。

Method: 研究采用两阶段预测模型：第一阶段预测AOD，第二阶段预测淡化效率损失；并开发了基于AOD的智能控制逻辑。

Result: 模型准确率达98%，SHAP分析揭示了系统退化的关键因素，控制逻辑优化了系统运行。

Conclusion: 研究成果通过交互式仪表盘支持管理决策，为气候适应性规划提供了工具。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [11] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: 论文提出了一种新的工具间匹配（TTTM）分析方法，解决了传统方法在商业制造环境中难以应用的问题，并在异构设备设置中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统TTTM方法依赖静态配置数据或黄金参考，难以在商业制造线上获取，且不适用于异构设备环境。

Method: 提出基于数据方差和模式数量的新TTTM分析流程，包括单变量和多变量方法。

Result: 最佳单变量方法与方差和模式数量的相关系数分别>0.95和>0.5；多变量方法与单变量方法的相关系数>0.75。

Conclusion: 新方法在TTTM问题中表现有效，且多变量方法对超参数敏感。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [12] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: 论文探讨了深度学习和Transformer的数学基础，提出了一个通用的Transformer近似定理，证明单层Transformer可以任意精度逼近连续序列到序列的映射。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和Transformer在实践中取得了巨大成功，但其理论理解仍然有限。本文旨在填补这一理论空白。

Method: 回顾了线性代数、概率和优化的关键概念，详细分析了多头自注意力机制和反向传播算法，并提出了一个通用的Transformer近似定理。

Result: 证明了单层Transformer可以逼近任何连续序列到序列的映射，并提供了完整的证明和案例研究。

Conclusion: 研究结果深化了对Transformer模型的理论理解，缩小了理论与实践之间的差距。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [13] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的替代框架，用于从部分观测的SPN轨迹中预测协变量依赖速率函数的系数，解决了传统参数估计的挑战。


<details>
  <summary>Details</summary>
Motivation: SPN的参数估计在协变量依赖速率函数和显式似然不可得时具有挑战性，尤其是在部分观测条件下。

Method: 使用轻量级1D卷积残差网络，通过端到端训练从Gillespie模拟的SPN实现中学习系统动态的反演。

Result: 在20%事件缺失的合成SPN上，替代模型以RMSE=0.108恢复速率函数系数，且速度显著快于传统贝叶斯方法。

Conclusion: 数据驱动的无似然替代模型能够在复杂、部分观测的离散事件系统中实现准确、鲁棒和实时的参数恢复。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [14] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出了多臂采样框架，作为多臂老虎机优化问题的采样对应。通过定义遗憾概念并建立下界，提出了一种简单算法达到最优遗憾界。研究发现采样无需探索，并通过温度参数统一了多臂采样与多臂老虎机问题。


<details>
  <summary>Details</summary>
Motivation: 研究采样中的探索-利用权衡，为神经采样器等提供理论基础。

Method: 定义采样框架的遗憾概念，建立下界并提出简单算法。通过温度参数统一多臂采样与多臂老虎机问题。

Result: 采样无需探索即可达到最优遗憾界，理论结果支持熵正则化强化学习等应用。

Conclusion: 多臂采样框架为采样研究奠定基础，揭示了探索需求与算法收敛性，对强化学习等领域有重要意义。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [15] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: 研究了在插值条件下SGD的收敛性，证明了在特定步长下，最后迭代的期望超额风险接近最优。


<details>
  <summary>Details</summary>
Motivation: 探索SGD在插值条件下的收敛行为，特别是在过参数化模型训练和持续学习中的应用。

Method: 分析SGD在β-平滑凸损失函数上的表现，使用步长η≤1/β，推导最后迭代的期望超额风险。

Result: 得到最后迭代的期望超额风险为O~(1/(ηT^(1−βη/2)) + ηT^(βη/2)σ⋆^2)，在最优步长下接近O~(1/T + σ⋆/√T)。

Conclusion: 扩展了Varre等人的结果，改进了Evron等人的收敛速率，特别是在σ⋆=0时达到O(1/√T)。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [16] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: 本文探讨了神经符号（NeSy）预测器中符号概念独立性假设的局限性，证明其会阻碍模型对某些概念组合的不确定性表示，导致推理捷径问题。


<details>
  <summary>Details</summary>
Motivation: 独立性假设在NeSy预测器中虽简化了概率推理，但可能阻碍学习和不确定性建模，引发争议。本文旨在明确其实际影响。

Method: 通过形式化分析，证明独立性假设如何限制模型对概念组合不确定性的表示能力。

Result: 独立性假设导致模型无法表示某些概念组合的不确定性，从而引发推理捷径问题。

Conclusion: 独立性假设在NeSy系统中存在根本性限制，需重新审视其适用性以避免推理捷径。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [17] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: 提出一种无需反向传播的神经网络训练方法，通过局部信号和层间损失函数在强化学习环境中实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播需要存储激活值且易出现梯度消失或爆炸问题，影响学习性能和稳定性。

Method: 采用局部层间损失函数，基于多维尺度匹配原则，结合奖励驱动信号，在正向传播中完成训练。

Result: 实验表明，该方法在性能上与基于反向传播的方法相当，且提升了稳定性和一致性，在复杂环境中表现更优。

Conclusion: 该方法有效解决了反向传播的局限性，为强化学习中的神经网络训练提供了新思路。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [18] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的线性自适应交叉熵损失函数，通过增加一个依赖于真实类预测概率的项，优化分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 标准交叉熵损失函数在分类任务中存在优化不足的问题，希望通过改进提升性能。

Method: 提出线性自适应交叉熵损失函数，增加一个与真实类预测概率相关的项，并在ResNet模型和CIFAR-100数据集上验证。

Result: 在分类准确率上优于标准交叉熵损失函数，同时保持相似的效率。

Conclusion: 该方法为损失函数设计提供了新的研究方向。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [19] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched是一种新型自适应学习率调度器，通过动态调整学习率提升深度神经网络训练效果。


<details>
  <summary>Details</summary>
Motivation: 预定义和自适应学习率调度器可能导致泛化性能不佳，需要更有效的动态调整方法。

Method: VolSched基于随机过程中的波动性概念（如几何布朗运动），通过长短时准确率波动比动态调整学习率。

Result: 在CIFAR-100数据集上，VolSched显著提升ResNet-18和ResNet-34的准确率（分别提高1.4和1.3个百分点），并找到更平坦的解。

Conclusion: VolSched通过动态调整学习率，有效探索损失空间，提升模型泛化性能。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [20] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: 论文提出了MH-FSF框架，用于解决特征选择研究中基准测试不足和依赖专有数据集的问题，支持17种方法的实现和系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前特征选择研究存在基准测试不足和依赖专有数据集的问题，影响可重复性和性能。

Method: 开发了MH-FSF框架，包含17种特征选择方法（11种经典，6种领域特定），并在10个公开Android恶意软件数据集上评估。

Result: 结果显示性能在不同数据集上存在差异，强调了数据预处理和选择标准的重要性。

Conclusion: MH-FSF框架为特征选择研究提供了统一平台，促进了方法一致性和严谨性，为Android恶意软件检测等新研究方向铺路。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [21] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: 论文提出OL-MDISF方法，解决在线学习中混合特征、数据漂移和标注不足的挑战，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线学习中，混合特征类型、数据漂移和标注不足是三大挑战，需要一种灵活且高效的方法来解决。

Method: OL-MDISF通过构建潜在copula表示、检测漂移和结构感知伪标注来解决这些问题。

Result: 在14个真实数据集上的实验表明，OL-MDISF在两种漂移场景下表现优异。

Conclusion: OL-MDISF为复杂、弱监督的流数据学习提供了可复现的基准。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [22] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: DTRGC是一种针对属性缺失图的深度图聚类方法，通过分层处理和聚类信息修正，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 属性缺失图的深度图聚类在实际应用中至关重要，但现有方法未能充分利用节点邻域信息，导致结果不可靠。

Method: DTRGC采用分层策略，包括动态聚类感知特征传播（DCFP）、分层邻域感知填补（HNAI）和跳数表示增强（HRE）。

Result: 在六个常用图数据集上，DTRGC显著提升了属性缺失图下多种DGC方法的聚类性能。

Conclusion: DTRGC通过分层处理和聚类信息修正，有效解决了属性缺失图的聚类问题。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [23] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne是一个针对社交网络服务（SNS）设计的领域特定大语言模型（LLM），通过三阶段训练策略显著提升多任务性能，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于孤立任务，无法灵活适应多样化的现实场景，且数据扩展效益递减。RedOne旨在解决这些问题，为SNS提供综合解决方案。

Method: 采用三阶段训练策略：持续预训练、监督微调和偏好优化，使用大规模真实数据集。

Result: RedOne在8个主要SNS任务中平均提升14.02%，在双语评估基准中提升7.56%，并在有害内容检测和搜索点击率方面显著优化。

Conclusion: RedOne作为领域特定LLM，在多任务和实际场景中表现出强大的泛化能力和应用潜力。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [24] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD是一个可扩展的框架，用于生成合成布局热图，以加速物理设计中的机器学习研究。它通过扩散模型快速生成多样化的热图，解决了高质量数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在物理设计任务中表现优异，但高质量、大规模训练数据集的稀缺限制了模型的泛化能力。公共数据集通常静态、生成缓慢且更新频率低。

Method: DALI-PD利用扩散模型快速生成多样化的布局热图（如电源、IR压降、拥塞等），并在几秒内完成推理。

Result: 生成了包含20,000多种布局配置的数据集，这些热图与真实布局高度相似，并提升了IR压降和拥塞预测等下游任务的ML准确性。

Conclusion: DALI-PD为物理设计中的机器学习研究提供了高效的数据生成解决方案，显著提升了模型性能。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [25] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA是一个针对医疗联邦学习中标签噪声问题的新框架，通过全局样本选择器和客户端自适应调整机制，显著提升了模型在噪声环境下的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 医疗联邦学习中的数据隐私保护需求与标签噪声问题（如机构间数据差异）导致模型性能下降，现有方法难以应对噪声异质性和数据不平衡。

Method: FedGSCA结合全局样本选择器（聚合噪声知识）和客户端自适应调整机制（自适应阈值伪标签生成和鲁棒标签损失），动态适应类别分布并管理噪声标签。

Result: 在真实结肠切片数据集和合成医疗数据集上，FedGSCA在极端和异质噪声场景中表现优于现有方法，显著提升模型稳定性和噪声处理能力。

Conclusion: FedGSCA为医疗联邦学习中的噪声问题提供了有效解决方案，适用于实际复杂噪声环境。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [26] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: 传统扩展法则认为增大模型规模和训练数据能提升性能，但近期研究发现大语言模型中性能提升会减速（称为次优扩展）。本文通过分析数据质量和训练策略的影响，提出次优扩展法则。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中性能提升减速的现象，探讨数据质量和训练策略对扩展法则的影响。

Method: 通过对400多个模型进行实证分析，识别数据密度和资源分配对性能的影响。

Result: 发现高数据密度导致收益递减，资源分配对持续性能提升至关重要，提出次优扩展法则。

Conclusion: 数据质量和多样性对模型性能至关重要，次优扩展法则能更准确预测性能。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [27] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了为算法设计定制LLM的必要性，提出了一种多样性感知的采样策略和偏好优化方法，实验表明定制LLM在性能上优于通用LLM，并展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖通用LLM，但算法设计是否需要定制LLM及其获取方式和泛化能力尚不明确。

Method: 采用多样性感知的采样策略（DAR）和直接偏好优化方法，在Llama-3.2-1B-Instruct和Llama-3.1-8B-Instruct上进行实验。

Result: 定制LLM显著优于通用LLM，小模型性能接近大模型，且在相关任务上表现出泛化能力。

Conclusion: 任务特定的LLM适配在算法设计中具有价值，为未来研究开辟了新方向。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [28] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 比较了强化学习（RL）和监督微调（SFT）在数学问题上的表现，发现RL在领域内略有提升但知识密集型任务表现下降，SFT变化更显著且影响模型中间层。尝试冻结部分模型以缓解性能下降，结果不一致。


<details>
  <summary>Details</summary>
Motivation: 理解RL和SFT在LLM后训练中的动态差异及其对模型能力的影响。

Method: 在同一模型和超参数下，比较RL和SFT在数学问题上的表现，并分析模型参数变化。

Result: RL在数学任务上略有提升，但知识密集型任务表现下降；SFT变化更显著且影响中间层。冻结部分模型的效果不一致。

Conclusion: RL可能放大现有能力，而SFT可能替换旧技能为新技能。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [29] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: 论文研究了大规模语言模型预训练中的算法创新对计算资源的需求，发现即使严格的计算限制也不会显著减缓AI算法进步。


<details>
  <summary>Details</summary>
Motivation: 探讨算法创新在预训练中的计算需求及其对AI进步的影响。

Method: 分析了36项Llama 3和DeepSeek-V3中的预训练算法创新，并估计了其开发和硬件使用的FLOP。

Result: 计算需求每年翻倍，但即使严格的计算限制（如GPT-2的训练计算量或8个H100 GPU）仍能支持半数创新。

Conclusion: 计算限制不太可能显著减缓AI算法进步。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [30] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的框架，用于5G/6G网络中动态频谱分配，解决了传统深度强化学习（DRL）样本复杂度高和探索风险大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统DRL在动态频谱分配中样本复杂度高且存在安全风险，需要一种更高效、更安全的方法。

Method: 提出了三种元学习架构（MAML、RNN和注意力增强RNN），并在模拟的动态IAB环境中与非元学习的PPO基线进行比较。

Result: 注意力元学习代理峰值吞吐量达48 Mbps，优于PPO的10 Mbps，且减少了50%以上的SINR和延迟违规，公平性指数为0.7。

Conclusion: 元学习是复杂无线系统中智能控制的高效且安全的选择。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [31] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: 本文概述了基于大语言模型（LLMs）的跨模态时间序列分析方法，分类了现有策略，并讨论了应用与挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在时间序列分析中具有潜力，但存在跨模态差距，本文旨在弥合这一差距并推动实际应用。

Method: 提出分类法，将现有方法分为转换、对齐和融合三类，并讨论其在下游任务中的应用。

Result: 总结了当前进展、方法和未来研究方向，旨在平衡效果与效率。

Conclusion: 本文为跨模态时间序列分析提供了全面指导，并指出了未来研究的方向。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [32] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 该论文将扩散和基于流的生成模型扩展到权重空间学习，通过梯度流匹配统一轨迹推断技术，优化权重生成和初始化。


<details>
  <summary>Details</summary>
Motivation: 将扩散和流生成模型的成功扩展到权重空间学习，利用优化动态的结构先验提升权重生成和下游任务性能。

Method: 通过梯度流匹配统一轨迹推断技术，结合奖励微调、自编码器、任务上下文数据等架构和算法选择。

Result: 实验表明，该方法在生成权重、下游训练初始化和微调性能上优于基线，并在安全关键系统中检测有害协变量偏移表现更优。

Conclusion: 该方法为权重空间学习提供了理论框架和实践应用，尤其在安全关键系统中表现突出。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [33] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer是一种基于图增强Transformer的深度学习模型，用于预测足球比赛结果，通过多级交互框架捕捉球员和团队的动态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 足球比赛结果预测具有挑战性，现有方法常忽略球员和团队间的异质性交互，而HIGFormer旨在填补这一空白。

Method: HIGFormer包含三个部分：Player Interaction Network（编码球员动态）、Team Interaction Network（建模团队历史关系）和Match Comparison Transformer（联合分析团队和球员信息）。

Result: 在WyScout数据集上的实验表明，HIGFormer在预测准确性上显著优于现有方法，并可用于球员表现评估。

Conclusion: HIGFormer为足球比赛预测提供了新方法，同时在球员评估和团队策略分析上具有潜在应用价值。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [34] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: 论文提出了一种名为GHPO的难度感知强化学习框架，通过动态调整任务难度和自适应提示优化，解决了现有RL方法在训练不稳定和效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于策略的强化学习方法在训练复杂推理任务时存在训练不稳定和效率低下的问题，尤其是对于资源有限的小型语言模型。

Method: GHPO框架通过自适应提示优化动态调整任务难度，结合模仿学习和探索性强化学习，形成优化的学习路径。

Result: 在六个数学基准测试中，GHPO平均性能提升约5%，显著优于现有强化学习和课程学习方法。

Conclusion: GHPO显著提升了训练稳定性和最终推理性能，为开发强大且稳健的推理模型提供了可扩展且高效的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [35] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: 提出RFF-GP-HSMM方法，通过随机傅里叶特征（RFF）降低高斯过程隐半马尔可夫模型（GP-HSMM）的计算成本，实现快速无监督时间序列分割。


<details>
  <summary>Details</summary>
Motivation: GP-HSMM在处理大规模数据时，由于需要计算N×N核矩阵的逆，计算成本高昂。

Method: 利用RFF近似高斯过程，将其转化为线性回归问题，避免核矩阵求逆。

Result: 在CMU运动捕捉数据集上，分割性能与传统方法相当，速度提升约278倍。

Conclusion: RFF-GP-HSMM是一种高效的时间序列分割方法，显著降低了计算成本。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [36] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet是一种基于Hopfield增强的稀疏空间注意力网络，用于解决无人机动态选址问题，显著提升了计算效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 城市低空无人机经济的快速发展对无人机着陆点和供应站的动态选址提出了新挑战，传统深度强化学习方法在处理大规模问题时面临计算复杂度瓶颈。

Method: 提出GeoHopNet，包含四个核心创新：距离偏置多头注意力机制、K近邻稀疏注意力、现代Hopfield外部记忆模块和记忆正则化策略。

Result: GeoHopNet在大规模问题（1000节点）中表现出色，仅需0.1秒找到高质量解（0.22%最优性差距），比ADNet基线在100节点实例中解决方案质量提升22.2%，速度快1.8倍。

Conclusion: GeoHopNet通过创新设计显著扩展了可解决问题的规模，为无人机动态选址提供了高效解决方案。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [37] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP是一种简单、低开销的基线方法，结合了ReLUDown和Decreasing Backpropagation机制，在Continual ImageNet基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中模型在适应新任务时遗忘旧知识的问题，平衡可塑性与稳定性。

Method: 结合ReLUDown（轻量级激活修改）和Decreasing Backpropagation（梯度调度方案）。

Result: 在Continual ImageNet基准测试中表现优于或匹配现有方法，同时降低计算成本。

Conclusion: RDBP为持续学习提供了实用解决方案，并为未来方法设定了清晰基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [38] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier提出了一种新的分类框架，用对角高斯分布替代传统确定性logits，通过最小化KL散度统一了不确定性校准和潜在控制。


<details>
  <summary>Details</summary>
Motivation: 解决传统softmax分类器在不确定性校准和潜在控制方面的不足。

Method: 使用对角高斯分布logits，最小化KL散度以统一不确定性校准和潜在控制。

Result: 在CIFAR-10和CIFAR-100上表现优于softmax分类器，提升了鲁棒性、校准性和潜在分离性。

Conclusion: ZClassifier为分类和生成任务提供了一种新的概率框架。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [39] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hopfield神经网络的轻量级AI模型，用于生物声学分析，解决了数据量大、能耗高和硬件需求等问题，具有快速、低能耗和高精度的特点。


<details>
  <summary>Details</summary>
Motivation: 被动声学监测设备产生的数据量巨大，现有AI模型在训练数据不足、环境影响和硬件需求方面存在挑战。

Method: 使用透明、可解释的Hopfield神经网络，通过关联记忆存储信号并分类物种，仅需一个代表性信号即可快速训练。

Result: 模型在标准设备上快速处理数据（5.4秒分类10384个蝙蝠录音），内存占用低（144.09MB），精度达86%，与专家手动分类一致。

Conclusion: 该模型为生物声学分析提供了快速、轻量、可持续、透明且准确的解决方案，具有广泛应用潜力。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [40] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: 论文探讨了神经网络如何通过对称性实现激进泛化，以基数加法为例，分析了不同进位函数对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何高效学习支持激进泛化的函数，特别是通过对称性发现和实现。

Method: 通过群论分析基数加法的进位函数，训练神经网络使用不同进位函数进行基数加法，比较学习效果。

Result: 发现简单神经网络在合适的输入格式和进位函数下可实现激进泛化，学习速度与进位函数结构密切相关。

Conclusion: 研究对认知科学和机器学习有重要意义，揭示了对称性学习中的归纳偏差。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [41] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种结合数据污染和分布偏移鲁棒性的方法，用于优化Wasserstein-1 DRO目标，并提供了高效算法和理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决分布鲁棒优化（DRO）在训练数据中存在异常值时效果受限的问题，同时应对数据污染和分布偏移的双重挑战。

Method: 提出了一种建模框架，结合鲁棒统计方法，优化Wasserstein-1 DRO目标，适用于广义线性模型和凸Lipschitz损失函数。

Result: 证明了在数据污染情况下，方法能以O(√ε)的误差估计真实DRO目标值，且计算高效。

Conclusion: 该研究首次为数据污染和分布偏移双重挑战下的学习问题提供了严格的理论保证和高效算法。

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [42] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: 提出了一个神经符号框架Ground-Compose-Reinforce，用于从数据中学习语言基础，并通过语言直接指导RL代理。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂感知和动作中语言基础化的挑战，避免手动设计或大规模数据标注。

Method: 使用数据驱动的学习框架，结合形式语言的组合语义，实现高效的语言基础化和泛化。

Result: 在图像网格世界和MuJoCo机器人领域的实验中，该方法在有限数据下成功映射语言指令到行为，而端到端数据驱动方法失败。

Conclusion: 该框架在语言基础化和行为引导方面表现出高效性和泛化能力。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [43] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 本文介绍了一个基于NVIDIA PhysicsNeMo-CFD框架的基准测试框架，用于系统评估AI模型在汽车空气动力学预测中的准确性、性能、可扩展性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为CAE社区提供一个标准化的方法，以透明和一致的方式评估AI模型性能，加速该领域的研究和创新。

Method: 框架包含多样化的评估指标，支持表面和体积流场预测的评估，并提供了集成新模型和数据集的方法。

Result: 通过DrivAerML数据集对三种AI模型（DoMINO、X-MeshGraphNet、FIGConvNet）进行了评估，展示了框架的实用性。

Conclusion: 该框架旨在帮助研究者和行业专业人士选择和优化AI驱动的空气动力学建模方法，推动更高效、准确和可解释的解决方案的发展。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [44] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个用于空间推理的软件框架，支持生成去噪模型处理连续变量。


<details>
  <summary>Details</summary>
Motivation: 生成去噪模型在图像生成中表现优异，但在多连续变量推理中的应用研究缺乏基础设施支持。

Method: 提供易用接口，支持变量映射、生成模型范式和推理策略的灵活控制。

Result: 框架开源，促进该领域研究。

Conclusion: Spatial Reasoners为生成推理研究提供了高效工具。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [45] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: 提出Phy-SSM方法，结合物理知识与状态空间模型，提升复杂环境中长期动态预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂环境中噪声和不规则采样数据下的长期动态预测问题，结合物理知识提升泛化能力。

Method: 将部分已知物理知识分解为已知和未知状态矩阵，融入Phy-SSM单元，并引入物理状态正则化项。

Result: 在车辆运动预测、无人机状态预测和COVID-19流行病学预测中表现优于基线方法。

Conclusion: Phy-SSM在长期预测任务中具有优越性能，代码已开源。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [46] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: 提出了一种新的因果框架，用于捕捉外域干预下时间过程中事件间的因果关系变化，并设计了无偏ATE估计器和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要关注域内事件类型，忽略了外域干预的影响，而这些干预可能显著改变因果动态。

Method: 设计了无偏ATE估计器，并开发了一个基于Transformer的神经网络模型，整合外域干预信息，处理长程时间依赖和局部模式。

Result: 在模拟和真实数据集上的实验表明，该方法在外域增强点过程中的ATE估计和拟合优度上优于基线。

Conclusion: 新框架和模型能有效捕捉外域干预下的因果动态变化，提升ATE估计的准确性。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [47] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: 论文提出语义上下文（SC）是工具编排的基础，通过理论、实证和实际应用验证其重要性。


<details>
  <summary>Details</summary>
Motivation: 研究语义上下文在工具编排中的作用，以提高样本效率、适应性和可扩展性。

Method: 结合上下文多臂老虎机理论（SC-LinUCB）和大语言模型实证验证，提出FiReAct流程。

Result: SC-LinUCB降低遗憾值，SC提升大语言模型在静态和非静态环境中的学习效果，FiReAct在大规模工具库中表现优异。

Conclusion: 语义上下文是构建高效、自适应和可扩展编排代理的关键。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [48] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: 利用图卷积网络（GCN）解决受限品类优化问题，通过小规模训练实现大规模高效优化。


<details>
  <summary>Details</summary>
Motivation: 品类优化是经典但NP难的问题，传统方法效率低，需新方法提升性能。

Method: 构建品类问题的图表示，训练GCN学习最优模式，提出两种推断策略。

Result: 小规模训练GCN可在大规模实例中实现90%+最优性，优于现有启发式方法。

Conclusion: GCN框架在模型未知时仍有效，扩展性强，性能高效。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [49] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein距离的离线强化学习方法，避免了对抗训练，并在D4RL数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在数据收集成本高的场景（如机器人）中很有价值，但分布偏移问题导致策略不可靠。现有方法多基于密度比度量，但作者认为Wasserstein距离更适合处理分布外数据。

Method: 使用输入凸神经网络（ICNNs）建模最优传输映射，计算Wasserstein距离，无需对抗训练。

Result: 在D4RL基准数据集上表现优于或与现有方法相当。

Conclusion: Wasserstein距离是一种有效的离线强化学习正则化方法，避免了对抗训练的不稳定性。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [50] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: 论文提出了一种基于视觉的认知重评增强方法，利用文本到图像扩散模型生成情感一致的视觉反馈，显著降低了负面情绪。


<details>
  <summary>Details</summary>
Motivation: 传统的认知重评依赖高阶认知和语言能力，对创伤或抑郁患者效果有限，因此需要一种更直观的干预方式。

Method: 通过稳定扩散模型和微调的IP适配器，将用户的口头重评转化为支持性视觉反馈，并在实验中对比了AI辅助与非AI条件下的效果。

Result: AI辅助的重评显著降低了负面情绪，且情感一致性高的视觉反馈与情绪缓解相关。

Conclusion: 生成式视觉输入能有效支持认知重评，为生成式AI、情感计算和治疗技术的结合开辟了新方向。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [51] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于图自动编码器的潜在动态替代模型（GALDS），用于高效模拟神经树中的物质运输，解决了传统方法的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 神经元网络的复杂几何结构对物质运输模拟提出了高计算需求，传统方法耗时且资源密集，因此需要一种优化的计算方法。

Method: GALDS结合图自动编码器和神经常微分方程（Neural ODEs）概念，通过潜在空间表示和动态预测模型，减少计算需求和误差累积。

Result: 在八种未见几何结构和四种异常运输案例中，GALDS的平均相对误差为3%，最大误差<8%，速度提升10倍。

Conclusion: GALDS为神经树物质运输模拟提供了一种高效、准确的替代方法，显著优于传统模型。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [52] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 提出一种基于编码器-解码器架构的小型语言模型（SLM），用于改进产品和服务的税收代码预测，解决了层次化税收代码序列预测问题，并在实验中表现优于传统分类器。


<details>
  <summary>Details</summary>
Motivation: 跨国企业需处理大量交易，税收代码的准确预测对合规至关重要，但现有方法难以捕捉层次化依赖关系。

Method: 采用编码器-解码器架构的SLM，利用非结构化数据生成层次化税收代码序列。

Result: 实验表明，该方法在HSN等税收代码预测任务中优于传统分类器和其他架构。

Conclusion: 该方法可扩展至其他税收代码体系，为结构化序列生成任务提供了新思路。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [53] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种基于模拟的生成模型（SiGMoID），用于从噪声、稀疏或部分可观测数据中精确推断非线性动态系统的ODE参数。


<details>
  <summary>Details</summary>
Motivation: 非线性动态模型的系统推断在数据噪声大、稀疏或部分可观测时具有挑战性，需要一种鲁棒且精确的方法。

Method: 结合物理信息神经网络与超网络构建ODE求解器，并使用Wasserstein生成对抗网络估计ODE参数。

Result: SiGMoID能有效量化数据噪声、估计系统参数并推断未观测系统组件，实验验证了其广泛适用性。

Conclusion: SiGMoID为科学研究和工程系统提供了强大的动态系统推断工具，能够发现完整的系统动态。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [54] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 论文研究了对抗性遗忘问题，提出了一种保护模型性能的新方法。


<details>
  <summary>Details</summary>
Motivation: AI模型需要遗忘以符合法律要求或应对数据问题，但遗忘可能导致性能下降。

Method: 研究了对抗性遗忘现象，分析了影响因素，并提出保护模型性能的方法。

Result: 发现对抗性遗忘的影响因素，并提出有效防护方法。

Conclusion: 新方法能有效防止因遗忘或对抗行为导致的模型性能下降。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [55] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: 论文研究了预测库存仓库单位消耗及运输成本的问题，提出了一种概率预测模型，用于强化学习环境中的库存规划。


<details>
  <summary>Details</summary>
Motivation: 准确建模库存消耗和运输成本对区域库存规划至关重要，尤其是在使用强化学习开发控制策略时。现有方法（如模拟内部软件系统）不可微且效率低，因此需要一种高效且可微的替代方案。

Method: 将问题转化为概率预测问题，建模所有仓库在每个时间段的单位消耗和运输成本的联合分布，条件为库存状态和外部客户需求。提出了一种验证方案，利用生产系统评估模型在非分布场景下的表现。

Result: 初步结果表明，模型在分布内设置下具有较高的准确性。

Conclusion: 提出的概率预测模型为强化学习环境中的库存规划提供了一种高效且可微的解决方案，并能处理非分布场景。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [56] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: 论文提出了一种基于类难度可分性的核心集选择方法，通过量化类难度差异，改进了现有方法在数据修剪中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法假设数据难度在类别间均匀分布，忽略了实际中类难度差异显著的问题，导致性能下降。

Method: 引入类难度可分性系数（CDSC），并提出类比例变体的采样策略（如CCS-CP），以优化数据修剪。

Result: 在多个数据集上，新方法在极端修剪率下表现稳定，性能下降显著低于基线方法。

Conclusion: 显式建模类难度可分性可提升数据修剪的效果和鲁棒性，尤其适用于高风险场景。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [57] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: 论文研究了在肽段从头测序中使用扩散解码器的方法，相比传统的自回归解码器，扩散解码器能够从任意肽段开始生成序列，提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码器在肽段测序中存在级联错误和未能有效利用高置信度区域的问题，扩散解码器为解决这些问题提供了新思路。

Method: 研究了三种扩散解码器设计，结合背包束搜索和不同损失函数（如DINOISER），并与自回归解码器基线模型Casanovo对比。

Result: 尽管肽段精度和召回率为0，但最佳扩散解码器设计在氨基酸召回率上显著提高了0.373。背包束搜索未提升性能。

Conclusion: 扩散解码器在提高模型敏感性和推动肽段从头测序技术进步方面具有潜力。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [58] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 综述了机器学习在半导体薄膜沉积中的应用，重点介绍了物理信息神经网络（PINNs）的潜力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 半导体薄膜沉积过程复杂，需要精确控制，机器学习尤其是PINNs为解决相关挑战提供了新思路。

Method: 通过主题分析，总结了ML在薄膜沉积中的趋势、局限性和研究空白，并探讨了PINNs的物理知识嵌入策略。

Result: 揭示了ML在提升薄膜沉积过程的解释性、准确性和鲁棒性方面的潜力，并提出了PINNs的集成方向。

Conclusion: 研究为未来物理信息ML框架的集成提供了清晰路径，旨在提升半导体制造的精度和效率。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [59] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: StellarF是一种新型恒星耀斑预测模型，结合LoRA和Adapter技术，通过多尺度模式识别实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 恒星耀斑预测领域受限于数据稀疏和缺乏大规模预测模型，StellarF旨在解决这些问题。

Method: StellarF整合耀斑统计信息和历史记录模块，利用LoRA和Adapter技术进行参数高效学习。

Result: 在Kepler和TESS数据集上，StellarF表现优于现有方法。

Conclusion: StellarF为天体物理研究和跨学科应用提供了新方法框架。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [60] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv是一个轻量级、与学习器无关的分布式环境执行接口，通过DETACH模式解耦模拟与训练，并提出AAPS机制减少同步开销。


<details>
  <summary>Details</summary>
Motivation: 现有框架将模拟、学习逻辑和编排耦合为单一系统，限制了模块化和可重用性。

Method: ClusterEnv采用DETACH模式，将reset()和step()操作卸载到远程工作节点，同时提出AAPS机制以减少同步开销。

Result: 实验表明，AAPS在离散控制任务中实现了高样本效率，且权重更新次数显著减少。

Conclusion: ClusterEnv无缝集成到现有RL流程中，支持多种方法，且代码改动极小。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [61] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了奖励函数中终端目标和工具性目标的混淆问题，指出这种混淆会导致强化学习中的严重对齐问题。


<details>
  <summary>Details</summary>
Motivation: 奖励函数通常不完美，可能混淆人类的终端目标和工具性目标，导致优化结果与真实目标不符。

Method: 通过一个简单示例展示工具性和终端目标混淆的影响，并分析其对强化学习的敏感性。

Result: 研究发现，即使轻微的混淆也会导致严重的对齐问题，优化错误的奖励函数会降低真实性能。

Conclusion: 论文强调了区分终端和工具性目标的重要性，并讨论了奖励学习中可能出现的类似问题。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [62] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: 提出了一种基于混合输入变分自编码器（VAE）的潜在空间扰动框架，用于生成不可察觉的对抗样本，解决了表格数据中异质特征的挑战。


<details>
  <summary>Details</summary>
Motivation: 表格数据的异质性（混合分类和数值特征）使得对抗攻击难以定义不可察觉的修改，传统梯度方法生成的对抗样本易偏离原始数据分布。

Method: 使用混合输入VAE将分类嵌入和数值特征整合到统一的潜在流形中，生成统计一致的对抗样本，并提出IDSR指标衡量统计不可区分性。

Result: 在六个公开数据集和三种模型架构上的评估表明，该方法显著降低了异常率，性能更一致，优于传统输入空间攻击和其他VAE方法。

Conclusion: 潜在流形扰动对表格数据的现实对抗攻击至关重要，该方法在训练数据充足时具有实际应用优势。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [63] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon是一个基于Muon优化器的自适应学习率框架，通过两个模块提升训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在大规模模型训练中表现优异，但仍有改进空间。AdaMuon旨在进一步提升其效率和适应性。

Method: AdaMuon引入两个模块：(1) 参数级二阶矩调制，捕捉正交梯度更新；(2) RMS对齐的重新缩放，调整更新幅度。

Result: 实验表明，AdaMuon在多种模型规模和学习率下均优于Muon，收敛更快且稳定。

Conclusion: AdaMuon无需额外调参，可直接集成到现有Muon训练流程中，显著提升性能。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [64] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: 利用温度数据预测湍流动能（TKE），通过机器学习模型揭示温度与TKE的关系，为火灾环境研究提供新方法。


<details>
  <summary>Details</summary>
Motivation: 探索温度数据与湍流动能（TKE）之间的关系，以改进火灾环境的研究和管理策略。

Method: 使用深度神经网络、随机森林回归、梯度提升和高斯过程回归等机器学习模型，分析温度扰动与TKE的时空动态相关性。

Result: 尽管预测变量与目标变量相关性较弱，机器学习模型仍能较准确地预测TKE，回归模型表现尤为突出。

Conclusion: 研究展示了机器学习在火灾环境大数据分析中的潜力，为理解火灾过程和改进模型预测提供了新途径。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [65] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM是一种新的后训练量化方法，通过显式结合一阶梯度项改进量化误差补偿，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有补偿方法假设一阶项可忽略，但实际累积偏差导致假设不成立，需改进。

Method: FOEM直接计算潜在与全精度权重差异近似梯度，利用预计算Cholesky因子高效恢复Hessian子矩阵逆。

Result: 在3位量化下，FOEM显著降低困惑度并提升准确率，接近全精度性能。

Conclusion: FOEM优于现有方法，可无缝结合其他技术，进一步缩小与全精度基线的差距。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [66] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 论文提出了一种基于价值梯度的策略优化方法（REPPO），结合了路径策略梯度的样本效率和标准策略学习的简单性，降低了训练方差和资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统策略梯度方法高方差和路径策略梯度依赖准确动作条件价值函数的问题，实现在策略学习中使用路径策略更新。

Method: 提出REPPO算法，通过平衡随机策略和约束策略更新，结合价值函数学习的架构优化，实现高效训练。

Result: 实验表明，REPPO在样本需求、训练时间、内存占用和超参数鲁棒性方面表现优异。

Conclusion: REPPO为策略优化提供了一种高效且稳定的解决方案，适用于多种任务。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [67] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: 论文提出了一种名为GATE的新框架，通过自适应图表示和注意力机制改进室内定位精度，显著降低了定位误差。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi RSS指纹定位在异构移动设备上泛化能力差，现有深度学习和图神经网络方法未能有效处理非欧几里得噪声分布和密集AP环境。

Method: GATE框架结合注意力超空间向量（AHV）、多维超空间向量（MDHV）和实时边构建（RTEC）技术，动态建模RSS噪声的非欧几里得结构。

Result: 在多个真实环境中测试，GATE的定位误差比现有最优方法低1.6x至4.72x，最坏情况误差低1.85x至4.57x。

Conclusion: GATE通过自适应图表示和噪声建模，显著提升了室内定位的精度和鲁棒性。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [68] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: 本文提出了一种基于数学公式的MILP实例距离度量方法，用于比较实例相似性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MILP实例相似性度量方法缺乏精确性或依赖标注数据，限制了其适用性。

Method: 通过离散化右端项、权重和变量，并借鉴Earth mover's距离，提出了一种数学距离度量方法。

Result: 贪婪版本在保持高精度的同时速度提升200倍，优于非学习方法，与监督分类器性能相当。

Conclusion: 该方法为MILP实例比较提供了有效的无监督解决方案。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [69] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LoRA和适配器的高效微调方法，用于检测大规模日志数据中的异常序列，相比传统方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或深度学习的日志异常检测方法在处理大规模复杂日志序列时效果不佳，因此需要更高效的方法。

Method: 采用参数高效微调技术（如LoRA和适配器），并在Thunderbird数据集上比较不同小型大语言模型。

Result: LoRA微调比LogBert全微调方法性能提升18-19%，准确率达到97.76%-98.83%，而后者为79.37%。

Conclusion: LoRA微调方法在日志异常检测中表现优异，显著优于传统方法。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [70] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯在线变化点检测（BOCPD）的方法，通过监测强化学习（RL）批评网络的时序变化，检测无人机导航中的细微行为偏差，以应对漂移规避欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖GNSS伪距测量进行定位和导航，但易受欺骗攻击，尤其是漂移规避攻击，传统检测方法因延迟问题难以快速响应。

Method: 采用BOCPD方法，结合RL批评网络的时序价值估计，检测攻击起始点。

Result: 实验表明，该方法在检测漂移规避攻击时，准确率更高，误报率和漏报率更低，优于传统方法。

Conclusion: 提出的时序价值框架显著提升了无人机对隐蔽欺骗攻击的检测能力，增强了系统韧性。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [71] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: 提出了基于梯度正则化的神经格兰杰因果模型（GRNGC），解决了现有方法计算成本高和捕捉复杂交互能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经格兰杰因果模型需要为每个时间序列构建单独模型，计算成本高，且稀疏性惩罚削弱了捕捉复杂交互的能力。

Method: GRNGC仅需一个时间序列预测模型，并在输入输出梯度上应用L1正则化以推断格兰杰因果，支持多种架构（如KAN、MLP、LSTM）。

Result: 在模拟和真实数据集（如DREAM、Lorenz-96、基因数据）上，GRNGC表现优于基线方法，显著降低计算开销。

Conclusion: GRNGC是一种灵活高效的神经格兰杰因果模型，适用于多种时间序列预测任务。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [72] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 本文综述了混合专家（MoE）架构在大语言模型中的应用，强调其在提升性能的同时保持低计算开销的能力，并探讨了其设计、应用及未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在大语言模型中的潜力，以提升模型性能并减少计算成本。

Method: 通过系统分析理论、架构设计、应用场景及挑战，探讨MoE的专家门控、路由机制、稀疏配置等。

Result: MoE展现出优于贝叶斯方法的模型容量、任务性能提升及高效扩展能力，但需注意专家多样性和校准。

Conclusion: MoE架构具有显著优势，但仍需解决当前局限和挑战，为未来研究提供方向。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [73] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: 提出了一种通信高效的联邦学习方案，通过低秩近似和量化减少网络负载，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中频繁交换模型更新导致通信开销大，影响效率。

Method: 利用神经网络梯度的低秩近似和量化技术，减少通信数据量。

Result: 显著降低了网络负载，对模型准确性影响极小。

Conclusion: 该方案有效解决了联邦学习中的通信效率问题。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [74] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: 该研究提出了一种结合分类和回归模型的机器学习框架，用于心脏病诊断和风险预测，使用SMOTE处理数据不平衡问题，随机森林和线性回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 心脏病是全球健康问题，传统诊断方法准确性不足，机器学习可提升诊断效率和准确性。

Method: 使用Heart Disease数据集（1,035例），应用SMOTE生成10万合成数据点，评估分类和回归模型的性能。

Result: 随机森林分类准确率97.2%（真实数据）和97.6%（合成数据）；线性回归R2值0.992（真实数据）和0.984（合成数据）。

Conclusion: 机器学习可显著改善心脏病诊断和风险预测，支持早期干预和临床决策。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [75] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种隐私保护的分布式学习框架，用于在线协作医疗预测平台，以解决隐私和预测质量的问题。


<details>
  <summary>Details</summary>
Motivation: 在线协作医疗预测平台虽然便捷，但隐私泄露和预测质量低的问题阻碍了患者和医生的参与。

Method: 提出了一种隐私保护机制，并集成到一次性分布式学习框架中，结合统计学习理论进行理论分析。

Result: 理论证明该框架在特定隐私要求下可实现最优预测性能，并通过模拟和真实数据实验验证。

Conclusion: 该隐私保护机制和分布式学习框架能同时满足隐私和性能需求。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [76] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: 研究了梯度下降在逻辑回归中的收敛性，发现数据幅度相等时在一维空间中全局收敛，但在高维中仍可能循环。


<details>
  <summary>Details</summary>
Motivation: 探索梯度下降在逻辑回归中是否能在数据幅度相等时全局收敛，以理解其行为。

Method: 分析梯度下降在数据幅度相等条件下的收敛性，比较一维和高维空间的结果。

Result: 一维空间中全局收敛，高维中仍可能循环。

Conclusion: 需进一步研究循环行为的普遍性及保证全局收敛的充分条件。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [77] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 提出了一种结合生成模型和判别模型的两阶段训练方法，用于提升CTR预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GPT）在表达能力上超越判别模型，但两者数据需求不同，需设计方法结合其优势。

Method: 两阶段训练：1) 生成模型预训练用于下一项预测；2) 在判别CTR框架中微调生成模型。

Result: 在新数据集和在线A/B测试中验证了方法的有效性，并已部署于大型电商平台。

Conclusion: 生成模型能显著提升CTR预测精度，未来将公开代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [78] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm是一种新型优化器，结合Adam的自适应矩估计和Lyapunov稳定性机制，动态调整学习率以提升收敛鲁棒性和减少训练噪声。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中常遇到梯度噪声和收敛不稳定问题，影响性能和泛化能力。

Method: LyAm整合Adam和Lyapunov稳定性理论，动态调整学习率，并在非凸设置中提供理论收敛保证。

Result: 在CIFAR-10和CIFAR-100等数据集上，LyAm在准确性、收敛速度和稳定性上优于现有优化器。

Conclusion: LyAm是深度学习优化中的强有力候选方法，具有鲁棒性和高效性。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [79] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: 论文提出了一种基于Neyman-Rubin潜在结果框架的深度强化学习方法，显著提高了样本效率并减少了经验回放缓冲区的需求。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习训练步骤多、资源消耗大的问题。

Method: 利用Neyman-Rubin框架建立因果边界，存储历史价值网络输出以优化数据利用。

Result: 在Atari 2600和MuJoCo实验中，奖励比提升2427%，缓冲区大小减少96%。

Conclusion: 该方法显著提升了样本效率，且成本可忽略。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [80] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: 提出了一种公平奖励模型（FRM）框架，通过评分减少语言模型决策中的偏见，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在高风险决策中可能放大偏见的问题，确保决策的公平性。

Method: 训练一个通用的公平奖励模型（FRM），为语言模型的推理链评分，减少偏见轨迹的权重。

Result: FRM在多个任务和领域中无需微调即可提升公平性，同时保持或超过基线准确性。

Conclusion: FRM是一种有效的方法，可在高风险决策中提升语言模型的公平性和准确性。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [81] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK是一种新的强化学习框架，通过双目标奖励系统优化答案质量和工具多样性，训练语言模型探索多样化工具使用模式。


<details>
  <summary>Details</summary>
Motivation: 传统的高温采样方法限制了工具使用的多样性，SPaRK旨在通过强化学习鼓励模型探索不常见但有效的工具。

Method: 采用离线PPO训练Llama-3.1 8B模型，使用双目标奖励系统，结合GPT-4o评分和稀有工具优先策略。

Result: 在14个MMLU-Pro类别中表现优异，工具选择熵显著高于基线方法，表明工具多样性可提升推理能力。

Conclusion: SPaRK通过明确鼓励工具多样性，在不牺牲准确性的情况下增强了模型的推理能力。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [82] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: 论文提出了一种结合变分自编码器（VAE）和现代Hopfield网络（MHN）的持续学习模型，以减少神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 人类能够在不遗忘旧知识的情况下学习新信息，而神经网络则存在灾难性遗忘问题。论文旨在通过模拟人脑的互补学习系统（CLS）理论来解决这一问题。

Method: 结合VAE的模式完成能力和MHN的模式分离能力，构建了一个神经合理的持续学习模型，并在Split-MNIST任务上进行了评估。

Result: 模型在Split-MNIST任务上达到了接近最先进的准确率（约90%），显著减少了遗忘现象。

Conclusion: 通过模拟人脑的模式分离和模式完成功能，该模型为生物和人工系统中的记忆巩固、泛化和持续学习提供了功能模板。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [83] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出了R-MTGB框架，通过梯度提升方法处理多任务学习中的异常任务，提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习（MTL）中异常任务会降低模型性能，需要一种鲁棒的方法来处理任务异质性。

Method: R-MTGB分三阶段：学习共享模式、划分异常任务、微调任务特定预测器。

Result: 实验表明R-MTGB能有效隔离异常任务并提升各任务性能。

Conclusion: R-MTGB在多任务学习中表现出鲁棒性和适应性。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [84] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: 研究探讨了激活函数在fNIRS深度学习中的影响，发现对称激活函数（如Tanh和Abs(x)）在某些架构中优于ReLU，并强调了选择适合fNIRS信号特性的激活函数的重要性。


<details>
  <summary>Details</summary>
Motivation: fNIRS领域中的非线性、低信噪比和信号变异性对模型准确性构成挑战，但激活函数的影响尚未被系统研究。

Method: 评估了多种传统和领域特定的激活函数，使用包括fNIRSNet、AbsoluteNet等在内的深度学习架构，并在单一数据集上进行测试。

Result: 对称激活函数（如Tanh和Abs(x）在某些架构中表现优于ReLU，且对称性分析进一步支持其性能优势。

Conclusion: 选择与fNIRS信号特性匹配的激活函数对提升深度学习性能至关重要。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [85] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: iTransformer在多元时间序列预测中表现优异，但其倒置框架存在局限性。作者提出了一种新的数据增强方法DAIF，通过频率过滤和跨变量修补策略解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: iTransformer的倒置框架虽然能有效捕捉多元相关性，但会削弱时间依赖性信息并引入噪声，因此需要改进。

Method: 提出了DAIF方法，包括频率过滤和跨变量修补两种策略，专门针对倒置框架设计。

Result: 实验证明DAIF在多个数据集和倒置模型中均有效。

Conclusion: DAIF是一种创新的实时数据增强方法，成功解决了iTransformer倒置框架的局限性。

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [86] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR框架通过两阶段LLM方法提升直肠癌淋巴结转移评估的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI评估和现有AI模型在淋巴结转移诊断中表现有限且缺乏可解释性。

Method: LRMR结合多模态LLM生成结构化报告，再通过文本LLM进行患者间风险排名。

Result: 在117例患者中，LRMR的AUC为0.7917，优于基线模型。

Conclusion: 两阶段LLM框架为淋巴结转移评估提供了高效且可解释的新方法。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [87] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 论文研究了联邦学习（FL）在处理非线性、非平稳时间序列数据时的性能，发现FL在非线性数据分布下表现不如集中式方法，但适当的去趋势技术可以提升FL性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）设备的普及，其收集的时间序列数据具有非线性和非平稳特性，传统集中式分析存在延迟和通信成本问题，而FL虽能分布式训练模型，但数据分布差异可能影响预测准确性。

Method: 通过生成合成时间序列数据集（基于广义极值和对数正态分布），使用LSTM模型在集中式和FL设置下进行训练，并评估不同去趋势技术对FL性能的影响。

Result: 实验表明，FL在非线性数据分布下表现较差，但合适的去趋势技术能显著降低损失。

Conclusion: FL在处理非线性时间序列数据时需结合去趋势技术以提升性能，为实际应用提供改进方向。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [88] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: 论文探讨了基于强化学习的TractOracle-RL框架的四种扩展，结合最新RL进展，提出了一种新的训练方案IRT，显著提升了纤维追踪的准确性和解剖学有效性。


<details>
  <summary>Details</summary>
Motivation: 传统纤维追踪方法存在局限性，强化学习（RL）框架在减少假阳性方面表现出潜力，但仍有改进空间。

Method: 扩展TractOracle-RL框架，结合RL进展，提出IRT训练方案，利用束过滤方法迭代优化奖励机制。

Result: 实验表明，结合RL框架的方法在多种数据集上表现稳健，IRT显著提升了追踪准确性。

Conclusion: RL结合反馈机制（如IRT）是纤维追踪的未来方向，能有效提升解剖学有效性。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [89] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wendland径向基函数（RBFs）的新型参数化激活函数，用于解决传统激活函数的局限性，并在实验中表现出竞争性能。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU、sigmoid和tanh）存在局限性，Wendland RBFs因其紧凑支持、平滑性和正定性，被引入以改进这些问题。

Method: 结合标准Wendland组件与线性和指数项，提出增强型Wendland激活函数，具有可调局部性、改进的梯度传播和训练稳定性。

Result: 在合成任务（如正弦波逼近）和基准数据集（MNIST、Fashion-MNIST）上表现出竞争性能，尤其在回归任务中准确率更高。

Conclusion: Wendland激活函数通过局部平滑变换缓解过拟合并提升泛化能力，未来可探索混合架构和领域特定适配。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [90] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM提出了一种基于分层扩散策略的通信感知LoRA适配框架，通过强化学习和扩散模型优化LoRA的秩配置，显著降低了传输成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上运行大型语言模型（LLMs）面临通信带宽和计算资源限制，现有LoRA方法的固定或启发式秩配置及参数传输效率低下。

Method: AirLLM将秩配置建模为结构化动作向量，结合PPO生成粗粒度决策，并通过DDIM细化为高分辨率、任务和信道自适应的秩向量。

Result: 在不同信噪比下，AirLLM显著提升了微调性能并降低了传输成本。

Conclusion: AirLLM通过强化学习和扩散模型的结合，实现了高效且可扩展的远程微调。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [91] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 论文提出LangevinFlow，一种基于Langevin方程的变分自编码器，用于建模神经群体的动态结构和外部未观测影响。


<details>
  <summary>Details</summary>
Motivation: 神经群体表现出潜在的动态结构，需要模型既能捕捉内在网络动态，又能处理外部未观测影响。

Method: 采用Langevin方程驱动的变分自编码器，结合物理先验（如惯性、阻尼、势函数和随机力），并使用局部耦合振荡器网络参数化势函数。

Result: 在合成数据和NLB基准测试中表现优异，优于现有方法，并成功解码行为指标（如手速）。

Conclusion: LangevinFlow是一个灵活、物理启发的高性能框架，适用于复杂神经群体动态及其未观测影响的建模。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [92] [Canonical Bayesian Linear System Identification](https://arxiv.org/abs/2507.11535)
*Andrey Bryutkin,Matthew E. Levine,Iñigo Urteaga,Youssef Marzouk*

Main category: stat.ML

TL;DR: 论文提出了一种基于规范形式的贝叶斯方法，解决了LTI系统识别中的参数不可识别性问题，提高了推断效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯方法在LTI系统识别中因参数不可识别性导致复杂多模态后验，推断效率低下。

Method: 通过将LTI系统的规范形式嵌入贝叶斯框架，解决了参数不可识别性问题。

Result: 规范形式实现了更高的计算效率，生成可解释且行为良好的后验，并提供鲁棒的不确定性估计。

Conclusion: 该方法为LTI系统识别提供了高效、结构感知的贝叶斯解决方案。

Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system
identification are hindered by parameter non-identifiability; the resulting
complex, multi-modal posteriors make inference inefficient and impractical. We
solve this problem by embedding canonical forms of LTI systems within the
Bayesian framework. We rigorously establish that inference in these minimal
parameterizations fully captures all invariant system dynamics (e.g., transfer
functions, eigenvalues, predictive distributions of system outputs) while
resolving identifiability. This approach unlocks the use of meaningful,
structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures
conditions for a Bernstein--von Mises theorem -- a link between Bayesian and
frequentist large-sample asymptotics that is broken in standard forms.
Extensive simulations with modern MCMC methods highlight advantages over
standard parameterizations: canonical forms achieve higher computational
efficiency, generate interpretable and well-behaved posteriors, and provide
robust uncertainty estimates, particularly from limited data.

</details>


### [93] [TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models](https://arxiv.org/abs/2507.10643)
*Yuchi Tang,Iñaki Esnaola,Suzanne Mason,George Panoutsos*

Main category: stat.ML

TL;DR: 论文提出TaylorPODA方法，基于泰勒展开框架，通过引入‘精度’、‘联邦’和‘零差异’三个公设，量化特征贡献，并提供任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有后验模型无关方法缺乏系统框架量化特征贡献，需要更强的理论基础。

Method: 基于泰勒展开框架，提出TaylorPODA方法，引入三个公设和适应性属性。

Result: TaylorPODA在实验中表现优于基线方法，提供理论支持的可视化解释。

Conclusion: TaylorPODA为不透明模型的可靠部署提供了理论更强的解释方法。

Abstract: Existing post-hoc model-agnostic methods generate external explanations for
opaque models, primarily by locally attributing the model output to its input
features. However, they often lack an explicit and systematic framework for
quantifying the contribution of individual features. Building on the Taylor
expansion framework introduced by Deng et al. (2024) to unify existing local
attribution methods, we propose a rigorous set of postulates -- "precision",
"federation", and "zero-discrepancy" -- to govern Taylor term-specific
attribution. Guided by these postulates, we introduce TaylorPODA (Taylor
expansion-derived imPortance-Order aDapted Attribution), which incorporates an
additional "adaptation" property. This property enables alignment with
task-specific goals, especially in post-hoc settings lacking ground-truth
explanations. Empirical evaluations demonstrate that TaylorPODA achieves
competitive results against baseline methods, providing principled and
visualization-friendly explanations. This work represents a step toward the
trustworthy deployment of opaque models by offering explanations with stronger
theoretical grounding.

</details>


### [94] [Robust Multi-Manifold Clustering via Simplex Paths](https://arxiv.org/abs/2507.10710)
*Haoyu Chen,Anna Little,Akin Narayan*

Main category: stat.ML

TL;DR: 本文提出了一种新的几何方法用于多流形聚类（MMC），通过计算d-单纯形的局部图，利用相邻单纯形之间的二面角作为权重，并计算无穷路径距离，得到一种称为最大角度路径距离（LAPD）的度量。该方法在噪声、曲率和小交角下表现鲁棒，且计算复杂度接近线性。


<details>
  <summary>Details</summary>
Motivation: 多流形聚类（MMC）需要处理潜在相交的高维流形，现有方法在噪声和复杂几何结构下表现不佳。本文旨在提出一种更鲁棒且高效的解决方案。

Method: 通过构建d-单纯形的局部图，利用二面角作为权重，计算无穷路径距离（LAPD），并结合去噪步骤分离流形成分。

Result: 实验表明，该方法在合成和真实数据集上优于其他MMC算法，对噪声、曲率和小交角具有鲁棒性。

Conclusion: LAPD是一种有效的多流形聚类方法，具有高可扩展性和鲁棒性，适用于复杂几何结构的数据集。

Abstract: This article introduces a novel, geometric approach for multi-manifold
clustering (MMC), i.e. for clustering a collection of potentially intersecting,
d-dimensional manifolds into the individual manifold components. We first
compute a locality graph on d-simplices, using the dihedral angle in between
adjacent simplices as the graph weights, and then compute infinity path
distances in this simplex graph. This procedure gives a metric on simplices
which we refer to as the largest angle path distance (LAPD). We analyze the
properties of LAPD under random sampling, and prove that with an appropriate
denoising procedure, this metric separates the manifold components with high
probability. We validate the proposed methodology with extensive numerical
experiments on both synthetic and real-world data sets. These experiments
demonstrate that the method is robust to noise, curvature, and small
intersection angle, and generally out-performs other MMC algorithms. In
addition, we provide a highly scalable implementation of the proposed
algorithm, which leverages approximation schemes for infinity path distance to
achieve quasi-linear computational complexity.

</details>


### [95] [GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering](https://arxiv.org/abs/2507.10956)
*Zhaoyu Xing,Yang Wan,Juan Wen,Wei Zhong*

Main category: stat.ML

TL;DR: 提出了一种名为GOLFS的无监督特征选择方法，结合全局和局部信息，用于高维聚类问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏聚类标签，监督特征选择的正则化方法无法直接应用，因此需要一种无监督方法来同时学习伪标签和选择判别特征。

Method: GOLFS结合了流形学习的局部几何结构和正则化自表示的全局相关结构，通过迭代算法优化问题并证明收敛性。

Result: 模拟和实际数据应用表明，GOLFS在特征选择和聚类方面表现出色。

Conclusion: GOLFS通过综合利用全局和局部信息，提高了特征选择和聚类的准确性。

Abstract: It is important to identify the discriminative features for high dimensional
clustering. However, due to the lack of cluster labels, the regularization
methods developed for supervised feature selection can not be directly applied.
To learn the pseudo labels and select the discriminative features
simultaneously, we propose a new unsupervised feature selection method, named
GlObal and Local information combined Feature Selection (GOLFS), for high
dimensional clustering problems. The GOLFS algorithm combines both local
geometric structure via manifold learning and global correlation structure of
samples via regularized self-representation to select the discriminative
features. The combination improves the accuracy of both feature selection and
clustering by exploiting more comprehensive information. In addition, an
iterative algorithm is proposed to solve the optimization problem and the
convergency is proved. Simulations and two real data applications demonstrate
the excellent finite-sample performance of GOLFS on both feature selection and
clustering.

</details>


### [96] [Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection](https://arxiv.org/abs/2507.11136)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: 论文提出了一种贝叶斯张量网络核机器，通过概率框架自动推断模型复杂度，提升预测准确性、不确定度量和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有张量网络核方法多为确定性且忽略参数不确定性，需手动调整超参数，效率低。

Method: 采用稀疏诱导分层先验，通过变分推断近似后验，实现自动推断张量秩和特征维度。

Result: 实验显示模型在预测准确性、不确定度量、可解释性和扩展性上表现优越。

Conclusion: 贝叶斯张量网络核机器为高效、自动化的模型复杂度推断提供了可行方案。

Abstract: Tensor Network (TN) Kernel Machines speed up model learning by representing
parameters as low-rank TNs, reducing computation and memory use. However, most
TN-based Kernel methods are deterministic and ignore parameter uncertainty.
Further, they require manual tuning of model complexity hyperparameters like
tensor rank and feature dimensions, often through trial-and-error or
computationally costly methods like cross-validation. We propose Bayesian
Tensor Network Kernel Machines, a fully probabilistic framework that uses
sparsity-inducing hierarchical priors on TN factors to automatically infer
model complexity. This enables automatic inference of tensor rank and feature
dimensions, while also identifying the most relevant features for prediction,
thereby enhancing model interpretability. All the model parameters and
hyperparameters are treated as latent variables with corresponding priors.
Given the Bayesian approach and latent variable dependencies, we apply a
mean-field variational inference to approximate their posteriors. We show that
applying a mean-field approximation to TN factors yields a Bayesian ALS
algorithm with the same computational complexity as its deterministic
counterpart, enabling uncertainty quantification at no extra computational
cost. Experiments on synthetic and real-world datasets demonstrate the superior
performance of our model in prediction accuracy, uncertainty quantification,
interpretability, and scalability.

</details>


### [97] [How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction](https://arxiv.org/abs/2507.11161)
*Jun Chen,Hong Chen,Yonghua Yu,Yiming Ying*

Main category: stat.ML

TL;DR: 本文研究了标签错误对对比学习下游分类性能的理论影响，提出通过数据降维（如SVD）减少假阳性样本，并发现SVD的双刃剑效应。建议使用中等嵌入维度、数据膨胀、弱增强和SVD以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 对比学习在自监督表示学习中表现优异，但其理论理解依赖于标签一致性假设，该假设在实践中可能因增强策略的随机性而不成立。本文旨在探讨标签错误对下游分类性能的影响。

Method: 通过数据降维（如SVD）减少假阳性样本，并进行理论和实证评估。同时分析了SVD对增强图连通性的影响。

Result: 标签错误对下游分类风险有显著负面影响。SVD可减少假阳性样本，但可能因降低增强图连通性而损害分类精度。

Conclusion: 建议使用中等嵌入维度、数据膨胀、弱增强和SVD，以平衡增强图连通性和标签错误，从而提升模型性能。

Abstract: In recent years, contrastive learning has achieved state-of-the-art
performance in the territory of self-supervised representation learning. Many
previous works have attempted to provide the theoretical understanding
underlying the success of contrastive learning. Almost all of them rely on a
default assumption, i.e., the label consistency assumption, which may not hold
in practice (the probability of failure is called labeling error) due to the
strength and randomness of common augmentation strategies, such as random
resized crop (RRC). This paper investigates the theoretical impact of labeling
error on the downstream classification performance of contrastive learning. We
first reveal several significant negative impacts of labeling error on
downstream classification risk. To mitigate these impacts, data dimensionality
reduction method (e.g., singular value decomposition, SVD) is applied on
original data to reduce false positive samples, and establish both theoretical
and empirical evaluations. Moreover, it is also found that SVD acts as a
double-edged sword, which may lead to the deterioration of downstream
classification accuracy due to the reduced connectivity of the augmentation
graph. Based on the above observations, we give the augmentation suggestion
that we should use some moderate embedding dimension (such as $512, 1024$ in
our experiments), data inflation, weak augmentation, and SVD to ensure large
graph connectivity and small labeling error to improve model performance.

</details>


### [98] [From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies](https://arxiv.org/abs/2507.11381)
*Rom Gutman,Shimon Sheiba,Omer Noy Klien,Naama Dekel Bird,Amit Gruber,Doron Aronson,Oren Caspi,Uri Shalit*

Main category: stat.ML

TL;DR: 提出一个基于患者特异性治疗推荐的框架，结合因果模型和目标试验范式，强调安全性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决观察性数据中的因果识别问题，并整合现有方法为实用流程。

Method: 集成现有方法构建治疗推荐流程，未提出具体模型。

Result: 在心力衰竭患者急性肾损伤治疗中，流程优于当前治疗方案。

Conclusion: 框架能提升患者治疗效果，具有实际应用潜力。

Abstract: We propose a framework for building patient-specific treatment recommendation
models, building on the large recent literature on learning patient-level
causal models and inspired by the target trial paradigm of Hernan and Robins.
We focus on safety and validity, including the crucial issue of causal
identification when using observational data. We do not provide a specific
model, but rather a way to integrate existing methods and know-how into a
practical pipeline. We further provide a real world use-case of treatment
optimization for patients with heart failure who develop acute kidney injury
during hospitalization. The results suggest our pipeline can improve patient
outcomes over the current treatment regime.

</details>


### [99] [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](https://arxiv.org/abs/2507.11385)
*George D. Pasparakis,Ioannis A. Kougioumtzoglou,Michael D. Shields*

Main category: stat.ML

TL;DR: 提出了一种基于非参数贝叶斯字典学习的联合时空风场数据外推方法，通过有限测量数据估计相关统计量。


<details>
  <summary>Details</summary>
Motivation: 解决在有限传感器条件下高维风场数据的外推和统计估计问题。

Method: 利用稀疏测量数据，通过时间依赖优化问题确定随机风场的低维表示系数。

Result: 相比传统压缩感知方法，该方法具有更高的外推精度和自适应基选择能力。

Conclusion: 该方法适用于多种风工程应用，尤其在数据高维和非高斯特性时表现优异。

Abstract: A methodology is developed, based on nonparametric Bayesian dictionary
learning, for joint space-time wind field data extrapolation and estimation of
related statistics by relying on limited/incomplete measurements. Specifically,
utilizing sparse/incomplete measured data, a time-dependent optimization
problem is formulated for determining the expansion coefficients of an
associated low-dimensional representation of the stochastic wind field.
Compared to an alternative, standard, compressive sampling treatment of the
problem, the developed methodology exhibits the following advantages. First,
the Bayesian formulation enables also the quantification of the uncertainty in
the estimates. Second, the requirement in standard CS-based applications for an
a priori selection of the expansion basis is circumvented. Instead, this is
done herein in an adaptive manner based on the acquired data. Overall, the
methodology exhibits enhanced extrapolation accuracy, even in cases of
high-dimensional data of arbitrary form, and of relatively large extrapolation
distances. Thus, it can be used, potentially, in a wide range of wind
engineering applications where various constraints dictate the use of a limited
number of sensors. The efficacy of the methodology is demonstrated by
considering two case studies. The first relates to the extrapolation of
simulated wind velocity records consistent with a prescribed joint
wavenumber-frequency power spectral density in a three-dimensional domain (2D
and time). The second pertains to the extrapolation of four-dimensional (3D and
time) boundary layer wind tunnel experimental data that exhibit significant
spatial variability and non-Gaussian characteristics.

</details>
