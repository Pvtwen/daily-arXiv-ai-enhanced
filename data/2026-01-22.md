<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 73]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Experimental Performance of Bidirectional Phase Coherent Transmission and Sensing for mmWave Cell-free Massive MIMO Systems with Reciprocity Calibration](https://arxiv.org/abs/2601.14648)
*Qingji Jiang,Jing jin,Qixing Wang,Yuanyuan Tang,Yang Cao,Bin Kuang,Jing Dong,Siying Lv,Dongming Wang,Yongming Huang,Jiangzhou Wang,Xiaohu You*

Main category: eess.SP

TL;DR: 提出毫米波无蜂窝大规模MIMO系统中相位同步的双向校准方案，利用感知结果实现动态场景下的校准系数相位跟踪，实现通信与感知的双向赋能。


<details>
  <summary>Details</summary>
Motivation: 分布式传输接收点之间的相位同步是实现相干联合传输和高精度感知的前提，需要解决非理想因素影响和动态场景下的校准问题。

Method: 提出双向校准方案和校准系数估计方法，利用单向上行/下行信道状态信息进行校准系数相位跟踪，通过互易性校准消除感知中的非理想因素。

Result: 仿真表明该方法能以较低开销实现互易性校准，实现相干协作传输，降低感知误差；实验验证在毫米波频段，空中双向校准能为协作TRP和协作UE实现相干协作传输，获得波束成形增益和长时间相干感知能力。

Conclusion: 提出的方法有效解决了毫米波无蜂窝大规模MIMO系统中的相位同步问题，通过通信与感知的双向赋能，实现了相干协作传输和高精度感知能力。

Abstract: Phase synchronization among distributed transmission reception points (TRPs) is a prerequisite for enabling coherent joint transmission and high-precision sensing in millimeter wave (mmWave) cell-free massive multiple-input and multiple-output (MIMO) systems. This paper proposes a bidirectional calibration scheme and a calibration coefficient estimation method for phase synchronization, and presents a calibration coefficient phase tracking method using unilateral uplink/downlink channel state information (CSI). Furthermore, this paper introduces the use of reciprocity calibration to eliminate non-ideal factors in sensing and leverages sensing results to achieve calibration coefficient phase tracking in dynamic scenarios, thus enabling bidirectional empowerment of both communication and sensing. Simulation results demonstrate that the proposed method can effectively implement reciprocal calibration with lower overhead, enabling coherent collaborative transmission, and resolving non-ideal factors to acquire lower sensing error in sensing applications. Experimental results show that, in the mmWave band, over-the-air (OTA) bidirectional calibration enables coherent collaborative transmission for both collaborative TRPs and collaborative user equipments (UEs), achieving beamforming gain and long-time coherent sensing capabilities.

</details>


### [2] [Improved GPR-Based CSI Acquisition via Spatial-Correlation Kernel](https://arxiv.org/abs/2601.14759)
*Syed Luqman Shah,Nurul Huda Mahmood,Italo Atzeni*

Main category: eess.SP

TL;DR: 提出一种基于高斯过程回归的信道估计框架，使用新颖的空间相关核函数，在减少50%导频开销下实现最低归一化均方误差


<details>
  <summary>Details</summary>
Motivation: 多天线无线系统需要低导频开销和低计算复杂度的准确信道估计。随着信道模型从纯统计描述向物理和几何感知模型演进，需要将信道信息融入高斯过程回归框架以提高估计精度

Method: 提出基于高斯过程回归的信道估计框架，设计新颖的空间相关核函数来显式捕获信道的二阶统计特性。推导了SC-GPR估计器的闭式表达式，并证明其后验均值在相同二阶统计下是最小均方误差最优的

Result: 在减少50%导频开销下，该方法实现了最低的归一化均方误差、最高的经验95%置信区间覆盖率，以及优于基准估计器的频谱效率保持能力，同时计算复杂度低于传统MMSE估计器

Conclusion: 提出的SC-GPR信道估计框架能够有效利用信道二阶统计信息，在显著降低导频开销的同时保持高精度和低复杂度，为多天线系统提供了高效的信道估计解决方案

Abstract: Accurate channel estimation with low pilot overhead and computational complexity is key to efficiently utilizing multi-antenna wireless systems. Motivated by the evolution from purely statistical descriptions toward physics- and geometry-aware propagation models, this work focuses on incorporating channel information into a Gaussian process regression (GPR) framework for improving the channel estimation accuracy. In this work, we propose a GPR-based channel estimation framework along with a novel Spatial-correlation (SC) kernel that explicitly captures the channel's second-order statistics. We derive a closed-form expression of the proposed SC-based GPR estimator and prove that its posterior mean is optimal in terms of minimum mean-square error (MMSE) under the same second-order statistics, without requiring the underlying channel distribution to be Gaussian. Our analysis reveals that, with up to 50% pilot overhead reduction, the proposed method achieves the lowest normalized mean-square error, the highest empirical 95% credible-interval coverage, and superior preservation of spectral efficiency compared to benchmark estimators, while maintaining lower computational complexity than the conventional MMSE estimator.

</details>


### [3] [Integrated Sensing, Communication and Control enabled Agile UAV Swarm](https://arxiv.org/abs/2601.14783)
*Zhiqing Wei,Yucong Du,Zhiyong Feng,Haotian Liu,Yanpeng Cui,Tao Zhang,Ying Zhou,Huici Wu*

Main category: eess.SP

TL;DR: 本文提出了一种无人机集群的集成感知、通信与控制（ISCC）深度耦合方案，通过协同优化打破传统独立设计的局限，建立紧密耦合的闭环系统。


<details>
  <summary>Details</summary>
Motivation: 无人机集群在灾害救援、空中基站和物流运输等应用中至关重要，需要精确感知、高效通信和灵活控制能力。然而传统研究中感知、通信和控制各自独立研究，限制了无人机集群的整体性能。

Method: 提出ISCC系统范式，包括三个关键技术：通信与控制增强的感知、感知与控制增强的通信、感知与通信增强的控制，通过协同优化建立紧密耦合的闭环。

Result: 仿真结果验证了所提ISCC框架的性能，展示了其在未来应用中的潜力。

Conclusion: ISCC方案超越了传统孤立设计，通过深度耦合感知、通信和控制，为无人机集群提供了实时可靠的任务执行能力，具有广阔的应用前景。

Abstract: Uncrewed aerial vehicle (UAV) swarms are pivotal in the applications such as disaster relief, aerial base station (BS) and logistics transportation. These scenarios require the capabilities in accurate sensing, efficient communication and flexible control for real-time and reliable task execution. However, sensing, communication and control are studied independently in traditional research, which limits the overall performance of UAV swarms. To overcome this disadvantage, we propose a deeply coupled scheme of integrated sensing, communication and control (ISCC) for UAV swarms, which is a systemic paradigm that transcends traditional isolated designs of sensing, communication and control by establishing a tightly-coupled closed-loop through the co-optimization of sensing, communication and control. In this article, we firstly analyze the requirements of scenarios and key performance metrics. Subsequently, the enabling technologies are proposed, including communication-and-control-enhanced sensing, sensing-and-control-enhanced communication, and sensing-and-communication-enhanced control. Simulation results validate the performance of the proposed ISCC framework, demonstrating its application potential in the future.

</details>


### [4] [Absorption mode broadband 2D MS for proteomics and metabolomics](https://arxiv.org/abs/2601.14820)
*Maria A van Agthoven,Marek Polák,Jan Fiala,Claude Nelcy Ounounou,Petr Halada,Michael Palasser,Anne Briot-Dietsch,Alan Kádek,Kathrin Breuker,Petr Novák,Carlos Afonso,Marc-André Delsuc*

Main category: eess.SP

TL;DR: 该研究将吸收模式数据处理扩展到任意尺寸和频率范围的二维质谱，显著提高了信噪比和分辨率，在蛋白质组学和代谢组学应用中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 二维质谱（2D MS）无需离子隔离即可关联前体和碎片离子，但传统数据处理方式在信噪比和分辨率方面存在局限。吸收模式数据处理在有限数据集上已显示出信噪比和分辨率提升2倍的潜力，但需要扩展到任意尺寸和频率范围的二维质谱。

Method: 在傅里叶变换离子回旋共振质谱仪上，发现吸收模式数据处理的相位校正函数在前体离子维度呈线性，在碎片离子维度呈二次函数。将吸收模式数据处理扩展到任意尺寸和频率范围的二维质谱，应用于快速光化学氧化蛋白质（FPOP）产生的各种氧化泛素蛋白型，以及麦角生物碱提取物。

Result: 吸收模式数据处理相比标准幅度模式，显著提高了二维质谱的信噪比和分辨率。在自上而下蛋白质组学中提高了序列覆盖率，在代谢组学中提高了前体-碎片关联的准确性。

Conclusion: 吸收模式数据处理可成功应用于任意尺寸的二维质谱，为蛋白质组学和代谢组学分析提供了更高质量的数据处理方案，显著提升了分析性能。

Abstract: Two-dimensional mass spectrometry (2D MS) is a method for tandem mass spectrometry that enables the correlation between precursor and fragment ions without the need for ion isolation. On a Fourier transform ion cyclotron resonance mass spectrometer, the phase correction functions for absorption mode data processing were found to be linear in the precursor ion dimension and quadratic in the fragment ion dimension. Absorption mode data processing on limited data sets has previously shown improvements in signal-to-noise ratio and resolving power by a factor of 2. Here, we have expanded absorption mode data processing to 2D mass spectra regardless of size and frequency range. We have applied absorption mode 2D MS to top-down analysis of variously oxidized ubiquitin proteoforms generated by fast photochemical oxidation of proteins (FPOP) and to an extract of ergot alkaloids. We show that absorption mode data processing significantly improves both the signal-to-noise ratio and the resolving power of the 2D mass spectrum compared to standard magnitude mode in terms of sequence coverage in top-down proteomics, as well as the accuracy of precursor-fragment correlation in metabolomics.

</details>


### [5] [Movable Antenna Empowered Covert Dual-Functional Radar-Communication](https://arxiv.org/abs/2601.14868)
*Ran Yang,Ning Wei,Zheng Dong,Lin Zhang,Wanting Lyu,Yue Xiu,Ahmad Bazzi,Chadi Assi*

Main category: eess.SP

TL;DR: 本文研究了一种基于可移动天线的安全双功能雷达通信系统，通过联合优化波束成形、接收滤波和天线位置，在满足雷达性能和隐蔽性约束下最大化通信安全速率。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术能够灵活重构无线信道，为提升通信安全提供了新途径。在双功能雷达通信系统中，需要平衡通信性能和雷达性能，同时应对窃听者的威胁，特别是非共谋和共谋两种窃听模式下的安全挑战。

Method: 针对非共谋窃听者，采用拉格朗日对偶变换将问题重构，开发基于块坐标下降的算法，结合半定松弛、投影梯度下降、Dinkelbach变换和逐次凸逼近技术。针对共谋窃听者，推导最小检测错误概率，提出基于最小均方误差的算法处理共谋检测问题。

Result: 仿真结果表明，所提方法能显著提高隐蔽和速率，相比现有基准方案，在通信和雷达性能之间实现了更好的平衡。

Conclusion: 可移动天线技术能有效增强双功能雷达通信系统的安全性能，所提出的统一设计框架在处理不同窃听模式时都表现出色，为未来安全通信系统设计提供了有价值的参考。

Abstract: Movable antenna (MA) has emerged as a promising technology to flexibly reconfigure wireless channels by adjusting antenna placement. In this paper, we study a secured dual-functional radar-communication (DFRC) system aided by movable antennas. To enhance the communication security, we aim to maximize the achievable sum rate by jointly optimizing the transmitter beamforming vectors, receiving filter, and antenna placement, subject to radar signal-to-noise ratio (SINR) and transmission covertness constraints. We consider multiple Willies operating in both non-colluding and colluding modes. For noncolluding Willies, we first employ a Lagrangian dual transformation procedure to reformulate the challenging optimization problem into a more tractable form. Subsequently, we develop an efficient block coordinate descent (BCD) algorithm that integrates semidefinite relaxation (SDR), projected gradient descent (PGD), Dinkelbach transformation, and successive convex approximation (SCA) techniques to tackle the resulting problem. For colluding Willies, we first derive the minimum detection error probability (DEP) by characterizing the optimal detection statistic, which is proven to follow the generalized Erlang distribution. Then, we develop a minimum mean square error (MMSE)-based algorithm to address the colluding detection problem. We further provide a comprehensive complexity analysis on the unified design framework. Simulation results demonstrate that the proposed method can significantly improve the covert sum rate, and achieve a superior balance between communication and radar performance compared with existing benchmark schemes.

</details>


### [6] [Analysis of Sensing in OFDM-based ISAC under the Influence of Sampling Jitter](https://arxiv.org/abs/2601.14881)
*Lucas Giroto,Ândrei Camponogara,Yueheng Li,Jiayi Chen,Lukas Sigg,Thomas Zwick,Benjamin Nuss*

Main category: eess.SP

TL;DR: 本文分析了采样抖动对基于OFDM的集成感知与通信系统的影响，发现当RMS采样抖动超过10^-11秒时，性能下降变得不可忽视，但现有硬件已能提供足够的抗抖动能力。


<details>
  <summary>Details</summary>
Motivation: 在蜂窝网络中实现集成感知与通信时，采样抖动成为一个关键挑战。虽然OFDM通信系统中的采样抖动已有研究，但其对OFDM雷达感知性能的影响尚未得到充分分析，特别是考虑到PLL振荡器产生的有色采样抖动。

Method: 分析了基于PLL振荡器的采样时钟产生的有色采样抖动对OFDM-ISAC系统的影响，比较了基带采样和带通采样策略在不同过采样因子下的性能。研究了采样抖动对数字星座调制和感知性能的失真影响。

Result: 对于基带采样，采样抖动引起载波间干扰；对于带通采样，则导致载波相位误差和更严重的载波间干扰。只有当DAC和ADC的RMS采样抖动值超过10^-11秒（约为无过采样时临界采样周期的0.5*10^-2倍）时，性能下降才变得不可忽视。

Conclusion: 现有硬件技术能够实现飞秒量级的RMS采样抖动，这为基于OFDM的集成感知与通信系统提供了足够的通信和感知鲁棒性，能够有效抵抗采样抖动的影响。

Abstract: To enable integrated sensing and communication (ISAC) in cellular networks, a wide range of additional requirements and challenges are either imposed or become more critical. One such impairment is sampling jitter (SJ), which arises due to imperfections in the sampling instants of the clocks of digital-to-analog converters (DACs) and analog-to-digital converters (ADCs). While SJ is already well studied for communication systems based on orthogonal frequency-division multiplexing (OFDM), which is expected to be the waveform of choice for most sixth-generation (6G) scenarios where ISAC could be possible, the implications of SJ on the OFDM-based radar sensing must still be thoroughly analyzed. Considering that phase-locked loop (PLL)-based oscillators are used to derive sampling clocks, which leads to colored SJ, i.e., SJ with non-flat power spectral density, this article analyzes the resulting distortion of the adopted digital constellation modulation and sensing performance in OFDM-based ISAC for both baseband (BB) and bandpass (BP) sampling strategies and different oversampling factors. For BB sampling, it is seen that SJ induces intercarrier interference (ICI), while for BP sampling, it causes carrier phase error and more severe ICI due to a phase noise-like effect at the digital intermediate frequency. Obtained results for a single-input single-output OFDM-based ISAC system with various OFDM signal parameterizations demonstrate that SJ-induced degradation becomes non-negligible for both BB and BP sampling only for root mean square (RMS) SJ values above 10^-11 s at both DAC and ADC, which corresponds to 0.5*10^-2 times the considered critical sampling period without oversampling. Based on the achieved results, it can be concluded that state-of-the-art hardware enables sufficient communication and sensing robustness against SJ, as RMS SJ values in the femtosecond range can be achieved.

</details>


### [7] [Deep Learning assisted Port-Cycling based Channel Sounding for Precoder Estimation in Massive MIMO Arrays](https://arxiv.org/abs/2601.14953)
*Advaith Arun,Shiv Shankar,Dhivagar Baskaran,Klutto Milleth,Bhaskar Ramamurthi*

Main category: eess.SP

TL;DR: 提出基于深度学习的CSI重建框架，通过端口循环机制降低参考信号开销，同时保持高精度信道状态信息重建


<details>
  <summary>Details</summary>
Motivation: 未来无线系统需要更多发射端口进行CSI估计，但增加端口会提高参考信号资源开销，降低数据吞吐量，需要解决这一矛盾

Method: 设计端口循环机制，在时间上顺序探测不同CSI端口子集，降低开销；提出CsiAdaNet模型，利用稀疏测量捕获时空相关性来重建完整端口CSI

Result: 仿真结果显示该方法在降低开销的同时保持了高CSI重建精度

Conclusion: 提出的DL-based CSI重建框架是未来6G系统中可靠CSI获取的有效使能技术，通过端口循环和深度学习实现了开销与精度的平衡

Abstract: Future wireless systems are expected to employ a substantially larger number of transmit ports for channel state information (CSI) estimation compared to current specifications. Although scaling ports improves spectral efficiency, it also increases the resource overhead to transmit reference signals across the time-frequency grid, ultimately reducing achievable data throughput. In this work, we propose an deep learning (DL)-based CSI reconstruction framework that serves as an enabler for reliable CSI acquisition in future 6G systems. The proposed solution involves designing a port-cycling mechanism that sequentially sounds different portions of CSI ports across time, thereby lowering the overhead while preserving channel observability. The proposed CSI Adaptive Network (CsiAdaNet) model exploits the resulting sparse measurements and captures both spatial and temporal correlations to accurately reconstruct the full-port CSI. The simulation results show that our method achieves overhead reduction while maintaining high CSI reconstruction accuracy.

</details>


### [8] [Alternative Shapes of Modulation Schemes Detailed Exposition and Simulation Methodology](https://arxiv.org/abs/2601.15004)
*Nipun Agarwal*

Main category: eess.SP

TL;DR: 该论文从几何、概率、优化和机器学习角度统一研究调制星座设计，评估多种星座方案在AWGN和瑞利衰落信道下的性能，发现SER最优设计不一定能量最优，机器学习方法能灵活联合优化可靠性、鲁棒性和能量效率。


<details>
  <summary>Details</summary>
Motivation: 传统PSK和QAM等调制方案在现实信道和非线性硬件约束下往往失去最优性，需要更全面的星座设计方法来同时满足频谱效率、鲁棒性和能量消耗的严格要求。

Method: 从几何、概率、优化和机器学习四个角度统一研究星座设计，通过大规模蒙特卡洛模拟评估经典、基于格点、非对称、概率整形、黄金角、启发式优化和机器学习辅助星座在AWGN和瑞利衰落信道下的性能，并纳入PAPR感知和功率放大器模型。

Result: SER最优设计并不总是能量最优，小的SER权衡可以带来显著的能量节省；机器学习方法通过将信道和硬件约束嵌入学习目标，能够灵活地联合优化可靠性、鲁棒性和能量效率。

Conclusion: 星座设计需要综合考虑SER、衰落鲁棒性、PAPR和能量效率等多重指标，机器学习为联合优化这些相互冲突的目标提供了有前景的框架，而传统SER最优设计在实际系统中可能不是能量最优的选择。

Abstract: Modulation constellation design is a core challenge in digital communications, especially under stringent demands on spectral efficiency, robustness, and energy consumption. Classical schemes like PSK and QAM, while analytically tractable, often lose optimality under realistic channels and nonlinear hardware constraints. This paper provides a unified study of constellation design from geometric, probabilistic, optimization, and machine learning perspectives, focusing on symbol error rate (SER), fading robustness, peak-to-average power ratio (PAPR), and energy efficiency. We evaluate classical, lattice-based, asymmetric, probabilistically shaped, Golden Angle, heuristic-optimized, and machine learning assisted constellations under AWGN and Rayleigh fading via large-scale Monte Carlo simulations. Incorporating PAPR-aware and power amplifier models reveals that SER-optimal designs are not always energy-optimal; small SER trade-offs can yield substantial energy savings. Machine learning approaches offer flexible joint optimization of reliability, robustness, and energy efficiency by embedding channel and hardware constraints into the learning objective.

</details>


### [9] [Physical Layer Security in Massive MIMO: Challenges and Open Research Directions Against Passive Eavesdroppers](https://arxiv.org/abs/2601.15024)
*Nipun Agarwal*

Main category: eess.SP

TL;DR: 本文研究了大规模MIMO系统中存在被动窃听者时的物理层安全传输方案性能，比较了MRT、ZF和AN辅助波束赋形等方案在不同系统参数下的保密性能。


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO是5G及未来网络的关键技术，但存在被动窃听者时难以保证通信安全。物理层安全利用无线信道随机性提供安全保证，但其在大规模MIMO中的有效性依赖于资源分配和传输策略，需要系统评估不同方案的性能。

Method: 采用广泛的蒙特卡洛仿真方法，评估最大比传输(MRT)、迫零(ZF)和人工噪声(AN)辅助波束赋形等安全传输方案在不同系统参数（如天线数量、信噪比、功率分配）下的性能。

Result: 通过仿真评估了能量效率、保密中断概率和保密和速率等关键性能指标，为不同物理层安全策略提供了比较性见解，揭示了各自的优势和局限性。

Conclusion: 研究结果为未来6G网络设计可扩展、高能效和鲁棒的安全传输技术提供了指导，并指出了开放的研究方向，包括如何优化资源分配以在安全性和效率之间取得平衡。

Abstract: Massive Multiple-Input Multiple-Output (MIMO) has become a crucial enabling technology for 5G and beyond, providing previously unheard-of increases in energy and spectrum efficiency. It is still difficult to guarantee secure communication in these systems, particularly when it comes to passive eavesdroppers whose base station is unaware of their channel state information. By taking advantage of the inherent randomness of wireless channels, Physical Layer Security (PLS) offers a promising paradigm; however, its efficacy in massive MIMO is heavily reliant on resource allocation and transmission strategies. In this work, the performance of secure transmission schemes, such as Maximum Ratio Transmission (MRT), Zero-Forcing (ZF), and Artificial Noise (AN)-aided beamforming, is examined when passive eavesdroppers are present. This work will use extensive Monte Carlo simulations to assess important performance metrics such as energy efficiency, secrecy outage probability, and secrecy sum rate under different system parameters (e.g., number of antennas, Signal-to-Noise Ratio (SNR), power allocation). The results aim to provide comparative insight into the strengths and limitations of different PLS strategies and to highlight open research directions to design scalable, energy-efficient, and robust secure transmission techniques in future 6G networks.

</details>


### [10] [Neural Tracking of Sustained Attention, Attention Switching, and Natural Conversation in Audiovisual Environments using Mobile EEG](https://arxiv.org/abs/2601.15097)
*Johanna Wilroth,Oskar Keding,Martin A. Skoglund,Maria Sandsten,Martin Enqvist,Emina Alickovic*

Main category: eess.SP

TL;DR: 移动EEG系统在动态多感官场景中可靠追踪选择性注意力，包括持续注意、注意力切换和对话条件


<details>
  <summary>Details</summary>
Motivation: 日常交流是动态多感官的，但现有神经注意力追踪研究多局限于实验室控制环境，使用清洁刺激且要求持续注意单一说话者，需要填补这一研究空白

Method: 使用移动EEG系统（44个头皮电极和20个cEEGrid电极）在视听范式下收集24名正常听力参与者数据，包括三种条件：双说话者环境中持续注意单一说话者、在两个说话者间切换注意力、以及有竞争说话者的非脚本对话

Result: 头皮EEG显示各条件下注意与忽略语音的P2峰存在显著差异；注意力切换与持续注意性能无显著变化；对话条件比单一说话者视听刺激的峰值更窄；选择性注意分类准确率55-70%；cEEGrid数据相关性较低

Conclusion: 移动EEG可可靠追踪动态多感官场景中的选择性注意力，为未来视听范式和现实世界注意力追踪应用提供指导

Abstract: Everyday communication is dynamic and multisensory, often involving shifting attention, overlapping speech and visual cues. Yet, most neural attention tracking studies are still limited to highly controlled lab settings, using clean, often audio-only stimuli and requiring sustained attention to a single talker. This work addresses that gap by introducing a novel dataset from 24 normal-hearing participants. We used a mobile electroencephalography (EEG) system (44 scalp electrodes and 20 cEEGrid electrodes) in an audiovisual (AV) paradigm with three conditions: sustained attention to a single talker in a two-talker environment, attention switching between two talkers, and unscripted two-talker conversations with a competing single talker. Analysis included temporal response functions (TRFs) modeling, optimal lag analysis, selective attention classification with decision windows ranging from 1.1s to 35s, and comparisons of TRFs for attention to AV conversations versus side audio-only talkers. Key findings show significant differences in the attention-related P2-peak between attended and ignored speech across conditions for scalp EEG. No significant change in performance between switching and sustained attention suggests robustness for attention switches. Optimal lag analysis revealed narrower peak for conversation compared to single-talker AV stimuli, reflecting the additional complexity of multi-talker processing. Classification of selective attention was consistently above chance (55-70% accuracy) for scalp EEG, while cEEGrid data yielded lower correlations, highlighting the need for further methodological improvements. These results demonstrate that mobile EEG can reliably track selective attention in dynamic, multisensory listening scenarios and provide guidance for designing future AV paradigms and real-world attention tracking applications.

</details>


### [11] [Sparse Sensor Arrays for Active Sensing: Models, Configurations and Applications](https://arxiv.org/abs/2601.15126)
*Robin Rajamäki,Visa Koivunen*

Main category: eess.SP

TL;DR: 本章聚焦于使用稀疏阵列进行主动感知，探讨了稀疏阵列在雷达、声纳、无线通信和医疗超声等主动感知应用中的优势，包括通过更少的物理传感器实现更高分辨率，以及通过和共阵列获得更多虚拟传感器。


<details>
  <summary>Details</summary>
Motivation: 在主动感知应用中，传感器阵列通过发射自身产生的能量来探测环境。稀疏传感器阵列相比传统均匀阵列具有显著优势：使用更少的物理传感器实现更高的分辨率，并且能够识别比传感器数量更多的散射体。这些优势通过和共阵列（虚拟阵列）实现，该阵列可以拥有比物理发射或接收传感器多得多的虚拟传感器。

Method: 1. 设计低冗余稀疏阵列配置；2. 使用稀疏阵列进行发射-接收波束成形；3. 讨论最优但计算复杂的最小冗余阵列；4. 提出可扩展的对称阵列框架，将被动稀疏阵列几何扩展到主动情况；5. 通过图像相加合成波束成形方法减轻空间欠采样引起的旁瓣；6. 寻找合成所需发射-接收波束模式的物理波束成形权重；7. 考虑相关的时空权衡。

Result: 提出了一个系统性的稀疏阵列设计和波束成形框架，包括最小冗余阵列和对称阵列设计方法，以及通过图像相加技术减轻旁瓣效应的方法。这些方法使得稀疏阵列能够在主动感知应用中实现高性能，同时减少物理传感器数量。

Conclusion: 稀疏阵列在主动感知应用中具有重要价值，能够以更少的物理传感器实现高性能探测。通过和共阵列概念、低冗余阵列设计、对称阵列框架和合成波束成形技术，可以充分发挥稀疏阵列的优势。最后讨论了稀疏阵列在主动感知中的具体应用。

Abstract: This chapter focuses on active sensing using sparse arrays. In active sensing applications, such as radar, sonar, wireless communications, and medical ultrasound, a collection of sensors probes the environment by emitting self-generated energy. A key benefit of such active multi-sensor arrays is their ability to focus and steer energy in desired directions by beamforming on transmit. Sparse sensor arrays offer several advantages over conventional uniform arrays, including improved resolution using fewer physical sensors and the capability to identify more scatterers than sensors. This is facilitated by the effective transmit-receive virtual array known as the sum co-array, which can have many more virtual sensors than the number of physical transmit or receive sensors. Herein, we focus on the design of low-redundancy sparse array configurations and on employing transmit-receive (Tx-Rx) beamforming using sparse arrays. We discuss the optimal, but computationally intractable Minimum-redundancy array, and a scalable symmetric array framework, which extends many well-known passive sparse array geometries to the active case. We also examine mitigating side lobes arising from spatial undersampling by a synthetic beamforming method known as image addition. We briefly present approaches for finding the physical beamforming weights synthesizing a desired Tx-Rx beampattern, and consider related spatio-temporal trade-offs. We conclude by discussing selected applications of sparse arrays in active sensing.

</details>


### [12] [Weather Estimation for Integrated Sensing and Communication](https://arxiv.org/abs/2601.15145)
*Victoria Palhares,Artjom Grudnitsky,Silvio Mandelli*

Main category: eess.SP

TL;DR: 6G ISAC网络可用于天气感知，通过基站检测和估计降水率、风速等天气条件，准确率高达99%以上。


<details>
  <summary>Details</summary>
Motivation: 传统天气雷达昂贵且覆盖范围有限，而6G ISAC网络基站部署密集，可复用其雷达副产品实现低成本、高覆盖的天气感知服务。

Method: 使用卷积神经网络实现分类器和回归器，在ISAC概念验证平台上进行多周实验，训练数据包含不同降水率和风速的测量值。

Result: 降水率分类准确率99.38%，风速分类准确率98.99%；降水率估计误差1.2 mm/h，风速估计误差1.5 km/h。

Conclusion: 6G ISAC网络可可靠部署天气感知服务，扩展服务组合并提升市场价值。

Abstract: One of the key features of sixth generation (6G) mobile communications will be integrated sensing and communication (ISAC). While the main goal of ISAC in standardization efforts is to detect objects, the byproducts of radar operations can be used to enable new services in 6G, such as weather sensing. Even though weather radars are the most prominent technology for weather detection and monitoring, they are expensive and usually neglect areas in close vicinity. To this end, we propose reusing the dense deployment of 6G base stations for weather sensing purposes by detecting and estimating weather conditions. We implement both a classifier and a regressor as a convolutional neural network trained across measurements with varying precipitation rates and wind speeds. We implement our approach in an ISAC proof-of-concept, and conduct a multi-week experiment campaign. Experimental results show that we are able to jointly and accurately classify weather conditions with accuracies of 99.38% and 98.99% for precipitation rate and wind speed, respectively. For estimation, we obtain errors of 1.2 mm/h and 1.5 km/h, for precipitation rate and wind speed, respectively. These findings indicate that weather sensing services can be reliably deployed in 6G ISAC networks, broadening their service portfolio and boosting their market value.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Call2Instruct: Automated Pipeline for Generating Q&A Datasets from Call Center Recordings for LLM Fine-Tuning](https://arxiv.org/abs/2601.14263)
*Alex Echeverria,Sávio Salvarino Teles de Oliveira,Fernando Marques Federson*

Main category: cs.LG

TL;DR: 提出從客服通話錄音自動生成問答指令微調數據集的端到端管道，包含音頻處理、文本處理、語義提取與配對，成功用於微調Llama 2 7B模型。


<details>
  <summary>Details</summary>
Motivation: 大型語言模型在特定領域的適應依賴高質量微調數據集，但從客服通話錄音等非結構化數據生成問答格式數據集面臨噪音和混亂的挑戰。

Method: 端到端自動化管道：音頻處理（說話人分離、降噪、自動轉錄）、文本處理（清洗、標準化、匿名化）、使用向量嵌入進行客戶需求與客服回應的語義提取，通過語義搜索配對形成最終問答對。

Result: 成功實現完整管道，生成專為指令微調格式化的數據集，並通過微調Llama 2 7B模型驗證了數據集的實用價值和可行性。

Conclusion: 該方法可行，能將客服通話的非結構化對話數據轉化為訓練大型語言模型的寶貴資源，為客服領域問答任務開發更有效的AI系統開闢途徑，代碼已公開以促進可重現性和未來研究。

Abstract: The adaptation of Large-Scale Language Models (LLMs) to specific domains depends on high-quality fine-tuning datasets, particularly in instructional format (e.g., Question-Answer - Q&A). However, generating these datasets, particularly from unstructured sources such as call center audio recordings, poses a significant challenge due to the noisy and disorganized nature of the data. This paper presents a solution to this challenge by offering an end-to-end automated pipeline for generating Q&A instructional datasets from such recordings. The methodology developed comprises sequential steps of audio processing (including diarization, noise removal and automatic transcription), textual processing (cleaning, normalization, and anonymization), semantic extraction of customer demands and attendant responses using vector embeddings, and matching via semantic search to form the final Q&A pairs. As a result, the complete pipeline was successfully implemented, generating a dataset specifically formatted for Instruct Fine Tuning. The practical value and feasibility of the generated dataset were substantiated and functionally demonstrated through the successful fine-tuning of an LLM model (based on Llama 2 7B). The conclusion of the paper states that the proposed approach is viable for converting unstructured conversational data from call centers into valuable resources for training LLMs. This development has the potential to open up avenues for creating more effective AI systems for Q&A tasks in the customer service domain. The developed codes have been made publicly available to promote reproducibility and future research.

</details>


### [14] [GCG Attack On A Diffusion LLM](https://arxiv.org/abs/2601.14266)
*Ruben Neyroud,Sam Corley*

Main category: cs.LG

TL;DR: 探索GCG风格对抗提示攻击在扩散语言模型LLaDA上的适用性，评估多种攻击变体对有害提示的效果


<details>
  <summary>Details</summary>
Motivation: 虽然大多数LLM是自回归的，但扩散语言模型作为替代生成方法出现。GCG攻击对自回归模型有效，但其在扩散语言模型上的适用性尚未充分探索

Method: 在开源扩散LLM LLaDA上进行GCG风格对抗提示攻击的探索性研究，评估包括前缀扰动和后缀对抗生成在内的多种攻击变体，使用AdvBench数据集中的有害提示

Result: 研究提供了扩散语言模型鲁棒性和攻击面的初步见解，表明需要为这种设置开发替代优化和评估策略

Conclusion: 扩散语言模型的对抗攻击研究仍处于早期阶段，需要开发专门针对这类模型的优化和评估方法

Abstract: While most LLMs are autoregressive, diffusion-based LLMs have recently emerged as an alternative method for generation. Greedy Coordinate Gradient (GCG) attacks have proven effective against autoregressive models, but their applicability to diffusion language models remains largely unexplored. In this work, we present an exploratory study of GCG-style adversarial prompt attacks on LLaDA (Large Language Diffusion with mAsking), an open-source diffusion LLM. We evaluate multiple attack variants, including prefix perturbations and suffix-based adversarial generation, on harmful prompts drawn from the AdvBench dataset. Our study provides initial insights into the robustness and attack surface of diffusion language models and motivates the development of alternative optimization and evaluation strategies for adversarial analysis in this setting.

</details>


### [15] [Divide and Refine: Enhancing Multimodal Representation and Explainability for Emotion Recognition in Conversation](https://arxiv.org/abs/2601.14274)
*Anh-Tuan Mai,Cam-Van Thi Nguyen,Duc-Trong Le*

Main category: cs.LG

TL;DR: 提出DnR框架，通过分解模态为独有、冗余、协同三部分，再分别优化，提升多模态对话情感识别性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法难以平衡模态独有信息、跨模态冗余信息和协同信息，数据增强过程容易模糊这些信号边界

Method: 提出Divide and Refine两阶段框架：Divide阶段将每个模态分解为独有、成对冗余、协同三部分；Refine阶段设计特定目标优化各部分信息量，保持其区分性

Result: 在IEMOCAP和MELD数据集上，与多种MERC骨干网络结合均取得一致性能提升

Conclusion: 显式分解、优化和重组多模态表示是提升情感识别的有效策略，DnR框架具有即插即用兼容性

Abstract: Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \emph{unique}, \emph{redundant}, and \emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \emph{\textbf{D}ivide and \textbf{R}efine} (\textbf{DnR}). In the \textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026

</details>


### [16] [Quality or Quantity? Error-Informed Selective Online Learning with Gaussian Processes in Multi-Agent Systems: Extended Version](https://arxiv.org/abs/2601.14275)
*Zewen Yang,Xiaobing Dai,Jiajun Cheng,Yulong Huang,Peng Shi*

Main category: cs.LG

TL;DR: 本文提出首个分布式高斯过程回归的选择性在线学习框架EIGP，通过质量优先的选择机制提升合作学习效果，并开发了加速预测的gEIGP和提高精度的aEIGP算法。


<details>
  <summary>Details</summary>
Motivation: 在多智能体分布式学习中，盲目包含所有模型进行联合预测是不合理的，需要优先考虑模型质量而非数量。现有方法缺乏对协作模型质量的智能选择机制。

Method: 提出分布式误差感知高斯过程（EIGP）框架，每个智能体使用选择函数评估邻居协作模型，选择预测误差较小的高质量GP模型。包含贪心算法gEIGP加速预测，自适应算法aEIGP提高精度，以及快速预测、模型更新、误差量化项迭代和数据删除策略实现实时学习。

Result: 数值模拟验证了所提方法的有效性，展示了其相对于现有最先进分布式GP方法在不同基准测试中的优越性。

Conclusion: 在分布式学习中，质量优先于数量的选择性合作策略至关重要。EIGP框架通过智能模型选择机制实现了更有效的分布式高斯过程回归，为实时学习操作提供了实用解决方案。

Abstract: Effective cooperation is pivotal in distributed learning for multi-agent systems, where the interplay between the quantity and quality of the machine learning models is crucial. This paper reveals the irrationality of indiscriminate inclusion of all models on agents for joint prediction, highlighting the imperative to prioritize quality over quantity in cooperative learning. Specifically, we present the first selective online learning framework for distributed Gaussian process (GP) regression, namely distributed error-informed GP (EIGP), that enables each agent to assess its neighboring collaborators, using the proposed selection function to choose the higher quality GP models with less prediction errors. Moreover, algorithmic enhancements are embedded within the EIGP, including a greedy algorithm (gEIGP) for accelerating prediction and an adaptive algorithm (aEIGP) for improving prediction accuracy. In addition, approaches for fast prediction and model update are introduced in conjunction with the error-informed quantification term iteration and a data deletion strategy to achieve real-time learning operations. Numerical simulations are performed to demonstrate the effectiveness of the developed methodology, showcasing its superiority over the state-of-the-art distributed GP methods with different benchmarks.

</details>


### [17] [Which Quantization Should I Use? A Unified Evaluation of llama.cpp Quantization on Llama-3.1-8B-Instruct](https://arxiv.org/abs/2601.14277)
*Uygar Kurt*

Main category: cs.LG

TL;DR: 对llama.cpp量化格式的统一实证研究，评估Llama-3.1-8B-Instruct模型在3-8位K-quant和传统格式下的性能表现，为实际应用提供量化方案选择指南。


<details>
  <summary>Details</summary>
Motivation: 量化技术能降低大语言模型的部署门槛，但现有量化格式评估不一致，用户难以选择合适的量化方案。特别是在llama.cpp中，缺乏统一的量化格式性能比较研究。

Method: 使用Llama-3.1-8B-Instruct模型，系统评估3-8位K-quant和传统量化格式。通过标准推理、知识、指令遵循和真实性基准测试下游任务性能，同时测量困惑度、CPU吞吐量、模型大小、压缩率和量化时间。

Result: 提供了不同量化格式在性能、效率和资源消耗方面的全面比较数据，揭示了精度与效率之间的权衡关系，为不同使用场景下的量化选择提供了实证依据。

Conclusion: 本研究为llama.cpp量化方案选择提供了实用的指导框架，帮助用户根据具体使用场景和资源预算做出明智的、上下文感知的决策。

Abstract: Quantization is a practical technique for making large language models easier to deploy by reducing the precision used to store and operate on model weights. This can lower memory use and improve runtime feasibility on constrained hardware, which is especially relevant for users running models locally. Quantization in llama.cpp enables large language models to run on commodity hardware, but available formats are often evaluated inconsistently, making it hard to choose among schemes. We present a unified empirical study of the llama.cpp quantization on a single modern model, Llama-3.1-8B-Instruct (FP16, GGUF), covering 3-8 bit K-quant and legacy formats. We evaluate downstream task performance across standard reasoning, knowledge, instruction-following, and truthfulness benchmarks, and also measure perplexity and CPU throughput (prefill/decoding) alongside model size, compression, and quantization time. Ultimately, this work is a practical guide for choosing a llama.cpp quantization scheme, helping readers make informed, context-aware decisions for their intended use and resource budget.

</details>


### [18] [On the Limits of Learned Importance Scoring for KV Cache Compression](https://arxiv.org/abs/2601.14279)
*Brady Steele*

Main category: cs.LG

TL;DR: 学习型KV缓存压缩方法SIP未能超越简单基线，位置启发式方法表现更优


<details>
  <summary>Details</summary>
Motivation: 研究通过学习KV表示来预测token重要性，实现KV缓存压缩，以提升推理效率

Method: 提出Speculative Importance Prediction (SIP)，一个170万参数的非查询感知评分器，使用多视野前瞻和交叉注意力架构，从KV表示预测token重要性

Result: SIP在5个种子、4个保留水平、3个任务上未能超越简单基线（包括随机选择）。位置启发式方法（保留前4+后N个token）表现最佳

Conclusion: KV表示中超出位置和预填充注意力的边际信息有限，未来查询与生成轨迹的循环依赖增加了重要性预测的难度

Abstract: We investigate learned KV cache compression through Speculative Importance Prediction (SIP), a 1.7M parameter non-query-aware scorer that predicts token importance from KV representations alone. Despite architectural sophistication (multi-horizon lookahead, cross-attention), SIP does not outperform simple baselines, including random selection, across 5 seeds, 4 retention levels, and 3 tasks. Key findings: (1) position-based heuristics (keep first 4 + last N tokens) match or exceed learned approaches; (2) prefill attention provides equivalent signal to complex learned scorers; (3) marginal information in KV representations beyond position and prefill attention appears limited for importance prediction. We hypothesize that circular dependence between future queries and generation trajectories contributes to this difficulty.

</details>


### [19] [Beyond Affinity: A Benchmark of 1D, 2D, and 3D Methods Reveals Critical Trade-offs in Structure-Based Drug Design](https://arxiv.org/abs/2601.14283)
*Kangyu Zheng,Kai Zhang,Jiale Tan,Xuehan Chen,Yingzhou Lu,Zaixi Zhang,Lichao Sun,Marinka Zitnik,Tianfan Fu,Zhiding Liang*

Main category: cs.LG

TL;DR: 本文建立了首个跨算法类别的结构药物设计基准，评估了15种不同算法模型在药物性质、对接亲和力和构象方面的表现，揭示了各类算法的独特优势和局限性。


<details>
  <summary>Details</summary>
Motivation: 目前结构药物设计领域主要存在搜索算法、深度生成模型和强化学习三类算法，但现有研究通常只比较同一算法类别内的模型，缺乏跨算法类别的系统比较。本文旨在填补这一空白，建立统一的评估基准。

Method: 建立了一个基准测试框架，评估15种不同算法基础的模型，通过分析生成分子的药物性质、与特定靶蛋白的对接亲和力和构象质量来进行跨算法比较。特别强调将对接函数作为黑盒预言机，将1D/2D配体中心方法应用于SBDD。

Result: 评估揭示了不同模型类别的明显模式：3D结构模型在结合亲和力方面表现优异，但在化学有效性和构象质量方面存在不一致；1D模型在标准分子指标上表现可靠，但很少达到最佳结合亲和力；2D模型提供平衡性能，保持高化学有效性的同时获得中等结合分数。

Conclusion: 通过多蛋白靶点的详细分析，为每类模型确定了关键改进领域，为研究人员提供了结合不同方法优势、解决其局限性的见解。强调未来SBDD模型设计应考虑跨算法整合，所有基准测试代码已开源。

Abstract: Currently, the field of structure-based drug design is dominated by three main types of algorithms: search-based algorithms, deep generative models, and reinforcement learning. While existing works have typically focused on comparing models within a single algorithmic category, cross-algorithm comparisons remain scarce. In this paper, to fill the gap, we establish a benchmark to evaluate the performance of fifteen models across these different algorithmic foundations by assessing the pharmaceutical properties of the generated molecules and their docking affinities and poses with specified target proteins. We highlight the unique advantages of each algorithmic approach and offer recommendations for the design of future SBDD models. We emphasize that 1D/2D ligand-centric drug design methods can be used in SBDD by treating the docking function as a black-box oracle, which is typically neglected. Our evaluation reveals distinct patterns across model categories. 3D structure-based models excel in binding affinities but show inconsistencies in chemical validity and pose quality. 1D models demonstrate reliable performance in standard molecular metrics but rarely achieve optimal binding affinities. 2D models offer balanced performance, maintaining high chemical validity while achieving moderate binding scores. Through detailed analysis across multiple protein targets, we identify key improvement areas for each model category, providing insights for researchers to combine strengths of different approaches while addressing their limitations. All the code that are used for benchmarking is available in https://github.com/zkysfls/2025-sbdd-benchmark

</details>


### [20] [engGNN: A Dual-Graph Neural Network for Omics-Based Disease Classification and Feature Selection](https://arxiv.org/abs/2601.14536)
*Tiantian Yang,Yuxuan Wang,Zhenwei Zhou,Ching-Ti Liu*

Main category: cs.LG

TL;DR: engGNN提出了一种双图神经网络框架，同时利用外部已知生物网络和数据驱动生成的图，用于高维组学数据的疾病分类和生物标志物发现。


<details>
  <summary>Details</summary>
Motivation: 组学数据（如转录组学、蛋白质组学、代谢组学）虽然能提供疾病机制和临床结果的重要见解，但其高维度、小样本量和复杂生物网络给可靠预测和有意义解释带来了挑战。现有图神经网络方法通常只依赖外部策划的特征图或数据驱动生成的图，限制了捕捉互补信息的能力。

Method: engGNN采用双图框架：1）从已建立的网络数据库构建生物信息无向特征图；2）从树集成模型推导出有向特征图。这种双图设计产生更全面的嵌入表示，从而提高预测性能和可解释性。

Result: 通过广泛的模拟和基因表达数据的实际应用，engGNN在性能上持续优于最先进的基线方法。除了分类任务，engGNN还提供可解释的特征重要性评分，促进生物学意义的发现，如通路富集分析。

Conclusion: engGNN是一个稳健、灵活且可解释的框架，适用于高维组学背景下的疾病分类和生物标志物发现，通过整合外部知识和数据驱动信息来克服现有方法的局限性。

Abstract: Omics data, such as transcriptomics, proteomics, and metabolomics, provide critical insights into disease mechanisms and clinical outcomes. However, their high dimensionality, small sample sizes, and intricate biological networks pose major challenges for reliable prediction and meaningful interpretation. Graph Neural Networks (GNNs) offer a promising way to integrate prior knowledge by encoding feature relationships as graphs. Yet, existing methods typically rely solely on either an externally curated feature graph or a data-driven generated one, which limits their ability to capture complementary information. To address this, we propose the external and generated Graph Neural Network (engGNN), a dual-graph framework that jointly leverages both external known biological networks and data-driven generated graphs. Specifically, engGNN constructs a biologically informed undirected feature graph from established network databases and complements it with a directed feature graph derived from tree-ensemble models. This dual-graph design produces more comprehensive embeddings, thereby improving predictive performance and interpretability. Through extensive simulations and real-world applications to gene expression data, engGNN consistently outperforms state-of-the-art baselines. Beyond classification, engGNN provides interpretable feature importance scores that facilitate biologically meaningful discoveries, such as pathway enrichment analysis. Taken together, these results highlight engGNN as a robust, flexible, and interpretable framework for disease classification and biomarker discovery in high-dimensional omics contexts.

</details>


### [21] [A Comparison of Polynomial-Based Tree Clustering Methods](https://arxiv.org/abs/2601.14285)
*Pengyu Liu,Mariel Vázquez,Nataša Jonoska*

Main category: cs.LG

TL;DR: 比较不同距离度量在基于树多项式聚类方法中的性能，发现基于条目级归一化距离的方法具有最高的聚类准确率


<details>
  <summary>Details</summary>
Motivation: 生命科学中树结构数据（如RNA二级结构、系统发育树）日益增多，需要新的树结构数据分析方法。树多项式提供了一种高效、可解释的树结构编码方式，但需要评估不同距离度量在聚类中的性能。

Method: 1. 使用树区分多项式编码树结构；2. 比较不同距离度量（包括Canberra距离）在树聚类方法中的性能；3. 实现两种基本的自编码器模型用于树聚类

Result: 基于条目级归一化距离的方法在所有比较方法中具有最高的聚类准确率

Conclusion: 对于基于树多项式的树结构聚类，条目级归一化距离度量是最有效的选择，为生命科学中的树结构数据分析提供了实用指导

Abstract: Tree structures appear in many fields of the life sciences, including phylogenetics, developmental biology and nucleic acid structures. Trees can be used to represent RNA secondary structures, which directly relate to the function of non-coding RNAs. Recent developments in sequencing technology and artificial intelligence have yielded numerous biological data that can be represented with tree structures. This requires novel methods for tree structure data analytics. Tree polynomials provide a computationally efficient, interpretable and comprehensive way to encode tree structures as matrices, which are compatible with most data analytics tools. Machine learning methods based on the Canberra distance between tree polynomials have been introduced to analyze phylogenies and nucleic acid structures. In this paper, we compare the performance of different distances in tree clustering methods based on a tree distinguishing polynomial. We also implement two basic autoencoder models for clustering trees using the polynomial. We find that the distance based methods with entry-level normalized distances have the highest clustering accuracy among the compared methods.

</details>


### [22] [Factorizable joint shift revisited](https://arxiv.org/abs/2601.15036)
*Dirk Tasche*

Main category: cs.LG

TL;DR: 提出一个分析一般标签空间分布偏移的框架，将因子化联合偏移（FJS）推广到回归和分类问题，并扩展EM算法用于类别先验概率估计


<details>
  <summary>Details</summary>
Motivation: 现有FJS研究局限于分类标签空间，需要扩展到一般标签空间以涵盖回归和分类问题，从而更全面地分析分布偏移

Method: 构建一般标签空间的分布偏移分析框架，将FJS理论推广到一般标签空间，扩展EM算法用于类别先验概率估计，重新审视广义标签偏移（GLS）

Result: 建立了适用于一般标签空间的分布偏移分析框架，成功将FJS理论从分类扩展到回归问题，提出了相应的EM算法扩展

Conclusion: 提出的框架统一了分类和回归中的分布偏移分析，为处理一般标签空间的因子化联合偏移提供了理论基础和实用算法

Abstract: Factorizable joint shift (FJS) was proposed as a type of distribution shift (or dataset shift) that comprises both covariate and label shift. Recently, it has been observed that FJS actually arises from consecutive label and covariate (or vice versa) shifts. Research into FJS so far has been confined to the case of categorical label spaces. We propose a framework for analysing distribution shift in the case of general label spaces, thus covering both classification and regression models. Based on the framework, we generalise existing results on FJS to general label spaces and propose a related extension of the expectation maximisation (EM) algorithm for class prior probabilities. We also take a fresh look at generalized label shift (GLS) in the case of general label spaces.

</details>


### [23] [Chain-of-Memory: Lightweight Memory Construction with Dynamic Evolution for LLM Agents](https://arxiv.org/abs/2601.14287)
*Xiucheng Xu,Bingbing Xu,Xueyun Tian,Zihe Huang,Rongxin Chen,Yunfan Li,Huawei Shen*

Main category: cs.LG

TL;DR: CoM框架提出轻量级内存构建与复杂利用的新范式，通过Chain-of-Memory机制组织检索片段形成连贯推理路径，显著提升性能同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有外部内存系统存在两个根本问题：复杂的内存构建成本高但性能提升有限；简单的上下文拼接无法弥合检索召回与推理准确性之间的差距。

Method: 提出CoM框架，采用轻量级构建与复杂利用相结合的新范式，引入Chain-of-Memory机制，通过动态演化将检索片段组织成连贯推理路径，并使用自适应截断来修剪无关噪声。

Result: 在LongMemEval和LoCoMo基准测试中，CoM相比强基线获得7.5%-10.4%的准确率提升，同时将计算开销大幅降低至复杂内存架构的约2.7%令牌消耗和6.0%延迟。

Conclusion: CoM通过轻量级构建与复杂利用的新范式，有效解决了现有外部内存系统的局限性，在提升LLM代理性能的同时显著降低了计算成本。

Abstract: External memory systems are pivotal for enabling Large Language Model (LLM) agents to maintain persistent knowledge and perform long-horizon decision-making. Existing paradigms typically follow a two-stage process: computationally expensive memory construction (e.g., structuring data into graphs) followed by naive retrieval-augmented generation. However, our empirical analysis reveals two fundamental limitations: complex construction incurs high costs with marginal performance gains, and simple context concatenation fails to bridge the gap between retrieval recall and reasoning accuracy. To address these challenges, we propose CoM (Chain-of-Memory), a novel framework that advocates for a paradigm shift toward lightweight construction paired with sophisticated utilization. CoM introduces a Chain-of-Memory mechanism that organizes retrieved fragments into coherent inference paths through dynamic evolution, utilizing adaptive truncation to prune irrelevant noise. Extensive experiments on the LongMemEval and LoCoMo benchmarks demonstrate that CoM outperforms strong baselines with accuracy gains of 7.5%-10.4%, while drastically reducing computational overhead to approximately 2.7% of token consumption and 6.0% of latency compared to complex memory architectures.

</details>


### [24] [Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity](https://arxiv.org/abs/2601.14300)
*Jun Liu,Leo Yu Zhang,Fengpeng Li,Isao Echizen,Jiantao Zhou*

Main category: cs.LG

TL;DR: 该论文提出了一种新的硬标签黑盒攻击框架，通过零查询频域初始化和模式驱动优化策略，显著提高了攻击成功率和查询效率，并能绕过最先进的防御系统。


<details>
  <summary>Details</summary>
Motivation: 硬标签黑盒设置（仅能观察到top-1预测标签）是一个重要但受限的反馈模型。核心挑战是从这种离散响应中恢复有意义的梯度信息。现有攻击方法缺乏统一的理论解释，需要更高效、更通用的攻击框架。

Method: 提出了一个统一的理论视角，将现有硬标签攻击解释为隐式近似真实损失梯度的符号。基于此，提出了新攻击框架：1）零查询频域初始化，在温和假设下获得比随机基线更高的期望余弦相似度；2）模式驱动优化（PDO）策略，显著降低查询复杂度。

Result: 在CIFAR-10、ImageNet、ObjectNet等数据集上，涵盖标准模型、对抗训练模型、商业API和CLIP模型，新方法在攻击成功率和查询效率上均超越现有SOTA硬标签攻击，尤其在低查询场景。还能有效泛化到损坏数据、生物医学数据集和密集预测任务，并能完全绕过Blacklight防御（0%检测率）。

Conclusion: 该工作为硬标签攻击提供了统一的理论框架，提出的新方法在多个基准测试中表现出优越性能，展示了在极端受限反馈下恢复梯度信息的可行性，对模型安全评估具有重要意义。

Abstract: Hard-label black-box settings, where only top-1 predicted labels are observable, pose a fundamentally constrained yet practically important feedback model for understanding model behavior. A central challenge in this regime is whether meaningful gradient information can be recovered from such discrete responses. In this work, we develop a unified theoretical perspective showing that a wide range of existing sign-flipping hard-label attacks can be interpreted as implicitly approximating the sign of the true loss gradient. This observation reframes hard-label attacks from heuristic search procedures into instances of gradient sign recovery under extremely limited feedback. Motivated by this first-principles understanding, we propose a new attack framework that combines a zero-query frequency-domain initialization with a Pattern-Driven Optimization (PDO) strategy. We establish theoretical guarantees demonstrating that, under mild assumptions, our initialization achieves higher expected cosine similarity to the true gradient sign compared to random baselines, while the proposed PDO procedure attains substantially lower query complexity than existing structured search approaches. We empirically validate our framework through extensive experiments on CIFAR-10, ImageNet, and ObjectNet, covering standard and adversarially trained models, commercial APIs, and CLIP-based models. The results show that our method consistently surpasses SOTA hard-label attacks in both attack success rate and query efficiency, particularly in low-query regimes. Beyond image classification, our approach generalizes effectively to corrupted data, biomedical datasets, and dense prediction tasks. Notably, it also successfully circumvents Blacklight, a SOTA stateful defense, resulting in a $0\%$ detection rate. Our code will be released publicly soon at https://github.com/csjunjun/DPAttack.git.

</details>


### [25] [Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2601.14327)
*YuanLab. ai,Shawn Wu,Jiangang Luo,Tong Yu,Darcy Chen,Sean Wang,Xudong Zhao,Louie Li,Claire Wang,Hunter He,Carol Wang,Allen Wang*

Main category: cs.LG

TL;DR: 提出LAEP算法，在MoE LLMs预训练阶段自适应剪枝专家，提升训练效率并减少参数


<details>
  <summary>Details</summary>
Motivation: MoE LLMs虽然能减少激活参数提升精度，但预训练阶段存在专家利用率低、训练效率受限的问题

Method: 提出层自适应专家剪枝算法，在预训练阶段根据token分布统计选择性剪枝未充分利用的专家，并跨计算设备重新组织专家

Result: LAEP能有效减少模型大小并显著提升预训练效率。在从头预训练1010B Base模型时，训练效率提升48.3%，参数减少33.3%，同时在多个领域保持优异性能

Conclusion: LAEP算法为MoE LLMs的预训练提供了有效的效率优化方案，解决了专家利用率低的问题

Abstract: Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.

</details>


### [26] [Hierarchical Contextual Uplift Bandits for Catalog Personalization](https://arxiv.org/abs/2601.14333)
*Anupam Agrawal,Rajesh Mohanty,Shamik Bhattacharjee,Abhimanyu Mittal*

Main category: cs.LG

TL;DR: 提出分层上下文提升赌博机框架，通过动态调整上下文粒度解决幻想体育平台推荐中的冷启动和动态环境问题，实现显著收入提升。


<details>
  <summary>Details</summary>
Motivation: 传统上下文赌博机算法在幻想体育等动态环境中表现不佳，用户行为快速变化和外部因素导致的奖励分布剧烈波动需要频繁重新训练，存在冷启动问题。

Method: 提出分层上下文提升赌博机框架，动态调整上下文粒度（从系统级到用户级），利用上下文相似性促进策略迁移，并整合提升建模原则。

Result: 在Dream11幻想体育平台的大规模A/B测试中，推荐质量显著提升，收入提高0.4%，用户满意度指标改善；2025年5月部署生产后进一步实现0.5%收入提升。

Conclusion: 分层上下文提升赌博机框架有效解决了动态环境中的推荐挑战，在幻想体育平台实现了显著的业务价值提升，已成功部署为默认目录个性化系统。

Abstract: Contextual Bandit (CB) algorithms are widely adopted for personalized recommendations but often struggle in dynamic environments typical of fantasy sports, where rapid changes in user behavior and dramatic shifts in reward distributions due to external influences necessitate frequent retraining. To address these challenges, we propose a Hierarchical Contextual Uplift Bandit framework. Our framework dynamically adjusts contextual granularity from broad, system-wide insights to detailed, user-specific contexts, using contextual similarity to facilitate effective policy transfer and mitigate cold-start issues. Additionally, we integrate uplift modeling principles into our approach. Results from large-scale A/B testing on the Dream11 fantasy sports platform show that our method significantly enhances recommendation quality, achieving a 0.4% revenue improvement while also improving user satisfaction metrics compared to the current production system. We subsequently deployed this system to production as the default catalog personalization system in May 2025 and observed a further 0.5% revenue improvement.

</details>


### [27] [Communication-Efficient Multi-Modal Edge Inference via Uncertainty-Aware Distributed Learning](https://arxiv.org/abs/2601.14942)
*Hang Zhao,Hongru Li,Dongfang Xu,Shenghui Song,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出三阶段通信感知分布式学习框架，用于多模态边缘推理，通过自监督学习、分布式微调和不确定性反馈机制，在减少通信开销的同时保持无线信道下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态边缘推理面临两大挑战：1) 多模态特性导致分布式学习在带宽有限无线链路上的通信开销过大；2) 在变化信道和噪声多模态输入下鲁棒性有限。需要一种既能提高训练推理效率又能保持鲁棒性的通信感知解决方案。

Method: 三阶段框架：阶段I - 设备本地多模态自监督学习，无需设备-服务器交换，获得共享和模态特定编码器；阶段II - 分布式微调与集中证据融合，校准每模态不确定性并可靠聚合噪声或信道衰落扭曲的特征；阶段III - 不确定性引导反馈机制，为不确定样本选择性请求额外特征，优化分布式设置中的通信-精度权衡。

Result: 在RGB-深度室内场景分类实验中，该框架以更少的训练通信轮次获得更高精度，对模态退化或信道变化保持鲁棒性，优于现有自监督和完全监督基线方法。

Conclusion: 提出的三阶段通信感知分布式学习框架有效解决了多模态边缘推理中的通信效率和鲁棒性问题，通过自监督学习减少通信开销，通过证据融合和不确定性反馈机制增强鲁棒性，为分布式边缘智能提供了实用解决方案。

Abstract: Semantic communication is emerging as a key enabler for distributed edge intelligence due to its capability to convey task-relevant meaning. However, achieving communication-efficient training and robust inference over wireless links remains challenging. This challenge is further exacerbated for multi-modal edge inference (MMEI) by two factors: 1) prohibitive communication overhead for distributed learning over bandwidth-limited wireless links, due to the \emph{multi-modal} nature of the system; and 2) limited robustness under varying channels and noisy multi-modal inputs. In this paper, we propose a three-stage communication-aware distributed learning framework to improve training and inference efficiency while maintaining robustness over wireless channels. In Stage~I, devices perform local multi-modal self-supervised learning to obtain shared and modality-specific encoders without device--server exchange, thereby reducing the communication cost. In Stage~II, distributed fine-tuning with centralized evidential fusion calibrates per-modality uncertainty and reliably aggregates features distorted by noise or channel fading. In Stage~III, an uncertainty-guided feedback mechanism selectively requests additional features for uncertain samples, optimizing the communication--accuracy tradeoff in the distributed setting. Experiments on RGB--depth indoor scene classification show that the proposed framework attains higher accuracy with far fewer training communication rounds and remains robust to modality degradation or channel variation, outperforming existing self-supervised and fully supervised baselines.

</details>


### [28] [Log anomaly detection via Meta Learning and Prototypical Networks for Cross domain generalization](https://arxiv.org/abs/2601.14336)
*Krishna Sharma,Vivek Yelleti*

Main category: cs.LG

TL;DR: 提出基于元学习的端到端框架，通过动态漂移标注和语义模糊匹配解决日志异常检测中的类别不平衡和跨域适应问题，在HDFS和Linux等跨域场景中取得最佳F1分数。


<details>
  <summary>Details</summary>
Motivation: 日志异常检测面临类别不平衡和跨域适应两大挑战：源域训练的模型无法直接应用于目标域，且目标域缺乏标注异常数据。传统方法难以处理数据漂移和跨域泛化问题。

Method: 1) 使用Drain3日志解析和动态漂移标注技术，通过语义模糊匹配将源域异常知识迁移到目标域；2) 提取BERT语义嵌入并进行特征选择降维；3) 采用MAML和原型网络进行元学习快速适应；4) 使用SMOTE处理数据不平衡。

Result: 采用留一源域方法评估，提出的元学习驱动方法在所有跨域设置中获得了最高的平均F1分数，验证了其在跨域日志异常检测中的有效性。

Conclusion: 基于元学习的框架成功解决了日志异常检测中的跨域适应和类别不平衡问题，通过动态漂移标注和快速适应机制，在HDFS和Linux等不同系统间实现了有效的异常检测迁移。

Abstract: Log anomaly detection is essential for system reliability, but it is extremely challenging to do considering it involves class imbalance. Additionally, the models trained in one domain are not applicable to other domains, necessitating the need for cross-domain adaptation (such as HDFS and Linux). Traditional detection models often fail to generalize due to significant data drift and the inherent absence of labeled anomalies in new target domains. To handle the above challenges, we proposed a new end-to-end framework based on a meta-learning approach. Our methodology first gets the data ready by combining a Drain3 log parsing mechanism with a dynamic drift-based labeling technique that uses semantic and fuzzy matching to move existing anomaly knowledge from one source to another. BERT-based semantic embeddings are obtained, and the feature selection is invoked to reduce the dimensionality. Later, Model Agnostic Meta-Learning (MAML) and Prototypical Networks models are trained to adapt quickly and effectively. The SMOTE oversampling method is employed to handle imbalances in the data. All the results are obtained by employing the leave-one-out source method, and the corresponding mean F1 scores are reported. Our empirical findings validate that the proposed meta-learning-driven approach yielded the highest mean F1 score and proved to be effective for cross-domain settings.

</details>


### [29] [DiSPA: Differential Substructure-Pathway Attention for Drug Response Prediction](https://arxiv.org/abs/2601.14346)
*Yewon Han,Sunghyun Kim,Eunyi Jeong,Sungkyung Lee,Seokwoo Yun,Sangsoo Lim*

Main category: cs.LG

TL;DR: DiSPA是一个药物反应预测框架，通过化学亚结构与通路表达的双向条件化来解耦结构驱动和上下文驱动的药物作用机制，在GDSC基准测试中达到SOTA性能，并能实现零样本迁移到空间转录组学。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型通常独立处理化学和转录组模态或在后期才结合它们，限制了捕捉精细的、上下文依赖的药物作用机制的能力。标准注意力机制对高维生物网络中的噪声和稀疏性敏感，阻碍了泛化性和可解释性。

Method: DiSPA通过化学亚结构与通路水平基因表达的双向条件化，明确解耦结构驱动和上下文驱动的药物反应机制。引入差分交叉注意力模块，抑制虚假的通路-亚结构关联，同时放大上下文相关的相互作用。

Result: 在GDSC基准测试的多个评估设置中，DiSPA实现了最先进的性能，特别是在评估未见药物-细胞组合泛化性的不相交集设置中表现出显著改进。学习到的注意力模式能够恢复已知的药效团，区分结构驱动和上下文依赖的化合物，并在生物通路中表现出连贯的组织结构。

Conclusion: DiSPA作为一个稳健且可解释的整合药物基因组学建模框架，能够超越事后解释，实现对药物反应机制的原则性分析。仅使用批量RNA-seq数据训练的DiSPA能够零样本迁移到空间转录组学，揭示区域特异性的药物敏感性模式。

Abstract: Accurate prediction of drug response in precision medicine requires models that capture how specific chemical substructures interact with cellular pathway states. However, most existing deep learning approaches treat chemical and transcriptomic modalities independently or combine them only at late stages, limiting their ability to model fine-grained, context-dependent mechanisms of drug action. In addition, standard attention mechanisms are often sensitive to noise and sparsity in high-dimensional biological networks, hindering both generalization and interpretability. We present DiSPA, a representation learning framework that explicitly disentangles structure-driven and context-driven mechanisms of drug response through bidirectional conditioning between chemical substructures and pathway-level gene expression. DiSPA introduces a differential cross-attention module that suppresses spurious pathway-substructure associations while amplifying contextually relevant interactions. Across multiple evaluation settings on the GDSC benchmark, DiSPA achieves state-of-the-art performance, with particularly strong improvements in the disjoint-set setting, which assesses generalization to unseen drug-cell combinations. Beyond predictive accuracy, DiSPA yields mechanistically informative representations: learned attention patterns recover known pharmacophores, distinguish structure-driven from context-dependent compounds, and exhibit coherent organization across biological pathways. Furthermore, we demonstrate that DiSPA trained solely on bulk RNA-seq data enables zero-shot transfer to spatial transcriptomics, revealing region-specific drug sensitivity patterns without retraining. Together, these results establish DiSPA as a robust and interpretable framework for integrative pharmacogenomic modeling, enabling principled analysis of drug response mechanisms beyond post hoc interpretation.

</details>


### [30] [VJEPA: Variational Joint Embedding Predictive Architectures as Probabilistic World Models](https://arxiv.org/abs/2601.14354)
*Yongchao Huang*

Main category: cs.LG

TL;DR: VJEPA是JEPA的概率泛化版本，通过变分目标学习未来潜在状态的预测分布，统一了表示学习与预测状态表示，无需自回归观测似然，为高维噪声环境中的不确定性感知规划提供基础框架。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法依赖确定性回归目标，掩盖了概率语义，限制了其在随机控制中的应用。需要一种概率泛化方法，能够处理不确定性并在高维噪声环境中进行鲁棒规划。

Method: 提出VJEPA（变分JEPA），通过变分目标学习未来潜在状态的预测分布。进一步提出BJEPA（贝叶斯JEPA），将预测信念分解为学习到的动态专家和模块化先验专家，通过专家乘积实现零样本任务迁移和约束满足。

Result: 理论证明VJEPA表示可以作为最优控制的充分信息状态，无需像素重建，并提供避免表示崩溃的形式保证。实验显示VJEPA和BJEPA能成功过滤高方差干扰因素，而生成基线方法会出现表示崩溃。

Conclusion: VJEPA提供了一个基础框架，支持原则性不确定性估计（如通过采样构建可信区间），同时保持对观测的似然无关性，为高维噪声环境中的可扩展、鲁棒、不确定性感知规划奠定基础。

Abstract: Joint Embedding Predictive Architectures (JEPA) offer a scalable paradigm for self-supervised learning by predicting latent representations rather than reconstructing high-entropy observations. However, existing formulations rely on \textit{deterministic} regression objectives, which mask probabilistic semantics and limit its applicability in stochastic control. In this work, we introduce \emph{Variational JEPA (VJEPA)}, a \textit{probabilistic} generalization that learns a predictive distribution over future latent states via a variational objective. We show that VJEPA unifies representation learning with Predictive State Representations (PSRs) and Bayesian filtering, establishing that sequential modeling does not require autoregressive observation likelihoods. Theoretically, we prove that VJEPA representations can serve as sufficient information states for optimal control without pixel reconstruction, while providing formal guarantees for collapse avoidance. We further propose \emph{Bayesian JEPA (BJEPA)}, an extension that factorizes the predictive belief into a learned dynamics expert and a modular prior expert, enabling zero-shot task transfer and constraint (e.g. goal, physics) satisfaction via a Product of Experts. Empirically, through a noisy environment experiment, we demonstrate that VJEPA and BJEPA successfully filter out high-variance nuisance distractors that cause representation collapse in generative baselines. By enabling principled uncertainty estimation (e.g. constructing credible intervals via sampling) while remaining likelihood-free regarding observations, VJEPA provides a foundational framework for scalable, robust, uncertainty-aware planning in high-dimensional, noisy environments.

</details>


### [31] [Adaptive KDE for Real-Time Thresholding: Prioritized Queues for Financial Crime Investigation](https://arxiv.org/abs/2601.14473)
*Danny Butvinik,Nana Boateng,Achi Hackmon*

Main category: cs.LG

TL;DR: 提出一种在线自适应核密度估计方法，将风险评分流转换为审查队列，无需标签且支持多队列路由，在满足容量约束的同时减少阈值抖动。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用top-K或手动调整阈值来将风险评分流转换为审查队列，但这些方法在动态流数据中难以保持稳定的容量控制，且阈值容易产生抖动。

Method: 1) 对评分流拟合在线自适应核密度估计；2) 将密度转换为尾质量曲线以满足容量约束；3) 将得到的切割点"捕捉"到跨带宽检测到的持久密度谷值；4) 支持滑动窗口或指数遗忘的实时操作。

Result: 在合成、漂移、多模态流数据上，该方法在保持竞争性容量依从性的同时，显著减少了阈值抖动。每个事件的更新成本为O(G)，每个活动需要常数内存。

Conclusion: 该方法提供了一种无标签、支持多队列路由的实时解决方案，能够在动态流环境中有效管理审查队列，平衡容量约束和阈值稳定性。

Abstract: We study the problem of converting a stream of risk scores into one or more review queues under explicit intake constraints[cite: 6]. Instead of top-$K$ or manually tuned cutoffs, we fit an online adaptive kernel density to the score stream, transform the density into a tail-mass curve to meet capacity, and ``snap'' the resulting cut to a persistent density valley detected across bandwidths[cite: 7]. The procedure is label-free, supports multi-queue routing, and operates in real time with sliding windows or exponential forgetting[cite: 8]. On synthetic, drifting, multimodal streams, the method achieves competitive capacity adherence while reducing threshold jitter[cite: 9]. Updates cost $O(G)$ per event with constant memory per activity

</details>


### [32] [GPU-accelerated simulated annealing based on p-bits with real-world device-variability modeling](https://arxiv.org/abs/2601.14476)
*Naoya Onizawa,Takahiro Hanyu*

Main category: cs.LG

TL;DR: 该研究发现p-bit器件变异性不仅能降低计算性能，还能通过时序变异性增强算法性能，并开发了GPU加速的开源模拟退火框架，在MAX-CUT问题上实现两个数量级加速。


<details>
  <summary>Details</summary>
Motivation: 基于磁隧道结等新兴器件的p-bit实现存在器件变异性问题，传统观点认为这会负面影响计算性能，但需要深入探究变异性对算法性能的实际影响。

Method: 开发了基于p-bit的GPU加速开源模拟退火框架，通过CUDA模拟建模时序、强度和偏移三种关键器件变异性因素，反映真实器件行为。

Result: 在800到20,000节点的MAX-CUT基准测试中，GPU实现相比CPU实现获得两个数量级的速度提升，同时发现器件变异性特别是时序变性能增强算法性能。

Conclusion: 器件变异性在概率计算中具有双重作用，既能降低也能提升性能，该框架为概率计算研究提供了可扩展的开放工具，促进优化应用发展。

Abstract: Probabilistic computing using probabilistic bits (p-bits) presents an efficient alternative to traditional CMOS logic for complex problem-solving, including simulated annealing and machine learning. Realizing p-bits with emerging devices such as magnetic tunnel junctions (MTJs) introduces device variability, which was expected to negatively impact computational performance. However, this study reveals an unexpected finding: device variability can not only degrade but also enhance algorithm performance, particularly by leveraging timing variability. This paper introduces a GPU-accelerated, open-source simulated annealing framework based on p-bits that models key device variability factors -- timing, intensity, and offset -- to reflect real-world device behavior. Through CUDA-based simulations, our approach achieves a two-order magnitude speedup over CPU implementations on the MAX-CUT benchmark with problem sizes ranging from 800 to 20,000 nodes. By providing a scalable and accessible tool, this framework aims to advance research in probabilistic computing, enabling optimization applications in diverse fields.

</details>


### [33] [Stabilizing autoregressive forecasts in chaotic systems via multi-rate latent recurrence](https://arxiv.org/abs/2601.14487)
*Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: MSR-HINE是一种用于混沌动力系统长期预测的分层隐式预测器，通过多尺度潜在先验和多速率循环模块显著提升了预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 混沌动力系统的长期自回归预测面临误差快速放大和分布偏移的挑战：微小的一步误差会累积成物理不一致的推演结果，导致大尺度统计特性崩溃。现有方法难以在长期预测中保持精度。

Method: 提出MSR-HINE分层隐式预测器，结合多尺度潜在先验和多速率循环模块。在每个时间步：1) 粗到细循环状态生成潜在先验；2) 隐式一步预测器通过多尺度潜在注入细化状态；3) 门控融合与后验潜在确保尺度一致性更新；4) 轻量级隐藏状态校正对齐循环记忆与融合潜在。该架构在慢流形上保持长期上下文，同时保留快速尺度变异性。

Result: 在两个基准测试中表现优异：在Kuramoto-Sivashinsky系统上，将端到端RMSE降低62.8%（H=400），ACC从-0.155提升到0.828；将ACC≥0.5的可预测性范围从241步扩展到400步。在Lorenz-96系统上，RMSE降低27.0%（H=100），ACC从0.144提升到0.545；将ACC≥0.5范围从58步扩展到100步。

Conclusion: MSR-HINE通过分层多尺度架构有效缓解了混沌系统中的误差累积问题，显著提升了长期预测的准确性和稳定性，为混沌动力系统的自回归预测提供了有效的解决方案。

Abstract: Long-horizon autoregressive forecasting of chaotic dynamical systems remains challenging due to rapid error amplification and distribution shift: small one-step inaccuracies compound into physically inconsistent rollouts and collapse of large-scale statistics. We introduce MSR-HINE, a hierarchical implicit forecaster that augments multiscale latent priors with multi-rate recurrent modules operating at distinct temporal scales. At each step, coarse-to-fine recurrent states generate latent priors, an implicit one-step predictor refines the state with multiscale latent injections, and a gated fusion with posterior latents enforces scale-consistent updates; a lightweight hidden-state correction further aligns recurrent memories with fused latents. The resulting architecture maintains long-term context on slow manifolds while preserving fast-scale variability, mitigating error accumulation in chaotic rollouts. Across two canonical benchmarks, MSR-HINE yields substantial gains over a U-Net autoregressive baseline: on Kuramoto-Sivashinsky it reduces end-horizon RMSE by 62.8% at H=400 and improves end-horizon ACC by +0.983 (from -0.155 to 0.828), extending the ACC >= 0.5 predictability horizon from 241 to 400 steps; on Lorenz-96 it reduces RMSE by 27.0% at H=100 and improves end horizon ACC by +0.402 (from 0.144 to 0.545), extending the ACC >= 0.5 horizon from 58 to 100 steps.

</details>


### [34] [Learning PDE Solvers with Physics and Data: A Unifying View of Physics-Informed Neural Networks and Neural Operators](https://arxiv.org/abs/2601.14517)
*Yilong Dai,Shengyu Chen,Ziyi Wang,Xiaowei Jia,Yiqun Xie,Vipin Kumar,Runlong Yu*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架来分析物理信息神经网络(PINNs)和神经算子(NOs)这两种主流范式，从三个基本维度组织现有方法：学习内容、物理结构如何融入学习过程、计算负载如何在问题实例间分摊。


<details>
  <summary>Details</summary>
Motivation: 尽管出现了各种物理感知的数据驱动方法，但该领域仍缺乏统一视角来揭示它们之间的关系、局限性以及在科学工作流中的适当角色。需要建立统一框架来理解学习型PDE求解器的结构特性。

Method: 提出一个统一的设计空间，从三个基本维度组织现有方法：1) 学习内容（what is learned）；2) 物理结构如何融入学习过程；3) 计算负载如何在问题实例间分摊（amortization）。通过这个框架分析PINNs和NOs等主流方法。

Result: 建立了一个统一视角，将PINNs和NOs置于共享设计空间中，揭示了这些方法的结构特性及其带来的挑战。通过这个框架可以更好地理解现有方法的优缺点。

Conclusion: 该统一框架有助于促进可靠的学习型PDE求解器的发展，并推动物理与数据的融合。通过分析这些结构特性，可以更好地理解学习PDE的挑战并指导未来研究。

Abstract: Partial differential equations (PDEs) are central to scientific modeling. Modern workflows increasingly rely on learning-based components to support model reuse, inference, and integration across large computational processes. Despite the emergence of various physics-aware data-driven approaches, the field still lacks a unified perspective to uncover their relationships, limitations, and appropriate roles in scientific workflows. To this end, we propose a unifying perspective to place two dominant paradigms: Physics-Informed Neural Networks (PINNs) and Neural Operators (NOs), within a shared design space. We organize existing methods from three fundamental dimensions: what is learned, how physical structures are integrated into the learning process, and how the computational load is amortized across problem instances. In this way, many challenges can be best understood as consequences of these structural properties of learning PDEs. By analyzing advances through this unifying view, our survey aims to facilitate the development of reliable learning-based PDE solvers and catalyze a synthesis of physics and data.

</details>


### [35] [How Worst-Case Are Adversarial Attacks? Linking Adversarial and Statistical Robustness](https://arxiv.org/abs/2601.14519)
*Giulio Rossolini*

Main category: cs.LG

TL;DR: 该论文研究了对抗性攻击作为随机扰动鲁棒性评估的有效性问题，提出了一个概率框架来量化方向性偏置扰动分布下的噪声风险，并通过实验评估对抗性攻击在多大程度上能反映真实的噪声风险。


<details>
  <summary>Details</summary>
Motivation: 对抗性攻击被广泛用于评估模型鲁棒性，但其作为随机扰动鲁棒性代理的有效性存在争议。作者质疑对抗性扰动是否真正代表了相同幅度随机噪声下的鲁棒性估计，还是仅仅反映了非典型的极端情况。

Method: 引入了一个概率度量框架，通过浓度因子κ参数化方向性偏置扰动分布，在各项同性噪声和对抗性方向之间进行插值。提出了在统计上更接近均匀噪声的机制下操作的攻击策略，并在ImageNet和CIFAR-10上系统性地评估了广泛使用的攻击方法。

Result: 实验系统地评估了广泛使用的对抗性攻击方法，揭示了对抗性攻击成功在哪些情况下能有意义地反映噪声风险，在哪些情况下会失效，从而为安全导向的评估提供了指导。

Conclusion: 对抗性扰动作为噪声风险估计器的有效性存在局限性，需要更细致的评估框架来区分对抗性攻击何时能有效代表随机扰动鲁棒性，何时不能。该研究为安全导向的模型评估提供了更可靠的指导。

Abstract: Adversarial attacks are widely used to evaluate model robustness, yet their validity as proxies for robustness to random perturbations remains debated. We ask whether an adversarial perturbation provides a representative estimate of robustness under random noise of the same magnitude, or instead reflects an atypical worst-case event. To this end, we introduce a probabilistic metric that quantifies noisy risk with respect to directionally biased perturbation distributions, parameterized by a concentration factor $κ$ that interpolates between isotropic noise and adversarial direction. Using this framework, we study the limits of adversarial perturbations as estimators of noisy risk by proposing an attack strategy designed to operate in regimes statistically closer to uniform noise. Experiments on ImageNet and CIFAR-10 systematically benchmark widely used attacks, highlighting when adversarial success meaningfully reflects noisy risk and when it fails, thereby informing their use in safety-oriented evaluation.

</details>


### [36] [On the Runway Cascade of Transformers for Language Modeling](https://arxiv.org/abs/2601.14522)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 论文提出"跑道感知重连"机制，通过总结每个token的"跑道"信息来重新连接注意力模式，改善因果transformer中的信息传播问题。


<details>
  <summary>Details</summary>
Motivation: 因果transformer中，因果掩码创建的计算图通过直接路径注意力和中间token形成的间接路径传播信息。最近研究发现，这两种信息传播模式之间的不匹配可能导致某些故障模式加剧，造成冗余和无关信息在token表示中传播。

Method: 提出跑道感知重连机制，将每个token的"跑道"上下文直接整合到其直接路径注意力中。该方法基于token的跑道景观总结来重新连接注意力模式，使模型能够感知累积的表征影响，实现更平衡的信息传播。该方法无需额外参数，可无缝集成到标准注意力机制中。

Result: 实验表明，重连后的transformer在通用语言建模方面有稳定提升，同时在信息检索和外推能力方面明显优于标准transformer。

Conclusion: 跑道感知重连机制有效解决了因果transformer中直接路径和间接路径信息传播不匹配的问题，改善了模型性能，特别是在信息检索和外推任务上表现更优。

Abstract: In decoder-only (causal) transformers, the computation graph created by causal masking routes information through both direct-path attention and indirect paths formed by intermediate tokens. We denote these indirect paths between token pairs as their runways. We argue that certain failure modes of causal transformers as observed by a growing body of recent works are likely exacerbated by a misalignment between these two information propagation modes. We formalize runway cascade as a phenomenon whereby this misalignment results in redundancies and irrelevant information cascading to token representations despite adequately learned attention patterns. As a solution, we propose runway-aware rewiring as a more explicit way of incorporating runway context directly into each token's direct-path attention. This mechanism re-wires the attention pattern for each token based on a summary of its runway landscape, enabling awareness of accumulating representational influences and allowing for more balanced information propagation. Our proposed methodology introduces no additional parameters and can seamlessly be integrated into standard attention mechanism. Empirically, our rewired transformer results in steady improvements in general language modeling as well as noticeably stronger information retrieval and extrapolation abilities compared to standard transformers.

</details>


### [37] [Search over Self-Edit Strategies for LLM Adaptation](https://arxiv.org/abs/2601.14532)
*Alistair Cheong,Haolin Cong,Tyler Yang,Dustin Miao*

Main category: cs.LG

TL;DR: 研究探索LLM能否利用任务反馈自主决定如何更新权重，在SEAL框架中放宽固定模板限制，让模型生成自编辑模板来控制训练数据和超参数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM开放搜索系统通常冻结基础模型，可能限制长期进展。虽然已有研究探索在测试时更新提案模型，但更新策略仍需人工指定。因此本研究探讨LLM能否利用任务反馈自主决定权重更新方式。

Method: 在SEAL框架中放宽固定人类模板约束，允许模型生成自编辑模板，从而控制训练数据和关键NTP超参数。研究两种变体：无存档版本和基于轻量级历史模板存档的版本。在Qwen3-8B模型上使用SQuAD数据集进行单段落知识整合实验。

Result: 无存档变体表现与较弱的"Implications"基线相当，存档变体优于"Implications"并接近最强人工设计"Rewrite"基线但未超越。分析发现朴素存档能提供短期鲁棒性，但也会加速同质化，可能需要显式新颖性压力才能超越优化的人工策略。

Conclusion: LLM能够利用任务反馈自主决定权重更新，但需要更精细的探索策略设计。朴素存档虽能提供短期改进，但可能导致同质化，需要显式新颖性压力来持续超越人工优化策略。

Abstract: Many LLM-based open-ended search systems freeze the foundation model that proposes improvements to existing solutions, which may bottleneck long-run progress. Recent work has explored updating the proposal model at test time [arXiv:2511.23473], but the update strategy is still typically hand-specified. Therefore, this study investigated whether an LLM can use task feedback to decide how it should update its weights. For tractability, we focused on the simpler case where there is only one round of self-improvement, and restricted the update operator to self-supervised next token prediction (NTP), leaving the model freedom in choosing its training data and key NTP hyperparameters. Using the Self-Adapting Language Models (SEAL) [arXiv:2506.10943] framework as a testbed, we relaxed its fixed human template constraint and allowed the model to generate its own self-edit templates, thereby giving it more control over its training data and hyperparameters. Two variants were studied, differing in whether template generation was conditioned on a lightweight archive of past templates. In SEAL's Single-Passage Knowledge Incorporation setting with Qwen3-8B on SQuAD [arXiv:1606.05250], the no-archive variant performed comparably to the weaker "Implications" baseline, while the archive variant outperformed "Implications" and approached the strongest human-designed "Rewrite" baseline without surpassing it. Further analysis of collapse in the model's exploration revealed that a naive archive can confer some short-term robustness but can also accelerate homogenization, suggesting that explicit novelty pressure may be required to consistently advance beyond carefully optimized human strategies. Our code is available at https://github.com/cheongalc/search-self-edit-strategies .

</details>


### [38] [Report for NSF Workshop on AI for Electronic Design Automation](https://arxiv.org/abs/2601.14541)
*Deming Chen,Vijay Ganesh,Weikai Li,Yingyan,Lin,Yong Liu,Subhasish Mitra,David Z. Pan,Ruchir Puri,Jason Cong,Yizhou Sun*

Main category: cs.LG

TL;DR: NSF研讨会报告总结了AI在电子设计自动化(EDA)领域的应用前景与挑战，涵盖物理设计、高层次综合、优化工具箱、测试验证四大主题，并提出促进AI/EDA协作、投资基础设施、培养人才等建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术（LLM、GNN、RL等）的快速发展，探索如何将这些技术应用于电子设计自动化领域，以缩短设计周期、提高设计效率，并推动下一代硬件系统的发展。

Method: 通过NSF组织的专家研讨会形式，汇集机器学习与EDA领域的专家，围绕四个主题进行深入讨论：1) AI用于物理设计与制造；2) AI用于高层次与逻辑级综合；3) AI优化工具箱；4) AI用于测试与验证。

Result: 研讨会明确了AI在EDA各环节的应用潜力与挑战，形成了具体的政策建议：促进跨领域合作、投资基础AI研究、建立数据基础设施、发展可扩展计算资源、加强人才培养。

Conclusion: AI技术有望革命性地改变EDA流程，但需要系统性支持，包括跨学科合作、基础设施建设和人才培养，以实现硬件设计的民主化和下一代硬件系统的开发。

Abstract: This report distills the discussions and recommendations from the NSF Workshop on AI for Electronic Design Automation (EDA), held on December 10, 2024 in Vancouver alongside NeurIPS 2024. Bringing together experts across machine learning and EDA, the workshop examined how AI-spanning large language models (LLMs), graph neural networks (GNNs), reinforcement learning (RL), neurosymbolic methods, etc.-can facilitate EDA and shorten design turnaround. The workshop includes four themes: (1) AI for physical synthesis and design for manufacturing (DFM), discussing challenges in physical manufacturing process and potential AI applications; (2) AI for high-level and logic-level synthesis (HLS/LLS), covering pragma insertion, program transformation, RTL code generation, etc.; (3) AI toolbox for optimization and design, discussing frontier AI developments that could potentially be applied to EDA tasks; and (4) AI for test and verification, including LLM-assisted verification tools, ML-augmented SAT solving, security/reliability challenges, etc. The report recommends NSF to foster AI/EDA collaboration, invest in foundational AI for EDA, develop robust data infrastructures, promote scalable compute infrastructure, and invest in workforce development to democratize hardware design and enable next-generation hardware systems. The workshop information can be found on the website https://ai4eda-workshop.github.io/.

</details>


### [39] [QMC: Efficient SLM Edge Inference via Outlier-Aware Quantization and Emergent Memories Co-Design](https://arxiv.org/abs/2601.14549)
*Nilesh Prasad Pandey,Jangseon Park,Onat Gungor,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: QMC提出了一种无需重新训练的量化方法，结合新型异构内存架构，通过识别SLM中的内点和离群权重，分别存储在ReRAM和MRAM中，显著提升边缘设备上小语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘平台上部署小语言模型面临内存、延迟和能耗限制。传统量化方法受新兴非易失性存储器设备噪声影响，而传统内存层次结构（SRAM密度低、DRAM带宽争用、Flash推理时闲置）进一步限制了效率，需要针对LLM推理的混合内存组织方案。

Method: 提出QMC（Outlier-aware Quantization with Memory Co-design），无需重新训练的量化方法结合异构内存架构：1）识别小语言模型中的内点权重和离群权重；2）内点权重存储在紧凑的多级ReRAM中；3）关键离群权重保留在高精度片上MRAM中，减轻噪声引起的性能下降。

Result: 在语言建模和推理基准测试中，QMC优于或匹配使用先进算法和混合数据格式的最先进量化方法，同时在算法评估和实际部署场景下实现更大压缩。相比FP16，内存使用减少6.3-7.3倍，外部数据传输减少7.6倍，能耗降低11.7倍，延迟降低12.5倍。

Conclusion: QMC作为一种可扩展、可直接部署的协同设计，为高效的设备端推理提供了解决方案，通过量化与内存架构的协同设计，有效解决了边缘设备上小语言模型部署的挑战。

Abstract: Deploying Small Language Models (SLMs) on edge platforms is critical for real-time, privacy-sensitive generative AI, yet constrained by memory, latency, and energy budgets. Quantization reduces model size and cost but suffers from device noise in emerging non-volatile memories, while conventional memory hierarchies further limit efficiency. SRAM provides fast access but has low density, DRAM must simultaneously accommodate static weights and dynamic KV caches, which creates bandwidth contention, and Flash, although dense, is primarily used for initialization and remains inactive during inference. These limitations highlight the need for hybrid memory organizations tailored to LLM inference. We propose Outlier-aware Quantization with Memory Co-design (QMC), a retraining-free quantization with a novel heterogeneous memory architecture. QMC identifies inlier and outlier weights in SLMs, storing inlier weights in compact multi-level Resistive-RAM (ReRAM) while preserving critical outliers in high-precision on-chip Magnetoresistive-RAM (MRAM), mitigating noise-induced degradation. On language modeling and reasoning benchmarks, QMC outperforms and matches state-of-the-art quantization methods using advanced algorithms and hybrid data formats, while achieving greater compression under both algorithm-only evaluation and realistic deployment settings. Specifically, compared against SoTA quantization methods on the latest edge AI platform, QMC reduces memory usage by 6.3x-7.3x, external data transfers by 7.6x, energy by 11.7x, and latency by 12.5x when compared to FP16, establishing QMC as a scalable, deployment-ready co-design for efficient on-device inference.

</details>


### [40] [Constructing Multi-label Hierarchical Classification Models for MITRE ATT&CK Text Tagging](https://arxiv.org/abs/2601.14556)
*Andrew Crossman,Jonah Dodd,Viralam Ramamurthy Chaithanya Kumar,Riyaz Mohammed,Andrew R. Plummer,Chandra Sekharudu,Deepak Warrier,Mohammad Yekrangian*

Main category: cs.LG

TL;DR: 提出分層任務空間框架來組織MITRE ATT&CK文本標註的自動化工作，並構建了基於經典機器學習的多標籤分層分類模型，在戰術層準確率達94%，技術層達82%，超越GPT-4o等複雜方法


<details>
  <summary>Details</summary>
Motivation: MITRE ATT&CK作為網絡安全知識庫，目前標註過程仍主要依賴人工。需要自動化方法來提高威脅情報報告、漏洞描述等文本的標註效率，但現有自動化方法缺乏系統性框架

Method: 1) 提出分層"任務空間"框架來組織ATT&CK文本標註任務；2) 基於該框架構建多標籤分層分類模型，使用經典機器學習方法；3) 在通用網絡威脅情報文本上進行實驗；4) 將模型擴展到金融應用威脅場景語料庫

Result: 1) 戰術層準確率約94%，技術層準確率約82%；2) 性能達到或超越現有最佳方法；3) 僅使用經典機器學習，無需LLM、RAG等複雜技術；4) GPT-4o在戰術層準確率僅約60%，顯著低於本方法

Conclusion: 提出的分層任務空間框架有效組織了ATT&CK文本標註的自動化工作，基於經典機器學習的多標籤分層分類模型在準確性和效率上表現優異，為安全社區提供了實用的自動化工具

Abstract: MITRE ATT&CK is a cybersecurity knowledge base that organizes threat actor and cyber-attack information into a set of tactics describing the reasons and goals threat actors have for carrying out attacks, with each tactic having a set of techniques that describe the potential methods used in these attacks. One major application of ATT&CK is the use of its tactic and technique hierarchy by security specialists as a framework for annotating cyber-threat intelligence reports, vulnerability descriptions, threat scenarios, inter alia, to facilitate downstream analyses. To date, the tagging process is still largely done manually. In this technical note, we provide a stratified "task space" characterization of the MITRE ATT&CK text tagging task for organizing previous efforts toward automation using AIML methods, while also clarifying pathways for constructing new methods. To illustrate one of the pathways, we use the task space strata to stage-wise construct our own multi-label hierarchical classification models for the text tagging task via experimentation over general cyber-threat intelligence text -- using shareable computational tools and publicly releasing the models to the security community (via https://github.com/jpmorganchase/MITRE_models). Our multi-label hierarchical approach yields accuracy scores of roughly 94% at the tactic level, as well as accuracy scores of roughly 82% at the technique level. The models also meet or surpass state-of-the-art performance while relying only on classical machine learning methods -- removing any dependence on LLMs, RAG, agents, or more complex hierarchical approaches. Moreover, we show that GPT-4o model performance at the tactic level is significantly lower (roughly 60% accuracy) than our own approach. We also extend our baseline model to a corpus of threat scenarios for financial applications produced by subject matter experts.

</details>


### [41] [Place with Intention: An Empirical Attendance Predictive Study of Expo 2025 Osaka, Kansai, Japan](https://arxiv.org/abs/2601.14570)
*Xiaojie Yang,Dizhi Huang,Hangli Ge,Masahiro Sano,Takeaki Ohdake,Kazuma Hatano,Noboru Koshizuka*

Main category: cs.LG

TL;DR: 提出基于Transformer的框架，利用预约动态（门票预订和更新）作为游客出席意向的代理，用于大规模国际活动的出席预测，避免多源外部数据的复杂性。


<details>
  <summary>Details</summary>
Motivation: 大规模国际活动（如2025年大阪世博会）的日常出席预测对交通、人流和服务管理至关重要。现有方法依赖多源外部数据（天气、交通、社交媒体），但在历史数据不足时可能导致不可靠结果。

Method: 提出Transformer-based框架，利用预约动态（门票预订和后续更新）作为游客出席意向的代理，假设这些意向最终反映在预约模式中。构建包含入场记录和预约动态的数据集，在单通道（总出席）和双通道（东、西门分开）设置下评估模型。

Result: 结果显示，分开建模东、西门能持续提高预测准确性，特别是对短期和中期预测。消融研究进一步证实了编码器-解码器结构、逆风格嵌入和自适应融合模块的重要性。

Conclusion: 预约动态为大规模国际活动的出席预测提供了实用且信息丰富的基础，避免了多源数据整合的复杂性，同时通过预约模式隐式捕捉了天气、促销等外部影响。

Abstract: Accurate forecasting of daily attendance is vital for managing transportation, crowd flows, and services at large-scale international events such as Expo 2025 Osaka, Kansai, Japan. However, existing approaches often rely on multi-source external data (such as weather, traffic, and social media) to improve accuracy, which can lead to unreliable results when historical data are insufficient. To address these challenges, we propose a Transformer-based framework that leverages reservation dynamics, i.e., ticket bookings and subsequent updates within a time window, as a proxy for visitors' attendance intentions, under the assumption that such intentions are eventually reflected in reservation patterns. This design avoids the complexity of multi-source integration while still capturing external influences like weather and promotions implicitly embedded in reservation dynamics. We construct a dataset combining entrance records and reservation dynamics and evaluate the model under both single-channel (total attendance) and two-channel (separated by East and West gates) settings. Results show that separately modeling East and West gates consistently improves accuracy, particularly for short- and medium-term horizons. Ablation studies further confirm the importance of the encoder-decoder structure, inverse-style embedding, and adaptive fusion module. Overall, our findings indicate that reservation dynamics offer a practical and informative foundation for attendance forecasting in large-scale international events.

</details>


### [42] [Counterfactual Modeling with Fine-Tuned LLMs for Health Intervention Design and Sensor Data Augmentation](https://arxiv.org/abs/2601.14590)
*Shovito Barua Soumma,Asiful Arefeen,Stephanie M. Carpenter,Melanie Hingle,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 该研究评估了使用大语言模型（LLM）生成反事实解释（CFEs）的效果，在临床数据集上测试了GPT-4和开源模型，发现微调后的LLM能产生高质量、临床可操作的CFEs，并能有效用于数据增强提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 反事实解释能提供人本可解释性，识别改变机器学习预测所需的最小可行变化。它们可用于异常预防干预和作为增强数据训练鲁棒模型。研究旨在全面评估LLM生成CFEs的能力，特别是在临床健康领域的应用潜力。

Method: 使用多模态AI-READI临床数据集，评估了GPT-4（零样本和少样本）以及两个开源模型BioMistral-7B和LLaMA-3.1-8B在预训练和微调配置下的CF生成。从干预质量、特征多样性和增强效果三个维度评估CFs，并与DiCE、CFNOW、NICE等优化基线方法比较。

Result: 微调后的LLM，特别是LLaMA-3.1-8B，能产生高合理性（高达99%）、强有效性（高达0.99）且临床可操作的CFEs。在标签稀缺场景下，LLM生成的CFs用于数据增强能显著恢复分类器性能，在三种稀缺场景下平均F1恢复20%。相比优化基线方法，LLMs提供了更灵活、模型无关的方法，生成更具临床可操作性和语义连贯性的反事实。

Conclusion: 这项工作展示了LLM驱动的反事实在传感器数字健康领域中，既可用于可解释的干预设计，也可用于数据高效的模型训练。SenseCF方法通过微调LLM生成有效的反事实解释，并补充不平衡数据集中的少数类，从而改善模型训练、提升鲁棒性和预测性能。

Abstract: Counterfactual explanations (CFEs) provide human-centric interpretability by identifying the minimal, actionable changes required to alter a machine learning model's prediction. Therefore, CFs can be used as (i) interventions for abnormality prevention and (ii) augmented data for training robust models. We conduct a comprehensive evaluation of CF generation using large language models (LLMs), including GPT-4 (zero-shot and few-shot) and two open-source models-BioMistral-7B and LLaMA-3.1-8B, in both pretrained and fine-tuned configurations. Using the multimodal AI-READI clinical dataset, we assess CFs across three dimensions: intervention quality, feature diversity, and augmentation effectiveness. Fine-tuned LLMs, particularly LLaMA-3.1-8B, produce CFs with high plausibility (up to 99%), strong validity (up to 0.99), and realistic, behaviorally modifiable feature adjustments. When used for data augmentation under controlled label-scarcity settings, LLM-generated CFs substantially restore classifier performance, yielding an average 20% F1 recovery across three scarcity scenarios. Compared with optimization-based baselines such as DiCE, CFNOW, and NICE, LLMs offer a flexible, model-agnostic approach that generates more clinically actionable and semantically coherent counterfactuals. Overall, this work demonstrates the promise of LLM-driven counterfactuals for both interpretable intervention design and data-efficient model training in sensor-based digital health.
  Impact: SenseCF fine-tunes an LLM to generate valid, representative counterfactual explanations and supplement minority class in an imbalanced dataset for improving model training and boosting model robustness and predictive performance

</details>


### [43] [Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective](https://arxiv.org/abs/2601.14599)
*Xiao Hu,Hong Xie,Tao Tan,Defu Lian,Jianyu Han*

Main category: cs.LG

TL;DR: 该论文提出了一种自底向上的实验框架来系统分析强化学习微调LLM中各种优化选择的作用和瓶颈，通过最小化配置连接多臂老虎机理论，揭示了设计选择的新理解。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习微调领域存在大量启发式方法，但各种主张相互矛盾，缺乏对两个基本问题的清晰理解：1) 每个优化选择的作用是什么？2) 哪些是瓶颈？

Method: 提出自底向上的实验流程：底层是最小化配置（单一训练数据、每轮单次rollout、奖励直接作为学习信号无优势函数设计），该配置连接具有极大离散动作空间的多臂老虎机学习；然后逐层扩展配置，检验每个设计选择的作用。

Result: 在三个LLM和两个推理数据集上的实验不仅揭示了设计选择的新理解，还为塑造该领域提供了重要见解。

Conclusion: 通过系统分析LLM强化学习微调中的优化选择，该研究澄清了领域中的模糊认识，为未来的优化方向提供了理论基础和实践指导。

Abstract: A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.

</details>


### [44] [Variance-Adaptive Muon: Accelerating LLM Pretraining with NSR-Modulated and Variance-Scaled Momentum](https://arxiv.org/abs/2601.14603)
*Jingru Li,Yibo Fan,Huan Li*

Main category: cs.LG

TL;DR: Muon优化器通过正交动量更新加速LLM预训练，提出Muon-NSR和Muon-VS两个变体，在GPT-2和LLaMA预训练中比AdamW和Muon基线收敛更快、验证损失更低。


<details>
  <summary>Details</summary>
Motivation: LLM预训练计算成本高昂，优化器效率成为重要实践考虑。虽然Adam是方差自适应符号更新算法，但仍有改进空间，需要更高效的优化方法来加速LLM预训练。

Method: 提出Muon优化器，使用正交动量更新作为元素级符号算子的矩阵类比。进一步提出两个变体：Muon-NSR应用信噪比调制，Muon-VS执行基于方差的缩放而不引入额外超参数，两者都在正交化前对动量进行方差自适应归一化。

Result: 在GPT-2和LLaMA预训练实验中，Muon-NSR和Muon-VS加速收敛，持续获得比调优良好的AdamW和Muon基线更低的验证损失。例如在LLaMA-1.2B模型上，达到目标验证损失所需的迭代次数比调优良好的Muon减少1.36倍。

Conclusion: Muon优化器及其变体通过正交动量更新和方差自适应归一化，显著提高了LLM预训练的效率，为大规模语言模型训练提供了更高效的优化方法。

Abstract: Large Language Models (LLMs) achieve competitive performance across diverse natural language processing (NLP) tasks, yet pretraining is computationally demanding, making optimizer efficiency an important practical consideration. Muon accelerates LLM pretraining via orthogonal momentum updates that serve as a matrix analogue of the element-wise sign operator. Motivated by the recent perspective that Adam is a variance-adaptive sign update algorithm, we propose two variants of Muon, Muon-NSR and Muon-VS, which apply variance-adaptive normalization to momentum before orthogonalization. Muon-NSR applies noise-to-signal ratio (NSR) modulation, while Muon-VS performs variance-based scaling without introducing additional hyperparameters. Experiments on GPT-2 and LLaMA pretraining demonstrate that our proposed methods accelerate convergence and consistently achieve lower validation loss than both competitive, well-tuned AdamW and Muon baselines. For example, on the LLaMA-1.2B model, Muon-NSR and Muon-VS reduce the iterations required to reach the target validation loss by $1.36\times$ relative to the well-tuned Muon following the recent benchmark.

</details>


### [45] [Relational Graph Modeling for Credit Default Prediction: Heterogeneous GNNs and Hybrid Ensemble Learning](https://arxiv.org/abs/2601.14633)
*Yvonne Yang,Eranki Vasistha*

Main category: cs.LG

TL;DR: 该研究构建了一个包含3100万节点和5000万边的大规模异质图来建模信用违约风险，发现单独的图神经网络提升有限，但将GNN生成的客户嵌入与表格特征结合的混合集成方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 信用违约风险涉及借款人、金融机构和交易行为之间的复杂交互。虽然表格模型在信用评分中表现优异，但难以显式捕捉多表金融历史中的跨实体依赖关系。

Method: 构建包含3100万节点和5000万边的大规模异质图，集成借款人属性和细粒度交易级实体（分期付款、POS现金余额、信用卡历史）。评估异质图神经网络（包括异质GraphSAGE和关系感知注意力异质GNN），并与强大的表格基线对比。

Result: 单独的GNN相比梯度提升树基线提升有限，但将表格特征与GNN生成的客户嵌入结合的混合集成方法取得了最佳性能，提升了ROC-AUC和PR-AUC。对比预训练能改善优化稳定性，但在通用图增强下下游收益有限。

Conclusion: 图神经网络能够捕捉信用风险中的关系信号，但单独使用效果有限。最佳策略是将GNN嵌入与表格特征结合，形成混合集成方法。研究还通过可解释性和公平性分析揭示了关系信号如何影响子群行为和筛选结果。

Abstract: Credit default risk arises from complex interactions among borrowers, financial institutions, and transaction-level behaviors. While strong tabular models remain highly competitive in credit scoring, they may fail to explicitly capture cross-entity dependencies embedded in multi-table financial histories. In this work, we construct a massive-scale heterogeneous graph containing over 31 million nodes and more than 50 million edges, integrating borrower attributes with granular transaction-level entities such as installment payments, POS cash balances, and credit card histories.
  We evaluate heterogeneous graph neural networks (GNNs), including heterogeneous GraphSAGE and a relation-aware attentive heterogeneous GNN, against strong tabular baselines. We find that standalone GNNs provide limited lift over a competitive gradient-boosted tree baseline, while a hybrid ensemble that augments tabular features with GNN-derived customer embeddings achieves the best overall performance, improving both ROC-AUC and PR-AUC. We further observe that contrastive pretraining can improve optimization stability but yields limited downstream gains under generic graph augmentations. Finally, we conduct structured explainability and fairness analyses to characterize how relational signals affect subgroup behavior and screening-oriented outcomes.

</details>


### [46] [Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport](https://arxiv.org/abs/2601.14653)
*Yuyu Liu,Jiannan Yang,Ziyang Yu,Weishen Pan,Fei Wang,Tengfei Ma*

Main category: cs.LG

TL;DR: CROT是一种基于最优传输的插补算法，专门处理表格数据中的大块缺失数据，在保持高精度的同时显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 单细胞测序数据中的缺失数据对提取生物学见解构成重大挑战。现有插补方法通常假设数据均匀且完整，难以处理大块缺失数据的情况。

Method: 提出CROT算法，基于最优传输理论设计，专门处理表格格式的基于块的缺失数据，能够有效捕捉存在显著缺失情况下的底层数据结构。

Result: CROT实现了优越的插补精度，同时显著减少了运行时间，展示了其在大规模数据集上的可扩展性和效率。

Conclusion: 这项工作为具有结构化数据缺失的异质高维数据集提供了稳健的插补解决方案，解决了生物和临床数据分析中的关键挑战。

Abstract: Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github.

</details>


### [47] [Beyond Denial-of-Service: The Puppeteer's Attack for Fine-Grained Control in Ranking-Based Federated Learning](https://arxiv.org/abs/2601.14687)
*Zhihao Chen,Zirui Gong,Jianting Ning,Yanjun Zhang,Leo Yu Zhang*

Main category: cs.LG

TL;DR: FRL虽然通过离散排名机制增强了联邦学习的抗攻击能力，但仍面临新型细粒度控制攻击（ECA）的威胁，该攻击能精确控制目标模型的准确率而不被检测。


<details>
  <summary>Details</summary>
Motivation: 联邦排名学习（FRL）因其离散排名机制被认为对模型投毒攻击具有强鲁棒性，但作者发现FRL仍存在安全漏洞，需要研究新型攻击方法来揭示其脆弱性。

Method: 提出边缘控制攻击（ECA），包含两个阶段：1）识别并操纵上升和下降边缘使全局模型对齐目标模型；2）扩大选择边界差距以稳定全局模型在目标准确率。

Result: 在7个基准数据集和9种拜占庭鲁棒聚合规则上的实验表明，ECA能实现细粒度准确率控制，平均误差仅0.224%，比基线方法提升高达17倍。

Conclusion: FRL虽然减少了攻击面，但仍易受细粒度控制攻击，ECA攻击的成功表明需要更强的防御机制来应对先进的投毒攻击。

Abstract: Federated Rank Learning (FRL) is a promising Federated Learning (FL) paradigm designed to be resilient against model poisoning attacks due to its discrete, ranking-based update mechanism. Unlike traditional FL methods that rely on model updates, FRL leverages discrete rankings as a communication parameter between clients and the server. This approach significantly reduces communication costs and limits an adversary's ability to scale or optimize malicious updates in the continuous space, thereby enhancing its robustness. This makes FRL particularly appealing for applications where system security and data privacy are crucial, such as web-based auction and bidding platforms. While FRL substantially reduces the attack surface, we demonstrate that it remains vulnerable to a new class of local model poisoning attack, i.e., fine-grained control attacks. We introduce the Edge Control Attack (ECA), the first fine-grained control attack tailored to ranking-based FL frameworks. Unlike conventional denial-of-service (DoS) attacks that cause conspicuous disruptions, ECA enables an adversary to precisely degrade a competitor's accuracy to any target level while maintaining a normal-looking convergence trajectory, thereby avoiding detection. ECA operates in two stages: (i) identifying and manipulating Ascending and Descending Edges to align the global model with the target model, and (ii) widening the selection boundary gap to stabilize the global model at the target accuracy. Extensive experiments across seven benchmark datasets and nine Byzantine-robust aggregation rules (AGRs) show that ECA achieves fine-grained accuracy control with an average error of only 0.224%, outperforming the baseline by up to 17x. Our findings highlight the need for stronger defenses against advanced poisoning attacks. Our code is available at: https://github.com/Chenzh0205/ECA

</details>


### [48] [Beyond Error-Based Optimization: Experience-Driven Symbolic Regression with Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2601.14693)
*Jianwen Sun,Xinrui Li,Fuqing Li,Xiaoxuan Shen*

Main category: cs.LG

TL;DR: EGRL-SR：基于经验驱动目标条件强化学习的符号回归新框架，通过历史轨迹和动作价值网络主动引导搜索，而非传统误差驱动方法


<details>
  <summary>Details</summary>
Motivation: 传统基于误差的符号回归方法在庞大的表达式空间中面临搜索方向模糊的问题，因为许多结构不同但误差相似的候选表达式会阻碍收敛到真实函数

Method: 将符号回归建模为目标条件强化学习问题，引入后见经验回放，设计全点满足二元奖励函数（关注结构模式而非低误差），并提出结构引导启发式探索策略

Result: 在公共基准测试中，EGRL-SR在恢复率和鲁棒性方面始终优于最先进方法，在相同搜索预算下能恢复更复杂的表达式

Conclusion: EGRL-SR通过强化学习框架有效引导符号回归搜索，奖励函数和探索策略对性能提升起关键作用，为符号回归提供了新的搜索视角

Abstract: Symbolic Regression aims to automatically identify compact and interpretable mathematical expressions that model the functional relationship between input and output variables. Most existing search-based symbolic regression methods typically rely on the fitting error to inform the search process. However, in the vast expression space, numerous candidate expressions may exhibit similar error values while differing substantially in structure, leading to ambiguous search directions and hindering convergence to the underlying true function. To address this challenge, we propose a novel framework named EGRL-SR (Experience-driven Goal-conditioned Reinforcement Learning for Symbolic Regression). In contrast to traditional error-driven approaches, EGRL-SR introduces a new perspective: leveraging precise historical trajectories and optimizing the action-value network to proactively guide the search process, thereby achieving a more robust expression search. Specifically, we formulate symbolic regression as a goal-conditioned reinforcement learning problem and incorporate hindsight experience replay, allowing the action-value network to generalize common mapping patterns from diverse input-output pairs. Moreover, we design an all-point satisfaction binary reward function that encourages the action-value network to focus on structural patterns rather than low-error expressions, and concurrently propose a structure-guided heuristic exploration strategy to enhance search diversity and space coverage. Experiments on public benchmarks show that EGRL-SR consistently outperforms state-of-the-art methods in recovery rate and robustness, and can recover more complex expressions under the same search budget. Ablation results validate that the action-value network effectively guides the search, with both the reward function and the exploration strategy playing critical roles.

</details>


### [49] [Re-understanding Graph Unlearning through Memorization](https://arxiv.org/abs/2601.14694)
*Pengfei Ding,Yan Wang,Guanfeng Liu*

Main category: cs.LG

TL;DR: 提出MGU框架，基于GNN记忆化视角解决图遗忘的三个核心问题：难度评估不准确、硬任务效果差、评估协议不匹配。


<details>
  <summary>Details</summary>
Motivation: 现有图遗忘方法缺乏对遗忘效果关键因素的理解，存在三个根本局限：1) 难度评估不实用不准确；2) 难以遗忘的任务效果差；3) 评估协议与真实遗忘能力不匹配。

Method: 建立GNN记忆化作为理解图遗忘的新视角，提出MGU记忆化引导的图遗忘框架，包含：1) 准确实用的难度评估；2) 基于难度动态调整遗忘目标的自适应策略；3) 符合实际需求的全面评估协议。

Result: 在十个真实世界图上的实验表明，MGU在遗忘质量、计算效率和效用保持方面持续优于最先进的基线方法。

Conclusion: MGU通过记忆化视角解决了图遗忘的关键挑战，提供了更准确、实用和有效的遗忘解决方案。

Abstract: Graph unlearning (GU), which removes nodes, edges, or features from trained graph neural networks (GNNs), is crucial in Web applications where graph data may contain sensitive, mislabeled, or malicious information. However, existing GU methods lack a clear understanding of the key factors that determine unlearning effectiveness, leading to three fundamental limitations: (1) impractical and inaccurate GU difficulty assessment due to test-access requirements and invalid assumptions, (2) ineffectiveness on hard-to-unlearn tasks, and (3) misaligned evaluation protocols that overemphasize easy tasks and fail to capture true forgetting capability. To address these issues, we establish GNN memorization as a new perspective for understanding graph unlearning and propose MGU, a Memorization-guided Graph Unlearning framework. MGU achieves three key advances: it provides accurate and practical difficulty assessment across different GU tasks, develops an adaptive strategy that dynamically adjusts unlearning objectives based on difficulty levels, and establishes a comprehensive evaluation protocol that aligns with practical requirements. Extensive experiments on ten real-world graphs demonstrate that MGU consistently outperforms state-of-the-art baselines in forgetting quality, computational efficiency, and utility preservation.

</details>


### [50] [CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation](https://arxiv.org/abs/2601.14695)
*Yutong Chen,Jiandong Gao,Ji Wu*

Main category: cs.LG

TL;DR: 提出CoScale-RL方法，通过扩展解决方案和计算rollout来提升大型推理模型的训练稳定性和效率，相比传统方法在四个基准测试上平均提升3.76倍准确率。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型的训练不稳定且难以预测，特别是在处理困难问题或基础模型较弱的情况下。现有的后训练扩展策略在这些情况下仍有改进空间。

Method: 提出CoScale-RL方法：1）扩展解决方案：为每个问题收集多个解决方案而非简单扩大数据集；2）扩展rollout计算：稳定强化学习训练；3）使用Re-distillation模型合并技术维持计算效率。

Result: 在四个基准测试上平均获得3.76倍的准确率提升，显著提高了数据和计算效率，能够在无需大量监督微调数据集的情况下提升大型推理模型的能力边界。

Conclusion: CoScale-RL为提升大型推理模型的推理能力提供了新的扩展方向，通过更好的数据和计算效率解决了训练不稳定问题。

Abstract: Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.

</details>


### [51] [Case-Guided Sequential Assay Planning in Drug Discovery](https://arxiv.org/abs/2601.14710)
*Tianchi Chen,Jan Bima,Sean L. Wu,Otto Ritter,Bingjia Yang,Xiang Yu*

Main category: cs.LG

TL;DR: IBMDP框架用于无模拟器的药物发现实验序列规划，通过历史数据构建隐式贝叶斯模型，使用集成MCTS规划，相比启发式方法减少92%资源消耗


<details>
  <summary>Details</summary>
Motivation: 药物发现中的实验序列规划面临严重不确定性和资源约束，标准强化学习缺乏环境模拟器或转移数据，只能依赖静态历史数据库

Method: 提出隐式贝叶斯马尔可夫决策过程（IBMDP），通过相似历史结果构建非参数信念分布，实现贝叶斯信念更新，采用集成蒙特卡洛树搜索规划生成稳定策略

Result: 在真实CNS药物发现任务中减少92%资源消耗；在合成环境中与可计算最优策略相比，比确定性值迭代方法有更高对齐度

Conclusion: IBMDP为数据丰富但模拟器匮乏领域的顺序实验设计提供了实用解决方案，证明了集成规划器的优越性

Abstract: Optimally sequencing experimental assays in drug discovery is a high-stakes planning problem under severe uncertainty and resource constraints. A primary obstacle for standard reinforcement learning (RL) is the absence of an explicit environment simulator or transition data $(s, a, s')$; planning must rely solely on a static database of historical outcomes. We introduce the Implicit Bayesian Markov Decision Process (IBMDP), a model-based RL framework designed for such simulator-free settings. IBMDP constructs a case-guided implicit model of transition dynamics by forming a nonparametric belief distribution using similar historical outcomes. This mechanism enables Bayesian belief updating as evidence accumulates and employs ensemble MCTS planning to generate stable policies that balance information gain toward desired outcomes with resource efficiency. We validate IBMDP through comprehensive experiments. On a real-world central nervous system (CNS) drug discovery task, IBMDP reduced resource consumption by up to 92\% compared to established heuristics while maintaining decision confidence. To rigorously assess decision quality, we also benchmarked IBMDP in a synthetic environment with a computable optimal policy. Our framework achieves significantly higher alignment with this optimal policy than a deterministic value iteration alternative that uses the same similarity-based model, demonstrating the superiority of our ensemble planner. IBMDP offers a practical solution for sequential experimental design in data-rich but simulator-poor domains.

</details>


### [52] [PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning](https://arxiv.org/abs/2601.14716)
*Yao Lu,Dengdong Fan,Jianzheng Nie,Fan Xu,Jie Chen,Bin Zhou,Yonghong Tian*

Main category: cs.LG

TL;DR: PCL-Reasoner-V1.5是基于Qwen2.5-32B构建的320亿参数数学推理大模型，采用监督微调+强化学习训练，创新性地使用离线RL方法提升训练稳定性，在AIME竞赛中取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 当前数学推理大模型训练面临稳定性挑战，特别是标准在线RL方法（如GRPO）存在效率问题。研究者旨在开发更稳定高效的训练范式来提升LLM的数学推理能力。

Method: 基于Qwen2.5-32B构建320亿参数模型，采用两阶段训练：1) 监督微调(SFT)；2) 强化学习(RL)，其中创新性地提出离线RL方法替代标准在线RL，在华为昇腾910C NPU上进行实验。

Result: 在AIME 2024上达到90.9%平均准确率，在AIME 2025上达到85.6%平均准确率，在基于Qwen2.5-32B后训练的模型中取得最先进性能。

Conclusion: 离线RL方法为LLM推理能力提升提供了稳定高效的训练范式，PCL-Reasoner-V1.5展示了该方法在数学推理任务上的有效性。

Abstract: We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.

</details>


### [53] [FSX: Message Flow Sensitivity Enhanced Structural Explainer for Graph Neural Networks](https://arxiv.org/abs/2601.14730)
*Bizu Feng,Zhimu Yang,Shaode Yu,Zixin Hu*

Main category: cs.LG

TL;DR: FSX是一个新颖的GNN可解释性框架，结合了内部消息流分析和外部图数据的合作博弈方法，通过流敏感性分析识别关键消息流，在子图上进行流感知合作博弈，实现高效且高保真的解释。


<details>
  <summary>Details</summary>
Motivation: 现有GNN可解释性方法存在权衡：基于梯度的方法计算高效但忽略结构交互，而基于博弈论的方法能捕捉交互但计算开销大且可能偏离模型真实推理路径。需要一种能兼顾效率和结构理解的方法。

Method: FSX采用混合框架：1）通过流敏感性分析在单次前向传播中模拟局部节点扰动，测量消息流强度变化，识别关键消息流；2）将敏感度排名的流投影到输入图，定义紧凑的语义子图；3）在每个子图上进行流感知合作博弈，通过类Shapley值公平评估节点贡献，同时考虑节点特征重要性和其在维持/破坏关键流中的作用。

Result: 在多个数据集和GNN架构上的广泛评估表明，FSX实现了优越的解释保真度，同时显著减少了运行时间，并提供了前所未有的关于模型预测背后结构逻辑的洞察——特别是重要子结构如何通过控制关键内部计算路径的稳定性来施加影响。

Conclusion: FSX成功解决了现有GNN可解释性方法的权衡问题，通过结合内部消息流分析和外部图结构的合作博弈，实现了高效、高保真且能深入理解模型结构逻辑的解释框架。

Abstract: Despite the widespread success of Graph Neural Networks (GNNs), understanding the reasons behind their specific predictions remains challenging. Existing explainability methods face a trade-off that gradient-based approaches are computationally efficient but often ignore structural interactions, while game-theoretic techniques capture interactions at the cost of high computational overhead and potential deviation from the model's true reasoning path. To address this gap, we propose FSX (Message Flow Sensitivity Enhanced Structural Explainer), a novel hybrid framework that synergistically combines the internal message flows of the model with a cooperative game approach applied to the external graph data. FSX first identifies critical message flows via a novel flow-sensitivity analysis: during a single forward pass, it simulates localized node perturbations and measures the resulting changes in message flow intensities. These sensitivity-ranked flows are then projected onto the input graph to define compact, semantically meaningful subgraphs. Within each subgraph, a flow-aware cooperative game is conducted, where node contributions are evaluated fairly through a Shapley-like value that incorporates both node-feature importance and their roles in sustaining or destabilizing the identified critical flows. Extensive evaluation across multiple datasets and GNN architectures demonstrates that FSX achieves superior explanation fidelity with significantly reduced runtime, while providing unprecedented insights into the structural logic underlying model predictions--specifically, how important sub-structures exert influence by governing the stability of key internal computational pathways.

</details>


### [54] [RefProtoFL: Communication-Efficient Federated Learning via External-Referenced Prototype Alignment](https://arxiv.org/abs/2601.14746)
*Hongyue Wu,Hangyu Li,Guodong Fan,Haoran Zhu,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: RefProtoFL：一种通信高效的联邦学习框架，通过外部参考原型对齐和自适应概率更新丢弃，在有限带宽下实现更好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘环境中面临通信带宽有限和客户端数据分布异构的挑战。现有的基于原型的FL方法虽然通过交换类别特征原型而非完整模型参数来缓解通信问题，但在严重通信约束下仍存在泛化性能不佳的问题。

Method: 1. 将模型分解为私有主干网络和轻量级共享适配器，仅通过适配器参数进行联邦通信；2. 自适应概率更新丢弃（APUD）：基于幅度的Top-K稀疏化，仅传输最重要的适配器更新；3. 外部参考原型对齐（ERPA）：利用服务器持有的少量公共数据集构建外部参考原型作为共享语义锚点，对齐客户端表示。

Result: 在标准基准测试上的广泛实验表明，RefProtoFL比最先进的基于原型的联邦学习基线方法获得了更高的分类准确率。

Conclusion: RefProtoFL通过结合通信高效的APUD和表示一致的ERPA，在有限通信带宽下有效解决了联邦学习中的异构数据分布问题，实现了更好的泛化性能。

Abstract: Federated learning (FL) enables collaborative model training without sharing raw data in edge environments, but is constrained by limited communication bandwidth and heterogeneous client data distributions. Prototype-based FL mitigates this issue by exchanging class-wise feature prototypes instead of full model parameters; however, existing methods still suffer from suboptimal generalization under severe communication constraints. In this paper, we propose RefProtoFL, a communication-efficient FL framework that integrates External-Referenced Prototype Alignment (ERPA) for representation consistency with Adaptive Probabilistic Update Dropping (APUD) for communication efficiency. Specifically, we decompose the model into a private backbone and a lightweight shared adapter, and restrict federated communication to the adapter parameters only. To further reduce uplink cost, APUD performs magnitude-aware Top-K sparsification, transmitting only the most significant adapter updates for server-side aggregation. To address representation inconsistency across heterogeneous clients, ERPA leverages a small server-held public dataset to construct external reference prototypes that serve as shared semantic anchors. For classes covered by public data, clients directly align local representations to public-induced prototypes, whereas for uncovered classes, alignment relies on server-aggregated global reference prototypes via weighted averaging. Extensive experiments on standard benchmarks demonstrate that RefProtoFL attains higher classification accuracy than state-of-the-art prototype-based FL baselines.

</details>


### [55] [Mechanism Shift During Post-training from Autoregressive to Masked Diffusion Language Models](https://arxiv.org/abs/2601.14758)
*Injin Kong,Hyoungjoon Lee,Yohan Jo*

Main category: cs.LG

TL;DR: 研究通过电路分析比较自回归模型(ARMs)与掩码扩散模型(MDMs)，发现扩散后训练不仅调整参数，还从根本上重组内部计算机制，支持非顺序全局规划。


<details>
  <summary>Details</summary>
Motivation: 探索将自回归模型后训练为掩码扩散模型时，模型内部算法转换的本质：这种范式转变是否让MDMs获得真正的双向推理能力，还是仅仅重新包装了自回归启发式方法。

Method: 对ARMs及其MDM对应模型进行对比电路分析，研究机制转变与任务结构性质的关系。

Result: 发现系统性"机制转变"：对于局部因果依赖任务，MDMs基本保留自回归电路；对于全局规划任务，MDMs放弃初始化路径，表现出早期层处理增加的重新布线。语义上从ARMs的尖锐局部专业化转变为MDMs的分布式集成。

Conclusion: 扩散后训练不仅仅是调整模型参数，而是从根本上重组内部计算以支持非顺序全局规划。

Abstract: Post-training pretrained Autoregressive models (ARMs) into Masked Diffusion models (MDMs) has emerged as a cost-effective strategy to overcome the limitations of sequential generation. However, the internal algorithmic transformations induced by this paradigm shift remain unexplored, leaving it unclear whether post-trained MDMs acquire genuine bidirectional reasoning capabilities or merely repackage autoregressive heuristics. In this work, we address this question by conducting a comparative circuit analysis of ARMs and their MDM counterparts. Our analysis reveals a systematic "mechanism shift" dependent on the structural nature of the task. Structurally, we observe a distinct divergence: while MDMs largely retain autoregressive circuitry for tasks dominated by local causal dependencies, they abandon initialized pathways for global planning tasks, exhibiting distinct rewiring characterized by increased early-layer processing. Semantically, we identify a transition from sharp, localized specialization in ARMs to distributed integration in MDMs. Through these findings, we conclude that diffusion post-training does not merely adapt model parameters but fundamentally reorganizes internal computation to support non-sequential global planning.

</details>


### [56] [Anytime Optimal Decision Tree Learning with Continuous Features](https://arxiv.org/abs/2601.14765)
*Harold Kiossou,Pierre Schaus,Siegfried Nijssen*

Main category: cs.LG

TL;DR: 提出一种基于有限差异搜索的随时完整算法，用于学习具有连续特征的最优决策树，改善现有深度优先搜索方法的随时性能。


<details>
  <summary>Details</summary>
Motivation: 现有连续特征最优决策树学习算法采用深度优先搜索，虽然能找到最优解但需要大量时间，在早期中断时往往只能得到高度不平衡的次优树，而贪婪方法如C4.5反而可能表现更好。需要一种能保证随时性能的完整算法。

Method: 提出基于有限差异搜索的随时完整方法，将计算努力更均匀地分配到整个树结构中，而不是像深度优先搜索那样先完全优化左子树再探索右子树。

Result: 实验结果表明，该方法在随时性能方面优于现有算法，能够在任何中断点都提供高质量的决策树。

Conclusion: 有限差异搜索策略能够有效改善连续特征最优决策树学习算法的随时性能，提供更均衡的搜索过程，确保在有限时间内获得更好的解决方案。

Abstract: In recent years, significant progress has been made on algorithms for learning optimal decision trees, primarily in the context of binary features. Extending these methods to continuous features remains substantially more challenging due to the large number of potential splits for each feature. Recently, an elegant exact algorithm was proposed for learning optimal decision trees with continuous features; however, the rapidly increasing computational time limits its practical applicability to shallow depths (typically 3 or 4). It relies on a depth-first search optimization strategy that fully optimizes the left subtree of each split before exploring the corresponding right subtree. While effective in finding optimal solutions given sufficient time, this strategy can lead to poor anytime behavior: when interrupted early, the best-found tree is often highly unbalanced and suboptimal. In such cases, purely greedy methods such as C4.5 may, paradoxically, yield better solutions. To address this limitation, we propose an anytime, yet complete approach leveraging limited discrepancy search, distributing the computational effort more evenly across the entire tree structure, and thus ensuring that a high-quality decision tree is available at any interruption point. Experimental results show that our approach outperforms the existing one in terms of anytime performance.

</details>


### [57] [Robustness of Mixtures of Experts to Feature Noise](https://arxiv.org/abs/2601.14792)
*Dong Sun,Rahul Nittala,Rebekka Burkholz*

Main category: cs.LG

TL;DR: MoE模型通过稀疏专家激活作为噪声过滤器，在特征噪声下比密集网络有更好的泛化误差、鲁棒性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型在实践中很成功，但除了参数规模扩展外，为什么它们能超越密集网络仍不清楚。研究者希望理解在输入具有潜在模块化结构但受到特征噪声污染的情况下，MoE的优势机制。

Method: 研究在等参数规模下，输入具有潜在模块化结构但受特征噪声污染的场景。通过理论分析证明稀疏专家激活作为噪声过滤器的作用，并在合成数据和真实语言任务上进行实证验证。

Result: 与密集估计器相比，MoE在特征噪声下获得更低的泛化误差、对扰动的更好鲁棒性以及更快的收敛速度。合成数据和真实语言任务的实证结果证实了理论见解。

Conclusion: 稀疏模块化计算通过噪声过滤机制提供了鲁棒性和效率优势，这解释了MoE模型超越密集网络的原因，而不仅仅是参数规模扩展。

Abstract: Despite their practical success, it remains unclear why Mixture of Experts (MoE) models can outperform dense networks beyond sheer parameter scaling. We study an iso-parameter regime where inputs exhibit latent modular structure but are corrupted by feature noise, a proxy for noisy internal activations. We show that sparse expert activation acts as a noise filter: compared to a dense estimator, MoEs achieve lower generalization error under feature noise, improved robustness to perturbations, and faster convergence speed. Empirical results on synthetic data and real-world language tasks corroborate the theoretical insights, demonstrating consistent robustness and efficiency gains from sparse modular computation.

</details>


### [58] [Reflecting in the Reflection: Integrating a Socratic Questioning Framework into Automated AI-Based Question Generation](https://arxiv.org/abs/2601.14798)
*Ondřej Holub,Essi Ryymin,Rodrigo Alves*

Main category: cs.LG

TL;DR: 本文提出了一种基于大语言模型的双智能体反思对话框架，通过学生-教师和教师-教育者角色的多轮对话迭代生成高质量的反思问题。


<details>
  <summary>Details</summary>
Motivation: 设计高质量的反思问题对教学很重要，但这对教师来说耗时且支持不均。需要一种自动化的方法来生成有效的反思问题，以减轻教师负担并提高教学质量。

Method: 采用反思中的反思框架，协调两个角色专业化智能体：学生-教师提出候选问题及理由，教师-教育者从清晰度、深度、相关性、参与度和概念关联性五个维度评估，通过苏格拉底式多轮对话迭代优化问题。使用GPT-4o-mini作为骨干模型，GPT-4级模型作为外部评估器。

Result: 动态停止机制结合上下文信息（学生水平和教学材料）优于固定的5步或10步迭代；双智能体协议生成的问题在相关性、深度和整体质量上显著优于单次生成基线；过长对话容易导致问题偏离或过度复杂化。

Conclusion: 反思中的反思框架能够有效生成高质量的反思问题，动态停止机制和上下文信息是关键成功因素，为教师提供了实用的自动化反思问题生成工具。

Abstract: Designing good reflection questions is pedagogically important but time-consuming and unevenly supported across teachers. This paper introduces a reflection-in-reflection framework for automated generation of reflection questions with large language models (LLMs). Our approach coordinates two role-specialized agents, a Student-Teacher and a Teacher-Educator, that engage in a Socratic multi-turn dialogue to iteratively refine a single question given a teacher-specified topic, key concepts, student level, and optional instructional materials. The Student-Teacher proposes candidate questions with brief rationales, while the Teacher-Educator evaluates them along clarity, depth, relevance, engagement, and conceptual interconnections, responding only with targeted coaching questions or a fixed signal to stop the dialogue. We evaluate the framework in an authentic lower-secondary ICT setting on the topic, using GPT-4o-mini as the backbone model and a stronger GPT- 4-class LLM as an external evaluator in pairwise comparisons of clarity, relevance, depth, and overall quality. First, we study how interaction design and context (dynamic vs.fixed iteration counts; presence or absence of student level and materials) affect question quality. Dynamic stopping combined with contextual information consistently outperforms fixed 5- or 10-step refinement, with very long dialogues prone to drift or over-complication. Second, we show that our two-agent protocol produces questions that are judged substantially more relevant and deeper, and better overall, than a one-shot baseline using the same backbone model.

</details>


### [59] [Statistical Learning Theory for Distributional Classification](https://arxiv.org/abs/2601.14818)
*Christian Fiedler*

Main category: cs.LG

TL;DR: 该论文研究了两阶段采样设置下分布输入的监督学习问题，特别关注使用支持向量机进行分布分类。通过核均值嵌入将分布映射到希尔伯特空间，建立了新的Oracle不等式，推导了收敛性和学习率结果，并针对高斯核和铰链损失提出了新的噪声假设。


<details>
  <summary>Details</summary>
Motivation: 在基于学习的医学筛查或因果学习等应用中，输入是概率分布，但在学习阶段只能获得这些分布的样本（两阶段采样设置）。现有方法使用核均值嵌入将分布映射到希尔伯特空间，然后应用标准核方法如SVM，但缺乏对这种方法的理论分析。

Method: 使用核均值嵌入将分布输入映射到再生核希尔伯特空间，然后应用支持向量机进行分类。建立了新的Oracle不等式，推导了理论保证，特别针对高斯核和铰链损失提出了新的噪声假设变体。

Result: 建立了新的Oracle不等式，证明了方法的收敛性，推导了学习率结果。针对高斯核和铰链损失的SVM，提出了新的噪声假设，在此假设下能够建立学习率。开发的技术工具（如高斯核在希尔伯特空间的新特征空间）具有独立价值。

Conclusion: 该工作为两阶段采样设置下分布输入的SVM分类提供了理论分析框架，建立了收敛性和学习率保证，提出的噪声假设和技术工具对相关研究有重要贡献。

Abstract: In supervised learning with distributional inputs in the two-stage sampling setup, relevant to applications like learning-based medical screening or causal learning, the inputs (which are probability distributions) are not accessible in the learning phase, but only samples thereof. This problem is particularly amenable to kernel-based learning methods, where the distributions or samples are first embedded into a Hilbert space, often using kernel mean embeddings (KMEs), and then a standard kernel method like Support Vector Machines (SVMs) is applied, using a kernel defined on the embedding Hilbert space. In this work, we contribute to the theoretical analysis of this latter approach, with a particular focus on classification with distributional inputs using SVMs. We establish a new oracle inequality and derive consistency and learning rate results. Furthermore, for SVMs using the hinge loss and Gaussian kernels, we formulate a novel variant of an established noise assumption from the binary classification literature, under which we can establish learning rates. Finally, some of our technical tools like a new feature space for Gaussian kernels on Hilbert spaces are of independent interest.

</details>


### [60] [From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps](https://arxiv.org/abs/2601.14848)
*Mohamed Abouras,Catherine M. Elias*

Main category: cs.LG

TL;DR: 该论文研究高速公路匝道区域的车辆行为预测，使用多层LSTM架构在ExiD无人机数据集上进行训练，在4秒预测范围内取得了良好结果。


<details>
  <summary>Details</summary>
Motivation: 高速公路匝道区域是研究不足的路段，这些区域引入了更高程度的交通交互变化。预测这些区域的车辆行为可以减少不确定性影响并提高道路安全性。

Method: 使用多层LSTM架构训练匝道区域模型，基于ExiD无人机数据集。测试了不同的预测时间范围和模型工作流程，比较了匝道区域与直线高速公路路段的差异。

Result: 在4秒预测范围内表现出良好前景：匝道区域预测准确率从约76%开始，一般高速公路场景在最大预测范围内达到94%准确率。

Conclusion: 研究表明LSTM架构在高速公路匝道区域车辆行为预测方面具有潜力，特别是在4秒预测范围内，为改善匝道区域交通安全提供了有效方法。

Abstract: On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles' behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models' workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.

</details>


### [61] [Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box Variational Inference](https://arxiv.org/abs/2601.14855)
*Baojun Che,Yifan Chen,Daniel Zhengyu Huang,Xinying Mao,Weijie Wang*

Main category: cs.LG

TL;DR: 提出了一种稳定高效的BBVI框架，结合自然梯度、指数积分器和自适应时间步长，用于高斯混合变分推断


<details>
  <summary>Details</summary>
Motivation: 黑盒变分推断(BBVI)使用高斯混合族可以灵活逼近复杂后验分布，但标准数值优化方法存在不稳定和低效的问题

Method: 结合三个关键组件：1)通过自然梯度公式进行仿射不变预处理；2)无条件保持协方差矩阵正定性的指数积分器；3)确保稳定性并适应预热和收敛阶段的自适应时间步长

Result: 对于高斯后验，证明了无噪声情况下的指数收敛性和蒙特卡洛估计下的几乎必然收敛；在多种分布、Neal's multiscale funnel和基于PDE的贝叶斯反问题上的数值实验验证了方法的有效性

Conclusion: 提出的方法具有与流形优化和镜像下降的自然联系，为BBVI提供了稳定高效的优化框架

Abstract: Black-box variational inference (BBVI) with Gaussian mixture families offers a flexible approach for approximating complex posterior distributions without requiring gradients of the target density. However, standard numerical optimization methods often suffer from instability and inefficiency. We develop a stable and efficient framework that combines three key components: (1) affine-invariant preconditioning via natural gradient formulations, (2) an exponential integrator that unconditionally preserves the positive definiteness of covariance matrices, and (3) adaptive time stepping to ensure stability and to accommodate distinct warm-up and convergence phases. The proposed approach has natural connections to manifold optimization and mirror descent. For Gaussian posteriors, we prove exponential convergence in the noise-free setting and almost-sure convergence under Monte Carlo estimation, rigorously justifying the necessity of adaptive time stepping. Numerical experiments on multimodal distributions, Neal's multiscale funnel, and a PDE-based Bayesian inverse problem for Darcy flow demonstrate the effectiveness of the proposed method.

</details>


### [62] [Strategic Doctrine Language Models (sdLM): A Learning-System Framework for Doctrinal Consistency and Geopolitical Forecasting](https://arxiv.org/abs/2601.14862)
*Olaf Yunus Laitinen Imanov,Taner Yilmaz,Derya Umut Kulali*

Main category: cs.LG

TL;DR: sdLM框架通过多文档注意力、时间编码和教义一致性层，在战略推理中实现教义一致性约束和校准不确定性，提升长期预测和计划合理性


<details>
  <summary>Details</summary>
Motivation: 现有通用大语言模型在战略推理中缺乏教义一致性约束，导致长期预测不准确和严重教义违规，需要专门框架来改善战略决策质量

Method: 结合多文档注意力机制、时间编码和教义一致性层，构建战略教义语言模型框架，确保战略推理符合教义约束并校准不确定性

Result: 在战略场景专家评分、336份教义出版物一致性评估和127个历史反事实地缘政治预测中，sdLM表现优于通用LLM基线，在长期判断上与人类专家竞争力相当

Conclusion: sdLM框架显著提升战略推理质量，减少教义违规，在长期预测中保持竞争力，为战略决策提供可靠工具

Abstract: We introduce Strategic Doctrine Language Models (sdLM), a learning-system framework for multi-document strategic reasoning with doctrinal consistency constraints and calibrated uncertainty. The approach combines multi-document attention, temporal encoding, and a doctrine-consistency layer to improve long-horizon forecasting and plan plausibility while reducing severe doctrinal violations. We evaluate sdLM using (i) expert-panel scoring of strategic scenarios (N=47), (ii) doctrine consistency on 336 doctrine publications (12,847 statements), and (iii) geopolitical forecasting on 127 historical counterfactuals (1945-2020) across 12-60 month horizons. Across these benchmarks, sdLM achieves higher strategic quality and better calibration than strong general-purpose LLM baselines, and remains competitive with human experts on long-horizon judgments. We further report ablations, scaling trends, and deployment-oriented performance/latency characteristics to clarify which components drive improvements and how they translate to operational settings.

</details>


### [63] [What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study](https://arxiv.org/abs/2601.14888)
*Keyu Lv,Manyi Zhang,Xiaobo Xia,Jingchen Ni,Shannan Yan,Xianzhi Yu,Lu Hou,Chun Yuan,Haoli Bai*

Main category: cs.LG

TL;DR: 该研究提出了针对推理模型的量化感知训练方法，通过知识蒸馏、PTQ初始化、强化学习等技术，显著提升了低比特量化下推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 推理模型在复杂任务上表现出色，但推理速度慢且token效率低。传统的后训练量化方法在低比特设置下会导致推理任务准确率大幅下降，需要更有效的量化方案。

Method: 采用量化感知训练方法，结合知识蒸馏作为训练目标，使用PTQ作为QAT的初始化，探索强化学习在量化模型中的应用，并优化PTQ校准域与QAT训练域的对齐。

Result: 提出的Reasoning-QAT方法在多个LLM骨干网络和推理数据集上持续优于最先进的PTQ方法。例如在Qwen3-0.6B上，MATH-500数据集上超越GPTQ 44.53%，并在2比特量化下恢复性能。

Conclusion: 通过系统化的量化感知训练方法，可以有效提升推理模型的量化性能，在保持准确率的同时显著提升推理效率，为低比特量化下的推理模型部署提供了有效解决方案。

Abstract: Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.

</details>


### [64] [Tailoring Adverse Event Prediction in Type 1 Diabetes with Patient-Specific Deep Learning Models](https://arxiv.org/abs/2601.14917)
*Giorgia Rigamonti,Mirko Paolo Barbato,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的个性化血糖预测方法，通过患者特定数据提高预测准确性，相比传统通用模型能更好地处理个体差异，为下一代糖尿病管理提供支持。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴血糖监测设备和移动健康应用的普及，准确的血糖预测对于增强自动化胰岛素输送和决策支持系统至关重要。传统通用模型无法充分处理个体差异，需要个性化方法来提高预测精度和实时响应能力。

Method: 采用深度学习框架进行个性化血糖预测，比较留一受试者交叉验证与微调策略。使用多模态患者特定数据（而非仅CGM数据），并进行消融研究以确定有效个性化所需的最小训练数据量。

Result: 个性化模型显著改善了不良事件的预测能力，能够实现更精确和及时的干预。多模态患者特定方法优于传统仅使用CGM的方法，消融研究确定了有效个性化所需的最小数据量。

Conclusion: 自适应、个性化的血糖预测模型在下一代糖尿病管理中具有巨大潜力，特别是在可穿戴和移动健康平台中，能够增强面向消费者的糖尿病护理解决方案。

Abstract: Effective management of Type 1 Diabetes requires continuous glucose monitoring and precise insulin adjustments to prevent hyperglycemia and hypoglycemia. With the growing adoption of wearable glucose monitors and mobile health applications, accurate blood glucose prediction is essential for enhancing automated insulin delivery and decision-support systems. This paper presents a deep learning-based approach for personalized blood glucose prediction, leveraging patient-specific data to improve prediction accuracy and responsiveness in real-world scenarios. Unlike traditional generalized models, our method accounts for individual variability, enabling more effective subject-specific predictions. We compare Leave-One-Subject-Out Cross-Validation with a fine-tuning strategy to evaluate their ability to model patient-specific dynamics. Results show that personalized models significantly improve the prediction of adverse events, enabling more precise and timely interventions in real-world scenarios. To assess the impact of patient-specific data, we conduct experiments comparing a multimodal, patient-specific approach against traditional CGM-only methods. Additionally, we perform an ablation study to investigate model performance with progressively smaller training sets, identifying the minimum data required for effective personalization-an essential consideration for real-world applications where extensive data collection is often challenging. Our findings underscore the potential of adaptive, personalized glucose prediction models for advancing next-generation diabetes management, particularly in wearable and mobile health platforms, enhancing consumer-oriented diabetes care solutions.

</details>


### [65] [Multimodal Rumor Detection Enhanced by External Evidence and Forgery Features](https://arxiv.org/abs/2601.14954)
*Han Li,Hua Sun*

Main category: cs.LG

TL;DR: 提出一种结合外部证据和伪造特征的多模态谣言检测模型，通过双对比学习检测图像文本语义不一致，在微博和Twitter数据集上优于主流基线方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中混合图文帖子传播谣言，特别是深度语义不匹配谣言（图像文本表面一致但实际不一致）难以检测。现有多模态谣言检测方法存在特征提取有限、噪声对齐、融合策略不灵活等问题，且忽略了验证复杂谣言所需的外部事实证据。

Method: 使用ResNet34视觉编码器和BERT文本编码器，通过傅里叶变换提取频域痕迹和压缩伪影的伪造特征模块。BLIP生成图像描述桥接图像和文本语义空间。双对比学习模块计算文本-图像和文本-描述对的对比损失，检测语义不一致。门控自适应特征缩放融合机制动态调整多模态融合并减少冗余。

Result: 在微博和Twitter数据集上的实验表明，该模型在宏观准确率、召回率和F1分数上优于主流基线方法。

Conclusion: 提出的结合外部证据和伪造特征的多模态谣言检测模型能有效检测深度语义不匹配谣言，通过双对比学习和自适应融合机制提高了检测性能。

Abstract: Social media increasingly disseminates information through mixed image text posts, but rumors often exploit subtle inconsistencies and forged content, making detection based solely on post content difficult. Deep semantic mismatch rumors, which superficially align images and texts, pose particular challenges and threaten online public opinion. Existing multimodal rumor detection methods improve cross modal modeling but suffer from limited feature extraction, noisy alignment, and inflexible fusion strategies, while ignoring external factual evidence necessary for verifying complex rumors. To address these limitations, we propose a multimodal rumor detection model enhanced with external evidence and forgery features. The model uses a ResNet34 visual encoder, a BERT text encoder, and a forgery feature module extracting frequency-domain traces and compression artifacts via Fourier transformation. BLIP-generated image descriptions bridge image and text semantic spaces. A dual contrastive learning module computes contrastive losses between text image and text description pairs, improving detection of semantic inconsistencies. A gated adaptive feature-scaling fusion mechanism dynamically adjusts multimodal fusion and reduces redundancy. Experiments on Weibo and Twitter datasets demonstrate that our model outperforms mainstream baselines in macro accuracy, recall, and F1 score.

</details>


### [66] [Improving Regret Approximation for Unsupervised Dynamic Environment Generation](https://arxiv.org/abs/2601.14957)
*Harry Mead,Bruno Lacerda,Jakob Foerster,Nick Hawes*

Main category: cs.LG

TL;DR: 提出DEGen方法和MNA指标，解决UED中的信用分配问题，提升大规模环境下的课程生成效果


<details>
  <summary>Details</summary>
Motivation: 当前UED方法在环境参数空间大时面临信用分配困难，难以识别真正具有挑战性的训练关卡，限制了强化学习智能体的泛化能力提升

Method: 提出DEGen（动态环境生成）方法，通过更密集的关卡生成器奖励信号改善信用分配；引入MNA（最大化负优势）作为新的遗憾近似指标，更好地识别高难度关卡

Result: 实验证明MNA优于现有遗憾近似方法，DEGen与MNA结合后在大规模环境中表现显著优于现有方法

Conclusion: DEGen和MNA有效解决了UED中的信用分配和挑战性关卡识别问题，使课程生成方法能够扩展到更大规模的环境

Abstract: Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED.

</details>


### [67] [InstructTime++: Time Series Classification with Multimodal Language Modeling via Implicit Feature Enhancement](https://arxiv.org/abs/2601.14968)
*Mingyue Cheng,Xiaoyu Tao,Huajian Zhang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: InstructTime++：将时间序列分类重构为多模态生成任务，通过离散化时间序列、跨模态对齐和隐式特征建模，利用语言模型生成文本类别标签。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法采用判别式范式，直接将输入序列映射到one-hot编码的类别标签。这种方法难以融入上下文特征，也无法捕捉类别间的语义关系。

Method: 1) 将时间序列分类重构为多模态生成任务：连续数值序列、上下文文本特征和任务指令作为多模态输入，类别标签作为语言模型生成的文本输出；2) 时间序列离散化模块将连续序列转换为离散时间标记；3) 对齐投影层和生成式自监督预训练策略增强跨模态表示对齐；4) InstructTime++扩展：通过统计特征提取和视觉语言图像描述等工具挖掘隐式模式，转换为文本描述进行集成。

Result: 在多个基准数据集上的广泛实验表明，InstructTime++具有优越的性能。

Conclusion: 将时间序列分类重构为多模态生成任务，结合隐式特征建模，能够有效解决传统判别式方法的局限性，提升分类性能。

Abstract: Most existing time series classification methods adopt a discriminative paradigm that maps input sequences directly to one-hot encoded class labels. While effective, this paradigm struggles to incorporate contextual features and fails to capture semantic relationships among classes. To address these limitations, we propose InstructTime, a novel framework that reformulates time series classification as a multimodal generative task. Specifically, continuous numerical sequences, contextual textual features, and task instructions are treated as multimodal inputs, while class labels are generated as textual outputs by tuned language models. To bridge the modality gap, InstructTime introduces a time series discretization module that converts continuous sequences into discrete temporal tokens, together with an alignment projection layer and a generative self-supervised pre-training strategy to enhance cross-modal representation alignment. Building upon this framework, we further propose InstructTime++, which extends InstructTime by incorporating implicit feature modeling to compensate for the limited inductive bias of language models. InstructTime++ leverages specialized toolkits to mine informative implicit patterns from raw time series and contextual inputs, including statistical feature extraction and vision-language-based image captioning, and translates them into textual descriptions for seamless integration. Extensive experiments on multiple benchmark datasets demonstrate the superior performance of InstructTime++.

</details>


### [68] [Fine-Grained Traceability for Transparent ML Pipelines](https://arxiv.org/abs/2601.14971)
*Liping Chen,Mujie Liu,Haytham Fayek*

Main category: cs.LG

TL;DR: FG-Trac是一个模型无关的框架，为机器学习流水线提供可验证的细粒度样本级追踪能力，通过加密承诺确保数据使用历史的完整性和可审计性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习系统通常是多阶段流水线，但现有透明度机制仅在模型层面运作，无法追踪单个样本在流水线中的使用情况。缺乏可验证的样本级追踪使得实践者和用户无法确定特定样本是否被使用、何时被处理，以及相应记录是否随时间保持完整。

Method: FG-Trac定义了一个明确的机制来捕获和验证预处理和训练过程中的样本生命周期事件，基于训练检查点计算贡献分数，并将这些追踪锚定到防篡改的加密承诺中。该框架无需修改模型架构或训练目标即可集成。

Result: 在卷积神经网络和多模态图学习流水线上的实验表明，FG-Trac能够保持预测性能，同时使机器学习系统能够提供关于单个样本在模型执行过程中如何被使用和传播的可验证证据。

Conclusion: FG-Trac为机器学习流水线建立了可验证的细粒度样本级追踪能力，通过实际计算开销重建完整且可审计的数据使用历史，解决了现有透明度机制的局限性。

Abstract: Modern machine learning systems are increasingly realised as multistage pipelines, yet existing transparency mechanisms typically operate at a model level: they describe what a system is and why it behaves as it does, but not how individual data samples are operationally recorded, tracked, and verified as they traverse the pipeline. This absence of verifiable, sample-level traceability leaves practitioners and users unable to determine whether a specific sample was used, when it was processed, or whether the corresponding records remain intact over time. We introduce FG-Trac, a model-agnostic framework that establishes verifiable, fine-grained sample-level traceability throughout machine learning pipelines. FG-Trac defines an explicit mechanism for capturing and verifying sample lifecycle events across preprocessing and training, computes contribution scores explicitly grounded in training checkpoints, and anchors these traces to tamper-evident cryptographic commitments. The framework integrates without modifying model architectures or training objectives, reconstructing complete and auditable data-usage histories with practical computational overhead. Experiments on a canonical convolutional neural network and a multimodal graph learning pipeline demonstrate that FG-Trac preserves predictive performance while enabling machine learning systems to furnish verifiable evidence of how individual samples were used and propagated during model execution.

</details>


### [69] [Lineup Regularized Adjusted Plus-Minus (L-RAPM): Basketball Lineup Ratings with Informed Priors](https://arxiv.org/abs/2601.15000)
*Christos Petridis,Konstantinos Pelechrinis*

Main category: cs.LG

TL;DR: 论文提出L-RAPM回归方法，通过控制对手阵容并利用球员信息，解决篮球阵容评估中数据稀疏和噪声大的问题，相比现有基线方法具有更好的预测能力。


<details>
  <summary>Details</summary>
Motivation: 篮球等体育运动中，识别表现良好的球员组合（阵容）是体育分析的重要任务。主要挑战在于比赛中频繁换人导致数据高度稀疏，例如NBA球队一个赛季会使用600多个阵容，每个阵容平均只有25-30次进攻回合，这使得收集的统计数据噪声大、预测价值低，而现有公开研究尚未解决这一问题。

Method: 提出基于回归的方法L-RAPM，该方法控制每个阵容面对的对手因素，同时利用组成阵容的球员信息来评估阵容表现。

Result: 实验表明，L-RAPM相比当前使用的基线方法具有更好的预测能力，并且随着阵容样本量变小，这种改进效果更加明显。

Conclusion: L-RAPM方法有效解决了篮球阵容评估中的数据稀疏问题，通过综合考虑对手因素和球员信息，提供了更准确的阵容表现预测，特别是在小样本情况下优势更显著。

Abstract: Identifying combinations of players (that is, lineups) in basketball - and other sports - that perform well when they play together is one of the most important tasks in sports analytics. One of the main challenges associated with this task is the frequent substitutions that occur during a game, which results in highly sparse data. In particular, a National Basketball Association (NBA) team will use more than 600 lineups during a season, which translates to an average lineup having seen the court in approximately 25-30 possessions. Inevitably, any statistics that one collects for these lineups are going to be noisy, with low predictive value. Yet, there is no existing work (in the public at least) that addresses this problem. In this work, we propose a regression-based approach that controls for the opposition faced by each lineup, while it also utilizes information about the players making up the lineups. Our experiments show that L-RAPM provides improved predictive power than the currently used baseline, and this improvement increases as the sample size for the lineups gets smaller.

</details>


### [70] [RadixMLP -- Intra-batch Deduplication for Causal Transformers](https://arxiv.org/abs/2601.15013)
*Michael Feil,Julius Lipp*

Main category: cs.LG

TL;DR: RadixMLP通过前缀压缩技术消除因果Transformer模型批量推理中共享前缀的冗余计算，实现1.44-1.59倍加速


<details>
  <summary>Details</summary>
Motivation: 因果Transformer模型批量推理中经常处理具有共享前缀的序列（如系统提示、few-shot示例等），传统推理引擎独立处理每个序列，导致对相同MLP激活的冗余计算

Method: RadixMLP利用MLP、LayerNorm、线性投影和嵌入的位置特性，将批次映射到前缀树，将共享段压缩为紧凑表示进行位置计算，仅在注意力边界处散射结果

Result: 在MS~MARCO v1.1的Qwen3模型（0.6B到8B参数）重排序任务中实现1.44-1.59倍加速，在具有更长共享前缀的合成基准测试中达到5倍加速

Conclusion: RadixMLP通过消除共享前缀的冗余计算，显著提升因果Transformer模型批量推理效率，且无状态、单前向传播完成

Abstract: Batch inference workloads for causal transformer models frequently process sequences that share common prefixes, such as system prompts, few-shot examples, or shared queries. Standard inference engines treat each sequence independently, redundantly recomputing identical MLP activations for every copy of the shared prefix. We introduce RadixMLP, a technique that exploits the position-wise nature of MLPs, LayerNorms, linear projections, and embeddings to eliminate this redundancy. RadixMLP dynamically maps batches to a prefix trie, gathering shared segments into a compressed representation for position-wise computation and scattering results back only at attention boundaries. RadixMLP is stateless and operates within a single forward pass. In end-to-end serving benchmarks on MS~MARCO v1.1 with Qwen3 models (0.6B to 8B parameters), RadixMLP achieves 1.44-1.59$\times$ speedups in realistic reranking workloads, with up to $5\times$ speedups on synthetic benchmarks with longer shared prefixes. Our code is available at https://github.com/michaelfeil/radix-mlp.

</details>


### [71] [Plug-and-Play Benchmarking of Reinforcement Learning Algorithms for Large-Scale Flow Control](https://arxiv.org/abs/2601.15015)
*Jannis Becktepe,Aleksandra Franz,Nils Thuerey,Sebastian Peitz*

Main category: cs.LG

TL;DR: FluidGym：首个独立的、完全可微的强化学习主动流控制基准套件，基于PyTorch和GPU加速的PICT求解器，无需外部CFD软件，提供标准化评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在主动流控制中的研究难以评估进展，因为使用异构的观测与执行方案、数值设置和评估协议。当前基准依赖外部CFD求解器、不完全可微，且3D和多智能体支持有限。

Method: 基于PyTorch和GPU加速的PICT求解器构建完全可微的基准套件，在单一Python堆栈中运行，无需外部CFD软件，提供标准化评估协议，包含PPO和SAC基线结果。

Result: 开发了FluidGym基准套件，包含所有环境、数据集和训练模型作为公共资源，支持控制方法的系统比较，为基于学习的流控制研究建立可扩展基础。

Conclusion: FluidGym克服了现有基准的限制，提供了首个独立的、完全可微的强化学习主动流控制基准，促进该领域研究的系统比较和未来发展。

Abstract: Reinforcement learning (RL) has shown promising results in active flow control (AFC), yet progress in the field remains difficult to assess as existing studies rely on heterogeneous observation and actuation schemes, numerical setups, and evaluation protocols. Current AFC benchmarks attempt to address these issues but heavily rely on external computational fluid dynamics (CFD) solvers, are not fully differentiable, and provide limited 3D and multi-agent support. To overcome these limitations, we introduce FluidGym, the first standalone, fully differentiable benchmark suite for RL in AFC. Built entirely in PyTorch on top of the GPU-accelerated PICT solver, FluidGym runs in a single Python stack, requires no external CFD software, and provides standardized evaluation protocols. We present baseline results with PPO and SAC and release all environments, datasets, and trained models as public resources. FluidGym enables systematic comparison of control methods, establishes a scalable foundation for future research in learning-based flow control, and is available at https://github.com/safe-autonomous-systems/fluidgym.

</details>


### [72] [Mixture-of-Experts Models in Vision: Routing, Optimization, and Generalization](https://arxiv.org/abs/2601.15021)
*Adam Rokah,Daniel Veress,Caleb Caulk,Sourav Sharan*

Main category: cs.LG

TL;DR: 研究在图像分类任务中比较了密集、SoftMoE和SparseMoE三种分类头架构，发现MoE变体在CIFAR10上略优于密集基线，但条件路由在当前硬件规模下未能实现推理加速。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在图像分类任务中的行为，关注预测性能、专家利用率和泛化能力，填补MoE在计算机视觉领域的研究空白。

Method: 在CIFAR10数据集上比较密集、SoftMoE和SparseMoE分类头，在可比模型容量下评估性能；使用正则化防止专家崩溃；通过Hessian矩阵特征值和迹分析泛化性；进行损失曲面扰动分析；评估推理效率。

Result: 两种MoE变体验证准确率略高于密集基线，专家利用率保持平衡；SoftMoE表现出更高的尖锐度，而密集和SparseMoE处于相似的曲率状态；条件路由在当前硬件规模下未能实现推理加速。

Conclusion: MoE架构在图像分类中能获得轻微性能提升，但尖锐度指标与泛化性能不完全相关；稀疏MoE的理论效率优势在当前硬件规模下难以实现，需要进一步优化。

Abstract: Mixture-of-Experts (MoE) architectures enable conditional computation by routing inputs to multiple expert subnetworks and are often motivated as a mechanism for scaling large language models. In this project, we instead study MoE behavior in an image classification setting, focusing on predictive performance, expert utilization, and generalization. We compare dense, SoftMoE, and SparseMoE classifier heads on the CIFAR10 dataset under comparable model capacity. Both MoE variants achieve slightly higher validation accuracy than the dense baseline while maintaining balanced expert utilization through regularization, avoiding expert collapse. To analyze generalization, we compute Hessian-based sharpness metrics at convergence, including the largest eigenvalue and trace of the loss Hessian, evaluated on both training and test data. We find that SoftMoE exhibits higher sharpness by these metrics, while Dense and SparseMoE lie in a similar curvature regime, despite all models achieving comparable generalization performance. Complementary loss surface perturbation analyses reveal qualitative differences in non-local behavior under finite parameter perturbations between dense and MoE models, which help contextualize curvature-based measurements without directly explaining validation accuracy. We further evaluate empirical inference efficiency and show that naively implemented conditional routing does not yield inference speedups on modern hardware at this scale, highlighting the gap between theoretical and realized efficiency in sparse MoE models.

</details>


### [73] [A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem](https://arxiv.org/abs/2601.15038)
*Mertcan Daysalilar,Fuat Uyguroglu,Gabriel Nicolosi,Adam Meyers*

Main category: cs.LG

TL;DR: 提出基于课程学习的深度强化学习框架(CB-DRL)，通过三阶段渐进式训练解决电动汽车路径规划问题的训练不稳定问题，在小规模实例训练后能泛化到大规模问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在处理电动汽车路径规划问题时面临训练不稳定、难以收敛和泛化的问题，特别是在约束密集的情况下。需要一种更稳定的训练框架来提升模型性能。

Method: 采用三阶段课程学习框架：阶段A学习距离和车队优化，阶段B学习电池管理，阶段C学习完整问题。使用改进的近端策略优化算法，结合异构图注意力编码器、全局-局部注意力机制和特征线性调制。

Result: 仅用10个客户的小规模实例训练，模型能泛化到5-100个客户的未见实例，在中规模问题上显著优于基线方法，在分布外实例上实现高可行性和竞争性解质量。

Conclusion: 课程学习框架有效解决了深度强化学习在电动汽车路径规划中的训练不稳定问题，实现了神经网络速度与操作可靠性的平衡，为复杂约束优化问题提供了稳定训练方案。

Abstract: The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.

</details>


### [74] [HyperNet-Adaptation for Diffusion-Based Test Case Generation](https://arxiv.org/abs/2601.15041)
*Oliver Weißl,Vincenzo Riccio,Severin Kacianka,Andrea Stocco*

Main category: cs.LG

TL;DR: HyNeA是一种基于超网络的扩散模型可控生成方法，用于高效生成深度学习系统的现实故障测试用例，无需依赖特定数据集或架构条件机制。


<details>
  <summary>Details</summary>
Motivation: 传统梯度对抗攻击产生的微小扰动不反映现实故障，主要评估鲁棒性而非功能行为；现有生成测试方法局限于简单数据集或受限输入域；扩散模型虽能合成高保真图像，但计算成本高且可控性有限，难以用于大规模测试。

Method: HyNeA使用超网络实现数据集无关的可控性，通过超网络直接控制扩散生成过程，无需架构特定条件机制或数据集驱动的微调；采用支持实例级调优的训练策略，无需包含类似故障示例的数据集即可识别引发故障的测试用例。

Result: HyNeA相比现有生成测试方法提高了可控性和测试多样性；在故障标记训练数据不可用的领域具有良好泛化能力；能以比基于搜索的方法显著更低的计算成本生成目标现实故障案例。

Conclusion: HyNeA提供了一种高效、可控的生成测试方法，能够大规模生成现实故障测试用例，为深度学习系统的可靠性评估提供了新工具。

Abstract: The increasing deployment of deep learning systems requires systematic evaluation of their reliability in real-world scenarios. Traditional gradient-based adversarial attacks introduce small perturbations that rarely correspond to realistic failures and mainly assess robustness rather than functional behavior. Generative test generation methods offer an alternative but are often limited to simple datasets or constrained input domains. Although diffusion models enable high-fidelity image synthesis, their computational cost and limited controllability restrict their applicability to large-scale testing. We present HyNeA, a generative testing method that enables direct and efficient control over diffusion-based generation. HyNeA provides dataset-free controllability through hypernetworks, allowing targeted manipulation of the generative process without relying on architecture-specific conditioning mechanisms or dataset-driven adaptations such as fine-tuning. HyNeA employs a distinct training strategy that supports instance-level tuning to identify failure-inducing test cases without requiring datasets that explicitly contain examples of similar failures. This approach enables the targeted generation of realistic failure cases at substantially lower computational cost than search-based methods. Experimental results show that HyNeA improves controllability and test diversity compared to existing generative test generators and generalizes to domains where failure-labeled training data is unavailable.

</details>


### [75] [LoRAP: Low-Rank Aggregation Prompting for Quantized Graph Neural Networks Training](https://arxiv.org/abs/2601.15079)
*Chenyu Liu,Haige Li,Luca Rossi*

Main category: cs.LG

TL;DR: 提出LoRAP方法，通过低秩聚合提示优化GNN量化训练，在低比特量化下提升性能且计算开销小


<details>
  <summary>Details</summary>
Motivation: GNN量化可减少模型大小并加速推理，但现有方法在量化聚合操作时存在性能损失问题。仅提示节点特征无法使量化聚合结果达到最优，需要更有效的优化方法

Method: 提出低秩聚合提示(LoRAP)，为每个聚合特征注入轻量级、输入相关的提示，优化量化聚合结果。该方法在4个主流QAT框架和9个图数据集上进行评估

Result: LoRAP能持续提升低比特量化GNN的性能，同时引入的计算开销极小，在各种QAT框架和图数据集上表现一致

Conclusion: LoRAP通过优化量化聚合过程，有效解决了GNN量化中的性能损失问题，为资源受限环境下的高效GNN部署提供了实用解决方案

Abstract: Graph Neural Networks (GNNs) are neural networks that aim to process graph data, capturing the relationships and interactions between nodes using the message-passing mechanism. GNN quantization has emerged as a promising approach for reducing model size and accelerating inference in resource-constrained environments. Compared to quantization in LLMs, quantizing graph features is more emphasized in GNNs. Inspired by the above, we propose to leverage prompt learning, which manipulates the input data, to improve the performance of quantization-aware training (QAT) for GNNs. To mitigate the issue that prompting the node features alone can only make part of the quantized aggregation result optimal, we introduce Low-Rank Aggregation Prompting (LoRAP), which injects lightweight, input-dependent prompts into each aggregated feature to optimize the results of quantized aggregations. Extensive evaluations on 4 leading QAT frameworks over 9 graph datasets demonstrate that LoRAP consistently enhances the performance of low-bit quantized GNNs while introducing a minimal computational overhead.

</details>


### [76] [Memory Retention Is Not Enough to Master Memory Tasks in Reinforcement Learning](https://arxiv.org/abs/2601.15086)
*Oleg Shchendrigin,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 该论文提出了一个评估强化学习智能体记忆重写能力的基准，发现传统循环模型在现代结构化记忆和基于Transformer的智能体失败的情况下表现出更好的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的有效决策需要记忆既稳定又适应性强：环境会随时间变化，智能体必须在长时间内保留相关信息，同时在情况变化时更新或覆盖过时内容。现有的强化学习基准和记忆增强智能体主要关注记忆保留，而同样重要的记忆重写能力在很大程度上未被探索。

Method: 引入了一个基准，明确测试部分可观测性下的持续记忆更新（即智能体必须依赖记忆而非当前观察的自然设置），并使用该基准比较循环、基于Transformer和结构化记忆架构。

Result: 实验表明，尽管经典循环模型简单，但在记忆重写任务中表现出比现代结构化记忆更大的灵活性和鲁棒性。结构化记忆仅在狭窄条件下成功，而基于Transformer的智能体通常在简单保留案例之外失败。

Conclusion: 这些发现揭示了当前方法的基本局限性，强调了需要平衡稳定保留与适应性更新的记忆机制。该工作突出了这一被忽视的挑战，引入了评估基准，并为设计具有明确且可训练遗忘机制的未来强化学习智能体提供了见解。

Abstract: Effective decision-making in the real world depends on memory that is both stable and adaptive: environments change over time, and agents must retain relevant information over long horizons while also updating or overwriting outdated content when circumstances shift. Existing Reinforcement Learning (RL) benchmarks and memory-augmented agents focus primarily on retention, leaving the equally critical ability of memory rewriting largely unexplored. To address this gap, we introduce a benchmark that explicitly tests continual memory updating under partial observability, i.e. the natural setting where an agent must rely on memory rather than current observations, and use it to compare recurrent, transformer-based, and structured memory architectures. Our experiments reveal that classic recurrent models, despite their simplicity, demonstrate greater flexibility and robustness in memory rewriting tasks than modern structured memories, which succeed only under narrow conditions, and transformer-based agents, which often fail beyond trivial retention cases. These findings expose a fundamental limitation of current approaches and emphasize the necessity of memory mechanisms that balance stable retention with adaptive updating. Our work highlights this overlooked challenge, introduces benchmarks to evaluate it, and offers insights for designing future RL agents with explicit and trainable forgetting mechanisms. Code: https://quartz-admirer.github.io/Memory-Rewriting/

</details>


### [77] [Field-Space Autoencoder for Scalable Climate Emulators](https://arxiv.org/abs/2601.15102)
*Johannes Meuer,Maximilian Witte,Étiénne Plésiat,Thomas Ludwig,Christopher Kadow*

Main category: cs.LG

TL;DR: 提出Field-Space Autoencoder，一种基于球面压缩模型的可扩展气候模拟框架，通过Field-Space Attention处理原生气候模型输出，避免几何失真，支持零样本超分辨率和生成扩散模型训练。


<details>
  <summary>Details</summary>
Motivation: 千米尺度地球系统模型计算成本高且输出数据量巨大（PB级），限制了其在概率风险评估等应用中的实用性，需要开发更高效的模拟框架。

Method: 开发Field-Space Autoencoder框架，使用Field-Space Attention直接在球面数据上操作，避免将球面数据强制映射到欧几里得网格造成的几何失真。通过结构化压缩场作为生成模拟的基线，支持零样本超分辨率，并在压缩场上训练生成扩散模型。

Result: 该方法比卷积基线更好地保留物理结构，能够将低分辨率大集合数据和稀缺高分辨率数据映射到共享表示中，同时从丰富的低分辨率数据学习内部变异性，从稀疏高分辨率数据学习精细尺度物理。

Conclusion: 该工作弥合了高容量低分辨率集合统计与稀缺高分辨率物理细节之间的差距，为可扩展气候模拟提供了有效框架。

Abstract: Kilometer-scale Earth system models are essential for capturing local climate change. However, these models are computationally expensive and produce petabyte-scale outputs, which limits their utility for applications such as probabilistic risk assessment. Here, we present the Field-Space Autoencoder, a scalable climate emulation framework based on a spherical compression model that overcomes these challenges. By utilizing Field-Space Attention, the model efficiently operates on native climate model output and therefore avoids geometric distortions caused by forcing spherical data onto Euclidean grids. This approach preserves physical structures significantly better than convolutional baselines. By producing a structured compressed field, it serves as a good baseline for downstream generative emulation. In addition, the model can perform zero-shot super-resolution that maps low-resolution large ensembles and scarce high-resolution data into a shared representation. We train a generative diffusion model on these compressed fields. The model can simultaneously learn internal variability from abundant low-resolution data and fine-scale physics from sparse high-resolution data. Our work bridges the gap between the high volume of low-resolution ensemble statistics and the scarcity of high-resolution physical detail.

</details>


### [78] [Auditing Language Model Unlearning via Information Decomposition](https://arxiv.org/abs/2601.15111)
*Anmol Goel,Alan Ritter,Iryna Gurevych*

Main category: cs.LG

TL;DR: 当前机器学习遗忘方法存在关键局限：尽管遗忘算法看似成功，但被遗忘数据的信息仍能从模型内部表示中线性解码出来。


<details>
  <summary>Details</summary>
Motivation: 揭示当前语言模型遗忘方法的关键局限性，即被遗忘数据的信息实际上仍残留在模型表示中，这可能导致隐私泄露风险。

Method: 引入基于部分信息分解（PID）的可解释信息论框架，通过比较遗忘前后的模型表示，将与被遗忘数据的互信息分解为不同成分，形式化"已遗忘知识"和"残留知识"的概念。

Result: 分析表明，冗余信息（在两个模型间共享）构成了遗忘后仍存在的残留知识，这与已知对抗性重建攻击的易感性相关。基于此提出表示层面的风险评分，可在推理时指导对敏感输入的回避。

Conclusion: 该工作为遗忘提供了原则性的表示层面审计方法，为语言模型的安全部署提供了理论洞见和实用工具。

Abstract: We expose a critical limitation in current approaches to machine unlearning in language models: despite the apparent success of unlearning algorithms, information about the forgotten data remains linearly decodable from internal representations. To systematically assess this discrepancy, we introduce an interpretable, information-theoretic framework for auditing unlearning using Partial Information Decomposition (PID). By comparing model representations before and after unlearning, we decompose the mutual information with the forgotten data into distinct components, formalizing the notions of unlearned and residual knowledge. Our analysis reveals that redundant information, shared across both models, constitutes residual knowledge that persists post-unlearning and correlates with susceptibility to known adversarial reconstruction attacks. Leveraging these insights, we propose a representation-based risk score that can guide abstention on sensitive inputs at inference time, providing a practical mechanism to mitigate privacy leakage. Our work introduces a principled, representation-level audit for unlearning, offering theoretical insight and actionable tools for safer deployment of language models.

</details>


### [79] [Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.15124)
*Haonan Yuan,Qingyun Sun,Jiacheng Tao,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: RAG-GFM：基于检索增强生成的图基础模型，通过将知识从参数中卸载到外部存储，解决现有图基础模型的内存瓶颈问题，实现更好的可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型（GFMs）存在内存瓶颈问题：它们试图将所有知识编码到模型参数中，这限制了语义容量，引入了严重的损失压缩和冲突，并且将图表示与知识纠缠在一起，阻碍了高效适应，影响了可扩展性和可解释性。

Method: 提出RAG-GFM模型，采用检索增强生成方法。构建双模态统一检索模块：基于前缀结构文本的语义存储和基于中心性基元的结构存储。设计双视图对齐目标来保留异构信息。通过上下文增强技术，用检索到的文本和基元作为上下文证据来丰富支持实例。

Result: 在五个基准图数据集上的广泛实验表明，RAG-GFM在跨领域节点和图分类任务中，持续优于13个最先进的基线方法，实现了卓越的有效性和效率。

Conclusion: RAG-GFM通过将知识从参数中卸载并补充参数化学习，解决了图基础模型的内存瓶颈问题，为图学习提供了更可扩展、可适应和可解释的解决方案。

Abstract: Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.

</details>


### [80] [DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search](https://arxiv.org/abs/2601.15127)
*Bostan Khan,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: DeepFedNAS是一个新颖的联邦神经架构搜索框架，通过两阶段方法解决FedNAS的两个关键瓶颈：使用帕累托最优架构缓存指导超网训练，以及无需预测器的快速子网搜索，实现精度提升、效率优化和61倍速度加速。


<details>
  <summary>Details</summary>
Motivation: 联邦神经架构搜索(FedNAS)面临两个关键瓶颈：无指导的超网训练导致次优模型，以及后训练子网发现需要多小时的昂贵流程。这使得硬件感知的联邦学习部署不切实际。

Method: 提出两阶段框架：1) 联邦帕累托最优超网训练：使用预计算的帕累托最优架构缓存作为智能课程来优化共享超网权重；2) 无预测器搜索方法：利用多目标适应度函数作为零成本精度代理，实现秒级子网发现。

Result: 在CIFAR-100上实现最高1.21%的绝对精度提升，具有优越的参数和通信效率，后训练搜索流程时间加速约61倍，从超过20小时减少到约20分钟，单个子网搜索仅需20秒。

Conclusion: DeepFedNAS通过解决FedNAS的关键瓶颈，使硬件感知的联邦学习部署变得即时且实用，为隐私保护的自动化模型设计提供了高效解决方案。

Abstract: Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS

</details>


### [81] [CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning](https://arxiv.org/abs/2601.15141)
*Tianshi Xu,Yuteng Chen,Meng Li*

Main category: cs.LG

TL;DR: CLEANER提出了一种利用LLM内在自校正能力来净化强化学习轨迹的方法，通过相似性感知自适应回滚机制，用成功的自校正替换失败步骤，从而解决参数受限模型的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 参数受限的LLM（4B-7B）在强化学习探索阶段经常出现执行失败，产生噪声轨迹，导致信用分配问题：错误动作与成功结果一起被强化。现有方法面临两难：密集奖励容易引发奖励黑客行为，而超采样则计算成本过高。

Method: CLEANER利用模型内在自校正能力，在数据收集过程中直接消除错误污染。核心是相似性感知自适应回滚（SAAR）机制，通过回顾性地用成功的自校正替换失败步骤，自适应地调节替换粒度（从浅层执行修复到深层推理替换），构建纯净轨迹。

Result: 在AIME24/25、GPQA和LiveCodeBench上的实验显示，相比基线平均准确率分别提升6%、3%和5%。特别值得注意的是，CLEANER仅用三分之一训练步骤就达到了最先进性能。

Conclusion: 轨迹净化是高效智能体强化学习的可扩展解决方案，通过训练自净化路径，模型能够内化正确的推理模式而非错误恢复循环。

Abstract: Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model's intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub

</details>


### [82] [Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data](https://arxiv.org/abs/2601.15158)
*Yuval Ran-Milo,Yotam Alexander,Shahar Mendel,Nadav Cohen*

Main category: cs.LG

TL;DR: 论文分析了Transformer通过强化学习稀疏奖励训练时，如何自发产生链式推理能力，揭示了梯度流驱动模型学习结构化算法的机制


<details>
  <summary>Details</summary>
Motivation: 理解基于结果监督的强化学习训练中，稀疏奖励如何驱动梯度下降发现系统性推理步骤（链式思维）的机制

Method: 分析单层Transformer在合成图遍历任务上的梯度流动态，该任务需要链式推理但允许简单迭代解；理论证明梯度流驱动模型收敛到结构化算法

Result: 证明仅基于最终答案正确性训练，梯度流能使模型收敛到可解释的迭代图遍历算法；识别"简单示例"（需要较少推理步骤的实例）在分布中的关键作用

Conclusion: 训练分布中简单实例的足够质量是模型学习可泛化推理策略的关键，理论发现在合成数据和真实语言模型的数学推理任务中得到验证

Abstract: Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of "simple examples": instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.

</details>


### [83] [ZENITH: Automated Gradient Norm Informed Stochastic Optimization](https://arxiv.org/abs/2601.15212)
*Dhrubo Saha*

Main category: cs.LG

TL;DR: ZENITH优化器通过梯度范数的时间演化自动调整学习率，无需手动调度，在多种视觉任务上实现更高精度和更快训练速度


<details>
  <summary>Details</summary>
Motivation: 现有自适应优化器存在计算内存开销大、与正则化不兼容、学习率选择次优等问题，需要手动调整学习率调度

Method: 提出ZENITH优化器，利用梯度范数的时间演化信息来自适应调整学习率，实现零开销的自动学习率调度

Result: 在6个CNN架构和6个基准测试中，ZENITH在更短的墙上时间内达到更高测试精度；在MS COCO的目标检测、关键点检测和实例分割任务上获得更优mAP；与正则化兼容进一步提升泛化能力

Conclusion: ZENITH优化器通过梯度范数演化自动调整学习率，解决了现有自适应优化器的问题，在多种视觉任务上表现出优越性能

Abstract: Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.

</details>


### [84] [Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism](https://arxiv.org/abs/2601.15249)
*Garrett G. Wen,Buxin Su,Natalie Collina,Zhun Deng,Weijie Su*

Main category: cs.LG

TL;DR: 提出一种基于等渗机制的作者辅助最佳论文评选方法，通过作者对自己论文的排名评估来调整原始评审分数，从而提高奖项评选质量。


<details>
  <summary>Details</summary>
Motivation: 随着NeurIPS、ICML等AI会议投稿量激增（数万篇），最佳论文奖项评选面临质量和一致性的挑战，当前评选过程存在争议，需要更可靠的评选机制。

Method: 采用等渗机制（Isotonic Mechanism）让作者对自己的投稿进行排名评估，利用这些排名信息调整原始评审分数，从而更准确地估计论文的真实质量。机制还扩展到处理作者重叠的常见情况。

Result: 理论证明：当作者的效用函数是凸可加函数时，作者有激励真实报告；特别当作者只有一个提名配额时，只需效用函数非递减且可加即可保证真实性。使用ICLR（2019-2023）和NeurIPS（2021-2023）的公开评审数据验证了凸性假设。模拟结果显示该机制显著提高了奖项论文的评选质量。

Conclusion: 作者辅助的等渗机制为大规模AI会议的最佳论文评选提供了有效的解决方案，显著放松了先前工作所需的假设条件，并能处理作者重叠的实际情况，有望提升奖项评选的准确性和公信力。

Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.

</details>


### [85] [MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs](https://arxiv.org/abs/2601.15279)
*Christoph Bartmann,Johannes Schimunek,Mykyta Ielanskyi,Philipp Seidl,Günter Klambauer,Sohvi Luukkonen*

Main category: cs.LG

TL;DR: MolecularIQ是一个专门用于评估大语言模型在分子结构推理能力的基准测试，专注于符号可验证的任务，能够精细评估模型对分子图的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有化学基准测试大多侧重于一般化学知识，依赖文献或代理标签（存在泄漏或偏见风险），或简化为多项选择题，缺乏专门评估分子结构推理能力的工具。

Method: 引入MolecularIQ基准测试，专注于符号可验证的任务，通过分子图解析和理解来评估LLMs的推理能力，能够定位模型失败的具体任务和分子结构。

Result: MolecularIQ能够揭示当前化学LLMs的能力模式，将模型失败定位到特定任务和分子结构，为模型开发提供可操作的见解。

Conclusion: MolecularIQ为评估化学LLMs的分子结构推理能力提供了专门工具，能够指导开发能够忠实推理分子结构的模型，填补了现有基准测试的空白。

Abstract: A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [86] [Meta Flow Maps enable scalable reward alignment](https://arxiv.org/abs/2601.14430)
*Peter Potaptchik,Adhi Saravanan,Abbas Mammadov,Alvaro Prat,Michael S. Albergo,Yee Whye Teh*

Main category: stat.ML

TL;DR: 提出Meta Flow Maps (MFMs)框架，通过单步后验采样实现高效生成模型控制，无需昂贵的轨迹模拟，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 当前生成模型控制方法（推理时引导或微调）需要估计价值函数，这通常需要访问条件后验分布p_{1|t}(x_1|x_t)，导致需要昂贵的轨迹模拟计算

Method: 扩展一致性模型和流映射到随机领域，训练MFMs执行随机单步后验采样，从任何中间状态生成任意多个i.i.d.的干净数据样本，提供可微分重参数化以实现高效价值函数估计

Result: 单粒子引导的MFM采样器在ImageNet上优于Best-of-1000基线，在多个奖励函数上以极低的计算成本取得更好效果

Conclusion: MFMs框架解决了生成模型控制中的计算瓶颈，实现了无需内部滚动的推理时引导和针对通用奖励的无偏、离策略微调

Abstract: Controlling generative models is computationally expensive. This is because optimal alignment with a reward function--whether via inference-time steering or fine-tuning--requires estimating the value function. This task demands access to the conditional posterior $p_{1|t}(x_1|x_t)$, the distribution of clean data $x_1$ consistent with an intermediate state $x_t$, a requirement that typically compels methods to resort to costly trajectory simulations. To address this bottleneck, we introduce Meta Flow Maps (MFMs), a framework extending consistency models and flow maps into the stochastic regime. MFMs are trained to perform stochastic one-step posterior sampling, generating arbitrarily many i.i.d. draws of clean data $x_1$ from any intermediate state. Crucially, these samples provide a differentiable reparametrization that unlocks efficient value function estimation. We leverage this capability to solve bottlenecks in both paradigms: enabling inference-time steering without inner rollouts, and facilitating unbiased, off-policy fine-tuning to general rewards. Empirically, our single-particle steered-MFM sampler outperforms a Best-of-1000 baseline on ImageNet across multiple rewards at a fraction of the compute.

</details>


### [87] [Large Data Limits of Laplace Learning for Gaussian Measure Data in Infinite Dimensions](https://arxiv.org/abs/2601.14515)
*Zhengang Zhong,Yury Korolev,Matthew Thorpe*

Main category: stat.ML

TL;DR: 本文分析了无限维希尔伯特空间中高斯测度生成数据下的Laplace学习图狄利克雷能量的逐点收敛性


<details>
  <summary>Details</summary>
Motivation: Laplace学习在半监督学习中用于从部分标记数据中恢复缺失标签，但现有理论主要针对有限维数据。当数据生成于无限维空间时，由于缺乏勒贝格测度，需要重新分析其渐近行为。

Method: 研究高斯测度在希尔伯特空间上生成数据的情况，分析图狄利克雷能量在无限维空间中的渐近性质，证明其逐点收敛性。

Result: 证明了在无限维希尔伯特空间中，当数据由高斯测度生成时，图狄利克雷能量能够实现逐点收敛。

Conclusion: 这是首次在无限维数据生成测度下分析Laplace学习的收敛性，为理解无限维空间中的半监督学习理论迈出了重要一步。

Abstract: Laplace learning is a semi-supervised method, a solution for finding missing labels from a partially labeled dataset utilizing the geometry given by the unlabeled data points. The method minimizes a Dirichlet energy defined on a (discrete) graph constructed from the full dataset. In finite dimensions the asymptotics in the large (unlabeled) data limit are well understood with convergence from the graph setting to a continuum Sobolev semi-norm weighted by the Lebesgue density of the data-generating measure. The lack of the Lebesgue measure on infinite-dimensional spaces requires rethinking the analysis if the data aren't finite-dimensional. In this paper we make a first step in this direction by analyzing the setting when the data are generated by a Gaussian measure on a Hilbert space and proving pointwise convergence of the graph Dirichlet energy.

</details>


### [88] [Communication-Efficient Federated Risk Difference Estimation for Time-to-Event Clinical Outcomes](https://arxiv.org/abs/2601.14609)
*Ziwen Wang,Siqi Li,Marcus Eng Hock Ong,Nan Liu*

Main category: stat.ML

TL;DR: FedRD：一种通信高效的联邦风险差异估计框架，用于分布式生存数据分析，无需持续服务器连接，提供临床可解释的绝对风险评估。


<details>
  <summary>Details</summary>
Motivation: 医学研究中，隐私保护模型协同训练常受限于服务器依赖架构与医院数据系统不兼容，且现有方法主要关注相对效应指标（如风险比），缺乏临床可解释的绝对生存风险评估。

Method: 提出FedRD框架，采用服务器独立架构，最小化通信：分层模型只需一轮汇总统计交换，非分层模型只需三轮。提供有效置信区间和假设检验能力。

Result: 理论证明FedRD具有渐近性质，非分层模型与个体层面分析渐近等价。仿真和真实临床应用显示FedRD在估计准确性和预测性能上优于本地和联邦基线方法。

Conclusion: FedRD为隐私受限的多中心临床研究提供了架构可行的绝对风险评估解决方案，解决了现有联邦学习框架在医学研究中的关键限制。

Abstract: Privacy-preserving model co-training in medical research is often hindered by server-dependent architectures incompatible with protected hospital data systems and by the predominant focus on relative effect measures (hazard ratios) which lack clinical interpretability for absolute survival risk assessment. We propose FedRD, a communication-efficient framework for federated risk difference estimation in distributed survival data. Unlike typical federated learning frameworks (e.g., FedAvg) that require persistent server connections and extensive iterative communication, FedRD is server-independent with minimal communication: one round of summary statistics exchange for the stratified model and three rounds for the unstratified model. Crucially, FedRD provides valid confidence intervals and hypothesis testing--capabilities absent in FedAvg-based frameworks. We provide theoretical guarantees by establishing the asymptotic properties of FedRD and prove that FedRD (unstratified) is asymptotically equivalent to pooled individual-level analysis. Simulation studies and real-world clinical applications across different countries demonstrate that FedRD outperforms local and federated baselines in both estimation accuracy and prediction performance, providing an architecturally feasible solution for absolute risk assessment in privacy-restricted, multi-site clinical studies.

</details>


### [89] [Semi-Supervised Mixture Models under the Concept of Missing at Radom with Margin Confidence and Aranda Ordaz Function](https://arxiv.org/abs/2601.14631)
*Jinyang Liao,Ziyang Lyu*

Main category: stat.ML

TL;DR: 提出一个半监督高斯混合模型框架，在MAR机制下通过建模分类不确定性来参数化缺失机制，使用AO链接函数和ECM算法联合估计参数，有效减轻忽略缺失机制带来的偏差。


<details>
  <summary>Details</summary>
Motivation: 在半监督学习中，标签缺失通常不是完全随机的（MCAR），而是与数据特征相关的（MAR）。忽略缺失机制会导致估计偏差，影响分类性能。需要开发能够显式建模缺失机制的方法。

Method: 1) 将缺失概率建模为分类不确定性的函数；2) 引入边界置信度量化不确定性；3) 使用Aranda Ordaz链接函数灵活捕捉不确定性与缺失概率之间的非对称关系；4) 开发ECM算法联合估计GMM和缺失机制的所有参数；5) 基于拟合的混合模型使用贝叶斯分类器填补缺失标签。

Result: 该方法有效减轻了忽略缺失机制引起的偏差，增强了半监督学习的鲁棒性。在具有大量缺失标签的现实MAR场景中，该不确定性感知框架提供了可靠的分类性能。

Conclusion: 提出的半监督学习框架通过显式参数化MAR缺失机制，将缺失概率与分类不确定性联系起来，使用AO链接函数和ECM算法实现了有效的参数估计和标签填补，为现实MAR场景提供了可靠的分类解决方案。

Abstract: This paper presents a semi-supervised learning framework for Gaussian mixture modelling under a Missing at Random (MAR) mechanism. The method explicitly parameterizes the missingness mechanism by modelling the probability of missingness as a function of classification uncertainty. To quantify classification uncertainty, we introduce margin confidence and incorporate the Aranda Ordaz (AO) link function to flexibly capture the asymmetric relationships between uncertainty and missing probability. Based on this formulation, we develop an efficient Expectation Conditional Maximization (ECM) algorithm that jointly estimates all parameters appearing in both the Gaussian mixture model (GMM) and the missingness mechanism, and subsequently imputes the missing labels by a Bayesian classifier derived from the fitted mixture model. This method effectively alleviates the bias induced by ignoring the missingness mechanism while enhancing the robustness of semi-supervised learning. The resulting uncertainty-aware framework delivers reliable classification performance in realistic MAR scenarios with substantial proportions of missing labels.

</details>


### [90] [Efficient and Minimax-optimal In-context Nonparametric Regression with Transformers](https://arxiv.org/abs/2601.15014)
*Michelle Ching,Ioana Popescu,Nico Smith,Tianyi Ma,William G. Underwood,Richard J. Samworth*

Main category: stat.ML

TL;DR: 本文研究了非参数回归中的上下文学习，证明了具有Θ(log n)参数的预训练transformer能够实现极小极大最优收敛速率O(n^{-2α/(2α+d)})。


<details>
  <summary>Details</summary>
Motivation: 研究transformer在上下文学习中的理论性能，特别是针对α-Hölder光滑回归函数的非参数回归问题，探索transformer能否以更少的参数和预训练数据实现最优收敛速率。

Method: 通过证明transformer能够高效近似局部多项式估计器，实现核加权多项式基并运行梯度下降。使用Θ(log n)参数的transformer和Ω(n^{2α/(2α+d)}log^3 n)预训练序列。

Result: 证明了预训练transformer能够以均方误差O(n^{-2α/(2α+d)})实现极小极大最优收敛速率，所需参数和预训练序列数量显著少于先前文献结果。

Conclusion: transformer能够以对数级参数实现非参数回归的上下文学习，通过近似局部多项式估计器达到最优收敛速率，为transformer的理论理解提供了新视角。

Abstract: We study in-context learning for nonparametric regression with $α$-Hölder smooth regression functions, for some $α>0$. We prove that, with $n$ in-context examples and $d$-dimensional regression covariates, a pretrained transformer with $Θ(\log n)$ parameters and $Ω\bigl(n^{2α/(2α+d)}\log^3 n\bigr)$ pretraining sequences can achieve the minimax-optimal rate of convergence $O\bigl(n^{-2α/(2α+d)}\bigr)$ in mean squared error. Our result requires substantially fewer transformer parameters and pretraining sequences than previous results in the literature. This is achieved by showing that transformers are able to approximate local polynomial estimators efficiently by implementing a kernel-weighted polynomial basis and then running gradient descent.

</details>


### [91] [Multi-context principal component analysis](https://arxiv.org/abs/2601.15239)
*Kexin Wang,Salil Bhate,João M. Pereira,Joe Kileel,Matylda Figlerowicz,Anna Seigal*

Main category: stat.ML

TL;DR: MCPCA是一种新的多上下文主成分分析方法，能够识别跨不同数据子集共享的变异因素，在基因表达和语言模型分析中展示了独特价值。


<details>
  <summary>Details</summary>
Motivation: 当前数据收集通常跨越多个上下文（如不同疾病患者、不同细胞类型、不同文本），虽然变异因素在不同子集间共享，但缺乏系统方法来恢复这些共享因素。

Method: 开发了多上下文主成分分析（MCPCA）的理论和算法框架，将数据分解为跨上下文子集共享的因素，是PCA的原则性泛化。

Result: 在基因表达分析中，MCPCA揭示了跨癌症类型子集共享的变异轴，以及一个与肺癌进展相关的肿瘤细胞变异轴；在语言模型分析中，成功映射了关于人性辩论的发展阶段。

Conclusion: MCPCA是PCA的重要扩展，能够解决跨上下文数据分析的挑战，揭示传统方法（跨上下文合并数据或单独分析单个上下文）无法发现的模式。

Abstract: Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts.

</details>


### [92] [Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?](https://arxiv.org/abs/2601.15254)
*Felix Schur,Niklas Pfister,Peng Ding,Sach Mukherjee,Jonas Peters*

Main category: stat.ML

TL;DR: 提出一种针对无配对数据场景的因果效应估计方法，通过交叉折叠样本分割处理高维工具变量，在环境数量多但每个环境样本少的情况下仍能保持一致性


<details>
  <summary>Details</summary>
Motivation: 解决在隐藏混杂因素下，当观测数据不成对（X和Y在不同实验条件下分别观测）时的因果效应估计问题。传统两样本IV估计器在环境多但每个环境样本少的情况下会失效

Method: 提出基于GMM的估计器，通过对工具变量-协变量样本进行交叉折叠样本分割，将环境作为（可能高维的）工具变量。还扩展到稀疏因果效应，使用ℓ₁正则化估计和后选择重拟合

Result: 证明了当环境数量增长但每个环境的样本量保持恒定时，该估计器具有一致性。方法能够处理高维工具变量和稀疏因果效应

Conclusion: 该方法为无配对数据场景下的因果推断提供了有效的解决方案，特别是在环境多但每个环境样本有限的情况下，克服了传统IV方法的局限性

Abstract: We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.

</details>
