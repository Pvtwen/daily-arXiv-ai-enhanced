<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 6]
- [cs.LG](#cs.LG) [Total: 59]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Secure ISAC Systems Empowered by Compound Reconfigurable Antenna Arrays](https://arxiv.org/abs/2508.16055)
*Mengzhen Liu,Ming Li,Rang Liu,Qian Liu*

Main category: eess.SP

TL;DR: 本文提出了一种基于复合可重配天线数组的新题安全ISAC框架，通过联合设计电磁域和基带域预编码器，在保持通信安全的同时实现了远超传统方案的雷达感知性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统ISAC系统中双功能信号在雷达感知时容易泄露通信信息，特别是当感知目标作为偷听者时。传统基带域安全方案受限于空间分辨率不足和额空间相关频道的影响。

Method: 提出使用复合可重配天线数组(CRA)在电磁域实现辐射图框和偏振状态的同时可重配性。建立包含虚拟角域、空间域和去偏振效应的综合频道模型，并通过分解基于迭代算法联合设计EM域和BB域预编码器。

Result: 模拟结果显示，提出的CRA数组架构在安全ISAC系统中实现了显著性能提升，雷达感知收益远超传统根形成等效达12dB，同时保持了稳健的通信安全性能。

Conclusion: 通过联合利用电磁域的额外自由度，本文提出的方案为安全ISAC系统设计带来了重大改进，显著提升了感知性能而不以牺牲通信安全为代价。

Abstract: In integrated sensing and communication (ISAC) systems, the use of
dual-functional signals inherently exposes confidential communication
information during radar sensing, particularly when the sensing target itself
acts as an eavesdropper. Conventional physical-layer security solutions rely on
directional beamforming or artificial noise injection implemented via signal
processing in the baseband (BB) domain. However, these BB-domain approaches are
constrained by insufficient spatial resolution and the adverse effects of
spatially correlated channels. To overcome these limitations, this paper
proposes a novel secure ISAC framework empowered by compound reconfigurable
antenna (CRA) arrays, which offer simultaneous reconfigurability of radiation
patterns and polarization states in the electromagnetic (EM) domain.
Specifically, we develop a comprehensive channel model incorporating virtual
angular domain, spatial domain, and depolarization effects, and formulate a
mixed-integer nonlinear programming (MINLP) problem to jointly design EM-domain
and BB-domain precoders and combiners. To efficiently solve this complex
optimization problem, we propose an iterative decomposition-based algorithm
leveraging fractional programming (FP), majorization-minimization (MM),
second-order cone programming (SOCP), and penalty methods. Extensive simulation
results demonstrate that the CRA array architecture with proposed joint EM-and
BB-domain design achieves significant performance improvements in secure ISAC
systems. In particular, radar sensing gains of up to 12dB are observed over
conventional beamforming, while robust communication security is maintained.
These results highlight the considerable benefits attainable by jointly
leveraging additional degrees of freedom (DoFs) in the EM domain for secure
ISAC system design.

</details>


### [2] [FM OFDM Unifying High Mobility Communications and Sensing](https://arxiv.org/abs/2508.16107)
*Amir Bouziane,Huseyin Arslan*

Main category: eess.SP

TL;DR: FM-OFDM是一种恒定包络波形，在6G集成感知与通信中表现出色，特别是在高移动性环境下相比传统OFDM具有更优越的距离和速度估计精度。


<details>
  <summary>Details</summary>
Motivation: 6G系统需要同时支持高速数据传输和精确环境感知，但传统OFDM的高峰均功率比(PAPR)在高频段带来挑战，需要寻找更适合的波形设计。

Method: 研究频率调制正交频分复用(FM-OFDM)技术，推导其在时变多径信道中的输入输出关系，分析载波间干扰、多普勒效应和有效信道增益，并与传统CP-OFDM和CE-OFDM进行对比仿真。

Result: 仿真结果表明FM-OFDM在高信噪比和高移动性条件下具有更优越的距离和速度估计精度，特别适合高频ISAC应用。

Conclusion: FM-OFDM作为恒定包络波形，是6G高频集成感知与通信应用的理想统一波形解决方案，能够有效应对高移动性双弥散信道条件。

Abstract: Integrated Sensing and Communication (ISAC) is foundational to future sixth
generation (6G) systems, demanding waveform co-design that supports both high
throughput data transmission and accurate environmental perception. While
Orthogonal Frequency Division Multiplexing (OFDM) offers flexibility and
backward compatibility, its high Peak to Average Power Ratio (PAPR) poses
significant challenges at higher frequency bands. To address this, we
investigate Frequency Modulated Orthogonal Frequency Division Multiplexing (FM
OFDM) a constant envelope waveform that facilitates robust joint sensing and
communication under highly mobile, doubly dispersive channel conditions. We
derive a comprehensive input output relationship for FM OFDM in time varying
multipath channels, including analytical expressions for Inter Carrier
Interference (ICI), Doppler effects, and effective channel gains. Extensive
simulations comparing FM OFDM with conventional Cyclic Prefix Orthogonal
Frequency Division Multiplexing (CP OFDM) and Constant Envelope Orthogonal
Frequency Division Multiplexing (CE OFDM) demonstrate superior range and
velocity estimation accuracy of FM-OFDM, particularly at high Signal to Noise
Ratios (SNRs) and under high mobility, highlighting its suitability as a
unified waveform for high frequency ISAC applications in 6G.

</details>


### [3] [A Scalable Hybrid Track-Before-Detect Tracking System: Application to Coastal Maritime Radar Surveillance](https://arxiv.org/abs/2508.16169)
*Lukas Herrmann,Ángel F. García-Fernández,Edmund F. Brekke,Egil Eide*

Main category: eess.SP

TL;DR: 提出了一种结合TBD和检测式跟踪算法的混合框架，用于海岸雷达监视，能够在资源受限条件下实现大规模多目标跟踪


<details>
  <summary>Details</summary>
Motivation: 传统TBD方法虽然理论上有优势，但由于计算复杂性和可扩展性限制，在实际多目标跟踪应用中很少使用，需要开发更实用的解决方案

Method: 结合IE-PHPMHT TBD模块和传统PMBM点跟踪器，处理原始雷达数据，通过高阈值检测跟踪强目标，低阈值检测用于TBD模块的自适应新生，实现弱目标的早期启动和持续跟踪

Result: 使用挪威特隆赫姆峡湾的真实X波段雷达数据进行验证，在资源约束下的大规模观测区域中表现出鲁棒的多目标跟踪性能

Conclusion: 该方法适用于复杂海上环境中的海岸监视和自主系统支持，具备实际部署的适用性

Abstract: Despite their theoretical advantages, track-before-detect (TBD) methods
remain largely absent from real-world multi-target tracking applications due to
their computational complexity and limited scalability. This paper presents a
scalable hybrid tracking framework that combines a TBD multi-target tracking
algorithm with a detection-based multi-target tracking algorithm for coastal
radar surveillance. In particular, the approach uses an integrated existence
Poisson histogram-probabilistic multi-hypothesis tracking (IE-PHPMHT)-based TBD
module with a conventional Poisson multi-Bernoulli Mixture (PMBM) point
tracker. The system processes raw radar data through land clutter suppression,
cell-wise detection, and clustering-based feature extraction. High-threshold
detections are used to track strong targets via the point tracker, while
low-threshold detections are employed for adaptive birth in the TBD module,
enabling early initiation and sustained tracking of weak or ambiguous targets.
Validated using real X-band radar data from the Trondheim Fjord, Norway, the
approach demonstrates robust multi-target tracking performance in a full-scale
application with a large observation area under resource constraints,
highlighting its suitability for operational deployment in complex maritime
environments needed for coastal surveillance and to support autonomy.

</details>


### [4] [Hybrid Precoding Revisited: Low-Dimensional Subspace Perspective for MU-MIMO Systems](https://arxiv.org/abs/2508.16218)
*Mintaek Oh,Jinseok Choi*

Main category: eess.SP

TL;DR: 提出了一种基于低维子空间特性的低复杂度混合预编码框架，用于多用户MIMO系统，包括动态子阵列天线分区和统计信道信息利用


<details>
  <summary>Details</summary>
Motivation: 多用户MIMO系统中传统混合预编码方法计算复杂度高，需要开发低复杂度且性能优越的预编码方案

Method: 利用低维子空间特性识别无约束最优RF预编码器，通过降复杂度预编码方法优化混合预编码器，并扩展到动态子阵列天线分区和统计CSIT利用

Result: 仿真验证所提算法在显著降低复杂度的同时，相比现有方法实现了更优越的性能

Conclusion: 该低复杂度混合预编码框架为多用户MIMO系统提供了有效的解决方案，在性能和复杂度之间取得了良好平衡

Abstract: This letter presents a low-complexity hybrid precoding framework for
multiuser multiple-input multiple-output (MIMO) systems by leveraging a
low-dimensional subspace property. Under the low-dimensional subspace
perspective, we first identify an unconstrained optimal radio-frequency (RF)
precoder. We then optimize a hybrid precoder via a reduced-complexity precoding
method. We further extend the proposed framework to (i) a dynamic-subarray
antenna partitioning algorithm that adaptively allocates subsets of antennas
associated with RF chains, and (ii) a channel covariance-based approach to
exploit statistical channel state information at a transmitter (CSIT), ensuring
robustness with partial CSIT. Simulations validate that our proposed algorithms
achieve superior performance while significantly reducing complexity compared
to existing methods.

</details>


### [5] [Parameter-Free Logit Distillation via Sorting Mechanism](https://arxiv.org/abs/2508.16544)
*Stephen Ekaputra Limantoro*

Main category: eess.SP

TL;DR: 提出一种基于排序机制的logit处理方案，通过修正教师模型的错误预测并重新排序分布来改进知识蒸馏效果


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法通常直接使用教师模型的原始分布，忽略了错误预测的潜在影响，这与通过交叉熵损失进行硬标签学习的动机相矛盾，可能导致在某些样本上的知识蒸馏效果不佳

Method: 提出一种新颖的logit处理方案，通过排序机制实现两个目标：(1)基于标签修正教师模型的错误预测；(2)按照优先级排名自然重新排序分布。该方法是一个即插即用的预处理方案

Result: 在CIFAR-100和ImageNet数据集上的大量实验证明了该方法的有效性

Conclusion: 所提出的排序方法可以有效应用于现有的基于logit的知识蒸馏方法，提升知识蒸馏性能

Abstract: Knowledge distillation (KD) aims to distill the knowledge from the teacher
(larger) to the student (smaller) model via soft-label for the efficient neural
network. In general, the performance of a model is determined by accuracy,
which is measured with labels. However, existing KD approaches usually use the
teacher with its original distribution, neglecting the potential of incorrect
prediction. This may contradict the motivation of hard-label learning through
cross-entropy loss, which may lead to sub-optimal knowledge distillation on
certain samples. To address this issue, we propose a novel logit processing
scheme via a sorting mechanism. Specifically, our method has a two-fold goal:
(1) fixing the incorrect prediction of the teacher based on the labels and (2)
reordering the distribution in a natural way according to priority rank at
once. As an easy-to-use, plug-and-play pre-processing, our sort method can be
effectively applied to existing logit-based KD methods. Extensive experiments
on the CIFAR-100 and ImageNet datasets demonstrate the effectiveness of our
method.

</details>


### [6] [Continuous Determination of Respiratory Rate in Hospitalized Patients using Machine Learning Applied to Electrocardiogram Telemetry](https://arxiv.org/abs/2508.15947)
*Thomas Kite,Brian Ayers,Nicholas Houstis,Asishana A. Osho,Thoralf M. Sundt,Aaron D Aguirre*

Main category: eess.SP

TL;DR: 使用神经网络从心电图信号中自动监测呼吸频率，在多个验证集上表现出高精度（平均绝对误差小于1.78次/分钟），并通过回顾性分析证明其临床价值。


<details>
  <summary>Details</summary>
Motivation: 呼吸频率是重要的生命体征，但人工计数不准确且耗时。目前自动监测主要限于ICU患者，大多数普通病房患者缺乏有效监测，存在临床恶化风险。

Method: 训练神经网络从心电图遥测波形中提取呼吸频率标签，利用生物信号中的呼吸变化特征。

Result: 在内部和外部验证集上均显示高准确性，平均绝对误差小于1.78次/分钟。回顾性分析显示连续监测能有效追踪呼吸衰竭等不良事件。

Conclusion: 结合现有遥测系统和人工智能技术，可提供准确、自动化和可扩展的患者监测，为医院范围内的AI早期预警系统奠定基础。

Abstract: Respiration rate (RR) is an important vital sign for clinical monitoring of
hospitalized patients, with changes in RR being strongly tied to changes in
clinical status leading to adverse events. Human labels for RR, based on
counting breaths, are known to be inaccurate and time consuming for medical
staff. Automated monitoring of RR is in place for some patients, typically
those in intensive care units (ICUs), but is absent for the majority of
inpatients on standard medical wards who are still at risk for clinical
deterioration. This work trains a neural network (NN) to label RR from
electrocardiogram (ECG) telemetry waveforms, which like many biosignals, carry
multiple signs of respiratory variation. The NN shows high accuracy on multiple
validation sets (internal and external, same and different sources of RR
labels), with mean absolute errors less than 1.78 breaths per minute (bpm) in
the worst case. The clinical utility of such a technology is exemplified by
performing a retrospective analysis of two patient cohorts that suffered
adverse events including respiratory failure, showing that continuous RR
monitoring could reveal dynamics that strongly tracked with intubation events.
This work exemplifies the method of combining pre-existing telemetry monitoring
systems and artificial intelligence (AI) to provide accurate, automated and
scalable patient monitoring, all of which builds towards an AI-based
hospital-wide early warning system (EWS).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: 这篇论文提出了一种创新的车载测量系统，通过使用位移和温度传感器结合机器学习算法和IIR滤波器，实现了高精度的轮轮边缝磨损深度监测，精度达到98.2%，为铁路系统安全运营提供了实时监控解决方案。


<details>
  <summary>Details</summary>
Motivation: 轨道与轮轮交互作用对铁路系统安全至关重要，需要准确的测量系统来监测轮轮边缝磨损，以保证铁路运营的最佳安全状态。

Method: 设计了一种车载测量系统，使用位移和温度传感器。通过实验室实验模拟轮轮边缝磨损深度和温度变化，收集数据训练基于回归模型的机器学习算法。设计了IIR滤波器来减少车辆动力学和传感器噪声干扰。

Result: 动态机器学习算法有效对抗了传感器对温度效应的非线性响应，达到96.5%的精度。IIR滤波器的实时噪声降低将精度提升到98.2%，运行时间最小。

Conclusion: 该先进监测系统与物联网嵌入式系统集成，能够提供无与伦比的实时见解，监测轮轮边缝磨损和轨道不规则条件，确保铁路系统运营的高安全性和效率。

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [8] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 本文提出了在奖励向量按偏好锥排序的分布偏移环境下，基于自适应离散化和乐观消除的上下文赌博机学习策略，并引入了基于偏好的遗憾概念来衡量性能。


<details>
  <summary>Details</summary>
Motivation: 研究在分布偏移情况下，当奖励向量按照给定偏好锥排序时的上下文赌博机学习问题，现有方法无法有效处理分布偏移环境。

Method: 提出了一种基于自适应离散化和乐观消除的策略，该策略能够自我调整以适应底层分布偏移。

Result: 建立了在各种分布偏移假设下的遗憾上界，这些界限推广了无分布偏移和向量奖励设置的已知结果，并在存在分布偏移时与问题参数优雅地缩放。

Conclusion: 所提出的策略在分布偏移环境下表现良好，遗憾界限具有良好的扩展性，为处理偏好排序的分布偏移问题提供了有效解决方案。

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [9] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA是一种针对噪声、稀疏和稀缺数据的多目标贝叶斯优化算法，通过整合实验不确定性先验知识和信任区域策略，在有限实验预算下更高效地收敛到帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 传统多目标贝叶斯优化方法在处理具有实验不确定性（相同输入产生不同输出）的稀疏、稀缺数据集时表现不佳，这在物理实验和模拟实验中很常见，导致实验资源分配效率低下和次优设计。

Method: 提出NOSTRA算法框架，整合实验不确定性的先验知识构建更准确的代理模型，同时采用信任区域策略将采样集中在设计空间的有希望区域，通过战略性地利用先验信息和细化搜索区域来优化采样。

Result: 通过在具有不同实验不确定性水平的两个测试函数上的验证，NOSTRA在处理噪声、稀疏和稀缺数据方面优于现有方法，能够有效优先选择能提高已识别帕累托前沿精度的区域进行采样。

Conclusion: NOSTRA提供了一种资源高效的算法，在有限实验预算的实际场景中确保高效性能，加速向帕累托前沿的收敛，提高数据效率并改善解决方案质量。

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [10] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: 提出了一个完整的TinyML流程，用于工业4.0中的铣削过程质量监测，包括数据集生成、模型开发和微控制器实现，实现了100%测试精度和低功耗推理。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0背景下，为老旧工业机器添加过程监控能力，利用无线监测系统和TinyML范式实现智能工厂改造。

Method: 开发完整的TinyML流程：创建MillingVibes数据集，构建8位量化卷积神经网络模型，在ARM Cortex M4F微控制器上实现预处理和分类流水线。

Result: 模型参数量12.59kiB，测试精度100%，推理时间15.4ms，每次推理能耗1.462mJ，性能优异。

Conclusion: 证明了TinyML系统在结构集成过程质量监测中的可行性，为未来TinyML过程监控解决方案提供了参考基准。

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [11] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: Z-Pruner是一种新颖的后训练剪枝方法，无需重新训练即可在预训练大语言模型中诱导稀疏性，通过结合权重更新幅度和激活模式来更有效地识别和消除冗余参数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模不断增大带来了部署、可扩展性和能效方面的挑战，现有剪枝方法要么导致性能显著下降，要么需要计算成本高昂的微调。

Method: Z-Pruner利用权重更新幅度和激活模式来识别冗余参数，是模型无关、高效且易于实现的后训练剪枝方法，无需任何重新训练。

Result: 在LLaMA-2、LLaMA-3和OPT等多种LLM架构上的实验表明，Z-Pruner超越了需要密集权重更新的最先进剪枝方法，获得了最低的困惑度分数和最高的零样本准确率平均分数。

Conclusion: Z-Pruner提供了一种有效的后训练剪枝解决方案，能够在保持性能的同时显著减少模型大小和推理延迟，相关代码已开源。

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [12] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net是一个新颖的多模态情感分析深度学习框架，通过渐进式门控融合、自适应门控仲裁和参数高效微调策略，实现了高效且可解释的多模态融合，在MOSI数据集上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决多模态情感分析中深度、动态和可解释的融合问题，同时保持参数效率，特别是在资源受限的场景下。

Method: 提出渐进式层内融合范式（Cross-Attention机制）、自适应门控仲裁机制和混合参数高效微调策略（LoRA+Post-Fusion Adapters）

Result: 在MOSI数据集上取得MAE 0.691和F1-Score 86.9%的state-of-the-art性能，仅需3.09M可训练参数

Conclusion: PGF-Net通过创新的融合机制和参数效率策略，成功实现了高性能、高效率且可解释的多模态情感分析

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [13] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 提出了一种结合频谱分析和概率预测的简化ECG分割架构，替代复杂的BiLSTM模型，提高计算效率同时保持高精度分割性能，并引入可解释AI增强模型透明度


<details>
  <summary>Details</summary>
Motivation: 现有ECG分割模型依赖复杂的多层架构如BiLSTM，计算量大且效率低，需要开发更高效、可解释的解决方案

Method: 结合频谱分析和概率预测的简化架构，用简单层替代复杂层捕获P、QRS和T波的时域和频谱特征，应用可解释AI(XAI)方法增强模型可解释性

Result: 达到高分割准确率：QRS波97.00%、T波93.33%、P波96.07%，在提高计算效率的同时提供精确分割

Conclusion: 简化架构不仅提高了计算效率，还提供了精确的分割性能，结合物理AI原理确保了ECG分析的可靠性和透明度，是心脏信号监测的实用有效解决方案

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [14] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出张量并行潜在注意力(TPLA)方案，解决MLA在张量并行中的缓存效率问题，在保持KV缓存压缩优势的同时实现高效并行计算。


<details>
  <summary>Details</summary>
Motivation: 多头潜在注意力(MLA)在张量并行(TP)环境中，每个设备仍需加载完整缓存，导致内存优势失效，需要解决这一问题以保持KV缓存压缓的优势。

Method: 将潜在表示和每个头的输入维度分片到不同设备，每个切片独立计算注意力，然后通过all-reduce操作结合结果。使用正交变换(如Hadamard变换或PCA)减少切片间干扰。

Result: 在DeepSeek-V3和Kimi-K2上实现了1.79x和1.93x的速度提升，在32K标记上下文长度下保持了性能，兼容FlashAttention-3。

Conclusion: TPLA方案有效解决了MLA在张量并行中的缓存效率问题，在保持表达能力的同时实现了显著的计算加速，并且无需重新训练。

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [15] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 新的时序因果发现框架，利用Transformer预测器捕捉非线性依赖关系，通过梯度分析提取因果结构和滞后时间，使用注意力掩码机制集成先验知识减少偏偏相关。


<details>
  <summary>Details</summary>
Motivation: 解决时序因果发现中的两大挑战：复杂的非线性依赖关系和偏偏相关问题。

Method: 使用多层Transformer时间序列预测器捕捉长程非线性关系，通过梯度分析提取因果结构，采用注意力掩码机制集成先验知识避免偏偏因果关系。

Result: 在因果发现F1分数上比最佳方法提升12.8%，因果滞后时间估计准确度达98.9%。

Conclusion: 该框架能够有效处理复杂的时序因果关系，显著提升了因果发现的性能和准确性。

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [16] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 这是一篇关于高维数据降维嵌入算法的综述论文，系统回顾了该领域近年来的发展，提供了最佳实践指南，评估了流行方法，并讨论了该领域的挑战和开放问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据在各领域广泛应用，但直接处理高维数据存在挑战，需要降维嵌入算法进行可视化、探索和分析。然而该领域研究碎片化，缺乏对实践者的清晰指导。

Method: 通过详细批判性综述近期发展，推导创建和使用低维嵌入的最佳实践清单，在各种数据集上评估流行方法。

Result: 提供了该领域的系统性综述和最佳实践指南，帮助研究人员和实践者更好地理解和应用降维嵌入技术。

Conclusion: 该综述旨在增加领域内的一致性，促进未来工作，并讨论了该领域剩余的挑战和开放问题。

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [17] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 论文提出将图表示学习(GRL)与GRASP元启发式算法结合，构建GL-GRASP方法来解决约束增量图绘制问题，相比传统方法在求解质量和效率上都有显著提升


<details>
  <summary>Details</summary>
Motivation: 传统机器学习与元启发式结合方法耗时且效果不佳，需要寻找更高效的学习策略来提取图结构信息，提升启发式算法的决策能力

Method: 将图表示学习技术融入GRASP算法的构建阶段，使用深度学习节点嵌入技术来指导搜索过程，形成GL-GRASP混合算法

Result: 基于深度学习的GL-GRASP方法在原始积分度量上优于现有文献中的GRASP启发式算法，在更密集的实例上也表现出良好的可扩展性和鲁棒性

Conclusion: 图表示学习为元启发式算法提供了有效的结构信息提取方式，GL-GRASP方法在约束增量图绘制问题上展现出优越性能，证明了这种混合策略的有效性

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [18] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 该论文提出了一种基于MiniRocket特征提取和线性分类器的MI-EEG信号分类新方法，在PhysioNet数据集上达到98.63%的准确率，优于深度学习模型且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 解决MI-EEG信号分类面临的挑战，包括信号的非平稳性、时变性和个体差异性，以及多类别分类和个体间自然变异导致的分类准确率提升困难。

Method: 提出使用Minimally Random Convolutional Kernel Transform (MiniRocket)高效提取特征，然后用线性分类器进行分类。同时构建了基于CNN-LSTM的深度学习模型作为基线进行比较。

Result: 在PhysioNet数据集上，MiniRocket方法达到98.63%的平均准确率，CNN-LSTM模型达到98.06%的准确率，表明MiniRocket特征提取方法性能更优且计算成本更低。

Conclusion: 所提出的方法能显著提高运动想象EEG信号的分类准确率，为MI-EEG的特征提取和分类提供了新的见解，证明了MiniRocket在脑机接口应用中的有效性。

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [19] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 本文主张医疗AI模型必须具备可解释性、可共享性、可重现性和可问责性，提出了基于内核方法、原型学习和深度内核模型等内在可解释方法替代黑盒模型，并通过生成AI和联邦学习实现隐私保护的跨机构数据共享。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的高风险特性要求机器学习模型不仅要准确，更要透明可信。黑盒模型虽然准确率高，但缺乏可解释性难以获得医疗从业者和监管机构的信任，阻碍了AI在临床环境中的实际应用。

Method: 采用内在可解释建模方法：稀疏内核方法、原型学习、深度内核模型；实施严格评估、公平性分析和不确定性量化；利用生成AI和联邦学习实现隐私保护的跨机构数据协作。

Result: 提出了一个全面的医疗AI设计框架，确保模型在保持高精度的同时满足可解释性、可共享性、可重现性和可问责性要求，为医疗AI的临床转化提供了可行路径。

Conclusion: 通过重新思考机器学习的基础设计原则，可以开发出不仅准确而且透明、可信、可转化为现实临床环境的医疗AI系统，这对于推动AI在医疗领域的负责任部署至关重要。

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [20] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-José-Revuelta,Xinheng Wang,José Ramón Garmendia-Leiza,Carlos Alberola-López,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: 基于可解释AI的咳嗽声谱分析框架，使用CNN和遮挡图谱识别慢性呼吸疾病（特别是COPD）的诊断相关频段特征


<details>
  <summary>Details</summary>
Motivation: 开发一种可解释的AI方法来分析咳嗽声谱特征，以区分慢性阻塞性肺疾病（COPD）和其他呼吸系统疾病，提供对咳嗽声学病理生理特征的深入理解

Method: 使用卷积神经网络（CNN）训练咳嗽信号的时频表示，采用遮挡图谱识别频谱图中诊断相关区域，并将这些区域分解为五个频率子带进行针对性频谱特征提取和分析

Result: 发现不同子带和疾病组之间的频谱模式存在差异，揭示了频率谱上的互补和补偿趋势。该方法能够基于可解释的频谱标记区分COPD与其他呼吸系统疾病，以及慢性与非慢性患者组

Conclusion: 该方法为咳嗽声学的病理生理特征提供了新的见解，证明了频率分辨、XAI增强的分析方法在生物医学信号解释和转化性呼吸疾病诊断中的价值

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [21] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出了FraPPE算法，通过高效求解下界中的最小化和最大化问题，解决了偏好锥纯探索问题，显著加速了计算并达到最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的Preference-based Pure Exploration算法在任意偏好锥下无法高效跟踪理论下界，需要开发计算效率高的最优算法。

Method: 推导下界的三个结构性质来简化最小化问题，使用Frank-Wolfe优化器加速最大化问题，将计算复杂度降至O(KL²)。

Result: FraPPE算法在合成和真实数据集上都能以最低样本复杂度准确识别帕累托最优集，优于现有算法。

Conclusion: FraPPE算法成功填补了计算效率空白，实现了任意偏好锥下的最优样本复杂度，为多目标bandit问题提供了高效解决方案。

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [22] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 提出了PSD算法，用于逃离非凸优化中的严格鞍点，具有显式常数和梯度下降与鞍点逃离阶段的严格分离，理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 非凸优化中的鞍点问题是优化算法面临的主要挑战之一，需要开发能够有效逃离严格鞍点的一阶方法。

Method: 提出了扰动鞍点逃离下降（PSD）算法，包含梯度下降阶段和鞍点逃离阶段，使用有限差分变体PSD-Probe和随机扩展PSGD。

Result: PSD算法以高概率找到近似二阶稳定点，梯度评估次数为O(ℓΔ_f/ε²)加上每次逃离事件的O((ℓ/√ρε)log(d/δ))次评估，最多需要O(ℓΔ_f/ε²)次逃离事件。

Conclusion: PSD算法在理论和实验上都表现出色，验证了对数维度依赖性和预测的每次事件函数下降，为逃离鞍点问题提供了有效的解决方案。

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [23] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种新的平衡传播(EP)框架，通过引入中间误差信号来解决深层网络中的梯度消失问题，首次将知识蒸馏和局部误差信号整合到EP中，在CIFAR-10和CIFAR-100数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的平衡传播(EP)方法在深层架构中存在梯度消失问题，导致能量最小化和梯度计算收敛困难，限制了其在深度网络中的应用。

Method: 提出了一种新颖的EP框架，通过整合中间误差信号来增强信息流和神经元动力学的收敛性，首次将知识蒸馏和局部误差信号引入EP方法。

Result: 在CIFAR-10和CIFAR-100数据集上实现了最先进的性能，展示了在深度VGG架构上的可扩展性。

Conclusion: 这项工作显著推进了EP方法的可扩展性，为其在现实世界系统中的应用铺平了道路。

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [24] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 量子联邦学习(QFL)是分布式量子计算与联邦机器学习的结合，利用量子增强能力实现隐私保护的分散式学习，在分布式量子系统中提供高效安全的模型训练。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算和联邦学习的优势，解决分布式量子系统中的隐私保护和高效模型训练挑战，为多个领域提供量子增强的机器学习解决方案。

Method: 采用系统性综述方法，涵盖QFL的关键概念、基础原理、应用场景和挑战。包括联邦架构、网络拓扑、通信方案、优化技术和安全机制的分类讨论，以及原型实现和案例研究。

Result: 全面梳理了QFL的技术框架和应用领域，包括车载网络、医疗健康、卫星网络、元宇宙和网络安全等多个领域的应用潜力。

Conclusion: QFL是一个快速发展的新兴领域，虽然面临诸多挑战，但在隐私保护、计算效率和安全性方面具有巨大潜力，需要进一步研究解决当前的技术难题。

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [25] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: 本文继续发展Cartan神经网络计划，重点研究非缩简对称空间的数学基础，包括分离器构造、制盖群、自守群、拉普拉斯绿函数和热核新表示等数学成果。


<details>
  <summary>Details</summary>
Motivation: 为了在神经网络中引入非缩简对称空间层，通过可解群同态映射连接，并在Tits Satake向量数据包框架下开展数学基础研究。

Method: 使用群论方法构造所有非缩简对称空间的分离器，研究制盖群和正规Fuchsian子群，探索双曲空间上拉普拉斯绿函数和热核的新表示，并通过Abel-Jacobi映射和Siegel谚函数构造Bolza曲面上拉普拉斯本征函数。

Result: 完成了所有非缩简对称空间分离器的群论构造，得到了负负四次曲面和Bolza曲面的均匀化结果，发现了双曲空间上拉普拉斯绿函数和热核的新表示形式，并提出了通过Abel-Jacobi映射构造Bolza曲面本征函数的新策略。

Conclusion: 本文为Cartan神经网络提供了强大的数学基础，在非缩简对称空间、自守群、谚函数等多个数学领域取得了重要进展，为下一步实际应用奠定了坚实的理论基础。

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [26] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: PAC-MCoFL是一个基于博弈论的多智能体强化学习框架，用于解决多服务提供商联邦学习中的非合作动态问题，通过联合优化客户端分配、自适应量化和资源分配来实现帕累托最优均衡。


<details>
  <summary>Details</summary>
Motivation: 多服务提供商联邦学习生态系统中的隐私约束和竞争利益阻碍了集中式优化，导致通信和计算资源无法有效协调，需要一种能够处理非合作动态的解决方案。

Method: 整合帕累托行动者-评论家(PAC)原则与期望回归，开发三元笛卡尔分解(TCAD)机制处理高维动作空间，并提出可扩展变体PAC-MCoFL-p降低计算复杂度。

Result: 在总奖励和超体积指标(HVI)上分别实现约5.8%和4.2%的提升，在规模化部署和多样化数据异构环境下更有效地平衡个体服务提供商和系统性能。

Conclusion: 该框架通过博弈论和强化学习的结合，成功解决了多服务提供商联邦学习中的资源优化问题，提供了理论收敛保证和实际性能优势。

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [27] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 提出了非平稳线性判别分析(NSLDA)和非平稳二次判别分析(NSQDA)框架，通过状态空间模型处理时间分布漂移问题，显著优于传统静态分类器


<details>
  <summary>Details</summary>
Motivation: 传统判别分析假设训练数据同分布，但在实际应用中观测数据随时间收集，类条件分布会发生漂移，导致静态分类器不可靠

Method: 将判别分析嵌入状态空间模型，使用卡尔曼平滑处理线性高斯动态，开发EM方法联合估计系统参数，GMM-卡尔曼方法同时恢复未观测时间标签和参数，对于非线性或非高斯漂移使用粒子平滑

Result: 大量仿真显示相比静态LDA、QDA和SVM基线有持续改进，对噪声、缺失数据和类别不平衡具有鲁棒性

Conclusion: 为时间分布漂移下的判别分析建立了统一且数据高效的基础框架

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [28] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 任务算术的理论基础：证明任务向量与损失梯度等价，单轮微调即可获得有效模型合并效果


<details>
  <summary>Details</summary>
Motivation: 尽管任务算术在实践中表现出色，但缺乏对其为何有效及何时有效的理论解释，需要建立严格的理论基础

Method: 建立任务向量与任务损失梯度的理论联系，证明在标准梯度下降下单轮微调生成的任务向量等价于负梯度缩放，并对前馈网络的多轮设置给出近似等价证明和误差界

Result: 在七个视觉基准测试中验证理论，证明首轮梯度在范数和方向上主导微调轨迹，单轮微调模型合并效果与完全收敛模型相当

Conclusion: 任务算术可视为近似多任务学习形式，其有效性源于早期训练动态，为模型合并提供了理论依据

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [29] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 提出基于遗传编程的对称相位紧急度函数，通过共享子树表示转向运动紧急度，显著提升交通信号控制策略性能


<details>
  <summary>Details</summary>
Motivation: 解决现有基于遗传编程的交通信号控制方法无法一致处理不同相位交通特征的问题

Method: 使用对称相位紧急度函数计算相位紧急度，表示为两个共享子树的聚合，每个子树代表相位中转向运动的紧急度

Result: 在CityFlow交通模拟器上基于多个真实数据集验证，对称紧急度函数表示显著优于传统GP表示

Conclusion: 该方法能够演化出有效、人类可理解且易于部署的交通信号控制策略

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [30] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: CommonKV是一种无需训练的方法，通过相邻参数共享实现跨层KV缓存压缩，使用SVD分解和自适应预算分配策略，在多种压缩比下优于现有方法，最高可达98%压缩率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的KV缓存随序列长度增长而急剧增加，现有跨层共享方法要么需要修改架构重新训练，要么在高压缩率下性能显著下降。

Method: 利用跨层隐藏状态的高相似性，通过奇异值分解(SVD)实现相邻参数权重共享，创建更易合并的潜在KV缓存，并引入基于余弦相似度的自适应预算分配策略。

Result: 在多个骨干模型和基准测试中，该方法在各种压缩比下始终优于现有的低秩和跨层方法，且与量化和驱逐方法正交，可组合使用。

Conclusion: CommonKV提供了一种有效的训练无关的KV缓存压缩方案，通过参数共享和自适应策略实现了高性能压缩，为解决LLM内存挑战提供了实用解决方案。

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [31] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了机器学习在微出行系统中的数据集、技术方法和应用，包括需求预测、能源管理和安全等方向，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 微出行系统已成为城市交通重要组成部分，但缺乏针对机器学习在微出行中应用的系统性文献综述，本文旨在填补这一空白。

Method: 收集和分析各种微出行相关数据集，从空间、时间和特征维度进行讨论；详细概述机器学习模型在微出行中的应用，包括优势、挑战和具体用例。

Result: 提供了微出行领域机器学习应用的全面综述，涵盖了数据集特征分析、模型技术讨论以及多个应用场景的深入探索。

Conclusion: 论文为未来研究者提供了该领域的系统性理解，并提出了需要解决的关键问题和未来研究方向，以促进微出行系统的优化发展。

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [32] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 一种无需微调LLM的自适应代理学习方法，通过内存基于强化学习实现低成本持续适应，在GAIA和DeepResearcher数据集上达到独占龚头的性能


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM代理方法的问题：静态手工反思流程太固化，而微调方法计算成本过高，需要一种更高效的自适应学习方案

Method: 采用内存增强马尔可夫决策过程(M-MDP)，通过神经案例选择策略指导行动决策。将过往经验存储在节体内存中，通过内存重写机制根据环境反馈更新策略，通过高效内存读取实现策略改进

Result: 在GAIA验证集上达到87.88% Pass@3，测试集79.40%，在DeepResearcher数据集上达到66.6% F1和80.4% PM，超过最佳培训方法。案例基内存在分布外任务上提升4.7%-9.6%的绝对收益

Conclusion: 该方法为开发能够持续实时学习的通用型LLM代理提供了可扩展且高效的路径，无需梯度更新，推动机器学习向开放式技能获取和深度研究场景进行

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [33] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: 本文发现并分析了基于ODE的扩散采样中的崩溃错误现象，即采样数据在局部数据空间中过度集中的问题，并揭示了其与得分学习在高噪声区域失配的关联。


<details>
  <summary>Details</summary>
Motivation: 尽管确定性采样器在扩散模型中广泛应用，但其潜在局限性尚未被充分探索。本文旨在识别和分析ODE扩散采样中先前未被认识的崩溃错误现象。

Method: 引入新的度量指标来量化崩溃错误，通过多种设置验证该现象，并观察得分学习中的跷跷板效应。应用现有的采样、训练和架构技术来实证支持对崩溃错误的解释。

Result: 研究发现崩溃错误在各种设置中普遍存在，高噪声区域的得分学习失配与确定性采样器动态共同导致崩溃错误。提供了ODE扩散采样中崩溃错误的密集实证证据。

Conclusion: 这项工作强调了得分学习与确定性采样之间相互作用的重要性，这是扩散模型中一个被忽视但基本的方面，需要进一步研究。

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [34] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN是一个基于图神经网络的时空克里金框架，通过解耦相位模块、动态数据驱动的元数据图建模和对抗迁移学习策略，提高时空模式的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决时空任务中数据不完整问题，当前模型在确保推断时空模式的有效性和泛化性方面存在困难，特别是在捕捉动态空间依赖性和时间偏移方面。

Method: 提出STA-GANN框架，包含三个核心组件：1）解耦相位模块感知和调整时间戳偏移；2）动态数据驱动的元数据图建模利用时序数据和元数据更新空间关系；3）对抗迁移学习策略确保泛化性。

Result: 在来自四个领域的九个数据集上进行广泛验证，理论和实验证据均表明STA-GANN具有优越性能。

Conclusion: STA-GANN是一个有效的时空克里金框架，能够显著提高时空模式推断的有效性和泛化能力，在多个领域的数据集上表现出色。

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [35] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: SPL-LNS是一种基于采样的神经大邻域搜索求解器，通过局部信息提案逃离局部最优，并使用新颖的后见之明重标记方法进行高效训练，在整数线性规划问题上显著优于现有神经LNS求解器。


<details>
  <summary>Details</summary>
Motivation: 解决传统贪婪神经LNS求解器存在的两个关键问题：(1) 贪婪提案容易陷入局部最优；(2) 如何提高长期样本效率。

Method: 将LNS建模为随机过程，提出SPL-LNS求解器，利用局部信息提案逃离局部最优，并开发后见之明重标记方法进行高效自生成数据训练。

Result: 实验结果表明SPL-LNS在不同规模的各类ILP问题上显著超越了先前的神经LNS求解器。

Conclusion: SPL-LNS通过采样增强和高效训练方法有效解决了神经LNS求解器的局部最优和样本效率问题，在组合优化中表现出优越性能。

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [36] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM是一种参数规模感知的稀疏微调框架，通过梯度权重比和熵引导掩码，在仅更新0.1%参数的情况下，性能超越全参数微调1.6%


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法只关注更新参数的绝对大小，而不考虑参数原始规模，导致模型行为变化有限。需要一种考虑参数相对规模的有效微调方法

Method: 提出梯度权重比和熵引导掩码(GEM)框架，优先更新相对于预训练值变化显著的参数，并根据参数值熵自适应确定每层需要微调的参数数量

Result: 在GLUE、SuperGLUE、GSM8k和MBPP等任务上，仅更新0.1%参数就能达到比全参数微调高1.6%的准确率

Conclusion: GEM通过参数规模感知和分布敏感的稀疏微调，有效利用了计算预算，在极低参数更新率下实现了优异的性能表现

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [37] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO是一种改进的降维技术，通过两阶段优化过程同时保留数据的局部和全局结构特征，在全局结构保持方面优于UMAP等现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的降维技术要么专注于保持局部邻域结构，要么专注于保持全局结构，但都无法同时很好地保留两者，这可能导致对高维数据流形结构的错误解读。

Method: UMATO将UMAP的优化过程分为两个阶段：第一阶段使用代表性点构建骨架布局，第二阶段在保持区域特征的同时投影剩余点。

Result: 定量实验表明UMATO在全局结构保持方面优于包括UMAP在内的广泛使用的降维技术，同时在可扩展性和初始化稳定性方面也表现更好。

Conclusion: UMATO通过有效捕获局部和全局结构，能够生成更忠实的投影，提高了使用降维技术进行可视化分析的可靠性。

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [38] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: PIANO是一个基于自回归建模的物理信息神经网络框架，通过显式地将未来预测条件于过去状态来解决时间相关PDE的稳定性问题，显著提升了预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在求解时间相关PDE时存在点式预测问题，忽略了动力系统的自回归特性，导致不稳定性和预测不准确。

Method: 重新设计PINNs为自回归框架，通过自监督展开机制进行训练，同时强制物理约束，显式地将未来预测条件于过去状态。

Result: 理论分析证明PIANO通过自回归建模实现稳定性，在挑战性时间相关PDE上达到最先进性能，在天气预报任务中超越现有方法。

Conclusion: PIANO框架通过自回归建模有效解决了PINNs的时间不稳定性问题，为时间相关PDE求解提供了更准确和稳定的解决方案。

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [39] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: 在尼日利亚北部空气质量预测研究中，Prophet模型在季节性和长期趋势数据中表现与LSTM相当或更优，而LSTM在结构突变数据中表现更好，表明简单模型在资源受限环境下可能优于复杂深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决低资源地区空气质量数据不规则和稀缺问题，系统比较先进机器学习模型在约束条件下的性能，为资源受限地区的环境管理提供实用指导。

Method: 使用2018-2023年月度观测数据，在19个州对多种污染物（CO、SO2、SO4）进行预测，比较LSTM网络和Facebook Prophet模型的性能。

Result: Prophet模型在季节性和长期趋势主导的时间序列中通常达到或超过LSTM的准确性，而LSTM在具有突然结构变化的数据集中表现更好。

Conclusion: 研究挑战了深度学习模型必然优于简单方法的假设，强调了模型与数据匹配的重要性，支持在资源受限环境中采用上下文敏感、计算高效的预测方法而非盲目追求复杂性。

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [40] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: FEST是一个用于评估合成表格数据的系统框架，集成了隐私指标、相似性指标和机器学习效用指标，提供全面的隐私-效用权衡分析。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏全面的合成数据评估框架，特别是在平衡隐私保护和数据效用方面存在空白。

Method: 开发FEST开源Python库，整合多种隐私指标（攻击型和距离型）、相似性指标和机器学习效用指标，在多个数据集上进行验证。

Result: FEST框架有效分析了不同合成数据生成模型的隐私-效用权衡，证明了其评估能力。

Conclusion: FEST提供了一个系统化的评估框架，有助于更好地理解和优化合成数据在隐私保护和数据效用之间的平衡关系。

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [41] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: 提出Chunked Data Shapley (C-DaSh)方法，通过分块处理和优化子集选择，大幅提升Data Shapley的计算效率，在保持高质量结果的同时实现80-2300倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着数据集规模和多样性不断增加，评估数据质量对可靠的机器学习分析变得至关重要。现有的Data Shapley计算方法在面对大规模数据集时面临严重的可扩展性挑战，限制了其实际应用。

Method: 将数据集分成可管理的块，使用优化的子集选择和单次迭代随机梯度下降来估计每个块的贡献值，显著减少计算时间。

Result: 在多样化的真实世界分类和回归任务上实证验证，C-DaSh在计算效率（80-2300倍加速）和检测低质量数据区域的准确性方面均优于现有的Shapley近似方法。

Conclusion: 该方法使得在大规模表格数据集上实际测量数据质量成为可能，支持分类和回归流程，为数据质量评估提供了实用的解决方案。

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [42] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文对联邦学习中的大语言模型调优进行了全面综述，提出了基于模型访问权限和参数效率优化的分类法，将FedLLM方法分为白盒、灰盒和黑盒技术，并讨论了黑盒推理API的新兴研究和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习方法通常需要访问LLM的内部信息，这在现实场景中往往受限，因此需要研究仅使用推理能力的黑盒FedLLM方法来应对这些限制。

Method: 提出一个分类法，沿两个轴对现有研究进行分类：基于模型访问权限和基于参数效率的优化。将FedLLM方法分为白盒、灰盒和黑盒技术，并评述各类代表性方法。

Result: 建立了FedLLM技术的系统分类框架，识别了不同访问权限级别下的优化方法，并总结了黑盒推理API研究的最新进展。

Conclusion: 该综述为联邦学习中大语言模型调优提供了系统的分类和分析框架，指出了黑盒FedLLM作为有前景的研究方向，并明确了未来研究面临的开放挑战。

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [43] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: 提出了一种学习稀疏二元表示（辅助知识概念）的深度学习模型，用于改进知识追踪和个性化推荐，超越了人工标注的局限性


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型依赖人工标注的知识概念，但这些标注可能不完整、有错误或过于笼统，限制了模型的准确性和推荐效果

Method: 使用深度学习模型学习练习的稀疏二元表示（辅助知识概念），每个位表示潜在概念的存在与否，可与经典模型和现代深度学习架构兼容

Result: 辅助知识概念提高了学生建模的预测性能，增强了基于强化学习的推荐策略和规划方法，在模拟学生环境中显著改善了学习效果

Conclusion: 学习到的辅助知识概念能够捕捉超越人工标注的概念结构，有效提升知识追踪和个性化推荐系统的性能

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [44] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: REFINE是一个教师-学生框架，通过结构化错误分析和针对性反馈来提升多模态大语言模型的推理能力，相比现有方法显著提高了推理效率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏结构化框架来分析多模态环境中的错误，特别是在整合视觉和文本输入时增加了复杂性，需要系统化的错误缓解机制。

Method: 提出REFINE框架，使用三种结构化查询（Feed-Target、Feed-Check、Feed-Path）构建针对性反馈，优先处理相关视觉信息、诊断关键失败点并制定纠正措施，优化结构化反馈检索。

Result: 实验结果显示显著的速度提升、计算成本降低以及成功的泛化能力，证明了REFINE在增强多模态推理方面的潜力。

Conclusion: REFINE通过系统化的错误结构化分析和针对性反馈机制，为多模态大语言模型提供了高效的错误缓解框架，在推理效率和可扩展性方面表现出色。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [45] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: 提出基于意图的威胁评估框架，整合能力和意图分析，通过多任务学习提升空间网络威胁检测的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统方法将可靠性和安全性分开分析会导致对系统特定标准的过拟合，需要更全面的网络物理感知框架

Method: 三步骤框架：1)信号特征提取算法；2)多任务学习架构（可靠性能力评估+意图解析）；3)可适应不同安全可靠性需求的威胁评估

Result: 提出的框架在威胁检测和评估方面优于传统顺序方法，增强了鲁棒性

Conclusion: 该框架使具有新兴层间链路的空间网络能够有效应对复杂威胁场景

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [46] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: 通过强化学习训练的专业化8-32B OwkinZero模型在生物医学推理任务上显著超越大型商业LLM，展现了跨任务的泛化能力


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在生物医学推理任务上存在明显短板，限制了其在转化医学和生物医学发现中的应用

Method: 创建包含30万个问答对的8个基准数据集，通过基于可验证奖励的强化学习策略对开源LLM进行后训练

Result: 专业化OwkinZero模型在生物基准测试中大幅超越最先进商业LLM，单任务训练模型在新任务上表现优于基础模型

Conclusion: 针对性的强化学习和精心策划的数据可以解锁专业化模型的泛化性能，加速AI驱动的生物医学发现

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [47] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM-VAE的无监督在线学习框架，用于检测供水管网中的管道堵塞和背景泄漏故障，在非平稳条件下实现鲁棒检测和自适应。


<details>
  <summary>Details</summary>
Motivation: 供水管网对公共福祉和经济稳定至关重要，但面临管道堵塞和背景泄漏等挑战，这些挑战因数据非平稳性和标记数据有限等操作约束而加剧。

Method: 结合长短期记忆变分自编码器(LSTM-VAE)与双重漂移检测机制的无监督在线学习框架，能够检测集体异常(管道堵塞)和概念漂移(背景泄漏)。

Result: 在两个实际供水管网上的实验表明，该方法在检测异常和适应循环漂移方面始终优于强基线方法。

Conclusion: 该方法在动态供水管网环境中进行无监督事件检测方面表现出有效性，其轻量级、内存高效的设计支持实时边缘级监控。

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [48] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: NIAQUE是一个用于概率回归迁移学习的神经网络模型，通过排列不变性实现可迁移性，在预训练和微调后能提升回归任务性能，在Kaggle竞赛中表现优于树模型和神经基础模型。


<details>
  <summary>Details</summary>
Motivation: 概率回归领域的迁移学习研究相对不足，需要开发能够有效进行迁移学习的概率回归模型。

Method: 提出NIAQUE模型，利用排列不变性设计，先在多样化下游回归数据集上进行预训练，然后在特定目标数据集上进行微调。

Result: NIAQUE在个体回归任务中表现出性能提升，在Kaggle竞赛中优于树模型、TabPFN和TabDPT等基线模型。

Conclusion: NIAQUE是一个强大且可扩展的概率回归框架，通过迁移学习有效提升了预测性能。

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [49] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille是一个用于轮廓数据的深度学习框架，通过复值循环卷积实现旋转和循环平移等变性，并在形状分类、重建和轮廓回归任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 轮廓数据在多个领域都很常见，如计算机视觉中的物体边界、气象学中的等值线等。深度学习模型需要对这些数据的旋转和循环平移具有等变性，因为输入轮廓的旋转会导致输出的相应旋转，且轮廓的起始点选择是任意的。

Method: 使用复值循环卷积实现旋转和循环平移等变性，并引入了等变非线性、粗化层和全局池化层来获得下游任务的不变表示。

Result: 在形状分类、重建和轮廓回归等实验中证明了RotaTouille框架的有效性。

Conclusion: RotaTouille框架成功实现了对轮廓数据的旋转和循环平移等变性，为处理轮廓数据提供了一种有效的深度学习解决方案。

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [50] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: 这篇论文通过分析GitHub上204个使用偏见检测和缓解API的代码库，研究了开发者在实际中如何使用这些公平性API库以及遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习系统在敏感环境中做出生活改变的决定，确保系统不会对特定群体产生歧视是至关重要的。需要了解开源公平性API库在实际中的使用情况和挑战。

Method: 定性研究方法，分析了1885个GitHub仓库中的204个使用13种偏见检测缓解API的项目，研究其使用场景、用途和遇到的问题。

Result: 发现这些API主要用于两种目的：学习和解决实际问题，涉及17种不同使用场景。开发者对偏见检测缓解技术不熟悉，遇到许多问题排除困难，经常需要寻求帮助和资源。

Conclusion: 研究结果对未来偏见相关软件工程研究具有重要意义，同时为教育工作者开发更先进的课程提供了指导。

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [51] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 研究大学招生中的公平队列选择问题，比较一次性设置和顺序设置的公平政策优化方法


<details>
  <summary>Details</summary>
Motivation: 解决大学招生中从未知人群进行公平队列选择的问题，特别关注招生政策的透明性和公平性

Method: 使用一次性设置（预先固定透明政策）和顺序设置（根据新申请者数据更新政策），通过基于历史招生数据训练的人口模型来优化招生政策

Result: 分析了一次性设置下所得政策的公平性特性，包括精英制度和群体平等

Conclusion: 提出了两种不同的招生政策优化框架，并评估了它们在保证公平性方面的效果

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [52] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级、数据高效的图神经网络模型，用于预测多种拓扑结构主动无线电路的关键性能指标，在减少数据需求的同时显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统无线电路模拟工具计算成本高，现有的机器学习假模型需要大量数据才能在不同拓扑结构上实现良好的泛化能力，且对偏斜和多模态性能指标的建模效果不佳。

Method: 采用设备端子级别的图神经网络(GNN)模型，通过针脚级转换将电路转换为图结构，捕捉二极管级对称性和细粒度连接信息。模型包含掩码自回归流(MAF)输出头，以提高对复杂分布目标的建模稳健性。

Result: 在多个数据集上实验显示，对称平均绝对百分比误差(sMAPE)和平均相对误差(MRE)分别为2.40%和2.91%。与之前的方法相比，MRE提高了3.14倍，同时训练样本数量减少2.24倍。

Conclusion: 该方法通过针脚级电路转图和能够建模复杂密度的ML架构，实现了快速且准确的无线电路设计自动化，为高效的RF电路性能预测提供了有效解决方案。

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [53] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: 提出了Doctor方法，通过双重检查机制改进离线强化学习中的目标对齐问题，在数据集内外都能实现精确的性能控制


<details>
  <summary>Details</summary>
Motivation: 现有RvS方法如Decision Transformer在目标回报对齐方面存在不足，特别是在数据集中代表性不足的回报区间或超出数据集范围时，无法可靠地将实际回报与指定目标对齐

Method: Doctor方法采用双重检查机制，通过目标对齐来验证和调整Transformer的输出，确保策略性能与指定目标回报精确匹配

Result: 在动态治疗机制基准EpiCare上，Doctor方法能够有效调节治疗策略的激进程度，平衡治疗效果回报与不良事件风险

Conclusion: Doctor方法显著提升了离线强化学习中目标对齐的准确性和灵活性，为需要精确性能控制的实际应用提供了有效解决方案

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [54] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 研究探索大型语言模型能否从自然语言规则描述中实现棋盘游戏的代码生成，通过测试三个先进LLM在12款游戏上的表现，发现Claude 3.7 Sonnet表现最佳，55.6%的游戏实现无错误。


<details>
  <summary>Details</summary>
Motivation: 利用LLM从自然语言规则自动生成棋盘游戏代码，旨在开发LLM辅助的快速棋盘游戏代码生成框架，减少人工实现的时间消耗。

Method: 使用三个先进LLM（Claude、DeepSeek和ChatGPT）对12款流行和冷门游戏进行编码测试，在自由形式和提出的Boardwalk通用游戏API两种模式下进行，通过匿名化处理避免预训练知识影响，测试可玩性和规则符合性。

Result: 方法被证明可行，最佳模型Claude 3.7 Sonnet实现了55.6%的无错误游戏。虽然API合规性增加了错误频率，但错误严重程度更依赖于LLM本身。

Conclusion: 研究为创建集成此过程的框架奠定了基础，使棋盘游戏的开发更加便捷，并指出了未来改进方向。

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [55] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: BAD-ACTS基准测试评估LLM智能体系统安全性，发现单个恶意智能体可高效诱导有害行为，并提出基于消息监控的防御方案


<details>
  <summary>Details</summary>
Motivation: 确保智能体系统安全使用需要全面了解其在受攻击时可能表现的各种恶意行为，评估LLM智能体系统在遭受攻击时的鲁棒性

Method: 提出智能体系统危害分类法，构建BAD-ACTS基准测试，包含4个不同应用环境的智能体系统实现和188个有害行为示例，分析攻击者控制单个智能体时的系统安全性

Result: 攻击成功率很高，单个敌对智能体对系统安全有显著影响，即使使用简单提示防御策略，攻击仍然有效

Conclusion: 提出的基于消息监控的防御更有效，BAD-ACTS基准为智能体系统安全研究提供了多样化测试平台

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [56] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine是一种模型无关的即插即用后处理方法，通过结合成对排序的专家知识来改进回归预测，在数据稀缺情况下显著提升准确性


<details>
  <summary>Details</summary>
Motivation: 深度学习回归器在标签充足时表现优异，但在数据稀缺情况下准确性下降，需要利用专家知识来改善回归性能

Method: 通过逆方差加权将基础回归器的输出与基于排名的估计相结合，无需重新训练，仅需少量成对比较即可实现改进

Result: 在分子属性预测任务中，仅使用20个通过通用大语言模型获得的成对比较，就实现了高达10%的平均绝对误差相对减少

Conclusion: RankRefine具有实用性和广泛适用性，特别是在低数据设置下，通过人类专家或通用LLM提供的排名即可跨领域改进回归性能

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [57] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: 该论文探讨了零样本强化学习在现实世界应用中的挑战，提出了应对数据质量、可观测性和数据可用性三大约束的方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多问题需要强化学习解决，但无法廉价模拟新数据，导致训练环境与部署环境存在不可避免的错位，需要开发能够在零样本条件下泛化的RL方法。

Method: 提出了一套应对三大约束（数据质量、可观测性、数据可用性）的零样本强化学习方法，通过一系列实证研究验证方法的有效性。

Result: 研究揭示了现有方法的缺陷，并证明了所提出技术能够有效弥补这些缺陷，使RL方法更接近实际部署应用。

Conclusion: 这项工作为开发能够解决现实世界问题的强化学习方法迈出了重要一步，特别是在处理数据限制和环境错位方面取得了进展。

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [58] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: 提出了MuST2-Learn框架，通过多视角时空类型学习来预测市政服务请求的服务时间，在真实数据集上比现有方法减少至少32.5%的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 现有的市政311系统缺乏对服务请求处理时间的预测，导致透明度低、居民满意度下降和重复查询增加。预测服务时间面临时空相关性复杂、异质服务类型交互和同类请求内服务时长差异大等挑战。

Method: 提出多视角时空类型学习框架MuST2-Learn，包含类型间编码器捕捉异质服务类型关系、类型内变异编码器建模同类请求服务时间变异，以及时空编码器捕捉各类型的时空相关性。

Result: 在两个真实数据集上的实验表明，MuST2-Learn相比最先进方法至少减少32.5%的平均绝对误差。

Conclusion: MuST2-Learn通过联合建模空间、时间和服务类型维度，有效解决了市政服务时间预测的复杂挑战，显著提升了预测性能。

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [59] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: FLAMES框架系统评估数学推理数据合成策略，发现增加问题复杂度、保持高覆盖率比筛选可靠解更重要，并开发了新的合成策略和数据集，在多个数学基准上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 现有改进LLM数学推理的合成数据研究采用不同实验设置，难以比较不同数据合成策略的效果，需要系统研究影响合成数学推理数据性能的各种因素

Method: 提出FLAMES框架，系统研究10种现有数据合成策略和多个影响因素，设计两种新的数据合成策略用于提升跨域泛化和鲁棒性，开发FLAMES数据集

Result: FLAMES数据集在OlympiadBench(+15.7)、CollegeMath(+4.5)、GSMPlus(+6.5)、MATH(+3.1)等基准上超越公开数据集，微调Qwen2.5-Math-7B达到81.4%的MATH分数，超过更大模型

Conclusion: 数据合成策略中增加问题复杂度和保持高覆盖率是关键，FLAMES框架和数据集为数学推理数据合成提供了系统评估方法和有效解决方案

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [60] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: RLPF是一个结合强化学习和物理反馈的3D分子生成框架，通过力场评估奖励函数指导扩散模型生成更稳定的分子结构


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成3D分子结构时难以保证物理合理性，特别是无法生成符合力场平衡条件的稳定结构

Method: 提出RLPF框架，将3D分子生成建模为马尔可夫决策过程，使用近端策略优化微调等变扩散模型，并引入基于力场评估的物理奖励函数

Result: 在QM9和GEOM-drug数据集上实验表明，RLPF显著提高了分子稳定性，优于现有方法

Conclusion: 将物理反馈融入生成模型对提高分子结构质量具有重要价值

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [61] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: 本文提出了一种使基于LSTM的太阳风暴预测模型可解释的方法，通过注意力机制和事后模型无关技术来理解模型预测的推理过程。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通常被视为黑盒，缺乏透明度使得难以理解模型预测背后的推理过程，特别是在太阳风暴预测这种关键应用中。

Method: 使用带有注意力机制的LSTM网络建模太阳活动区的时间序列数据，并应用事后模型无关技术来解释模型预测。

Result: 成功实现了首个基于LSTM的太阳风暴预测模型的可解释性，能够阐明输入序列对预测输出的贡献因素。

Conclusion: 该方法为深度学习太阳风暴预测模型提供了可解释性，增强了模型的可靠性和可信度，为空间天气预报提供了更透明的工具。

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [62] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: RL微调可以恢复SFT导致的OOD性能损失，主要通过修正奇异向量方向偏移而非寻找新解决方案，低秩和浅层恢复策略可有效恢复70-80%性能


<details>
  <summary>Details</summary>
Motivation: 重新审视监督微调(SFT)和强化学习微调(RL-FT)如何影响模型表示和分布外(OOD)性能，解决SFT导致的过拟合和分布偏移问题

Method: 使用24点纸牌游戏的OOD变体和基于频谱的诊断方法，分析奇异向量方向和奇异值变化，探索低秩和浅层恢复策略

Result: RL-FT可恢复大部分SFT导致的OOD性能损失，方向偏移集中在最大和最小奇异值方向，低秩(前20%)和浅层(前25%)恢复可达到70-80%性能恢复

Conclusion: RL主要对抗SFT引起的方向漂移而非寻找新解，频谱感知分析提供了低秩UV合并和浅层重置等低成本恢复方法

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [63] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 本文研究发现稀疏自编码器(SAE)的L0超参数设置对特征学习至关重要，过高或过低都会导致特征混合，提出了确定正确L0值的方法


<details>
  <summary>Details</summary>
Motivation: 现有研究将L0视为自由参数，但缺乏对其影响特征学习质量的系统研究，需要探索L0设置对SAE学习真实特征能力的影响

Method: 研究BatchTopK SAE在不同L0值下的表现，分析特征混合现象，开发确定正确L0值的方法并在玩具模型和LLM中进行验证

Result: 发现L0设置不当会导致特征混合：过低时SAE会合并相关特征，过高时会产生退化解。提出的方法能准确找到真实L0值，且发现常用SAE的L0设置普遍偏低

Conclusion: 正确设置L0超参数对训练具有正确特征的SAE至关重要，现有方法大多L0设置过低，需要精确调整以获得最佳特征表示

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [64] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 基础模型在隐私敏感应用中的聚合学习适配方案，解决边缘设备计算资源有限和标签数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 基础模型需要适配下游任务，但隐私规定限制了云端模型直接访问边缘数据。现有聚合学习方法没有充分考虑边缘设备的计算资源有限和标签数据稀缺问题

Method: 提出实用半监督聚合学习(PSSFL)模型，其中边缘设备只有未标签低分辨率数据，服务器有限量标签高分辨率数据。提出聚合专家混合(FedMox)框架，使用稀疏专家混合结构和空间路由器来对齐不同分辨率特征，采用软混合策略稳定半监督学习

Result: 在自动驾驶数据集上的实验表明，FedMox能够在聚合场景下有效适配基础模型，显著提升性能同时控制边缘设备的内存成本

Conclusion: 该工作为聚合场景下的可扩展和隐私保护基础模型适配探索了新路径

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [65] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: 比较了儿童语音识别中从头训练与微调预训练模型的性能差异，发现SSL表示存在成人语音偏见，从头训练可缓解此问题，模型规模在1B参数前持续改进，开源模型更适合儿童语音研究


<details>
  <summary>Details</summary>
Motivation: 儿童语音识别面临声学变异性和标注数据有限的问题，现有研究主要关注微调成人ASR模型，但缺乏与从头训练方法的系统比较

Method: 使用ESPnet框架，在多个数据集上比较从头训练方法，评估不同SSL表示（WavLM、XEUS）和解码器架构，分析模型规模和年龄相关的ASR性能

Result: SSL表示存在成人语音偏见，从头训练儿童语音可缓解偏见；模型规模在1B参数前性能持续提升，之后趋于平稳；专有模型（如Whisper）在儿童语音处理上存在局限

Conclusion: 开源数据模型对于可靠的儿童语音研究至关重要，公开基准为鲁棒儿童语音处理提供了训练策略见解，从头训练是缓解SSL偏见有效方法

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [66] [Interpretable Kernels](https://arxiv.org/abs/2508.15932)
*Patrick J. F. Groenen,Michael Greenacre*

Main category: stat.ML

TL;DR: 本文证明在特征数多于观测数的情况下，核方法解可以重新表示为原始特征的线性组合，从而保持可解释性；在特征数少于观测数时，提出了最小二乘近似方法。


<details>
  <summary>Details</summary>
Motivation: 核方法虽然广泛应用于非线性预测，但存在一个主要缺点：失去了对原始特征的解释能力。本文旨在解决核方法缺乏可解释性的问题。

Method: 对于宽矩阵（特征数多于观测数），将核解重新表示为原始特征的线性组合和特殊度量的岭惩罚；对于特征数少于观测数的情况，使用核矩阵的最小二乘近似方法。

Result: 研究表明，对于任何具有系数最小化和岭惩罚的函数（如核逻辑回归、核泊松回归），都可以获得与原始特征线性组合相同的预测值，从而保持可解释性。

Conclusion: 这项工作为可解释人工智能做出了贡献，通过将核方法解重新表示为原始特征的线性组合，解决了核方法缺乏可解释性的根本问题。

Abstract: The use of kernels for nonlinear prediction is widespread in machine
learning. They have been popularized in support vector machines and used in
kernel ridge regression, amongst others. Kernel methods share three aspects.
First, instead of the original matrix of predictor variables or features, each
observation is mapped into an enlarged feature space. Second, a ridge penalty
term is used to shrink the coefficients on the features in the enlarged feature
space. Third, the solution is not obtained in this enlarged feature space, but
through solving a dual problem in the observation space. A major drawback in
the present use of kernels is that the interpretation in terms of the original
features is lost. In this paper, we argue that in the case of a wide matrix of
features, where there are more features than observations, the kernel solution
can be re-expressed in terms of a linear combination of the original matrix of
features and a ridge penalty that involves a special metric. Consequently, the
exact same predicted values can be obtained as a weighted linear combination of
the features in the usual manner and thus can be interpreted. In the case where
the number of features is less than the number of observations, we discuss a
least-squares approximation of the kernel matrix that still allows the
interpretation in terms of a linear combination. It is shown that these results
hold for any function of a linear combination that minimizes the coefficients
and has a ridge penalty on these coefficients, such as in kernel logistic
regression and kernel Poisson regression. This work makes a contribution to
interpretable artificial intelligence.

</details>


### [67] [Optimal Dynamic Regret by Transformers for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2508.16027)
*Baiyuan Chen,Shinji Ito,Masaaki Imaizumi*

Main category: stat.ML

TL;DR: Transformer在非平稳环境中能够实现近乎最优的动态遗憾边界，理论证明和实验验证表明其可以近似处理非平稳环境的策略，并在上下文学习中学习这些近似器。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer在强化学习的上下文学习方面已有理论和实证基础，但其在非平稳环境中的行为仍未被充分理解，本研究旨在填补这一空白。

Method: 通过理论证明Transformer能够近似处理非平稳环境的策略，并在上下文学习设置中学习这些近似器，同时通过实验验证其性能。

Result: 实验结果显示，Transformer在非平稳环境中能够匹配甚至超越现有的专家算法性能。

Conclusion: Transformer在非平稳强化学习环境中表现出色，能够实现近乎最优的动态遗憾边界，为处理非平稳环境提供了有效的解决方案。

Abstract: Transformers have demonstrated exceptional performance across a wide range of
domains. While their ability to perform reinforcement learning in-context has
been established both theoretically and empirically, their behavior in
non-stationary environments remains less understood. In this study, we address
this gap by showing that transformers can achieve nearly optimal dynamic regret
bounds in non-stationary settings. We prove that transformers are capable of
approximating strategies used to handle non-stationary environments and can
learn the approximator in the in-context learning setup. Our experiments
further show that transformers can match or even outperform existing expert
algorithms in such environments.

</details>


### [68] [A Sharp KL-Convergence Analysis for Diffusion Models under Minimal Assumptions](https://arxiv.org/abs/2508.16306)
*Nishant Jain,Tong Zhang*

Main category: stat.ML

TL;DR: 通过精细的分析改进了源于正向反向ODE的模型的收敏性，在无光滑假设下实现了更好的步长依赖性


<details>
  <summary>Details</summary>
Motivation: 现有的模型在KL散度上对数据维度d和误差参数ε的依赖性有限，需要改进收敏性分析

Method: 将生成过程模型为两步组合：先通过反向ODE控制Wasserstein误差，再通过小噪声步将其转换为KL散度约束

Result: 实现了ε的逆一次依赖，需要的步数从之前的ε的逆二次改善为ε的逆一次，具体为O~(d log^{3/2}(1/δ)/ε)

Conclusion: 通过新题的两步构造和分析方法，在无光滑假设下显著改善了模型的收敏性界

Abstract: Diffusion-based generative models have emerged as highly effective methods
for synthesizing high-quality samples. Recent works have focused on analyzing
the convergence of their generation process with minimal assumptions, either
through reverse SDEs or Probability Flow ODEs. The best known guarantees,
without any smoothness assumptions, for the KL divergence so far achieve a
linear dependence on the data dimension $d$ and an inverse quadratic dependence
on $\varepsilon$. In this work, we present a refined analysis that improves the
dependence on $\varepsilon$. We model the generation process as a composition
of two steps: a reverse ODE step, followed by a smaller noising step along the
forward process. This design leverages the fact that the ODE step enables
control in Wasserstein-type error, which can then be converted into a KL
divergence bound via noise addition, leading to a better dependence on the
discretization step size. We further provide a novel analysis to achieve the
linear $d$-dependence for the error due to discretizing this Probability Flow
ODE in absence of any smoothness assumptions. We show that
$\tilde{O}\left(\tfrac{d\log^{3/2}(\frac{1}{\delta})}{\varepsilon}\right)$
steps suffice to approximate the target distribution corrupted with Gaussian
noise of variance $\delta$ within $O(\varepsilon^2)$ in KL divergence,
improving upon the previous best result, requiring
$\tilde{O}\left(\tfrac{d\log^2(\frac{1}{\delta})}{\varepsilon^2}\right)$ steps.

</details>


### [69] [Deep Intrinsic Coregionalization Multi-Output Gaussian Process Surrogate with Active Learning](https://arxiv.org/abs/2508.16434)
*Chun-Yi Chang,Chih-Li Sung*

Main category: stat.ML

TL;DR: 提出了deepICMGP模型，通过分层协区域化结构扩展深度高斯过程到多输出场景，有效建模非线性依赖关系，并整合主动学习策略优化序列设计任务。


<details>
  <summary>Details</summary>
Motivation: 深度高斯过程在扩展到多输出设置时面临依赖关系建模效率的挑战，传统多输出高斯过程存在关键局限性。

Method: 基于内在协区域化模型(ICM)，引入跨层的分层协区域化结构，构建deepICMGP代理模型，并整合主动学习策略进行序列设计优化。

Result: 与最先进模型相比表现出竞争性性能，能够有效建模多输出间的非线性和结构化依赖关系。

Conclusion: deepICMGP为解决计算机仿真实验中多输出系统的建模和优化设计提供了有效的解决方案。

Abstract: Deep Gaussian Processes (DGPs) are powerful surrogate models known for their
flexibility and ability to capture complex functions. However, extending them
to multi-output settings remains challenging due to the need for efficient
dependency modeling. We propose the Deep Intrinsic Coregionalization
Multi-Output Gaussian Process (deepICMGP) surrogate for computer simulation
experiments involving multiple outputs, which extends the Intrinsic
Coregionalization Model (ICM) by introducing hierarchical coregionalization
structures across layers. This enables deepICMGP to effectively model nonlinear
and structured dependencies between multiple outputs, addressing key
limitations of traditional multi-output GPs. We benchmark deepICMGP against
state-of-the-art models, demonstrating its competitive performance.
Furthermore, we incorporate active learning strategies into deepICMGP to
optimize sequential design tasks, enhancing its ability to efficiently select
informative input locations for multi-output systems.

</details>


### [70] [Underdamped Langevin MCMC with third order convergence](https://arxiv.org/abs/2508.16485)
*Maximilian Scott,Dáire O'Kane,Andraž Jelinčič,James Foster*

Main category: stat.ML

TL;DR: 本文提出了一种新的欠阻尼朗之万扩散数值方法，在强对数凹目标分布下实现了2-Wasserstein距离的非渐近误差分析，并在三阶导数Lipschitz连续假设下获得了更快的收敛速度


<details>
  <summary>Details</summary>
Motivation: 现有的朗之万MCMC算法在复杂目标分布下的采样效率有待提高，特别是在高阶光滑性假设下缺乏高效的梯度方法

Method: 提出基于欠阻尼朗之万扩散的数值方法，利用目标函数梯度、Hessian和三阶导数的Lipschitz连续性假设，分析2-Wasserstein距离的采样误差

Result: 在梯度、Hessian Lipschitz连续假设下，分别达到O(√d/ε)和O(√d/√ε)步数；在三阶导数Lipschitz连续假设下，达到O(√d/ε^(1/3))步数，这是首个具有三阶收敛的梯度方法

Conclusion: 该方法在贝叶斯逻辑回归实验中表现出与现有欠阻尼朗之万MCMC和NUTS算法相当的性能，为高阶光滑目标分布提供了高效的采样方案

Abstract: In this paper, we propose a new numerical method for the underdamped Langevin
diffusion (ULD) and present a non-asymptotic analysis of its sampling error in
the 2-Wasserstein distance when the $d$-dimensional target distribution
$p(x)\propto e^{-f(x)}$ is strongly log-concave and has varying degrees of
smoothness. Precisely, under the assumptions that the gradient and Hessian of
$f$ are Lipschitz continuous, our algorithm achieves a 2-Wasserstein error of
$\varepsilon$ in $\mathcal{O}(\sqrt{d}/\varepsilon)$ and
$\mathcal{O}(\sqrt{d}/\sqrt{\varepsilon})$ steps respectively. Therefore, our
algorithm has a similar complexity as other popular Langevin MCMC algorithms
under matching assumptions. However, if we additionally assume that the third
derivative of $f$ is Lipschitz continuous, then our algorithm achieves a
2-Wasserstein error of $\varepsilon$ in
$\mathcal{O}(\sqrt{d}/\varepsilon^{\frac{1}{3}})$ steps. To the best of our
knowledge, this is the first gradient-only method for ULD with third order
convergence. To support our theory, we perform Bayesian logistic regression
across a range of real-world datasets, where our algorithm achieves competitive
performance compared to an existing underdamped Langevin MCMC algorithm and the
popular No U-Turn Sampler (NUTS).

</details>
