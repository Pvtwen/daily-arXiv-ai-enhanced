<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 44]
- [cs.LG](#cs.LG) [Total: 110]
- [stat.ML](#stat.ML) [Total: 10]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks](https://arxiv.org/abs/2508.11640)
*Danny Scott,William LaForest,Hritom Das,Ioannis Polykretis,Catherine D. Schuman,Charles Rizzo,James Plank,Sai Swaminathan*

Main category: eess.SP

TL;DR: Vibe2Spike是一个无电池无线传感框架，利用压电盘、齐纳二极管和LED组成的超低成本标签，通过可见光通信和脉冲神经网络实现基于振动的活动识别。


<details>
  <summary>Details</summary>
Motivation: 现有传感解决方案在电池维护、无线传输开销和数据处理复杂性方面存在能量、可扩展性和可靠性之间的权衡问题，需要一种更高效的无电池传感方案。

Method: 使用压电盘收集振动能量，通过LED发射稀疏可见光脉冲，事件相机捕获光脉冲，采用EONS框架优化的脉冲神经网络进行分类。

Result: 在五个设备类别上实现了94.9%的平均分类准确率，分析了不同时间分箱策略的延迟-准确性权衡。

Conclusion: Vibe2Spike展示了一种可扩展、高能效的无电池智能环境实现方法。

Abstract: The deployment of dense, low-cost sensors is critical for realizing
ubiquitous smart environments. However, existing sensing solutions struggle
with the energy, scalability, and reliability trade-offs imposed by battery
maintenance, wireless transmission overhead, and data processing complexity. In
this work, we present Vibe2Spike, a novel battery-free, wireless sensing
framework that enables vibration-based activity recognition using visible light
communication (VLC) and spiking neural networks (SNNs). Our system uses
ultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and
an LED, which harvest vibration energy and emit sparse visible light spikes
without requiring batteries or RF radios. These optical spikes are captured by
event cameras and classified using optimized SNN models evolved via the EONS
framework. We evaluate Vibe2Spike across five device classes, achieving 94.9\%
average classification fitness while analyzing the latency-accuracy trade-offs
of different temporal binning strategies. Vibe2Spike demonstrates a scalable,
and energy-efficient approach for enabling intelligent environments in a
batteryless manner.

</details>


### [2] [Data-driven RF Tomography via Cross-modal Sensing and Continual Learning](https://arxiv.org/abs/2508.11654)
*Yang Zhao,Tao Wang,Said Elhadi*

Main category: eess.SP

TL;DR: 基于深度学习的跨模态学习和持续学习方法，提出DRIFT框架来提高地下根块藤的无线电射频成像的准确性和稳健性


<details>
  <summary>Details</summary>
Motivation: 解决动态环境下地下目标检测的挑战，尤其是在RF信号发生显著变化时保持准确的成像性能

Method: 设计无线电射频和视觉传感器的跨模态感知系统，采用跨模态学习方法训练RF成像DNN模型，并在动态环境中使用持续学习自动更新模型

Result: 平均相当直径误差为2.29cm，比最先进方法提高23.2%的性能改善

Conclusion: DRIFT框架能够在动态环境下实现高准确和高稳健性的地下根块藤成像，为地下目标检测提供了有效解决方案

Abstract: Data-driven radio frequency (RF) tomography has demonstrated significant
potential for underground target detection, due to the penetrative nature of RF
signals through soil. However, it is still challenging to achieve accurate and
robust performance in dynamic environments. In this work, we propose a
data-driven radio frequency tomography (DRIFT) framework with the following key
components to reconstruct cross section images of underground root tubers, even
with significant changes in RF signals. First, we design a cross-modal sensing
system with RF and visual sensors, and propose to train an RF tomography deep
neural network (DNN) model following the cross-modal learning approach. Then we
propose to apply continual learning to automatically update the DNN model, once
environment changes are detected in a dynamic environment. Experimental results
show that our approach achieves an average equivalent diameter error of 2.29
cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and
dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.

</details>


### [3] [Inductive transfer learning from regression to classification in ECG analysis](https://arxiv.org/abs/2508.11656)
*Ridma Jayasundara,Ishan Fernando,Adeepa Fernando,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: eess.SP

TL;DR: 这篇论文探索了使用合成ECG数据训练深度学习模型，并通过从回归任务到分类任务的迁移学习来提高对真实ECG数据的分类性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球死因主因，早期诊断可预防死亡。ECG是关键诊断工具，但病人隐私问题促使人们寻找合成数据方案。

Method: 使用深度学习模型预测四个关键心臔参数（心率、PR间期、QT间期、QRS复合波），然后利用这些回归模型进行迁移学习完成5类ECG信号分类。

Result: 迁移学习从回归到分类能够提高分类性能，为更好利用合成ECG数据提供了可行方案。

Conclusion: 这种迁移学习方法有助于最大化利用现有数据，推动深度学习在ECG预测领域的应用进步。

Abstract: Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,
accounting for over 30% of global deaths according to the World Health
Organization (WHO). Importantly, one-third of these deaths are preventable with
timely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive
method for recording the electrical activity of the heart, is crucial for
diagnosing CVDs. However, privacy concerns surrounding the use of patient ECG
data in research have spurred interest in synthetic data, which preserves the
statistical properties of real data without compromising patient
confidentiality. This study explores the potential of synthetic ECG data for
training deep learning models from regression to classification tasks and
evaluates the feasibility of transfer learning to enhance classification
performance on real ECG data. We experimented with popular deep learning models
to predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,
QT interval, and QRS complex-using separate regression models. Subsequently, we
leveraged these regression models for transfer learning to perform 5-class ECG
signal classification. Our experiments systematically investigate whether
transfer learning from regression to classification is viable, enabling better
utilization of diverse open-access and synthetic ECG datasets. Our findings
demonstrate that transfer learning from regression to classification improves
classification performance, highlighting its potential to maximize the utility
of available data and advance deep learning applications in this domain.

</details>


### [4] [Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding](https://arxiv.org/abs/2508.11657)
*Yuanhao Li,Badong Chen,Wenjun Bai,Yasuharu Koike,Okito Yamashita*

Main category: eess.SP

TL;DR: 基于最小错误盈的稀疏贝叶斯学习框架，通过MEE准则提高对脚术异常倾吐的鲁棒性，在高维脑电信号解码任务中获得更优的性能表现


<details>
  <summary>Details</summary>
Motivation: 传统的高斯和二项分布假设在对付脑电信号的噪声时可能不充分，需要一种更鲁棒的稀疏贝叶斯学习方法来处理高维度脑活动解码中的噪声问题

Method: 采用最小错误盈(MEE)准则构建鲁棒的可能性函数，并将其集成到稀疏贝叶斯学习框架中，以更好地处理复杂数据分布和噪声

Result: 在回归和分类两种高维度脑电解码任务中，新方法在解码指标和生物模式方面都超过了传统方法和最新的状态下的方法

Conclusion: 通过MEE基于可能性模型，稀疏贝叶斯学习能够同时解决脑电解码任务中的噪声和高维度挑战，为脑机接口等生物医学工程应用提供了强大工具

Abstract: Objective: Sparse Bayesian learning provides an effective scheme to solve the
high-dimensional problem in brain signal decoding. However, traditional
assumptions regarding data distributions such as Gaussian and binomial are
potentially inadequate to characterize the noisy signals of brain activity.
Hence, this study aims to propose a robust sparse Bayesian learning framework
to address noisy highdimensional brain activity decoding. Methods: Motivated by
the commendable robustness of the minimum error entropy (MEE) criterion for
handling complex data distributions, we proposed an MEE-based likelihood
function to facilitate the accurate inference of sparse Bayesian learning in
analyzing noisy brain datasets. Results: Our proposed approach was evaluated
using two high-dimensional brain decoding tasks in regression and
classification contexts, respectively. The experimental results showed that,
our approach can realize superior decoding metrics and physiological patterns
than the conventional and state-of-the-art methods. Conclusion: Utilizing the
proposed MEE-based likelihood model, sparse Bayesian learning is empowered to
simultaneously address the challenges of noise and high dimensionality in the
brain decoding task. Significance: This work provides a powerful tool to
realize robust brain decoding, advancing biomedical engineering applications
such as brain-computer interface.

</details>


### [5] [CECGSR: Circular ECG Super-Resolution](https://arxiv.org/abs/2508.11658)
*Honggui Li,Zhengyang Zhang,Dingtai Li,Sinan Chen,Nahid Md Lokman Hossain,Xinfeng Xu,Yuting Feng,Hantao Lu,Yinlu Qin,Ruobing Wang,Maria Trocan,Dimitri Galayko,Amara Amara,Mohamad Sawan*

Main category: eess.SP

TL;DR: 闭环循环心电图超分辨率方法CECGSR，通过建模降级过程和负反馈机制，在PTB-XL数据集上超越现有开环方法的重建性能


<details>
  <summary>Details</summary>
Motivation: 心电图信号存在分辨率低、噪声干扰问题，现有超分辨率方法主要使用开环架构，而闭环控制理论表明闭环系统具有更优的动态和静态性能

Method: 提出CECGSR闭环方法，建模从高分辨率到低分辨率的降级过程，通过低分辨率信号差异构建负反馈机制，使用数学循环方程和泰勒级数展开证明稳态误差近零，采用Plug-and-Play策略集成现有超分辨率方法

Result: 在PTB-XL数据集的无噪声和有噪声子集上进行模拟实验，结果显示CECGSR在心电图信号重建性能方面超过了最先进的开环超分辨率算法

Conclusion: 闭环循环超分辨率方法CECGSR通过理论基础和实验验证，证明了在心电图超分辨率任务中的优勒性，为心电图信号处理提供了更有效的解决方案

Abstract: The electrocardiogram (ECG) plays a crucial role in the diagnosis and
treatment of various cardiac diseases. ECG signals suffer from low-resolution
(LR) due to the use of convenient acquisition devices, as well as internal and
external noises and artifacts. Classical ECG super-resolution (ECGSR) methods
adopt an open-loop architecture that converts LR ECG signals to
super-resolution (SR) ones. According to the theory of automatic control, a
closed-loop framework exhibits superior dynamic and static performance compared
with its open-loop counterpart. This paper proposes a closed-loop approach,
termed circular ECGSR (CECGSR), which models the degradation process from SR
ECG signals to LR ones. The negative feedback mechanism of the closed-loop
system is based on the differences between the LR ECG signals. A mathematical
loop equation is constructed to characterize the closed-loop infrastructure.
The Taylor series expansion is employed to demonstrate the near-zero
steady-state error of the proposed method. A Plug-and-Play strategy is
considered to establish the SR unit of the proposed architecture, leveraging
any existing advanced open-loop ECGSR methods. Simulation experiments on both
noiseless and noisy subsets of the PTB-XL datasets demonstrate that the
proposed CECGSR outperforms state-of-the-art open-loop ECGSR algorithms in the
reconstruction performance of ECG signals.

</details>


### [6] [Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation](https://arxiv.org/abs/2508.11663)
*Guangli Li,Canbiao Wu,Zhen Liang*

Main category: eess.SP

TL;DR: 提出McdPL框架，通过双对抗分类器和三阶段对抗训练实现跨语料库情感识别，在SEED系列数据集上取得显著准确率提升


<details>
  <summary>Details</summary>
Motivation: 解决跨语料库情感识别中由于被试生理差异、实验环境和设备变化导致的决策边界附近样本分类困难的问题

Method: 基于域对抗迁移学习的优化方法，设计双对抗分类器（Ada和RMS分类器），采用三阶段对抗训练最大化分类差异并最小化特征分布差异，同时引入成对学习将分类问题转化为样本相似性问题

Result: 在SEED、SEED-IV和SEED-V公开数据集上，McdPL模型优于其他基线模型，平均准确率分别提升4.76%和3.97%

Conclusion: McdPL框架为跨语料库情感识别提供了有前景的解决方案，能够有效对齐决策边界附近的争议样本并减轻标签噪声影响

Abstract: Affective computing is a rapidly developing interdisciplinary research
direction in the field of brain-computer interface. In recent years, the
introduction of deep learning technology has greatly promoted the development
of the field of emotion recognition. However, due to physiological differences
between subjects, as well as the variations in experimental environments and
equipment, cross-corpus emotion recognition faces serious challenges,
especially for samples near the decision boundary. To solve the above problems,
we propose an optimization method based on domain adversarial transfer learning
to fine-grained alignment of affective features, named Maximum classifier
discrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a
dual adversarial classifier (Ada classifier and RMS classifier), and apply a
three-stage adversarial training to maximize classification discrepancy and
minimize feature distribution to align controversy samples near the decision
boundary. In the process of domain adversarial training, the two classifiers
also maintain an adversarial relationship, ultimately enabling precise
cross-corpus feature alignment. In addition, the introduction of pairwise
learning transforms the classification problem of samples into a similarity
problem between samples, alleviating the influence of label noise. We conducted
systematic experimental evaluation of the model using publicly available SEED,
SEED-IV and SEED-V databases. The results show that the McdPL model is superior
to other baseline models in the cross-corpus emotion recognition task, and the
average accuracy improvements of 4.76\% and 3.97\%, respectively. Our work
provides a promising solution for emotion recognition cross-corpus. The source
code is available at https://github.com/WuCB-BCI/Mcd_PL.

</details>


### [7] [Energy-Efficient Real-Time 4-Stage Sleep Classification at 10-Second Resolution: A Comprehensive Study](https://arxiv.org/abs/2508.11664)
*Zahra Mohammadi,Parnian Fazel,Siamak Mohammadi*

Main category: eess.SP

TL;DR: 基于单导联电心图的能效睡眠分期分类系统，通过轻量级深度学习模型实现高准确度和低能耗，适合可穿戴睡眠监测


<details>
  <summary>Details</summary>
Motivation: 传统多导联睡眠监测方法成本高且不适合长期家庭使用，需要一种能效、可穿戴的单导联电心图睡眠分期方案

Method: 提出两种窗口切分策略：5分钟窗口用于机器学习模型，30秒窗口用于深度学习模型。设计了轻量级自定义模型SleepLiteCNN，并应用8位量化技术

Result: MobileNet-v1达到92%准确度和91% F1分数，SleepLiteCNN达到89%准确度和89% F1分数，每次推理能耗仅为5.48微焦耳，FPGA部署证明低资源占用

Conclusion: 该系统提供了一种实用的可续续、可穿戴的电心图基睡眠监测解决方案，在保持高准确性的同时显著降低了能耗

Abstract: Sleep stage classification is crucial for diagnosing and managing disorders
such as sleep apnea and insomnia. Conventional clinical methods like
polysomnography are costly and impractical for long-term home use. We present
an energy-efficient pipeline that detects four sleep stages (wake, REM, light,
and deep) from a single-lead ECG. Two windowing strategies are introduced: (1)
a 5-minute window with 30-second steps for machine-learning models that use
handcrafted features, and (2) a 30-second window with 10-second steps for
deep-learning models, enabling near-real-time 10-second resolution. Lightweight
networks such as MobileNet-v1 reach 92 percent accuracy and 91 percent F1-score
but still draw significant energy. We therefore design SleepLiteCNN, a custom
model that achieves 89 percent accuracy and 89 percent F1-score while lowering
energy use to 5.48 microjoules per inference at 45 nm. Applying eight-bit
quantization preserves accuracy and further reduces power, and FPGA deployment
confirms low resource usage. The proposed system offers a practical solution
for continuous, wearable ECG-based sleep monitoring.

</details>


### [8] [Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate vs Late Fusion](https://arxiv.org/abs/2508.11666)
*Timothy Oladunni,Ehimen Aneni*

Main category: eess.SP

TL;DR: 这篇论文研究了多模态深度神经网络在ECG心血管疾病分类中的融合策略，发现中间融合策略比后期融合更有效，达到97%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 单模态深度学习模型容易过拟合且通用性有限，而多模态融合策略能够整合多种数据领域但最优融合方法在临床应用中研究不充分。

Method: 采用ECG信号的时域、频域和时频域三种域数据，对比研究中间融合（特征层次）和后期融合（决策层次）两种策略的效果。

Result: 中间融合策略在所有实验中都优于后期融合，达到97%的最高准确率，Cohen's d > 0.8（相比单模态）和d = 0.40（相比后期融合）。

Conclusion: 提出的基于ECG域的多模态模型具有更优的预测能力和更好的可解释性，超越了现有最先进模型，在医疗AI应用中具有重要价值。

Abstract: The limitations of unimodal deep learning models, particularly their tendency
to overfit and limited generalizability, have renewed interest in multimodal
fusion strategies. Multimodal deep neural networks (MDNN) have the capability
of integrating diverse data domains and offer a promising solution for robust
and accurate predictions. However, the optimal fusion strategy, intermediate
fusion (feature-level) versus late fusion (decision-level) remains
insufficiently examined, especially in high-stakes clinical contexts such as
ECG-based cardiovascular disease (CVD) classification. This study investigates
the comparative effectiveness of intermediate and late fusion strategies using
ECG signals across three domains: time, frequency, and time-frequency. A series
of experiments were conducted to identify the highest-performing fusion
architecture. Results demonstrate that intermediate fusion consistently
outperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's
d > 0.8 relative to standalone models and d = 0.40 compared to late fusion.
Interpretability analyses using saliency maps reveal that both models align
with the discretized ECG signals. Statistical dependency between the
discretized ECG signals and corresponding saliency maps for each class was
confirmed using Mutual Information (MI). The proposed ECG domain-based
multimodal model offers superior predictive capability and enhanced
explainability, crucial attributes in medical AI applications, surpassing
state-of-the-art models.

</details>


### [9] [Neural Gaussian Radio Fields for Channel Estimation](https://arxiv.org/abs/2508.11668)
*Muhammad Umer,Muhammad Ahmed Mohsin,Ahsan Bilal,John M. Cioffi*

Main category: eess.SP

TL;DR: nGRF是一种基于3D高斯原语的无线信道估计新框架，相比传统方法在精度、速度和数据效率方面有显著提升，特别适用于移动环境和大规模场景。


<details>
  <summary>Details</summary>
Motivation: 传统信道状态信息估计方法在移动环境下表现不佳，导频开销大、延迟高、频谱效率低，需要新的解决方案来应对现代无线网络的挑战。

Method: 使用显式3D高斯原语进行直接电磁场聚合，每个高斯作为局部无线电调制器，避免了基于NeRF的慢速隐式表示或非物理2D投影方法。

Result: 室内场景预测SNR提高10.9倍，推理延迟从242ms降至1.1ms；室外场景达到26.2dB SNR；数据收集负担减少18倍，训练时间从小时级降至分钟级。

Conclusion: nGRF框架在信道估计方面实现了突破性进展，为动态无线环境提供了高效、准确的解决方案，具有重要的实际应用价值。

Abstract: Accurate channel state information (CSI) remains the most critical bottleneck
in modern wireless networks, with pilot overhead consuming up to 11-21% of
transmission bandwidth, increasing latency by 20-40% in massive MIMO systems,
and reducing potential spectral efficiency by over 53%. Traditional estimation
techniques fundamentally fail under mobility, with feedback delays as small as
4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We
present neural Gaussian radio fields (nGRF), a novel framework that leverages
explicit 3D Gaussian primitives to synthesize complex channel matrices
accurately and efficiently. Unlike NeRF-based approaches that rely on slow
implicit representations or existing Gaussian splatting methods that use
non-physical 2D projections, nGRF performs direct 3D electromagnetic field
aggregation, with each Gaussian acting as a localized radio modulator. nGRF
demonstrates superior performance across diverse environments: in indoor
scenarios, it achieves a 10.9$\times$ higher prediction SNR than state of the
art methods while reducing inference latency from 242 ms to just 1.1 ms (a
220$\times$ speedup). For large-scale outdoor environments, where existing
approaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF
requires only 0.011 measurements per cubic foot compared to 0.2-178.1 for
existing methods, thereby reducing data collection burden by 18$\times$.
Training time is similarly reduced from hours to minutes (a 180$\times$
reduction), enabling rapid adaptation to dynamic environments. The code and
datasets are available at: https://github.com/anonym-auth/n-grf

</details>


### [10] [Direction of Arrival Estimation: A Tutorial Survey of Classical and Modern Methods](https://arxiv.org/abs/2508.11675)
*Amgad A. Salama*

Main category: eess.SP

TL;DR: 这是一份关于到达角度估计的综述性教程，涵盖了从经典到现代的各种方法，包括数学推导、Python实现和实践指南。


<details>
  <summary>Details</summary>
Motivation: 到达角度估计是数组信号处理领域的基础问题，应用于雷达、声纳、无线通信等多个领域。本文旨在为入门者提供全面的引导，缩小理论与实践之间的差距。

Method: 采用统一线性数组进行窄带信号处理，涵盖了经典波束形成方法、子空间技术（MUSIC、ESPRIT）、最大似然方法和稀疏信号处理方法。每种方法都有详细的数学推导和Python实现。

Result: 提供了开源的Python实现库，支持可复现研究和实践学习。通过系统性的性能比较，为方法选择和参数调整提供了实践指南。

Conclusion: 该教程成功地为入门者提供了全面的到达角度估计引导，同时也是领域专家的完整参考资料，有力地推动了理论与实践的结合。

Abstract: Direction of arrival (DOA) estimation is a fundamental problem in array
signal processing with applications spanning radar, sonar, wireless
communications, and acoustic signal processing. This tutorial survey provides a
comprehensive introduction to classical and modern DOA estimation methods,
specifically designed for students and researchers new to the field. We focus
on narrowband signal processing using uniform linear arrays, presenting
step-by-step mathematical derivations with geometric intuition. The survey
covers classical beamforming methods, subspace-based techniques (MUSIC,
ESPRIT), maximum likelihood approaches, and sparse signal processing methods.
Each method is accompanied by Python implementations available in an
open-source repository, enabling reproducible research and hands-on learning.
Through systematic performance comparisons across various scenarios, we provide
practical guidelines for method selection and parameter tuning. This work aims
to bridge the gap between theoretical foundations and practical implementation,
making DOA estimation accessible to beginners while serving as a comprehensive
reference for the field. See https://github.com/AmgadSalama/DOA for detail
implementation of the methods.

</details>


### [11] [Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study](https://arxiv.org/abs/2508.11682)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: eess.SP

TL;DR: 通过年龄正则化HRV特征提高睡眠期间血糖预测精度，在43名受试者中实现R2提升25.6%


<details>
  <summary>Details</summary>
Motivation: 非侵入性血糖监测是糖尿病管理的关键挑战，但年龄相关的自主神经变化影响传统HRV分析的准确性

Method: 收集43名受试者的多模态数据，采用新的年龄正则化技术处理HRV特征，使用贝叶斯岭返归和5折交叉验证进血糖预测

Result: 年龄正则化HRV特征实现R2=0.161(MAE=0.182)，比非正则化特征提高25.6%，最佳预测特征包括hr rem和ds阶段的年龄正则化HRV特征以及舔张压

Conclusion: 年龄正则化HRV特征显著提高了血糖预测准确性，该方法解决了自主神经功能评估的基本限制，为非侵入性血糖监测提供了初步可行性，但需要在更大群体中验证

Abstract: Non-invasive glucose monitoring remains a critical challenge in the
management of diabetes. HRV during sleep shows promise for glucose prediction
however, age-related autonomic changes significantly confound traditional HRV
analyses. We analyzed 43 subjects with multi-modal data including sleep-stage
specific ECG, HRV features, and clinical measurements. A novel
age-normalization technique was applied to the HRV features by, dividing the
raw values by age-scaled factors. BayesianRidge regression with 5-fold
cross-validation was employed for log-glucose prediction. Age-normalized HRV
features achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction,
representing a 25.6% improvement over non-normalized features (R2 = 0.132). The
top predictive features were hrv rem mean rr age normalized (r = 0.443, p =
0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic
blood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed
age-normalization as the critical component, with sleep-stage specific features
providing additional predictive value. Age-normalized HRV features
significantly enhance glucose prediction accuracy compared with traditional
approaches. This sleep-aware methodology addresses fundamental limitations in
autonomic function assessment and suggests a preliminary feasibility for
non-invasive glucose monitoring applications. However, these results require
validation in larger cohorts before clinical consideration.

</details>


### [12] [A Graph Neural Network based on a Functional Topology Model: Unveiling the Dynamic Mechanisms of Non-Suicidal Self-Injury in Single-Channel EEG](https://arxiv.org/abs/2508.11684)
*BG Tong*

Main category: eess.SP

TL;DR: 通过图神经网络分析单导联电脑电流数据，提出功能-能量拓扑模型，发现自伤行为状态下身体感知调控循环出现功能障碍和方向逆转


<details>
  <summary>Details</summary>
Motivation: 研究非自杀性自伤行为(NSSI)的神经动力学机制，通过理论驱动的图神经网络处理实际环境中收集的稀疏单导联EEG数据

Method: 使用手机应用和便携式Fp1 EEG头带收集三名青少年的EEG数据，构建七个功能节点的GNN模型，采用内部验证和交叉验证方法，使用GNNExplainer进行解释性分析

Result: 模型在内部验证中达到>85%的准确率，交叉验证中达到73.7%。发现NSSI状态下身体感知调控循环出现功能障碍和方向逆转，脑部失去自我纠正能力

Conclusion: 证明了理论驱动GNN在稀疏单导联EEG中解码复杂精神状态的可行性，提供了新题的动态NSSI机制模型，为客观生物标记物和数字疗法开拓了道路

Abstract: Objective: This study proposes and preliminarily validates a novel
"Functional-Energetic Topology Model" to uncover neurodynamic mechanisms of
Non-Suicidal Self-Injury (NSSI), using Graph Neural Networks (GNNs) to decode
brain network patterns from single-channel EEG in real-world settings.Methods:
EEG data were collected over ~1 month from three adolescents with NSSI using a
smartphone app and a portable Fp1 EEG headband during impulsive and
non-impulsive states. A theory-driven GNN with seven functional nodes was
built. Performance was evaluated via intra-subject (80/20 split) and
leave-one-subject-out cross-validation (LOSOCV). GNNExplainer was used for
interpretability.Results: The model achieved high intra-subject accuracy (>85%)
and significantly above-chance cross-subject performance (approximately73.7%).
Explainability analysis revealed a key finding: during NSSI states, a critical
feedback loop regulating somatic sensation exhibits dysfunction and directional
reversal. Specifically, the brain loses its ability to self-correct via
negative bodily feedback, and the regulatory mechanism enters an "ineffective
idling" state.Conclusion: This work demonstrates the feasibility of applying
theory-guided GNNs to sparse, single-channel EEG for decoding complex mental
states. The identified "feedback loop reversal" offers a novel, dynamic, and
computable model of NSSI mechanisms, paving the way for objective biomarkers
and next-generation Digital Therapeutics (DTx).

</details>


### [13] [Enhancing Corrosion Resistance of Aluminum Alloys Through AI and ML Modeling](https://arxiv.org/abs/2508.11685)
*Farnaz Kaboudvand,Maham Khalid,Nydia Assaf,Vardaan Sahgal,Jon P. Ruffley,Brian J. McDermott*

Main category: eess.SP

TL;DR: 机器学习模型预测铝合金海水腐蚀速率，高斯过程回归表现最优


<details>
  <summary>Details</summary>
Motivation: 铝合金在海洋环境中面临严重腐蚀问题，需要有效预测和优化腐蚀抵役能力

Method: 使用综合开源数据集，采用直接法（材料成分+环境条件预测腐蚀率）和逆向法（根据腐蚀率选择材料）。比较了随机森林、神经网络和高斯过程回归三种机器学习方法

Result: 高斯过程回归表现最优，混合内核函数和对数变换后预测精度更高

Conclusion: 机器学习特别是GPR在腐蚀预测和材料选择方面具有高效性，为铝合金腐蚀防护提供了新方法

Abstract: Corrosion poses a significant challenge to the performance of aluminum
alloys, particularly in marine environments. This study investigates the
application of machine learning (ML) algorithms to predict and optimize
corrosion resistance, utilizing a comprehensive open-source dataset compiled
from various sources. The dataset encompasses corrosion rate data and
environmental conditions, preprocessed to standardize units and formats. We
explored two different approaches, a direct approach, where the material's
composition and environmental conditions were used as inputs to predict
corrosion rates; and an inverse approach, where corrosion rate served as the
input to identify suitable material compositions as output. We employed and
compared three distinct ML methodologies for forward predictions: Random Forest
regression, optimized via grid search; a feed-forward neural network, utilizing
ReLU activation and Adam optimization; and Gaussian Process Regression (GPR),
implemented with GPyTorch and employing various kernel functions. The Random
Forest and neural network models provided predictive capabilities based on
elemental compositions and environmental conditions. Notably, Gaussian Process
Regression demonstrated superior performance, particularly with hybrid kernel
functions. Log-transformed GPR further refined predictions. This study
highlights the efficacy of ML, particularly GPR, in predicting corrosion rates
and material properties.

</details>


### [14] [The Lost-K and Shorter-J Phenomenon in Non-Standard Ballistocardiography Data](https://arxiv.org/abs/2508.11686)
*Shuai Jiao,Jian Fang,Tianshu Zhou,Jinsong Li,Yanhong Liu,Ye Liu,Ming Ju*

Main category: eess.SP

TL;DR: 本文提出两种导致BCG信号J峰不显著的现象（短J现象和失K现象），并提出三种信号变换方法来改善这些问题，在40名参与者的数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 非标准BCG数据通常缺乏显著的J峰，这会影响BCG信号的分析和应用。研究者发现了两种导致J峰不显著的现象，需要找到有效的方法来改善这些问题。

Method: 提出了三种信号变换方法，专门用于改善短J现象和失K现象。这些方法通过信号处理技术来增强J峰的显著性。

Result: 在40名参与者的时间对齐ECG-BCG数据集上评估，结果显示经过变换的信号能够让简单的J峰基础方法（如检测局部极大值或极小值）在定位J峰和提取BCG周期方面取得更好的性能，尤其是对非标准BCG数据。

Conclusion: 所提出的信号变换方法能够有效改善BCG信号中J峰不显著的问题，为非标准BCG数据的处理提供了有效的解决方案，提高了简单算法在BCG分析中的性能。

Abstract: Non-standard ballistocardiogram(BCG) data generally do not have prominent J
peaks. This paper introduces two phenomena that reduce the prominence of
Jpeaks: the shorter-J phenomenon and the lost-K phenomenon, both of which are
commonly observed in non-standard BCG signals . This paper also proposes three
signal transformation methods that effectively improve the lost-K and shorter-J
phenomena. The methods were evaluated on a time-aligned ECG-BCG dataset with 40
subjects. The results show that based on the transformed signal, simple
J-peak-based methods using only the detection of local maxima or minima show
better performance in locating J-peaks and extracting BCG cycles, especially
for non-standard BCG data.

</details>


### [15] [Agent-Based Anti-Jamming Techniques for UAV Communications in Adversarial Environments: A Comprehensive Survey](https://arxiv.org/abs/2508.11687)
*Jingpu Yang,Mingxuan Cui,Hang Zhang,Fengxian Ji,Zhengzhao Lai,Yufeng Wang*

Main category: eess.SP

TL;DR: 本文是一个关于无人机通信智能抗干扰技术的综述性论文，通过"感知-决策-行动"闭环框架系统分析了游戏理论和强化学习在适应性抗干扰策略中的应用，并提出了现有技术的局限性和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 无人机通信在动态对抗环境中遇到越来越严重的多源干扰挑战，对可靠性和弹性提出了更高要求，需要研究智能化的自主抗干扰技术来应对这些挑战。

Method: 构建了以"感知-决策-行动"为核心的闭环决策框架，系统分析了各阶段的关键技术，重点采用游戏理论建模无人机与干扰器的交互作用，并集成基于强化学习的智能算法来推导适应性抗干扰策略。

Result: 形成了一个系统化的智能抗干扰技术分析框架，为无人机通信抗干扰领域提供了理论支撑和技术指南，明确了游戏理论与强化学习相结合的研究路径。

Conclusion: 本文为发展更智能、更稳健的无人机抗干扰通信系统提供了有价值的参考，持续推进这一领域的研究将会推动无人机通信在复杂对抗环境中的可靠性和弹性。

Abstract: Unmanned Aerial Vehicle communications are encountering increasingly severe
multi-source interference challenges in dynamic adversarial environments, which
impose higher demands on their reliability and resilience. To address these
challenges, agent-based autonomous anti-jamming techniques have emerged as a
crucial research direction. This paper presents a comprehensive survey that
first formalizes the concept of intelligent anti-jamming agents for UAV
communications and establishes a closed-loop decision-making framework centered
on the "Perception-Decision-Action" (P-D-A) paradigm. Within this framework, we
systematically review key technologies at each stage, with particular emphasis
on employing game theory to model UAV-jammer interactions and integrating
reinforcement learning-based intelligent algorithms to derive adaptive
anti-jamming strategies. Furthermore, we discuss potential limitations of
current approaches, identify critical engineering challenges, and outline
promising future research directions, aiming to provide valuable references for
developing more intelligent and robust anti-jamming communication systems for
UAVs.

</details>


### [16] [Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception](https://arxiv.org/abs/2508.11691)
*Mathis Rezzouk,Fabrice Gagnon,Alyson Champagne,Mathieu Roy,Philippe Albouy,Michel-Pierre Coll,Cem Subakan*

Main category: eess.SP

TL;DR: 这篇论文系统性评估了多种机器学习模型在EEG信号跨参与者模式识别中的性能，发现深度学习模型在跨个体泛化方面表现更优，并分享了标准化的数据集作为未来研究的基准。


<details>
  <summary>Details</summary>
Motivation: 解决EEG信号在疼痛知觉识别中存在的高跨个体变异性问题，提高机器学习模型在不同参与者之间的泛化能力。

Method: 使用108名参与者的EEG记录数据集，对比评估传统分类器和深度神经网络在内部和跨参与者评估设置下的性能。

Result: 深度学习模型在跨参与者泛化方面表现更优异，尤其是基于图论的模型能够抓取主体不变的EEG信号结构，但性能变异性仍然较高。

Conclusion: 深度学习模型在跨个体EEG解码方面具有潜力，研究分享的标准化数据集将为未来算法评估提供基准。

Abstract: EEG-based analysis of pain perception, enhanced by machine learning, reveals
how the brain encodes pain by identifying neural patterns evoked by noxious
stimulation. However, a major challenge that remains is the generalization of
machine learning models across individuals, given the high cross-participant
variability inherent to EEG signals and the limited focus on direct pain
perception identification in current research. In this study, we systematically
evaluate the performance of cross-participant generalization of a wide range of
models, including traditional classifiers and deep neural classifiers for
identifying the sensory modality of thermal pain and aversive auditory
stimulation from EEG recordings. Using a novel dataset of EEG recordings from
108 participants, we benchmark model performance under both within- and
cross-participant evaluation settings. Our findings show that traditional
models suffered the largest drop from within- to cross-participant performance,
while deep learning models proved more resilient, underscoring their potential
for subject-invariant EEG decoding. Even though performance variability
remained high, the strong results of the graph-based model highlight its
potential to capture subject-invariant structure in EEG signals. On the other
hand, we also share the preprocessed dataset used in this study, providing a
standardized benchmark for evaluating future algorithms under the same
generalization constraints.

</details>


### [17] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 基于深度学习的轻量化方法，仅需电力信号进行软件切换机故障检测，达到极高准确率


<details>
  <summary>Details</summary>
Motivation: 传统方法需多种输入和特征工程，且专用于特定技术，缺乏扩展性。软件切换机故障会导致服务中断，需要预防性维护

Method: 仅使用单一电力信号输入，应用深度学习模型分析电力消耗模式，进行故障分类，使用遵循预测提供信心度指标

Result: 达到>99.99%的精确度，<0.01%的假正率，可忽略的假阿率，在真实环境和测试台上验证了扩展性

Conclusion: 该方法通用性强、技术无关，仅需单一输入即可实现高精度故障检测，符合ISO-17359标准要求，为预防性维护提供可靠解决方案

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [18] [Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data](https://arxiv.org/abs/2508.11693)
*Francisco López,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 基于SVM分类器的智能轨道电路故障自动识别方法，能够准确分类15种故障类型，提升维护效率。


<details>
  <summary>Details</summary>
Motivation: 轨道电路是轨道信号设备的核心，传统的故障检测方法效率低下，需要一种能够自动识别具体故障组件的智能化方案来改善维护工作。

Method: 采用支持向量机(SVM)分类器，利用STDS智能轨道检测系统的高低频流数据进行故障分类识别。

Result: 在10个不同轨道电路的实际地数据测试中，所有用例都被正确分类，经STDS专家和维护人员验证有效。

Conclusion: 该方法能够有效地自动识别轨道电路故障，对应15种不同故障类型，为轨道运营维护提供了可靠的智能化解决方案。

Abstract: Track Circuits (TC) are the main signalling devices used to detect the
presence of a train on a rail track. It has been used since the 19th century
and nowadays there are many types depending on the technology. As a general
classification, Track Circuits can be divided into 2 main groups, DC (Direct
Current) and AC (Alternating Current) circuits. This work is focused on a
particular AC track circuit, called "Smart Train Detection System" (STDS),
designed with both high and low-frequency bands. This approach uses STDS
current data applied to an SVM (support vector machine) classifier as a type of
failure identifier. The main purpose of this work consists on determine
automatically which is the component of the track that is failing to improve
the maintenance action. Model was trained to classify 15 different failures
that belong to 3 more general categories. The method was tested with field data
from 10 different track circuits and validated by the STDS track circuit expert
and maintainers. All use cases were correctly classified by the method.

</details>


### [19] [Operational machine learning for park-scale irrigation to support urban cooling](https://arxiv.org/abs/2508.11700)
*Mesut Koçyiğit,Bahman Javadi,Russell Thomson,Sebastian Pfautsch,Oliver Obst*

Main category: eess.SP

TL;DR: SIMPaCT是一个智能灌溉系统，通过土壤湿度预测优化公园灌溉来降低城市热岛效应，使用kNN算法和异常检测实现稳健的预测和控制。


<details>
  <summary>Details</summary>
Motivation: 传统公园灌溉系统主要关注节水，但忽视了灌溉对城市降温的潜在效益。需要开发一种能够同时优化节水和降温效果的智能灌溉方案。

Method: 使用202个土壤湿度传感器、50个温湿度节点和13个气象站数据，采用kNN算法进行短期滚动预测，配合异常检测和虚拟传感器技术处理设备故障。

Result: 平均绝对误差0.78%，优于SARIMA等复杂基线方法（P75误差0.71% vs 0.93%），系统已实现日常运行并生成灌溉设定值建议。

Conclusion: SIMPaCT提供了一个在城市公园尺度上实现稳健、以降温为导向的智能灌溉操作方案，成功平衡了节水和降温双重目标。

Abstract: Urban parks can mitigate local heat, yet irrigation control is usually tuned
for water savings rather than cooling. We report on SIMPaCT (Smart Irrigation
Management for Parks and Cool Towns), a park-scale deployment that links
per-zone soil-moisture forecasts to overnight irrigation set-points in support
of urban cooling. SIMPaCT ingests data from 202 soil-moisture sensors, 50
temperature-relative humidity (TRH) nodes, and 13 weather stations, and trains
a per-sensor k-nearest neighbours (kNN) predictor on short rolling windows
(200-900h). A rule-first anomaly pipeline screens missing and stuck-at signals,
with model-based checks (Isolation Forest and ARIMA). When a device fails, a
mutual-information neighbourhood selects the most informative neighbour and a
small multilayer perceptron supplies a "virtual sensor" until restoration.
Across sensors the mean absolute error was 0.78%, comparable to more complex
baselines; the upper-quartile error (P75) was lower for kNN than SARIMA (0.71%
vs 0.93%). SIMPaCT runs daily and writes proposed set-points to the existing
controller for operator review. This short communication reports an operational
recipe for robust, cooling-oriented irrigation at city-park scale.

</details>


### [20] [Scaling Wideband Massive MIMO Radar via Beamspace Dimension Reduction](https://arxiv.org/abs/2508.11790)
*Oveys Delafrooz Noroozi,Jiyoon Han,Wei Tang,Zhengya Zhang,Upamanyu Madhow*

Main category: eess.SP

TL;DR: 基于空间FFT的窗化束空间MVDR材形处理架构，通过权衡窗口大小和FFT分辨率来降低计算复杂度同时保持检测性能


<details>
  <summary>Details</summary>
Motivation: 传统空间处理在大规模MIMO雷达数组中计算复杂度过高（MVDR材形处理复杂度为O(N^3)），需要找到高效的复杂度降低方案

Method: 利用材形空间能量聚集特性，通过空间FFT转换到材形空间（复杂度O(NlogN)），设计窗化材形空间MVDR材形处理架构，并在目标咄子带上并行化处理

Result: 在DARPA SOAP程序的广带雷达数据上评估，该方法在显著降低计算和训练开销的同时，实现了与全维度基准相可比的检测性能，有效抑制干扰

Conclusion: 材形空间处理为大规模MIMO雷达提供了高效的复杂度降低解决方案，通过权衡窗口大小咄FFT分辨率可以在复杂度、检测准确性咄干扰抑制之间达到最优平衡

Abstract: We present an architecture for scaling digital beamforming for wideband
massive MIMO radar. Conventional spatial processing becomes computationally
prohibitive as array size grows; for example, the computational complexity of
MVDR beamforming scales as O(N^3) for an N-element array. In this paper, we
show that energy concentration in beamspace provides the basis for drastic
complexity reduction, with array scaling governed by the O(NlogN) complexity of
the spatial FFT used for beamspace transformation. Specifically, we propose an
architecture for windowed beamspace MVDR beamforming, parallelized across
targets and subbands, and evaluate its efficacy for beamforming and
interference suppression for government-supplied wideband radar data from the
DARPA SOAP (Scalable On-Array Processing) program. We demonstrate that our
approach achieves detection performance comparable to full-dimensional
benchmarks while significantly reducing computational and training overhead,
and provide insight into tradeoffs between beamspace window size and FFT
resolution in balancing complexity, detection accuracy, and interference
suppression.

</details>


### [21] [Digital Post-Distortion Architectures for Nonlinear Power Amplifiers: Volterra and Kernel Methods](https://arxiv.org/abs/2508.11792)
*Daniel Schäufele,Jochen Fink,Renato L. G. Cavalcante,Sławomir Stańczak*

Main category: eess.SP

TL;DR: 本文探讨在基站端实施数字后扭曲(DPoD)技术来补偿5G用户设备功放的非线性扭曲，在低计算复杂度下实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决5G用户设备功放在减少功耗时导致的非线性扭曲问题，避免复杂的反馈机制和增加的功耗。

Method: 在时域、频域和DFT-s域对比研究DPoD技术，提出在时域实施DPoD并配合频域通道均衡的方案，使用实希尔伯空间重构问题，以及基于Volterra级数的核方法降低复杂度。

Result: 模拟结果显示，提出的算法在低计算复杂度下能显著提升性能，超越现有最优算法。

Conclusion: 在基站端实施DPoD是一种有效的解决方案，时域DPoD配合频域均衡能在复杂度和性能之间取得良好平衡。

Abstract: In modern 5G user equipments (UEs), the power amplifier (PA) contributes
significantly to power consumption during uplink transmissions, especially in
cell-edge scenarios. While reducing power backoff can enhance PA efficiency, it
introduces nonlinear distortions that degrade signal quality. Existing
solutions, such as digital pre-distortion, require complex feedback mechanisms
for optimal performance, leading to increased UE complexity and power
consumption. Instead, in this study we explore digital post-distortion (DPoD)
techniques, which compensate for these distortions at the base station,
leveraging its superior computational resources. In this study, we conduct an
comprehensive study concerning the challenges and advantages associated with
applying DPoD in time-domain, frequency-domain, and DFT-s-domain. Our findings
suggest that implementing DPoD in the time-domain, complemented by
frequency-domain channel equalization, strikes a good balance between low
computational complexity and efficient nonlinearity compensation. In addition,
we demonstrate that memory has to be taken into account regardless of the
memory of the PA. Subsequently, we show how to pose the complex-valued problem
of nonlinearity compensation in a real Hilbert space, emphasizing the potential
performance enhancements as a result. We then discuss the traditional Volterra
series and show an equivalent kernel method that can reduce algorithmic
complexity. Simulations validate the results of our analysis and show that our
proposed algorithm can significantly improve performance compared to
state-of-the-art algorithms.

</details>


### [22] [Autonomous Driving with RSMA-Enabled Finite Blocklength Transmissions: Ergodic Performance Analysis and Optimization](https://arxiv.org/abs/2508.12012)
*Yi Wang,Yingyang Chen,Li Wang,Donghong Cai,Xiaofan Li,Pingzhi Fan*

Main category: eess.SP

TL;DR: RSMA在有限块长度传输下的性能分析与优化，通过联合优化功率分配和速率分割，显著提升和速率性能并保证用户公平性


<details>
  <summary>Details</summary>
Motivation: 针对高移动性自动驾驶场景中URLLC的严格需求，需要解决RSMA在有限块长度传输下的性能评估和优化问题

Method: 推导RSMA遍历和速率的闭式下界，基于梯度下降联合优化全局功率系数、私有功率分配和公共速率分割，采用序列二次规划进行优化

Result: 仿真结果表明该RSMA方案显著提升遍历性能，降低块长度和误块率，优于平均私有功率的RSMA和SDMA，保证最差信道条件用户的速率

Conclusion: 所提出的RSMA FBL传输方案能够有效满足高移动性场景下的URLLC需求，在性能和公平性方面均表现出色

Abstract: Rate-splitting multiple access (RSMA) is a key technology for next-generation
multiple access systems due to its robustness against imperfect channel state
information (CSI). This makes RSMA particularly suitable for high-mobility
autonomous driving, where ultra-reliable and low-latency communication (URLLC)
is essential. To address the stringent requirements, this study enables RSMA
finite blocklength (FBL) transmissions and explicitly evaluates the ergodic
performance. We derive the closed-form lower bound for the ergodic sum-rate of
RSMA, considering vital factors such as the vehicle velocities, vehicle
positions, power allocation of each stream, blocklengths, and block error rates
(BLERs). To further enhance the ergodic sum-rate while complying with quality
of service (QoS) rate constraints, we jointly optimize the global power
coefficient, private power distribution, and common rate splitting. Guided by
gradient descent, we first adjust the global power coefficient based on its
sum-rate solution. This parameter regulates the power state of the common
stream, allowing for dynamic activation or deactivation: if active, we optimize
the private power distribution and adjust the common rate splitting to meet
minimum transmission constraints; if inactive, we use the sequential quadratic
programming for private power distribution optimization. Simulation results
confirm that our RSMA scheme significantly improves the ergodic performance,
reduces blocklength and BLER, surpassing the RSMA counterpart with average
private power and space division multiple access (SDMA). Furthermore, our
approach is validated to guarantee the rates for users with the poorest channel
conditions, thereby enhancing fairness across the network.

</details>


### [23] [A Generalized Multidimensional Chinese Remainder Theorem (MD-CRT) for Multiple Integer Vectors](https://arxiv.org/abs/2508.12099)
*Guangpu Guo,Xiang-Gen Xia*

Main category: eess.SP

TL;DR: 本文研究多维中国剩余定理的广义形式，重点解决多个整数向量从矩阵模的向量余数恢复问题，推导了唯一可定范围和实现最大动态范围的条件。


<details>
  <summary>Details</summary>
Motivation: 多维CRT在加密、编码和信号处理中有广泛应用，但现有研究主要集中在单向量恢复。需要扩展到多个整数向量的情况，而矩阵模的非交换性和多维范围的复杂性使得这个问题更具挑战性。

Method: 首先推导了在没有先验信息情况下的唯一可定范围，并提出了实现该范围的算法。然后重点研究了仅包含两个整数向量的特殊情况，探讨实现最大动态范围的条件。

Result: 得到了新的条件来实现最大可能动态范围，当维数降低到1时，这个新条件甚至比现有的标量整数广义CRT条件更好。

Conclusion: 本文结果为多维CRT的广义形式提供了理论基础，对于多维信号处理中的频率检测等应用具有潜在价值。

Abstract: Chinese remainder theorem (CRT) is widely applied in cryptography, coding
theory, and signal processing. It has been extended to the multidimensional CRT
(MD-CRT), which reconstructs an integer vector from its vector remainders
modulo multiple integer matrices. This paper investigates a generalized MD-CRT
for multiple integer vectors, where the goal is to determine multiple integer
vectors from multiple vector residue sets modulo multiple integer
matrices.Comparing to the existing generalized CRT for multiple scalar
integers, the challenge is that the moduli in MD-CRT are matrices that do not
commute and the corresponding uniquely determinable range is multidimensional
and the inclusion relationship is much more complicated. In this paper,we
address two fundamental questions regarding the generalized MD-CRT. The first
question concerns the uniquely determinable range of multiple integer vectors
when no prior information about them is available. The second question is about
the conditions under which the maximal possible dynamic range can be
achieved.To answer these two questions, we first derive a uniquely determinable
range without prior information and accordingly propose an algorithm to achieve
it. A special case involving only two integer vectors is investigated for the
second question, leading to a new condition for achieving the maximal possible
dynamic range. Interestingly, this newly obtained condition, when the dimension
is reduced to $1$, is even better than the existing ones for the conventional
generalized CRT for scalar integers.These results may have applications for
frequency detection in multidimensional signal processing.

</details>


### [24] [RFSS: A Comprehensive Multi-Standard RF Signal Source Separation Dataset with Advanced Channel Modeling](https://arxiv.org/abs/2508.12106)
*Hao Chen,Rui Jin,Dayuan Tan*

Main category: eess.SP

TL;DR: RFSS是一个包含52,847个多标准RF信号样本的开源数据集，支持2G/3G/4G/5G信号分离研究，CNN-LSTM架构在信号分离任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统的快速发展导致复杂电磁环境中多种蜂窝标准共存，需要先进的信号源分离技术来处理多标准RF信号。

Method: 开发了RFSS开源数据集，生成符合3GPP标准的GSM、UMTS、LTE和5G NR基带信号，包含多径衰落、8×8 MIMO处理和真实干扰场景的先进信道建模。

Result: CNN-LSTM架构在信号源分离任务中实现了26.7 dB的SINR改善，显著优于传统ICA（15.2 dB）和NMF（18.3 dB）方法。

Conclusion: RFSS数据集为RF信号源分离、认知无线电和机器学习应用提供了可重复研究的基础，同时保持完全开源可访问性。

Abstract: The rapid evolution of wireless communication systems has created complex
electromagnetic environments where multiple cellular standards (2G/3G/4G/5G)
coexist, necessitating advanced signal source separation techniques. We present
RFSS (RF Signal Source Separation), a comprehensive open-source dataset
containing 52,847 realistic multi-standard RF signal samples with complete 3GPP
standards compliance. Our framework generates authentic baseband signals for
GSM, UMTS, LTE, and 5G NR with advanced channel modeling including multipath
fading, MIMO processing up to 8 by 8 antennas, and realistic interference
scenarios. Experimental validation demonstrates superior performance of
CNN-LSTM architectures achieving 26.7 dB SINR improvement in source separation
tasks, significantly outperforming traditional ICA (15.2 dB) and NMF (18.3 dB)
approaches. The RFSS dataset enables reproducible research in RF source
separation, cognitive radio, and machine learning applications while
maintaining complete open-source accessibility

</details>


### [25] [Effect of Phase Shift Errors on the Security of UAV-assisted STAR-RIS IoT Networks](https://arxiv.org/abs/2508.12114)
*Mustafa Gusaibat,Mohammed Hnaish,Abdelhamid Salem,Khaled Rabie,Zubair Md Fadlullah,Wali Ullah Khan,Mohamad A. Alawad,Yazeed Alkhrijah*

Main category: eess.SP

TL;DR: 研究无人机搭载STAR-RIS系统中相位偏差对网络安全性能的影响，通过异正分布建模并推导秘密速率的分析表达式，使用线性网格算法优化无人机位置以最大化加权秘密速率。


<details>
  <summary>Details</summary>
Motivation: 无人机搭载STAR-RIS系统在6G物联网中提供全方位覆盖和灵活部署，但实际中无人机的振动和气流等实际缺陷可能影响STAR-RIS的相位调节，从而影响网络安全性能。

Method: 采用von Mises分布建模相位估计误差，推导不完整相位调整下的道秘密速率分析表达式，并通过线性网格算法优化无人机位置以最大化加权秘密速率。

Result: 得到了在存在相位估计误差情况下的道秘密速率分析表达式，并通过Monte Carlo模拟验证了分析结果的正确性。

Conclusion: 分析了相位估计误差对系统安全性能的影响，为STAR-RIS在安全无人机物联网网络中的实际部署提供了重要见解。

Abstract: Unmanned aerial vehicles (UAV)-mounted simultaneous transmitting and
reflecting reconfigurable intelligent surface (STAR-RIS) systems can provide
full-dimensional coverage and flexible deployment opportunities in future
6G-enabled IoT networks. However, practical imperfections such as jittering and
airflow of UAV could affect the phase shift of STAR-RIS, and consequently
degrade network security. In this respect, this paper investigates the impact
of phase shift errors on the secrecy performance of UAV-mounted
STAR-RIS-assisted IoT systems. More specifically, we consider a UAV-mounted
STAR-RIS-assisted non-orthogonal multiple access (NOMA) system where IoT
devices are grouped into two groups: one group on each side of the STAR-RIS.
The nodes in each group are considered as potential Malicious nodes for the
ones on the other side. By modeling phase estimation errors using a von Mises
distribution, analytical closed-form expressions for the ergodic secrecy rates
under imperfect phase adjustment are derived. An optimization problem to
maximize the weighted sum secrecy rate (WSSR) by optimizing the UAV placement
is formulated and is then solved using a linear grid-based algorithm. Monte
Carlo simulations are provided to validate the analytical derivations. The
impact of phase estimation errors on system's secrecy performance is analyzed,
providing critical insights for the practical realisation of STAR-RIS
deployments for secure UAV-enabled IoT networks.

</details>


### [26] [ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search](https://arxiv.org/abs/2508.12204)
*Mauro Belgiovine,Suyash Pradhan,Johannes Lange,Michael Löhning,Kaushik Chowdhury*

Main category: eess.SP

TL;DR: ATLAS是一个AI引导的测试生成方法，用于测试预训练的AI原生无线接收器模型，通过梯度优化高效发现故障配置，相比网格搜索减少19%的测试量。


<details>
  <summary>Details</summary>
Motivation: AI原生无线接收器缺乏可解释性，且无法穷尽测试所有环境条件，存在网络功能故障风险，需要高效的测试方法来验证模型可靠性。

Method: 使用基于梯度的优化方法，在线生成高风险故障配置的测试用例，避免穷举所有环境条件，在NVIDIA Sionna环境中实现和验证。

Result: ATLAS发现了AI原生DeepRx接收器在特定移动性、信道延迟扩展和噪声组合下性能不佳的情况，相比网格搜索减少19%的测试量。

Conclusion: ATLAS提供了一种高效的AI模型测试方法，解决了高维测试空间的可扩展性问题，为AI无线接收器的可靠性验证提供了实用解决方案。

Abstract: Industry adoption of Artificial Intelligence (AI)-native wireless receivers,
or even modular, Machine Learning (ML)-aided wireless signal processing blocks,
has been slow. The main concern is the lack of explainability of these trained
ML models and the significant risks posed to network functionalities in case of
failures, especially since (i) testing on every exhaustive case is infeasible
and (ii) the data used for model training may not be available. This paper
proposes ATLAS, an AI-guided approach that generates a battery of tests for
pre-trained AI-native receiver models and benchmarks the performance against a
classical receiver architecture. Using gradient-based optimization, it avoids
spanning the exhaustive set of all environment and channel conditions; instead,
it generates the next test in an online manner to further probe specific
configurations that offer the highest risk of failure. We implement and
validate our approach by adopting the well-known DeepRx AI-native receiver
model as well as a classical receiver using differentiable tensors in NVIDIA's
Sionna environment. ATLAS uncovers specific combinations of mobility, channel
delay spread, and noise, where fully and partially trained variants of
AI-native DeepRx perform suboptimally compared to the classical receivers. Our
proposed method reduces the number of tests required per failure found by 19%
compared to grid search for a 3-parameters input optimization problem,
demonstrating greater efficiency. In contrast, the computational cost of the
grid-based approach scales exponentially with the number of variables, making
it increasingly impractical for high-dimensional problems.

</details>


### [27] [Weighted Covariance Intersection for Range-based Distributed Cooperative Localization of Multi-Agent Systems](https://arxiv.org/abs/2508.12207)
*Chenxin Tu,Xiaowei Cui,Gang Liu,Mingquan Lu*

Main category: eess.SP

TL;DR: 提出加权协方差交集(WCI)方法改进3D分布式协同定位，解决经典CI在状态分量尺度和可观测性差异大时的性能下降问题


<details>
  <summary>Details</summary>
Motivation: 3D环境中多智能体系统协同定位面临状态空间复杂、状态分量尺度和可观测性差异大的挑战，经典CI方法存在尺度不平衡和相关失配问题

Method: 引入加权协方差交集(WCI)机制，设计基于惯性导航系统误差传播规则的权重矩阵，开发多距离测量的并发融合策略

Result: 仿真结果表明WCI相比经典CI显著提升协同定位性能，分布式方法在鲁棒性、可扩展性方面优于集中式方法

Conclusion: WCI方法有效解决了3D分布式协同定位中的相关融合问题，更适合大规模集群应用

Abstract: Precise localization of multi-agent systems (MAS) in harsh environments is a
critical challenge for swarm applications, and cooperative localization is
considered a key solution to this issue. Among all solutions, distributed
cooperative localization (DCL) has garnered widespread attention due to its
robustness and scalability. The main challenge of DCL lies in how to fuse
relative measurements between agents under unknown correlations. To address
this, covariance intersection (CI) was introduced to DCL. However, the
classical CI optimization criteria suffer from issues such as scale imbalance
and correlation mismatch during the fusion process. These deficiencies are not
as pronounced in 2D scenarios, where the state space is relatively simple and
the observability of each state component is well. However, in 3D scenarios,
where the state space is more complex and there are significant disparities in
the scale and observability of state components, performance degradation
becomes severe. This necessitates the design of specialized mechanisms to
improve the data fusion process. In this paper, we identify three main
drawbacks of the classical CI optimization criteria in recursive filtering and
introduce a weighting mechanism, namely weighted covariance intersection (WCI),
to improve its performance. We then introduce WCI into range-based distributed
cooperative localization in 3D scenarios, developing a concurrent fusion
strategy for multiple distance measurements and designing a weighting matrix
based on the error propagation rule of the inertial navigation system (INS).
Simulation results demonstrate that the proposed WCI significantly enhances
cooperative localization performance compared to classical CI, while the
distributed approach outperforms the centralized approach in terms of
robustness, scalability, and is more suitable for large-scale swarms.

</details>


### [28] [Towards Generalizable Human Activity Recognition: A Survey](https://arxiv.org/abs/2508.12213)
*Yize Cai,Baoshen Guo,Flora Salim,Zhiqing Hong*

Main category: eess.SP

TL;DR: 这篇综述论文系统回顾了基于IMU的可泛化人体活动识别领域，涵盖了229篇研究论文和25个公开数据集，从模型中心和数据中心两个角度分类方法，并讨论了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管IMU-based HAR在特定场景下性能有所提升，但其泛化能力仍然是实际应用的主要障碍。领域偏移（用户、传感器位置、环境变化）会导致性能显著下降，因此需要系统研究可泛化的HAR方法。

Method: 从两个角度分类代表性方法：(i)模型中心方法：包括预训练方法、端到端方法、基于大语言模型的学习方法；(ii)数据中心方法：包括多模态学习和数据增强技术。同时总结了广泛使用的数据集、工具和基准。

Result: 提供了基于IMU的可泛化HAR领域的全面概述，包括方法论分类、数据集总结、工具和基准，并建立了持续更新的资源库。

Conclusion: 讨论了持续存在的挑战（如数据稀缺、高效训练、可靠评估）并展望了未来方向，包括基础和大语言模型的采用、物理信息和上下文感知推理、生成建模以及资源高效的训练和推理。

Abstract: As a critical component of Wearable AI, IMU-based Human Activity Recognition
(HAR) has attracted increasing attention from both academia and industry in
recent years. Although HAR performance has improved considerably in specific
scenarios, its generalization capability remains a key barrier to widespread
real-world adoption. For example, domain shifts caused by variations in users,
sensor positions, or environments can significantly decrease the performance in
practice. As a result, in this survey, we explore the rapidly evolving field of
IMU-based generalizable HAR, reviewing 229 research papers alongside 25
publicly available datasets to provide a broad and insightful overview. We
first present the background and overall framework of IMU-based HAR tasks, as
well as the generalization-oriented training settings. Then, we categorize
representative methodologies from two perspectives: (i) model-centric
approaches, including pre-training method, end-to-end method, and large
language model (LLM)-based learning method; and (ii) data-centric approaches,
including multi-modal learning and data augmentation techniques. In addition,
we summarize widely used datasets in this field, as well as relevant tools and
benchmarks. Building on these methodological advances, the broad applicability
of IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent
challenges (e.g., data scarcity, efficient training, and reliable evaluation)
and also outline future directions for HAR, including the adoption of
foundation and large language models, physics-informed and context-aware
reasoning, generative modeling, and resource-efficient training and inference.
The complete list of this survey is available at
https://github.com/rh20624/Awesome-IMU-Sensing, which will be updated
continuously.

</details>


### [29] [A Novel Symbol Level Precoding based AFDM Transmission Framework: Offloading Equalization Burden to Transmitter Side](https://arxiv.org/abs/2508.12215)
*Shuntian Tang,Zesong Fei,Xinyi Wang,Dongkai Zhou,Zhiqiang Wei,Christos Masouros*

Main category: eess.SP

TL;DR: 提出一种基于符号级预编码(SLP)的AFDM传输框架，通过将处理负担从用户端转移到基站端，降低接收端计算复杂度。上行链路采用SBL源估计算法，下行链路采用SLP技术设计发射波形。


<details>
  <summary>Details</summary>
Motivation: AFDM虽然在多普勒效应方面表现优异，但其高的接收端计算复杂度仍是实际部署的主要障碍。需要找到一种方案，能够保持AFDM的性能优势同时大幅度降低接收端的计算要求。

Method: 1. 上行链路：提出基于稀疏贝叶斯学习(SBL)的通道估计算法，利用阿奇频域通道的内在稀疏性，通过层次拉普拉斯分布建模并使用EM算法迭代更新参数
2. 下行链路：基站采用符号级预编码(SLP)技术，基于估计的上行链路通道状态信息和通道互逆性设计发射波形
3. 将优化问题形式化为二阶锥规划(SOCP)问题，并通过拉格朗日函数和KKT条件研究对偶问题

Result: 1. 提出的SBL估计器在准确性和对离网格效应的鲁棒性方面都超过传统的正交匹配追踪(OMP)算法
2. 基于SLP的波形设计方案能够达到与传统AFDM接收机相当的性能，同时显著降低了接收端的计算复杂度
3. 模拟结果验证了该方案的实用性

Conclusion: 该研究提出的SLP基AFDM传输框架有效解决了AFDM高接收端计算复杂度的问题。通过在基站端采用先进的通道估计和波形设计技术，实现了在保持系统性能的同时大幅减载接收端处理复杂度，为AFDM的实际部署提供了可行的解决方案。

Abstract: Affine Frequency Division Multiplexing (AFDM) has attracted considerable
attention for its robustness to Doppler effects. However, its high
receiver-side computational complexity remains a major barrier to practical
deployment. To address this, we propose a novel symbol-level precoding
(SLP)-based AFDM transmission framework, which shifts the signal processing
burden in downlink communications from user side to the base station (BS),
enabling direct symbol detection without requiring channel estimation or
equalization at the receiver. Specifically, in the uplink phase, we propose a
Sparse Bayesian Learning (SBL) based channel estimation algorithm by exploiting
the inherent sparsity of affine frequency (AF) domain channels. In particular,
the sparse prior is modeled via a hierarchical Laplace distribution, and
parameters are iteratively updated using the Expectation-Maximization (EM)
algorithm. We also derive the Bayesian Cramer-Rao Bound (BCRB) to characterize
the theoretical performance limit. In the downlink phase, the BS employs the
SLP technology to design the transmitted waveform based on the estimated uplink
channel state information (CSI) and channel reciprocity. The resulting
optimization problem is formulated as a second-order cone programming (SOCP)
problem, and its dual problem is investigated by Lagrangian function and
Karush-Kuhn-Tucker conditions. Simulation results demonstrate that the proposed
SBL estimator outperforms traditional orthogonal matching pursuit (OMP) in
accuracy and robustness to off-grid effects, while the SLP-based waveform
design scheme achieves performance comparable to conventional AFDM receivers
while significantly reducing the computational complexity at receiver,
validating the practicality of our approach.

</details>


### [30] [Polarization Reconfigurable Transmit-Receive Beam Alignment with Interpretable Transformer](https://arxiv.org/abs/2508.12298)
*Seungcheol Oh,Han Han,Joongheon Kim,Sean Kwon*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于可解释Transformer的深度学习框架，用于大规模MIMO系统中的极化重配和核心网络设计，以减少导频链限制下的导频开销。


<details>
  <summary>Details</summary>
Motivation: 利用可重配极化天线技术提升大规模MIMO系统性能，但极化变量增加了通道维度，而限的RF链数量导致低维导频测量无法满足高维通道估计需求，产生大量导频开销。

Method: 提出了一种可解释Transformer深度学习框架，在发送端和接收端都使用该模型来主动设计极化和核心网络向量。模型基于累积的接收导频序列来进行学习和决策。

Result: 数值实验表明，该框架在性能上显著超过现有的非适应性和主动数据驱动方法。同时模型的可解释性还允许分析学习能力。

Conclusion: 该研究成功地解决了极化重配MIMO系统中的导频开销问题，通过深度学习框架实现了更高效的通道重配和核心对准，为无线通信系统提供了性能显著提升。

Abstract: Recent advancement in next generation reconfigurable antenna and fluid
antenna technology has influenced the wireless system with polarization
reconfigurable (PR) channels to attract significant attention for promoting
beneficial channel condition. We exploit the benefit of PR antennas by
integrating such technology into massive multiple-input-multiple-output (MIMO)
system. In particular, we aim to jointly design the polarization and
beamforming vectors on both transceivers for simultaneous channel
reconfiguration and beam alignment, which remarkably enhance the beamforming
gain. However, joint optimization over polarization and beamforming vectors
without channel state information (CSI) is a challenging task, since
depolarization increases the channel dimension; whereas massive MIMO systems
typically have low-dimensional pilot measurement from limited radio frequency
(RF) chain. This leads to pilot overhead because the transceivers can only
observe low-dimensional measurement of the high-dimension channel. This paper
pursues the reduction of the pilot overhead in such systems by proposing to
employ \emph{interpretable transformer}-based deep learning framework on both
transceivers to actively design the polarization and beamforming vectors for
pilot stage and transmission stage based on the sequence of accumulated
received pilots. Numerical experiments demonstrate the significant performance
gain of our proposed framework over the existing non-adaptive and active
data-driven methods. Furthermore, we exploit the interpretability of our
proposed framework to analyze the learning capabilities of the model.

</details>


### [31] [Jamming Identification with Differential Transformer for Low-Altitude Wireless Networks](https://arxiv.org/abs/2508.12320)
*Pengyu Wang,Zhaocheng Wang,Tianqi Mao,Weijie Yuan,Haijun Zhang,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 提出了一种基于差分变换器的无线干扰识别框架，通过差分自注意力机制和随机掩码训练策略来提升对抗样本的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 无人机等低空无线网络易受电磁干扰，而现有的深度学习干扰识别方案在面对精心设计的对抗样本时存在鲁棒性下降的问题

Method: 1) 差分变换器网络进行差分自注意力操作来区分干扰信号；2) 随机掩码训练策略创建并行特征提取分支；3) 双分支正则化的一致性训练框架

Result: 仿真结果表明该方法在提升对抗样本鲁棒性方面优于现有方法

Conclusion: 该框架通过差分变换器和随机掩码训练有效提升了无线干扰识别系统对抗对抗攻击的鲁棒性

Abstract: Wireless jamming identification, which detects and classifies electromagnetic
jamming from non-cooperative devices, is crucial for emerging low-altitude
wireless networks consisting of many drone terminals that are highly
susceptible to electromagnetic jamming. However, jamming identification schemes
adopting deep learning (DL) are vulnerable to attacks involving carefully
crafted adversarial samples, resulting in inevitable robustness degradation. To
address this issue, we propose a differential transformer framework for
wireless jamming identification. Firstly, we introduce a differential
transformer network in order to distinguish jamming signals, which overcomes
the attention noise when compared with its traditional counterpart by
performing self-attention operations in a differential manner. Secondly, we
propose a randomized masking training strategy to improve network robustness,
which leverages the patch partitioning mechanism inherent to transformer
architectures in order to create parallel feature extraction branches. Each
branch operates on a distinct, randomly masked subset of patches, which
fundamentally constrains the propagation of adversarial perturbations across
the network. Additionally, the ensemble effect generated by fusing predictions
from these diverse branches demonstrates superior resilience against
adversarial attacks. Finally, we introduce a novel consistent training
framework that significantly enhances adversarial robustness through dualbranch
regularization. Simulation results demonstrate that our proposed methodology is
superior to existing methods in boosting robustness to adversarial samples.

</details>


### [32] [Coherent Compensation-Based Sensing for Long-Range Targets in Integrated Sensing and Communication System](https://arxiv.org/abs/2508.12371)
*Lin Wang,Zhiqing Wei,Xu Chen,Zhiyong Feng*

Main category: eess.SP

TL;DR: 基于MUSIC和LS的空间信号分离策略与相干补偿处理方法，解决OFDM作为ISAC波形时的距离限制问题，显著提升了远程目标的SINR和检测概率


<details>
  <summary>Details</summary>
Motivation: 解决OFDM作为ISAC波形时因循环前缀(CP)时长限制导致的最大感知距离限制，以及远距离目标被近距离目标强回波干扰的问题

Method: 提出基于MUSIC和最小二乘(LS)的空间信号分离方法来分离不同目标的回波信号，以及基于相干补偿的感知信号处理方法来提升SINR

Result: 模拟结果显示，在500米远程目标上，方法比传统二维快速归一变换(2D-FFT)方法显著提升了RDM的SINR 10dB，检测概率也显著提高

Conclusion: 该方法有效解决了ISAC系统中OFDM波形的距离限制问题，为6G集成感知与通信的实际应用提供了有效的技术解决方案

Abstract: Integrated sensing and communication (ISAC) is a promising candidate
technology for 6G due to its improvement in spectral efficiency and energy
efficiency. Orthogonal frequency division multiplexing (OFDM) signal is a
mainstream candidate ISAC waveform. However, there are inter-symbol
interference (ISI) and inter-carrier interference (ICI) when the round-trip
delay exceeds the cyclic prefix (CP) duration for OFDM signals, which limits
the maximum sensing range of ISAC system. When detecting a long-range target,
the wide beam inevitably covers the close-range target, of which the echo's
power is much larger than that of the long-range target. In order to tackle the
above problem, a multiple signal classification (MUSIC) and least squares
(LS)-based spatial signal separation method is proposed to separate the echo
signals reflected from different targets. Moreover, a coherent
compensation-based sensing signal processing method at the receiver is proposed
to enhance the signal to interference plus noise power ratio (SINR) of the OFDM
block for generating the range-Doppler map (RDM) with higher SINR. Simulation
results reveal that the proposed method greatly enhances the SINR of RDM by 10
dB for a target at 500 m compared with two-dimensional fast Fourier transform
(2D-FFT) method. Besides, the detection probability is also significantly
improved compared to the benchmarking method.

</details>


### [33] [On the Extension of Differential Beamforming Theory to Arbitrary Planar Arrays of First-Order Elements](https://arxiv.org/abs/2508.12403)
*Federico Miotello,Davide Albertini,Alberto Bernardini*

Main category: eess.SP

TL;DR: 基于环形调和展开的广义模态匹配框架，用于带有向性传感器的平面数组频率不变微分波束形成


<details>
  <summary>Details</summary>
Motivation: 传统差分波束形成技术假设传感器全向性，而实际传感器存在频率相关的向性性，导致性能降级

Method: 通过将期望波束图表示为截断的环形调和展开，并与实际元件响应进行拟合，支持任意平面布局和元件方位

Result: 模拟结果证明，在设计阶段考虑传感器向性性能够在不同频率、不同布局和噪声条件下实现准确而稳健的性能

Conclusion: 该方法允许合成任意阶数和扭转方向的波束图，无需严格的数组布局要求，为带向性传感器的广带差分波束形成提供了有效解决方案

Abstract: Small-size acoustic arrays exploit spatial diversity to achieve capabilities
beyond those of single-element devices, with applications ranging from
teleconferencing to immersive multimedia. A key requirement for broadband array
processing is a frequency-invariant spatial response, which ensures consistent
directivity across wide bandwidths and prevents spectral coloration.
Differential beamforming offers an inherently frequency-invariant solution by
leveraging pressure differences between closely spaced elements of small-size
arrays. Traditional approaches, however, assume the array elements to be
omnidirectional, whereas real transducers exhibit frequency-dependent
directivity that can degrade performance if not properly modeled. To address
this limitation, we propose a generalized modal matching framework for
frequency-invariant differential beamforming, applicable to unconstrained
planar arrays of first-order directional elements. By representing the desired
beampattern as a truncated circular harmonic expansion and fitting it to the
actual element responses, our method accommodates arbitrary planar geometries
and element orientations. This approach enables the synthesis of beampatterns
of any order and steering direction without imposing rigid layout requirements.
Simulations confirm that accounting for sensor directivity at the design stage
yields accurate and robust performance across varying frequencies, geometries,
and noise conditions.

</details>


### [34] [Towards SISO Bistatic Sensing for ISAC](https://arxiv.org/abs/2508.12614)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Min Xu,Y. Jay Guo*

Main category: eess.SP

TL;DR: 这篇论文提出了WiDFS 3.0轻量级双基站SISO感知框架，通过自参考互相关和延迟域波束成技术，有效解决了单天线配置下的钟步异步和多普勒镜像歧义问题，实现了精确的延迟和多普勒估计。


<details>
  <summary>Details</summary>
Motivation: 集成感知与通信(ISAC)是下一代无线系统的关键技术，但实际部署常受限于低成本单天线收发器。在这种双基站SISO配置下，钟步异步会导致通道状态信息(CSI)中出现随机相位偏移，既往的多天线方法无法解决这个问题。

Method: 提出了自参考互相关(SRCC)方法来消除SISO中的随机相位偏移，并采用延迟域波束成技术来解决多普勒歧义问题。通过提取无歧义的延迟-多普勒-时间特征，使用简洁神经网络进行稳健感知。

Result: 实验结果显示WiDFS 3.0能够实现精准的参数估计，性能可与甚至超过之前的多天线方法相比，尤其是在延迟估计方面。在单目标和多目标场景下都表现出优异的感知准确性和泛化能力。例如，在仅有1.3M参数的MobileViT-XXS上部署时，性能仍然超过传统的CSI振幅、镜像多普勒和多收发器聚合多普勒等特征。

Conclusion: WiDFS 3.0为低复杂度部署提供了一种高效的解决方案，通过创新的相位消除和歧义解决技术，在单天线配置下实现了与多天线系统相当的感知性能，为ISAC技术的普及和应用推广提供了重要支撑。

Abstract: Integrated Sensing and Communication (ISAC) is a key enabler for
next-generation wireless systems. However, real-world deployment is often
limited to low-cost, single-antenna transceivers. In such bistatic Single-Input
Single-Output (SISO) setup, clock asynchrony introduces random phase offsets in
Channel State Information (CSI), which cannot be mitigated using conventional
multi-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic
SISO sensing framework that enables accurate delay and Doppler estimation from
distorted CSI by effectively suppressing Doppler mirroring ambiguity. It
operates with only a single antenna at both the transmitter and receiver,
making it suitable for low-complexity deployments. We propose a
self-referencing cross-correlation (SRCC) method for SISO random phase removal
and employ delay-domain beamforming to resolve Doppler ambiguity. The resulting
unambiguous delay-Doppler-time features enable robust sensing with compact
neural networks. Extensive experiments show that WiDFS 3.0 achieves accurate
parameter estimation, with performance comparable to or even surpassing that of
prior multi-antenna methods, especially in delay estimation. Validated under
single- and multi-target scenarios, the extracted ambiguity-resolved features
show strong sensing accuracy and generalization. For example, when deployed on
the embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0
consistently outperforms conventional features such as CSI amplitude, mirrored
Doppler, and multi-receiver aggregated Doppler.

</details>


### [35] [Factorized Disentangled Representation Learning for Interpretable Radio Frequency Fingerprint](https://arxiv.org/abs/2508.12660)
*Yezhuo Zhang,Zinan Zhou,Guangyu Li,Xuanpeng Li*

Main category: eess.SP

TL;DR: 本文提出了一种新的解耦表示学习框架，用于从无线电信号中提取明确、独立的多因素表示，包括设备的无线电颜纹特征，以提高IoT设备识别的稳健性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的无线电颜纹特征识别方法主要依赖域适配技术，缺乏明确的因素表示，导致稳健性不足和对下游任务的控制性有限。需要一种能够明确解耦多个因素的方法来提高RFFI的效果。

Method: 提出了一种解耦表示学习框架，通过设计两个专门模块（因素分类和信号重构）来实现明确性、模块化和紧凑性的解耦原则。每个模块都有特制的损失函数来促进有效解耦并增强对下游任务的支持。

Result: 在两个公开数据集和一个自收集数据集上评估，方法在多个DRL指标上取得了显著成绩。在下游RFFI任务和条件信号生成任务上都显示出有效性，所有模块都有助于提高分类准确率，并能够实现对条件生成信号的精确控制。

Conclusion: 该解耦表示学习框架为解释性和明确的无线电颜纹特征提供了潜力，通过明确解耦多种因素来提高IoT设备识别的稳健性和可控性。

Abstract: In response to the rapid growth of Internet of Things (IoT) devices and
rising security risks, Radio Frequency Fingerprint (RFF) has become key for
device identification and authentication. However, various changing factors -
beyond the RFF itself - can be entangled from signal transmission to reception,
reducing the effectiveness of RFF Identification (RFFI). Existing RFFI methods
mainly rely on domain adaptation techniques, which often lack explicit factor
representations, resulting in less robustness and limited controllability for
downstream tasks. To tackle this problem, we propose a novel Disentangled
Representation Learning (DRL) framework that learns explicit and independent
representations of multiple factors, including the RFF. Our framework
introduces modules for disentanglement, guided by the principles of
explicitness, modularity, and compactness. We design two dedicated modules for
factor classification and signal reconstruction, each with tailored loss
functions that encourage effective disentanglement and enhance support for
downstream tasks. Thus, the framework can extract a set of interpretable
vectors that explicitly represent corresponding factors. We evaluate our
approach on two public benchmark datasets and a self-collected dataset. Our
method achieves impressive performance on multiple DRL metrics. We also analyze
the effectiveness of our method on downstream RFFI task and conditional signal
generation task. All modules of the framework contribute to improved
classification accuracy, and enable precise control over conditional generated
signals. These results highlight the potential of our DRL framework for
interpretable and explicit RFFs.

</details>


### [36] [Multi-Domain Supervised Contrastive Learning for UAV Radio-Frequency Open-Set Recognition](https://arxiv.org/abs/2508.12689)
*Ning Gao,Tianrui Zeng,Bowen Chen,Donghong Cai,Shi Jin,Michail Matthaiou*

Main category: eess.SP

TL;DR: 提出一种基于多域监督对比学习的无人机开收集识别框架Open-RFNet，通过融合ResNet和TransformerEncoder特征，使用改进生成式OpenMax算法，在25种无人机类型上实现了闭集识别95.12%和开集识别96.08%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决5G-进阶网络中非法无人机飞行事件频发对低空域感知通信网络造成的安全威胁，需要有效监控非协作无人机。

Method: 提出MD-SupContrast框架：1)融合ResNet的纹理特征和TransformerEncoder的时频位置特征 2)使用监督对比学习优化闭集样本特征表征 3)提出改进生成式IG-OpenMax算法构建Open-RFNet模型 4)冻结特征提取层，重新训练分类层处理未知样本

Result: 在大规模无人机开收集数据集上进行实验，Open-RFNet在25种无人机类型上实现：闭集识别准确率95.12%，开集识别准确率96.08%，性能超过现有基准方法。

Conclusion: 该方法能够有效监控LA-ISAC网络中的非法无人机，通过多域特征融合和改进开收集识别算法，在闭集和开集场景下都取得了优异的识别性能。

Abstract: 5G-Advanced (5G-A) has enabled the vibrant development of low altitude
integrated sensing and communication (LA-ISAC) networks. As a core component of
these networks, unmanned aerial vehicles (UAVs) have witnessed rapid growth in
recent years. However, due to the lag in traditional industry regulatory norms,
unauthorized flight incidents occur frequently, posing a severe security threat
to LA-ISAC networks. To surveil the non-cooperative UAVs, in this paper, we
propose a multi-domain supervised contrastive learning (MD-SupContrast)
framework for UAV radio frequency (RF) open-set recognition. Specifically,
first, the texture features and the time-frequency position features from the
ResNet and the TransformerEncoder are fused, and then the supervised
contrastive learning is applied to optimize the feature representation of the
closed-set samples. Next, to surveil the invasive UAVs that appear in real
life, we propose an improved generative OpenMax (IG-OpenMax) algorithm and
construct an open-set recognition model, namely Open-RFNet. According to the
unknown samples, we first freeze the feature extraction layers and then only
retrain the classification layer, which achieves excellent recognition
performance both in closed-set and open-set recognitions. We analyze the
computational complexity of the proposed model. Experiments are conducted with
a large-scale UAV open dataset. The results show that the proposed Open-RFNet
outperforms the existing benchmark methods in terms of recognition accuracy
between the known and the unknown UAVs, as it achieves 95.12% in closed-set and
96.08% in open-set under 25 UAV types, respectively.

</details>


### [37] [LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent Metasurface Antenna Systems](https://arxiv.org/abs/2508.12728)
*Yunsong Huang,Hui-Ming Wang,Qingli Yan,Zhaowei Wang*

Main category: eess.SP

TL;DR: LLM-RIMSA是一个将大语言模型与新型可重构智能超表面天线架构相结合的创新框架，通过LLM的跨模态推理和少样本学习能力动态优化天线配置，在6G网络中实现卓越性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要超大规模连接和智能无线电环境，但现有可重构智能表面技术在硬件效率、动态控制和可扩展性方面存在严重限制，传统优化和深度学习方法难以处理高维状态空间和训练成本问题。

Method: 提出RIMSA架构，采用平行同轴馈电和2D超表面集成，使每个超材料单元能独立调整幅度和相位；利用预训练大语言模型的跨模态推理和少样本学习能力来动态优化RIMSA配置。

Result: 仿真显示LLM-RIMSA实现了最先进的性能，在总速率方面优于传统的基于深度学习的方法，同时显著降低了训练开销。

Conclusion: 该框架为LLM驱动的智能无线电环境开辟了新途径，解决了6G网络中的关键技术和可扩展性挑战。

Abstract: The evolution of 6G networks demands ultra-massive connectivity and
intelligent radio environments, yet existing reconfigurable intelligent surface
(RIS) technologies face critical limitations in hardware efficiency, dynamic
control, and scalability. This paper introduces LLM-RIMSA, a transformative
framework that integrates large language models (LLMs) with a novel
reconfigurable intelligent metasurface antenna (RIMSA) architecture to address
these challenges. Unlike conventional RIS designs, RIMSA employs parallel
coaxial feeding and 2D metasurface integration, enabling each individual
metamaterial element to independently adjust both its amplitude and phase.
While traditional optimization and deep learning (DL) methods struggle with
high-dimensional state spaces and prohibitive training costs for RIMSA control,
LLM-RIMSA leverages pre-trained LLMs cross-modal reasoning and few-shot
learning capabilities to dynamically optimize RIMSA configurations. Simulations
demonstrate that LLM-RIMSA achieves state-of-the-art performance, outperforming
conventional DL-based methods in sum rate while reducing training overhead. The
proposed framework pave the way for LLM-driven intelligent radio environments.

</details>


### [38] [Range-Angle Likelihood Maps for Indoor Positioning Using Deep Neural Networks](https://arxiv.org/abs/2508.12746)
*Muhammad Ammad,Paul Schwarzbach,Michael Schultz,Oliver Michler*

Main category: eess.SP

TL;DR: 使用ResNet神经网络模型，通过超参数优化实现飞机舱内厘米级精度的室内定位


<details>
  <summary>Details</summary>
Motivation: 室内精确定位在飞机舱环境中与室外导航同样重要，需要高可靠性和准确性的定位技术来监控舱内位置

Method: 利用模拟飞机舱环境测量数据，将标签与锚点之间的距离和角度映射为残差网格，转换为似然网格图，然后使用ResNet模型进行训练，并通过超参数优化获得最佳参数设置

Result: 经过超参数优化后的ResNet模型能够实现厘米级精度的定位准确度

Conclusion: 提出的基于ResNet的深度学习方法能够有效解决飞机舱内高精度定位问题，达到厘米级定位精度

Abstract: Accurate and high precision of the indoor positioning is as important as
ensuring reliable navigation in outdoor environments. Using the
state-of-the-art deep learning models provides better reliability and accuracy
to navigate and monitor the accurate positions in the aircraft cabin
environment. We utilize the simulated aircraft cabin environment measurements
and propose a residual neural network (ResNet) model to predict the accurate
positions inside the cabin. The measurements include the ranges and angles
between a tag and the anchors points which are then mapped onto a grid as range
and angle residuals. These residual maps are then transformed into the
likelihood grid maps where each cell of the grid shows the likelihood of being
a true location. These grid maps along with the true positions are then passed
as inputs to train the ResNet model. Since any deep learning model involve
numerous parameter settings, hyperparameter optimization is performed to get
the optimal parameters for training the model effectively with the highest
accuracy. Once we get the best hyperparameters settings of the model, it is
then trained to predict the positions which provides a centimeter-level
accuracy of the localization.

</details>


### [39] [A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN](https://arxiv.org/abs/2508.12892)
*Mahdi Abdollahpour,Marco Bertuletti,Yichao Zhang,Yawei Li,Luca Benini,Alessandro Vanelli-Coralli*

Main category: eess.SP

TL;DR: 低复杂度模型驱动神经网基带处理接收机，在MU-MIMO系统中实现更高性能和更低计算开销


<details>
  <summary>Details</summary>
Motivation: 解决AI基带处理方案计算和内存要求高的问题，以便在RAN网络边缘部署和支持大带宽多天线7G系统

Method: 提出一种与5G NR兼容的低复杂度模型驱动神经网络基础的接收机，支持多种调制方案、带宽、用户数和天线数

Result: 在PUSCH处理模拟中，该方案在TBLER性能上超过现有最优方法，同时减少浮点运算次数66倍和可学参数396倍

Conclusion: 该方案为6G系统提供了高效、可扩展的基带处理解决方案，适合在网络边缘部署

Abstract: Artificial intelligence approaches for base-band processing for radio
receivers have demonstrated significant performance gains. Most of the proposed
methods are characterized by high compute and memory requirements, hindering
their deployment at the edge of the Radio Access Networks (RAN) and limiting
their scalability to large bandwidths and many antenna 6G systems. In this
paper, we propose a low-complexity, model-driven neural network-based receiver,
designed for multi-user multiple-input multiple-output (MU-MIMO) systems and
suitable for implementation at the RAN edge. The proposed solution is compliant
with the 5G New Radio (5G NR), and supports different modulation schemes,
bandwidths, number of users, and number of base-station antennas with a single
trained model without the need for further training. Numerical simulations of
the Physical Uplink Shared Channel (PUSCH) processing show that the proposed
solution outperforms the state-of-the-art methods in terms of achievable
Transport Block Error Rate (TBLER), while reducing the Floating Point
Operations (FLOPs) by 66$\times$, and the learnable parameters by 396$\times$.

</details>


### [40] [Interference-Asymmetric UAV Remote Control Links: Measurements and Performance Evaluation](https://arxiv.org/abs/2508.12941)
*Donggu Lee,Sung Joon Maeng,Ozgur Ozdemir,Mani Bharathi Pandian,Ismail Guvenc*

Main category: eess.SP

TL;DR: 通过实际测量和模拟分析，证实了无人机遥控链路中上行下行干扰不对称问题，上行HARQ指示符失败会导致下行速率显著下降。


<details>
  <summary>Details</summary>
Motivation: 无人机遥控链路的可靠性和安全性至关重要，但无人机围网内干扰源更多，导致上行下行干扰不对称问题，影响HARQ反馈机制效果。

Method: 首先在NC State大学主校区使用汽球飞船平台进行实际测量，然后使用MATLAB LTE和5G工具箱模拟评估HARQ指示符失败对速率的影响。

Result: 数值结果确认上行干扰不对称会导致HARQ指示符反馈失败，从而对下行速率性能造成显著的性能降级。

Conclusion: 无人机遥控系统存在上行下行干扰不对称问题，HARQ反馈机制容易受干扰影响，需要重点关注和优化上行链路的可靠性。

Abstract: Reliable and secure connectivity is crucial for remote control (RC) and
uncrewed aerial vehicles (UAVs) links. A major problem for UAV RC links is that
interference sources within the coverage may degrade the link quality. Such
interference problems are a higher concern for the UAV than the RC unit on the
ground due to the UAV being in line of sight (LoS) with a larger number of
interference sources. As a result, lost hybrid automatic repeat request (HARQ)
indicators (ACK/NACK) feedback in the uplink (UL, RC to UAV) may degrade the
downlink (DL, UAV to RC) throughput. To get physical evidence for our
interference asymmetry argument, we first conducted a measurement campaign
using a helikite platform at the Main Campus area of NC State University during
the 2024 Packapalooza festival. Subsequently, we evaluated the throughput
impact of the loss of HARQ indicator feedback caused by UL asymmetry using
MATLAB long-term-evolution (LTE) and fifth-generation (5G) toolboxes. Our
numerical results confirm that UL interference asymmetry substantially degrades
the throughput performance due to the loss of HARQ indicator feedback.

</details>


### [41] [A Novel CNN Based Standalone Detector for Faster-than-Nyquist Signaling](https://arxiv.org/abs/2508.12964)
*Osman Tokluoglu,Enver Cavus,Ebrahim Bedeer,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于卷积神经网络的新题检测器，通过结构化固定内核层和领域知识掩码技术，有效减少超奈相特信号的码间干扰，在保持计算效率的同时提高了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 超奈相特信号的码间干扰(ISI)问题是限制其应用的主要挑战，需要查找更高效的检测方法来替代复杂的BCJR算法。

Method: 采用结构化固定卷积内核层，通过领域知识掩码明确学习不同距离的ISI模式，并使用层次满满分配策略优化满器分配。

Result: 该检测器在压缩因子τ≥0.7时达到近优的BER性能，与BCJR算法相当，同时计算成本减少达46%(BPSK)和84%(QPSK)，具备适应高阶调制和多路衰落环境的稳健性。

Conclusion: 该方法为FTN信号检测提供了一种高效、低复杂度的解决方案，具有强大的实际应用潜力。

Abstract: This paper presents a novel convolutional neural network (CNN)-based detector
for faster-than-Nyquist (FTN) signaling, introducing structured fixed kernel
layers with domain-informed masking to effectively mitigate intersymbol
interference (ISI). Unlike standard CNN architectures that rely on moving
kernels, the proposed approach employs fixed convolutional kernels at
predefined positions to explicitly learn ISI patterns at varying distances from
the central symbol. To enhance feature extraction, a hierarchical filter
allocation strategy is employed, assigning more filters to earlier layers for
stronger ISI components and fewer to later layers for weaker components. This
structured design improves feature representation, eliminates redundant
computations, and enhances detection accuracy while maintaining computational
efficiency. Simulation results demonstrate that the proposed detector achieves
near-optimal bit error rate (BER) performance, comparable to the BCJR algorithm
for the compression factor $\tau \geq 0.7$, while offering up to $46\%$ and
$84\%$ computational cost reduction over M-BCJR for BPSK and QPSK,
respectively. Additional evaluations confirm the method's adaptability to
high-order modulations (up to 64-QAM), resilience in quasi-static multipath
Rayleigh fading channels, and effectiveness under LDPC-coded FTN transmission,
highlighting its robustness and practicality.

</details>


### [42] [Wavefield Correlation Imaging in Arbitrary Media with Inherent Aberration Correction](https://arxiv.org/abs/2508.13017)
*Scott Schoen Jr,Brian Lause,Marko Jakovljevic,Rimon Tadross,Mike Washburn,Anthony E. Samir*

Main category: eess.SP

TL;DR: 通过扩展波场相关成像(WCI)技术，开发了能够直接处理任意知道声速分布的异质性WCI(HWCI)方法，在计算机模拟、离体实验和在体实验中实现了超过30%的分辨率提升和约10%的对比度改善。


<details>
  <summary>Details</summary>
Motivation: 超声成像在形态异质性主体(如超重或肥胖症患者)中遇到挑战，因为传统成像算法在聚集过程中不考虑这种变异。虽然知道空间变异可以补偿算法，但会增加计算复杂度。

Method: 提出了异质性WCI(HWCI)方法，在图像形成过程中直接考虑任意知道的声速分布，而非假设均匀介质。通过空间频率域的高效图像形成来实现。

Result: 在计算机模拟、离体实验和在体实验中验证了可行性，与传统WCI相比，分辨率提高超过30%，对比度提高约10%。

Conclusion: HWCI技术具有高的转化潜力，能够显著提高超声图像的客观质量，从而增强其临床应用价值。

Abstract: Ultrasound (US) imaging is an indispensable tool for diagnostic imaging,
particularly given its cost, safety, and portability profiles compared to other
modalities. However, US is challenged in subjects with morphological
heterogeneity (e.g., those with overweight or obesity), largely because
conventional imaging algorithms do not account for such variation in the
beamforming process. Specific knowledge of the these spatial variations enables
supplemental corrections of these algorithms, but with added computational
complexity. Wavefield correlation imaging (WCI) enables efficient image
formation in the spatial frequency domain that, in its canonical formulation,
assumes a uniform medium. In this work, we present an extension of WCI to
arbitrary known speed-of-sound distributions directly in the image formation
process, and demonstrate its feasibility in silico, in vitro, and in vivo. We
report resolution improvements of over 30% and contrast improvements of order
10% over conventional WCI imaging. Together our results suggest heterogeneous
WCI (HWCI) may have high translational potential to improve the objective
quality, and thus clinical utility, of ultrasound images.

</details>


### [43] [Low-complexity Leakage Minimization Beamforming for Large-scale Multi-user Cell-Free Massive MIMO](https://arxiv.org/abs/2508.13067)
*Iván Alexander Morales Sandoval,Getuar Rexhepi,Kengo Ando,Giuseppe Thadeu Freitas de Abreu*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose a low-complexity beamforming (BF) design for information leakage
minimization in multi-user (MU) cell-free massive multiple-input
multiple-output (CF-mMIMO) systems. Our approach leverages fractional
programming (FP) to reformulate the secrecy rate maximization problem into a
tractable difference-of-convex form. To efficiently solve the resulting
non-convex problem, we employ the Concave-Convex Procedure (CCP), enabling fast
convergence to a local optimum. Simulation results demonstrate that the
proposed scheme achieves secrecy rates comparable to state-of-the-art (SotA)
methods, while significantly reducing computational complexity and improving
convergence speed.

</details>


### [44] [BeamSeek: Deep Learning-based DOA Estimation for Low-Complexity mmWave Phased Arrays](https://arxiv.org/abs/2508.13075)
*Arav Sharma,Lei Chi,Ari Gebhardt,Alon S. Levin,Timothy R. Hoerning,Sam Keene*

Main category: eess.SP

TL;DR: BeamSeek结合敏捷波束切换和深度学习，在60GHz毫米波系统中显著提升了DOA估计的速度和精度，相比传统方法平均误差降低8度，特别适用于多径干扰环境。


<details>
  <summary>Details</summary>
Motivation: 传统DOA方法需要直接访问单个天线单元，不适用于现代毫米波系统中普遍采用的模拟或混合波束成形系统。现有的敏捷波束切换技术虽然快速但精度和鲁棒性有待提升。

Method: 采用多层感知器(MLP)和专门的数据增强技术来模拟真实传播条件，结合敏捷波束切换技术，在NSF PAWR COSMOS测试床上进行60GHz实验验证。

Result: 相比基于相关性的基准方法，在不同信噪比水平下均表现出显著改进，平均估计误差最多降低8度，在噪声信道中优势尤其明显。

Conclusion: BeamSeek特别适用于具有多径干扰和硬件约束的实际毫米波部署环境，为低复杂度硬件实现提供了有效的DOA估计解决方案。

Abstract: A novel approach combining agile beam switching with deep learning to enhance
the speed and accuracy of Direction of Arrival (DOA) estimation for
millimeter-wave (mmWave) phased array systems with low-complexity hardware
implementations is proposed and evaluated. Traditional DOA methods requiring
direct access to individual antenna elements are impractical for analog or
hybrid beamforming systems prevalent in modern mmWave implementations. Recent
agile beam switching techniques have demonstrated rapid DOA estimation, but
their accuracy and robustness can be further improved via deep learning.
BeamSeek addresses these limitations by employing a Multi-Layer Perceptron
(MLP) and specialized data augmentation that emulates real-world propagation
conditions. The proposed approach was experimentally validated at 60 GHz using
the NSF PAWR COSMOS testbed, demonstrating significant improvements over a
correlation-based method across various Signal-to-Noise Ratio (SNR) levels.
Results show that BeamSeek achieves up to an 8 degree reduction in average
estimation error compared to this baseline, with particular advantages in noisy
channels. This makes it especially suitable for practical mmWave deployments in
environments characterized by multipath interference and hardware constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: BuilDa是一个热建筑数据生成框架，无需深厚建筑模拟知识即可生成大量高质量合成数据，用于迁移学习研究。


<details>
  <summary>Details</summary>
Motivation: 迁移学习可以改进建筑热动力学建模，但现有公共数据集和数据生成器无法满足迁移学习研究对数据质量和数量的需求，且通常需要建筑模拟专业知识。

Method: 使用单区域Modelica模型导出为功能模拟单元(FMU)，在Python中进行模拟，构建BuilDa框架生成合成数据。

Result: 成功生成数据并用于迁移学习模型的预训练和微调，证明了框架的有效性。

Conclusion: BuilDa框架能够为迁移学习研究提供足够质量和数量的热建筑数据，且不需要深厚的建筑模拟专业知识。

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [46] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV首次探索了多上下文KV缓存的注意力稀疏化方法，通过考虑其他上下文的互补信息进行稀疏化并局部重计算，在RAG场景中实现15%的序列长度压缩且不损失精度


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存重用方法只适用于单上下文场景，在RAG多上下文场景中由于缺乏跨上下文注意力而失效，现有方法需要保留全部KV缓存导致内存开销大

Method: SamKV在多上下文KV缓存稀疏化时考虑其他上下文的互补信息，然后对稀疏化信息进行局部重计算

Result: 实验表明该方法能将序列长度压缩至15%，相比完全重计算基线无精度损失，显著提升多上下文RAG场景的吞吐量

Conclusion: SamKV是首个针对多上下文KV缓存注意力稀疏化的有效解决方案，成功解决了RAG场景中的内存和效率问题

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [47] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: 提出了Representation Stability (RS)框架，通过测量掩码重要词汇时嵌入表示的变化来检测对抗文本攻击，无需重新训练模型，在多个数据集和攻击类型上达到88%以上的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗文本防御方法通常是攻击特定的或需要昂贵的模型重新训练，需要一种模型无关的检测框架来应对持续存在的对抗文本攻击威胁。

Method: RS框架首先使用重要性启发式方法对词汇进行排名，然后测量掩码前k个关键词汇时的嵌入敏感性，最后使用BiLSTM检测器处理产生的模式。使用NDCG衡量扰动识别质量。

Result: 在三个数据集、三种攻击类型和两个受害者模型上，RS实现了超过88%的检测准确率，计算成本较低，且能很好地泛化到未见过的数据集、攻击和模型。

Conclusion: RS提供了一个实用的对抗文本检测解决方案，具有模型无关性、高检测准确率和良好的泛化能力，梯度基排名方法在扰动识别方面表现优于注意力和随机选择方法。

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [48] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级的KDCL_sInvResUNet模型，通过协同学习方案在嵌入式设备上实现了实时无创血压监测，计算负载仅0.02 GFLOPS，在大规模困术期数据集上达到了与大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多深度学习模型能够从非侵入性生理信号重建血压波形，但现有研究对模型在嵌入式系统上部署的性能和计算负载问题关注不够，特别是在围手术期设置中实现实时监测的需求。

Method: 研究提出了轻量级的sInvResUNet模型，结合协同学习方案KDCL_sInvResUNet。模型仅包0.89万参数，计算负载0.02 GFLOPS，在嵌入式设备上实现了推理时间8.49毫秒（对10秒输出）。在包含2,154名患者、1,257,141个数据段的大规模困术期数据集上进行了主体独立验证。

Result: KDCL_sInvResUNet在广泛血压范围内（缩式血压41-257 mmHg，舒张压31-234 mmHg）达到了略优于大模型的性能：平均绝对误差10.06 mmHg，平均Pearson相关系数0.88。然而，所有深度学习模型都显示出在不同人口统计和心血管条件下的显著性能差异。

Conclusion: 这项研究为在真实围术期环境中实现实时、无干扰的血压监测奠定了基础，为该领域的未来发展提供了基准。但模型在广泛多样人群中的普遍性仍有限，需要进一步改进。

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [49] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: 提出MSLoRA-CR方法解决多模态生物医学图像增量学习问题，通过模态特定LoRA模块和对比正则化实现知识保持和跨模态知识迁移


<details>
  <summary>Details</summary>
Motivation: 生物医学领域需要处理多种模态和任务，为每个模态单独训练模型会显著增加推理成本，需要统一的增量学习模型

Method: 基于大型视觉语言模型，冻结预训练参数，为每个模态增量添加特定LoRA模块，并引入对比正则化促进模态内知识共享和模态间知识区分

Result: 在生物医学图像增量学习实验中，MSLoRA-CR相比为每个模态单独训练模型和通用增量学习方法表现更好，整体性能提升1.88%，同时保持计算效率

Conclusion: MSLoRA-CR有效解决了多模态生物医学图像增量学习的两个核心挑战，在性能和效率方面都优于现有方法

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [50] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的终身学习框架，通过Transformer网络和上下文调度器，增量式训练神经网络求解器来处理不同上下文中的车辆路由问题，提高了模型的适用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络路由求解器通常在单一上下文中训练，如使用欧几里得距离和固定问题规模，导致在不同场景下应用效果不佳。需要提高模型的多样性和适应能力。

Method: 设计了终身学习者(LL)，使用Transformer作为核心网络，通过上下文间自注意力机制进行知识转移。同时开发了动态上下文调度器(DCS)，利用跨上下文经验重现技术来回顾之前的解决策略。

Result: 在合成和标准数据集(问题规模达18k)上的实验结果显示，该方法能够发现有效的策略来处理不同上下文中的车辆路由问题，性能超过其他神经求解器，在大部分VRP问题上达到最佳表现。

Conclusion: 该终身学习框架通过增量式训练和知识转移，显著提升了神经网络路由求解器在不同场景下的适用性和性能，为处理多样化VRP问题提供了有效解决方案。

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [51] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: 本研究评估了时间序列基础模型(TimesFM)在美国人口预测中的表现，相比传统方法在86.67%的测试案例中取得了最低的MSE，特别是在历史数据稀疏的少数族裔群体预测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 人口结构变化受全球化、经济状况、地缘政治事件和环境因素影响，给政策制定者带来重大挑战。准确的人口预测对于城市规划、医疗保健和经济政策等领域的决策至关重要。

Method: 使用美国人口普查局和FRED的数据集，将TimesFM与LSTM、ARIMA和线性回归等传统基线方法进行比较，在六个不同人口特征的州进行实验评估。

Result: TimesFM在86.67%的测试案例中实现了最低的均方误差，在历史数据稀疏的少数族裔群体预测方面表现尤为突出。

Conclusion: 预训练的基础模型有潜力增强人口分析能力，无需大量任务特定微调即可为主动政策干预提供信息支持。

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [52] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: 基于多源数据的基地规划布局指标（SPLI）系统，通过五大维度系统化量化城市空间布局，提高功能分类准确性和自动化分析能力


<details>
  <summary>Details</summary>
Motivation: 传统基地规划依赖经验判断和单一数据源，导致多功能布局系统化量化不足，需要数据驱动的结构化分析框架

Method: 整合OSM、POI、建筑形态、土地利用和卫星影像等多源数据，构建五大维度指标系统：层级功能分类、空间组织、功能多样性、基础服务可达性、土地利用强度，使用RGNN和GNN深度学习处理数据缺失

Result: 实验结果显示SPLI系统显著提高了功能分类的准确性，为自动化的数据驱动城市空间分析提供了标准化基础

Conclusion: SPLI系统成功将经验性知识与多源异构数据相结合，构建了结构化的城市空间信息生成框架，为城市规划提供了更系统、量化的分析工具

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [53] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: 该论文提出了一个识别多元霍克斯过程中潜在子过程和因果影响的方法，通过离散时间模型表示连续时间事件序列，建立了可识别性的充要条件，并开发了两阶段迭代算法来恢复因果结构。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统通常只有部分被观测，存在潜在子过程对现有方法构成重大挑战，现有方法主要关注已观测子过程的因果结构发现。

Method: 将连续时间事件序列表示为离散时间模型，建立潜在子过程和因果影响可识别性的充要条件，提出两阶段迭代算法：交替推断已发现子过程间的因果关系和发现新的潜在子过程。

Result: 在合成和真实数据集上的实验表明，该方法在存在潜在子过程的情况下能有效恢复因果结构。

Conclusion: 该方法为存在潜在子过程的多元霍克斯过程提供了有效的因果结构识别解决方案，通过离散时间建模和路径条件保证了可识别性。

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [54] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: 这篇论文提出了一种受脑细胞机制启发的新题径物联合框架BRIEF，通过改进神经网络连接搜索策略和Transformer融合模块，在精神分裂症和孤独症识别中实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在fMRI分类中存在网络架构确定依赖经验、特征融合方式简单（主要为连接）缺乏相互学习机制等问题。

Method: 提出BRIEF框架：1）提取4种fMRI时间表征（时间序列、静态/动态功能连接、多尺度分散熵）构建编码器；2）使用改进Q学习动态优化神经网络连接搜索（NCS）；3）通过Transformer融合所有特征向量，利用稳定/时变连接和多尺度依赖关系进行最终分类。

Result: 在精神分裂症（SZ，n=1100）和孤独症识别（ASD，n=1550）任务中，与21个最新模型相比，BRIEF实现了2.2%到12.1%的显著提升，SZ达到91.5%±0.6%的AUC，ASD达到78.4%±0.5%的AUC。

Conclusion: 这是首次尝试将受脑启发的强化学习策略用于fMRI基础的精神障碍分类，显示了在识别精确神经影像生物标记方面的重要潜力。

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [55] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 利用AlphaEarth Foundations全球地球科表示扩展地理标签数据集的地理覆盖范围，通过基础模型在美加植被分类任务中实现了超过73%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 高质量地理标签数据集通常受限于特定地理区域，无法覆盖全球范围，影响地球科研究的深度和广度

Method: 利用Google DeepMind的AlphaEarth Foundations(AEF)全球地理表示，通过随机森林和逻辑回归等基础模型扩展标签数据集的地理范围

Result: 在LANDFIRE植被类型数据扩展到加拿大的案例中，EvtPhys(13类)和EvtGp(80类)两个粒度级别上分别获得81%和73%的分类准确率，预测结果与真实数据一致

Conclusion: AEF全球表示能够有效支持地理标签数据的跨区域扩展，即使使用简单模型也能获得良好效果，为全球地理数据分析提供新的可能性

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [56] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: Fed-Meta-Align是一个四阶段联邦学习框架，通过序列化元初始化和双标准聚合机制，在非IID物联网设备数据上实现91.27%的平均故障分类准确率，比现有方法提升3%以上。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限物联网设备在非IID数据环境下实时故障分类的挑战，传统联邦学习在异构环境中容易导致模型发散的问题。

Method: 四阶段框架：1）在公共数据集上训练基础模型；2）序列化元初始化阶段学习异构感知初始化；3）并行联邦学习阶段使用基于本地性能和余弦相似度的双标准聚合；4）设备端个性化阶段适配专用专家模型。

Result: 在异构物联网设备上达到91.27%的平均测试准确率，比个性化FedAvg和FedProx分别提升3.87%和3.37%（电气和机械故障数据集）。

Conclusion: 通过序列化初始化和自适应聚合的多阶段方法，为多样化TinyML网络部署高性能智能提供了稳健路径。

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [57] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文研究了强化学习方法在具有随机结果的领域（如科学实验）中的有效性，发现GRPO方法会导致过度自信的概率预测，而PPO和RLOO能产生良好校准的模型。


<details>
  <summary>Details</summary>
Motivation: 探索当前强化学习方法在具有随机结果的可验证领域（如科学实验）中优化语言模型的有效性，超越确定性数学领域的应用。

Method: 通过合成数据和真实生物实验应用，比较Group Relative Policy Optimization (GRPO)、Proximal Policy Optimization (PPO)和REINFORCE Leave-One-Out (RLOO)等方法在随机结果领域的表现。

Result: GRPO会导致二元随机结果的过度自信概率预测，而PPO和RLOO产生良好校准的模型。移除GRPO中的组标准化可以修复其校准问题。

Conclusion: 研究结果反对在GRPO中使用标准标准化，为强化学习在超越确定性领域的推理语言模型应用铺平了道路。

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [58] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: 提出一种基于谱滤波的算法，用于学习具有有限个边缘稳定模式的非线性动力系统，通过在线凸优化技术实现预测误差的消失。


<details>
  <summary>Details</summary>
Motivation: 解决学习未知非线性动力系统的基本问题，特别是针对具有边缘稳定模式的系统，这些系统在控制理论中具有重要意义但学习难度较大。

Method: 基于谱表示技术，开发新的谱滤波算法，该算法能够处理线性动力系统的不对称动态和噪声校正，并将其扩展到非线性系统。

Result: 证明了对于任何具有有限边缘稳定模式的非线性动力系统，该算法能够实现预测误差的消失，学习速率由新的可学习性量化控制理论概念决定。

Conclusion: 该方法显著推广了原始谱滤波算法，能够处理更一般的噪声和边缘稳定系统，为非线性动力系统学习提供了有效的解决方案。

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [59] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen是一个基于大语言模型的公平性感知表格数据生成框架，通过整合反事实和因果公平性定义，在保持数据效用的同时显著提升公平性指标，仅需20%原始数据即可实现优异性能


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感和数据稀缺的环境中生成合成表格数据时，需要同时提高反事实和因果公平性，同时保持高数据效用，现有方法在这方面存在不足

Method: 使用基于大语言模型的框架，整合多种公平性定义到生成和评估流程中，采用上下文学习、提示优化和公平性感知数据筛选来平衡公平性和效用

Result: 在多个数据集上超越最先进的GAN和LLM方法，公平性指标（如人口统计均等和路径特定因果效应）提升高达10%，同时保持统计效用，仅需不到20%的原始数据

Conclusion: 该方法提供了一个原则性和实用性的途径，用于生成公平且有用的合成表格数据，在低数据环境下表现出高效性

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [60] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 通过提出局部后验冲突概念和潜在重建损失，解决VAE后验冲突问题，无需网络结构限制


<details>
  <summary>Details</summary>
Motivation: VAE模型存在后验冲突问题，导致生成样本多样性不足，现有方法需要网络结构限制

Method: 定义局部后验冲突概念，提出Latent Reconstruction损失函数，利用单射和复合函数的数学性质控制后验冲突

Result: 在MNIST、fashionMNIST、Omniglot、CelebA、FFHQ等多个数据集上验证了方法的有效性

Conclusion: 新方法能够有效控制VAE后验冲突，且不需要特定网络结构限制，提高了方法的通用性

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [61] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: 使用ReLU和三角函数等快速计算函数替代传统多项式函数作为KANs的基础组件，在保持竞争力的同时提升计算效率和训练速度


<details>
  <summary>Details</summary>
Motivation: 传统的KANs使用B样条和RBF等多项式函数，但这些函数在GPU设备上支持不足且不够流行，需要寻找更高效的计算函数

Method: 提出在Kolmogorov-Arnold网络中使用ReLU、sin、cos、arctan等快速计算函数作为基础组件，将这些函数组合集成到网络结构中

Result: 实验结果表明这些函数组合在保持竞争性能的同时，在训练时间和泛化能力方面具有潜在改进

Conclusion: 使用快速计算函数作为KANs的基础组件是一种有效的替代方案，能够提升计算效率而不牺牲性能

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [62] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: 通过约化粒子轨迹时间序列学习粒子系统的多标度动力学模型，采用计量括号形式保证热力学定律和激光散射平衡，并提出自监督学习策略识别出现结构变量


<details>
  <summary>Details</summary>
Motivation: 多标度系统模拟面临短时空尺度与出现体物理联系的挑战，约化粒子动力学会导致信息损失和出现的散射性、历史依赖性、随机性软体物理

Method: 提出使用计量括号形式的框架，通过粒子离散化实现，保证热力学第一、第二定律、动量守息和激光散射平衡，并提出自监督学习策略识别出现结构变量

Result: 在标准系统上验证方法，成功应用于星形聚合物具有挑战性的约化水平保持非平衡统计，以及从高速摄像的胶体悬浮体学习措合局部重排事件与出现随机动力学的模型

Conclusion: 提供了一种能够机器学习保持热力学定律和激光散射平衡的粒子系统约化动力学框架，并开源了PyTorch和LAMMPS实现，支持大规模推理和扩展到多样化粒子系统

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [63] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: 提出PCA-Grad-CAM和SVM-Grad-CAM方法，解决传统Grad-CAM无法直接应用于CNN中PCA和SVM层的问题，通过求解闭式雅可比矩阵实现关注区域可视化。


<details>
  <summary>Details</summary>
Motivation: 当训练样本有限时，CNN中加入PCA层和/或SVM分类器可提升性能，但传统Grad-CAM无法直接应用于这些层，需要开发新的可视化方法来支持白盒方法的发展。

Method: 通过求解从最后卷积层到PCA和/或SVM层的闭式雅可比矩阵（偏导数矩阵），提出PCA-Grad-CAM和SVM-Grad-CAM方法来可视化这些层中的关注区域。

Result: 在多个主要数据集上展示了可视化结果，验证了方法的有效性。

Conclusion: 该研究成功开发了能够可视化CNN中PCA和SVM层关注区域的新方法，为有限训练数据情况下的白盒分析提供了技术支持。

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [64] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: 提出约束质心聚类(CCC)方法，通过限制簇中心到最远点的最大距离来扩展传统质心聚类，使用拉格朗日公式得到闭式解，在合成环形数据上验证了其优于K-means和GMM的性能。


<details>
  <summary>Details</summary>
Motivation: 传统质心聚类方法缺乏对簇分布的明确控制，需要一种能够约束簇内最大距离同时保持可解释性的聚类方法。

Method: 基于拉格朗日公式推导闭式解，在质心聚类基础上添加最大距离约束，控制簇的径向扩散。

Result: 在合成环形数据上，CCC通过减少径向扩散同时保持角度结构，实现了更紧凑的聚类，在环向熵、扇区熵和联合熵指标上优于K-means和GMM。

Conclusion: CCC方法适用于需要结构化聚类和扩散控制的应用场景，如传感器网络、协作机器人和可解释模式分析。

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [65] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 提出了Efficient N-dimensional Attention (ENA)架构，结合线性循环和高阶滑动窗口注意力，用于高效建模超长高维数据。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在处理长序列高维数据时效率不足，需要更高效的架构来扩展线性循环模型到高维数据。

Method: 研究扫描策略和注意力混合架构，发现注意力混合模型效果更好。采用平铺高阶滑动窗口注意力(SWA)与线性循环结合，形成ENA架构。

Result: 实验证明ENA在理论和实践中都高效，能够有效建模超长高维数据。

Conclusion: 线性循环压缩全局信息，SWA补充局部建模，两者结合为超长高维数据建模提供了有前景的实用解决方案。

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [66] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: 基于随机化PCA森的新题无监督异常检测方法，在多个数据集上表现超过传统和最新方法，具有高泛化能力和计算效率


<details>
  <summary>Details</summary>
Motivation: 受随机化PCA森在近似K近邻搜索中表现的启发，开发一种新的无监督异常检测方法

Method: 利用随机化PCA森（RPCA Forest）进行异常检测

Result: 在多个数据集上表现超过传统和最新方法，其他数据集上也具有竞争力

Conclusion: 该方法具有高泛化能力和计算效率，是无监督异常检测的良好选择

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [67] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 通过解耦多尺度时空特征的SDSTM框架，提升了长期交通排放预测的准确性，避免传统方法的级联错误放大问题


<details>
  <summary>Details</summary>
Motivation: 传统时空图模型在长期交通排放预测中存在多尺度时空特征缠络和级联错误放大的问题，影响预测准确性

Method: 提出尺度解耦时空建模(SDSTM)框架：1）基于Koopman提升算子的双流特征分解策略；2）通过门控小波分解划定预测性边界；3）构建包含双流独立约束的融合机制

Result: 在西安二环路路级交通排放数据集上进行的实验表明，该模型达到了最先进的性能

Conclusion: SDSTM框架通过有效解耦多尺度时空特征并使其保持独立但互补，显著提升了长期交通排放预测的准确性和稳定性

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [68] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge是一个统一的统计框架，通过线性变换建模人类与LLM评估之间的系统性差异，提高LLM评分与人类判断的一致性


<details>
  <summary>Details</summary>
Motivation: LLM作为评估者时，其评分与人类判断存在系统性偏差，需要建立统计框架来弥合这种差距

Method: 提出Bridge框架，假设每个提示-响应对存在潜在人类偏好分数，将LLM偏差建模为协变量的线性变换，并提供高效的拟合算法

Result: 在6个LLM评估者和两个基准测试上，Bridge在准确性、校准和KL散度方面与人类评分达成更高一致性，并揭示了系统性的人机差距

Conclusion: Bridge提供了一个简单而有原则的框架来改进LLM评分并量化人类与LLM评估之间的系统性差异

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [69] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [70] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: 提出FAML方法解决多视图证据学习中的偏见问题，通过自适应先验、公平性约束和意见对齐机制实现更平衡的证据分配，提升预测性能和不确定性估计可靠性


<details>
  <summary>Details</summary>
Motivation: 现有多视图证据学习方法假设视图特定证据学习是可靠的，但实践中证据学习过程存在偏见，样本倾向于为数据丰富的类别分配更多证据，导致不确定性估计不可靠

Method: FAML方法包含三个核心组件：1）基于训练轨迹的自适应先验作为正则化策略校准偏见证据学习；2）基于类间证据方差的公平性约束促进平衡证据分配；3）多视图融合阶段的意见对齐机制缓解视图间偏见

Result: 在五个真实世界多视图数据集上的广泛实验表明，FAML实现了更平衡的证据分配，在预测性能和不确定性估计可靠性方面均优于最先进方法

Conclusion: FAML有效解决了多视图证据学习中的偏见问题，通过系统性的偏见校准机制显著提升了方法的公平性和可靠性

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [71] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD是一个基于元学习的多模态OOD检测器选择框架，能够自动为不同分布偏移推荐合适的检测器，在12个测试场景中优于10个基线方法


<details>
  <summary>Details</summary>
Motivation: 解决多模态环境下OOD检测器选择难题，由于OOD检测的无监督特性，难以预测模型性能且系统比较成本高昂

Method: 结合多模态嵌入和手工设计的元特征来表示数据集，利用历史性能数据通过元学习推荐适合新分布偏移的检测器

Result: 在12个测试场景中一致优于10个竞争基线方法，计算开销最小

Conclusion: M3OOD框架为多模态OOD检测器选择提供了有效的自动化解决方案，显著提升了检测性能

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [72] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种完美真实的批处理设定中的检查测量方法（ATB），解决了现有检查测量在有限样本上不真实的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的检查测量在有限样本上存在不真实性，预测器会为了在样本上显得更检查而请谋。虽然有些约真实检查测量，但在批处理设定中仍缺乏完美真实的检查测量。

Method: 设计了平均双箱检查误差（ATB）测量方法，该方法不仅真实，还具有声音性、完整性、连续性等特性。ATB与现有的光滑检查误差和距离检查测量存在二次关系。

Result: ATB在计算效率和实现简单性方面都更优于现有方法，能够为检查测试问题提供更快的运行时间和更简单的实现。

Conclusion: 该研究成功构建了第一个完美真实的检查测量ATB，解决了检查测量预测器在有限样本上请谋的问题。还提供了构建真实测量的通用方法，可以生成其他真实检查测量。

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [73] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: 提出了一种基于直通估计器(STE)框架的噪声感知训练方法，用于解决模拟存内计算(CIM)架构中的硬件噪声问题，通过解耦前向噪声模拟和后向梯度计算，实现了更准确且计算可行的噪声建模。


<details>
  <summary>Details</summary>
Motivation: 模拟存内计算架构虽然能显著提升神经网络推理的能效，但面临复杂的硬件噪声挑战。现有的噪声感知训练方法依赖理想化且可微分的噪声模型，无法完全捕捉模拟CIM硬件的真实变化复杂性。

Method: 借鉴量化中的直通估计器(STE)框架，将前向噪声模拟与后向梯度计算解耦，使得可以使用更准确但计算上难以处理的噪声模型进行噪声感知训练，同时保持计算可行性和优化稳定性。

Result: 实验结果显示，该方法在图像分类任务上实现了5.3%的准确率提升，文本生成任务上困惑度降低0.72，训练时间加速2.2倍，峰值内存使用降低37.9%，相比标准噪声感知训练方法有显著改进。

Conclusion: 该扩展STE框架为模拟CIM系统的噪声感知训练提供了一种有效解决方案，能够在保持计算效率的同时处理更复杂的硬件噪声模型，显著提升了神经网络在模拟CIM硬件上的性能表现。

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [74] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: 该研究提出了CFF方法，结合反事实和事实解释来为标记时间点过程模型提供最小且合理的解释子集，解决了直接使用单一解释方法导致不合理结果的问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络标记时间点过程模型在高风险应用中广泛使用，但其输出的可信度存在担忧。需要找到历史事件的最小子集作为合理解释，使基于该子集的预测准确性与完整历史相当且优于补集。

Method: 提出CFF方法，将MTPP解释定义为反事实解释和事实解释的组合，采用一系列精心设计的技术来解决解释问题。

Result: 实验证明CFF在解释质量和处理效率方面优于基线方法，具有正确性和优越性。

Conclusion: 结合反事实和事实解释的CFF方法能够为MTPP模型提供更合理和有效的解释，解决了单一解释方法的局限性。

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [75] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: 基于Set-Valued Transformer网络的高排放汽车识别方法，通过时序相似性测量和集值识别算法，有效提升了高排放汽车检测准确率


<details>
  <summary>Details</summary>
Motivation: 解决实际监测数据中高排放状态数据比例迅少的长尾分布问题，以及汽车排放状态的高非线性特性和知识缺失导致的识别困难

Method: 提出Set-Valued Transformer网络(SVTN)，先用transformer测量微行程条件变化的时序相似性，将高维排放数据投射到低维特征空间，然后用集值识别算法对特征向量与标签关系进行概率建模

Result: 在合肥2020年柴油车监测数据上验证，方法比transformer基线降低了9.5%的高排放汽车漏检率

Conclusion: SVTN方法能够有效学习高排放样本的区分性特征，显著提升了移动污染源识别的准确性，为城市污染监管提供了有效技术支撑

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [76] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: 基于超位原理的偏差相加方法，可以通过简单的矩阵相加将独立训练的LoRA模块组合起来，无需额外训练就能达到类似合并数据训练的效果


<details>
  <summary>Details</summary>
Motivation: 利用超位原理，假设在不相交域上独立训练的LoRA模块是约近正交的，可以通过简单相加来组合

Method: 使用GPT-2 Small模型和LoRA技术（rank 4, alpha=64），在数学、医学、金融三个QA域训练适配器，然后通过直接相加矩阵来组合不同域的模块

Result: 数学+医学组合相对合并数据训练改善语言模型的困惑度-9.10%，而其他组合效果有所不同，且LoRA偏差的余弦相似性与困惑度变化呈正相关

Conclusion: 简单的矩阵相加方法无需额外训练、可在秒级完成，能达到类似合并数据训练的性能，同时能够呈现高阶组合中的干扰现象

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [77] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: 基于超高维计算(HDC)的无监督联邦学习框架FedUHD，解决了非IID数据分布、计算通信成本高和通信噪声问题，实现了显著的速度提升、能消耗优化和精度提高。


<details>
  <summary>Details</summary>
Motivation: 无监督联邦学习(UFL)在实际应用中遇到三大挑战：非IID数据分布、计算通信成本高、通信噪声故障。传统深度网络方案存在过重的计算和通信开销。

Method: 提出基于超高维计算(HDC)的FedUHD框架：客户端采用kNN聚类超向量删除法消除异常值；服务器端采用权重HDC聚合技术平衡非IID数据分布。

Result: 实验结果显示：训练速度提升173.6倍，能效提升612.7倍，通信成本降住271倍，平均精度提高15.50%，并且具有更好的噪声耐受性。

Conclusion: FedUHD作为首个基于HDC的UFL框架，通过轻量级计算和噪声耐受性设计，有效解决了无监督联邦学习的关键挑战，为实际部署提供了高效、节能的解决方案。

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [78] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: 联邦学习中的性能公平性问题研究，提出FairGrad方法在异质性数据环境下同时提升公平性和整体模型性能


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据异质性导致公平性问题，现有公平方法在异质数据环境下效果不明确且方法间关联性不清

Method: 重点研究显式正则化客户损失的公平方法，提出FairGrad(近似)和FairGrad*(精确)两种梯度方差正则化方法，并理论分析各方法间联系

Result: FairGrad和FairGrad*在异质性数据环境下同时提升了公平性和整体模型性能，并证明了各种公平方法之间的理论联系

Conclusion: 通过梯度方差正则化的FairGrad方法能有效解决联邦学习中的性能公平性问题，为异质性数据环境下的公平FL提供了有效方案

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [79] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN是一个动态层聚合框架，通过输入依赖的权重调整和专门化的探测头，为每个输入自适应地选择最佳特征层组合，在语音识别和情感识别任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的自监督语音模型层聚合方法（如使用最后一层或加权求和）存在信息瓶颈问题，对所有数据样本采用静态特征权重，无法根据输入内容动态调整。

Method: 使用层专门化的探测头和输入依赖的权重机制，动态地为每个输入样本调整不同层的特征权重，优先选择对当前输入最有用的特征层。

Result: 在自动语音识别和语音情感识别任务上的评估显示，VARAN相比传统方法具有显著优势，特别是在使用LoRA微调技术时效果更佳。

Conclusion: VARAN框架解决了保留层特定信息与实现灵活特征利用之间的权衡问题，推进了自监督语音表示的高效适应。

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [80] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的资源分配算法LPDRL-F，用于优化集成感知通信的AIGC网络中的感知-生成-通信三维资源分配，提高AIGC服务质量。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC服务假设输入数据准确，只关注生成质量，但在ISAC-AIGC网络中感知数据不准确且AIGC模型自身存在错误，需要考虑内容准确性和质量的综合评估。

Method: 提出CAQA评估指标，并设计LP导向的深度强化学习算法LPDRL-F，通过LP导向和动作筛选器将三维解空间降为二维，降低复杂度。

Result: 模拟显示LPDRL-F比现有算法收敛速度提高60%以上，AvgCAQA提高14%以上，与仅关注CGQ的方案相比AvgCAQA提高50%以上。

Conclusion: LPDRL-F算法能够高效解决ISAC-AIGC网络中的资源分配问题，显著提升AIGC服务质量，为集成感知通信的AIGC网络提供了有效的资源优化方案。

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [81] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET是基于160亿次医疗事件训练的医疗基础模型，通过自回归生成模拟患者健康时间线，在78个医疗任务中无需微调即可达到或超越专用监督模型性能。


<details>
  <summary>Details</summary>
Motivation: 实现规模化个性化医疗需要从纵向患者旅程中提取洞察，大规模医疗事件预训练的基础模型有望扩展真实世界证据生成并泛化到多样化下游任务。

Method: 使用Epic Cosmos数据集（163亿次就诊、3亿患者记录），训练解码器Transformer模型CoMET，进行医疗事件自回归生成，建立计算最优的缩放定律模型。

Result: 在诊断预测、疾病预后和医疗运营等78个任务中，CoMET无需任务特定微调或few-shot示例，普遍优于或匹配专用监督模型，预测能力随模型规模和预训练规模持续提升。

Conclusion: CoMET生成式医疗事件基础模型能有效捕捉复杂临床动态，为支持临床决策、简化医疗运营和改善患者结局提供可扩展和可泛化的框架。

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [82] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT是一种动态自动化的指令调优数据集混合优化方法，使用多臂老虎机框架和先验缩放玻尔兹曼探索，在Tulu-v2数据集集合上实现了2.2%的性能提升


<details>
  <summary>Details</summary>
Motivation: 随着后训练阶段大量指令调优数据集的出现，如何动态平衡和优化这些数据集的混合比例成为一个关键挑战

Method: 将问题建模为多臂老虎机，提出先验缩放玻尔兹曼探索方法，使用轻量级1步前瞻奖励来更新采样概率，保持原始数据集比例的软锚定

Result: 在包含16个指令调优数据集的Tulu-v2混合集合上，DynamixSFT在10个基准测试中实现了最高2.2%的性能提升

Conclusion: 该方法能够有效优化指令调优数据集的混合比例，同时保持数据集的多样性和覆盖范围，为自适应动态优化提供了深入见解

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [83] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 门控机制在RNN中通过耦合状态空间时间尺度与参数空间动力学，隐式地产生自适应学习率行为，即使使用固定全局学习率训练时也是如此


<details>
  <summary>Details</summary>
Motivation: 研究门控RNN中门控机制如何影响梯度传播和参数更新，揭示门控不仅控制隐藏状态记忆保留，还作为数据驱动的预处理器调节优化轨迹

Method: 通过推导泄漏积分器和门控RNN的精确雅可比矩阵，获得一阶展开式，分析标量和多维门控如何重塑梯度传播、调节有效步长并引入参数更新各向异性

Result: 门控机制自然地产生了学习率调度、动量和自适应方法（如Adam）等优化行为，数值实验验证了微扰分析的有效性

Conclusion: 这项工作提供了统一的动力学系统视角，解释了门控如何耦合状态演化与参数更新，说明了门控架构在实践中实现鲁棒可训练性和稳定性的原因

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [84] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE是一种基于微分熵的不确定性感知变分自编码器，用于改进参数化和可逆投影，在处理分布外样本时表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自编码器方法在处理数据空间或嵌入空间的分布外样本时表现不佳，需要一种能够处理不确定性的参数化和可逆投影方法。

Method: 使用微分熵的变分自编码器（DE-VAE），在固定投影下学习到2D空间的映射及其逆映射回原始空间。

Result: 在四个知名数据集上的定量和定性评估表明，DE-VAE能够创建与其他当前AE方法精度相当的参数化和逆投影，同时支持嵌入不确定性分析。

Conclusion: DE-VAE成功解决了现有方法在处理分布外样本时的局限性，提供了既准确又具有不确定性分析能力的参数化可逆投影解决方案。

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [85] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的深度学习模型AICRN，通过结合注意力机制和卷积残差网络来提高心电图参数回归的精确度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统心电图分析中的人为误差、关注点失、手动分析苦难等挑战，通过AI技术提高心脏疾病诊断的精确性和预测能力。

Method: 设计了注意力集成卷积残差网络(AICRN)，包含空间和通道注意力机制，专门处理心电图特征的类型和空间位置。使用卷积残差网络解决核心问题。

Result: AICRN模型在心电图参数回归任务中表现超过现有模型，具有更高的精确度。能够快速识别心脏事件，减少手动分析的工作量。

Conclusion: 深度学习在心电图分析的可解释性和精确性方面发挥关键作用，为心脏监测和管理开启了新的临床应用前景。

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [86] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC是一个轻量级的两阶段压缩框架，通过联合嵌入压缩和自压缩模块，将蛋白质输入长度减少一半，演示长度从751个token压缩到16个以下，在少样本设置下显著提升ProtTeX模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决ProtTeX模型的两个主要限制：1）序列和结构token连接导致蛋白质长度翻倍并破坏模态间对齐；2）受限于训练语料和上下文窗口，无法进行上下文学习，限制了泛化能力。

Method: 两阶段压缩框架：1）联合嵌入压缩机制在残基级别融合序列和结构表示；2）自压缩模块将完整演示聚合到最后几个语言token的潜在空间中。采用PEFT微调和单投影层增加少量参数。

Result: 在16-shot设置下实现约93.68%的总提示长度压缩比。蛋白质功能预测实验中，域内基准性能提升2%，域外数据集性能提升11%。

Conclusion: ProtTeX-CC在不修改主干模型的情况下，通过轻量级压缩框架有效解决了ProtTeX的局限性，显著提升了少样本学习能力和跨域泛化性能。

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [87] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: 该论文提出了一种针对大语言模型遗忘学习（unlearning）的系统化方法，通过记录训练过程的确定性日志，实现精确的数据删除和模型状态恢复。


<details>
  <summary>Details</summary>
Motivation: 研究GDPR第17条规定的被遗忘权在大语言模型中的应用，将遗忘学习作为一个可复现的系统问题来处理，确保模型能够精确删除特定数据而不影响其他数据。

Method: 将训练视为确定性程序，记录每个微批次的日志（包括ID哈希、RNG种子、学习率值、优化器步数计数器和累积边界）。在固定堆栈和确定性内核下，重放训练尾部同时过滤遗忘闭包，实现与在保留集上训练相同的参数。还补充了三种路径：精确回滚最近步骤、队列范围适配器删除、以及曲率引导的反向更新加短期保留调优。

Result: 在满足前提条件的受控运行中，证明了模型和优化器状态的字节级完全相同性，并报告了存储/延迟预算和验证机制的玩具工件。

Conclusion: 该方法为大规模语言模型实现精确的遗忘学习提供了可行的系统化解决方案，能够满足GDPR被遗忘权的技术要求，同时兼顾延迟和可用性约束。

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [88] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: 提出了一种基于一致性模型的分布匹配新方法，结合了连续归一化流的简单目标函数和GAN的约束适应性优势


<details>
  <summary>Details</summary>
Motivation: 生成模型在分布匹配任务中发挥重要作用，但GAN存在训练困难和模式崩溃问题，需要更稳定高效的替代方案

Method: 受连续归一化流中一致性模型启发，设计新的分布匹配方法，具有简单的范数最小化目标，同时保持类似GAN的约束适应性

Result: 通过理论验证和合成/真实数据集实验证明了方法的有效性

Conclusion: 该方法成功结合了CNF的简单目标优势和GAN的约束灵活性，为分布匹配提供了新的解决方案

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [89] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: 在异步ADMM中引入粗粗量化来减少通信开销，适用于大规模聚合学习和分布式优化


<details>
  <summary>Details</summary>
Motivation: 分布式优化和聚合学习中，异步ADMM虽然有许多优点，但通信成本很容易成为瓶颈，特别是当节点通信预算有限或数据量极大时

Method: 在异步ADMM中对交换数据进行粗粗量化，以降低通信开销

Result: 通过实验验证了方法在多个分布式学习任务上的收敛性，包括神经网络

Conclusion: 粗量化ADMM能够有效减少通信成本同时保持算法的收敛性，适用于大规模聚合学习应用

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [90] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: 基于预训语言模型的时间序列预测方法CC-Time，通过跨模态和跨模态学习结合时间序序列和文本描述，实现了更准确的预测效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于预训语言模型的时间序列预测方法无法实现满意的预测准确性，尽管语言模型具有强大的序列建模能力。

Method: CC-Time方法从两个方面探索PLMs在时间序列预测中的潜力：1）通过跨模态学习建模时间序序列的时间依赖性和通道相关性；2）通过跨模型融合块自适应地整合PLMs和时间序列模型的知识。

Result: 在9个真实世界数据集上的实验表明，CC-Time在全数据训练和少样本学习情况下都达到了最先进的预测准确性。

Conclusion: CC-Time通过跨模态和跨模型学习技术，充分发挥了预训语言模型在时间序列预测中的潜力，实现了更全面的时间序列建模。

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [91] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: DHG-Bench是第一个全面的深度超图学习基准，集成了20个数据集和16种最先进的超图神经网络算法，在统一的数据处理和实验协议下系统评估算法在效果、效率、鲁棒性和公平性四个维度的表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度图模型专注于成对关系，无法有效学习现实复杂系统中普遍存在的高阶交互，而现有的超图神经网络方法缺乏全面的基准测试，存在数据集覆盖不足、评估维度狭窄、实验设置不一致等问题。

Method: 构建DHG-Bench基准，整合20个多样化数据集（涵盖节点、边和图级任务）和16种state-of-the-art HNN算法，采用一致的数据处理和实验协议，从效果、效率、鲁棒性和公平性四个维度系统评估算法性能。

Result: 通过DHG-Bench进行的大量实验揭示了现有算法的优势和固有局限性，为未来研究提供了有价值的见解和方向。

Conclusion: DHG-Bench填补了深度超图学习领域缺乏全面基准的空白，提供了可复现的研究基础，有助于推动该领域的发展。

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [92] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: 提出了STM2和STM3两种模型，通过多尺度Mamba架构和自适应图因果卷积网络，有效解决长时空依赖学习中的多尺度信息提取和时空相关性建模问题，在长时空时间序列预测中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以有效学习复杂的长期时空依赖关系，主要面临两个挑战：1）长期时间序列天然包含难以高效提取的多尺度信息；2）不同节点的多尺度时间信息高度相关且难以建模。

Method: STM2采用多尺度Mamba架构高效捕获多尺度信息，结合自适应图因果卷积网络学习复杂多尺度时空依赖；STM3进一步使用混合专家架构，包含更稳定的路由策略和因果对比学习策略来增强尺度区分性。

Result: 在真实世界基准测试上的广泛实验表明，STM2/STM3在长时空时间序列预测中取得了最先进的性能表现。

Conclusion: 提出的STM2和STM3模型通过创新的多尺度Mamba架构和混合专家设计，成功解决了长时空依赖学习的关键挑战，为时空时间序列预测提供了有效的解决方案。

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [93] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，使用LIME和SHAP来解释时间序列预测，结合了ARIMA的可解释性和XGBoost的高精度，通过无泄漏监督学习转换时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 传统ARIMA模型具有可解释性但难以处理非线性，而树模型如XGBoost精度高但缺乏透明度。需要一种能够兼顾准确性和可解释性的时间序列预测解释方法。

Method: 将单变量时间序列转换为无泄漏的监督学习问题，训练梯度提升树和ARIMA基线模型，然后应用LIME和SHAP进行事后解释分析。

Result: 在航空乘客数据集上的研究表明，少量滞后特征（特别是12个月滞后）和季节性编码能够解释大部分预测方差。

Conclusion: 提出了将LIME和SHAP应用于时间序列的方法论，提供了理论基础、实证评估和实践指南，为时间序列预测提供了可解释的解决方案。

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [94] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出了一种新颖的学习型二阶优化器，通过可训练预处理单元增强经典SR1算法，在单目人体网格恢复任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习依赖大标注数据集且泛化性差，传统优化方法数据高效但收敛慢。学习型二阶优化方法尚未充分探索

Method: 引入可训练预处理单元生成数据驱动向量，构建正半定秩一矩阵，通过学习投影与割线约束对齐

Result: 在分析实验和单目人体网格恢复任务中优于现有学习型优化方法，具有轻量级、无需标注数据或微调的特点

Conclusion: 该方法泛化性强，适合集成到更广泛的基于优化的框架中，为学习型二阶优化提供了新思路

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [95] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC是一个用于图异常检测的框架，通过上下文重构和对比学习，在标签有限的情况下有效利用未标记数据，显著提升GNN在异常检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络训练需要大量标注数据，但在图异常检测中异常样本稀少、标注成本高且存在伪装模式，这严重限制了GAD的发展。

Method: 提出CRoC框架：1）利用GAD中的类别不平衡重构节点上下文，保持交互模式的同时重组节点属性；2）分别编码异质关系并整合到消息传递中；3）结合对比学习范式联合利用有限标注和大量未标注数据。

Result: 在7个真实世界GAD数据集上的实验表明，CRoC相比基线GNN提升高达14%的AUC，在有限标签设置下优于最先进的GAD方法。

Conclusion: CRoC通过上下文重构和对比学习的有效结合，解决了GAD中标签稀缺和异常伪装的问题，为图异常检测提供了有效的解决方案。

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [96] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文分析了Lion优化器的收敛性能，提出了标准版本、方差缩减版本、分布式版本以及通信高效的分布式变体，并给出了各自的收敛速率理论分析。


<details>
  <summary>Details</summary>
Motivation: Lion优化器作为一种新兴的优化算法，其理论收敛性质尚未得到充分研究。本文旨在填补这一空白，特别是在分布式设置下的收敛性能分析。

Method: 采用理论分析方法，首先建立标准Lion优化器的收敛速率，然后引入方差缩减技术改进收敛性能，接着扩展到分布式设置，最后提出通信高效的分布式变体并使用无偏符号压缩技术。

Result: 获得了多个收敛速率结果：标准Lion为O(d^{1/2}T^{-1/4})，方差缩减版本为O(d^{1/2}T^{-1/3})，分布式版本分别为O(d^{1/2}(nT)^{-1/4})和O(d^{1/2}(nT)^{-1/3})，通信高效变体为O(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})})和O(d^{1/4}/T^{1/4})。

Conclusion: Lion优化器具有良好的收敛性能，通过方差缩减和分布式技术可以进一步提升收敛速率，而通信高效的分布式变体在保持收敛性能的同时显著减少了通信开销。

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [97] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 通过Funnel Schedule和Adaptive Temperature策略解决扩散模型中的探索-利用困境，在不增加计算成本的情况下显著提升样本质量


<details>
  <summary>Details</summary>
Motivation: 将语言模型中成功的推理时缩放技术适配到扩散模型，解决SMC方法在扩散模型中遇到的早期样本评估困难和后期样本可逆性差的困境

Method: 提出Funnel Schedule（逐渐减少维持的粒子数量）和Adaptive Temperature（适应性调节早期奖励权重）两种策略，专门为扩散模型的生成动力学和相变行为而设计

Result: 在多个标准测试集和最新文本生成图像扩散模型上过了之前的基准方法，显著提升了样本质量

Conclusion: 通过从搜索算法角度出发的简单有效策略，成功解决了扩散模型中的探索-利用困境，为扩散模型的推理时缩放提供了有效方案

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [98] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: 提出了Bi-Axial Transformer (BAT)模型，通过同时关注临床变量和时间轴来处理电子健康记录数据，在脓毒症预测和死亡率分类任务上达到先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHRs)数据日益复杂，包含更大数据集、更长时序和多模态集成。传统Transformer在处理EHR分类时受限于数据表示方式，可能降低性能或无法捕捉信息性缺失。

Method: 开发了双向轴Transformer(BAT)模型，同时关注临床变量轴和时间点轴，学习更丰富的数据关系并解决数据稀疏性问题。

Result: BAT在脓毒症预测上达到最先进性能，在死亡率分类上与顶级方法竞争。相比其他Transformer，BAT对数据缺失更具鲁棒性，并能学习可用于迁移学习的独特传感器嵌入。

Conclusion: BAT模型有效解决了EHR数据处理的挑战，提供了更好的性能和对数据缺失的鲁棒性，同时提供了可复现的基准模型实现。

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [99] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: 一个集成机器学习框架，直接从2D工程图预测制造成本，减少人工过程规划需求，达到10%的平均绝对百分比误差


<details>
  <summary>Details</summary>
Motivation: 改变传统制造成本估算方式，免去劳动密集型的过程规划，缩短报价周期，提供一致和透明的成本评估

Method: 从13,684张汽车悬挂和转向部件DWG图纸中提取约200个几何和统计描述符，使用梯度提升决策树模型(XGBoost, CatBoost, LightGBM)进行训练

Result: 在24个产品组中达到近10%的平均绝对百分比误差，显示了超越部件特定经验法则的稳健扩展性

Conclusion: 该端到端的CAD到成本流水线缩短了报价周期，为产业4.0制造环境中实时、ERP集成的决策支持提供了可部署的途径

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [100] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: 提出了一种自适应均值漂移算法，通过局部距离分布估计聚类基数，动态调整带宽和核半径阈值，在变尺度数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统KDE方法只能提供聚类局部区域的洞察，无法处理具有变化局部尺度和聚类基数的数据集，需要一种能够自适应调整参数的方法

Method: 利用点到所有其他点的局部距离分布，通过识别距离分布密度中的局部最小值来估计局部聚类基数，基于基数估计计算整个聚类的局部参数，在均值漂移执行过程中自适应调整带宽和核半径阈值

Result: 该算法在原始数据集上优于最近提出的自适应均值漂移方法，在更广泛的聚类基准测试中表现出竞争力

Conclusion: 基于局部距离分布的自适应均值漂移算法能够有效处理变尺度数据集，通过动态参数调整实现了更好的聚类性能

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [101] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL是一个基于强化学习的缓存淘汰策略，通过Dueling DQN网络在NGINX中替代传统LRU，在严格微秒级预算下显著提升命中率


<details>
  <summary>Details</summary>
Motivation: 传统LRU缓存淘汰策略对对象大小不敏感，在周期性突发流量和混合对象大小场景下容易产生颠簸问题，需要更智能的淘汰机制

Method: 使用Dueling Deep Q-Network作为学习策略，通过ONNX侧车服务在500微秒超时内处理淘汰决策，提取年龄、大小、命中次数等6个轻量级特征，离线训练策略

Result: 在25MB缓存下命中率从0.1436提升至0.3538（146%提升），100MB下从0.7530提升至0.8675（15%提升），推理增加不到2%CPU开销，保持95%分位淘汰延迟在预算内

Conclusion: Cold-RL是首个集成到NGINX中具有严格SLO的强化学习淘汰策略，在小型缓存场景下表现优异，随着缓存增大与传统方法性能趋近

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [102] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: CSCR是一个轻量级框架，通过将提示和模型映射到共享嵌入空间，实现快速、成本敏感的模型选择，在多个基准测试中比基线方法提升25%的准确率-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略提示特定上下文、依赖昂贵的模型分析、假设固定专家集合或使用低效的试错策略，需要更高效的LLM路由方案。

Method: 使用紧凑的logit足迹（开源模型）和困惑度指纹（黑盒API），通过对比编码器训练在自适应成本带内选择最便宜准确的专家，推理时通过FAISS索引进行k-NN查找。

Result: 在多个基准测试中持续优于基线方法，准确率-成本权衡提升达25%，对未见过的LLM和分布外提示具有强泛化能力。

Conclusion: CSCR提供了高效的LLM路由解决方案，支持微秒级延迟，专家池变化时无需重新训练，实现了优秀的成本-性能平衡。

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [103] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: 提出基于信任区域的迭代优化方法，通过几何退火策略逐步逼近目标路径空间测度，显著提升随机最优控制问题的求解性能


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的优化方法在处理目标测度与先验分布差异较大的随机最优控制问题时面临挑战，需要更有效的优化策略

Method: 采用信任区域约束的迭代优化方法，通过几何退火策略从先验分布逐步逼近目标测度，系统性地选择退火路径的时间步长

Result: 在扩散采样、过渡路径采样和扩散模型微调等多个最优控制应用中，新方法显著提升了性能表现

Conclusion: 基于信任区域的几何退火方法为随机最优控制问题提供了一种原理性且有效的优化框架，能够处理目标与先验分布差异较大的挑战性场景

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [104] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: NeurIPS 2023 Neural MMO竞赛吸引了200多名参与者，参赛者训练的目标条件策略能够泛化到训练中未见过的任务、地图和对手。最佳解决方案在单张4090 GPU上训练8小时后，得分比基线高4倍。所有相关资源均已开源。


<details>
  <summary>Details</summary>
Motivation: 举办Neural MMO竞赛旨在推动目标条件策略在复杂多智能体环境中的泛化能力研究，探索策略在面对未知任务、地图和对手时的适应性表现。

Method: 参赛者使用目标条件策略训练方法，在Neural MMO环境中进行训练和评估。竞赛设置了训练中未见过的任务、地图和对手作为测试场景，以检验策略的泛化能力。

Result: 竞赛吸引了200多名参与者，最佳解决方案在单张4090 GPU上训练8小时后，得分达到基线水平的4倍，显示出优秀的性能和效率。

Conclusion: Neural MMO竞赛成功展示了目标条件策略在复杂环境中的强大泛化能力，开源所有资源将为社区提供宝贵的研究基础，推动多智能体强化学习的发展。

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [105] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: 通过系统化调优超参数和EMA动量技术，可以在微调语言模型时保持安全性而无需额外安全措施，将有害响应从16%降至5%


<details>
  <summary>Details</summary>
Motivation: 承成微调语言模型必然会损害其安全性的传统观点，需要额外安全措施，但研究者认为这是由于优化选择不当而非本质问题

Method: 系统化测试和选择关键训练超参数（学习率、批处理大小、梯度步长），提出指数移动平均（EMA）动量技术在参数空间中创建稳定优化路径

Result: 在Llama模型家族和多个数据集（Dolly、Alpaca、ORCA）上实验，将不安全响应从16%降至约5%，同时保持模型效能，超越了需要额外安全数据的现有方法

Conclusion: 微调过程中的安全问题可以大量避免而无需专门干预，提供了同时保持模型性能和安全性的实用指南

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [106] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: 这篇论文从数据中心AI角度系统研究了功能怠散共振成像(fMRI)构建脑图的设计空间，通过对比不同数据处理组合在下游分类任务上的表现，证明了数据层面的细微选择对怠散共振成像图学习性能的重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前构建脑图的流水线通常依赖固定模式，忽视了数据中心的关键选择，本文采用数据中心AI角度来系统研究这些选择对下游怠散共振成像图学习性能的影响。

Method: 将脑图构建设计空间组织为三个阶段：时序信号处理、拓扑提取和图特征化，研究了高振幅BOLD信号筛波、聚合性策略、替代相关指标以及多视图节点和边特征等技术组合。

Result: 在HCP1200和ABIDE数据集上的实验显示，细心的数据中心配置能够持续提高分类准确性，超过标准流水线。

Conclusion: 上游数据决策在脑图构建中具有关键作用，系统性探索数据中心设计空间对基于图的怠散共振成像学习至关重要。

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [107] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1是一个基于规则强化学习的Linux内核调优框架，通过将内核配置空间抽象为RL环境，利用LLM进行高效探索，实现了比启发式调优方法高达5.6%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的Linux内核调优方法在效率、可扩展性和泛化性方面存在挑战，需要一种更智能、高效的自动化调优解决方案。

Method: 提出OS-R1框架，将内核配置空间抽象为RL环境，设计定制奖励函数增强LLM的推理标准化和配置修改准确性，采用两阶段训练过程加速收敛。

Result: 实验结果显示OS-R1显著优于现有基线方法，性能提升最高达5.6%，保持高数据效率，并在各种实际应用中表现出良好的适应性。

Conclusion: OS-R1展示了在实际多样化环境中部署的潜力，为Linux内核自动化调优提供了有效的解决方案。

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [108] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: 提出了一种视觉分析系统，用于改善对LLM驱动编码代理迭代过程的审查和调整效果


<details>
  <summary>Details</summary>
Motivation: 当前手动检查代理编码输出的方式效率低下，难以跟踪代码迭代过程和评估改进机会

Method: 开发了一种视视觉分析系统，支持代码、过程和LLM三个层次的对比分析，重点关注AIDE框架

Result: 通过在Kaggle竞赛中的案例研究，证明系统能够提供有价值的迭代编码过程洞察

Conclusion: 该系统能够帮助ML科学家结构化理解代理行为，提高调试和提示工程效析

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [109] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: 结合滑动窗口和变分模态分解(VMD)的金融时间序列预测模型，通过VMD分解非平稳序列为平滑子分量，再用LSTM进行预测，相比原始序列表现更好更稳定


<details>
  <summary>Details</summary>
Motivation: 解决金融时间序列的复杂性，提高预测模型的适应性和准确性

Method: 使用滑动窗口和VMD方法分解历史股价和市场指标数据，将分解后的子分量输入深度学习模型进行预测

Result: VMD处理的序列训练的LSTM模型相比原始时间序列具有更好的预测效果和稳定性

Conclusion: VMD分解能有效处理金融时间序列的非平稳特性，提升深度学习模型的预测性能

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [110] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: 使用预训练的蛋白质大语言模型提取序列上下文特征，通过双向LSTM和GRU模型预测蛋白质和臉毒肽胀氧胆素沉积区域，达到了84.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的胆素沉积预测方法主要基于进化模体和氨基酸特性，序列信息特征显示出高预测性能，因此研究考虑利用大语言模型提取上下文特征来提升预测准确性。

Method: 从预训练的蛋白质大语言模型中提取序列上下文特征，使用双向LSTM和GRU模型进行胆素沉积区域预测。

Result: 在10折交叉验证中获得84.5%的准确率，在测试数据集上获得83%的准确率，显示出竞争性能。

Conclusion: 蛋白质大语言模型在提高胆素沉积预测准确性方面具有很大潜力，为生物信息学预测提供了新的方法。

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [111] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: 这篇论文分析了过参数化联邦学习(FedAvg)的收敛性，证明网络宽度增加时数据异质性影响逐渐减小，在无限宽度时消失，并且FedAvg可以达到与集中式学习相同的汇总性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的非IID特性导致全局模型在异质数据分布上的汇总性挑战，需要理论分析过参数化模型如何减少这种影响。

Method: 使用梯度下降法分析过参数化FedAvg的收敛性，理论证明网络宽度与数据异质性影响的关系，并在无限宽度情况下分析全局和本地模型的行为。

Result: 理论证明数据异质性影响随网络宽度增加而减小，在无限宽度时消失；在无限宽度时，FedAvg中的全局和本地模型都行为如线性模型，且FedAvg可以达到与集中式学习相同的汇总性能。

Conclusion: 通过增加神经网络的宽度，可以有效减少联邦学习中数据异质性对模型汇总性的负面影响，在过参数化情况下FedAvg能够实现与集中式学习相等的性能。

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [112] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于令牌级判断的混合语言模型推理方法，通过结合认知不确定性和关注重要性来选择性地上传信息权的令牌，在保持准确性的同时大幅节省能消和通信成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决资源受限环境下设备上LLM推理的需求，混合语言模型得到了关注。但现有研究多关注准确性和延迟，而忽视了通信和能消效率这些关键问题。

Method: 提出了一种基于令牌级判断的过滤机制，利用认知不确定性和关注重要性来识别信息权的令牌，只上传有信息价值的令牌到云端处理，减少LLM使用和通信成本。

Result: 在TinyLlama-1.1B和LLaMA-2-7B模型上的实验显示，该方法达到了87.5%的BERT Score和0.37 tokens/sec的令牌速率，相比标准HLM节省了40.7%的能消。与之前的U-HLM基线相比，BERTScore从85.8%提升到87.0%，能消节省从31.6%提升到43.6%，速率从0.36提升到0.40。

Conclusion: 该方法能够在带宽受限的边缘环境中实现能效高、准确性好的LLM部署，为资源约束环境下的大模型推理提供了有效解决方案。

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [113] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息深度算子网络(PI-DeepONet)的交通状态估计框架，将TSE重新定义为算子学习问题，通过将交通流守恒定律直接整合到算子学习过程中，在NGSIM数据集上展现了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络(PINNs)只能逐点强制执行PDE约束，而交通状态估计本质上需要解决高维时空偏微分方程，需要一种能够更好地处理算子学习问题的方法。

Method: 采用物理信息深度算子网络(PI-DeepONet)框架，训练参数化神经算子将稀疏输入数据映射到完整的时空交通状态场，直接将交通流守恒模型和基本图整合到算子学习过程中。

Result: 在NGSIM数据集上的实验表明，该方法在性能上优于最先进的基线方法，能够有效捕捉拥堵传播、空间相关性和时间演化，同时保持物理一致性。

Conclusion: PI-DeepONet框架为交通状态估计提供了一种有效的解决方案，通过将物理约束直接整合到算子学习中，实现了更好的性能和物理一致性，同时分析了输入函数生成策略和分支网络复杂度的影响。

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [114] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE是一种线性复杂度的自注意力机制，通过固定长度的潜在序列路由注意力，解决了传统自注意力二次复杂度的问题，在大型非结构化网格上实现了更好的可扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力的二次复杂度限制了其在大规模非结构化网格上的应用和可扩展性，需要一种更高效的注意力机制来处理大规模问题。

Method: FLARE通过可学习的查询令牌将输入序列投影到固定长度的潜在序列（M << N），在瓶颈序列中路由注意力，学习低秩形式的注意力，实现O(NM)的计算成本。

Result: FLARE不仅能够扩展到前所未有的问题规模，而且在各种基准测试中相比最先进的神经PDE代理模型提供了更优越的准确性。

Conclusion: FLARE通过低秩注意力路由机制成功解决了自注意力的可扩展性问题，为大规模非结构化网格处理提供了有效的解决方案，并发布了新的增材制造数据集以促进进一步研究。

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [115] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: 系统性方法构建异不变和等变操作，支持卡尔坐标张量和球形张量，并应用于几何图神经网络和材料本构关系学习


<details>
  <summary>Details</summary>
Motivation: 设计包含对称性的神经网络对几何深度学习至关重要，需要开发异不变和等变操作

Method: 提出系统性方法构建有效的异不变和等变操作，使用对称张量网络的图形表示来简化证明和构造

Result: 方法能够处理不同等级的卡尔坐标张量和不同类型的球形张量

Conclusion: 该方法成功应用于设计几何图神经网络的等变交互消息和学习材料本构关系的等变机器学习模型

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [116] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: 一种基于傀合模型的电动汽车参数估计方法，结合了谦神经网统计算机和可微物理模块，从速度和加速度估计汽车的多种参数和电消耗


<details>
  <summary>Details</summary>
Motivation: 需要一种能够从简单的车辆数据估计出多种重要参数的方法，以支持路径优化、生态路由、车辆诊断等应用

Method: 使用新颖的谦参数运算算法（Spectral Parameter Operator）基于Fourier神经网统构造全局上下文，结合可微物理模块进行前向传播。从速度和加速度估计电机效率、制动效率、空气力、滚动阻力等参数

Result: 在Tesla Model 3、Model S和Kia EV9实际数据上进行评估，Tesla车型平均绝对误差为0.2kW（高速时拉力功率的约1%），Kia EV9为0.8kW

Conclusion: 该模型具有良好的可解释性和通用性，能够适应未见条件和采样率，适用于路径优化、生态路由、车辆诊断和健康管理等应用场景

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [117] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: 无需辅助模型或步骤标注，通过自我追踪步骤偏好优化（SSPO）压缩推理过程，解决大语言模型的过分思考问题


<details>
  <summary>Details</summary>
Motivation: 主流的调效方法（如强化学习配合思维链推理）存在计算成本高、辅助模型负担大和过分思考问题，错误答案部分来自于缺乏自我修正的繁琐推理过程

Method: 提出SSPO框架，利用模型自身生成的步骤偏好信号进行精细化优化，无需人工标注或辅助模型，实现推理过程的压缩优化

Result: 实验表明SSPO生成的推理序列准确且简洁，有效减轻了过分思考行为，在多域和多语言下均未爆弃模型性能

Conclusion: SSPO作为一种插件式的强化学习过程监督框架，能够高效地优化大语言模型的推理过程，提高效率和准确性

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [118] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: 本文提出了可解释人工智能(XAI)的可信度评估框架，重点关注解释鲁棒性(ER)和解释方法鲁棒性(EMR)两个关键标准，为深度学习算法的可信解释提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 深度学习算法虽然预测准确，但其内部工作机制不透明，缺乏可信的解释方法。当前XAI方法存在局限性，需要建立更可靠的评估标准来确保解释的可信度。

Method: 作者提出了一个理论框架，形式化定义了解释鲁棒性(ER)和解释方法鲁棒性(EMR)两个标准。ER要求不同XAI方法在可比环境下产生相同解释，EMR要求单个方法在不同条件下保持解释一致性。

Result: 建立了一个系统性的可信度评估框架，能够帮助研究人员和实践者评估XAI方法的可靠性，并识别可能产生错误解释的情况。

Conclusion: 仅靠单个XAI方法的鲁棒性不足以保证可信度，必须同时满足ER和EMR标准。该框架为构建可信的深度学习解释系统提供了重要理论基础，并指出了未来研究方向。

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [119] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3是一个开源的多模态流匹配模型，通过三种架构无关的技术（自条件、假原子和训练时几何扭曲）显著提升了小分子生成性能，在药物样分子上达到近100%的有效性，且参数量比同类方法少一个数量级。


<details>
  <summary>Details</summary>
Motivation: 开发能够生成具有所需性质的现实分子的生成模型，以加速化学发现。现有模型在联合采样分子拓扑和3D结构方面仍有改进空间。

Method: 基于流匹配的多模态生成模型，采用三种架构无关技术：自条件（self-conditioning）、假原子（fake atoms）和训练时几何扭曲（train-time geometry distortion），无需改变图神经网络架构或流匹配基础公式。

Result: 在药物样分子上实现近100%的有效性，更准确地复现训练数据的功能基团组成和几何结构，参数量比同类方法少一个数量级。

Conclusion: 这些技术缓解了基于传输的生成模型的普遍病理问题，能够在推理过程中检测和纠正分布漂移，为改进扩散和流基分子生成模型的稳定性和质量提供了简单可转移的策略。

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [120] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: 提出了SciNO模型来稳定估计Hessian对角矩阵，改进了基于排序的因果发现方法，在计算效率和数值稳定性方面优于现有方法DiffAN


<details>
  <summary>Details</summary>
Motivation: 现有的因果排序方法需要准确估计对数密度的Hessian对角矩阵，但之前的Stein梯度估计器计算昂贵且内存密集，而DiffAN方法虽然解决了这些问题但仍存在数值不稳定性

Method: 提出了Score-informed Neural Operator (SciNO)，一种在平滑函数空间中的概率生成模型，用于稳定近似Hessian对角矩阵并在分数建模过程中保持结构信息

Result: 在合成图上平均减少42.7%的排序分歧，在真实数据集上减少31.5%，同时保持内存效率和可扩展性。还提出了与自回归模型结合的概率控制算法

Conclusion: 该方法无需额外微调或提示工程就能增强LLMs的因果推理能力，为因果发现提供了更稳定高效的解决方案

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [121] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可能在导致Byzantine攻击的惠应学习场景中有效运作的方法，仅需服务器和一个客户端是可信的，无需预先知道恶意客户端的数量。


<details>
  <summary>Details</summary>
Motivation: 解决惠应学习中存在Byzantine攻击的问题，当服务器有可靠的边缘数据集时，如何在不知道恶意客户端数量的情况下保证模型训练的可靠性。

Method: 利用服务器的可靠边缘数据集，仅需服务器和一个客户端是可信的。通过理论分析证明方法在强Byzantine攻击下仍能保持有界的最优性间隔。

Result: 在MNIST、FMNIST和CIFAR-10数据集上，该算法在各种攻击策略（标签翻转、符号翻转、高斯噪声等）下显著超过了Mean、Trimmed Mean、Median、Krum和Multi-Krum等标准和稳健惠应学习基准方法。

Conclusion: 该方法为惠应学习提供了一种高效的Byzantine攻击防御方案，在仅有两个可信参与者的情况下便能实现可靠的协作模型训练，具有强烈的实践意义。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [122] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero是一个新颖的联邦学习方法，通过超网络动态生成针对非参与客户端的专用模型，解决了数据异构性和资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理数据异构性方面取得进展，但无法泛化到具有域内分布偏移和资源限制的非参与客户端。

Method: 使用基于分布感知嵌入的超网络动态生成专用模型，采用NoisyEmbed增强的提取器和平衡惩罚来防止特征崩溃，分块生成模型以适应不同数据分布。

Result: 在多个数据集和模型上的实验显示，HyperFedZero性能显著优于竞争方法，计算、存储和通信开销最小。消融研究和可视化验证了各组件的必要性。

Conclusion: HyperFedZero通过分布感知的超网络方法有效解决了联邦学习中的数据异构性问题，为非参与客户端提供了良好的适应性和泛化能力。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [123] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 这篇论文提出了一种用于车辆网络中交通标志检测的联邦学习框架，通过分布式模型训练避免原始数据共享，解决隐私和通信挑战。


<details>
  <summary>Details</summary>
Motivation: 连接和自动化车辆每天产生巨量传感器数据，中心化机器学习方法在感知任务中带来了重大隐私和通信挑战。

Method: 将交通标志类别在车辆之间分区进行专门化本地训练，使用轻量级对象检测器，通过FedProx、FedAdam和FedAVG等算法在Flower框架中联合模型参数，评估了不同服务器轮次、本地迭代、客户参与比例和数据分布配置。

Result: 实验结果显示：服务器轮次从2增加到20时准确率从0.1以下提升到0.8以上；适中本地迭代（8-10）准确率约0.67；更高客户参与比例提升泛化能力至0.83；FedProx在处理异质性方面表现最佳；非IID数据分布性能不如IID。

Conclusion: 这种联邦学习方法可能为实际车辆部署提供可扩展、保护隐私的解决方案，为未来集成健壮联合算法和通信优化推动智能交通系统发展提供指导。

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [124] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: FedSODA是一个资源高效的联邦微调框架，通过层剪枝和蒸馏对齐技术，在保持模型性能的同时显著降低通信、存储和计算需求。


<details>
  <summary>Details</summary>
Motivation: 联邦微调在资源受限设备上进行全模型微调时面临计算和内存需求过高的问题，限制了其实际应用潜力。

Method: 提出相似性组剪枝(SGP)模块剪枝冗余层，保留关键层；引入协调蒸馏对齐(ODA)模块减少子模型与全模型间的梯度差异；结合QLoRA技术进行量化子模型和轻量适配器微调。

Result: 平均减少70.6%通信开销，降低75.6%存储使用，提升3.1%任务准确率。

Conclusion: FedSODA框架在资源约束下实现了高效的联邦微调，适合实际应用部署。

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [125] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: 这篇论文提出了FedUNet，一种轻量级的结构无关联部署学习框架，通过U-Net模块实现异构客户端间的高效知识传递，在低通信开销下达到了93.11%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统联部学习方法偏差地假设所有客户端模型结构相同的问题，以满足异质性真实环境中的应用需求。

Method: 在每个客户端的主干网络上附加U-Net受启发的加性模块，仅共享紧凑的U-Net瓶颈部分，通过编码器-解码器设计和跳连接捕获不同级别的特征，实现客户端不变表征提取。

Result: 在VGG变种模型上进行实验，FedUNet达到了93.11%的准确率，轻量版本也达到92.68%的准确率，仅需0.89MB的低通信开销。

Conclusion: FedUNet提供了一种高效、轻量的异构联部学习方案，通过U-Net模块实现了在低通信成本下的知识共享，为异质性环境下的联部学习开启了新方向。

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [126] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: 这篇论文提出了一个系统性评估神经网络空间推理能力的基准框架，采用空间模型检查器生成路径连通性和空间距离计算任务的合成数据集，并发现神经网络在基本几何和拓扑理解任务中存在系统性失败。


<details>
  <summary>Details</summary>
Motivation: 建立一个维度全面的基准框架，用于系统性评估神经网络的空间推理能力，特别是形态学性质如连通性和距离关系。

Method: 利用空间模型检查器VoxLogicA生成两类合成数据集：路径连通性问题用于拓扑分析，空间距离计算任务用于几何理解。构建了包含数据生成、标准化训练、推理执行和综合评估的自动化流水线，使用Dice系数和IoU指标进行评估。

Result: 初步实验结果显示神经网络在空间推理能力方面遇到重大挑战，在基本几何和拓扑理解任务中出现系统性失败。

Conclusion: 该框架提供了可复现的实验协议，能够识别神经网络的具体局限性，为研究神经网络与符号推理混合方法提高空间理解能力奠定基础，特别是在临床应用中。

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [127] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: 一种基于极限学习机(ELM)的新题短期能源预测方法，通过MIMO结构预测多种能源产出和总产量，在短期预测中显著超过持续性预测，具有计算效率高的优势。


<details>
  <summary>Details</summary>
Motivation: 解决能源系统的非稳定性、季节性变化和多种能源源的复杂预测需求，需要一种能够动态适应波动、计算效率高且适合实时应用的预测方法。

Method: 使用极限学习机(ELM)结合多输入多输出(MIMO)架构，采用滑动窗口技术和周期时间编码来处理非稳定性和季节性变化，基于科西嘉六年小时数据进行多能源(太阳能、风能、水能、热能、生物能源和进口电力)预测。

Result: 模型显著超过持续性预测，太阳能和热能的nRMSE分别为17.9%和5.1%，R²>0.98(1小时预测)，在5小时内保持高精度，MIMO比SISO有边际改善，且计算效率高于LSTM等深度学习方法。

Conclusion: 该ELM-MIMO方法为短期能源预测提供了一种计算效率高、适合实时应用的有效方案，能够适应不同地区的资源可用性、网格特性和市场结构等本地约束。

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [128] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: 提出E3Former在线集成模型用于服务器less系统的工作负载预测，相比单模型方法平均减少10%预测误差，已在字节跳动IHPA平台部署，支持60万+CPU核心的预测自动扩缩容，资源利用率降低40%以上。


<details>
  <summary>Details</summary>
Motivation: 服务器less系统中需要预测性自动扩缩容来优化资源分配，但现有预测模型难以快速适应在线工作负载的动态变化和捕获细粒度高频任务的复杂周期性。

Method: 提出E3Former在线集成模型，通过协同多个子网络的预测能力来克服单模型方法的局限性，在计算开销最小的情况下确保准确性和鲁棒性。

Result: 在真实工作负载数据集上的实验表明，在线预测任务中平均减少10%预测误差，在字节跳动IHPA平台成功部署，支持30+应用稳定运行，预测自动扩缩容能力达60万+CPU核心。

Conclusion: E3Former模型有效解决了服务器less系统中工作负载预测的挑战，实现了高精度预测和资源优化，在实际生产环境中证明了其有效性和实用性。

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [129] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Wavy Transformer通过引入二阶波动动力学解决transformer中的过度平滑问题，在NLP和CV任务中提升性能且参数增加极少


<details>
  <summary>Details</summary>
Motivation: 深度transformer模型存在过度平滑问题，即token表示在连续transformer块中收敛到相似值。本文从图神经扩散的物理角度解释此现象，并提出解决方案

Method: 建立注意力层与完全图上图神经扩散的等价关系，提出基于二阶波动动力学的新型注意力层，设计保持物理状态-速度关系的FFN和归一化层

Result: 在多种NLP和CV任务的transformer模型上验证，Wavy Transformer能一致提升性能，且只需极少额外参数，无需额外超参数调优

Conclusion: 从物理动力学角度理解transformer的过度平滑问题，提出的Wavy Transformer架构能有效缓解此问题并提升模型性能

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [130] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文重新审视因果建模在AI泛化中的作用，挑战了现有领域泛化基准的结论，提出了更细致的因果理论框架


<details>
  <summary>Details</summary>
Motivation: 近期关于领域泛化基准的研究对因果建模能够带来稳健AI泛化的承诺提出了质疑，需要重新审视因果性与泛化之间的关系

Method: 通过理论分析和文献综述，调和因果建模与领域泛化文献中明显的矛盾，提出更细致的因果理论框架

Result: 建立了更全面的因果理论来解释泛化问题，并提供了交互式演示来展示研究成果

Conclusion: 因果性在AI泛化中仍然具有重要作用，但需要更细致的理论框架来理解其作用机制和局限性

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [131] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore是一种新的MoE路由范式，通过建模为最小成本最大流问题并集成SoftTopk算子，解决了传统MoE网络中的token丢弃和硬件效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 传统MoE网络存在专家容量约束导致token丢弃和硬件效率低下的问题，而去除约束又会损害负载平衡和计算效率。

Method: 提出Maximum Score Routing (MaxScore)，将路由建模为最小成本最大流问题，集成SoftTopk算子，避免迭代重路由和最优传输公式的局限性。

Result: 在相同FLOPs下，相比有约束和无约束基线，实现了更低的训练损失和更高的评估分数。

Conclusion: MaxScore有效解决了MoE路由中的根本限制，在保持计算效率的同时提升了模型性能。

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [132] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: 提出L2S方法，通过训练小型辅助模块预测输入特定的引导向量，实现多模态大语言模型的精细化引导，减少幻觉并增强安全性


<details>
  <summary>Details</summary>
Motivation: 现有引导技术依赖单一静态引导向量，无法处理依赖具体输入的行为需求（如安全回答需要根据问题类型采取不同策略）

Method: 使用对比性输入特定提示计算输入特定的线性偏移，训练小型辅助模块来预测测试时的输入特定引导向量

Result: L2S方法在减少幻觉和增强安全性方面优于其他静态基线方法

Conclusion: 输入特定的精细化引导是提升多模态大语言模型安全性和准确性的有效途径

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [133] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: 这篇论文通过实验研究探讨了设备机器学习中的存储问题，发现根据样本效应性进行适应性压缩比统一压缩或数据投弃更有效。


<details>
  <summary>Details</summary>
Motivation: 设备机机器学习常遇到存储空间限制，特别是在连续数据收集场景中。需要找到数据数量与质量之间的最佳平衡点。

Method: 通过实验研究分析不同压缩策略的效果，包括统一数据投弃、一尺寸压缩策略，以及根据样本效应性进行适应性压缩的方法。

Result: 发现简单的统一数据处理策略效果并不理想，不同数据样本对压缩的效应性存在显著差异，这支持了根据样本特征进行适应性压缩的可行性。

Conclusion: 这项研究系统性地描述了存储敏感学习这个尚未充分探索的挑战，为开发新一代存储敏感学习系统奠定了基础，提供了重要的见解。

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [134] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 这篇论文研究了变换器模型在上下文词汇预测任务中的损失地形，发现子n-元语言模型是经高斯损失的近稳定点，为阶段性学习动态提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 受实验观察到训练过程中持续的平台期和阶段性进展的骚动，研究者想要探索变换器模型在上下文词汇预测任务中的损失地形特性。

Method: 采用简化的变换器模型，重点研究在交叉熵损失下学习上下文n-元语言模型。建立了参数配置为稳定点的充分条件，并构造了一组表示k-元估计器的参数配置。

Result: 证明在无限序列长度和参数范数极限下，这些解的沿损失梯度消失，表明子n-元语言模型是经高斯损失的近稳定点。

Conclusion: 这些发现为广泛观察到的阶段性学习动态和出现性相变现象提供了理论解释，数值实验进一步支持了这些见解。

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [135] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: HRS是一个混合表示框架，通过整合数值和图像表示来捕捉极端负载动态，并引入调度感知损失函数来减少SLA违规率和提高利润。


<details>
  <summary>Details</summary>
Motivation: 流媒体服务的快速发展导致网络负载具有高度时变性和突发性，现有方法要么导致峰值时段SLA违规，要么采用保守的过度配置策略增加资源支出。

Method: 提出HRS混合表示框架，整合数值和图像表示来捕捉极端负载动态，引入调度感知损失函数(SAL)来捕获预测误差的不对称影响。

Result: 在四个真实数据集上的实验表明，HRS持续优于十个基线方法，达到最先进性能，SLA违规率降低63.1%，总利润损失减少32.3%。

Conclusion: HRS框架通过混合表示和调度感知损失函数有效解决了流媒体服务负载预测中的SLA违规和资源浪费问题，显著提升了服务质量和经济效益。

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [136] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 基于动态图模型和深度异常检测的TGN-SVDD入侵检测方法，在现实入侵检测数据上显示优势


<details>
  <summary>Details</summary>
Motivation: 随着全球数字化发展，网络安全日益重要。机器学习入侵检测面临检测新题网络事件、时间序列数据和网络通信图结构等挑战

Method: 提出TGN-SVDD方法，结合现代动态图建模技术和深度异常检测算法

Result: 在现实入侵检测数据上表现超过多个基准方法，并建议了更具挑战性的数据变体

Conclusion: TGN-SVDD作为一种新题入侵检测方法，能够有效处理网络安全中的动态图结构和时间序列特征

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [137] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ是一种面向TinyML流式应用的单次通过、无标签不确定性监测方法，通过轻量级信号捕捉时间一致性，转换为校准风险分数，在资源受限设备上实现高效监控。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性监测方法（如早期退出和深度集成）在TinyML设备上计算开销大、内存占用高，需要开发资源高效且无需在线标签的流式不确定性监测方案。

Method: 使用环形缓冲区和O(1)每步更新，通过后验和特征的短时域时间一致性生成风险分数，结合流式共形校准层实现预算化的接受/弃权决策。

Result: 在微控制器上比早期退出和深度集成方法减少50-60%内存占用和30-45%延迟，在分布内数据流损坏情况下提升3-7个AUPRC点，最高达到0.86 AUPRC和0.92 AUROC。

Conclusion: 时间一致性结合流式共形校准为TinyML设备监控提供了实用且资源高效的基础方案。

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [138] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: SparseMap是一个基于进化策略的稀疏张量加速器优化框架，通过联合优化映射策略和稀疏策略，在巨大的设计空间（O(10^41)）中高效搜索最优解。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏张量加速器多为手动设计，局限于特定场景且难以调整。传统方法只关注映射或稀疏策略的单一优化，缺乏综合考虑，导致设计次优。

Method: 提出SparseMap框架，构建包含映射和稀疏策略的综合设计空间，改进遗传编码和进化算子，使进化策略能高效探索巨大而多样的设计空间。

Result: 与先前工作和经典优化方法相比，SparseMap始终能找到更优的解决方案。

Conclusion: SparseMap成功解决了稀疏张量加速器设计中映射和稀疏策略联合优化的挑战，为自动化设计提供了有效解决方案。

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [139] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ是一种面向TinyML的单次前向传播、无需标签的不确定性量化方法，通过深度方向的下层激活预测来估计风险，具有极低的内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化方法（如早退机制和深度集成）在TinyML设备上存在内存占用大、延迟高的问题，需要多次前向传播或额外退出点，不适合资源受限的微控制器部署。

Method: 使用int8量化的小型预测头来预测下一层的统计特征，通过轻量级单调映射器将预测误差转换为可操作的不确定性分数，无需时间缓冲区、辅助退出点或重复前向传播。

Result: 在视觉和音频任务上，相比早退机制和深度集成方法，SNAP-UQ减少了40-60%的存储空间和25-35%的延迟，在数据损坏流中提高了AUPRC指标，单次前向传播即可达到约0.9的AUROC故障检测性能。

Conclusion: 基于层间动态的不确定性量化为TinyML设备上的实时监控提供了实用且资源高效的解决方案，能够在极低资源开销下实现可靠的不确定性估计。

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [140] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC是一个新颖的联邦学习框架，同时确保差分隐私、拜占庭鲁棒性和通信效率，通过RobAJoL方法结合JL变换压缩和鲁棒聚合


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法难以同时实现差分隐私保护、拜占庭攻击鲁棒性和通信效率，需要一种综合解决方案

Method: 提出Robust-compatible压缩概念，使用Johnson-Lindenstrauss变换进行压缩，结合鲁棒聚合方法RobAJoL

Result: 理论证明JL变换与鲁棒聚合兼容，实验在CIFAR-10和Fashion MNIST上验证了方法的有效性，优于现有方法

Conclusion: Fed-DPRoC框架成功实现了隐私保护、鲁棒性和通信效率的三重目标，为联邦学习提供了实用的解决方案

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [141] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: SL-ACC是一个通信高效的拆分学习框架，通过自适应通道重要性识别和通道分组压缩技术，显著减少数据传输量，在保持训练精度的同时大幅缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络复杂度的增加和参与设备数量的增长，拆分学习中过度的粉碎数据传输成为主要瓶颈，严重拖慢模型训练速度。

Method: 提出SL-ACC框架，包含两个核心组件：1）基于香农熵的自适应通道重要性识别(ACII)来评估每个通道对模型训练的贡献；2）基于熵值的通道分组压缩(CGC)进行分组自适应压缩以减少传输量。

Result: 在多个数据集上的广泛实验验证，SL-ACC框架相比最先进的基准方法，在达到目标精度时所需时间显著减少。

Conclusion: SL-ACC框架有效解决了拆分学习中的通信瓶颈问题，通过智能的通道重要性识别和压缩策略，在不影响训练精度的情况下大幅提升了训练效率。

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [142] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: 图卷积网络(GCN)性能与图的代数连通性(Fiedler值)密切相关，相似Fiedler值的图具有类似结构特性，可用作GCN性能预测指标和迁移学习参考。


<details>
  <summary>Details</summary>
Motivation: GCN文献中观察到堆叠GCN层并不总能提升节点分类和边预测性能，需要找到能预测GCN性能的图结构指标。

Method: 通过理论和实验分析，在合成图和真实图数据(Cora、CiteSeer、Polblogs)上验证Fiedler值与GCN性能的关系，探索多种Fiedler值聚合方法。

Result: 实证发现图的代数连通性(Fiedler值)是GCN性能的良好预测指标，相似Fiedler值的图可使用相同滤波器和超参数获得类似结果。

Conclusion: Fiedler值可作为GCN性能预测的有效指标，并为图之间的迁移学习提供指导，相似代数连通性的图更适合参数迁移。

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [143] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta是一种Adam风格的优化器，通过动态调整beta2参数来处理梯度尖峰问题，在物理驱动的PDE代理模型和PINNs中显著提升训练稳定性和最终性能


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的PDE代理模型和物理信息神经网络(PINNs)中，不同边界和初始条件的训练样本会导致不稳定的损失和尖峰梯度，传统Adam优化器的固定beta2参数无法有效处理这种情况

Method: 提出Kourkoutas-Beta优化器，用层级的动态beta2值替代固定值，该值由当前池化梯度范数与历史指数移动平均范数的比值（sunspike比率）驱动，尖峰时降低beta2，平稳时保持高beta2

Result: 在四个测试场景中（Transformer PDE代理、3D PINN热传导、MLX合成任务、字符级Transformer），相比固定beta2的Adam，Kourkoutas-Beta提升了稳定性和最终损失性能，在small-enwik8上比特率降低38-58%

Conclusion: 该方法保持Adam式收敛保证的同时，在尖峰梯度条件下提高了鲁棒性，运行时开销与Adam相当，可作为即插即用的优化器替代方案

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [144] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: 通过蒙特卡洛采样和矩分析方法接近模型预测分布，结合Wasserstein距离和KL散度构建函数正则化，提高持续学习的效果和效率


<details>
  <summary>Details</summary>
Motivation: 解决传统函数正则化持续学习方法存在的计算成本高和线性接近误差大的问题

Method: 提出MCFRCL框架，采用蒙特卡洛采样接近模型预测分布，利用三种连续分布通过矩分析方法捕捉MC样本的统计特征，并使用Wasserstein距离和KL散度构建正则化函数

Result: 在MNIST和CIFAR数据集上评估，结果显示在预测准确性和训练效率方面都显示出显著效果

Conclusion: MCFRCL框枵通过蒙特卡洛采样和矩分析方法，有效解决了函数正则化方法的计算效率和接近误差问题，为持续学习领域提供了一种高效的新方法

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [145] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: 提出了一种叫FXHEKM算法，用于阻挡冲击性噪声环境下的主动噪声控制，在α稳定噪声中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在冲击性噪声环境下主动噪声控制的稳健性问题，提高噪声拖除效果。

Method: 发展了过滤-x双曲正切指数广义内核M估计函数(FXHEKM)算法，进行统计分析和计算成本研究。

Result: 数值实验显示，该算法在α稳定噪声中能够高效拖除加性伪信号，在MSE和ANR性能指标上都超过竞争算法。

Conclusion: FXHEKM算法为冲击性噪声环境下的主动噪声控制提供了一种稳健高效的解决方案。

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [146] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 本文研究使用BERT和层次注意网络进行网络攻击后果的多标签分类，达到了97.2%的高准确率，显著超越传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 随着网络攻击越来越复杂，对自动化攻击描述分析和后果预测的需求日益突出，以及时取行动和合理分配资源。

Method: 利用MITRE CWE数据库的文本描述，采用BERT模型结合层次注意网络(HAN)进行多标签分类，将攻击后果分为可用性、访问控制、保密性、完整性和其他五个主要类别。

Result: BERT模型在多标签分类中达到总体准确率0.972，显著超过传统CNN和LSTM模型。HAN在特定安全标签上表现更好，但BERT在精度和召回率方面更一致优势。

Conclusion: BERT模型在预测网络攻击后果方面表现最优，适合用于安全专业人员进行风险评估和资源分配。

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [147] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 这篇论文提出了一种方法，利用分离的内部和外部数据源来估计AI模型的公平性指标，解决因法律和隐私问题导致的完整数据获取困难。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域确保AI系统公平性至关重要，但实际中因法律和隐私限制导致人口学等保护属性数据难以获取，数据常分布在不同源头中。

Method: 利用可用的分离数据估计可行的联合分布集合，然后计算合理的公平性指标范围。

Result: 通过模拟和实验验证，该方法能够得到有意义的公平性指标上下界，并获得真实指标的可靠估计。

Conclusion: 这种方法可以作为实际应用中公平性测试的实用有效解决方案，特别是在完整数据访问受限的情况下。

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [148] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 本文比较了FMAE和HEF两种评估函数在多元时间序列预测中的表现，HEF在全局指标上更优，适合战略规划；FMAE在局部指标和运行时间上更好，适合短期操作。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列建模面临数据复杂性、不确定性和制度转移挑战，传统评估指标存在偏差和限制模型普适性的问题。

Method: 设计了FMAE（聚焦均绝对误差）和HEF（层次评估函数）两种评估方法，在不同数据分割比例（91:9、80:20、70:30）下使用三种优化器（网格搜索、PSO、Optuna）进行实验。

Result: HEF在全局指标（R2、相对准确度、RMSE、RMSSE）上一贵优于FMAE，提升了模型稳健性和解释力；FMAE在局部指标（MAE、MASE）和执行时间上更优。

Conclusion: 研究呈现了方法论上的权衡：HEF适用于战略规划场景，FMAE更适合操作效率需求，并提出了一个可复现的预测模型优化框架。

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [149] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: 这篇论文提出了一种通过神经代理模型来反向探索生成特定输出特征的可能参数分布的方法，解决了代理模型近似误差和交互式参数分布构建的挑战。


<details>
  <summary>Details</summary>
Motivation: 目前的代理模型逆向问题解决方案主要关注找到少量匹配参数，而忽视了生成特定输出特征的更广泛可能参数分布的整体图景。

Method: 通过密度估计模型化代理模型的近似误差，以训练参数的输入和输出空间距离为基础报呈高密度。结合参数的先验信念和特征的可能性，有效地采样生成目标输出特征的可能参数配置。

Result: 在三个模拟数据集上进行了特征驱动的参数分析，开发了可视化界面来展示参数分布。

Conclusion: 该方法能够建模并可视化生成特定输出特征的可能输入参数分布，为科学模拟提供了更全面的逆向问题解决方案。

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [150] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: 提出基于海底声学传感器网络和LGCP的海上空间异常检测框架，通过二阶概率近似和动态传感器部署提升异常分类和检测性能


<details>
  <summary>Details</summary>
Motivation: 传统方法仅使用均值信息进行异常检测，精度有限。需要开发能够同时利用均值和方差信息的更精确异常概率估计方法，并配合动态传感器部署策略来提升海上异常检测效果

Method: 使用对数高斯Cox过程(LGCP)建模目标到达，将事件分为正常和异常过程混合。提出二阶概率近似方法，同时考虑正常强度函数的均值和方差。结合实时近最优传感器部署策略动态调整传感器位置

Result: 在弗吉尼亚州诺福克真实船舶交通数据上的数值实验表明，该方法相比仅使用均值的方法显著提高了分类精度，并通过传感器部署有效提升了异常检测性能

Conclusion: 所提出的二阶概率近似框架为海上空间异常检测提供了更精确的概率估计，结合动态传感器部署策略，能够有效提升异常分类和检测的整体性能

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [151] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: 这篇论文提出了CGPT模型，通过因果图导向的成对建模方法，解决了工业系统中多维时间序列数据建模的通用性和精确性问题，在预测准确性方面显著超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业系统中多维时间序列数据建模存在两种方法的困境：通道依赖(CD)模型能抓取特定交叉变量动力学但缺乏稳健性和适配性，而通道独立(CI)模型虽有通用性却无法模拟关键的明确相互作用。需要找到一种方法来解决这个争议。

Method: 提出了因果导向的成对Transformer(CGPT)架构，将已知因果图作为引导偏置集成到模型中。核心是成对建模范式，将多维数据分解为成对。模型使用通道无关的可学习层，所有参数维度都与变量数量独立。CGPT在成对级别强化CD信息流动，同时在成对之间实现CI类似的通用性。

Result: 在合成和真实工业数据集上进行了长期预测和单步预测任务的验证，这些任务模拟了常见的工业复杂性。结果显示CGPT在预测准确性方面显著超过了CI和CD基线模型，并且与端到端训练的CD模型保持竞争性能，同时保持对问题维度的无关性。

Conclusion: CGPT通过因果导向的成对建模方法，成功地解决了工业系统中多维时间序列数据建模的通用性与精确性之间的争议。该方法能够解构复杂系统动力学，构建了高度灵活的架构，确保了可扩展性和任意变量适配能力，为工业预测回归任务提供了有效的解决方案。

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [152] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: CRTR方法通过组合负采样方案消除伪特征，使时间对比学习能够捕捉时间结构，在Sokoban和魔方等复杂时序任务上取得优异表现，首次实现仅通过学习表征高效解决任意魔方状态。


<details>
  <summary>Details</summary>
Motivation: 传统AI中感知学习状态表征，规划通过搜索实现时序推理。本文研究是否可以从同时捕捉感知和时间结构的表征中涌现出这种推理能力，解决标准时间对比学习依赖伪特征而无法捕捉时间结构的问题。

Method: 提出组合时序推理表征(CRTR)，采用负采样方案来可证明地移除伪特征，促进时序推理。该方法在复杂时序结构领域（如Sokoban和魔方）进行验证。

Result: CRTR在魔方任务上学习的表征能够泛化到所有初始状态，相比BestFS使用更少的搜索步骤（尽管解决方案更长）。这是首个仅通过学习表征就能高效解决任意魔方状态的方法，无需依赖外部搜索算法。

Conclusion: CRTR方法成功解决了时间对比学习中的伪特征问题，证明了从学习表征中涌现时序推理的可能性，为结合感知和时序推理的AI系统提供了新途径。

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [153] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: 人类移动预测新方法：通过统计分析、语义信息整合和样本重新采样提升长期轨迹预测精度


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注短期微观移动预测，对宏观层面的生活规律和长期轨迹预测研究不足，需要探索如何使用历史数据进行有效的长期移动预测

Method: 进行全面的实验分析，包括LSTM和Transformer模型、参数配置和训练策略，深入研究移动模式的统计分布，采用用户语义聚类和层化采样来减少数据偏斜

Result: 显式包含周内天和用户特定历史信息能够提升预测效果，小批量随机梯度优化在数据有限时表现更佳，重新采样策略有助于保持数据多样性和预测准确性

Conclusion: 通过统计分析、语义信息整合和样本重新采样等方法，可以有效提升个体长期移动轨迹的预测精度，为传染病监测、老幼养护等应用领域提供更有效的解决方案

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [154] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: 本文提出MDPO方法来解决掩码扩散语言模型训练与推理阶段的结构差异问题，通过强化学习优化去噪轨迹，在减少60倍梯度更新的情况下达到SOTA性能，并在MATH500和Countdown任务上分别提升9.6%和54.2%。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型存在训练与推理阶段的关键差异：推理时逐步揭示生成序列结构，而训练时随机掩码忽略这种结构，这导致次优性能但先前研究大多忽视此问题。

Method: 将学习有效去噪轨迹问题建模为序列决策问题，提出Masked Diffusion Policy Optimization (MDPO)方法，利用扩散的马尔可夫性质，在推理使用的渐进精炼调度下显式训练模型。

Result: MDPO以60倍更少的梯度更新匹配先前SOTA性能，在相同权重更新次数下，MATH500提升9.6%，Countdown提升54.2%。同时提出的RCR重掩码策略作为即插即用推理替代进一步提升性能。

Conclusion: 研究结果展示了研究MDLMs预训练与推理差异的巨大潜力，MDPO和RCR策略有效解决了训练-推理不一致性问题并显著提升模型性能。

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [155] [BaMANI: Bayesian Multi-Algorithm causal Network Inference](https://arxiv.org/abs/2508.11741)
*Habibolla Latifizadeh,Anika C. Pirkey,Alanna Gould,David J. Klinke II*

Main category: stat.ML

TL;DR: 通过集成学习方法BaMANI，综合多种计算算法来提高贝叶斯因果网络推断的稳健性和可靠性


<details>
  <summary>Details</summary>
Motivation: 不同计算算法导致预测的因果网络存在偏差，需要减少单一算法对结果的影响

Method: 采用"集体智慧"策略，开发BaMANI软件工具，通过多算法集成来推断因果网络

Result: 实现了一种集成学习框架，并在人类乳腺癌研究中应用验证

Conclusion: 集成学习方法可有效降低单一算法的偏差，提高贝叶斯因果网络推断的可靠性

Abstract: Improved computational power has enabled different disciplines to predict
causal relationships among modeled variables using Bayesian network inference.
While many alternative algorithms have been proposed to improve the efficiency
and reliability of network prediction, the predicted causal networks reflect
the generative process but also bear an opaque imprint of the specific
computational algorithm used. Following a ``wisdom of the crowds" strategy, we
developed an ensemble learning approach to marginalize the impact of a single
algorithm on Bayesian causal network inference. To introduce the approach, we
first present the theoretical foundation of this framework. Next, we present a
comprehensive implementation of the framework in terms of a new software tool
called BaMANI (Bayesian Multi-Algorithm causal Network Inference). Finally, we
describe a BaMANI use-case from biology, particularly within human breast
cancer studies.

</details>


### [156] [Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings](https://arxiv.org/abs/2508.11847)
*Jenny Y. Huang,Yunyi Shen,Dennis Wei,Tamara Broderick*

Main category: stat.ML

TL;DR: 这篇论文提出了一种评估Bradley-Terry排名系统稳健性的方法，发现尽管只删除0.02%的最坏情况数据，也能导致顶级模型排名发生翻转，展现了人类偏好评估系统的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究Bradley-Terry排名系统对极小部分数据删除的敏感性，以了解当前人工智能模型评估系统的稳健性和可靠性。

Method: 设计计算高效的算法来评估排名系统的稳健性，并在Chatbot Arena和MT-Bench两个人类偏好平台的数据上进行实验。方法能够识别导致排名变化的关键评估数据。

Result: 发现顶级模型的Bradley-Terry排名对极小数据删除非常敏感，仅删除0.02%的数据即可改变最佳模型排名。MT-Bench的排名比Chatbot Arena更稳健，但众包人类评估和LLM作为判官的系统都呈现同样的敏感性。

Conclusion: 当前人工智能模型排名系统存在显著的稳健性问题，需要更加稳健的评估方法和数据收集策略来提高排名结果的可靠性。

Abstract: We propose a method for evaluating the robustness of a widely used LLM
ranking system -- the Bradley--Terry ranking system -- to dropping a worst-case
very small fraction of evaluation data. Our approach is computationally fast
and easy to adopt. When we apply our method to matchups from two popular
human-preference platforms, Chatbot Arena and MT-Bench, we find that the
Bradley--Terry rankings of top-performing models are remarkably sensitive to
the removal of a small fraction of evaluations. Our framework also identifies
the specific evaluations most responsible for such ranking flips, allowing for
inspections of these influential preferences. We observe that the rankings
derived from MT-Bench preferences are notably more robust than those from
Chatbot Arena, likely due to MT-bench's use of expert annotators and carefully
constructed prompts. Finally, we find that rankings based on crowdsourced
human-evaluated systems are just as sensitive as those based on LLM-as-a-judge
evaluations, where in both, dropping as little as 0.02% of the total
evaluations in the dataset can change the top-ranked model.

</details>


### [157] [Robust Data Fusion via Subsampling](https://arxiv.org/abs/2508.12048)
*Jing Wang,HaiYing Wang,Kun Chen*

Main category: stat.ML

TL;DR: 这篇论文研究在外部数据存在异常值污染情况下的转移学习和子采样方法，通过偏差最小化和方差最小化策略来提高估计器的性能，并在A380飞机硬着陆风险分析中实验了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决目标数据有限而外部数据存在异常值污染的实际问题，填补转移学习和子采样在数据污染情况下研究的空白。

Method: 研究了两种子采样策略：偏差最小化策略和方差最小化策略，并提出组合这两种策略的方法来提高估计器性能，还提供了非齐次误差界分析。

Result: 广泛的模拟实验显示了所提方法的优异性能，并在A380飞机硬着陆风险分析中成功应用，证明稳健的转移学习可以提高对稀有飞机类型的估计效率。

Conclusion: 该研究为处理外部数据异常值污染的转移学习问题提供了有效的子采样策略，通过组合偏差和方差最小化方法显著提高了估计器的性能和稳健性。

Abstract: Data fusion and transfer learning are rapidly growing fields that enhance
model performance for a target population by leveraging other related data
sources or tasks. The challenges lie in the various potential heterogeneities
between the target and external data, as well as various practical concerns
that prevent a na\"ive data integration. We consider a realistic scenario where
the target data is limited in size while the external data is large but
contaminated with outliers; such data contamination, along with other
computational and operational constraints, necessitates proper selection or
subsampling of the external data for transfer learning. To our
knowledge,transfer learning and subsampling under data contamination have not
been thoroughly investigated. We address this gap by studying various transfer
learning methods with subsamples of the external data, accounting for outliers
deviating from the underlying true model due to arbitrary mean shifts. Two
subsampling strategies are investigated: one aimed at reducing biases and the
other at minimizing variances. Approaches to combine these strategies are also
introduced to enhance the performance of the estimators. We provide
non-asymptotic error bounds for the transfer learning estimators, clarifying
the roles of sample sizes, signal strength, sampling rates, magnitude of
outliers, and tail behaviors of model error distributions, among other factors.
Extensive simulations show the superior performance of the proposed methods.
Additionally, we apply our methods to analyze the risk of hard landings in A380
airplanes by utilizing data from other airplane types,demonstrating that robust
transfer learning can improve estimation efficiency for relatively rare
airplane types with the help of data from other types of airplanes.

</details>


### [158] [An Introduction to Sliced Optimal Transport](https://arxiv.org/abs/2508.12519)
*Khai Nguyen*

Main category: stat.ML

TL;DR: 切片最优运输是一种通过投影到一维空间来高效计算概率济度距离的方法，结合了最优运输、积分几何和计算统计学的工具


<details>
  <summary>Details</summary>
Motivation: 解决传统最优运输在高维度计算复杂度过高的问题，提供高效可扩展的计算方法

Method: 利用Radon变换将概率济度投影到一维空间，在一维上解决最优运输问题，包括非线性投影、改进的蒙特卡洛近似、统计估计技术等

Result: 开发了高效的距离计算、质心计算、内核构造和嵌入方法，并扩展到不平衡、部分、多边际和Gromov-Wasserstein场景

Conclusion: 切片最优运输作为一种高效可扩展的计算工具，在机器学习、统计学、计算机图形学和计算机视觉等领域具有广泛应用前景

Abstract: Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal
transport (OT) that exploits the tractability of one-dimensional OT problems.
By combining tools from OT, integral geometry, and computational statistics,
SOT enables fast and scalable computation of distances, barycenters, and
kernels for probability measures, while retaining rich geometric structure.
This paper provides a comprehensive review of SOT, covering its mathematical
foundations, methodological advances, computational methods, and applications.
We discuss key concepts of OT and one-dimensional OT, the role of tools from
integral geometry such as Radon transform in projecting measures, and
statistical techniques for estimating sliced distances. The paper further
explores recent methodological advances, including non-linear projections,
improved Monte Carlo approximations, statistical estimation techniques for
one-dimensional optimal transport, weighted slicing techniques, and
transportation plan estimation methods. Variational problems, such as minimum
sliced Wasserstein estimation, barycenters, gradient flows, kernel
constructions, and embeddings are examined alongside extensions to unbalanced,
partial, multi-marginal, and Gromov-Wasserstein settings. Applications span
machine learning, statistics, computer graphics and computer visions,
highlighting SOT's versatility as a practical computational tool. This work
will be of interest to researchers and practitioners in machine learning, data
sciences, and computational disciplines seeking efficient alternatives to
classical OT.

</details>


### [159] [On computing and the complexity of computing higher-order $U$-statistics, exactly](https://arxiv.org/abs/2508.12627)
*Xingyu Chen,Ruiqi Zhang,Lin Liu*

Main category: stat.ML

TL;DR: 本文提出了计算高阶U统计量的新算法，通过分解为V统计量、利用爱因斯坦求和优化计算，并开发了Python包u-stats，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 高阶U统计量在统计学、机器学习和计算机科学中广泛应用，但计算耗时严重，缺乏系统的计算复杂性研究。

Method: 1) 将m阶U统计量分解为不超过m阶的V统计量的线性组合；2) 探索V统计量与爱因斯坦求和的关系；3) 基于核函数关联图的树宽给出时间复杂度估计

Result: 开发了新的高效算法和Python包u-stats，在三个统计应用中相比现有基准实现了显著的运行时性能提升

Conclusion: 该研究推动了U统计量算法发展，为实践者提供了有价值的工具，使基于高阶U统计量的方法实现更加高效便捷

Abstract: Higher-order $U$-statistics abound in fields such as statistics, machine
learning, and computer science, but are known to be highly time-consuming to
compute in practice. Despite their widespread appearance, a comprehensive study
of their computational complexity is surprisingly lacking. This paper aims to
fill that gap by presenting several results related to the computational aspect
of $U$-statistics. First, we derive a useful decomposition from an $m$-th order
$U$-statistic to a linear combination of $V$-statistics with orders not
exceeding $m$, which are generally more feasible to compute. Second, we explore
the connection between exactly computing $V$-statistics and Einstein summation,
a tool often used in computational mathematics, quantum computing, and quantum
information sciences for accelerating tensor computations. Third, we provide an
optimistic estimate of the time complexity for exactly computing
$U$-statistics, based on the treewidth of a particular graph associated with
the $U$-statistic kernel. The above ingredients lead to a new, much more
runtime-efficient algorithm of exactly computing general higher-order
$U$-statistics. We also wrap our new algorithm into an open-source Python
package called $\texttt{u-stats}$. We demonstrate via three statistical
applications that $\texttt{u-stats}$ achieves impressive runtime performance
compared to existing benchmarks. This paper aspires to achieve two goals: (1)
to capture the interest of researchers in both statistics and other related
areas further to advance the algorithmic development of $U$-statistics, and (2)
to offer the package $\texttt{u-stats}$ as a valuable tool for practitioners,
making the implementation of methods based on higher-order $U$-statistics a
more delightful experience.

</details>


### [160] [Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation](https://arxiv.org/abs/2508.12674)
*Haruka Ezoe,Hiroki Matsumoto,Ryohei Hisano*

Main category: stat.ML

TL;DR: 这篇论文提出了Unfolded Laplacian Spectral Embedding方法，通过将拉普拉斯谱嵌入扩展到动态关系结构中，同时保持跨截面和纵向稳定性，并连接嵌入与图导电性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 动态关系结构在AI任务中重要但演化特性带来衡量表示的挑战，需要方法能够给出一致可解释的表示。

Method: 提出Unfolded Laplacian Spectral Embedding方法，将Unfolded Adjacency Spectral Embedding框架扩展到标准化拉普拉斯矩阵，保持跨截面和纵向稳定性。

Result: 提供了方法满足稳定性条件的正式证明，建立了嵌入与动态图导电性之间的Cheeger风格不等式，并通过综合和实际数据集验证了理论发现和方法性能。

Conclusion: 该方法为动态网络表示建立了基于谱图论的原则性稳定框架。

Abstract: Dynamic relational structures play a central role in many AI tasks, but their
evolving nature presents challenges for consistent and interpretable
representation. A common approach is to learn time-varying node embeddings,
whose effectiveness depends on satisfying key stability properties. In this
paper, we propose Unfolded Laplacian Spectral Embedding, a new method that
extends the Unfolded Adjacency Spectral Embedding framework to normalized
Laplacians while preserving both cross-sectional and longitudinal stability. We
provide formal proof that our method satisfies these stability conditions. In
addition, as a bonus of using the Laplacian matrix, we establish a new
Cheeger-style inequality that connects the embeddings to the conductance of the
underlying dynamic graphs. Empirical evaluations on synthetic and real-world
datasets support our theoretical findings and demonstrate the strong
performance of our method. These results establish a principled and stable
framework for dynamic network representation grounded in spectral graph theory.

</details>


### [161] [Optimal Condition for Initialization Variance in Deep Neural Networks: An SGD Dynamics Perspective](https://arxiv.org/abs/2508.12834)
*Hiroshi Horii,Sothea Has*

Main category: stat.ML

TL;DR: 通过分析SGD的连续时间动力学模型，推导出深度神经网络初始化方差的数学最优条件，提供了比以往经验法更理论化的初始化指南


<details>
  <summary>Details</summary>
Motivation: 将SGD优化过程推广到连续时间动力学模型，通过理论分析探索初始化分布与预期损失函数之间的关系，以获得数学上严格的初始化参数选择准则

Method: 使用Fokker-Planck方程描述SGD的Langevin动力学，通过求解几何稳态分布与初始分布的KL散度，得到预期损失函数的界限表达式，并在高斯分布假设下求解最优初始方差

Result: 推导出了深度神经网络初始化方差的数学最优条件，实验结果显示该条件下的模型在MNIST和Fashion-MNIST数据集上达到更低的训练损失和更高的测试精度，超过传统的He-normal初始化方法

Conclusion: 该研究为深度神经网络的初始化参数选择提供了数学上严格的理论基础，替代了经验性的驱动方法，同时清楚地解释了参数动力学过程的物理含义

Abstract: Stochastic gradient descent (SGD), one of the most fundamental optimization
algorithms in machine learning (ML), can be recast through a continuous-time
approximation as a Fokker-Planck equation for Langevin dynamics, a viewpoint
that has motivated many theoretical studies. Within this framework, we study
the relationship between the quasi-stationary distribution derived from this
equation and the initial distribution through the Kullback-Leibler (KL)
divergence. As the quasi-steady-state distribution depends on the expected cost
function, the KL divergence eventually reveals the connection between the
expected cost function and the initialization distribution. By applying this to
deep neural network models (DNNs), we can express the bounds of the expected
loss function explicitly in terms of the initialization parameters. Then, by
minimizing this bound, we obtain an optimal condition of the initialization
variance in the Gaussian case. This result provides a concrete mathematical
criterion, rather than a heuristic approach, to select the scale of weight
initialization in DNNs. In addition, we experimentally confirm our theoretical
results by using the classical SGD to train fully connected neural networks on
the MNIST and Fashion-MNIST datasets. The result shows that if the variance of
the initialization distribution satisfies our theoretical optimal condition,
then the corresponding DNN model always achieves lower final training loss and
higher test accuracy than the conventional He-normal initialization. Our work
thus supplies a mathematically grounded indicator that guides the choice of
initialization variance and clarifies its physical meaning of the dynamics of
parameters in DNNs.

</details>


### [162] [The path to a goal: Understanding soccer possessions via path signatures](https://arxiv.org/abs/2508.12930)
*David Hirnschall,Robert Bajons*

Main category: stat.ML

TL;DR: 使用路径签名技术编码足球控球的空间时间结构，预测下一步动作，超越传统Transformer方法的性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 克服现有方法依赖固定历史窗口和手工特征的限制，避免包含无关或误导性信息，并处理变长度时闸序列。

Method: 提出一种新的框架，利用路径签名来编码整个控球过程的复杂空间时间结构，自动捐描事件的顺序和相互作用。

Result: 方法在多种损失指标上超过Transformer基准方法，显著降低计算成本，并提出了更可靠的控球评价指标。

Conclusion: 通过2017/18赛季详细分析验证了方法的有效性，该方法为足球分析提供了数学基础牢固的特征编码方法，具有广阔的应用前景。

Abstract: We present a novel framework for predicting next actions in soccer
possessions by leveraging path signatures to encode their complex
spatio-temporal structure. Unlike existing approaches, we do not rely on fixed
historical windows and handcrafted features, but rather encode the entire
recent possession, thereby avoiding the inclusion of potentially irrelevant or
misleading historical information. Path signatures naturally capture the order
and interaction of events, providing a mathematically grounded feature encoding
for variable-length time series of irregular sampling frequencies without the
necessity for manual feature engineering. Our proposed approach outperforms a
transformer-based benchmark across various loss metrics and considerably
reduces computational cost. Building on these results, we introduce a new
possession evaluation metric based on well-established frameworks in soccer
analytics, incorporating both predicted action type probabilities and action
location. Our metric shows greater reliability than existing metrics in
domain-specific comparisons. Finally, we validate our approach through a
detailed analysis of the 2017/18 Premier League season and discuss further
applications and future extensions.

</details>


### [163] [Simulation-Based Inference: A Practical Guide](https://arxiv.org/abs/2508.12939)
*Michael Deistler,Jan Boelts,Peter Steinbach,Guy Moss,Thomas Moreau,Manuel Gloeckler,Pedro L. C. Rodrigues,Julia Linhart,Janne K. Lappalainen,Benjamin Kurt Miller,Pedro J. Gonçalves,Jan-Matthis Lueckmann,Cornelius Schröder,Jakob H. Macke*

Main category: stat.ML

TL;DR: 基于模拟的推理（SBI）教程，提供使用神经网络进行贝叶斯推断的实用指南，无需似然函数评估，适用于科学发现中的参数推断。


<details>
  <summary>Details</summary>
Motivation: 解决传统贝叶斯推断在随机模拟器模型中计算成本过高的问题，使研究人员能够高效地进行参数推断。

Method: 使用神经网络在模拟器生成的数据上进行训练，实现摊销推断（amortized inference），无需额外训练即可快速执行贝叶斯推断。

Result: 提供了结构化的SBI工作流程、实用指南和诊断工具，涵盖从模拟器设置到结果验证的全过程。

Conclusion: 本教程使研究人员能够应用最先进的SBI方法，促进科学发现中的高效参数推断。

Abstract: A central challenge in many areas of science and engineering is to identify
model parameters that are consistent with prior knowledge and empirical data.
Bayesian inference offers a principled framework for this task, but can be
computationally prohibitive when models are defined by stochastic simulators.
Simulation-based Inference (SBI) is a suite of methods developed to overcome
this limitation, which has enabled scientific discoveries in fields such as
particle physics, astrophysics, and neuroscience. The core idea of SBI is to
train neural networks on data generated by a simulator, without requiring
access to likelihood evaluations. Once trained, inference is amortized: The
neural network can rapidly perform Bayesian inference on empirical observations
without requiring additional training or simulations. In this tutorial, we
provide a practical guide for practitioners aiming to apply SBI methods. We
outline a structured SBI workflow and offer practical guidelines and diagnostic
tools for every stage of the process -- from setting up the simulator and
prior, choosing and training inference networks, to performing inference and
validating the results. We illustrate these steps through examples from
astrophysics, psychophysics, and neuroscience. This tutorial empowers
researchers to apply state-of-the-art SBI methods, facilitating efficient
parameter inference for scientific discovery.

</details>


### [164] [Shapley Values: Paired-Sampling Approximations](https://arxiv.org/abs/2508.12947)
*Michael Mayer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: 这篇论文分析了计算Shapley值的两种采样近似方法（KernelSHAP和PermutationSHAP），证明了它们的高斯正态性，并揭示了在特定条件下配对采样方法的准确性和加性恢复性质。


<details>
  <summary>Details</summary>
Motivation: 解决Shapley值计算的计算复杂度问题，提供更有效的采样近似算法来支持机器学习预测的解释性分析。

Method: 采用数理统计方法，分析了两种采样近似算法（KernelSHAP和PermutationSHAP）的高斯正态性质，并研究了在特定交互作用条件下的准确性。

Result: 证明了采样近似方法的高斯正态性，发现在最大二阶交互作用时配对采样方法能提供准确结果，并告知PermutationSHAP具有加性恢复性质而KernelSHAP不具备。

Conclusion: 这些理论结果为选择适合的Shapley值计算方法提供了理论基础，对于实际应用中的算法选择具有重要指导意义。

Abstract: Originally introduced in cooperative game theory, Shapley values have become
a very popular tool to explain machine learning predictions. Based on Shapley's
fairness axioms, every input (feature component) gets a credit how it
contributes to an output (prediction). These credits are then used to explain
the prediction. The only limitation in computing the Shapley values (credits)
for many different predictions is of computational nature. There are two
popular sampling approximations, sampling KernelSHAP and sampling
PermutationSHAP. Our first novel contributions are asymptotic normality results
for these sampling approximations. Next, we show that the paired-sampling
approaches provide exact results in case of interactions being of maximal order
two. Furthermore, the paired-sampling PermutationSHAP possesses the additive
recovery property, whereas its kernel counterpart does not.

</details>
