{"id": "2507.10609", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10609", "abs": "https://arxiv.org/abs/2507.10609", "authors": ["Obumneme Nwafor", "Chioma Nwafor", "Amro Zakaria", "Nkechi Nwankwo"], "title": "A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights", "comment": null, "summary": "The United Arab Emirates (UAE) relies heavily on seawater desalination to\nmeet over 90% of its drinking water needs. Desalination processes are highly\nenergy intensive and account for approximately 15% of the UAE's electricity\nconsumption, contributing to over 22% of the country's energy-related CO2\nemissions. Moreover, these processes face significant sustainability challenges\nin the face of climate uncertainties such as rising seawater temperatures,\nsalinity, and aerosol optical depth (AOD). AOD greatly affects the operational\nand economic performance of solar-powered desalination systems through\nphotovoltaic soiling, membrane fouling, and water turbidity cycles.\n  This study proposes a novel pipelined two-stage predictive modelling\narchitecture: the first stage forecasts AOD using satellite-derived time series\nand meteorological data; the second stage uses the predicted AOD and other\nmeteorological factors to predict desalination performance efficiency losses.\nThe framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)\nwas used to reveal key drivers of system degradation. Furthermore, this study\nproposes a dust-aware rule-based control logic for desalination systems based\non predicted values of AOD and solar efficiency. This control logic is used to\nadjust the desalination plant feed water pressure, adapt maintenance\nscheduling, and regulate energy source switching.\n  To enhance the practical utility of the research findings, the predictive\nmodels and rule-based controls were packaged into an interactive dashboard for\nscenario and predictive analytics. This provides a management decision-support\nsystem for climate-adaptive planning."}
{"id": "2507.10706", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10706", "abs": "https://arxiv.org/abs/2507.10706", "authors": ["Pradyumna Kunchala", "Ashish Patwari"], "title": "A Leap-on-Success Exhaustive Search Method to Find Optimal Robust Minimum Redundancy Arrays (RMRAs): New Array Configurations for Sensor Counts 11 to 20", "comment": "21 pages, 8 Tables, 4 Figures", "summary": "Two-fold redundant sparse arrays (TFRAs) are designed to maintain accurate\ndirection estimation even in the event of a single sensor failure, leveraging\nthe deliberate coarray redundancy infused into their design. Robust Minimum\nRedundancy Arrays (RMRAs), a specialized class of TFRAs, optimize this\nredundancy to achieve the maximum possible aperture for a given number of\nsensors. However, finding optimal RMRA configurations is an NP-hard problem,\nwith prior research reporting optimal solutions only for arrays of up to ten\nsensors. This paper presents newly discovered optimal RMRA configurations for\narray sizes 11 to 15, identified using a novel Leap-on-Success exhaustive\nsearch algorithm that efficiently reduces computational effort by terminating\nthe search upon locating optimal solutions. The robustness of these arrays was\nvalidated under all single-element failure scenarios using MATLAB simulations,\nconfirming their superior resilience compared to some existing TFRAs vulnerable\nto failures at specific sensor positions. Furthermore, near-optimal\nconfigurations for array sizes 16 to 20 are also reported, highlighting the\npotential applicability of the proposed method for larger array designs given\nsufficient computational resources. This work not only advances the\nstate-of-the-art in RMRA design but also introduces an effective search\nmethodology that can be leveraged for future explorations in array\nconfiguration optimization."}
{"id": "2507.11535", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.11535", "abs": "https://arxiv.org/abs/2507.11535", "authors": ["Andrey Bryutkin", "Matthew E. Levine", "Iñigo Urteaga", "Youssef Marzouk"], "title": "Canonical Bayesian Linear System Identification", "comment": "46 pages, 9 figures", "summary": "Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data."}
{"id": "2507.10706", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10706", "abs": "https://arxiv.org/abs/2507.10706", "authors": ["Pradyumna Kunchala", "Ashish Patwari"], "title": "A Leap-on-Success Exhaustive Search Method to Find Optimal Robust Minimum Redundancy Arrays (RMRAs): New Array Configurations for Sensor Counts 11 to 20", "comment": "21 pages, 8 Tables, 4 Figures", "summary": "Two-fold redundant sparse arrays (TFRAs) are designed to maintain accurate\ndirection estimation even in the event of a single sensor failure, leveraging\nthe deliberate coarray redundancy infused into their design. Robust Minimum\nRedundancy Arrays (RMRAs), a specialized class of TFRAs, optimize this\nredundancy to achieve the maximum possible aperture for a given number of\nsensors. However, finding optimal RMRA configurations is an NP-hard problem,\nwith prior research reporting optimal solutions only for arrays of up to ten\nsensors. This paper presents newly discovered optimal RMRA configurations for\narray sizes 11 to 15, identified using a novel Leap-on-Success exhaustive\nsearch algorithm that efficiently reduces computational effort by terminating\nthe search upon locating optimal solutions. The robustness of these arrays was\nvalidated under all single-element failure scenarios using MATLAB simulations,\nconfirming their superior resilience compared to some existing TFRAs vulnerable\nto failures at specific sensor positions. Furthermore, near-optimal\nconfigurations for array sizes 16 to 20 are also reported, highlighting the\npotential applicability of the proposed method for larger array designs given\nsufficient computational resources. This work not only advances the\nstate-of-the-art in RMRA design but also introduces an effective search\nmethodology that can be leveraged for future explorations in array\nconfiguration optimization."}
{"id": "2507.10643", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10643", "abs": "https://arxiv.org/abs/2507.10643", "authors": ["Yuchi Tang", "Iñaki Esnaola", "Suzanne Mason", "George Panoutsos"], "title": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models", "comment": "17 pages, 6 figures, Submitted to NeurIPS 2025", "summary": "Existing post-hoc model-agnostic methods generate external explanations for\nopaque models, primarily by locally attributing the model output to its input\nfeatures. However, they often lack an explicit and systematic framework for\nquantifying the contribution of individual features. Building on the Taylor\nexpansion framework introduced by Deng et al. (2024) to unify existing local\nattribution methods, we propose a rigorous set of postulates -- \"precision\",\n\"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific\nattribution. Guided by these postulates, we introduce TaylorPODA (Taylor\nexpansion-derived imPortance-Order aDapted Attribution), which incorporates an\nadditional \"adaptation\" property. This property enables alignment with\ntask-specific goals, especially in post-hoc settings lacking ground-truth\nexplanations. Empirical evaluations demonstrate that TaylorPODA achieves\ncompetitive results against baseline methods, providing principled and\nvisualization-friendly explanations. This work represents a step toward the\ntrustworthy deployment of opaque models by offering explanations with stronger\ntheoretical grounding."}
{"id": "2507.10838", "categories": ["eess.SP", "cs.IT", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10838", "abs": "https://arxiv.org/abs/2507.10838", "authors": ["Gokberk Yaylali", "Ahmad Ali Khan", "Dionysios S. Kalogerias"], "title": "Waterfilling at the Edge: Optimal Percentile Resource Allocation via Risk-Averse Reduction", "comment": "5 pages, 5 figures", "summary": "We address deterministic resource allocation in point-to-point multi-terminal\nAWGN channels without inter-terminal interference, with particular focus on\noptimizing quantile transmission rates for cell-edge terminal service.\nClassical utility-based approaches -- such as minimum rate, sumrate, and\nproportional fairness -- are either overconservative, or inappropriate, or do\nnot provide a rigorous and/or interpretable foundation for fair rate\noptimization at the edge. To overcome these challenges, we employ Conditional\nValue-at-Risk (CVaR), a popular coherent risk measure, and establish its\nequivalence with the sum-least-$\\alpha$th-quantile (SL$\\alpha$Q) utility. This\nconnection enables an exact convex reformulation of the SL$\\alpha$Q\nmaximization problem, facilitating analytical tractability and precise and\ninterpretable control over cell-edge terminal performance. Utilizing Lagrangian\nduality, we provide (for the first time) parameterized closed-form solutions\nfor the optimal resource policy -- which is of waterfilling-type -- as well as\nthe associated (auxiliary) Value-at-Risk variable. We further develop a novel\ninexact dual subgradient descent algorithm of minimal complexity to determine\nglobally optimal resource policies, and we rigorously establish its\nconvergence. The resulting edge waterfilling algorithm iteratively and\nefficiently allocates resources while explicitly ensuring transmission rate\nfairness across (cell-edge) terminals. Several (even large-scale) numerical\nexperiments validate the effectiveness of the proposed method for enabling\nrobust quantile rate optimization at the edge."}
{"id": "2507.10710", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10710", "abs": "https://arxiv.org/abs/2507.10710", "authors": ["Haoyu Chen", "Anna Little", "Akin Narayan"], "title": "Robust Multi-Manifold Clustering via Simplex Paths", "comment": null, "summary": "This article introduces a novel, geometric approach for multi-manifold\nclustering (MMC), i.e. for clustering a collection of potentially intersecting,\nd-dimensional manifolds into the individual manifold components. We first\ncompute a locality graph on d-simplices, using the dihedral angle in between\nadjacent simplices as the graph weights, and then compute infinity path\ndistances in this simplex graph. This procedure gives a metric on simplices\nwhich we refer to as the largest angle path distance (LAPD). We analyze the\nproperties of LAPD under random sampling, and prove that with an appropriate\ndenoising procedure, this metric separates the manifold components with high\nprobability. We validate the proposed methodology with extensive numerical\nexperiments on both synthetic and real-world data sets. These experiments\ndemonstrate that the method is robust to noise, curvature, and small\nintersection angle, and generally out-performs other MMC algorithms. In\naddition, we provide a highly scalable implementation of the proposed\nalgorithm, which leverages approximation schemes for infinity path distance to\nachieve quasi-linear computational complexity."}
{"id": "2507.11036", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11036", "abs": "https://arxiv.org/abs/2507.11036", "authors": ["Salman Liaquat", "Ijaz Haider Naqvi", "Nor Muzlifah Mahyuddin"], "title": "Dual RIS-Assisted Monostatic L-Band Radar Target Detection in NLoS Scenarios", "comment": "Accepted for presentation at the 9th International Conference on\n  Communications and Future Internet", "summary": "The use of a single Reconfigurable Intelligent Surface (RIS) to boost the\nsignal-to-noise ratio (SNR) at the radar offers significant improvement in\ndetecting targets, especially in non-line-of-sight (NLoS) scenarios. However,\nthere are scenarios where no path exists between the radar and the target, even\nwith a single RIS-assisted radar, due to other present obstacles. This paper\nderives an expression for SNR in target detection scenarios where dual RISs\nassist a monostatic radar in NLoS situations. We calculate the power received\nat the radar through a dual RIS configuration. We show that the SNR performance\nof RIS-assisted radars can improve with known locations of the radar and RISs.\nOur results demonstrate that the required accuracy in target localization can\nbe achieved by controlling the number of RISs, the number of unit cells in each\nRIS, and properly selecting the locations of RISs to cover the desired region.\nThe performance of dual RIS-assisted radar systems can surpass that of single\nRIS-assisted radar systems under favourable alignment and sufficiently large\nRIS sizes."}
{"id": "2507.10956", "categories": ["stat.ML", "cs.LG", "62-08", "G.3"], "pdf": "https://arxiv.org/pdf/2507.10956", "abs": "https://arxiv.org/abs/2507.10956", "authors": ["Zhaoyu Xing", "Yang Wan", "Juan Wen", "Wei Zhong"], "title": "GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering", "comment": null, "summary": "It is important to identify the discriminative features for high dimensional\nclustering. However, due to the lack of cluster labels, the regularization\nmethods developed for supervised feature selection can not be directly applied.\nTo learn the pseudo labels and select the discriminative features\nsimultaneously, we propose a new unsupervised feature selection method, named\nGlObal and Local information combined Feature Selection (GOLFS), for high\ndimensional clustering problems. The GOLFS algorithm combines both local\ngeometric structure via manifold learning and global correlation structure of\nsamples via regularized self-representation to select the discriminative\nfeatures. The combination improves the accuracy of both feature selection and\nclustering by exploiting more comprehensive information. In addition, an\niterative algorithm is proposed to solve the optimization problem and the\nconvergency is proved. Simulations and two real data applications demonstrate\nthe excellent finite-sample performance of GOLFS on both feature selection and\nclustering."}
{"id": "2507.11093", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.11093", "abs": "https://arxiv.org/abs/2507.11093", "authors": ["Wenxuan Sun", "Mingjie Shao", "Luteng Zhu", "Yao Ge", "Tong Zhang", "Zhi Liu"], "title": "Optimizing Fluid Antenna Configurations for Constructive Interference Precoding", "comment": null, "summary": "The fluid antenna system (FAS) has emerged as a new physical-layer concept to\nprovide enhanced propagation conditions for multiuser multiple-input\nmultiple-output (MIMO) communications over conventional fixed arrays. This work\nfocuses on minimizing the maximum symbol error probability (SEP) under $M$-ary\nphase shift keying (MPSK) signaling in a multiuser downlink equipped with FAS,\nwhere each antenna moves within nonoverlapping intervals. This specific problem\nof joint SEP minimization with FAS and constructive interference (CI) precoding\nhas not been previously addressed. The resulting problem turns out to be a\nnonconvex and nonsmooth optimization challenge. We transform the SEP\nminimization problem into a safety margin maximization problem in constructive\ninterference precoding. Then, we customize a smoothing technique and a block\ncoordinate descent (BCD) algorithm, with emphasis on low computational\ncomplexity. Simulation results show that our approach can reduce bit error rate\n(BER) compared to both the fixed arrays and FAS designed by existing particle\nswarm optimization (PSO). Also, our approach shows attractively low\ncomputational complexity compared to PSO benchmarks."}
{"id": "2507.11136", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11136", "abs": "https://arxiv.org/abs/2507.11136", "authors": ["Afra Kilic", "Kim Batselier"], "title": "Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection", "comment": "39 pages, 5 figures, 4 tables. Submitted to Journal of Machine\n  Learning Research. The code is available at:\n  https://github.com/afrakilic/BTN-Kernel-Machines. arXiv admin note: text\n  overlap with arXiv:1401.6497 by other authors", "summary": "Tensor Network (TN) Kernel Machines speed up model learning by representing\nparameters as low-rank TNs, reducing computation and memory use. However, most\nTN-based Kernel methods are deterministic and ignore parameter uncertainty.\nFurther, they require manual tuning of model complexity hyperparameters like\ntensor rank and feature dimensions, often through trial-and-error or\ncomputationally costly methods like cross-validation. We propose Bayesian\nTensor Network Kernel Machines, a fully probabilistic framework that uses\nsparsity-inducing hierarchical priors on TN factors to automatically infer\nmodel complexity. This enables automatic inference of tensor rank and feature\ndimensions, while also identifying the most relevant features for prediction,\nthereby enhancing model interpretability. All the model parameters and\nhyperparameters are treated as latent variables with corresponding priors.\nGiven the Bayesian approach and latent variable dependencies, we apply a\nmean-field variational inference to approximate their posteriors. We show that\napplying a mean-field approximation to TN factors yields a Bayesian ALS\nalgorithm with the same computational complexity as its deterministic\ncounterpart, enabling uncertainty quantification at no extra computational\ncost. Experiments on synthetic and real-world datasets demonstrate the superior\nperformance of our model in prediction accuracy, uncertainty quantification,\ninterpretability, and scalability."}
{"id": "2507.11224", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11224", "abs": "https://arxiv.org/abs/2507.11224", "authors": ["Ali Khandan Boroujeni", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Stefan Köpsell", "Ghazal Bagheri", "Rafael F. Schaefer"], "title": "Fairness-Aware Secure Integrated Sensing and Communications with Fractional Programming", "comment": "Submitted to an IEEE journal", "summary": "We propose a novel secure integrated sensing and communications (ISAC) system\ndesigned to serve multiple communication users (CUs) and targets. To that end,\nwe formulate an optimization problem that maximizes the secrecy rate under\nconstraints balancing both communication and sensing requirements. To enhance\nfairness among users, an entropy-regularized fairness metric is introduced\nwithin the problem framework. We then propose a solution employing an\naccelerated quadratic transform (QT) with a non-homogeneous bound to\niteratively solve two subproblems, thereby effectively optimizing the overall\nobjective. This approach ensures robust security and fairness in resource\nallocation for ISAC systems. Finally, simulation results verify the performance\ngains in terms of average secrecy rate, average data rate, and beam gain."}
{"id": "2507.11161", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11161", "abs": "https://arxiv.org/abs/2507.11161", "authors": ["Jun Chen", "Hong Chen", "Yonghua Yu", "Yiming Ying"], "title": "How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction", "comment": "Accepted by ICML2025 as a poster", "summary": "In recent years, contrastive learning has achieved state-of-the-art\nperformance in the territory of self-supervised representation learning. Many\nprevious works have attempted to provide the theoretical understanding\nunderlying the success of contrastive learning. Almost all of them rely on a\ndefault assumption, i.e., the label consistency assumption, which may not hold\nin practice (the probability of failure is called labeling error) due to the\nstrength and randomness of common augmentation strategies, such as random\nresized crop (RRC). This paper investigates the theoretical impact of labeling\nerror on the downstream classification performance of contrastive learning. We\nfirst reveal several significant negative impacts of labeling error on\ndownstream classification risk. To mitigate these impacts, data dimensionality\nreduction method (e.g., singular value decomposition, SVD) is applied on\noriginal data to reduce false positive samples, and establish both theoretical\nand empirical evaluations. Moreover, it is also found that SVD acts as a\ndouble-edged sword, which may lead to the deterioration of downstream\nclassification accuracy due to the reduced connectivity of the augmentation\ngraph. Based on the above observations, we give the augmentation suggestion\nthat we should use some moderate embedding dimension (such as $512, 1024$ in\nour experiments), data inflation, weak augmentation, and SVD to ensure large\ngraph connectivity and small labeling error to improve model performance."}
{"id": "2507.11249", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.11249", "abs": "https://arxiv.org/abs/2507.11249", "authors": ["Ruohai Guo", "Jiang Zhu", "Xing Jiang", "Fengzhong Qu"], "title": "Fast and Efficient Implementation of the Maximum Likelihood Estimation for the Linear Regression with Gaussian Model Uncertainty", "comment": null, "summary": "The linear regression model with a random variable (RV) measurement matrix,\nwhere the mean of the random measurement matrix has full column rank, has been\nextensively studied. In particular, the quasiconvexity of the maximum\nlikelihood estimation (MLE) problem was established, and the corresponding\nCramer-Rao bound (CRB) was derived, leading to the development of an efficient\nbisection-based algorithm known as RV-ML. In contrast, this work extends the\nanalysis to both overdetermined and underdetermined cases, allowing the mean of\nthe random measurement matrix to be rank-deficient. A remarkable contribution\nis the proof that the equivalent MLE problem is convex and satisfies strong\nduality, strengthening previous quasiconvexity results. Moreover, it is shown\nthat in underdetermined scenarios, the randomness in the measurement matrix can\nbe beneficial for estimation under certain conditions. In addition, a fast and\nunified implementation of the MLE solution, referred to as generalized RV-ML\n(GRV-ML), is proposed, which handles a more general case including both\nunderdetermined and overdetermined systems. Extensive numerical simulations are\nprovided to validate the theoretical findings."}
{"id": "2507.11381", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.11381", "abs": "https://arxiv.org/abs/2507.11381", "authors": ["Rom Gutman", "Shimon Sheiba", "Omer Noy Klien", "Naama Dekel Bird", "Amit Gruber", "Doron Aronson", "Oren Caspi", "Uri Shalit"], "title": "From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies", "comment": null, "summary": "We propose a framework for building patient-specific treatment recommendation\nmodels, building on the large recent literature on learning patient-level\ncausal models and inspired by the target trial paradigm of Hernan and Robins.\nWe focus on safety and validity, including the crucial issue of causal\nidentification when using observational data. We do not provide a specific\nmodel, but rather a way to integrate existing methods and know-how into a\npractical pipeline. We further provide a real world use-case of treatment\noptimization for patients with heart failure who develop acute kidney injury\nduring hospitalization. The results suggest our pipeline can improve patient\noutcomes over the current treatment regime."}
{"id": "2507.11284", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11284", "abs": "https://arxiv.org/abs/2507.11284", "authors": ["Mohamed-Amine Lahmeri", "Pouya Fakharizadeh", "Víctor Mustieles-Pérez", "Martin Vossiek", "Gerhard Krieger", "Robert Schober"], "title": "Sensing Accuracy Optimization for Multi-UAV SAR Interferometry with Data Offloading", "comment": null, "summary": "The integration of unmanned aerial vehicles (UAVs) with radar imaging sensors\nhas revolutionized the monitoring of dynamic and local Earth surface processes\nby enabling high-resolution and cost-effective remote sensing. This paper\ninvestigates the optimization of the sensing accuracy of a UAV swarm deployed\nto perform multi-baseline interferometric synthetic aperture radar (InSAR)\nsensing. In conventional single-baseline InSAR systems, only one synthetic\naperture radar (SAR) antenna pair acquires two SAR images from two distinct\nangles to generate a digital elevation model (DEM) of the target area. However,\nmulti-baseline InSAR extends this concept by aggregating multiple acquisitions\nfrom different angles, thus, significantly enhancing the vertical accuracy of\nthe DEM. The heavy computations required for this process are performed on the\nground and, therefore, the radar data is transmitted in real time to a ground\nstation (GS) via a frequency-division multiple access (FDMA) air-to-ground\nbackhaul link. This work focuses on improving the sensing precision by\nminimizing the height error of the averaged DEM while simultaneously ensuring\nsensing and communication quality-of-service (QoS). To this end, the UAV\nformation, velocity, and communication power allocation are jointly optimized\nusing evolutionary algorithms (EAs). Our approach is benchmarked against\nestablished optimization methods, including genetic algorithms (GAs), simulated\nannealing (SA), and deep reinforcement learning (DRL) techniques. Numerical\nresults show that the proposed solution outperforms these baseline schemes and\nachieves sub-decimeter vertical accuracy in several scenarios. These findings\nunderline the potential of coordinated UAV swarms for delivering high-precision\nand real-time Earth observations through radar interferometry."}
{"id": "2507.11385", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11385", "abs": "https://arxiv.org/abs/2507.11385", "authors": ["George D. Pasparakis", "Ioannis A. Kougioumtzoglou", "Michael D. Shields"], "title": "Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning", "comment": null, "summary": "A methodology is developed, based on nonparametric Bayesian dictionary\nlearning, for joint space-time wind field data extrapolation and estimation of\nrelated statistics by relying on limited/incomplete measurements. Specifically,\nutilizing sparse/incomplete measured data, a time-dependent optimization\nproblem is formulated for determining the expansion coefficients of an\nassociated low-dimensional representation of the stochastic wind field.\nCompared to an alternative, standard, compressive sampling treatment of the\nproblem, the developed methodology exhibits the following advantages. First,\nthe Bayesian formulation enables also the quantification of the uncertainty in\nthe estimates. Second, the requirement in standard CS-based applications for an\na priori selection of the expansion basis is circumvented. Instead, this is\ndone herein in an adaptive manner based on the acquired data. Overall, the\nmethodology exhibits enhanced extrapolation accuracy, even in cases of\nhigh-dimensional data of arbitrary form, and of relatively large extrapolation\ndistances. Thus, it can be used, potentially, in a wide range of wind\nengineering applications where various constraints dictate the use of a limited\nnumber of sensors. The efficacy of the methodology is demonstrated by\nconsidering two case studies. The first relates to the extrapolation of\nsimulated wind velocity records consistent with a prescribed joint\nwavenumber-frequency power spectral density in a three-dimensional domain (2D\nand time). The second pertains to the extrapolation of four-dimensional (3D and\ntime) boundary layer wind tunnel experimental data that exhibit significant\nspatial variability and non-Gaussian characteristics."}
{"id": "2507.11383", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11383", "abs": "https://arxiv.org/abs/2507.11383", "authors": ["V S V Sandeep", "Sai Dinesh Kancharana", "Arun Pachai Kannu"], "title": "Sparse Regression Codes exploit Multi-User Diversity without CSI", "comment": null, "summary": "We study sparse regression codes (SPARC) for multiple access channels with\nmultiple receive antennas, in non-coherent flat fading channels. We propose a\nnovel practical decoder, referred to as maximum likelihood matching pursuit\n(MLMP), which greedily finds the support of the codewords of users with partial\nmaximum likelihood metrics. As opposed to the conventional\nsuccessive-cancellation based greedy algorithms, MLMP works as a\nsuccessive-combining energy detector. We also propose MLMP modifications to\nimprove the performance at high code rates. Our studies in short block lengths\nshow that, even without any channel state information, SPARC with MLMP decoder\nachieves multi-user diversity in some scenarios, giving better error\nperformance with multiple users than that of the corresponding single-user\ncase. We also show that SPARC with MLMP performs better than conventional\nsparse recovery algorithms and pilot-aided transmissions with polar codes."}
{"id": "2507.11535", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.11535", "abs": "https://arxiv.org/abs/2507.11535", "authors": ["Andrey Bryutkin", "Matthew E. Levine", "Iñigo Urteaga", "Youssef Marzouk"], "title": "Canonical Bayesian Linear System Identification", "comment": "46 pages, 9 figures", "summary": "Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data."}
{"id": "2507.11413", "categories": ["eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11413", "abs": "https://arxiv.org/abs/2507.11413", "authors": ["Christos N. Efrem", "Ioannis Krikidis"], "title": "Joint Power Allocation and Reflecting-Element Activation for Energy Efficiency Maximization in IRS-Aided Communications Under CSI Uncertainty", "comment": "5 pages, 3 figures", "summary": "We study the joint power allocation and reflecting element (RE) activation to\nmaximize the energy efficiency (EE) in communication systems assisted by an\nintelligent reflecting surface (IRS), taking into account imperfections in\nchannel state information (CSI). The robust optimization problem is mixed\ninteger, i.e., the optimization variables are continuous (transmit power) and\ndiscrete (binary states of REs). In order to solve this challenging problem we\ndevelop two algorithms. The first one is an alternating optimization (AO)\nmethod that attains a suboptimal solution with low complexity, based on the\nLambert W function and a dynamic programming (DP) algorithm. The second one is\na branch-and-bound (B&B) method that uses AO as its subroutine and is formally\nguaranteed to achieve a globally optimal solution. Both algorithms do not\nrequire any external optimization solver for their implementation. Furthermore,\nnumerical results show that the proposed algorithms outperform the baseline\nschemes, AO achieves near-optimal performance in most cases, and B&B has low\ncomputational complexity on average."}
{"id": "2507.10564", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10564", "abs": "https://arxiv.org/abs/2507.10564", "authors": ["Sameera Bharadwaja H.", "Siddhrath Jandial", "Shashank S. Agashe", "Rajesh Kumar Reddy Moore", "Youngkwan Kim"], "title": "Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing", "comment": null, "summary": "We consider the problem of tool-to-tool matching (TTTM), also called, chamber\nmatching in the context of a semiconductor manufacturing equipment. Traditional\nTTTM approaches utilize static configuration data or depend on a golden\nreference which are difficult to obtain in a commercial manufacturing line.\nFurther, existing methods do not extend very well to a heterogeneous setting,\nwhere equipment are of different make-and-model, sourced from different\nequipment vendors. We propose novel TTTM analysis pipelines to overcome these\nissues. We hypothesize that a mismatched equipment would have higher variance\nand/or higher number of modes in the data. Our best univariate method achieves\na correlation coefficient >0.95 and >0.5 with the variance and number of modes,\nrespectively showing that the proposed methods are effective. Also, the best\nmultivariate method achieves a correlation coefficient >0.75 with the\ntop-performing univariate methods, showing its effectiveness. Finally, we\nanalyze the sensitivity of the multivariate algorithms to the algorithm\nhyper-parameters."}
{"id": "2507.10564", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10564", "abs": "https://arxiv.org/abs/2507.10564", "authors": ["Sameera Bharadwaja H.", "Siddhrath Jandial", "Shashank S. Agashe", "Rajesh Kumar Reddy Moore", "Youngkwan Kim"], "title": "Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing", "comment": null, "summary": "We consider the problem of tool-to-tool matching (TTTM), also called, chamber\nmatching in the context of a semiconductor manufacturing equipment. Traditional\nTTTM approaches utilize static configuration data or depend on a golden\nreference which are difficult to obtain in a commercial manufacturing line.\nFurther, existing methods do not extend very well to a heterogeneous setting,\nwhere equipment are of different make-and-model, sourced from different\nequipment vendors. We propose novel TTTM analysis pipelines to overcome these\nissues. We hypothesize that a mismatched equipment would have higher variance\nand/or higher number of modes in the data. Our best univariate method achieves\na correlation coefficient >0.95 and >0.5 with the variance and number of modes,\nrespectively showing that the proposed methods are effective. Also, the best\nmultivariate method achieves a correlation coefficient >0.75 with the\ntop-performing univariate methods, showing its effectiveness. Finally, we\nanalyze the sensitivity of the multivariate algorithms to the algorithm\nhyper-parameters."}
{"id": "2507.10581", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10581", "abs": "https://arxiv.org/abs/2507.10581", "authors": ["Esmail Gumaan"], "title": "Universal Approximation Theorem for a Single-Layer Transformer", "comment": "7 pages, 2 figures, 1 theorem, 10 formulas", "summary": "Deep learning employs multi-layer neural networks trained via the\nbackpropagation algorithm. This approach has achieved success across many\ndomains and relies on adaptive gradient methods such as the Adam optimizer.\nSequence modeling evolved from recurrent neural networks to attention-based\nmodels, culminating in the Transformer architecture. Transformers have achieved\nstate-of-the-art performance in natural language processing (for example, BERT\nand GPT-3) and have been applied in computer vision and computational biology.\nHowever, theoretical understanding of these models remains limited. In this\npaper, we examine the mathematical foundations of deep learning and\nTransformers and present a novel theoretical result. We review key concepts\nfrom linear algebra, probability, and optimization that underpin deep learning,\nand we analyze the multi-head self-attention mechanism and the backpropagation\nalgorithm in detail. Our main contribution is a universal approximation theorem\nfor Transformers: we prove that a single-layer Transformer, comprising one\nself-attention layer followed by a position-wise feed-forward network with ReLU\nactivation, can approximate any continuous sequence-to-sequence mapping on a\ncompact domain to arbitrary precision. We provide a formal statement and a\ncomplete proof. Finally, we present case studies that demonstrate the practical\nimplications of this result. Our findings advance the theoretical understanding\nof Transformer models and help bridge the gap between theory and practice."}
{"id": "2507.10714", "categories": ["cs.LG", "q-bio.QM", "stat.ML", "68, 92", "I.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.10714", "abs": "https://arxiv.org/abs/2507.10714", "authors": ["Bright Kwaku Manu", "Trevor Reckell", "Beckett Sterner", "Petar Jevtic"], "title": "A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models", "comment": "12 pages, 10 figures, for all associated codes and files, see\n  https://github.com/BrightManu-lang/SPN-param-recovery.git", "summary": "Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for\nmodeling discrete-event dynamics in areas such as epidemiology and systems\nbiology, yet their parameter estimation remains challenging in general and in\nparticular when transition rates depend on external covariates and explicit\nlikelihoods are unavailable. We introduce a neural-surrogate\n(neural-network--based approximation of the posterior distribution) framework\nthat predicts the coefficients of known covariate-dependent rate functions\ndirectly from noisy, partially observed token trajectories. Our model employs a\nlightweight 1D Convolutional Residual Network trained end-to-end on\nGillespie-simulated SPN realizations, learning to invert system dynamics under\nrealistic conditions of event dropout. During inference, Monte Carlo dropout\nprovides calibrated uncertainty bounds together with point estimates. On\nsynthetic SPNs with 20% missing events, our surrogate recovers rate-function\ncoefficients with an RMSE = 0.108 and substantially runs faster than\ntraditional Bayesian approaches. These results demonstrate that data-driven,\nlikelihood-free surrogates can enable accurate, robust, and real-time parameter\nrecovery in complex, partially observed discrete-event systems."}
{"id": "2507.10797", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10797", "abs": "https://arxiv.org/abs/2507.10797", "authors": ["Mohammad Pedramfar", "Siamak Ravanbakhsh"], "title": "Multi-Armed Sampling Problem and the End of Exploration", "comment": null, "summary": "This paper introduces the framework of multi-armed sampling, as the sampling\ncounterpart to the optimization problem of multi-arm bandits. Our primary\nmotivation is to rigorously examine the exploration-exploitation trade-off in\nthe context of sampling. We systematically define plausible notions of regret\nfor this framework and establish corresponding lower bounds. We then propose a\nsimple algorithm that achieves these optimal regret bounds. Our theoretical\nresults demonstrate that in contrast to optimization, sampling does not require\nexploration. To further connect our findings with those of multi-armed bandits,\nwe define a continuous family of problems and associated regret measures that\nsmoothly interpolates and unifies multi-armed sampling and multi-armed bandit\nproblems using a temperature parameter. We believe the multi-armed sampling\nframework, and our findings in this setting can have a foundational role in the\nstudy of sampling including recent neural samplers, akin to the role of\nmulti-armed bandits in reinforcement learning. In particular, our work sheds\nlight on the need for exploration and the convergence properties of algorithm\nfor entropy-regularized reinforcement learning, fine-tuning of pretrained\nmodels and reinforcement learning with human feedback (RLHF)."}
{"id": "2507.11274", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11274", "abs": "https://arxiv.org/abs/2507.11274", "authors": ["Amit Attia", "Matan Schliserman", "Uri Sherman", "Tomer Koren"], "title": "Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime", "comment": "27 pages", "summary": "We study population convergence guarantees of stochastic gradient descent\n(SGD) for smooth convex objectives in the interpolation regime, where the noise\nat optimum is zero or near zero. The behavior of the last iterate of SGD in\nthis setting -- particularly with large (constant) stepsizes -- has received\ngrowing attention in recent years due to implications for the training of\nover-parameterized models, as well as to analyzing forgetting in continual\nlearning and to understanding the convergence of the randomized Kaczmarz method\nfor solving linear systems. We establish that after $T$ steps of SGD on\n$\\beta$-smooth convex loss functions with stepsize $\\eta \\leq 1/\\beta$, the\nlast iterate exhibits expected excess risk $\\widetilde{O}(1/(\\eta\nT^{1-\\beta\\eta/2}) + \\eta T^{\\beta\\eta/2} \\sigma_\\star^2)$, where\n$\\sigma_\\star^2$ denotes the variance of the stochastic gradients at the\noptimum. In particular, for a well-tuned stepsize we obtain a near optimal\n$\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt{T})$ rate for the last iterate,\nextending the results of Varre et al. (2021) beyond least squares regression;\nand when $\\sigma_\\star=0$ we obtain a rate of $O(1/\\sqrt{T})$ with\n$\\eta=1/\\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently\nestablished by Evron et al. (2025) in the special case of realizable linear\nregression."}
{"id": "2507.11357", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11357", "abs": "https://arxiv.org/abs/2507.11357", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "title": "Neurosymbolic Reasoning Shortcuts under the Independence Assumption", "comment": "Accepted at NeSy 2025", "summary": "The ubiquitous independence assumption among symbolic concepts in\nneurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors\nuse it to speed up probabilistic reasoning. Recent works like van Krieken et\nal. (2024) and Marconato et al. (2024) argued that the independence assumption\ncan hinder learning of NeSy predictors and, more crucially, prevent them from\ncorrectly modelling uncertainty. There is, however, scepticism in the NeSy\ncommunity around the scenarios in which the independence assumption actually\nlimits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle\nthis question by formally showing that assuming independence among symbolic\nconcepts entails that a model can never represent uncertainty over certain\nconcept combinations. Thus, the model fails to be aware of reasoning shortcuts,\ni.e., the pathological behaviour of NeSy predictors that predict correct\ndownstream tasks but for the wrong reasons."}
{"id": "2507.10564", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10564", "abs": "https://arxiv.org/abs/2507.10564", "authors": ["Sameera Bharadwaja H.", "Siddhrath Jandial", "Shashank S. Agashe", "Rajesh Kumar Reddy Moore", "Youngkwan Kim"], "title": "Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing", "comment": null, "summary": "We consider the problem of tool-to-tool matching (TTTM), also called, chamber\nmatching in the context of a semiconductor manufacturing equipment. Traditional\nTTTM approaches utilize static configuration data or depend on a golden\nreference which are difficult to obtain in a commercial manufacturing line.\nFurther, existing methods do not extend very well to a heterogeneous setting,\nwhere equipment are of different make-and-model, sourced from different\nequipment vendors. We propose novel TTTM analysis pipelines to overcome these\nissues. We hypothesize that a mismatched equipment would have higher variance\nand/or higher number of modes in the data. Our best univariate method achieves\na correlation coefficient >0.95 and >0.5 with the variance and number of modes,\nrespectively showing that the proposed methods are effective. Also, the best\nmultivariate method achieves a correlation coefficient >0.75 with the\ntop-performing univariate methods, showing its effectiveness. Finally, we\nanalyze the sensitivity of the multivariate algorithms to the algorithm\nhyper-parameters."}
{"id": "2507.11367", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11367", "abs": "https://arxiv.org/abs/2507.11367", "authors": ["Daniel Tanneberg"], "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning", "comment": "accepted at the European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Training neural networks with reinforcement learning (RL) typically relies on\nbackpropagation (BP), necessitating storage of activations from the forward\npass for subsequent backward updates. Furthermore, backpropagating error\nsignals through multiple layers often leads to vanishing or exploding\ngradients, which can degrade learning performance and stability. We propose a\nnovel approach that trains each layer of the neural network using local signals\nduring the forward pass in RL settings. Our approach introduces local,\nlayer-wise losses leveraging the principle of matching pairwise distances from\nmulti-dimensional scaling, enhanced with optional reward-driven guidance. This\nmethod allows each hidden layer to be trained using local signals computed\nduring forward propagation, thus eliminating the need for backward passes and\nstoring intermediate activations. Our experiments, conducted with policy\ngradient methods across common RL benchmarks, demonstrate that this\nbackpropagation-free method achieves competitive performance compared to their\nclassical BP-based counterpart. Additionally, the proposed method enhances\nstability and consistency within and across runs, and improves performance\nespecially in challenging environments."}
{"id": "2507.10574", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.10574", "abs": "https://arxiv.org/abs/2507.10574", "authors": ["Jae Wan Shim"], "title": "Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance", "comment": "13 pages, 2 figures", "summary": "We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel\nmeasure derived from the information theory. In comparison to the standard\ncross entropy loss function, the proposed one has an additional term that\ndepends on the predicted probability of the true class. This feature serves to\nenhance the optimization process in classification tasks involving one-hot\nencoded class labels. The proposed one has been evaluated on a ResNet-based\nmodel using the CIFAR-100 dataset. Preliminary results show that the proposed\none consistently outperforms the standard cross entropy loss function in terms\nof classification accuracy. Moreover, the proposed one maintains simplicity,\nachieving practically the same efficiency to the traditional cross entropy\nloss. These findings suggest that our approach could broaden the scope for\nfuture research into loss function design."}
{"id": "2507.10575", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10575", "abs": "https://arxiv.org/abs/2507.10575", "authors": ["Kieran Chai Kai Ren"], "title": "An Adaptive Volatility-based Learning Rate Scheduler", "comment": null, "summary": "Effective learning rate (LR) scheduling is crucial for training deep neural\nnetworks. However, popular pre-defined and adaptive schedulers can still lead\nto suboptimal generalization. This paper introduces VolSched, a novel adaptive\nLR scheduler inspired by the concept of volatility in stochastic processes like\nGeometric Brownian Motion to dynamically adjust the learning rate. By\ncalculating the ratio between long-term and short-term accuracy volatility,\nVolSched increases the LR to escape plateaus and decreases it to stabilize\ntraining, allowing the model to explore the loss landscape more effectively. We\nevaluate VolSched on the CIFAR-100 dataset against a strong baseline using a\nstandard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our\nscheduler delivers consistent performance gains, improving top-1 accuracy by\n1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals\nthat VolSched promotes a longer exploration phase. A quantitative analysis of\nthe Hessian shows that VolSched finds a final solution that is 38% flatter than\nthe next-best baseline, allowing the model to obtain wider minima and hence\nbetter generalization performance."}
{"id": "2507.10581", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10581", "abs": "https://arxiv.org/abs/2507.10581", "authors": ["Esmail Gumaan"], "title": "Universal Approximation Theorem for a Single-Layer Transformer", "comment": "7 pages, 2 figures, 1 theorem, 10 formulas", "summary": "Deep learning employs multi-layer neural networks trained via the\nbackpropagation algorithm. This approach has achieved success across many\ndomains and relies on adaptive gradient methods such as the Adam optimizer.\nSequence modeling evolved from recurrent neural networks to attention-based\nmodels, culminating in the Transformer architecture. Transformers have achieved\nstate-of-the-art performance in natural language processing (for example, BERT\nand GPT-3) and have been applied in computer vision and computational biology.\nHowever, theoretical understanding of these models remains limited. In this\npaper, we examine the mathematical foundations of deep learning and\nTransformers and present a novel theoretical result. We review key concepts\nfrom linear algebra, probability, and optimization that underpin deep learning,\nand we analyze the multi-head self-attention mechanism and the backpropagation\nalgorithm in detail. Our main contribution is a universal approximation theorem\nfor Transformers: we prove that a single-layer Transformer, comprising one\nself-attention layer followed by a position-wise feed-forward network with ReLU\nactivation, can approximate any continuous sequence-to-sequence mapping on a\ncompact domain to arbitrary precision. We provide a formal statement and a\ncomplete proof. Finally, we present case studies that demonstrate the practical\nimplications of this result. Our findings advance the theoretical understanding\nof Transformer models and help bridge the gap between theory and practice."}
{"id": "2507.10591", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.PF", "68T01", "I.2"], "pdf": "https://arxiv.org/pdf/2507.10591", "abs": "https://arxiv.org/abs/2507.10591", "authors": ["Vanderson Rocha", "Diego Kreutz", "Gabriel Canto", "Hendrio Bragança", "Eduardo Feitosa"], "title": "MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation", "comment": "11 pages; 4 figures; 5 tables; submitted to JBCS", "summary": "Feature selection is vital for building effective predictive models, as it\nreduces dimensionality and emphasizes key features. However, current research\noften suffers from limited benchmarking and reliance on proprietary datasets.\nThis severely hinders reproducibility and can negatively impact overall\nperformance. To address these limitations, we introduce the MH-FSF framework, a\ncomprehensive, modular, and extensible platform designed to facilitate the\nreproduction and implementation of feature selection methods. Developed through\ncollaborative research, MH-FSF provides implementations of 17 methods (11\nclassical, 6 domain-specific) and enables systematic evaluation on 10 publicly\navailable Android malware datasets. Our results reveal performance variations\nacross both balanced and imbalanced datasets, highlighting the critical need\nfor data preprocessing and selection criteria that account for these\nasymmetries. We demonstrate the importance of a unified platform for comparing\ndiverse feature selection techniques, fostering methodological consistency and\nrigor. By providing this framework, we aim to significantly broaden the\nexisting literature and pave the way for new research directions in feature\nselection, particularly within the context of Android malware detection."}
{"id": "2507.10594", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10594", "abs": "https://arxiv.org/abs/2507.10594", "authors": ["Shengda Zhuo", "Di Wu", "Yi He", "Shuqiang Huang", "Xindong Wu"], "title": "Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features", "comment": null, "summary": "Online learning, where feature spaces can change over time, offers a flexible\nlearning paradigm that has attracted considerable attention. However, it still\nfaces three significant challenges. First, the heterogeneity of real-world data\nstreams with mixed feature types presents challenges for traditional parametric\nmodeling. Second, data stream distributions can shift over time, causing an\nabrupt and substantial decline in model performance. Third, it is often\ninfeasible to label every data instance due to time and cost constraints. To\naddress these issues, we proposed OL-MDISF (Online Learning from Mix-typed,\nDrifted, and Incomplete Streaming Features), which constructs a latent\ncopula-based representation for heterogeneous features, detects drifts via\nensemble entropy and latent mismatch, and performs structure-aware\npseudo-labeling.\n  This companion paper serves as a standalone technical reference to OL-MDISF.\nIt provides a contextual discussion of related work in mixed-type modeling,\ndrift adaptation, and weak supervision, as well as a comprehensive set of\nexperiments across 14 real-world datasets under two types of drift scenarios.\nThese include CER trends, ablation studies, sensitivity analyses, and temporal\nensemble dynamics. We hope this document offers a reproducible benchmark for\nonline learning on complex, weakly supervised streaming data."}
{"id": "2507.10595", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10595", "abs": "https://arxiv.org/abs/2507.10595", "authors": ["Yaowen Hu", "Wenxuan Tu", "Yue Liu", "Miaomiao Li", "Wenpeng Lu", "Zhigang Luo", "Xinwang Liu", "Ping Chen"], "title": "Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs", "comment": null, "summary": "Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised\ntask aimed at partitioning nodes with incomplete attributes into distinct\nclusters. Addressing this challenging issue is vital for practical\napplications. However, research in this area remains underexplored. Existing\nimputation methods for attribute-missing graphs often fail to account for the\nvarying amounts of information available across node neighborhoods, leading to\nunreliable results, especially for nodes with insufficient known neighborhood.\nTo address this issue, we propose a novel method named Divide-Then-Rule Graph\nCompletion (DTRGC). This method first addresses nodes with sufficient known\nneighborhood information and treats the imputed results as new knowledge to\niteratively impute more challenging nodes, while leveraging clustering\ninformation to correct imputation errors. Specifically, Dynamic Cluster-Aware\nFeature Propagation (DCFP) initializes missing node attributes by adjusting\npropagation weights based on the clustering structure. Subsequently,\nHierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing\nnodes into three groups based on the completeness of their neighborhood\nattributes. The imputation is performed hierarchically, prioritizing the groups\nwith nodes that have the most available neighborhood information. The cluster\nstructure is then used to refine the imputation and correct potential errors.\nFinally, Hop-wise Representation Enhancement (HRE) integrates information\nacross multiple hops, thereby enriching the expressiveness of node\nrepresentations. Experimental results on six widely used graph datasets show\nthat DTRGC significantly improves the clustering performance of various DGC\nmethods under attribute-missing graphs."}
{"id": "2507.10605", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.10605", "abs": "https://arxiv.org/abs/2507.10605", "authors": ["Fei Zhao", "Chonggang Lu", "Yue Wang", "Zheyong Xie", "Ziyan Liu", "Haofu Qian", "JianZhao Huang", "Fangcheng Shi", "Zijie Meng", "Hongcheng Guo", "Mingqian He", "Xinze Lyu", "Yiming Lu", "Ziyang Xiang", "Zheyu Ye", "Chengqiang Lu", "Zhe Xu", "Yi Wu", "Yao Hu", "Yan Gao", "Jun Fan", "Xiaolong Jiang", "Weiting Liu", "Boyang Wang", "Shaosheng Cao"], "title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services", "comment": null, "summary": "As a primary medium for modern information dissemination, social networking\nservices (SNS) have experienced rapid growth, which has proposed significant\nchallenges for platform content management and interaction quality improvement.\nRecently, the development of large language models (LLMs) has offered potential\nsolutions but existing studies focus on isolated tasks, which not only\nencounter diminishing benefit from the data scaling within individual scenarios\nbut also fail to flexibly adapt to diverse real-world context. To address these\nchallenges, we introduce RedOne, a domain-specific LLM designed to break the\nperformance bottleneck of single-task baselines and establish a comprehensive\nfoundation for the SNS. RedOne was developed through a three-stage training\nstrategy consisting of continue pretraining, supervised fine-tuning, and\npreference optimization, using a large-scale real-world dataset. Through\nextensive experiments, RedOne maintains strong general capabilities, and\nachieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%\nin SNS bilingual evaluation benchmark, compared with base models. Furthermore,\nthrough online testing, RedOne reduced the exposure rate in harmful content\ndetection by 11.23% and improved the click page rate in post-view search by\n14.95% compared with single-tasks finetuned baseline models. These results\nestablish RedOne as a robust domain-specific LLM for SNS, demonstrating\nexcellent generalization across various tasks and promising applicability in\nreal-world scenarios."}
{"id": "2507.10606", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.10606", "abs": "https://arxiv.org/abs/2507.10606", "authors": ["Bing-Yue Wu", "Vidya A. Chhabria"], "title": "DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design", "comment": "Under review at Asia and South Pacific Design Automation Conference\n  (ASP-DAC'26)", "summary": "Machine learning (ML) has demonstrated significant promise in various\nphysical design (PD) tasks. However, model generalizability remains limited by\nthe availability of high-quality, large-scale training datasets. Creating such\ndatasets is often computationally expensive and constrained by IP. While very\nfew public datasets are available, they are typically static, slow to generate,\nand require frequent updates. To address these limitations, we present DALI-PD,\na scalable framework for generating synthetic layout heatmaps to accelerate ML\nin PD research. DALI-PD uses a diffusion model to generate diverse layout\nheatmaps via fast inference in seconds. The heatmaps include power, IR drop,\ncongestion, macro placement, and cell density maps. Using DALI-PD, we created a\ndataset comprising over 20,000 layout configurations with varying macro counts\nand placements. These heatmaps closely resemble real layouts and improve ML\naccuracy on downstream ML tasks such as IR drop or congestion prediction."}
{"id": "2507.10609", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10609", "abs": "https://arxiv.org/abs/2507.10609", "authors": ["Obumneme Nwafor", "Chioma Nwafor", "Amro Zakaria", "Nkechi Nwankwo"], "title": "A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights", "comment": null, "summary": "The United Arab Emirates (UAE) relies heavily on seawater desalination to\nmeet over 90% of its drinking water needs. Desalination processes are highly\nenergy intensive and account for approximately 15% of the UAE's electricity\nconsumption, contributing to over 22% of the country's energy-related CO2\nemissions. Moreover, these processes face significant sustainability challenges\nin the face of climate uncertainties such as rising seawater temperatures,\nsalinity, and aerosol optical depth (AOD). AOD greatly affects the operational\nand economic performance of solar-powered desalination systems through\nphotovoltaic soiling, membrane fouling, and water turbidity cycles.\n  This study proposes a novel pipelined two-stage predictive modelling\narchitecture: the first stage forecasts AOD using satellite-derived time series\nand meteorological data; the second stage uses the predicted AOD and other\nmeteorological factors to predict desalination performance efficiency losses.\nThe framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)\nwas used to reveal key drivers of system degradation. Furthermore, this study\nproposes a dust-aware rule-based control logic for desalination systems based\non predicted values of AOD and solar efficiency. This control logic is used to\nadjust the desalination plant feed water pressure, adapt maintenance\nscheduling, and regulate energy source switching.\n  To enhance the practical utility of the research findings, the predictive\nmodels and rule-based controls were packaged into an interactive dashboard for\nscenario and predictive analytics. This provides a management decision-support\nsystem for climate-adaptive planning."}
{"id": "2507.10611", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10611", "abs": "https://arxiv.org/abs/2507.10611", "authors": ["Mengwen Ye", "Yingzi Huangfu", "Shujian Gao", "Wei Ren", "Weifan Liu", "Zekuan Yu"], "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise", "comment": null, "summary": "Federated Learning (FL) emerged as a solution for collaborative medical image\nclassification while preserving data privacy. However, label noise, which\narises from inter-institutional data variability, can cause training\ninstability and degrade model performance. Existing FL methods struggle with\nnoise heterogeneity and the imbalance in medical data. Motivated by these\nchallenges, we propose FedGSCA, a novel framework for enhancing robustness in\nnoisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates\nnoise knowledge from all clients, effectively addressing noise heterogeneity\nand improving global model stability. Furthermore, we develop a Client Adaptive\nAdjustment (CAA) mechanism that combines adaptive threshold pseudo-label\ngeneration and Robust Credal Labeling Loss. CAA dynamically adjusts to class\ndistributions, ensuring the inclusion of minority samples and carefully\nmanaging noisy labels by considering multiple plausible labels. This dual\napproach mitigates the impact of noisy data and prevents overfitting during\nlocal training, which improves the generalizability of the model. We evaluate\nFedGSCA on one real-world colon slides dataset and two synthetic medical\ndatasets under various noise conditions, including symmetric, asymmetric,\nextreme, and heterogeneous types. The results show that FedGSCA outperforms the\nstate-of-the-art methods, excelling in extreme and heterogeneous noise\nscenarios. Moreover, FedGSCA demonstrates significant advantages in improving\nmodel stability and handling complex noise, making it well-suited for\nreal-world medical federated learning scenarios."}
{"id": "2507.10613", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10613", "abs": "https://arxiv.org/abs/2507.10613", "authors": ["Zhengyu Chen", "Siqi Wang", "Teng Xiao", "Yudong Wang", "Shiqi Chen", "Xunliang Cai", "Junxian He", "Jingang Wang"], "title": "Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs", "comment": null, "summary": "Traditional scaling laws in natural language processing suggest that\nincreasing model size and training data enhances performance. However, recent\nstudies reveal deviations, particularly in large language models, where\nperformance improvements decelerate, which is a phenomenon known as\nsub-scaling. This paper revisits these scaling laws by examining the impact of\ndata quality and training strategies on model performance. Through extensive\nempirical analysis of over 400 models, we identify high data density and\nnon-optimal resource allocation as key factors contributing to sub-scaling.\nHigh data density leads to diminishing returns due to redundant information,\nwhile optimal resource allocation is crucial for sustained performance\nimprovements. We propose a sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlighting the importance of data quality\nand diversity."}
{"id": "2507.10614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10614", "abs": "https://arxiv.org/abs/2507.10614", "authors": ["Fei Liu", "Rui Zhang", "Xi Lin", "Zhichao Lu", "Qingfu Zhang"], "title": "Fine-tuning Large Language Model for Automated Algorithm Design", "comment": null, "summary": "The integration of large language models (LLMs) into automated algorithm\ndesign has shown promising potential. A prevalent approach embeds LLMs within\nsearch routines to iteratively generate and refine candidate algorithms.\nHowever, most existing methods rely on off-the-shelf LLMs trained for general\ncoding tasks,leaving a key question open: Do we need LLMs specifically tailored\nfor algorithm design? If so, how can such LLMs be effectively obtained and how\nwell can they generalize across different algorithm design tasks? In this\npaper, we take a first step toward answering these questions by exploring\nfine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank\nbased (DAR) sampling strategy to balance training data diversity and quality,\nthen we leverage direct preference optimization to efficiently align LLM\noutputs with task objectives. Our experiments, conducted on\nLlama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm\ndesign tasks. Results suggest that finetuned LLMs can significantly outperform\ntheir off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and\nmatch the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,\nwe observe promising generalization: LLMs finetuned on specific algorithm\ndesign tasks also improve performance on related tasks with varying settings.\nThese findings highlight the value of task-specific adaptation for LLMs in\nalgorithm design and open new avenues for future research."}
{"id": "2507.10616", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10616", "abs": "https://arxiv.org/abs/2507.10616", "authors": ["Neel Rajani", "Aryo Pradipta Gema", "Seraphina Goldfarb-Tarrant", "Ivan Titov"], "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them", "comment": null, "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones."}
{"id": "2507.10618", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10618", "abs": "https://arxiv.org/abs/2507.10618", "authors": ["Peter Barnett"], "title": "Compute Requirements for Algorithmic Innovation in Frontier AI Models", "comment": null, "summary": "Algorithmic innovation in the pretraining of large language models has driven\na massive reduction in the total compute required to reach a given level of\ncapability. In this paper we empirically investigate the compute requirements\nfor developing algorithmic innovations. We catalog 36 pre-training algorithmic\ninnovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate\nboth the total FLOP used in development and the FLOP/s of the hardware\nutilized. Innovations using significant resources double in their requirements\neach year. We then use this dataset to investigate the effect of compute caps\non innovation. Our analysis suggests that compute caps alone are unlikely to\ndramatically slow AI algorithmic progress. Even stringent compute caps -- such\nas capping total operations to the compute used to train GPT-2 or capping\nhardware capacity to 8 H100 GPUs -- could still have allowed for half of the\ncataloged innovations."}
{"id": "2507.10619", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.10619", "abs": "https://arxiv.org/abs/2507.10619", "authors": ["Oluwaseyi Giwa", "Tobi Awodunmila", "Muhammad Ahmed Mohsin", "Ahsan Bilal", "Muhammad Ali Jamshed"], "title": "Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks", "comment": "5 pages, 6 figures, under review at IEEE Wireless Communications\n  Letters", "summary": "The dynamic allocation of spectrum in 5G / 6G networks is critical to\nefficient resource utilization. However, applying traditional deep\nreinforcement learning (DRL) is often infeasible due to its immense sample\ncomplexity and the safety risks associated with unguided exploration, which can\ncause severe network interference. To address these challenges, we propose a\nmeta-learning framework that enables agents to learn a robust initial policy\nand rapidly adapt to new wireless scenarios with minimal data. We implement\nthree meta-learning architectures, model-agnostic meta-learning (MAML),\nrecurrent neural network (RNN), and an attention-enhanced RNN, and evaluate\nthem against a non-meta-learning DRL algorithm, proximal policy optimization\n(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)\nenvironment. Our results show a clear performance gap. The attention-based\nmeta-learning agent reaches a peak mean network throughput of 48 Mbps, while\nthe PPO baseline decreased drastically to 10 Mbps. Furthermore, our method\nreduces SINR and latency violations by more than 50% compared to PPO. It also\nshows quick adaptation, with a fairness index 0.7, showing better resource\nallocation. This work proves that meta-learning is a very effective and safer\noption for intelligent control in complex wireless systems."}
{"id": "2507.10620", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10620", "abs": "https://arxiv.org/abs/2507.10620", "authors": ["Chenxi Liu", "Hao Miao", "Cheng Long", "Yan Zhao", "Ziyue Li", "Panos Kalnis"], "title": "LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions", "comment": "Accepted at SSTD 2025 (Tutorial). arXiv admin note: text overlap with\n  arXiv:2505.02583", "summary": "Large Language Models (LLMs) have emerged as a promising paradigm for time\nseries analytics, leveraging their massive parameters and the shared sequential\nnature of textual and time series data. However, a cross-modality gap exists\nbetween time series and textual data, as LLMs are pre-trained on textual\ncorpora and are not inherently optimized for time series. In this tutorial, we\nprovide an up-to-date overview of LLM-based cross-modal time series analytics.\nWe introduce a taxonomy that classifies existing approaches into three groups\nbased on cross-modal modeling strategies, e.g., conversion, alignment, and\nfusion, and then discuss their applications across a range of downstream tasks.\nIn addition, we summarize several open challenges. This tutorial aims to expand\nthe practical application of LLMs in solving real-world problems in cross-modal\ntime series analytics while balancing effectiveness and efficiency.\nParticipants will gain a thorough understanding of current advancements,\nmethodologies, and future research directions in cross-modal time series\nanalytics."}
{"id": "2507.10623", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10623", "abs": "https://arxiv.org/abs/2507.10623", "authors": ["Daniel Saragih", "Deyu Cao", "Tejas Balaji"], "title": "Flows and Diffusions on the Neural Manifold", "comment": "40 pages, 6 figures, 13 tables", "summary": "Diffusion and flow-based generative models have achieved remarkable success\nin domains such as image synthesis, video generation, and natural language\nmodeling. In this work, we extend these advances to weight space learning by\nleveraging recent techniques to incorporate structural priors derived from\noptimization dynamics. Central to our approach is modeling the trajectory\ninduced by gradient descent as a trajectory inference problem. We unify several\ntrajectory inference techniques under the framework of gradient flow matching,\nproviding a theoretical framework for treating optimization paths as inductive\nbias. We further explore architectural and algorithmic choices, including\nreward fine-tuning by adjoint matching, the use of autoencoders for latent\nweight representation, conditioning on task-specific context data, and adopting\ninformative source distributions such as Kaiming uniform. Experiments\ndemonstrate that our method matches or surpasses baselines in generating\nin-distribution weights, improves initialization for downstream training, and\nsupports fine-tuning to enhance performance. Finally, we illustrate a practical\napplication in safety-critical systems: detecting harmful covariate shifts,\nwhere our method outperforms the closest comparable baseline."}
{"id": "2507.10626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10626", "abs": "https://arxiv.org/abs/2507.10626", "authors": ["Lintao Wang", "Shiwen Xu", "Michael Horton", "Joachim Gudmundsson", "Zhiyong Wang"], "title": "Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction", "comment": null, "summary": "Predicting soccer match outcomes is a challenging task due to the inherently\nunpredictable nature of the game and the numerous dynamic factors influencing\nresults. While it conventionally relies on meticulous feature engineering, deep\nlearning techniques have recently shown a great promise in learning effective\nplayer and team representations directly for soccer outcome prediction.\nHowever, existing methods often overlook the heterogeneous nature of\ninteractions among players and teams, which is crucial for accurately modeling\nmatch dynamics. To address this gap, we propose HIGFormer (Heterogeneous\nInteraction Graph Transformer), a novel graph-augmented transformer-based deep\nlearning model for soccer outcome prediction. HIGFormer introduces a\nmulti-level interaction framework that captures both fine-grained player\ndynamics and high-level team interactions. Specifically, it comprises (1) a\nPlayer Interaction Network, which encodes player performance through\nheterogeneous interaction graphs, combining local graph convolutions with a\nglobal graph-augmented transformer; (2) a Team Interaction Network, which\nconstructs interaction graphs from a team-to-team perspective to model\nhistorical match relationships; and (3) a Match Comparison Transformer, which\njointly analyzes both team and player-level information to predict match\noutcomes. Extensive experiments on the WyScout Open Access Dataset, a\nlarge-scale real-world soccer dataset, demonstrate that HIGFormer significantly\noutperforms existing methods in prediction accuracy. Furthermore, we provide\nvaluable insights into leveraging our model for player performance evaluation,\noffering a new perspective on talent scouting and team strategy analysis."}
{"id": "2507.10628", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10628", "abs": "https://arxiv.org/abs/2507.10628", "authors": ["Ziru Liu", "Cheng Gong", "Xinyu Fu", "Yaofang Liu", "Ran Chen", "Shoubo Hu", "Suiyun Zhang", "Rui Liu", "Qingfu Zhang", "Dandan Tu"], "title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels."}
{"id": "2507.10632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10632", "abs": "https://arxiv.org/abs/2507.10632", "authors": ["Issei Saito", "Masatoshi Nagano", "Tomoaki Nakamura", "Daichi Mochihashi", "Koki Mimura"], "title": "Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process", "comment": null, "summary": "In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series\nsegmentation method that incorporates random Fourier features (RFF) to address\nthe high computational cost of the Gaussian process hidden semi-Markov model\n(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring\ninversion of an N times N kernel matrix during training, where N is the number\nof data points. As the scale of the data increases, matrix inversion incurs a\nsignificant computational cost. To address this, the proposed method\napproximates the Gaussian process with linear regression using RFF, preserving\nexpressive power while eliminating the need for inversion of the kernel matrix.\nExperiments on the Carnegie Mellon University (CMU) motion-capture dataset\ndemonstrate that the proposed method achieves segmentation performance\ncomparable to that of conventional methods, with approximately 278 times faster\nsegmentation on time-series data comprising 39,200 frames."}
{"id": "2507.10636", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.RO", "90B06", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.10636", "abs": "https://arxiv.org/abs/2507.10636", "authors": ["Jianing Zhi", "Xinghua Li", "Zidong Chen"], "title": "GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem", "comment": "12 Pages, 5 Figures", "summary": "The rapid development of urban low-altitude unmanned aerial vehicle (UAV)\neconomy poses new challenges for dynamic site selection of UAV landing points\nand supply stations. Traditional deep reinforcement learning methods face\ncomputational complexity bottlenecks, particularly with standard attention\nmechanisms, when handling large-scale urban-level location problems. This paper\nproposes GeoHopNet, a Hopfield-augmented sparse spatial attention network\nspecifically designed for dynamic UAV site location problems. Our approach\nintroduces four core innovations: (1) distance-biased multi-head attention\nmechanism that explicitly encodes spatial geometric information; (2) K-nearest\nneighbor sparse attention that reduces computational complexity from $O(N^2)$\nto $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory\nregularization strategy. Experimental results demonstrate that GeoHopNet\nextends the boundary of solvable problem sizes. For large-scale instances with\n1,000 nodes, where standard attention models become prohibitively slow (over 3\nseconds per instance) and traditional solvers fail, GeoHopNet finds\nhigh-quality solutions (0.22\\% optimality gap) in under 0.1 seconds. Compared\nto the state-of-the-art ADNet baseline on 100-node instances, our method\nimproves solution quality by 22.2\\% and is 1.8$\\times$ faster."}
{"id": "2507.10637", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10637", "abs": "https://arxiv.org/abs/2507.10637", "authors": ["É. Künzel", "A. Jaziri", "V. Ramesh"], "title": "A Simple Baseline for Stable and Plastic Neural Networks", "comment": "11 pages, 50 figures", "summary": "Continual learning in computer vision requires that models adapt to a\ncontinuous stream of tasks without forgetting prior knowledge, yet existing\napproaches often tip the balance heavily toward either plasticity or stability.\nWe introduce RDBP, a simple, low-overhead baseline that unites two\ncomplementary mechanisms: ReLUDown, a lightweight activation modification that\npreserves feature sensitivity while preventing neuron dormancy, and Decreasing\nBackpropagation, a biologically inspired gradient-scheduling scheme that\nprogressively shields early layers from catastrophic updates. Evaluated on the\nContinual ImageNet benchmark, RDBP matches or exceeds the plasticity and\nstability of state-of-the-art methods while reducing computational cost. RDBP\nthus provides both a practical solution for real-world continual learning and a\nclear benchmark against which future continual learning strategies can be\nmeasured."}
{"id": "2507.10638", "categories": ["cs.LG", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.10638", "abs": "https://arxiv.org/abs/2507.10638", "authors": ["Shim Soon Yong"], "title": "ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space", "comment": null, "summary": "We introduce a novel classification framework, ZClassifier, that replaces\nconventional deterministic logits with diagonal Gaussian-distributed logits.\nOur method simultaneously addresses temperature scaling and manifold\napproximation by minimizing the Kullback-Leibler (KL) divergence between the\npredicted Gaussian distributions and a unit isotropic Gaussian. This unifies\nuncertainty calibration and latent control in a principled probabilistic\nmanner, enabling a natural interpretation of class confidence and geometric\nconsistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier\nimproves over softmax classifiers in robustness, calibration, and latent\nseparation. We also demonstrate its effectiveness for classifier-guided\ngeneration by interpreting logits as Gaussian semantic potentials."}
{"id": "2507.10642", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10642", "abs": "https://arxiv.org/abs/2507.10642", "authors": ["Andrew Gascoyne", "Wendy Lomas"], "title": "First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network", "comment": "12 pages, 5 figures", "summary": "A growing issue within conservation bioacoustics is the task of analysing the\nvast amount of data generated from the use of passive acoustic monitoring\ndevices. In this paper, we present an alternative AI model which has the\npotential to help alleviate this problem. Our model formulation addresses the\nkey issues encountered when using current AI models for bioacoustic analysis,\nnamely the: limited training data available; environmental impact, particularly\nin energy consumption and carbon footprint of training and implementing these\nmodels; and associated hardware requirements. The model developed in this work\nuses associative memory via a transparent, explainable Hopfield neural network\nto store signals and detect similar signals which can then be used to classify\nspecies. Training is rapid ($3$\\,ms), as only one representative signal is\nrequired for each target sound within a dataset. The model is fast, taking only\n$5.4$\\,s to pre-process and classify all $10384$ publicly available bat\nrecordings, on a standard Apple MacBook Air. The model is also lightweight with\na small memory footprint of $144.09$\\,MB of RAM usage. Hence, the low\ncomputational demands make the model ideal for use on a variety of standard\npersonal devices with potential for deployment in the field via edge-processing\ndevices. It is also competitively accurate, with up to $86\\%$ precision on the\ndataset used to evaluate the model. In fact, we could not find a single case of\ndisagreement between model and manual identification via expert field guides.\nAlthough a dataset of bat echolocation calls was chosen to demo this\nfirst-of-its-kind AI model, trained on only two representative calls, the model\nis not species specific. In conclusion, we propose an equitable AI model that\nhas the potential to be a game changer for fast, lightweight, sustainable,\ntransparent, explainable and accurate bioacoustic analysis."}
{"id": "2507.10678", "categories": ["cs.LG", "cs.AI", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.10678", "abs": "https://arxiv.org/abs/2507.10678", "authors": ["Cutter Dawes", "Simon Segert", "Kamesh Krishnamurthy", "Jonathan D. Cohen"], "title": "A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks", "comment": "22 pages, 6 figures", "summary": "A major challenge in the use of neural networks both for modeling human\ncognitive function and for artificial intelligence is the design of systems\nwith the capacity to efficiently learn functions that support radical\ngeneralization. At the roots of this is the capacity to discover and implement\nsymmetry functions. In this paper, we investigate a paradigmatic example of\nradical generalization through the use of symmetry: base addition. We present a\ngroup theoretic analysis of base addition, a fundamental and defining\ncharacteristic of which is the carry function -- the transfer of the remainder,\nwhen a sum exceeds the base modulus, to the next significant place. Our\nanalysis exposes a range of alternative carry functions for a given base, and\nwe introduce quantitative measures to characterize these. We then exploit\ndifferences in carry functions to probe the inductive biases of neural networks\nin symmetry learning, by training neural networks to carry out base addition\nusing different carries, and comparing efficacy and rate of learning as a\nfunction of their structure. We find that even simple neural networks can\nachieve radical generalization with the right input format and carry function,\nand that learning speed is closely correlated with carry function structure. We\nthen discuss the relevance this has for cognitive science and machine learning."}
{"id": "2507.10714", "categories": ["cs.LG", "q-bio.QM", "stat.ML", "68, 92", "I.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.10714", "abs": "https://arxiv.org/abs/2507.10714", "authors": ["Bright Kwaku Manu", "Trevor Reckell", "Beckett Sterner", "Petar Jevtic"], "title": "A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models", "comment": "12 pages, 10 figures, for all associated codes and files, see\n  https://github.com/BrightManu-lang/SPN-param-recovery.git", "summary": "Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for\nmodeling discrete-event dynamics in areas such as epidemiology and systems\nbiology, yet their parameter estimation remains challenging in general and in\nparticular when transition rates depend on external covariates and explicit\nlikelihoods are unavailable. We introduce a neural-surrogate\n(neural-network--based approximation of the posterior distribution) framework\nthat predicts the coefficients of known covariate-dependent rate functions\ndirectly from noisy, partially observed token trajectories. Our model employs a\nlightweight 1D Convolutional Residual Network trained end-to-end on\nGillespie-simulated SPN realizations, learning to invert system dynamics under\nrealistic conditions of event dropout. During inference, Monte Carlo dropout\nprovides calibrated uncertainty bounds together with point estimates. On\nsynthetic SPNs with 20% missing events, our surrogate recovers rate-function\ncoefficients with an RMSE = 0.108 and substantially runs faster than\ntraditional Bayesian approaches. These results demonstrate that data-driven,\nlikelihood-free surrogates can enable accurate, robust, and real-time parameter\nrecovery in complex, partially observed discrete-event systems."}
{"id": "2507.10718", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10718", "abs": "https://arxiv.org/abs/2507.10718", "authors": ["Shuyao Li", "Ilias Diakonikolas", "Jelena Diakonikolas"], "title": "Distributionally Robust Optimization with Adversarial Data Contamination", "comment": null, "summary": "Distributionally Robust Optimization (DRO) provides a framework for\ndecision-making under distributional uncertainty, yet its effectiveness can be\ncompromised by outliers in the training data. This paper introduces a\nprincipled approach to simultaneously address both challenges. We focus on\noptimizing Wasserstein-1 DRO objectives for generalized linear models with\nconvex Lipschitz loss functions, where an $\\epsilon$-fraction of the training\ndata is adversarially corrupted. Our primary contribution lies in a novel\nmodeling framework that integrates robustness against training data\ncontamination with robustness against distributional shifts, alongside an\nefficient algorithm inspired by robust statistics to solve the resulting\noptimization problem. We prove that our method achieves an estimation error of\n$O(\\sqrt{\\epsilon})$ for the true DRO objective value using only the\ncontaminated data under the bounded covariance assumption. This work\nestablishes the first rigorous guarantees, supported by efficient computation,\nfor learning under the dual challenges of data contamination and distributional\nshifts."}
{"id": "2507.10741", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10741", "abs": "https://arxiv.org/abs/2507.10741", "authors": ["Andrew C. Li", "Toryn Q. Klassen", "Andrew Wang", "Parand A. Alamdari", "Sheila A. McIlraith"], "title": "Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language", "comment": null, "summary": "Grounding language in complex perception (e.g. pixels) and action is a key\nchallenge when building situated agents that can interact with humans via\nlanguage. In past works, this is often solved via manual design of the language\ngrounding or by curating massive datasets relating language to elements of the\nenvironment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for\ngrounding formal language from data, and eliciting behaviours by directly\ntasking RL agents through this language. By virtue of data-driven learning, our\nframework avoids the manual design of domain-specific elements like reward\nfunctions or symbol detectors. By virtue of compositional formal language\nsemantics, our framework achieves data-efficient grounding and generalization\nto arbitrary language compositions. Experiments on an image-based gridworld and\na MuJoCo robotics domain show that our approach reliably maps formal language\ninstructions to behaviours with limited data while end-to-end, data-driven\napproaches fail."}
{"id": "2507.10747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10747", "abs": "https://arxiv.org/abs/2507.10747", "authors": ["Kaustubh Tangsali", "Rishikesh Ranade", "Mohammad Amin Nabian", "Alexey Kamenev", "Peter Sharpe", "Neil Ashton", "Ram Cherukuri", "Sanjay Choudhry"], "title": "A Benchmarking Framework for AI models in Automotive Aerodynamics", "comment": null, "summary": "In this paper, we introduce a benchmarking framework within the open-source\nNVIDIA PhysicsNeMo-CFD framework designed to systematically assess the\naccuracy, performance, scalability, and generalization capabilities of AI\nmodels for automotive aerodynamics predictions. The open extensible framework\nenables incorporation of a diverse set of metrics relevant to the\nComputer-Aided Engineering (CAE) community. By providing a standardized\nmethodology for comparing AI models, the framework enhances transparency and\nconsistency in performance assessment, with the overarching goal of improving\nthe understanding and development of these models to accelerate research and\ninnovation in the field. To demonstrate its utility, the framework includes\nevaluation of both surface and volumetric flow field predictions on three AI\nmodels: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It\nalso includes guidelines for integrating additional models and datasets, making\nit extensible for physically consistent metrics. This benchmarking study aims\nto enable researchers and industry professionals in selecting, refining, and\nadvancing AI-driven aerodynamic modeling approaches, ultimately fostering the\ndevelopment of more efficient, accurate, and interpretable solutions in\nautomotive aerodynamics"}
{"id": "2507.10768", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10768", "abs": "https://arxiv.org/abs/2507.10768", "authors": ["Bart Pogodzinski", "Christopher Wewer", "Bernt Schiele", "Jan Eric Lenssen"], "title": "Spatial Reasoners for Continuous Variables in Any Domain", "comment": "For the project documentation see https://spatialreasoners.github.io/\n  . The SRM project website is available at\n  https://geometric-rl.mpi-inf.mpg.de/srm/ . The work was published on ICML\n  2025 CODEML workshop", "summary": "We present Spatial Reasoners, a software framework to perform spatial\nreasoning over continuous variables with generative denoising models. Denoising\ngenerative models have become the de-facto standard for image generation, due\nto their effectiveness in sampling from complex, high-dimensional\ndistributions. Recently, they have started being explored in the context of\nreasoning over multiple continuous variables. Providing infrastructure for\ngenerative reasoning with such models requires a high effort, due to a wide\nrange of different denoising formulations, samplers, and inference strategies.\nOur presented framework aims to facilitate research in this area, providing\neasy-to-use interfaces to control variable mapping from arbitrary data domains,\ngenerative model paradigms, and inference strategies. Spatial Reasoners are\nopenly available at https://spatialreasoners.github.io/"}
{"id": "2507.10792", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10792", "abs": "https://arxiv.org/abs/2507.10792", "authors": ["Yuchen Wang", "Hongjue Zhao", "Haohong Lin", "Enze Xu", "Lifang He", "Huajie Shao"], "title": "A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments", "comment": "8 pages, 6 figures, accepted in ICML 2025", "summary": "This work aims to address the problem of long-term dynamic forecasting in\ncomplex environments where data are noisy and irregularly sampled. While recent\nstudies have introduced some methods to improve prediction performance, these\napproaches still face a significant challenge in handling long-term\nextrapolation tasks under such complex scenarios. To overcome this challenge,\nwe propose Phy-SSM, a generalizable method that integrates partial physics\nknowledge into state space models (SSMs) for long-term dynamics forecasting in\ncomplex environments. Our motivation is that SSMs can effectively capture\nlong-range dependencies in sequential data and model continuous dynamical\nsystems, while the incorporation of physics knowledge improves generalization\nability. The key challenge lies in how to seamlessly incorporate partially\nknown physics into SSMs. To achieve this, we decompose partially known system\ndynamics into known and unknown state matrices, which are integrated into a\nPhy-SSM unit. To further enhance long-term prediction performance, we introduce\na physics state regularization term to make the estimated latent states align\nwith system dynamics. Besides, we theoretically analyze the uniqueness of the\nsolutions for our method. Extensive experiments on three real-world\napplications, including vehicle motion prediction, drone state prediction, and\nCOVID-19 epidemiology forecasting, demonstrate the superior performance of\nPhy-SSM over the baselines in both long-term interpolation and extrapolation\ntasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025."}
{"id": "2507.10797", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10797", "abs": "https://arxiv.org/abs/2507.10797", "authors": ["Mohammad Pedramfar", "Siamak Ravanbakhsh"], "title": "Multi-Armed Sampling Problem and the End of Exploration", "comment": null, "summary": "This paper introduces the framework of multi-armed sampling, as the sampling\ncounterpart to the optimization problem of multi-arm bandits. Our primary\nmotivation is to rigorously examine the exploration-exploitation trade-off in\nthe context of sampling. We systematically define plausible notions of regret\nfor this framework and establish corresponding lower bounds. We then propose a\nsimple algorithm that achieves these optimal regret bounds. Our theoretical\nresults demonstrate that in contrast to optimization, sampling does not require\nexploration. To further connect our findings with those of multi-armed bandits,\nwe define a continuous family of problems and associated regret measures that\nsmoothly interpolates and unifies multi-armed sampling and multi-armed bandit\nproblems using a temperature parameter. We believe the multi-armed sampling\nframework, and our findings in this setting can have a foundational role in the\nstudy of sampling including recent neural samplers, akin to the role of\nmulti-armed bandits in reinforcement learning. In particular, our work sheds\nlight on the need for exploration and the convergence properties of algorithm\nfor entropy-regularized reinforcement learning, fine-tuning of pretrained\nmodels and reinforcement learning with human feedback (RLHF)."}
{"id": "2507.10809", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10809", "abs": "https://arxiv.org/abs/2507.10809", "authors": ["Kazi Tasnim Zinat", "Yun Zhou", "Xiang Lyu", "Yawei Wang", "Zhicheng Liu", "Panpan Xu"], "title": "Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions", "comment": "Accepted at ICANN 2025", "summary": "Inferring causal relationships between event pairs in a temporal sequence is\napplicable in many domains such as healthcare, manufacturing, and\ntransportation. Most existing work on causal inference primarily focuses on\nevent types within the designated domain, without considering the impact of\nexogenous out-of-domain interventions. In real-world settings, these\nout-of-domain interventions can significantly alter causal dynamics. To address\nthis gap, we propose a new causal framework to define average treatment effect\n(ATE), beyond independent and identically distributed (i.i.d.) data in classic\nRubin's causal framework, to capture the causal relation shift between events\nof temporal process under out-of-domain intervention. We design an unbiased ATE\nestimator, and devise a Transformer-based neural network model to handle both\nlong-range temporal dependencies and local patterns while integrating\nout-of-domain intervention information into process modeling. Extensive\nexperiments on both simulated and real-world datasets demonstrate that our\nmethod outperforms baselines in ATE estimation and goodness-of-fit under\nout-of-domain-augmented point processes."}
{"id": "2507.10820", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10820", "abs": "https://arxiv.org/abs/2507.10820", "authors": ["Robert Müller"], "title": "Semantic Context for Tool Orchestration", "comment": "Workshop on Computer Use Agents @ ICML2025", "summary": "This paper demonstrates that Semantic Context (SC), leveraging descriptive\ntool information, is a foundational component for robust tool orchestration.\nOur contributions are threefold. First, we provide a theoretical foundation\nusing contextual bandits, introducing SC-LinUCB and proving it achieves lower\nregret and adapts favourably in dynamic action spaces. Second, we provide\nparallel empirical validation with Large Language Models, showing that SC is\ncritical for successful in-context learning in both static (efficient learning)\nand non-stationary (robust adaptation) settings. Third, we propose the FiReAct\npipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based\nretrieval enables an LLM to effectively orchestrate over a large action space.\nThese findings provide a comprehensive guide to building more sample-efficient,\nadaptive, and scalable orchestration agents."}
{"id": "2507.10834", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10834", "abs": "https://arxiv.org/abs/2507.10834", "authors": ["Guokai Li", "Pin Gao", "Stefanus Jasin", "Zizhuo Wang"], "title": "From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems", "comment": "Conference version. The journal version will be updated soon", "summary": "Assortment optimization involves selecting a subset of substitutable products\n(subject to certain constraints) to maximize the expected revenue. It is a\nclassic problem in revenue management and finds applications across various\nindustries. However, the problem is usually NP-hard due to its combinatorial\nand non-linear nature. In this work, we explore how graph concolutional\nnetworks (GCNs) can be leveraged to efficiently solve constrained assortment\noptimization under the mixed multinomial logit choice model. We first develop a\ngraph representation of the assortment problem, then train a GCN to learn the\npatterns of optimal assortments, and lastly propose two inference policies\nbased on the GCN's output. Due to the GCN's inherent ability to generalize\nacross inputs of varying sizes, we can use a GCN trained on small-scale\ninstances to facilitate large-scale instances. Extensive numerical experiments\ndemonstrate that given a GCN trained on small-scale instances (e.g., with 20\nproducts), the proposed policies can achieve superior performance (90%+\noptimality) on large-scale instances (with up to 2,000 products) within\nseconds, which outperform existing heuristic policies in both performance and\nefficiency. Furthermore, we extend our framework to a model-free setting where\nthe underlying choice model is unknown but transaction data is available. We\nalso conduct numerical experiments to demonstrate the effectiveness and\nefficiency of our proposed policies in this setting."}
{"id": "2507.10843", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10843", "abs": "https://arxiv.org/abs/2507.10843", "authors": ["Motoki Omura", "Yusuke Mukuta", "Kazuki Ota", "Takayuki Osa", "Tatsuya Harada"], "title": "Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps", "comment": "Accepted at RLC 2025", "summary": "Offline reinforcement learning (RL) aims to learn an optimal policy from a\nstatic dataset, making it particularly valuable in scenarios where data\ncollection is costly, such as robotics. A major challenge in offline RL is\ndistributional shift, where the learned policy deviates from the dataset\ndistribution, potentially leading to unreliable out-of-distribution actions. To\nmitigate this issue, regularization techniques have been employed. While many\nexisting methods utilize density ratio-based measures, such as the\n$f$-divergence, for regularization, we propose an approach that utilizes the\nWasserstein distance, which is robust to out-of-distribution data and captures\nthe similarity between actions. Our method employs input-convex neural networks\n(ICNNs) to model optimal transport maps, enabling the computation of the\nWasserstein distance in a discriminator-free manner, thereby avoiding\nadversarial training and ensuring stable learning. Our approach demonstrates\ncomparable or superior performance to widely used existing methods on the D4RL\nbenchmark dataset. The code is available at\nhttps://github.com/motokiomura/Q-DOT ."}
{"id": "2507.10861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10861", "abs": "https://arxiv.org/abs/2507.10861", "authors": ["Edoardo Pinzuti", "Oliver Tüscher", "André Ferreira Castro"], "title": "Visually grounded emotion regulation via diffusion models and user-driven reappraisal", "comment": null, "summary": "Cognitive reappraisal is a key strategy in emotion regulation, involving\nreinterpretation of emotionally charged stimuli to alter affective responses.\nDespite its central role in clinical and cognitive science, real-world\nreappraisal interventions remain cognitively demanding, abstract, and primarily\nverbal. This reliance on higher-order cognitive and linguistic processes is\noften impaired in individuals with trauma or depression, limiting the\neffectiveness of standard approaches. Here, we propose a novel, visually based\naugmentation of cognitive reappraisal by integrating large-scale text-to-image\ndiffusion models into the emotional regulation process. Specifically, we\nintroduce a system in which users reinterpret emotionally negative images via\nspoken reappraisals, which are transformed into supportive, emotionally\ncongruent visualizations using stable diffusion models with a fine-tuned\nIP-adapter. This generative transformation visually instantiates users'\nreappraisals while maintaining structural similarity to the original stimuli,\nexternalizing and reinforcing regulatory intent. To test this approach, we\nconducted a within-subject experiment (N = 20) using a modified cognitive\nemotion regulation (CER) task. Participants reappraised or described aversive\nimages from the International Affective Picture System (IAPS), with or without\nAI-generated visual feedback. Results show that AI-assisted reappraisal\nsignificantly reduced negative affect compared to both non-AI and control\nconditions. Further analyses reveal that sentiment alignment between\nparticipant reappraisals and generated images correlates with affective relief,\nsuggesting that multimodal coherence enhances regulatory efficacy. These\nfindings demonstrate that generative visual input can support cogitive\nreappraisal and open new directions at the intersection of generative AI,\naffective computing, and therapeutic technology."}
{"id": "2507.10871", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.10871", "abs": "https://arxiv.org/abs/2507.10871", "authors": ["Tsung Yeh Hsieh", "Yongjie Jessica Zhang"], "title": "GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport", "comment": null, "summary": "Neurons exhibit intricate geometries within their neurite networks, which\nplay a crucial role in processes such as signaling and nutrient transport.\nAccurate simulation of material transport in the networks is essential for\nunderstanding these biological phenomena but poses significant computational\nchallenges because of the complex tree-like structures involved. Traditional\napproaches are time-intensive and resource-demanding, yet the inherent\nproperties of neuron trees, which consists primarily of pipes with steady-state\nparabolic velocity profiles and bifurcations, provide opportunities for\ncomputational optimization. To address these challenges, we propose a\nGraph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is\nspecifically designed to streamline the simulation of material transport in\nneural trees. GALDS employs a graph autoencoder to encode latent\nrepresentations of the network's geometry, velocity fields, and concentration\nprofiles. These latent space representations are then assembled into a global\ngraph, which is subsequently used to predict system dynamics in the latent\nspace via a trained graph latent space system dynamic model, inspired by the\nNeural Ordinary Differential Equations (Neural ODEs) concept. The integration\nof an autoencoder allows for the use of smaller graph neural network models\nwith reduced training data requirements. Furthermore, the Neural ODE component\neffectively mitigates the issue of error accumulation commonly encountered in\nrecurrent neural networks. The effectiveness of the GALDS model is demonstrated\nthrough results on eight unseen geometries and four abnormal transport\nexamples, where our approach achieves mean relative error of 3% with maximum\nrelative error <8% and demonstrates a 10-fold speed improvement compared to\nprevious surrogate model approaches."}
{"id": "2507.10880", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10880", "abs": "https://arxiv.org/abs/2507.10880", "authors": ["Souvik Nath", "Sumit Wadhwa", "Luiz Perez"], "title": "Domain-Adaptive Small Language Models for Structured Tax Code Prediction", "comment": "10 pages, 3 figures", "summary": "Every day, multinational firms process thousands of transactions, each of\nwhich must adhere to tax regulations that vary by jurisdiction and are often\nnuanced. The determination of product and service tax codes, such as HSN or SAC\nis a major use case in Tax compliance. An accurate determination of such codes\nis imperative to avoid any tax penalties. This paper proposes a domain-adaptive\nsmall language model (SLM) with an encoder-decoder architecture for the\nenhanced prediction of product and service tax codes. In this approach, we\naddress the problem of predicting hierarchical tax code sequences using\nunstructured product and services data. We employ an SLM based upon\nencoder-decoder architecture as this enables sequential generation of tax codes\nto capture the hierarchical dependencies present within the tax codes. Our\nexperiments demonstrate that encoder-decoder SLMs can be successfully applied\nto the sequential prediction of structured tax codes, a domain that remains\ncomparatively unexplored in current NLP research. In this paper, we demonstrate\nthe superior performance of the domain-adaptive encoder-decoder SLMs over flat\nclassifiers when applied to the Harmonized System of Nomenclature (HSN), and\nachieve superior results compared to decoder-only and encoder-only\narchitectures for structured sequence generation tasks. This approach can also\nbe scaled to other government-mandated tax commodity codes, such as United\nNations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura\nComum do Mercosul (NCM)."}
{"id": "2507.10884", "categories": ["cs.LG", "math.DS", "68T07, 68T05, 70G60"], "pdf": "https://arxiv.org/pdf/2507.10884", "abs": "https://arxiv.org/abs/2507.10884", "authors": ["Hyunwoo Cho", "Hyeontae Jo", "Hyung Ju Hwang"], "title": "Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model", "comment": null, "summary": "System inference for nonlinear dynamic models, represented by ordinary\ndifferential equations (ODEs), remains a significant challenge in many fields,\nparticularly when the data are noisy, sparse, or partially observable. In this\npaper, we propose a Simulation-based Generative Model for Imperfect Data\n(SiGMoID) that enables precise and robust inference for dynamic systems. The\nproposed approach integrates two key methods: (1) physics-informed neural\nnetworks with hyper-networks that constructs an ODE solver, and (2) Wasserstein\ngenerative adversarial networks that estimates ODE parameters by effectively\ncapturing noisy data distributions. We demonstrate that SiGMoID quantifies data\nnoise, estimates system parameters, and infers unobserved system components.\nIts effectiveness is validated validated through realistic experimental\nexamples, showcasing its broad applicability in various domains, from\nscientific research to engineered systems, and enabling the discovery of full\nsystem dynamics."}
{"id": "2507.10886", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10886", "abs": "https://arxiv.org/abs/2507.10886", "authors": ["Patryk Jasiorski", "Marek Klonowski", "Michał Woźniak"], "title": "How to Protect Models against Adversarial Unlearning?", "comment": null, "summary": "AI models need to be unlearned to fulfill the requirements of legal acts such\nas the AI Act or GDPR, and also because of the need to remove toxic content,\ndebiasing, the impact of malicious instances, or changes in the data\ndistribution structure in which a model works. Unfortunately, removing\nknowledge may cause undesirable side effects, such as a deterioration in model\nperformance. In this paper, we investigate the problem of adversarial\nunlearning, where a malicious party intentionally sends unlearn requests to\ndeteriorate the model's performance maximally. We show that this phenomenon and\nthe adversary's capabilities depend on many factors, primarily on the backbone\nmodel itself and strategy/limitations in selecting data to be unlearned. The\nmain result of this work is a new method of protecting model performance from\nthese side effects, both in the case of unlearned behavior resulting from\nspontaneous processes and adversary actions."}
{"id": "2507.10890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10890", "abs": "https://arxiv.org/abs/2507.10890", "authors": ["Riccardo Savorgnan", "Udaya Ghai", "Carson Eisenach", "Dean Foster"], "title": "Outbound Modeling for Inventory Management", "comment": "KDD - AI for Supply Chain Workshop", "summary": "We study the problem of forecasting the number of units fulfilled (or\n``drained'') from each inventory warehouse to meet customer demand, along with\nthe associated outbound shipping costs. The actual drain and shipping costs are\ndetermined by complex production systems that manage the planning and execution\nof customers' orders fulfillment, i.e. from where and how to ship a unit to be\ndelivered to a customer. Accurately modeling these processes is critical for\nregional inventory planning, especially when using Reinforcement Learning (RL)\nto develop control policies. For the RL usecase, a drain model is incorporated\ninto a simulator to produce long rollouts, which we desire to be\ndifferentiable. While simulating the calls to the internal software systems can\nbe used to recover this transition, they are non-differentiable and too slow\nand costly to run within an RL training environment. Accordingly, we frame this\nas a probabilistic forecasting problem, modeling the joint distribution of\noutbound drain and shipping costs across all warehouses at each time period,\nconditioned on inventory positions and exogenous customer demand. To ensure\nrobustness in an RL environment, the model must handle out-of-distribution\nscenarios that arise from off-policy trajectories. We propose a validation\nscheme that leverages production systems to evaluate the drain model on\ncounterfactual inventory states induced by RL policies. Preliminary results\ndemonstrate the model's accuracy within the in-distribution setting."}
{"id": "2507.10904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10904", "abs": "https://arxiv.org/abs/2507.10904", "authors": ["Elisa Tsai", "Haizhong Zheng", "Atul Prakash"], "title": "Class-Proportional Coreset Selection for Difficulty-Separable Data", "comment": "This paper has been accepted to the ICCV 2025 Workshop on Curated\n  Data for Efficient Learning (CDEL)", "summary": "High-quality training data is essential for building reliable and efficient\nmachine learning systems. One-shot coreset selection addresses this by pruning\nthe dataset while maintaining or even improving model performance, often\nrelying on training-dynamics-based data difficulty scores. However, most\nexisting methods implicitly assume class-wise homogeneity in data difficulty,\noverlooking variation in data difficulty across different classes.\n  In this work, we challenge this assumption by showing that, in domains such\nas network intrusion detection and medical imaging, data difficulty often\nclusters by class. We formalize this as class-difficulty separability and\nintroduce the Class Difficulty Separability Coefficient (CDSC) as a\nquantitative measure. We demonstrate that high CDSC values correlate with\nperformance degradation in class-agnostic coreset methods, which tend to\noverrepresent easy majority classes while neglecting rare but informative ones.\n  To address this, we introduce class-proportional variants of multiple\nsampling strategies. Evaluated on five diverse datasets spanning security and\nmedical domains, our methods consistently achieve state-of-the-art data\nefficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a\nclass-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows\nremarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and\nrecall 0.19%. In contrast, the class-agnostic CCS baseline, the next best\nmethod, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and\n4.11% in recall.\n  We further show that aggressive pruning enhances generalization in noisy,\nimbalanced, and large-scale datasets. Our results underscore that explicitly\nmodeling class-difficulty separability leads to more effective, robust, and\ngeneralizable data pruning, particularly in high-stakes scenarios."}
{"id": "2507.10955", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10955", "abs": "https://arxiv.org/abs/2507.10955", "authors": ["Chi-en Amy Tai", "Alexander Wong"], "title": "Diffusion Decoding for Peptide De Novo Sequencing", "comment": null, "summary": "Peptide de novo sequencing is a method used to reconstruct amino acid\nsequences from tandem mass spectrometry data without relying on existing\nprotein sequence databases. Traditional deep learning approaches, such as\nCasanovo, mainly utilize autoregressive decoders and predict amino acids\nsequentially. Subsequently, they encounter cascading errors and fail to\nleverage high-confidence regions effectively. To address these issues, this\npaper investigates using diffusion decoders adapted for the discrete data\ndomain. These decoders provide a different approach, allowing sequence\ngeneration to start from any peptide segment, thereby enhancing prediction\naccuracy. We experiment with three different diffusion decoder designs,\nknapsack beam search, and various loss functions. We find knapsack beam search\ndid not improve performance metrics and simply replacing the transformer\ndecoder with a diffusion decoder lowered performance. Although peptide\nprecision and recall were still 0, the best diffusion decoder design with the\nDINOISER loss function obtained a statistically significant improvement in\namino acid recall by 0.373 compared to the baseline autoregressive\ndecoder-based Casanovo model. These findings highlight the potential of\ndiffusion decoders to not only enhance model sensitivity but also drive\nsignificant advancements in peptide de novo sequencing."}
{"id": "2507.10983", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10983", "abs": "https://arxiv.org/abs/2507.10983", "authors": ["Tao Han", "Zahra Taheri", "Hyunwoong Ko"], "title": "Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review", "comment": "11 pages, 1 figure, 3 tables, IDETC-CIE 2025", "summary": "Semiconductor manufacturing relies heavily on film deposition processes, such\nas Chemical Vapor Deposition and Physical Vapor Deposition. These complex\nprocesses require precise control to achieve film uniformity, proper adhesion,\nand desired functionality. Recent advancements in Physics-Informed Neural\nNetworks (PINNs), an innovative machine learning (ML) approach, have shown\nsignificant promise in addressing challenges related to process control,\nquality assurance, and predictive modeling within semiconductor film deposition\nand other manufacturing domains. This paper provides a comprehensive review of\nML applications targeted at semiconductor film deposition processes. Through a\nthematic analysis, we identify key trends, existing limitations, and research\ngaps, offering insights into both the advantages and constraints of current\nmethodologies. Our structured analysis aims to highlight the potential\nintegration of these ML techniques to enhance interpretability, accuracy, and\nrobustness in film deposition processes. Additionally, we examine\nstate-of-the-art PINN methods, discussing strategies for embedding physical\nknowledge, governing laws, and partial differential equations into advanced\nneural network architectures tailored for semiconductor manufacturing. Based on\nthis detailed review, we propose novel research directions that integrate the\nstrengths of PINNs to significantly advance film deposition processes. The\ncontributions of this study include establishing a clear pathway for future\nresearch in integrating physics-informed ML frameworks, addressing existing\nmethodological gaps, and ultimately improving precision, scalability, and\noperational efficiency within semiconductor manufacturing."}
{"id": "2507.10986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10986", "abs": "https://arxiv.org/abs/2507.10986", "authors": ["Tianyu Su", "Zhiqiang Zou", "Ali Luo", "Xiao Kong", "Qingyu Lu", "Min Li"], "title": "StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data", "comment": null, "summary": "Stellar flare forecasting, a critical research frontier in astronomy, offers\nprofound insights into stellar activity. However, the field is constrained by\nboth the sparsity of recorded flare events and the absence of domain-specific\nlarge-scale predictive models. To address these challenges, this study\nintroduces StellarF (Stellar Flare Forecasting), a novel large model that\nleverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient\nlearning for stellar flare forecasting. At its core, StellarF integrates an\nflare statistical information module with a historical flare record module,\nenabling multi-scale pattern recognition from observational data. Extensive\nexperiments on our self-constructed datasets (derived from Kepler and TESS\nlight curves) demonstrate that StellarF achieves state-of-the-art performance\ncompared to existing methods. The proposed prediction paradigm establishes a\nnovel methodological framework for advancing astrophysical research and\ncross-disciplinary applications."}
{"id": "2507.10990", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10990", "abs": "https://arxiv.org/abs/2507.10990", "authors": ["Rodney Lafuente-Mercado"], "title": "High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization", "comment": null, "summary": "Scaling reinforcement learning (RL) workloads often requires distributing\nenvironment simulation across compute clusters. Existing frameworks entangle\nsimulation, learning logic, and orchestration into monolithic systems, limiting\nmodularity and reusability. We present ClusterEnv, a lightweight,\nlearner-agnostic interface for distributed environment execution that mirrors\nthe Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples\nsimulation from training by offloading reset() and step() operations to remote\nworkers while keeping learning centralized. To address policy staleness in\ndistributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),\na divergence-triggered update mechanism that reduces synchronization overhead\nwithout sacrificing performance. ClusterEnv integrates cleanly into existing RL\npipelines, supports both on-policy and off-policy methods, and requires minimal\ncode changes. Experiments on discrete control tasks demonstrate that AAPS\nachieves high sample efficiency with significantly fewer weight updates. Source\ncode is available at https://github.com/rodlaf/ClusterEnv."}
{"id": "2507.10995", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10995", "abs": "https://arxiv.org/abs/2507.10995", "authors": ["Henrik Marklund", "Alex Infanger", "Benjamin Van Roy"], "title": "Misalignment from Treating Means as Ends", "comment": null, "summary": "Reward functions, learned or manually specified, are rarely perfect. Instead\nof accurately expressing human goals, these reward functions are often\ndistorted by human beliefs about how best to achieve those goals. Specifically,\nthese reward functions often express a combination of the human's terminal\ngoals -- those which are ends in themselves -- and the human's instrumental\ngoals -- those which are means to an end. We formulate a simple example in\nwhich even slight conflation of instrumental and terminal goals results in\nsevere misalignment: optimizing the misspecified reward function results in\npoor performance when measured by the true reward function. This example\ndistills the essential properties of environments that make reinforcement\nlearning highly sensitive to conflation of instrumental and terminal goals. We\ndiscuss how this issue can arise with a common approach to reward learning and\nhow it can manifest in real environments."}
{"id": "2507.10998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10998", "abs": "https://arxiv.org/abs/2507.10998", "authors": ["Zhipeng He", "Alexander Stevens", "Chun Ouyang", "Johannes De Smedt", "Alistair Barros", "Catarina Moreira"], "title": "Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data", "comment": "32 pages", "summary": "Adversarial attacks on tabular data present fundamental challenges distinct\nfrom image or text domains due to the heterogeneous nature of mixed categorical\nand numerical features. Unlike images where pixel perturbations maintain visual\nsimilarity, tabular data lacks intuitive similarity metrics, making it\ndifficult to define imperceptible modifications. Additionally, traditional\ngradient-based methods prioritise $\\ell_p$-norm constraints, often producing\nadversarial examples that deviate from the original data distributions, making\nthem detectable. We propose a latent space perturbation framework using a\nmixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial\nexamples. The proposed VAE integrates categorical embeddings and numerical\nfeatures into a unified latent manifold, enabling perturbations that preserve\nstatistical consistency. We specify In-Distribution Success Rate (IDSR) to\nmeasure the proportion of adversarial examples that remain statistically\nindistinguishable from the input distribution. Evaluation across six publicly\navailable datasets and three model architectures demonstrates that our method\nachieves substantially lower outlier rates and more consistent performance\ncompared to traditional input-space attacks and other VAE-based methods adapted\nfrom image domain approaches. Our comprehensive analysis includes\nhyperparameter sensitivity, sparsity control mechanisms, and generative\narchitectural comparisons, revealing that VAE-based attacks depend critically\non reconstruction quality but offer superior practical utility when sufficient\ntraining data is available. This work highlights the importance of on-manifold\nperturbations for realistic adversarial attacks on tabular data, offering a\nrobust approach for practical deployment. The source code can be accessed\nthrough https://github.com/ZhipengHe/VAE-TabAttack."}
{"id": "2507.11005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11005", "abs": "https://arxiv.org/abs/2507.11005", "authors": ["Chongjie Si", "Debing Zhang", "Wei Shen"], "title": "AdaMuon: Adaptive Muon Optimizer", "comment": null, "summary": "We propose AdaMuon, an adaptive learning-rate framework built upon the\nrecently validated Muon optimizer, which has demonstrated substantial\nefficiency gains over AdamW in large-scale model training. AdaMuon augments\nMuon with two mutually dependent modules: (1) a per-parameter second-moment\nmodulation that captures orthogonal gradient updates to ensure update-level\nadaptivity, and (2) a RMS-aligned rescaling that regulates the overall update\nmagnitude by aligning it with the intrinsic structure of the parameter space.\nEmpirical results on multiple model scales and learning-rate regimes confirm\nthat AdaMuon consistently outperforms the original Muon, delivering higher\nacceleration in convergence while maintaining training stability. Our method\nintroduces no additional tuning burden and can be seamlessly integrated into\nexisting Muon training pipelines."}
{"id": "2507.11012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11012", "abs": "https://arxiv.org/abs/2507.11012", "authors": ["Dipak Dulal", "Joseph J. Charney", "Michael R. Gallagher", "Pitambar Acharya", "Carmeliza Navasca", "Nicholas S. Skowronski"], "title": "Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire", "comment": "arXiv admin note: text overlap with arXiv:2311.05128", "summary": "This study explores the potential for predicting turbulent kinetic energy\n(TKE) from more readily acquired temperature data using temperature profiles\nand turbulence data collected concurrently at 10 Hz during a small experimental\nprescribed burn in the New Jersey Pine Barrens. Machine learning models,\nincluding Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and\nGaussian Process Regressor, were employed to assess the potential to predict\nTKE from temperature perturbations and explore temporal and spatial dynamics of\ncorrelations. Data visualization and correlation analyses revealed patterns and\nrelationships between thermocouple temperatures and TKE, providing insight into\nthe underlying dynamics. More accurate predictions of TKE were achieved by\nemploying various machine learning models despite a weak correlation between\nthe predictors and the target variable. The results demonstrate significant\nsuccess, particularly from regression models, in accurately predicting the TKE.\nThe findings of this study demonstrate a novel numerical approach to\nidentifying new relationships between temperature and airflow processes in and\naround the fire environment. These relationships can help refine our\nunderstanding of combustion environment processes and the coupling and\ndecoupling of fire environment processes necessary for improving fire\noperations strategy and fire and smoke model predictions. The findings of this\nstudy additionally highlight the valuable role of machine learning techniques\nin analyzing the complex large datasets of the fire environments, showcasing\ntheir potential to advance fire research and management practices."}
{"id": "2507.11017", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11017", "abs": "https://arxiv.org/abs/2507.11017", "authors": ["Xingyu Zheng", "Haotong Qin", "Yuye Li", "Jiakai Wang", "Jinyang Guo", "Michele Magno", "Xianglong Liu"], "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models", "comment": null, "summary": "Post-training quantization (PTQ) offers an efficient approach to compressing\nlarge language models (LLMs), significantly reducing memory access and\ncomputational costs. Existing compensation-based weight calibration methods\noften rely on a second-order Taylor expansion to model quantization error,\nunder the assumption that the first-order term is negligible in well-trained\nfull-precision models. However, we reveal that the progressive compensation\nprocess introduces accumulated first-order deviations between latent weights\nand their full-precision counterparts, making this assumption fundamentally\nflawed. To address this, we propose FOEM, a novel PTQ method that explicitly\nincorporates first-order gradient terms to improve quantization error\ncompensation. FOEM approximates gradients by directly computing the difference\nbetween latent and full-precision weights, avoiding the high cost and limited\ngeneralization of backpropagation-based gradient computation. This approach\nintroduces minimal additional computational overhead. Moreover, FOEM leverages\nprecomputed Cholesky factors to efficiently recover the inverse of Hessian\nsubmatrices in real time. Extensive experiments across a wide range of models\nand benchmarks demonstrate that FOEM consistently outperforms the classical\nGPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of\nLlama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from\n51.7% to 74.9%, approaching the full-precision performance of 78.6%.\nFurthermore, FOEM can be seamlessly integrated with advanced techniques such as\nGPTAQ and SpinQuant, yielding additional improvements under the challenging\nW4A4KV4 setting, and further narrowing the accuracy gap with full-precision\nbaselines beyond what current state-of-the-art methods achieve. The code is\navailable at https://github.com/Xingyu-Zheng/FOEM."}
{"id": "2507.11019", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11019", "abs": "https://arxiv.org/abs/2507.11019", "authors": ["Claas Voelcker", "Axel Brunnbauer", "Marcel Hussing", "Michal Nauman", "Pieter Abbeel", "Eric Eaton", "Radu Grosu", "Amir-massoud Farahmand", "Igor Gilitschenski"], "title": "Relative Entropy Pathwise Policy Optimization", "comment": null, "summary": "Score-function policy gradients have delivered strong results in\ngame-playing, robotics and language-model fine-tuning. Yet its high-variance\noften undermines training stability. On the other hand, pathwise policy\ngradients alleviate the training variance, but are reliable only when driven by\nan accurate action-conditioned value function which is notoriously hard to\ntrain without relying on past off-policy data. In this paper, we discuss how to\nconstruct a value-gradient driven, on-policy algorithm that allow training\nQ-value models purely from on-policy data, unlocking the possibility of using\npathwise policy updates in the context of on-policy learning. We show how to\nbalance stochastic policies for exploration with constrained policy updates for\nstable training, and evaluate important architectural components that\nfacilitate accurate value function learning. Building on these insights, we\npropose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient\non-policy algorithm that combines the sample-efficiency of pathwise policy\ngradients with the simplicity and minimal memory footprint of standard\non-policy learning. We demonstrate that REPPO provides strong empirical\nperformance at decreased sample requirements, wall-clock time, memory footprint\nas well as high hyperparameter robustness in a set of experiments on two\nstandard GPU-parallelized benchmarks."}
{"id": "2507.11053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11053", "abs": "https://arxiv.org/abs/2507.11053", "authors": ["Danish Gufran", "Sudeep Pasricha"], "title": "GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices", "comment": null, "summary": "Accurate indoor localization is crucial for enabling spatial context in smart\nenvironments and navigation systems. Wi-Fi Received Signal Strength (RSS)\nfingerprinting is a widely used indoor localization approach due to its\ncompatibility with mobile embedded devices. Deep Learning (DL) models improve\naccuracy in localization tasks by learning RSS variations across locations, but\nthey assume fingerprint vectors exist in a Euclidean space, failing to\nincorporate spatial relationships and the non-uniform distribution of\nreal-world RSS noise. This results in poor generalization across heterogeneous\nmobile devices, where variations in hardware and signal processing distort RSS\nreadings. Graph Neural Networks (GNNs) can improve upon conventional DL models\nby encoding indoor locations as nodes and modeling their spatial and signal\nrelationships as edges. However, GNNs struggle with non-Euclidean noise\ndistributions and suffer from the GNN blind spot problem, leading to degraded\naccuracy in environments with dense access points (APs). To address these\nchallenges, we propose GATE, a novel framework that constructs an adaptive\ngraph representation of fingerprint vectors while preserving an indoor\nstate-space topology, modeling the non-Euclidean structure of RSS noise to\nmitigate environmental noise and address device heterogeneity. GATE introduces\n1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a\nnovel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind\nspot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic\ngraph adaptation. Extensive real-world evaluations across multiple indoor\nspaces with varying path lengths, AP densities, and heterogeneous devices\ndemonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and\n1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor\nlocalization frameworks."}
{"id": "2507.11063", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11063", "abs": "https://arxiv.org/abs/2507.11063", "authors": ["Gwen Maudet", "Grégoire Danoy"], "title": "A Distance Metric for Mixed Integer Programming Instances", "comment": "Accepted to ECAI 2025", "summary": "Mixed-integer linear programming (MILP) is a powerful tool for addressing a\nwide range of real-world problems, but it lacks a clear structure for comparing\ninstances. A reliable similarity metric could establish meaningful\nrelationships between instances, enabling more effective evaluation of instance\nset heterogeneity and providing better guidance to solvers, particularly when\nmachine learning is involved. Existing similarity metrics often lack precision\nin identifying instance classes or rely heavily on labeled data, which limits\ntheir applicability and generalization. To bridge this gap, this paper\nintroduces the first mathematical distance metric for MILP instances, derived\ndirectly from their mathematical formulations. By discretizing right-hand\nsides, weights, and variables into classes, the proposed metric draws\ninspiration from the Earth mover's distance to quantify mismatches in\nweight-variable distributions for constraint comparisons. This approach\nnaturally extends to enable instance-level comparisons. We evaluate both an\nexact and a greedy variant of our metric under various parameter settings,\nusing the StrIPLIB dataset. Results show that all components of the metric\ncontribute to class identification, and that the greedy version achieves\naccuracy nearly identical to the exact formulation while being nearly 200 times\nfaster. Compared to state-of-the-art baselines, including feature-based,\nimage-based, and neural network models, our unsupervised method consistently\noutperforms all non-learned approaches and rivals the performance of a\nsupervised classifier on class and subclass grouping tasks."}
{"id": "2507.11071", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11071", "abs": "https://arxiv.org/abs/2507.11071", "authors": ["Isaiah Thompson Ocansey", "Ritwik Bhattacharya", "Tanmay Sen"], "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection", "comment": null, "summary": "Log anomaly detection using traditional rule based or deep learning based\nmethods is often challenging due to the large volume and highly complex nature\nof log sequence. So effective way of detection of anomalous sequence of logs is\ncrucial for system maintenance and development. This paper proposes parameter\nefficient finetuning specifically low rank adaptation (LoRA) and adapter based\napproaches for finding contextual anomalies in sequence of logs in large log\ndata set. It compares different tiny large language models (LLMs) on the\nThunderbird dataset. The results show that LoRA based finetuning provides\nsubstantial performance improvements of 18 to 19 percentage over LogBert based\nfull finetuning approach, achieving accuracy scores between 97.76% and 98.83%\ncompared to 79.37%."}
{"id": "2507.11173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11173", "abs": "https://arxiv.org/abs/2507.11173", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "title": "Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction", "comment": null, "summary": "Autonomous unmanned aerial vehicles (UAVs) rely on global navigation\nsatellite system (GNSS) pseudorange measurements for accurate real-time\nlocalization and navigation. However, this dependence exposes them to\nsophisticated spoofing threats, where adversaries manipulate pseudoranges to\ndeceive UAV receivers. Among these, drift-evasive spoofing attacks subtly\nperturb measurements, gradually diverting the UAVs trajectory without\ntriggering conventional signal-level anti-spoofing mechanisms. Traditional\ndistributional shift detection techniques often require accumulating a\nthreshold number of samples, causing delays that impede rapid detection and\ntimely response. Consequently, robust temporal-scale detection methods are\nessential to identify attack onset and enable contingency planning with\nalternative sensing modalities, improving resilience against stealthy\nadversarial manipulations. This study explores a Bayesian online change point\ndetection (BOCPD) approach that monitors temporal shifts in value estimates\nfrom a reinforcement learning (RL) critic network to detect subtle behavioural\ndeviations in UAV navigation. Experimental results show that this temporal\nvalue-based framework outperforms conventional GNSS spoofing detectors,\ntemporal semi-supervised learning frameworks, and the Page-Hinkley test,\nachieving higher detection accuracy and lower false-positive and false-negative\nrates for drift-evasive spoofing attacks."}
{"id": "2507.11178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11178", "abs": "https://arxiv.org/abs/2507.11178", "authors": ["Meiliang Liu", "Huiwen Dong", "Xiaoxiao Yang", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xinyue Yang", "Zhiwen Zhao"], "title": "Gradient Regularization-based Neural Granger Causality", "comment": "9 pages,3 figures, conference", "summary": "With the advancement of deep learning technologies, various neural\nnetwork-based Granger causality models have been proposed. Although these\nmodels have demonstrated notable improvements, several limitations remain. Most\nexisting approaches adopt the component-wise architecture, necessitating the\nconstruction of a separate model for each time series, which results in\nsubstantial computational costs. In addition, imposing the sparsity-inducing\npenalty on the first-layer weights of the neural network to extract causal\nrelationships weakens the model's ability to capture complex interactions. To\naddress these limitations, we propose Gradient Regularization-based Neural\nGranger Causality (GRNGC), which requires only one time series prediction model\nand applies $L_{1}$ regularization to the gradient between model's input and\noutput to infer Granger causality. Moreover, GRNGC is not tied to a specific\ntime series forecasting model and can be implemented with diverse architectures\nsuch as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical\nsimulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC\noutperforms existing baselines and significantly reduces computational\noverhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder\nurothelial carcinoma datasets further validate the model's effectiveness in\nreconstructing gene regulatory networks."}
{"id": "2507.11181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11181", "abs": "https://arxiv.org/abs/2507.11181", "authors": ["Danyang Zhang", "Junhao Song", "Ziqian Bi", "Yingfang Yuan", "Tianyang Wang", "Joe Yeong", "Junfeng Hao"], "title": "Mixture of Experts in Large Language Models", "comment": null, "summary": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE)\narchitecture in large language models, highlighting its ability to\nsignificantly enhance model performance while maintaining minimal computational\noverhead. Through a systematic analysis spanning theoretical foundations, core\narchitectural designs, and large language model (LLM) applications, we examine\nexpert gating and routing mechanisms, hierarchical and sparse MoE\nconfigurations, meta-learning approaches, multimodal and multitask learning\nscenarios, real-world deployment cases, and recent advances and challenges in\ndeep learning. Our analysis identifies key advantages of MoE, including\nsuperior model capacity compared to equivalent Bayesian approaches, improved\ntask-specific performance, and the ability to scale model capacity efficiently.\nWe also underscore the importance of ensuring expert diversity, accurate\ncalibration, and reliable inference aggregation, as these are essential for\nmaximizing the effectiveness of MoE architectures. Finally, this review\noutlines current research limitations, open challenges, and promising future\ndirections, providing a foundation for continued innovation in MoE architecture\nand its applications."}
{"id": "2507.11183", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11183", "abs": "https://arxiv.org/abs/2507.11183", "authors": ["Dimitrios Kritsiolis", "Constantine Kotropoulos"], "title": "Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications", "comment": "In Proceedings of the 2025 IARIA Annual Congress on Frontiers in\n  Science, Technology, Services, and Applications (IARIA Congress 2025),\n  Venice, Italy, July 6-10, 2025", "summary": "Federated learning is a machine learning approach that enables multiple\ndevices (i.e., agents) to train a shared model cooperatively without exchanging\nraw data. This technique keeps data localized on user devices, ensuring privacy\nand security, while each agent trains the model on their own data and only\nshares model updates. The communication overhead is a significant challenge due\nto the frequent exchange of model updates between the agents and the central\nserver. In this paper, we propose a communication-efficient federated learning\nscheme that utilizes low-rank approximation of neural network gradients and\nquantization to significantly reduce the network load of the decentralized\nlearning process with minimal impact on the model's accuracy."}
{"id": "2507.11185", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11185", "abs": "https://arxiv.org/abs/2507.11185", "authors": ["Md. Emon Akter Sourov", "Md. Sabbir Hossen", "Pabon Shaha", "Mohammad Minoar Hossain", "Md Sadiq Iqbal"], "title": "An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment", "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library", "summary": "Heart disease remains a major global health concern, particularly in regions\nwith limited access to medical resources and diagnostic facilities. Traditional\ndiagnostic methods often fail to accurately identify and manage heart disease\nrisks, leading to adverse outcomes. Machine learning has the potential to\nsignificantly enhance the accuracy, efficiency, and speed of heart disease\ndiagnosis. In this study, we proposed a comprehensive framework that combines\nclassification models for heart disease detection and regression models for\nrisk prediction. We employed the Heart Disease dataset, which comprises 1,035\ncases. To address the issue of class imbalance, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, resulting in the generation of an\nadditional 100,000 synthetic data points. Performance metrics, including\naccuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to\nevaluate the model's effectiveness. Among the classification models, Random\nForest emerged as the standout performer, achieving an accuracy of 97.2% on\nreal data and 97.6% on synthetic data. For regression tasks, Linear Regression\ndemonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic\ndatasets, respectively, with the lowest error metrics. Additionally,\nExplainable AI techniques were employed to enhance the interpretability of the\nmodels. This study highlights the potential of machine learning to\nrevolutionize heart disease diagnosis and risk prediction, thereby facilitating\nearly intervention and enhancing clinical decision-making."}
{"id": "2507.11187", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11187", "abs": "https://arxiv.org/abs/2507.11187", "authors": ["Shao-Bo Lin", "Xiaotong Liu", "Yao Wang"], "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms", "comment": null, "summary": "Online collaborative medical prediction platforms offer convenience and\nreal-time feedback by leveraging massive electronic health records. However,\ngrowing concerns about privacy and low prediction quality can deter patient\nparticipation and doctor cooperation. In this paper, we first clarify the\nprivacy attacks, namely attribute attacks targeting patients and model\nextraction attacks targeting doctors, and specify the corresponding privacy\nprinciples. We then propose a privacy-preserving mechanism and integrate it\ninto a novel one-shot distributed learning framework, aiming to simultaneously\nmeet both privacy requirements and prediction performance objectives. Within\nthe framework of statistical learning theory, we theoretically demonstrate that\nthe proposed distributed learning framework can achieve the optimal prediction\nperformance under specific privacy requirements. We further validate the\ndeveloped privacy-preserving collaborative medical prediction platform through\nboth toy simulations and real-world data experiments."}
{"id": "2507.11228", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11228", "abs": "https://arxiv.org/abs/2507.11228", "authors": ["Si Yi Meng", "Baptiste Goujaud", "Antonio Orvieto", "Christopher De Sa"], "title": "Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?", "comment": null, "summary": "Gradient descent (GD) on logistic regression has many fascinating properties.\nWhen the dataset is linearly separable, it is known that the iterates converge\nin direction to the maximum-margin separator regardless of how large the step\nsize is. In the non-separable case, however, it has been shown that GD can\nexhibit a cycling behaviour even when the step sizes is still below the\nstability threshold $2/\\lambda$, where $\\lambda$ is the largest eigenvalue of\nthe Hessian at the solution. This short paper explores whether restricting the\ndata to have equal magnitude is a sufficient condition for global convergence,\nunder any step size below the stability threshold. We prove that this is true\nin a one dimensional space, but in higher dimensions cycling behaviour can\nstill occur. We hope to inspire further studies on quantifying how common these\ncycles are in realistic datasets, as well as finding sufficient conditions to\nguarantee global convergence with large step sizes."}
{"id": "2507.11246", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11246", "abs": "https://arxiv.org/abs/2507.11246", "authors": ["Lingwei Kong", "Lu Wang", "Changping Peng", "Zhangang Lin", "Ching Law", "Jingping Shao"], "title": "Generative Click-through Rate Prediction with Applications to Search Advertising", "comment": "This work was first submitted on February 9, 2024", "summary": "Click-Through Rate (CTR) prediction models are integral to a myriad of\nindustrial settings, such as personalized search advertising. Current methods\ntypically involve feature extraction from users' historical behavior sequences\ncombined with product information, feeding into a discriminative model that is\ntrained on user feedback to estimate CTR. With the success of models such as\nGPT, the potential for generative models to enrich expressive power beyond\ndiscriminative models has become apparent. In light of this, we introduce a\nnovel model that leverages generative models to enhance the precision of CTR\npredictions in discriminative models. To reconcile the disparate data\naggregation needs of both model types, we design a two-stage training process:\n1) Generative pre-training for next-item prediction with the given item\ncategory in user behavior sequences; 2) Fine-tuning the well-trained generative\nmodel within a discriminative CTR prediction framework. Our method's efficacy\nis substantiated through extensive experiments on a new dataset, and its\nsignificant utility is further corroborated by online A/B testing results.\nCurrently, the model is deployed on one of the world's largest e-commerce\nplatforms, and we intend to release the associated code and dataset in the\nfuture."}
{"id": "2507.11262", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11262", "abs": "https://arxiv.org/abs/2507.11262", "authors": ["Elmira Mirzabeigi", "Sepehr Rezaee", "Kourosh Parand"], "title": "LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments", "comment": null, "summary": "Training deep neural networks, particularly in computer vision tasks, often\nsuffers from noisy gradients and unstable convergence, which hinder performance\nand generalization. In this paper, we propose LyAm, a novel optimizer that\nintegrates Adam's adaptive moment estimation with Lyapunov-based stability\nmechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability\ntheory to enhance convergence robustness and mitigate training noise. We\nprovide a rigorous theoretical framework proving the convergence guarantees of\nLyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10\nand CIFAR-100 show that LyAm consistently outperforms state-of-the-art\noptimizers in terms of accuracy, convergence speed, and stability, establishing\nit as a strong candidate for robust deep learning optimization."}
{"id": "2507.11269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11269", "abs": "https://arxiv.org/abs/2507.11269", "authors": ["Tal Fiskus", "Uri Shaham"], "title": "Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound", "comment": "51 pages, 16 figures", "summary": "Deep reinforcement learning (DRL) agents excel in solving complex\ndecision-making tasks across various domains. However, they often require a\nsubstantial number of training steps and a vast experience replay buffer,\nleading to significant computational and resource demands. To address these\nchallenges, we introduce a novel theoretical result that leverages the\nNeyman-Rubin potential outcomes framework into DRL. Unlike most methods that\nfocus on bounding the counterfactual loss, we establish a causal bound on the\nfactual loss, which is analogous to the on-policy loss in DRL. This bound is\ncomputed by storing past value network outputs in the experience replay buffer,\neffectively utilizing data that is usually discarded. Extensive experiments\nacross the Atari 2600 and MuJoCo domains on various agents, such as DQN and\nSAC, achieve up to 2,427% higher reward ratio, outperforming the same agents\nwithout our proposed term, and reducing the experience replay buffer size by up\nto 96%, significantly improving sample efficiency at negligible cost."}
{"id": "2507.11274", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11274", "abs": "https://arxiv.org/abs/2507.11274", "authors": ["Amit Attia", "Matan Schliserman", "Uri Sherman", "Tomer Koren"], "title": "Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime", "comment": "27 pages", "summary": "We study population convergence guarantees of stochastic gradient descent\n(SGD) for smooth convex objectives in the interpolation regime, where the noise\nat optimum is zero or near zero. The behavior of the last iterate of SGD in\nthis setting -- particularly with large (constant) stepsizes -- has received\ngrowing attention in recent years due to implications for the training of\nover-parameterized models, as well as to analyzing forgetting in continual\nlearning and to understanding the convergence of the randomized Kaczmarz method\nfor solving linear systems. We establish that after $T$ steps of SGD on\n$\\beta$-smooth convex loss functions with stepsize $\\eta \\leq 1/\\beta$, the\nlast iterate exhibits expected excess risk $\\widetilde{O}(1/(\\eta\nT^{1-\\beta\\eta/2}) + \\eta T^{\\beta\\eta/2} \\sigma_\\star^2)$, where\n$\\sigma_\\star^2$ denotes the variance of the stochastic gradients at the\noptimum. In particular, for a well-tuned stepsize we obtain a near optimal\n$\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt{T})$ rate for the last iterate,\nextending the results of Varre et al. (2021) beyond least squares regression;\nand when $\\sigma_\\star=0$ we obtain a rate of $O(1/\\sqrt{T})$ with\n$\\eta=1/\\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently\nestablished by Evron et al. (2025) in the special case of realizable linear\nregression."}
{"id": "2507.11344", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11344", "abs": "https://arxiv.org/abs/2507.11344", "authors": ["Zara Hall", "Melanie Subbiah", "Thomas P Zollo", "Kathleen McKeown", "Richard Zemel"], "title": "Guiding LLM Decision-Making with Fairness Reward Models", "comment": null, "summary": "Large language models are increasingly used to support high-stakes decisions,\npotentially influencing who is granted bail or receives a loan. Naive\nchain-of-thought sampling can improve average decision accuracy, but has also\nbeen shown to amplify unfair bias. To address this challenge and enable the\ntrustworthy use of reasoning models in high-stakes decision-making, we propose\na framework for training a generalizable Fairness Reward Model (FRM). Our model\nassigns a fairness score to LLM reasoning, enabling the system to down-weight\nbiased trajectories and favor equitable ones when aggregating decisions across\nreasoning chains. We show that a single Fairness Reward Model, trained on\nweakly supervised, LLM-annotated examples of biased versus unbiased reasoning,\ntransfers across tasks, domains, and model families without additional\nfine-tuning. Applied to real-world decision-making tasks including recidivism\nprediction and social media moderation, we show that our approach consistently\nimproves fairness while matching, or even surpassing, baseline accuracy."}
{"id": "2507.11357", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11357", "abs": "https://arxiv.org/abs/2507.11357", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "title": "Neurosymbolic Reasoning Shortcuts under the Independence Assumption", "comment": "Accepted at NeSy 2025", "summary": "The ubiquitous independence assumption among symbolic concepts in\nneurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors\nuse it to speed up probabilistic reasoning. Recent works like van Krieken et\nal. (2024) and Marconato et al. (2024) argued that the independence assumption\ncan hinder learning of NeSy predictors and, more crucially, prevent them from\ncorrectly modelling uncertainty. There is, however, scepticism in the NeSy\ncommunity around the scenarios in which the independence assumption actually\nlimits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle\nthis question by formally showing that assuming independence among symbolic\nconcepts entails that a model can never represent uncertainty over certain\nconcept combinations. Thus, the model fails to be aware of reasoning shortcuts,\ni.e., the pathological behaviour of NeSy predictors that predict correct\ndownstream tasks but for the wrong reasons."}
{"id": "2507.11367", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11367", "abs": "https://arxiv.org/abs/2507.11367", "authors": ["Daniel Tanneberg"], "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning", "comment": "accepted at the European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Training neural networks with reinforcement learning (RL) typically relies on\nbackpropagation (BP), necessitating storage of activations from the forward\npass for subsequent backward updates. Furthermore, backpropagating error\nsignals through multiple layers often leads to vanishing or exploding\ngradients, which can degrade learning performance and stability. We propose a\nnovel approach that trains each layer of the neural network using local signals\nduring the forward pass in RL settings. Our approach introduces local,\nlayer-wise losses leveraging the principle of matching pairwise distances from\nmulti-dimensional scaling, enhanced with optional reward-driven guidance. This\nmethod allows each hidden layer to be trained using local signals computed\nduring forward propagation, thus eliminating the need for backward passes and\nstoring intermediate activations. Our experiments, conducted with policy\ngradient methods across common RL benchmarks, demonstrate that this\nbackpropagation-free method achieves competitive performance compared to their\nclassical BP-based counterpart. Additionally, the proposed method enhances\nstability and consistency within and across runs, and improves performance\nespecially in challenging environments."}
{"id": "2507.11371", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11371", "abs": "https://arxiv.org/abs/2507.11371", "authors": ["Gabriel Bo", "Koa Chang", "Justin Gu"], "title": "Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs", "comment": "12 pages, 4 figures", "summary": "We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel\nreinforcement learning framework that teaches large language models to explore\ndiverse tool usage patterns beyond conventional high-temperature sampling.\nBuilding on recent advances in step-wise reinforcement learning, we introduce a\ndual-objective reward system that simultaneously optimizes for answer quality\nand tool diversity, training a Llama-3.1 8B model through offline PPO on\nsynthetically generated trajectories from the MMLU-Pro dataset. Our approach\nuniquely employs a rarity-first exploitation strategy where a GPT-4o judge\nscores candidate actions across eight distinct tools plus chain-of-thought\nreasoning, with the policy favoring less-frequently used but still viable tools\nto encourage systematic exploration. Empirical results demonstrate that SPaRK\nachieves competitive performance across 14 MMLU-Pro categories while exhibiting\nsignificantly higher entropy in tool selection compared to both baseline and\nsupervised fine-tuning approaches, suggesting that algorithmic exploration\nthrough explicit tool diversity can enhance reasoning capabilities without\nsacrificing accuracy."}
{"id": "2507.11393", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11393", "abs": "https://arxiv.org/abs/2507.11393", "authors": ["James P Jun", "Vijay Marupudi", "Raj Sanjay Shah", "Sashank Varma"], "title": "A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning", "comment": "Accepted to CogSci 2025. 7 pages, 7 figures", "summary": "Learning new information without forgetting prior knowledge is central to\nhuman intelligence. In contrast, neural network models suffer from catastrophic\nforgetting: a significant degradation in performance on previously learned\ntasks when acquiring new information. The Complementary Learning Systems (CLS)\ntheory offers an explanation for this human ability, proposing that the brain\nhas distinct systems for pattern separation (encoding distinct memories) and\npattern completion (retrieving complete memories from partial cues). To capture\nthese complementary functions, we leverage the representational generalization\ncapabilities of variational autoencoders (VAEs) and the robust memory storage\nproperties of Modern Hopfield networks (MHNs), combining them into a neurally\nplausible continual learning model. We evaluate this model on the Split-MNIST\ntask, a popular continual learning benchmark, and achieve close to\nstate-of-the-art accuracy (~90%), substantially reducing forgetting.\nRepresentational analyses empirically confirm the functional dissociation: the\nVAE underwrites pattern completion, while the MHN drives pattern separation. By\ncapturing pattern separation and completion in scalable architectures, our work\nprovides a functional template for modeling memory consolidation,\ngeneralization, and continual learning in both biological and artificial\nsystems."}
{"id": "2507.11411", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11411", "abs": "https://arxiv.org/abs/2507.11411", "authors": ["Seyedsaman Emami", "Gonzalo Martínez-Muñoz", "Daniel Hernández-Lobato"], "title": "Robust-Multi-Task Gradient Boosting", "comment": null, "summary": "Multi-task learning (MTL) has shown effectiveness in exploiting shared\ninformation across tasks to improve generalization. MTL assumes tasks share\nsimilarities that can improve performance. In addition, boosting algorithms\nhave demonstrated exceptional performance across diverse learning problems,\nprimarily due to their ability to focus on hard-to-learn instances and\niteratively reduce residual errors. This makes them a promising approach for\nlearning multi-task problems. However, real-world MTL scenarios often involve\ntasks that are not well-aligned (known as outlier or adversarial tasks), which\ndo not share beneficial similarities with others and can, in fact, deteriorate\nthe performance of the overall model. To overcome this challenge, we propose\nRobust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that\nexplicitly models and adapts to task heterogeneity during training. R-MTGB\nstructures the learning process into three sequential blocks: (1) learning\nshared patterns, (2) partitioning tasks into outliers and non-outliers with\nregularized parameters, and (3) fine-tuning task-specific predictors. This\narchitecture enables R-MTGB to automatically detect and penalize outlier tasks\nwhile promoting effective knowledge transfer among related tasks. Our method\nintegrates these mechanisms seamlessly within gradient boosting, allowing\nrobust handling of noisy or adversarial tasks without sacrificing accuracy.\nExtensive experiments on both synthetic benchmarks and real-world datasets\ndemonstrate that our approach successfully isolates outliers, transfers\nknowledge, and consistently reduces prediction errors for each task\nindividually, and achieves overall performance gains across all tasks. These\nresults highlight robustness, adaptability, and reliable convergence of R-MTGB\nin challenging MTL environments."}
{"id": "2507.11436", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11436", "abs": "https://arxiv.org/abs/2507.11436", "authors": ["Behtom Adeli", "John McLinden", "Pankaj Pandey", "Ming Shao", "Yalda Shahriari"], "title": "Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures", "comment": null, "summary": "Activation functions are critical to the performance of deep neural networks,\nparticularly in domains such as functional near-infrared spectroscopy (fNIRS),\nwhere nonlinearity, low signal-to-noise ratio (SNR), and signal variability\nposes significant challenges to model accuracy. However, the impact of\nactivation functions on deep learning (DL) performance in the fNIRS domain\nremains underexplored and lacks systematic investigation in the current\nliterature. This study evaluates a range of conventional and field-specific\nactivation functions for fNIRS classification tasks using multiple deep\nlearning architectures, including the domain-specific fNIRSNet, AbsoluteNet,\nMDNN, and shallowConvNet (as the baseline), all tested on a single dataset\nrecorded during an auditory task. To ensure fair a comparison, all networks\nwere trained and tested using standardized preprocessing and consistent\ntraining parameters. The results show that symmetrical activation functions\nsuch as Tanh and the Absolute value function Abs(x) can outperform commonly\nused functions like the Rectified Linear Unit (ReLU), depending on the\narchitecture. Additionally, a focused analysis of the role of symmetry was\nconducted using a Modified Absolute Function (MAF), with results further\nsupporting the effectiveness of symmetrical activation functions on performance\ngains. These findings underscore the importance of selecting proper activation\nfunctions that align with the signal characteristics of fNIRS data."}
{"id": "2507.11439", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11439", "abs": "https://arxiv.org/abs/2507.11439", "authors": ["Hongming Tan", "Ting Chen", "Ruochong Jin", "Wai Kin Chan"], "title": "Data Augmentation in Time Series Forecasting through Inverted Framework", "comment": null, "summary": "Currently, iTransformer is one of the most popular and effective models for\nmultivariate time series (MTS) forecasting. Thanks to its inverted framework,\niTransformer effectively captures multivariate correlation. However, the\ninverted framework still has some limitations. It diminishes temporal\ninterdependency information, and introduces noise in cases of nonsignificant\nvariable correlation. To address these limitations, we introduce a novel data\naugmentation method on inverted framework, called DAIF. Unlike previous data\naugmentation methods, DAIF stands out as the first real-time augmentation\nspecifically designed for the inverted framework in MTS forecasting. We first\ndefine the structure of the inverted sequence-to-sequence framework, then\npropose two different DAIF strategies, Frequency Filtering and Cross-variation\nPatching to address the existing challenges of the inverted framework.\nExperiments across multiple datasets and inverted models have demonstrated the\neffectiveness of our DAIF."}
{"id": "2507.11457", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11457", "abs": "https://arxiv.org/abs/2507.11457", "authors": ["Yaoxian Dong", "Yifan Gao", "Haoyue Li", "Yanfen Cui", "Xin Gao"], "title": "LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer", "comment": null, "summary": "Accurate preoperative assessment of lymph node (LN) metastasis in rectal\ncancer guides treatment decisions, yet conventional MRI evaluation based on\nmorphological criteria shows limited diagnostic performance. While some\nartificial intelligence models have been developed, they often operate as black\nboxes, lacking the interpretability needed for clinical trust. Moreover, these\nmodels typically evaluate nodes in isolation, overlooking the patient-level\ncontext. To address these limitations, we introduce LRMR, an LLM-Driven\nRelational Multi-node Ranking framework. This approach reframes the diagnostic\ntask from a direct classification problem into a structured reasoning and\nranking process. The LRMR framework operates in two stages. First, a multimodal\nlarge language model (LLM) analyzes a composite montage image of all LNs from a\npatient, generating a structured report that details ten distinct radiological\nfeatures. Second, a text-based LLM performs pairwise comparisons of these\nreports between different patients, establishing a relative risk ranking based\non the severity and number of adverse features. We evaluated our method on a\nretrospective cohort of 117 rectal cancer patients. LRMR achieved an area under\nthe curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of\ndeep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies\nconfirmed the value of our two main contributions: removing the relational\nranking stage or the structured prompting stage led to a significant\nperformance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our\nwork demonstrates that decoupling visual perception from cognitive reasoning\nthrough a two-stage LLM framework offers a powerful, interpretable, and\neffective new paradigm for assessing lymph node metastasis in rectal cancer."}
{"id": "2507.11471", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11471", "abs": "https://arxiv.org/abs/2507.11471", "authors": ["Harsha Varun Marisetty", "Manik Gupta", "Yogesh Simmhan"], "title": "D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data", "comment": "Preprint of paper to appear in the proceedings of IEEE INTERNATIONAL\n  CONFERENCE ON EDGE COMPUTING & COMMUNICATIONS EDGE 2025", "summary": "With advancements in computing and communication technologies, the Internet\nof Things (IoT) has seen significant growth. IoT devices typically collect data\nfrom various sensors, such as temperature, humidity, and energy meters. Much of\nthis data is temporal in nature. Traditionally, data from IoT devices is\ncentralized for analysis, but this approach introduces delays and increased\ncommunication costs. Federated learning (FL) has emerged as an effective\nalternative, allowing for model training across distributed devices without the\nneed to centralize data. In many applications, such as smart home energy and\nenvironmental monitoring, the data collected by IoT devices across different\nlocations can exhibit significant variation in trends and seasonal patterns.\nAccurately forecasting such non-stationary, non-linear time-series data is\ncrucial for applications like energy consumption estimation and weather\nforecasting. However, these data variations can severely impact prediction\naccuracy. The key contributions of this paper are: (1) Investigating how\nnon-linear, non-stationary time-series data distributions, like generalized\nextreme value (gen-extreme) and log norm distributions, affect FL performance.\n(2) Analyzing how different detrending techniques for non-linear time-series\ndata influence the forecasting model's performance in a FL setup. We generated\nseveral synthetic time-series datasets using non-linear data distributions and\ntrained an LSTM-based forecasting model using both centralized and FL\napproaches. Additionally, we evaluated the impact of detrending on real-world\ndatasets with non-linear time-series data distributions. Our experimental\nresults show that: (1) FL performs worse than centralized approaches when\ndealing with non-linear data distributions. (2) The use of appropriate\ndetrending techniques improves FL performance, reducing loss across different\ndata distributions."}
{"id": "2507.11486", "categories": ["cs.LG", "I.2.1"], "pdf": "https://arxiv.org/pdf/2507.11486", "abs": "https://arxiv.org/abs/2507.11486", "authors": ["Jeremi Levesque", "Antoine Théberge", "Maxime Descoteaux", "Pierre-Marc Jodoin"], "title": "Exploring the robustness of TractOracle methods in RL-based tractography", "comment": "38 pages, 8 figures. Submitted to Medical Image Analysis", "summary": "Tractography algorithms leverage diffusion MRI to reconstruct the fibrous\narchitecture of the brain's white matter. Among machine learning approaches,\nreinforcement learning (RL) has emerged as a promising framework for\ntractography, outperforming traditional methods in several key aspects.\nTractOracle-RL, a recent RL-based approach, reduces false positives by\nincorporating anatomical priors into the training process via a reward-based\nmechanism. In this paper, we investigate four extensions of the original\nTractOracle-RL framework by integrating recent advances in RL, and we evaluate\ntheir performance across five diverse diffusion MRI datasets. Results\ndemonstrate that combining an oracle with the RL framework consistently leads\nto robust and reliable tractography, regardless of the specific method or\ndataset used. We also introduce a novel RL training scheme called Iterative\nReward Training (IRT), inspired by the Reinforcement Learning from Human\nFeedback (RLHF) paradigm. Instead of relying on human input, IRT leverages\nbundle filtering methods to iteratively refine the oracle's guidance throughout\ntraining. Experimental results show that RL methods trained with oracle\nfeedback significantly outperform widely used tractography techniques in terms\nof accuracy and anatomical validity."}
{"id": "2507.11493", "categories": ["cs.LG", "cs.NE", "68T07"], "pdf": "https://arxiv.org/pdf/2507.11493", "abs": "https://arxiv.org/abs/2507.11493", "authors": ["Majid Darehmiraki"], "title": "A parametric activation function based on Wendland RBF", "comment": "11 pages, 2 figures", "summary": "This paper introduces a novel parametric activation function based on\nWendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,\nknown for their compact support, smoothness, and positive definiteness in\napproximation theory, are adapted to address limitations of traditional\nactivation functions like ReLU, sigmoid, and tanh. The proposed enhanced\nWendland activation combines a standard Wendland component with linear and\nexponential terms, offering tunable locality, improved gradient propagation,\nand enhanced stability during training. Theoretical analysis highlights its\nmathematical properties, including smoothness and adaptability, while empirical\nexperiments on synthetic tasks (e.g., sine wave approximation) and benchmark\ndatasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results\nshow that the Wendland-based activation achieves superior accuracy in certain\nscenarios, particularly in regression tasks, while maintaining computational\nefficiency. The study bridges classical RBF theory with modern deep learning,\nsuggesting that Wendland activations can mitigate overfitting and improve\ngeneralization through localized, smooth transformations. Future directions\ninclude hybrid architectures and domain-specific adaptations."}
{"id": "2507.11515", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11515", "abs": "https://arxiv.org/abs/2507.11515", "authors": ["Shiyi Yang", "Xiaoxue Yu", "Rongpeng Li", "Jianhang Zhu", "Zhifeng Zhao", "Honggang Zhang"], "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air", "comment": "11 pages, 8 figures", "summary": "Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air."}
{"id": "2507.11531", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.11531", "abs": "https://arxiv.org/abs/2507.11531", "authors": ["Yue Song", "T. Anderson Keller", "Yisong Yue", "Pietro Perona", "Max Welling"], "title": "Langevin Flows for Modeling Neural Latent Dynamics", "comment": "Full version of the Cognitive Computational Neuroscience (CCN) 2025\n  poster", "summary": "Neural populations exhibit latent dynamical structures that drive\ntime-evolving spiking activities, motivating the search for models that capture\nboth intrinsic network dynamics and external unobserved influences. In this\nwork, we introduce LangevinFlow, a sequential Variational Auto-Encoder where\nthe time evolution of latent variables is governed by the underdamped Langevin\nequation. Our approach incorporates physical priors -- such as inertia,\ndamping, a learned potential function, and stochastic forces -- to represent\nboth autonomous and non-autonomous processes in neural systems. Crucially, the\npotential function is parameterized as a network of locally coupled\noscillators, biasing the model toward oscillatory and flow-like behaviors\nobserved in biological neural populations. Our model features a recurrent\nencoder, a one-layer Transformer decoder, and Langevin dynamics in the latent\nspace. Empirically, our method outperforms state-of-the-art baselines on\nsynthetic neural populations generated by a Lorenz attractor, closely matching\nground-truth firing rates. On the Neural Latents Benchmark (NLB), the model\nachieves superior held-out neuron likelihoods (bits per spike) and forward\nprediction accuracy across four challenging datasets. It also matches or\nsurpasses alternative methods in decoding behavioral metrics such as hand\nvelocity. Overall, this work introduces a flexible, physics-inspired,\nhigh-performing framework for modeling complex neural population dynamics and\ntheir unobserved influences."}
{"id": "2507.10643", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10643", "abs": "https://arxiv.org/abs/2507.10643", "authors": ["Yuchi Tang", "Iñaki Esnaola", "Suzanne Mason", "George Panoutsos"], "title": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models", "comment": "17 pages, 6 figures, Submitted to NeurIPS 2025", "summary": "Existing post-hoc model-agnostic methods generate external explanations for\nopaque models, primarily by locally attributing the model output to its input\nfeatures. However, they often lack an explicit and systematic framework for\nquantifying the contribution of individual features. Building on the Taylor\nexpansion framework introduced by Deng et al. (2024) to unify existing local\nattribution methods, we propose a rigorous set of postulates -- \"precision\",\n\"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific\nattribution. Guided by these postulates, we introduce TaylorPODA (Taylor\nexpansion-derived imPortance-Order aDapted Attribution), which incorporates an\nadditional \"adaptation\" property. This property enables alignment with\ntask-specific goals, especially in post-hoc settings lacking ground-truth\nexplanations. Empirical evaluations demonstrate that TaylorPODA achieves\ncompetitive results against baseline methods, providing principled and\nvisualization-friendly explanations. This work represents a step toward the\ntrustworthy deployment of opaque models by offering explanations with stronger\ntheoretical grounding."}
{"id": "2507.10710", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10710", "abs": "https://arxiv.org/abs/2507.10710", "authors": ["Haoyu Chen", "Anna Little", "Akin Narayan"], "title": "Robust Multi-Manifold Clustering via Simplex Paths", "comment": null, "summary": "This article introduces a novel, geometric approach for multi-manifold\nclustering (MMC), i.e. for clustering a collection of potentially intersecting,\nd-dimensional manifolds into the individual manifold components. We first\ncompute a locality graph on d-simplices, using the dihedral angle in between\nadjacent simplices as the graph weights, and then compute infinity path\ndistances in this simplex graph. This procedure gives a metric on simplices\nwhich we refer to as the largest angle path distance (LAPD). We analyze the\nproperties of LAPD under random sampling, and prove that with an appropriate\ndenoising procedure, this metric separates the manifold components with high\nprobability. We validate the proposed methodology with extensive numerical\nexperiments on both synthetic and real-world data sets. These experiments\ndemonstrate that the method is robust to noise, curvature, and small\nintersection angle, and generally out-performs other MMC algorithms. In\naddition, we provide a highly scalable implementation of the proposed\nalgorithm, which leverages approximation schemes for infinity path distance to\nachieve quasi-linear computational complexity."}
{"id": "2507.10956", "categories": ["stat.ML", "cs.LG", "62-08", "G.3"], "pdf": "https://arxiv.org/pdf/2507.10956", "abs": "https://arxiv.org/abs/2507.10956", "authors": ["Zhaoyu Xing", "Yang Wan", "Juan Wen", "Wei Zhong"], "title": "GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering", "comment": null, "summary": "It is important to identify the discriminative features for high dimensional\nclustering. However, due to the lack of cluster labels, the regularization\nmethods developed for supervised feature selection can not be directly applied.\nTo learn the pseudo labels and select the discriminative features\nsimultaneously, we propose a new unsupervised feature selection method, named\nGlObal and Local information combined Feature Selection (GOLFS), for high\ndimensional clustering problems. The GOLFS algorithm combines both local\ngeometric structure via manifold learning and global correlation structure of\nsamples via regularized self-representation to select the discriminative\nfeatures. The combination improves the accuracy of both feature selection and\nclustering by exploiting more comprehensive information. In addition, an\niterative algorithm is proposed to solve the optimization problem and the\nconvergency is proved. Simulations and two real data applications demonstrate\nthe excellent finite-sample performance of GOLFS on both feature selection and\nclustering."}
{"id": "2507.11136", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11136", "abs": "https://arxiv.org/abs/2507.11136", "authors": ["Afra Kilic", "Kim Batselier"], "title": "Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection", "comment": "39 pages, 5 figures, 4 tables. Submitted to Journal of Machine\n  Learning Research. The code is available at:\n  https://github.com/afrakilic/BTN-Kernel-Machines. arXiv admin note: text\n  overlap with arXiv:1401.6497 by other authors", "summary": "Tensor Network (TN) Kernel Machines speed up model learning by representing\nparameters as low-rank TNs, reducing computation and memory use. However, most\nTN-based Kernel methods are deterministic and ignore parameter uncertainty.\nFurther, they require manual tuning of model complexity hyperparameters like\ntensor rank and feature dimensions, often through trial-and-error or\ncomputationally costly methods like cross-validation. We propose Bayesian\nTensor Network Kernel Machines, a fully probabilistic framework that uses\nsparsity-inducing hierarchical priors on TN factors to automatically infer\nmodel complexity. This enables automatic inference of tensor rank and feature\ndimensions, while also identifying the most relevant features for prediction,\nthereby enhancing model interpretability. All the model parameters and\nhyperparameters are treated as latent variables with corresponding priors.\nGiven the Bayesian approach and latent variable dependencies, we apply a\nmean-field variational inference to approximate their posteriors. We show that\napplying a mean-field approximation to TN factors yields a Bayesian ALS\nalgorithm with the same computational complexity as its deterministic\ncounterpart, enabling uncertainty quantification at no extra computational\ncost. Experiments on synthetic and real-world datasets demonstrate the superior\nperformance of our model in prediction accuracy, uncertainty quantification,\ninterpretability, and scalability."}
{"id": "2507.11161", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11161", "abs": "https://arxiv.org/abs/2507.11161", "authors": ["Jun Chen", "Hong Chen", "Yonghua Yu", "Yiming Ying"], "title": "How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction", "comment": "Accepted by ICML2025 as a poster", "summary": "In recent years, contrastive learning has achieved state-of-the-art\nperformance in the territory of self-supervised representation learning. Many\nprevious works have attempted to provide the theoretical understanding\nunderlying the success of contrastive learning. Almost all of them rely on a\ndefault assumption, i.e., the label consistency assumption, which may not hold\nin practice (the probability of failure is called labeling error) due to the\nstrength and randomness of common augmentation strategies, such as random\nresized crop (RRC). This paper investigates the theoretical impact of labeling\nerror on the downstream classification performance of contrastive learning. We\nfirst reveal several significant negative impacts of labeling error on\ndownstream classification risk. To mitigate these impacts, data dimensionality\nreduction method (e.g., singular value decomposition, SVD) is applied on\noriginal data to reduce false positive samples, and establish both theoretical\nand empirical evaluations. Moreover, it is also found that SVD acts as a\ndouble-edged sword, which may lead to the deterioration of downstream\nclassification accuracy due to the reduced connectivity of the augmentation\ngraph. Based on the above observations, we give the augmentation suggestion\nthat we should use some moderate embedding dimension (such as $512, 1024$ in\nour experiments), data inflation, weak augmentation, and SVD to ensure large\ngraph connectivity and small labeling error to improve model performance."}
{"id": "2507.11381", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.11381", "abs": "https://arxiv.org/abs/2507.11381", "authors": ["Rom Gutman", "Shimon Sheiba", "Omer Noy Klien", "Naama Dekel Bird", "Amit Gruber", "Doron Aronson", "Oren Caspi", "Uri Shalit"], "title": "From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies", "comment": null, "summary": "We propose a framework for building patient-specific treatment recommendation\nmodels, building on the large recent literature on learning patient-level\ncausal models and inspired by the target trial paradigm of Hernan and Robins.\nWe focus on safety and validity, including the crucial issue of causal\nidentification when using observational data. We do not provide a specific\nmodel, but rather a way to integrate existing methods and know-how into a\npractical pipeline. We further provide a real world use-case of treatment\noptimization for patients with heart failure who develop acute kidney injury\nduring hospitalization. The results suggest our pipeline can improve patient\noutcomes over the current treatment regime."}
{"id": "2507.11385", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11385", "abs": "https://arxiv.org/abs/2507.11385", "authors": ["George D. Pasparakis", "Ioannis A. Kougioumtzoglou", "Michael D. Shields"], "title": "Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning", "comment": null, "summary": "A methodology is developed, based on nonparametric Bayesian dictionary\nlearning, for joint space-time wind field data extrapolation and estimation of\nrelated statistics by relying on limited/incomplete measurements. Specifically,\nutilizing sparse/incomplete measured data, a time-dependent optimization\nproblem is formulated for determining the expansion coefficients of an\nassociated low-dimensional representation of the stochastic wind field.\nCompared to an alternative, standard, compressive sampling treatment of the\nproblem, the developed methodology exhibits the following advantages. First,\nthe Bayesian formulation enables also the quantification of the uncertainty in\nthe estimates. Second, the requirement in standard CS-based applications for an\na priori selection of the expansion basis is circumvented. Instead, this is\ndone herein in an adaptive manner based on the acquired data. Overall, the\nmethodology exhibits enhanced extrapolation accuracy, even in cases of\nhigh-dimensional data of arbitrary form, and of relatively large extrapolation\ndistances. Thus, it can be used, potentially, in a wide range of wind\nengineering applications where various constraints dictate the use of a limited\nnumber of sensors. The efficacy of the methodology is demonstrated by\nconsidering two case studies. The first relates to the extrapolation of\nsimulated wind velocity records consistent with a prescribed joint\nwavenumber-frequency power spectral density in a three-dimensional domain (2D\nand time). The second pertains to the extrapolation of four-dimensional (3D and\ntime) boundary layer wind tunnel experimental data that exhibit significant\nspatial variability and non-Gaussian characteristics."}
{"id": "2507.11535", "categories": ["stat.ML", "cs.LG", "cs.SY", "eess.SY", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.11535", "abs": "https://arxiv.org/abs/2507.11535", "authors": ["Andrey Bryutkin", "Matthew E. Levine", "Iñigo Urteaga", "Youssef Marzouk"], "title": "Canonical Bayesian Linear System Identification", "comment": "46 pages, 9 figures", "summary": "Standard Bayesian approaches for linear time-invariant (LTI) system\nidentification are hindered by parameter non-identifiability; the resulting\ncomplex, multi-modal posteriors make inference inefficient and impractical. We\nsolve this problem by embedding canonical forms of LTI systems within the\nBayesian framework. We rigorously establish that inference in these minimal\nparameterizations fully captures all invariant system dynamics (e.g., transfer\nfunctions, eigenvalues, predictive distributions of system outputs) while\nresolving identifiability. This approach unlocks the use of meaningful,\nstructure-aware priors (e.g., enforcing stability via eigenvalues) and ensures\nconditions for a Bernstein--von Mises theorem -- a link between Bayesian and\nfrequentist large-sample asymptotics that is broken in standard forms.\nExtensive simulations with modern MCMC methods highlight advantages over\nstandard parameterizations: canonical forms achieve higher computational\nefficiency, generate interpretable and well-behaved posteriors, and provide\nrobust uncertainty estimates, particularly from limited data."}
