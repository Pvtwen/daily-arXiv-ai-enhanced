<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 52]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Joint Motion, Angle, and Range Estimation in Near-Field under Array Calibration Imperfections](https://arxiv.org/abs/2507.13463)
*Ahmed Hussain,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil*

Main category: eess.SP

TL;DR: 论文提出了一种低复杂度算法，通过2D-DFT和MUSIC方法联合估计近场目标的位置和速度参数，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 在超大规模MIMO系统中，近场通信中目标位置和速度参数耦合，传统联合估计方法计算复杂度高。

Method: 将接收到的二维时空信号通过2D-DFT投影到角度-多普勒域，利用谱特性进行粗估计，再用一维MUSIC方法细化。

Result: 仿真结果显示，与最大似然估计相比，归一化均方误差为-40 dB，计算复杂度显著降低。

Conclusion: 该方法实现了近场目标运动参数的高效准确估计。

Abstract: Ultra-massive multiple-input multiple-output MIMO (UM-MIMO) leverages large
antenna arrays at high frequencies, transitioning communication paradigm into
the radiative near-field (NF), where spherical wavefronts enable full-vector
estimation of both target location and velocity. However, location and motion
parameters become inherently coupled in this regime, making their joint
estimation computationally demanding. To overcome this, we propose a novel
approach that projects the received two-dimensional space-time signal onto the
angle-Doppler domain using a two-dimensional discrete Fourier transform
(2D-DFT). Our analysis reveals that the resulting angular spread is centered at
the target's true angle, with its width determined by the target's range.
Similarly, transverse motion induces a Doppler spread centered at the true
radial velocity, with the width of Doppler spread proportional to the
transverse velocity. Exploiting these spectral characteristics, we develop a
low-complexity algorithm that provides coarse estimates of angle, range, and
velocity, which are subsequently refined using one-dimensional multiple signal
classification (MUSIC) applied independently to each parameter. The proposed
method enables accurate and efficient estimation of NF target motion
parameters. Simulation results demonstrate a normalized mean squared error
(NMSE) of -40 dB for location and velocity estimates compared to maximum
likelihood estimation, while significantly reducing computational complexity.

</details>


### [2] [Passive Body-Area Electrostatic Field (Human Body Capacitance) for Ubiquitous Computing](https://arxiv.org/abs/2507.13520)
*Sizhen Bian,Mengxi Liu,Paul Lukowicz*

Main category: eess.SP

TL;DR: 本文综述了被动人体电容（HBC）传感的原理、发展、硬件架构及应用，讨论了环境变化带来的挑战，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索人体静电特性在感知行为中的高效与非侵入性应用潜力。

Method: 综述被动HBC传感的原理、历史、硬件架构及应用，分析挑战并提出解决方案。

Result: 总结了HBC传感的多领域应用，并提供了开源资源以推动未来研究。

Conclusion: 被动HBC传感在可穿戴和环境智能系统中有广阔前景，未来需关注传感器融合和硬件优化。

Abstract: Passive body-area electrostatic field sensing, also referred to as human body
capacitance (HBC), is an energy-efficient and non-intrusive sensing modality
that exploits the human body's inherent electrostatic properties to perceive
human behaviors. This paper presents a focused overview of passive HBC sensing,
including its underlying principles, historical evolution, hardware
architectures, and applications across research domains. Key challenges, such
as susceptibility to environmental variation, are discussed to trigger
mitigation techniques. Future research opportunities in sensor fusion and
hardware enhancement are highlighted. To support continued innovation, this
work provides open-source resources and aims to empower researchers and
developers to leverage passive electrostatic sensing for next-generation
wearable and ambient intelligence systems.

</details>


### [3] [Space Shift Keying-Enabled ISAC for Efficient Debris Detection and Communication in LEO Satellite Networks](https://arxiv.org/abs/2507.13526)
*Gedeon Ghislain Nkwewo Ngoufo,Khaled Humadi,Elham Baladi,Gunes Karabulut Kurt*

Main category: eess.SP

TL;DR: 研究探讨了在ISAC框架中使用SSK调制，结合正弦和啁啾雷达波形，验证了SSK在通信和感知中的有效性。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道（LEO）空间碎片的增加对轨道安全构成挑战，ISAC系统通过集成感知和通信功能提供解决方案。

Method: 研究采用空间移位键控（SSK）调制，结合正弦和啁啾雷达波形，评估其性能。

Result: 两种波形在SSK下具有相当的误码率性能，但啁啾波形在感知能力（如距离和速度检测）上表现更优。

Conclusion: SSK是ISAC的有效调制方案，波形选择对优化感知能力至关重要，支持空间碎片监测的高效系统设计。

Abstract: The proliferation of space debris in low Earth orbit (LEO) presents critical
challenges for orbital safety, particularly for satellite constellations.
Integrated sensing and communication (ISAC) systems provide a promising dual
function solution by enabling both environmental sensing and data
communication. This study explores the use of space shift keying (SSK)
modulation within ISAC frameworks, evaluating its performance when combined
with sinusoidal and chirp radar waveforms. SSK is particularly attractive due
to its low hardware complexity and robust communication performance. Our
results demonstrate that both waveforms achieve comparable bit error rate (BER)
performance under SSK, validating its effectiveness for ISAC applications.
However, waveform selection significantly affects sensing capability: while the
sinusoidal waveform supports simpler implementation, its high ambiguity limits
range detection. In contrast, the chirp waveform enables range estimation and
provides a modest improvement in velocity detection accuracy. These findings
highlight the strength of SSK as a modulation scheme for ISAC and emphasize the
importance of selecting appropriate waveforms to optimize sensing accuracy
without compromising communication performance. This insight supports the
design of efficient and scalable ISAC systems for space applications,
particularly in the context of orbital debris monitoring.

</details>


### [4] [Sensing and Stopping Interfering Secondary Users: Validation of an Efficient Spectrum Sharing System](https://arxiv.org/abs/2507.13554)
*Meles Weldegebriel,Zihan Li,Dustin Maas,Greg Hellbourg,Ning Zhang,Neal Patwari*

Main category: eess.SP

TL;DR: StopSec是一种隐私保护协议，能快速识别并停止次级用户对主用户的干扰。


<details>
  <summary>Details</summary>
Motivation: 解决次级用户对主用户干扰的问题，确保频谱共享的高效性和隐私保护。

Method: 引入轻量级水印方法标记次级用户的OFDM数据包，并通过数据库反馈机制实现快速干扰停止。

Result: 实验表明，StopSec能在150毫秒内停止干扰，且对次级用户数据链路无负面影响。

Conclusion: StopSec是一种高效的频谱共享协议，适用于需要快速自动停止干扰的场景。

Abstract: We present the design and validation of Stoppable Secondary Use (StopSec), a
privacy-preserving protocol with the capability to identify a secondary user
(SU) causing interference to a primary user (PU) and to act quickly to stop the
interference. All users are served by a database that provides a feedback
mechanism from a PU to an interfering SU. We introduce a new lightweight and
robust method to watermark an SU's OFDM packet. Through extensive over-the-air
real-time experiments, we evaluate StopSec in terms of interference detection,
identification, and stopping latency, as well as impact on SUs. We show that
the watermarking method avoids negative impact to the secondary data link and
is robust to real-world time-varying channels. Interfering SUs can be stopped
in under 150 milliseconds, and when multiple users are simultaneously
interfering, they can all be stopped. Even when the interference is 10 dB lower
than the noise power, StopSec successfully stops interfering SUs within a few
seconds of their appearance in the channel. StopSec can be an effective
spectrum sharing protocol for cases when interference to a PU must be quickly
and automatically stopped.

</details>


### [5] [Device-Free Localization Using Commercial UWB Transceivers](https://arxiv.org/abs/2507.13938)
*Hyun Seok Lee*

Main category: eess.SP

TL;DR: 论文提出了一种基于深度学习的粒子滤波方法，用于解决超宽带（UWB）设备自由定位中的低信噪比和环境干扰问题，实现了高精度且低成本的目标定位。


<details>
  <summary>Details</summary>
Motivation: 商业UWB收发器虽然可以实现设备自由定位，但在实际场景中，由于低信噪比和环境干扰，目标位置估计精度较低。

Method: 通过分析信道脉冲响应（CIR）方差捕捉目标运动变化，使用基于深度学习的注意力U-Net提取目标反射成分并抑制噪声，最后通过粒子滤波估计目标位置。

Result: 实验结果显示，系统在物联网和汽车应用中具有实用性，均方根误差约15厘米，平均处理时间4毫秒，性能优于现有方法。

Conclusion: 该方法为UWB设备自由定位提供了一种高效且经济的解决方案，适用于实际应用场景。

Abstract: Recently, commercial ultra-wideband (UWB) transceivers have enabled not only
measuring device-to-device distance but also tracking the position of a
pedestrian who does not carry a UWB device. UWB-based device-free localization
that does not require dedicated radar equipment is compatible with existing
anchor infrastructure and can be reused to reduce hardware deployment costs.
However, it is difficult to estimate the target's position accurately in
real-world scenarios due to the low signal-to-noise ratio (SNR) and the
cluttered environment. In this paper, we propose a deep learning (DL)-assisted
particle filter to overcome these challenges. First, the channel impulse
response (CIR) variance is analyzed to capture the variability induced by the
target's movement. Then, a DL-based one-dimensional attention U-Net is used to
extract only the reflection components caused by the target and suppress the
noise components within the CIR variance profile. Finally, multiple
preprocessed CIR variance profiles are used as input to a particle filter to
estimate the target's position. Experimental results demonstrate that the
proposed system is a practical and cost-effective solution for IoT and
automotive applications with a root mean square error (RMSE) of about 15 cm and
an average processing time of 4 ms. Furthermore, comparisons with existing
state-of-the-art methods show that the proposed method provides the best
performance with reasonable computational costs.

</details>


### [6] [Towards channel foundation models (CFMs): Motivations, methodologies and opportunities](https://arxiv.org/abs/2507.13637)
*Jun Jiang,Yuan Gao,Xinyi Wu,Shugong Xu*

Main category: eess.SP

TL;DR: 本文提出了一种名为通道基础模型（CFMs）的新框架，用于解决无线通信系统中AI模型的局限性，如依赖标注数据和泛化能力差。CFMs利用自监督学习处理大规模未标注数据。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型在无线通信系统中存在依赖标注数据、泛化能力有限等问题，需要一种更通用的解决方案。

Method: 提出CFMs框架，利用自监督学习和先进AI架构，实现通用通道特征提取。

Result: CFMs能够有效利用未标注数据，减少人工标注需求，并支持多种通道相关任务。

Conclusion: CFMs为无线通信系统中的AI应用提供了新方向，未来需在模型架构和数据集构建上进一步研究。

Abstract: Artificial intelligence (AI) has emerged as a pivotal enabler for
next-generation wireless communication systems. However, conventional AI-based
models encounter several limitations, such as heavy reliance on labeled data,
limited generalization capability, and task-specific design. To address these
challenges, this paper introduces, for the first time, the concept of channel
foundation models (CFMs)-a novel and unified framework designed to tackle a
wide range of channel-related tasks through a pretrained, universal channel
feature extractor. By leveraging advanced AI architectures and self-supervised
learning techniques, CFMs are capable of effectively exploiting large-scale
unlabeled data without the need for extensive manual annotation. We further
analyze the evolution of AI methodologies, from supervised learning and
multi-task learning to self-supervised learning, emphasizing the distinct
advantages of the latter in facilitating the development of CFMs. Additionally,
we provide a comprehensive review of existing studies on self-supervised
learning in this domain, categorizing them into generative, discriminative and
the combined paradigms. Given that the research on CFMs is still at an early
stage, we identify several promising future research directions, focusing on
model architecture innovation and the construction of high-quality, diverse
channel datasets.

</details>


### [7] [Elastic Buffer Design for Real-Time All-Digital Clock Recovery Enabling Free-Running Receiver Clock with Negative and Positive Clock Frequency Offsets](https://arxiv.org/abs/2507.13748)
*Patrick Matalla,Joel Dittmer,Md Salek Mahmud,Christian Koos,Sebastian Randel*

Main category: eess.SP

TL;DR: 提出一种弹性缓冲设计，支持全数字时钟恢复，接收端时钟自由运行且支持正负频率偏移。


<details>
  <summary>Details</summary>
Motivation: 解决传统时钟恢复方法在正负频率偏移下的限制，实现更灵活的数据传输。

Method: 设计弹性缓冲器，允许接收端时钟自由运行，支持正负频率偏移。

Result: 实验证明，在-400 ppm至+400 ppm范围内实现无误码实时数据传输。

Conclusion: 该弹性缓冲设计有效解决了时钟频率偏移问题，适用于高灵活性数据传输场景。

Abstract: We present an elastic buffer design that enables all-digital clock recovery
implementation with free-running receiver clock featuring negative and positive
clock frequency offsets. Error-free real-time data transmission is demonstrated
from -400 ppm to +400 ppm.

</details>


### [8] [ISAC: From Human to Environmental Sensing](https://arxiv.org/abs/2507.13766)
*Kai Wu,Zhongqin Wang,Shu-Lin Chen,J. Andrew Zhang,Y. Jay Guo*

Main category: eess.SP

TL;DR: 本文首次统一综述了集成感知与通信（ISAC）在人类活动和环境感知中的应用，重点关注信号级机制、感知特征和实际可行性。


<details>
  <summary>Details</summary>
Motivation: 6G无线通信系统需要同时支持高吞吐量通信和情境感知，但现有研究在人类中心应用和环境监测领域较为分散。

Method: 通过分析无线信号传播中人类活动和环境现象的影响，提取可测量的信号特征（如CSI、多普勒剖面），并结合实验验证非视距条件下的可行性。

Result: 实验结果表明，ISAC在基础设施受限的场景下具有可行性，能够支持人类定位、活动识别、生命体征监测以及降雨、土壤湿度等环境感知。

Conclusion: 未来研究需解决信号融合、领域适应和通用感知架构等挑战，以实现可扩展和自主的ISAC。

Abstract: Integrated Sensing and Communications (ISAC) is poised to become one of the
defining capabilities of the sixth generation (6G) wireless communications
systems, enabling the network infrastructure to jointly support high-throughput
communications and situational awareness. While recent advances have explored
ISAC for both human-centric applications and environmental monitoring, existing
research remains fragmented across these domains. This paper provides the first
unified review of ISAC-enabled sensing for both human activities and
environment, focusing on signal-level mechanisms, sensing features, and
real-world feasibility. We begin by characterising how diverse physical
phenomena, ranging from human vital sign and motion to precipitation and flood
dynamics, impact wireless signal propagation, producing measurable signatures
in channel state information (CSI), Doppler profiles, and signal statistics. A
comprehensive analysis is then presented across two domains: human sensing
applications including localisation, activity recognition, and vital sign
monitoring; and environmental sensing for rainfall, soil moisture, and water
level. Experimental results from Long-Term Evolution (LTE) sensing under
non-line-of-sight (NLOS) conditions are incorporated to highlight the
feasibility in infrastructure-limited scenarios. Open challenges in signal
fusion, domain adaptation, and generalisable sensing architectures are
discussed to facilitate future research toward scalable and autonomous ISAC.

</details>


### [9] [Simulation for Noncontact Radar-Based Physiological Sensing Using Depth-Camera-Derived Human 3D Model with Electromagnetic Scattering Analysis](https://arxiv.org/abs/2507.13826)
*Kimitaka Sumi,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 提出一种基于深度相机获取的人体几何和位移数据模拟调频连续波雷达呼吸监测信号的方法，相比传统模型方法在雷达图像、位移和频谱图相关性上分别提升7.5%、58.2%和3.2%。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖简化的人体几何或位移模型，无法准确模拟高频散射中心，本研究旨在通过真实深度相机数据提升模拟精度。

Method: 利用深度相机获取真实人体形状和运动数据，模拟高频散射中心，并在不同条件下（目标距离、坐姿、雷达类型）进行实验验证。

Result: 与传统方法相比，雷达图像、位移和频谱图的相关系数分别提高了7.5%、58.2%和3.2%。

Conclusion: 该方法通过模拟生成雷达生理数据集，提升了非接触传感的准确性理解。

Abstract: This study proposes a method for simulating signals received by
frequency-modulated continuous-wave radar during respiratory monitoring, using
human body geometry and displacement data acquired via a depth camera. Unlike
previous studies that rely on simplified models of body geometry or
displacement, the proposed approach models high-frequency scattering centers
based on realistic depth-camera-measured body shapes and motions. Experiments
were conducted with six participants under varying conditions, including
varying target distances, seating orientations, and radar types, with
simultaneous acquisition from the radar and depth camera. Relative to
conventional model-based methods, the proposed technique achieved improvements
of 7.5%, 58.2%, and 3.2% in the correlation coefficients of radar images,
displacements, and spectrograms, respectively. This work contributes to the
generation of radar-based physiological datasets through simulation and
enhances our understanding of factors affecting the accuracy of non-contact
sensing.

</details>


### [10] [On two fundamental properties of the zeros of spectrograms of noisy signals](https://arxiv.org/abs/2507.13829)
*Arnaud Poinas,Rémi Bardenet*

Main category: eess.SP

TL;DR: 论文研究了信号加入高斯白噪声后，谱图零点的空间分布变化，发现零点会勾勒信号支撑区域，并在干扰下形成确定性结构。通过简单信号模型，作者用零点密度和Rouché定理解释了这一现象。


<details>
  <summary>Details</summary>
Motivation: 探讨信号在噪声中如何通过谱图零点分布被识别，以及干扰下零点形成的确定性结构。

Method: 使用简单信号模型，结合零点密度和Rouché定理分析零点分布。

Result: 发现零点能勾勒信号支撑区域，干扰下形成易检测的确定性结构。

Conclusion: 零点分布可用于信号检测，干扰下仍能保持清晰结构。

Abstract: The spatial distribution of the zeros of the spectrogram is significantly
altered when a signal is added to white Gaussian noise. The zeros tend to
delineate the support of the signal, and deterministic structures form in the
presence of interference, as if the zeros were trapped. While sophisticated
methods have been proposed to detect signals as holes in the pattern of
spectrogram zeros, few formal arguments have been made to support the
delineation and trapping effects. Through detailed computations for simple toy
signals, we show that two basic mathematical arguments, the intensity of zeros
and Rouch\'e's theorem, allow discussing delineation and trapping, and the
influence of parameters like the signal-to-noise ratio. In particular,
interfering chirps, even nearly superimposed, yield an easy-to-detect
deterministic structure among zeros.

</details>


### [11] [Distortion-Aware Hybrid Beamforming for Integrated Sensing and Communication](https://arxiv.org/abs/2507.14018)
*Zeyuan Zhang,Yue Xiu,Phee Lep Yeoh,Guangyi Liu,Zixing Wu,Ning Wei*

Main category: eess.SP

TL;DR: 论文研究了带有非线性功率放大失真的部分连接混合波束成形发射机，用于集成感知与通信（ISAC），提出了一种失真感知的混合波束成形设计方法。


<details>
  <summary>Details</summary>
Motivation: 解决ISAC系统中非线性功率放大失真对通信速率和感知互信息的影响。

Method: 通过流形优化（MO）和闭式解交替求解三个子问题，得到全数字波束成形矩阵，再通过分解算法获得模拟和数字波束成形矩阵。

Result: 数值结果表明，所提算法相比传统波束成形方法能提升ISAC整体性能。

Conclusion: 提出的失真感知混合波束成形设计有效提升了ISAC系统的性能。

Abstract: This paper investigates a practical partially-connected hybrid beamforming
transmitter for integrated sensing and communication (ISAC) with distortion
from nonlinear power amplification. For this ISAC system, we formulate a
communication rate and sensing mutual information maximization problem driven
by our distortion-aware hybrid beamforming design. To address this non-convex
problem, we first solve for a fully digital beamforming matrix by alternatively
solving three sub-problems using manifold optimization (MO) and our derived
closed-form solutions. The analog and digital beamforming matrices are then
obtained through a decomposition algorithm. Numerical results demonstrate that
the proposed algorithm can improve overall ISAC performance compared to
traditional beamforming methods.

</details>


### [12] [Toward Practical Fluid Antenna Systems: Co-Optimizing Hardware and Software for Port Selection and Beamforming](https://arxiv.org/abs/2507.14035)
*Sai Xu,Kai-Kit Wong,Yanan Du,Hanjiang Hong,Chan-Byoung Chae,Baiyang Liu,Kin-Fai Tong*

Main category: eess.SP

TL;DR: 提出了一种硬件-软件协同设计方法，用于优化流体天线系统中的波束成形和端口选择。结合图神经网络和随机端口选择，并通过FPGA加速器实现低延迟推理。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统在多小区MIMO网络中需要高效优化波束成形和端口选择，以提升加权和速率（WSR）。

Method: 1. 建模FA-enabled多小区MIMO网络并优化WSR；2. 提出GNN-RPS方法联合优化波束成形和端口选择；3. 开发基于FPGA的深度学习加速器以减少推理延迟；4. 引入调度算法减少冗余计算。

Result: GNN-RPS方法在通信性能上表现优异；FPGA加速器在低延迟下支持多端口选择的波束成形推理。

Conclusion: 硬件-软件协同设计有效提升了流体天线系统的性能，GNN-RPS和FPGA加速器的结合展示了高效且低延迟的解决方案。

Abstract: This paper proposes a hardware-software co-design approach to efficiently
optimize beamforming and port selection in fluid antenna systems (FASs). To
begin with, a fluid-antenna (FA)-enabled downlink multi-cell multiple-input
multiple-output (MIMO) network is modeled, and a weighted sum-rate (WSR)
maximization problem is formulated. Second, a method that integrates graph
neural networks (GNNs) with random port selection (RPS) is proposed to jointly
optimize beamforming and port selection, while also assessing the benefits and
limitations of random selection. Third, an instruction-driven deep learning
accelerator based on a field-programmable gate array (FPGA) is developed to
minimize inference latency. To further enhance efficiency, a scheduling
algorithm is introduced to reduce redundant computations and minimize the idle
time of computing cores. Simulation results demonstrate that the proposed
GNN-RPS approach achieves competitive communication performance. Furthermore,
experimental evaluations indicate that the FPGA-based accelerator maintains low
latency while simultaneously executing beamforming inference for multiple port
selections.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: 论文从物理角度分析了Transformer架构，提出了一种基于开放量子系统的物理模型。


<details>
  <summary>Details</summary>
Motivation: 填补对Transformer架构理论理解的空白，尤其是其物理工作原理。

Method: 在Fock空间上构建物理模型，将大型语言模型视为开放量子系统。

Result: 提出了支持Transformer架构的物理模型。

Conclusion: 从物理角度为Transformer架构提供了理论基础。

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [14] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: 该论文提出了一种多元对齐方法，通过引入DIVE数据集和实证研究，改进文本到图像（T2I）模型的对齐性，以更好地反映多样的人类价值观。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型未能充分体现多样的人类经验，导致系统与人类价值观不一致。论文旨在通过多元对齐解决这一问题。

Method: 1. 引入DIVE数据集，用于多元对齐的多模态评估；2. 实证研究人口统计学因素对观点多样性的影响；3. 探讨对齐T2I模型的策略。

Result: DIVE数据集成功捕捉了多样化的安全观点，实证显示人口统计学因素是观点多样性的关键代理。

Conclusion: 论文为构建更公平和对齐的T2I系统提供了基础工具，强调了多元对齐的重要性。

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [15] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: 论文探讨了在机器学习中使用CDF归一化的优势，特别是在Kolmogorov-Arnold Networks（KANs）中的应用，相比传统归一化方法能提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习中常用的归一化方法（如均值标准差或固定范围缩放）在金融领域的copula理论中较少使用。作者希望通过展示CDF归一化的优势，填补这一空白。

Method: 使用CDF归一化方法，将数据转换为近似均匀分布，应用于Kolmogorov-Arnold Networks（KANs），并与传统方法（如Legendre-KAN）进行对比。

Result: 在KANs中，CDF归一化显著提升了预测性能。此外，HCR解释表明，神经元权重可以建模局部联合分布，并支持概率分布传播和方向调整。

Conclusion: CDF归一化在机器学习中具有潜力，尤其是在KANs中表现优异，为数据表示和模型优化提供了新思路。

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [16] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 提出了一种名为选择性嵌入的新数据加载策略，通过交替多源数据短片段提升深度学习模型的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在非平稳条件和跨域数据中表现不佳，传统数据加载策略泛化能力有限或计算成本高。

Method: 选择性嵌入策略，在单一输入通道中交替多源数据短片段，模仿人类信息处理方式。

Result: 在六个时域数据集上验证，分类准确率高且显著减少训练时间。

Conclusion: 该方法在多源数据复杂系统中表现优异，适用于医疗、重机械等领域。

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [17] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab是一个结合LLM代码生成和多个AutoML工具的多代理系统，用于处理表格数据任务，提高了灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前AutoML在处理复杂任务时效率受限于对特定底层工具的依赖。

Method: 结合LLM代码生成和多个AutoML工具，设计多代理系统。

Result: 在多个Kaggle数据科学任务上优于现有开源解决方案。

Conclusion: LightAutoDS-Tab通过多代理系统提升了AutoML的灵活性和鲁棒性。

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [18] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models是一种新型生成流模型，通过在学习流ODE中引入可学习的Gauge Field，性能优于传统流模型。


<details>
  <summary>Details</summary>
Motivation: 传统流模型在某些任务中表现有限，因此需要一种更高效的生成模型。

Method: 在流ODE中引入可学习的Gauge Field，构建数学框架并通过高斯混合模型实验验证。

Result: 实验显示Gauge Flow Models性能显著优于传统流模型，且未发表研究显示其在更广泛任务中潜力。

Conclusion: Gauge Flow Models是一种有前景的新型生成模型，性能优越且潜力巨大。

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [19] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: 论文探讨了模型自由强化学习（RL）的局限性，提出模型驱动的智能体作为替代方案，结合系统动力学、成本和约束模型，以提高样本效率、安全性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代自主系统需要高效、安全和可解释的决策方法，而模型自由RL存在样本效率低、学习不安全及可解释性差的问题。

Method: 提出模型驱动的智能体，利用系统动力学、成本和约束模型进行策略学习，并通过贝叶斯优化、策略搜索RL和离线策略等方法实现。

Result: 模型驱动的智能体能够结合先验知识，提高学习效率和安全性，同时模型自由RL可弥补模型不匹配的缺陷。

Conclusion: 模型驱动与模型自由RL的结合为高效、安全和可解释的决策智能体提供了新的研究方向。

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [20] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 论文提出了一种层次化的多保真度数据驱动学习方法，能够量化认知不确定性并区分数据噪声，适用于从简单单保真度神经网络到复杂贝叶斯循环神经网络的多种场景。


<details>
  <summary>Details</summary>
Motivation: 解决多保真度数据驱动学习中认知不确定性和数据噪声的量化与区分问题，以支持更广泛的实际应用。

Method: 提出了一种层次化的多保真度学习方法，包括单保真度确定性神经网络和多保真度贝叶斯循环神经网络。

Result: 方法能够准确预测响应、量化模型误差，并发现噪声分布（若存在），展示了其通用性和适应性。

Conclusion: 该方法为科学和工程领域中的不确定性设计和分析提供了新的可能性。

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [21] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: 论文提出了一种基于信念函数的聚类算法Soft-ECM，适用于复杂数据（如混合数据和时间序列），解决了现有算法无法处理非欧几里得空间数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于信念函数的聚类算法无法处理复杂数据（如混合数据或时间序列），因为它们依赖于欧几里得空间的性质。

Method: 提出Soft-ECM算法，仅需半度量即可定位不精确簇的质心，适用于非欧几里得空间数据。

Result: 实验表明，Soft-ECM在数值数据上与模糊聚类方法效果相当，并能有效处理混合数据和时间序列。

Conclusion: Soft-ECM扩展了信念函数聚类的应用范围，为复杂数据提供了一种有效的解决方案。

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [22] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出了一种拜占庭容错的联邦高斯过程回归算法，用于在部分代理存在拜占庭故障时协作学习潜在函数。


<details>
  <summary>Details</summary>
Motivation: 研究如何在代理存在拜占庭行为（任意或敌对）的情况下，通过联邦学习提升高斯过程回归的性能。

Method: 设计了一种拜占庭容错的联邦GPR算法，包括本地GPR预测、云端聚合规则和全局模型广播，代理通过融合全局模型优化本地预测。

Result: 量化了融合GPR相比本地GPR的精度提升，并在玩具示例和真实数据集上验证了算法性能。

Conclusion: 该算法有效提升了在拜占庭代理存在时的联邦学习性能。

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [23] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: 本文提出了一种基于图神经网络（GNN）的可解释框架，用于实时评估空中交通管制员（ATCO）的任务需求，优于现有启发式方法和基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有复杂性指标难以捕捉空中交通管制任务中的细微操作驱动因素，需要一种更可靠的方法来评估任务需求。

Method: 采用注意力机制的图神经网络（GNN）框架，通过静态交通场景中的交互预测即将发布的指令数量，并通过系统消融飞机来生成可解释的任务需求评分。

Result: 该框架显著优于ATCO启发式方法，并比现有基线模型更可靠地估计场景复杂性。

Conclusion: 该工具能够将任务需求归因于特定飞机，为控制器培训和空域重新设计提供了新的分析手段。

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [24] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: 多目标贝叶斯优化（MOBO）在分子设计中优于标量化方法，尤其是在低数据量和复杂权衡情况下。


<details>
  <summary>Details</summary>
Motivation: 探讨MOBO在分子设计中的实际优势，尤其是与标量化方法相比的性能差异。

Method: 使用基于Pareto的MOBO策略（EHVI）与固定权重的标量化方法（EI）进行对比实验，控制变量相同。

Result: EHVI在Pareto前沿覆盖、收敛速度和化学多样性方面均优于EI，尤其在低数据量下。

Conclusion: Pareto感知的获取策略在分子优化中具有实际优势，特别是在预算有限和权衡复杂的情况下。

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [25] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 提出了一种跨模态自监督预训练方法，用于从无标签的IMU-视频数据中学习表示，提高了在分布外IMU数据集上的人体活动识别（HAR）任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于IMU的HAR方法依赖特定应用标签、泛化能力不足的问题，特别是在不同环境或人群中的数据。

Method: 采用跨模态自监督预训练方法，利用大规模无标签的IMU-视频数据学习表示。

Result: 在零样本和少样本评估中，该方法优于当前最先进的IMU-视频预训练方法和仅IMU预训练方法。

Conclusion: 跨模态预训练是学习IMU信号等动态数据模态中通用表示的有效工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [26] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: 论文提出了一种结合时间因果表示学习与非规则张量分解的框架CaRTeD，用于高维变长数据，理论证明其收敛性，实验验证其在真实数据上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现实数据多为高维且长度不一的非规则张量，现有方法难以有效提取因果信息，需结合时间因果表示与张量分解。

Method: 提出CaRTeD框架，整合时间因果表示学习与非规则张量分解，提供灵活正则化设计，理论证明算法收敛性。

Result: 实验证明CaRTeD在合成和真实数据（如MIMIC-III）上优于现有方法，提升了因果表示的可解释性。

Conclusion: CaRTeD填补了非规则张量分解理论空白，为下游任务提供了有效工具，具有实际应用价值。

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


### [27] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: Kaggle竞赛“Fake or Real”旨在解决AI安全威胁，要求参与者区分正常和恶意修改的LLM输出。


<details>
  <summary>Details</summary>
Motivation: 竞赛基于欧洲航天局资助的项目，关注数据投毒和对大型语言模型的过度依赖问题。

Method: 参与者需开发新技术或调整现有方法，以区分正常和恶意修改的LLM输出。

Result: 暂无具体结果，竞赛旨在推动相关研究。

Conclusion: 该竞赛为解决AI安全威胁提供了新思路和研究方向。

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [28] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: 本文提出了一个双收敛的统一框架，解释了大型语言模型（LLMs）如何通过上下文学习（ICL）从输入序列中学习新行为，而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 研究ICL的机制，解释LLMs如何通过隐藏表示内部化数据生成过程（DGP）的结构。

Method: 引入双收敛框架，分析隐藏表示在上下文和层间的收敛过程，证明其对低频表示的隐式偏好。

Result: 理论解释了ICL中表示的结构化几何和能量衰减现象，并预测了其对高频噪声的鲁棒性。

Conclusion: 研究为ICL的机制提供了理论支持，并为更广泛的数据分布和设置奠定了基础。

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [29] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: 论文提出了一种名为Acoustic Index的新型AI衍生超声心动图参数，用于量化心脏功能障碍，结合了Koopman算子理论和混合神经网络，并在临床测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统超声心动图参数（如EF和GLS）在早期检测心脏功能障碍时存在局限性，需要一种可重复、可解释且操作者无关的新参数。

Method: 通过结合Koopman算子理论的EDMD和混合神经网络，提取超声心动图序列的时空动态特征，并结合临床数据生成连续评分。

Result: 在736名患者的独立测试集中，Acoustic Index的AUC达到0.89，交叉验证显示敏感性和特异性均超过0.8。

Conclusion: Acoustic Index是一种物理驱动的可解释AI生物标志物，有望成为早期检测和监测心脏功能的工具，未来需进一步验证和优化。

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [30] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: 论文提出两种指标（谱可预测性分数和最大李雅普诺夫指数）来量化时间序列的预测性，避免传统模型评估的滞后性。通过合成和真实数据验证，这两种指标能有效反映预测性，并与实际模型表现强相关。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估指标在模型开发后才起作用，无法提前评估时间序列的预测性。本文旨在通过谱可预测性分数和李雅普诺夫指数，提前量化数据的预测性特征。

Method: 使用谱可预测性分数评估时间序列频率成分的强度和规律性，李雅普诺夫指数量化数据生成系统的混沌和稳定性。通过合成数据和M5预测竞赛的真实数据验证。

Result: 两种指标能准确反映时间序列的固有预测性，并与多种模型的实际预测表现强相关。

Conclusion: 通过提前评估时间序列的预测性，实践者可以更合理地分配资源，对可预测性高的产品投入更多精力，对预测性有限的产品采取替代策略。

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [31] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: SELF-Transformer通过内部迭代更新注意力权重，提升编码器Transformer的表达能力，无需依赖自回归。


<details>
  <summary>Details</summary>
Motivation: 提升编码器Transformer的表达能力，避免依赖外部化的中间状态（如自回归）。

Method: 引入SELF-Transformer，通过迭代更新注意力权重至固定点，实现输入自适应对齐。

Result: 在编码器基准测试中，准确率提升高达20%，且不增加参数量。

Conclusion: SELF-Transformer在保持编码器架构简洁的同时，显著提升了表达能力。

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [32] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: 苹果推出了两款多语言、多模态的基础语言模型，分别用于设备端和服务器端，支持多语言、图像理解和工具调用，性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为苹果设备和服务提供智能功能，同时兼顾性能、成本与隐私保护。

Method: 设备端模型采用KV缓存共享和2位量化训练；服务器模型采用并行轨道混合专家（PT-MoE）架构，结合稀疏计算和全局-局部注意力。训练数据来自网络爬取、授权语料和合成数据，并通过监督微调和强化学习优化。

Result: 两款模型在公共基准测试和人工评估中表现优于或持平同类开源模型。

Conclusion: 苹果通过技术创新和负责任AI策略，实现了高性能、隐私保护的智能模型，并提供了开发者友好的框架。

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [33] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: PLUS框架通过生成用户偏好摘要，实现个性化奖励模型，提升LLM对不同用户的响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法无法区分用户差异，需个性化响应。

Method: PLUS框架学习用户偏好摘要，动态更新奖励模型。

Result: PLUS能捕捉用户偏好，适应新用户和话题，并可零迁移至GPT-4。

Conclusion: PLUS提供透明、可解释的用户摘要，增强LLM个性化与用户控制。

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [34] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出两种新的离线策略评估（OPE）方法DiPS和DPR，针对匹配市场的双向性和大规模特性，优化了偏差-方差控制，并在理论和实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统OPE方法在匹配市场中因双向交互和大规模数据导致方差问题和奖励稀疏性，效果不佳，需要针对性的改进方法。

Method: 结合直接法（DM）、逆倾向得分（IPS）和双重稳健（DR）估计器，并引入中间标签（如初始参与信号）以优化偏差-方差控制。

Result: 理论分析和实验表明，DiPS和DPR在偏差和方差控制上优于传统方法，并能扩展到离线策略学习中。

Conclusion: 提出的方法在匹配市场的离线评估和学习任务中表现优越，为推荐策略优化提供了有效工具。

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [35] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: 提出了一种结合GCN、AE和Graph Transformer的Tri-GFN框架，通过三学习机制和特征融合策略提升图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 解决GCN在大规模复杂图数据中的过平滑和过压缩问题，以及Graph Transformer在异构图数据中的性能限制。

Method: 结合GCN、AE和Graph Transformer，通过三学习机制和特征融合策略增强全局与局部信息的区分度和一致性。

Result: 在ACM、Reuters和USPS数据集上性能显著提升，分别提高0.87%、14.14%和7.58%。

Conclusion: Tri-GFN框架在异构图数据上表现优异，适用于新闻分类和主题检索等领域。

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [36] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin是一种基于服务器端数字孪生的客户端跳过算法，通过LSTM预测梯度更新幅度和不确定性，减少通信开销，提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的通信开销是主要瓶颈，尤其在移动和物联网设备带宽受限的场景下。

Method: 使用LSTM实现的数字孪生预测客户端梯度更新幅度和不确定性，仅在预测值超过阈值时请求通信。

Result: 在非IID数据分布下，FedSkipTwin减少通信12-15.5%，同时模型精度提升0.5个百分点。

Conclusion: 预测引导的跳过策略在带宽受限的边缘环境中是实用且有效的。

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [37] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: 本文综述了基于Transformer的语言模型在蛋白质序列分析与设计中的最新进展，包括基因本体、功能与结构蛋白识别、蛋白质生成与结合等应用，并探讨了现有研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP领域的成功促使其在生物信息学中的应用，本文旨在总结其在蛋白质序列分析中的进展，为研究者提供全面参考。

Method: 综述并分析了大量关于Transformer模型在蛋白质序列分析中的应用研究，涵盖基因本体、功能识别、蛋白质生成等多个方面。

Result: 总结了现有研究的优势与不足，为读者提供了该领域的全面视角。

Conclusion: 本文为研究者提供了该领域的最新进展与未来方向，有助于指导未来的研究。

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [38] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 论文提出GRU-KAN和LSTM-KAN两种新架构，用于提前三个月以上预测贷款违约，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有贷款违约预测模型在早期预测中准确性不足及依赖特定时间框架的问题，帮助金融机构提前采取预防措施。

Method: 结合Kolmogorov-Arnold Networks (KAN)与GRU和LSTM网络，提出GRU-KAN和LSTM-KAN模型，并与基线模型（LSTM、GRU等）在多个指标上对比。

Result: 新模型在提前三个月和八个月的预测中分别达到92%和88%的准确率，显著优于基线模型。

Conclusion: GRU-KAN和LSTM-KAN能有效提升贷款违约的早期预测能力，具有实际应用价值。

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [39] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: PI-GNNs在组合优化问题中表现良好，但随着问题图密度增加性能下降。研究发现训练动态中存在相变，并提出基于模糊逻辑和二值化神经网络的改进方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究PI-GNNs在组合优化问题中的性能表现，尤其是随着问题图密度增加时的性能下降现象。

Method: 分析了PI-GNNs的训练动态，提出基于模糊逻辑和二值化神经网络的改进方法。

Result: 改进方法显著提升了PI-GNNs在高密度问题中的性能。

Conclusion: 通过模糊逻辑和二值化神经网络的方法，可以有效解决PI-GNNs在高密度组合优化问题中的性能下降问题。

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [40] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: 提出了一种自适应空间标记化（AST）方法，通过网格划分和注意力机制高效模拟可变形体交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 学习型方法（如GNN）在模拟可变形体交互时存在计算量大、扩展性差的问题，需要更高效的解决方案。

Method: 将模拟空间划分为网格单元，映射非结构化网格到结构化网格，并通过交叉注意力模块生成紧凑的固定长度嵌入。

Result: 实验表明，该方法在模拟大规模网格（超过10万个节点）时仍保持高效，显著优于现有方法。

Conclusion: AST方法结合了标记化的高效性和注意力机制的表达能力，为可变形体交互模拟提供了可扩展的解决方案。

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [41] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究系统比较了传统机器学习和深度学习模型在帕金森病（PD）分类中的表现，旨在为开发有效的学习系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）的早期诊断对临床干预至关重要，但开发可靠的自动化诊断模型仍具挑战性。脑电图（EEG）是一种非侵入性且经济的方法，可用于检测PD相关神经变化。

Method: 研究使用公开的oddball任务数据集，实施了统一的七步预处理流程，并应用一致的交叉验证和评估标准，比较了传统机器学习（如XGBoost）和深度学习（如CNN-LSTM）模型的表现。

Result: 结果表明，CNN-LSTM等深度学习模型性能最佳，强调了捕捉长时程时间依赖性的重要性；同时，XGBoost等传统分类器也表现出较强的预测准确性和校准决策边界。

Conclusion: 研究为未来开发更复杂或专用架构提供了可靠的基准框架，强调了科学严谨性和可重复性在EEG神经诊断领域的重要性。

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [42] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究提出了一种基于Bi-GRU神经网络的EEG信号分类方法，用于检测欺骗行为，测试准确率达97%。


<details>
  <summary>Details</summary>
Motivation: 欺骗检测在安全、心理学和法医学等领域具有重要意义，但现有方法面临挑战。

Method: 使用Bag-of-Lies数据集的EEG信号，训练Bi-GRU神经网络进行二元分类。

Result: 模型测试准确率为97%，且在各指标（精确度、召回率、F1分数）上表现优异。

Conclusion: 双向时序建模在EEG欺骗检测中有效，具有实时应用潜力，未来可探索更先进的神经架构。

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [43] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: 本文提出了一种混合特征融合框架，用于构建自主货船故障模式的图结构数据集，显著提升了文献检索效率和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 解决自主货船组件故障引发的级联反应和应急决策中的不确定性。

Method: 采用改进的布谷鸟搜索算法（HN-CSA）提升检索效率，构建分层特征融合框架，结合Word2Vec、BERT-KPCA和Sentence-BERT处理特征。

Result: 数据集覆盖12个系统、1,262种故障模式和6,150条传播路径，GATE-GNN模型分类准确率为0.735，预测F1分数达0.93。

Conclusion: 为自主货船的故障分析、风险评估和智能决策系统提供了可靠支持。

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [44] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: 研究探讨了对抗训练在音频分类中如何提升模型对数据分布偏移和对抗攻击的鲁棒性，发现输出空间攻击策略显著改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 对抗训练对音频分类中数据分布偏移的影响尚未充分研究，本文旨在填补这一空白。

Method: 使用两种模型架构（ConvNeXt和AudioProtoPNet）和两种对抗训练策略（输出空间攻击和嵌入空间攻击），在鸟类声音分类基准上评估。

Result: 输出空间攻击策略使模型在干净测试数据上的性能平均提升10.5%，同时增强了对抗鲁棒性。

Conclusion: 对抗训练在音频分类中具有潜力，能同时应对数据分布偏移和对抗攻击。

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [45] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 提出了一种基于SpiNNaker2芯片的多层DNN调度框架，支持从PyTorch模型到推理的端到端流程。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上高效执行复杂DNN（如Transformer）的挑战。

Method: 结合量化、降阶步骤的前端，扩展OctopuScheduler为多层DNN调度框架。

Result: 实现了在SpiNNaker2上高效运行大规模复杂DNN的能力。

Conclusion: 该框架为边缘计算中的复杂DNN推理提供了可行方案。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [46] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: SamGoG是一种基于采样的图-图学习框架，有效解决了图分类任务中的类别和大小不平衡问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据常存在类别和大小不平衡问题，导致模型学习偏差和性能下降，现有方法通常只能解决一种不平衡或计算成本高。

Method: SamGoG通过重要性采样机制构建多个图-图结构，并依次训练，结合可学习的相似性和自适应节点度以增强边同质性。

Result: 实验表明，SamGoG在基准数据集上实现了最先进的性能，准确率提升高达15.66%，训练速度加快6.7倍。

Conclusion: SamGoG能无缝集成多种下游GNN，为图分类任务提供高效解决方案。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [47] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: 论文提出了一种基于监督学习的变压器模型优化方法，用于生物医学本体对齐，通过动态量化和优化工具显著提升了推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型规模和计算需求的增加，在资源受限环境中部署模型面临挑战，如能耗、内存和延迟问题。

Method: 使用监督学习的变压器模型，结合余弦语义相似度进行本体对齐，并通过Microsoft Olive、ONNX Runtime、Intel Neural Compressor和IPEX进行动态量化优化。

Result: 在DEFT 2020评估任务中达到新SOTA，推理速度提升20倍，内存使用减少约70%，性能指标保持不变。

Conclusion: 该方法有效解决了资源受限环境中的模型优化问题，显著提升了效率和性能。

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [48] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: 论文提出了一种新的参数插值流模型（PIF），用于分子生成，解决了贝叶斯流网络（BFNs）在灵活性和适应性上的限制，并在药物设计中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯流网络（BFNs）在分子生成任务中表现出色，但其基于贝叶斯推断的策略限制了分布变换路径的灵活性，难以适应多样化的数据分布和任务需求。此外，参数空间模型的潜力尚未充分探索。

Method: 提出了一种名为PIF的参数插值流模型，并详细阐述了其理论基础、训练和推理过程。进一步开发了MolPIF用于基于结构的药物设计。

Result: MolPIF在多种指标上优于基线模型，验证了参数空间生成建模范式在分子生成中的有效性。

Conclusion: PIF模型为分子生成提供了新的设计视角，并展示了参数空间方法的潜力。

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [49] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种基于邻居分布特性的双中心图聚类方法（DCGC），通过邻居分布作为监督信号和双中心优化，提升了聚类效果。


<details>
  <summary>Details</summary>
Motivation: 传统目标导向聚类方法仅利用特征构建单目标分布，导致监督信号不可靠且优化不完整。

Method: 利用邻居分布作为监督信号挖掘对比学习中的困难负样本，并引入邻居分布中心与特征中心共同构建双目标分布进行优化。

Result: 实验证明DCGC方法在性能和效果上优于现有方法。

Conclusion: DCGC通过可靠的监督信号和双中心优化，显著提升了图聚类的效果。

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [50] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯神经网络的方法，用于微调预训练的基础模型，并通过实时学习工作流自动检测和采样稀有事件。


<details>
  <summary>Details</summary>
Motivation: 由于第一性原理计算原子间力的复杂性，机器学习力场的训练数据生成成本高，尤其是对于稀有事件或大构型空间系统。预训练基础模型的微调可以减少数据需求，但缺乏不确定性量化。

Method: 采用贝叶斯神经网络方法微调基础模型，结合实时学习工作流，利用模型不确定性自动更新训练数据并维持预设精度。

Result: 该方法能够自动检测稀有事件（如过渡态）并以更高频率采样，同时保持模型精度。

Conclusion: 提出的方法有效解决了基础模型微调中的不确定性量化问题，为稀有事件建模提供了实用解决方案。

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [51] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: 本文研究了一种强化学习问题变体，其中奖励函数是子模的，提出了一种基于剪枝子模图的近似解法，实验表明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中奖励函数通常为加性，但现实中许多问题（如路径规划）的奖励函数表现为子模性（收益递减），因此需要研究子模奖励函数的优化策略。

Method: 提出了一种基于剪枝子模图的方法，能够在可行计算时间内提供近似最优解，并分析了其时间和空间复杂度。

Result: 实验结果表明，所提方法获得的策略比基线方法产生更高的奖励。

Conclusion: 该方法为子模奖励函数的强化学习问题提供了有效的近似解决方案，具有实际应用潜力。

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [52] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: 研究探讨了自监督学习方法在基因表达数据表型预测中的应用，表明其能减少对标注数据的依赖并提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习需要大量标注数据，成本高且耗时；自监督学习可直接从未标注数据中提取信息，为解决这一问题提供了新途径。

Method: 选择了三种自监督学习方法，评估其在基因表达数据中提取信息的能力，并用于下游预测任务。

Result: 自监督学习方法在多个公开数据集上表现优于传统监督模型，显著降低了对标注数据的依赖。

Conclusion: 自监督学习在基因表达数据分析中具有潜力，未来研究可进一步优化其应用。

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [53] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种动态因果框架（Causal Process）及其实现模型（Causal Process Model），将Transformer的注意力机制与强化学习结合，用于从视觉观察中推断可解释的动态因果过程。


<details>
  <summary>Details</summary>
Motivation: 现有神经因果模型多假设静态因果图，忽视了因果交互的动态性，因此需要一种新理论来动态表示因果结构。

Method: 通过强化学习代理构建动态因果图假设，将因果推断任务嵌套于原始强化学习问题中，并采用类似Transformer注意力机制的链接方式。

Result: 在强化学习环境中，该方法在因果表示学习和代理性能上优于现有方法，并能恢复动态因果过程图。

Conclusion: Causal Process框架为动态因果建模提供了新思路，其实现模型在性能和可解释性上均表现出色。

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [54] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: 提出了一种名为MoDyGAN的新方法，结合分子动力学模拟和生成对抗网络，用于高效探索蛋白质构象空间。


<details>
  <summary>Details</summary>
Motivation: 由于基于物理的动态模拟计算成本高，探索蛋白质构象空间仍是一个挑战。

Method: MoDyGAN利用生成器将高斯分布映射到MD模拟轨迹，并通过集成学习和双判别器优化构象合理性。创新性地将3D蛋白质结构转换为2D矩阵，以利用图像GAN架构。

Result: 实验证明MoDyGAN能生成合理的新构象，且潜在空间插值与SMD模拟轨迹高度一致。

Conclusion: 将蛋白质表示为类图像数据为生物分子模拟提供了新思路，框架可扩展至其他复杂3D结构。

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [55] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出两种基于平均可控性的图学习方法，用于提升稀疏和不平衡数据中的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 复杂领域中的异常检测面临标记数据不足和样本不平衡的挑战，图机器学习模型虽有效但受限于异常数据稀缺。

Method: 提出两种方法：1) 将平均可控性作为边权重；2) 将其编码为独热边属性向量。

Result: 在真实和合成网络上的实验表明，该方法优于六种基线模型，验证了可控性度量的有效性。

Conclusion: 平均可控性作为额外指标，能显著提升图机器学习模型在稀疏和不平衡数据集中的异常检测性能。

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [56] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: 论文研究了机器学习在楔形文字分类中的应用，分析了数据差异对模型性能的影响，并提出了未来数据采集标准的建议。


<details>
  <summary>Details</summary>
Motivation: 楔形文字因来源、用途、书写者和数字化方式的不同而存在很大变异性，导致模型在不同数据集上表现不佳。本文旨在研究这种差异对性能的影响。

Method: 使用ResNet50模型对来自三个美索不达米亚城市的手写古巴比伦文本进行训练和测试。

Result: 模型在至少20个实例的符号上取得了87.1%的top-1准确率和96.5%的top-5准确率。

Conclusion: 研究为未来楔形文字分类任务提供了基础，并建议改进数据采集标准。

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [57] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: 论文提出了一种基于图卷积自编码器的结构连接组（SC）跨站点协调方法，解决了传统方法依赖元数据或忽略图拓扑的问题，并在模拟实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 神经影像学中样本量小及多站点数据采集的异质性限制了可靠生物标志物的开发，现有协调方法依赖元数据或忽视图结构。

Method: 提出了一种无需元数据的站点条件深度协调框架，测试了三种深度架构（全连接自编码器、卷积自编码器、图卷积自编码器）与线性回归基线的性能。

Result: 图卷积自编码器在保留拓扑结构和个体特征方面表现最佳，而线性回归基线虽数值性能高但实用性受限。

Conclusion: 图结构方法在多站点SC协调中具有优势，适合大规模应用。

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [58] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: 论文提出了一种动态加权机制ParallelTime Weighter，结合局部窗口注意力和Mamba，优化了时间序列预测中长短期依赖的权重分配，并设计了ParallelTime架构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间序列预测中对长短期依赖赋予等权重并不理想，需要更灵活的权重分配机制。

Method: 提出动态加权机制ParallelTime Weighter，根据输入和模型知识计算长短期依赖的权重，并设计ParallelTime架构。

Result: ParallelTime架构在多个基准测试中表现优异，计算效率高，参数少，且适用于更长的预测范围。

Conclusion: ParallelTime为时间序列预测中的并行Attention-Mamba架构提供了有前景的发展方向。

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [59] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: 论文探讨了动态规划方法在MDP中寻找静态CVaR最优策略时的失败原因，并提出风险分配一致性约束的概念。


<details>
  <summary>Details</summary>
Motivation: 研究动态规划方法在CVaR优化中的失败根源，从策略评估角度分析问题。

Method: 将静态CVaR评估问题转化为两个最小化问题，提出风险分配一致性约束。

Result: 发现约束交集为空导致评估错误，并证明双CVaR分解方法存在根本限制。

Conclusion: 双CVaR分解方法无法在所有初始风险水平下找到统一最优策略。

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [60] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: DONUT是一种基于物理感知的神经网络，用于快速自动分析纳米束衍射数据，无需标记数据集或预训练，效率比传统方法高200倍。


<details>
  <summary>Details</summary>
Motivation: 实时分析X射线纳米衍射数据的瓶颈在于计算复杂性和伪影问题，传统方法效率低下。

Method: DONUT通过将可微分的几何衍射模型嵌入神经网络架构，实时预测晶格应变和取向。

Result: 实验证明，DONUT能高效提取数据特征，效率是传统拟合方法的200倍以上。

Conclusion: DONUT为X射线科学中的实时数据分析提供了高效解决方案，克服了监督学习的局限性。

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [61] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: 论文研究了持续学习中的稳定性间隙问题，提出了一种基于不确定性调节增益的动态机制来平衡知识整合与干扰最小化。


<details>
  <summary>Details</summary>
Motivation: 持续学习中存在稳定性间隙，即在掌握新任务时已掌握任务的性能会短暂下降，这与持续学习的目标相矛盾。研究旨在解决这一问题。

Method: 受生物大脑多时间尺度动态的启发，提出了一种不确定性调节增益的动态机制，模拟两时间尺度优化器。

Result: 在MNIST和CIFAR基准测试中，该机制有效减少了稳定性间隙。

Conclusion: 增益调节机制模拟了去甲肾上腺素在皮质回路中的功能，为减少稳定性间隙和提升持续学习性能提供了机制性见解。

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [62] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的多目标强化学习方法（Pb-MORL），通过偏好指导策略优化，避免了复杂奖励函数设计，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）依赖预定义奖励函数，难以平衡冲突目标且易简化问题。偏好更灵活直观，可替代复杂奖励设计。

Method: 提出Pb-MORL框架，将偏好融入MORL，构建与偏好对齐的多目标奖励模型，并理论证明其优化等价于帕累托最优策略训练。

Result: 在基准任务、多能源管理任务和自动驾驶任务中，Pb-MORL表现优异，超越使用真实奖励函数的基准方法。

Conclusion: Pb-MORL通过偏好指导策略优化，在复杂任务中展现出实际应用潜力。

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [63] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出了一种双过程多尺度心智理论框架（DPMT），用于提升AI与人类在动态场景中的协作能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）难以准确建模复杂的人类心智特征，尤其是在缺乏直接沟通时。

Method: 结合认知科学的双过程理论，设计多尺度心智理论模块，进行心智特征推理。

Result: 实验表明DPMT显著提升了人机协作效果，消融研究验证了多尺度心智理论在慢系统中的贡献。

Conclusion: DPMT框架为解决动态场景中AI适应人类行为的挑战提供了有效方案。

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


### [64] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) 在类别不平衡分类任务中表现优于传统多层感知机 (MLPs)，但传统不平衡策略与 KANs 的数学结构冲突，导致性能下降且计算成本高。


<details>
  <summary>Details</summary>
Motivation: 研究 KANs 在类别不平衡分类任务中的表现，探索其与传统不平衡策略的兼容性及计算效率。

Method: 在十个基准数据集上对 KANs 和 MLPs 进行实证评估，比较其在原始不平衡数据和使用不平衡策略后的表现。

Result: KANs 在原始不平衡数据上表现优于 MLPs，但不平衡策略显著降低其性能且计算成本高；MLPs 结合不平衡策略与 KANs 性能相当但成本更低。

Conclusion: KANs 适用于资源充足的原始不平衡数据场景，但需进一步优化其计算效率和与不平衡策略的兼容性。

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [65] [Differential Privacy in Kernelized Contextual Bandits via Random Projections](https://arxiv.org/abs/2507.13639)
*Nikola Pavlovic,Sudeep Salgia,Qing Zhao*

Main category: stat.ML

TL;DR: 论文提出了一种在差分隐私约束下的上下文核赌博机问题的新算法，实现了最优的累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究在差分隐私保护下，上下文核赌博机问题的解决方案，确保查询序列对上下文和奖励的隐私性。

Method: 提出了一种基于私有核岭回归估计器的新算法，结合私有协方差估计和私有随机投影，降低敏感性并保持高预测精度。

Result: 算法在联合和局部差分隐私模型下分别实现了最优的累积遗憾，分别为$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T}{\varepsilon_{\mathrm{DP}}})$和$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T\sqrt{T}}{\varepsilon_{\mathrm{DP}}})$。

Conclusion: 新算法通过私有核岭回归估计器实现了高性能和隐私保护的平衡，为上下文核赌博机问题提供了最优解。

Abstract: We consider the problem of contextual kernel bandits with stochastic
contexts, where the underlying reward function belongs to a known Reproducing
Kernel Hilbert Space. We study this problem under an additional constraint of
Differential Privacy, where the agent needs to ensure that the sequence of
query points is differentially private with respect to both the sequence of
contexts and rewards. We propose a novel algorithm that achieves the
state-of-the-art cumulative regret of
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T}{\varepsilon_{\mathrm{DP}}})$
and
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T\sqrt{T}}{\varepsilon_{\mathrm{DP}}})$
over a time horizon of $T$ in the joint and local models of differential
privacy, respectively, where $\gamma_T$ is the effective dimension of the
kernel and $\varepsilon_{\mathrm{DP}} > 0$ is the privacy parameter. The key
ingredient of the proposed algorithm is a novel private kernel-ridge regression
estimator which is based on a combination of private covariance estimation and
private random projections. It offers a significantly reduced sensitivity
compared to its classical counterpart while maintaining a high prediction
accuracy, allowing our algorithm to achieve the state-of-the-art performance
guarantees.

</details>


### [66] [Conformal Data Contamination Tests for Trading or Sharing of Data](https://arxiv.org/abs/2507.13835)
*Martin V. Vejling,Shashi Raj Pandey,Christophe A. N. Biscio,Petar Popovski*

Main category: stat.ML

TL;DR: 提出了一种无分布、污染感知的数据共享框架，通过新型两样本测试方法识别对模型个性化最有价值的外部数据。


<details>
  <summary>Details</summary>
Motivation: 解决数据购买者对外部数据质量的担忧，避免依赖分布假设和昂贵的后验数据评估。

Method: 引入基于严格理论基础的共形离群值检测的两样本测试方法，判断数据是否超过污染阈值。

Result: 提出的共形数据污染测试在任意污染水平下有效，并通过Benjamini-Hochberg程序控制错误发现率。

Conclusion: 共形数据污染测试是一种通用的数据聚合方法，具有统计严谨的质量保证。

Abstract: The amount of quality data in many machine learning tasks is limited to what
is available locally to data owners. The set of quality data can be expanded
through trading or sharing with external data agents. However, data buyers need
quality guarantees before purchasing, as external data may be contaminated or
irrelevant to their specific learning task. Previous works primarily rely on
distributional assumptions about data from different agents, relegating quality
checks to post-hoc steps involving costly data valuation procedures. We propose
a distribution-free, contamination-aware data-sharing framework that identifies
external data agents whose data is most valuable for model personalization. To
achieve this, we introduce novel two-sample testing procedures, grounded in
rigorous theoretical foundations for conformal outlier detection, to determine
whether an agent's data exceeds a contamination threshold. The proposed tests,
termed conformal data contamination tests, remain valid under arbitrary
contamination levels while enabling false discovery rate control via the
Benjamini-Hochberg procedure. Empirical evaluations across diverse
collaborative learning scenarios demonstrate the robustness and effectiveness
of our approach. Overall, the conformal data contamination test distinguishes
itself as a generic procedure for aggregating data with statistically rigorous
quality guarantees.

</details>


### [67] [A Survey of Dimension Estimation Methods](https://arxiv.org/abs/2507.13887)
*James A. D. Binnie,Paweł Dłotko,John Harvey,Jakub Malinowski,Ka Man Yim*

Main category: stat.ML

TL;DR: 本文综述了多种维度估计方法，分类讨论了其几何基础，并评估了它们在噪声、曲率等条件下的性能，指出许多方法存在过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据集通常具有低维内在结构，但缺乏可靠的维度估计方法指导，因此需要系统评估现有方法的性能。

Method: 将维度估计方法分为三类：基于局部仿射结构的切向估计器、依赖维度相关概率分布的参数估计器，以及利用拓扑或度量不变性的估计器。

Result: 研究发现许多方法对超参数选择敏感，样本量要求高，且在非线性几何中表现不佳，过拟合现象普遍。

Conclusion: 现有维度估计方法在泛化性上存在局限，需进一步改进以适应更广泛的数据集。

Abstract: It is a standard assumption that datasets in high dimension have an internal
structure which means that they in fact lie on, or near, subsets of a lower
dimension. In many instances it is important to understand the real dimension
of the data, hence the complexity of the dataset at hand. A great variety of
dimension estimators have been developed to find the intrinsic dimension of the
data but there is little guidance on how to reliably use these estimators.
  This survey reviews a wide range of dimension estimation methods,
categorising them by the geometric information they exploit: tangential
estimators which detect a local affine structure; parametric estimators which
rely on dimension-dependent probability distributions; and estimators which use
topological or metric invariants.
  The paper evaluates the performance of these methods, as well as
investigating varying responses to curvature and noise. Key issues addressed
include robustness to hyperparameter selection, sample size requirements,
accuracy in high dimensions, precision, and performance on non-linear
geometries. In identifying the best hyperparameters for benchmark datasets,
overfitting is frequent, indicating that many estimators may not generalise
well beyond the datasets on which they have been tested.

</details>


### [68] [Conformalized Regression for Continuous Bounded Outcomes](https://arxiv.org/abs/2507.14023)
*Zhanli Wu,Fabrizio Leisen,F. Javier Rubio*

Main category: stat.ML

TL;DR: 论文提出了基于转换模型和Beta回归的保形预测区间方法，用于处理有界连续结果的回归问题，解决了现有方法在点预测或渐近区间预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 解决有界连续结果回归问题中的预测挑战，特别是在模型错误设定和异方差性存在的情况下，提供更可靠的预测区间。

Method: 引入基于残差的非一致性度量，结合转换模型和Beta回归，开发了保形预测区间方法，包括全保形预测和分割保形预测。

Result: 理论证明了方法的渐近边际和条件有效性，模拟研究表明其在有限样本下仍能提供有效的预测覆盖。

Conclusion: 提出的方法在实际数据中表现良好，优于基于Bootstrap的替代方法，适用于模型错误设定的场景。

Abstract: Regression problems with bounded continuous outcomes frequently arise in
real-world statistical and machine learning applications, such as the analysis
of rates and proportions. A central challenge in this setting is predicting a
response associated with a new covariate value. Most of the existing
statistical and machine learning literature has focused either on point
prediction of bounded outcomes or on interval prediction based on asymptotic
approximations. We develop conformal prediction intervals for bounded outcomes
based on transformation models and beta regression. We introduce tailored
non-conformity measures based on residuals that are aligned with the underlying
models, and account for the inherent heteroscedasticity in regression settings
with bounded outcomes. We present a theoretical result on asymptotic marginal
and conditional validity in the context of full conformal prediction, which
remains valid under model misspecification. For split conformal prediction, we
provide an empirical coverage analysis based on a comprehensive simulation
study. The simulation study demonstrates that both methods provide valid
finite-sample predictive coverage, including settings with model
misspecification. Finally, we demonstrate the practical performance of the
proposed conformal prediction intervals on real data and compare them with
bootstrap-based alternatives.

</details>


### [69] [Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design](https://arxiv.org/abs/2507.14057)
*Marcel Hedman,Desi R. Ivanova,Cong Guan,Tom Rainforth*

Main category: stat.ML

TL;DR: Step-DAD是一种半摊销、基于策略的贝叶斯实验设计方法，通过动态更新策略提升灵活性和鲁棒性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有完全摊销的贝叶斯实验设计方法在实验过程中策略固定，缺乏灵活性和适应性。

Method: Step-DAD在实验前训练设计策略，并在实验过程中定期更新策略以适应具体实验实例。

Result: Step-DAD在决策能力和鲁棒性上优于当前最先进的贝叶斯实验设计方法。

Conclusion: Step-DAD通过动态策略更新，显著提升了贝叶斯实验设计的性能和适应性。

Abstract: We develop a semi-amortized, policy-based, approach to Bayesian experimental
design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,
fully amortized, policy-based BED approaches, Step-DAD trains a design policy
upfront before the experiment. However, rather than keeping this policy fixed,
Step-DAD periodically updates it as data is gathered, refining it to the
particular experimental instance. This test-time adaptation improves both the
flexibility and the robustness of the design strategy compared with existing
approaches. Empirically, Step-DAD consistently demonstrates superior
decision-making and robustness compared with current state-of-the-art BED
methods.

</details>
