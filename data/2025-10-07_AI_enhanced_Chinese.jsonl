{"id": "2510.03516", "categories": ["eess.SP", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03516", "abs": "https://arxiv.org/abs/2510.03516", "authors": ["Boyang Chen", "Mohd Tasleem Khan", "George Goussetis", "Mathini Sellathurai", "Yuan Ding", "Jo\u00e3o F. C. Mota"], "title": "COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques", "comment": null, "summary": "Convolutional Neural Networks (CNNs) are highly effective for computer vision\nand pattern recognition tasks; however, their computational intensity and\nreliance on hardware such as FPGAs pose challenges for deployment on low-power\nedge devices. In this work, we present COMET, a framework of CNN designs that\nemploy efficient hardware offset-binary coding (OBC) techniques to enable\nco-optimization of performance and resource utilization. The approach\nformulates CNN inference with OBC representations of inputs (Scheme A) and\nweights (Scheme B) separately, enabling exploitation of bit-width asymmetry.\nThe shift-accumulate operation is modified by incorporating the offset term\nwith the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we\nintroduce four novel look-up table (LUT) techniques -- parallel, shared, split,\nand hybrid -- and analyze them to identify the most efficient options. Building\non this foundation, we develop an OBC-based general matrix multiplication core\nusing the im2col transformation, enabling efficient acceleration of a\nfixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the\nproposed co-optimization approach significantly reduces resource utilization\ncompared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on\naccuracy.", "AI": {"tldr": "COMET\u6846\u67b6\u901a\u8fc7\u504f\u79fb\u4e8c\u8fdb\u5236\u7f16\u7801\u6280\u672f\u4f18\u5316CNN\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u7684\u534f\u540c\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4eFPGA\u8d44\u6e90\u6d88\u8017\u4e14\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "CNN\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8ba1\u7b97\u5bc6\u96c6\u6027\u548c\u5bf9FPGA\u7b49\u786c\u4ef6\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u504f\u79fb\u4e8c\u8fdb\u5236\u7f16\u7801\u5206\u522b\u8868\u793a\u8f93\u5165\u548c\u6743\u91cd\uff0c\u5229\u7528\u4f4d\u5bbd\u4e0d\u5bf9\u79f0\u6027\uff1b\u4fee\u6539\u79fb\u4f4d\u7d2f\u52a0\u64cd\u4f5c\uff0c\u5f15\u5165\u504f\u79fb\u9879\u4e0e\u9884\u7f29\u653e\u504f\u7f6e\uff1b\u63d0\u51fa\u56db\u79cd\u65b0\u9896\u7684LUT\u6280\u672f\uff08\u5e76\u884c\u3001\u5171\u4eab\u3001\u62c6\u5206\u3001\u6df7\u5408\uff09\uff1b\u57fa\u4e8eim2col\u53d8\u6362\u5f00\u53d1OBC\u901a\u7528\u77e9\u9635\u4e58\u6cd5\u6838\u5fc3\u3002", "result": "FPGA\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709LeNet-5 CNN\u8bbe\u8ba1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8d44\u6e90\u5229\u7528\uff0c\u5bf9\u7cbe\u5ea6\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "COMET\u6846\u67b6\u901a\u8fc7\u786c\u4ef6\u53cb\u597d\u7684\u504f\u79fb\u4e8c\u8fdb\u5236\u7f16\u7801\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86CNN\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\uff0c\u4e3a\u4f4e\u529f\u8017\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03594", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03594", "abs": "https://arxiv.org/abs/2510.03594", "authors": ["Tuo Wu", "Kwai-Man Luk", "Jie Tang", "Kai-Kit Wong", "Jianchao Zheng", "Baiyang Liu", "David Morales-Jimenez", "Maged Elkashlan", "Kin-Fai Tong", "Chan-Byoung Chae", "Fumiyuki Adachi", "George K. Karagiannidis"], "title": "Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems", "comment": "13 pages", "summary": "Fluid antenna systems (FAS) are emerging as a transformative enabler for\nsixth-generation (6G) wireless communications, providing unprecedented spatial\ndiversity through dynamic reconfiguration of antenna ports. However, the\ninherent spatial correlation among ports poses significant challenges for\naccurate analysis. Conventional models such as Jakes are analytically\nintractable, while oversimplified constant-correlation models fail to capture\nthe true behavior. In this work, we address these challenges by applying the\nvariable block-correlation model (VBCM) -- originally proposed by\nRam\\'{i}rez-Espinosa \\textit{et al.} in 2024 -- to FAS security analysis, and\nby developing comprehensive optimization methods to enhance analytical\naccuracy. We derive new closed-form expressions for average secrecy capacity\n(ASC) and secrecy outage probability (SOP), demonstrating that the VBCM\nframework achieves simulation-aligned accuracy, with relative errors\nconsistently below $5\\%$ (compared to $10$--$15\\%$ for constant-correlation\nmodels). To maximize ASC, we further design two algorithms: a grid search (GS)\nmethod and a gradient descent (GD) method. Numerical results reveal that the\nVBCM-based approach not only provides reliable insights into FAS security\nperformance, but also yields substantial gains -- ASC improvements exceeding\n$120\\%$ in high-threat scenarios and $18$--$19\\%$ performance enhancements for\ncompact antenna configurations. These findings underscore the practical value\nof integrating VBCM into FAS security analysis and optimization, establishing\nit as a powerful tool for advancing 6G communication systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u53ef\u53d8\u5757\u76f8\u5173\u6a21\u578b(VBCM)\u5e94\u7528\u4e8e\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf(FAS)\u5b89\u5168\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u80fd\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u76f8\u5173\u6a21\u578b(\u5982Jakes\u6a21\u578b)\u5728FAS\u5b89\u5168\u5206\u6790\u4e2d\u5b58\u5728\u5206\u6790\u56f0\u96be\uff0c\u800c\u7b80\u5316\u7684\u6052\u5b9a\u76f8\u5173\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u771f\u5b9e\u884c\u4e3a\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528VBCM\u6a21\u578b\u8fdb\u884cFAS\u5b89\u5168\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u5e73\u5747\u4fdd\u5bc6\u5bb9\u91cf(ASC)\u548c\u4fdd\u5bc6\u4e2d\u65ad\u6982\u7387(SOP)\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7f51\u683c\u641c\u7d22\u548c\u68af\u5ea6\u4e0b\u964d\u4e24\u79cd\u4f18\u5316\u7b97\u6cd5\u3002", "result": "VBCM\u6846\u67b6\u5b9e\u73b0\u4e86\u4e0e\u4eff\u771f\u4e00\u81f4\u7684\u51c6\u786e\u6027\uff0c\u76f8\u5bf9\u8bef\u5dee\u59cb\u7ec8\u4f4e\u4e8e5%\uff0c\u5728\u9ad8\u5a01\u80c1\u573a\u666f\u4e0bASC\u63d0\u5347\u8d85\u8fc7120%\uff0c\u7d27\u51d1\u5929\u7ebf\u914d\u7f6e\u6027\u80fd\u63d0\u534718-19%\u3002", "conclusion": "VBCM\u4e3aFAS\u5b89\u5168\u5206\u6790\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u5bf9\u63a8\u8fdb6G\u901a\u4fe1\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2510.03626", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03626", "abs": "https://arxiv.org/abs/2510.03626", "authors": ["Jun Tong"], "title": "On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models", "comment": "This paper was presented at 2025 IEEE International Conference on\n  Communications Workshops (ICC Workshops)", "summary": "Significant studies on communications over doubly selective channels have\nutilized on-grid DD channel models, which are previously investigated in\nBello's seminar paper in 1963. The DD grid is typically specified by the\nbandwidth and time duration of the transmission frames. However, the physical\nchannels are determined by the propagation environments and they are typically\noff-grid. Hence, there is often a gap between an actual physical channel and\nthe on-grid model. This paper revisits the on-grid modeling of practical\nphysical channels. We study the associated on-grid DD-domain representations\nfor continuous-time, doubly selective channels with off-grid delay and Doppler\nshifts, accounting for practical time/frequency-domain windowing at the\ntransceivers. The universal models obtained are applicable under the mild\nassumption that the windows have finite supports, and they extend Bello's\nclassical results to account for more general windows. We also discuss the\nfeatures and implications of the equivalent on-grid models.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5b9e\u9645\u7269\u7406\u4fe1\u9053\u7684on-grid\u5efa\u6a21\uff0c\u7814\u7a76\u4e86\u5177\u6709off-grid\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u79fb\u7684\u8fde\u7eed\u65f6\u95f4\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u7684\u7b49\u6548on-grid DD\u57df\u8868\u793a\uff0c\u8003\u8651\u4e86\u6536\u53d1\u5668\u7aef\u7684\u5b9e\u9645\u65f6\u9891\u57df\u52a0\u7a97\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u901a\u4fe1\u7814\u7a76\u5927\u591a\u91c7\u7528on-grid DD\u4fe1\u9053\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u7269\u7406\u4fe1\u9053\u901a\u5e38\u7531\u4f20\u64ad\u73af\u5883\u51b3\u5b9a\u4e14\u662foff-grid\u7684\uff0c\u5bfc\u81f4\u5b9e\u9645\u7269\u7406\u4fe1\u9053\u4e0eon-grid\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u7814\u7a76\u5177\u6709off-grid\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u79fb\u7684\u8fde\u7eed\u65f6\u95f4\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u7684\u7b49\u6548on-grid DD\u57df\u8868\u793a\uff0c\u8003\u8651\u6536\u53d1\u5668\u7aef\u7684\u5b9e\u9645\u65f6\u9891\u57df\u52a0\u7a97\u5904\u7406\uff0c\u83b7\u5f97\u5728\u7a97\u53e3\u5177\u6709\u6709\u9650\u652f\u6491\u7684\u6e29\u548c\u5047\u8bbe\u4e0b\u9002\u7528\u7684\u901a\u7528\u6a21\u578b\u3002", "result": "\u83b7\u5f97\u4e86\u6269\u5c55Bello\u7ecf\u5178\u7ed3\u679c\u7684\u901a\u7528\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u66f4\u4e00\u822c\u7684\u7a97\u53e3\u51fd\u6570\uff0c\u5e76\u8ba8\u8bba\u4e86\u7b49\u6548on-grid\u6a21\u578b\u7684\u7279\u5f81\u548c\u610f\u4e49\u3002", "conclusion": "\u672c\u6587\u6269\u5c55\u4e86Bello\u7684\u7ecf\u5178\u7ed3\u679c\uff0c\u4e3a\u5904\u7406\u5177\u6709off-grid\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u79fb\u7684\u5b9e\u9645\u7269\u7406\u4fe1\u9053\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684on-grid\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2510.03277", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62G30, 62M20, 68T05", "I.2.6"], "pdf": "https://arxiv.org/pdf/2510.03277", "abs": "https://arxiv.org/abs/2510.03277", "authors": ["Tunde Fahd Egunjobi"], "title": "Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback", "comment": "28 pages, 7 figures", "summary": "Bayesian Optimization (BO) is widely used for optimizing expensive black-box\nfunctions, particularly in hyperparameter tuning. However, standard BO assumes\naccess to precise objective values, which may be unavailable, noisy, or\nunreliable in real-world settings where only relative or rank-based feedback\ncan be obtained. In this study, we propose Quantile-Scaled Bayesian\nOptimization (QS-BO), a principled rank-based optimization framework. QS-BO\nconverts ranks into heteroscedastic Gaussian targets through a quantile-scaling\npipeline, enabling the use of Gaussian process surrogates and standard\nacquisition functions without requiring explicit metric scores. We evaluate\nQS-BO on synthetic benchmark functions, including one- and two-dimensional\nnonlinear functions and the Branin function, and compare its performance\nagainst Random Search. Results demonstrate that QS-BO consistently achieves\nlower objective values and exhibits greater stability across runs. Statistical\ntests further confirm that QS-BO significantly outperforms Random Search at the\n1\\% significance level. These findings establish QS-BO as a practical and\neffective extension of Bayesian Optimization for rank-only feedback, with\npromising applications in preference learning, recommendation, and\nhuman-in-the-loop optimization where absolute metric values are unavailable or\nunreliable.", "AI": {"tldr": "\u63d0\u51faQS-BO\u65b9\u6cd5\uff0c\u5c06\u6392\u540d\u53cd\u9988\u8f6c\u6362\u4e3a\u5f02\u65b9\u5dee\u9ad8\u65af\u76ee\u6807\uff0c\u4f7f\u8d1d\u53f6\u65af\u4f18\u5316\u80fd\u5728\u4ec5\u6709\u76f8\u5bf9\u6392\u540d\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5de5\u4f5c\uff0c\u663e\u8457\u4f18\u4e8e\u968f\u673a\u641c\u7d22\u3002", "motivation": "\u6807\u51c6\u8d1d\u53f6\u65af\u4f18\u5316\u9700\u8981\u7cbe\u786e\u7684\u76ee\u6807\u51fd\u6570\u503c\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u53ea\u80fd\u83b7\u5f97\u76f8\u5bf9\u6392\u540d\u53cd\u9988\uff0c\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8e\u6392\u540d\u53cd\u9988\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u4f4d\u6570\u7f29\u653e\u6d41\u7a0b\u5c06\u6392\u540d\u8f6c\u6362\u4e3a\u5f02\u65b9\u5dee\u9ad8\u65af\u76ee\u6807\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u548c\u6807\u51c6\u91c7\u96c6\u51fd\u6570\uff0c\u65e0\u9700\u663e\u5f0f\u5ea6\u91cf\u5206\u6570\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u51fd\u6570\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cQS-BO\u59cb\u7ec8\u83b7\u5f97\u66f4\u4f4e\u7684\u76ee\u6807\u503c\uff0c\u8fd0\u884c\u66f4\u7a33\u5b9a\uff0c\u7edf\u8ba1\u68c0\u9a8c\u663e\u793a\u57281%\u663e\u8457\u6027\u6c34\u5e73\u4e0a\u663e\u8457\u4f18\u4e8e\u968f\u673a\u641c\u7d22\u3002", "conclusion": "QS-BO\u662f\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u4ec5\u6709\u6392\u540d\u53cd\u9988\u60c5\u51b5\u4e0b\u7684\u5b9e\u7528\u6709\u6548\u6269\u5c55\uff0c\u5728\u504f\u597d\u5b66\u4e60\u3001\u63a8\u8350\u7cfb\u7edf\u548c\u4eba\u673a\u534f\u540c\u4f18\u5316\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.03628", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03628", "abs": "https://arxiv.org/abs/2510.03628", "authors": ["Haochen Li"], "title": "Pinching Antenna Systems (PASS) for Cell-Free Communications", "comment": "5 pages, 5 figures", "summary": "A pinching antenna system (PASS) assisted cell-free communication system is\nproposed. A sum rate maximization problem under the BS power budget constraint\nand PA deployment constraint is formulated. To tackle the proposed non-convex\noptimization problem, an alternating optimization (AO) algorithm is developed.\nIn particular, the digital beamforming sub-problem is solved using the weighted\nminimum mean square error (WMMSE) method, whereas the pinching beamforming\nsub-problem is handled via a penalty based approach combined with element-wise\noptimization. Simulation results demonstrate that: 1) the PASS assisted\ncell-free systems achieve superior performance over benchmark schemes; 2)\nincreasing the number of PAs per waveguides can improve the advantage of PASS\nassisted cell-free systems; and 3) the cell-free architecture mitigates the\naverage user rate degradation as the number of users increases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\uff0c\u5728\u57fa\u7ad9\u529f\u7387\u9884\u7b97\u548cPA\u90e8\u7f72\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6027\u80fd\u63d0\u5347", "motivation": "\u4f20\u7edf\u65e0\u8702\u7a9d\u7cfb\u7edf\u9762\u4e34\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u5929\u7ebf\u67b6\u6784\u6765\u63d0\u5347\u7cfb\u7edf\u5bb9\u91cf\u548c\u8986\u76d6\u6027\u80fd", "method": "\u4f7f\u7528\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u5176\u4e2d\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u91c7\u7528WMMSE\u65b9\u6cd5\uff0c\u5939\u6301\u6ce2\u675f\u6210\u5f62\u91c7\u7528\u57fa\u4e8e\u60e9\u7f5a\u7684\u65b9\u6cd5\u7ed3\u5408\u9010\u5143\u7d20\u4f18\u5316", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff1a1\uff09PASS\u8f85\u52a9\u7cfb\u7edf\u6027\u80fd\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff1b2\uff09\u589e\u52a0\u6bcf\u4e2a\u6ce2\u5bfc\u7684PA\u6570\u91cf\u53ef\u63d0\u5347\u7cfb\u7edf\u4f18\u52bf\uff1b3\uff09\u65e0\u8702\u7a9d\u67b6\u6784\u80fd\u7f13\u89e3\u7528\u6237\u6570\u589e\u52a0\u5e26\u6765\u7684\u5e73\u5747\u7528\u6237\u901f\u7387\u4e0b\u964d", "conclusion": "PASS\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u901a\u4fe1\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u6570\u91cf\u589e\u52a0\u65f6\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u7684\u5e73\u5747\u7528\u6237\u901f\u7387"}}
{"id": "2510.03281", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03281", "abs": "https://arxiv.org/abs/2510.03281", "authors": ["David van Batenburg"], "title": "Mathematically rigorous proofs for Shapley explanations", "comment": null, "summary": "Machine Learning is becoming increasingly more important in today's world. It\nis therefore very important to provide understanding of the decision-making\nprocess of machine-learning models. A popular way to do this is by looking at\nthe Shapley-Values of these models as introduced by Lundberg and Lee.\n  In this thesis, we discuss the two main results by Lundberg and Lee from a\nmathematically rigorous standpoint and provide full proofs, which are not\navailable from the original material.\n  The first result of this thesis is an axiomatic characterization of the\nShapley values in machine learning based on axioms by Young. We show that the\nShapley values are the unique explanation to satisfy local accuracy,\nmissingness, symmetry and consistency. Lundberg and Lee claim that the symmetry\naxiom is redundant for explanations. However, we provide a counterexample that\nshows the symmetry axiom is in fact essential.\n  The second result shows that we can write the Shapley values as the unique\nsolution to a weighted linear regression problem. This result is proven with\nthe use of dimensionality reduction.", "AI": {"tldr": "\u672c\u6587\u4ece\u6570\u5b66\u4e25\u8c28\u89d2\u5ea6\u91cd\u65b0\u8bc1\u660e\u4e86Lundberg\u548cLee\u5173\u4e8eShapley\u503c\u7684\u4e24\u4e2a\u4e3b\u8981\u7ed3\u679c\uff0c\u5305\u62ecShapley\u503c\u7684\u516c\u7406\u5316\u7279\u5f81\u53ca\u5176\u4f5c\u4e3a\u52a0\u6743\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u7684\u552f\u4e00\u89e3\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u65e5\u76ca\u91cd\u8981\uff0cShapley\u503c\u662f\u7406\u89e3\u6a21\u578b\u51b3\u7b56\u7684\u6d41\u884c\u65b9\u6cd5\u3002\u539f\u8bba\u6587\u7f3a\u4e4f\u5b8c\u6574\u7684\u6570\u5b66\u8bc1\u660e\uff0c\u9700\u8981\u4ece\u6570\u5b66\u4e25\u8c28\u89d2\u5ea6\u91cd\u65b0\u8bc1\u660e\u8fd9\u4e9b\u7ed3\u679c\u3002", "method": "\u4f7f\u7528Young\u7684\u516c\u7406\u7cfb\u7edf\u5bf9Shapley\u503c\u8fdb\u884c\u516c\u7406\u5316\u7279\u5f81\u5316\uff0c\u5e76\u901a\u8fc7\u7ef4\u5ea6\u7ea6\u7b80\u8bc1\u660eShapley\u503c\u53ef\u4f5c\u4e3a\u52a0\u6743\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u7684\u552f\u4e00\u89e3\u3002", "result": "\u8bc1\u660e\u4e86Shapley\u503c\u662f\u6ee1\u8db3\u5c40\u90e8\u51c6\u786e\u6027\u3001\u7f3a\u5931\u6027\u3001\u5bf9\u79f0\u6027\u548c\u4e00\u81f4\u6027\u7684\u552f\u4e00\u89e3\u91ca\uff1b\u8bc1\u660e\u4e86\u5bf9\u79f0\u6027\u516c\u7406\u662f\u5fc5\u9700\u7684\uff1b\u8bc1\u660e\u4e86Shapley\u503c\u53ef\u8868\u793a\u4e3a\u52a0\u6743\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u7684\u552f\u4e00\u89e3\u3002", "conclusion": "\u4ece\u6570\u5b66\u4e25\u8c28\u89d2\u5ea6\u5b8c\u6574\u8bc1\u660e\u4e86Shapley\u503c\u7684\u4e24\u4e2a\u5173\u952e\u6027\u8d28\uff0c\u7ea0\u6b63\u4e86\u539f\u8bba\u6587\u4e2d\u5173\u4e8e\u5bf9\u79f0\u6027\u516c\u7406\u53ef\u7701\u7565\u7684\u9519\u8bef\u89c2\u70b9\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.03243", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.03243", "abs": "https://arxiv.org/abs/2510.03243", "authors": ["Yiheng Tao", "Yihe Zhang", "Matthew T. Dearing", "Xin Wang", "Yuping Fan", "Zhiling Lan"], "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank", "comment": null, "summary": "Efficient scheduling of LLM inference tasks is essential for achieving low\nlatency and high throughput, particularly with the growing use of\nreasoning-capable LLMs. Traditional strategies like First-Come-First-Serve\n(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks\ndelay shorter ones queued behind them. In this paper, we introduce PARS, a\nprompt-aware LLM task scheduler that improves serving efficiency by\napproximating shortest-job-first (SJF) scheduling through pairwise ranking with\nmargin ranking loss. PARS focuses on impactful scheduling decisions and is\nseamlessly integrated into the state-of-the-art LLM serving system vLLM. It\neffectively predicts response-length-based task ordering, reducing latency with\nminimal overhead. Extensive experiments across multiple LLMs and real-world\ninference datasets show that PARS significantly improves performance, including\nfor reasoning workloads. Furthermore, our cross-model evaluations demonstrate\nthat the design generalizes well, enabling effective scheduling even when\npredictors are trained on different LLMs.", "AI": {"tldr": "PARS\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u611f\u77e5\u7684LLM\u4efb\u52a1\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u8fd1\u4f3c\u6700\u77ed\u4f5c\u4e1a\u4f18\u5148\u8c03\u5ea6\u6765\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u51cf\u5c11\u5934\u90e8\u963b\u585e\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfFCFS\u8c03\u5ea6\u7b56\u7565\u5b58\u5728\u5934\u90e8\u963b\u585e\u95ee\u9898\uff0c\u957f\u4efb\u52a1\u4f1a\u5ef6\u8fdf\u77ed\u4efb\u52a1\uff0c\u5f71\u54cdLLM\u63a8\u7406\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u3002", "method": "\u4f7f\u7528\u6210\u5bf9\u6392\u5e8f\u548c\u8fb9\u754c\u6392\u5e8f\u635f\u5931\u6765\u9884\u6d4b\u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u7684\u4efb\u52a1\u6392\u5e8f\uff0c\u8fd1\u4f3cSJF\u8c03\u5ea6\uff0c\u5e76\u96c6\u6210\u5230vLLM\u7cfb\u7edf\u4e2d\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u771f\u5b9e\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPARS\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5305\u62ec\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e14\u8de8\u6a21\u578b\u8bc4\u4f30\u663e\u793a\u8bbe\u8ba1\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u3002", "conclusion": "PARS\u901a\u8fc7\u63d0\u793a\u611f\u77e5\u8c03\u5ea6\u6709\u6548\u51cf\u5c11\u4e86LLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u5177\u6709\u4f4e\u5f00\u9500\u548c\u826f\u597d\u7684\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03520", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03520", "abs": "https://arxiv.org/abs/2510.03520", "authors": ["Kartik Pandit", "Sourav Ganguly", "Arnesh Banerjee", "Shaahin Angizi", "Arnob Ghosh"], "title": "Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models", "comment": null, "summary": "Ensuring safety is a foundational requirement for large language models\n(LLMs). Achieving an appropriate balance between enhancing the utility of model\noutputs and mitigating their potential for harm is a complex and persistent\nchallenge. Contemporary approaches frequently formalize this problem within the\nframework of Constrained Markov Decision Processes (CMDPs) and employ\nestablished CMDP optimization techniques. However, these methods exhibit two\nnotable limitations. First, their reliance on reward and cost functions renders\nperformance highly sensitive to the underlying scoring mechanism, which must\ncapture semantic meaning rather than being triggered by superficial keywords.\nSecond, CMDP-based training entails tuning dual-variable, a process that is\nboth computationally expensive and does not provide any provable safety\nguarantee for a fixed dual variable that can be exploitable through adversarial\njailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF\n(CS-RLHF) that introduces a cost model trained on a large-scale corpus to\nassign semantically grounded safety scores. In contrast to the lagrangian-based\napproach, CS-RLHF adopts a rectified penalty-based formulation. This design\ndraws on the theory of exact penalty functions in constrained optimization,\nwherein constraint satisfaction is enforced directly through a suitably chosen\npenalty term. With an appropriately scaled penalty, feasibility of the safety\nconstraints can be guaranteed at the optimizer, eliminating the need for\ndual-variable updates. Empirical evaluation demonstrates that CS-RLHF\noutperforms state-of-the-art LLM model responses rendering at-least 5 times\nefficient against nominal and jail-breaking prompts", "AI": {"tldr": "CS-RLHF\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8ba4\u8bc1\u7684\u5b89\u5168RLHF\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u4fee\u6b63\u60e9\u7f5a\u7684\u516c\u5f0f\u66ff\u4ee3\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff0c\u76f4\u63a5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u65e0\u9700\u53cc\u53d8\u91cf\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCMDP\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u6027\u80fd\u5bf9\u8bc4\u5206\u673a\u5236\u9ad8\u5ea6\u654f\u611f\uff1b2\uff09\u53cc\u53d8\u91cf\u8c03\u4f18\u8ba1\u7b97\u6602\u8d35\u4e14\u65e0\u6cd5\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u4fdd\u8bc1\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5e73\u8861LLM\u7684\u5b9e\u7528\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u5f15\u5165\u4e86CS-RLHF\u65b9\u6cd5\uff0c\u4f7f\u7528\u5728\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\u7684\u6210\u672c\u6a21\u578b\u5206\u914d\u8bed\u4e49\u57fa\u7840\u7684\u5b89\u5168\u5206\u6570\uff0c\u91c7\u7528\u57fa\u4e8e\u4fee\u6b63\u60e9\u7f5a\u7684\u516c\u5f0f\uff0c\u5229\u7528\u7ea6\u675f\u4f18\u5316\u4e2d\u7684\u7cbe\u786e\u60e9\u7f5a\u51fd\u6570\u7406\u8bba\u76f4\u63a5\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cCS-RLHF\u5728\u5bf9\u6297\u666e\u901a\u548c\u8d8a\u72f1\u63d0\u793a\u65b9\u9762\uff0c\u6bd4\u6700\u5148\u8fdb\u7684LLM\u6a21\u578b\u54cd\u5e94\u81f3\u5c11\u9ad8\u65485\u500d\u3002", "conclusion": "CS-RLHF\u901a\u8fc7\u6d88\u9664\u53cc\u53d8\u91cf\u66f4\u65b0\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u4e14\u5177\u6709\u53ef\u8bc1\u660e\u5b89\u5168\u4fdd\u8bc1\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5b89\u5168RL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.03749", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03749", "abs": "https://arxiv.org/abs/2510.03749", "authors": ["Fanghao Xia", "Zesong Fei", "Xinyi Wang", "Nanchi Su", "Zhaolin Wang", "Yuanwei Liu", "Jie Xu"], "title": "Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?", "comment": "13 pages, 12 figures", "summary": "In this paper, sensing-assisted secure communication in a multi-user\nmulti-eavesdropper integrated sensing and communication (ISAC) system is\ninvestigated. Confidential communication signals and dedicated sensing signals\nare jointly transmitted by a base station (BS) to simultaneously serve users\nand sense aerial eavesdroppers (AEs). A sum rate maximization problem is\nformulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and\nsensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A\nfractional-programming-based alternating optimization algorithm is developed to\nsolve this problem for fully digital arrays, where successive convex\napproximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle\nnon-convex constraints. Furthermore, the minimum number of dedicated sensing\nbeams is analyzed via a worst-case rank bound, upon which the proposed\nbeamforming design is further extended to the hybrid analog-digital (HAD) array\narchitecture, where the unit-modulus constraint is addressed by manifold\noptimization. Simulation results demonstrate that only a small number of\nsensing beams are sufficient for both sensing and jamming AEs, and the proposed\ndesigns consistently outperform strong baselines while also revealing the\ncommunication-sensing trade-off.", "AI": {"tldr": "\u7814\u7a76\u591a\u7528\u6237\u591a\u7a83\u542c\u8005\u7684ISAC\u7cfb\u7edf\u4e2d\u7684\u611f\u77e5\u8f85\u52a9\u5b89\u5168\u901a\u4fe1\uff0c\u901a\u8fc7\u8054\u5408\u4f20\u8f93\u901a\u4fe1\u4fe1\u53f7\u548c\u4e13\u7528\u611f\u77e5\u4fe1\u53f7\u6765\u670d\u52a1\u7528\u6237\u5e76\u611f\u77e5\u7a7a\u4e2d\u7a83\u542c\u8005\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u6570\u89c4\u5212\u548c\u4ea4\u66ff\u4f18\u5316\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u3002", "motivation": "\u5728\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u901a\u4fe1\u6027\u80fd\u548c\u611f\u77e5\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u591a\u4e2a\u7a7a\u4e2d\u7a83\u542c\u8005\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u8bbe\u8ba1\u6709\u6548\u7684\u5b89\u5168\u901a\u4fe1\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5206\u6570\u89c4\u5212\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u7ed3\u5408\u9010\u6b21\u51f8\u8fd1\u4f3c\u548c\u534a\u5b9a\u677e\u5f1b\u5904\u7406\u975e\u51f8\u7ea6\u675f\uff0c\u5206\u6790\u6700\u5c0f\u611f\u77e5\u6ce2\u675f\u6570\u91cf\uff0c\u5e76\u5c06\u8bbe\u8ba1\u6269\u5c55\u5230\u6df7\u5408\u6a21\u62df\u6570\u5b57\u9635\u5217\u67b6\u6784\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5c11\u91cf\u611f\u77e5\u6ce2\u675f\u5373\u53ef\u540c\u65f6\u5b9e\u73b0\u611f\u77e5\u548c\u5e72\u6270\u7a83\u542c\u8005\uff0c\u6240\u63d0\u8bbe\u8ba1\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u901a\u4fe1-\u611f\u77e5\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aISAC\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u611f\u77e5\u8f85\u52a9\u901a\u4fe1\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03624", "categories": ["stat.ML", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.03624", "abs": "https://arxiv.org/abs/2510.03624", "authors": ["Kun Zhao", "Haoke Zhang", "Jiayi Wang", "Yifei Lou"], "title": "Transformed $\\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding", "comment": "Submitted to Journal of Machine Learning", "summary": "Robust Principal Component Analysis (RPCA) aims to recover a low-rank\nstructure from noisy, partially observed data that is also corrupted by sparse,\npotentially large-magnitude outliers. Traditional RPCA models rely on convex\nrelaxations, such as nuclear norm and $\\ell_1$ norm, to approximate the rank of\na matrix and the $\\ell_0$ functional (the number of non-zero elements) of\nanother. In this work, we advocate a nonconvex regularization method, referred\nto as transformed $\\ell_1$ (TL1), to improve both approximations. The rationale\nis that by varying the internal parameter of TL1, its behavior asymptotically\napproaches either $\\ell_0$ or $\\ell_1$. Since the rank is equal to the number\nof non-zero singular values and the nuclear norm is defined as their sum,\napplying TL1 to the singular values can approximate either the rank or the\nnuclear norm, depending on its internal parameter. We conduct a fine-grained\ntheoretical analysis of statistical convergence rates, measured in the\nFrobenius norm, for both the low-rank and sparse components under general\nsampling schemes. These rates are comparable to those of the classical RPCA\nmodel based on the nuclear norm and $\\ell_1$ norm. Moreover, we establish\nconstant-order upper bounds on the estimated rank of the low-rank component and\nthe cardinality of the sparse component in the regime where TL1 behaves like\n$\\ell_0$, assuming that the respective matrices are exactly low-rank and\nexactly sparse. Extensive numerical experiments on synthetic data and\nreal-world applications demonstrate that the proposed approach achieves higher\naccuracy than the classic convex model, especially under non-uniform sampling\nschemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u975e\u51f8\u6b63\u5219\u5316\u65b9\u6cd5\uff08\u53d8\u6362L1\u8303\u6570\uff0cTL1\uff09\u6765\u6539\u8fdb\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\uff08RPCA\uff09\uff0c\u901a\u8fc7\u8c03\u6574TL1\u7684\u5185\u90e8\u53c2\u6570\uff0c\u53ef\u4ee5\u6e10\u8fdb\u903c\u8fd1L0\u6216L1\u8303\u6570\uff0c\u4ece\u800c\u66f4\u597d\u5730\u8fd1\u4f3c\u77e9\u9635\u7684\u79e9\u548c\u7a00\u758f\u6027\u3002", "motivation": "\u4f20\u7edfRPCA\u6a21\u578b\u4f9d\u8d56\u51f8\u677e\u5f1b\u65b9\u6cd5\uff08\u5982\u6838\u8303\u6570\u548cL1\u8303\u6570\uff09\u6765\u8fd1\u4f3c\u77e9\u9635\u7684\u79e9\u548c\u7a00\u758f\u6027\uff0c\u4f46\u8fd9\u4e9b\u8fd1\u4f3c\u53ef\u80fd\u4e0d\u591f\u7cbe\u786e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u975e\u51f8\u6b63\u5219\u5316\u65b9\u6cd5\u63d0\u9ad8\u8fd1\u4f3c\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u53d8\u6362L1\uff08TL1\uff09\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u5176\u5185\u90e8\u53c2\u6570\u6765\u63a7\u5236\u903c\u8fd1L0\u6216L1\u8303\u6570\u7684\u7a0b\u5ea6\u3002\u5c06TL1\u5e94\u7528\u4e8e\u5947\u5f02\u503c\u6765\u8fd1\u4f3c\u77e9\u9635\u7684\u79e9\u6216\u6838\u8303\u6570\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u6536\u655b\u7387\u5206\u6790\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u65b9\u6cd5\u5728Frobenius\u8303\u6570\u4e0b\u7684\u7edf\u8ba1\u6536\u655b\u7387\u4e0e\u7ecf\u5178\u51f8\u6a21\u578b\u76f8\u5f53\uff0c\u5728TL1\u903c\u8fd1L0\u8303\u6570\u7684\u673a\u5236\u4e0b\uff0c\u5bf9\u4f4e\u79e9\u5206\u91cf\u548c\u7a00\u758f\u5206\u91cf\u7684\u4f30\u8ba1\u79e9\u548c\u57fa\u6570\u5efa\u7acb\u4e86\u5e38\u6570\u9636\u4e0a\u754c\u3002\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u975e\u5747\u5300\u91c7\u6837\u4e0b\u6bd4\u7ecf\u5178\u51f8\u6a21\u578b\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684TL1\u975e\u51f8\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u9c81\u68d2\u4e3b\u6210\u5206\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u7ecf\u5178\u51f8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u975e\u5747\u5300\u91c7\u6837\u573a\u666f\u4e0b\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6062\u590d\u4f4e\u79e9\u7ed3\u6784\u548c\u7a00\u758f\u5f02\u5e38\u503c\u3002"}}
{"id": "2510.03244", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03244", "abs": "https://arxiv.org/abs/2510.03244", "authors": ["Yanlong Wang", "Hang Yu", "Jian Xu", "Fei Ma", "Hongkang Zhang", "Tongtong Feng", "Zijian Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "comment": null, "summary": "Large time series foundation models often adopt channel-independent\narchitectures to handle varying data dimensions, but this design ignores\ncrucial cross-channel dependencies. Concurrently, existing multimodal\napproaches have not fully exploited the power of large vision models (LVMs) to\ninterpret spatiotemporal data. Additionally, there remains significant\nunexplored potential in leveraging the advantages of information extraction\nfrom different modalities to enhance time series forecasting performance. To\naddress these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO\nuniquely renders multivariate time series into image, enabling pre-trained LVM\nto extract complex cross-channel patterns that are invisible to\nchannel-independent models. These visual features are then aligned and fused\nwith representations from the time series modality. By freezing the LVM and\ntraining only 7.45% of its parameters, VIFO achieves competitive performance on\nmultiple benchmarks, offering an efficient and effective solution for capturing\ncross-variable relationships in", "AI": {"tldr": "VIFO\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u901a\u9053\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u91c7\u7528\u901a\u9053\u72ec\u7acb\u67b6\u6784\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u8de8\u901a\u9053\u4f9d\u8d56\u5173\u7cfb\uff1b\u540c\u65f6\u591a\u6a21\u6001\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u578b\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65f6\u7a7a\u6570\u636e\u7684\u80fd\u529b\uff1b\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u7684\u4f18\u52bf\u5c1a\u672a\u5145\u5206\u7528\u4e8e\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u89c6\u89c9\u6a21\u578b\u63d0\u53d6\u590d\u6742\u7684\u8de8\u901a\u9053\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u89c6\u89c9\u7279\u5f81\u4e0e\u65f6\u95f4\u5e8f\u5217\u6a21\u6001\u7684\u8868\u5f81\u8fdb\u884c\u5bf9\u9f50\u548c\u878d\u5408\uff0c\u4ec5\u8bad\u7ec37.45%\u7684\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u6355\u6349\u8de8\u53d8\u91cf\u5173\u7cfb\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "VIFO\u901a\u8fc7\u8de8\u6a21\u6001\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u901a\u9053\u72ec\u7acb\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03571", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.2.7; C.2.1"], "pdf": "https://arxiv.org/pdf/2510.03571", "abs": "https://arxiv.org/abs/2510.03571", "authors": ["Burak Karabulut", "Carlo Manna", "Chris Develder"], "title": "Generalization of Graph Neural Network Models for Distribution Grid Fault Detection", "comment": "This paper has been submitted and accepted for IEEE SmartGridComm\n  2025", "summary": "Fault detection in power distribution grids is critical for ensuring system\nreliability and preventing costly outages. Moreover, fault detection\nmethodologies should remain robust to evolving grid topologies caused by\nfactors such as reconfigurations, equipment failures, and Distributed Energy\nResource (DER) integration. Current data-driven state-of-the-art methods use\nRecurrent Neural Networks (RNNs) for temporal modeling and Graph Neural\nNetworks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in\nshort). Specifically, for power system fault diagnosis, Graph Convolutional\nNetworks (GCNs) have been adopted. Yet, various more advanced GNN architectures\nhave been proposed and adopted in domains outside of power systems. In this\npaper, we set out to systematically and consistently benchmark various GNN\narchitectures in an RNN+GNN pipeline model. Specifically, to the best of our\nknowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention\n(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive\nbenchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN\nmodels (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring\ntheir generalization potential for deployment in different settings than those\nused for training them. Our experimental results on the IEEE 123-node\ndistribution network show that RGATv2 has superior generalization capabilities,\nmaintaining high performance with an F1-score reduction of $\\sim$12% across\ndifferent topology settings. In contrast, pure RNN models largely fail,\nexperiencing an F1-score reduction of up to $\\sim$60%, while other RGNN\nvariants also exhibit significant performance degradation, i.e., up to\n$\\sim$25% lower F1-scores.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u4e0d\u540c\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0RGATv2\u6a21\u578b\u5728\u4e0d\u540c\u62d3\u6251\u53d8\u5316\u4e0b\u5177\u6709\u6700\u4f73\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u7eafRNN\u6a21\u578b\u8868\u73b0\u6700\u5dee\u3002", "motivation": "\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u7535\u7f51\u62d3\u6251\u3002\u5f53\u524d\u57fa\u4e8eRNN+GNN\u7684\u65b9\u6cd5\u4e2d\uff0cGCN\u67b6\u6784\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u9009\u62e9\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u66f4\u5148\u8fdb\u7684GNN\u67b6\u6784\u3002", "method": "\u91c7\u7528RNN+GNN\u6d41\u6c34\u7ebf\u6a21\u578b\uff0c\u9996\u6b21\u5c06GraphSAGE\u3001GAT\u548cGATv2\u5e94\u7528\u4e8e\u6545\u969c\u8bca\u65ad\uff0c\u5e76\u4e0eRGCN\u548c\u7eafRNN\u6a21\u578b\uff08\u7279\u522b\u662fGRU\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u5bf9\u6bd4\uff0c\u91cd\u70b9\u8bc4\u4f30\u5b83\u4eec\u5728\u4e0d\u540c\u62d3\u6251\u8bbe\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728IEEE 123\u8282\u70b9\u914d\u7535\u7f51\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRGATv2\u5177\u6709\u6700\u4f73\u6cdb\u5316\u6027\u80fd\uff0cF1\u5206\u6570\u4ec5\u4e0b\u964d\u7ea612%\uff1b\u7eafRNN\u6a21\u578b\u8868\u73b0\u6700\u5dee\uff0cF1\u5206\u6570\u4e0b\u964d\u9ad8\u8fbe60%\uff1b\u5176\u4ed6RGNN\u53d8\u4f53\u6027\u80fd\u4e0b\u964d\u7ea625%\u3002", "conclusion": "RGATv2\u5728\u914d\u7535\u7f51\u6545\u969c\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u7535\u7f51\u62d3\u6251\u53d8\u5316\uff0c\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u4f18\u9009\u65b9\u6848\u3002"}}
{"id": "2510.03780", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03780", "abs": "https://arxiv.org/abs/2510.03780", "authors": ["Yiqiao Chen"], "title": "A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification", "comment": "8 pages, 5 figures", "summary": "Cardiovascular disease (CVD) is a major pediatric health burden, and early\nscreening is of critical importance. Electrocardiography (ECG), as a\nnoninvasive and accessible tool, is well suited for this purpose. This paper\npresents the first benchmark study of deep learning for multi-label pediatric\nCVD classification on the recently released ZZU-pECG dataset, comprising 3716\nrecordings with 19 CVD categories. We systematically evaluate four\nrepresentative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under\nboth 9-lead and 12-lead configurations. All models achieved strong results,\nwith Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.\nResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and\nTransformer also showed competitive performance. Per-class analysis indicated\nchallenges for rare conditions such as hypertrophic cardiomyopathy in the\n9-lead subset, reflecting the effect of limited positive samples. This\nbenchmark establishes reusable baselines and highlights complementary strengths\nacross paradigms. It further points to the need for larger-scale, multi-center\nvalidation, age-stratified analysis, and broader disease coverage to support\nreal-world pediatric ECG applications.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5728\u513f\u79d1\u5fc3\u8840\u7ba1\u75be\u75c5\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u9762\u8fdb\u884c\u57fa\u51c6\u7814\u7a76\uff0c\u4f7f\u7528ZZU-pECG\u6570\u636e\u96c6\uff083716\u4e2a\u8bb0\u5f55\uff0c19\u79cdCVD\u7c7b\u522b\uff09\uff0c\u8bc4\u4f30\u4e86ResNet-1D\u3001BiLSTM\u3001Transformer\u548cMamba 2\u56db\u79cd\u6a21\u578b\u57289\u5bfc\u8054\u548c12\u5bfc\u8054\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u513f\u79d1\u4e3b\u8981\u5065\u5eb7\u8d1f\u62c5\uff0c\u65e9\u671f\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\u3002\u5fc3\u7535\u56fe\u4f5c\u4e3a\u65e0\u521b\u4e14\u6613\u5f97\u7684\u5de5\u5177\u975e\u5e38\u9002\u5408\u6b64\u76ee\u7684\uff0c\u4f46\u7f3a\u4e4f\u513f\u79d1CVD\u5206\u7c7b\u7684\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u7814\u7a76\u3002", "method": "\u4f7f\u7528ZZU-pECG\u6570\u636e\u96c6\uff083716\u4e2a\u8bb0\u5f55\uff0c19\u79cdCVD\u7c7b\u522b\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30ResNet-1D\u3001BiLSTM\u3001Transformer\u548cMamba 2\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u57289\u5bfc\u8054\u548c12\u5bfc\u8054\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u7ed3\u679c\uff0c\u6c49\u660e\u635f\u5931\u4f4e\u81f30.0069\uff0c\u5927\u591a\u6570\u8bbe\u7f6e\u4e0bF1\u5206\u6570\u8d85\u8fc785%\u3002ResNet-1D\u572812\u5bfc\u8054\u5b50\u96c6\u4e0a\u8fbe\u523094.67%\u7684\u5b8fF1\uff0cBiLSTM\u548cTransformer\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002\u4f469\u5bfc\u8054\u5b50\u96c6\u4e2d\u7f55\u89c1\u75be\u75c5\u5982\u80a5\u539a\u578b\u5fc3\u808c\u75c5\u5206\u7c7b\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "\u8be5\u57fa\u51c6\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u590d\u7528\u7684\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u4e0d\u540c\u8303\u5f0f\u7684\u4e92\u8865\u4f18\u52bf\u3002\u672a\u6765\u9700\u8981\u66f4\u5927\u89c4\u6a21\u3001\u591a\u4e2d\u5fc3\u9a8c\u8bc1\u3001\u5e74\u9f84\u5206\u5c42\u5206\u6790\u548c\u66f4\u5e7f\u6cdb\u7684\u75be\u75c5\u8986\u76d6\uff0c\u4ee5\u652f\u6301\u771f\u5b9e\u4e16\u754c\u7684\u513f\u79d1\u5fc3\u7535\u56fe\u5e94\u7528\u3002"}}
{"id": "2510.03685", "categories": ["stat.ML", "cs.LG", "math.LO", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.03685", "abs": "https://arxiv.org/abs/2510.03685", "authors": ["Nikitin Nikita"], "title": "The analogy theorem in Hoare logic", "comment": null, "summary": "The introduction of machine learning methods has led to significant advances\nin automation, optimization, and discoveries in various fields of science and\ntechnology. However, their widespread application faces a fundamental\nlimitation: the transfer of models between data domains generally lacks a\nrigorous mathematical justification. The key problem is the lack of formal\ncriteria to guarantee that a model trained on one type of data will retain its\nproperties on another.This paper proposes a solution to this problem by\nformalizing the concept of analogy between data sets and models using\nfirst-order logic and Hoare logic.We formulate and rigorously prove a theorem\nthat sets out the necessary and sufficient conditions for analogy in the task\nof knowledge transfer between machine learning models. Practical verification\nof the analogy theorem on model data obtained using the Monte Carlo method, as\nwell as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and\n0.88 for convolutional neural networks and random forests, respectively.The\nproposed approach not only allows us to justify the correctness of transfer\nbetween domains but also provides tools for comparing the applicability of\nmodels to different types of data.The main contribution of the work is a\nrigorous formalization of analogy at the level of program logic, providing\nverifiable guarantees of the correctness of knowledge transfer, which opens new\nopportunities for both theoretical research and the practical use of machine\nlearning models in previously inaccessible areas.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u548c\u970d\u5c14\u903b\u8f91\u7684\u5f62\u5f0f\u5316\u7c7b\u6bd4\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u57df\u4e4b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u6570\u5b66\u4fdd\u8bc1\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u57df\u95f4\u7684\u8fc1\u79fb\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u5b66\u7406\u8bba\u57fa\u7840\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u6a21\u578b\u5728\u76ee\u6807\u57df\u4e0a\u7684\u6027\u80fd\u4fdd\u6301\u3002", "method": "\u4f7f\u7528\u4e00\u9636\u903b\u8f91\u548c\u970d\u5c14\u903b\u8f91\u5f62\u5f0f\u5316\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e4b\u95f4\u7684\u7c7b\u6bd4\u6982\u5ff5\uff0c\u63d0\u51fa\u5e76\u8bc1\u660e\u7c7b\u6bd4\u5b9a\u7406\uff0c\u5e76\u5728\u6a21\u578b\u6570\u636e\u548cMNIST\u3001USPS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u968f\u673a\u68ee\u6797\u4e0a\u5206\u522b\u8fbe\u52300.84\u548c0.88\u7684F1\u5206\u6570\uff0c\u9a8c\u8bc1\u4e86\u7c7b\u6bd4\u5b9a\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7a0b\u5e8f\u903b\u8f91\u5c42\u9762\u7684\u4e25\u683c\u5f62\u5f0f\u5316\uff0c\u4e3a\u77e5\u8bc6\u8fc1\u79fb\u7684\u6b63\u786e\u6027\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u7684\u4fdd\u8bc1\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e4b\u524d\u4e0d\u53ef\u8fbe\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2510.03245", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03245", "abs": "https://arxiv.org/abs/2510.03245", "authors": ["Ali Yavari", "Alireza Mohamadi", "Elham Beydaghi", "Rainer A. Leitgeb"], "title": "Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability", "comment": "Preprint", "summary": "Ensuring the reliability of deep neural networks (DNNs) in the presence of\nreal world noise and intentional perturbations remains a significant challenge.\nTo address this, attribution methods have been proposed, though their efficacy\nremains suboptimal and necessitates further refinement. In this paper, we\npropose a novel category of transferable adversarial attacks, called\ntransferable frequency-aware attacks, enabling frequency-aware exploration via\nboth high-and low-frequency components. Based on this type of attacks, we also\npropose a novel attribution method, named Frequency-Aware Model Parameter\nExplorer (FAMPE), which improves the explainability for DNNs. Relative to the\ncurrent state-of-the-art method AttEXplore, our FAMPE attains an average gain\nof 13.02% in Insertion Score, thereby outperforming existing approaches.\nThrough detailed ablation studies, we also investigate the role of both high-\nand low-frequency components in explainability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8f6c\u79fb\u7684\u9891\u7387\u611f\u77e5\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u9891\u7387\u611f\u77e5\u6a21\u578b\u53c2\u6570\u63a2\u7d22\u5668(FAMPE)\u6765\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u63d2\u5165\u5f97\u5206\u4e0a\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u5e73\u5747\u63d0\u534713.02%\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u771f\u5b9e\u4e16\u754c\u566a\u58f0\u548c\u6709\u610f\u6270\u52a8\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u6548\u679c\u4e0d\u7406\u60f3\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u53ef\u8f6c\u79fb\u7684\u9891\u7387\u611f\u77e5\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u9ad8\u4f4e\u9891\u5206\u91cf\u8fdb\u884c\u9891\u7387\u611f\u77e5\u63a2\u7d22\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1FAMPE\u5f52\u56e0\u65b9\u6cd5\u3002", "result": "\u76f8\u6bd4\u5f53\u524d\u6700\u4f73\u65b9\u6cd5AttEXplore\uff0cFAMPE\u5728\u63d2\u5165\u5f97\u5206\u4e0a\u5e73\u5747\u63d0\u534713.02%\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u9ad8\u4f4e\u9891\u5206\u91cf\u5728\u53ef\u89e3\u91ca\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u9891\u7387\u611f\u77e5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9ad8\u4f4e\u9891\u5206\u91cf\u5728\u6a21\u578b\u89e3\u91ca\u4e2d\u90fd\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.03657", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03657", "abs": "https://arxiv.org/abs/2510.03657", "authors": ["Aymeric Fabre"], "title": "Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast", "comment": null, "summary": "In electricity markets around the world, the ability to anticipate price\nmovements with precision can be the difference between profit and loss,\nespecially for fast-acting assets like battery energy storage systems (BESS).\nAs grid volatility increases due to renewables and market decentralisation,\noperators and forecasters alike face growing pressure to transform prediction\ninto strategy. Yet while forecast data is abundant, especially in advanced\nmarkets like Australia's National Electricity Market (NEM), its practical value\nin driving real-world BESS trading decisions remains largely unexplored. This\nthesis dives into that gap. This work addresses a key research question: Can\nthe accuracy of the Australian Energy Market Operator (AEMO) energy price\nforecasts be systematically leveraged to develop a reliable and profitable\nbattery energy storage system trading algorithm? Despite the availability of\nAEMO price forecasts, no existing framework evaluates their reliability or\nincorporates them into practical BESS trading strategies. By analysing patterns\nin forecast accuracy based on time of day, forecast horizon, and regional\nvariations, this project creates a novel, forecast-informed BESS trading model\nto optimise arbitrage financial returns. The performance of this\nforecast-driven algorithm is benchmarked against a basic trading algorithm with\nno knowledge of forecast data. The study further explores the potential of\nmachine learning techniques to predict future energy prices by enhancing AEMO\nforecasts to govern a more advanced trading strategy. The research outcomes\nwill inform future improvements in energy market trading models and promote\nmore efficient BESS integration into market operations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6fb3\u5927\u5229\u4e9a\u80fd\u6e90\u5e02\u573a\u8fd0\u8425\u5546(AEMO)\u7684\u7535\u4ef7\u9884\u6d4b\u6570\u636e\u5f00\u53d1\u53ef\u9760\u7684\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf(BESS)\u4ea4\u6613\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u9884\u6d4b\u51c6\u786e\u6027\u6a21\u5f0f\u521b\u5efa\u57fa\u4e8e\u9884\u6d4b\u7684\u4ea4\u6613\u6a21\u578b\uff0c\u5e76\u4e0e\u65e0\u9884\u6d4b\u7684\u57fa\u672c\u7b97\u6cd5\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "motivation": "\u968f\u7740\u7535\u7f51\u6ce2\u52a8\u6027\u589e\u52a0\uff0c\u5c06\u7535\u4ef7\u9884\u6d4b\u8f6c\u5316\u4e3a\u5b9e\u9645\u4ea4\u6613\u7b56\u7565\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u867d\u7136AEMO\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u9884\u6d4b\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6570\u636e\u5728\u5b9e\u9645BESS\u4ea4\u6613\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5206\u6790\u57fa\u4e8e\u65f6\u95f4\u3001\u9884\u6d4b\u5468\u671f\u548c\u5730\u533a\u53d8\u5316\u7684\u9884\u6d4b\u51c6\u786e\u6027\u6a21\u5f0f\uff0c\u521b\u5efa\u57fa\u4e8e\u9884\u6d4b\u7684BESS\u4ea4\u6613\u6a21\u578b\uff0c\u5e76\u4e0e\u65e0\u9884\u6d4b\u7684\u57fa\u672c\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u540c\u65f6\u63a2\u7d22\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u589e\u5f3aAEMO\u9884\u6d4b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8e\u9884\u6d4b\u7684BESS\u4ea4\u6613\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316\u5957\u5229\u8d22\u52a1\u56de\u62a5\uff0c\u5e76\u8bc4\u4f30\u4e86\u9884\u6d4b\u9a71\u52a8\u7b97\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u5c06\u4e3a\u80fd\u6e90\u5e02\u573a\u4ea4\u6613\u6a21\u578b\u7684\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4fe1\u606f\uff0c\u5e76\u4fc3\u8fdbBESS\u66f4\u6709\u6548\u5730\u878d\u5165\u5e02\u573a\u8fd0\u8425\u3002"}}
{"id": "2510.03787", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03787", "abs": "https://arxiv.org/abs/2510.03787", "authors": ["Jacopo Pegoraro", "Gianmaria Ventura", "Dario Tagliaferri", "Marco Mezzavilla", "Andrea Bedin", "Michele Rossi", "Joerg Widmer"], "title": "Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms", "comment": "19 pages, 14 figures", "summary": "Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G\nwireless networks. The bandwidth availability and diversity of FR3 offer\nunprecedented opportunities for coherent multiband Integrated Sensing and\nCommunications (ISAC), which aggregates the carrier phase information from\nmultiple frequency bands to increase the sensing resolution to the cm-level.\nHowever, the frequency anisotropy of sensing targets over GHz-wide bands and\nthe non-contiguity of the 6G spectrum, pose critical challenges to the\napplication of existing multiband ISAC techniques. We present the first study\non coherent multiband sensing in FR3. We experimentally characterize the\nfrequency anisotropy of targets and propose new phase coherence metrics for\nmultiband processing. Then, we analyze the impact of non-contiguous FR3 bands\nconsidered by 3GPP, and design a new algorithm to mitigate the resulting\nsensing artifacts, outperforming existing techniques. Our results represent a\nfirst step toward fully developing multiband ISAC for FR3.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86FR3\u9891\u6bb5\uff087-24 GHz\uff09\u7684\u76f8\u5e72\u591a\u9891\u6bb5\u611f\u77e5\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u76f8\u4f4d\u76f8\u5e72\u6027\u6307\u6807\u548c\u7b97\u6cd5\u6765\u89e3\u51b3\u9891\u7387\u5404\u5411\u5f02\u6027\u548c\u975e\u8fde\u7eed\u9891\u8c31\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "FR3\u9891\u6bb5\u4e3a6G\u7f51\u7edc\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u591a\u9891\u6bb5\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u673a\u4f1a\uff0c\u4f46\u9891\u7387\u5404\u5411\u5f02\u6027\u548c\u9891\u8c31\u975e\u8fde\u7eed\u6027\u5bf9\u73b0\u6709\u6280\u672f\u6784\u6210\u4e86\u5173\u952e\u6311\u6218\u3002", "method": "\u5b9e\u9a8c\u8868\u5f81\u4e86\u76ee\u6807\u7684\u9891\u7387\u5404\u5411\u5f02\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u591a\u9891\u6bb5\u5904\u7406\u76f8\u4f4d\u76f8\u5e72\u6027\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7b97\u6cd5\u6765\u51cf\u8f7b\u975e\u8fde\u7eed\u9891\u6bb5\u5bfc\u81f4\u7684\u611f\u77e5\u4f2a\u5f71\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u7f13\u89e3\u975e\u8fde\u7eedFR3\u9891\u6bb5\u5e26\u6765\u7684\u611f\u77e5\u95ee\u9898\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u662f\u5f00\u53d1FR3\u591a\u9891\u6bb5ISAC\u6280\u672f\u7684\u91cd\u8981\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2510.03809", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03809", "abs": "https://arxiv.org/abs/2510.03809", "authors": ["William Hao-Cheng Huang"], "title": "Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning", "comment": null, "summary": "In high-dimensional learning, models remain stable until they collapse\nabruptly once the sample size falls below a critical level. This instability is\nnot algorithm-specific but a geometric mechanism: when the weakest Fisher\neigendirection falls beneath sample-level fluctuations, identifiability fails.\nOur Fisher Threshold Theorem formalizes this by proving that stability requires\nthe minimal Fisher eigenvalue to exceed an explicit $O(\\sqrt{d/n})$ bound.\nUnlike prior asymptotic or model-specific criteria, this threshold is\nfinite-sample and necessary, marking a sharp phase transition between reliable\nconcentration and inevitable failure. To make the principle constructive, we\nintroduce the Fisher floor, a verifiable spectral regularization robust to\nsmoothing and preconditioning. Synthetic experiments on Gaussian mixtures and\nlogistic models confirm the predicted transition, consistent with $d/n$\nscaling. Statistically, the threshold sharpens classical eigenvalue conditions\ninto a non-asymptotic law; learning-theoretically, it defines a spectral\nsample-complexity frontier, bridging theory with diagnostics for robust\nhigh-dimensional inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Fisher\u9608\u503c\u5b9a\u7406\uff0c\u63ed\u793a\u4e86\u9ad8\u7ef4\u5b66\u4e60\u4e2d\u6a21\u578b\u7a33\u5b9a\u6027\u4e0e\u6700\u5c0fFisher\u7279\u5f81\u503c\u4e4b\u95f4\u7684\u4e34\u754c\u5173\u7cfb\uff1a\u5f53\u6700\u5c0fFisher\u7279\u5f81\u503c\u4f4e\u4e8eO(\u221a(d/n))\u754c\u9650\u65f6\uff0c\u6a21\u578b\u4f1a\u7a81\u7136\u5d29\u6e83\u3002", "motivation": "\u9ad8\u7ef4\u5b66\u4e60\u4e2d\u6a21\u578b\u4f1a\u7a81\u7136\u5d29\u6e83\u7684\u73b0\u8c61\u4e0d\u662f\u7b97\u6cd5\u7279\u5b9a\u7684\uff0c\u800c\u662f\u51e0\u4f55\u673a\u5236\u5bfc\u81f4\u7684\uff0c\u9700\u8981\u7406\u89e3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u5f15\u5165Fisher\u9608\u503c\u5b9a\u7406\u8bc1\u660e\u7a33\u5b9a\u6027\u9700\u8981\u6700\u5c0fFisher\u7279\u5f81\u503c\u8d85\u8fc7\u663e\u5f0f\u7684O(\u221a(d/n))\u754c\u9650\uff0c\u5e76\u63d0\u51faFisher floor\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u8c31\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u9ad8\u65af\u6df7\u5408\u548c\u903b\u8f91\u6a21\u578b\u4e0a\u7684\u5408\u6210\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u9884\u6d4b\u7684\u76f8\u53d8\uff0c\u4e0ed/n\u7f29\u653e\u4e00\u81f4\u3002", "conclusion": "\u8be5\u9608\u503c\u5c06\u7ecf\u5178\u7279\u5f81\u503c\u6761\u4ef6\u9510\u5316\u4e3a\u975e\u6e10\u8fd1\u5b9a\u5f8b\uff0c\u5b9a\u4e49\u4e86\u8c31\u6837\u672c\u590d\u6742\u5ea6\u524d\u6cbf\uff0c\u4e3a\u7a33\u5065\u9ad8\u7ef4\u63a8\u65ad\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2510.03246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03246", "abs": "https://arxiv.org/abs/2510.03246", "authors": ["Xinyuan Song", "Guangji Bai", "Liang Zhao"], "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory", "comment": null, "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning\nachieves strong performance but requires $\\mathcal{O}(N)$ memory, which is\ninfeasible for billion-parameter models. Local pruning reduces GPU memory usage\nto that of a single layer by pruning layers independently, but it neglects\ninter-layer dependencies and often leads to suboptimal performance in\nhigh-sparsity regimes. Unlike unstructured pruning, structured pruning produces\nregular sparsity patterns that align well with GPU kernels and library\noptimizations, making it more hardware-efficient. However, structured pruning\ntypically relies on global pruning, since structured patterns are more prone to\nsevere performance degradation under local optimization. To jointly achieve\nstructured pruning and the memory efficiency of local pruning, we propose a\ndivide-and-conquer strategy that decomposes the global pruning problem into\ncoordinated subproblems across different modules, each of which fits within\nlimited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an\nADMM-based framework that integrates structured sparsity into the pruning\nprocess, combining the memory efficiency of local pruning with the hardware\ncompatibility of structured methods. We derive a closed-form analytical\nsolution for structured pruning masks that provides an explicit rule for\nlayer-wise sparsity allocation, and further develop an energy-based asymptotic\nframework yielding a softmax-form allocation scheme that simplifies\noptimization while adapting to heterogeneous layer importance. Experiments\ndemonstrate that STRUPRUNE matches the perplexity of global structured pruning\nwhile reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$,\nenabling practical deployment at the billion-parameter scale.", "AI": {"tldr": "\u63d0\u51fa\u4e86STRUPRUNE\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u526a\u679d\u7684\u786c\u4ef6\u6548\u7387\u548c\u5c40\u90e8\u526a\u679d\u7684\u5185\u5b58\u6548\u7387\uff0c\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u5c06\u5168\u5c40\u526a\u679d\u95ee\u9898\u5206\u89e3\u4e3a\u534f\u8c03\u7684\u5b50\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u5168\u5c40\u526a\u679d\u867d\u7136\u6027\u80fd\u597d\u4f46\u5185\u5b58\u9700\u6c42\u9ad8\uff08O(N)\uff09\uff0c\u4e0d\u9002\u5408\u5341\u4ebf\u53c2\u6570\u6a21\u578b\uff1b\u5c40\u90e8\u526a\u679d\u5185\u5b58\u6548\u7387\u9ad8\u4f46\u5ffd\u7565\u5c42\u95f4\u4f9d\u8d56\uff0c\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u6027\u80fd\u4e0d\u4f73\uff1b\u7ed3\u6784\u5316\u526a\u679d\u786c\u4ef6\u6548\u7387\u9ad8\u4f46\u901a\u5e38\u4f9d\u8d56\u5168\u5c40\u526a\u679d\u3002\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u7ed3\u6784\u5316\u526a\u679d\u548c\u5c40\u90e8\u526a\u679d\u7684\u5185\u5b58\u6548\u7387\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u5c06\u5168\u5c40\u526a\u679d\u5206\u89e3\u4e3a\u8de8\u6a21\u5757\u7684\u534f\u8c03\u5b50\u95ee\u9898\uff1b\u57fa\u4e8eADMM\u6846\u67b6\u6574\u5408\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff1b\u63a8\u5bfc\u7ed3\u6784\u5316\u526a\u679d\u63a9\u7801\u7684\u95ed\u5f0f\u89e3\u6790\u89e3\uff0c\u63d0\u4f9b\u5c42\u95f4\u7a00\u758f\u5ea6\u5206\u914d\u7684\u663e\u5f0f\u89c4\u5219\uff1b\u5f00\u53d1\u57fa\u4e8e\u80fd\u91cf\u7684\u6e10\u8fdb\u6846\u67b6\uff0c\u4ea7\u751fsoftmax\u5f62\u5f0f\u7684\u5206\u914d\u65b9\u6848\u3002", "result": "STRUPRUNE\u5728\u5339\u914d\u5168\u5c40\u7ed3\u6784\u5316\u526a\u679d\u56f0\u60d1\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u5185\u5b58\u6210\u672c\u4eceO(N)\u964d\u4f4e\u5230O(\u221aN)\uff0c\u4f7f\u5f97\u5728\u5341\u4ebf\u53c2\u6570\u89c4\u6a21\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "STRUPRUNE\u6210\u529f\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u526a\u679d\u4e0e\u5185\u5b58\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u526a\u679d\u65b9\u6848\u3002"}}
{"id": "2510.03830", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03830", "abs": "https://arxiv.org/abs/2510.03830", "authors": ["Alex Durkin", "Jasper Stolte", "Mehmet Mercang\u00f6z"], "title": "HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control", "comment": "31 pages, 15 figures, submitted to Computers and Chemical Engineering", "summary": "Start-ups and product grade-changes are critical steps in continuous-process\nplant operation, because any misstep immediately affects product quality and\ndrives operational losses. These transitions have long relied on manual\noperation by a handful of expert operators, but the progressive retirement of\nthat workforce is leaving plant owners without the tacit know-how needed to\nexecute them consistently. In the absence of a process model, offline\nreinforcement learning (RL) promises to capture and even surpass human\nexpertise by mining historical start-up and grade-change logs, yet standard\noffline RL struggles with distribution shift and value-overestimation whenever\na learned policy ventures outside the data envelope. We introduce HOFLON\n(Hybrid Offline Learning + Online Optimization) to overcome those limitations.\nOffline, HOFLON learns (i) a latent data manifold that represents the feasible\nregion spanned by past transitions and (ii) a long-horizon Q-critic that\npredicts the cumulative reward from state-action pairs. Online, it solves a\none-step optimization problem that maximizes the Q-critic while penalizing\ndeviations from the learned manifold and excessive rates of change in the\nmanipulated variables. We test HOFLON on two industrial case studies: a\npolymerization reactor start-up and a paper-machine grade-change problem, and\nbenchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.\nIn both plants HOFLON not only surpasses IQL but also delivers, on average,\nbetter cumulative rewards than the best start-up or grade-change observed in\nthe historical data, demonstrating its potential to automate transition\noperations beyond current expert capability.", "AI": {"tldr": "HOFLON\uff08\u6df7\u5408\u79bb\u7ebf\u5b66\u4e60+\u5728\u7ebf\u4f18\u5316\uff09\u65b9\u6cd5\u89e3\u51b3\u4e86\u8fde\u7eed\u8fc7\u7a0b\u5de5\u5382\u542f\u52a8\u548c\u4ea7\u54c1\u7b49\u7ea7\u8f6c\u6362\u4e2d\u7684\u81ea\u52a8\u5316\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u5b66\u4e60\u6570\u636e\u6d41\u5f62\u548cQ\u503c\u8bc4\u4f30\u5668\uff0c\u4ee5\u53ca\u5728\u7ebf\u4f18\u5316\u63a7\u5236\u7b56\u7565\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u5de5\u5382\u542f\u52a8\u548c\u4ea7\u54c1\u7b49\u7ea7\u8f6c\u6362\u4f9d\u8d56\u5c11\u6570\u4e13\u5bb6\u64cd\u4f5c\u5458\u7684\u624b\u52a8\u64cd\u4f5c\uff0c\u4f46\u968f\u7740\u8fd9\u4e9b\u4e13\u5bb6\u9000\u4f11\uff0c\u5de5\u5382\u9762\u4e34\u7f3a\u4e4f\u9690\u6027\u77e5\u8bc6\u7684\u95ee\u9898\u3002\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u6316\u6398\u5386\u53f2\u6570\u636e\uff0c\u4f46\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u4ef7\u503c\u9ad8\u4f30\u7684\u6311\u6218\u3002", "method": "HOFLON\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u79bb\u7ebf\u9636\u6bb5\u5b66\u4e60\u6570\u636e\u6d41\u5f62\u548c\u957f\u65f6\u57dfQ\u503c\u8bc4\u4f30\u5668\uff1b\u5728\u7ebf\u9636\u6bb5\u901a\u8fc7\u5355\u6b65\u4f18\u5316\u6700\u5927\u5316Q\u503c\uff0c\u540c\u65f6\u60e9\u7f5a\u504f\u79bb\u5b66\u4e60\u6d41\u5f62\u548c\u64cd\u7eb5\u53d8\u91cf\u53d8\u5316\u7387\u8fc7\u5927\u7684\u884c\u4e3a\u3002", "result": "\u5728\u805a\u5408\u53cd\u5e94\u5668\u542f\u52a8\u548c\u9020\u7eb8\u673a\u7b49\u7ea7\u8f6c\u6362\u4e24\u4e2a\u5de5\u4e1a\u6848\u4f8b\u4e2d\uff0cHOFLON\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u79bb\u7ebfRL\u7b97\u6cd5IQL\uff0c\u5e73\u5747\u7d2f\u79ef\u5956\u52b1\u8fd8\u8d85\u8fc7\u4e86\u5386\u53f2\u6570\u636e\u4e2d\u89c2\u5bdf\u5230\u7684\u6700\u4f73\u542f\u52a8\u6216\u7b49\u7ea7\u8f6c\u6362\u3002", "conclusion": "HOFLON\u5c55\u793a\u4e86\u8d85\u8d8a\u5f53\u524d\u4e13\u5bb6\u80fd\u529b\u7684\u81ea\u52a8\u5316\u8fc7\u6e21\u64cd\u4f5c\u6f5c\u529b\uff0c\u4e3a\u8fde\u7eed\u8fc7\u7a0b\u5de5\u5382\u7684\u542f\u52a8\u548c\u4ea7\u54c1\u8f6c\u6362\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03818", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.03818", "abs": "https://arxiv.org/abs/2510.03818", "authors": ["Lulu Song", "Di Zhang", "Tingting Zhang"], "title": "Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime", "comment": null, "summary": "Source polar coding is a potential solution for short blocklength-based\nlow-latency key generation with limited sources, which is a critical aspect of\nsix generation (6G) Internet of things. However, existing source coding schemes\nstill suffer from significant degradation in key generation rate and\nreconciliation reliability in short blocklength regime. To address this issue,\nwe introduce a multilevel source polarization-adjusted convolutional (PAC)\ncoding framework. Furthermore, we propose a novel code construction algorithm\nthat jointly leverages polarization effects and the maximum likelihood (ML)\ndecoding error coefficient. Simulations demonstrate that the multilevel source\nPAC scheme with the proposed code construction achieves superior key generation\nrate under key disagreement constraints compared to conventional and multilevel\nsource polar coding methods even in short blocklength regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u6e90\u6781\u5316\u8c03\u6574\u5377\u79ef(PAC)\u7f16\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u77ed\u5757\u957f\u4e0b\u5bc6\u94a5\u751f\u6210\u7387\u548c\u534f\u8c03\u53ef\u9760\u6027\u7684\u663e\u8457\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u6e90\u6781\u5316\u7f16\u7801\u662f6G\u7269\u8054\u7f51\u4e2d\u57fa\u4e8e\u77ed\u5757\u957f\u7684\u4f4e\u5ef6\u8fdf\u5bc6\u94a5\u751f\u6210\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u5728\u77ed\u5757\u957f\u4e0b\u4ecd\u5b58\u5728\u5bc6\u94a5\u751f\u6210\u7387\u548c\u534f\u8c03\u53ef\u9760\u6027\u7684\u663e\u8457\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u591a\u7ea7\u6e90\u6781\u5316\u8c03\u6574\u5377\u79ef(PAC)\u7f16\u7801\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7801\u6784\u9020\u7b97\u6cd5\uff0c\u8054\u5408\u5229\u7528\u6781\u5316\u6548\u5e94\u548c\u6700\u5927\u4f3c\u7136(ML)\u89e3\u7801\u9519\u8bef\u7cfb\u6570\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u548c\u591a\u7ea7\u6e90\u6781\u5316\u7f16\u7801\u65b9\u6cd5\u76f8\u6bd4\uff0c\u91c7\u7528\u6240\u63d0\u7801\u6784\u9020\u7684\u591a\u7ea7\u6e90PAC\u65b9\u6848\u5728\u77ed\u5757\u957f\u4e0b\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5bc6\u94a5\u751f\u6210\u7387\u3002", "conclusion": "\u591a\u7ea7\u6e90PAC\u7f16\u7801\u6846\u67b6\u5728\u77ed\u5757\u957f\u4e0b\u80fd\u591f\u663e\u8457\u63d0\u5347\u5bc6\u94a5\u751f\u6210\u6027\u80fd\uff0c\u6ee1\u8db3\u5bc6\u94a5\u4e0d\u4e00\u81f4\u6027\u7ea6\u675f\u3002"}}
{"id": "2510.03929", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03929", "abs": "https://arxiv.org/abs/2510.03929", "authors": ["Andrew Campbell", "Valentin De Bortoli", "Jiaxin Shi", "Arnaud Doucet"], "title": "Self-Speculative Masked Diffusions", "comment": "32 pages, 7 figures, 3 tables", "summary": "We present self-speculative masked diffusions, a new class of masked\ndiffusion generative models for discrete data that require significantly fewer\nfunction evaluations to generate samples. Standard masked diffusion models\npredict factorized logits over currently masked positions. A number of masked\npositions are then sampled, however, the factorization approximation means that\nsampling too many positions in one go leads to poor sample quality. As a\nresult, many simulation steps and therefore neural network function evaluations\nare required to generate high-quality data. We reduce the computational burden\nby generating non-factorized predictions over masked positions. This is\nachieved by modifying the final transformer attention mask from non-causal to\ncausal, enabling draft token generation and parallel validation via a novel,\nmodel-integrated speculative sampling mechanism. This results in a\nnon-factorized predictive distribution over masked positions in a single\nforward pass. We apply our method to GPT2 scale text modelling and protein\nsequences generation, finding that we can achieve a ~2x reduction in the\nrequired number of network forward passes relative to standard masked diffusion\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u63a8\u6d4b\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4fee\u6539\u6ce8\u610f\u529b\u63a9\u7801\u5b9e\u73b0\u5e76\u884c\u9a8c\u8bc1\uff0c\u5c06\u79bb\u6563\u6570\u636e\u751f\u6210\u7684\u7f51\u7edc\u524d\u5411\u4f20\u64ad\u6b21\u6570\u51cf\u5c11\u7ea62\u500d", "motivation": "\u6807\u51c6\u63a9\u7801\u6269\u6563\u6a21\u578b\u7531\u4e8e\u56e0\u5b50\u5316\u8fd1\u4f3c\uff0c\u9700\u8981\u591a\u6b21\u91c7\u6837\u6b65\u9aa4\u624d\u80fd\u4fdd\u8bc1\u6837\u672c\u8d28\u91cf\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927", "method": "\u4fee\u6539transformer\u6ce8\u610f\u529b\u63a9\u7801\u4e3a\u975e\u56e0\u679c\u5230\u56e0\u679c\uff0c\u5b9e\u73b0\u8349\u7a3ftoken\u751f\u6210\u548c\u5e76\u884c\u9a8c\u8bc1\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u83b7\u5f97\u975e\u56e0\u5b50\u5316\u7684\u9884\u6d4b\u5206\u5e03", "result": "\u5728GPT2\u89c4\u6a21\u6587\u672c\u5efa\u6a21\u548c\u86cb\u767d\u8d28\u5e8f\u5217\u751f\u6210\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u63a9\u7801\u6269\u6563\u6a21\u578b\u51cf\u5c11\u4e86\u7ea62\u500d\u7f51\u7edc\u524d\u5411\u4f20\u64ad\u6b21\u6570", "conclusion": "\u81ea\u63a8\u6d4b\u63a9\u7801\u6269\u6563\u6a21\u578b\u80fd\u663e\u8457\u964d\u4f4e\u79bb\u6563\u6570\u636e\u751f\u6210\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6837\u672c\u751f\u6210"}}
{"id": "2510.03247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03247", "abs": "https://arxiv.org/abs/2510.03247", "authors": ["Jiancheng Zhang", "Yinglun Zhu"], "title": "Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data", "comment": null, "summary": "Active learning (AL) is a principled strategy to reduce annotation cost in\ndata-hungry deep learning. However, existing AL algorithms focus almost\nexclusively on unimodal data, overlooking the substantial annotation burden in\nmultimodal learning. We introduce the first framework for multimodal active\nlearning with unaligned data, where the learner must actively acquire\ncross-modal alignments rather than labels on pre-aligned pairs. This setting\ncaptures the practical bottleneck in modern multimodal pipelines such as CLIP\nand SigLIP, where unimodal features are easy to obtain but high-quality\nalignment is costly. We develop a new algorithm that combines uncertainty and\ndiversity principles in a modality-aware design, achieves linear-time\nacquisition, and applies seamlessly to both pool-based and streaming-based\nsettings. Extensive experiments on benchmark datasets demonstrate that our\napproach consistently reduces multimodal annotation cost while preserving\nperformance; for instance, on the ColorSwap dataset it cuts annotation\nrequirements by up to $40\\%$ without loss in accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u672a\u5bf9\u9f50\u591a\u6a21\u6001\u6570\u636e\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u83b7\u53d6\u8de8\u6a21\u6001\u5bf9\u9f50\u800c\u975e\u6807\u7b7e\u6765\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u9ad8\u8fbe40%\u7684\u6807\u6ce8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u6807\u6ce8\u5bf9\u9f50\u7684\u9ad8\u6602\u6210\u672c\uff0c\u7279\u522b\u662f\u5728CLIP\u548cSigLIP\u7b49\u73b0\u4ee3\u591a\u6a21\u6001\u7ba1\u9053\u4e2d\uff0c\u5355\u6a21\u6001\u7279\u5f81\u5bb9\u6613\u83b7\u53d6\u4f46\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u539f\u5219\u7684\u6a21\u6001\u611f\u77e5\u7b97\u6cd5\uff0c\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u83b7\u53d6\uff0c\u53ef\u65e0\u7f1d\u5e94\u7528\u4e8e\u57fa\u4e8e\u6c60\u548c\u57fa\u4e8e\u6d41\u7684\u8bbe\u7f6e\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u51cf\u5c11\u591a\u6a21\u6001\u6807\u6ce8\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u5728ColorSwap\u6570\u636e\u96c6\u4e0a\u53ef\u51cf\u5c11\u9ad8\u8fbe40%\u7684\u6807\u6ce8\u9700\u6c42\u800c\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u5bf9\u9f50\u6807\u6ce8\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u964d\u4f4e\u591a\u6a21\u6001\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04203", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04203", "abs": "https://arxiv.org/abs/2510.04203", "authors": ["Aayushya Agarwal", "Larry Pileggi", "Gauri Joshi"], "title": "Adaptive Federated Learning via Dynamical System Model", "comment": null, "summary": "Hyperparameter selection is critical for stable and efficient convergence of\nheterogeneous federated learning, where clients differ in computational\ncapabilities, and data distributions are non-IID. Tuning hyperparameters is a\nmanual and computationally expensive process as the hyperparameter space grows\ncombinatorially with the number of clients. To address this, we introduce an\nend-to-end adaptive federated learning method in which both clients and central\nagents adaptively select their local learning rates and momentum parameters.\nOur approach models federated learning as a dynamical system, allowing us to\ndraw on principles from numerical simulation and physical design. Through this\nperspective, selecting momentum parameters equates to critically damping the\nsystem for fast, stable convergence, while learning rates for clients and\ncentral servers are adaptively selected to satisfy accuracy properties from\nnumerical simulation. The result is an adaptive, momentum-based federated\nlearning algorithm in which the learning rates for clients and servers are\ndynamically adjusted and controlled by a single, global hyperparameter. By\ndesigning a fully integrated solution for both adaptive client updates and\ncentral agent aggregation, our method is capable of handling key challenges of\nheterogeneous federated learning, including objective inconsistency and client\ndrift. Importantly, our approach achieves fast convergence while being\ninsensitive to the choice of the global hyperparameter, making it well-suited\nfor rapid prototyping and scalable deployment. Compared to state-of-the-art\nadaptive methods, our framework is shown to deliver superior convergence for\nheterogeneous federated learning while eliminating the need for hyperparameter\ntuning both client and server updates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8054\u90a6\u5b66\u4e60\u5efa\u6a21\u4e3a\u52a8\u6001\u7cfb\u7edf\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u5ba2\u6237\u7aef\u548c\u4e2d\u592e\u670d\u52a1\u5668\u7684\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u53c2\u6570\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u8d85\u53c2\u6570\u9009\u62e9\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u624b\u52a8\u8c03\u53c2\u8fc7\u7a0b\u7e41\u7410\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7279\u522b\u662f\u5728\u5ba2\u6237\u7aef\u8ba1\u7b97\u80fd\u529b\u548c\u6570\u636e\u5206\u5e03\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5c06\u8054\u90a6\u5b66\u4e60\u5efa\u6a21\u4e3a\u52a8\u6001\u7cfb\u7edf\uff0c\u501f\u9274\u6570\u503c\u6a21\u62df\u548c\u7269\u7406\u8bbe\u8ba1\u539f\u7406\uff0c\u901a\u8fc7\u4e34\u754c\u963b\u5c3c\u9009\u62e9\u52a8\u91cf\u53c2\u6570\uff0c\u6839\u636e\u6570\u503c\u6a21\u62df\u7684\u7cbe\u5ea6\u8981\u6c42\u81ea\u9002\u5e94\u9009\u62e9\u5ba2\u6237\u7aef\u548c\u4e2d\u592e\u670d\u52a1\u5668\u7684\u5b66\u4e60\u7387\uff0c\u6240\u6709\u53c2\u6570\u7531\u5355\u4e2a\u5168\u5c40\u8d85\u53c2\u6570\u63a7\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff08\u5982\u76ee\u6807\u4e0d\u4e00\u81f4\u6027\u548c\u5ba2\u6237\u7aef\u6f02\u79fb\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u81ea\u9002\u5e94\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u6536\u655b\u6027\u80fd\uff0c\u4e14\u5bf9\u5168\u5c40\u8d85\u53c2\u6570\u9009\u62e9\u4e0d\u654f\u611f\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u6d88\u9664\u4e86\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u66f4\u65b0\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u9700\u6c42\uff0c\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u5728\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u655b\u3002"}}
{"id": "2510.03848", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03848", "abs": "https://arxiv.org/abs/2510.03848", "authors": ["Jianyu Wang", "Zhichao Li", "Wenchi Cheng", "Wei Zhang", "Hailin Zhang"], "title": "Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums", "comment": null, "summary": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. However, the propagation mediums in practical MI based underground\nemergency communications are usually diverse and composed randomly due to the\nimpact of disasters, which poses a challenge for MI communication in practical\napplications. In this paper, we formulate a statistical fading channel model,\nwhich reflects the random composition of diverse mediums and is shown to follow\na lognormal distribution. To mitigate the impact of diverse medium fading,\nMulti-frequency Resonating Compensation (MuReC) based coils are used to achieve\nmultiband transmission. Then, we analyze the performance of MuReC based\nmulti-band MI communication with diverse medium fading and derive the\nexpressions of signal-to-noise ratio (SNR) probability density functions,\nergodic capacities, average bit error rates (BERs), and outage probabilities\nfor both multiplexing and diversity cases. Numerical results show that MuReC\nbased multiband transmission schemes can effectively reduce the impact of\ndiverse medium fading and enhance the performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u8ba1\u8870\u843d\u4fe1\u9053\u6a21\u578b\u6765\u5e94\u5bf9\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u4e2d\u591a\u6837\u4ecb\u8d28\u7684\u968f\u673a\u7ec4\u5408\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u591a\u9891\u8c10\u632f\u8865\u507f\u7ebf\u5708\u5b9e\u73b0\u591a\u9891\u5e26\u4f20\u8f93\u4ee5\u51cf\u8f7b\u591a\u6837\u4ecb\u8d28\u8870\u843d\u7684\u5f71\u54cd\u3002", "motivation": "\u78c1\u611f\u5e94\u901a\u4fe1\u5728\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4f20\u64ad\u4ecb\u8d28\u7684\u591a\u6837\u6027\u548c\u968f\u673a\u7ec4\u5408\u5bf9\u901a\u4fe1\u6027\u80fd\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u591a\u6837\u4ecb\u8d28\u8870\u843d\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u9075\u5faa\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u7684\u7edf\u8ba1\u8870\u843d\u4fe1\u9053\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u9891\u8c10\u632f\u8865\u507f\u7ebf\u5708\u5b9e\u73b0\u591a\u9891\u5e26\u4f20\u8f93\uff0c\u5206\u6790\u591a\u9891\u5e26\u78c1\u611f\u5e94\u901a\u4fe1\u5728\u591a\u6837\u4ecb\u8d28\u8870\u843d\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u591a\u9891\u8c10\u632f\u8865\u507f\u7684\u591a\u9891\u5e26\u4f20\u8f93\u65b9\u6848\u80fd\u6709\u6548\u51cf\u5c11\u591a\u6837\u4ecb\u8d28\u8870\u843d\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u591a\u9891\u8c10\u632f\u8865\u507f\u7684\u591a\u9891\u5e26\u4f20\u8f93\u662f\u5e94\u5bf9\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u4e2d\u591a\u6837\u4ecb\u8d28\u8870\u843d\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u6539\u5584\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u6307\u6807\u3002"}}
{"id": "2510.04042", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04042", "abs": "https://arxiv.org/abs/2510.04042", "authors": ["Dan Leonte", "Rapha\u00ebl Huser", "Almut E. D. Veraart"], "title": "Simulation-based inference via telescoping ratio estimation for trawl processes", "comment": null, "summary": "The growing availability of large and complex datasets has increased interest\nin temporal stochastic processes that can capture stylized facts such as\nmarginal skewness, non-Gaussian tails, long memory, and even non-Markovian\ndynamics. While such models are often easy to simulate from, parameter\nestimation remains challenging. Simulation-based inference (SBI) offers a\npromising way forward, but existing methods typically require large training\ndatasets or complex architectures and frequently yield confidence (credible)\nregions that fail to attain their nominal values, raising doubts on the\nreliability of estimates for the very features that motivate the use of these\nmodels. To address these challenges, we propose a fast and accurate,\nsample-efficient SBI framework for amortized posterior inference applicable to\nintractable stochastic processes. The proposed approach relies on two main\nsteps: first, we learn the posterior density by decomposing it sequentially\nacross parameter dimensions. Then, we use Chebyshev polynomial approximations\nto efficiently generate independent posterior samples, enabling accurate\ninference even when Markov chain Monte Carlo methods mix poorly. We further\ndevelop novel diagnostic tools for SBI in this context, as well as post-hoc\ncalibration techniques; the latter not only lead to performance improvements of\nthe learned inferential tool, but also to the ability to reuse it directly with\nnew time series of varying lengths, thus amortizing the training cost. We\ndemonstrate the method's effectiveness on trawl processes, a class of flexible\ninfinitely divisible models that generalize univariate Gaussian processes,\napplied to energy demand data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u51c6\u786e\u4e14\u6837\u672c\u9ad8\u6548\u7684\u6a21\u62df\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u96be\u5904\u7406\u7684\u968f\u673a\u8fc7\u7a0b\u7684\u540e\u9a8c\u63a8\u65ad\uff0c\u901a\u8fc7\u53c2\u6570\u7ef4\u5ea6\u5206\u89e3\u548c\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u8fd1\u4f3c\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u3002", "motivation": "\u5927\u578b\u590d\u6742\u6570\u636e\u96c6\u7684\u589e\u957f\u589e\u52a0\u4e86\u5bf9\u80fd\u591f\u6355\u6349\u8fb9\u9645\u504f\u5ea6\u3001\u975e\u9ad8\u65af\u5c3e\u90e8\u3001\u957f\u8bb0\u5fc6\u751a\u81f3\u975e\u9a6c\u5c14\u53ef\u592b\u52a8\u529b\u5b66\u7b49\u98ce\u683c\u5316\u4e8b\u5b9e\u7684\u65f6\u5e8f\u968f\u673a\u8fc7\u7a0b\u7684\u9700\u6c42\u3002\u867d\u7136\u8fd9\u4e9b\u6a21\u578b\u6613\u4e8e\u6a21\u62df\uff0c\u4f46\u53c2\u6570\u4f30\u8ba1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u53c2\u6570\u7ef4\u5ea6\u5206\u89e3\u987a\u5e8f\u5b66\u4e60\u540e\u9a8c\u5bc6\u5ea6\uff0c\u7136\u540e\u4f7f\u7528\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u8fd1\u4f3c\u9ad8\u6548\u751f\u6210\u72ec\u7acb\u540e\u9a8c\u6837\u672c\u3002\u8fd8\u5f00\u53d1\u4e86\u8bca\u65ad\u5de5\u5177\u548c\u540e\u9a8c\u6821\u51c6\u6280\u672f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u62d6\u7f51\u8fc7\u7a0b\uff08\u4e00\u7c7b\u7075\u6d3b\u7684\u65e0\u9650\u53ef\u5206\u6a21\u578b\uff09\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u5e94\u7528\u4e8e\u80fd\u6e90\u9700\u6c42\u6570\u636e\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u644a\u9500\u540e\u9a8c\u63a8\u65ad\uff0c\u5373\u4f7f\u5728\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u6df7\u5408\u4e0d\u826f\u65f6\u4e5f\u80fd\u8fdb\u884c\u51c6\u786e\u63a8\u65ad\uff0c\u5e76\u4e14\u80fd\u591f\u91cd\u7528\u8bad\u7ec3\u597d\u7684\u63a8\u7406\u5de5\u5177\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u7684\u65f6\u95f4\u5e8f\u5217\u3002"}}
{"id": "2510.03248", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2510.03248", "abs": "https://arxiv.org/abs/2510.03248", "authors": ["Anusha Agarwal", "Dibakar Roy Sarkar", "Somdatta Goswami"], "title": "Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models", "comment": null, "summary": "Traumatic brain injury (TBI) remains a major public health concern, with over\n69 million cases annually worldwide. Finite element (FE) models offer\nhigh-fidelity predictions of brain deformation but are computationally\nexpensive, requiring hours per simulation and limiting their clinical utility\nfor rapid decision-making. This study benchmarks state-of-the-art neural\noperator (NO) architectures for rapid, patient-specific prediction of brain\ndisplacement fields, aiming to enable real-time TBI modeling in clinical and\ntranslational settings. We formulated TBI modeling as an operator learning\nproblem, mapping subject-specific anatomical MRI, magnetic resonance\nelastography (MRE) stiffness maps, and demographic features to full-field 3D\nbrain displacement predictions. Four architectures - Fourier Neural Operator\n(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator\nNetwork (DeepONet) were trained and evaluated on 249 MRE datasets across\nphysiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest\naccuracy (MSE = 0.0023, 94.3\\% spatial fidelity) and preserved fine-scale\nfeatures, while F-FNO converged 2$\\times$ faster than standard FNO. DeepONet\noffered the fastest inference (14.5 iterations/s) with a 7$\\times$\ncomputational speed-up over MG-FNO, suggesting utility for embedded or edge\ncomputing applications. All NOs reduced computation time from hours to\nmilliseconds without sacrificing anatomical realism. NOs provide an efficient,\nresolution-invariant approach for predicting brain deformation, opening the\ndoor to real-time, patient-specific TBI risk assessment, clinical triage\nsupport, and optimization of protective equipment. These results highlight the\npotential for NO-based digital twins of the human brain, enabling scalable,\non-demand biomechanical modeling in both clinical and population health\ncontexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\u7528\u4e8e\u5feb\u901f\u9884\u6d4b\u8111\u4f4d\u79fb\u573a\uff0c\u65e8\u5728\u5b9e\u73b0\u521b\u4f24\u6027\u8111\u635f\u4f24\u7684\u5b9e\u65f6\u5efa\u6a21\u3002MG-FNO\u83b7\u5f97\u6700\u9ad8\u7cbe\u5ea6\uff0cF-FNO\u6536\u655b\u6700\u5feb\uff0cDeepONet\u63a8\u7406\u901f\u5ea6\u6700\u4f18\uff0c\u6240\u6709\u6a21\u578b\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u6beb\u79d2\u7ea7\u3002", "motivation": "\u6709\u9650\u5143\u6a21\u578b\u867d\u7136\u80fd\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u8111\u53d8\u5f62\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u5feb\u901f\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u5feb\u901f\u3001\u60a3\u8005\u7279\u5b9a\u7684\u8111\u4f4d\u79fb\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u5b9e\u65f6TBI\u5efa\u6a21\u3002", "method": "\u5c06TBI\u5efa\u6a21\u6784\u5efa\u4e3a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u56db\u79cd\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff08FNO\u3001F-FNO\u3001MG-FNO\u3001DeepONet\uff09\uff0c\u5728249\u4e2aMRE\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u8f93\u5165\u5305\u62ec\u89e3\u5256MRI\u3001MRE\u521a\u5ea6\u56fe\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u3002", "result": "MG-FNO\u8fbe\u5230\u6700\u9ad8\u7cbe\u5ea6\uff08MSE=0.0023\uff0c94.3%\u7a7a\u95f4\u4fdd\u771f\u5ea6\uff09\uff0cF-FNO\u6536\u655b\u901f\u5ea6\u6bd4\u6807\u51c6FNO\u5feb2\u500d\uff0cDeepONet\u63a8\u7406\u901f\u5ea6\u6700\u5feb\uff0814.5\u6b21\u8fed\u4ee3/\u79d2\uff09\uff0c\u6bd4MG-FNO\u5feb7\u500d\u3002\u6240\u6709\u6a21\u578b\u5c06\u8ba1\u7b97\u65f6\u95f4\u4ece\u5c0f\u65f6\u51cf\u5c11\u5230\u6beb\u79d2\u7ea7\u3002", "conclusion": "\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5206\u8fa8\u7387\u4e0d\u53d8\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u8111\u53d8\u5f62\uff0c\u4e3a\u5b9e\u65f6\u60a3\u8005\u7279\u5b9aTBI\u98ce\u9669\u8bc4\u4f30\u3001\u4e34\u5e8a\u5206\u8bca\u652f\u6301\u548c\u9632\u62a4\u8bbe\u5907\u4f18\u5316\u6253\u5f00\u4e86\u5927\u95e8\uff0c\u5c55\u793a\u4e86\u57fa\u4e8eNO\u7684\u4eba\u8111\u6570\u5b57\u5b6a\u751f\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04900", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04900", "abs": "https://arxiv.org/abs/2510.04900", "authors": ["Nick Jan\u00dfen", "Melanie Schaller", "Bodo Rosenhahn"], "title": "Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models", "comment": "Number of pages: 13 Number of figures: 16 Number of Tables: 1\n  Submitted to: IEEE Transactions on Signal Processing", "summary": "Understanding the robustness of deep learning models for multivariate\nlong-term time series forecasting (M-LTSF) remains challenging, as evaluations\ntypically rely on real-world datasets with unknown noise properties. We propose\na simulation-based evaluation framework that generates parameterizable\nsynthetic datasets, where each dataset instance corresponds to a different\nconfiguration of signal components, noise types, signal-to-noise ratios, and\nfrequency characteristics. These configurable components aim to model\nreal-world multivariate time series data without the ambiguity of unknown\nnoise. This framework enables fine-grained, systematic evaluation of M-LTSF\nmodels under controlled and diverse scenarios. We benchmark four representative\narchitectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear\n(linear), and Autoformer (decomposition-based). Our analysis reveals that all\nmodels degrade severely when lookback windows cannot capture complete periods\nof seasonal patters in the data. S-Mamba and Autoformer perform best on\nsawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.\nWhite and Brownian noise universally degrade performance with lower\nsignal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer\nshows seasonal-noise vulnerability. Further spectral analysis shows that\nS-Mamba and iTransformer achieve superior frequency reconstruction. This\ncontrolled approach, based on our synthetic and principle-driven testbed,\noffers deeper insights into model-specific strengths and limitations through\nthe aggregation of MSE scores and provides concrete guidance for model\nselection based on signal characteristics and noise conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6a21\u62df\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u53c2\u6570\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u591a\u5143\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u5b63\u8282\u6027\u6a21\u5f0f\u3001\u566a\u58f0\u7c7b\u578b\u548c\u9891\u7387\u7279\u6027\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u566a\u58f0\u7279\u6027\u672a\u77e5\uff0c\u7406\u89e3\u591a\u5143\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u63a7\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u4fe1\u53f7\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6a21\u62df\u8bc4\u4f30\u6846\u67b6\uff0c\u751f\u6210\u53ef\u914d\u7f6e\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e0d\u540c\u7684\u4fe1\u53f7\u7ec4\u4ef6\u3001\u566a\u58f0\u7c7b\u578b\u3001\u4fe1\u566a\u6bd4\u548c\u9891\u7387\u7279\u6027\uff0c\u5e76\u5bf9\u56db\u79cd\u4ee3\u8868\u6027\u67b6\u6784\uff08S-Mamba\u3001iTransformer\u3001R-Linear\u3001Autoformer\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u56de\u770b\u7a97\u53e3\u65e0\u6cd5\u6355\u6349\u5b8c\u6574\u5b63\u8282\u6027\u5468\u671f\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff1bS-Mamba\u548cAutoformer\u5728\u952f\u9f7f\u6ce2\u6a21\u5f0f\u8868\u73b0\u6700\u4f73\uff0cR-Linear\u548ciTransformer\u5728\u6b63\u5f26\u4fe1\u53f7\u8868\u73b0\u66f4\u597d\uff1b\u767d\u566a\u58f0\u548c\u5e03\u6717\u566a\u58f0\u666e\u904d\u964d\u4f4e\u6027\u80fd\uff0cS-Mamba\u5bf9\u8d8b\u52bf\u566a\u58f0\u654f\u611f\uff0ciTransformer\u5bf9\u5b63\u8282\u6027\u566a\u58f0\u654f\u611f\uff1bS-Mamba\u548ciTransformer\u5728\u9891\u7387\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u5408\u6210\u548c\u539f\u5219\u9a71\u52a8\u6d4b\u8bd5\u5e8a\u7684\u53d7\u63a7\u65b9\u6cd5\u901a\u8fc7\u805a\u5408MSE\u5206\u6570\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u7279\u5b9a\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u4e3a\u57fa\u4e8e\u4fe1\u53f7\u7279\u6027\u548c\u566a\u58f0\u6761\u4ef6\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2510.03850", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03850", "abs": "https://arxiv.org/abs/2510.03850", "authors": ["Fernando Dar\u00edo Almeida Garc\u00eda", "Francisco Raimundo Albuquerque Parente", "Michel Daoud Yacoub", "Jose C\u00e2ndido Silveira Santos Filho"], "title": "On the Exact Sum PDF and CDF of \u03b1-\u03bc Variates", "comment": null, "summary": "The sum of random variables (RVs) appears extensively in wireless\ncommunications, at large, both conventional and advanced, and has been subject\nof longstanding research. The statistical characterization of the referred sum\nis crucial to determine the performance of such communications systems.\nAlthough efforts have been undertaken to unveil these sum statistics, e.g.,\nprobability density function (PDF) and cumulative distribution function (CDF),\nno general efficient nor manageable solutions capable of evaluating the exact\nsum PDF and CDF are available to date. The only formulations are given in terms\nof either the multi-fold Brennan's integral or the multivariate Fox H-function.\nUnfortunately, these methods are only feasible up to a certain number of RVs,\nmeaning that when the number of RVs in the sum increases, the computation of\nthe sum PDF and CDF is subject to stability problems, convergence issues, or\ninaccurate results. In this paper, we derive new, simple, exact formulations\nfor the PDF and CDF of the sum of L independent and identically distributed\n{\\alpha}-{\\mu} RVs. Unlike the available solutions, the computational\ncomplexity of our analytical expressions is independent of the number of\nsummands. Capitalizing on our unprecedented findings, we analyze, in exact and\nasymptotic manners, the performance of L-branch pre-detection equal-gain\ncombining and maximal-ratio combining receivers over {\\alpha}-{\\mu} fading\nenvironments. The coding and diversity gains of the system for both receivers\nare analyzed and quantified. Moreover, numerical simulations show that the\ncomputation time reduces drastically when using our expressions, which are\narguably the most efficient and manageable formulations derived so far.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba1\u7b97\u72ec\u7acb\u540c\u5206\u5e03\u03b1-\u03bc\u968f\u673a\u53d8\u91cf\u548c\u7684\u65b0\u9896\u3001\u7b80\u5355\u4e14\u7cbe\u786e\u7684PDF\u548cCDF\u516c\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u53d8\u91cf\u6570\u91cf\u589e\u52a0\u65f6\u7684\u8ba1\u7b97\u95ee\u9898\u3002", "motivation": "\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\uff0c\u968f\u673a\u53d8\u91cf\u548c\u7684\u7edf\u8ba1\u7279\u6027\u5bf9\u7cfb\u7edf\u6027\u80fd\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u53d8\u91cf\u6570\u91cf\u589e\u52a0\u65f6\u5b58\u5728\u8ba1\u7b97\u7a33\u5b9a\u6027\u3001\u6536\u655b\u6027\u548c\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u63a8\u5bfc\u4e86\u72ec\u7acb\u540c\u5206\u5e03\u03b1-\u03bc\u968f\u673a\u53d8\u91cf\u548c\u7684\u7cbe\u786ePDF\u548cCDF\u8868\u8fbe\u5f0f\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u6c42\u548c\u9879\u6570\u91cf\u65e0\u5173\u3002", "result": "\u65b0\u516c\u5f0f\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u9ad8\u6548\u4e14\u6613\u5904\u7406\u7684\u516c\u5f0f\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eL\u5206\u652f\u9884\u68c0\u6d4b\u7b49\u589e\u76ca\u5408\u5e76\u548c\u6700\u5927\u6bd4\u5408\u5e76\u63a5\u6536\u673a\u7684\u6027\u80fd\u5206\u6790\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u53d8\u91cf\u79ef\u5206\u548cFox H\u51fd\u6570\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u03b1-\u03bc\u8870\u843d\u73af\u5883\u4e0b\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\u80fd\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.04276", "categories": ["stat.ML", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04276", "abs": "https://arxiv.org/abs/2510.04276", "authors": ["Joseph Ramsey", "Bryan Andrews"], "title": "Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests", "comment": "30 pages, 11 figures, 5 tables", "summary": "Learning graphical conditional independence structures from nonlinear,\ncontinuous or mixed data is a central challenge in machine learning and the\nsciences, and many existing methods struggle to scale to thousands of samples\nor hundreds of variables. We introduce two basis-expansion tools for scalable\ncausal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated\nadditive expansions to approximate nonlinear dependencies. BF-BIC is\ntheoretically consistent under additive models and extends to post-nonlinear\n(PNL) models via an invertible reparameterization. It remains robust under\nmoderate interactions and supports mixed data through a degenerate-Gaussian\nembedding for discrete variables. In simulations with fully nonlinear neural\ncausal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods\n(e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function\nLikelihood Ratio Test (BF-LRT) provides an approximate conditional independence\ntest that is substantially faster than kernel tests while retaining competitive\naccuracy. Extensive simulations and a real-data application to Canadian\nwildfire risk show that, when integrated into hybrid searches, BF-based methods\nenable interpretable and scalable causal discovery. Implementations are\navailable in Python, R, and Java.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u57fa\u51fd\u6570\u6269\u5c55\u7684\u53ef\u6269\u5c55\u56e0\u679c\u53d1\u73b0\u5de5\u5177\uff1aBF-BIC\u8bc4\u5206\u548cBF-LRT\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\uff0c\u7528\u4e8e\u4ece\u975e\u7ebf\u6027\u3001\u8fde\u7eed\u6216\u6df7\u5408\u6570\u636e\u4e2d\u5b66\u4e60\u56fe\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u8bb8\u591a\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u6570\u5343\u4e2a\u6837\u672c\u6216\u6570\u767e\u4e2a\u53d8\u91cf\u7684\u89c4\u6a21\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u622a\u65ad\u52a0\u6027\u5c55\u5f00\u6765\u8fd1\u4f3c\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0cBF-BIC\u5728\u52a0\u6027\u6a21\u578b\u4e0b\u5177\u6709\u7406\u8bba\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u53ef\u9006\u91cd\u53c2\u6570\u5316\u6269\u5c55\u5230\u540e\u975e\u7ebf\u6027\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9000\u5316\u9ad8\u65af\u5d4c\u5165\u652f\u6301\u6df7\u5408\u6570\u636e\u3002BF-LRT\u63d0\u4f9b\u8fd1\u4f3c\u6761\u4ef6\u72ec\u7acb\u6027\u68c0\u9a8c\u3002", "result": "\u5728\u5b8c\u5168\u975e\u7ebf\u6027\u795e\u7ecf\u56e0\u679c\u6a21\u578b\u6a21\u62df\u4e2d\uff0cBF-BIC\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u6838\u548c\u7ea6\u675f\u7684\u65b9\u6cd5\u3002BF-LRT\u6bd4\u6838\u68c0\u9a8c\u5feb\u5f97\u591a\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u6027\u3002", "conclusion": "\u57fa\u4e8e\u57fa\u51fd\u6570\u7684\u65b9\u6cd5\u5728\u6df7\u5408\u641c\u7d22\u4e2d\u80fd\u591f\u5b9e\u73b0\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u5e76\u5728\u52a0\u62ff\u5927\u91ce\u706b\u98ce\u9669\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.03250", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.03250", "abs": "https://arxiv.org/abs/2510.03250", "authors": ["Lukas R\u00fcttgers", "Till Aczel", "Andreas Plesner", "Roger Wattenhofer"], "title": "Light Differentiable Logic Gate Networks", "comment": null, "summary": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency\nat inference while sustaining competitive accuracy. But vanishing gradients,\ndiscretization errors, and high training cost impede scaling these networks.\nEven with dedicated parameter initialization schemes from subsequent works,\nincreasing depth still harms accuracy. We show that the root cause of these\nissues lies in the underlying parametrization of logic gate neurons themselves.\nTo overcome this issue, we propose a reparametrization that also shrinks the\nparameter size logarithmically in the number of inputs per gate. For binary\ninputs, this already reduces the model size by 4x, speeds up the backward pass\nby up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we\nshow that the accuracy on CIFAR-100 remains stable and sometimes superior to\nthe original parametrization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc(DLGNs)\u7684\u91cd\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u3001\u79bb\u6563\u5316\u8bef\u5dee\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c0f\u4e86\u6a21\u578b\u89c4\u6a21\u5e76\u52a0\u901f\u4e86\u8bad\u7ec3\u3002", "motivation": "\u53ef\u5fae\u5206\u903b\u8f91\u95e8\u7f51\u7edc\u5728\u63a8\u7406\u65f6\u6548\u7387\u6781\u9ad8\u4e14\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\uff0c\u4f46\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u3001\u79bb\u6563\u5316\u8bef\u5dee\u548c\u9ad8\u8bad\u7ec3\u6210\u672c\u95ee\u9898\uff0c\u5373\u4f7f\u4f7f\u7528\u4e13\u95e8\u7684\u53c2\u6570\u521d\u59cb\u5316\u65b9\u6848\uff0c\u589e\u52a0\u6df1\u5ea6\u4ecd\u4f1a\u635f\u5bb3\u51c6\u786e\u7387\u3002", "method": "\u63d0\u51fa\u5bf9\u903b\u8f91\u95e8\u795e\u7ecf\u5143\u8fdb\u884c\u91cd\u65b0\u53c2\u6570\u5316\uff0c\u5c06\u6bcf\u4e2a\u95e8\u7684\u53c2\u6570\u89c4\u6a21\u6309\u8f93\u5165\u6570\u91cf\u7684\u5bf9\u6570\u7f29\u5c0f\u3002\u5bf9\u4e8e\u4e8c\u5143\u8f93\u5165\uff0c\u8fd9\u5df2\u7ecf\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e864\u500d\u3002", "result": "\u91cd\u65b0\u53c2\u6570\u5316\u4f7f\u53cd\u5411\u4f20\u64ad\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe1.86\u500d\uff0c\u8bad\u7ec3\u6b65\u6570\u51cf\u5c118.5\u500d\uff0c\u5728CIFAR-100\u4e0a\u7684\u51c6\u786e\u7387\u4fdd\u6301\u7a33\u5b9a\u751a\u81f3\u4f18\u4e8e\u539f\u59cb\u53c2\u6570\u5316\u3002", "conclusion": "\u91cd\u65b0\u53c2\u6570\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86DLGNs\u7684\u7f29\u653e\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c0f\u4e86\u6a21\u578b\u89c4\u6a21\u5e76\u52a0\u901f\u4e86\u8bad\u7ec3\uff0c\u800c\u4e0d\u635f\u5bb3\u51c6\u786e\u7387\u3002"}}
{"id": "2510.04924", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04924", "abs": "https://arxiv.org/abs/2510.04924", "authors": ["Ardavan Rahimian"], "title": "Steady-State Spread Bounds for Graph Diffusion via Laplacian Regularisation", "comment": null, "summary": "We study how far a diffusion process on a graph can drift from a designed\nstarting pattern when that pattern is produced using Laplacian regularisation.\nUnder standard stability conditions for undirected, entrywise nonnegative\ngraphs, we give a closed-form, instance-specific upper bound on the\nsteady-state spread, measured as the relative change between the final and\ninitial profiles. The bound separates two effects: (i) an irreducible term\ndetermined by the graph's maximum node degree, and (ii) a design-controlled\nterm that shrinks as the regularisation strength increases (following an\ninverse square-root law). This leads to a simple design rule: given any target\nlimit on spread, one can choose a sufficient regularisation strength in closed\nform. Although one motivating application is array beamforming, where the\ninitial pattern is the squared magnitude of the beamformer weights, the result\napplies to any scenario that first enforces Laplacian smoothness and then\nevolves by linear diffusion on a graph. Overall, the guarantee is\nnon-asymptotic, easy to compute, and certifies how much steady-state deviation\ncan occur.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u6269\u6563\u8fc7\u7a0b\u5728\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u8bbe\u8ba1\u7684\u521d\u59cb\u6a21\u5f0f\u4e0b\u80fd\u504f\u79bb\u591a\u8fdc\uff0c\u7ed9\u51fa\u4e86\u7a33\u6001\u4f20\u64ad\u7684\u95ed\u5f0f\u4e0a\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u8bbe\u8ba1\u89c4\u5219\u6765\u63a7\u5236\u4f20\u64ad\u8303\u56f4\u3002", "motivation": "\u7814\u7a76\u56fe\u6269\u6563\u8fc7\u7a0b\u5728\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u8bbe\u8ba1\u7684\u521d\u59cb\u6a21\u5f0f\u4e0b\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u9635\u5217\u6ce2\u675f\u6210\u5f62\u7b49\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u786e\u4fdd\u7a33\u6001\u504f\u5dee\u5728\u53ef\u63a7\u8303\u56f4\u5185\u3002", "method": "\u5728\u65e0\u5411\u3001\u975e\u8d1f\u56fe\u7684\u6807\u51c6\u7a33\u5b9a\u6027\u6761\u4ef6\u4e0b\uff0c\u63a8\u5bfc\u51fa\u7a33\u6001\u4f20\u64ad\u7684\u95ed\u5f0f\u4e0a\u754c\uff0c\u8be5\u4e0a\u754c\u5206\u79bb\u4e86\u7531\u56fe\u6700\u5927\u8282\u70b9\u5ea6\u51b3\u5b9a\u7684\u4e0d\u53ef\u7ea6\u9879\u548c\u7531\u6b63\u5219\u5316\u5f3a\u5ea6\u63a7\u5236\u7684\u8bbe\u8ba1\u9879\u3002", "result": "\u5f97\u5230\u4e86\u4e00\u4e2a\u975e\u6e10\u8fd1\u3001\u6613\u4e8e\u8ba1\u7b97\u7684\u4fdd\u8bc1\uff0c\u53ef\u4ee5\u8bc1\u660e\u7a33\u6001\u504f\u5dee\u7684\u7a0b\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u76ee\u6807\u4f20\u64ad\u9650\u5236\u9009\u62e9\u8db3\u591f\u6b63\u5219\u5316\u5f3a\u5ea6\u7684\u7b80\u5355\u8bbe\u8ba1\u89c4\u5219\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u62c9\u666e\u62c9\u65af\u5e73\u6ed1\u540e\u5728\u56fe\u4e0a\u6f14\u5316\u7ebf\u6027\u6269\u6563\u7684\u4efb\u4f55\u573a\u666f\u63d0\u4f9b\u4e86\u7a33\u6001\u504f\u5dee\u7684\u91cf\u5316\u4fdd\u8bc1\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.03852", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03852", "abs": "https://arxiv.org/abs/2510.03852", "authors": ["Jianyu Wang", "Tianrui Hou", "Wenchi Cheng", "Hailin Zhang"], "title": "Robust Beamforming for Magnetic Induction Based Underground Emergency Communications", "comment": null, "summary": "Magnetic induction (MI) communication is an effective underground emergency\ncommunication technique after disasters such as landslides, mine collapses, and\nearthquakes, due to its advantages in mediums such as soil, concrete, and\nmetals. Based on channel state information (CSI), magnetic beamforming can\nsignificantly improve the performance of MI communication. However, in\npost-disaster underground communication, channel estimation may suffer from\nerrors due to factors such as complex environmental interferences. Taking\nchannel estimation error into account, we formulate a beamforming optimization\nproblem for multi-user MI underground emergency communications, which aims to\nminimize the power consumption under the constraints of sum rate and signal to\ninterference plus noise ratio (SINR) of each user. Based on the worst-case\noptimization criterion and the S-procedure, the non-convex optimization problem\nis transformed into convex and solved. Numerical results show that the proposed\nrobust beamforming scheme can effectively enhance communication reliability and\neffective throughput in the presence of channel estimation errors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u7684\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u65b9\u6848\uff0c\u7528\u4e8e\u591a\u7528\u6237\u78c1\u611f\u5e94\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\uff0c\u4ee5\u6700\u5c0f\u5316\u529f\u8017\u5e76\u4fdd\u8bc1\u901a\u4fe1\u53ef\u9760\u6027\u3002", "motivation": "\u78c1\u611f\u5e94\u901a\u4fe1\u5728\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u707e\u540e\u590d\u6742\u73af\u5883\u4f1a\u5bfc\u81f4\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\uff0c\u5f71\u54cd\u6ce2\u675f\u6210\u5f62\u6027\u80fd\uff0c\u9700\u8981\u8bbe\u8ba1\u9c81\u68d2\u65b9\u6848\u6765\u5e94\u5bf9\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u57fa\u4e8e\u6700\u574f\u60c5\u51b5\u4f18\u5316\u51c6\u5219\u548cS-\u8fc7\u7a0b\uff0c\u5c06\u8003\u8651\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u7684\u975e\u51f8\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u95ee\u9898\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u65b9\u6848\u5728\u5b58\u5728\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u65f6\u80fd\u6709\u6548\u63d0\u9ad8\u901a\u4fe1\u53ef\u9760\u6027\u548c\u6709\u6548\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u65b9\u6848\u80fd\u591f\u5728\u5730\u4e0b\u5e94\u6025\u901a\u4fe1\u4e2d\u6709\u6548\u5e94\u5bf9\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.04277", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04277", "abs": "https://arxiv.org/abs/2510.04277", "authors": ["Hamish Flynn"], "title": "Relative Information Gain and Gaussian Process Regression", "comment": "28 pages", "summary": "The sample complexity of estimating or maximising an unknown function in a\nreproducing kernel Hilbert space is known to be linked to both the effective\ndimension and the information gain associated with the kernel. While the\ninformation gain has an attractive information-theoretic interpretation, the\neffective dimension typically results in better rates. We introduce a new\nquantity called the relative information gain, which measures the sensitivity\nof the information gain with respect to the observation noise. We show that the\nrelative information gain smoothly interpolates between the effective dimension\nand the information gain, and that the relative information gain has the same\ngrowth rate as the effective dimension. In the second half of the paper, we\nprove a new PAC-Bayesian excess risk bound for Gaussian process regression. The\nrelative information gain arises naturally from the complexity term in this\nPAC-Bayesian bound. We prove bounds on the relative information gain that\ndepend on the spectral properties of the kernel. When these upper bounds are\ncombined with our excess risk bound, we obtain minimax-optimal rates of\nconvergence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u7684\u6982\u5ff5\uff0c\u5b83\u5e73\u6ed1\u5730\u63d2\u503c\u4e8e\u6709\u6548\u7ef4\u5ea6\u548c\u4fe1\u606f\u589e\u76ca\u4e4b\u95f4\uff0c\u5e76\u5177\u6709\u4e0e\u6709\u6548\u7ef4\u5ea6\u76f8\u540c\u7684\u589e\u957f\u7387\u3002\u57fa\u4e8e\u6b64\uff0c\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5efa\u7acb\u4e86\u65b0\u7684PAC-Bayesian\u8d85\u989d\u98ce\u9669\u754c\uff0c\u7ed3\u5408\u6838\u7684\u5149\u8c31\u7279\u6027\u83b7\u5f97\u4e86\u6781\u5c0f\u6781\u5927\u6700\u4f18\u6536\u655b\u7387\u3002", "motivation": "\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4f30\u8ba1\u6216\u6700\u5927\u5316\u672a\u77e5\u51fd\u6570\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u6838\u7684\u6709\u6548\u7ef4\u5ea6\u548c\u4fe1\u606f\u589e\u76ca\u76f8\u5173\u3002\u4fe1\u606f\u589e\u76ca\u5177\u6709\u4fe1\u606f\u8bba\u89e3\u91ca\uff0c\u4f46\u6709\u6548\u7ef4\u5ea6\u901a\u5e38\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6536\u655b\u7387\u3002\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u91cf\u6765\u8fde\u63a5\u8fd9\u4e24\u4e2a\u6982\u5ff5\u3002", "method": "\u5f15\u5165\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u7684\u6982\u5ff5\uff0c\u6d4b\u91cf\u4fe1\u606f\u589e\u76ca\u5bf9\u89c2\u6d4b\u566a\u58f0\u7684\u654f\u611f\u6027\u3002\u8bc1\u660e\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u5e73\u6ed1\u63d2\u503c\u4e8e\u6709\u6548\u7ef4\u5ea6\u548c\u4fe1\u606f\u589e\u76ca\u4e4b\u95f4\u3002\u5efa\u7acb\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7684PAC-Bayesian\u8d85\u989d\u98ce\u9669\u754c\uff0c\u5176\u4e2d\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u81ea\u7136\u51fa\u73b0\u5728\u590d\u6742\u5ea6\u9879\u4e2d\u3002", "result": "\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u5177\u6709\u4e0e\u6709\u6548\u7ef4\u5ea6\u76f8\u540c\u7684\u589e\u957f\u7387\u3002\u7ed3\u5408\u6838\u7684\u5149\u8c31\u7279\u6027\uff0c\u83b7\u5f97\u4e86\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u7684\u4e0a\u754c\u3002\u5f53\u8fd9\u4e9b\u4e0a\u754c\u4e0e\u8d85\u989d\u98ce\u9669\u754c\u7ed3\u5408\u65f6\uff0c\u5f97\u5230\u4e86\u6781\u5c0f\u6781\u5927\u6700\u4f18\u7684\u6536\u655b\u7387\u3002", "conclusion": "\u76f8\u5bf9\u4fe1\u606f\u589e\u76ca\u662f\u8fde\u63a5\u6709\u6548\u7ef4\u5ea6\u548c\u4fe1\u606f\u589e\u76ca\u7684\u6709\u7528\u6982\u5ff5\uff0c\u901a\u8fc7PAC-Bayesian\u6846\u67b6\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u4f18\u6536\u655b\u7387\u3002"}}
{"id": "2510.03251", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03251", "abs": "https://arxiv.org/abs/2510.03251", "authors": ["Hanzhong Cao", "Wenbo Yan", "Ying Tan"], "title": "Numerion: A Multi-Hypercomplex Model for Time Series Forecasting", "comment": null, "summary": "Many methods aim to enhance time series forecasting by decomposing the series\nthrough intricate model structures and prior knowledge, yet they are inevitably\nlimited by computational complexity and the robustness of the assumptions. Our\nresearch uncovers that in the complex domain and higher-order hypercomplex\nspaces, the characteristic frequencies of time series naturally decrease.\nLeveraging this insight, we propose Numerion, a time series forecasting model\nbased on multiple hypercomplex spaces. Specifically, grounded in theoretical\nsupport, we generalize linear layers and activation functions to hypercomplex\nspaces of arbitrary power-of-two dimensions and introduce a novel\nReal-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.\nNumerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces\nof varying dimensions, naturally decomposing and independently modeling the\nseries, and adaptively fuses the latent patterns exhibited in different spaces\nthrough a dynamic fusion mechanism. Experiments validate the model`s\nperformance, achieving state-of-the-art results on multiple public datasets.\nVisualizations and quantitative analyses comprehensively demonstrate the\nability of multi-dimensional RHR-MLPs to naturally decompose time series and\nreveal the tendency of higher dimensional hypercomplex spaces to capture lower\nfrequency features.", "AI": {"tldr": "\u63d0\u51faNumerion\u6a21\u578b\uff0c\u5229\u7528\u8d85\u590d\u6570\u7a7a\u95f4\u81ea\u7136\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6RHR-MLP\u67b6\u6784\u5728\u4e0d\u540c\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u6574\u5408\u6a21\u5f0f\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u901a\u8fc7\u590d\u6742\u6a21\u578b\u7ed3\u6784\u548c\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u5206\u89e3\uff0c\u4f46\u53d7\u9650\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5047\u8bbe\u7684\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u73b0\u8d85\u590d\u6570\u7a7a\u95f4\u4e2d\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u5f81\u9891\u7387\u4f1a\u81ea\u7136\u964d\u4f4e\u3002", "method": "\u63d0\u51faNumerion\u6a21\u578b\uff0c\u5c06\u7ebf\u6027\u5c42\u548c\u6fc0\u6d3b\u51fd\u6570\u63a8\u5e7f\u5230\u4efb\u610f2\u7684\u5e42\u6b21\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\uff0c\u5f15\u5165RHR-MLP\u67b6\u6784\uff0c\u4f7f\u7528\u591a\u4e2aRHR-MLP\u5c06\u65f6\u95f4\u5e8f\u5217\u6620\u5c04\u5230\u4e0d\u540c\u7ef4\u5ea6\u7684\u8d85\u590d\u6570\u7a7a\u95f4\u8fdb\u884c\u81ea\u7136\u5206\u89e3\u548c\u72ec\u7acb\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u81ea\u9002\u5e94\u878d\u5408\u4e0d\u540c\u7a7a\u95f4\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\u3002\u53ef\u89c6\u5316\u548c\u5b9a\u91cf\u5206\u6790\u5168\u9762\u8bc1\u660e\u4e86\u591a\u7ef4RHR-MLP\u81ea\u7136\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u9ad8\u7ef4\u8d85\u590d\u6570\u7a7a\u95f4\u503e\u5411\u4e8e\u6355\u83b7\u4f4e\u9891\u7279\u5f81\u7684\u8d8b\u52bf\u3002", "conclusion": "Numerion\u6a21\u578b\u901a\u8fc7\u8d85\u590d\u6570\u7a7a\u95f4\u5b9e\u73b0\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u81ea\u7136\u5206\u89e3\u548c\u5efa\u6a21\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u9ad8\u7ef4\u8d85\u590d\u6570\u7a7a\u95f4\u5728\u6355\u83b7\u4f4e\u9891\u7279\u5f81\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.03901", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03901", "abs": "https://arxiv.org/abs/2510.03901", "authors": ["Vincent Savaux", "Steve Sawadogo", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu"], "title": "On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications", "comment": "9 pages, 5 figures, conference", "summary": "This paper investigates the robustness of affine frequency division\nmultiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation\nschemes against non-white Gaussian noise, which can model various sources of\nadditive disturbances to the received signal. The proposed approach\ndemonstrates that the performance of these waveforms depends on the ability of\nthe demodulation matrix to whiten the noise-a property that is, in turn,\nrelated to the sparsity of the matrix. AFDM is shown to outperform OTFS and\northogonal frequency division multiplexing (OFDM), as its demodulation matrix\nis generally less sparse than those of the other waveforms. Based on this\nanalysis, several application examples and use cases are presented, such as the\nuse of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals.\nFinally, simulation results confirm that AFDM achieves better performance than\nOTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in\nmost application scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86AFDM\u548cOTFS\u8c03\u5236\u65b9\u6848\u5728\u975e\u767d\u9ad8\u65af\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0AFDM\u7531\u4e8e\u89e3\u8c03\u77e9\u9635\u7a00\u758f\u5ea6\u8f83\u4f4e\u800c\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5728\u5927\u591a\u6570\u5e94\u7528\u573a\u666f\u4e2d\u6bd4OTFS\u548cOFDM\u6709\u8d85\u8fc71dB\u7684\u6027\u80fd\u589e\u76ca\u3002", "motivation": "\u7814\u7a76AFDM\u548cOTFS\u8c03\u5236\u65b9\u6848\u5bf9\u5404\u79cd\u52a0\u6027\u5e72\u6270\u6e90\u5efa\u6a21\u7684\u975e\u767d\u9ad8\u65af\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u5206\u6790\u4e0d\u540c\u6ce2\u5f62\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790\u89e3\u8c03\u77e9\u9635\u7684\u767d\u5316\u80fd\u529b\u548c\u7a00\u758f\u5ea6\u7279\u6027\uff0c\u6bd4\u8f83AFDM\u3001OTFS\u548cOFDM\u7684\u6027\u80fd\uff0c\u5e76\u8fdb\u884c\u4e86\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "AFDM\u5728\u975e\u767d\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8eOTFS\u548cOFDM\uff0c\u5176\u89e3\u8c03\u77e9\u9635\u901a\u5e38\u6bd4\u5176\u4ed6\u6ce2\u5f62\u7684\u77e9\u9635\u7a00\u758f\u5ea6\u66f4\u4f4e\uff0c\u5728\u5927\u591a\u6570\u5e94\u7528\u573a\u666f\u4e2d\u6027\u80fd\u589e\u76ca\u8d85\u8fc71dB\u3002", "conclusion": "AFDM\u8c03\u5236\u65b9\u6848\u5728\u975e\u767d\u9ad8\u65af\u566a\u58f0\u73af\u5883\u4e0b\u5177\u6709\u6700\u4f73\u6027\u80fd\uff0c\u9002\u5408\u5728\u7a84\u5e26\u4fe1\u53f7\u6216\u4e0eOFDM\u4fe1\u53f7\u5171\u5b58\u7b49\u5e94\u7528\u573a\u666f\u4e2d\u4f7f\u7528\u3002"}}
{"id": "2510.04318", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04318", "abs": "https://arxiv.org/abs/2510.04318", "authors": ["Etienne Gauthier", "Francis Bach", "Michael I. Jordan"], "title": "Adaptive Coverage Policies in Conformal Prediction", "comment": "Code at: https://github.com/GauthierE/adaptive-coverage-policies", "summary": "Traditional conformal prediction methods construct prediction sets such that\nthe true label falls within the set with a user-specified coverage level.\nHowever, poorly chosen coverage levels can result in uninformative predictions,\neither producing overly conservative sets when the coverage level is too high,\nor empty sets when it is too low. Moreover, the fixed coverage level cannot\nadapt to the specific characteristics of each individual example, limiting the\nflexibility and efficiency of these methods. In this work, we leverage recent\nadvances in e-values and post-hoc conformal inference, which allow the use of\ndata-dependent coverage levels while maintaining valid statistical guarantees.\nWe propose to optimize an adaptive coverage policy by training a neural network\nusing a leave-one-out procedure on the calibration set, allowing the coverage\nlevel and the resulting prediction set size to vary with the difficulty of each\nindividual example. We support our approach with theoretical coverage\nguarantees and demonstrate its practical benefits through a series of\nexperiments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u8986\u76d6\u7b56\u7565\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u6570\u636e\u4f9d\u8d56\u7684\u8986\u76d6\u6c34\u5e73\uff0c\u4f7f\u9884\u6d4b\u96c6\u5927\u5c0f\u80fd\u6839\u636e\u6bcf\u4e2a\u6837\u672c\u7684\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u3002", "motivation": "\u4f20\u7edf\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u7684\u56fa\u5b9a\u8986\u76d6\u6c34\u5e73\u4f1a\u5bfc\u81f4\u9884\u6d4b\u96c6\u4e0d\u5177\u4fe1\u606f\u6027\uff1a\u8986\u76d6\u6c34\u5e73\u8fc7\u9ad8\u4ea7\u751f\u8fc7\u4e8e\u4fdd\u5b88\u7684\u96c6\u5408\uff0c\u8fc7\u4f4e\u5219\u4ea7\u751f\u7a7a\u96c6\uff0c\u4e14\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u6837\u672c\u7684\u7279\u6027\u3002", "method": "\u5229\u7528e\u503c\u548c\u4e8b\u540e\u5171\u5f62\u63a8\u65ad\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5728\u9a8c\u8bc1\u96c6\u4e0a\u901a\u8fc7\u7559\u4e00\u6cd5\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6765\u4f18\u5316\u81ea\u9002\u5e94\u8986\u76d6\u7b56\u7565\uff0c\u4f7f\u8986\u76d6\u6c34\u5e73\u548c\u9884\u6d4b\u96c6\u5927\u5c0f\u968f\u6837\u672c\u96be\u5ea6\u53d8\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5177\u6709\u7406\u8bba\u8986\u76d6\u4fdd\u8bc1\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u8986\u76d6\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u5171\u5f62\u9884\u6d4b\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002"}}
{"id": "2510.03252", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03252", "abs": "https://arxiv.org/abs/2510.03252", "authors": ["Duc Kieu", "Kien Do", "Tuan Hoang", "Thao Minh Le", "Tung Kieu", "Dang Nguyen", "Thin Nguyen"], "title": "Universal Multi-Domain Translation via Diffusion Routers", "comment": null, "summary": "Multi-domain translation (MDT) aims to learn translations between multiple\ndomains, yet existing approaches either require fully aligned tuples or can\nonly handle domain pairs seen in training, limiting their practicality and\nexcluding many cross-domain mappings. We introduce universal MDT (UMDT), a\ngeneralization of MDT that seeks to translate between any pair of $K$ domains\nusing only $K-1$ paired datasets with a central domain. To tackle this problem,\nwe propose Diffusion Router (DR), a unified diffusion-based framework that\nmodels all central$\\leftrightarrow$non-central translations with a single noise\npredictor conditioned on the source and target domain labels. DR enables\nindirect non-central translations by routing through the central domain. We\nfurther introduce a novel scalable learning strategy with a variational-bound\nobjective and an efficient Tweedie refinement procedure to support direct\nnon-central mappings. Through evaluation on three large-scale UMDT benchmarks,\nDR achieves state-of-the-art results for both indirect and direct translations,\nwhile lowering sampling cost and unlocking novel tasks such as\nsketch$\\leftrightarrow$segmentation. These results establish DR as a scalable\nand versatile framework for universal translation across multiple domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u901a\u7528\u591a\u57df\u7ffb\u8bd1(UMDT)\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u8def\u7531\u5668(DR)\u5b9e\u73b0\u4efb\u610f\u57df\u5bf9\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u4ec5\u9700K-1\u4e2a\u4e2d\u5fc3\u57df\u914d\u5bf9\u6570\u636e\u96c6\u5373\u53ef\u652f\u6301K\u4e2a\u57df\u7684\u6240\u6709\u7ffb\u8bd1\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u591a\u57df\u7ffb\u8bd1\u65b9\u6cd5\u9700\u8981\u5b8c\u5168\u5bf9\u9f50\u7684\u5143\u7ec4\u6216\u53ea\u80fd\u5904\u7406\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u7684\u57df\u5bf9\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u5e76\u6392\u9664\u4e86\u8bb8\u591a\u8de8\u57df\u6620\u5c04\u3002", "method": "\u63d0\u51fa\u6269\u6563\u8def\u7531\u5668(DR)\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00\u566a\u58f0\u9884\u6d4b\u5668\u5efa\u6a21\u6240\u6709\u4e2d\u5fc3\u57df\u4e0e\u975e\u4e2d\u5fc3\u57df\u4e4b\u95f4\u7684\u7ffb\u8bd1\uff0c\u901a\u8fc7\u4e2d\u5fc3\u57df\u8def\u7531\u5b9e\u73b0\u95f4\u63a5\u975e\u4e2d\u5fc3\u7ffb\u8bd1\uff0c\u5e76\u5f15\u5165\u53d8\u5206\u8fb9\u754c\u76ee\u6807\u548cTweedie\u7cbe\u70bc\u652f\u6301\u76f4\u63a5\u6620\u5c04\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21UMDT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u540c\u65f6\u964d\u4f4e\u91c7\u6837\u6210\u672c\uff0c\u89e3\u9501\u4e86\u8349\u56fe\u2194\u5206\u5272\u7b49\u65b0\u4efb\u52a1\u3002", "conclusion": "DR\u88ab\u8bc1\u660e\u662f\u8de8\u591a\u4e2a\u57df\u8fdb\u884c\u901a\u7528\u7ffb\u8bd1\u7684\u53ef\u6269\u5c55\u4e14\u591a\u529f\u80fd\u7684\u6846\u67b6\u3002"}}
{"id": "2510.04037", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04037", "abs": "https://arxiv.org/abs/2510.04037", "authors": ["Mohammad Salman", "Hadi Zayyani", "Hasan Abu Hilal", "Mostafa Rashdan"], "title": "Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate", "comment": null, "summary": "This letter presents a novel method for estimating the position, velocity,\nand acceleration of a moving target using range-based measurements. Although\nmost existing studies focus on position and velocity estimation, the framework\nof this letter is extended to include acceleration. To achieve this, we propose\nusing the derivative of the range rate, in addition to the range and range rate\nmeasurements. The proposed method estimates the position at first using\nTime-of-Arrival (TOA)-based techniques; then, develops a reformulated least\nsquares (LS) and weighted least squares (WLS) approaches for velocity\nestimation; and finally, employs the derivative of the range rate to estimate\nthe acceleration using previous position and velocity estimates. On the other\nhand, closed-form LS and WLS solutions are derived for both velocity and\nacceleration. The simulation results show that the proposed approach provides\nimproved performance in estimating moving target kinematics compared to\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u6d4b\u91cf\u7684\u79fb\u52a8\u76ee\u6807\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u8ddd\u79bb\u53d8\u5316\u7387\u7684\u5bfc\u6570\u6765\u6269\u5c55\u4f20\u7edf\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u4e86\u95ed\u5f0f\u6700\u5c0f\u4e8c\u4e58\u89e3\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u53ea\u5173\u6ce8\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\uff0c\u7f3a\u4e4f\u5bf9\u52a0\u901f\u5ea6\u7684\u4f30\u8ba1\u80fd\u529b\uff0c\u9700\u8981\u6269\u5c55\u6846\u67b6\u6765\u66f4\u5168\u9762\u5730\u63cf\u8ff0\u79fb\u52a8\u76ee\u6807\u8fd0\u52a8\u5b66\u3002", "method": "\u9996\u5148\u4f7f\u7528TOA\u6280\u672f\u4f30\u8ba1\u4f4d\u7f6e\uff0c\u7136\u540e\u5f00\u53d1\u91cd\u6784\u7684\u6700\u5c0f\u4e8c\u4e58\u548c\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u65b9\u6cd5\u8fdb\u884c\u901f\u5ea6\u4f30\u8ba1\uff0c\u6700\u540e\u5229\u7528\u8ddd\u79bb\u53d8\u5316\u7387\u7684\u5bfc\u6570\u7ed3\u5408\u5148\u524d\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u6765\u4f30\u8ba1\u52a0\u901f\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f30\u8ba1\u79fb\u52a8\u76ee\u6807\u8fd0\u52a8\u5b66\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u79fb\u52a8\u76ee\u6807\u7684\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\uff0c\u4e3a\u8fd0\u52a8\u76ee\u6807\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04406", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04406", "abs": "https://arxiv.org/abs/2510.04406", "authors": ["William Zhang", "Saurabh Amin", "Georgia Perakis"], "title": "Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition", "comment": "11 pages, (37 with appendix), 15 figures", "summary": "Conformal prediction offers finite-sample coverage guarantees under minimal\nassumptions. However, existing methods treat the entire modeling process as a\nblack box, overlooking opportunities to exploit modular structure. We introduce\na conformal prediction framework for two-stage sequential models, where an\nupstream predictor generates intermediate representations for a downstream\nmodel. By decomposing the overall prediction residual into stage-specific\ncomponents, our method enables practitioners to attribute uncertainty to\nspecific pipeline stages. We develop a risk-controlled parameter selection\nprocedure using family-wise error rate (FWER) control to calibrate stage-wise\nscaling parameters, and propose an adaptive extension for non-stationary\nsettings that preserves long-run coverage guarantees. Experiments on synthetic\ndistribution shifts, as well as real-world supply chain and stock market data,\ndemonstrate that our approach maintains coverage under conditions that degrade\nstandard conformal methods, while providing interpretable stage-wise\nuncertainty attribution. This framework offers diagnostic advantages and robust\ncoverage that standard conformal methods lack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4e24\u9636\u6bb5\u987a\u5e8f\u6a21\u578b\u7684\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9884\u6d4b\u6b8b\u5dee\u4e3a\u9636\u6bb5\u7279\u5b9a\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u5230\u7279\u5b9a\u7ba1\u9053\u9636\u6bb5\u7684\u5f52\u56e0\uff0c\u5e76\u5f00\u53d1\u4e86\u98ce\u9669\u63a7\u5236\u53c2\u6570\u9009\u62e9\u7a0b\u5e8f\u3002", "motivation": "\u73b0\u6709\u4fdd\u5f62\u9884\u6d4b\u65b9\u6cd5\u5c06\u6574\u4e2a\u5efa\u6a21\u8fc7\u7a0b\u89c6\u4e3a\u9ed1\u76d2\uff0c\u5ffd\u7565\u4e86\u5229\u7528\u6a21\u5757\u5316\u7ed3\u6784\u7684\u673a\u4f1a\uff0c\u65e0\u6cd5\u5c06\u4e0d\u786e\u5b9a\u6027\u5f52\u56e0\u5230\u7279\u5b9a\u7ba1\u9053\u9636\u6bb5\u3002", "method": "\u5f15\u5165\u4e24\u9636\u6bb5\u987a\u5e8f\u6a21\u578b\u7684\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u5c06\u603b\u4f53\u9884\u6d4b\u6b8b\u5dee\u5206\u89e3\u4e3a\u9636\u6bb5\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4f7f\u7528\u65cf\u9519\u8bef\u7387\u63a7\u5236\u6765\u6821\u51c6\u9636\u6bb5\u7ea7\u7f29\u653e\u53c2\u6570\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u6269\u5c55\u7528\u4e8e\u975e\u5e73\u7a33\u8bbe\u7f6e\u3002", "result": "\u5728\u5408\u6210\u5206\u5e03\u504f\u79fb\u4ee5\u53ca\u771f\u5b9e\u4f9b\u5e94\u94fe\u548c\u80a1\u7968\u5e02\u573a\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u4fdd\u5f62\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u7684\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u8986\u76d6\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9636\u6bb5\u7ea7\u4e0d\u786e\u5b9a\u6027\u5f52\u56e0\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u6807\u51c6\u4fdd\u5f62\u65b9\u6cd5\u6240\u7f3a\u4e4f\u7684\u8bca\u65ad\u4f18\u52bf\u548c\u7a33\u5065\u8986\u76d6\u4fdd\u8bc1\u3002"}}
{"id": "2510.03253", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences.", "AI": {"tldr": "HPL\u662f\u4e00\u4e2a\u5206\u5c42\u504f\u597d\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\u4f18\u5316LLM\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u8f68\u8ff9\u7ea7\u504f\u597d\u5b66\u4e60\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\u800c\u6b65\u7ea7\u504f\u597d\u5b66\u4e60\u8fc7\u4e8e\u77ed\u89c6\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u504f\u597d\u5b66\u4e60\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\u2014\u2014\u8f68\u8ff9\u7ea7DPO\u4fe1\u53f7\u8fc7\u4e8e\u7c97\u7cd9\uff0c\u6b65\u7ea7DPO\u8fc7\u4e8e\u77ed\u89c6\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u591a\u6b65\u884c\u4e3a\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u504f\u597d\u5b66\u4e60(HPL)\u6846\u67b6\uff0c\u5305\u542b\u8f68\u8ff9\u7ea7\u3001\u6b65\u7ea7\u548c\u7ec4\u7ea7DPO\uff0c\u6838\u5fc3\u521b\u65b0\u662f\u53cc\u5c42\u7ea7\u8bfe\u7a0b\uff1a\u5c06\u4e13\u5bb6\u8f68\u8ff9\u5206\u89e3\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u52a8\u4f5c\u7ec4\uff0c\u751f\u6210\u5bf9\u6bd4\u6b21\u4f18\u7ec4\u8fdb\u884c\u7ec6\u7c92\u5ea6\u504f\u597d\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u8bfe\u7a0b\u8c03\u5ea6\u5668\u6309\u7ec4\u957f\u5ea6\u548c\u6837\u672c\u96be\u5ea6\u7ec4\u7ec7\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHPL\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5206\u6790\u8868\u660e\u5206\u5c42DPO\u635f\u5931\u6709\u6548\u6574\u5408\u4e86\u591a\u7c92\u5ea6\u504f\u597d\u4fe1\u53f7\uff0c\u53cc\u5c42\u7ea7\u8bfe\u7a0b\u5bf9\u89e3\u51b3\u4ece\u7b80\u5355\u884c\u4e3a\u5230\u590d\u6742\u591a\u6b65\u5e8f\u5217\u7684\u5e7f\u6cdb\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "HPL\u901a\u8fc7\u5206\u5c42\u504f\u597d\u5b66\u4e60\u548c\u53cc\u5c42\u7ea7\u8bfe\u7a0b\u8c03\u5ea6\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u504f\u597d\u5b66\u4e60\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04160", "abs": "https://arxiv.org/abs/2510.04160", "authors": ["Mohammad Kazzazi", "Mohammad Morsali", "Rouhollah Amiri"], "title": "CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization", "comment": "Mohammad Kazzazi and Mohammad Morsali contributed equally to this\n  work", "summary": "This paper presents CLEAR -- a closed-form localization estimator with a\nreduced sensor network. The proposed method is a computationally efficient,\ntwo-stage estimator that fuses time-difference-of-arrival (TDOA) and\nfrequency-difference-of-arrival (FDOA) measurements with a minimal number of\nsensors. CLEAR localizes a moving source in N-dimensional space using only N+1\nsensors, achieving the theoretical minimum sensor count. The first stage\nintroduces auxiliary range and range-rate parameters to construct a set of\npseudo-linear equations, solved via weighted least squares. An algebraic\nelimination using Sylvester's resultant then reduces the problem to a quartic\nequation, yielding closed-form estimates for the nuisance variables. A second,\nlightweight linear refinement stage is applied to mitigate residual bias. Under\nmild Gaussian noise assumptions, the estimator's position and velocity\nestimates are statistically efficient, closely approaching the Cramer-Rao lower\nbound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios\ndemonstrate CRLB-level accuracy and consistent performance gains over\nrepresentative two-stage and iterative baselines, confirming the method's high\nsuitability for power-constrained, distributed Internet of Things (IoT)\napplications such as UAV tracking and smart transportation.", "AI": {"tldr": "CLEAR\u662f\u4e00\u79cd\u4f7f\u7528\u6700\u5c11\u4f20\u611f\u5668\uff08N+1\u4e2a\uff09\u7684\u95ed\u5f0f\u5b9a\u4f4d\u4f30\u8ba1\u5668\uff0c\u878d\u5408TDOA\u548cFDOA\u6d4b\u91cf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u79fb\u52a8\u6e90\u5b9a\u4f4d\uff0c\u5728\u566a\u58f0\u5047\u8bbe\u4e0b\u8fbe\u5230CRLB\u7ea7\u522b\u7684\u7edf\u8ba1\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u529f\u7387\u53d7\u9650\u7684\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u5e94\u7528\uff08\u5982\u65e0\u4eba\u673a\u8ddf\u8e2a\u548c\u667a\u80fd\u4ea4\u901a\uff09\uff0c\u9700\u8981\u5f00\u53d1\u8ba1\u7b97\u9ad8\u6548\u4e14\u4f7f\u7528\u6700\u5c11\u4f20\u611f\u5668\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u5ea6\u548c\u529f\u8017\u3002", "method": "\u4e24\u9636\u6bb5\u4f30\u8ba1\u5668\uff1a\u7b2c\u4e00\u9636\u6bb5\u5f15\u5165\u8f85\u52a9\u53c2\u6570\u6784\u5efa\u4f2a\u7ebf\u6027\u65b9\u7a0b\uff0c\u901a\u8fc7\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\u6c42\u89e3\uff0c\u4f7f\u7528Sylvester\u7ed3\u5f0f\u4ee3\u6570\u6d88\u5143\u7b80\u5316\u4e3a\u56db\u6b21\u65b9\u7a0b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u8f7b\u91cf\u7ea7\u7ebf\u6027\u7ec6\u5316\u4ee5\u51cf\u5c11\u6b8b\u5dee\u504f\u5dee\u3002", "result": "\u57282D\u548c3D\u573a\u666f\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230CRLB\u7ea7\u522b\u7684\u7cbe\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u4ee3\u8868\u6027\u4e24\u9636\u6bb5\u548c\u8fed\u4ee3\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f30\u8ba1\u5177\u6709\u7edf\u8ba1\u6548\u7387\u3002", "conclusion": "CLEAR\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7406\u8bba\u6700\u5c0f\u4f20\u611f\u5668\u6570\u91cf\u8981\u6c42\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u5408\u529f\u7387\u53d7\u9650\u7684\u7269\u8054\u7f51\u5e94\u7528\uff0c\u4e3a\u79fb\u52a8\u6e90\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u95ed\u5f0f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04421", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.04421", "abs": "https://arxiv.org/abs/2510.04421", "authors": ["Yuta Shikuri", "Hironori Fujisawa"], "title": "Learning Survival Models with Right-Censored Reporting Delays", "comment": "21 pages, 3 figures, 4 tables", "summary": "Survival analysis is a statistical technique used to estimate the time until\nan event occurs. Although it is applied across a wide range of fields,\nadjusting for reporting delays under practical constraints remains a\nsignificant challenge in the insurance industry. Such delays render event\noccurrences unobservable when their reports are subject to right censoring.\nThis issue becomes particularly critical when estimating hazard rates for newly\nenrolled cohorts with limited follow-up due to administrative censoring. Our\nstudy addresses this challenge by jointly modeling the parametric hazard\nfunctions of event occurrences and report timings. The joint probability\ndistribution is marginalized over the latent event occurrence status. We\nconstruct an estimator for the proposed survival model and establish its\nasymptotic consistency. Furthermore, we develop an expectation-maximization\nalgorithm to compute its estimates. Using these findings, we propose a\ntwo-stage estimation procedure based on a parametric proportional hazards model\nto evaluate observations subject to administrative censoring. Experimental\nresults demonstrate that our method effectively improves the timeliness of risk\nevaluation for newly enrolled cohorts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u53d1\u751f\u548c\u62a5\u544a\u65f6\u95f4\u7684\u751f\u5b58\u5206\u6790\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4fdd\u9669\u884c\u4e1a\u4e2d\u62a5\u544a\u5ef6\u8fdf\u5bfc\u81f4\u7684\u53f3\u5220\u5931\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u65b0\u5165\u7ec4\u961f\u5217\u7684\u6709\u9650\u968f\u8bbf\u6570\u636e\u3002", "motivation": "\u4fdd\u9669\u884c\u4e1a\u4e2d\u62a5\u544a\u5ef6\u8fdf\u5bfc\u81f4\u4e8b\u4ef6\u53d1\u751f\u65f6\u95f4\u65e0\u6cd5\u89c2\u6d4b\uff0c\u7279\u522b\u662f\u5728\u65b0\u5165\u7ec4\u961f\u5217\u4e2d\u7531\u4e8e\u884c\u653f\u5220\u5931\u5bfc\u81f4\u968f\u8bbf\u65f6\u95f4\u6709\u9650\uff0c\u8fd9\u4f7f\u5f97\u5371\u9669\u7387\u4f30\u8ba1\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u8054\u5408\u5efa\u6a21\u4e8b\u4ef6\u53d1\u751f\u548c\u62a5\u544a\u65f6\u95f4\u7684\u53c2\u6570\u5316\u5371\u9669\u51fd\u6570\uff0c\u901a\u8fc7\u5bf9\u6f5c\u5728\u4e8b\u4ef6\u53d1\u751f\u72b6\u6001\u8fdb\u884c\u8fb9\u9645\u5316\u5904\u7406\uff0c\u6784\u5efa\u751f\u5b58\u6a21\u578b\u4f30\u8ba1\u5668\u5e76\u5f00\u53d1\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u8fdb\u884c\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u5bf9\u65b0\u5165\u7ec4\u961f\u5217\u98ce\u9669\u8bc4\u4f30\u7684\u53ca\u65f6\u6027\uff0c\u5efa\u7acb\u7684\u4f30\u8ba1\u5668\u5177\u6709\u6e10\u8fd1\u4e00\u81f4\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u4f30\u8ba1\u7a0b\u5e8f\u57fa\u4e8e\u53c2\u6570\u6bd4\u4f8b\u5371\u9669\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u884c\u653f\u5220\u5931\u4e0b\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u4e3a\u4fdd\u9669\u884c\u4e1a\u7684\u98ce\u9669\u8bc4\u4ef7\u63d0\u4f9b\u4e86\u66f4\u53ca\u65f6\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.03254", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.03254", "abs": "https://arxiv.org/abs/2510.03254", "authors": ["David Benfield", "Stefano Coniglio", "Phan Tu Vuong", "Alain Zemkoho"], "title": "Adversarial training with restricted data manipulation", "comment": "21 page, 5 figures", "summary": "Adversarial machine learning concerns situations in which learners face\nattacks from active adversaries. Such scenarios arise in applications such as\nspam email filtering, malware detection and fake image generation, where\nsecurity methods must be actively updated to keep up with the everimproving\ngeneration of malicious data. Pessimistic Bilevel optimisation has been shown\nto be an effective method of training resilient classifiers against such\nadversaries. By modelling these scenarios as a game between the learner and the\nadversary, we anticipate how the adversary will modify their data and then\ntrain a resilient classifier accordingly. However, since existing pessimistic\nbilevel approaches feature an unrestricted adversary, the model is vulnerable\nto becoming overly pessimistic and unrealistic. When finding the optimal\nsolution that defeats the classifier, it is possible that the adversary's data\nbecomes nonsensical and loses its intended nature. Such an adversary will not\nproperly reflect reality, and consequently, will lead to poor classifier\nperformance when implemented on real-world data. By constructing a constrained\npessimistic bilevel optimisation model, we restrict the adversary's movements\nand identify a solution that better reflects reality. We demonstrate through\nexperiments that this model performs, on average, better than the existing\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u901a\u8fc7\u9650\u5236\u5bf9\u624b\u7684\u79fb\u52a8\u6765\u66f4\u597d\u5730\u53cd\u6620\u73b0\u5b9e\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5e73\u5747\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u4e2d\u7684\u5bf9\u624b\u4e0d\u53d7\u9650\u5236\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8fc7\u4e8e\u60b2\u89c2\u548c\u4e0d\u73b0\u5b9e\uff0c\u5f53\u5bf9\u624b\u6570\u636e\u53d8\u5f97\u65e0\u610f\u4e49\u65f6\uff0c\u65e0\u6cd5\u6b63\u786e\u53cd\u6620\u73b0\u5b9e\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u5206\u7c7b\u5668\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u6a21\u578b\uff0c\u9650\u5236\u5bf9\u624b\u7684\u79fb\u52a8\uff0c\u8bc6\u522b\u66f4\u7b26\u5408\u73b0\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5e73\u5747\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7ea6\u675f\u60b2\u89c2\u53cc\u5c42\u4f18\u5316\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u73b0\u5b9e\u5bf9\u6297\u573a\u666f\uff0c\u63d0\u9ad8\u5206\u7c7b\u5668\u5728\u5b9e\u9645\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.04240", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04240", "abs": "https://arxiv.org/abs/2510.04240", "authors": ["Dario Tagliaferri", "Silvia Mura", "Musa Furkan Keskin", "Sauradeep Dey", "Henk Wymeersch"], "title": "Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks", "comment": "13 pages", "summary": "This paper addresses the challenge of integrating multistatic coherent\nimaging functionalities in the downlink (DL) of a phase-coherent distributed\nmultiple input multiple output (D-MIMO) communication network. During DL, the\nD-MIMO access points (APs) jointly precode the transmitted signals to maximize\nthe spectral efficiency (SE) at the users (UEs) locations. However, imaging\nrequires that \\textit{(i)} a fraction of the APs work as receivers for sensing\nand \\textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals\nto illuminate the area to be imaged and allow multistatic operation. In these\nsettings, our contribution is twofold. We propose a novel distributed\nintegrated sensing and communication (D-ISAC) system that superposes a\npurposely designed AP-specific signal for imaging to the legacy UE-specific\ncommunication one, with a tunable trade-off factor. We detail both the imaging\nwaveform design according to the \\textit{extended orthogonality condition} and\nthe space-frequency precoder design. Then, we propose an optimized selection\nstrategy for the receiving APs, in order to maximize imaging performance under\nhalf-duplex constraints. Extensive numerical results prove the feasibility and\nbenefits of our proposal, materializing the potential of joint multistatic\nimaging and communications in practical D-MIMO deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5206\u5e03\u5f0fMIMO\u901a\u4fe1\u7f51\u7edc\u4e2d\u96c6\u6210\u591a\u57fa\u5730\u76f8\u5e72\u6210\u50cf\u529f\u80fd\u7684D-ISAC\u7cfb\u7edf\uff0c\u901a\u8fc7\u53e0\u52a0\u4e13\u95e8\u8bbe\u8ba1\u7684AP\u7279\u5b9a\u6210\u50cf\u4fe1\u53f7\u4e0e\u4f20\u7edf\u901a\u4fe1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u53ef\u8c03\u8c10\u7684\u6743\u8861\uff0c\u5e76\u4f18\u5316\u63a5\u6536AP\u9009\u62e9\u7b56\u7565\u4ee5\u6700\u5927\u5316\u6210\u50cf\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u76f8\u4f4d\u76f8\u5e72\u5206\u5e03\u5f0fMIMO\u901a\u4fe1\u7f51\u7edc\u4e0b\u884c\u94fe\u8def\u4e2d\u96c6\u6210\u591a\u57fa\u5730\u76f8\u5e72\u6210\u50cf\u529f\u80fd\u7684\u6311\u6218\uff0c\u4f20\u7edf\u901a\u4fe1\u9700\u8981AP\u8054\u5408\u9884\u7f16\u7801\u4ee5\u6700\u5927\u5316\u7528\u6237\u9891\u8c31\u6548\u7387\uff0c\u800c\u6210\u50cf\u9700\u8981\u90e8\u5206AP\u4f5c\u4e3a\u63a5\u6536\u5668\u4e14\u53d1\u5c04AP\u53d1\u5c04AP\u7279\u5b9a\u7684\u6b63\u4ea4\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u5f0f\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u5c06\u4e13\u95e8\u8bbe\u8ba1\u7684AP\u7279\u5b9a\u6210\u50cf\u4fe1\u53f7\u53e0\u52a0\u5230\u4f20\u7edf\u901a\u4fe1\u4fe1\u53f7\u4e0a\uff1b\u8be6\u7ec6\u8bbe\u8ba1\u7b26\u5408\u6269\u5c55\u6b63\u4ea4\u6761\u4ef6\u7684\u6210\u50cf\u6ce2\u5f62\u548c\u7a7a\u9891\u9884\u7f16\u7801\u5668\uff1b\u63d0\u51fa\u4f18\u5316\u7684\u63a5\u6536AP\u9009\u62e9\u7b56\u7565\u4ee5\u5728\u534a\u53cc\u5de5\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u6210\u50cf\u6027\u80fd\u3002", "result": "\u5e7f\u6cdb\u7684\u6570\u503c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6848\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5728\u5b9e\u9645D-MIMO\u90e8\u7f72\u4e2d\u8054\u5408\u591a\u57fa\u5730\u6210\u50cf\u548c\u901a\u4fe1\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5206\u5e03\u5f0fMIMO\u7f51\u7edc\u4e2d\u540c\u65f6\u8fdb\u884c\u901a\u4fe1\u548c\u6210\u50cf\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4fe1\u53f7\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04426", "categories": ["stat.ML", "math.FA", "47N70"], "pdf": "https://arxiv.org/pdf/2510.04426", "abs": "https://arxiv.org/abs/2510.04426", "authors": ["Magaly Catanzariti", "Hugo Aimar", "Diego M. Mateos"], "title": "Divergence Phase Index: A Riesz-Transform Framework for Multidimensional Phase Difference Analysis", "comment": "19 pages; 4 figures", "summary": "We introduce the Divergence Phase Index (DPI), a novel framework for\nquantifying phase differences in one and multidimensional signals, grounded in\nharmonic analysis via the Riesz transform. Based on classical Hilbert Transform\nphase measures, the DPI extends these principles to higher dimensions, offering\na geometry-aware metric that is invariant to intensity scaling and sensitive to\nstructural changes. We applied this method on both synthetic and real-world\ndatasets, including intracranial EEG (iEEG) recordings during epileptic\nseizures, high-resolution microscopy images, and paintings. In the 1D case, the\nDPI robustly detects hypersynchronization associated with generalized epilepsy,\nwhile in 2D, it reveals subtle, imperceptible changes in images and artworks.\nAdditionally, it can detect rotational variations in highly isotropic\nmicroscopy images. The DPI's robustness to amplitude variations and its\nadaptability across domains enable its use in diverse applications from\nnonlinear dynamics, complex systems analysis, to multidimensional signal\nprocessing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53d1\u6563\u76f8\u4f4d\u6307\u6570\uff08DPI\uff09\u6846\u67b6\uff0c\u57fa\u4e8eRiesz\u53d8\u6362\u7684\u8c10\u6ce2\u5206\u6790\uff0c\u7528\u4e8e\u91cf\u5316\u4e00\u7ef4\u548c\u591a\u7ef4\u4fe1\u53f7\u7684\u76f8\u4f4d\u5dee\u5f02\u3002\u8be5\u51e0\u4f55\u611f\u77e5\u5ea6\u91cf\u5bf9\u5f3a\u5ea6\u7f29\u653e\u4e0d\u53d8\uff0c\u5bf9\u7ed3\u6784\u53d8\u5316\u654f\u611f\uff0c\u9002\u7528\u4e8e\u766b\u75eb\u8111\u7535\u3001\u663e\u5fae\u56fe\u50cf\u548c\u7ed8\u753b\u7b49\u591a\u79cd\u6570\u636e\u3002", "motivation": "\u6269\u5c55\u7ecf\u5178\u5e0c\u5c14\u4f2f\u7279\u53d8\u6362\u76f8\u4f4d\u6d4b\u91cf\u5230\u9ad8\u7ef4\uff0c\u5f00\u53d1\u5bf9\u5f3a\u5ea6\u7f29\u653e\u4e0d\u53d8\u4e14\u5bf9\u7ed3\u6784\u53d8\u5316\u654f\u611f\u7684\u51e0\u4f55\u611f\u77e5\u76f8\u4f4d\u5dee\u5f02\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eRiesz\u53d8\u6362\u7684\u8c10\u6ce2\u5206\u6790\uff0c\u6784\u5efa\u53d1\u6563\u76f8\u4f4d\u6307\u6570\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u4e00\u7ef4\u548c\u591a\u7ef4\u4fe1\u53f7\u5206\u6790\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5305\u62ec\u766b\u75ebiEEG\u8bb0\u5f55\u3001\u9ad8\u5206\u8fa8\u7387\u663e\u5fae\u56fe\u50cf\u548c\u7ed8\u753b\u4f5c\u54c1\u3002", "result": "\u5728\u4e00\u7ef4\u60c5\u51b5\u4e0b\uff0cDPI\u80fd\u7a33\u5065\u68c0\u6d4b\u4e0e\u5168\u8eab\u6027\u766b\u75eb\u76f8\u5173\u7684\u8d85\u540c\u6b65\u5316\uff1b\u5728\u4e8c\u7ef4\u60c5\u51b5\u4e0b\uff0c\u80fd\u63ed\u793a\u56fe\u50cf\u548c\u827a\u672f\u54c1\u4e2d\u7ec6\u5fae\u4e0d\u53ef\u5bdf\u89c9\u7684\u53d8\u5316\uff1b\u8fd8\u80fd\u68c0\u6d4b\u9ad8\u5ea6\u5404\u5411\u540c\u6027\u663e\u5fae\u56fe\u50cf\u4e2d\u7684\u65cb\u8f6c\u53d8\u5316\u3002", "conclusion": "DPI\u5bf9\u5e45\u5ea6\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u8de8\u9886\u57df\u9002\u5e94\u6027\u5f3a\uff0c\u53ef\u5728\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3001\u590d\u6742\u7cfb\u7edf\u5206\u6790\u548c\u591a\u7ef4\u4fe1\u53f7\u5904\u7406\u7b49\u591a\u6837\u5316\u5e94\u7528\u4e2d\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2510.03255", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03255", "abs": "https://arxiv.org/abs/2510.03255", "authors": ["Wen Wu", "Ziyang Zhang", "Liwei Liu", "Xuenan Xu", "Junlin Liu", "Ke Fan", "Qitan Lv", "Jimin Zhuang", "Chen Zhang", "Zheqi Yuan", "Siyuan Hou", "Tianyi Lin", "Kai Chen", "Bowen Zhou", "Chao Zhang"], "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "comment": null, "summary": "The scientific reasoning ability of large language models (LLMs) has recently\nattracted significant attention. Time series, as a fundamental modality in\nscientific data, presents unique challenges that are often overlooked in\ncurrent multimodal LLMs, which either encode numerical sequences as text or\nconvert them into images. Such approaches may be insufficient for comprehensive\nscientific time series understanding and generation. Existing unified time\nseries models typically specialise in either forecasting or analysis, and their\neffectiveness on non-periodic, heterogeneous scientific signals remains\nunclear. To address these gaps, we introduce SciTS, a benchmark spanning 12\nscientific domains and 43 tasks, with over 50k+ instances, both univariate and\nmultivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz\nin frequency. We benchmark 17 models, including text-only LLMs, multimodal\nLLMs, and unified time series models, and find that general-purpose LLMs\nexhibit stronger generalisability than specialised time series models, while\nrepresenting time series as text or images limits their performance due to\nexcessively long sequences and loss of numerical precision, respectively. We\nthen introduce TimeOmni, a framework that equips LLMs with the ability to\nunderstand and generate time series while remaining compatible with\ngeneral-purpose LLM training. This work fills a gap in both dedicated\nbenchmarks and modelling frameworks for scientific time series, paving the way\nfor LLMs to understand and generate complex temporal scientific data.", "AI": {"tldr": "\u63d0\u51fa\u4e86SciTS\u57fa\u51c6\u6d4b\u8bd5\u548cTimeOmni\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5904\u7406\u5b58\u5728\u4e0d\u8db3\uff0c\u8981\u4e48\u5c06\u6570\u503c\u5e8f\u5217\u7f16\u7801\u4e3a\u6587\u672c\uff0c\u8981\u4e48\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u8fd9\u65e0\u6cd5\u6ee1\u8db3\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u7684\u5168\u9762\u7406\u89e3\u548c\u751f\u6210\u9700\u6c42\u3002", "method": "\u5f15\u5165SciTS\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d612\u4e2a\u79d1\u5b66\u9886\u57df\u548c43\u4e2a\u4efb\u52a1\uff0c\u5305\u542b5\u4e07\u591a\u4e2a\u5b9e\u4f8b\uff1b\u63d0\u51faTimeOmni\u6846\u67b6\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd517\u4e2a\u6a21\u578b\u53d1\u73b0\uff0c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6bd4\u4e13\u95e8\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5c06\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u4e3a\u6587\u672c\u6216\u56fe\u50cf\u4f1a\u56e0\u5e8f\u5217\u8fc7\u957f\u6216\u6570\u503c\u7cbe\u5ea6\u635f\u5931\u800c\u9650\u5236\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u586b\u8865\u4e86\u79d1\u5b66\u65f6\u95f4\u5e8f\u5217\u4e13\u7528\u57fa\u51c6\u6d4b\u8bd5\u548c\u5efa\u6a21\u6846\u67b6\u7684\u7a7a\u767d\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u548c\u751f\u6210\u590d\u6742\u65f6\u95f4\u79d1\u5b66\u6570\u636e\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.04258", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04258", "abs": "https://arxiv.org/abs/2510.04258", "authors": ["Ziang Zhao", "Weixi Liang", "Kai Hu", "Qun Zhang", "Xiongbin Yu", "Qiang Li"], "title": "Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments", "comment": null, "summary": "Accurate channel modeling is essential for realizing the potential of\nterahertz (THz) communications in 6G indoor networks, where existing models\nstruggle with severe frequency selectivity and multipath effects. We propose a\nphysically grounded Rician fading channel model that jointly incorporates\ndeterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS)\ncomponents, enhanced by frequency-dependent attenuation characterized by\noptimized exponents alpha and beta. Unlike conventional approaches, our model\nintegrates a two-ray reflection framework to capture standing wave phenomena\nand employs wideband spectral averaging to mitigate frequency selectivity over\nbandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning\n0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE)\nas low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and\nreducing RMSE by 73.3% as bandwidth increases. These findings underscore the\nimportance of bandwidth in suppressing oscillatory artifacts and improving\nmodeling accuracy. Our approach provides a robust foundation for THz system\ndesign, supporting reliable indoor wireless personal area networks (WPANs),\ndevice-to-device (D2D) communications, and precise localization in future 6G\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684Rician\u8870\u843d\u4fe1\u9053\u6a21\u578b\uff0c\u7528\u4e8e6G\u5ba4\u5185\u592a\u8d6b\u5179\u901a\u4fe1\uff0c\u901a\u8fc7\u8054\u5408\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u5206\u91cf\u4ee5\u53ca\u9891\u7387\u76f8\u5173\u8870\u51cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u9053\u5efa\u6a21\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u592a\u8d6b\u5179\u901a\u4fe1\u4e2d\u7684\u4e25\u91cd\u9891\u7387\u9009\u62e9\u6027\u548c\u591a\u5f84\u6548\u5e94\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u4fe1\u9053\u5efa\u6a21\u6765\u5b9e\u73b06G\u5ba4\u5185\u7f51\u7edc\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7269\u7406\u57fa\u7840\u7684Rician\u8870\u843d\u4fe1\u9053\u6a21\u578b\uff0c\u7ed3\u5408\u786e\u5b9a\u6027LOS\u548c\u968f\u673a\u6027NLOS\u5206\u91cf\uff0c\u4f7f\u7528\u4f18\u5316\u7684alpha\u548cbeta\u6307\u6570\u8868\u5f81\u9891\u7387\u76f8\u5173\u8870\u51cf\uff0c\u96c6\u6210\u53cc\u5c04\u7ebf\u53cd\u5c04\u6846\u67b6\u6355\u6349\u9a7b\u6ce2\u73b0\u8c61\uff0c\u5e76\u91c7\u7528\u5bbd\u5e26\u9891\u8c31\u5e73\u5747\u6765\u51cf\u8f7b\u9891\u7387\u9009\u62e9\u6027\u3002", "result": "\u5728208 GHz\u8f7d\u6ce2\u9891\u7387\u4e0b\uff0c0.1-0.9\u7c73\u8303\u56f4\u5185\u7684\u5b9e\u6d4b\u6570\u636e\u663e\u793a\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u4f4e\u81f32.54 dB\u7684RMSE\uff0c\u6bd4\u81ea\u7531\u7a7a\u95f4\u8def\u5f84\u635f\u8017\u6a21\u578b\u63d0\u534714.2%\uff0c\u5e26\u5bbd\u589e\u52a0\u65f6RMSE\u964d\u4f4e73.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u592a\u8d6b\u5179\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u652f\u6301\u53ef\u9760\u7684\u5ba4\u5185\u65e0\u7ebf\u4e2a\u57df\u7f51\u3001\u8bbe\u5907\u95f4\u901a\u4fe1\u548c\u672a\u67656G\u5e94\u7528\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\uff0c\u5f3a\u8c03\u4e86\u5e26\u5bbd\u5728\u6291\u5236\u632f\u8361\u4f2a\u5f71\u548c\u63d0\u9ad8\u5efa\u6a21\u7cbe\u5ea6\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.04556", "categories": ["stat.ML", "cs.LG", "math.ST", "q-fin.ST", "stat.AP", "stat.TH", "62, 68", "G.3"], "pdf": "https://arxiv.org/pdf/2510.04556", "abs": "https://arxiv.org/abs/2510.04556", "authors": ["Alexej Brauer", "Paul Menzel"], "title": "Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing", "comment": null, "summary": "In a dynamic landscape where portfolios and environments evolve, maintaining\nthe accuracy of pricing models is critical. To the best of our knowledge, this\nis the first study to systematically examine concept drift in non-life\ninsurance pricing. We (i) provide an overview of the relevant literature and\ncommonly used methodologies, clarify the distinction between virtual drift and\nconcept drift, and explain their implications for long-run model performance;\n(ii) review and formalize common performance measures, including the Gini index\nand deviance loss, and articulate their interpretation; (iii) derive the\nasymptotic distribution of the Gini index, enabling valid inference and\nhypothesis testing; and (iv) present a standardized monitoring procedure that\nindicates when refitting is warranted. We illustrate the framework using a\nmodified real-world portfolio with induced concept drift and discuss practical\nconsiderations and pitfalls.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u975e\u5bff\u9669\u5b9a\u4ef7\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u533a\u5206\u4e86\u865a\u62df\u6f02\u79fb\u548c\u6982\u5ff5\u6f02\u79fb\uff0c\u63a8\u5bfc\u4e86\u57fa\u5c3c\u7cfb\u6570\u7684\u6e10\u8fd1\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u7684\u76d1\u63a7\u7a0b\u5e8f\u6765\u6307\u5bfc\u6a21\u578b\u91cd\u6784\u65f6\u673a\u3002", "motivation": "\u5728\u52a8\u6001\u53d8\u5316\u7684\u4fdd\u9669\u73af\u5883\u4e2d\uff0c\u4fdd\u6301\u5b9a\u4ef7\u6a21\u578b\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u975e\u5bff\u9669\u5b9a\u4ef7\u4e2d\u6982\u5ff5\u6f02\u79fb\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u5efa\u7acb\u6709\u6548\u7684\u76d1\u63a7\u548c\u91cd\u6784\u673a\u5236\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u65b9\u6cd5\u8bba\u68b3\u7406\uff0c\u533a\u5206\u865a\u62df\u6f02\u79fb\u4e0e\u6982\u5ff5\u6f02\u79fb\uff1b\u5f62\u5f0f\u5316\u6027\u80fd\u5ea6\u91cf\u6307\u6807\uff1b\u63a8\u5bfc\u57fa\u5c3c\u7cfb\u6570\u7684\u6e10\u8fd1\u5206\u5e03\uff1b\u8bbe\u8ba1\u6807\u51c6\u5316\u76d1\u63a7\u7a0b\u5e8f\u3002", "result": "\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u76d1\u63a7\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u6982\u5ff5\u6f02\u79fb\u5e76\u786e\u5b9a\u6a21\u578b\u91cd\u6784\u65f6\u673a\uff0c\u901a\u8fc7\u4fee\u6539\u7684\u771f\u5b9e\u4e16\u754c\u6295\u8d44\u7ec4\u5408\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u4fdd\u9669\u5b9a\u4ef7\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u6982\u5ff5\u6f02\u79fb\u76d1\u63a7\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u7ef4\u6301\u957f\u671f\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6ce8\u610f\u4e8b\u9879\u548c\u9677\u9631\u3002"}}
{"id": "2510.03257", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03257", "abs": "https://arxiv.org/abs/2510.03257", "authors": ["Zijian Zhao", "Sen Li"], "title": "Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?", "comment": null, "summary": "On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate\nreal-time challenge of bundling and matching passengers-each with distinct\norigins and destinations-to available vehicles, all while navigating\nsignificant system uncertainties. Due to the extensive observation space\narising from the large number of drivers and orders, order dispatching, though\nfundamentally a centralized task, is often addressed using Multi-Agent\nReinforcement Learning (MARL). However, independent MARL methods fail to\ncapture global information and exhibit poor cooperation among workers, while\nCentralized Training Decentralized Execution (CTDE) MARL methods suffer from\nthe curse of dimensionality. To overcome these challenges, we propose\nTriple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method\ndesigned specifically for large-scale order dispatching on ride-sharing\nplatforms. Built on a variant TD3, our approach addresses the vast action space\nthrough an action decomposition strategy that breaks down the joint action\nprobability into individual driver action probabilities. To handle the\nextensive observation space, we introduce a novel BERT-based network, where\nparameter reuse mitigates parameter growth as the number of drivers and orders\nincreases, and the attention mechanism effectively captures the complex\nrelationships among the large pool of driver and orders. We validate our method\nusing a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves\napproximately an 11.95% improvement over current state-of-the-art methods, with\na 4.26% increase in served orders and a 22.25% reduction in pickup times. Our\ncode, trained model parameters, and processed data are publicly available at\nthe repository https://github.com/RS2002/Triple-BERT .", "AI": {"tldr": "\u63d0\u51fa\u4e86Triple-BERT\u65b9\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8eTD3\u7684\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u7ea6\u8f66\u5e73\u53f0\u5927\u89c4\u6a21\u8ba2\u5355\u8c03\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u89e3\u548cBERT\u7f51\u7edc\u5904\u7406\u9ad8\u7ef4\u52a8\u4f5c\u548c\u89c2\u6d4b\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709MARL\u65b9\u6cd5\u5728\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u72ec\u7acbMARL\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u4fe1\u606f\u548c\u667a\u80fd\u4f53\u534f\u4f5c\uff0cCTDE\u65b9\u6cd5\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\u3002\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21\u53f8\u673a\u548c\u8ba2\u5355\u7684\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eTD3\u53d8\u4f53\u6784\u5efa\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u52a8\u4f5c\u5206\u89e3\u7b56\u7565\u5c06\u8054\u5408\u52a8\u4f5c\u6982\u7387\u5206\u89e3\u4e3a\u5355\u4e2a\u53f8\u673a\u52a8\u4f5c\u6982\u7387\uff0c\u5f15\u5165BERT\u7f51\u7edc\u901a\u8fc7\u53c2\u6570\u590d\u7528\u548c\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u5927\u89c4\u6a21\u89c2\u6d4b\u7a7a\u95f4\u3002", "result": "\u5728\u66fc\u54c8\u987f\u771f\u5b9e\u7f51\u7ea6\u8f66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534711.95%\uff0c\u670d\u52a1\u8ba2\u5355\u6570\u589e\u52a04.26%\uff0c\u63a5\u9a7e\u65f6\u95f4\u51cf\u5c1122.25%\u3002", "conclusion": "Triple-BERT\u901a\u8fc7\u96c6\u4e2d\u5f0f\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7f51\u7ea6\u8f66\u8ba2\u5355\u8c03\u5ea6\u4e2d\u7684\u9ad8\u7ef4\u52a8\u4f5c\u548c\u89c2\u6d4b\u7a7a\u95f4\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u6027\u80fd\u3002"}}
{"id": "2510.04359", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04359", "abs": "https://arxiv.org/abs/2510.04359", "authors": ["Minsu Kim", "Walid Saad", "Dour Calin"], "title": "Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data", "comment": "Submitted to IEEE TWC", "summary": "In 6G wireless networks, multi-modal ML models can be leveraged to enable\nsituation-aware network decisions in dynamic environments. However, trained ML\nmodels often fail to generalize under domain shifts when training and test data\ndistributions are different because they often focus on modality-specific\nspurious features. In practical wireless systems, domain shifts occur\nfrequently due to dynamic channel statistics, moving obstacles, or hardware\nconfiguration. Thus, there is a need for learning frameworks that can achieve\nrobust generalization under scarce multi-modal data in wireless networks. In\nthis paper, a novel and data-efficient two-phase learning framework is proposed\nto improve generalization performance in unseen and unfamiliar wireless\nenvironments with minimal amount of multi-modal data. In the first stage, a\nphysics-based loss function is employed to enable each BS to learn the physics\nunderlying its wireless environment captured by multi-modal data. The\ndata-efficiency of the physics-based loss function is analytically\ninvestigated. In the second stage, collaborative domain adaptation is proposed\nto leverage the wireless environment knowledge of multiple BSs to guide\nunder-performing BSs under domain shift. Specifically, domain-similarity-aware\nmodel aggregation is proposed to utilize the knowledge of BSs that experienced\nsimilar domains. To validate the proposed framework, a new dataset generation\nframework is developed by integrating CARLA and MATLAB-based mmWave channel\nmodeling to predict mmWave RSS. Simulation results show that the proposed\nphysics-based training requires only 13% of data samples to achieve the same\nperformance as a state-of-the-art baseline that does not use physics-based\ntraining. Moreover, the proposed collaborative domain adaptation needs only 25%\nof data samples and 20% of FLOPs to achieve the convergence compared to\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e6G\u65e0\u7ebf\u7f51\u7edc\u7684\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u77e5\u8bc6\u5f15\u5bfc\u548c\u534f\u4f5c\u57df\u9002\u5e94\uff0c\u5728\u7a00\u7f3a\u591a\u6a21\u6001\u6570\u636e\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6cdb\u5316", "motivation": "\u89e3\u51b36G\u7f51\u7edc\u4e2d\u591a\u6a21\u6001ML\u6a21\u578b\u5728\u57df\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b9e\u9645\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7ecf\u5e38\u51fa\u73b0\u4fe1\u9053\u7edf\u8ba1\u53d8\u5316\u3001\u79fb\u52a8\u969c\u788d\u7269\u7b49\u5bfc\u81f4\u7684\u57df\u504f\u79fb", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u635f\u5931\u51fd\u6570\u5b66\u4e60\u65e0\u7ebf\u73af\u5883\u7269\u7406\u7279\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u63d0\u51fa\u534f\u4f5c\u57df\u9002\u5e94\uff0c\u901a\u8fc7\u57df\u76f8\u4f3c\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u6765\u5229\u7528\u591a\u4e2a\u57fa\u7ad9\u7684\u65e0\u7ebf\u73af\u5883\u77e5\u8bc6", "result": "\u57fa\u4e8e\u7269\u7406\u7684\u8bad\u7ec3\u4ec5\u970013%\u6570\u636e\u6837\u672c\u5373\u53ef\u8fbe\u5230\u4e0d\u4f7f\u7528\u7269\u7406\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u6027\u80fd\uff1b\u534f\u4f5c\u57df\u9002\u5e94\u4ec5\u970025%\u6570\u636e\u6837\u672c\u548c20%FLOPs\u5373\u53ef\u6536\u655b", "conclusion": "\u8be5\u6846\u67b6\u5728\u7a00\u7f3a\u591a\u6a21\u6001\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a6G\u7f51\u7edc\u4e2d\u7684\u60c5\u5883\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.04602", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04602", "abs": "https://arxiv.org/abs/2510.04602", "authors": ["Eduardo Fernandes Montesuma", "Yassir Bendou", "Mike Gartrell"], "title": "Computing Wasserstein Barycenters through Gradient Flows", "comment": "4 Figures, 3 Tables, under review", "summary": "Wasserstein barycenters provide a powerful tool for aggregating probability\nmeasures, while leveraging the geometry of their ambient space. Existing\ndiscrete methods suffer from poor scalability, as they require access to the\ncomplete set of samples from input measures. We address this issue by recasting\nthe original barycenter problem as a gradient flow in the Wasserstein space.\nOur approach offers two advantages. First, we achieve scalability by sampling\nmini-batches from the input measures. Second, we incorporate functionals over\nprobability measures, which regularize the barycenter problem through internal,\npotential, and interaction energies. We present two algorithms for empirical\nand Gaussian mixture measures, providing convergence guarantees under the\nPolyak-{\\L}ojasiewicz inequality. Experimental validation on toy datasets and\ndomain adaptation benchmarks show that our methods outperform previous discrete\nand neural net-based methods for computing Wasserstein barycenters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u7a7a\u95f4\u4e2d\u68af\u5ea6\u6d41\u7684\u53ef\u6269\u5c55Wasserstein\u91cd\u5fc3\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6279\u91cf\u91c7\u6837\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u5728\u73a9\u5177\u6570\u636e\u96c6\u548c\u9886\u57df\u81ea\u9002\u5e94\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u6563Wasserstein\u91cd\u5fc3\u8ba1\u7b97\u65b9\u6cd5\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u9700\u8981\u8bbf\u95ee\u8f93\u5165\u5ea6\u91cf\u7684\u5b8c\u6574\u6837\u672c\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "method": "\u5c06\u539f\u59cb\u91cd\u5fc3\u95ee\u9898\u91cd\u65b0\u6784\u9020\u4e3aWasserstein\u7a7a\u95f4\u4e2d\u7684\u68af\u5ea6\u6d41\uff0c\u901a\u8fc7\u4ece\u8f93\u5165\u5ea6\u91cf\u4e2d\u91c7\u6837\u5c0f\u6279\u91cf\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5f15\u5165\u6982\u7387\u5ea6\u91cf\u4e0a\u7684\u6cdb\u51fd\u6765\u6b63\u5219\u5316\u91cd\u5fc3\u95ee\u9898\u3002", "result": "\u5728\u73a9\u5177\u6570\u636e\u96c6\u548c\u9886\u57df\u81ea\u9002\u5e94\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97Wasserstein\u91cd\u5fc3\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u79bb\u6563\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u68af\u5ea6\u6d41\u7684\u65b9\u6cd5\u4e3a\u8ba1\u7b97Wasserstein\u91cd\u5fc3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002"}}
{"id": "2510.03258", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03258", "abs": "https://arxiv.org/abs/2510.03258", "authors": ["Chang'an Yi", "Xiaohui Deng", "Shuaicheng Niu", "Yan Zhou"], "title": "POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation", "comment": "11pages,6 figures", "summary": "Test-time adaptation (TTA) aims to transfer knowledge from a source model to\nunknown test data with potential distribution shifts in an online manner. Many\nexisting TTA methods rely on entropy as a confidence metric to optimize the\nmodel. However, these approaches are sensitive to the predefined entropy\nthreshold, influencing which samples are chosen for model adaptation.\nConsequently, potentially reliable target samples are often overlooked and\nunderutilized. For instance, a sample's entropy might slightly exceed the\nthreshold initially, but fall below it after the model is updated. Such samples\ncan provide stable supervised information and offer a normal range of gradients\nto guide model adaptation. In this paper, we propose a general approach,\n\\underline{POEM}, to promote TTA via ex\\underline{\\textbf{p}}loring the\npreviously unexpl\\underline{\\textbf{o}}red reliabl\\underline{\\textbf{e}}\nsa\\underline{\\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch\nnetwork to strike a balance between extracting domain-agnostic representations\nand achieving high performance on target data. Comprehensive experiments across\nmultiple architectures demonstrate that POEM consistently outperforms existing\nTTA methods in both challenging scenarios and real-world domain shifts, while\nremaining computationally efficient. The effectiveness of POEM is evaluated\nthrough extensive analyses and thorough ablation studies. Moreover, the core\nidea behind POEM can be employed as an augmentation strategy to boost the\nperformance of existing TTA approaches. The source code is publicly available\nat \\emph{https://github.com/ycarobot/POEM}", "AI": {"tldr": "\u63d0\u51faPOEM\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a2\u7d22\u5148\u524d\u672a\u88ab\u5229\u7528\u7684\u53ef\u9760\u6837\u672c\u6765\u6539\u8fdb\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94(TTA)\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u71b5\u9608\u503c\u5bfc\u81f4\u53ef\u9760\u6837\u672c\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709TTA\u65b9\u6cd5\u4f9d\u8d56\u71b5\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u5bf9\u9884\u5b9a\u4e49\u71b5\u9608\u503c\u654f\u611f\uff0c\u5bfc\u81f4\u8bb8\u591a\u6f5c\u5728\u53ef\u9760\u7684\u6d4b\u8bd5\u6837\u672c\u88ab\u5ffd\u89c6\u548c\u672a\u5145\u5206\u5229\u7528\u3002\u8fd9\u4e9b\u6837\u672c\u867d\u7136\u521d\u59cb\u71b5\u503c\u7565\u9ad8\u4e8e\u9608\u503c\uff0c\u4f46\u5728\u6a21\u578b\u66f4\u65b0\u540e\u53ef\u80fd\u53d8\u5f97\u53ef\u9760\uff0c\u80fd\u591f\u63d0\u4f9b\u7a33\u5b9a\u7684\u76d1\u7763\u4fe1\u606f\u548c\u6b63\u5e38\u8303\u56f4\u7684\u68af\u5ea6\u6765\u6307\u5bfc\u6a21\u578b\u9002\u5e94\u3002", "method": "\u63d0\u51faPOEM\u65b9\u6cd5\uff0c\u63a2\u7d22\u5148\u524d\u672a\u88ab\u5229\u7528\u7684\u53ef\u9760\u6837\u672c\u8fdb\u884cTTA\u3002\u5f15\u5165\u989d\u5916\u7684Adapt Branch\u7f51\u7edc\uff0c\u5728\u63d0\u53d6\u9886\u57df\u65e0\u5173\u8868\u793a\u548c\u5728\u76ee\u6807\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5728\u591a\u79cd\u67b6\u6784\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cPOEM\u5728\u6311\u6218\u6027\u573a\u666f\u548c\u771f\u5b9e\u4e16\u754c\u9886\u57df\u504f\u79fb\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709TTA\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002POEM\u7684\u6838\u5fc3\u601d\u60f3\u53ef\u4f5c\u4e3a\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u73b0\u6709TTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "POEM\u901a\u8fc7\u6709\u6548\u5229\u7528\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u53ef\u9760\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u7684\u6027\u80fd\uff0c\u4e3aTTA\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.04402", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04402", "abs": "https://arxiv.org/abs/2510.04402", "authors": ["Binyu Lu", "Matthias Frey", "Stark Draper", "Jingge Zhu"], "title": "Low-Rank-Based Approximate Computation with Memristors", "comment": "5 pages, 2 figures, submitted to an IEEE conference for possible\n  publication", "summary": "Memristor crossbars enable vector-matrix multiplication (VMM), and are\npromising for low-power applications. However, it can be difficult to write the\nmemristor conductance values exactly. To improve the accuracy of VMM, we\npropose a scheme based on low-rank matrix approximation. Specifically, singular\nvalue decomposition (SVD) is first applied to obtain a low-rank approximation\nof the target matrix, which is then factored into a pair of smaller matrices.\nSubsequently, a two-step serial VMM is executed, where the stochastic write\nerrors are mitigated through step-wise averaging. To evaluate the performance\nof the proposed scheme, we derive a general expression for the resulting\ncomputation error and provide an asymptotic analysis under a prescribed\nsingular-value profile, which reveals how the error scales with matrix size and\nrank. Both analytical and numerical results confirm the superiority of the\nproposed scheme compared with the benchmark scheme.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u7684\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u65b9\u6848\uff0c\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3\u548c\u4e24\u6b65\u4e32\u884c\u8ba1\u7b97\u6765\u7f13\u89e3\u5199\u5165\u8bef\u5dee\uff0c\u63d0\u9ad8\u8ba1\u7b97\u7cbe\u5ea6\u3002", "motivation": "\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\u96be\u4ee5\u7cbe\u786e\u5199\u5165\u7535\u5bfc\u503c\uff0c\u5bfc\u81f4\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u7cbe\u5ea6\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u65b9\u6848\u6765\u63d0\u9ad8\u8ba1\u7b97\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5947\u5f02\u503c\u5206\u89e3\u83b7\u5f97\u76ee\u6807\u77e9\u9635\u7684\u4f4e\u79e9\u8fd1\u4f3c\uff0c\u5206\u89e3\u4e3a\u4e24\u4e2a\u8f83\u5c0f\u77e9\u9635\uff0c\u901a\u8fc7\u4e24\u6b65\u4e32\u884c\u5411\u91cf\u77e9\u9635\u4e58\u6cd5\u548c\u6b65\u8fdb\u5e73\u5747\u6765\u51cf\u8f7b\u968f\u673a\u5199\u5165\u8bef\u5dee\u3002", "result": "\u63a8\u5bfc\u4e86\u8ba1\u7b97\u8bef\u5dee\u7684\u4e00\u822c\u8868\u8fbe\u5f0f\uff0c\u5728\u89c4\u5b9a\u7684\u5947\u5f02\u503c\u5206\u5e03\u4e0b\u8fdb\u884c\u6e10\u8fd1\u5206\u6790\uff0c\u663e\u793a\u8bef\u5dee\u5982\u4f55\u968f\u77e9\u9635\u5927\u5c0f\u548c\u79e9\u7f29\u653e\uff0c\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u5747\u8bc1\u5b9e\u65b9\u6848\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u65b9\u6848\u80fd\u6709\u6548\u63d0\u9ad8\u5fc6\u963b\u5668\u4ea4\u53c9\u9635\u5217\u7684\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u5728\u77e9\u9635\u5927\u5c0f\u548c\u79e9\u7684\u7f29\u653e\u5173\u7cfb\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04762", "categories": ["stat.ML", "astro-ph.IM", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04762", "abs": "https://arxiv.org/abs/2510.04762", "authors": ["Thorsten Gl\u00fcsenkamp"], "title": "Fisher-Bingham-like normalizing flows on the sphere", "comment": null, "summary": "A generic D-dimensional Gaussian can be conditioned or projected onto the D-1\nunit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular\nGaussian (AG) distribution families, respectively. These are some of the most\nfundamental distributions on the sphere, yet cannot straightforwardly be\nwritten as a normalizing flow except in two special cases: the von-Mises Fisher\nin D=3 and the central angular Gaussian in any D. In this paper, we describe\nhow to generalize these special cases to a family of normalizing flows that\nbehave similarly to the full FB or AG family in any D. We call them\n\"zoom-linear-project\" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham\ndistribution, their composition allows to gradually add complexity as needed.\nFurthermore, they can naturally handle conditional density estimation with\ntarget distributions that vary by orders of magnitude in scale - a setting that\nis important in astronomical applications but that existing flows often\nstruggle with. A particularly useful member of the new family is the Kent\nanalogue that can cheaply upgrade any flow in this situation to yield better\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86ZLP-Fisher\u6d41\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u5f52\u4e00\u5316\u6d41\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7403\u9762\u4e0a\u5efa\u6a21\u7c7b\u4f3cFisher-Bingham\u5206\u5e03\u7684\u590d\u6742\u5206\u5e03\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5c3a\u5ea6\u53d8\u5316\u5f88\u5927\u7684\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684Fisher-Bingham\u548c\u89d2\u9ad8\u65af\u5206\u5e03\u867d\u7136\u662f\u5728\u7403\u9762\u4e0a\u7684\u57fa\u672c\u5206\u5e03\uff0c\u4f46\u9664\u4e86\u4e24\u4e2a\u7279\u6b8a\u60c5\u51b5\u5916\uff0c\u65e0\u6cd5\u76f4\u63a5\u8868\u793a\u4e3a\u5f52\u4e00\u5316\u6d41\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u5c3a\u5ea6\u53d8\u5316\u5f88\u5927\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\"\u7f29\u653e-\u7ebf\u6027-\u6295\u5f71\"(ZLP)\u64cd\u4f5c\u6784\u5efa\u5f52\u4e00\u5316\u6d41\uff0c\u53ef\u4ee5\u9010\u6b65\u589e\u52a0\u590d\u6742\u5ea6\uff0c\u5e76\u81ea\u7136\u5904\u7406\u5c3a\u5ea6\u53d8\u5316\u5f88\u5927\u7684\u6761\u4ef6\u5206\u5e03\u3002", "result": "\u5f00\u53d1\u4e86ZLP-Fisher\u6d41\u7cfb\u5217\uff0c\u5176\u4e2dKent\u7c7b\u6bd4\u65b9\u6cd5\u53ef\u4ee5\u5ec9\u4ef7\u5730\u63d0\u5347\u73b0\u6709\u6d41\u7684\u6027\u80fd\uff0c\u7279\u522b\u5728\u5929\u6587\u5b66\u5e94\u7528\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "ZLP-Fisher\u6d41\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u7403\u9762\u4e0a\u7684\u590d\u6742\u5206\u5e03\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5c3a\u5ea6\u53d8\u5316\u5f88\u5927\u7684\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.03259", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03259", "abs": "https://arxiv.org/abs/2510.03259", "authors": ["Yoonjeon Kim", "Doohyuk Jang", "Eunho Yang"], "title": "Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning", "comment": "preprint", "summary": "Recent studies on reasoning models explore the meta-awareness of language\nmodels, the ability to know how to think by itself. We argue that large\nreasoning models lack this meta-awareness property by proving severe\nmisalignment between true rollouts and predicted meta information. We posit\nthat aligning meta-prediction with true rollouts will lead to significant\nperformance gains. To verify this hypothesis, we design a training pipeline\nthat boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced\nmeta-awareness directly translates to improved accuracy. Unlike existing\nmeta-cognitive reasoning models, our method does not require external training\nsources but leverages self-generated signals to train meta-awareness. Moreover,\nour method enables efficient training by i) filtering out zero-variance prompts\nthat are either trivial or unsolvable and ii) cutting off lengthy rollouts when\nthey are unlikely to lead to correct answers. The results are inspiring: our\nstrategy yields significant improvements in both accuracy and training\nefficiency on in-domain tasks and shows strong generalization to out-of-domain\nbenchmarks. More specifically, our method can speed up GRPO training by over\n1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on\nAIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with\nmeta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %\nboost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks\nspanning logical, scientific, and coding domains.", "AI": {"tldr": "\u63d0\u51faMASA\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5bf9\u9f50\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u8bc1\u660e\u5143\u8ba4\u77e5\u5bf9\u9f50\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5728\u6570\u5b66\u548c\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5373\u77e5\u9053\u81ea\u5df1\u5982\u4f55\u601d\u8003\u7684\u80fd\u529b\uff0c\u8fd9\u5bfc\u81f4\u771f\u5b9e\u63a8\u7406\u8fc7\u7a0b\u4e0e\u5143\u9884\u6d4b\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u4e0d\u5bf9\u9f50", "method": "\u8bbe\u8ba1MASA\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u4fe1\u53f7\u8bad\u7ec3\u5143\u8ba4\u77e5\uff0c\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u3002\u65b9\u6cd5\u5305\u62ec\uff1a\u8fc7\u6ee4\u96f6\u65b9\u5dee\u63d0\u793a\u3001\u622a\u65ad\u4e0d\u53ef\u80fd\u6210\u529f\u7684\u5197\u957f\u63a8\u7406\u94fe", "result": "\u5728\u9886\u57df\u5185\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548c\u8bad\u7ec3\u6548\u7387\uff0cGRPO\u8bad\u7ec3\u52a0\u901f1.28\u500d\uff0cAIME25\u51c6\u786e\u7387\u63d0\u534719.3%\uff0c6\u4e2a\u6570\u5b66\u57fa\u51c6\u5e73\u5747\u63d0\u53476.2%\u3002\u572813\u4e2a\u8de8\u9886\u57df\u57fa\u51c6\u4e0a\u83b7\u5f972.08%\u5e73\u5747\u63d0\u5347", "conclusion": "\u5143\u8ba4\u77e5\u5bf9\u9f50\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0cMASA\u65b9\u6cd5\u8bc1\u660e\u901a\u8fc7\u81ea\u5bf9\u9f50\u589e\u5f3a\u5143\u8ba4\u77e5\u662f\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u9014\u5f84"}}
{"id": "2510.04409", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04409", "abs": "https://arxiv.org/abs/2510.04409", "authors": ["Samyadip Sarkar", "Arunashish Datta", "David Yang", "Mayukh Nath", "Shovan Maity", "Shreyas Sen"], "title": "Effect of nearby Metals on Electro-Quasistatic Human Body Communication", "comment": "18 pages, 25 Figures, 2 Tables, 5 Appendix", "summary": "In recent decades Human Body Communication has emerged as a promising\nalternative to traditional radio wave communication, utilizing the body's\nconductive properties for low-power connectivity among wearables. This method\nharnesses the human body as an energy-efficient channel for data transmission\nwithin the electro-quasistatic frequency range, enabling advancements in\nhuman-machine interaction. While prior work has noted the role of parasitic\nreturn paths in such capacitively coupled systems, the influence of surrounding\nmetallic objects on these paths, which are critical for EQS wireless signaling,\nhas not been fully explored. This paper fills that gap with a structured study\nof how various conducting objects, from non-grounded (floating) metals and\ngrounded metals to enclosed metallic environments such as elevators and cars,\naffect the body-communication channel. We present a theoretical framework\nsupported by finite element method simulations and experiments with wearable\ndevices. Results show that metallic objects within 20 cm of devices can reduce\ntransmission loss by about 10 dB. When a device ground connects to a grounded\nmetallic object, channel gain can increase by at least 20 dB. Contact area\nduring touch-based interactions with grounded metals produces contact-impedance\ndependent high-pass channel characteristics. Proximity to metallic objects\nintroduces variability within a critical distance, with grounded metals\nproducing a larger overall effect than floating metals. These findings improve\nunderstanding of body-centric communication links and inform design for\nhealthcare, consumer electronics, defense, and industrial applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u91d1\u5c5e\u7269\u4f53\u5bf9\u4eba\u4f53\u901a\u4fe1\u4fe1\u9053\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8fd1\u8ddd\u79bb\u91d1\u5c5e\u7269\u4f53\u53ef\u964d\u4f4e\u4f20\u8f93\u635f\u801710dB\uff0c\u63a5\u5730\u91d1\u5c5e\u8fde\u63a5\u53ef\u63d0\u5347\u4fe1\u9053\u589e\u76ca\u81f3\u5c1120dB\uff0c\u63a5\u89e6\u9762\u79ef\u5f71\u54cd\u9ad8\u9891\u4fe1\u9053\u7279\u6027\u3002", "motivation": "\u867d\u7136\u524d\u4eba\u7814\u7a76\u6ce8\u610f\u5230\u5bc4\u751f\u8fd4\u56de\u8def\u5f84\u5728\u7535\u5bb9\u8026\u5408\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\uff0c\u4f46\u5468\u56f4\u91d1\u5c5e\u7269\u4f53\u5bf9\u8fd9\u4e9b\u8def\u5f84\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u5bf9EQS\u65e0\u7ebf\u4fe1\u53f7\u4f20\u8f93\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7406\u8bba\u6846\u67b6\u7ed3\u5408\u6709\u9650\u5143\u65b9\u6cd5\u6a21\u62df\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u7814\u7a76\u5404\u79cd\u5bfc\u7535\u7269\u4f53\uff08\u975e\u63a5\u5730\u91d1\u5c5e\u3001\u63a5\u5730\u91d1\u5c5e\u3001\u5c01\u95ed\u91d1\u5c5e\u73af\u5883\uff09\u5bf9\u4eba\u4f53\u901a\u4fe1\u4fe1\u9053\u7684\u5f71\u54cd\u3002", "result": "\u8ddd\u79bb\u8bbe\u590720cm\u5185\u7684\u91d1\u5c5e\u7269\u4f53\u53ef\u51cf\u5c11\u4f20\u8f93\u635f\u8017\u7ea610dB\uff1b\u8bbe\u5907\u63a5\u5730\u8fde\u63a5\u5230\u63a5\u5730\u91d1\u5c5e\u7269\u4f53\u65f6\uff0c\u4fe1\u9053\u589e\u76ca\u53ef\u589e\u52a0\u81f3\u5c1120dB\uff1b\u89e6\u6478\u63a5\u5730\u91d1\u5c5e\u65f6\u7684\u63a5\u89e6\u9762\u79ef\u4f1a\u4ea7\u751f\u63a5\u89e6\u963b\u6297\u4f9d\u8d56\u7684\u9ad8\u901a\u4fe1\u9053\u7279\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u589e\u8fdb\u4e86\u5bf9\u4eba\u4f53\u4e2d\u5fc3\u901a\u4fe1\u94fe\u8def\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u533b\u7597\u4fdd\u5065\u3001\u6d88\u8d39\u7535\u5b50\u3001\u56fd\u9632\u548c\u5de5\u4e1a\u5e94\u7528\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2510.04780", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04780", "abs": "https://arxiv.org/abs/2510.04780", "authors": ["Arie Wortsman", "Bruno Loureiro"], "title": "Kernel ridge regression under power-law data: spectrum and generalization", "comment": null, "summary": "In this work, we investigate high-dimensional kernel ridge regression (KRR)\non i.i.d. Gaussian data with anisotropic power-law covariance. This setting\ndiffers fundamentally from the classical source & capacity conditions for KRR,\nwhere power-law assumptions are typically imposed on the kernel eigen-spectrum\nitself. Our contributions are twofold. First, we derive an explicit\ncharacterization of the kernel spectrum for polynomial inner-product kernels,\ngiving a precise description of how the kernel eigen-spectrum inherits the data\ndecay. Second, we provide an asymptotic analysis of the excess risk in the\nhigh-dimensional regime for a particular kernel with this spectral behavior,\nshowing that the sample complexity is governed by the effective dimension of\nthe data rather than the ambient dimension. These results establish a\nfundamental advantage of learning with power-law anisotropic data over\nisotropic data. To our knowledge, this is the first rigorous treatment of\nnon-linear KRR under power-law data.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9ad8\u7ef4\u6838\u5cad\u56de\u5f52\u5728\u5177\u6709\u5404\u5411\u5f02\u6027\u5e42\u5f8b\u534f\u65b9\u5dee\u7684\u9ad8\u65af\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u63a8\u5bfc\u4e86\u591a\u9879\u5f0f\u5185\u79ef\u6838\u7684\u6838\u8c31\u7279\u5f81\uff0c\u5e76\u5206\u6790\u4e86\u5728\u8fd9\u79cd\u8c31\u884c\u4e3a\u4e0b\u7684\u8d85\u989d\u98ce\u9669\uff0c\u8868\u660e\u6837\u672c\u590d\u6742\u5ea6\u7531\u6570\u636e\u6709\u6548\u7ef4\u5ea6\u800c\u975e\u73af\u5883\u7ef4\u5ea6\u51b3\u5b9a\u3002", "motivation": "\u7814\u7a76\u9ad8\u7ef4\u6838\u5cad\u56de\u5f52\u5728\u5177\u6709\u5404\u5411\u5f02\u6027\u5e42\u5f8b\u534f\u65b9\u5dee\u7684\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u8fd9\u4e0e\u4f20\u7edfKRR\u7684\u6e90\u548c\u5bb9\u91cf\u6761\u4ef6\u4e0d\u540c\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5bf9\u6838\u7279\u5f81\u8c31\u672c\u8eab\u65bd\u52a0\u5e42\u5f8b\u5047\u8bbe\u3002", "method": "\u63a8\u5bfc\u591a\u9879\u5f0f\u5185\u79ef\u6838\u7684\u6838\u8c31\u663e\u5f0f\u7279\u5f81\u5316\uff0c\u5e76\u5bf9\u5177\u6709\u8fd9\u79cd\u8c31\u884c\u4e3a\u7684\u7279\u5b9a\u6838\u8fdb\u884c\u8d85\u989d\u98ce\u9669\u7684\u6e10\u8fd1\u5206\u6790\u3002", "result": "\u6838\u7279\u5f81\u8c31\u7ee7\u627f\u4e86\u6570\u636e\u7684\u8870\u51cf\u7279\u6027\uff0c\u6837\u672c\u590d\u6742\u5ea6\u7531\u6570\u636e\u6709\u6548\u7ef4\u5ea6\u800c\u975e\u73af\u5883\u7ef4\u5ea6\u51b3\u5b9a\uff0c\u8fd9\u786e\u7acb\u4e86\u5728\u5e42\u5f8b\u5404\u5411\u5f02\u6027\u6570\u636e\u4e0a\u5b66\u4e60\u76f8\u5bf9\u4e8e\u5404\u5411\u540c\u6027\u6570\u636e\u7684\u6839\u672c\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u662f\u5bf9\u5e42\u5f8b\u6570\u636e\u4e0b\u975e\u7ebf\u6027KRR\u7684\u9996\u6b21\u4e25\u683c\u5904\u7406\uff0c\u63ed\u793a\u4e86\u5728\u5177\u6709\u5404\u5411\u5f02\u6027\u5e42\u5f8b\u534f\u65b9\u5dee\u7684\u6570\u636e\u4e0a\u5b66\u4e60\u7684\u57fa\u672c\u4f18\u52bf\u3002"}}
{"id": "2510.03260", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03260", "abs": "https://arxiv.org/abs/2510.03260", "authors": ["Juan Jose Herrera-Aranda", "Guillermo Gomez-Trenado", "Francisco Herrera", "Isaac Triguero"], "title": "Semantic-Inductive Attribute Selection for Zero-Shot Learning", "comment": "26 pages, 9 figures, code available at\n  https://kiedie.github.io/Semantic-Inductive-Attribute-Selection-for-Zero-Shot-Learning/", "summary": "Zero-Shot Learning is an important paradigm within General-Purpose Artificial\nIntelligence Systems, particularly in those that operate in open-world\nscenarios where systems must adapt to new tasks dynamically. Semantic spaces\nplay a pivotal role as they bridge seen and unseen classes, but whether\nhuman-annotated or generated by a machine learning model, they often contain\nnoisy, redundant, or irrelevant attributes that hinder performance. To address\nthis, we introduce a partitioning scheme that simulates unseen conditions in an\ninductive setting (which is the most challenging), allowing attribute relevance\nto be assessed without access to semantic information from unseen classes.\nWithin this framework, we study two complementary feature-selection strategies\nand assess their generalisation. The first adapts embedded feature selection to\nthe particular demands of ZSL, turning model-driven rankings into meaningful\nsemantic pruning; the second leverages evolutionary computation to directly\nexplore the space of attribute subsets more broadly. Experiments on five\nbenchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods\nconsistently improve accuracy on unseen classes by reducing redundancy, but in\ncomplementary ways: RFS is efficient and competitive though dependent on\ncritical hyperparameters, whereas GA is more costly yet explores the search\nspace more broadly and avoids such dependence. These results confirm that\nsemantic spaces are inherently redundant and highlight the proposed\npartitioning scheme as an effective tool to refine them under inductive\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u6765\u4f18\u5316\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u5c5e\u6027\u6765\u63d0\u9ad8\u672a\u89c1\u7c7b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8bed\u4e49\u7a7a\u95f4\u901a\u5e38\u5305\u542b\u566a\u58f0\u3001\u5197\u4f59\u6216\u4e0d\u76f8\u5173\u5c5e\u6027\uff0c\u8fd9\u4f1a\u963b\u788d\u6a21\u578b\u6027\u80fd\u3002\u9700\u8981\u5728\u4e0d\u8bbf\u95ee\u672a\u89c1\u7c7b\u8bed\u4e49\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u5c5e\u6027\u76f8\u5173\u6027\u3002", "method": "\u5f15\u5165\u5206\u533a\u65b9\u6848\u6a21\u62df\u672a\u89c1\u6761\u4ef6\uff0c\u7814\u7a76\u4e24\u79cd\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff1a\u57fa\u4e8e\u5d4c\u5165\u7684\u7279\u5f81\u9009\u62e9\uff08RFS\uff09\u548c\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u7279\u5f81\u9009\u62e9\uff08GA\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u901a\u8fc7\u51cf\u5c11\u5197\u4f59\u6301\u7eed\u63d0\u9ad8\u672a\u89c1\u7c7b\u7684\u51c6\u786e\u7387\uff0cRFS\u9ad8\u6548\u4f46\u4f9d\u8d56\u8d85\u53c2\u6570\uff0cGA\u6210\u672c\u66f4\u9ad8\u4f46\u641c\u7d22\u66f4\u5e7f\u6cdb\u3002", "conclusion": "\u8bed\u4e49\u7a7a\u95f4\u672c\u8d28\u4e0a\u662f\u5197\u4f59\u7684\uff0c\u63d0\u51fa\u7684\u5206\u533a\u65b9\u6848\u662f\u5728\u5f52\u7eb3\u6761\u4ef6\u4e0b\u7cbe\u70bc\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.04413", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.04413", "abs": "https://arxiv.org/abs/2510.04413", "authors": ["Muhammad Umar Farooq Qaisar", "Weijie Yuan", "Onur G\u00fcnl\u00fc", "Taneli Riihonen", "Yuanhao Cui", "Lin Zhang", "Nuria Gonzalez-Prelcic", "Marco Di Renzo", "Zhu Han"], "title": "The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems", "comment": "28 pages, 6 figures, and 5 tables", "summary": "The commencement of the sixth-generation (6G) wireless networks represents a\nfundamental shift in the integration of communication and sensing technologies\nto support next-generation applications. Integrated sensing and communication\n(ISAC) is a key concept in this evolution, enabling end-to-end support for both\ncommunication and sensing within a unified framework. It enhances spectrum\nefficiency, reduces latency, and supports diverse use cases, including smart\ncities, autonomous systems, and perceptive environments. This tutorial provides\na comprehensive overview of ISAC's role in 6G networks, beginning with its\nevolution since 5G and the technical drivers behind its adoption. Core\nprinciples and system variations of ISAC are introduced, followed by an\nin-depth discussion of the enabling technologies that facilitate its practical\ndeployment. The paper further analyzes current research directions to highlight\nkey challenges, open issues, and emerging trends. Design insights and\nrecommendations are also presented to support future development and\nimplementation. This work ultimately try to address three central questions:\nWhy is ISAC essential for 6G? What innovations does it bring? How will it shape\nthe future of wireless communication?", "AI": {"tldr": "\u672c\u6587\u662f\u5173\u4e8e6G\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7684\u6559\u7a0b\uff0c\u9610\u8ff0\u4e86ISAC\u57286G\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3001\u6280\u672f\u539f\u7406\u3001\u5b9e\u73b0\u65b9\u6cd5\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u5c06\u901a\u4fe1\u4e0e\u611f\u77e5\u6280\u672f\u6df1\u5ea6\u878d\u5408\uff0c\u4ee5\u652f\u6301\u4e0b\u4e00\u4ee3\u5e94\u7528\u3002ISAC\u80fd\u591f\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3001\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5e76\u4e3a\u667a\u6167\u57ce\u5e02\u3001\u81ea\u4e3b\u7cfb\u7edf\u7b49\u591a\u6837\u5316\u7528\u4f8b\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u901a\u8fc7\u6982\u8ff0ISAC\u4ece5G\u52306G\u7684\u6f14\u8fdb\u5386\u7a0b\uff0c\u4ecb\u7ecd\u5176\u6838\u5fc3\u539f\u7406\u548c\u7cfb\u7edf\u53d8\u4f53\uff0c\u6df1\u5165\u8ba8\u8bba\u4fc3\u8fdb\u5176\u5b9e\u9645\u90e8\u7f72\u7684\u4f7f\u80fd\u6280\u672f\uff0c\u5e76\u5206\u6790\u5f53\u524d\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86ISAC\u57286G\u7f51\u7edc\u4e2d\u7684\u8bbe\u8ba1\u89c1\u89e3\u548c\u5efa\u8bae\uff0c\u89e3\u51b3\u4e86\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\uff1aISAC\u5bf96G\u7684\u5fc5\u8981\u6027\u3001\u5e26\u6765\u7684\u521b\u65b0\u4ee5\u53ca\u5982\u4f55\u5851\u9020\u65e0\u7ebf\u901a\u4fe1\u7684\u672a\u6765\u3002", "conclusion": "ISAC\u662f6G\u7f51\u7edc\u53d1\u5c55\u7684\u5173\u952e\u6280\u672f\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u6846\u67b6\u5b9e\u73b0\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u7aef\u5230\u7aef\u652f\u6301\uff0c\u5c06\u6df1\u523b\u5f71\u54cd\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2510.04811", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04811", "abs": "https://arxiv.org/abs/2510.04811", "authors": ["Malith Premarathna", "Fabrizio Ruggeri", "Dixon Vimalajeewa"], "title": "A Noise Resilient Approach for Robust Hurst Exponent Estimation", "comment": null, "summary": "Understanding signal behavior across scales is vital in areas such as natural\nphenomena analysis and financial modeling. A key property is self-similarity,\nquantified by the Hurst exponent (H), which reveals long-term dependencies.\nWavelet-based methods are effective for estimating H due to their multi-scale\nanalysis capability, but additive noise in real-world measurements often\ndegrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an\nenhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),\nincorporating noise mitigation and generating multiple level-pairwise estimates\nfrom signal energy pairs. A neural network (NN) combines these estimates,\nreplacing traditional averaging. This adaptive learning maintains ALPHEE's\nbehavior in noise-free cases while improving performance in noisy conditions.\nExtensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's\naccuracy using both averaging and NN-based methods. Under noise, however,\ntraditional averaging deteriorates and requires impractical level restrictions,\nwhile NC-ALPHEE consistently outperforms existing techniques without such\nconstraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,\nsignificantly enhancing the reliability of wavelet-based methods in noisy\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86NC-ALPHEE\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u6291\u5236\u548c\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u591a\u4e2a\u6c34\u5e73\u5bf9\u4f30\u8ba1\u503c\uff0c\u6539\u8fdb\u4e86\u57fa\u4e8e\u5c0f\u6ce2\u7684Hurst\u6307\u6570\u4f30\u8ba1\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6d4b\u91cf\u4e2d\u7684\u52a0\u6027\u566a\u58f0\u4f1a\u964d\u4f4e\u57fa\u4e8e\u5c0f\u6ce2\u7684Hurst\u6307\u6570\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u566a\u58f0\u7684\u9c81\u68d2\u65b9\u6cd5\u3002", "method": "\u5728ALPHEE\u57fa\u7840\u4e0a\u52a0\u5165\u566a\u58f0\u6291\u5236\uff0c\u4ece\u4fe1\u53f7\u80fd\u91cf\u5bf9\u751f\u6210\u591a\u4e2a\u6c34\u5e73\u5bf9\u4f30\u8ba1\u503c\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u5e73\u5747\u65b9\u6cd5\u8fdb\u884c\u7ec4\u5408\u3002", "result": "\u65e0\u566a\u58f0\u6570\u636e\u4e2dNC-ALPHEE\u4e0eALPHEE\u7cbe\u5ea6\u76f8\u5f53\uff1b\u6709\u566a\u58f0\u65f6\u4f20\u7edf\u5e73\u5747\u65b9\u6cd5\u6027\u80fd\u4e0b\u964d\u4e14\u9700\u8981\u4e0d\u5207\u5b9e\u9645\u7684\u6c34\u5e73\u9650\u5236\uff0c\u800cNC-ALPHEE\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "NC-ALPHEE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u81ea\u9002\u5e94Hurst\u6307\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u6ce2\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.03261", "categories": ["cs.LG", "cs.CE", "J.2; I.2"], "pdf": "https://arxiv.org/pdf/2510.03261", "abs": "https://arxiv.org/abs/2510.03261", "authors": ["C. Coelho", "M. Hohmann", "D. Fern\u00e1ndez", "L. Penter", "S. Ihlenfeldt", "O. Niggemann"], "title": "Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark", "comment": null, "summary": "Thermal errors in machine tools significantly impact machining precision and\nproductivity. Traditional thermal error correction/compensation methods rely on\nmeasured temperature-deformation fields or on transfer functions. Most existing\ndata-driven compensation strategies employ neural networks (NNs) to directly\npredict thermal errors or specific compensation values. While effective, these\napproaches are tightly bound to particular error types, spatial locations, or\nmachine configurations, limiting their generality and adaptability. In this\nwork, we introduce a novel paradigm in which NNs are trained to predict\nhigh-fidelity temperature and heat flux fields within the machine tool. The\nproposed framework enables subsequent computation and correction of a wide\nrange of error types using modular, swappable downstream components. The NN is\ntrained using data obtained with the finite element method under varying\ninitial conditions and incorporates a correlation-based selection strategy that\nidentifies the most informative measurement points, minimising hardware\nrequirements during inference. We further benchmark state-of-the-art\ntime-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,\nLong-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal\nConvolutional Network, by training both specialised models, tailored for\nspecific initial conditions, and general models, capable of extrapolating to\nunseen scenarios. The results show accurate and low-cost prediction of\ntemperature and heat flux fields, laying the basis for enabling flexible and\ngeneralisable thermal error correction in machine tool environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u673a\u5e8a\u5185\u90e8\u7684\u9ad8\u4fdd\u771f\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\uff0c\u4e3a\u7075\u6d3b\u901a\u7528\u7684\u70ed\u8bef\u5dee\u6821\u6b63\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u70ed\u8bef\u5dee\u6821\u6b63\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u8bef\u5dee\u7c7b\u578b\u3001\u7a7a\u95f4\u4f4d\u7f6e\u6216\u673a\u5e8a\u914d\u7f6e\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u901a\u7528\u7684\u70ed\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6709\u9650\u5143\u65b9\u6cd5\u83b7\u53d6\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\uff0c\u91c7\u7528\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u6d4b\u91cf\u70b9\u9009\u62e9\u7b56\u7565\u51cf\u5c11\u786c\u4ef6\u9700\u6c42\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u6e29\u5ea6\u548c\u70ed\u901a\u91cf\u573a\u7684\u51c6\u786e\u4f4e\u6210\u672c\u9884\u6d4b\uff0c\u4e3a\u673a\u5e8a\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u901a\u7528\u70ed\u8bef\u5dee\u6821\u6b63\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u6a21\u5757\u5316\u4e0b\u6e38\u7ec4\u4ef6\u8ba1\u7b97\u548c\u6821\u6b63\u591a\u79cd\u8bef\u5dee\u7c7b\u578b\uff0c\u63d0\u9ad8\u4e86\u70ed\u8bef\u5dee\u8865\u507f\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.04492", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04492", "abs": "https://arxiv.org/abs/2510.04492", "authors": ["Zhou Zhang", "Yizhu Wang", "Saman Atapattu", "Sumei Sun"], "title": "Joint Probing and Scheduling for Cache-Aided Hybrid Satellite-Terrestrial Networks", "comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan", "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u505c\u6b62\u7406\u8bba\u7684\u8054\u5408\u63a2\u6d4b\u4e0e\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7LEO\u536b\u661f\u63a2\u6d4b\u536b\u661f-\u5730\u9762\u94fe\u8def\u548c\u591a\u4e2a\u534f\u4f5c\u5730\u9762\u7ad9\u7684\u7f13\u5b58\u72b6\u6001\uff0c\u52a8\u6001\u8c03\u5ea6\u7528\u6237\u8fdb\u884c\u5185\u5bb9\u5206\u53d1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u536b\u661f-\u5730\u9762\u6df7\u5408\u7f51\u7edc\u7684\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u536b\u661f\u7cfb\u7edf\u4e2d\uff0c\u7f13\u5b58\u5bf9\u4e8e\u964d\u4f4e\u5ef6\u8fdf\u3001\u4f18\u5316\u541e\u5410\u91cf\u548c\u63d0\u9ad8\u6570\u636e\u53ef\u7528\u6027\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u6218\u7565\u6027\u7684MAC\u5c42\u6765\u652f\u6301\u536b\u661f-\u5730\u9762\u6df7\u5408\u7f51\u7edc\u4e2d\u7684\u541e\u5410\u91cf\u4f18\u5316\u3002", "method": "\u91c7\u7528\u6700\u4f18\u505c\u6b62\u7406\u8bba\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e24\u7ea7\u4e0d\u5b8c\u5168\u4fe1\u606f\u7684\u8054\u5408\u63a2\u6d4b\u548c\u8c03\u5ea6\u7b56\u7565\u3002\u5229\u7528LEO\u536b\u661f\u63a2\u6d4b\u536b\u661f-\u5730\u9762\u94fe\u8def\u548c\u534f\u4f5c\u5730\u9762\u7ad9\u7684\u7f13\u5b58\u72b6\u6001\uff0c\u5b9e\u73b0\u52a8\u6001\u7528\u6237\u8c03\u5ea6\u548c\u5185\u5bb9\u5206\u53d1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u901a\u8fc7\u5229\u7528\u534f\u4f5c\u7f13\u5b58\u3001\u536b\u661f-\u5730\u9762\u94fe\u8def\u4f20\u8f93\u548c\u52a8\u6001\u7528\u6237\u8bf7\u6c42\u7684\u65f6\u95f4\u5206\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "\u57fa\u4e8e\u9608\u503c\u7684\u6700\u4f18\u505c\u6b62\u7b56\u7565\u80fd\u591f\u6709\u6548\u4f18\u5316\u63a2\u6d4b\u548c\u8c03\u5ea6\u8fc7\u7a0b\uff0c\u5728\u536b\u661f-\u5730\u9762\u6df7\u5408\u7f51\u7edc\u4e2d\u5b9e\u73b0\u541e\u5410\u91cf\u7684\u5927\u5e45\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u534f\u4f5c\u7f13\u5b58\u7b56\u7565\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04926", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04926", "abs": "https://arxiv.org/abs/2510.04926", "authors": ["Eyal Cohen", "Christophe Denis", "Mohamed Hebiri"], "title": "Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification", "comment": null, "summary": "Set-valued classification is used in multiclass settings where confusion\nbetween classes can occur and lead to misleading predictions. However, its\napplication may amplify discriminatory bias motivating the development of\nset-valued approaches under fairness constraints. In this paper, we address the\nproblem of set-valued classification under demographic parity and expected size\nconstraints. We propose two complementary strategies: an oracle-based method\nthat minimizes classification risk while satisfying both constraints, and a\ncomputationally efficient proxy that prioritizes constraint satisfaction. For\nboth strategies, we derive closed-form expressions for the (optimal) fair\nset-valued classifiers and use these to build plug-in, data-driven procedures\nfor empirical predictions. We establish distribution-free convergence rates for\nviolations of the size and fairness constraints for both methods, and under\nmild assumptions we also provide excess-risk bounds for the oracle-based\napproach. Empirical results demonstrate the effectiveness of both strategies\nand highlight the efficiency of our proxy method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u548c\u671f\u671b\u5927\u5c0f\u7ea6\u675f\u4e0b\u7684\u96c6\u5408\u503c\u5206\u7c7b\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8eoracle\u7684\u65b9\u6cd5\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u4ee3\u7406\u65b9\u6cd5\u3002", "motivation": "\u96c6\u5408\u503c\u5206\u7c7b\u5728\u591a\u7c7b\u8bbe\u7f6e\u4e2d\u53ef\u80fd\u653e\u5927\u6b67\u89c6\u6027\u504f\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u5728\u516c\u5e73\u7ea6\u675f\u4e0b\u5f00\u53d1\u96c6\u5408\u503c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\uff1a\u57fa\u4e8eoracle\u7684\u65b9\u6cd5\uff08\u6700\u5c0f\u5316\u5206\u7c7b\u98ce\u9669\u540c\u65f6\u6ee1\u8db3\u7ea6\u675f\uff09\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u4ee3\u7406\u65b9\u6cd5\uff08\u4f18\u5148\u6ee1\u8db3\u7ea6\u675f\uff09\u3002", "result": "\u63a8\u5bfc\u4e86\u516c\u5e73\u96c6\u5408\u503c\u5206\u7c7b\u5668\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5efa\u7acb\u4e86\u5206\u5e03\u65e0\u5173\u7684\u6536\u655b\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8eoracle\u65b9\u6cd5\u7684\u8d85\u989d\u98ce\u9669\u754c\u9650\u3002", "conclusion": "\u5b9e\u8bc1\u7ed3\u679c\u8bc1\u660e\u4e86\u4e24\u79cd\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u4ee3\u7406\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2510.03262", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03262", "abs": "https://arxiv.org/abs/2510.03262", "authors": ["Andi Zhang", "Xuan Ding", "Haofan Wang", "Steven McDonagh", "Samuel Kaski"], "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout", "comment": null, "summary": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict\northogonality when combining sparse semantic vectors without extra time\ncomplexity. LoRA, a popular fine-tuning method for large models, typically\ntrains a module to represent a specific concept such as an object or a style.\nWhen multiple LoRAs are merged, for example to generate an object in a\nparticular style, their semantic vectors may interfere with each other. Our\nmethod guarantees, at the theoretical and runtime levels, that merged LoRAs\nremain orthogonal and thus free from direct interference. However, empirical\nanalysis reveals that such orthogonality does not lead to the semantic\ndisentanglement or compositionality highlighted in prior work on compositional\nadaptation. This finding suggests that inter-LoRA orthogonality alone may be\ninsufficient for achieving true semantic compositionality, prompting a\nre-examination of its role in adapter merging.", "AI": {"tldr": "\u63d0\u51fa\u6b63\u4ea4\u8499\u7279\u5361\u6d1bDropout\u65b9\u6cd5\uff0c\u5728\u5408\u5e76LoRA\u6a21\u5757\u65f6\u5f3a\u5236\u6b63\u4ea4\u6027\u4ee5\u907f\u514d\u8bed\u4e49\u5411\u91cf\u5e72\u6270\uff0c\u4f46\u5b9e\u8bc1\u53d1\u73b0\u6b63\u4ea4\u6027\u672c\u8eab\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u8bed\u4e49\u89e3\u8026\u548c\u7ec4\u5408\u6027\u3002", "motivation": "LoRA\u6a21\u5757\u5408\u5e76\u65f6\u8bed\u4e49\u5411\u91cf\u4f1a\u76f8\u4e92\u5e72\u6270\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u4fdd\u8bc1\u5408\u5e76\u540e\u7684\u6b63\u4ea4\u6027\u4ee5\u907f\u514d\u76f4\u63a5\u5e72\u6270\u3002", "method": "\u6b63\u4ea4\u8499\u7279\u5361\u6d1bDropout\uff0c\u5728\u4e0d\u589e\u52a0\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\u5f3a\u5236\u7a00\u758f\u8bed\u4e49\u5411\u91cf\u7ec4\u5408\u65f6\u7684\u4e25\u683c\u6b63\u4ea4\u6027\u3002", "result": "\u7406\u8bba\u4e0a\u548c\u8fd0\u884c\u65f6\u90fd\u80fd\u4fdd\u8bc1\u5408\u5e76LoRA\u7684\u6b63\u4ea4\u6027\uff0c\u4f46\u5b9e\u8bc1\u5206\u6790\u663e\u793a\u6b63\u4ea4\u6027\u5e76\u4e0d\u80fd\u5e26\u6765\u8bed\u4e49\u89e3\u8026\u6216\u7ec4\u5408\u6027\u3002", "conclusion": "\u4ec5\u9760LoRA\u95f4\u7684\u6b63\u4ea4\u6027\u53ef\u80fd\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u4e49\u7ec4\u5408\u6027\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u5176\u5728\u9002\u914d\u5668\u5408\u5e76\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2510.04530", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04530", "abs": "https://arxiv.org/abs/2510.04530", "authors": ["Gayathri Shekar", "Saman Atapattu", "Prathapasinghe Dharmawansa", "Kandeepan Sithamparanathan"], "title": "Performance Analysis for Multi-User Holographic MIMO Downlink with Matched Filter Precoding", "comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan", "summary": "Holographic MIMO (HMIMO) has emerged as a promising solution for future\nwireless systems by enabling ultra-dense, spatially continuous antenna\ndeployments. While prior studies have primarily focused on electromagnetic (EM)\nmodeling or simulation-based performance analysis, a rigorous\ncommunication-theoretic framework remains largely unexplored. This paper\npresents the first analytical performance study of a multi-user HMIMO downlink\nsystem with matched filter (MF) precoding - a low-complexity baseline scheme.\nBy incorporating multipath propagation, mutual coupling, and element\nexcitation, we derive a novel closed-form expression for the MF\nsignal-to-interference-plus-noise ratio (SINR) using an equivalent random\nvariable model. Leveraging bivariate gamma distributions, we then develop\ntractable throughput approximations under full, partial, and no channel state\ninformation (CSI) scenarios. Additionally, we formulate a max-min beamforming\nproblem to benchmark optimal user fairness performance. Numerical results\nvalidate the accuracy of the proposed framework and reveal that MF precoding\nachieves competitive performance with strong robustness to low SINR and CSI\nuncertainty.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u591a\u7528\u6237\u5168\u606fMIMO\u4e0b\u884c\u7cfb\u7edf\u8fdb\u884c\u4e86\u5206\u6790\u6027\u80fd\u7814\u7a76\uff0c\u63a8\u5bfc\u4e86\u5339\u914d\u6ee4\u6ce2\u9884\u7f16\u7801\u7684\u95ed\u5f0fSINR\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u5b8c\u6574\u3001\u90e8\u5206\u548c\u65e0CSI\u573a\u666f\u4e0b\u7684\u541e\u5410\u91cf\u8fd1\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u867d\u7136\u5168\u606fMIMO\u5728\u7535\u78c1\u5efa\u6a21\u548c\u4eff\u771f\u65b9\u9762\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u7f3a\u4e4f\u4e25\u8c28\u7684\u901a\u4fe1\u7406\u8bba\u6846\u67b6\u6765\u5206\u6790\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7b49\u6548\u968f\u673a\u53d8\u91cf\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u5f84\u4f20\u64ad\u3001\u4e92\u8026\u5408\u548c\u5143\u4ef6\u6fc0\u52b1\uff0c\u63a8\u5bfc\u5339\u914d\u6ee4\u6ce2\u9884\u7f16\u7801\u7684\u95ed\u5f0fSINR\u8868\u8fbe\u5f0f\uff0c\u5e76\u5229\u7528\u53cc\u53d8\u91cf\u4f3d\u9a6c\u5206\u5e03\u5f00\u53d1\u53ef\u5904\u7406\u7684\u541e\u5410\u91cf\u8fd1\u4f3c\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u6846\u67b6\u7684\u51c6\u786e\u6027\uff0c\u5e76\u663e\u793a\u5339\u914d\u6ee4\u6ce2\u9884\u7f16\u7801\u5728\u4f4eSINR\u548cCSI\u4e0d\u786e\u5b9a\u6027\u4e0b\u5177\u6709\u7ade\u4e89\u6027\u6027\u80fd\u548c\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u5339\u914d\u6ee4\u6ce2\u9884\u7f16\u7801\u5728\u5168\u606fMIMO\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4fe1\u9053\u6761\u4ef6\u4e0b\u3002"}}
{"id": "2510.04970", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04970", "abs": "https://arxiv.org/abs/2510.04970", "authors": ["Marcel Wien\u00f6bst", "Leonard Henckel", "Sebastian Weichwald"], "title": "Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning", "comment": null, "summary": "We present FLOP (Fast Learning of Order and Parents), a score-based causal\ndiscovery algorithm for linear models. It pairs fast parent selection with\niterative Cholesky-based score updates, cutting run-times over prior\nalgorithms. This makes it feasible to fully embrace discrete search, enabling\niterated local search with principled order initialization to find graphs with\nscores at or close to the global optimum. The resulting structures are highly\naccurate across benchmarks, with near-perfect recovery in standard settings.\nThis performance calls for revisiting discrete search over graphs as a\nreasonable approach to causal discovery.", "AI": {"tldr": "FLOP\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u7684\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5feb\u901f\u7236\u8282\u70b9\u9009\u62e9\u548c\u8fed\u4ee3Cholesky\u5206\u6570\u66f4\u65b0\uff0c\u663e\u8457\u51cf\u5c11\u8fd0\u884c\u65f6\u95f4\uff0c\u4f7f\u79bb\u6563\u641c\u7d22\u53d8\u5f97\u53ef\u884c\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7ed3\u6784\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u8fd0\u884c\u65f6\u95f4\u8f83\u957f\uff0c\u9650\u5236\u4e86\u79bb\u6563\u641c\u7d22\u7684\u5e94\u7528\u3002FLOP\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u6548\u7387\uff0c\u4f7f\u79bb\u6563\u641c\u7d22\u6210\u4e3a\u56e0\u679c\u53d1\u73b0\u7684\u53ef\u884c\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5feb\u901f\u7236\u8282\u70b9\u9009\u62e9\u4e0e\u8fed\u4ee3Cholesky\u5206\u6570\u66f4\u65b0\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u8fed\u4ee3\u5c40\u90e8\u641c\u7d22\u548c\u57fa\u4e8e\u539f\u5219\u7684\u987a\u5e8f\u521d\u59cb\u5316\uff0c\u4ee5\u627e\u5230\u63a5\u8fd1\u5168\u5c40\u6700\u4f18\u7684\u56fe\u7ed3\u6784\u3002", "result": "\u5728\u6807\u51c6\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u7ed3\u6784\u6062\u590d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u8fd0\u884c\u65f6\u95f4\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7b97\u6cd5\u3002", "conclusion": "FLOP\u7684\u6210\u529f\u8868\u660e\u79bb\u6563\u641c\u7d22\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u662f\u4e00\u4e2a\u5408\u7406\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u503c\u5f97\u91cd\u65b0\u5ba1\u89c6\u5176\u5728\u9886\u57df\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.03263", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03263", "abs": "https://arxiv.org/abs/2510.03263", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Joanna Waczy\u0144ska", "Piotr Borycki", "Przemys\u0142aw Spurek"], "title": "Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models", "comment": null, "summary": "The impressive capability of modern text-to-image models to generate\nrealistic visuals has come with a serious drawback: they can be misused to\ncreate harmful, deceptive or unlawful content. This has accelerated the push\nfor machine unlearning. This new field seeks to selectively remove specific\nknowledge from a model's training data without causing a drop in its overall\nperformance. However, it turns out that actually forgetting a given concept is\nan extremely difficult task. Models exposed to attacks using adversarial\nprompts show the ability to generate so-called unlearned concepts, which can be\nnot only harmful but also illegal. In this paper, we present considerations\nregarding the ability of models to forget and recall knowledge, introducing the\nMemory Self-Regeneration task. Furthermore, we present MemoRa strategy, which\nwe consider to be a regenerative approach supporting the effective recovery of\npreviously lost knowledge. Moreover, we propose that robustness in knowledge\nretrieval is a crucial yet underexplored evaluation measure for developing more\nrobust and effective unlearning techniques. Finally, we demonstrate that\nforgetting occurs in two distinct ways: short-term, where concepts can be\nquickly recalled, and long-term, where recovery is more challenging.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u673a\u5668\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u8bb0\u5fc6\u81ea\u6211\u518d\u751f\u4efb\u52a1\u548cMemoRa\u7b56\u7565\u6765\u6062\u590d\u88ab\u9057\u5fd8\u7684\u77e5\u8bc6\uff0c\u5e76\u53d1\u73b0\u9057\u5fd8\u5b58\u5728\u77ed\u671f\u548c\u957f\u671f\u4e24\u79cd\u6a21\u5f0f\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u903c\u771f\u56fe\u50cf\u7684\u80fd\u529b\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u521b\u5efa\u6709\u5bb3\u5185\u5bb9\uff0c\u8fd9\u63a8\u52a8\u4e86\u673a\u5668\u9057\u5fd8\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f46\u5b9e\u9645\u9057\u5fd8\u7279\u5b9a\u6982\u5ff5\u975e\u5e38\u56f0\u96be\uff0c\u6a21\u578b\u4ecd\u80fd\u901a\u8fc7\u5bf9\u6297\u63d0\u793a\u751f\u6210\u672c\u5e94\u88ab\u9057\u5fd8\u7684\u6982\u5ff5\u3002", "method": "\u5f15\u5165\u8bb0\u5fc6\u81ea\u6211\u518d\u751f\u4efb\u52a1\uff0c\u63d0\u51faMemoRa\u7b56\u7565\u4f5c\u4e3a\u518d\u751f\u65b9\u6cd5\u652f\u6301\u6709\u6548\u6062\u590d\u5148\u524d\u4e22\u5931\u7684\u77e5\u8bc6\uff0c\u5e76\u5f3a\u8c03\u77e5\u8bc6\u68c0\u7d22\u9c81\u68d2\u6027\u4f5c\u4e3a\u91cd\u8981\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u7814\u7a76\u8868\u660e\u9057\u5fd8\u4ee5\u4e24\u79cd\u4e0d\u540c\u65b9\u5f0f\u53d1\u751f\uff1a\u77ed\u671f\u9057\u5fd8\u53ef\u4ee5\u5feb\u901f\u56de\u5fc6\uff0c\u800c\u957f\u671f\u9057\u5fd8\u6062\u590d\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u77e5\u8bc6\u68c0\u7d22\u9c81\u68d2\u6027\u662f\u5f00\u53d1\u66f4\u5f3a\u5927\u6709\u6548\u9057\u5fd8\u6280\u672f\u7684\u5173\u952e\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u9057\u5fd8\u8fc7\u7a0b\u5b58\u5728\u77ed\u671f\u548c\u957f\u671f\u4e24\u79cd\u6a21\u5f0f\u3002"}}
{"id": "2510.04600", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04600", "abs": "https://arxiv.org/abs/2510.04600", "authors": ["Meidong Xia", "Zhenyao He", "Wei Xu", "Yongming Huang", "Derrick Wing Kwan Ng", "Naofal Al-Dhahir"], "title": "Coordinated Beamforming for Networked Integrated Communication and Multi-TMT Localization", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Networked integrated sensing and communication (ISAC) has gained significant\nattention as a promising technology for enabling next-generation wireless\nsystems. To further enhance networked ISAC, delegating the reception of sensing\nsignals to dedicated target monitoring terminals (TMTs) instead of base\nstations (BSs) offers significant advantages in terms of sensing capability and\ndeployment flexibility. Despite its potential, the coordinated beamforming\ndesign for networked integrated communication and time-of-arrival (ToA)-based\nmulti-TMT localization remains largely unexplored. In this paper, we present a\ncomprehensive study to fill this gap. Specifically, we first establish signal\nmodels for both communication and localization, and, for the first time, derive\na closed-form Cram\\'er-Rao lower bound (CRLB) to characterize the localization\nperformance. Subsequently, we exploit this CRLB to formulate two optimization\nproblems, focusing on sensing-centric and communication-centric criteria,\nrespectively. For the sensing-centric problem, we develop a globally optimal\nalgorithm based on semidefinite relaxation (SDR) when each BS is equipped with\nmore antennas than the total number of communication users. While for the\ncommunication-centric problem, we design a globally optimal algorithm for the\nsingle-BS case using bisection search. For the general case of both problems,\nwe propose a unified successive convex approximation (SCA)-based algorithm,\nwhich is suboptimal yet efficient, and further extend it from single-target\nscenarios to more practical multi-target scenarios. Finally, simulation results\ndemonstrate the effectiveness of our proposed algorithms, reveal the intrinsic\nperformance trade-offs between communication and localization, and further show\nthat deploying more TMTs is always preferable to deploying more BSs in\nnetworked ISAC systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7f51\u7edc\u5316\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7cfb\u7edf\u4e2d\u7684\u534f\u8c03\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u91cd\u70b9\u5173\u6ce8\u57fa\u4e8e\u5230\u8fbe\u65f6\u95f4(ToA)\u7684\u591a\u76ee\u6807\u76d1\u6d4b\u7ec8\u7aef(TMT)\u5b9a\u4f4d\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u4f18\u5316\u95ee\u9898(\u611f\u77e5\u4e2d\u5fc3\u548c\u901a\u4fe1\u4e2d\u5fc3)\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u5168\u5c40\u6700\u4f18\u548c\u6b21\u4f18\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u7f51\u7edc\u5316ISAC\u7cfb\u7edf\u5c06\u611f\u77e5\u4fe1\u53f7\u63a5\u6536\u59d4\u6258\u7ed9\u4e13\u7528\u76ee\u6807\u76d1\u6d4b\u7ec8\u7aef\u800c\u975e\u57fa\u7ad9\uff0c\u53ef\u663e\u8457\u63d0\u5347\u611f\u77e5\u80fd\u529b\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u3002\u7136\u800c\uff0c\u9488\u5bf9\u7f51\u7edc\u5316\u96c6\u6210\u901a\u4fe1\u548c\u57fa\u4e8eToA\u7684\u591aTMT\u5b9a\u4f4d\u7684\u534f\u8c03\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u9996\u5148\u5efa\u7acb\u4e86\u901a\u4fe1\u548c\u5b9a\u4f4d\u7684\u4fe1\u53f7\u6a21\u578b\uff0c\u9996\u6b21\u63a8\u5bfc\u4e86\u5b9a\u4f4d\u6027\u80fd\u7684\u95ed\u5f0f\u514b\u62c9\u7f8e-\u7f57\u4e0b\u754c(CRLB)\u3002\u5229\u7528CRLB\u5236\u5b9a\u4e86\u4e24\u79cd\u4f18\u5316\u95ee\u9898\uff1a\u611f\u77e5\u4e2d\u5fc3\u548c\u901a\u4fe1\u4e2d\u5fc3\u3002\u9488\u5bf9\u611f\u77e5\u4e2d\u5fc3\u95ee\u9898\uff0c\u5728\u57fa\u7ad9\u5929\u7ebf\u6570\u591a\u4e8e\u901a\u4fe1\u7528\u6237\u603b\u6570\u65f6\u5f00\u53d1\u4e86\u57fa\u4e8e\u534a\u5b9a\u677e\u5f1b(SDR)\u7684\u5168\u5c40\u6700\u4f18\u7b97\u6cd5\uff1b\u9488\u5bf9\u901a\u4fe1\u4e2d\u5fc3\u95ee\u9898\uff0c\u5728\u5355\u57fa\u7ad9\u60c5\u51b5\u4e0b\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u4e8c\u5206\u641c\u7d22\u7684\u5168\u5c40\u6700\u4f18\u7b97\u6cd5\u3002\u5bf9\u4e8e\u4e00\u822c\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8fde\u7eed\u51f8\u8fd1\u4f3c(SCA)\u7684\u7edf\u4e00\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u901a\u4fe1\u4e0e\u5b9a\u4f4d\u4e4b\u95f4\u7684\u5185\u5728\u6027\u80fd\u6743\u8861\uff0c\u5e76\u8fdb\u4e00\u6b65\u53d1\u73b0\u5728\u7f51\u7edc\u5316ISAC\u7cfb\u7edf\u4e2d\u90e8\u7f72\u66f4\u591aTMT\u6bd4\u90e8\u7f72\u66f4\u591a\u57fa\u7ad9\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7f51\u7edc\u5316\u96c6\u6210\u901a\u4fe1\u548c\u57fa\u4e8eToA\u7684\u591aTMT\u5b9a\u4f4d\u534f\u8c03\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u548c\u6027\u80fd\u5206\u6790\u4e3a\u7f51\u7edc\u5316ISAC\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2510.05013", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05013", "abs": "https://arxiv.org/abs/2510.05013", "authors": ["Theodore Jerome Tinker", "Kenji Doya", "Jun Tani"], "title": "Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration", "comment": "26 pages, 14 pages of supplementary material", "summary": "Human infants acquire language and action co-developmentally, achieving\nremarkable generalization capabilities from only a minimal number of learning\nexamples. In contrast, recent large language models require exposure to\nbillions of training tokens to achieve such generalization. What mechanisms\nunderlie such efficient developmental learning in humans? This study addresses\nthis question through simulation experiments in which robots learn to perform\nvarious actions corresponding to imperative sentences (e.g., \\textit{push red\ncube}) via trials of self-guided exploration. Our approach integrates the\nactive inference framework with reinforcement learning, enabling\ncuriosity-driven developmental learning. The simulations yielded several\nnontrivial findings: i) Curiosity-driven exploration combined with motor noise\nsubstantially outperforms learning without curiosity. ii) Simpler,\nprerequisite-like actions emerge earlier in development, while more complex\nactions involving these prerequisites develop later. iii) Rote pairing of\nsentences and actions occurs before the emergence of compositional\ngeneralization. iv) Generalization is drastically improved as the number of\ncompositional elements increases. These results shed light into possible\nmechanisms underlying efficient co-developmental learning in infants and\nprovide computational parallels to findings in developmental psychology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u673a\u5668\u4eba\u6a21\u62df\u5b9e\u9a8c\u63a2\u7d22\u4eba\u7c7b\u5a74\u513f\u9ad8\u6548\u8bed\u8a00\u548c\u52a8\u4f5c\u534f\u540c\u53d1\u5c55\u7684\u673a\u5236\uff0c\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u53d1\u80b2\u5b66\u4e60\uff0c\u53d1\u73b0\u597d\u5947\u5fc3\u63a2\u7d22\u3001\u52a8\u4f5c\u53d1\u5c55\u987a\u5e8f\u3001\u7ec4\u5408\u6cdb\u5316\u7b49\u5173\u952e\u673a\u5236\u3002", "motivation": "\u63a2\u7a76\u4eba\u7c7b\u5a74\u513f\u4e3a\u4f55\u80fd\u4ece\u5c11\u91cf\u5b66\u4e60\u6837\u672c\u4e2d\u83b7\u5f97\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6570\u5341\u4ebf\u8bad\u7ec3\u6807\u8bb0\u624d\u80fd\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\uff0c\u63ed\u793a\u9ad8\u6548\u53d1\u80b2\u5b66\u4e60\u7684\u6f5c\u5728\u673a\u5236\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\uff0c\u901a\u8fc7\u81ea\u4e3b\u63a2\u7d22\u5b66\u4e60\u6267\u884c\u547d\u4ee4\u53e5\u5bf9\u5e94\u7684\u52a8\u4f5c\uff0c\u6574\u5408\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u53d1\u80b2\u5b66\u4e60\u3002", "result": "\u53d1\u73b0\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\u663e\u8457\u4f18\u4e8e\u65e0\u597d\u5947\u5fc3\u5b66\u4e60\uff1b\u7b80\u5355\u5148\u51b3\u52a8\u4f5c\u5148\u53d1\u5c55\uff0c\u590d\u6742\u52a8\u4f5c\u540e\u53d1\u5c55\uff1b\u673a\u68b0\u914d\u5bf9\u5148\u4e8e\u7ec4\u5408\u6cdb\u5316\u51fa\u73b0\uff1b\u7ec4\u5408\u5143\u7d20\u8d8a\u591a\u6cdb\u5316\u80fd\u529b\u8d8a\u5f3a\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5a74\u513f\u9ad8\u6548\u534f\u540c\u5b66\u4e60\u53ef\u80fd\u7684\u8ba1\u7b97\u673a\u5236\uff0c\u4e3a\u53d1\u80b2\u5fc3\u7406\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u8ba1\u7b97\u5c42\u9762\u7684\u5e73\u884c\u8bc1\u636e\uff0c\u5c55\u793a\u4e86\u597d\u5947\u5fc3\u9a71\u52a8\u63a2\u7d22\u5728\u8bed\u8a00\u52a8\u4f5c\u534f\u540c\u53d1\u5c55\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2510.03264", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03264", "abs": "https://arxiv.org/abs/2510.03264", "authors": ["Syeda Nahida Akter", "Shrimai Prabhumoye", "Eric Nyberg", "Mostofa Patwary", "Mohammad Shoeybi", "Yejin Choi", "Bryan Catanzaro"], "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data", "comment": null, "summary": "The prevailing paradigm for enhancing the reasoning abilities of LLMs\nrevolves around post-training on high-quality, reasoning-intensive data. While\nemerging literature suggests that reasoning data is increasingly incorporated\nalso during the mid-training stage-a practice that is relatively more\nproprietary and less openly characterized-the role of such data in pretraining\nremains unclear. In particular, due to the opaqueness of pretraining corpora in\nmost frontier models, the effect of reasoning data introduced at different\nphases of pre- and/or post-training is relatively less reported in the\nscientific literature. This raises several important questions: Is adding\nreasoning data earlier during pretraining any better than introducing it during\npost-training? Could earlier inclusion risk overfitting and harm\ngeneralization, or instead establish durable foundations that later fine-tuning\ncannot recover? We conduct the first systematic study of how reasoning\ndata-varying in scale, diversity, and quality-affects LLM performance when\nintroduced at different stages of training. We find that front-loading\nreasoning data into pretraining is critical (19% avg gain), establishing\nfoundational capabilities that cannot be fully replicated by later-stage SFT,\neven with more data. We uncover an asymmetric principle for optimal data\nallocation: pretraining benefits most from broad diversity in reasoning\npatterns (11% avg gain), while SFT is more sensitive to data quality (15% avg\ngain). We show that high-quality pretraining data has latent effects, activated\nonly after SFT, and that naively scaling SFT data can be detrimental, washing\naway the benefits of early reasoning injection. Our results challenge the\nconventional separation of language modeling and reasoning, providing a\nprincipled guide for strategically allocating data across the entire training\npipeline to build more capable models.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u63a8\u7406\u6570\u636e\u5728\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u4e0d\u540c\u9636\u6bb5\u7684\u5f15\u5165\u6548\u679c\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u81f3\u5173\u91cd\u8981\uff08\u5e73\u5747\u63d0\u534719%\uff09\uff0c\u80fd\u5efa\u7acb\u540e\u671fSFT\u65e0\u6cd5\u5b8c\u5168\u590d\u5236\u7684\u57fa\u7840\u80fd\u529b\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u5206\u914d\u7684\u4e0d\u5bf9\u79f0\u539f\u5219\uff1a\u9884\u8bad\u7ec3\u6700\u53d7\u76ca\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u591a\u6837\u6027\uff0c\u800cSFT\u5bf9\u6570\u636e\u8d28\u91cf\u66f4\u654f\u611f\u3002", "motivation": "\u5f53\u524d\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u6d41\u65b9\u6cd5\u662f\u5728\u540e\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\uff0c\u4f46\u63a8\u7406\u6570\u636e\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\u3002\u7531\u4e8e\u524d\u6cbf\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u8bed\u6599\u4e0d\u900f\u660e\uff0c\u4e0d\u540c\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u7684\u6548\u679c\u5728\u79d1\u5b66\u6587\u732e\u4e2d\u8f83\u5c11\u62a5\u9053\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u63a8\u7406\u6570\u636e\uff08\u89c4\u6a21\u3001\u591a\u6837\u6027\u548c\u8d28\u91cf\u53d8\u5316\uff09\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u9636\u6bb5\u548c\u540e\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u7684\u6548\u679c\u5dee\u5f02\u3002", "result": "\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u80fd\u5e26\u676519%\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\uff0c\u5efa\u7acb\u7684\u57fa\u7840\u80fd\u529b\u65e0\u6cd5\u88ab\u540e\u671fSFT\u5b8c\u5168\u590d\u5236\u3002\u9884\u8bad\u7ec3\u6700\u53d7\u76ca\u4e8e\u63a8\u7406\u6a21\u5f0f\u591a\u6837\u6027\uff0811%\u5e73\u5747\u589e\u76ca\uff09\uff0cSFT\u5bf9\u6570\u636e\u8d28\u91cf\u66f4\u654f\u611f\uff0815%\u5e73\u5747\u589e\u76ca\uff09\u3002\u9ad8\u8d28\u91cf\u9884\u8bad\u7ec3\u6570\u636e\u5177\u6709\u6f5c\u5728\u6548\u5e94\uff0c\u4ec5\u5728SFT\u540e\u88ab\u6fc0\u6d3b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u8bed\u8a00\u5efa\u6a21\u4e0e\u63a8\u7406\u7684\u4f20\u7edf\u5206\u79bb\uff0c\u4e3a\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u6218\u7565\u6027\u5206\u914d\u6570\u636e\u4ee5\u6784\u5efa\u66f4\u5f3a\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\u3002\u9884\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u63a8\u7406\u6570\u636e\u5bf9\u5efa\u7acb\u57fa\u7840\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.04734", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04734", "abs": "https://arxiv.org/abs/2510.04734", "authors": ["Juan Vidal Alegr\u00eda"], "title": "Dimensionally-Efficient Transmission and Storage of Unitary Matrices", "comment": "13 pages, 10 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Unitary matrices are the basis of a large number of signal processing\napplications. In many of these applications, finding ways to efficiently store,\nand even transmit these matrices, can significantly reduce memory and\nthroughput requirements. In this work, we study the problem of efficient\ntransmission and storage of unitary matrices. Specifically, we explicitly\nderive a dimensionally-efficient parametrization (DEP) for unitary matrices\nthat allows identifying them with sequences of real numbers, where the\ndimension coincides with the dimension of the unitary group where they lie. We\nalso characterize its inverse map that allows retrieving the original unitary\nmatrices from their DEP. The proposed approach effectively allows halving the\ndimension with respect to naively considering all the entries of each unitary\nmatrix, thus reducing the resources required to store and transmit these\nmatrices. Furthermore, we show that the sequence of real numbers associated to\nthe proposed DEP is bounded, and we delimit the interval where these numbers\nare contained, facilitating the implementation of quantization approaches with\nlimited distortion. On the other hand, we outline ways to further reduce the\ndimension of the DEP when considering more restrictive constraints for matrices\nthat show up in certain applications. The numerical results showcase the\npotential of the proposed approach in general settings, as well as in three\nspecific applications of current interest for wireless communications research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ef4\u5ea6\u9ad8\u6548\u53c2\u6570\u5316(DEP)\u65b9\u6cd5\uff0c\u53ef\u5c06\u9149\u77e9\u9635\u8868\u793a\u4e3a\u5b9e\u6570\u5e8f\u5217\uff0c\u7ef4\u5ea6\u4e0e\u9149\u7fa4\u7ef4\u5ea6\u4e00\u81f4\uff0c\u6709\u6548\u5c06\u5b58\u50a8\u548c\u4f20\u8f93\u9700\u6c42\u51cf\u534a\u3002", "motivation": "\u9149\u77e9\u9635\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u9ad8\u3002\u9700\u8981\u627e\u5230\u9ad8\u6548\u8868\u793a\u65b9\u6cd5\u6765\u51cf\u5c11\u5185\u5b58\u548c\u541e\u5410\u91cf\u9700\u6c42\u3002", "method": "\u663e\u5f0f\u63a8\u5bfc\u9149\u77e9\u9635\u7684\u7ef4\u5ea6\u9ad8\u6548\u53c2\u6570\u5316\uff0c\u5efa\u7acb\u9149\u77e9\u9635\u4e0e\u5b9e\u6570\u5e8f\u5217\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u7ed9\u51fa\u9006\u6620\u5c04\u65b9\u6cd5\u3002\u540c\u65f6\u5206\u6790\u4e86\u53c2\u6570\u7684\u6709\u754c\u6027\u548c\u91cf\u5316\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684DEP\u65b9\u6cd5\u80fd\u5c06\u9149\u77e9\u9635\u7684\u7ef4\u5ea6\u51cf\u534a\uff0c\u53c2\u6570\u5e8f\u5217\u6709\u754c\u4e14\u6613\u4e8e\u91cf\u5316\uff0c\u5728\u65e0\u7ebf\u901a\u4fe1\u7b49\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9149\u77e9\u9635\u7684\u9ad8\u6548\u5b58\u50a8\u548c\u4f20\u8f93\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.05033", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05033", "abs": "https://arxiv.org/abs/2510.05033", "authors": ["Markus Englberger", "Devendra Singh Dhami"], "title": "Causal Abstractions, Categorically Unified", "comment": null, "summary": "We present a categorical framework for relating causal models that represent\nthe same system at different levels of abstraction. We define a causal\nabstraction as natural transformations between appropriate Markov functors,\nwhich concisely consolidate desirable properties a causal abstraction should\nexhibit. Our approach unifies and generalizes previously considered causal\nabstractions, and we obtain categorical proofs and generalizations of existing\nresults on causal abstractions. Using string diagrammatical tools, we can\nexplicitly describe the graphs that serve as consistent abstractions of a\nlow-level graph under interventions. We discuss how methods from mechanistic\ninterpretability, such as circuit analysis and sparse autoencoders, fit within\nour categorical framework. We also show how applying do-calculus on a\nhigh-level graphical abstraction of an acyclic-directed mixed graph (ADMG),\nwhen unobserved confounders are present, gives valid results on the low-level\ngraph, thus generalizing an earlier statement by Anand et al. (2023). We argue\nthat our framework is more suitable for modeling causal abstractions compared\nto existing categorical frameworks. Finally, we discuss how notions such as\n$\\tau$-consistency and constructive $\\tau$-abstractions can be recovered with\nour framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5173\u8054\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u56e0\u679c\u6a21\u578b\u7684\u8303\u7574\u6846\u67b6\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51fd\u5b50\u7684\u81ea\u7136\u53d8\u6362\u5b9a\u4e49\u56e0\u679c\u62bd\u8c61\uff0c\u7edf\u4e00\u5e76\u63a8\u5e7f\u4e86\u73b0\u6709\u56e0\u679c\u62bd\u8c61\u6982\u5ff5\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u62bd\u8c61\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u9700\u8981\u5efa\u7acb\u80fd\u591f\u6574\u5408\u5404\u79cd\u56e0\u679c\u62bd\u8c61\u6027\u8d28\u5e76\u8bc1\u660e\u73b0\u6709\u7ed3\u679c\u7684\u7cfb\u7edf\u5316\u7406\u8bba\u3002", "method": "\u4f7f\u7528\u8303\u7574\u8bba\u5de5\u5177\uff0c\u5b9a\u4e49\u56e0\u679c\u62bd\u8c61\u4e3a\u9a6c\u5c14\u53ef\u592b\u51fd\u5b50\u95f4\u7684\u81ea\u7136\u53d8\u6362\uff0c\u5229\u7528\u5f26\u56fe\u5de5\u5177\u63cf\u8ff0\u5e72\u9884\u4e0b\u7684\u4e00\u81f4\u6027\u62bd\u8c61\u56fe\u3002", "result": "\u7edf\u4e00\u5e76\u63a8\u5e7f\u4e86\u73b0\u6709\u56e0\u679c\u62bd\u8c61\u6982\u5ff5\uff0c\u83b7\u5f97\u4e86\u73b0\u6709\u7ed3\u679c\u7684\u8303\u7574\u8bc1\u660e\uff0c\u5c55\u793a\u4e86\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u6846\u67b6\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u63a8\u5e7f\u4e86do-\u6f14\u7b97\u5728\u9ad8\u9636\u62bd\u8c61\u56fe\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8303\u7574\u6846\u67b6\u6bd4\u73b0\u6709\u6846\u67b6\u66f4\u9002\u5408\u5efa\u6a21\u56e0\u679c\u62bd\u8c61\uff0c\u80fd\u591f\u6062\u590d\u03c4\u4e00\u81f4\u6027\u548c\u6784\u9020\u6027\u03c4\u62bd\u8c61\u7b49\u6982\u5ff5\uff0c\u4e3a\u56e0\u679c\u62bd\u8c61\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5408\u9002\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2510.03265", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03265", "abs": "https://arxiv.org/abs/2510.03265", "authors": ["Bowei Tian", "Yexiao He", "Wanghao Ye", "Ziyao Wang", "Meng Liu", "Ang Li"], "title": "MindCraft: How Concept Trees Take Shape In Deep Models", "comment": null, "summary": "Large-scale foundation models demonstrate strong performance across language,\nvision, and reasoning tasks. However, how they internally structure and\nstabilize concepts remains elusive. Inspired by causal inference, we introduce\nthe MindCraft framework built upon Concept Trees. By applying spectral\ndecomposition at each layer and linking principal directions into branching\nConcept Paths, Concept Trees reconstruct the hierarchical emergence of\nconcepts, revealing exactly when they diverge from shared representations into\nlinearly separable subspaces. Empirical evaluations across diverse scenarios\nacross disciplines, including medical diagnosis, physics reasoning, and\npolitical decision-making, show that Concept Trees recover semantic\nhierarchies, disentangle latent concepts, and can be widely applied across\nmultiple domains. The Concept Tree establishes a widely applicable and powerful\nframework that enables in-depth analysis of conceptual representations in deep\nmodels, marking a significant step forward in the foundation of interpretable\nAI.", "AI": {"tldr": "\u63d0\u51fa\u4e86MindCraft\u6846\u67b6\u548c\u6982\u5ff5\u6811\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c31\u5206\u89e3\u548c\u6982\u5ff5\u8def\u5f84\u91cd\u5efa\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\uff0c\u63ed\u793a\u6982\u5ff5\u4ece\u5171\u4eab\u8868\u793a\u5230\u7ebf\u6027\u53ef\u5206\u5b50\u7a7a\u95f4\u7684\u5206\u5316\u8fc7\u7a0b\u3002", "motivation": "\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u5728\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5185\u90e8\u5982\u4f55\u7ec4\u7ec7\u548c\u7a33\u5b9a\u6982\u5ff5\u7ed3\u6784\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u6784\u5efaMindCraft\u6846\u67b6\uff0c\u5e94\u7528\u8c31\u5206\u89e3\u5728\u6bcf\u4e00\u5c42\uff0c\u5c06\u4e3b\u65b9\u5411\u8fde\u63a5\u6210\u5206\u652f\u6982\u5ff5\u8def\u5f84\uff0c\u5f62\u6210\u6982\u5ff5\u6811\u3002", "result": "\u5728\u533b\u5b66\u8bca\u65ad\u3001\u7269\u7406\u63a8\u7406\u548c\u653f\u6cbb\u51b3\u7b56\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0c\u6982\u5ff5\u6811\u80fd\u591f\u6062\u590d\u8bed\u4e49\u5c42\u6b21\u3001\u5206\u79bb\u6f5c\u5728\u6982\u5ff5\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u6982\u5ff5\u6811\u5efa\u7acb\u4e86\u4e00\u4e2a\u5e7f\u6cdb\u9002\u7528\u4e14\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u80fd\u591f\u6df1\u5165\u5206\u6790\u6df1\u5ea6\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u8868\u793a\uff0c\u662f\u8fc8\u5411\u53ef\u89e3\u91caAI\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.04744", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04744", "abs": "https://arxiv.org/abs/2510.04744", "authors": ["Wali Ullah Khan", "Chandan Kumar Sheemar", "Eva Lagunas", "Xingwang Li", "Symeon Chatzinotas", "Petar Popovski", "Zhu Han"], "title": "Multilayer Non-Terrestrial Networks with Spectrum Access aided by Beyond-Diagonal RIS", "comment": "13, 10", "summary": "In this work, we study a multi-user NTN in which a satellite serves as the\nprimary network and a high-altitude platform station (HAPS) operates as the\nsecondary network, acting as a cognitive radio. To reduce the cost, complexity,\nand power consumption of conventional antenna arrays, we equip the HAPS with a\ntransmissive BD-RIS antenna front end. We then formulate a joint optimization\nproblem for the BD-RIS phase response and the HAPS transmit power allocation\nunder strict per-user interference temperature constraints. To tackle the\nresulting highly nonconvex problem, we propose an alternating-optimization\nframework: the power-allocation subproblem admits a closed-form,\nwater-filling-type solution derived from the Karush-Kuhn-Tucker (KKT)\nconditions, while the BD-RIS configuration is refined via Riemannian manifold\noptimization. Simulation results show significant gains in data rate and\ninterference suppression over diagonal RIS-assisted benchmarks, establishing\nBD-RIS as a promising enabler for future multilayer NTNs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u5c42\u975e\u5730\u9762\u7f51\u7edc(NTN)\uff0c\u5176\u4e2d\u536b\u661f\u4f5c\u4e3a\u4e3b\u7f51\u7edc\uff0c\u9ad8\u7a7a\u5e73\u53f0\u7ad9(HAPS)\u4f5c\u4e3a\u8ba4\u77e5\u65e0\u7ebf\u7535\u7684\u6b21\u7ea7\u7f51\u7edc\u3002\u901a\u8fc7\u4f7f\u7528\u900f\u5c04\u5f0fBD-RIS\u5929\u7ebf\u524d\u7aef\uff0c\u5e76\u8054\u5408\u4f18\u5316BD-RIS\u76f8\u4f4d\u54cd\u5e94\u548cHAPS\u53d1\u5c04\u529f\u7387\u5206\u914d\uff0c\u5728\u4e25\u683c\u5e72\u6270\u6e29\u5ea6\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6570\u636e\u901f\u7387\u63d0\u5347\u548c\u5e72\u6270\u6291\u5236\u3002", "motivation": "\u4f20\u7edf\u5929\u7ebf\u9635\u5217\u6210\u672c\u9ad8\u3001\u590d\u6742\u5ea6\u5927\u3001\u529f\u8017\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u672a\u6765\u591a\u5c42NTN\u7684\u53d1\u5c55\u3002BD-RIS\u6280\u672f\u80fd\u591f\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u5ea6\u548c\u6210\u672c\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff1a\u529f\u7387\u5206\u914d\u5b50\u95ee\u9898\u901a\u8fc7KKT\u6761\u4ef6\u5f97\u5230\u95ed\u5f0f\u7684\u6c34\u586b\u5145\u89e3\uff0cBD-RIS\u914d\u7f6e\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u5bf9\u89d2RIS\u8f85\u52a9\u57fa\u51c6\uff0c\u5728\u6570\u636e\u901f\u7387\u548c\u5e72\u6270\u6291\u5236\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u589e\u76ca\u3002", "conclusion": "BD-RIS\u662f\u672a\u6765\u591a\u5c42NTN\u7684\u6709\u524d\u666f\u7684\u4f7f\u80fd\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u540c\u65f6\u964d\u4f4e\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.03305", "categories": ["cs.LG", "physics.ao-ph", "stat.AP", "stat.ML", "62P12 62p12"], "pdf": "https://arxiv.org/pdf/2510.03305", "abs": "https://arxiv.org/abs/2510.03305", "authors": ["Tian Zheng", "Subashree Venkatasubramanian", "Shuolin Li", "Amy Braverman", "Xinyi Ke", "Zhewen Hou", "Peter Jin", "Samarth Sanjay Agrawal"], "title": "Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies", "comment": "Supplement", "summary": "Machine learning has been increasingly applied in climate modeling on system\nemulation acceleration, data-driven parameter inference, forecasting, and\nknowledge discovery, addressing challenges such as physical consistency,\nmulti-scale coupling, data sparsity, robust generalization, and integration\nwith scientific workflows. This paper analyzes a series of case studies from\napplied machine learning research in climate modeling, with a focus on design\nchoices and workflow structure. Rather than reviewing technical details, we aim\nto synthesize workflow design patterns across diverse projects in ML-enabled\nclimate modeling: from surrogate modeling, ML parameterization, probabilistic\nprogramming, to simulation-based inference, and physics-informed transfer\nlearning. We unpack how these workflows are grounded in physical knowledge,\ninformed by simulation data, and designed to integrate observations. We aim to\noffer a framework for ensuring rigor in scientific machine learning through\nmore transparent model development, critical evaluation, informed adaptation,\nand reproducibility, and to contribute to lowering the barrier for\ninterdisciplinary collaboration at the interface of data science and climate\nmodeling.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u91cd\u70b9\u5173\u6ce8\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5305\u62ec\u66ff\u4ee3\u5efa\u6a21\u3001ML\u53c2\u6570\u5316\u3001\u6982\u7387\u7f16\u7a0b\u7b49\uff0c\u65e8\u5728\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e25\u8c28\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u6c14\u5019\u5efa\u6a21\u4e2d\u7269\u7406\u4e00\u81f4\u6027\u3001\u591a\u5c3a\u5ea6\u8026\u5408\u3001\u6570\u636e\u7a00\u758f\u6027\u7b49\u6311\u6218\uff0c\u4fc3\u8fdb\u6570\u636e\u79d1\u5b66\u4e0e\u6c14\u5019\u5efa\u6a21\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e00\u7cfb\u5217\u5e94\u7528\u673a\u5668\u5b66\u4e60\u7814\u7a76\u6848\u4f8b\uff0c\u7efc\u5408\u4e0d\u540c\u9879\u76ee\u4e2d\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5173\u6ce8\u7269\u7406\u77e5\u8bc6\u57fa\u7840\u3001\u6a21\u62df\u6570\u636e\u4fe1\u606f\u548c\u89c2\u6d4b\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u786e\u4fdd\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e25\u8c28\u6027\u7684\u6846\u67b6\uff0c\u5305\u62ec\u900f\u660e\u6a21\u578b\u5f00\u53d1\u3001\u5173\u952e\u8bc4\u4f30\u3001\u77e5\u60c5\u9002\u5e94\u548c\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "\u4e3a\u673a\u5668\u5b66\u4e60\u5728\u6c14\u5019\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u8de8\u5b66\u79d1\u5408\u4f5c\u95e8\u69db\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.03266", "categories": ["cs.LG", "stat.ME", "stat.OT"], "pdf": "https://arxiv.org/pdf/2510.03266", "abs": "https://arxiv.org/abs/2510.03266", "authors": ["Bharat Sharma", "Jitendra Kumar"], "title": "Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model", "comment": null, "summary": "Climate anomalies significantly impact terrestrial carbon cycle dynamics,\nnecessitating robust methods for detecting and analyzing anomalous behavior in\nplant productivity. This study presents a novel application of variational\nautoencoders (VAE) for identifying extreme events in gross primary productivity\n(GPP) from Community Earth System Model version 2 simulations across four AR6\nregions in the Continental United States. We compare VAE-based anomaly\ndetection with traditional singular spectral analysis (SSA) methods across\nthree time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.\nThe VAE architecture employs three dense layers and a latent space with an\ninput sequence length of 12 months, trained on a normalized GPP time series to\nreconstruct the GPP and identifying anomalies based on reconstruction errors.\nExtreme events are defined using 5th percentile thresholds applied to both VAE\nand SSA anomalies. Results demonstrate strong regional agreement between VAE\nand SSA methods in spatial patterns of extreme event frequencies, despite VAE\nproducing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA\nacross regions and periods). Both methods reveal increasing magnitudes and\nfrequencies of negative carbon cycle extremes toward 2050-80, particularly in\nWestern and Central North America. The VAE approach shows comparable\nperformance to established SSA techniques, while offering computational\nadvantages and enhanced capability for capturing non-linear temporal\ndependencies in carbon cycle variability. Unlike SSA, the VAE method does not\nrequire one to define the periodicity of the signals in the data; it discovers\nthem from the data.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u5e94\u7528\u4e8e\u68c0\u6d4b\u7f8e\u56fd\u5927\u9646\u56db\u4e2aAR6\u533a\u57df\u7684GPP\u6781\u7aef\u4e8b\u4ef6\uff0c\u4e0e\u4f20\u7edf\u5947\u5f02\u8c31\u5206\u6790(SSA)\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793aVAE\u5728\u6027\u80fd\u76f8\u5f53\u7684\u540c\u65f6\u5177\u6709\u8ba1\u7b97\u4f18\u52bf\u548c\u6355\u6349\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u80fd\u529b\u3002", "motivation": "\u6c14\u5019\u5f02\u5e38\u663e\u8457\u5f71\u54cd\u9646\u5730\u78b3\u5faa\u73af\u52a8\u6001\uff0c\u9700\u8981\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u5206\u6790\u690d\u7269\u751f\u4ea7\u529b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5bc6\u96c6\u5c42\u548c\u6f5c\u5728\u7a7a\u95f4\uff0c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u4e3a12\u4e2a\u6708\uff0c\u901a\u8fc7\u91cd\u5efa\u8bef\u5dee\u8bc6\u522bGPP\u5f02\u5e38\uff0c\u5e76\u4e0e\u4f20\u7edfSSA\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "VAE\u548cSSA\u65b9\u6cd5\u5728\u6781\u7aef\u4e8b\u4ef6\u9891\u7387\u7684\u7a7a\u95f4\u6a21\u5f0f\u4e0a\u8868\u73b0\u51fa\u5f3a\u533a\u57df\u4e00\u81f4\u6027\uff0c\u4f46VAE\u4ea7\u751f\u66f4\u9ad8\u7684\u9608\u503c\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u793a\u52302050-80\u5e74\u8d1f\u78b3\u5faa\u73af\u6781\u7aef\u4e8b\u4ef6\u7684\u5e45\u5ea6\u548c\u9891\u7387\u589e\u52a0\uff0c\u7279\u522b\u662f\u5728\u897f\u90e8\u548c\u4e2d\u90e8\u5317\u7f8e\u5730\u533a\u3002", "conclusion": "VAE\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u6210\u719f\u7684SSA\u6280\u672f\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u8ba1\u7b97\u4f18\u52bf\u5e76\u589e\u5f3a\u4e86\u5bf9\u78b3\u5faa\u73af\u53d8\u5f02\u4e2d\u975e\u7ebf\u6027\u65f6\u95f4\u4f9d\u8d56\u6027\u7684\u6355\u6349\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u6570\u636e\u7684\u5468\u671f\u6027\u3002"}}
{"id": "2510.04745", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04745", "abs": "https://arxiv.org/abs/2510.04745", "authors": ["Lucas Semp\u00e9r\u00e9", "Yue Bi", "Yue Wu", "Pengwenlong Gu", "Selma Boumerdassi"], "title": "Interference Alignment for Multi-cluster Over-the-Air Computation", "comment": null, "summary": "One of the main challenges facing Internet of Things (IoT) networks is\nmanaging interference caused by the large number of devices communicating\nsimultaneously, particularly in multi-cluster networks where multiple devices\nsimultaneously transmit to their respective receiver. Over-the-Air Computation\n(AirComp) has emerged as a promising solution for efficient real-time data\naggregation, yet its performance suffers in dense, interference-limited\nenvironments. To address this, we propose a novel Interference Alignment (IA)\nscheme tailored for up-link AirComp systems. Unlike previous approaches, the\nproposed method scales to an arbitrary number $\\sf K$ of clusters and enables\neach cluster to exploit half of the available channels, instead of only\n$\\tfrac{1}{\\sf K}$ as in time-sharing. In addition, we develop schemes tailored\nto scenarios where users are shared between adjacent clusters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u4e0a\u884c\u94fe\u8defAirComp\u7cfb\u7edf\u7684\u5e72\u6270\u5bf9\u9f50\u65b9\u6848\uff0c\u80fd\u591f\u5728\u591a\u7c07\u7f51\u7edc\u4e2d\u6709\u6548\u7ba1\u7406\u5e72\u6270\uff0c\u8ba9\u6bcf\u4e2a\u7c07\u53ef\u5229\u7528\u4e00\u534a\u53ef\u7528\u4fe1\u9053\u800c\u975e\u4f20\u7edf\u65f6\u95f4\u5171\u4eab\u76841/K\u3002", "motivation": "\u89e3\u51b3\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u5927\u91cf\u8bbe\u5907\u540c\u65f6\u901a\u4fe1\u5bfc\u81f4\u7684\u5e72\u6270\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u7c07\u7f51\u7edc\u4e2d\uff0c\u63d0\u5347AirComp\u5728\u5bc6\u96c6\u5e72\u6270\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e72\u6270\u5bf9\u9f50\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u6570\u91cfK\u7684\u96c6\u7fa4\uff0c\u5e76\u9488\u5bf9\u76f8\u90bb\u7c07\u5171\u4eab\u7528\u6237\u7684\u573a\u666f\u5f00\u53d1\u4e86\u4e13\u95e8\u65b9\u6848\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u6bcf\u4e2a\u7c07\u80fd\u591f\u5229\u7528\u4e00\u534a\u53ef\u7528\u4fe1\u9053\uff0c\u76f8\u6bd4\u4f20\u7edf\u65f6\u95f4\u5171\u4eab\u65b9\u6848\uff08\u4ec51/K\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u4fe1\u9053\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u5e72\u6270\u5bf9\u9f50\u65b9\u6848\u4e3a\u5bc6\u96c6\u591a\u7c07\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684AirComp\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5e72\u6270\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2510.03362", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03362", "abs": "https://arxiv.org/abs/2510.03362", "authors": ["Lijiao Wang", "Muhammad Usama", "Haris N. Koutsopoulos", "Zhengbing He"], "title": "Estimating link level traffic emissions: enhancing MOVES with open-source data", "comment": null, "summary": "Open-source data offers a scalable and transparent foundation for estimating\nvehicle activity and emissions in urban regions. In this study, we propose a\ndata-driven framework that integrates MOVES and open-source GPS trajectory\ndata, OpenStreetMap (OSM) road networks, regional traffic datasets and\nsatellite imagery-derived feature vectors to estimate the link level operating\nmode distribution and traffic emissions. A neural network model is trained to\npredict the distribution of MOVES-defined operating modes using only features\nderived from readily available data. The proposed methodology was applied using\nopen-source data related to 45 municipalities in the Boston Metropolitan area.\nThe \"ground truth\" operating mode distribution was established using OSM\nopen-source GPS trajectories. Compared to the MOVES baseline, the proposed\nmodel reduces RMSE by over 50% for regional scale traffic emissions of key\npollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the\nfeasibility of low-cost, replicable, and data-driven emissions estimation using\nfully open data sources.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408MOVES\u6a21\u578b\u548c\u5f00\u6e90\u6570\u636e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u7b97\u8f66\u8f86\u8fd0\u884c\u6a21\u5f0f\u5206\u5e03\u548c\u4ea4\u901a\u6392\u653e\uff0c\u76f8\u6bd4MOVES\u57fa\u51c6\u6a21\u578b\u5c06\u5173\u952e\u6c61\u67d3\u7269\u6392\u653e\u4f30\u7b97\u7684RMSE\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u3002", "motivation": "\u5229\u7528\u5f00\u6e90\u6570\u636e\u4e3a\u57ce\u5e02\u533a\u57df\u7684\u8f66\u8f86\u6d3b\u52a8\u548c\u6392\u653e\u4f30\u7b97\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u57fa\u7840\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u53ef\u590d\u5236\u7684\u6392\u653e\u4f30\u7b97\u3002", "method": "\u6574\u5408MOVES\u6a21\u578b\u3001GPS\u8f68\u8ff9\u6570\u636e\u3001OpenStreetMap\u9053\u8def\u7f51\u7edc\u3001\u533a\u57df\u4ea4\u901a\u6570\u636e\u96c6\u548c\u536b\u661f\u56fe\u50cf\u7279\u5f81\u5411\u91cf\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9884\u6d4bMOVES\u5b9a\u4e49\u7684\u8fd0\u884c\u6a21\u5f0f\u5206\u5e03\u3002", "result": "\u5728\u6ce2\u58eb\u987f\u5927\u90fd\u4f1a\u533a45\u4e2a\u57ce\u5e02\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u76f8\u6bd4MOVES\u57fa\u51c6\u6a21\u578b\uff0c\u5173\u952e\u6c61\u67d3\u7269\uff08CO\u3001NOx\u3001CO2\u3001PM2.5\uff09\u7684\u533a\u57df\u5c3a\u5ea6\u4ea4\u901a\u6392\u653eRMSE\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4f7f\u7528\u5b8c\u5168\u5f00\u6e90\u6570\u636e\u6e90\u8fdb\u884c\u4f4e\u6210\u672c\u3001\u53ef\u590d\u5236\u548c\u6570\u636e\u9a71\u52a8\u7684\u6392\u653e\u4f30\u7b97\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03267", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03267", "abs": "https://arxiv.org/abs/2510.03267", "authors": ["Xianglong Yan", "Chengzhu Bao", "Zhiteng Li", "Tianao Zhang", "Kaicheng Yang", "Haotong Qin", "Ruobing Xie", "Xingwu Sun", "Yulun Zhang"], "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities across\ndiverse tasks, but their large memory and compute demands hinder deployment.\nTernarization has gained attention as a promising compression technique,\ndelivering substantial size reduction and high computational efficiency.\nHowever, its potential in the post-training quantization (PTQ) setting remains\nunderexplored, due to the challenge of training-free parameter optimization and\nthe quantization difficulty posed by outliers and dispersed weights. To address\nthese issues, we propose PT$^2$-LLM, a post-training ternarization framework\ntailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with\na two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which\nalternates between optimal ternary grid construction and flexible rounding to\nminimize quantization error, and (2) Activation-aware Grid Alignment (AGA),\nwhich further refines the ternary grid to better match full-precision outputs.\nIn addition, we propose a plug-and-play Structural Similarity-based Reordering\n(SSR) strategy that leverages inter-column structural similarity to ease\nquantization and mitigate outlier effects, further enhancing overall\nperformance. Extensive experiments demonstrate that PT$^2$-LLM delivers\ncompetitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with\nlower memory cost, while also accelerating both prefill and decoding to achieve\nend-to-end speedup. The code and models will be available at\nhttps://github.com/XIANGLONGYAN/PT2-LLM.", "AI": {"tldr": "PT^2-LLM\u662f\u4e00\u79cd\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e09\u503c\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u4e09\u503c\u91cf\u5316\u5668\u548c\u4e24\u9636\u6bb5\u4f18\u5316\u6d41\u7a0b\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6210\u672c\u5e76\u52a0\u901f\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u90e8\u7f72\u65f6\u9762\u4e34\u5de8\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4e09\u503c\u5316\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u538b\u7f29\u6280\u672f\uff0c\u5728\u540e\u8bad\u7ec3\u91cf\u5316\u573a\u666f\u4e0b\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e3b\u8981\u6311\u6218\u5305\u62ec\u65e0\u9700\u8bad\u7ec3\u7684\u53c2\u6570\u91cf\u5316\u548c\u5f02\u5e38\u503c/\u5206\u6563\u6743\u91cd\u5e26\u6765\u7684\u91cf\u5316\u56f0\u96be\u3002", "method": "\u63d0\u51faPT^2-LLM\u6846\u67b6\uff0c\u6838\u5fc3\u5305\u62ec\uff1a1\uff09\u8fed\u4ee3\u4e09\u503c\u62df\u5408(ITF)\uff0c\u4ea4\u66ff\u8fdb\u884c\u6700\u4f18\u4e09\u503c\u7f51\u683c\u6784\u5efa\u548c\u7075\u6d3b\u820d\u5165\uff1b2\uff09\u6fc0\u6d3b\u611f\u77e5\u7f51\u683c\u5bf9\u9f50(AGA)\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e09\u503c\u7f51\u683c\uff1b3\uff09\u57fa\u4e8e\u7ed3\u6784\u76f8\u4f3c\u6027\u7684\u91cd\u6392\u5e8f(SSR)\u7b56\u7565\uff0c\u5229\u7528\u5217\u95f4\u7ed3\u6784\u76f8\u4f3c\u6027\u7f13\u89e3\u5f02\u5e38\u503c\u5f71\u54cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPT^2-LLM\u5728\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb2\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u7ade\u4e89\u529b\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5177\u6709\u66f4\u4f4e\u7684\u5185\u5b58\u6210\u672c\uff0c\u5e76\u80fd\u540c\u65f6\u52a0\u901f\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "PT^2-LLM\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u4e09\u503c\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6027\u80fd\u3001\u5185\u5b58\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2510.04913", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04913", "abs": "https://arxiv.org/abs/2510.04913", "authors": ["Andreas Bathelt", "Benjamin Deutschmann", "Hyeon Seok Rou", "Kuranage Roche Rayan Ranasinghe", "Giuseppe Thadeu Freitas de Abreu", "Peter Vouras"], "title": "The IEEE Signal Processing Society's Leading Role in Developing Standards for Computational Imaging and Sensing: Part II", "comment": "Submitted to the IEEE for possible publication", "summary": "In every imaging or sensing application, the physical hardware creates\nconstraints that must be overcome or they limit system performance. Techniques\nthat leverage additional degrees of freedom can effectively extend performance\nbeyond the inherent physical capabilities of the hardware. An example includes\nsynchronizing distributed sensors so as to synthesize a larger aperture for\nremote sensing applications. An additional example is integrating the\ncommunication and sensing functions in a wireless system through the clever\ndesign of waveforms and optimized resource management. As these technologies\nmature beyond the conceptual and prototype phase they will ultimately\ntransition to the commercial market. Here, standards play a critical role in\nensuring success. Standards ensure interoperability between systems\nmanufactured by different vendors and define industry best practices for\nvendors and customers alike. The Signal Processing Society of the Institute for\nElectrical and Electronics Engineers (IEEE) plays a leading role in developing\nhigh-quality standards for computational sensing technologies through the\nworking groups of the Synthetic Aperture Standards Committee (SASC). In this\ncolumn we highlight the standards activities of the P3383 Performance Metrics\nfor Integrated Sensing and Communication (ISAC) Systems Working Group and the\nP3343 Spatio-Temporal Synchronization of a Synthetic Aperture of Distributed\nSensors Working Group.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u5982\u4f55\u901a\u8fc7\u5229\u7528\u989d\u5916\u81ea\u7531\u5ea6\u6765\u514b\u670d\u6210\u50cf\u548c\u4f20\u611f\u5e94\u7528\u4e2d\u786c\u4ef6\u7ea6\u675f\uff0c\u5e76\u5f3a\u8c03\u4e86IEEE\u4fe1\u53f7\u5904\u7406\u534f\u4f1a\u5728\u5236\u5b9a\u8ba1\u7b97\u4f20\u611f\u6280\u672f\u6807\u51c6\u65b9\u9762\u7684\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u5728\u6210\u50cf\u548c\u4f20\u611f\u5e94\u7528\u4e2d\uff0c\u7269\u7406\u786c\u4ef6\u5b58\u5728\u56fa\u6709\u7ea6\u675f\uff0c\u9700\u8981\u901a\u8fc7\u989d\u5916\u81ea\u7531\u5ea6\u6765\u6269\u5c55\u7cfb\u7edf\u6027\u80fd\u3002\u968f\u7740\u8fd9\u4e9b\u6280\u672f\u4ece\u6982\u5ff5\u539f\u578b\u9636\u6bb5\u8d70\u5411\u5546\u4e1a\u5316\uff0c\u6807\u51c6\u5236\u5b9a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u540c\u6b65\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u5408\u6210\u66f4\u5927\u5b54\u5f84\uff0c\u4ee5\u53ca\u901a\u8fc7\u6ce2\u5f62\u8bbe\u8ba1\u548c\u8d44\u6e90\u7ba1\u7406\u96c6\u6210\u901a\u4fe1\u4e0e\u4f20\u611f\u529f\u80fd\u3002IEEE\u4fe1\u53f7\u5904\u7406\u534f\u4f1a\u901a\u8fc7SASC\u59d4\u5458\u4f1a\u7684\u5de5\u4f5c\u7ec4\u5236\u5b9a\u76f8\u5173\u6807\u51c6\u3002", "result": "\u91cd\u70b9\u4ecb\u7ecd\u4e86P3383 ISAC\u7cfb\u7edf\u6027\u80fd\u6307\u6807\u5de5\u4f5c\u7ec4\u548cP3343\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u5408\u6210\u5b54\u5f84\u65f6\u7a7a\u540c\u6b65\u5de5\u4f5c\u7ec4\u7684\u6807\u51c6\u5236\u5b9a\u6d3b\u52a8\u3002", "conclusion": "\u6807\u51c6\u786e\u4fdd\u4e0d\u540c\u5382\u5546\u7cfb\u7edf\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u4e3a\u884c\u4e1a\u5b9a\u4e49\u6700\u4f73\u5b9e\u8df5\uff0c\u5bf9\u8ba1\u7b97\u4f20\u611f\u6280\u672f\u7684\u5546\u4e1a\u5316\u6210\u529f\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.03419", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03419", "abs": "https://arxiv.org/abs/2510.03419", "authors": ["Joseph Rawson", "Domniki Ladopoulou", "Petros Dellaportas"], "title": "Multi-task neural diffusion processes for uncertainty-quantified wind power prediction", "comment": "36 pages, 13 figures, 2 tables,", "summary": "Uncertainty-aware wind power prediction is essential for grid integration and\nreliable wind farm operation. We apply neural diffusion processes (NDPs)-a\nrecent class of models that learn distributions over functions-and extend them\nto a multi-task NDP (MT-NDP) framework for wind power prediction. We provide\nthe first empirical evaluation of NDPs in real supervisory control and data\nacquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture\ncross-turbine correlations and enable few-shot adaptation to unseen turbines.\nThe proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of\npoint accuracy and calibration, particularly for wind turbines whose behaviour\ndeviates from the fleet average. In general, NDP-based models deliver\ncalibrated and scalable predictions suitable for operational deployment,\noffering sharper, yet trustworthy, predictive intervals that can support\ndispatch and maintenance decisions in modern wind farms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u4efb\u52a1\u795e\u7ecf\u6269\u6563\u8fc7\u7a0b\uff08MT-NDP\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u98ce\u7535\u573a\u529f\u7387\u9884\u6d4b\uff0c\u901a\u8fc7\u4efb\u52a1\u7f16\u7801\u5668\u6355\u83b7\u8de8\u6da1\u8f6e\u673a\u76f8\u5173\u6027\uff0c\u5e76\u5728\u771f\u5b9eSCADA\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9996\u6b21NDP\u5b9e\u8bc1\u8bc4\u4f30\u3002", "motivation": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u98ce\u7535\u529f\u7387\u9884\u6d4b\u5bf9\u4e8e\u7535\u7f51\u96c6\u6210\u548c\u98ce\u7535\u573a\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u63d0\u4f9b\u6821\u51c6\u4e14\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u795e\u7ecf\u6269\u6563\u8fc7\u7a0b\uff08NDPs\uff09\u5230\u591a\u4efb\u52a1\u6846\u67b6MT-NDP\uff0c\u5f15\u5165\u4efb\u52a1\u7f16\u7801\u5668\u6765\u6355\u83b7\u8de8\u6da1\u8f6e\u673a\u76f8\u5173\u6027\uff0c\u5e76\u652f\u6301\u5bf9\u672a\u89c1\u6da1\u8f6e\u673a\u7684\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "MT-NDP\u6846\u67b6\u5728\u70b9\u7cbe\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u4f18\u4e8e\u5355\u4efb\u52a1NDPs\u548cGPs\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u504f\u79bb\u673a\u7fa4\u5e73\u5747\u884c\u4e3a\u7684\u6da1\u8f6e\u673a\uff0c\u63d0\u4f9b\u4e86\u66f4\u9510\u5229\u4f46\u53ef\u4fe1\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u57fa\u4e8eNDP\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u9002\u5408\u8fd0\u8425\u90e8\u7f72\u7684\u6821\u51c6\u548c\u53ef\u6269\u5c55\u9884\u6d4b\uff0c\u80fd\u591f\u652f\u6301\u73b0\u4ee3\u98ce\u7535\u573a\u7684\u8c03\u5ea6\u548c\u7ef4\u62a4\u51b3\u7b56\u3002"}}
{"id": "2510.03268", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03268", "abs": "https://arxiv.org/abs/2510.03268", "authors": ["Lingjie Yi", "Raphael Douady", "Chao Chen"], "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment", "comment": null, "summary": "Multimodal contrastive learning (MCL) aims to embed data from different\nmodalities in a shared embedding space. However, empirical evidence shows that\nrepresentations from different modalities occupy completely separate regions of\nembedding space, a phenomenon referred to as the modality gap. Moreover,\nexperimental findings on how the size of the modality gap influences downstream\nperformance are inconsistent. These observations raise two key questions: (1)\nWhat causes the modality gap? (2) How does it affect downstream tasks? To\naddress these questions, this paper introduces the first theoretical framework\nfor analyzing the convergent optimal representations of MCL and the modality\nalignment when training is optimized. Specifically, we prove that without any\nconstraint or under the cone constraint, the modality gap converges to zero.\nUnder the subspace constraint (i.e., representations of two modalities fall\ninto two distinct hyperplanes due to dimension collapse), the modality gap\nconverges to the smallest angle between the two hyperplanes. This result\nidentifies \\emph{dimension collapse} as the fundamental origin of the modality\ngap. Furthermore, our theorems demonstrate that paired samples cannot be\nperfectly aligned under the subspace constraint. The modality gap influences\ndownstream performance by affecting the alignment between sample pairs. We\nprove that, in this case, perfect alignment between two modalities can still be\nachieved via two ways: hyperplane rotation and shared space projection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5206\u6790\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6536\u655b\u6700\u4f18\u8868\u793a\u548c\u6a21\u6001\u5bf9\u9f50\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u7ef4\u5ea6\u584c\u9677\u662f\u6a21\u6001\u95f4\u9699\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u8bc1\u660e\u4e86\u901a\u8fc7\u8d85\u5e73\u9762\u65cb\u8f6c\u548c\u5171\u4eab\u7a7a\u95f4\u6295\u5f71\u53ef\u4ee5\u5b9e\u73b0\u5b8c\u7f8e\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u4e0d\u540c\u6a21\u6001\u7684\u8868\u793a\u5360\u636e\u5d4c\u5165\u7a7a\u95f4\u7684\u4e0d\u540c\u533a\u57df\uff08\u6a21\u6001\u95f4\u9699\uff09\uff0c\u4e14\u5173\u4e8e\u6a21\u6001\u95f4\u9699\u5927\u5c0f\u5bf9\u4e0b\u6e38\u6027\u80fd\u5f71\u54cd\u7684\u7814\u7a76\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4ec0\u4e48\u5bfc\u81f4\u4e86\u6a21\u6001\u95f4\u9699\uff1f\u5b83\u5982\u4f55\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\uff1f", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u5206\u6790\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u7684\u6536\u655b\u6700\u4f18\u8868\u793a\u548c\u6a21\u6001\u5bf9\u9f50\uff0c\u5728\u4e0d\u540c\u7ea6\u675f\u6761\u4ef6\u4e0b\uff08\u65e0\u7ea6\u675f\u3001\u9525\u7ea6\u675f\u3001\u5b50\u7a7a\u95f4\u7ea6\u675f\uff09\u8bc1\u660e\u6a21\u6001\u95f4\u9699\u7684\u6536\u655b\u884c\u4e3a\u3002", "result": "\u8bc1\u660e\u5728\u65e0\u7ea6\u675f\u6216\u9525\u7ea6\u675f\u4e0b\u6a21\u6001\u95f4\u9699\u6536\u655b\u4e3a\u96f6\uff1b\u5728\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\uff08\u7531\u4e8e\u7ef4\u5ea6\u584c\u9677\uff09\uff0c\u6a21\u6001\u95f4\u9699\u6536\u655b\u4e3a\u4e24\u4e2a\u8d85\u5e73\u9762\u95f4\u7684\u6700\u5c0f\u89d2\u5ea6\uff0c\u8bc6\u522b\u7ef4\u5ea6\u584c\u9677\u662f\u6a21\u6001\u95f4\u9699\u7684\u6839\u672c\u539f\u56e0\u3002", "conclusion": "\u6a21\u6001\u95f4\u9699\u901a\u8fc7\u5f71\u54cd\u6837\u672c\u5bf9\u4e4b\u95f4\u7684\u5bf9\u9f50\u6765\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\uff0c\u4f46\u5728\u5b50\u7a7a\u95f4\u7ea6\u675f\u4e0b\u4ecd\u53ef\u901a\u8fc7\u8d85\u5e73\u9762\u65cb\u8f6c\u548c\u5171\u4eab\u7a7a\u95f4\u6295\u5f71\u5b9e\u73b0\u4e24\u4e2a\u6a21\u6001\u7684\u5b8c\u7f8e\u5bf9\u9f50\u3002"}}
{"id": "2510.03437", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03437", "abs": "https://arxiv.org/abs/2510.03437", "authors": ["Jairo Diaz-Rodriguez", "Mumin Jia"], "title": "Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation", "comment": null, "summary": "Kernel change-point detection (KCPD) has become a widely used tool for\nidentifying structural changes in complex data. While existing theory\nestablishes consistency under independence assumptions, real-world sequential\ndata such as text exhibits strong dependencies. We establish new guarantees for\nKCPD under $m$-dependent data: specifically, we prove consistency in the number\nof detected change points and weak consistency in their locations under mild\nadditional assumptions. We perform an LLM-based simulation that generates\nsynthetic $m$-dependent text to validate the asymptotics. To complement these\nresults, we present the first comprehensive empirical study of KCPD for text\nsegmentation with modern embeddings. Across diverse text datasets, KCPD with\ntext embeddings outperforms baselines in standard text segmentation metrics. We\ndemonstrate through a case study on Taylor Swift's tweets that KCPD not only\nprovides strong theoretical and simulated reliability but also practical\neffectiveness for text segmentation tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6838\u53d8\u70b9\u68c0\u6d4b\u5728\u6587\u672c\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5728m-\u4f9d\u8d56\u6570\u636e\u4e0b\u7684\u7406\u8bba\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7LLM\u6a21\u62df\u548c\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u73b0\u4ee3\u6587\u672c\u5d4c\u5165\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6838\u53d8\u70b9\u68c0\u6d4b\u7406\u8bba\u4e3b\u8981\u57fa\u4e8e\u72ec\u7acb\u6027\u5047\u8bbe\uff0c\u4f46\u771f\u5b9e\u6587\u672c\u6570\u636e\u5b58\u5728\u5f3a\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u7b26\u5408\u5b9e\u9645\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u5efa\u7acb\u4e86m-\u4f9d\u8d56\u6570\u636e\u4e0b\u7684\u7406\u8bba\u4e00\u81f4\u6027\u8bc1\u660e\uff0c\u4f7f\u7528LLM\u751f\u6210\u5408\u6210m-\u4f9d\u8d56\u6587\u672c\u8fdb\u884c\u6a21\u62df\u9a8c\u8bc1\uff0c\u5e76\u5728\u591a\u4e2a\u6587\u672c\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u73b0\u4ee3\u5d4c\u5165\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u8bc1\u660e\u4e86\u68c0\u6d4b\u53d8\u70b9\u6570\u91cf\u7684\u5f3a\u4e00\u81f4\u6027\u548c\u4f4d\u7f6e\u4f30\u8ba1\u7684\u5f31\u4e00\u81f4\u6027\uff0c\u5b9e\u8bc1\u663e\u793a\u6838\u53d8\u70b9\u68c0\u6d4b\u5728\u6587\u672c\u5206\u5272\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6838\u53d8\u70b9\u68c0\u6d4b\u4e0d\u4ec5\u5177\u6709\u7406\u8bba\u53ef\u9760\u6027\uff0c\u5728\u5b9e\u9645\u6587\u672c\u5206\u5272\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u73b0\u4ee3\u6587\u672c\u5d4c\u5165\u65f6\u3002"}}
{"id": "2510.03269", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03269", "abs": "https://arxiv.org/abs/2510.03269", "authors": ["Wendi Li", "Changdae Oh", "Yixuan Li"], "title": "General Exploratory Bonus for Optimistic Exploration in RLHF", "comment": null, "summary": "Optimistic exploration is central to improving sample efficiency in\nreinforcement learning with human feedback, yet existing exploratory bonus\nmethods to incentivize exploration often fail to realize optimism. We provide a\ntheoretical analysis showing that current formulations, under KL or\n$\\alpha$-divergence regularization, unintentionally bias exploration toward\nhigh-probability regions of the reference model, thereby reinforcing\nconservative behavior instead of promoting discovery of uncertain regions. To\naddress this pitfall, we introduce the General Exploratory Bonus (GEB), a novel\ntheoretical framework that provably satisfies the optimism principle. GEB\ncounteracts divergence-induced bias via reference-dependent reward regulation\nand unifies prior heuristic bonuses as special cases, while extending naturally\nacross the full $\\alpha$-divergence family. Empirically, GEB consistently\noutperforms baselines on alignment tasks across multiple divergence settings\nand large language model backbones. These results demonstrate that GEB offers\nboth a principled and practical solution for optimistic exploration in RLHF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u901a\u7528\u63a2\u7d22\u5956\u52b1(GEB)\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709KL\u548c\u03b1-\u6563\u5ea6\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\u4e2d\u63a2\u7d22\u504f\u5411\u4fdd\u5b88\u533a\u57df\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u8003\u76f8\u5173\u5956\u52b1\u8c03\u8282\u5b9e\u73b0\u4e50\u89c2\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eKL\u6216\u03b1-\u6563\u5ea6\u6b63\u5219\u5316\u7684\u63a2\u7d22\u5956\u52b1\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u4e50\u89c2\u63a2\u7d22\uff0c\u53cd\u800c\u504f\u5411\u53c2\u8003\u6a21\u578b\u7684\u9ad8\u6982\u7387\u533a\u57df\uff0c\u5f3a\u5316\u4e86\u4fdd\u5b88\u884c\u4e3a\u800c\u975e\u53d1\u73b0\u4e0d\u786e\u5b9a\u533a\u57df\u3002", "method": "\u5f15\u5165\u901a\u7528\u63a2\u7d22\u5956\u52b1(GEB)\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u76f8\u5173\u5956\u52b1\u8c03\u8282\u6765\u62b5\u6d88\u6563\u5ea6\u5f15\u8d77\u7684\u504f\u5dee\uff0c\u7edf\u4e00\u4e86\u5148\u524d\u7684\u542f\u53d1\u5f0f\u5956\u52b1\u65b9\u6cd5\uff0c\u5e76\u81ea\u7136\u6269\u5c55\u5230\u5b8c\u6574\u7684\u03b1-\u6563\u5ea6\u65cf\u3002", "result": "\u5728\u591a\u4e2a\u6563\u5ea6\u8bbe\u7f6e\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u4e0a\u7684\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0cGEB\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GEB\u4e3aRLHF\u4e2d\u7684\u4e50\u89c2\u63a2\u7d22\u63d0\u4f9b\u4e86\u65e2\u6709\u7406\u8bba\u4f9d\u636e\u53c8\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05000", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.05000", "abs": "https://arxiv.org/abs/2510.05000", "authors": ["Xiang-Gen Xia"], "title": "My First Five Years of Faculty Career at the University of Delaware", "comment": null, "summary": "In this short article, I would like to briefly summarize my research in the\nfirst 5 years in my university academia life in USA. I think that my research\nresults obtained in these 5 years are the best in my career, at least which I\nlike the most by myself. I wish that my experience in my junior academia career\ncould be of some help to young researchers.", "AI": {"tldr": "\u4f5c\u8005\u603b\u7ed3\u4e86\u81ea\u5df1\u5728\u7f8e\u56fd\u5927\u5b66\u524d5\u5e74\u7684\u7814\u7a76\u6210\u679c\uff0c\u8ba4\u4e3a\u8fd9\u662f\u81ea\u5df1\u804c\u4e1a\u751f\u6daf\u4e2d\u6700\u597d\u7684\u6210\u679c\uff0c\u5e76\u5e0c\u671b\u8fd9\u4e9b\u7ecf\u9a8c\u80fd\u5e2e\u52a9\u5e74\u8f7b\u7814\u7a76\u8005\u3002", "motivation": "\u5206\u4eab\u4e2a\u4eba\u5728\u5b66\u672f\u751f\u6daf\u521d\u671f\u7684\u7814\u7a76\u7ecf\u9a8c\u548c\u6210\u679c\uff0c\u4e3a\u5e74\u8f7b\u7814\u7a76\u8005\u63d0\u4f9b\u53c2\u8003\u548c\u5e2e\u52a9\u3002", "method": "\u901a\u8fc7\u4e2a\u4eba\u603b\u7ed3\u548c\u56de\u987e\u7684\u65b9\u5f0f\uff0c\u6574\u7406\u524d5\u5e74\u7684\u7814\u7a76\u6210\u679c\u3002", "result": "\u4f5c\u8005\u8ba4\u4e3a\u8fd95\u5e74\u7684\u7814\u7a76\u6210\u679c\u662f\u81ea\u5df1\u804c\u4e1a\u751f\u6daf\u4e2d\u6700\u559c\u6b22\u7684\u6210\u679c\u3002", "conclusion": "\u4f5c\u8005\u5e0c\u671b\u81ea\u5df1\u7684\u7ecf\u9a8c\u80fd\u591f\u5bf9\u5e74\u8f7b\u7814\u7a76\u8005\u6709\u6240\u5e2e\u52a9\u3002"}}
{"id": "2510.03470", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03470", "abs": "https://arxiv.org/abs/2510.03470", "authors": ["Benoit Dherin", "Michael Munn"], "title": "On residual network depth", "comment": null, "summary": "Deep residual architectures, such as ResNet and the Transformer, have enabled\nmodels of unprecedented depth, yet a formal understanding of why depth is so\neffective remains an open question. A popular intuition, following Veit et al.\n(2016), is that these residual networks behave like ensembles of many shallower\nmodels. Our key finding is an explicit analytical formula that verifies this\nensemble perspective, proving that increasing network depth is mathematically\nequivalent to expanding the size of this implicit ensemble. Furthermore, our\nexpansion reveals a hierarchical ensemble structure in which the combinatorial\ngrowth of computation paths leads to an explosion in the output signal,\nexplaining the historical necessity of normalization layers in training deep\nmodels. This insight offers a first principles explanation for the historical\ndependence on normalization layers and sheds new light on a family of\nsuccessful normalization-free techniques like SkipInit and Fixup. However,\nwhile these previous approaches infer scaling factors through optimizer\nanalysis or a heuristic analogy to Batch Normalization, our work offers the\nfirst explanation derived directly from the network's inherent functional\nstructure. Specifically, our Residual Expansion Theorem reveals that scaling\neach residual module provides a principled solution to taming the combinatorial\nexplosion inherent to these architectures. We further show that this scaling\nacts as a capacity controls that also implicitly regularizes the model's\ncomplexity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6b8b\u5dee\u6269\u5c55\u5b9a\u7406\u8bc1\u660e\u4e86\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5b9e\u9645\u4e0a\u7b49\u540c\u4e8e\u6d45\u5c42\u6a21\u578b\u7684\u9690\u5f0f\u96c6\u6210\uff0c\u5e76\u89e3\u91ca\u4e86\u5f52\u4e00\u5316\u5c42\u7684\u5386\u53f2\u5fc5\u8981\u6027\u3002", "motivation": "\u7406\u89e3\u4e3a\u4ec0\u4e48\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5982\u6b64\u6709\u6548\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u9700\u8981\u5f52\u4e00\u5316\u5c42\u6765\u8bad\u7ec3\u6df1\u5c42\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u6b8b\u5dee\u6269\u5c55\u5b9a\u7406\uff0c\u901a\u8fc7\u5206\u6790\u6b8b\u5dee\u7f51\u7edc\u7684\u7ec4\u5408\u8def\u5f84\u6765\u63ed\u793a\u5176\u9690\u5f0f\u96c6\u6210\u7ed3\u6784\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u6df1\u5ea6\u7684\u589e\u52a0\u7b49\u4ef7\u4e8e\u9690\u5f0f\u96c6\u6210\u89c4\u6a21\u7684\u6269\u5927\uff0c\u7ec4\u5408\u8def\u5f84\u7684\u7206\u70b8\u6027\u589e\u957f\u5bfc\u81f4\u8f93\u51fa\u4fe1\u53f7\u6fc0\u589e\uff0c\u8fd9\u89e3\u91ca\u4e86\u5f52\u4e00\u5316\u5c42\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u6b8b\u5dee\u6a21\u5757\u7684\u7f29\u653e\u63d0\u4f9b\u4e86\u63a7\u5236\u7ec4\u5408\u7206\u70b8\u7684\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4e5f\u4f5c\u4e3a\u5bb9\u91cf\u63a7\u5236\u673a\u5236\u9690\u5f0f\u6b63\u5219\u5316\u6a21\u578b\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.03270", "categories": ["cs.LG", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03270", "abs": "https://arxiv.org/abs/2510.03270", "authors": ["Haolin Chen", "Shiyu Wang", "Can Qin", "Bo Pang", "Zuxin Liu", "Jielin Qiu", "Jianguo Zhang", "Yingbo Zhou", "Zeyuan Chen", "Ran Xu", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Weiran Yao"], "title": "CoDA: Coding LM via Diffusion Adaptation", "comment": null, "summary": "Diffusion language models promise bidirectional context and infilling\ncapabilities that autoregressive coders lack, yet practical systems remain\nheavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU\nwith a fully open-source training pipeline. CoDA pairs large-scale diffusion\npre-training with code-centric mid-training and instruction tuning, enabling\nconfidence-guided sampling that keeps inference latency competitive. On\nHumaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses\ndiffusion models up to 7B parameters. Our release includes model checkpoints,\nevaluation harnesses, and TPU training pipelines to accelerate research on\nlightweight diffusion-based coding assistants.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a1.7B\u53c2\u6570\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u4e3a\u4ee3\u7801\u751f\u6210\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6269\u6563\u9884\u8bad\u7ec3\u3001\u4ee3\u7801\u4e2d\u5fc3\u7684\u4e2d\u671f\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u5ef6\u8fdf\u7ade\u4e89\u529b\u7684\u540c\u65f6\uff0c\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5177\u6709\u53cc\u5411\u4e0a\u4e0b\u6587\u548c\u586b\u5145\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u8f83\u91cd\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u6027\u80fd\u4f18\u5f02\u7684\u6269\u6563\u4ee3\u7801\u751f\u6210\u6a21\u578b\u3002", "method": "\u91c7\u75281.7B\u53c2\u6570\u7684\u6269\u6563\u7f16\u7801\u5668\uff0c\u5728TPU\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u6269\u6563\u9884\u8bad\u7ec3\u3001\u4ee3\u7801\u4e2d\u5fc3\u7684\u4e2d\u671f\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u91c7\u6837\u6765\u4fdd\u6301\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728Humaneval\u3001MBPP\u548cEvalPlus\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoDA-1.7B-Instruct\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u53c2\u6570\u9ad8\u8fbe7B\u7684\u6269\u6563\u6a21\u578b\u3002", "conclusion": "CoDA\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u6269\u6563\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u6a21\u578b\u68c0\u67e5\u70b9\u3001\u8bc4\u4f30\u5de5\u5177\u548cTPU\u8bad\u7ec3\u6d41\u6c34\u7ebf\u6765\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2510.03601", "categories": ["cs.LG", "cs.DC", "cs.NI", "eess.SP", "I.2.6; C.2.4"], "pdf": "https://arxiv.org/pdf/2510.03601", "abs": "https://arxiv.org/abs/2510.03601", "authors": ["Wei-Lung Mao", "Chun-Chi Wang", "Po-Heng Chou", "Kai-Chun Liu", "Yu Tsao"], "title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation", "comment": "15 pages, 7 figures, and published in IEEE Sensors Journal", "summary": "The rising aging population has increased the importance of fall detection\n(FD) systems as an assistive technology, where deep learning techniques are\nwidely applied to enhance accuracy. FD systems typically use edge devices (EDs)\nworn by individuals to collect real-time data, which are transmitted to a cloud\ncenter (CC) or processed locally. However, this architecture faces challenges\nsuch as a limited ED model size and data transmission latency to the CC. Mobile\nedge computing (MEC), which allows computations at MEC servers deployed between\nEDs and CC, has been explored to address these challenges. We propose a\nmultilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC\nsplits the architecture into stations, each with a neural network model. If\nfront-end equipment cannot detect falls reliably, data are transmitted to a\nstation with more robust back-end computing. The knowledge distillation (KD)\napproach was employed to improve front-end detection accuracy by allowing\nhigh-power back-end stations to provide additional learning experiences,\nenhancing precision while reducing latency and processing loads. Simulation\nresults demonstrate that the KD approach improved accuracy by 11.65% on the\nSisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also\nreduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on\nthe SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD\nsystem exhibits improved accuracy and reduced latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MLMEC\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6280\u672f\u6765\u5e73\u8861\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u63d0\u5347\u548c\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u968f\u7740\u8001\u9f84\u5316\u4eba\u53e3\u589e\u52a0\uff0c\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u4f20\u7edf\u57fa\u4e8e\u8fb9\u7f18\u8bbe\u5907\u6216\u4e91\u8ba1\u7b97\u7684\u67b6\u6784\u9762\u4e34\u6a21\u578b\u5927\u5c0f\u9650\u5236\u548c\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u7684\u6311\u6218\uff0c\u9700\u8981\u5bfb\u627e\u5e73\u8861\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u5c42\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\uff08MLMEC\uff09\u6846\u67b6\uff0c\u5c06\u67b6\u6784\u5206\u4e3a\u591a\u4e2a\u7ad9\u70b9\uff0c\u6bcf\u4e2a\u7ad9\u70b9\u914d\u5907\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002\u524d\u7aef\u8bbe\u5907\u65e0\u6cd5\u53ef\u9760\u68c0\u6d4b\u8dcc\u5012\u65f6\uff0c\u6570\u636e\u4f1a\u4f20\u8f93\u5230\u5177\u6709\u66f4\u5f3a\u540e\u7aef\u8ba1\u7b97\u80fd\u529b\u7684\u7ad9\u70b9\u3002\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\uff0c\u8ba9\u9ad8\u529f\u7387\u540e\u7aef\u7ad9\u70b9\u4e3a\u524d\u7aef\u63d0\u4f9b\u989d\u5916\u5b66\u4e60\u7ecf\u9a8c\u3002", "result": "\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728SisFall\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u51c6\u786e\u738711.65%\uff0c\u5728FallAllD\u6570\u636e\u96c6\u4e0a\u63d0\u9ad82.78%\u3002MLMEC+KD\u76f8\u6bd4\u65e0KD\u7684MLMEC\uff0c\u5728FallAllD\u6570\u636e\u96c6\u4e0a\u964d\u4f4e\u6570\u636e\u5ef6\u8fdf\u738754.15%\uff0c\u5728SisFall\u6570\u636e\u96c6\u4e0a\u964d\u4f4e46.67%\u3002", "conclusion": "MLMEC\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf\u5728\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u65b9\u9762\u90fd\u5f97\u5230\u4e86\u663e\u8457\u6539\u5584\uff0c\u4e3a\u5b9e\u65f6\u8dcc\u5012\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03494", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03494", "abs": "https://arxiv.org/abs/2510.03494", "authors": ["Volodymyr Tkachuk", "Csaba Szepesv\u00e1ri", "Xiaoqi Tan"], "title": "Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^\u03c0$-Realizability and Concentrability", "comment": null, "summary": "We study finite-horizon offline reinforcement learning (RL) with function\napproximation for both policy evaluation and policy optimization. Prior work\nestablished that statistically efficient learning is impossible for either of\nthese problems when the only assumptions are that the data has good coverage\n(concentrability) and the state-action value function of every policy is\nlinearly realizable ($q^\\pi$-realizability) (Foster et al., 2021). Recently,\nTkachuk et al. (2024) gave a statistically efficient learner for policy\noptimization, if in addition the data is assumed to be given as trajectories.\nIn this work we present a statistically efficient learner for policy evaluation\nunder the same assumptions. Further, we show that the sample complexity of the\nlearner used by Tkachuk et al. (2024) for policy optimization can be improved\nby a tighter analysis.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6709\u9650\u65f6\u57df\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u5728\u8f68\u8ff9\u6570\u636e\u3001\u8986\u76d6\u6027\u548cq\u03c0\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\uff0c\u63d0\u51fa\u4e86\u7edf\u8ba1\u9ad8\u6548\u7684\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u6539\u8fdb\u4e86\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u5728\u4ec5\u6709\u6570\u636e\u8986\u76d6\u6027\u548cq\u03c0\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\uff0c\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u90fd\u65e0\u6cd5\u5b9e\u73b0\u7edf\u8ba1\u9ad8\u6548\u5b66\u4e60\u3002\u6700\u8fd1\u6709\u5de5\u4f5c\u8bc1\u660e\u4e86\u5728\u8f68\u8ff9\u6570\u636e\u5047\u8bbe\u4e0b\u7b56\u7565\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u7b56\u7565\u8bc4\u4f30\u95ee\u9898\u5e76\u6539\u8fdb\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "method": "\u5728\u8f68\u8ff9\u6570\u636e\u3001\u8986\u76d6\u6027\u548cq\u03c0\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\uff0c\u5f00\u53d1\u4e86\u7edf\u8ba1\u9ad8\u6548\u7684\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u5bf9\u73b0\u6709\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fdb\u884c\u4e86\u66f4\u4e25\u683c\u7684\u5206\u6790\u3002", "result": "\u6210\u529f\u63d0\u51fa\u4e86\u7edf\u8ba1\u9ad8\u6548\u7684\u7b56\u7565\u8bc4\u4f30\u5b66\u4e60\u5668\uff0c\u5e76\u663e\u8457\u6539\u8fdb\u4e86\u7b56\u7565\u4f18\u5316\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u754c\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u8f68\u8ff9\u6570\u636e\u3001\u8986\u76d6\u6027\u548cq\u03c0\u53ef\u5b9e\u73b0\u6027\u5047\u8bbe\u4e0b\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u8bc4\u4f30\u548c\u7b56\u7565\u4f18\u5316\u90fd\u53ef\u4ee5\u5b9e\u73b0\u7edf\u8ba1\u9ad8\u6548\u5b66\u4e60\uff0c\u4e3a\u79bb\u7ebfRL\u7684\u7406\u8bba\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2510.03271", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03271", "abs": "https://arxiv.org/abs/2510.03271", "authors": ["Zi Liang", "Zhiyao Wu", "Haoyang Shang", "Yulin Jin", "Qingqing Ye", "Huadi Zheng", "Peizhao Hu", "Haibo Hu"], "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary", "comment": "Source code: https://github.com/liangzid/DPS", "summary": "Decision boundary, the subspace of inputs where a machine learning model\nassigns equal classification probabilities to two classes, is pivotal in\nrevealing core model properties and interpreting behaviors. While analyzing the\ndecision boundary of large language models (LLMs) has raised increasing\nattention recently, constructing it for mainstream LLMs remains computationally\ninfeasible due to the enormous vocabulary-sequence sizes and the\nauto-regressive nature of LLMs. To address this issue, in this paper we propose\nDecision Potential Surface (DPS), a new notion for analyzing LLM decision\nboundary. DPS is defined on the confidences in distinguishing different\nsampling sequences for each input, which naturally captures the potential of\ndecision boundary. We prove that the zero-height isohypse in DPS is equivalent\nto the decision boundary of an LLM, with enclosed regions representing decision\nregions. By leveraging DPS, for the first time in the literature, we propose an\napproximate decision boundary construction algorithm, namely $K$-DPS, which\nonly requires K-finite times of sequence sampling to approximate an LLM's\ndecision boundary with negligible error. We theoretically derive the upper\nbounds for the absolute error, expected error, and the error concentration\nbetween K-DPS and the ideal DPS, demonstrating that such errors can be\ntrade-off with sampling times. Our results are empirically validated by\nextensive experiments across various LLMs and corpora.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u51b3\u7b56\u52bf\u80fd\u9762(DPS)\u4f5c\u4e3a\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u51b3\u7b56\u8fb9\u754c\u7684\u65b0\u6982\u5ff5\uff0c\u901a\u8fc7\u6709\u9650\u6b21\u5e8f\u5217\u91c7\u6837\u6765\u8fd1\u4f3c\u6784\u5efaLLM\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002", "motivation": "\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\u5bf9\u4e8e\u7406\u89e3\u6a21\u578b\u6838\u5fc3\u7279\u6027\u548c\u89e3\u91ca\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8bcd\u6c47\u5e8f\u5217\u89c4\u6a21\u5e9e\u5927\u548c\u81ea\u56de\u5f52\u7279\u6027\uff0c\u4e3a\u4e3b\u6d41LLMs\u6784\u5efa\u51b3\u7b56\u8fb9\u754c\u5728\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u52bf\u80fd\u9762(DPS)\u6982\u5ff5\uff0c\u5b9a\u4e49\u5728\u533a\u5206\u4e0d\u540c\u91c7\u6837\u5e8f\u5217\u7684\u7f6e\u4fe1\u5ea6\u4e0a\uff0c\u6355\u6349\u51b3\u7b56\u8fb9\u754c\u7684\u6f5c\u529b\u3002\u5f00\u53d1K-DPS\u7b97\u6cd5\uff0c\u4ec5\u9700K\u6b21\u6709\u9650\u5e8f\u5217\u91c7\u6837\u5373\u53ef\u8fd1\u4f3cLLM\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u4e86K-DPS\u4e0e\u7406\u60f3DPS\u4e4b\u95f4\u7684\u7edd\u5bf9\u8bef\u5dee\u3001\u671f\u671b\u8bef\u5dee\u548c\u8bef\u5dee\u96c6\u4e2d\u7684\u4e0a\u754c\uff0c\u8bc1\u660e\u8bef\u5dee\u53ef\u901a\u8fc7\u91c7\u6837\u6b21\u6570\u8fdb\u884c\u6743\u8861\u3002\u901a\u8fc7\u591a\u79cdLLM\u548c\u8bed\u6599\u5e93\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u679c\u3002", "conclusion": "DPS\u4e3a\u5206\u6790LLM\u51b3\u7b56\u8fb9\u754c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u65b0\u65b9\u6cd5\uff0cK-DPS\u7b97\u6cd5\u80fd\u591f\u4ee5\u53ef\u63a5\u53d7\u7684\u8bef\u5dee\u6709\u6548\u8fd1\u4f3c\u51b3\u7b56\u8fb9\u754c\uff0c\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2510.04622", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04622", "abs": "https://arxiv.org/abs/2510.04622", "authors": ["Youngjoon Lee", "Seongmin Cho", "Yehhyun Jo", "Jinu Gong", "Hyunjoo Jenny Lee", "Joonhyuk Kang"], "title": "Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI", "comment": "Under Review", "summary": "The limited data availability due to strict privacy regulations and\nsignificant resource demands severely constrains biomedical time-series AI\ndevelopment, which creates a critical gap between data requirements and\naccessibility. Synthetic data generation presents a promising solution by\nproducing artificial datasets that maintain the statistical properties of real\nbiomedical time-series data without compromising patient confidentiality. We\npropose a framework for synthetic biomedical time-series data generation based\non advanced forecasting models that accurately replicates complex\nelectrophysiological signals such as EEG and EMG with high fidelity. These\nsynthetic datasets preserve essential temporal and spectral properties of real\ndata, which enables robust analysis while effectively addressing data scarcity\nand privacy challenges. Our evaluations across multiple subjects demonstrate\nthat the generated synthetic data can serve as an effective substitute for real\ndata and also significantly boost AI model performance. The approach maintains\ncritical biomedical features while provides high scalability for various\napplications and integrates seamlessly into open-source repositories,\nsubstantially expanding resources for AI-driven biomedical research.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5148\u8fdb\u9884\u6d4b\u6a21\u578b\u7684\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u80fd\u9ad8\u4fdd\u771f\u590d\u5236EEG\u548cEMG\u7b49\u590d\u6742\u7535\u751f\u7406\u4fe1\u53f7\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u4e25\u683c\u7684\u9690\u79c1\u6cd5\u89c4\u548c\u8d44\u6e90\u9700\u6c42\u9650\u5236\u4e86\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217AI\u53d1\u5c55\uff0c\u5bfc\u81f4\u6570\u636e\u9700\u6c42\u4e0e\u53ef\u8bbf\u95ee\u6027\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8e\u5148\u8fdb\u9884\u6d4b\u6a21\u578b\u7684\u5408\u6210\u751f\u7269\u533b\u5b66\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u51c6\u786e\u590d\u5236\u590d\u6742\u7535\u751f\u7406\u4fe1\u53f7\u3002", "result": "\u5408\u6210\u6570\u636e\u4fdd\u6301\u4e86\u771f\u5b9e\u6570\u636e\u7684\u5173\u952e\u65f6\u95f4\u548c\u9891\u8c31\u7279\u6027\uff0c\u53ef\u4f5c\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u6709\u6548\u66ff\u4ee3\u54c1\uff0c\u663e\u8457\u63d0\u5347AI\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5173\u952e\u751f\u7269\u533b\u5b66\u7279\u5f81\u7684\u540c\u65f6\u63d0\u4f9b\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u65e0\u7f1d\u96c6\u6210\u5230\u5f00\u6e90\u5b58\u50a8\u5e93\u4e2d\uff0c\u5927\u5e45\u6269\u5c55AI\u9a71\u52a8\u751f\u7269\u533b\u5b66\u7814\u7a76\u7684\u8d44\u6e90\u3002"}}
{"id": "2510.03535", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03535", "abs": "https://arxiv.org/abs/2510.03535", "authors": ["William Anderson", "Seung Whan Chung", "Youngsoo Choi"], "title": "Sequential decoder training for improved latent space dynamics identification", "comment": null, "summary": "Accurate numerical solutions of partial differential equations are essential\nin many scientific fields but often require computationally expensive solvers,\nmotivating reduced-order models (ROMs). Latent Space Dynamics Identification\n(LaSDI) is a data-driven ROM framework that combines autoencoders with equation\ndiscovery to learn interpretable latent dynamics. However, enforcing latent\ndynamics during training can compromise reconstruction accuracy of the model\nfor simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that\nimproves reconstruction and prediction accuracy by sequentially learning\nadditional decoders to correct residual errors from previous stages. Applied to\nthe 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,\nachieving lower prediction errors and reduced training time across a wide range\nof architectures.", "AI": {"tldr": "\u63d0\u51fa\u591a\u9636\u6bb5LaSDI\u6846\u67b6\uff0c\u901a\u8fc7\u987a\u5e8f\u5b66\u4e60\u989d\u5916\u89e3\u7801\u5668\u6765\u4fee\u6b63\u6b8b\u5dee\u8bef\u5dee\uff0c\u63d0\u9ad8\u964d\u9636\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u6548\u7387", "motivation": "\u4f20\u7edfLaSDI\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f3a\u5236\u5b9e\u65bd\u6f5c\u5728\u52a8\u529b\u5b66\u53ef\u80fd\u4f1a\u635f\u5bb3\u6a21\u578b\u5bf9\u4eff\u771f\u6570\u636e\u7684\u91cd\u6784\u7cbe\u5ea6\uff0c\u9700\u8981\u6539\u8fdb", "method": "\u591a\u9636\u6bb5LaSDI\u6846\u67b6\uff0c\u987a\u5e8f\u5b66\u4e60\u989d\u5916\u89e3\u7801\u5668\u6765\u4fee\u6b63\u524d\u9636\u6bb5\u7684\u6b8b\u5dee\u8bef\u5dee", "result": "\u57281D-1V Vlasov\u65b9\u7a0b\u4e0a\uff0cmLaSDI\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6LaSDI\uff0c\u5b9e\u73b0\u66f4\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\u548c\u66f4\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4", "conclusion": "\u591a\u9636\u6bb5LaSDI\u6846\u67b6\u80fd\u6709\u6548\u63d0\u9ad8\u964d\u9636\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387"}}
{"id": "2510.03272", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03272", "abs": "https://arxiv.org/abs/2510.03272", "authors": ["Yukun Zhang", "Xueqing Zhou"], "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling", "comment": null, "summary": "The Transformer architecture has revolutionized artificial intelligence, yet\na principled theoretical understanding of its internal mechanisms remains\nelusive. This paper introduces a novel analytical framework that\nreconceptualizes the Transformer's discrete, layered structure as a continuous\nspatiotemporal dynamical system governed by a master Partial Differential\nEquation (PDE). Within this paradigm, we map core architectural components to\ndistinct mathematical operators: self-attention as a non-local interaction, the\nfeed-forward network as a local reaction, and, critically, residual connections\nand layer normalization as indispensable stabilization mechanisms. We do not\npropose a new model, but rather employ the PDE system as a theoretical probe to\nanalyze the mathematical necessity of these components. By comparing a standard\nTransformer with a PDE simulator that lacks explicit stabilizers, our\nexperiments provide compelling empirical evidence for our central thesis. We\ndemonstrate that without residual connections, the system suffers from\ncatastrophic representational drift, while the absence of layer normalization\nleads to unstable, explosive training dynamics. Our findings reveal that these\nseemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers\nrequired to tame an otherwise powerful but inherently unstable continuous\nsystem. This work offers a first-principles explanation for the Transformer's\ndesign and establishes a new paradigm for analyzing deep neural networks\nthrough the lens of continuous dynamics.", "AI": {"tldr": "\u5c06Transformer\u67b6\u6784\u91cd\u65b0\u6982\u5ff5\u5316\u4e3a\u7531\u4e3b\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u529b\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u662f\u7a33\u5b9a\u8be5\u7cfb\u7edf\u7684\u5fc5\u8981\u6570\u5b66\u7a33\u5b9a\u5668\u3002", "motivation": "\u867d\u7136Transformer\u67b6\u6784\u5df2\u7ecf\u5f7b\u5e95\u6539\u53d8\u4e86\u4eba\u5de5\u667a\u80fd\uff0c\u4f46\u5bf9\u5176\u5185\u90e8\u673a\u5236\u7684\u7406\u8bba\u7406\u89e3\u4ecd\u7136\u7f3a\u4e4f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8fde\u7eed\u52a8\u529b\u7cfb\u7edf\u89c6\u89d2\u63d0\u4f9b\u5bf9Transformer\u8bbe\u8ba1\u7684\u539f\u7406\u6027\u89e3\u91ca\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u5c06Transformer\u7684\u79bb\u6563\u5206\u5c42\u7ed3\u6784\u6620\u5c04\u4e3a\u8fde\u7eed\u65f6\u7a7a\u52a8\u529b\u7cfb\u7edf\uff0c\u5176\u4e2d\u81ea\u6ce8\u610f\u529b\u5bf9\u5e94\u975e\u5c40\u90e8\u76f8\u4e92\u4f5c\u7528\uff0c\u524d\u9988\u7f51\u7edc\u5bf9\u5e94\u5c40\u90e8\u53cd\u5e94\uff0c\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316\u5bf9\u5e94\u7a33\u5b9a\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6ca1\u6709\u6b8b\u5dee\u8fde\u63a5\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u8868\u793a\u6f02\u79fb\uff0c\u6ca1\u6709\u5c42\u5f52\u4e00\u5316\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u7206\u70b8\u6027\u8bad\u7ec3\u52a8\u6001\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u662f\u7a33\u5b9a\u7cfb\u7edf\u7684\u5fc5\u8981\u6570\u5b66\u7a33\u5b9a\u5668\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aTransformer\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7b2c\u4e00\u6027\u539f\u7406\u89e3\u91ca\uff0c\u5e76\u5efa\u7acb\u4e86\u901a\u8fc7\u8fde\u7eed\u52a8\u529b\u5b66\u89c6\u89d2\u5206\u6790\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.04927", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.04927", "abs": "https://arxiv.org/abs/2510.04927", "authors": ["Usman Akram", "Yiyue Chen", "Haris Vikalo"], "title": "Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data", "comment": null, "summary": "Training automatic modulation classification (AMC) models on centrally\naggregated data raises privacy concerns, incurs communication overhead, and\noften fails to confer robustness to channel shifts. Federated learning (FL)\navoids central aggregation by training on distributed clients but remains\nsensitive to class imbalance, non-IID client distributions, and limited labeled\nsamples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with\ntriplet-loss self-supervision on unlabeled I/Q sequences across clients,\nfollowed by per-client SVMs on small labeled sets. We establish convergence of\nthe federated representation learning procedure and a separability guarantee\nfor the downstream classifier under feature noise. Experiments on synthetic and\nover-the-air datasets show consistent gains over supervised FL baselines under\nheterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.", "AI": {"tldr": "FedSSL-AMC\uff1a\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u7684\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u6807\u7b7eI/Q\u5e8f\u5217\u8bad\u7ec3\u56e0\u679c\u65f6\u95f4\u81a8\u80c0CNN\uff0c\u7ed3\u5408\u5ba2\u6237\u7aefSVM\u5206\u7c7b\u5668\uff0c\u89e3\u51b3\u9690\u79c1\u3001\u901a\u4fe1\u5f00\u9500\u548c\u4fe1\u9053\u504f\u79fb\u95ee\u9898\u3002", "motivation": "\u96c6\u4e2d\u5f0fAMC\u8bad\u7ec3\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u3001\u901a\u4fe1\u5f00\u9500\u5927\u548c\u5bf9\u4fe1\u9053\u504f\u79fb\u4e0d\u9c81\u68d2\u7684\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60\u867d\u7136\u907f\u514d\u4e86\u6570\u636e\u96c6\u4e2d\u4f46\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u3001\u975eIID\u6570\u636e\u5206\u5e03\u548c\u6807\u7b7e\u6837\u672c\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faFedSSL-AMC\uff0c\u5728\u5ba2\u6237\u7aef\u4f7f\u7528\u65e0\u6807\u7b7eI/Q\u5e8f\u5217\u901a\u8fc7\u4e09\u5143\u7ec4\u635f\u5931\u81ea\u76d1\u7763\u8bad\u7ec3\u56e0\u679c\u65f6\u95f4\u81a8\u80c0CNN\uff0c\u7136\u540e\u5728\u6bcf\u4e2a\u5ba2\u6237\u7aef\u4f7f\u7528\u5c11\u91cf\u6807\u7b7e\u6570\u636e\u8bad\u7ec3SVM\u5206\u7c7b\u5668\u3002", "result": "\u5efa\u7acb\u4e86\u8054\u90a6\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\u7684\u6536\u655b\u6027\u548c\u4e0b\u6e38\u5206\u7c7b\u5668\u5728\u7279\u5f81\u566a\u58f0\u4e0b\u7684\u53ef\u5206\u79bb\u6027\u4fdd\u8bc1\u3002\u5728\u5408\u6210\u548c\u7a7a\u4e2d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u5f02\u6784SNR\u3001\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u548c\u975eIID\u6807\u7b7e\u5206\u5e03\u4e0b\uff0c\u76f8\u6bd4\u76d1\u7763FL\u57fa\u7ebf\u6709\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "FedSSL-AMC\u901a\u8fc7\u8054\u90a6\u81ea\u76d1\u7763\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86AMC\u4e2d\u7684\u9690\u79c1\u3001\u901a\u4fe1\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u5728\u5f02\u6784\u4fe1\u9053\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2510.03569", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03569", "abs": "https://arxiv.org/abs/2510.03569", "authors": ["Mohammad Mohaiminul Islam", "Thijs P. Kuipers", "Sharvaree Vadgama", "Coen de Vente", "Afsana Khan", "Clara I. S\u00e1nchez", "Erik J. Bekkers"], "title": "Longitudinal Flow Matching for Trajectory Modeling", "comment": null, "summary": "Generative models for sequential data often struggle with sparsely sampled\nand high-dimensional trajectories, typically reducing the learning of dynamics\nto pairwise transitions. We propose \\textit{Interpolative Multi-Marginal Flow\nMatching} (IMMFM), a framework that learns continuous stochastic dynamics\njointly consistent with multiple observed time points. IMMFM employs a\npiecewise-quadratic interpolation path as a smooth target for flow matching and\njointly optimizes drift and a data-driven diffusion coefficient, supported by a\ntheoretical condition for stable learning. This design captures intrinsic\nstochasticity, handles irregular sparse sampling, and yields subject-specific\ntrajectories. Experiments on synthetic benchmarks and real-world longitudinal\nneuroimaging datasets show that IMMFM outperforms existing methods in both\nforecasting accuracy and further downstream tasks.", "AI": {"tldr": "\u63d0\u51faIMMFM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u65f6\u95f4\u70b9\u8054\u5408\u5b66\u4e60\u8fde\u7eed\u968f\u673a\u52a8\u529b\u5b66\uff0c\u4f7f\u7528\u5206\u6bb5\u4e8c\u6b21\u63d2\u503c\u8def\u5f84\u4f5c\u4e3a\u6d41\u5339\u914d\u7684\u5e73\u6ed1\u76ee\u6807\uff0c\u80fd\u591f\u5904\u7406\u7a00\u758f\u91c7\u6837\u548c\u9ad8\u7ef4\u8f68\u8ff9\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u7a00\u758f\u91c7\u6837\u548c\u9ad8\u7ef4\u8f68\u8ff9\u65f6\u901a\u5e38\u53ea\u80fd\u5b66\u4e60\u6210\u5bf9\u8f6c\u79fb\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u4e2a\u89c2\u6d4b\u65f6\u95f4\u70b9\u7684\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u5206\u6bb5\u4e8c\u6b21\u63d2\u503c\u8def\u5f84\u4f5c\u4e3a\u6d41\u5339\u914d\u7684\u5e73\u6ed1\u76ee\u6807\uff0c\u8054\u5408\u4f18\u5316\u6f02\u79fb\u9879\u548c\u6570\u636e\u9a71\u52a8\u7684\u6269\u6563\u7cfb\u6570\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u6761\u4ef6\u786e\u4fdd\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0cIMMFM\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IMMFM\u80fd\u591f\u6355\u6349\u5185\u5728\u968f\u673a\u6027\u3001\u5904\u7406\u4e0d\u89c4\u5219\u7a00\u758f\u91c7\u6837\uff0c\u5e76\u751f\u6210\u7279\u5b9a\u5bf9\u8c61\u7684\u8f68\u8ff9\uff0c\u4e3a\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03273", "abs": "https://arxiv.org/abs/2510.03273", "authors": ["Chenhao Ye", "Ming Tang"], "title": "Learning without Global Backpropagation via Synergistic Information Distillation", "comment": null, "summary": "Backpropagation (BP), while foundational to deep learning, imposes two\ncritical scalability bottlenecks: update locking, where network modules remain\nidle until the entire backward pass completes, and high memory consumption due\nto storing activations for gradient computation. To address these limitations,\nwe introduce Synergistic Information Distillation (SID), a novel training\nframework that reframes deep learning as a cascade of local cooperative\nrefinement problems. In SID, a deep network is structured as a pipeline of\nmodules, each imposed with a local objective to refine a probabilistic belief\nabout the ground-truth target. This objective balances fidelity to the target\nwith consistency to the belief from its preceding module. By decoupling the\nbackward dependencies between modules, SID enables parallel training and hence\neliminates update locking and drastically reduces memory requirements.\nMeanwhile, this design preserves the standard feed-forward inference pass,\nmaking SID a versatile drop-in replacement for BP. We provide a theoretical\nfoundation, proving that SID guarantees monotonic performance improvement with\nnetwork depth. Empirically, SID consistently matches or surpasses the\nclassification accuracy of BP, exhibiting superior scalability and pronounced\nrobustness to label noise.Code is available at:\nhttps://github.com/ychAlbert/sid-bp", "AI": {"tldr": "\u63d0\u51faSynergistic Information Distillation (SID)\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6df1\u5ea6\u5b66\u4e60\u91cd\u6784\u4e3a\u5c40\u90e8\u534f\u540c\u7cbe\u70bc\u95ee\u9898\uff0c\u89e3\u51b3\u53cd\u5411\u4f20\u64ad\u7684\u66f4\u65b0\u9501\u5b9a\u548c\u9ad8\u5185\u5b58\u6d88\u8017\u95ee\u9898\uff0c\u5b9e\u73b0\u5e76\u884c\u8bad\u7ec3\u5e76\u4fdd\u6301\u524d\u5411\u63a8\u7406\u4e0d\u53d8\u3002", "motivation": "\u89e3\u51b3\u53cd\u5411\u4f20\u64ad(BP)\u7684\u4e24\u4e2a\u5173\u952e\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff1a\u66f4\u65b0\u9501\u5b9a\uff08\u7f51\u7edc\u6a21\u5757\u9700\u7b49\u5f85\u6574\u4e2a\u53cd\u5411\u4f20\u64ad\u5b8c\u6210\uff09\u548c\u9ad8\u5185\u5b58\u6d88\u8017\uff08\u5b58\u50a8\u6fc0\u6d3b\u503c\u7528\u4e8e\u68af\u5ea6\u8ba1\u7b97\uff09\u3002", "method": "\u5c06\u6df1\u5ea6\u7f51\u7edc\u6784\u5efa\u4e3a\u6a21\u5757\u7ba1\u9053\uff0c\u6bcf\u4e2a\u6a21\u5757\u65bd\u52a0\u5c40\u90e8\u76ee\u6807\u6765\u7cbe\u70bc\u5bf9\u771f\u5b9e\u76ee\u6807\u7684\u6982\u7387\u4fe1\u5ff5\u3002\u8be5\u76ee\u6807\u5e73\u8861\u5bf9\u76ee\u6807\u7684\u4fdd\u771f\u5ea6\u548c\u4e0e\u524d\u4e00\u4e2a\u6a21\u5757\u4fe1\u5ff5\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u89e3\u8026\u6a21\u5757\u95f4\u7684\u53cd\u5411\u4f9d\u8d56\u3002", "result": "\u7406\u8bba\u8bc1\u660eSID\u4fdd\u8bc1\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u65f6\u6027\u80fd\u5355\u8c03\u63d0\u5347\u3002\u5b9e\u9a8c\u8868\u660eSID\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u5339\u914d\u6216\u8d85\u8d8aBP\uff0c\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SID\u662f\u4e00\u4e2a\u901a\u7528\u7684BP\u66ff\u4ee3\u65b9\u6848\uff0c\u6d88\u9664\u4e86\u66f4\u65b0\u9501\u5b9a\uff0c\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u524d\u5411\u63a8\u7406\u8fc7\u7a0b\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.03576", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03576", "abs": "https://arxiv.org/abs/2510.03576", "authors": ["Bongseok Kim", "Jiahao Zhang", "Guang Lin"], "title": "BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems", "comment": "29 pages, 22 figures", "summary": "Deep learning has gained attention for solving PDEs, but the black-box nature\nof neural networks hinders precise enforcement of boundary conditions. To\naddress this, we propose a boundary condition-guaranteed evolutionary\nKolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,\nwe propose three distinct and combinable approaches for incorporating\nDirichlet, periodic, and Neumann boundary conditions into the network. For\nDirichlet problem, we use smooth and global Gaussian RBFs to construct\nunivariate basis functions for approximating the solution and to encode\nboundary information at the activation level of the network. To handle periodic\nproblems, we employ a periodic layer constructed from a set of sinusoidal\nfunctions to enforce the boundary conditions exactly. For a Neumann problem, we\ndevise a least-squares formulation to guide the parameter evolution toward\nsatisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the\nperiodic layer, and the evolutionary framework, we can perform accurate PDE\nsimulations while rigorously enforcing boundary conditions. For demonstration,\nwe conducted extensive numerical experiments on Dirichlet, Neumann, periodic,\nand mixed boundary value problems. The results indicate that BEKAN outperforms\nboth multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In\nconclusion, the proposed approach enhances the capability of KANs in solving\nPDE problems while satisfying boundary conditions, thereby facilitating\nadvancements in scientific computing and engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8fb9\u754c\u6761\u4ef6\u4fdd\u8bc1\u7684\u8fdb\u5316Kolmogorov-Arnold\u7f51\u7edc(BEKAN)\uff0c\u901a\u8fc7\u4e09\u79cd\u53ef\u7ec4\u5408\u7684\u65b9\u6cd5\u7cbe\u786e\u5904\u7406Dirichlet\u3001\u5468\u671f\u6027\u548cNeumann\u8fb9\u754c\u6761\u4ef6\uff0c\u5728PDE\u6c42\u89e3\u4e2d\u4f18\u4e8eMLP\u548cB\u6837\u6761KAN\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u6c42\u89e3PDE\u65b9\u9762\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u7684\"\u9ed1\u7bb1\"\u7279\u6027\u963b\u788d\u4e86\u8fb9\u754c\u6761\u4ef6\u7684\u7cbe\u786e\u6267\u884c\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\u6784\u9020\u5355\u53d8\u91cf\u57fa\u51fd\u6570\u8fd1\u4f3c\u89e3\u5e76\u5728\u7f51\u7edc\u6fc0\u6d3b\u5c42\u7f16\u7801\u8fb9\u754c\u4fe1\u606f\uff1b\u5468\u671f\u6027\u8fb9\u754c\u4f7f\u7528\u6b63\u5f26\u51fd\u6570\u6784\u9020\u7684\u5468\u671f\u5c42\uff1bNeumann\u8fb9\u754c\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u516c\u5f0f\u6307\u5bfc\u53c2\u6570\u6f14\u5316\u3002", "result": "\u5728Dirichlet\u3001Neumann\u3001\u5468\u671f\u6027\u548c\u6df7\u5408\u8fb9\u754c\u503c\u95ee\u9898\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6570\u503c\u5b9e\u9a8c\uff0cBEKAN\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u591a\u5c42\u611f\u77e5\u673a\u548cB\u6837\u6761KAN\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86KAN\u5728\u6c42\u89e3PDE\u95ee\u9898\u540c\u65f6\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u7684\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u79d1\u5b66\u8ba1\u7b97\u548c\u5de5\u7a0b\u5e94\u7528\u7684\u8fdb\u6b65\u3002"}}
{"id": "2510.03274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03274", "abs": "https://arxiv.org/abs/2510.03274", "authors": ["Tianao Zhang", "Zhiteng Li", "Xianglong Yan", "Haotong Qin", "Yong Guo", "Yulun Zhang"], "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models", "comment": null, "summary": "Diffusion large language models (dLLMs), which offer bidirectional context\nand flexible masked-denoising generation, are emerging as a compelling\nalternative to autoregressive (AR) LLMs. However, like AR LLMs, their model\nsizes continue to grow, motivating weight compression for deployment. Although\npost-training quantization (PTQ) is effective for AR LLMs, directly\ntransferring it to dLLMs at 2-bit leads to unsatisfactory performance. To\ntackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework\ntailored to dLLMs. Since masked-denoising activations in dLLMs differ from the\nfully visible signals assumed by standard PTQ methods, we introduce Masked\nCalibration Simulation (MCS) to align calibration with the timestep-dependent\nmasking, which yields more reliable calibrations. Moreover, we propose a\nData-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight\nrepresentations via an optimization algorithm. It performs iterative\napproximation guided by our simulated calibration data. In addition, under a\nstrict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a\nsensitivity-based precision allocation scheme that adaptively assigns bit width\nacross channel groups. When restricted to 2-bit precision, Quant-dLLM\nconsistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer\nPTQ methods on dLLMs. The code and models will be available at:\nhttps://github.com/ZTA2785/Quant-dLLM.", "AI": {"tldr": "\u63d0\u51fa\u4e86Quant-dLLM\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8d85\u4f4e\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\uff0c\u89e3\u51b3\u4e86\u6807\u51c6PTQ\u65b9\u6cd5\u57282\u4f4d\u91cf\u5316\u4e0b\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u53cc\u5411\u4e0a\u4e0b\u6587\u548c\u7075\u6d3b\u63a9\u7801\u53bb\u566a\u751f\u6210\u7684\u4f18\u70b9\uff0c\u4f46\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u6743\u91cd\u538b\u7f29\u90e8\u7f72\u3002\u6807\u51c6\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u5230dLLMs\u57282\u4f4d\u91cf\u5316\u65f6\u6027\u80fd\u4e0d\u7406\u60f3\u3002", "method": "1) \u63a9\u7801\u6821\u51c6\u6a21\u62df(MCS)\u5bf9\u9f50\u65f6\u95f4\u6b65\u76f8\u5173\u7684\u63a9\u7801\u6821\u51c6\uff1b2) \u6570\u636e\u611f\u77e5\u4efb\u610f\u987a\u5e8f\u91cf\u5316\u5668(DAQ)\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u5b66\u4e60\u8d85\u4f4e\u4f4d\u6743\u91cd\u8868\u793a\uff1b3) \u81ea\u9002\u5e94\u5757\u7ea7\u6df7\u5408\u7cbe\u5ea6(ABMP)\u57fa\u4e8e\u654f\u611f\u5ea6\u5206\u914d\u4f4d\u5bbd\u3002", "result": "\u5728\u4e25\u683c\u76842\u4f4d\u9884\u7b97\u4e0b\uff0cQuant-dLLM\u5728dLLMs\u4e0a\u59cb\u7ec8\u6bd4\u6700\u5148\u8fdb\u7684AR\u8fc1\u79fbPTQ\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Quant-dLLM\u662f\u9488\u5bf9\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u8d85\u4f4e\u4f4d\u91cf\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e862\u4f4d\u91cf\u5316\u6027\u80fd\u3002"}}
{"id": "2510.03578", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03578", "abs": "https://arxiv.org/abs/2510.03578", "authors": ["Haoran Li", "Chenhan Xiao", "Muhao Guo", "Yang Weng"], "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning", "comment": "30 pages, 6 figures", "summary": "Learning dynamics is essential for model-based control and Reinforcement\nLearning in engineering systems, such as robotics and power systems. However,\nlimited system measurements, such as those from low-resolution sensors, demand\nsample-efficient learning. Symmetry provides a powerful inductive bias by\ncharacterizing equivariant relations in system states to improve sample\nefficiency. While recent methods attempt to discover symmetries from data, they\ntypically assume a single global symmetry group and treat symmetry discovery\nand dynamic learning as separate tasks, leading to limited expressiveness and\nerror accumulation. In this paper, we propose the Latent Mixture of Symmetries\n(Latent MoS), an expressive model that captures a mixture of symmetry-governed\nlatent factors from complex dynamical measurements. Latent MoS focuses on\ndynamic learning while locally and provably preserving the underlying symmetric\ntransformations. To further capture long-term equivariance, we introduce a\nhierarchical architecture that stacks MoS blocks. Numerical experiments in\ndiverse physical systems demonstrate that Latent MoS outperforms\nstate-of-the-art baselines in interpolation and extrapolation tasks while\noffering interpretable latent representations suitable for future geometric and\nsafety-critical analyses.", "AI": {"tldr": "\u63d0\u51faLatent Mixture of Symmetries (Latent MoS)\u6a21\u578b\uff0c\u901a\u8fc7\u6355\u6349\u590d\u6742\u52a8\u6001\u6d4b\u91cf\u4e2d\u7684\u5bf9\u79f0\u6027\u6df7\u5408\u6765\u5b66\u4e60\u52a8\u529b\u5b66\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u63d2\u503c\u548c\u5916\u63a8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5355\u4e00\u5168\u5c40\u5bf9\u79f0\u7fa4\uff0c\u5e76\u5c06\u5bf9\u79f0\u6027\u53d1\u73b0\u548c\u52a8\u6001\u5b66\u4e60\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u5bfc\u81f4\u8868\u8fbe\u80fd\u529b\u6709\u9650\u548c\u8bef\u5dee\u7d2f\u79ef\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u4ece\u6709\u9650\u6d4b\u91cf\u4e2d\u5b66\u4e60\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "method": "\u63d0\u51faLatent MoS\u6a21\u578b\uff0c\u6355\u6349\u5bf9\u79f0\u6027\u4e3b\u5bfc\u7684\u6f5c\u5728\u56e0\u5b50\u6df7\u5408\uff0c\u5728\u5c40\u90e8\u53ef\u8bc1\u660e\u5730\u4fdd\u6301\u57fa\u7840\u5bf9\u79f0\u53d8\u6362\u3002\u5f15\u5165\u5206\u5c42\u67b6\u6784\u5806\u53e0MoS\u5757\u6765\u6355\u83b7\u957f\u671f\u7b49\u53d8\u6027\u3002", "result": "\u5728\u591a\u79cd\u7269\u7406\u7cfb\u7edf\u4e2d\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cLatent MoS\u5728\u63d2\u503c\u548c\u5916\u63a8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u8868\u793a\u3002", "conclusion": "Latent MoS\u901a\u8fc7\u5bf9\u79f0\u6027\u6df7\u5408\u6709\u6548\u5b66\u4e60\u590d\u6742\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4e3a\u672a\u6765\u51e0\u4f55\u548c\u5b89\u5168\u5173\u952e\u5206\u6790\u63d0\u4f9b\u57fa\u7840\u3002"}}
{"id": "2510.03275", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03275", "abs": "https://arxiv.org/abs/2510.03275", "authors": ["Junhao Xia", "Ming Zhao", "Limin Xiao", "Xiujun Zhang"], "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size", "comment": null, "summary": "Large language models (LLMs) face significant computational and memory\nchallenges, making extremely low-bit quantization crucial for their efficient\ndeployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for\n1-bit LLMs of any size, a novel framework that enables extremely low-bit\nquantization of LLMs while preserving their linguistic reasoning capabilities.\nA distinctive feature of SDQ-LLM is the continuous adjustability of the\nOver-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM\nconstraints by selecting fractional OSR (e.g. 2.5 times) for an optimal\ntrade-off between model size and accuracy. SDQ-LLM uses upsampling combined\nwith Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding\nhigh-precision parameters into 1-bit or 1.58-bit representations, replacing the\nmultiplication operations within linear layers with addition. This approach\nsignificantly enhances inference efficiency under extremely low-bit\nquantization. To further reduce the loss of quantization precision, we\nincorporate Hadamard-based weight smoothing prior to quantization, improving\nthe stability and robustness of the weight representations. Furthermore, to\nfully leverage the continuity of the OSR and reduce precision loss, recognizing\nthe correlation between quantization sensitivity and weight variance, we\npropose a fine-grained, layer- and linear-wise OSR allocation strategy,\nMultiOSR. This strategy distributes OSR both across layers and within each\nlayer, based on weight variance and parameter scale. Finally, extensive\nexperiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a\nmore efficient and high-precision performance even under highly aggressive\nlow-OSR settings. Our code is available at\nhttps://github.com/Dreamlittlecat/LLM-Quant-Factory.", "AI": {"tldr": "SDQ-LLM\u662f\u4e00\u4e2a\u7528\u4e8e1\u4f4dLLM\u91cf\u5316\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Sigma-Delta\u91cf\u5316\u5668\u548c\u8fc7\u91c7\u6837\u6280\u672f\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u538b\u7f29\u52301\u4f4d\u62161.58\u4f4d\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u9700\u8981\u6781\u4f4e\u4f4d\u91cf\u5316\u6765\u5b9e\u73b0\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u4e0a\u91c7\u6837\u7ed3\u5408Sigma-Delta\u91cf\u5316\u5668\u5bf9LLM\u6743\u91cd\u8fdb\u884c\u4e8c\u503c\u5316\u6216\u4e09\u503c\u5316\uff0c\u91c7\u7528Hadamard\u57fa\u6743\u91cd\u5e73\u6ed1\u51cf\u5c11\u91cf\u5316\u7cbe\u5ea6\u635f\u5931\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u7684MultiOSR\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728OPT\u548cLLaMA\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSDQ-LLM\u5728\u9ad8\u5ea6\u6fc0\u8fdb\u7684\u4f4eOSR\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u9ad8\u7cbe\u5ea6\u7684\u6027\u80fd\u3002", "conclusion": "SDQ-LLM\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u6781\u4f4e\u4f4dLLM\u91cf\u5316\uff0c\u5728\u6a21\u578b\u5927\u5c0f\u548c\u7cbe\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u6700\u4f73\u6743\u8861\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2510.03613", "categories": ["cs.LG", "stat.ML", "I.2.6; I.2.m"], "pdf": "https://arxiv.org/pdf/2510.03613", "abs": "https://arxiv.org/abs/2510.03613", "authors": ["Meenakshi Manikandan", "Leilani Gilpin"], "title": "Explore the Loss space with Hill-ADAM", "comment": "14-15 pages", "summary": "This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus\ntowards escaping local minima in prescribed loss landscapes to find the global\nminimum. Hill-ADAM escapes minima by deterministically exploring the state\nspace. This eliminates uncertainty from random gradient updates in stochastic\nalgorithms while seldom converging at the first minimum that visits. In the\npaper we first derive an analytical approximation of the ADAM Optimizer step\nsize at a particular model state. From there define the primary condition\ndetermining ADAM limitations in escaping local minima. The proposed optimizer\nalgorithm Hill-ADAM alternates between error minimization and maximization. It\nmaximizes to escape the local minimum and minimizes again afterward. This\nalternation provides an overall exploration throughout the loss space. This\nallows the deduction of the global minimum's state. Hill-ADAM was tested with 5\nloss functions and 12 amber-saturated to cooler-shade image color correction\ninstances.", "AI": {"tldr": "Hill-ADAM\u662f\u4e00\u79cd\u4e13\u6ce8\u4e8e\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\u4ee5\u5bfb\u627e\u5168\u5c40\u6700\u5c0f\u503c\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u63a2\u7d22\u72b6\u6001\u7a7a\u95f4\u6765\u907f\u514d\u968f\u673a\u68af\u5ea6\u66f4\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3ADAM\u4f18\u5316\u5668\u5728\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u627e\u5230\u5168\u5c40\u6700\u4f18\u89e3\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790ADAM\u4f18\u5316\u5668\u6b65\u957f\u7684\u89e3\u6790\u8fd1\u4f3c\uff0c\u63d0\u51faHill-ADAM\u7b97\u6cd5\uff0c\u5728\u8bef\u5dee\u6700\u5c0f\u5316\u548c\u6700\u5927\u5316\u4e4b\u95f4\u4ea4\u66ff\u8fdb\u884c\uff0c\u6700\u5927\u5316\u7528\u4e8e\u9003\u79bb\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u6700\u5c0f\u5316\u7528\u4e8e\u6536\u655b\u3002", "result": "\u57285\u4e2a\u635f\u5931\u51fd\u6570\u548c12\u4e2a\u56fe\u50cf\u989c\u8272\u6821\u6b63\u5b9e\u4f8b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Hill-ADAM\u901a\u8fc7\u4ea4\u66ff\u6700\u5c0f\u5316\u548c\u6700\u5927\u5316\u7684\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63a2\u7d22\u635f\u5931\u7a7a\u95f4\u5e76\u627e\u5230\u5168\u5c40\u6700\u5c0f\u503c\u3002"}}
{"id": "2510.03276", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03276", "abs": "https://arxiv.org/abs/2510.03276", "authors": ["Qian Chen", "Linxin Yang", "Akang Wang", "Xiaodong Luo", "Yin Zhang"], "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "The combination of linear transformations and non-linear activation functions\nforms the foundation of most modern deep neural networks, enabling them to\napproximate highly complex functions. This paper explores the introduction of\nquadratic transformations to further increase nonlinearity in neural networks,\nwith the aim of enhancing the performance of existing architectures. To reduce\nparameter complexity and computational complexity, we propose a lightweight\nquadratic enhancer that uses low-rankness, weight sharing, and sparsification\ntechniques. For a fixed architecture, the proposed approach introduces\nquadratic interactions between features at every layer, while only adding\nnegligible amounts of additional model parameters and forward computations. We\nconduct a set of proof-of-concept experiments for the proposed method across\nthree tasks: image classification, text classification, and fine-tuning\nlarge-language models. In all tasks, the proposed approach demonstrates clear\nand substantial performance gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u6765\u589e\u52a0\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\uff0c\u540c\u65f6\u4f7f\u7528\u4f4e\u79e9\u3001\u6743\u91cd\u5171\u4eab\u548c\u7a00\u758f\u5316\u6280\u672f\u6765\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165\u4e8c\u6b21\u53d8\u6362\u6765\u589e\u5f3a\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u975e\u7ebf\u6027\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u6027\u3001\u6743\u91cd\u5171\u4eab\u548c\u7a00\u758f\u5316\u6280\u672f\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u5668\uff0c\u5728\u6bcf\u5c42\u7279\u5f81\u4e4b\u95f4\u5f15\u5165\u4e8c\u6b21\u4ea4\u4e92\uff0c\u4ec5\u589e\u52a0\u5c11\u91cf\u989d\u5916\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u3002", "result": "\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u6587\u672c\u5206\u7c7b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u5747\u663e\u793a\u51fa\u660e\u663e\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u4e8c\u6b21\u589e\u5f3a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2510.03614", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03614", "abs": "https://arxiv.org/abs/2510.03614", "authors": ["Christopher Solinas", "Radovan Haluska", "David Sychrovsky", "Finbarr Timbers", "Nolan Bard", "Michael Buro", "Martin Schmid", "Nathan R. Sturtevant", "Michael Bowling"], "title": "Neural Bayesian Filtering", "comment": null, "summary": "We present Neural Bayesian Filtering (NBF), an algorithm for maintaining\ndistributions over hidden states, called beliefs, in partially observable\nsystems. NBF is trained to find a good latent representation of the beliefs\ninduced by a task. It maps beliefs to fixed-length embedding vectors, which\ncondition generative models for sampling. During filtering, particle-style\nupdates compute posteriors in this embedding space using incoming observations\nand the environment's dynamics. NBF combines the computational efficiency of\nclassical filters with the expressiveness of deep generative models - tracking\nrapidly shifting, multimodal beliefs while mitigating the risk of particle\nimpoverishment. We validate NBF in state estimation tasks in three partially\nobservable environments.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u8d1d\u53f6\u65af\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u7ef4\u62a4\u9690\u85cf\u72b6\u6001\u7684\u5206\u5e03\uff0c\u7ed3\u5408\u7ecf\u5178\u6ee4\u6ce2\u5668\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b", "motivation": "\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u4fe1\u5ff5\u8ddf\u8e2a\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u6ee4\u6ce2\u5668\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u800c\u7c92\u5b50\u6ee4\u6ce2\u5668\u5bb9\u6613\u906d\u9047\u7c92\u5b50\u8d2b\u4e4f\u95ee\u9898", "method": "\u8bad\u7ec3\u7b97\u6cd5\u627e\u5230\u4efb\u52a1\u8bf1\u5bfc\u4fe1\u5ff5\u7684\u826f\u597d\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u4fe1\u5ff5\u6620\u5c04\u4e3a\u56fa\u5b9a\u957f\u5ea6\u7684\u5d4c\u5165\u5411\u91cf\uff0c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4f7f\u7528\u7c92\u5b50\u5f0f\u66f4\u65b0\u8ba1\u7b97\u540e\u9a8c\u5206\u5e03", "result": "\u5728\u4e09\u4e2a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u7684\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86NBF\u7684\u6709\u6548\u6027", "conclusion": "NBF\u80fd\u591f\u8ddf\u8e2a\u5feb\u901f\u53d8\u5316\u7684\u591a\u6a21\u6001\u4fe1\u5ff5\uff0c\u540c\u65f6\u51cf\u8f7b\u7c92\u5b50\u8d2b\u4e4f\u7684\u98ce\u9669"}}
{"id": "2510.03278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03278", "abs": "https://arxiv.org/abs/2510.03278", "authors": ["Filip Landgren"], "title": "Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition", "comment": "5 pages, 2 figures", "summary": "Bayesian physics-informed neural networks (B-PINNs) merge data with governing\nequations to solve differential equations under uncertainty. However,\ninterpreting uncertainty and overconfidence in B-PINNs requires care due to the\npoorly understood effects the physical constraints have on the network;\noverconfidence could reflect warranted precision, enforced by the constraints,\nrather than miscalibration. Motivated by the need to further clarify how\nindividual physical constraints shape these networks, we introduce a scalable,\nmatrix-free Laplace framework that decomposes the posterior Hessian into\ncontributions from each constraint and provides metrics to quantify their\nrelative influence on the loss landscape. Applied to the Van der Pol equation,\nour method tracks how constraints sculpt the network's geometry and shows,\ndirectly through the Hessian, how changing a single loss weight non-trivially\nredistributes curvature and effective dominance across the others.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u77e9\u9635\u81ea\u7531\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u89e3\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7269\u7406\u7ea6\u675f\u5bf9\u540e\u9a8cHessian\u7684\u8d21\u732e\uff0c\u91cf\u5316\u5404\u7ea6\u675f\u5bf9\u635f\u5931\u666f\u89c2\u7684\u76f8\u5bf9\u5f71\u54cd\u3002", "motivation": "\u9700\u8981\u6f84\u6e05\u5355\u4e2a\u7269\u7406\u7ea6\u675f\u5982\u4f55\u5f71\u54cd\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u56e0\u4e3a\u7269\u7406\u7ea6\u675f\u53ef\u80fd\u5bfc\u81f4\u7f51\u7edc\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u8fd9\u79cd\u8fc7\u5ea6\u81ea\u4fe1\u53ef\u80fd\u53cd\u6620\u4e86\u7ea6\u675f\u5e26\u6765\u7684\u5408\u7406\u7cbe\u5ea6\u800c\u975e\u6821\u51c6\u9519\u8bef\u3002", "method": "\u5f15\u5165\u53ef\u6269\u5c55\u7684\u77e9\u9635\u81ea\u7531\u62c9\u666e\u62c9\u65af\u6846\u67b6\uff0c\u5c06\u540e\u9a8cHessian\u5206\u89e3\u4e3a\u6bcf\u4e2a\u7ea6\u675f\u7684\u8d21\u732e\uff0c\u5e76\u63d0\u4f9b\u91cf\u5316\u6307\u6807\u6765\u8861\u91cf\u5b83\u4eec\u5728\u635f\u5931\u666f\u89c2\u4e2d\u7684\u76f8\u5bf9\u5f71\u54cd\u529b\u3002", "result": "\u5e94\u7528\u4e8eVan der Pol\u65b9\u7a0b\u65f6\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u8ffd\u8e2a\u7ea6\u675f\u5982\u4f55\u5851\u9020\u7f51\u7edc\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7Hessian\u663e\u793a\u5355\u4e2a\u635f\u5931\u6743\u91cd\u7684\u53d8\u5316\u5982\u4f55\u975e\u5e73\u51e1\u5730\u91cd\u65b0\u5206\u5e03\u66f2\u7387\u548c\u6709\u6548\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u7269\u7406\u7ea6\u675f\u5728\u8d1d\u53f6\u65af\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u63ed\u793a\u7ea6\u675f\u5bf9\u7f51\u7edc\u51e0\u4f55\u5f62\u72b6\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u5177\u4f53\u4f5c\u7528\u673a\u5236\u3002"}}
{"id": "2510.03638", "categories": ["cs.LG", "cs.AI", "math.RT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03638", "abs": "https://arxiv.org/abs/2510.03638", "authors": ["Jialin Liu", "Lisang Ding", "Stanley Osher", "Wotao Yin"], "title": "Implicit Models: Expressive Power Scales with Test-Time Compute", "comment": null, "summary": "Implicit models, an emerging model class, compute outputs by iterating a\nsingle parameter block to a fixed point. This architecture realizes an\ninfinite-depth, weight-tied network that trains with constant memory,\nsignificantly reducing memory needs for the same level of performance compared\nto explicit models. While it is empirically known that these compact models can\noften match or even exceed larger explicit networks by allocating more\ntest-time compute, the underlying mechanism remains poorly understood.\n  We study this gap through a nonparametric analysis of expressive power. We\nprovide a strict mathematical characterization, showing that a simple and\nregular implicit operator can, through iteration, progressively express more\ncomplex mappings. We prove that for a broad class of implicit models, this\nprocess lets the model's expressive power scale with test-time compute,\nultimately matching a much richer function class. The theory is validated\nacross three domains: image reconstruction, scientific computing, and\noperations research, demonstrating that as test-time iterations increase, the\ncomplexity of the learned mapping rises, while the solution quality\nsimultaneously improves and stabilizes.", "AI": {"tldr": "\u9690\u5f0f\u6a21\u578b\u901a\u8fc7\u5355\u53c2\u6570\u5757\u8fed\u4ee3\u5230\u56fa\u5b9a\u70b9\u8ba1\u7b97\u8f93\u51fa\uff0c\u5b9e\u73b0\u65e0\u9650\u6df1\u5ea6\u3001\u6743\u91cd\u7ed1\u5b9a\u7684\u7f51\u7edc\uff0c\u8bad\u7ec3\u65f6\u5185\u5b58\u9700\u6c42\u4f4e\u3002\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u7d27\u51d1\u6a21\u578b\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\uff0c\u53ef\u4ee5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u7684\u663e\u5f0f\u7f51\u7edc\u3002", "motivation": "\u7406\u89e3\u9690\u5f0f\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6765\u63d0\u5347\u8868\u8fbe\u80fd\u529b\u7684\u5185\u5728\u673a\u5236\uff0c\u76ee\u524d\u8fd9\u65b9\u9762\u7814\u7a76\u8fd8\u5f88\u7f3a\u4e4f\u3002", "method": "\u901a\u8fc7\u975e\u53c2\u6570\u5316\u5206\u6790\u7814\u7a76\u8868\u8fbe\u80fd\u529b\uff0c\u8bc1\u660e\u7b80\u5355\u7684\u9690\u5f0f\u7b97\u5b50\u901a\u8fc7\u8fed\u4ee3\u53ef\u4ee5\u9010\u6b65\u8868\u8fbe\u66f4\u590d\u6742\u7684\u6620\u5c04\uff0c\u4f7f\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u968f\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u91cf\u6269\u5c55\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u9690\u5f0f\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u53ef\u4ee5\u968f\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u6b21\u6570\u589e\u52a0\u800c\u6269\u5c55\uff0c\u6700\u7ec8\u5339\u914d\u66f4\u4e30\u5bcc\u7684\u51fd\u6570\u7c7b\u3002\u5728\u56fe\u50cf\u91cd\u5efa\u3001\u79d1\u5b66\u8ba1\u7b97\u548c\u8fd0\u7b79\u5b66\u4e09\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "conclusion": "\u9690\u5f0f\u6a21\u578b\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u5b9e\u73b0\u4e86\u8868\u8fbe\u80fd\u529b\u4e0e\u8ba1\u7b97\u91cf\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5728\u4fdd\u6301\u7d27\u51d1\u67b6\u6784\u7684\u540c\u65f6\u80fd\u591f\u8868\u8fbe\u590d\u6742\u6620\u5c04\uff0c\u540c\u65f6\u63d0\u9ad8\u89e3\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.03279", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03279", "abs": "https://arxiv.org/abs/2510.03279", "authors": ["Youjin Wang", "Yangjingyi Chen", "Jiahao Yan", "Jiaxuan Lu", "Xiao Sun"], "title": "MemMamba: Rethinking Memory Patterns in State Space Model", "comment": null, "summary": "With the explosive growth of data, long-sequence modeling has become\nincreasingly important in tasks such as natural language processing and\nbioinformatics. However, existing methods face inherent trade-offs between\nefficiency and memory. Recurrent neural networks suffer from gradient vanishing\nand explosion, making them hard to scale. Transformers can model global\ndependencies but are constrained by quadratic complexity. Recently, selective\nstate-space models such as Mamba have demonstrated high efficiency with O(n)\ntime and O(1) recurrent inference, yet their long-range memory decays\nexponentially. In this work, we conduct mathematical derivations and\ninformation-theoretic analysis to systematically uncover the memory decay\nmechanism of Mamba, answering a fundamental question: what is the nature of\nMamba's long-range memory and how does it retain information? To quantify key\ninformation loss, we further introduce horizontal-vertical memory fidelity\nmetrics that capture degradation both within and across layers. Inspired by how\nhumans distill and retain salient information when reading long documents, we\npropose MemMamba, a novel architectural framework that integrates state\nsummarization mechanism together with cross-layer and cross-token attention,\nwhich alleviates long-range forgetting while preserving linear complexity.\nMemMamba achieves significant improvements over existing Mamba variants and\nTransformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,\nwhile delivering a 48% speedup in inference efficiency. Both theoretical\nanalysis and empirical results demonstrate that MemMamba achieves a\nbreakthrough in the complexity-memory trade-off, offering a new paradigm for\nultra-long sequence modeling.", "AI": {"tldr": "MemMamba\u901a\u8fc7\u72b6\u6001\u603b\u7ed3\u673a\u5236\u548c\u8de8\u5c42\u8de8token\u6ce8\u610f\u529b\uff0c\u89e3\u51b3\u4e86Mamba\u6a21\u578b\u7684\u957f\u7a0b\u8bb0\u5fc6\u8870\u51cf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u5efa\u6a21\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u957f\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4e0e\u5185\u5b58\u7684\u6743\u8861\uff1aRNN\u6709\u68af\u5ea6\u95ee\u9898\uff0cTransformer\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0cMamba\u867d\u7136\u9ad8\u6548\u4f46\u957f\u7a0b\u8bb0\u5fc6\u5448\u6307\u6570\u8870\u51cf\u3002\u9700\u8981\u89e3\u51b3Mamba\u7684\u957f\u7a0b\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51faMemMamba\u6846\u67b6\uff0c\u96c6\u6210\u72b6\u6001\u603b\u7ed3\u673a\u5236\u4ee5\u53ca\u8de8\u5c42\u548c\u8de8token\u6ce8\u610f\u529b\uff0c\u7075\u611f\u6765\u81ea\u4eba\u7c7b\u9605\u8bfb\u957f\u6587\u6863\u65f6\u63d0\u70bc\u548c\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684\u65b9\u5f0f\u3002", "result": "\u5728PG19\u548cPasskey Retrieval\u7b49\u957f\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709Mamba\u53d8\u4f53\u548cTransformer\uff0c\u63a8\u7406\u6548\u7387\u63d0\u534748%\u3002", "conclusion": "MemMamba\u5728\u590d\u6742\u5ea6-\u5185\u5b58\u6743\u8861\u4e0a\u53d6\u5f97\u7a81\u7834\uff0c\u4e3a\u8d85\u957f\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.03659", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03659", "abs": "https://arxiv.org/abs/2510.03659", "authors": ["Xu Wang", "Yan Hu", "Benyou Wang", "Difan Zou"], "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders", "comment": "24 pages", "summary": "Sparse Autoencoders (SAEs) are widely used to steer large language models\n(LLMs), based on the assumption that their interpretable features naturally\nenable effective model behavior steering. Yet, a fundamental question remains\nunanswered: does higher interpretability indeed imply better steering utility?\nTo answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,\nQwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,\nand evaluate their interpretability and steering utility based on SAEBench\n(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a\nrank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis\nreveals only a relatively weak positive association (tau b approx 0.298),\nindicating that interpretability is an insufficient proxy for steering\nperformance. We conjecture the interpretability utility gap may stem from the\nselection of SAE features, as not all of them are equally effective for\nsteering. To further find features that truly steer the behavior of LLMs, we\npropose a novel selection criterion called Delta Token Confidence, which\nmeasures how much amplifying a feature changes the next token distribution. We\nshow that our method improves the steering performance of three LLMs by 52.52\npercent compared to the current best output score based criterion\n(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token\nConfidence, the correlation between interpretability and utility vanishes (tau\nb approx 0), and can even become negative. This further highlights the\ndivergence between interpretability and utility for the most effective steering\nfeatures.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6a21\u578b\u884c\u4e3a\u63a7\u5236\u6548\u7528\u4e4b\u95f4\u4ec5\u5b58\u5728\u5f31\u6b63\u76f8\u5173\uff0c\u63d0\u51fa\u65b0\u7684\u7279\u5f81\u9009\u62e9\u6807\u51c6Delta Token Confidence\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u6700\u6709\u6548\u7684\u63a7\u5236\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u5206\u6b67\u3002", "motivation": "\u9a8c\u8bc1\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7684\u53ef\u89e3\u91ca\u6027\u662f\u5426\u786e\u5b9e\u610f\u5473\u7740\u66f4\u597d\u7684\u6a21\u578b\u884c\u4e3a\u63a7\u5236\u6548\u7528\uff0c\u56e0\u4e3a\u8fd9\u4e00\u57fa\u672c\u5047\u8bbe\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u5728\u4e09\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8bad\u7ec390\u4e2aSAE\uff0c\u6db5\u76d65\u79cd\u67b6\u6784\u548c6\u79cd\u7a00\u758f\u5ea6\u6c34\u5e73\uff0c\u4f7f\u7528SAEBench\u548cAxBench\u5206\u522b\u8bc4\u4f30\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u6548\u7528\uff0c\u5e76\u901a\u8fc7Kendall\u79e9\u76f8\u5173\u7cfb\u6570\u8fdb\u884c\u79e9\u4e00\u81f4\u6027\u5206\u6790\u3002\u63d0\u51faDelta Token Confidence\u7279\u5f81\u9009\u62e9\u6807\u51c6\u6765\u8861\u91cf\u7279\u5f81\u653e\u5927\u5bf9\u4e0b\u4e00\u4e2atoken\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u53ef\u89e3\u91ca\u6027\u4e0e\u63a7\u5236\u6548\u7528\u4e4b\u95f4\u4ec5\u5b58\u5728\u5f31\u6b63\u76f8\u5173(tau b \u2248 0.298)\uff1b\u4f7f\u7528Delta Token Confidence\u9009\u62e9\u7279\u5f81\u540e\uff0c\u63a7\u5236\u6027\u80fd\u6bd4\u5f53\u524d\u6700\u4f73\u8f93\u51fa\u5206\u6570\u6807\u51c6\u63d0\u5347\u4e8652.52%\uff1b\u9009\u62e9\u9ad8Delta Token Confidence\u7279\u5f81\u540e\uff0c\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7528\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u6d88\u5931\u751a\u81f3\u53d8\u4e3a\u8d1f\u76f8\u5173\u3002", "conclusion": "\u53ef\u89e3\u91ca\u6027\u4e0d\u80fd\u4f5c\u4e3a\u63a7\u5236\u6027\u80fd\u7684\u5145\u5206\u4ee3\u7406\u6307\u6807\uff0c\u6700\u6709\u6548\u7684\u63a7\u5236\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u6548\u7528\u4e4b\u95f4\u5b58\u5728\u5206\u6b67\uff0cDelta Token Confidence\u662f\u66f4\u6709\u6548\u7684\u7279\u5f81\u9009\u62e9\u6807\u51c6\u3002"}}
{"id": "2510.03280", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03280", "abs": "https://arxiv.org/abs/2510.03280", "authors": ["Jinjie Ni", "Qian Liu", "Chao Du", "Longxu Dou", "Hang Yan", "Zili Wang", "Tianyu Pang", "Michael Qizhe Shieh"], "title": "Training Optimal Large Diffusion Language Models", "comment": null, "summary": "We introduce Quokka, the first systematic scaling law for diffusion language\nmodels (DLMs), encompassing both compute-constrained and data-constrained\nregimes, and studying the key modeling and optimization designs. Quokka is a\ngood friend of Chinchilla and provides wider scopes. We hope the results would\nbring short-term practical guidance in DLMs training and long-term inspirations\nfor the whole AI community.", "AI": {"tldr": "Quokka\u662f\u9996\u4e2a\u9488\u5bf9\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u5316\u7f29\u653e\u5b9a\u5f8b\uff0c\u6db5\u76d6\u8ba1\u7b97\u53d7\u9650\u548c\u6570\u636e\u53d7\u9650\u4e24\u79cd\u573a\u666f\uff0c\u7814\u7a76\u5173\u952e\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u77ed\u671f\u5b9e\u8df5\u6307\u5bfc\uff0c\u5e76\u4e3a\u6574\u4e2aAI\u793e\u533a\u5e26\u6765\u957f\u671f\u542f\u53d1\u3002", "method": "\u5efa\u7acb\u7cfb\u7edf\u5316\u7f29\u653e\u5b9a\u5f8b\uff0c\u5206\u6790\u8ba1\u7b97\u53d7\u9650\u548c\u6570\u636e\u53d7\u9650\u60c5\u51b5\u4e0b\u7684\u5173\u952e\u5efa\u6a21\u548c\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "\u5f00\u53d1\u51faQuokka\u7f29\u653e\u5b9a\u5f8b\uff0c\u4f5c\u4e3aChinchilla\u7684\u8865\u5145\uff0c\u63d0\u4f9b\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u8303\u56f4\u3002", "conclusion": "Quokka\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7f29\u653e\u6307\u5bfc\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.03678", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03678", "abs": "https://arxiv.org/abs/2510.03678", "authors": ["Zhao Song", "Shenghao Xie", "Samson Zhou"], "title": "Towards Sampling Data Structures for Tensor Products in Turnstile Streams", "comment": null, "summary": "This paper studies the computational challenges of large-scale\nattention-based models in artificial intelligence by utilizing importance\nsampling methods in the streaming setting. Inspired by the classical definition\nof the $\\ell_2$ sampler and the recent progress of the attention scheme in\nLarge Language Models (LLMs), we propose the definition of the attention\nsampler. Our approach significantly reduces the computational burden of\ntraditional attention mechanisms. We analyze the effectiveness of the attention\nsampler from a theoretical perspective, including space and update time.\nAdditionally, our framework exhibits scalability and broad applicability across\nvarious model architectures and domains.", "AI": {"tldr": "\u63d0\u51fa\u6ce8\u610f\u529b\u91c7\u6837\u5668\u5b9a\u4e49\uff0c\u5229\u7528\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u7684\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5e76\u5206\u6790\u5176\u7406\u8bba\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u6a21\u578b\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u53d7\u7ecf\u5178\u21132\u91c7\u6837\u5668\u548cLLM\u4e2d\u6ce8\u610f\u529b\u65b9\u6848\u8fdb\u5c55\u7684\u542f\u53d1\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u91c7\u6837\u5668\u5b9a\u4e49\uff0c\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e2d\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u5206\u6790\u7a7a\u95f4\u548c\u66f4\u65b0\u65f6\u95f4\u7b49\u7406\u8bba\u7279\u6027\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u8de8\u6a21\u578b\u67b6\u6784\u53ca\u9886\u57df\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u6ce8\u610f\u529b\u91c7\u6837\u5668\u4e3a\u5927\u89c4\u6a21\u6ce8\u610f\u529b\u6a21\u578b\u63d0\u4f9b\u6709\u6548\u7684\u8ba1\u7b97\u4f18\u5316\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.03282", "categories": ["cs.LG", "cs.CL", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03282", "abs": "https://arxiv.org/abs/2510.03282", "authors": ["Hao Gu", "Vibhas Nair", "Amrithaa Ashok Kumar", "Jayvart Sharma", "Ryan Lagasse"], "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework", "comment": "Accepted to the NeurIPS 2025 Workshop on Mechanistic Interpretability\n  (Mechinterp) and the NeurIPS 2025 Workshop on New Perspectives in Graph\n  Machine Learning", "summary": "Interpreting language models often involves circuit analysis, which aims to\nidentify sparse subnetworks, or circuits, that accomplish specific tasks.\nExisting circuit discovery algorithms face a fundamental trade-off: attribution\npatching is fast but unfaithful to the full model, while edge pruning is\nfaithful but computationally expensive. This research proposes a hybrid\nattribution and pruning (HAP) framework that uses attribution patching to\nidentify a high-potential subgraph, then applies edge pruning to extract a\nfaithful circuit from it. We show that HAP is 46\\% faster than baseline\nalgorithms without sacrificing circuit faithfulness. Furthermore, we present a\ncase study on the Indirect Object Identification task, showing that our method\npreserves cooperative circuit components (e.g. S-inhibition heads) that\nattribution patching methods prune at high sparsity. Our results show that HAP\ncould be an effective approach for improving the scalability of mechanistic\ninterpretability research to larger models. Our code is available at\nhttps://anonymous.4open.science/r/HAP-circuit-discovery.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u5f52\u56e0\u548c\u526a\u679d\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u7535\u8def\u5fe0\u5b9e\u5ea6\u7684\u540c\u65f6\u63d0\u534746%\u7684\u8ba1\u7b97\u6548\u7387", "motivation": "\u73b0\u6709\u7535\u8def\u53d1\u73b0\u7b97\u6cd5\u9762\u4e34\u57fa\u672c\u6743\u8861\uff1a\u5f52\u56e0\u4fee\u8865\u901f\u5ea6\u5feb\u4f46\u4e0d\u5fe0\u5b9e\uff0c\u8fb9\u526a\u679d\u5fe0\u5b9e\u4f46\u8ba1\u7b97\u6602\u8d35", "method": "\u4f7f\u7528\u5f52\u56e0\u4fee\u8865\u8bc6\u522b\u9ad8\u6f5c\u529b\u5b50\u56fe\uff0c\u7136\u540e\u5e94\u7528\u8fb9\u526a\u679d\u4ece\u4e2d\u63d0\u53d6\u5fe0\u5b9e\u7535\u8def", "result": "\u6bd4\u57fa\u7ebf\u7b97\u6cd5\u5feb46%\u4e14\u4e0d\u727a\u7272\u7535\u8def\u5fe0\u5b9e\u5ea6\uff0c\u5728\u95f4\u63a5\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u4e2d\u4fdd\u7559\u5408\u4f5c\u7535\u8def\u7ec4\u4ef6", "conclusion": "HAP\u662f\u63d0\u9ad8\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2510.03679", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03679", "abs": "https://arxiv.org/abs/2510.03679", "authors": ["Junhua Chen", "Zixi Zhang", "Hantao Zhong", "Rika Antonova"], "title": "Group Policy Gradient", "comment": null, "summary": "We introduce Group Policy Gradient (GPG), a family of critic-free\npolicy-gradient estimators for general MDPs. Inspired by the success of GRPO's\napproach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a\nlearned value function with a group-based Monte Carlo advantage estimator,\nremoving the memory, compute, and hyperparameter costs of training a critic\nwhile preserving PPO's clipped-objective structure. We prove the consistency of\nthe GPG estimator, analyze the bias-variance tradeoffs, and demonstrate\nempirically that GPG matches or outperforms PPO on standard benchmarks. GPG\nmakes better use of parallel simulations, which, together with its critic-free\ndesign, results in more efficient use of computational resources than PPO.", "AI": {"tldr": "GPG\u662f\u4e00\u79cd\u65e0\u8bc4\u8bba\u5bb6\u7684\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u5668\u5bb6\u65cf\uff0c\u7528\u57fa\u4e8e\u7ec4\u7684\u8499\u7279\u5361\u6d1b\u4f18\u52bf\u4f30\u8ba1\u5668\u66ff\u4ee3\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u5728\u4fdd\u6301PPO\u7ed3\u6784\u7684\u540c\u65f6\u6d88\u9664\u8bc4\u8bba\u5bb6\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u53d7GRPO\u5728RLHF\u4e2d\u6210\u529f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u6d88\u9664\u8bad\u7ec3\u8bc4\u8bba\u5bb6\u6240\u9700\u7684\u5185\u5b58\u3001\u8ba1\u7b97\u548c\u8d85\u53c2\u6570\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301PPO\u7684\u88c1\u526a\u76ee\u6807\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ec4\u7684\u8499\u7279\u5361\u6d1b\u4f18\u52bf\u4f30\u8ba1\u5668\u66ff\u4ee3\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u4fdd\u7559PPO\u7684\u88c1\u526a\u76ee\u6807\u7ed3\u6784\uff0c\u65e0\u9700\u8bad\u7ec3\u8bc4\u8bba\u5bb6\u7f51\u7edc\u3002", "result": "\u7ecf\u9a8c\u8bc1\u660eGPG\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u4f18\u4e8ePPO\uff0c\u80fd\u66f4\u597d\u5229\u7528\u5e76\u884c\u6a21\u62df\uff0c\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "GPG\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bc4\u8bba\u5bb6\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.03283", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03283", "abs": "https://arxiv.org/abs/2510.03283", "authors": ["Yufei Li", "Yu Fu", "Yue Dong", "Cong Liu"], "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment", "comment": "14 pages, 15 figures", "summary": "Large language models (LLMs) deployed on edge servers are increasingly used\nin latency-sensitive applications such as personalized assistants,\nrecommendation, and content moderation. However, the non-stationary nature of\nuser data necessitates frequent retraining, which introduces a fundamental\ntension between inference latency and model accuracy under constrained GPU\nresources. Existing retraining strategies either delay model updates,\nover-commit resources to retraining, or overlook iteration-level retraining\ngranularity. In this paper, we identify that iteration-level scheduling is\ncrucial for adapting retraining frequency to model drift without violating\nservice-level objectives (SLOs). We propose MACE, a hybrid LLM system that\ncolocates concurrent inference (prefill, decode) and fine-tuning, with\nintelligent memory management to maximize task performance while promising\ninference throughput. MACE leverages the insight that not all model updates\nequally affect output alignment and allocates GPU cycles accordingly to balance\nthroughput, latency, and update freshness. Our trace-driven evaluation shows\nthat MACE matches or exceeds continuous retraining while reducing inference\nlatency by up to 63% and maintaining throughput under resource constraints.\nCompared to periodic retraining, MACE improves latency breakdown across\nprefill, decode, and finetune stages, and sustains GPU utilization above 85% in\nNVIDIA AGX Orin. These results demonstrate that iteration-level hybrid\nscheduling is a promising direction for deploying LLMs with continual learning\ncapabilities on edge platforms.", "AI": {"tldr": "MACE\u662f\u4e00\u4e2a\u6df7\u5408LLM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u534f\u540c\u8c03\u5ea6\u63a8\u7406\u548c\u5fae\u8c03\u4efb\u52a1\uff0c\u5b9e\u73b0\u8fed\u4ee3\u7ea7\u522b\u7684\u8d44\u6e90\u5206\u914d\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u66f4\u65b0\u9891\u7387\u3002", "motivation": "\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u7684LLM\u5e94\u7528\u9700\u8981\u9891\u7e41\u91cd\u8bad\u7ec3\u4ee5\u9002\u5e94\u975e\u7a33\u6001\u7528\u6237\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u5ef6\u8fdf\u548c\u6a21\u578b\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u8981\u4e48\u5ef6\u8fdf\u6a21\u578b\u66f4\u65b0\uff0c\u8981\u4e48\u8fc7\u5ea6\u5206\u914d\u8d44\u6e90\u7ed9\u91cd\u8bad\u7ec3\u3002", "method": "\u63d0\u51faMACE\u7cfb\u7edf\uff0c\u5c06\u63a8\u7406\uff08\u9884\u586b\u5145\u3001\u89e3\u7801\uff09\u548c\u5fae\u8c03\u4efb\u52a1\u534f\u540c\u90e8\u7f72\uff0c\u91c7\u7528\u667a\u80fd\u5185\u5b58\u7ba1\u7406\uff0c\u6839\u636e\u6a21\u578b\u66f4\u65b0\u5bf9\u8f93\u51fa\u5bf9\u9f50\u7684\u5f71\u54cd\u7a0b\u5ea6\u5206\u914dGPU\u5468\u671f\uff0c\u5b9e\u73b0\u8fed\u4ee3\u7ea7\u522b\u7684\u6df7\u5408\u8c03\u5ea6\u3002", "result": "MACE\u5728\u4fdd\u6301\u541e\u5410\u91cf\u7684\u540c\u65f6\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe63%\uff0c\u5728NVIDIA AGX Orin\u4e0a\u7ef4\u6301GPU\u5229\u7528\u7387\u8d85\u8fc785%\uff0c\u4f18\u4e8e\u8fde\u7eed\u91cd\u8bad\u7ec3\u548c\u5468\u671f\u6027\u91cd\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8fed\u4ee3\u7ea7\u522b\u7684\u6df7\u5408\u8c03\u5ea6\u662f\u5728\u8fb9\u7f18\u5e73\u53f0\u4e0a\u90e8\u7f72\u5177\u6709\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684LLM\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u80fd\u591f\u5e73\u8861\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u66f4\u65b0\u65b0\u9c9c\u5ea6\u3002"}}
{"id": "2510.03690", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03690", "abs": "https://arxiv.org/abs/2510.03690", "authors": ["Ali Azizpour", "Reza Ramezanpour", "Ashutosh Sabharwal", "Santiago Segarra"], "title": "From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning", "comment": null, "summary": "Real-world graph datasets often consist of mixtures of populations, where\ngraphs are generated from multiple distinct underlying distributions. However,\nmodern representation learning approaches, such as graph contrastive learning\n(GCL) and augmentation methods like Mixup, typically overlook this mixture\nstructure. In this work, we propose a unified framework that explicitly models\ndata as a mixture of underlying probabilistic graph generative models\nrepresented by graphons. To characterize these graphons, we leverage graph\nmoments (motif densities) to cluster graphs arising from the same model. This\nenables us to disentangle the mixture components and identify their distinct\ngenerative mechanisms. This model-aware partitioning benefits two key graph\nlearning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data\naugmentation technique that interpolates in a semantically valid space guided\nby the estimated graphons, instead of assuming a single graphon per class. 2)\nFor GCL, it enables model-adaptive and principled augmentations. Additionally,\nby introducing a new model-aware objective, our proposed approach (termed MGCL)\nimproves negative sampling by restricting negatives to graphs from other\nmodels. We establish a key theoretical guarantee: a novel, tighter bound\nshowing that graphs sampled from graphons with small cut distance will have\nsimilar motif densities with high probability. Extensive experiments on\nbenchmark datasets demonstrate strong empirical performance. In unsupervised\nlearning, MGCL achieves state-of-the-art results, obtaining the top average\nrank across eight datasets. In supervised learning, GMAM consistently\noutperforms existing strategies, achieving new state-of-the-art accuracy in 6\nout of 7 datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u5f0f\u5efa\u6a21\u56fe\u6570\u636e\u4e3a\u56fe\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u77e9\u805a\u7c7b\u8bc6\u522b\u4e0d\u540c\u751f\u6210\u673a\u5236\uff0c\u6539\u8fdb\u4e86\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u6765\u81ea\u591a\u4e2a\u4e0d\u540c\u5206\u5e03\u7684\u6df7\u5408\u7fa4\u4f53\uff0c\u4f46\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u8fd9\u79cd\u6df7\u5408\u7ed3\u6784\u3002", "method": "\u5229\u7528\u56fe\u77e9\uff08\u6a21\u4f53\u5bc6\u5ea6\uff09\u805a\u7c7b\u6765\u81ea\u540c\u4e00\u6a21\u578b\u7684\u56fe\uff0c\u63d0\u51fa\u56fe\u6df7\u5408\u611f\u77e5\u7684Mixup\u589e\u5f3a\u65b9\u6cd5\u548c\u6a21\u578b\u81ea\u9002\u5e94\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u8fbe\u5230SOTA\uff0c\u57288\u4e2a\u6570\u636e\u96c6\u4e2d\u5e73\u5747\u6392\u540d\u7b2c\u4e00\uff1b\u5728\u6709\u76d1\u7763\u5b66\u4e60\u4e2d\uff0c\u57287\u4e2a\u6570\u636e\u96c6\u4e2d\u76846\u4e2a\u83b7\u5f97\u65b0\u7684SOTA\u51c6\u786e\u7387\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u56fe\u6570\u636e\u7684\u6df7\u5408\u7ed3\u6784\u80fd\u591f\u6709\u6548\u6539\u8fdb\u56fe\u8868\u793a\u5b66\u4e60\u6027\u80fd\uff0c\u4e3a\u56fe\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2510.03284", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03284", "abs": "https://arxiv.org/abs/2510.03284", "authors": ["Vinay Venkatesh", "Vamsidhar R Kamanuru", "Lav Kumar", "Nikita Kothari"], "title": "Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments", "comment": "7 pages, 1 figure", "summary": "This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a\nscalable framework for Federated Instruction Tuning (FIT) of Large Language\nModels (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail\nwhen confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT\nframework combines federated learning with 4-bit Quantized Low-Rank Adaptation\n(QLORA), mitigating the core issues of communication and computational\noverhead. We demonstrate this by filtering the general-purpose Databricks Dolly\n15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned\nLlama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable\ntrade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable\nframework for decentralized LLM deployment on home compute gateways.", "AI": {"tldr": "Edge-FIT\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8054\u90a6\u6307\u4ee4\u8c03\u4f18LLM\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c4\u4f4d\u91cf\u5316\u4f4e\u79e9\u9002\u5e94\u6765\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728LLM\u4e0a\u7684\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5982FedAvg\u5728\u9762\u5bf9LLM\u7684\u5de8\u5927\u53c2\u6570\u91cf\u65f6\u4f1a\u5931\u6548\uff0c\u9700\u8981\u89e3\u51b3\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u4ee5\u5b9e\u73b0\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684LLM\u90e8\u7f72\u3002", "method": "\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c4\u4f4d\u91cf\u5316\u4f4e\u79e9\u9002\u5e94(QLORA)\uff0c\u4f7f\u7528\u8fc7\u6ee4\u540e\u7684Databricks Dolly 15k\u6570\u636e\u96c6\u8fdb\u884cIoT\u9886\u57df\u7684\u6307\u4ee4\u8c03\u4f18\u3002", "result": "Edge-FIT\u8c03\u4f18\u7684Llama 2(7B)\u6a21\u578b\u8fbe\u5230F1\u5206\u65700.89\uff0c\u5e76\u57283.8B Phi-3-mini\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u7684\u6743\u8861\u3002", "conclusion": "Edge-FIT\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5728\u5bb6\u5ead\u8ba1\u7b97\u7f51\u5173\u4e0a\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684LLM\u90e8\u7f72\u3002"}}
{"id": "2510.03722", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03722", "abs": "https://arxiv.org/abs/2510.03722", "authors": ["Qianxin Yi", "Shao-Bo Lin", "Jun Fan", "Yao Wang"], "title": "Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach", "comment": null, "summary": "Reinforcement learning (RL) has been widely applied to sequential decision\nmaking, where interpretability and performance are both critical for practical\nadoption. Current approaches typically focus on performance and rely on post\nhoc explanations to account for interpretability. Different from these\napproaches, we focus on designing an interpretability-oriented yet\nperformance-enhanced RL approach. Specifically, we propose a spectral based\nlinear RL method that extends the ridge regression-based approach through a\nspectral filter function. The proposed method clarifies the role of\nregularization in controlling estimation error and further enables the design\nof an adaptive regularization parameter selection strategy guided by the\nbias-variance trade-off principle. Theoretical analysis establishes\nnear-optimal bounds for both parameter estimation and generalization error.\nExtensive experiments on simulated environments and real-world datasets from\nKuaishou and Taobao demonstrate that our method either outperforms or matches\nexisting baselines in decision quality. We also conduct interpretability\nanalyses to illustrate how the learned policies make decisions, thereby\nenhancing user trust. These results highlight the potential of our approach to\nbridge the gap between RL theory and practical decision making, providing\ninterpretability, accuracy, and adaptability in management contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u6ee4\u6ce2\u7684\u7ebf\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\uff0c\u4f9d\u8d56\u4e8b\u540e\u89e3\u91ca\u6765\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u4ee5\u53ef\u89e3\u91ca\u6027\u4e3a\u5bfc\u5411\u4f46\u6027\u80fd\u589e\u5f3a\u7684RL\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u57fa\u4e8e\u5cad\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c31\u6ee4\u6ce2\u51fd\u6570\u8bbe\u8ba1\u81ea\u9002\u5e94\u6b63\u5219\u5316\u53c2\u6570\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8e\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u539f\u5219\u3002", "result": "\u7406\u8bba\u5206\u6790\u5efa\u7acb\u4e86\u53c2\u6570\u4f30\u8ba1\u548c\u6cdb\u5316\u8bef\u5dee\u7684\u8fd1\u4f3c\u6700\u4f18\u754c\u3002\u5728\u5feb\u624b\u548c\u6dd8\u5b9d\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u51b3\u7b56\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u5f25\u5408RL\u7406\u8bba\u4e0e\u5b9e\u9645\u51b3\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728\u7ba1\u7406\u573a\u666f\u4e2d\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.03288", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.03288", "abs": "https://arxiv.org/abs/2510.03288", "authors": ["Chiming Duan", "Minghua He", "Pei Xiao", "Tong Jia", "Xin Zhang", "Zhewei Zhong", "Xiang Luo", "Yan Niu", "Lingzhe Zhang", "Yifan Wu", "Siyu Yu", "Weijie Hong", "Ying Li", "Gang Huang"], "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain", "comment": "The 40th IEEE/ACM International Conference on Automated Software\n  Engineering, ASE 2025", "summary": "Log-based anomaly detection is a essential task for ensuring the reliability\nand performance of software systems. However, the performance of existing\nanomaly detection methods heavily relies on labeling, while labeling a large\nvolume of logs is highly challenging. To address this issue, many approaches\nbased on transfer learning and active learning have been proposed.\nNevertheless, their effectiveness is hindered by issues such as the gap between\nsource and target system data distributions and cold-start problems. In this\npaper, we propose LogAction, a novel log-based anomaly detection model based on\nactive domain adaptation. LogAction integrates transfer learning and active\nlearning techniques. On one hand, it uses labeled data from a mature system to\ntrain a base model, mitigating the cold-start issue in active learning. On the\nother hand, LogAction utilize free energy-based sampling and uncertainty-based\nsampling to select logs located at the distribution boundaries for manual\nlabeling, thus addresses the data distribution gap in transfer learning with\nminimal human labeling efforts. Experimental results on six different\ncombinations of datasets demonstrate that LogAction achieves an average 93.01%\nF1 score with only 2% of manual labels, outperforming some state-of-the-art\nmethods by 26.28%. Website: https://logaction.github.io", "AI": {"tldr": "LogAction\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u9886\u57df\u81ea\u9002\u5e94\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\uff0c\u4ec5\u97002%\u4eba\u5de5\u6807\u6ce8\u5c31\u80fd\u8fbe\u523093.01%\u7684F1\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u6807\u6ce8\uff0c\u4f46\u5927\u89c4\u6a21\u65e5\u5fd7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u3002\u8fc1\u79fb\u5b66\u4e60\u548c\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6e90-\u76ee\u6807\u7cfb\u7edf\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6210\u719f\u7cfb\u7edf\u7684\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8e\u81ea\u7531\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u91c7\u6837\u65b9\u6cd5\u9009\u62e9\u5206\u5e03\u8fb9\u754c\u65e5\u5fd7\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\uff0c\u4ee5\u6700\u5c0f\u5316\u6807\u6ce8\u6210\u672c\u89e3\u51b3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u7ec4\u5408\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cLogAction\u4ec5\u75282%\u4eba\u5de5\u6807\u6ce8\u5c31\u8fbe\u5230\u5e73\u574793.01%\u7684F1\u5206\u6570\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u63d0\u534726.28%\u3002", "conclusion": "LogAction\u901a\u8fc7\u4e3b\u52a8\u9886\u57df\u81ea\u9002\u5e94\u6709\u6548\u89e3\u51b3\u4e86\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6807\u6ce8\u6210\u672c\u548c\u6570\u636e\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2510.03734", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03734", "abs": "https://arxiv.org/abs/2510.03734", "authors": ["Nirjhar Das", "Mohit Sharma", "Praharsh Nanavati", "Kirankumar Shiragur", "Amit Deshpande"], "title": "Cost Efficient Fairness Audit Under Partial Feedback", "comment": "Accepted at NeurIPS 2025 RegML Workshop; Reliable ML Workshop", "summary": "We study the problem of auditing the fairness of a given classifier under\npartial feedback, where true labels are available only for positively\nclassified individuals, (e.g., loan repayment outcomes are observed only for\napproved applicants). We introduce a novel cost model for acquiring additional\nlabeled data, designed to more accurately reflect real-world costs such as\ncredit assessment, loan processing, and potential defaults. Our goal is to find\noptimal fairness audit algorithms that are more cost-effective than random\nexploration and natural baselines.\n  In our work, we consider two audit settings: a black-box model with no\nassumptions on the data distribution, and a mixture model, where features and\ntrue labels follow a mixture of exponential family distributions. In the\nblack-box setting, we propose a near-optimal auditing algorithm under mild\nassumptions and show that a natural baseline can be strictly suboptimal. In the\nmixture model setting, we design a novel algorithm that achieves significantly\nlower audit cost than the black-box case. Our approach leverages prior work on\nlearning from truncated samples and maximum-a-posteriori oracles, and extends\nknown results on spherical Gaussian mixtures to handle exponential family\nmixtures, which may be of independent interest. Moreover, our algorithms apply\nto popular fairness metrics including demographic parity, equal opportunity,\nand equalized odds. Empirically, we demonstrate strong performance of our\nalgorithms on real-world fair classification datasets like Adult Income and Law\nSchool, consistently outperforming natural baselines by around 50% in terms of\naudit cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u90e8\u5206\u53cd\u9988\u4e0b\uff08\u4ec5\u89c2\u5bdf\u6b63\u5206\u7c7b\u4e2a\u4f53\u7684\u771f\u5b9e\u6807\u7b7e\uff09\u8fdb\u884c\u516c\u5e73\u6027\u5ba1\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u5f15\u5165\u66f4\u7b26\u5408\u73b0\u5b9e\u6210\u672c\u7684\u6210\u672c\u6a21\u578b\uff0c\u8bbe\u8ba1\u4e86\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6700\u4f18\u5ba1\u8ba1\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ba1\u8ba1\u6210\u672c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u5f88\u591a\u573a\u666f\u4e0b\u53ea\u80fd\u89c2\u5bdf\u5230\u6b63\u5206\u7c7b\u4e2a\u4f53\u7684\u771f\u5b9e\u6807\u7b7e\uff08\u5982\u8d37\u6b3e\u7533\u8bf7\u4e2d\u53ea\u6709\u83b7\u6279\u8005\u7684\u8fd8\u6b3e\u7ed3\u679c\uff09\uff0c\u800c\u4f20\u7edf\u516c\u5e73\u6027\u5ba1\u8ba1\u9700\u8981\u5b8c\u6574\u6807\u7b7e\u6570\u636e\uff0c\u8fd9\u5728\u5b9e\u9645\u4e2d\u6210\u672c\u9ad8\u6602\u3002", "method": "\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u8bbe\u8ba1\u7b97\u6cd5\uff1a\u9ed1\u76d2\u6a21\u578b\uff08\u65e0\u6570\u636e\u5206\u5e03\u5047\u8bbe\uff09\u548c\u6df7\u5408\u6a21\u578b\uff08\u7279\u5f81\u548c\u6807\u7b7e\u9075\u5faa\u6307\u6570\u65cf\u5206\u5e03\u6df7\u5408\uff09\u3002\u5229\u7528\u622a\u65ad\u6837\u672c\u5b66\u4e60\u548c\u6700\u5927\u540e\u9a8c\u6982\u7387\u9884\u8a00\u673a\uff0c\u5c06\u7403\u9762\u9ad8\u65af\u6df7\u5408\u7ed3\u679c\u6269\u5c55\u5230\u6307\u6570\u65cf\u6df7\u5408\u3002", "result": "\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u63d0\u51fa\u63a5\u8fd1\u6700\u4f18\u7684\u5ba1\u8ba1\u7b97\u6cd5\uff0c\u8bc1\u660e\u81ea\u7136\u57fa\u7ebf\u65b9\u6cd5\u4e25\u683c\u6b21\u4f18\uff1b\u5728\u6df7\u5408\u6a21\u578b\u8bbe\u7f6e\u4e0b\u8bbe\u8ba1\u7684\u7b97\u6cd5\u5ba1\u8ba1\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u9ed1\u76d2\u60c5\u51b5\u3002\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6bd4\u81ea\u7136\u57fa\u7ebf\u8282\u7701\u7ea650%\u7684\u5ba1\u8ba1\u6210\u672c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u516c\u5e73\u6027\u5ba1\u8ba1\u7b97\u6cd5\u5728\u90e8\u5206\u53cd\u9988\u573a\u666f\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u672c\u6548\u76ca\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u516c\u5e73\u6027\u6307\u6807\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03289", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03289", "abs": "https://arxiv.org/abs/2510.03289", "authors": ["Haocheng Sun", "Cynthia Xin Wen", "Edward Hong Wang"], "title": "Why mask diffusion does not work", "comment": null, "summary": "The main advantages of diffusion language models over autoregressive (AR)\nmodels lie in their ability to support parallel generation and bidirectional\nattention, enabling a more controllable generation process. In recent years,\nopen-source mask diffusion language models have emerged, most of which are\nbased on a variant known as absorbing diffusion. However, this paper\ndemonstrates why mask diffusion faces inherent difficulties in achieving\nparallel generation and bidirectional attention. We also propose the most\neffective training and inference strategies for mask diffusion.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u56fa\u6709\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u6709\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\u5177\u6709\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u7684\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7684\u5f00\u6e90\u63a9\u7801\u6269\u6563\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5438\u6536\u6269\u6563\u53d8\u4f53\uff0c\u5b58\u5728\u5b9e\u73b0\u8fd9\u4e9b\u4f18\u52bf\u7684\u56f0\u96be\u3002", "method": "\u5206\u6790\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u56fa\u6709\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u3002", "result": "\u63ed\u793a\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u548c\u53cc\u5411\u6ce8\u610f\u529b\u65b9\u9762\u5b58\u5728\u56fa\u6709\u56f0\u96be\uff0c\u9700\u8981\u91c7\u7528\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u7b56\u7565\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002"}}
{"id": "2510.03784", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03784", "abs": "https://arxiv.org/abs/2510.03784", "authors": ["Ruoxi Yu", "Haotian Jiang", "Jingpu Cheng", "Penghao Yu", "Qianxiao Li", "Zhong Li"], "title": "Allocation of Parameters in Transformers", "comment": null, "summary": "Transformers have achieved remarkable successes across a wide range of\napplications, yet the theoretical foundation of their model efficiency remains\nunderexplored. In this work, we investigate how the model parameters -- mainly\nattention heads and head dimensions -- should be allocated across layers to\nbalance expressivity and efficiency. We first provide mathematical analysis on\nthe role of early layers in information extraction from an approximation\nperspective, with a theoretical characterization on the trade-off between the\nnumber of heads and head dimension under a fixed parameter budget. In addition,\nwe uncover and prove the \\emph{saturation} behavior of softmax activations:\nContinuously increasing head dimensions can lead to diminishing returns in\nlearning errors, particularly for long sequences. Supported by both theory and\nexperiments, this saturation pattern suggests that later layers can operate\nmore efficiently with reduced parameters. Combining these insights, we propose\nprincipled strategies for allocating attention heads and dimensions across\nTransformers' layers, shedding light on theoretically-grounded model efficiency\nof Transformer-based architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u4e86Transformer\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u5934\u548c\u5934\u7ef4\u5ea6\u5728\u4e0d\u540c\u5c42\u95f4\u7684\u5206\u914d\u7b56\u7565\uff0c\u63ed\u793a\u4e86softmax\u6fc0\u6d3b\u7684\u9971\u548c\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u7684\u53c2\u6570\u5206\u914d\u539f\u5219\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5176\u6a21\u578b\u6548\u7387\u7684\u7406\u8bba\u57fa\u7840\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u6a21\u578b\u53c2\u6570\uff08\u4e3b\u8981\u662f\u6ce8\u610f\u529b\u5934\u548c\u5934\u7ef4\u5ea6\uff09\u5982\u4f55\u5728\u5404\u5c42\u95f4\u5206\u914d\u4ee5\u5e73\u8861\u8868\u8fbe\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u9996\u5148\u4ece\u8fd1\u4f3c\u89d2\u5ea6\u5bf9\u65e9\u671f\u5c42\u5728\u4fe1\u606f\u63d0\u53d6\u4e2d\u7684\u4f5c\u7528\u8fdb\u884c\u6570\u5b66\u5206\u6790\uff0c\u7406\u8bba\u523b\u753b\u4e86\u5728\u56fa\u5b9a\u53c2\u6570\u9884\u7b97\u4e0b\u6ce8\u610f\u529b\u5934\u6570\u91cf\u548c\u5934\u7ef4\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u53d1\u73b0\u5e76\u8bc1\u660e\u4e86softmax\u6fc0\u6d3b\u7684\u9971\u548c\u884c\u4e3a\uff1a\u6301\u7eed\u589e\u52a0\u5934\u7ef4\u5ea6\u4f1a\u5bfc\u81f4\u5b66\u4e60\u8bef\u5dee\u7684\u6536\u76ca\u9012\u51cf\uff0c\u5c24\u5176\u5bf9\u4e8e\u957f\u5e8f\u5217\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u90fd\u652f\u6301\u9971\u548c\u6a21\u5f0f\u7684\u5b58\u5728\uff0c\u8868\u660e\u540e\u7eed\u5c42\u53ef\u4ee5\u7528\u66f4\u5c11\u7684\u53c2\u6570\u66f4\u9ad8\u6548\u5730\u8fd0\u884c\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u5728Transformer\u5404\u5c42\u95f4\u5206\u914d\u6ce8\u610f\u529b\u5934\u548c\u7ef4\u5ea6\u7684\u539f\u5219\u6027\u7b56\u7565\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86Transformer\u67b6\u6784\u7684\u7406\u8bba\u57fa\u7840\u6a21\u578b\u6548\u7387\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684Transformer\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\uff0c\u7279\u522b\u662f\u5728\u53c2\u6570\u5206\u914d\u7b56\u7565\u65b9\u9762\u3002"}}
{"id": "2510.03290", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03290", "abs": "https://arxiv.org/abs/2510.03290", "authors": ["X. Angelo Huang", "Ruben Ciranni", "Giovanni Spadaccini", "Carla J. L\u00f3pez Zurita"], "title": "Single-Core Superscalar Optimization of Clifford Neural Layers", "comment": "9 pages", "summary": "Within the growing interest in the physical sciences in developing networks\nwith equivariance properties, Clifford neural layers shine as one approach that\ndelivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this\npaper, we analyze the inner structure of the computation within Clifford\nconvolutional layers and propose and implement several optimizations to speed\nup the inference process while maintaining correctness. In particular, we begin\nby analyzing the theoretical foundations of Clifford algebras to eliminate\nredundant matrix allocations and computations, then systematically apply\nestablished optimization techniques to enhance performance further. We report a\nfinal average speedup of 21.35x over the baseline implementation of eleven\nfunctions and runtimes comparable to and faster than the original PyTorch\nimplementation in six cases. In the remaining cases, we achieve performance in\nthe same order of magnitude as the original library.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Clifford\u5377\u79ef\u5c42\u7684\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u591a\u9879\u4f18\u5316\u65b9\u6cd5\u6765\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6b63\u786e\u6027\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u5e73\u574721.35\u500d\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u968f\u7740\u7269\u7406\u79d1\u5b66\u4e2d\u5bf9\u5177\u6709\u7b49\u53d8\u6027\u7279\u6027\u7684\u7f51\u7edc\u5174\u8da3\u65e5\u76ca\u589e\u957f\uff0cClifford\u795e\u7ecf\u7f51\u7edc\u5c42\u4f5c\u4e3a\u5b9e\u73b0E(n)\u548cO(n)\u7b49\u53d8\u6027\u7684\u4e00\u79cd\u65b9\u6cd5\u5907\u53d7\u5173\u6ce8\u3002\u672c\u6587\u65e8\u5728\u4f18\u5316\u5176\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u9996\u5148\u5206\u6790Clifford\u4ee3\u6570\u7684\u7406\u8bba\u57fa\u7840\u4ee5\u6d88\u9664\u5197\u4f59\u77e9\u9635\u5206\u914d\u548c\u8ba1\u7b97\uff0c\u7136\u540e\u7cfb\u7edf\u6027\u5730\u5e94\u7528\u5df2\u5efa\u7acb\u7684\u4f18\u5316\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u572811\u4e2a\u51fd\u6570\u4e0a\u5b9e\u73b0\u4e86\u5e73\u574721.35\u500d\u7684\u52a0\u901f\uff0c\u57286\u4e2a\u6848\u4f8b\u4e2d\u8fd0\u884c\u65f6\u95f4\u4e0e\u539f\u59cbPyTorch\u5b9e\u73b0\u76f8\u5f53\u6216\u66f4\u5feb\uff0c\u5176\u4f59\u6848\u4f8b\u4e2d\u6027\u80fd\u4e0e\u539f\u59cb\u5e93\u4fdd\u6301\u540c\u4e00\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u6210\u529f\u663e\u8457\u63d0\u5347\u4e86Clifford\u5377\u79ef\u5c42\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6b63\u786e\u6027\u3002"}}
{"id": "2510.03798", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03798", "abs": "https://arxiv.org/abs/2510.03798", "authors": ["Yunwen Guo", "Yunlun Shu", "Gongyi Zhuo", "Tianyu Wang"], "title": "Robust Batched Bandits", "comment": "39 pages", "summary": "The batched multi-armed bandit (MAB) problem, in which rewards are collected\nin batches, is crucial for applications such as clinical trials. Existing\nresearch predominantly assumes light-tailed reward distributions, yet many\nreal-world scenarios, including clinical outcomes, exhibit heavy-tailed\ncharacteristics. This paper bridges this gap by proposing robust batched bandit\nalgorithms designed for heavy-tailed rewards, within both finite-arm and\nLipschitz-continuous settings. We reveal a surprising phenomenon: in the\ninstance-independent regime, as well as in the Lipschitz setting,\nheavier-tailed rewards necessitate a smaller number of batches to achieve\nnear-optimal regret. In stark contrast, for the instance-dependent setting, the\nrequired number of batches to attain near-optimal regret remains invariant with\nrespect to tail heaviness.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u91cd\u5c3e\u5956\u52b1\u5206\u5e03\u7684\u6279\u5904\u7406\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9c81\u68d2\u7b97\u6cd5\uff0c\u53d1\u73b0\u5728\u5b9e\u4f8b\u65e0\u5173\u548cLipschitz\u8bbe\u7f6e\u4e2d\uff0c\u91cd\u5c3e\u5956\u52b1\u9700\u8981\u66f4\u5c11\u7684\u6279\u6b21\uff0c\u800c\u5728\u5b9e\u4f8b\u76f8\u5173\u8bbe\u7f6e\u4e2d\u6279\u6b21\u9700\u6c42\u4e0d\u53d8\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5047\u8bbe\u8f7b\u5c3e\u5956\u52b1\u5206\u5e03\uff0c\u4f46\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\uff08\u5982\u4e34\u5e8a\u8bd5\u9a8c\u7ed3\u679c\uff09\u8868\u73b0\u51fa\u91cd\u5c3e\u7279\u5f81\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u91cd\u5c3e\u5956\u52b1\u7684\u9c81\u68d2\u6279\u5904\u7406\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u6db5\u76d6\u6709\u9650\u81c2\u548cLipschitz\u8fde\u7eed\u8bbe\u7f6e\u3002", "result": "\u63ed\u793a\u4e86\u4e00\u4e2a\u4ee4\u4eba\u60ca\u8bb6\u7684\u73b0\u8c61\uff1a\u5728\u5b9e\u4f8b\u65e0\u5173\u548cLipschitz\u8bbe\u7f6e\u4e2d\uff0c\u91cd\u5c3e\u5956\u52b1\u9700\u8981\u66f4\u5c11\u7684\u6279\u6b21\u6765\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\uff1b\u800c\u5728\u5b9e\u4f8b\u76f8\u5173\u8bbe\u7f6e\u4e2d\uff0c\u6240\u9700\u6279\u6b21\u6570\u91cf\u4e0d\u968f\u5c3e\u90e8\u539a\u91cd\u7a0b\u5ea6\u53d8\u5316\u3002", "conclusion": "\u91cd\u5c3e\u5956\u52b1\u5206\u5e03\u5bf9\u6279\u5904\u7406\u8001\u864e\u673a\u7b97\u6cd5\u7684\u5f71\u54cd\u56e0\u8bbe\u7f6e\u800c\u5f02\uff0c\u5728\u5b9e\u4f8b\u65e0\u5173\u548cLipschitz\u8bbe\u7f6e\u4e2d\u53ef\u51cf\u5c11\u6279\u6b21\u9700\u6c42\uff0c\u4f46\u5728\u5b9e\u4f8b\u76f8\u5173\u8bbe\u7f6e\u4e2d\u4fdd\u6301\u7a33\u5b9a\u3002"}}
{"id": "2510.03291", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03291", "abs": "https://arxiv.org/abs/2510.03291", "authors": ["Yizhuo Ding", "Wanying Qu", "Jiawei Geng", "Wenqi Shao", "Yanwei Fu"], "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs", "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks\nbut face prohibitive computational and memory costs. Pruning offers a promising\npath by inducing sparsity while preserving architectural flexibility. However,\nexisting methods struggle to balance efficiency and robustness: local metric\napproaches prune layer by layer but often collapse under high sparsity, whereas\nglobal feedback methods enforce consistency at the cost of expensive weight\nupdates or restrictive semi-structured formats. We present UniPruning, a\nunified post-training pruning framework that combines the speed of local\nsaliency metrics with the stability of global coordination, enabled by a mirror\ndescent based optimization, all without updating model weights. UniPruning\nleverages fast layer-wise scoring and a lightweight global controller to\nallocate a single sparsity budget, supporting both unstructured and\nsemi-structured N :M pruning within one framework. After a brief calibration,\nit can generate pruning masks for arbitrary sparsity levels in one shot, and\nadapts seamlessly to hardware-aware constraints. Extensive experiments on\nmultiple pretrained LLM families and standard benchmarks show that UniPruning\nconsistently delivers competitive or superior perplexity and zero-shot\naccuracy. Ablation studies further highlight the importance of mirror descent\nand local saliency anchoring. Overall, UniPruning provides an efficient,\nprincipled, and scalable solution for sparsifying large-scale LLMs. Our code is\navailable at: https://github.com/RainbowQTT/UniPruning.", "AI": {"tldr": "UniPruning\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u540e\u8bad\u7ec3\u526a\u679d\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u663e\u8457\u6027\u5ea6\u91cf\u7684\u901f\u5ea6\u548c\u5168\u5c40\u534f\u8c03\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u955c\u50cf\u4e0b\u964d\u4f18\u5316\u5b9e\u73b0\uff0c\u65e0\u9700\u66f4\u65b0\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u9ad8\u6602\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6548\u7387\u548c\u9c81\u68d2\u6027\uff1a\u5c40\u90e8\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u5bb9\u6613\u5d29\u6e83\uff0c\u5168\u5c40\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u6743\u91cd\u66f4\u65b0\u6216\u9650\u5236\u534a\u7ed3\u6784\u5316\u683c\u5f0f\u3002", "method": "\u4f7f\u7528\u5feb\u901f\u5c42\u95f4\u8bc4\u5206\u548c\u8f7b\u91cf\u7ea7\u5168\u5c40\u63a7\u5236\u5668\u5206\u914d\u5355\u4e00\u7a00\u758f\u5ea6\u9884\u7b97\uff0c\u652f\u6301\u975e\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316N:M\u526a\u679d\uff0c\u901a\u8fc7\u955c\u50cf\u4e0b\u964d\u4f18\u5316\u5b9e\u73b0\u5168\u5c40\u534f\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u9884\u8bad\u7ec3LLM\u5bb6\u65cf\u548c\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniPruning\u6301\u7eed\u63d0\u4f9b\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u7684\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "UniPruning\u4e3a\u5927\u89c4\u6a21LLM\u7a00\u758f\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u539f\u5219\u6027\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03817", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03817", "abs": "https://arxiv.org/abs/2510.03817", "authors": ["Philipp Becker", "Niklas Freymuth", "Serge Thilges", "Fabian Otto", "Gerhard Neumann"], "title": "TROLL: Trust Regions improve Reinforcement Learning for Large Language Models", "comment": null, "summary": "On-policy Reinforcement Learning (RL) with PPO-like clip objectives has\nbecome the standard choice for reward-based fine-tuning of large language\nmodels (LLMs). Although recent work has explored improved estimators of\nadvantages and normalization, the clipping mechanism itself has remained\nuntouched. Originally introduced as a proxy for principled KL-based trust\nregions, clipping is a crude approximation that often causes unstable updates\nand suboptimal performance. We replace the clip objective with a novel discrete\ndifferentiable trust region projection, which provides principled token-level\nKL constraints. The projection operates on a sparse subset of the model's most\nimportant token logits to balance computational cost and projection\neffectiveness. Our approach, Trust Region Optimization for Large Language\nModels (TROLL), serves as a direct replacement for PPO-like clipping during\ntraining and does not alter the model's inference behavior. Across datasets,\nmodel families, and advantage-estimation methods, TROLL consistently\noutperforms PPO-like clipping in terms of training speed, stability, and final\nsuccess rates.", "AI": {"tldr": "TROLL\u65b9\u6cd5\u7528\u79bb\u6563\u53ef\u5fae\u4fe1\u4efb\u57df\u6295\u5f71\u66ff\u4ee3PPO\u7684\u526a\u88c1\u673a\u5236\uff0c\u4e3aLLM\u5956\u52b1\u5fae\u8c03\u63d0\u4f9b\u539f\u5219\u6027\u7684token\u7ea7KL\u7ea6\u675f\uff0c\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6210\u529f\u7387\u65b9\u9762\u5747\u4f18\u4e8ePPO\u526a\u88c1\u3002", "motivation": "PPO\u526a\u88c1\u673a\u5236\u4f5c\u4e3aKL\u4fe1\u4efb\u57df\u7684\u7c97\u7565\u8fd1\u4f3c\uff0c\u5e38\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u66f4\u65b0\u548c\u6b21\u4f18\u6027\u80fd\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u53ef\u5fae\u4fe1\u4efb\u57df\u6295\u5f71\uff0c\u5728\u6a21\u578b\u6700\u91cd\u8981\u7684token logits\u7a00\u758f\u5b50\u96c6\u4e0a\u64cd\u4f5c\uff0c\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u548c\u6295\u5f71\u6548\u679c\uff0c\u4f5c\u4e3aPPO\u526a\u88c1\u7684\u76f4\u63a5\u66ff\u4ee3\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6a21\u578b\u5bb6\u65cf\u548c\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\u4e0b\uff0cTROLL\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6210\u529f\u7387\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8ePPO\u526a\u88c1\u3002", "conclusion": "TROLL\u53ef\u4f5c\u4e3aPPO\u526a\u88c1\u7684\u76f4\u63a5\u66ff\u4ee3\uff0c\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684LLM\u5956\u52b1\u5fae\u8c03\u65b9\u6cd5\u3002"}}
{"id": "2510.03293", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03293", "abs": "https://arxiv.org/abs/2510.03293", "authors": ["Rana Shahout", "Colin Cai", "Yilun Du", "Minlan Yu", "Michael Mitzenmacher"], "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing", "comment": null, "summary": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each\ntoken to a subset of experts through a learned gate function. While conditional\nrouting reduces training costs, it shifts the burden on inference memory:\nexpert parameters and activations consume memory, limiting the number of\nexperts per device. As tokens are routed, some experts become overloaded while\nothers are underutilized. Because experts are mapped to GPUs, this imbalance\ntranslates directly into degraded system performance in terms of latency,\nthroughput, and cost. We present LASER, a plug-and-play, inference-time routing\nalgorithm that balances load while preserving accuracy. LASER adapts to the\nshape of the gate's score distribution. When scores provide a clear preference,\nit routes to the strongest experts; when scores are more uniform, it broadens\nthe set of viable experts and routes to the least-loaded among them. Because\nLASER relies only on gate scores from a trained model, it integrates directly\ninto existing MoE inference pipelines without retraining or finetuning. We\nevaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets\n(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,\ntranslating into lower latency and higher throughput, while keeping the\naccuracy changes negligible.", "AI": {"tldr": "LASER\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u65f6\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u4e13\u5bb6\u8d1f\u8f7d\u6765\u63d0\u5347MoE\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "MoE\u6a21\u578b\u901a\u8fc7\u6761\u4ef6\u8def\u7531\u51cf\u5c11\u8bad\u7ec3\u6210\u672c\uff0c\u4f46\u63a8\u7406\u65f6\u4e13\u5bb6\u53c2\u6570\u548c\u6fc0\u6d3b\u5360\u7528\u5185\u5b58\uff0c\u4e14\u4ee4\u724c\u8def\u7531\u5bfc\u81f4\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u5f71\u54cd\u7cfb\u7edf\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u548c\u6210\u672c\u3002", "method": "LASER\u6839\u636e\u95e8\u63a7\u5206\u6570\u5206\u5e03\u81ea\u9002\u5e94\u8def\u7531\uff1a\u5f53\u5206\u6570\u663e\u793a\u660e\u663e\u504f\u597d\u65f6\u8def\u7531\u5230\u6700\u5f3a\u4e13\u5bb6\uff1b\u5f53\u5206\u6570\u66f4\u5747\u5300\u65f6\u6269\u5c55\u5230\u53ef\u884c\u4e13\u5bb6\u96c6\u5e76\u8def\u7531\u5230\u8d1f\u8f7d\u6700\u8f7b\u7684\u4e13\u5bb6\u3002\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "result": "\u5728Mixtral-8x7B\u548cDeepSeek-MoE-16b-chat\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLASER\u6539\u5584\u4e86\u8d1f\u8f7d\u5e73\u8861\uff0c\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u540c\u65f6\u51c6\u786e\u7387\u53d8\u5316\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "LASER\u662f\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u65f6\u8def\u7531\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347MoE\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2510.03824", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03824", "abs": "https://arxiv.org/abs/2510.03824", "authors": ["Wei Guo", "Jaemoo Choi", "Yuchen Zhu", "Molei Tao", "Yongxin Chen"], "title": "Proximal Diffusion Neural Sampler", "comment": "31 pages, 12 figures", "summary": "The task of learning a diffusion-based neural sampler for drawing samples\nfrom an unnormalized target distribution can be viewed as a stochastic optimal\ncontrol problem on path measures. However, the training of neural samplers can\nbe challenging when the target distribution is multimodal with significant\nbarriers separating the modes, potentially leading to mode collapse. We propose\na framework named \\textbf{Proximal Diffusion Neural Sampler (PDNS)} that\naddresses these challenges by tackling the stochastic optimal control problem\nvia proximal point method on the space of path measures. PDNS decomposes the\nlearning process into a series of simpler subproblems that create a path\ngradually approaching the desired distribution. This staged procedure traces a\nprogressively refined path to the desired distribution and promotes thorough\nexploration across modes. For a practical and efficient realization, we\ninstantiate each proximal step with a proximal weighted denoising cross-entropy\n(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS\nthrough extensive experiments on both continuous and discrete sampling tasks,\nincluding challenging scenarios in molecular dynamics and statistical physics.", "AI": {"tldr": "\u63d0\u51faPDNS\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u6d4b\u5ea6\u4e0a\u7684\u8fd1\u7aef\u70b9\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u5206\u5e03\u91c7\u6837\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u5c06\u5b66\u4e60\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u9010\u6b65\u903c\u8fd1\u76ee\u6807\u5206\u5e03\u7684\u7b80\u5355\u5b50\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u795e\u7ecf\u91c7\u6837\u5668\u5728\u591a\u6a21\u6001\u5206\u5e03\u4e2d\u5bb9\u6613\u53d1\u751f\u6a21\u5f0f\u5d29\u6e83\uff0c\u7279\u522b\u662f\u5f53\u6a21\u5f0f\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u969c\u788d\u65f6\uff0c\u8bad\u7ec3\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u8def\u5f84\u6d4b\u5ea6\u4e0a\u7684\u8fd1\u7aef\u70b9\u65b9\u6cd5\uff0c\u5c06\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u7b80\u5355\u5b50\u95ee\u9898\uff0c\u6bcf\u4e2a\u8fd1\u7aef\u6b65\u9aa4\u4f7f\u7528\u52a0\u6743\u53bb\u566a\u4ea4\u53c9\u71b5\u76ee\u6807\u5b9e\u73b0\u3002", "result": "\u5728\u8fde\u7eed\u548c\u79bb\u6563\u91c7\u6837\u4efb\u52a1\u4e2d\uff0c\u5305\u62ec\u5206\u5b50\u52a8\u529b\u5b66\u548c\u7edf\u8ba1\u7269\u7406\u4e2d\u7684\u6311\u6218\u6027\u573a\u666f\uff0cPDNS\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PDNS\u6846\u67b6\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8def\u5f84\u6784\u5efa\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5206\u5e03\u91c7\u6837\u4e2d\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e86\u8de8\u6a21\u5f0f\u7684\u5145\u5206\u63a2\u7d22\u3002"}}
{"id": "2510.03298", "categories": ["cs.LG", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.03298", "abs": "https://arxiv.org/abs/2510.03298", "authors": ["Dongqi Zheng", "Wenjin Fu"], "title": "CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models", "comment": "Accepted by 39th NeurIPS - Constrained Optimization for Machine\n  Learning", "summary": "We introduce Constraint-Aware Federated Learning with Lagrangian Dual\nOptimization (CAFL-L), a principled extension of FedAvg that explicitly\nincorporates device-level resource constraints including energy, communication,\nmemory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to\ndynamically adapt training hyperparameters -- freezing depth, local steps,\nbatch size, and communication compression -- while preserving training\nstability through token-budget preservation via gradient accumulation.\nExperiments on a character-level language model demonstrate that CAFL-L\nachieves superior constraint satisfaction compared to standard FedAvg (reducing\nmemory usage by 20% and communication by 95%) while maintaining competitive\nvalidation performance, making it practical for deployment on\nresource-constrained edge devices.", "AI": {"tldr": "CAFL-L\u662f\u4e00\u4e2a\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728FedAvg\u57fa\u7840\u4e0a\u663e\u5f0f\u5730\u6574\u5408\u8bbe\u5907\u7ea7\u8d44\u6e90\u7ea6\u675f\uff08\u80fd\u91cf\u3001\u901a\u4fe1\u3001\u5185\u5b58\u3001\u70ed\u9884\u7b97\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8d85\u53c2\u6570\u6765\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u8bbe\u5907\u7ea7\u8d44\u6e90\u7ea6\u675f\u95ee\u9898\uff0c\u5982\u80fd\u91cf\u3001\u901a\u4fe1\u3001\u5185\u5b58\u548c\u70ed\u9884\u7b97\u9650\u5236\u3002", "method": "\u91c7\u7528\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u4f18\u5316\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u8d85\u53c2\u6570\uff08\u51bb\u7ed3\u6df1\u5ea6\u3001\u672c\u5730\u6b65\u6570\u3001\u6279\u91cf\u5927\u5c0f\u3001\u901a\u4fe1\u538b\u7f29\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u7d2f\u79ef\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5b57\u7b26\u7ea7\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6FedAvg\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120%\uff0c\u901a\u4fe1\u91cf\u51cf\u5c1195%\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u9a8c\u8bc1\u6027\u80fd\u3002", "conclusion": "CAFL-L\u5728\u6ee1\u8db3\u8d44\u6e90\u7ea6\u675f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.03301", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03301", "abs": "https://arxiv.org/abs/2510.03301", "authors": ["Arthur Sedek"], "title": "Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles", "comment": null, "summary": "This paper introduces a novel adaptive ensemble framework that\nsynergistically combines XGBoost and neural networks through sophisticated\nmeta-learning. The proposed method leverages advanced uncertainty\nquantification techniques and feature importance integration to dynamically\norchestrate model selection and combination. Experimental results demonstrate\nsuperior predictive performance and enhanced interpretability across diverse\ndatasets, contributing to the development of more intelligent and flexible\nmachine learning systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u9002\u5e94\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u534f\u540c\u7ed3\u5408XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u7279\u5f81\u91cd\u8981\u6027\u6765\u52a8\u6001\u9009\u62e9\u4e0e\u7ec4\u5408\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u66f4\u667a\u80fd\u548c\u7075\u6d3b\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u7684\u4f18\u52bf\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u5143\u5b66\u4e60\u6280\u672f\u5c06XGBoost\u548c\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u534f\u540c\u96c6\u6210\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u548c\u7279\u5f81\u91cd\u8981\u6027\u96c6\u6210\uff0c\u5b9e\u73b0\u52a8\u6001\u7684\u6a21\u578b\u9009\u62e9\u548c\u7ec4\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u548c\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u96c6\u6210\u6846\u67b6\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u7075\u6d3b\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u5728\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.03838", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03838", "abs": "https://arxiv.org/abs/2510.03838", "authors": ["Behraj Khan", "Tahir Qasim Syed"], "title": "Technical note on Fisher Information for Robust Federated Cross-Validation", "comment": null, "summary": "When training data are fragmented across batches or federated-learned across\ndifferent geographic locations, trained models manifest performance\ndegradation. That degradation partly owes to covariate shift induced by data\nhaving been fragmented across time and space and producing dissimilar empirical\ntraining distributions. Each fragment's distribution is slightly different to a\nhypothetical unfragmented training distribution of covariates, and to the\nsingle validation distribution. To address this problem, we propose Fisher\nInformation for Robust fEderated validation (\\textbf{FIRE}). This method\naccumulates fragmentation-induced covariate shift divergences from the global\ntraining distribution via an approximate Fisher information. That term, which\nwe prove to be a more computationally-tractable estimate, is then used as a\nper-fragment loss penalty, enabling scalable distribution alignment. FIRE\noutperforms importance weighting benchmarks by $5.1\\%$ at maximum and federated\nlearning (FL) benchmarks by up to $5.3\\%$ on shifted validation sets.", "AI": {"tldr": "\u63d0\u51faFIRE\u65b9\u6cd5\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u6765\u4f30\u8ba1\u548c\u8865\u507f\u8054\u90a6\u5b66\u4e60\u4e2d\u7531\u6570\u636e\u788e\u7247\u5316\u5f15\u8d77\u7684\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u63d0\u5347\u6a21\u578b\u5728\u504f\u79fb\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u8bad\u7ec3\u6570\u636e\u5728\u4e0d\u540c\u6279\u6b21\u6216\u5730\u7406\u4f4d\u7f6e\u95f4\u788e\u7247\u5316\u65f6\uff0c\u6a21\u578b\u4f1a\u51fa\u73b0\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u534f\u53d8\u91cf\u504f\u79fb\u5bfc\u81f4\u5404\u6570\u636e\u7247\u6bb5\u7684\u5206\u5e03\u4e0e\u5168\u5c40\u8bad\u7ec3\u5206\u5e03\u548c\u9a8c\u8bc1\u5206\u5e03\u5b58\u5728\u5dee\u5f02\u3002", "method": "FIRE\u65b9\u6cd5\u901a\u8fc7\u8fd1\u4f3cFisher\u4fe1\u606f\u6765\u7d2f\u79ef\u788e\u7247\u5316\u5f15\u8d77\u7684\u534f\u53d8\u91cf\u504f\u79fb\u5dee\u5f02\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6bcf\u4e2a\u6570\u636e\u7247\u6bb5\u7684\u635f\u5931\u60e9\u7f5a\u9879\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5bf9\u9f50\u3002", "result": "FIRE\u5728\u504f\u79fb\u9a8c\u8bc1\u96c6\u4e0a\u6bd4\u91cd\u8981\u6027\u52a0\u6743\u57fa\u51c6\u65b9\u6cd5\u6700\u591a\u63d0\u53475.1%\uff0c\u6bd4\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u65b9\u6cd5\u6700\u591a\u63d0\u53475.3%\u3002", "conclusion": "FIRE\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u4f30\u8ba1\u534f\u53d8\u91cf\u504f\u79fb\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.03302", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03302", "abs": "https://arxiv.org/abs/2510.03302", "authors": ["Daiheng Gao", "Nanxiang Jiang", "Andi Zhang", "Shilin Lu", "Yufei Tang", "Wenbo Zhou", "Weiming Zhang", "Zhaoxin Fan"], "title": "Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models", "comment": "21 pages, 10 figures", "summary": "Concept erasure techniques have been widely deployed in T2I diffusion models\nto prevent inappropriate content generation for safety and copyright\nconsiderations. However, as models evolve to next-generation architectures like\nFlux, established erasure methods (\\textit{e.g.}, ESD, UCE, AC) exhibit\ndegraded effectiveness, raising questions about their true mechanisms. Through\nsystematic analysis, we reveal that concept erasure creates only an illusion of\n``amnesia\": rather than genuine forgetting, these methods bias sampling\ntrajectories away from target concepts, making the erasure fundamentally\nreversible. This insight motivates the need to distinguish superficial safety\nfrom genuine concept removal. In this work, we propose \\textbf{RevAm}\n(\\underline{Rev}oking \\underline{Am}nesia), an RL-based trajectory optimization\nframework that resurrects erased concepts by dynamically steering the denoising\nprocess without modifying model weights. By adapting Group Relative Policy\nOptimization (GRPO) to diffusion models, RevAm explores diverse recovery\ntrajectories through trajectory-level rewards, overcoming local optima that\nlimit existing methods. Extensive experiments demonstrate that RevAm achieves\nsuperior concept resurrection fidelity while reducing computational time by\n10$\\times$, exposing critical vulnerabilities in current safety mechanisms and\nunderscoring the need for more robust erasure techniques beyond trajectory\nmanipulation.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u6982\u5ff5\u64e6\u9664\u6280\u672f\u5728\u6269\u6563\u6a21\u578b\u4e2d\u53ea\u662f\u5236\u9020\u4e86\"\u9057\u5fd8\"\u7684\u5047\u8c61\uff0c\u5b9e\u9645\u4e0a\u662f\u901a\u8fc7\u504f\u7f6e\u91c7\u6837\u8f68\u8ff9\u6765\u907f\u5f00\u76ee\u6807\u6982\u5ff5\uff0c\u800c\u975e\u771f\u6b63\u79fb\u9664\u6982\u5ff5\u3002\u4f5c\u8005\u63d0\u51fa\u4e86RevAm\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u590d\u6d3b\u88ab\u64e6\u9664\u7684\u6982\u5ff5\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u67b6\u6784\u6f14\u8fdb\u5230\u4e0b\u4e00\u4ee3\uff08\u5982Flux\uff09\uff0c\u73b0\u6709\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff08\u5982ESD\u3001UCE\u3001AC\uff09\u6548\u679c\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u975e\u771f\u6b63\u79fb\u9664\u6982\u5ff5\uff0c\u800c\u662f\u901a\u8fc7\u504f\u7f6e\u91c7\u6837\u8f68\u8ff9\u5236\u9020\u9057\u5fd8\u5047\u8c61\uff0c\u8fd9\u79cd\u64e6\u9664\u672c\u8d28\u4e0a\u662f\u53ef\u9006\u7684\u3002", "method": "\u63d0\u51fa\u4e86RevAm\u6846\u67b6\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002\u901a\u8fc7\u5c06Group Relative Policy Optimization (GRPO)\u9002\u914d\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u63a2\u7d22\u591a\u6837\u5316\u7684\u6062\u590d\u8f68\u8ff9\uff0c\u5229\u7528\u8f68\u8ff9\u7ea7\u5956\u52b1\u514b\u670d\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u52a8\u6001\u5f15\u5bfc\u53bb\u566a\u8fc7\u7a0b\u800c\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRevAm\u5728\u6982\u5ff5\u590d\u6d3b\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e8610\u500d\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u5b89\u5168\u673a\u5236\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u8f68\u8ff9\u64cd\u7eb5\u7684\u5b89\u5168\u673a\u5236\u5b58\u5728\u6839\u672c\u6027\u8106\u5f31\u6027\uff0c\u9700\u8981\u5f00\u53d1\u8d85\u8d8a\u8f68\u8ff9\u64cd\u7eb5\u7684\u66f4\u9c81\u68d2\u7684\u6982\u5ff5\u64e6\u9664\u6280\u672f\u3002"}}
{"id": "2510.03839", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03839", "abs": "https://arxiv.org/abs/2510.03839", "authors": ["Behraj Khan", "Tahir Qasim Syed"], "title": "Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting", "comment": null, "summary": "We present a theoretical framework for M-FISHER, a method for sequential\ndistribution shift detection and stable adaptation in streaming data. For\ndetection, we construct an exponential martingale from non-conformity scores\nand apply Ville's inequality to obtain time-uniform guarantees on false alarm\ncontrol, ensuring statistical validity at any stopping time. Under sustained\nshifts, we further bound the expected detection delay as\n$\\mathcal{O}(\\log(1/\\delta)/\\Gamma)$, where $\\Gamma$ reflects the post-shift\ninformation gain, thereby linking detection efficiency to distributional\ndivergence. For adaptation, we show that Fisher-preconditioned updates of\nprompt parameters implement natural gradient descent on the distributional\nmanifold, yielding locally optimal updates that minimize KL divergence while\npreserving stability and parameterization invariance. Together, these results\nestablish M-FISHER as a principled approach for robust, anytime-valid detection\nand geometrically stable adaptation in sequential decision-making under\ncovariate shift.", "AI": {"tldr": "M-FISHER\u662f\u4e00\u4e2a\u7528\u4e8e\u6d41\u6570\u636e\u4e2d\u5e8f\u5217\u5206\u5e03\u6f02\u79fb\u68c0\u6d4b\u548c\u7a33\u5b9a\u9002\u5e94\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u6570\u9785\u548cVille\u4e0d\u7b49\u5f0f\u5b9e\u73b0\u65f6\u95f4\u5747\u5300\u7684\u8bef\u62a5\u63a7\u5236\uff0c\u5e76\u57fa\u4e8eFisher\u4fe1\u606f\u8fdb\u884c\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u6d41\u6570\u636e\u4e2d\u5e8f\u5217\u5206\u5e03\u6f02\u79fb\u7684\u68c0\u6d4b\u548c\u9002\u5e94\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u4efb\u610f\u505c\u6b62\u65f6\u95f4\u90fd\u5177\u6709\u7edf\u8ba1\u6709\u6548\u6027\uff0c\u540c\u65f6\u5b9e\u73b0\u51e0\u4f55\u7a33\u5b9a\u7684\u53c2\u6570\u66f4\u65b0\u3002", "method": "\u4f7f\u7528\u975e\u7b26\u5408\u6027\u5206\u6570\u6784\u5efa\u6307\u6570\u9785\uff0c\u5e94\u7528Ville\u4e0d\u7b49\u5f0f\u8fdb\u884c\u6f02\u79fb\u68c0\u6d4b\uff1b\u91c7\u7528Fisher\u9884\u6761\u4ef6\u5316\u7684\u63d0\u793a\u53c2\u6570\u66f4\u65b0\u5b9e\u73b0\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u3002", "result": "\u68c0\u6d4b\u5ef6\u8fdf\u4e3aO(log(1/\u03b4)/\u0393)\uff0c\u5176\u4e2d\u0393\u53cd\u6620\u540e\u6f02\u79fb\u4fe1\u606f\u589e\u76ca\uff1b\u9002\u5e94\u8fc7\u7a0b\u5728\u5206\u5e03\u6d41\u5f62\u4e0a\u5b9e\u73b0\u5c40\u90e8\u6700\u4f18\u66f4\u65b0\uff0c\u6700\u5c0f\u5316KL\u6563\u5ea6\u3002", "conclusion": "M-FISHER\u4e3a\u534f\u53d8\u91cf\u6f02\u79fb\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u3001\u9c81\u68d2\u7684\u68c0\u6d4b\u548c\u51e0\u4f55\u7a33\u5b9a\u9002\u5e94\u65b9\u6cd5\u3002"}}
{"id": "2510.03866", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03866", "abs": "https://arxiv.org/abs/2510.03866", "authors": ["Xinwen Zhang", "Hongchang Gao"], "title": "On Provable Benefits of Muon in Federated Learning", "comment": null, "summary": "The recently introduced optimizer, Muon, has gained increasing attention due\nto its superior performance across a wide range of applications. However, its\neffectiveness in federated learning remains unexplored. To address this gap,\nthis paper investigates the performance of Muon in the federated learning\nsetting. Specifically, we propose a new algorithm, FedMuon, and establish its\nconvergence rate for nonconvex problems. Our theoretical analysis reveals\nmultiple favorable properties of FedMuon. In particular, due to its\northonormalized update direction, the learning rate of FedMuon is independent\nof problem-specific parameters, and, importantly, it can naturally accommodate\nheavy-tailed noise. The extensive experiments on a variety of neural network\narchitectures validate the effectiveness of the proposed algorithm.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86Muon\u4f18\u5316\u5668\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86FedMuon\u7b97\u6cd5\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u975e\u51f8\u95ee\u9898\u4e0a\u7684\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86FedMuon\u7b97\u6cd5\uff0c\u5229\u7528\u6b63\u4ea4\u5316\u66f4\u65b0\u65b9\u5411\uff0c\u4f7f\u5176\u5b66\u4e60\u7387\u72ec\u7acb\u4e8e\u95ee\u9898\u7279\u5b9a\u53c2\u6570\uff0c\u5e76\u80fd\u81ea\u7136\u5904\u7406\u91cd\u5c3e\u566a\u58f0\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eFedMuon\u5177\u6709\u591a\u4e2a\u6709\u5229\u7279\u6027\uff0c\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u5728\u5404\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "FedMuon\u7b97\u6cd5\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.03309", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2510.03309", "abs": "https://arxiv.org/abs/2510.03309", "authors": ["Mallikarjuna Tupakula"], "title": "Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval", "comment": null, "summary": "Multimodal foundation models hold promise for drug discovery and biomedical\napplications, but most existing approaches rely on heavy pretraining or large\nscale multimodal corpora. We investigate whether thin contrastive bridges,\nlightweight projection heads over frozen unimodal encoders can align chemical\nand textual representations without training a full multimodal model. Using\npaired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with\nbiomedical sentence embeddings through dual linear projections trained with a\ncontrastive objective. To better handle drugs sharing the same therapeutic\ntarget, we incorporate hard negative weighting and a margin loss. Evaluation\nunder scaffold based splits, which require generalization across disjoint\nchemical cores, demonstrates that our approach achieves non-trivial cross modal\nalignment and substantially improves within target discrimination compared to\nfrozen baselines. These results suggest that thin bridges offer a compute\nefficient alternative to large scale multimodal pretraining, enabling scaffold\naware drug text alignment and target specific retrieval in precision medicine.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5bf9\u6bd4\u6865\u63a5\u65b9\u6cd5\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\u548c\u7b80\u5355\u7684\u7ebf\u6027\u6295\u5f71\u5934\uff0c\u5b9e\u73b0\u5316\u5b66\u5206\u5b50\u6307\u7eb9\u4e0e\u751f\u7269\u533b\u5b66\u6587\u672c\u5d4c\u5165\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u3002", "motivation": "\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7e41\u91cd\u7684\u9884\u8bad\u7ec3\u6216\u5927\u89c4\u6a21\u591a\u6a21\u6001\u8bed\u6599\u5e93\u3002\u672c\u6587\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u5934\u5728\u51bb\u7ed3\u7684\u5355\u6a21\u6001\u7f16\u7801\u5668\u4e0a\u5b9e\u73b0\u5316\u5b66\u548c\u6587\u672c\u8868\u793a\u7684\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528ChEMBL\u4e2d\u7684\u914d\u5bf9\u673a\u5236\uff0c\u901a\u8fc7\u5bf9\u6bd4\u76ee\u6807\u8bad\u7ec3\u7684\u53cc\u7ebf\u6027\u6295\u5f71\u5c06ECFP4\u5206\u5b50\u6307\u7eb9\u4e0e\u751f\u7269\u533b\u5b66\u53e5\u5b50\u5d4c\u5165\u5bf9\u9f50\u3002\u5f15\u5165\u786c\u8d1f\u6837\u672c\u52a0\u6743\u548c\u8fb9\u754c\u635f\u5931\u6765\u5904\u7406\u5171\u4eab\u76f8\u540c\u6cbb\u7597\u9776\u70b9\u7684\u836f\u7269\u3002", "result": "\u5728\u57fa\u4e8e\u652f\u67b6\u7684\u5206\u5272\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u975e\u5e73\u51e1\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u5185\u533a\u5206\u80fd\u529b\uff0c\u76f8\u6bd4\u51bb\u7ed3\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8584\u6865\u63a5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7cbe\u51c6\u533b\u5b66\u4e2d\u5b9e\u73b0\u652f\u67b6\u611f\u77e5\u7684\u836f\u7269\u6587\u672c\u5bf9\u9f50\u548c\u9776\u70b9\u7279\u5f02\u6027\u68c0\u7d22\u3002"}}
{"id": "2510.03871", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.03871", "abs": "https://arxiv.org/abs/2510.03871", "authors": ["Oleg Filatov", "Jiangtao Wang", "Jan Ebert", "Stefan Kesselheim"], "title": "Optimal Scaling Needs Optimal Norm", "comment": null, "summary": "Despite recent progress in optimal hyperparameter transfer under model and\ndataset scaling, no unifying explanatory principle has been established. Using\nthe Scion optimizer, we discover that joint optimal scaling across model and\ndataset sizes is governed by a single invariant: the operator norm of the\noutput layer. Across models with up to 1.3B parameters trained on up to 138B\ntokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$\nconsistently has the same operator norm value - a phenomenon we term norm\ntransfer. This constant norm condition is necessary but not sufficient: while\nfor each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a\nunique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient\ncondition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$\nscaling with dataset size for Scion, and find that the scaling rules are\nconsistent with those of the Adam optimizer. Tuning per-layer-group learning\nrates also improves model performance, with the output layer being the most\nsensitive and hidden layers benefiting from lower learning rates. We provide\npractical insights on norm-guided optimal scaling and release our Distributed\nScion (Disco) implementation with logs from over two thousand runs to support\nresearch on LLM training dynamics at scale.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u548c\u6570\u636e\u96c6\u89c4\u6a21\u8054\u5408\u4f18\u5316\u7f29\u653e\u7531\u8f93\u51fa\u5c42\u7684\u7b97\u5b50\u8303\u6570\u8fd9\u4e00\u5355\u4e00\u4e0d\u53d8\u91cf\u63a7\u5236\uff0c\u79f0\u4e3a\u8303\u6570\u8f6c\u79fb\u73b0\u8c61\u3002", "motivation": "\u5c3d\u7ba1\u5728\u6a21\u578b\u548c\u6570\u636e\u96c6\u7f29\u653e\u4e0b\u7684\u6700\u4f18\u8d85\u53c2\u6570\u8f6c\u79fb\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5c1a\u672a\u5efa\u7acb\u7edf\u4e00\u7684\u89e3\u91ca\u539f\u5219\u3002", "method": "\u4f7f\u7528Scion\u4f18\u5316\u5668\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u5b66\u4e60\u7387\u548c\u6279\u6b21\u5927\u5c0f\u7684\u6700\u4f18\u7ec4\u5408\uff0c\u5e76\u6d4b\u91cf\u7b97\u5b50\u8303\u6570\u3002", "result": "\u57281.3B\u53c2\u6570\u6a21\u578b\u548c138B tokens\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u4f18\u5b66\u4e60\u7387/\u6279\u6b21\u5927\u5c0f\u7ec4\u5408\u5177\u6709\u76f8\u540c\u7684\u7b97\u5b50\u8303\u6570\u503c\uff1b\u8f93\u51fa\u5c42\u5bf9\u5b66\u4e60\u7387\u6700\u654f\u611f\uff0c\u9690\u85cf\u5c42\u53d7\u76ca\u4e8e\u8f83\u4f4e\u5b66\u4e60\u7387\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8303\u6570\u6307\u5bfc\u7684\u6700\u4f18\u7f29\u653e\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86\u5206\u5e03\u5f0fScion\u5b9e\u73b0\u548c\u4e24\u5343\u591a\u6b21\u8fd0\u884c\u7684\u65e5\u5fd7\u4ee5\u652f\u6301\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u52a8\u6001\u7814\u7a76\u3002"}}
{"id": "2510.03310", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03310", "abs": "https://arxiv.org/abs/2510.03310", "authors": ["Runze Zhang", "Xiaowei Zhang", "Mingyang Zhao"], "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management", "comment": null, "summary": "LLMs are emerging tools for simulating human behavior in business, economics,\nand social science, offering a lower-cost complement to laboratory experiments,\nfield studies, and surveys. This paper evaluates how well LLMs replicate human\nbehavior in operations management. Using nine published experiments in\nbehavioral operations, we assess two criteria: replication of hypothesis-test\noutcomes and distributional alignment via Wasserstein distance. LLMs reproduce\nmost hypothesis-level effects, capturing key decision biases, but their\nresponse distributions diverge from human data, including for strong commercial\nmodels. We also test two lightweight interventions -- chain-of-thought\nprompting and hyperparameter tuning -- which reduce misalignment and can\nsometimes let smaller or open-source models match or surpass larger systems.", "AI": {"tldr": "\u8bc4\u4f30LLMs\u5728\u8fd0\u8425\u7ba1\u7406\u4e2d\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u867d\u7136\u80fd\u590d\u73b0\u5927\u90e8\u5206\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\uff0c\u4f46\u54cd\u5e94\u5206\u5e03\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u5dee\u5f02\uff0c\u8f7b\u91cf\u7ea7\u5e72\u9884\u63aa\u65bd\u53ef\u6539\u5584\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "LLMs\u4f5c\u4e3a\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u4f4e\u6210\u672c\u5de5\u5177\u5728\u5546\u4e1a\u3001\u7ecf\u6d4e\u548c\u793e\u4f1a\u79d1\u5b66\u4e2d\u5174\u8d77\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u8fd0\u8425\u7ba1\u7406\u9886\u57df\u590d\u5236\u4eba\u7c7b\u884c\u4e3a\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u75289\u4e2a\u5df2\u53d1\u8868\u7684\u884c\u4e3a\u8fd0\u8425\u5b9e\u9a8c\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\u590d\u73b0\u548cWasserstein\u8ddd\u79bb\u5206\u5e03\u5bf9\u9f50\u4e24\u4e2a\u6807\u51c6\u6765\u8bc4\u4f30LLMs\uff0c\u5e76\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\u3002", "result": "LLMs\u80fd\u590d\u73b0\u5927\u90e8\u5206\u5047\u8bbe\u7ea7\u6548\u5e94\uff0c\u6355\u6349\u5173\u952e\u51b3\u7b56\u504f\u5dee\uff0c\u4f46\u54cd\u5e94\u5206\u5e03\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u5dee\u5f02\uff0c\u5305\u62ec\u5f3a\u5546\u4e1a\u6a21\u578b\u3002\u8f7b\u91cf\u7ea7\u5e72\u9884\u53ef\u51cf\u5c11\u4e0d\u5bf9\u9f50\uff0c\u6709\u65f6\u8ba9\u5c0f\u578b\u6216\u5f00\u6e90\u6a21\u578b\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u578b\u7cfb\u7edf\u3002", "conclusion": "LLMs\u5728\u8fd0\u8425\u7ba1\u7406\u4e2d\u80fd\u6709\u6548\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u5047\u8bbe\u68c0\u9a8c\u7ed3\u679c\uff0c\u4f46\u5206\u5e03\u5bf9\u9f50\u9700\u8981\u6539\u8fdb\uff0c\u8f7b\u91cf\u7ea7\u5e72\u9884\u63aa\u65bd\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u62df\u51c6\u786e\u6027\u3002"}}
{"id": "2510.04072", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04072", "abs": "https://arxiv.org/abs/2510.04072", "authors": ["Ziyan Wang", "Zheng Wang", "Jie Fu", "Xingwei Qu", "Qi Cheng", "Shengpu Tang", "Minjia Zhang", "Xiaoming Huo"], "title": "Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has become central to enhancing reasoning in\nlarge language models (LLMs). Yet on-policy algorithms such as Group Relative\nPolicy Optimization (GRPO) often suffer in early training: noisy gradients from\nlow-quality rollouts lead to unstable updates and inefficient exploration. We\nintroduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient\nframework to address these limitations via decomposing each step into three\nstages: a short fast trajectory of inner steps on the same batch, a reposition\nmechanism to control off-policy drift, and a final slow correction. This\nreposition-before-update design preserves the objective and rollout process\nunchanged, making SFPO plug-compatible with existing policy-gradient pipelines.\nExtensive experiments demonstrate that SFPO consistently improves stability,\nreduces rollouts, and accelerates convergence of reasoning RL training.\nSpecifically, it outperforms GRPO by up to 2.80 points in average on math\nreasoning benchmarks. It also achieves up to 4.93\\texttimes{} fewer rollouts\nand a 4.19\\texttimes{} reduction in wall-clock time to match GRPO's best\naccuracy.", "AI": {"tldr": "SFPO\u901a\u8fc7\u6162-\u5feb\u7b56\u7565\u4f18\u5316\u6846\u67b6\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65e9\u671f\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4GRPO\u63d0\u53472.80\u5206\uff0c\u51cf\u5c114.93\u500drollouts\uff0c\u52a0\u901f4.19\u500d\u6536\u655b\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u5982GRPO\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u56e0\u4f4e\u8d28\u91cfrollouts\u4ea7\u751f\u7684\u566a\u58f0\u68af\u5ea6\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u66f4\u65b0\u548c\u4f4e\u6548\u63a2\u7d22\u3002", "method": "\u63d0\u51faSFPO\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u5206\u89e3\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5728\u76f8\u540c\u6279\u6b21\u4e0a\u7684\u77ed\u5feb\u901f\u8f68\u8ff9\u3001\u63a7\u5236\u79bb\u7b56\u7565\u6f02\u79fb\u7684\u91cd\u5b9a\u4f4d\u673a\u5236\u3001\u4ee5\u53ca\u6700\u7ec8\u7684\u6162\u901f\u6821\u6b63\u3002", "result": "SFPO\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u6bd4GRPO\u5e73\u5747\u63d0\u53472.80\u5206\uff0c\u51cf\u5c114.93\u500drollouts\uff0c\u52a0\u901f4.19\u500d\u6536\u655b\u65f6\u95f4\u3002", "conclusion": "SFPO\u901a\u8fc7\u91cd\u5b9a\u4f4d-\u66f4\u65b0\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u4e0e\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u6d41\u7a0b\u517c\u5bb9\u3002"}}
{"id": "2510.03313", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03313", "abs": "https://arxiv.org/abs/2510.03313", "authors": ["Anirudh Subramanyam", "Yuxin Chen", "Robert L. Grossman"], "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining", "comment": "18 pages, 6 figures", "summary": "Scaling laws for language model training traditionally characterize how\nperformance scales with model size and dataset volume. Prior work has explored\narchitecture variants and data treatments such as dataset filtering and noise\ninjection in language model pretraining; however, these studies have not\nformalized data quality within a principled scaling law. We introduce a\ndimensionless data-quality parameter Q, and propose a quality-aware scaling law\nextending the Chinchilla framework to predict loss as a joint function of model\nsize, data volume, and data quality. The law is motivated by an\neffective-sample-size and information-theoretic view of noisy or redundant\ncorpora, and it admits two practical estimators for Q: (i) a corruption rate\nproxy and (ii) a deficiency measure. Through synthetic experiments in neural\nmachine translation and autoregressive modeling -- where we systematically\ncontrol data quality via multiple levels of noise injection and coverage\nvariation -- we show that loss scales predictably with data quality and that\nhigher-quality data can substantially reduce model size and hence compute\nrequirements. Our results demonstrate a sublinear decay of effective data with\nquality and robustness to moderate data corruption; out-of-sample evaluations\nfurther validate the predictive form of the law. Unlike prior empirical\nanalyses, our work establishes an explicit, generalizable law for data quality,\noffering concrete guidance for balancing data curation effort and model scale\nin large-scale pretraining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u6570\u636e\u8d28\u91cf\u53c2\u6570\u7684\u6269\u5c55\u7f29\u653e\u5b9a\u5f8b\uff0c\u5c06Chinchilla\u6846\u67b6\u6269\u5c55\u5230\u540c\u65f6\u8003\u8651\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u91cf\u548c\u6570\u636e\u8d28\u91cf\uff0c\u901a\u8fc7\u6709\u6548\u6837\u672c\u91cf\u548c\u4fe1\u606f\u8bba\u89c6\u89d2\u5f62\u5f0f\u5316\u6570\u636e\u8d28\u91cf\u5bf9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u7f29\u653e\u5b9a\u5f8b\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u5927\u5c0f\u548c\u6570\u636e\u91cf\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u8d28\u91cf\u7684\u5f62\u5f0f\u5316\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u5305\u542b\u6570\u636e\u8d28\u91cf\u53c2\u6570\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u6570\u636e\u7b5b\u9009\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5e73\u8861\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u5f15\u5165\u65e0\u91cf\u7eb2\u6570\u636e\u8d28\u91cf\u53c2\u6570Q\uff0c\u63d0\u51fa\u8d28\u91cf\u611f\u77e5\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc7\u6709\u6548\u6837\u672c\u91cf\u548c\u4fe1\u606f\u8bba\u89c6\u89d2\u5206\u6790\u566a\u58f0\u6216\u5197\u4f59\u8bed\u6599\u5e93\u3002\u5f00\u53d1\u4e86\u4e24\u79cdQ\u7684\u5b9e\u7528\u4f30\u8ba1\u5668\uff1a\u8150\u8d25\u7387\u4ee3\u7406\u548c\u7f3a\u9677\u5ea6\u91cf\uff0c\u5e76\u5728\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u548c\u81ea\u56de\u5f52\u5efa\u6a21\u4e2d\u8fdb\u884c\u5408\u6210\u5b9e\u9a8c\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u8986\u76d6\u53d8\u5316\u7cfb\u7edf\u63a7\u5236\u6570\u636e\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u635f\u5931\u968f\u6570\u636e\u8d28\u91cf\u53ef\u9884\u6d4b\u5730\u7f29\u653e\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u53ef\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u7ed3\u679c\u5c55\u793a\u4e86\u6709\u6548\u6570\u636e\u968f\u8d28\u91cf\u7684\u4e9a\u7ebf\u6027\u8870\u51cf\u548c\u5bf9\u9002\u5ea6\u6570\u636e\u8150\u8d25\u7684\u9c81\u68d2\u6027\uff0c\u6837\u672c\u5916\u8bc4\u4f30\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5b9a\u5f8b\u7684\u9884\u6d4b\u5f62\u5f0f\u3002", "conclusion": "\u4e0e\u5148\u524d\u7684\u5b9e\u8bc1\u5206\u6790\u4e0d\u540c\uff0c\u672c\u7814\u7a76\u4e3a\u6570\u636e\u8d28\u91cf\u5efa\u7acb\u4e86\u4e00\u4e2a\u660e\u786e\u3001\u53ef\u63a8\u5e7f\u7684\u5b9a\u5f8b\uff0c\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u6570\u636e\u7b5b\u9009\u5de5\u4f5c\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5e73\u8861\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2510.04088", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04088", "abs": "https://arxiv.org/abs/2510.04088", "authors": ["Nan Jiang", "Tengyang Xie"], "title": "Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees", "comment": "To appear in Statistical Science", "summary": "This article introduces the theory of offline reinforcement learning in large\nstate spaces, where good policies are learned from historical data without\nonline interactions with the environment. Key concepts introduced include\nexpressivity assumptions on function approximation (e.g., Bellman completeness\nvs. realizability) and data coverage (e.g., all-policy vs. single-policy\ncoverage). A rich landscape of algorithms and results is described, depending\non the assumptions one is willing to make and the sample and computational\ncomplexity guarantees one wishes to achieve. We also discuss open questions and\nconnections to adjacent areas.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u7b56\u7565\u800c\u4e0d\u9700\u8981\u4e0e\u73af\u5883\u5728\u7ebf\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u51b3\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u4ec5\u4f7f\u7528\u5386\u53f2\u6570\u636e\u5b66\u4e60\u6709\u6548\u7b56\u7565\u7684\u6311\u6218\uff0c\u907f\u514d\u5728\u7ebf\u4ea4\u4e92\u7684\u6210\u672c\u548c\u98ce\u9669\u3002", "method": "\u57fa\u4e8e\u51fd\u6570\u903c\u8fd1\u7684\u8868\u8fbe\u6027\u5047\u8bbe\uff08\u5982Bellman\u5b8c\u5907\u6027\u4e0e\u53ef\u5b9e\u73b0\u6027\uff09\u548c\u6570\u636e\u8986\u76d6\u5047\u8bbe\uff08\u5982\u5168\u7b56\u7565\u8986\u76d6\u4e0e\u5355\u7b56\u7565\u8986\u76d6\uff09\uff0c\u6784\u5efa\u7b97\u6cd5\u548c\u7406\u8bba\u5206\u6790\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u4e30\u5bcc\u7684\u7b97\u6cd5\u548c\u7ed3\u679c\u4f53\u7cfb\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u5047\u8bbe\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u7684\u6837\u672c\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4fdd\u8bc1\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u72b6\u6001\u7a7a\u95f4\u4e2d\u5177\u6709\u91cd\u8981\u7406\u8bba\u4ef7\u503c\uff0c\u4f46\u4ecd\u6709\u5f00\u653e\u6027\u95ee\u9898\u9700\u8981\u89e3\u51b3\uff0c\u5e76\u4e0e\u76f8\u5173\u9886\u57df\u5b58\u5728\u5bc6\u5207\u8054\u7cfb\u3002"}}
