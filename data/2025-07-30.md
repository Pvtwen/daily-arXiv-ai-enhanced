<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 17]
- [cs.LG](#cs.LG) [Total: 64]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [DOA Estimation via Continuous Aperture Arrays: MUSIC and CRLB](https://arxiv.org/abs/2507.21347)
*Haonan Si,Zhaolin Wang,Xiansheng Guo,Jin Zhang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 论文研究了使用连续孔径阵列（CAPA）进行方向到达（DOA）估计，提出了一种新的MUSIC算法，解决了传统算法不适用的问题，并通过理论分析和数值结果验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统离散阵列（SPDA）在DOA估计中的自由度有限，而连续孔径阵列（CAPA）虽然提供了更高的自由度，但其无限维连续信号使得传统估计算法无法直接应用。

Method: 提出了一种新的MUSIC算法，通过等效连续-离散变换实现连续算子的特征分解，并使用高斯-勒让德积分近似MUSIC谱以降低计算复杂度。

Result: 理论分析和数值结果表明，CAPA显著提高了DOA估计的精度，且提出的MUSIC算法在保持低计算复杂度的同时实现了接近最优的估计性能。

Conclusion: CAPA在DOA估计中具有显著优势，提出的MUSIC算法有效解决了传统方法的局限性，为实际应用提供了高效且准确的解决方案。

Abstract: Direction-of-arrival (DOA) estimation using continuous aperture array (CAPA)
is studied. Compared to the conventional spatially discrete array (SPDA), CAPA
significantly enhances the spatial degrees-of-freedoms (DoFs) for DOA
estimation, but its infinite-dimensional continuous signals render the
conventional estimation algorithm non-applicable. To address this challenge, a
new multiple signal classification (MUSIC) algorithm is proposed for CAPAs. In
particular, an equivalent continuous-discrete transformation is proposed to
facilitate the eigendecomposition of continuous operators. Subsequently, the
MUSIC spectrum is accurately approximated using the Gauss-Legendre quadrature,
effectively reducing the computational complexity. Furthermore, the
Cram\'er-Rao lower bounds (CRLBs) for DOA estimation using CAPAs are analyzed
for both cases with and without priori knowledge of snapshot signals. It is
theoretically proved that CAPAs significantly improve the DOA estimation
accuracy compared to traditional SPDAs. Numerical results further validate this
insight and demonstrate the effectiveness of the proposed MUSIC algorithm for
CAPA. The proposed method achieves near-optimal estimation performance while
maintaining a low computational complexity.

</details>


### [2] [Transmission With Machine Language Tokens: A Paradigm for Task-Oriented Agent Communication](https://arxiv.org/abs/2507.21454)
*Zhuoran Xiao,Chenhui Ye,Yijia Feng,Yunbo Hu,Tianyu Jiao,Liyu Cai,Guangyi Liu*

Main category: eess.SP

TL;DR: 本文提出了一种面向任务的智能体通信系统，利用LLM学习专用机器语言，并通过多模态LLM提取任务信息，结合联合令牌和信道编码（JTCC）降低传输开销。


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型的快速发展，未来生产过程中智能体将成为主要参与者，需要一种专为智能体通信设计的AI原生通信系统。

Method: 利用原始LLM学习专用机器语言，训练多模态LLM理解任务并提取信息，采用JTCC方案压缩令牌序列并增强抗噪能力。

Result: 实验表明，该方法在降低传输开销的同时，提高了下游任务的准确性。

Conclusion: 提出的系统显著提升了智能体通信的效率和准确性，优于现有方法。

Abstract: The rapid advancement in large foundation models is propelling the paradigm
shifts across various industries. One significant change is that agents,
instead of traditional machines or humans, will be the primary participants in
the future production process, which consequently requires a novel AI-native
communication system tailored for agent communications. Integrating the ability
of large language models (LLMs) with task-oriented semantic communication is a
potential approach. However, the output of existing LLM is human language,
which is highly constrained and sub-optimal for agent-type communication. In
this paper, we innovatively propose a task-oriented agent communication system.
Specifically, we leverage the original LLM to learn a specialized machine
language represented by token embeddings. Simultaneously, a multi-modal LLM is
trained to comprehend the application task and to extract essential implicit
information from multi-modal inputs, subsequently expressing it using machine
language tokens. This representation is significantly more efficient for
transmission over the air interface. Furthermore, to reduce transmission
overhead, we introduce a joint token and channel coding (JTCC) scheme that
compresses the token sequence by exploiting its sparsity while enhancing
robustness against channel noise. Extensive experiments demonstrate that our
approach reduces transmission overhead for downstream tasks while enhancing
accuracy relative to the SOTA methods.

</details>


### [3] [Two-Dimensional Nonseparable Fractional Fourier Transform: Theory and Application](https://arxiv.org/abs/2507.21511)
*Daxiang Li,Zhichao Zhang,Wei Yao*

Main category: eess.SP

TL;DR: 论文提出了一种新的2D非可分分数傅里叶变换（NSFRFT），解决了现有2D FRFT的局限性，并在应用中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D FRFT在理论统一性、处理非可分信号和保持与2D Wigner分布的一致性方面存在不足，限制了其在实际应用中的表现。

Method: 提出2D NSFRFT，具有四个自由度，包含现有2D FRFT为特例，并保持更一般的4D旋转关系。推导其性质并开发三种离散算法，其中两种为快速算法。

Result: 数值模拟和实验表明，2D NSFRFT在图像加密、解密、滤波和去噪等应用中表现优越。

Conclusion: 2D NSFRFT是一种更通用的2D FRFT定义，解决了现有方法的局限性，并在实际应用中展现出显著优势。

Abstract: The one-dimensional (1D) fractional Fourier transform (FRFT) generalizes the
1D Fourier transform, offering significant advantages in time-frequency
analysis of non-stationary signals. To extend the benefits of the 1D FRFT to
higher-dimensional signals, 2D FRFTs, such as the 2D separable FRFT (SFRFT),
gyrator transform (GT), and coupled FRFT (CFRFT), have been developed. However,
existing 2D FRFTs suffer from several limitations: (1) a lack of theoretical
uniformity and general applicability, (2) an inability to handle 2D
non-stationary signals with nonseparable terms, and (3) failure to maintain a
consistent 4D rotational relationship with the 2D Wigner distribution (WD),
which is essential for ensuring geometric consistency and symmetry in
time-frequency analysis. These limitations restrict the methods' performance in
practical applications, such as radar, communication, sonar, and optical
imaging, in which nonseparable terms frequently arise. To address these
challenges, we introduce a more general definition of the 2D FRFT, termed the
2D nonseparable FRFT (NSFRFT). The 2D NSFRFT has four degrees of freedom,
includes the 2D SFRFT, GT, and CFRFT as special cases, and maintains a more
general 4D rotational relationship with the 2D WD. We derive its properties and
present three discrete algorithms, two of which are fast algorithms with
computational complexity $O(N^2 \log N)$ comparable to that of the 2D SFRFT.
Numerical simulations and experiments demonstrate the superior performance of
the 2D NSFRFT in applications such as image encryption, decryption, filtering,
and denoising.

</details>


### [4] [Trainable Joint Time-Vertex Fractional Fourier Transform](https://arxiv.org/abs/2507.21527)
*Ziqi Yan,Zhichao Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于超微分形式的JFRFT滤波方法，通过梯度反向传播自适应选择变换阶数和滤波器系数，提升了时变图信号的去噪性能并降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 解决GFRFT和传统JFRFT维纳滤波的局限性，提出更高效的滤波方法。

Method: 构建JFRFT的超微分形式并分析其性质，将时变图信号分割为动态图序列，建立时空联合表示，并通过神经网络学习变换阶数和滤波器系数。

Result: 实验表明该方法提升了去噪性能并减少了计算负担。

Conclusion: 提出的参数自适应学习滤波框架在时变图信号处理中具有优越性。

Abstract: To address limitations of the graph fractional Fourier transform (GFRFT)
Wiener filtering and the traditional joint time-vertex fractional Fourier
transform (JFRFT) Wiener filtering, this study proposes a filtering method
based on the hyper-differential form of the JFRFT. The gradient backpropagation
mechanism is employed to enable the adaptive selection of transform order pair
and filter coefficients. First, leveraging the hyper-differential form of the
GFRFT and the fractional Fourier transform, the hyper-differential form of the
JFRFT is constructed and its properties are analyzed. Second, time-varying
graph signals are divided into dynamic graph sequences of equal span along the
temporal dimension. A spatiotemporal joint representation is then established
through vectorized reorganization, followed by the joint time-vertex Wiener
filtering. Furthermore, by rigorously proving the differentiability of the
transform orders, both the transform orders and filter coefficients are
embedded as learnable parameters within a neural network architecture. Through
gradient backpropagation, their synchronized iterative optimization is
achieved, constructing a parameters-adaptive learning filtering framework. This
method leverages a model-driven approach to learn the optimal transform order
pair and filter coefficients. Experimental results indicate that the proposed
framework improves the time-varying graph signals denoising performance, while
reducing the computational burden of the traditional grid search strategy.

</details>


### [5] [Causal Link Discovery with Unequal Edge Error Tolerance](https://arxiv.org/abs/2507.21570)
*Joni Shaska,Urbashi Mitra*

Main category: eess.SP

TL;DR: 提出了一种名为Neyman-Pearson因果发现的新框架，用于控制不对称误差，通过信息论技术找到性能极限，并针对线性高斯噪声模型提出了一种算法epsilon-CUT。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法未区分不同类型的边错误，也未提供有限样本保证，而实际应用中不同类型的错误重要性不同。

Method: 提出Neyman-Pearson因果发现框架，最小化一类错误的同时控制另一类错误在用户指定水平，并基于Rényi散度分析性能极限。针对线性高斯噪声模型，提出epsilon-CUT算法。

Result: epsilon-CUT算法在有限样本下保证假阳性率，同时与现有方法竞争。

Conclusion: 该框架填补了因果发现中不对称误差控制的空白，并通过epsilon-CUT算法实现了理论保证与实用性结合。

Abstract: This paper proposes a novel framework for causal discovery with asymmetric
error control, called Neyman-Pearson causal discovery. Despite the importance
of applications where different types of edge errors may have different
importance, current state-of-the-art causal discovery algorithms do not
differentiate between the types of edge errors, nor provide any finite-sample
guarantees on the edge errors. Hence, this framework seeks to minimize one type
of error while keeping the other below a user-specified tolerance level. Using
techniques from information theory, fundamental performance limits are found,
characterized by the R\'enyi divergence, for Neyman-Pearson causal discovery.
Furthermore, a causal discovery algorithm is introduced for the case of linear
additive Gaussian noise models, called epsilon-CUT, that provides finite-sample
guarantees on the false positive rate, while staying competitive with
state-of-the-art methods.

</details>


### [6] [Affine Invariant Semi-Blind Receiver: Joint Channel Estimation and High-Order Signal Detection for Multiuser Massive MIMO-OFDM Systems](https://arxiv.org/abs/2507.21593)
*Erdeng Zhang,Shuntian Zheng,Sheng Wu,Haoge Jia,Zhe Ji,Ailing Xiao*

Main category: eess.SP

TL;DR: 提出了一种新型半盲联合信道估计与信号检测（JCESD）方法，用于大规模MIMO-OFDM系统，通过混合预编码抑制用户间干扰，利用星座拟合优化和迭代细化策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 高用户密度导致严重的用户间干扰和导频开销，现有盲和半盲方法在频率选择性信道和高阶调制下性能下降且复杂度高。

Method: 采用混合预编码架构抑制干扰，将JCESD建模为非凸星座拟合优化，利用少量导频初始化，通过数据增强和迭代细化提升性能。

Result: 仿真结果显示，在多用户场景中平均吞吐量比广泛使用的基于导频的方法提高11%。

Conclusion: 该方法显著提升了频谱效率，适用于未来无线网络的高密度用户场景。

Abstract: Massive multiple input and multiple output (MIMO) systems with orthogonal
frequency division multiplexing (OFDM) are foundational for downlink multi-user
(MU) communication in future wireless networks, for their ability to enhance
spectral efficiency and support a large number of users simultaneously.
However, high user density intensifies severe inter-user interference (IUI) and
pilot overhead. Consequently, existing blind and semi-blind channel estimation
(CE) and signal detection (SD) algorithms suffer performance degradation and
increased complexity, especially when further challenged by frequency-selective
channels and high-order modulation demands. To this end, this paper proposes a
novel semi-blind joint channel estimation and signal detection (JCESD) method.
Specifically, the proposed approach employs a hybrid precoding architecture to
suppress IUI. Furthermore we formulate JCESD as a non-convex constellation
fitting optimization exploiting constellation affine invariance. Few pilots are
used to achieve coarse estimation for initialization and ambiguity resolution.
For high-order modulations, a data augmentation mechanism utilizes the symmetry
of quadrature amplitude modulation (QAM) constellations to increase the
effective number of samples. To address frequency-selective channels, CE
accuracy is then enhanced via an iterative refinement strategy that leverages
improved SD results. Simulation results demonstrate an average throughput gain
of 11\% over widely used pilot-based methods in MU scenarios, highlighting the
proposed method's potential to improve spectral efficiency.

</details>


### [7] [Comprehensive Analysis of Behavioral Hardware Impairments in Cell-Free Massive MIMO-OFDM Uplink: Centralized Operation](https://arxiv.org/abs/2507.21626)
*Özlem Tuğfe Demir,Muhammed Selman Somuncu,Ahmet M. Elbir,Emil Björnson*

Main category: eess.SP

TL;DR: 本文首次全面分析了硬件损伤对无蜂窝大规模MIMO上行链路频谱效率的影响，提出了基于Bussgang分解的失真感知合并向量优化方法。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO是6G关键技术，但其密集部署的低成本接入点（APs）不可避免存在硬件损伤，尤其是宽带系统中的影响尚未研究。

Method: 采用OFDM波形和集中式处理，APs共享信道状态信息进行联合上行链路合并，利用Bussgang分解建模失真为独立有色噪声，优化频谱效率。

Result: 提出了一种失真感知合并向量，能够有效优化频谱效率。

Conclusion: 该研究填补了宽带系统中硬件损伤对无蜂窝大规模MIMO影响的空白，为实际部署提供了理论支持。

Abstract: Cell-free massive MIMO is a key 6G technology, offering superior spectral and
energy efficiency. However, its dense deployment of low-cost access points
(APs) makes hardware impairments unavoidable. While narrowband impairments are
well-studied, their impact in wideband systems remains unexplored. This paper
provides the first comprehensive analysis of hardware impairments, such as
nonlinear distortion in low-noise amplifiers, phase noise, in-phase-quadrature
imbalance, and low-resolution analog-to-digital converters, on uplink spectral
efficiency in cell-free massive MIMO. Using an OFDM waveform and centralized
processing, APs share channel state information for joint uplink combining.
Leveraging Bussgang decomposition, we derive a distortion-aware combining
vector that optimizes spectral efficiency by modeling distortion as independent
colored noise.

</details>


### [8] [Impact of Phase Noise and Power Amplifier Non-Linearities on Downlink Cell-Free Massive MIMO-OFDM Systems](https://arxiv.org/abs/2507.21635)
*Özlem Tuğfe Demir,Emil Björnson*

Main category: eess.SP

TL;DR: 本文分析了在硬件损伤（如相位噪声和功率放大器非线性）下，无蜂窝大规模MIMO-OFDM系统的下行链路频谱效率（SE），并比较了集中式和分布式预编码策略的影响。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO是6G移动网络的关键技术，但其依赖低成本接入点（APs）引入了硬件损伤，这些损伤对宽带下行链路系统的影响尚未被充分研究。

Method: 利用Bussgang分解推导SE表达式，并通过仿真量化相位噪声和功率放大器非线性对系统性能的相对影响。

Result: 结果表明，相位噪声比功率放大器非线性对系统性能的损害更严重，尤其是在分布式操作中。

Conclusion: 未来需要设计能够考虑硬件损伤的预编码方案。

Abstract: Cell-free massive MIMO (multiple-input multiple-output) is a key enabler for
the sixth generation (6G) of mobile networks, offering significant spectral and
energy efficiency gains through user-centric operation of distributed access
points (APs). However, its reliance on low-cost APs introduces inevitable
hardware impairments, whose combined impact on wideband downlink systems
remains unexplored when analyzed using behavioral models. This paper presents a
comprehensive analysis of the downlink spectral efficiency (SE) in cell-free
massive MIMO-OFDM systems under practical hardware impairments, including phase
noise and third-order power amplifier nonlinearities. Both centralized and
distributed precoding strategies are examined. By leveraging the Bussgang
decomposition, we derive an SE expression and quantify the relative impact of
impairments through simulations. Our results reveal that phase noise causes
more severe degradation than power amplifier distortions, especially in
distributed operation, highlighting the need for future distortion-aware
precoding designs.

</details>


### [9] [Energy-Aware Resource Allocation for Multi-Operator Cell-Free Massive MIMO in V-CRAN Architectures](https://arxiv.org/abs/2507.21644)
*Derya Nurcan-Atceken,Özlem Tuğfe Demir,Aysegul Altin-Kayhan,Emil Björnson,Cicek Cavdar,Bulent Tavli*

Main category: eess.SP

TL;DR: 本文提出了一种在虚拟化云无线接入网络（V-CRAN）中部署无小区大规模MIMO的优化框架，旨在通过联合优化AP选择、UE关联、云资源分配和MNO分配，最小化多个MNO的总功耗。


<details>
  <summary>Details</summary>
Motivation: 提升下一代无线系统的频谱效率、网络灵活性和能源效率。

Method: 采用混合整数规划（MIP）模型，联合优化AP选择、UE关联、云资源分配和MNO分配，并考虑两种UE-MNO分配场景。

Result: 数值结果表明，灵活的UE-MNO分配显著降低了总功耗。

Conclusion: 研究为无小区大规模MIMO V-CRAN的资源管理优化提供了关键见解，有助于实现能源高效的无线网络部署。

Abstract: Cell-free massive multiple-input multiple-output (MIMO) implemented in
virtualized cloud radio access networks (V-CRAN) has emerged as a promising
architecture to enhance spectral efficiency (SE), network flexibility, and
energy efficiency (EE) in next-generation wireless systems. In this work, we
develop a holistic optimization framework for the efficient deployment of
cell-free massive MIMO in V-CRAN with multiple mobile network operators (MNOs).
Specifically, we formulate a set of mixed-integer programming (MIP) models to
jointly optimize access point (AP) selection, user equipment (UE) association,
cloud resource allocation, and MNO assignment while minimizing the maximum
total power consumption (TPC) across MNOs. We consider two different scenarios
based on whether UEs can be assigned to arbitrary MNOs or not. The numerical
results demonstrate the impact of different deployment assumptions on power
consumption, highlighting that flexible UE-MNO assignment significantly reduces
TPC. The findings provide key insights into optimizing resource management in
cell-free massive MIMO V-CRAN, paving the way for energy-efficient wireless
network implementations.

</details>


### [10] [Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN](https://arxiv.org/abs/2507.21696)
*Abdelaziz Salama,Zeinab Nezami,Mohammed M. H. Qazzaz,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: eess.SP

TL;DR: 本文提出了一种边缘AI框架，用于在Open RAN环境中实现自主网络优化，通过三项创新解决了6G网络的安全性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: AI代理在传统RAN基础设施中的部署对6G网络的安全性和可靠性提出了挑战，需要一种新的解决方案。

Method: 采用基于角色的多工具架构、主动异常检测代理和安全对齐奖励机制，结合多模态数据融合，实现动态网络条件的预测和响应。

Result: 在真实5G场景中，该框架实现了零网络中断，优于传统固定功率网络（8.4%）和基于LLM代理的方法（3.3%）。

Conclusion: 研究表明，配备适当工具和上下文感知能力的AI代理可以安全有效地部署在关键网络基础设施中，为智能自主的5G及未来网络奠定基础。

Abstract: The deployment of AI agents within legacy Radio Access Network (RAN)
infrastructure poses significant safety and reliability challenges for future
6G networks. This paper presents a novel Edge AI framework for autonomous
network optimisation in Open RAN environments, addressing these challenges
through three core innovations: (1) a persona-based multi-tools architecture
enabling distributed, context-aware decision-making; (2) proactive anomaly
detection agent powered by traffic predictive tool; and (3) a safety, aligned
reward mechanism that balances performance with operational stability.
Integrated into the RAN Intelligent Controller (RIC), our framework leverages
multimodal data fusion, including network KPIs, a traffic prediction model, and
external information sources, to anticipate and respond to dynamic network
conditions. Extensive evaluation using realistic 5G scenarios demonstrates that
the edge framework achieves zero network outages under high-stress conditions,
compared to 8.4% for traditional fixed-power networks and 3.3% for large
language model (LLM) agent-based approaches, while maintaining near real-time
responsiveness and consistent QoS. These results establish that, when equipped
with the right tools and contextual awareness, AI agents can be safely and
effectively deployed in critical network infrastructure, laying the framework
for intelligent and autonomous 5G and beyond network operations.

</details>


### [11] [EcoFL: Resource Allocation for Energy-Efficient Federated Learning in Multi-RAT ORAN Networks](https://arxiv.org/abs/2507.21698)
*Abdelaziz Salama,Mohammed M. H. Qazzaz,Syed Danial Ali Shah,Maryam Hafeez,Syed Ali Zaidi,Hamed Ahmadi*

Main category: eess.SP

TL;DR: EcoFL是一个集成联邦学习框架，利用ORAN架构和多RAT技术优化通信效率和鲁棒性，通过两阶段优化（RL和CNN）降低能耗19%。


<details>
  <summary>Details</summary>
Motivation: 解决无线网络中联邦学习的通信开销、不可靠连接和高能耗问题。

Method: 采用ORAN架构和多RAT技术，结合RL和CNN进行动态RAT选择和实时资源分配。

Result: 实验显示能耗降低19%，模型性能与基线相当。

Conclusion: EcoFL在动态网络中具有高效、节能的潜力。

Abstract: Federated Learning (FL) enables distributed model training on edge devices
while preserving data privacy. However, FL deployments in wireless networks
face significant challenges, including communication overhead, unreliable
connectivity, and high energy consumption, particularly in dynamic
environments. This paper proposes EcoFL, an integrated FL framework that
leverages the Open Radio Access Network (ORAN) architecture with multiple Radio
Access Technologies (RATs) to enhance communication efficiency and ensure
robust FL operations. EcoFL implements a two-stage optimisation approach: an
RL-based rApp for dynamic RAT selection that balances energy efficiency with
network performance, and a CNN-based xApp for near real-time resource
allocation with adaptive policies. This coordinated approach significantly
enhances communication resilience under fluctuating network conditions.
Experimental results demonstrate competitive FL model performance with 19\%
lower power consumption compared to baseline approaches, highlighting
substantial potential for scalable, energy-efficient collaborative learning
applications.

</details>


### [12] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: Recursive KalmanNet是一种由卡尔曼滤波引导的递归神经网络，能够在未知噪声特性的情况下估计动态系统的状态变量和误差协方差。本文探讨了其在分布外场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Recursive KalmanNet在测试数据与训练数据动态特性不同的情况下的泛化能力。

Method: 使用递归神经网络结合卡尔曼滤波，无需噪声先验知识。

Result: 展示了Recursive KalmanNet在分布外场景中的性能。

Conclusion: Recursive KalmanNet在动态特性不同的测试场景中表现出良好的泛化能力。

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


### [13] [Affine Frequency Division Multiplexing (AFDM) for 6G: Properties, Features, and Challenges](https://arxiv.org/abs/2507.21704)
*Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Vincent Savaux,Giuseppe Thadeu Freitas de Abreu,David González G.,Christos Masouros*

Main category: eess.SP

TL;DR: AFDM是一种新兴的6G波形候选技术，具有在高移动性和异构环境中的鲁棒性，以及适用于ISAC应用的潜力。它作为OFDM的自然扩展，保证了与现有系统的兼容性，并通过其固有的线性调频参数化提供物理层安全性。


<details>
  <summary>Details</summary>
Motivation: 研究AFDM作为6G标准化候选波形的潜力，强调其在复杂环境中的性能和兼容性优势。

Method: 概述AFDM的基本特性和独特特征，分析其优势，并讨论其在6G标准化中的潜力和挑战。

Result: AFDM在鲁棒性、兼容性和安全性方面表现出色，有望成为6G标准化的有力候选。

Conclusion: AFDM因其独特优势和兼容性，具备成为6G标准化波形的潜力，但仍需解决相关挑战。

Abstract: Affine frequency division multiplexing (AFDM) is an emerging waveform
candidate for future sixth generation (6G) systems offering a range of
promising features, such as enhanced robustness in heterogeneous and
high-mobility environments, as well as inherent suitability for integrated
sensing and communications (ISAC) applications. In addition, unlike other
candidates such as orthogonal time-frequency space (OTFS) modulation, AFDM
provides several unique advantages that strengthen its relevance to practical
deployment and standardization in 6G. Notably, as a natural generalization of
orthogonal frequency division multiplexing (OFDM), strong backward
compatibility with existing conventional systems is guaranteed, while also
offering novel possibilities in waveform design, for example to enable
physical-layer security through its inherent chirp parametrization. In all,
this article provides an overview of AFDM, emphasizing its suitability as a
candidate waveform for 6G standardization. First, we provide a concise
introduction to the fundamental properties and unique characteristics of AFDM,
followed by highlights of its advantageous features, and finally a discussion
of its potential and challenges in 6G standardization efforts and
representative requirements.

</details>


### [14] [CRB-Rate Tradeoff for Bistatic ISAC with Gaussian Information and Deterministic Sensing Signals](https://arxiv.org/abs/2507.21879)
*Xianxin Song,Xianghao Yu,Jie Xu,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 本文研究了一种双基地集成感知与通信（ISAC）系统，通过联合优化发射波束成形设计，实现了通信与感知性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 探索在双基地ISAC系统中，如何通过优化发射信号设计，同时满足通信用户的信息传输需求和感知接收器的目标方向估计需求。

Method: 推导了方向估计的Cramér-Rao界（CRB），并提出基于CRB的波束成形优化问题，分别针对仅高斯信息信号和混合信号设计了凸优化和SCA方法求解。

Result: 数值结果表明，仅使用高斯信息信号会显著降低感知性能，而提出的波束成形设计在ISAC性能边界上优于基准方案。

Conclusion: 通过联合优化发射信号设计，可以显著提升ISAC系统的综合性能，尤其是在混合信号传输场景下。

Abstract: In this paper, we investigate a bistatic integrated sensing and
communications (ISAC) system, consisting of a multi-antenna base station (BS),
a multi-antenna sensing receiver, a single-antenna communication user (CU), and
a point target to be sensed. Specifically, the BS transmits a superposition of
Gaussian information and deterministic sensing signals. The BS aims to deliver
information symbols to the CU, while the sensing receiver aims to estimate the
target's direction-of-arrival (DoA) with respect to the sensing receiver by
processing the echo signals. For the sensing receiver, we assume that only the
sequences of the deterministic sensing signals and the covariance matrix of the
information signals are perfectly known, whereas the specific realizations of
the information signals remain unavailable. Under this setup, we first derive
the corresponding Cram\'er-Rao bounds (CRBs) for DoA estimation and propose
practical estimators to accurately estimate the target's DoA. Subsequently, we
formulate the transmit beamforming design as an optimization problem aiming to
minimize the CRB, subject to a minimum signal-to-interference-plus-noise ratio
(SINR) requirement at the CU and a maximum transmit power constraint at the BS.
When the BS employs only Gaussian information signals, the resulting
beamforming optimization problem is convex, enabling the derivation of an
optimal solution. In contrast, when both Gaussian information and deterministic
sensing signals are transmitted, the resulting problem is non-convex and a
locally optimal solution is acquired by exploiting successive convex
approximation (SCA). Finally, numerical results demonstrate that employing
Gaussian information signals leads to a notable performance degradation for
target sensing and the proposed transmit beamforming design achieves a superior
ISAC performance boundary compared with various benchmark schemes.

</details>


### [15] [A Novel Framework for Near-Field Covert Communications with RIS and RSMA](https://arxiv.org/abs/2507.21956)
*Atiquzzaman Mondal,Amira Bendaimi,Huseyin Arslan*

Main category: eess.SP

TL;DR: 论文研究了近场（NF）隐蔽通信，结合速率分割多址接入（RSMA）和可重构智能表面（RIS），通过优化波束成形提升隐蔽通信速率。


<details>
  <summary>Details</summary>
Motivation: 探索近场环境下RIS和RSMA的结合，以增强合法用户的信号并抑制被动对手的检测能力。

Method: 提出交替优化（AO）算法，结合两阶段迭代方法和低复杂度技术，优化BS和RIS的波束成形。

Result: 仿真结果表明，算法显著提高了隐蔽通信速率，验证了近场RSMA-RIS集成的潜力。

Conclusion: 近场RSMA-RIS集成在隐蔽通信中具有显著优势，为未来研究提供了方向。

Abstract: This paper explores the near field (NF) covert communication with the aid of
rate-splitting multiple access (RSMA) and reconfigurable intelligent surfaces
(RIS). In particular, the RIS operates in the NF of both the legitimate user
and the passive adversary, enhancing the legitimate users received signal while
suppressing the adversarys detection capability. Whereas, the base station (BS)
applies RSMA to increase the covert communication rate composed of a private
and a shared rate component. To characterize system covertness, we derive
closed form expressions for the detection error probability (DEP), outage
probability (OP), and optimal detection threshold for the adversary. We
formulate a non-convex joint beamforming optimization problem at the BS and RIS
under unit-modulus constraints to maximize the covert rate. To tackle this, we
propose an alternating optimization (AO) algorithm, where the BS beamformer is
designed using a two-stage iterative method based on successive convex
approximation (SCA). Additionally, two low-complexity techniques are introduced
to further reduce the adversarys received power. Simulation results demonstrate
that the proposed algorithm effectively improves the covert communication rate,
highlighting the potential of near field RSMA-RIS integration in covert
communication.

</details>


### [16] [Zak-OTFS Based Coded Random Access for Uplink mMTC](https://arxiv.org/abs/2507.22013)
*Alessandro Mirri,Venkatesh Khammammetti,Beyza Dabak,Enrico Paolini,Krishna Narayanan,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种基于Zak-OTFS调度的免授权编码随机接入方案，适用于高移动性和用户密度的mMTC场景，显著降低了数据包丢失率。


<details>
  <summary>Details</summary>
Motivation: 传统OFDM-based CRA在双选择性无线信道中由于时频变化导致信道预测不可靠，需要一种更稳健的方案。

Method: 利用Zak-OTFS的可预测性，实现跨时隙的准确信道估计，支持用户数据包复本的可靠干扰消除。

Result: 与OFDM-based CRA相比，在高移动性和用户密度下显著降低数据包丢失率。

Conclusion: Zak-OTFS-based CRA在标准化Veh-A信道中表现出鲁棒性和可扩展性，适用于未来mMTC部署。

Abstract: This paper proposes a grant-free coded random access (CRA) scheme for uplink
massive machine-type communications (mMTC), based on Zak-orthogonal time
frequency space (Zak-OTFS) modulation in the delay-Doppler domain. The scheme
is tailored for doubly selective wireless channels, where conventional
orthogonal frequency-division multiplexing (OFDM)-based CRA suffers from
unreliable inter-slot channel prediction due to time-frequency variability. By
exploiting the predictable nature of Zak-OTFS, the proposed approach enables
accurate channel estimation across slots, facilitating reliable successive
interference cancellation across user packet replicas. A fair comparison with
an OFDM-based CRA baseline shows that the proposed scheme achieves
significantly lower packet loss rates under high mobility and user density.
Extensive simulations over the standardized Veh-A channel confirm the
robustness and scalability of Zak-OTFS-based CRA, supporting its applicability
to future mMTC deployments.

</details>


### [17] [Site-Specific Location Calibration and Validation of Ray-Tracing Simulator NYURay at Upper Mid-Band Frequencies](https://arxiv.org/abs/2507.22027)
*Mingjun Ying,Dipankar Shakya,Peijie Ma,Guanyue Qian,Theodore S. Rappaport*

Main category: eess.SP

TL;DR: 论文提出了一种位置校准算法，优化了NYURay射线追踪模拟器的精度，显著提高了LOS和NLOS场景下的位置准确性，并验证了其在路径损耗预测中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 射线追踪模拟器在无线数字孪生中至关重要，但精度常受限于测量数据不足和缺乏系统验证。本文旨在通过位置校准和验证提升NYURay的准确性。

Method: 提出了一种位置校准算法，通过优化TX-RX位置以对齐模拟和实测功率延迟剖面，并在6.75 GHz和16.95 GHz频段进行验证。

Result: 校准后，TX-RX位置精度在LOS和NLOS场景下分别提高了42.3%和13.5%，路径损耗预测偏差小于0.14。

Conclusion: 验证后的NYURay提升了射线追踪验证的可靠性，为6G部署提供了可信的信道统计数据。

Abstract: Ray-tracing (RT) simulators are essential for wireless digital twins,
enabling accurate site-specific radio channel prediction for next-generation
wireless systems. Yet, RT simulation accuracy is often limited by insufficient
measurement data and a lack of systematic validation. This paper presents
site-specific location calibration and validation of NYURay, NYU's in-house ray
tracer, at upper mid-band frequencies (6.75 GHz and 16.95 GHz). We propose a
location calibration algorithm that corrects GPS-induced position errors by
optimizing transmitter-receiver (TX-RX) locations to align simulated and
measured power delay profiles, improving TX-RX location accuracy by 42.3% for
line-of-sight (LOS) and 13.5% for non-line-of-sight (NLOS) scenarios.
Validation across 18 TX-RX locations shows excellent RT accuracy in path loss
prediction, with path loss exponent (PLE) deviations under 0.14. While RT
underestimates delay spread and angular spreads, their cumulative distributions
remain statistically similar. The validated NYURay advances RT validation and
provides reliable channel statistics for 6G deployment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [18] [Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers](https://arxiv.org/abs/2507.21153)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习（DRL）的电子商务数据中心能源管理系统，显著提升了能源效率、成本效益和环境可持续性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决数据中心能源管理中的动态优化问题，整合可再生能源、储能和电网电力，以应对实时波动的能源供应。

Method: 采用DRL算法动态管理能源资源，实时适应能源可用性变化。

Result: DRL系统实现了38%的能源成本降低、82%的能源效率提升和45%的碳减排，优于传统RL和启发式方法。

Conclusion: DRL为数据中心能源管理提供了高效、环保的解决方案，展示了其在能源优化和可持续发展中的潜力。

Abstract: This paper explores the implementation of a Deep Reinforcement Learning
(DRL)-optimized energy management system for e-commerce data centers, aimed at
enhancing energy efficiency, cost-effectiveness, and environmental
sustainability. The proposed system leverages DRL algorithms to dynamically
manage the integration of renewable energy sources, energy storage, and grid
power, adapting to fluctuating energy availability in real time. The study
demonstrates that the DRL-optimized system achieves a 38\% reduction in energy
costs, significantly outperforming traditional Reinforcement Learning (RL)
methods (28\%) and heuristic approaches (22\%). Additionally, it maintains a
low SLA violation rate of 1.5\%, compared to 3.0\% for RL and 4.8\% for
heuristic methods. The DRL-optimized approach also results in an 82\%
improvement in energy efficiency, surpassing other methods, and a 45\%
reduction in carbon emissions, making it the most environmentally friendly
solution. The system's cumulative reward of 950 reflects its superior
performance in balancing multiple objectives. Through rigorous testing and
ablation studies, the paper validates the effectiveness of the DRL model's
architecture and parameters, offering a robust solution for energy management
in data centers. The findings highlight the potential of DRL in advancing
energy optimization strategies and addressing sustainability challenges.

</details>


### [19] [Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students](https://arxiv.org/abs/2507.21109)
*Prital Bamnodkar*

Main category: cs.LG

TL;DR: 论文提出了一种名为TFC-SR的持续学习方法，通过结合主动回忆机制（Active Recall Probe）改进标准经验回放，显著提升了模型在Split MNIST和Split CIFAR-100任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在持续学习中面临的灾难性遗忘问题，借鉴人类学习策略（如主动回忆、刻意练习和间隔重复）来稳定过去任务的知识表示。

Method: 提出Task Focused Consolidation with Spaced Recall (TFC-SR)，引入Active Recall Probe机制，定期评估模型记忆以稳定知识表示。

Result: 在Split CIFAR-100上，TFC-SR的最终准确率为13.17%，显著优于标准回放方法的7.40%。实验表明其优势源于探针的稳定作用而非回放量。

Conclusion: TFC-SR是一种高效且鲁棒的持续学习方法，强调了在系统中集成主动记忆检索机制的重要性。

Abstract: Deep Neural Networks often suffer from a critical limitation known as
Catastrophic Forgetting, where performance on past tasks degrades after
learning new ones. This paper introduces a novel continual learning approach
inspired by human learning strategies like Active Recall, Deliberate Practice
and Spaced Repetition, named Task Focused Consolidation with Spaced Recall
(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we
termed the Active Recall Probe. It is a periodic, task-aware evaluation of the
model's memory that stabilizes the representations of past knowledge. We test
TFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading
regularization-based and replay-based baselines. Our results show that TFC-SR
performs significantly better than these methods. For instance, on the Split
CIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's
7.40%. We demonstrate that this advantage comes from the stabilizing effect of
the probe itself, and not from the difference in replay volume. Additionally,
we analyze the trade-off between memory size and performance and show that
while TFC-SR performs better in memory-constrained environments, higher replay
volume is still more effective when available memory is abundant. We conclude
that TFC-SR is a robust and efficient approach, highlighting the importance of
integrating active memory retrieval mechanisms into continual learning systems.

</details>


### [20] [Systolic Array-based Accelerator for State-Space Models](https://arxiv.org/abs/2507.21394)
*Shiva Raja,Cansu Demirkiran,Aakash Sarkar,Milos Popovic,Ajay Joshi*

Main category: cs.LG

TL;DR: 论文提出了一种名为EpochCore的硬件加速器，用于高效处理基于状态空间模型（SSMs）的长序列任务，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络（如RNN、CNN和Transformer）在处理长序列时因内存限制而表现不佳，SSMs虽能解决这一问题，但其计算和内存需求较高，需要专用硬件加速。

Method: 设计了基于脉动阵列（SAs）的EpochCore加速器，引入多功能处理单元LIMA-PE和高效数据流ProDF，以支持SSMs的高效执行。

Result: EpochCore在性能和能效上分别实现了250倍和45倍的提升，尽管面积成本增加了2倍，且在LRA数据集上的推理延迟比GPU提高了约2000倍。

Conclusion: EpochCore为SSM模型提供了一种高效的硬件解决方案，显著提升了长序列任务的处理能力。

Abstract: Sequence modeling is crucial for AI to understand temporal data and detect
complex time-dependent patterns. While recurrent neural networks (RNNs),
convolutional neural networks (CNNs), and Transformers have advanced in
capturing long-range dependencies, they struggle with achieving high accuracy
with very long sequences due to limited memory retention (fixed context
window). State-Space Models (SSMs) leverage exponentially decaying memory
enabling lengthy context window and so they process very long data sequences
more efficiently than recurrent and Transformer-based models. Unlike
traditional neural models like CNNs and RNNs, SSM-based models require solving
differential equations through continuous integration, making training and
inference both compute- and memory-intensive on conventional CPUs and GPUs. In
this paper we introduce a specialized hardware accelerator, EpochCore, for
accelerating SSMs. EpochCore is based on systolic arrays (SAs) and is designed
to enhance the energy efficiency and throughput of inference of SSM-based
models for long-range sequence tasks. Within the SA, we propose a versatile
processing element (PE) called LIMA-PE to perform traditional and specialized
MAC operations to support traditional DNNs and SSMs. To complement the
EpochCore microarchitecture, we propose a novel dataflow, ProDF, which enables
highly efficient execution of SSM-based models. By leveraging the LIMA-PE
microarchitecture and ProDF, EpochCore achieves on average 250x gains in
performance and 45x improvement in energy efficiency, at the expense of 2x
increase in area cost over traditional SA-based accelerators, and around
~2,000x improvement in latency/inference on LRA datasets compared to GPU kernel
operations.

</details>


### [21] [Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks](https://arxiv.org/abs/2507.21119)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,João Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 比较了光网络故障检测中类别不平衡缓解的预处理、处理中和后处理技术，阈值调整方法F1增益最高（15.3%），而随机欠采样（RUS）推理速度最快。


<details>
  <summary>Details</summary>
Motivation: 解决光网络故障检测中类别不平衡问题，比较不同方法的性能与效率。

Method: 采用预处理（如RUS）、处理中和后处理（如阈值调整）技术进行实验比较。

Result: 阈值调整方法F1增益最高（15.3%），RUS推理速度最快。

Conclusion: 不同方法在性能与效率上存在权衡，阈值调整效果最佳，RUS速度最快。

Abstract: We compare pre-, in-, and post-processing techniques for class imbalance
mitigation in optical network failure detection. Threshold Adjustment achieves
the highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the
fastest inference, highlighting a key performance-complexity trade-off.

</details>


### [22] [Capacity-Constrained Continual Learning](https://arxiv.org/abs/2507.21479)
*Zheng Wen,Doina Precup,Benjamin Van Roy,Satinder Singh*

Main category: cs.LG

TL;DR: 本文研究了有限容量代理在持续学习中的资源分配问题，提出了容量受限的线性-二次-高斯（LQG）序列预测问题的解决方案，并展示了如何最优分配子问题的容量。


<details>
  <summary>Details</summary>
Motivation: 现有研究对有限容量代理的资源分配问题关注较少，本文旨在填补这一空白。

Method: 通过研究容量受限的LQG序列预测问题，推导出解决方案，并探讨子问题的稳态容量分配。

Result: 在适当技术条件下，得出了问题的解决方案，并展示了子问题的最优容量分配方法。

Conclusion: 本文为容量约束下学习的系统性理论研究提供了初步成果。

Abstract: Any agents we can possibly build are subject to capacity constraints, as
memory and compute resources are inherently finite. However, comparatively
little attention has been dedicated to understanding how agents with limited
capacity should allocate their resources for optimal performance. The goal of
this paper is to shed some light on this question by studying a simple yet
relevant continual learning problem: the capacity-constrained
linear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a
solution to this problem under appropriate technical conditions. Moreover, for
problems that can be decomposed into a set of sub-problems, we also demonstrate
how to optimally allocate capacity across these sub-problems in the steady
state. We view the results of this paper as a first step in the systematic
theoretical study of learning under capacity constraints.

</details>


### [23] [Quantum Geometry of Data](https://arxiv.org/abs/2507.21135)
*Alexander G. Abanov,Luca Candelori,Harold C. Steinacker,Martin T. Wells,Jerome R. Busemeyer,Cameron J. Hogan,Vahagn Kirakosyan,Nicola Marzari,Sunil Pinnamaneni,Dario Villani,Mengjia Xu,Kharen Musaelian*

Main category: cs.LG

TL;DR: QCML通过量子几何编码数据，利用Hermitian矩阵表示特征，数据点映射到Hilbert空间，赋予数据集丰富的几何和拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 探索量子认知框架下数据表示的全局性质，避免局部方法的维度诅咒。

Method: 使用Hermitian矩阵表示数据特征，将数据点映射到Hilbert空间，提取量子几何结构（如维度、量子度量和Berry曲率）。

Result: 在合成和真实数据上展示了QCML的全局性质捕捉能力。

Conclusion: QCML的量子几何表示有助于在量子认知框架下理解认知现象。

Abstract: We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as
quantum geometry. In QCML, features of the data are represented by learned
Hermitian matrices, and data points are mapped to states in Hilbert space. The
quantum geometry description endows the dataset with rich geometric and
topological structure - including intrinsic dimension, quantum metric, and
Berry curvature - derived directly from the data. QCML captures global
properties of data, while avoiding the curse of dimensionality inherent in
local methods. We illustrate this on a number of synthetic and real-world
examples. Quantum geometric representation of QCML could advance our
understanding of cognitive phenomena within the framework of quantum cognition.

</details>


### [24] [A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning](https://arxiv.org/abs/2507.21136)
*Mojtaba Moattari*

Main category: cs.LG

TL;DR: 论文提出三种独立性准则，用于设计无监督和监督降维方法，并在线性和非线性设置中评估其性能，结果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖核函数捕捉数据非线性，但需专家确保非线性最大化数据多样性和变异性。

Method: 提出三种独立性准则，设计无监督和监督降维方法，并在不同设置下评估对比度、准确性和可解释性。

Result: 方法在性能和可解释性上优于基线方法（如tSNE、PCA等），为可解释机器学习开辟了新方向。

Conclusion: 研究为可解释机器学习提供了新方法，性能优于现有基线。

Abstract: Unsupervised and supervised learning methods conventionally use kernels to
capture nonlinearities inherent in data structure. However experts have to
ensure their proposed nonlinearity maximizes variability and capture inherent
diversity of data. We reviewed all independence criteria to design unsupervised
learners. Then we proposed 3 independence criteria and used them to design
unsupervised and supervised dimensionality reduction methods. We evaluated
contrast, accuracy and interpretability of these methods in both linear and
neural nonlinear settings. The results show that the methods have outperformed
the baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and
layer sharing) and opened a new line of interpretable machine learning (ML) for
the researchers.

</details>


### [25] [Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning](https://arxiv.org/abs/2507.21147)
*Fabrizio Lo Scudo,Alessio De Rango,Luca Furnari,Alfonso Senatore,Donato D'Ambrosio,Giuseppe Mendicino,Gianluigi Greco*

Main category: cs.LG

TL;DR: 论文探讨了如何利用对比学习框架解决野火预测中的数据不平衡和高维时空数据问题，提出了一种基于形态学的课程对比学习方法。


<details>
  <summary>Details</summary>
Motivation: 野火对生态系统和人类健康造成严重影响，气候变化加剧了这一问题。现有数据存在不平衡和高维复杂性，需要开发更有效的风险管理和预测方法。

Method: 提出了一种基于形态学的课程对比学习框架，通过增强潜在表示来处理动态特征，减少计算成本并支持更频繁的天气数据更新。

Result: 实验验证了所提方法的有效性，能够在保持性能的同时使用更小的补丁尺寸，并适应不同区域特征。

Conclusion: 对比学习框架为解决野火预测中的数据不平衡和高维问题提供了有效途径，为未来的风险管理和预测技术提供了新思路。

Abstract: Wildfires significantly impact natural ecosystems and human health, leading
to biodiversity loss, increased hydrogeological risks, and elevated emissions
of toxic substances. Climate change exacerbates these effects, particularly in
regions with rising temperatures and prolonged dry periods, such as the
Mediterranean. This requires the development of advanced risk management
strategies that utilize state-of-the-art technologies. However, in this
context, the data show a bias toward an imbalanced setting, where the incidence
of wildfire events is significantly lower than typical situations. This
imbalance, coupled with the inherent complexity of high-dimensional
spatio-temporal data, poses significant challenges for training deep learning
architectures. Moreover, since precise wildfire predictions depend mainly on
weather data, finding a way to reduce computational costs to enable more
frequent updates using the latest weather forecasts would be beneficial. This
paper investigates how adopting a contrastive framework can address these
challenges through enhanced latent representations for the patch's dynamic
features. We thus introduce a new morphology-based curriculum contrastive
learning that mitigates issues associated with diverse regional characteristics
and enables the use of smaller patch sizes without compromising performance. An
experimental analysis is performed to validate the effectiveness of the
proposed modeling strategies.

</details>


### [26] [Deep Unfolding for MIMO Signal Detection](https://arxiv.org/abs/2507.21152)
*Hangli Ge,Noboru Koshizuka*

Main category: cs.LG

TL;DR: 提出了一种基于深度展开神经网络的MIMO检测器，采用复数域计算，性能优越且复杂度低。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖实数域近似，而信号处理本质上是复数域问题，需更高效、可解释的解决方案。

Method: 提出动态部分收缩阈值（DPST）方法，直接在复数域操作，参数少且训练简单。

Result: 数值结果显示，该方法检测性能优越，迭代次数少，计算复杂度低。

Conclusion: 该方法适用于下一代大规模MIMO系统，具有实际应用价值。

Abstract: In this paper, we propose a deep unfolding neural network-based MIMO detector
that incorporates complex-valued computations using Wirtinger calculus. The
method, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables
efficient, interpretable, and low-complexity MIMO signal detection. Unlike
prior approaches that rely on real-valued approximations, our method operates
natively in the complex domain, aligning with the fundamental nature of signal
processing tasks. The proposed algorithm requires only a small number of
trainable parameters, allowing for simplified training. Numerical results
demonstrate that the proposed method achieves superior detection performance
with fewer iterations and lower computational complexity, making it a practical
solution for next-generation massive MIMO systems.

</details>


### [27] [SPADE-S: A Sparsity-Robust Foundational Forecaster](https://arxiv.org/abs/2507.21155)
*Malcolm Wolff,Matthew Li,Ravi Kiran Selvam,Hanjing Zhu,Kin G. Olivares,Ruijun Ma,Abhinav Katoch,Shankar Ramasubramanian,Mengfei Cao,Roberto Bandarra,Rahul Gopalsamy,Stefania La Vattiata,Sitan Yang,Michael M. Mahoney*

Main category: cs.LG

TL;DR: SPADE-S是一种针对时间序列预测的鲁棒架构，显著减少了基于幅度和稀疏性的系统偏差，并在需求预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习架构在幅度和稀疏性差异较大的时间序列预测中表现不佳，存在系统性偏差。

Method: SPADE-S通过改进损失函数、训练采样方法和时间序列编码方法，减少对高幅度序列的偏向。

Result: SPADE-S在多个数据集上显著提升预测准确率，最高可达15%，具体表现为P90和P50准确率的提升。

Conclusion: SPADE-S有效解决了时间序列预测中的异质性和稀疏性问题，具有广泛的应用潜力。

Abstract: Despite significant advancements in time series forecasting, accurate
modeling of time series with strong heterogeneity in magnitude and/or sparsity
patterns remains challenging for state-of-the-art deep learning architectures.
We identify several factors that lead existing models to systematically
underperform on low-magnitude and sparse time series, including loss functions
with implicit biases toward high-magnitude series, training-time sampling
methods, and limitations of time series encoding methods.
  SPADE-S is a robust forecasting architecture that significantly reduces
magnitude- and sparsity-based systematic biases and improves overall prediction
accuracy. Empirical results demonstrate that SPADE-S outperforms existing
state-of-the-art approaches across a diverse set of use cases in demand
forecasting. In particular, we show that, depending on the quantile forecast
and magnitude of the series, SPADE-S can improve forecast accuracy by up to
15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and
4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%,
respectively, for each of three distinct datasets, ranging from 3 million to
700 million series, from a large online retailer.

</details>


### [28] [Handling Out-of-Distribution Data: A Survey](https://arxiv.org/abs/2507.21160)
*Lakpa Tamang,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.LG

TL;DR: 本文总结了机器学习中数据分布变化的挑战，提出了处理协变量偏移和概念偏移的方法，并回顾了相关技术，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决训练与部署阶段数据分布变化的问题，提升模型在分布偏移下的性能。

Method: 形式化分布偏移问题，回顾现有方法，并提出未来研究方向。

Result: 总结了分布偏移的检测、测量和缓解技术，强调了OOD数据的重要性。

Conclusion: 分布偏移是机器学习中的重要问题，未来需开发更全面的处理方法。

Abstract: In the field of Machine Learning (ML) and data-driven applications, one of
the significant challenge is the change in data distribution between the
training and deployment stages, commonly known as distribution shift. This
paper outlines different mechanisms for handling two main types of distribution
shifts: (i) Covariate shift: where the value of features or covariates change
between train and test data, and (ii) Concept/Semantic-shift: where model
experiences shift in the concept learned during training due to emergence of
novel classes in the test phase. We sum up our contributions in three folds.
First, we formalize distribution shifts, recite on how the conventional method
fails to handle them adequately and urge for a model that can simultaneously
perform better in all types of distribution shifts. Second, we discuss why
handling distribution shifts is important and provide an extensive review of
the methods and techniques that have been developed to detect, measure, and
mitigate the effects of these shifts. Third, we discuss the current state of
distribution shift handling mechanisms and propose future research directions
in this area. Overall, we provide a retrospective synopsis of the literature in
the distribution shift, focusing on OOD data that had been overlooked in the
existing surveys.

</details>


### [29] [OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection](https://arxiv.org/abs/2507.21164)
*Nicolas Pinon,Carole Lartizien*

Main category: cs.LG

TL;DR: 提出了一种新方法，通过紧密耦合表示学习和一类SVM，直接优化潜在特征与决策边界对齐，解决了无监督异常检测中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法在特征空间或重建效果上存在不足，需要更鲁棒和表达性强的解决方案。

Method: 提出了一种新方法，通过自定义损失函数将表示学习与解析可解的一类SVM耦合。

Result: 在MNIST-C和脑MRI任务中表现优异，尤其在检测小、非高信号病变方面具有优势。

Conclusion: 该方法在无监督异常检测和医学影像应用中表现出色，具有实际应用潜力。

Abstract: Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled
data, a necessity in many machine learning applications where anomalous samples
are rare or not available. Most state-of-the-art methods fall into two
categories: reconstruction-based approaches, which often reconstruct anomalies
too well, and decoupled representation learning with density estimators, which
can suffer from suboptimal feature spaces. While some recent methods attempt to
couple feature learning and anomaly detection, they often rely on surrogate
objectives, restrict kernel choices, or introduce approximations that limit
their expressiveness and robustness. To address this challenge, we propose a
novel method that tightly couples representation learning with an analytically
solvable one-class SVM (OCSVM), through a custom loss formulation that directly
aligns latent features with the OCSVM decision boundary. The model is evaluated
on two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI
subtle lesion detection task. Unlike most methods that focus on large,
hyperintense lesions at the image level, our approach succeeds to target small,
non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a
more clinically relevant scenario. Both experiments evaluate a form of
robustness to domain shifts, including corruption types in MNIST-C and
scanner/age variations in MRI. Results demonstrate performance and robustness
of our proposed mode,highlighting its potential for general UAD and real-world
medical imaging applications. The source code is available at
https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning

</details>


### [30] [AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation](https://arxiv.org/abs/2507.21166)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: AGORA框架通过结构化交互提升复杂推理能力，超越传统模型参数增加方法，性能提升4.45%。


<details>
  <summary>Details</summary>
Motivation: 当前训练数据集的静态特性限制了复杂推理的进展，需要新的扩展方向。

Method: 提出AGORA自演化框架，通过协作集成实现推理能力提升。

Result: 在数学基准测试中，性能超过现有最佳系统4.45个百分点。

Conclusion: 协作生态系统工程是实现能力涌现的重要前沿。

Abstract: Progress in complex reasoning is constrained by the static nature of the
current training datasets. We propose structured interaction as a new scaling
axis, moving beyond the prevailing paradigm of increasing model parameters. Our
self-evolving framework, AGORA, enables a collaborative ensemble to achieve
reasoning performance exceeding state-of-the-art monolithic systems by up to
4.45 percentage points on challenging mathematical benchmarks. This gain stems
from group emergent ability-the synthesis of collective capabilities
unattainable by isolated models, validating interaction as a scalable driver of
intelligence. Our results position the engineering of collaborative ecosystems
as a vital frontier for capability emergence.

</details>


### [31] [LLM-Adapted Interpretation Framework for Machine Learning Models](https://arxiv.org/abs/2507.21179)
*Yuqi Jin,Zihan Hu,Weiteng Zhang,Weihao Xie,Jianwei Shuai,Xian Shen,Zhen Feng*

Main category: cs.LG

TL;DR: LAI-ML框架通过知识蒸馏和LLM生成可解释的诊断叙述，显著提升模型准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 解决高性能机器学习模型（如XGBoost）在临床应用中因缺乏可解释性而受限的问题。

Method: 提出LAI-ML框架，结合HAGA和CACS技术将XGBoost特征归因转化为概率格式，并利用LLM生成诊断叙述。

Result: LAI-ML预测准确率达83%，比基线XGBoost高13%，且LLM在21.7%不一致案例中修正预测。

Conclusion: LAI-ML成功将黑盒预测转化为可信且可解释的临床见解，为医疗AI提供可行解决方案。

Abstract: Background & Aims: High-performance machine learning models like XGBoost are
often "black boxes," limiting their clinical adoption due to a lack of
interpretability. This study aims to bridge the gap between predictive accuracy
and narrative transparency for sarcopenia risk assessment. Methods: We propose
the LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge
distillation architecture. LAI-ML transforms feature attributions from a
trained XGBoost model into a probabilistic format using specialized techniques
(HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement
learning loop and case-based retrieval, then generates data-faithful diagnostic
narratives. Results: The LAI-ML framework achieved 83% prediction accuracy,
significantly outperforming the baseline XGBoost model, 13% higher. Notably,
the LLM not only replicated the teacher model's logic but also corrected its
predictions in 21.7% of discordant cases, demonstrating enhanced reasoning.
Conclusion: LAI-ML effectively translates opaque model predictions into
trustworthy and interpretable clinical insights, offering a deployable solution
to the "black-box" problem in medical AI.

</details>


### [32] [AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction](https://arxiv.org/abs/2507.21197)
*Ling Liao,Eva Aagaard*

Main category: cs.LG

TL;DR: AdaptHetero是一个基于机器学习的框架，通过SHAP解释和无监督聚类，优化EHR数据中子群体的建模和预测性能。


<details>
  <summary>Details</summary>
Motivation: EHR数据的复杂性和异质性限制了机器学习解释在子群体建模中的有效性，AdaptHetero旨在解决这一问题。

Method: 结合SHAP解释和无监督聚类，将解释性洞察转化为子群体建模的具体指导。

Result: 在三个大规模EHR数据集上验证，AdaptHetero能识别异质性模型行为并提升预测性能。

Conclusion: AdaptHetero为EHR数据中的子群体建模提供了有效的解决方案，提升了预测准确性和临床意义。

Abstract: Machine learning interpretation has primarily been leveraged to build
clinician trust and uncover actionable insights in EHRs. However, the intrinsic
complexity and heterogeneity of EHR data limit its effectiveness in guiding
subgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven
framework that transforms interpretability insights into actionable guidance
for tailoring model training and evaluation across subpopulations within
individual hospital systems. Evaluated on three large-scale EHR datasets -
GOSSIS-1-eICU, WiDS, and MIMIC-IV - AdaptHetero consistently identifies
heterogeneous model behaviors in predicting ICU mortality, in-hospital death,
and hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised
clustering, the framework enhances the identification of clinically meaningful
subgroup-specific characteristics, leading to improved predictive performance.

</details>


### [33] [MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge](https://arxiv.org/abs/2507.21183)
*Guangchen Lan,Sipeng Zhang,Tianle Wang,Yuwei Zhang,Daoan Zhang,Xinpeng Wei,Xiaoman Pan,Hongming Zhang,Dong-Jun Han,Christopher G. Brinton*

Main category: cs.LG

TL;DR: MaPPO是一种新的偏好优化框架，通过引入先验奖励知识改进现有方法（如DPO），无需额外超参数，适用于离线和在线设置，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将偏好学习视为最大似然估计问题，忽略了先验奖励知识，可能导致对响应的二元分类过于简化。

Method: MaPPO将先验奖励知识整合到最大后验优化目标中，扩展了DPO及其变体，支持离线和在线设置，并可作为插件使用。

Result: 在MT-Bench、AlpacaEval 2.0和Arena-Hard等基准测试中，MaPPO显著提升了对齐性能，且计算效率未受影响。

Conclusion: MaPPO是一种高效且通用的偏好优化方法，优于现有技术，适用于多种场景。

Abstract: As the era of large language models (LLMs) on behalf of users unfolds,
Preference Optimization (PO) methods have become a central approach to aligning
LLMs with human preferences and improving performance. We propose Maximum a
Posteriori Preference Optimization (MaPPO), a framework for learning from
preferences that explicitly incorporates prior reward knowledge into the
optimization objective. While existing methods such as Direct Preference
Optimization (DPO) and its variants treat preference learning as a Maximum
Likelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating
prior reward estimates into a principled Maximum a Posteriori (MaP) objective.
This not only generalizes DPO and its variants, but also enhances alignment by
mitigating the oversimplified binary classification of responses. More
importantly, MaPPO introduces no additional hyperparameter, and supports
preference optimization in both offline and online settings. In addition, MaPPO
can be used as a plugin with consistent improvement on DPO variants, including
widely used SimPO, IPO, and CPO. Extensive empirical evaluations of different
model sizes and model series on three standard benchmarks, including MT-Bench,
AlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in
alignment performance without sacrificing computational efficiency.

</details>


### [34] [EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models](https://arxiv.org/abs/2507.21184)
*Haowei Lin,Xiangyu Wang,Jianzhu Ma,Yitao Liang*

Main category: cs.LG

TL;DR: EvoSLD是一个自动化框架，利用进化算法和大型语言模型（LLMs）共同演化符号表达式及其优化程序，用于发现神经网络的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要大量人工经验和手动实验来发现缩放规律，EvoSLD旨在通过自动化提高效率和准确性。

Method: EvoSLD结合进化算法和LLMs，处理缩放变量、控制变量和响应指标，寻找简洁且通用的函数形式。

Result: 在五个实际场景中，EvoSLD重新发现了两个已知规律，并在其他情况下超越人类结果，测试集误差显著降低。

Conclusion: EvoSLD在准确性、可解释性和效率上优于基线方法，有望加速AI研究。

Abstract: Scaling laws are fundamental mathematical relationships that predict how
neural network performance evolves with changes in variables such as model
size, dataset size, and computational resources. Traditionally, discovering
these laws requires extensive human expertise and manual experimentation. We
introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that
leverages evolutionary algorithms guided by Large Language Models (LLMs) to
co-evolve symbolic expressions and their optimization routines. Formulated to
handle scaling variables, control variables, and response metrics across
diverse experimental settings, EvoSLD searches for parsimonious, universal
functional forms that minimize fitting errors on grouped data subsets.
Evaluated on five real-world scenarios from recent literature, EvoSLD
rediscovers exact human-derived laws in two cases and surpasses them in others,
achieving up to orders-of-magnitude reductions in normalized mean squared error
on held-out test sets. Compared to baselines like symbolic regression and
ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and
efficiency, highlighting its potential to accelerate AI research. Code is
available at https://github.com/linhaowei1/SLD.

</details>


### [35] [Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs](https://arxiv.org/abs/2507.21188)
*Raj Krishnan Vijayaraj*

Main category: cs.LG

TL;DR: 论文提出了一种几何感知的评估框架LAPD，用于检测临床LLMs在对抗性输入变化下的潜在鲁棒性，并引入LDFR指标量化诊断不稳定性。


<details>
  <summary>Details</summary>
Motivation: 临床LLMs在静态基准测试中表现良好，但在输入微小变化（如症状掩盖或否定）时容易失败，而标准NLP指标无法检测这种潜在表示变化。

Method: 通过结构化对抗编辑（如掩盖、否定、同义词替换和数值变化）生成临床笔记，利用PCA降维的潜在空间计算LDFR，评估模型鲁棒性。

Result: 实验发现，即使表面变化极小，临床LLMs仍表现出潜在脆弱性，且在真实临床数据（DiReCT基准）中验证了LDFR的普适性。

Conclusion: 研究表明表面鲁棒性与语义稳定性存在差距，强调在安全关键临床AI中几何感知审计的重要性。

Abstract: LLMs for clinical decision support often fail under small but clinically
meaningful input shifts such as masking a symptom or negating a finding,
despite high performance on static benchmarks. These reasoning failures
frequently go undetected by standard NLP metrics, which are insensitive to
latent representation shifts that drive diagnosis instability. We propose a
geometry-aware evaluation framework, LAPD (Latent Agentic Perturbation
Diagnostics), which systematically probes the latent robustness of clinical
LLMs under structured adversarial edits. Within this framework, we introduce
Latent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that
captures representational instability when embeddings cross decision boundaries
in PCA-reduced latent space. Clinical notes are generated using a structured
prompting pipeline grounded in diagnostic reasoning, then perturbed along four
axes: masking, negation, synonym replacement, and numeric variation to simulate
common ambiguities and omissions. We compute LDFR across both foundation and
clinical LLMs, finding that latent fragility emerges even under minimal
surface-level changes. Finally, we validate our findings on 90 real clinical
notes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of
LDFR beyond synthetic settings. Our results reveal a persistent gap between
surface robustness and semantic stability, underscoring the importance of
geometry-aware auditing in safety-critical clinical AI.

</details>


### [36] [Operator-Based Machine Intelligence: A Hilbert Space Framework for Spectral Learning and Symbolic Reasoning](https://arxiv.org/abs/2507.21189)
*Andrew Kiruluta,Andreas Lemos,Priscilla Burity*

Main category: cs.LG

TL;DR: 该论文探讨了在无限维希尔伯特空间中表达学习任务的方法，利用泛函分析、信号处理和谱理论工具，对比传统神经网络的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型基于有限维参数空间和非线性函数逼近，本文旨在探索无限维希尔伯特空间中的学习任务表达，以提供更灵活和理论基础更强的框架。

Method: 通过回顾再生核希尔伯特空间（RKHS）、谱算子学习和小波域表示等基础概念，提出希尔伯特空间中的学习数学框架，并介绍基于散射变换和Koopman算子的新模型。

Result: 论文展示了希尔伯特空间学习框架的潜力，并讨论了其相对于传统神经架构的优势与局限性。

Conclusion: 未来研究方向包括基于希尔伯特信号处理的可扩展和可解释机器学习。

Abstract: Traditional machine learning models, particularly neural networks, are rooted
in finite-dimensional parameter spaces and nonlinear function approximations.
This report explores an alternative formulation where learning tasks are
expressed as sampling and computation in infinite dimensional Hilbert spaces,
leveraging tools from functional analysis, signal processing, and spectral
theory. We review foundational concepts such as Reproducing Kernel Hilbert
Spaces (RKHS), spectral operator learning, and wavelet-domain representations.
We present a rigorous mathematical formulation of learning in Hilbert spaces,
highlight recent models based on scattering transforms and Koopman operators,
and discuss advantages and limitations relative to conventional neural
architectures. The report concludes by outlining directions for scalable and
interpretable machine learning grounded in Hilbertian signal processing.

</details>


### [37] [Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals](https://arxiv.org/abs/2507.21190)
*Andrew Kiruluta,Andreas Lemos,Priscilla Burity*

Main category: cs.LG

TL;DR: 提出了一种基于图拉普拉斯小波变换（GLWT）的非神经网络框架，通过结构化多尺度滤波、非线性收缩和小波系数的符号逻辑，在图形谱域中操作，适用于去噪和标记分类等任务。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络架构依赖卷积、循环或注意力机制，缺乏透明性和效率。本文旨在提供一种可解释、资源高效的图形学习方法。

Method: 使用GLWT分解图形节点信号，通过非线性调制和符号逻辑重组信号，支持基于图小波激活的领域特定语言（DSL）进行组合推理。

Result: 在合成图形去噪和语言标记图形任务中，性能与轻量级GNN相当，但透明性和效率更高。

Conclusion: 该框架为图形学习提供了一种原则性、可解释且资源高效的替代方案，优于深度神经网络架构。

Abstract: We present a fully non neural learning framework based on Graph Laplacian
Wavelet Transforms (GLWT). Unlike traditional architectures that rely on
convolutional, recurrent, or attention based neural networks, our model
operates purely in the graph spectral domain using structured multiscale
filtering, nonlinear shrinkage, and symbolic logic over wavelet coefficients.
Signals defined on graph nodes are decomposed via GLWT, modulated with
interpretable nonlinearities, and recombined for downstream tasks such as
denoising and token classification. The system supports compositional reasoning
through a symbolic domain-specific language (DSL) over graph wavelet
activations. Experiments on synthetic graph denoising and linguistic token
graphs demonstrate competitive performance against lightweight GNNs with far
greater transparency and efficiency. This work proposes a principled,
interpretable, and resource-efficient alternative to deep neural architectures
for learning on graphs.

</details>


### [38] [Exploring Adaptive Structure Learning for Heterophilic Graphs](https://arxiv.org/abs/2507.21191)
*Garv Kaushik*

Main category: cs.LG

TL;DR: 论文提出通过结构学习重连浅层GCN的边，以解决异质图中长距离依赖问题，但方法在节点分类任务中表现不一致。


<details>
  <summary>Details</summary>
Motivation: 解决异质图中因局部特征聚合和消息传递范式导致的长距离依赖捕获不足问题。

Method: 通过参数化邻接矩阵学习非局部节点间的连接，扩展浅层GCN的跳数范围。

Result: 方法在部分异质图中表现良好，但在节点分类任务中因图结构不同而表现不一致。

Conclusion: 结构学习能改善长距离依赖问题，但方法的泛化性有限，需进一步优化。

Abstract: Graph Convolutional Networks (GCNs) gained traction for graph representation
learning, with recent attention on improving performance on heterophilic graphs
for various real-world applications. The localized feature aggregation in a
typical message-passing paradigm hinders the capturing of long-range
dependencies between non-local nodes of the same class. The inherent
connectivity structure in heterophilic graphs often conflicts with information
sharing between distant nodes of same class. We propose structure learning to
rewire edges in shallow GCNs itself to avoid performance degradation in
downstream discriminative tasks due to oversmoothing. Parameterizing the
adjacency matrix to learn connections between non-local nodes and extend the
hop span of shallow GCNs facilitates the capturing of long-range dependencies.
However, our method is not generalizable across heterophilic graphs and
performs inconsistently on node classification task contingent to the graph
structure.

</details>


### [39] [EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks](https://arxiv.org/abs/2507.21196)
*Abir Ray*

Main category: cs.LG

TL;DR: EdgeAgentX-DT通过结合数字孪生和生成式AI，提升军事网络边缘智能，显著优化训练和性能。


<details>
  <summary>Details</summary>
Motivation: 增强军事网络中的边缘智能，以应对复杂和对抗性环境。

Method: 利用数字孪生同步真实设备，结合扩散模型和Transformer生成多样化对抗场景，采用多层架构（边缘智能、数字孪生同步、生成式训练）。

Result: 实验显示更快的学习收敛、更高吞吐量、更低延迟及更强的抗干扰能力。

Conclusion: 数字孪生和生成式训练能有效提升边缘AI在对抗环境中的部署能力。

Abstract: We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework
that integrates digital twin simulations and generative AI-driven scenario
training to significantly enhance edge intelligence in military networks.
EdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized
with real-world edge devices, to provide a secure, realistic environment for
training and validation. Leveraging generative AI methods, such as diffusion
models and transformers, the system creates diverse and adversarial scenarios
for robust simulation-based agent training. Our multi-layer architecture
includes: (1) on-device edge intelligence; (2) digital twin synchronization;
and (3) generative scenario training. Experimental simulations demonstrate
notable improvements over EdgeAgentX, including faster learning convergence,
higher network throughput, reduced latency, and improved resilience against
jamming and node failures. A case study involving a complex tactical scenario
with simultaneous jamming attacks, agent failures, and increased network loads
illustrates how EdgeAgentX-DT sustains operational performance, whereas
baseline methods fail. These results highlight the potential of
digital-twin-enabled generative training to strengthen edge AI deployments in
contested environments.

</details>


### [40] [Uncovering Gradient Inversion Risks in Practical Language Model Training](https://arxiv.org/abs/2507.21198)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Eu Joe Chegne,Mengyao Ma,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.LG

TL;DR: Grab是一种针对语言模型的梯度反转攻击方法，通过混合优化显著提高了隐私数据恢复率。


<details>
  <summary>Details</summary>
Motivation: 揭示联邦学习中语言模型的隐私威胁，填补梯度反转攻击在离散数据领域的不足。

Method: 采用交替优化过程，包括层间丢弃掩码的同步优化和离散优化的令牌序列恢复。

Result: Grab在基准和实际设置中分别实现了高达92.9%和48.5%的恢复率提升。

Conclusion: Grab为理解语言模型联邦学习的隐私威胁提供了重要进展。

Abstract: The gradient inversion attack has been demonstrated as a significant privacy
threat to federated learning (FL), particularly in continuous domains such as
vision models. In contrast, it is often considered less effective or highly
dependent on impractical training settings when applied to language models, due
to the challenges posed by the discrete nature of tokens in text data. As a
result, its potential privacy threats remain largely underestimated, despite FL
being an emerging training method for language models. In this work, we propose
a domain-specific gradient inversion attack named Grab (gradient inversion with
hybrid optimization). Grab features two alternating optimization processes to
address the challenges caused by practical training settings, including a
simultaneous optimization on dropout masks between layers for improved token
recovery and a discrete optimization for effective token sequencing. Grab can
recover a significant portion (up to 92.9% recovery rate) of the private
training data, outperforming the attack strategy of utilizing discrete
optimization with an auxiliary model by notable improvements of up to 28.9%
recovery rate in benchmark settings and 48.5% recovery rate in practical
settings. Grab provides a valuable step forward in understanding this privacy
threat in the emerging FL training mode of language models.

</details>


### [41] [Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications](https://arxiv.org/abs/2507.21199)
*Xinye Cao,Hongcan Guo,Guoshun Nan,Jiaoyang Cui,Haoting Qian,Yihan Lin,Yilin Peng,Diyang Zhang,Yanzhao Hou,Huici Wu,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 提出了一种基于单一组合式LLM的交互式多模态应用（IMAs）新范式，解决了多任务适应性和资源效率问题。通过ContextLoRA和ContextGear方法，实现了任务依赖关系学习和训练优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多个LLM处理不同IMAs任务，资源消耗大且效率低。本文旨在通过单一LLM实现多样化IMAs任务，提升灵活性和效率。

Method: 提出ContextLoRA方法，通过任务依赖图学习结构化上下文，并分区参数矩阵；开发分步微调流程。ContextGear策略优化训练，降低计算和通信成本。

Result: 在三个基准测试中表现优越，并在真实无线测试平台上验证了实用性。

Conclusion: 单一组合式LLM范式在IMAs中具有高效性和实用性，ContextLoRA和ContextGear方法显著提升了任务适应性和资源效率。

Abstract: Interactive multimodal applications (IMAs), such as route planning in the
Internet of Vehicles, enrich users' personalized experiences by integrating
various forms of data over wireless networks. Recent advances in large language
models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple
IMAs, with each LLM trained individually for a specific task that presents
different business workflows. In contrast to existing approaches that rely on
multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes
various IMAs using a single compositional LLM over wireless networks. The two
primary challenges include 1) guiding a single LLM to adapt to diverse IMA
objectives and 2) ensuring the flexibility and efficiency of the LLM in
resource-constrained mobile environments. To tackle the first challenge, we
propose ContextLoRA, a novel method that guides an LLM to learn the rich
structured context among IMAs by constructing a task dependency graph. We
partition the learnable parameter matrix of neural layers for each IMA to
facilitate LLM composition. Then, we develop a step-by-step fine-tuning
procedure guided by task relations, including training, freezing, and masking
phases. This allows the LLM to learn to reason among tasks for better
adaptation, capturing the latent dependencies between tasks. For the second
challenge, we introduce ContextGear, a scheduling strategy to optimize the
training procedure of ContextLoRA, aiming to minimize computational and
communication costs through a strategic grouping mechanism. Experiments on
three benchmarks show the superiority of the proposed ContextLoRA and
ContextGear. Furthermore, we prototype our proposed paradigm on a real-world
wireless testbed, demonstrating its practical applicability for various IMAs.
We will release our code to the community.

</details>


### [42] [Learning from Limited and Imperfect Data](https://arxiv.org/abs/2507.21205)
*Harsh Rangwani*

Main category: cs.LG

TL;DR: 论文提出针对现实世界中数据分布不均衡和标注不足的问题，开发了四种深度学习算法，分别解决生成模型、归纳正则化、半监督学习和高效域适应的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据分布不均衡且标注不足，现有算法在非理想数据集上表现不佳，需开发更鲁棒的算法以扩展深度学习应用。

Method: 分四部分：1) 长尾数据生成模型；2) 归纳正则化提升尾部类泛化；3) 半监督学习优化指标；4) 高效域适应。

Result: 实现了在长尾数据下生成多样图像、提升尾部类泛化能力、优化半监督学习指标，以及高效适应新领域。

Conclusion: 通过四种算法解决了现实世界数据学习的关键问题，为深度学习在非理想数据上的应用提供了实用方案。

Abstract: The distribution of data in the world (eg, internet, etc.) significantly
differs from the well-curated datasets and is often over-populated with samples
from common categories. The algorithms designed for well-curated datasets
perform suboptimally when used for learning from imperfect datasets with
long-tailed imbalances and distribution shifts. To expand the use of deep
models, it is essential to overcome the labor-intensive curation process by
developing robust algorithms that can learn from diverse, real-world data
distributions. Toward this goal, we develop practical algorithms for Deep
Neural Networks which can learn from limited and imperfect data present in the
real world. This thesis is divided into four segments, each covering a scenario
of learning from limited or imperfect data. The first part of the thesis
focuses on Learning Generative Models from Long-Tail Data, where we mitigate
the mode-collapse and enable diverse aesthetic image generations for tail
(minority) classes. In the second part, we enable effective generalization on
tail classes through Inductive Regularization schemes, which allow tail classes
to generalize as effectively as the head classes without requiring explicit
generation of images. In the third part, we develop algorithms for Optimizing
Relevant Metrics for learning from long-tailed data with limited annotation
(semi-supervised), followed by the fourth part, which focuses on the Efficient
Domain Adaptation of the model to various domains with very few to zero labeled
samples.

</details>


### [43] [Bubbleformer: Forecasting Boiling with Transformers](https://arxiv.org/abs/2507.21244)
*Sheikh Md Shakeel Hassan,Xianwei Zou,Akash Dhruv,Vishwanath Ganesan,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Bubbleformer是一种基于Transformer的时空模型，能够自主预测沸腾动力学，包括成核、界面演化和热传递，无需依赖模拟数据。


<details>
  <summary>Details</summary>
Motivation: 现有模型在推理时需要未来输入，无法从过去状态学习成核，且无法建模流动沸腾速度场，限制了其自主预测能力。

Method: Bubbleformer采用因子化轴向注意力、频率感知缩放和热物理参数条件化，以泛化不同流体、几何和操作条件。

Result: Bubbleformer在预测和预报两相沸腾流动方面取得了新的基准结果。

Conclusion: Bubbleformer通过物理指标评估和高质量数据集BubbleML 2.0的发布，显著提升了沸腾动力学建模的自主性和准确性。

Abstract: Modeling boiling (an inherently chaotic, multiphase process central to energy
and thermal systems) remains a significant challenge for neural PDE surrogates.
Existing models require future input (e.g., bubble positions) during inference
because they fail to learn nucleation from past states, limiting their ability
to autonomously forecast boiling dynamics. They also fail to model flow boiling
velocity fields, where sharp interface-momentum coupling demands long-range and
directional inductive biases. We introduce Bubbleformer, a transformer-based
spatiotemporal model that forecasts stable and long-range boiling dynamics
including nucleation, interface evolution, and heat transfer without dependence
on simulation data during inference. Bubbleformer integrates factorized axial
attention, frequency-aware scaling, and conditions on thermophysical parameters
to generalize across fluids, geometries, and operating conditions. To evaluate
physical fidelity in chaotic systems, we propose interpretable physics-based
metrics that evaluate heat-flux consistency, interface geometry, and mass
conservation. We also release BubbleML 2.0, a high-fidelity dataset that spans
diverse working fluids (cryogens, refrigerants, dielectrics), boiling
configurations (pool and flow boiling), flow regimes (bubbly, slug, annular),
and boundary conditions. Bubbleformer sets new benchmark results in both
prediction and forecasting of two-phase boiling flows.

</details>


### [44] [Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors](https://arxiv.org/abs/2507.21260)
*Amartya Banerjee,Xingyu Xu,Caroline Moosmüller,Harlin Lee*

Main category: cs.LG

TL;DR: Adam-PnP是一个基于扩散模型的Plug-and-Play框架，用于从多源实验数据中恢复蛋白质结构，通过自适应噪声估计和动态模态加权减少手动调参需求。


<details>
  <summary>Details</summary>
Motivation: 解决多源噪声实验数据与扩散模型结合的挑战，减少对噪声水平和手动调参的依赖。

Method: 提出Adam-PnP框架，结合自适应噪声估计和动态模态加权机制，指导预训练蛋白质扩散模型。

Result: 在复杂重建任务中显著提高了准确性。

Conclusion: Adam-PnP为多源数据驱动的蛋白质结构恢复提供了一种高效且自动化的解决方案。

Abstract: In an inverse problem, the goal is to recover an unknown parameter (e.g., an
image) that has typically undergone some lossy or noisy transformation during
measurement. Recently, deep generative models, particularly diffusion models,
have emerged as powerful priors for protein structure generation. However,
integrating noisy experimental data from multiple sources to guide these models
remains a significant challenge. Existing methods often require precise
knowledge of experimental noise levels and manually tuned weights for each data
modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that
guides a pre-trained protein diffusion model using gradients from multiple,
heterogeneous experimental sources. Our framework features an adaptive noise
estimation scheme and a dynamic modality weighting mechanism integrated into
the diffusion process, which reduce the need for manual hyperparameter tuning.
Experiments on complex reconstruction tasks demonstrate significantly improved
accuracy using Adam-PnP.

</details>


### [45] [Deep Polynomial Chaos Expansion](https://arxiv.org/abs/2507.21273)
*Johannes Exenberger,Sascha Ranftl,Robert Peharz*

Main category: cs.LG

TL;DR: DeepPCE结合多项式混沌展开和概率电路思想，解决了高维问题中PCE的扩展性问题，性能接近MLP，同时保留了PCE的统计推断能力。


<details>
  <summary>Details</summary>
Motivation: 多项式混沌展开（PCE）在高维问题中因基函数数量指数增长而难以扩展，需要一种既能保持PCE统计推断能力又能适应高维输入的方法。

Method: 提出DeepPCE，将PCE与概率电路思想结合，形成一种深度推广的PCE模型。

Result: DeepPCE在高维输入空间中表现良好，预测性能接近多层感知机（MLP），同时能通过简单前向传播进行精确统计推断。

Conclusion: DeepPCE成功解决了PCE在高维问题中的扩展性限制，兼具高性能和统计推断能力。

Abstract: Polynomial chaos expansion (PCE) is a classical and widely used surrogate
modeling technique in physical simulation and uncertainty quantification. By
taking a linear combination of a set of basis polynomials - orthonormal with
respect to the distribution of uncertain input parameters - PCE enables
tractable inference of key statistical quantities, such as (conditional) means,
variances, covariances, and Sobol sensitivity indices, which are essential for
understanding the modeled system and identifying influential parameters and
their interactions. As the number of basis functions grows exponentially with
the number of parameters, PCE does not scale well to high-dimensional problems.
We address this challenge by combining PCE with ideas from probabilistic
circuits, resulting in the deep polynomial chaos expansion (DeepPCE) - a deep
generalization of PCE that scales effectively to high-dimensional input spaces.
DeepPCE achieves predictive performance comparable to that of multi-layer
perceptrons (MLPs), while retaining PCE's ability to compute exact statistical
inferences via simple forward passes.

</details>


### [46] [Large Language Model-Enhanced Reinforcement Learning for Diverse and Novel Recommendations](https://arxiv.org/abs/2507.21274)
*Jiin Woo,Alireza Bagheri Garakani,Tianchen Zhou,Zhishen Huang,Yan Gao*

Main category: cs.LG

TL;DR: LAAC利用LLM作为参考策略提出新颖推荐，通过轻量级策略优化，提升多样性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统通常注重点击相关性，而忽视多样性和新颖性，LAAC旨在解决这一问题。

Method: 结合LLM和强化学习，通过双层优化训练actor-critic网络，正则化LLM建议。

Result: 在真实数据集上，LAAC在多样性、新颖性和准确性上优于基线方法。

Conclusion: LAAC有效整合LLM知识，无需昂贵微调，提升推荐系统性能。

Abstract: In recommendation systems, diversity and novelty are essential for capturing
varied user preferences and encouraging exploration, yet many systems
prioritize click relevance. While reinforcement learning (RL) has been explored
to improve diversity, it often depends on random exploration that may not align
with user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a
novel method that leverages large language models (LLMs) as reference policies
to suggest novel items, while training a lightweight policy to refine these
suggestions using system-specific data. The method formulates training as a
bilevel optimization between actor and critic networks, enabling the critic to
selectively favor promising novel actions and the actor to improve its policy
beyond LLM recommendations. To mitigate overestimation of unreliable LLM
suggestions, we apply regularization that anchors critic values for unexplored
items close to well-estimated dataset actions. Experiments on real-world
datasets show that LAAC outperforms existing baselines in diversity, novelty,
and accuracy, while remaining robust on imbalanced data, effectively
integrating LLM knowledge without expensive fine-tuning.

</details>


### [47] [Blending data and physics for reduced-order modeling of systems with spatiotemporal chaotic dynamics](https://arxiv.org/abs/2507.21299)
*Alex Guo,Michael D. Graham*

Main category: cs.LG

TL;DR: 提出了一种结合数据和全阶模型（FOM）的混合降阶模型（ROM），用于预测时空混沌动力学，显著优于纯数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 利用已知物理模型（FOM）提升数据驱动技术在混沌动力学降阶建模中的预测能力。

Method: 通过自编码器找到不变流形坐标，将FOM的向量场投影到流形上，并用动态数据校正或作为贝叶斯先验更新。使用神经常微分方程方法。

Result: 在Kuramoto-Sivashinsky和复杂Ginzburg-Landau方程模拟数据中，混合方法在数据丰富、稀缺甚至FOM参数错误情况下均显著提升预测能力。

Conclusion: 混合方法结合物理模型和数据，显著提升了混沌动力学预测的鲁棒性和准确性。

Abstract: While data-driven techniques are powerful tools for reduced-order modeling of
systems with chaotic dynamics, great potential remains for leveraging known
physics (i.e. a full-order model (FOM)) to improve predictive capability. We
develop a hybrid reduced order model (ROM), informed by both data and FOM, for
evolving spatiotemporal chaotic dynamics on an invariant manifold whose
coordinates are found using an autoencoder. This approach projects the vector
field of the FOM onto the invariant manifold; then, this physics-derived vector
field is either corrected using dynamic data, or used as a Bayesian prior that
is updated with data. In both cases, the neural ordinary differential equation
approach is used. We consider simulated data from the Kuramoto-Sivashinsky and
complex Ginzburg-Landau equations. Relative to the data-only approach, for
scenarios of abundant data, scarce data, and even an incorrect FOM (i.e.
erroneous parameter values), the hybrid approach yields substantially improved
time-series predictions.

</details>


### [48] [DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation](https://arxiv.org/abs/2507.21350)
*Wenkai Tan,Alvaro Velasquez,Houbing Song*

Main category: cs.LG

TL;DR: 提出了一种结合神经辐射场（NeRF）和物理信息神经网络（PINN）的神经符号框架，用于从稀疏多视角图像序列重建和模拟弹性物体，无需显式几何信息。


<details>
  <summary>Details</summary>
Motivation: 解决纯经验方法可能偏离物理原理与传统数值求解器需要完整几何信息且计算成本高的问题。

Method: 整合NeRF用于物体重建和PINN用于弹性力学偏微分方程，结合图像监督和物理约束学习时空表示。

Result: 方法提高了模拟精度和结果可解释性，能处理复杂边界和初始条件。

Conclusion: 该框架为弹性物体重建和模拟提供了一种高效且物理一致的方法。

Abstract: Neural networks have emerged as a powerful tool for modeling physical
systems, offering the ability to learn complex representations from limited
data while integrating foundational scientific knowledge. In particular,
neuro-symbolic approaches that combine data-driven learning, the neuro, with
symbolic equations and rules, the symbolic, address the tension between methods
that are purely empirical, which risk straying from established physical
principles, and traditional numerical solvers that demand complete geometric
knowledge and can be prohibitively expensive for high-fidelity simulations. In
this work, we present a novel neuro-symbolic framework for reconstructing and
simulating elastic objects directly from sparse multi-view image sequences,
without requiring explicit geometric information. Specifically, we integrate a
neural radiance field (NeRF) for object reconstruction with physics-informed
neural networks (PINN) that incorporate the governing partial differential
equations of elasticity. In doing so, our method learns a spatiotemporal
representation of deforming objects that leverages both image supervision and
symbolic physical constraints. To handle complex boundary and initial
conditions, which are traditionally confronted using finite element methods,
boundary element methods, or sensor-based measurements, we employ an
energy-constrained Physics-Informed Neural Network architecture. This design
enhances both simulation accuracy and the explainability of results.

</details>


### [49] [A Contrastive Diffusion-based Network (CDNet) for Time Series Classification](https://arxiv.org/abs/2507.21357)
*Yaoyu Zhang,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: CDNet是一种基于对比扩散的网络，通过生成正负样本增强现有分类器，显著提升时间序列分类性能，尤其在噪声、相似和多模态数据条件下。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在时间序列分类中因类相似性、多模态分布和噪声导致的性能下降问题。

Method: 提出CDNet，利用卷积近似反向扩散步骤学习样本间转换，结合不确定性加权复合损失进行鲁棒训练。

Result: 在UCR Archive和模拟数据集上，CDNet显著提升了现有深度学习分类器的性能。

Conclusion: CDNet在挑战性数据条件下表现出色，为时间序列分类提供了有效解决方案。

Abstract: Deep learning models are widely used for time series classification (TSC) due
to their scalability and efficiency. However, their performance degrades under
challenging data conditions such as class similarity, multimodal distributions,
and noise. To address these limitations, we propose CDNet, a Contrastive
Diffusion-based Network that enhances existing classifiers by generating
informative positive and negative samples via a learned diffusion process.
Unlike traditional diffusion models that denoise individual samples, CDNet
learns transitions between samples--both within and across classes--through
convolutional approximations of reverse diffusion steps. We introduce a
theoretically grounded CNN-based mechanism to enable both denoising and mode
coverage, and incorporate an uncertainty-weighted composite loss for robust
training. Extensive experiments on the UCR Archive and simulated datasets
demonstrate that CDNet significantly improves state-of-the-art (SOTA) deep
learning classifiers, particularly under noisy, similar, and multimodal
conditions.

</details>


### [50] [Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2507.21386)
*Xuan Wu,Di Wang,Chunguo Wu,Kaifang Qi,Chunyan Miao,Yubin Xiao,Jian Zhang,You Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为ECHO的NCO求解器，用于解决MMHCVRP问题，通过双模态节点编码器和无参数交叉注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有NCO求解器多关注单车辆VRP变体，忽视了更现实的MMHCVRP问题，且现有方法存在短视解码决策和忽略关键特性的问题。

Method: ECHO采用双模态节点编码器捕捉局部拓扑关系，利用无参数交叉注意力机制减少短视决策，并通过数据增强策略稳定训练。

Result: 实验表明，ECHO在不同车辆和节点数量下均优于现有NCO求解器，并表现出良好的泛化能力。

Conclusion: ECHO通过创新的编码器和训练策略，显著提升了MMHCVRP问题的求解性能。

Abstract: Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed
to address Vehicle Routing Problems (VRPs). However, most of these solvers
focus exclusively on single-vehicle VRP variants, overlooking the more
realistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP),
which involves multiple vehicles. Existing MMHCVRP solvers typically select a
vehicle and its next node to visit at each decoding step, but often make myopic
decoding decisions and overlook key properties of MMHCVRP, including local
topological relationships, vehicle permutation invariance, and node symmetry,
resulting in suboptimal performance. To better address these limitations, we
propose ECHO, an efficient NCO solver. First, ECHO exploits the proposed
dual-modality node encoder to capture local topological relationships among
nodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed
Parameter-Free Cross-Attention mechanism to prioritize the vehicle selected in
the preceding decoding step. Finally, leveraging vehicle permutation invariance
and node symmetry, we introduce a tailored data augment strategy for MMHCVRP to
stabilize the Reinforcement Learning training process. To assess the
performance of ECHO, we conduct extensive experiments. The experimental results
demonstrate that ECHO outperforms state-of-the-art NCO solvers across varying
numbers of vehicles and nodes, and exhibits well-performing generalization
across both scales and distribution patterns. Finally, ablation studies
validate the effectiveness of all proposed methods.

</details>


### [51] [Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach](https://arxiv.org/abs/2507.21397)
*Fnu Hairi,Jiao Yang,Tianchen Zhou,Haibo Yang,Chaosheng Dong,Fan Yang,Michinari Momma,Yan Gao,Jia Liu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In many multi-objective reinforcement learning (MORL) applications, being
able to systematically explore the Pareto-stationary solutions under multiple
non-convex reward objectives with theoretical finite-time sample complexity
guarantee is an important and yet under-explored problem. This motivates us to
take the first step and fill the important gap in MORL. Specifically, in this
paper, we propose a \uline{M}ulti-\uline{O}bjective weighted-\uline{CH}ebyshev
\uline{A}ctor-critic (MOCHA) algorithm for MORL, which judiciously integrates
the weighted-Chebychev (WC) and actor-critic framework to enable
Pareto-stationarity exploration systematically with finite-time sample
complexity guarantee. Sample complexity result of MOCHA algorithm reveals an
interesting dependency on $p_{\min}$ in finding an $\epsilon$-Pareto-stationary
solution, where $p_{\min}$ denotes the minimum entry of a given weight vector
$\mathbf{p}$ in WC-scarlarization. By carefully choosing learning rates, the
sample complexity for each exploration can be
$\tilde{\mathcal{O}}(\epsilon^{-2})$. Furthermore, simulation studies on a
large KuaiRand offline dataset, show that the performance of MOCHA algorithm
significantly outperforms other baseline MORL approaches.

</details>


### [52] [Data Leakage and Redundancy in the LIT-PCBA Benchmark](https://arxiv.org/abs/2507.21404)
*Amber Huang,Ian Scott Knight,Slava Naprienko*

Main category: cs.LG

TL;DR: LIT-PCBA基准数据集存在严重的数据泄漏、重复和结构冗余问题，导致其无法公平评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 揭示LIT-PCBA数据集在虚拟筛选基准中的缺陷，以提醒社区开发更可靠的数据集。

Method: 通过审计数据集，识别数据泄漏、重复和结构冗余问题，并设计一个基于记忆的基线方法验证问题的影响。

Result: 发现大量数据泄漏和冗余，导致模型仅通过记忆而非泛化表现优异，甚至超越先进模型。

Conclusion: LIT-PCBA不适合作为公平评估的基准，呼吁社区改进数据集设计。

Abstract: LIT-PCBA is a widely used benchmark for virtual screening, but our audit
reveals it is fundamentally compromised. The dataset suffers from egregious
data leakage, rampant duplication, and pervasive analog redundancy -- flaws
that invalidate its use for fair model evaluation. Notably, we identify 2,491
inactives duplicated across training and validation sets, and thousands more
repeated within individual data splits (2,945 in training, 789 in validation).
Critically, three ligands in the query set -- meant to represent unseen test
cases -- are leaked: two appear in the training set, one in validation.
Structural redundancy compounds these issues: for some targets, over 80% of
query ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1
alone, we find 323 highly similar active pairs between training and validation
sets, invalidating claims of chemical diversity. These and other flaws
collectively cause models trained on LIT-PCBA to memorize rather than
generalize. To demonstrate the consequences of these data integrity failures,
we implement a trivial memorization-based baseline -- using no learning, no
physics, and no modeling -- that outperforms state-of-the-art models, including
deep neural networks like CHEESE, on LIT-PCBA simply by exploiting these
artifacts. Our findings render the benchmark unfit for its intended purpose and
call into question previous results based on its use. We share this audit to
raise awareness and provide tooling to help the community develop more rigorous
and reliable datasets going forward. All scripts necessary to reproduce our
audit and the baseline implementation are available at:
https://github.com/sievestack/LIT-PCBA-audit

</details>


### [53] [Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring](https://arxiv.org/abs/2507.21422)
*Sujia Huang,Lele Fu,Zhen Cui,Tong Zhang,Na Song,Bo Huang*

Main category: cs.LG

TL;DR: 提出了一种基于扭矩的分层图重连策略，通过动态调整消息传递来提升异质图和噪声图中的表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络（GNNs）在原生图结构上的消息传递可能不够高效，尤其是在异质图和噪声图中，因此需要开发图重连方法。

Method: 定义了一种干扰感知的扭矩度量，结合结构距离和能量分数，量化边的扰动，并通过分层修剪高扭矩边和添加低扭矩边来重构感受野。

Result: 在基准数据集上的实验表明，该方法在异质图和同质图上均优于现有方法，并在噪声图中保持高准确性。

Conclusion: 扭矩驱动的分层重连策略有效提升了GNNs在复杂图结构中的表现和鲁棒性。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning from
graph-structured data, leveraging message passing to diffuse information and
update node representations. However, most efforts have suggested that native
interactions encoded in the graph may not be friendly for this process,
motivating the development of graph rewiring methods. In this work, we propose
a torque-driven hierarchical rewiring strategy, inspired by the notion of
torque in classical mechanics, dynamically modulating message passing to
improve representation learning in heterophilous graphs and enhance robustness
against noisy graphs. Specifically, we define an interference-aware torque
metric that integrates structural distance and energy scores to quantify the
perturbation induced by edges, thereby encouraging each node to aggregate
information from its nearest low-energy neighbors. We use the metric to
hierarchically reconfigure the receptive field of each layer by judiciously
pruning high-torque edges and adding low-torque links, suppressing propagation
noise and boosting pertinent signals. Extensive evaluations on benchmark
datasets show that our approach surpasses state-of-the-art methods on both
heterophilous and homophilous graphs, and maintains high accuracy on noisy
graph.

</details>


### [54] [MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse](https://arxiv.org/abs/2507.21433)
*Kaiwen Chen,Xin Tan,Minchen Yu,Hong Xu*

Main category: cs.LG

TL;DR: MemShare是一种新型KV缓存管理方法，通过识别和重用高度相似的中间推理步骤的KV缓存块，显著减少内存开销并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中生成的冗长思维链导致内存开销大，而中间推理步骤高度相似，为KV缓存的重用提供了机会。

Method: 提出MemShare，采用协同过滤算法识别可重用的KV缓存块，并实现零拷贝缓存重用。

Result: 实验表明，MemShare在保持准确性的同时，吞吐量提升高达84.79%。

Conclusion: MemShare通过高效KV缓存管理，显著优化了LRMs的内存和性能表现。

Abstract: Large Reasoning Models (LRMs) have achieved significant advances in
mathematical reasoning and formal logic tasks. However, their tendency to
generate lengthy chain-of-thought sequences leads to substantial memory
overhead during inference. We observe that LRMs frequently produce highly
similar intermediate reasoning steps, which correspond to similar KV cache
states across layers. Motivated by this observation, we propose MemShare, a
novel KV cache management approach that effectively reduces memory overhead.
MemShare employs a collaborative filtering algorithm to efficiently identify
reusable KV cache blocks and enables zero copy cache reuse to significantly
reduce memory overhead, improve throughput while maintaining accuracy.
Experimental results demonstrate that MemShare delivers up to 84.79\%
improvement in throughput while maintaining better accuracy compared to
existing KV cache management methods.

</details>


### [55] [PVD-ONet: A Multi-scale Neural Operator Method for Singularly Perturbed Boundary Layer Problems](https://arxiv.org/abs/2507.21437)
*Tiantian Sun,Jian Zu*

Main category: cs.LG

TL;DR: 提出了两种新框架PVD-Net和PVD-ONet，用于解决奇异摄动问题，无需数据仅依赖控制方程，并在稳定性和高精度建模方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络在奇异摄动问题上收敛困难，需要新方法解决。

Method: PVD-Net采用两网络架构（稳定性优先）和五网络架构（高精度优先），PVD-ONet扩展为算子学习框架。

Result: 数值实验表明，新方法在多种误差指标下优于现有基线。

Conclusion: PVD-Net和PVD-ONet为多尺度问题提供了高效新方法。

Abstract: Physics-informed neural networks and Physics-informed DeepONet excel in
solving partial differential equations; however, they often fail to converge
for singularly perturbed problems. To address this, we propose two novel
frameworks, Prandtl-Van Dyke neural network (PVD-Net) and its operator learning
extension Prandtl-Van Dyke Deep Operator Network (PVD-ONet), which rely solely
on governing equations without data. To address varying task-specific
requirements, both PVD-Net and PVD-ONet are developed in two distinct versions,
tailored respectively for stability-focused and high-accuracy modeling. The
leading-order PVD-Net adopts a two-network architecture combined with Prandtl's
matching condition, targeting stability-prioritized scenarios. The high-order
PVD-Net employs a five-network design with Van Dyke's matching principle to
capture fine-scale boundary layer structures, making it ideal for high-accuracy
scenarios. PVD-ONet generalizes PVD-Net to the operator learning setting by
assembling multiple DeepONet modules, directly mapping initial conditions to
solution operators and enabling instant predictions for an entire family of
boundary layer problems without retraining. Numerical experiments on various
models show that our proposed methods consistently outperform existing
baselines under various error metrics, thereby offering a powerful new approach
for multi-scale problems.

</details>


### [56] [Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training](https://arxiv.org/abs/2507.21452)
*Sodtavilan Odonchimed,Tatsuya Matsushima,Simon Holk,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: RAGDP是一种无需额外训练的新框架，通过知识库加速预训练扩散策略的推理，提高准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 扩散策略（DPs）在模仿学习中表现优异，但生成动作耗时；现有方法如一致性策略（CP）训练时间长。

Method: RAGDP通过构建专家演示的向量数据库，结合中间噪声去除步骤，减少推理步骤。

Result: RAGDP在20倍加速下仍保持准确性优势，比CP高7%。

Conclusion: RAGDP在无需额外训练的情况下，显著提升了扩散策略的效率和准确性。

Abstract: Diffusion Policies (DPs) have attracted attention for their ability to
achieve significant accuracy improvements in various imitation learning tasks.
However, DPs depend on Diffusion Models, which require multiple noise removal
steps to generate a single action, resulting in long generation times. To solve
this problem, knowledge distillation-based methods such as Consistency Policy
(CP) have been proposed. However, these methods require a significant amount of
training time, especially for difficult tasks. In this study, we propose RAGDP
(Retrieve-Augmented Generation for Diffusion Policies) as a novel framework
that eliminates the need for additional training using a knowledge base to
expedite the inference of pre-trained DPs. In concrete, RAGDP encodes
observation-action pairs through the DP encoder to construct a vector database
of expert demonstrations. During inference, the current observation is
embedded, and the most similar expert action is extracted. This extracted
action is combined with an intermediate noise removal step to reduce the number
of steps required compared to the original diffusion step. We show that by
using RAGDP with the base model and existing acceleration methods, we improve
the accuracy and speed trade-off with no additional training. Even when
accelerating the models 20 times, RAGDP maintains an advantage in accuracy,
with a 7% increase over distillation models such as CP.

</details>


### [57] [Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning](https://arxiv.org/abs/2507.21494)
*Wenxuan Bao,Ruxi Deng,Ruizhong Qiu,Tianxin Wei,Hanghang Tong,Jingrui He*

Main category: cs.LG

TL;DR: Latte框架通过本地和外部内存存储历史测试数据，利用嵌入相似性和不确定性提升模型性能，适用于分散式学习环境。


<details>
  <summary>Details</summary>
Motivation: 解决分散式学习中测试数据有限和个性化分布适应的问题。

Method: 每个客户端维护本地和外部内存，通过服务器协调检索相似客户端的原型，结合嵌入相似性和不确定性进行本地适应。

Result: 在领域适应和损坏基准测试中表现优异，通信和计算成本极低。

Conclusion: Latte在分散式环境中高效且鲁棒，适用于多客户端分布适应。

Abstract: Test-time adaptation with pre-trained vision-language models has gained
increasing attention for addressing distribution shifts during testing. Among
these approaches, memory-based algorithms stand out due to their training-free
nature and ability to leverage historical test data. However, existing
test-time adaptation methods are typically designed for a single domain with
abundant data. In decentralized settings such as federated learning, applying
these methods individually to each client suffers from limited test data, while
directly sharing a single global memory via the server prevents proper
personalization to each client's unique distribution. To address this, we
propose Latte, a novel framework where each client maintains a local memory to
store embeddings from its own historical test data and an external memory to
store class prototypes from other relevant clients. During communication, each
client retrieves prototypes from similar clients under the server's
coordination to expand its memory. For local adaptation, Latte utilizes both
embedding similarity and uncertainty to enhance model performance. Our
theoretical analysis shows that Latte effectively leverages in-distribution
clients while remaining robust to out-of-distribution clients. Extensive
experiments on domain adaptation and corruption benchmarks validate that Latte
achieves superior performance in decentralized settings, while introducing only
negligible communication and computation costs. Our code is available at
https://github.com/baowenxuan/Latte .

</details>


### [58] [Evaluation and Benchmarking of LLM Agents: A Survey](https://arxiv.org/abs/2507.21504)
*Mahmoud Mohammadi,Yipeng Li,Jane Lo,Wendy Yip*

Main category: cs.LG

TL;DR: 本文综述了LLM智能体评估的现状，提出了一个二维分类法，并指出了企业应用中的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM智能体的评估领域复杂且不成熟，需要系统化的框架来指导研究和实践。

Method: 提出二维分类法，包括评估目标（如行为、能力、可靠性）和评估过程（如交互模式、数据集、工具）。

Result: 总结了现有工作，突出了企业应用中的挑战（如数据访问、可靠性）和未来研究方向（如全面、现实的评估）。

Conclusion: 本文为LLM智能体评估提供了系统化框架，助力实际应用中的评估工作。

Abstract: The rise of LLM-based agents has opened new frontiers in AI applications, yet
evaluating these agents remains a complex and underdeveloped area. This survey
provides an in-depth overview of the emerging field of LLM agent evaluation,
introducing a two-dimensional taxonomy that organizes existing work along (1)
evaluation objectives -- what to evaluate, such as agent behavior,
capabilities, reliability, and safety -- and (2) evaluation process -- how to
evaluate, including interaction modes, datasets and benchmarks, metric
computation methods, and tooling. In addition to taxonomy, we highlight
enterprise-specific challenges, such as role-based access to data, the need for
reliability guarantees, dynamic and long-horizon interactions, and compliance,
which are often overlooked in current research. We also identify future
research directions, including holistic, more realistic, and scalable
evaluation. This work aims to bring clarity to the fragmented landscape of
agent evaluation and provide a framework for systematic assessment, enabling
researchers and practitioners to evaluate LLM agents for real-world deployment.

</details>


### [59] [Hierarchical Stochastic Differential Equation Models for Latent Manifold Learning in Neural Time Series](https://arxiv.org/abs/2507.21531)
*Pedram Rajaei,Maryam Ostadsharif Memar,Navid Ziaei,Behzad Nazari,Ali Yousefi*

Main category: cs.LG

TL;DR: 提出了一种新型分层随机微分方程（SDE）模型，用于高效且可解释地建模高维神经时间序列的低维流形结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算效率和可解释性上存在局限，无法有效建模复杂时间序列的流形结构。

Method: 采用布朗桥SDE建模潜在空间，并通过多变量标记点过程采样，构建连续可微的潜在过程。

Result: 模型在合成数据和神经记录上验证了其准确恢复流形结构的能力，且计算成本随数据长度线性增长。

Conclusion: 该模型在计算效率和可解释性上优于现有方法，适用于复杂时间序列建模。

Abstract: The manifold hypothesis suggests that high-dimensional neural time series lie
on a low-dimensional manifold shaped by simpler underlying dynamics. To uncover
this structure, latent dynamical variable models such as state-space models,
recurrent neural networks, neural ordinary differential equations, and Gaussian
Process Latent Variable Models are widely used. We propose a novel hierarchical
stochastic differential equation (SDE) model that balances computational
efficiency and interpretability, addressing key limitations of existing
methods. Our model assumes the trajectory of a manifold can be reconstructed
from a sparse set of samples from the manifold trajectory. The latent space is
modeled using Brownian bridge SDEs, with points - specified in both time and
value - sampled from a multivariate marked point process. These Brownian
bridges define the drift of a second set of SDEs, which are then mapped to the
observed data. This yields a continuous, differentiable latent process capable
of modeling arbitrarily complex time series as the number of manifold points
increases. We derive training and inference procedures and show that the
computational cost of inference scales linearly with the length of the
observation data. We then validate our model on both synthetic data and neural
recordings to demonstrate that it accurately recovers the underlying manifold
structure and scales effectively with data dimensionality.

</details>


### [60] [Categorical Distributions are Effective Neural Network Outputs for Event Prediction](https://arxiv.org/abs/2507.21616)
*Kevin Doran,Tom Baden*

Main category: cs.LG

TL;DR: 论文探讨了使用简单的神经网络输出（分类概率分布）进行下一事件预测的有效性，并分析了其未被广泛采用的原因。


<details>
  <summary>Details</summary>
Motivation: 研究为何简单的输出结构在神经时间点过程模型中不常见。

Method: 通过扩展和创建新数据集，验证简单分类分布在多种数据集中的表现。

Result: 发现现有数据集未能充分揭示事件生成过程，而简单分类分布在广泛数据集中表现优异。

Conclusion: 简单分类分布是一种有效且通用的输出结构。

Abstract: We demonstrate the effectiveness of using a simple neural network output, a
categorical probability distribution, for the task of next spike prediction.
This case study motivates an investigation into why this simple output
structure is not commonly used with neural temporal point process models. We
find evidence that many existing datasets for evaluating temporal point process
models do not reveal much information about the underlying event generating
processes, and many existing models perform well due to regularization effects
of model size and constraints on output structure. We extend existing datasets
and create new ones in order to explore outside of this information limited
regime and find that outputting a simple categorical distribution is
competitive across a wide range of datasets.

</details>


### [61] [Hyperbolic Genome Embeddings](https://arxiv.org/abs/2507.21648)
*Raiyan R. Khan,Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: 该论文提出了一种基于双曲CNN的新方法，用于基因组序列建模，通过利用生物系统的进化结构，实现了更高效的DNA序列表示。该方法在多个基准数据集上表现优于欧几里得模型，甚至超越了一些先进的DNA语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前基因组序列建模方法难以将机器学习模型的归纳偏置与生物系统的进化结构对齐，因此需要一种更有效的方法。

Method: 采用双曲CNN，无需显式系统发育映射，直接捕捉序列的关键功能与调控特性。

Result: 在42个基因组解释基准数据集中，37个上双曲模型优于欧几里得模型，并在7个GUE数据集上达到最先进性能。

Conclusion: 双曲框架为基因组表示学习提供了稳健的范式，具有显著潜力。

Abstract: Current approaches to genomic sequence modeling often struggle to align the
inductive biases of machine learning models with the evolutionarily-informed
structure of biological systems. To this end, we formulate a novel application
of hyperbolic CNNs that exploits this structure, enabling more expressive DNA
sequence representations. Our strategy circumvents the need for explicit
phylogenetic mapping while discerning key properties of sequences pertaining to
core functional and regulatory behavior. Across 37 out of 42 genome
interpretation benchmark datasets, our hyperbolic models outperform their
Euclidean equivalents. Notably, our approach even surpasses state-of-the-art
performance on seven GUE benchmark datasets, consistently outperforming many
DNA language models while using orders of magnitude fewer parameters and
avoiding pretraining. Our results include a novel set of benchmark
datasets--the Transposable Elements Benchmark--which explores a major but
understudied component of the genome with deep evolutionary significance. We
further motivate our work by exploring how our hyperbolic models recognize
genomic signal under various data-generating conditions and by constructing an
empirical method for interpreting the hyperbolicity of dataset embeddings.
Throughout these assessments, we find persistent evidence highlighting the
potential of our hyperbolic framework as a robust paradigm for genome
representation learning. Our code and benchmark datasets are available at
https://github.com/rrkhan/HGE.

</details>


### [62] [DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs](https://arxiv.org/abs/2507.21653)
*Yuan Li,Jun Hu,Bryan Hooi,Bingsheng He,Cheng Chen*

Main category: cs.LG

TL;DR: 论文提出Dual Granularity Prompting (DGP)方法，通过细粒度文本细节和粗粒度邻居信息摘要，解决异构欺诈检测图中信息过载问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本提示在异构欺诈检测图中因多跳关系导致的信息过载问题，避免关键信号被淹没。

Method: 提出DGP方法，结合细粒度目标节点文本和粗粒度邻居信息摘要，针对不同数据模态采用定制化摘要策略。

Result: 实验表明DGP在可控的token预算内，将欺诈检测性能提升6.8%（AUPRC）。

Conclusion: DGP展示了图增强LLMs在欺诈检测中的潜力，有效平衡信息压缩与性能提升。

Abstract: Real-world fraud detection applications benefit from graph learning
techniques that jointly exploit node features, often rich in textual data, and
graph structural information. Recently, Graph-Enhanced LLMs emerge as a
promising graph learning approach that converts graph information into prompts,
exploiting LLMs' ability to reason over both textual and structural
information. Among them, text-only prompting, which converts graph information
to prompts consisting solely of text tokens, offers a solution that relies only
on LLM tuning without requiring additional graph-specific encoders. However,
text-only prompting struggles on heterogeneous fraud-detection graphs:
multi-hop relations expand exponentially with each additional hop, leading to
rapidly growing neighborhoods associated with dense textual information. These
neighborhoods may overwhelm the model with long, irrelevant content in the
prompt and suppress key signals from the target node, thereby degrading
performance. To address this challenge, we propose Dual Granularity Prompting
(DGP), which mitigates information overload by preserving fine-grained textual
details for the target node while summarizing neighbor information into
coarse-grained text prompts. DGP introduces tailored summarization strategies
for different data modalities, bi-level semantic abstraction for textual fields
and statistical aggregation for numerical features, enabling effective
compression of verbose neighbor content into concise, informative prompts.
Experiments across public and industrial datasets demonstrate that DGP operates
within a manageable token budget while improving fraud detection performance by
up to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of
Graph-Enhanced LLMs for fraud detection.

</details>


### [63] [Probabilistic Consistency in Machine Learning and Its Connection to Uncertainty Quantification](https://arxiv.org/abs/2507.21670)
*Paul Patrone,Anthony Kearsley*

Main category: cs.LG

TL;DR: 本文通过诊断推理探讨了机器学习模型的不确定性量化问题，提出了一种基于流行度的分类理论，证明了自洽模型与类条件概率分布的等价性。


<details>
  <summary>Details</summary>
Motivation: 机器学习的黑箱特性使其预测置信度难以量化，本文旨在解决这一问题并探讨其与不确定性量化的联系。

Method: 通过分析流行度的多种解释，推导出分类的层次集理论，并研究二元贝叶斯最优分类器的性质及其参数化。

Result: 证明了贝叶斯分类器满足单调性和类切换性质，可用于推断密度比，并推导出多类分类的归一化和自洽条件。

Conclusion: 这些结果为机器学习模型提供了有效的概率解释，并通过不确定性传播框架为不确定性量化提供了理论基础。

Abstract: Machine learning (ML) is often viewed as a powerful data analysis tool that
is easy to learn because of its black-box nature. Yet this very nature also
makes it difficult to quantify confidence in predictions extracted from ML
models, and more fundamentally, to understand how such models are mathematical
abstractions of training data. The goal of this paper is to unravel these
issues and their connections to uncertainty quantification (UQ) by pursuing a
line of reasoning motivated by diagnostics. In such settings, prevalence - i.e.
the fraction of elements in class - is often of inherent interest. Here we
analyze the many interpretations of prevalence to derive a level-set theory of
classification, which shows that certain types of self-consistent ML models are
equivalent to class-conditional probability distributions. We begin by studying
the properties of binary Bayes optimal classifiers, recognizing that their
boundary sets can be reinterpreted as level-sets of pairwise density ratios. By
parameterizing Bayes classifiers in terms of the prevalence, we then show that
they satisfy important monotonicity and class-switching properties that can be
used to deduce the density ratios without direct access to the boundary sets.
Moreover, this information is sufficient for tasks such as constructing the
multiclass Bayes-optimal classifier and estimating inherent uncertainty in the
class assignments. In the multiclass case, we use these results to deduce
normalization and self-consistency conditions, the latter being equivalent to
the law of total probability for classifiers. We also show that these are
necessary conditions for arbitrary ML models to have valid probabilistic
interpretations. Throughout we demonstrate how this analysis informs the
broader task of UQ for ML via an uncertainty propagation framework.

</details>


### [64] [PREIG: Physics-informed and Reinforcement-driven Interpretable GRU for Commodity Demand Forecasting](https://arxiv.org/abs/2507.21710)
*Hongwei Ma,Junbin Gao,Minh-Ngoc Tran*

Main category: cs.LG

TL;DR: PREIG是一种新颖的深度学习框架，结合GRU和PINN原理，通过嵌入经济约束提升商品需求预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 商品需求预测面临市场动态波动和非线性依赖等挑战，需要经济一致的预测方法。

Method: PREIG整合GRU和PINN，通过定制损失函数嵌入价格与需求的负弹性约束，并采用混合优化策略（NAdam、L-BFGS和POP）。

Result: 实验表明，PREIG在RMSE和MAPE上显著优于传统计量模型和深度学习基线，同时保持良好可解释性。

Conclusion: PREIG通过结合领域知识、优化理论和深度学习，为高维非线性时间序列预测提供了稳健、可解释且可扩展的解决方案。

Abstract: Accurately forecasting commodity demand remains a critical challenge due to
volatile market dynamics, nonlinear dependencies, and the need for economically
consistent predictions. This paper introduces PREIG, a novel deep learning
framework tailored for commodity demand forecasting. The model uniquely
integrates a Gated Recurrent Unit (GRU) architecture with physics-informed
neural network (PINN) principles by embedding a domain-specific economic
constraint: the negative elasticity between price and demand. This constraint
is enforced through a customized loss function that penalizes violations of the
physical rule, ensuring that model predictions remain interpretable and aligned
with economic theory. To further enhance predictive performance and stability,
PREIG incorporates a hybrid optimization strategy that couples NAdam and L-BFGS
with Population-Based Training (POP). Experiments across multiple commodities
datasets demonstrate that PREIG significantly outperforms traditional
econometric models (ARIMA,GARCH) and deep learning baselines (BPNN,RNN) in both
RMSE and MAPE. When compared with GRU,PREIG maintains good explainability while
still performing well in prediction. By bridging domain knowledge, optimization
theory and deep learning, PREIG provides a robust, interpretable, and scalable
solution for high-dimensional nonlinear time series forecasting in economy.

</details>


### [65] [Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins](https://arxiv.org/abs/2507.21720)
*Gang Wang,Peng Hu*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络和物理知识的模型，用于预测氢氟烯烃制冷剂的剩余热力学性质，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 氢氟烯烃因其极低的全球变暖潜能值成为下一代制冷剂的有力候选，但缺乏可靠的热力学数据阻碍了其发现和应用。

Method: 结合理论方法和数据驱动方法，提出了一种神经网络扩展对应状态模型，通过图神经网络模块表征分子微观结构。

Result: 模型在液体和超临界区域的密度和能量性质预测上显著优于传统方法，平均绝对偏差分别为1.49%（液体）和2.42%（超临界）。

Conclusion: 该模型通过嵌入物理知识显著提升了预测精度，有望加速新型氢氟烯烃制冷剂的发现。

Abstract: Hydrofluoroolefins are considered the most promising next-generation
refrigerants due to their extremely low global warming potential values, which
can effectively mitigate the global warming effect. However, the lack of
reliable thermodynamic data hinders the discovery and application of newer and
superior hydrofluoroolefin refrigerants. In this work, integrating the
strengths of theoretical method and data-driven method, we proposed a neural
network extended corresponding state model to predict the residual
thermodynamic properties of hydrofluoroolefin refrigerants. The innovation is
that the fluids are characterized through their microscopic molecular
structures by the inclusion of graph neural network module and the specialized
design of model architecture to enhance its generalization ability. The
proposed model is trained using the highly accurate data of available known
fluids, and evaluated via the leave-one-out cross-validation method. Compared
to conventional extended corresponding state models or cubic equation of state,
the proposed model shows significantly improved accuracy for density and energy
properties in liquid and supercritical regions, with average absolute deviation
of 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for
residual entropy, 1.85% and 1.34% for residual enthalpy. These results
demonstrate the effectiveness of embedding physics knowledge into the machine
learning model. The proposed neural network extended corresponding state model
is expected to significantly accelerate the discovery of novel
hydrofluoroolefin refrigerants.

</details>


### [66] [Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation](https://arxiv.org/abs/2507.21738)
*Huiqiang Chen,Tianqing Zhu,Xin Yu,Wanlei Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为ZS-PAG的新框架，解决了零样本机器遗忘中的过遗忘问题，通过生成对抗样本和设计基于影响的伪标签策略，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘算法依赖剩余数据，无法适用于仅提供遗忘样本的零样本场景，因此需要一种新方法来解决这一问题。

Method: 提出了ZS-PAG框架，包括生成对抗样本近似剩余数据、在特定子空间进行遗忘以防止过遗忘，以及设计基于影响的伪标签策略。

Result: 实验验证了该方法的有效性和优越性，理论分析也支持其可行性。

Conclusion: ZS-PAG在零样本机器遗忘中表现优异，解决了过遗忘问题并提升了模型性能。

Abstract: Machine unlearning aims to remove the influence of specific samples from a
trained model. A key challenge in this process is over-unlearning, where the
model's performance on the remaining data significantly drops due to the change
in the model's parameters. Existing unlearning algorithms depend on the
remaining data to prevent this issue. As such, these methods are inapplicable
in a more practical scenario, where only the unlearning samples are available
(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to
fill this gap. Our approach offers three key innovations: (1) we approximate
the inaccessible remaining data by generating adversarial samples; (2)
leveraging the generated samples, we pinpoint a specific subspace to perform
the unlearning process, therefore preventing over-unlearning in the challenging
zero-shot scenario; and (3) we consider the influence of the unlearning process
on the remaining samples and design an influence-based pseudo-labeling
strategy. As a result, our method further improves the model's performance
after unlearning. The proposed method holds a theoretical guarantee, and
experiments on various benchmarks validate the effectiveness and superiority of
our proposed method over several baselines.

</details>


### [67] [evoxels: A differentiable physics framework for voxel-based microstructure simulations](https://arxiv.org/abs/2507.21748)
*Simon Daubner,Alexander E. Cohen,Benjamin Dörich,Samuel J. Cooper*

Main category: cs.LG

TL;DR: 论文提出了一种基于Python的统一体素方法evoxels，结合3D显微镜数据、物理模拟、逆建模和机器学习，以加速材料科学中的逆向设计。


<details>
  <summary>Details</summary>
Motivation: 材料科学需要跨学科整合，尤其是实验与理论的结合，以实现从性能需求出发的逆向材料设计。

Method: 采用可微分物理框架evoxels，统一处理3D显微镜数据、物理模拟和机器学习优化。

Result: 该方法能够高效链接材料加工、结构与性能的关系，加速发现过程。

Conclusion: evoxels框架为材料科学的逆向设计提供了高效工具，促进了跨学科整合。

Abstract: Materials science inherently spans disciplines: experimentalists use advanced
microscopy to uncover micro- and nanoscale structure, while theorists and
computational scientists develop models that link processing, structure, and
properties. Bridging these domains is essential for inverse material design
where you start from desired performance and work backwards to optimal
microstructures and manufacturing routes. Integrating high-resolution imaging
with predictive simulations and data-driven optimization accelerates discovery
and deepens understanding of process-structure-property relationships. The
differentiable physics framework evoxels is based on a fully Pythonic, unified
voxel-based approach that integrates segmented 3D microscopy data, physical
simulations, inverse modeling, and machine learning.

</details>


### [68] [TempRe: Template generation for single and direct multi-step retrosynthesis](https://arxiv.org/abs/2507.21762)
*Nguyen Xuan-Vu,Daniel Armstrong,Zlatko Joncev,Philippe Schwaller*

Main category: cs.LG

TL;DR: TempRe是一种生成框架，将基于模板的方法重新定义为序列生成，实现了可扩展、灵活且化学合理的逆合成分析。


<details>
  <summary>Details</summary>
Motivation: 逆合成规划在分子发现中是一个核心挑战，传统方法存在可扩展性和泛化性不足的问题，而模板无关的生成方法可能产生无效反应。

Method: 提出TempRe框架，将基于模板的方法转化为序列生成，支持单步和多步逆合成任务。

Result: 在PaRoutes多步基准测试中，TempRe表现出色，优于模板分类和SMILES生成方法。

Conclusion: TempRe展示了模板生成建模在计算机辅助合成规划中的潜力。

Abstract: Retrosynthesis planning remains a central challenge in molecular discovery
due to the vast and complex chemical reaction space. While traditional
template-based methods offer tractability, they suffer from poor scalability
and limited generalization, and template-free generative approaches risk
generating invalid reactions. In this work, we propose TempRe, a generative
framework that reformulates template-based approaches as sequence generation,
enabling scalable, flexible, and chemically plausible retrosynthesis. We
evaluated TempRe across single-step and multi-step retrosynthesis tasks,
demonstrating its superiority over both template classification and
SMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe
achieves strong top-k route accuracy. Furthermore, we extend TempRe to direct
multi-step synthesis route generation, providing a lightweight and efficient
alternative to conventional single-step and search-based approaches. These
results highlight the potential of template generative modeling as a powerful
paradigm in computer-aided synthesis planning.

</details>


### [69] [Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer](https://arxiv.org/abs/2507.21799)
*Xie Zhang,Yina Wang,Chenshu Wu*

Main category: cs.LG

TL;DR: RF-CRATE是一种数学可解释的深度网络架构，用于RF传感，基于复杂稀疏率降低原理，性能与黑盒模型相当，同时提供完全可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度无线传感（DWS）模型多为黑盒，缺乏可解释性，限制了其通用性并引发安全问题。

Method: 通过非平凡的理论推导，将实值白盒变换器扩展到复数域，构建完全复数域的白盒变换器，并引入子空间正则化增强特征多样性。

Result: RF-CRATE在多个传感任务中性能提升19.98%，分类增益5.08%，回归误差降低10.34%。

Conclusion: RF-CRATE不仅性能优异，还提供了数学可解释性，为RF传感领域提供了新的解决方案。

Abstract: The empirical success of deep learning has spurred its application to the
radio-frequency (RF) domain, leading to significant advances in Deep Wireless
Sensing (DWS). However, most existing DWS models function as black boxes with
limited interpretability, which hampers their generalizability and raises
concerns in security-sensitive physical applications. In this work, inspired by
the remarkable advances of white-box transformers, we present RF-CRATE, the
first mathematically interpretable deep network architecture for RF sensing,
grounded in the principles of complex sparse rate reduction. To accommodate the
unique RF signals, we conduct non-trivial theoretical derivations that extend
the original real-valued white-box transformer to the complex domain. By
leveraging the CR-Calculus framework, we successfully construct a fully
complex-valued white-box transformer with theoretically derived self-attention
and residual multi-layer perceptron modules. Furthermore, to improve the
model's ability to extract discriminative features from limited wireless data,
we introduce Subspace Regularization, a novel regularization strategy that
enhances feature diversity, resulting in an average performance improvement of
19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against
seven baselines with multiple public and self-collected datasets involving
different RF signals. The results show that RF-CRATE achieves performance on
par with thoroughly engineered black-box models, while offering full
mathematical interpretability. More importantly, by extending CRATE to the
complex domain, RF-CRATE yields substantial improvements, achieving an average
classification gain of 5.08% and reducing regression error by 10.34% across
diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:
https://github.com/rfcrate/RF_CRATE.

</details>


### [70] [Bayesian Neural Network Surrogates for Bayesian Optimization of Carbon Capture and Storage Operations](https://arxiv.org/abs/2507.21803)
*Sofianos Panagiotis Fotias,Vassilis Gaganis*

Main category: cs.LG

TL;DR: 本文通过贝叶斯优化方法比较了碳捕获与封存（CCS）项目中的决策变量优化策略，探讨了高斯过程之外的随机模型在复杂环境中的表现，并证明了其在提高经济可行性和可持续性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 碳捕获与封存（CCS）是实现可持续发展的重要技术，但其优化决策变量的方法需要改进，尤其是在高斯过程表现不佳的复杂环境中。

Method: 采用贝叶斯优化框架，比较了高斯过程与其他新型随机模型在CCS项目优化中的表现，重点关注决策变量多或目标函数尺度不同的情况。

Result: 研究表明，在复杂环境中使用更高级的随机模型可以提高优化效果，并通过净现值（NPV）证明了其经济可行性。

Conclusion: 贝叶斯优化及其新型随机模型在CCS项目中的应用展示了其在能源行业可持续发展中的潜力，为未来研究提供了新方向。

Abstract: Carbon Capture and Storage (CCS) stands as a pivotal technology for fostering
a sustainable future. The process, which involves injecting supercritical
CO$_2$ into underground formations, a method already widely used for Enhanced
Oil Recovery, serves a dual purpose: it not only curbs CO$_2$ emissions and
addresses climate change but also extends the operational lifespan and
sustainability of oil fields and platforms, easing the shift toward greener
practices. This paper delivers a thorough comparative evaluation of strategies
for optimizing decision variables in CCS project development, employing a
derivative-free technique known as Bayesian Optimization. In addition to
Gaussian Processes, which usually serve as the gold standard in BO, various
novel stochastic models were examined and compared within a BO framework. This
research investigates the effectiveness of utilizing more exotic stochastic
models than GPs for BO in environments where GPs have been shown to
underperform, such as in cases with a large number of decision variables or
multiple objective functions that are not similarly scaled. By incorporating
Net Present Value (NPV) as a key objective function, the proposed framework
demonstrates its potential to improve economic viability while ensuring the
sustainable deployment of CCS technologies. Ultimately, this study represents
the first application in the reservoir engineering industry of the growing body
of BO research, specifically in the search for more appropriate stochastic
models, highlighting its potential as a preferred method for enhancing
sustainability in the energy sector.

</details>


### [71] [Analysis of Fourier Neural Operators via Effective Field Theory](https://arxiv.org/abs/2507.21833)
*Taeyoung Kim*

Main category: cs.LG

TL;DR: 本文通过有效场论分析Fourier神经算子（FNOs），揭示了其非线性激活如何耦合频率输入到高频模式，并提出了权重初始化的临界条件。


<details>
  <summary>Details</summary>
Motivation: 研究FNOs的稳定性、泛化性和频率行为缺乏理论解释，旨在填补这一空白。

Method: 采用无限维函数空间的有效场论分析，推导层核和四点顶点的闭式递推关系，并研究三种实际场景。

Result: 理论表明非线性激活会耦合频率输入到高频模式，实验验证了频率转移现象；同时提出了权重初始化的临界条件。

Conclusion: 非线性激活使FNOs能捕捉非平凡特征，临界分析为超参数选择提供依据，解释了尺度不变激活和残差连接对特征学习的增强作用。

Abstract: Fourier Neural Operators (FNOs) have emerged as leading surrogates for
high-dimensional partial-differential equations, yet their stability,
generalization and frequency behavior lack a principled explanation. We present
the first systematic effective-field-theory analysis of FNOs in an
infinite-dimensional function space, deriving closed recursion relations for
the layer kernel and four-point vertex and then examining three practically
important settings-analytic activations, scale-invariant cases and
architectures with residual connections. The theory shows that nonlinear
activations inevitably couple frequency inputs to high-frequency modes that are
otherwise discarded by spectral truncation, and experiments confirm this
frequency transfer. For wide networks we obtain explicit criticality conditions
on the weight-initialization ensemble that keep small input perturbations to
have uniform scale across depth, and empirical tests validate these
predictions. Taken together, our results quantify how nonlinearity enables
neural operators to capture non-trivial features, supply criteria for
hyper-parameter selection via criticality analysis, and explain why
scale-invariant activations and residual connections enhance feature learning
in FNOs.

</details>


### [72] [Discovering Interpretable Ordinary Differential Equations from Noisy Data](https://arxiv.org/abs/2507.21841)
*Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: 提出一种无监督参数估计方法，通过近似解和样条变换线性估计ODE系数，实现高精度和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏物理意义和可解释性，无法反映真实物理系统。

Method: 先找到近似通解，再通过样条变换线性估计ODE系数。

Result: 方法能高精度发现ODE，且对噪声数据鲁棒。

Conclusion: 该方法适合将数据驱动技术应用于真实实验环境。

Abstract: The data-driven discovery of interpretable models approximating the
underlying dynamics of a physical system has gained attraction in the past
decade. Current approaches employ pre-specified functional forms or basis
functions and often result in models that lack physical meaning and
interpretability, let alone represent the true physics of the system. We
propose an unsupervised parameter estimation methodology that first finds an
approximate general solution, followed by a spline transformation to linearly
estimate the coefficients of the governing ordinary differential equation
(ODE). The approximate general solution is postulated using the same functional
form as the analytical solution of a general homogeneous, linear,
constant-coefficient ODE. An added advantage is its ability to produce a
high-fidelity, smooth functional form even in the presence of noisy data. The
spline approximation obtains gradient information from the functional form
which are linearly independent and creates the basis of the gradient matrix.
This gradient matrix is used in a linear system to find the coefficients of the
ODEs. From the case studies, we observed that our modeling approach discovers
ODEs with high accuracy and also promotes sparsity in the solution without
using any regularization techniques. The methodology is also robust to noisy
data and thus allows the integration of data-driven techniques into real
experimental setting for data-driven learning of physical phenomena.

</details>


### [73] [Cardiovascular Disease Prediction using Machine Learning: A Comparative Analysis](https://arxiv.org/abs/2507.21898)
*Risshab Srinivas Ramesh,Roshani T S Udupa,Monisha J,Kushi K K S*

Main category: cs.LG

TL;DR: 该研究通过统计分析68,119条心血管疾病（CVD）数据，发现年龄、血压和胆固醇是主要风险因素，而体育活动是保护因素。CatBoost模型表现最佳，但数据预处理需改进。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，研究旨在探索数值和分类因素对CVD发生的影响。

Method: 使用t检验、卡方检验、ANOVA等统计方法分析数据，并构建逻辑回归模型和CatBoost模型。

Result: 年龄、高血压、高体重和异常胆固醇与CVD强相关，CatBoost模型准确率为0.734。

Conclusion: 研究强调了数据预处理的重要性，并指出CatBoost在预测中的优越性。

Abstract: Cardiovascular diseases (CVDs) are a main cause of mortality globally,
accounting for 31% of all deaths. This study involves a cardiovascular disease
(CVD) dataset comprising 68,119 records to explore the influence of numerical
(age, height, weight, blood pressure, BMI) and categorical gender, cholesterol,
glucose, smoking, alcohol, activity) factors on CVD occurrence. We have
performed statistical analyses, including t-tests, Chi-square tests, and ANOVA,
to identify strong associations between CVD and elderly people, hypertension,
higher weight, and abnormal cholesterol levels, while physical activity (a
protective factor). A logistic regression model highlights age, blood pressure,
and cholesterol as primary risk factors, with unexpected negative associations
for smoking and alcohol, suggesting potential data issues. Model performance
comparisons reveal CatBoost as the top performer with an accuracy of 0.734 and
an ECE of 0.0064 and excels in probabilistic prediction (Brier score = 0.1824).
Data challenges, including outliers and skewed distributions, indicate a need
for improved preprocessing to enhance predictive reliability.

</details>


### [74] [Multi-state Protein Design with DynamicMPNN](https://arxiv.org/abs/2507.21938)
*Alex Abrudan,Sebastian Pujalte Ojeda,Chaitanya K. Joshi,Matthew Greenig,Felipe Engelberger,Alena Khmelinskaia,Jens Meiler,Michele Vendruscolo,Tuomas P. J. Knowles*

Main category: cs.LG

TL;DR: DynamicMPNN是一种逆向折叠模型，通过联合学习多构象集合，显著提升了多状态蛋白质设计的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖单状态预测的聚合，实验成功率低，而多构象蛋白质在生物过程中至关重要。

Method: DynamicMPNN通过联合学习46,033个构象对，覆盖75%的CATH超家族，利用AlphaFold初始猜测进行评估。

Result: DynamicMPNN在结构标准化RMSD上比ProteinMPNN提升高达13%。

Conclusion: DynamicMPNN为多构象蛋白质设计提供了更高效的解决方案。

Abstract: Structural biology has long been dominated by the one sequence, one
structure, one function paradigm, yet many critical biological processes - from
enzyme catalysis to membrane transport - depend on proteins that adopt multiple
conformational states. Existing multi-state design approaches rely on post-hoc
aggregation of single-state predictions, achieving poor experimental success
rates compared to single-state design. We introduce DynamicMPNN, an inverse
folding model explicitly trained to generate sequences compatible with multiple
conformations through joint learning across conformational ensembles. Trained
on 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated
using AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13%
on structure-normalized RMSD across our challenging multi-state protein
benchmark.

</details>


### [75] [SLA-Centric Automated Algorithm Selection Framework for Cloud Environments](https://arxiv.org/abs/2507.21963)
*Siana Rizwan,Tasnim Ahmed,Salimur Choudhury*

Main category: cs.LG

TL;DR: 提出了一种基于SLA的自动化算法选择框架，用于资源受限的云计算环境中的组合优化问题，并通过机器学习模型预测性能。


<details>
  <summary>Details</summary>
Motivation: SLA违规会影响云服务效率与提供商利润，因此需要一种自动化方法来优化算法选择以满足SLA约束。

Method: 使用机器学习模型集成预测性能，并基于SLA约束对算法-硬件对进行排序；应用于0-1背包问题。

Result: 通过分类和回归任务评估框架，并研究了超参数、学习方法和大型语言模型在回归中的效果。

Conclusion: 框架能有效优化算法选择，满足SLA约束，提升云服务效率。

Abstract: Cloud computing offers on-demand resource access, regulated by Service-Level
Agreements (SLAs) between consumers and Cloud Service Providers (CSPs). SLA
violations can impact efficiency and CSP profitability. In this work, we
propose an SLA-aware automated algorithm-selection framework for combinatorial
optimization problems in resource-constrained cloud environments. The framework
uses an ensemble of machine learning models to predict performance and rank
algorithm-hardware pairs based on SLA constraints. We also apply our framework
to the 0-1 knapsack problem. We curate a dataset comprising instance specific
features along with memory usage, runtime, and optimality gap for 6 algorithms.
As an empirical benchmark, we evaluate the framework on both classification and
regression tasks. Our ablation study explores the impact of hyperparameters,
learning approaches, and large language models effectiveness in regression, and
SHAP-based interpretability.

</details>


### [76] [Improving Generative Ad Text on Facebook using Reinforcement Learning](https://arxiv.org/abs/2507.21983)
*Daniel R. Jiang,Alex Nikulkov,Yu-Chia Chen,Yang Bai,Zheqing Zhu*

Main category: cs.LG

TL;DR: 研究探讨了强化学习（RL）后训练对大型语言模型（LLMs）的经济影响，通过Meta的广告生成工具AdLlama展示了RLPF方法的有效性，显著提升了广告点击率。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习后训练对LLMs在现实任务中的经济影响，填补这一领域的量化研究空白。

Method: 提出RLPF（基于性能反馈的强化学习）方法，利用历史广告数据作为奖励信号训练AdLlama模型，并进行大规模A/B测试。

Result: AdLlama将广告点击率提升6.7%，广告主满意度提高，生成更多广告变体。

Conclusion: RLPF是一种有前景的通用方法，能够将语言模型能力转化为实际成果，为生成式AI的经济影响提供了重要数据支持。

Abstract: Generative artificial intelligence (AI), in particular large language models
(LLMs), is poised to drive transformative economic change. LLMs are pre-trained
on vast text data to learn general language patterns, but a subsequent
post-training phase is critical to align them for specific real-world tasks.
Reinforcement learning (RL) is the leading post-training technique, yet its
economic impact remains largely underexplored and unquantified. We examine this
question through the lens of the first deployment of an RL-trained LLM for
generative advertising on Facebook. Integrated into Meta's Text Generation
feature, our model, "AdLlama," powers an AI tool that helps advertisers create
new variations of human-written ad text. To train this model, we introduce
reinforcement learning with performance feedback (RLPF), a post-training method
that uses historical ad performance data as a reward signal. In a large-scale
10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad
variations, we find that AdLlama improves click-through rates by 6.7%
(p=0.0296) compared to a supervised imitation model trained on curated ads.
This represents a substantial improvement in advertiser return on investment on
Facebook. We also find that advertisers who used AdLlama generated more ad
variations, indicating higher satisfaction with the model's outputs. To our
knowledge, this is the largest study to date on the use of generative AI in an
ecologically valid setting, offering an important data point quantifying the
tangible impact of RL post-training. Furthermore, the results show that RLPF is
a promising and generalizable approach for metric-driven post-training that
bridges the gap between highly capable language models and tangible outcomes.

</details>


### [77] [Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation](https://arxiv.org/abs/2507.21992)
*Siddhartha Pradhan,Shikshya Shiwakoti,Neha Bathuri*

Main category: cs.LG

TL;DR: 研究通过多教师知识蒸馏（KD）提升对抗样本的可迁移性，学生模型在攻击成功率和效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索知识蒸馏是否可以从多个异构教师模型中提取知识，以提升对抗样本的可迁移性。

Method: 使用ResNet50和DenseNet-161作为教师模型，通过课程切换和联合优化策略训练学生模型，生成对抗样本并评估其效果。

Result: 学生模型的攻击成功率与集成方法相当，生成时间减少六倍，低温和硬标签监督显著提升可迁移性。

Conclusion: 知识蒸馏不仅能压缩模型，还能高效提升黑盒对抗攻击的效果。

Abstract: We investigate whether knowledge distillation (KD) from multiple
heterogeneous teacher models can enhance the generation of transferable
adversarial examples. A lightweight student model is trained using two KD
strategies: curriculum-based switching and joint optimization, with ResNet50
and DenseNet-161 as teachers. The trained student is then used to generate
adversarial examples using FG, FGS, and PGD attacks, which are evaluated
against a black-box target model (GoogLeNet). Our results show that student
models distilled from multiple teachers achieve attack success rates comparable
to ensemble-based baselines, while reducing adversarial example generation time
by up to a factor of six. An ablation study further reveals that lower
temperature settings and the inclusion of hard-label supervision significantly
enhance transferability. These findings suggest that KD can serve not only as a
model compression technique but also as a powerful tool for improving the
efficiency and effectiveness of black-box adversarial attacks.

</details>


### [78] [Classification of Honey Botanical and Geographical Sources using Mineral Profiles and Machine Learning](https://arxiv.org/abs/2507.22032)
*Mokhtar Al-Awadhi,Ratnadeep Deshmukh*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的蜂蜜花源和地理来源识别方法，通过矿物质元素谱实现分类。


<details>
  <summary>Details</summary>
Motivation: 利用蜂蜜中的矿物质元素谱信息，区分不同花源和地理来源，为蜂蜜质量控制和溯源提供技术支持。

Method: 方法分为预处理（缺失值处理和数据标准化）和分类（多种监督分类模型）两步。

Result: 实验结果表明矿物质元素谱对蜂蜜分类有效，随机森林（RF）分类器表现最佳，花源和地理来源分类准确率分别为99.30%和98.01%。

Conclusion: 矿物质元素谱可用于蜂蜜花源和地理来源的高精度分类，RF分类器在此任务中表现最优。

Abstract: This paper proposes a machine learning-based approach for identifying honey
floral and geographical sources using mineral element profiles. The proposed
method comprises two steps: preprocessing and classification. The preprocessing
phase involves missing-value treatment and data normalization. In the
classification phase, we employ various supervised classification models for
discriminating between six botanical sources and 13 geographical origins of
honey. We test the classifiers' performance on a publicly available honey
mineral element dataset. The dataset contains mineral element profiles of
honeys from various floral and geographical origins. Results show that mineral
element content in honey provides discriminative information useful for
classifying honey botanical and geographical sources. Results also show that
the Random Forests (RF) classifier obtains the best performance on this
dataset, achieving a cross-validation accuracy of 99.30% for classifying honey
botanical origins and 98.01% for classifying honey geographical origins.

</details>


### [79] [Structure-Informed Deep Reinforcement Learning for Inventory Management](https://arxiv.org/abs/2507.22040)
*Alvaro Maggiar,Sohrab Andaz,Akhil Bagaria,Carson Eisenach,Dean Foster,Omer Gottesman,Dominique Perrault-Joncas*

Main category: cs.LG

TL;DR: 该论文研究了深度强化学习（DRL）在经典库存管理问题中的应用，重点关注实际实施考虑。通过DirectBackprop算法，论文在多种库存管理场景中验证了DRL的性能，并提出了Structure-Informed Policy Network技术以提升策略的可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统库存管理方法依赖于对需求分布或参数的假设，而DRL仅需历史数据即可学习策略，避免了不现实的假设。

Method: 采用基于DirectBackprop的DRL算法，应用于多周期系统、易腐库存管理等场景，并提出Structure-Informed Policy Network技术。

Result: DRL在多种场景中表现优于或与现有基准方法相当，且无需大量参数调整。学习到的策略自然捕捉了传统方法的最优结构特性。

Conclusion: 论文通过结合数据驱动学习和分析洞察，填补了库存管理中理论与实践的差距，同时保持了实际适用性。

Abstract: This paper investigates the application of Deep Reinforcement Learning (DRL)
to classical inventory management problems, with a focus on practical
implementation considerations. We apply a DRL algorithm based on DirectBackprop
to several fundamental inventory management scenarios including multi-period
systems with lost sales (with and without lead times), perishable inventory
management, dual sourcing, and joint inventory procurement and removal. The DRL
approach learns policies across products using only historical information that
would be available in practice, avoiding unrealistic assumptions about demand
distributions or access to distribution parameters. We demonstrate that our
generic DRL implementation performs competitively against or outperforms
established benchmarks and heuristics across these diverse settings, while
requiring minimal parameter tuning. Through examination of the learned
policies, we show that the DRL approach naturally captures many known
structural properties of optimal policies derived from traditional operations
research methods. To further improve policy performance and interpretability,
we propose a Structure-Informed Policy Network technique that explicitly
incorporates analytically-derived characteristics of optimal policies into the
learning process. This approach can help interpretability and add robustness to
the policy in out-of-sample performance, as we demonstrate in an example with
realistic demand data. Finally, we provide an illustrative application of DRL
in a non-stationary setting. Our work bridges the gap between data-driven
learning and analytical insights in inventory management while maintaining
practical applicability.

</details>


### [80] [Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling](https://arxiv.org/abs/2507.22045)
*Haley Rosso,Lars Ruthotto,Khachik Sargsyan*

Main category: cs.LG

TL;DR: 论文研究了在神经ODE和ResNet架构中，使用多项式基函数（如Legendre多项式）参数化时间依赖权重的方法，发现Legendre基在训练稳定性、计算成本和准确性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决连续时间深度学习模型（如神经ODE）在训练中学习表达力强且稳定的时间依赖权重的挑战。

Method: 评估了多项式基函数（包括单项式和Legendre多项式）在神经ODE和ResNet架构中的表现，比较了离散优化和优化离散两种训练范式。

Result: 实验结果表明，Legendre基在训练稳定性、计算成本和准确性方面优于单项式基和无约束权重模型。

Conclusion: 选择正交多项式基（如Legendre）在模型表达力和训练效率之间提供了更好的平衡。

Abstract: Continuous-time deep learning models, such as neural ordinary differential
equations (ODEs), offer a promising framework for surrogate modeling of complex
physical systems. A central challenge in training these models lies in learning
expressive yet stable time-varying weights, particularly under computational
constraints. This work investigates weight parameterization strategies that
constrain the temporal evolution of weights to a low-dimensional subspace
spanned by polynomial basis functions. We evaluate both monomial and Legendre
polynomial bases within neural ODE and residual network (ResNet) architectures
under discretize-then-optimize and optimize-then-discretize training paradigms.
Experimental results across three high-dimensional benchmark problems show that
Legendre parameterizations yield more stable training dynamics, reduce
computational cost, and achieve accuracy comparable to or better than both
monomial parameterizations and unconstrained weight models. These findings
elucidate the role of basis choice in time-dependent weight parameterization
and demonstrate that using orthogonal polynomial bases offers a favorable
tradeoff between model expressivity and training efficiency.

</details>


### [81] [Foundation Models for Demand Forecasting via Dual-Strategy Ensembling](https://arxiv.org/abs/2507.22053)
*Wei Yang,Defu Cao,Yan Liu*

Main category: cs.LG

TL;DR: 提出一种统一的集成框架，结合层次集成和架构集成策略，提升销售预测性能。


<details>
  <summary>Details</summary>
Motivation: 需求预测对供应链优化至关重要，但由于层次复杂性、领域转移和外部因素变化，实际应用中仍具挑战性。现有基础模型在时间序列预测中表现有限。

Method: 采用层次集成（HE）和架构集成（AE）策略，分别捕捉局部模式和集成多样化模型预测以提高稳定性。

Result: 在M5基准和三个外部销售数据集上验证，方法显著优于基线，提升了跨层次预测准确性。

Conclusion: 该框架为复杂预测环境中的泛化性能提升提供了简单有效的解决方案。

Abstract: Accurate demand forecasting is critical for supply chain optimization, yet
remains difficult in practice due to hierarchical complexity, domain shifts,
and evolving external factors. While recent foundation models offer strong
potential for time series forecasting, they often suffer from architectural
rigidity and limited robustness under distributional change. In this paper, we
propose a unified ensemble framework that enhances the performance of
foundation models for sales forecasting in real-world supply chains. Our method
combines two complementary strategies: (1) Hierarchical Ensemble (HE), which
partitions training and inference by semantic levels (e.g., store, category,
department) to capture localized patterns; and (2) Architectural Ensemble (AE),
which integrates predictions from diverse model backbones to mitigate bias and
improve stability. We conduct extensive experiments on the M5 benchmark and
three external sales datasets, covering both in-domain and zero-shot
forecasting. Results show that our approach consistently outperforms strong
baselines, improves accuracy across hierarchical levels, and provides a simple
yet effective mechanism for boosting generalization in complex forecasting
environments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [82] [Graph neural networks for residential location choice: connection to classical logit models](https://arxiv.org/abs/2507.21334)
*Zhanhong Cheng,Lingqian Hu,Yuheng Bu,Yuqi Zhou,Shenhao Wang*

Main category: stat.ML

TL;DR: 论文提出了一种基于图神经网络（GNN）的离散选择模型（GNN-DCMs），用于解决传统深度学习方法无法显式捕捉选择替代项之间关系的问题。该模型在理论和实证上均表现出色，并提供了清晰的模型解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在离散选择分析中无法显式捕捉替代项之间的关系，而这是经典离散选择模型的核心关注点。

Method: 引入图神经网络（GNN）作为新框架，构建GNN-DCMs，通过消息传递捕捉空间替代项之间的依赖关系。

Result: GNN-DCMs在芝加哥77个社区区域的住宅选址预测中表现优于MNL、SCL和前馈神经网络，并能捕捉个体异质性和空间感知的替代模式。

Conclusion: GNN-DCMs为复杂空间选择背景下离散选择建模与深度学习的结合提供了统一且表达力强的框架。

Abstract: Researchers have adopted deep learning for classical discrete choice analysis
as it can capture complex feature relationships and achieve higher predictive
performance. However, the existing deep learning approaches cannot explicitly
capture the relationship among choice alternatives, which has been a
long-lasting focus in classical discrete choice models. To address the gap,
this paper introduces Graph Neural Network (GNN) as a novel framework to
analyze residential location choice. The GNN-based discrete choice models
(GNN-DCMs) offer a structured approach for neural networks to capture
dependence among spatial alternatives, while maintaining clear connections to
classical random utility theory. Theoretically, we demonstrate that the
GNN-DCMs incorporate the nested logit (NL) model and the spatially correlated
logit (SCL) model as two specific cases, yielding novel algorithmic
interpretation through message passing among alternatives' utilities.
Empirically, the GNN-DCMs outperform benchmark MNL, SCL, and feedforward neural
networks in predicting residential location choices among Chicago's 77
community areas. Regarding model interpretation, the GNN-DCMs can capture
individual heterogeneity and exhibit spatially-aware substitution patterns.
Overall, these results highlight the potential of GNN-DCMs as a unified and
expressive framework for synergizing discrete choice modeling and deep learning
in the complex spatial choice contexts.

</details>


### [83] [From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions](https://arxiv.org/abs/2507.21429)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 论文通过证明在NTK稳定性假设下，非凸损失函数区域满足局部PL条件，解释了梯度下降在深度学习中观察到的指数收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决梯度下降在深度神经网络非凸损失函数中收敛速率理论与实际观察不符的问题。

Method: 引入局部PL区域（LPLR）概念，证明有限宽度网络在初始化附近存在LPLR，并分析梯度下降在LPLR内的线性收敛性。

Result: 实验验证了LPLR结构在多种深度学习场景中的鲁棒性，理论首次匹配了实际观察到的收敛速率。

Conclusion: 通过NTK框架将局部几何与快速优化联系起来，为梯度下降在深度学习中的高效性提供了理论解释。

Abstract: The convergence of gradient descent (GD) on the non-convex loss landscapes of
deep neural networks (DNNs) presents a fundamental theoretical challenge. While
recent work has established that GD converges to a stationary point at a
sublinear rate within locally quasi-convex regions (LQCRs), this fails to
explain the exponential convergence rates consistently observed in practice. In
this paper, we resolve this discrepancy by proving that under a mild assumption
on Neural Tangent Kernel (NTK) stability, these same regions satisfy a local
Polyak-Lojasiewicz (PL) condition. We introduce the concept of a Locally
Polyak-Lojasiewicz Region (LPLR), where the squared gradient norm lower-bounds
the suboptimality gap, prove that properly initialized finite-width networks
admit such regions around initialization, and establish that GD achieves linear
convergence within an LPLR, providing the first finite-width guarantee that
matches empirically observed rates. We validate our theory across diverse
settings, from controlled experiments on fully-connected networks to modern
ResNet architectures trained with stochastic methods, demonstrating that LPLR
structure emerges robustly in practical deep learning scenarios. By rigorously
connecting local landscape geometry to fast optimization through the NTK
framework, our work provides a definitive theoretical explanation for the
remarkable efficiency of gradient-based optimization in deep learning.

</details>


### [84] [Measuring Sample Quality with Copula Discrepancies](https://arxiv.org/abs/2507.21434)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: 论文提出了一种名为Copula Discrepancy (CD)的诊断工具，用于评估近似MCMC采样器的依赖结构准确性，解决了传统方法在偏置采样器中的失效问题。


<details>
  <summary>Details</summary>
Motivation: 现代贝叶斯机器学习中的可扩展MCMC算法（如SGLD）牺牲了渐近精确性以换取计算速度，导致传统样本质量诊断方法在偏置采样器中失效。需要一种新的诊断工具来评估依赖结构的准确性。

Method: 基于Sklar定理，提出Copula Discrepancy (CD)，通过分离和量化样本的依赖结构来评估其准确性。该方法包括基于矩的CD和基于MLE的变体。

Result: 实验表明，CD在超参数选择中显著优于传统方法（如有效样本量），并能检测传统方法无法发现的尾部依赖差异。

Conclusion: CD不仅为MCMC实践者提供了实用的诊断工具，还为下一代结构感知样本质量评估奠定了理论基础。

Abstract: The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern
Bayesian machine learning, such as Stochastic Gradient Langevin Dynamics
(SGLD), sacrifice asymptotic exactness for computational speed, creating a
critical diagnostic gap: traditional sample quality measures fail
catastrophically when applied to biased samplers. While powerful Stein-based
diagnostics can detect distributional mismatches, they provide no direct
assessment of dependence structure, often the primary inferential target in
multivariate problems. We introduce the Copula Discrepancy (CD), a principled
and computationally efficient diagnostic that leverages Sklar's theorem to
isolate and quantify the fidelity of a sample's dependence structure
independent of its marginals. Our theoretical framework provides the first
structure-aware diagnostic specifically designed for the era of approximate
inference. Empirically, we demonstrate that a moment-based CD dramatically
outperforms standard diagnostics like effective sample size for hyperparameter
selection in biased MCMC, correctly identifying optimal configurations where
traditional methods fail. Furthermore, our robust MLE-based variant can detect
subtle but critical mismatches in tail dependence that remain invisible to rank
correlation-based approaches, distinguishing between samples with identical
Kendall's tau but fundamentally different extreme-event behavior. With
computational overhead orders of magnitude lower than existing Stein
discrepancies, the CD provides both immediate practical value for MCMC
practitioners and a theoretical foundation for the next generation of
structure-aware sample quality assessment.

</details>


### [85] [From Global to Local: A Scalable Benchmark for Local Posterior Sampling](https://arxiv.org/abs/2507.21449)
*Rohan Hitchcock,Jesse Hoogland*

Main category: stat.ML

TL;DR: 论文探讨了随机梯度MCMC（SGMCMC）算法在神经网络退化损失景观中的表现，提出从全局收敛转向局部后验采样，并引入新基准评估局部采样性能。


<details>
  <summary>Details</summary>
Motivation: 理解SGMCMC算法如何与退化损失景观交互，填补当前全局收敛假设与退化景观不兼容的理论空白。

Method: 引入可扩展的局部采样性能评估基准，测试多种常见SGMCMC算法，重点分析RMSProp预处理的SGLD。

Result: RMSProp预处理的SGLD最能准确反映后验分布的局部几何特性，且在O(100M)参数模型中仍能提取有效局部信息。

Conclusion: 尽管缺乏全局收敛理论保证，局部采样方法在退化景观中表现良好，为后续研究提供了新方向。

Abstract: Degeneracy is an inherent feature of the loss landscape of neural networks,
but it is not well understood how stochastic gradient MCMC (SGMCMC) algorithms
interact with this degeneracy. In particular, current global convergence
guarantees for common SGMCMC algorithms rely on assumptions which are likely
incompatible with degenerate loss landscapes. In this paper, we argue that this
gap requires a shift in focus from global to local posterior sampling, and, as
a first step, we introduce a novel scalable benchmark for evaluating the local
sampling performance of SGMCMC algorithms. We evaluate a number of common
algorithms, and find that RMSProp-preconditioned SGLD is most effective at
faithfully representing the local geometry of the posterior distribution.
Although we lack theoretical guarantees about global sampler convergence, our
empirical results show that we are able to extract non-trivial local
information in models with up to O(100M) parameters.

</details>


### [86] [Stochastic forest transition model dynamics and parameter estimation via deep learning](https://arxiv.org/abs/2507.21486)
*Satoshi Kumabe,Tianyu Song,Ton Viet Ta*

Main category: stat.ML

TL;DR: 本文开发了一个随机微分方程模型来研究森林转型的复杂动态，提出了参数估计的深度学习方法，并分析了模型参数对森林砍伐激励的影响。


<details>
  <summary>Details</summary>
Motivation: 森林转型涉及森林、农业和废弃土地之间的动态变化，研究其复杂动态有助于理解森林砍伐趋势。

Method: 使用随机微分方程模型，并通过深度学习从时间序列数据中估计所有参数。

Result: 建立了模型的全局正解，并通过数值分析评估了参数对森林砍伐的影响。

Conclusion: 提出的深度学习方法能有效预测未来森林转型动态和砍伐趋势。

Abstract: Forest transitions, characterized by dynamic shifts between forest,
agricultural, and abandoned lands, are complex phenomena. This study developed
a stochastic differential equation model to capture the intricate dynamics of
these transitions. We established the existence of global positive solutions
for the model and conducted numerical analyses to assess the impact of model
parameters on deforestation incentives. To address the challenge of parameter
estimation, we proposed a novel deep learning approach that estimates all model
parameters from a single sample containing time-series observations of forest
and agricultural land proportions. This innovative approach enables us to
understand forest transition dynamics and deforestation trends at any future
time.

</details>


### [87] [An Equal-Probability Partition of the Sample Space: A Non-parametric Inference from Finite Samples](https://arxiv.org/abs/2507.21712)
*Urban Eriksson*

Main category: stat.ML

TL;DR: 论文研究了从有限样本中推断任意连续概率分布的性质，发现N个排序样本点将实线划分为N+1段，每段期望概率质量均为1/(N+1)。


<details>
  <summary>Details</summary>
Motivation: 探索如何从有限样本中推断连续概率分布的特性，特别是基于排序样本点的非参数方法。

Method: 利用顺序统计量的基本性质，分析N个排序样本点对实线的划分及其概率质量分布。

Result: 发现每段划分的期望概率质量为1/(N+1)，离散熵为log2(N+1)比特，与Shannon的连续变量结果形成对比。

Conclusion: 该框架为非参数推断（如密度和尾部估计）提供了新的视角，并与传统ECDF进行了比较。

Abstract: This paper investigates what can be inferred about an arbitrary continuous
probability distribution from a finite sample of $N$ observations drawn from
it. The central finding is that the $N$ sorted sample points partition the real
line into $N+1$ segments, each carrying an expected probability mass of exactly
$1/(N+1)$. This non-parametric result, which follows from fundamental
properties of order statistics, holds regardless of the underlying
distribution's shape. This equal-probability partition yields a discrete
entropy of $\log_2(N+1)$ bits, which quantifies the information gained from the
sample and contrasts with Shannon's results for continuous variables. I compare
this partition-based framework to the conventional ECDF and discuss its
implications for robust non-parametric inference, particularly in density and
tail estimation.

</details>


### [88] [MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation](https://arxiv.org/abs/2507.21807)
*Robert Kuchen*

Main category: stat.ML

TL;DR: 论文提出了一种名为MIBoost的新算法，用于在缺失数据情况下统一变量选择机制，扩展了梯度提升框架。


<details>
  <summary>Details</summary>
Motivation: 缺失数据是统计分析中的常见问题，现有方法如多重插补在模型选择中存在不足，需要更高效且易于实现的解决方案。

Method: 通过扩展梯度提升框架，提出MIBoost算法，在多重插补数据集中实现统一的变量选择机制。

Result: 模拟研究表明，MIBoost的预测性能与现有方法相当。

Conclusion: MIBoost为缺失数据下的变量选择提供了一种有效且易于实现的方法。

Abstract: Statistical learning methods for automated variable selection, such as LASSO,
elastic nets, or gradient boosting, have become increasingly popular tools for
building powerful prediction models. Yet, in practice, analyses are often
complicated by missing data. The most widely used approach to address
missingness is multiple imputation, which creates several completed datasets.
However, there is an ongoing debate on how to perform model selection in the
presence of multiple imputed datasets. Simple strategies, such as pooling
models across datasets, have been shown to have suboptimal properties. Although
more sophisticated methods exist, they are often difficult to implement and
therefore not widely applied. In contrast, two recent approaches modify the
regularization methods LASSO and elastic nets by defining a single loss
function, resulting in a unified set of coefficients across imputations. Our
key contribution is to extend this principle to the framework of component-wise
gradient boosting by proposing MIBoost, a novel algorithm that employs a
uniform variable-selection mechanism across imputed datasets. Simulation
studies suggest that our approach yields prediction performance comparable to
that of these recently proposed methods.

</details>
