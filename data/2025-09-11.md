<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.LG](#cs.LG) [Total: 66]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Signals vs. Videos: Advancing Motion Intention Recognition for Human-Robot Collaboration in Construction](https://arxiv.org/abs/2509.07990)
*Charan Gajjala Chenchu,Kinam Kim,Gao Lu,Zia Ud Din*

Main category: eess.SP

TL;DR: 研究比较了表面电机残体画（sEMG）信号和视频序列两种数据模态在早期识别干墙安装任务中工人运动意图的性能。sEMG模型准确率87%速度0.04秒，视频模型准确率94%但需0.15秒。


<details>
  <summary>Details</summary>
Motivation: 人机协作中准确识别人类运动意图对安全和效率至关重要，需要比较不同数据模态的性能以填补研究空白。

Method: 使用CNN-LSTM模型处理sEMG信号，以及预训练的Video Swin Transformer结合迁移学习处理视频序列来识别运动意图。

Result: sEMG模型准确率87%预测时间0.04秒，视频模型准确率94%但需0.15秒。两种模态各有优势和交换。

Conclusion: 研究强调了不同数据模态的特性和特长，为实际建筑项目中的人机协作提供了系统性部署指南。

Abstract: Human-robot collaboration (HRC) in the construction industry depends on
precise and prompt recognition of human motion intentions and actions by robots
to maximize safety and workflow efficiency. There is a research gap in
comparing data modalities, specifically signals and videos, for motion
intention recognition. To address this, the study leverages deep learning to
assess two different modalities in recognizing workers' motion intention at the
early stage of movement in drywall installation tasks. The Convolutional Neural
Network - Long Short-Term Memory (CNN-LSTM) model utilizing surface
electromyography (sEMG) data achieved an accuracy of around 87% with an average
time of 0.04 seconds to perform prediction on a sample input. Meanwhile, the
pre-trained Video Swin Transformer combined with transfer learning harnessed
video sequences as input to recognize motion intention and attained an accuracy
of 94% but with a longer average time of 0.15 seconds for a similar prediction.
This study emphasizes the unique strengths and trade-offs of both data formats,
directing their systematic deployments to enhance HRC in real-world
construction projects.

</details>


### [2] [Privacy Preserving Semantic Communications Using Vision Language Models: A Segmentation and Generation Approach](https://arxiv.org/abs/2509.08142)
*Haoran Chang,Mingzhe Chen,Huaxia Wang,Qianqian Zhang*

Main category: eess.SP

TL;DR: 一种基于视觉-语言模型的隐私保护语义通信框架，通过检测和移除图像中的敏感区域，并利用文本嵌入重构被掩盖区域，提高了通信效率和隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 解决单模态表示在差通道条件下重构质量下降的问题，以及语义信息攻击带来的隐私涉嫌。

Method: 利用视觉-语言模型检测并移除图像中的隐私内容区域，通过共享隐私数据库确保发送端和接收端的语义对齐，在接收端使用生成模块根据接收到的文本嵌入重构被掩盖区域。

Result: 框架能够良好地渐迁到未见的图像处理任务，在权媒接收端通过文本嵌入提高重构质量超10%，并将漏洩到偷听者的身份信息减少超过50%。

Conclusion: 该框架有效地解决了语义通信中的隐私保护问题，在提高通信效率的同时保障敏感信息安全，为下一代无线系统提供了有效的隐私保护方案。

Abstract: Semantic communication has emerged as a promising paradigm for
next-generation wireless systems, improving the communication efficiency by
transmitting high-level semantic features. However, reliance on unimodal
representations can degrade reconstruction under poor channel conditions, and
privacy concerns of the semantic information attack also gain increasing
attention. In this work, a privacy-preserving semantic communication framework
is proposed to protect sensitive content of the image data. Leveraging a
vision-language model (VLM), the proposed framework identifies and removes
private content regions from input images prior to transmission. A shared
privacy database enables semantic alignment between the transmitter and
receiver to ensure consistent identification of sensitive entities. At the
receiver, a generative module reconstructs the masked regions using learned
semantic priors and conditioned on the received text embedding. Simulation
results show that generalizes well to unseen image processing tasks, improves
reconstruction quality at the authorized receiver by over 10% using text
embedding, and reduces identity leakage to the eavesdropper by more than 50%.

</details>


### [3] [RTR: A Transformer-Based Lossless Crossover with Perfect Phase Alignment](https://arxiv.org/abs/2509.08272)
*Xiangying Li,Jiankuan Li,Yong Tang*

Main category: eess.SP

TL;DR: 提出基于变压器的无损分频方法RTR，实现频率分离的同时保证低频和高频通道在分频频率处的完美相位对齐，具有线性互补特性和完美信号重建能力。


<details>
  <summary>Details</summary>
Motivation: 传统LC分频器和数字滤波器存在能量损耗、相位不一致和对元件容差敏感等问题，需要开发低损耗、低延迟的硬件辅助滤波解决方案。

Method: 使用变压器结构实现谐振变压器路由器(RTR)，其频率响应满足线性互补关系HLF(f)+HHF(f)=1，通过理论推导和电路仿真验证。

Result: RTR在能量效率、相位一致性和对元件容差的鲁棒性方面表现优越，相比传统方法提供更好的性能。

Conclusion: RTR为高保真音频和通信前端提供了一种有效的低损耗、低延迟硬件滤波解决方案，基于中国专利技术进行了扩展验证。

Abstract: This paper proposes a transformer-based lossless crossover method, termed
Resonant Transformer Router (RTR), which achieves frequency separation while
ensuring perfect phase alignment between low-frequency (LF) and high-frequency
(HF) channels at the crossover frequency. The core property of RTR is that its
frequency responses satisfy a linear complementary relation HLF(f)+HHF(f)=1. so
that the original signal can be perfectly reconstructed by linear summation of
the two channels. Theoretical derivation and circuit simulations demonstrate
that RTR provides superior energy efficiency, phase consistency, and robustness
against component tolerances. Compared with conventional LC crossovers and
digital FIR/IIR filters, RTR offers a low-loss, low-latency hardware-assisted
filtering solution suitable for high-fidelity audio and communication
front-ends.
  The core theory behind this paper's work, lossless crossover, is based on a
Chinese patent [CN116318117A] developed from the previous research of one of
the authors, Jianluan Li. We provide a comprehensive experimental validation of
this theory and propose a new extension.

</details>


### [4] [Fundamental Trade-off in Wideband Stacked Intelligent Metasurface Assisted OFDMA Systems](https://arxiv.org/abs/2509.08294)
*Zheao Li,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: 提出一种基于堆叠智能超表面(SIM)的全模拟波束赋形系统，通过灵活的子载波分配策略解决宽带多用户OFDMA系统中的干扰问题，实现近零干扰和硬件简化。


<details>
  <summary>Details</summary>
Motivation: 传统数字波束赋形需要大量功耗组件，硬件成本高且复杂。SIM能在近光速下进行波束赋形，但宽带多用户传输中的跨子载波干扰问题尚未解决。

Method: 提出灵活的子载波分配策略，每个子载波选择性地服务一个或多个用户以平衡干扰抑制和资源利用。采用迭代算法联合优化子载波分配矩阵和SIM传输系数。

Result: 系统具有低拟合误差，允许每个用户使用更多子载波，实现近零干扰和鲁棒的数据可靠性，同时避免了数字预编码的硬件负担。

Conclusion: 该方法在SIM辅助的全模拟波束赋形系统中实现了干扰与资源利用的有效平衡，为宽带多用户通信提供了硬件简化且性能优越的解决方案。

Abstract: Conventional digital beamforming for wideband multiuser orthogonal
frequency-division multiplexing (OFDM) demands numerous power-hungry
components, increasing hardware costs and complexity. By contrast, the stacked
intelligent metasurfaces (SIM) can perform wave-based precoding at near-light
speed, drastically reducing baseband overhead. However, realizing SIM-enhanced
fully-analog beamforming for wideband multiuser transmissions remains
challenging, as the SIM configuration has to handle interference across all
subcarriers. To address this, this paper proposes a flexible subcarrier
allocation strategy to fully reap the SIM-assisted fully-analog beamforming
capability in an orthogonal frequency-division multiple access (OFDMA) system,
where each subcarrier selectively serves one or more users to balance
interference mitigation and resource utilization of SIM. We propose an
iterative algorithm to jointly optimize the subcarrier assignment matrix and
SIM transmission coefficients, approximating an interference-free channel for
those selected subcarriers. Results show that the proposed system has low
fitting errors yet allows each user to exploit more subcarriers. Further
comparisons highlight a fundamental trade-off: our system achieves near-zero
interference and robust data reliability without incurring the hardware burdens
of digital precoding.

</details>


### [5] [Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain Location](https://arxiv.org/abs/2509.08432)
*Yingjie Wu,Junshan Luo,Weiyu Chen,Shilian Wang,Fanggang Wang,Haiyang Ding*

Main category: eess.SP

TL;DR: 提出了一种结合流体天线和人工噪声技术的自主飞行器安全通信框架，通过联合优化部署、预编码和天线位置来最大化最差情况下的保密率，有效应对窃听威胁。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线缺乏足够的空间自由度，使得视距主导的自主飞行器通信链路容易受到窃听攻击，需要新的安全增强方案。

Method: 采用流体天线和人工噪声技术，通过凸包处理任意形状的窃听者不确定区域，针对自由移动和区域移动两种模式分别应用不同的优化技术。

Result: 数值结果表明，流体天线方案通过利用额外的空间自由度而非发射功率来提升安全性，人工噪声在高发射功率下提供显著增益，两者协同作用超过各自贡献之和。

Conclusion: 该框架在有限资源下实现了安全性和可靠性的平衡，流体天线与人工噪声的协同作用为自主飞行器安全通信提供了有效的解决方案。

Abstract: For autonomous aerial vehicle (AAV) secure communications, traditional
designs based on fixed position antenna (FPA) lack sufficient spatial degrees
of freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable
to eavesdropping. To overcome this problem, this paper proposes a framework
that effectively incorporates the fluid antenna (FA) and the artificial noise
(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple
eavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN
precoders, and FA positions. In particular, the worst-case MSR is considered by
taking the channel uncertainties due to the uncertainty about eavesdropping
locations into account. To tackle the highly coupled optimization variables and
the channel uncertainties in the formulated problem, an efficient and robust
algorithm is proposed. Particularly, the uncertain regions of eavesdroppers,
whose shapes can be arbitrary, are disposed by constructing convex hull. In
addition, two movement modes of FAs are considered, namely, free movement mode
and zonal movement mode, for which different optimization techniques are
applied, respectively. Numerical results show that, the proposed FA schemes
boost security by exploiting additional spatial DoF rather than transmit power,
while AN provides remarkable gains under high transmit power. Furthermore, the
synergy between FA and AN results in a secure advantage that exceeds the sum of
their individual contributions, achieving a balance between security and
reliability under limited resources.

</details>


### [6] [Information and Communication Theoretical Foundations of the Internet of Plants, Principles, Challenges, and Future Directions](https://arxiv.org/abs/2509.08434)
*Ahmet B. Kilic,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 这篇论文从信息通信技术角度系统分析植物多模态信号交换，提出植物互联网新范式，为精准农业和生态监测提供理论基础


<details>
  <summary>Details</summary>
Motivation: 将植物多模态信号交换（化学、电气、菌根、声学）从信息通信技术角度进行系统分析，补充现有研究的不足

Method: 提供生物学背景知识，将植物信号重新形式化为信息通信框架，综合发射、传播和接收模型，分析实证研究和感知技术

Result: 建立了植物通信的ICT理论框架，识别了各模态的发射-传播-接收模型，为植物互联网提供了理论基础

Conclusion: 通过整合生物学和ICT知识，该研究为通信领域研究者提供了植物通信全面教程，为多学科发展和植物互联网应用研究奠定了基础

Abstract: Plants exchange information through multiple modalities, including chemical,
electrical, mycorrhizal, and acoustic signaling, which collectively support
survival, defense, and adaptation. While these processes are well documented in
biology, their systematic analysis from an Information and Communication
Technology (ICT) perspective remains limited. To address this gap, this article
is presented as a tutorial with survey elements. It provides the necessary
biological background, reformulates inter-plant signaling within ICT
frameworks, and surveys empirical studies to guide future research and
applications. First, the paper introduces the fundamental biological processes
to establish a foundation for readers in communications and networking.
Building on this foundation, existing models of emission, propagation, and
reception are synthesized for each modality and reformulated in terms of
transmitter, channel, and receiver blocks. To complement theory, empirical
studies and state-of-the-art sensing approaches are critically examined.
Looking forward, the paper identifies open challenges and outlines future
research directions, with particular emphasis on the emerging vision of the
Internet of Plants (IoP). This paradigm frames plants as interconnected nodes
within ecological and technological networks, offering new opportunities for
applications in precision agriculture, ecosystem monitoring, climate
resilience, and bio-inspired communication systems. By integrating biological
insights with ICT frameworks and projecting toward the IoP, this article
provides a comprehensive tutorial on plant communication for the communications
research community and establishes a foundation for interdisciplinary advances.

</details>


### [7] [On the Performance of ISAC over the D-Band in a Phase-Noise Aware OFDM Systems](https://arxiv.org/abs/2509.08504)
*Didem Aydogan,Mohaned Chraiti,Korkut Kaan Tokgöz*

Main category: eess.SP

TL;DR: 该论文评估了在130GHz频段下，OFDM ISAC感知性能受相位噪声影响的情况，发现相位噪声会导致距离和速度估计误差平台，以及多普勒旁瓣指标饱和。


<details>
  <summary>Details</summary>
Motivation: D波段(110-170GHz)是5G/6G ISAC系统的有前景候选频段，但相位噪声是该频段的关键损伤因素，需要评估其对感知性能的影响。

Method: 使用硬件调谐的3GPP相位噪声模型在130GHz频段，采用OFDM波形和480kHz numerology，通过FFT雷达处理评估ISAC感知性能。

Result: 相位噪声导致距离RMSE平台为0.04-0.05m，速度RMSE平台为0.12-0.18m/s，多普勒旁瓣指标饱和(PSLR约-6dB，ISLR约-4dB)。

Conclusion: 距离精度仍受带宽限制，而速度估计和旁瓣抑制对相位噪声敏感，强调了相位噪声感知的波形和numerology设计对太赫兹ISAC的重要性。

Abstract: Phase noise (PN) is a critical impairment at D-band frequencies (110 to 170
GHz), which are widely investigated as promising candidates for beyond 5G/6G
ISAC systems. This paper evaluates OFDM based ISAC sensing performance under
realistic oscillator impairments using a hardware-tuned 3GPP PN model at 130
GHz and FFT based radar processing. With a numerology of 480 kHz, results show
that PN introduces range RMSE floors of 0.04 to 0.05 m and velocity RMSE floors
of 0.12 to 0.18 m/s. Doppler sidelobe metrics also saturate, with PSLR around
minus 6 dB and ISLR around minus 4 dB. These findings confirm that range
accuracy remains bandwidth limited, while velocity estimation and sidelobe
suppression are strongly PN-sensitive. The study highlights the importance of
PN-aware waveform and numerology design for sub-THz ISAC and provides insights
for future multi-band transceivers. Communication metrics and PN mitigation
strategies such as PTRS and CPE tracking are left for future work.

</details>


### [8] [Modular PE-Structured Learning for Cross-Task Wireless Communications](https://arxiv.org/abs/2509.08614)
*Yuxuan Duan,Chenyang Yang*

Main category: eess.SP

TL;DR: 提出基于置换等变性的模块化神经网络框架PE-MoFormer，用于高效学习多种无线策略，相比大型模型在学习和推理效率上表现更优


<details>
  <summary>Details</summary>
Motivation: 现有无线策略学习依赖大型DNN模型，难以在无线边缘进行预训练和微调，需要更紧凑高效的解决方案

Method: 利用无线问题的结构化知识（置换等变性质），设计三种PE感知模块（包括Transformer风格子层），通过模块组装构建紧凑DNN

Result: 模拟实验显示该框架在学习和推理效率上优于相关大型模型，具有强泛化能力和低样本空间复杂度

Conclusion: 基于结构化知识的模块化方法为无线通信的跨任务学习提供了新方向，能够高效处理预编码、波束成形、功率分配和信道估计等多种任务

Abstract: Recent trends in learning wireless policies attempt to develop deep neural
networks (DNNs) for handling multiple tasks with a single model. Existing
approaches often rely on large models, which are hard to pre-train and
fine-tune at the wireless edge. In this work, we challenge this paradigm by
leveraging the structured knowledge of wireless problems -- specifically,
permutation equivariant (PE) properties. We design three types of PE-aware
modules, two of which are Transformer-style sub-layers. These modules can serve
as building blocks to assemble compact DNNs applicable to the wireless policies
with various PE properties. To guide the design, we analyze the hypothesis
space associated with each PE property, and show that the PE-structured module
assembly can boost the learning efficiency. Inspired by the reusability of the
modules, we propose PE-MoFormer, a compositional DNN capable of learning a wide
range of wireless policies -- including but not limited to precoding,
coordinated beamforming, power allocation, and channel estimation -- with
strong generalizability, low sample and space complexity. Simulations
demonstrate that the proposed modular PE-based framework outperforms relevant
large model in both learning efficiency and inference time, offering a new
direction for structured cross-task learning for wireless communications.

</details>


### [9] [RIS-Assisted Near-Field ISAC for Multi-Target Indication in NLoS Scenarios](https://arxiv.org/abs/2509.08642)
*Hang Ruan,Homa Nikbakht,Ruizhi Zhang,Honglei Chen,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 提出了一种利用可重构智能表面(RIS)的近场集成感知与通信多目标感知波束成形框架，通过联合优化基站波束成形和RIS相位偏移，在保证通信速率的同时最大化最差情况下的感知性能


<details>
  <summary>Details</summary>
Motivation: 解决近场集成感知与通信系统中多目标感知的关键挑战，特别是在视距路径被阻挡的情况下实现多目标指示

Method: 将经典波束模式增益和跨目标互相关度量扩展到近场，利用角度和距离信息区分多个用户和目标；采用交替优化算法和半定松弛技术解决非凸优化问题

Result: 仿真结果表明，所提出的RIS辅助框架能够在阻挡场景下实现共角度目标的高分辨率感知

Conclusion: 该研究为近场ISAC系统中的多目标感知提供了一种有效的解决方案，特别是在视距路径受阻的情况下，通过RIS辅助实现了优越的感知性能

Abstract: Enabling multi-target sensing in near-field integrated sensing and
communication (ISAC) systems is a key challenge, particularly when
line-of-sight paths are blocked. This paper proposes a beamforming framework
that leverages a reconfigurable intelligent surface (RIS) to achieve
multi-target indication. Our contribution is the extension of classic
beampattern gain and inter-target cross-correlation metrics to the near-field,
leveraging both angle and distance information to discriminate between multiple
users and targets. We formulate a problem to maximize the worst-case sensing
performance by jointly designing the beamforming at the base station and the
phase shifts at the RIS, while guaranteeing communication rates. The non-convex
problem is solved via an efficient alternating optimization (AO) algorithm that
utilizes semidefinite relaxation (SDR). Simulations demonstrate that our
RIS-assisted framework enables high-resolution sensing of co-angle targets in
blocked scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization](https://arxiv.org/abs/2509.07993)
*Federico Fontana,Anxhelo Diko,Romeo Lanzino,Marco Raoul Marini,Bachir Kaddar,Gian Luca Foresti,Luigi Cinque*

Main category: cs.LG

TL;DR: 将深度伪造检测重新定义为持续学习问题，提出高效框架来增量适应新兴伪造技术，同时保留历史知识，训练速度比完全重训练快155倍，但对未来生成器的泛化能力接近随机水平。


<details>
  <summary>Details</summary>
Motivation: 深度伪造生成技术快速演变，传统非持续学习方法需要频繁且昂贵的重训练，无法有效应对新兴伪造技术。

Method: 提出持续学习框架，模拟7年间深度伪造技术的真实时间演变，基于轻量级视觉骨干网络实现实时检测性能，并引入两个新评估指标。

Result: 实现了高效适应（比完全重训练快155倍）和稳健的历史知识保留，但对未来生成器的泛化能力接近随机水平（FWT-AUC≈0.5）。

Conclusion: 提出了非通用深度伪造分布假设，指出当前方法在没有额外训练的情况下对未来生成器的泛化能力有限，每个现有生成器都有独特特征印记。

Abstract: The rapid evolution of deepfake generation technologies poses critical
challenges for detection systems, as non-continual learning methods demand
frequent and expensive retraining. We reframe deepfake detection (DFD) as a
Continual Learning (CL) problem, proposing an efficient framework that
incrementally adapts to emerging visual manipulation techniques while retaining
knowledge of past generators. Our framework, unlike prior approaches that rely
on unreal simulation sequences, simulates the real-world chronological
evolution of deepfake technologies in extended periods across 7 years.
Simultaneously, our framework builds upon lightweight visual backbones to allow
for the real-time performance of DFD systems. Additionally, we contribute two
novel metrics: Continual AUC (C-AUC) for historical performance and Forward
Transfer AUC (FWT-AUC) for future generalization. Through extensive
experimentation (over 600 simulations), we empirically demonstrate that while
efficient adaptation (+155 times faster than full retraining) and robust
retention of historical knowledge is possible, the generalization of current
approaches to future generators without additional training remains near-random
(FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing
generator. Such observations are the foundation of our newly proposed
Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}

</details>


### [11] [How Far Are We from True Unlearnability?](https://arxiv.org/abs/2509.08058)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.LG

TL;DR: 本文发现现有的不可学习样本方法在多任务学习中失效，从模型优化角度分析损失景观与不可学习性的关系，提出SAL指标和UD距离来量化不可学习性，并评估现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有不可学习方法生成的不可学习样本在多任务场景下仍然有效，无法真正保护数据所有者权益，需要探究真正不可学习样本的实现边界。

Method: 通过观察干净模型和中毒模型的收敛过程差异，分析损失景观与不可学习性的关系，提出Sharpness-Aware Learnability (SAL)指标量化参数不可学习性，并基于此定义Unlearnable Distance (UD)来测量数据的不可学习性。

Result: 发现现有不可学习方法在多任务数据集Taskonomy上失效，只有部分关键参数优化路径存在显著差异，损失景观与不可学习性密切相关。

Conclusion: 现有不可学习方法存在局限性，提出的SAL和UD指标能够有效评估不可学习性，促进社区对现有方法能力边界的认识。

Abstract: High-quality data plays an indispensable role in the era of large models, but
the use of unauthorized data for model training greatly damages the interests
of data owners. To overcome this threat, several unlearnable methods have been
proposed, which generate unlearnable examples (UEs) by compromising the
training availability of data. Clearly, due to unknown training purposes and
the powerful representation learning capabilities of existing models, these
data are expected to be unlearnable for models across multiple tasks, i.e.,
they will not help improve the model's performance. However, unexpectedly, we
find that on the multi-task dataset Taskonomy, UEs still perform well in tasks
such as semantic segmentation, failing to exhibit cross-task unlearnability.
This phenomenon leads us to question: How far are we from attaining truly
unlearnable examples? We attempt to answer this question from the perspective
of model optimization. To this end, we observe the difference in the
convergence process between clean and poisoned models using a simple model
architecture. Subsequently, from the loss landscape we find that only a part of
the critical parameter optimization paths show significant differences,
implying a close relationship between the loss landscape and unlearnability.
Consequently, we employ the loss landscape to explain the underlying reasons
for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the
unlearnability of parameters based on this explanation. Furthermore, we propose
an Unlearnable Distance (UD) to measure the unlearnability of data based on the
SAL distribution of parameters in clean and poisoned models. Finally, we
conduct benchmark tests on mainstream unlearnable methods using the proposed
UD, aiming to promote community awareness of the capability boundaries of
existing unlearnable methods.

</details>


### [12] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

TL;DR: JEL是一个新颖的计算高效端到端多神经网络实体链接模型，在性能上超越了当前最先进模型


<details>
  <summary>Details</summary>
Motivation: 知识图谱在整合多源异构数据和捕获实体关系方面具有重要价值，但需要将文本中的提及正确链接到知识图谱中的实体。在JP摩根等金融机构中，新闻分析是重要任务，实体链接能够桥接非结构化新闻文本与知识图谱，为用户提供大量精选数据

Method: 采用端到端多神经网络架构的实体链接模型，具有计算高效的特点

Result: 该模型超越了当前最先进的实体链接模型性能

Conclusion: JEL模型在实体链接任务上表现出色，能够有效支持新闻分析等实际应用场景

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>


### [13] [Performance Assessment Strategies for Generative AI Applications in Healthcare](https://arxiv.org/abs/2509.08087)
*Victor Garcia,Mariia Sidulova,Aldo Badano*

Main category: cs.LG

TL;DR: 这篇论文讨论了医疗健康领域生成式AI应用的评估方法，指出传统量化指标的局限性，并推荐结合人类专业知识和计算模型的评估策略。


<details>
  <summary>Details</summary>
Motivation: 因为生成式AI在医疗领域应用日益普及，但传统的量化指标评估方法存在过拟合问题，影响模型的普适性，需要更全面的评估框架。

Method: 分析当前最先进的评估方法，包括利用人类专业知识和成本效益高的计算模型作为评估器的新兴策略。

Result: 识别了传统量化指标评估的局限性，如模型可能优化在特定测试集上的表现而牺牲了其他任务和数据分布上的普适性。

Conclusion: 医疗健康领域生成式AI应用的评估需要综合考虑临床任务特点和实际应用环境，应采用结合人类专业知识和计算模型的多元化评估方法。

Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm
within artificial intelligence, with applications throughout the medical
enterprise. Assessing GenAI applications necessitates a comprehensive
understanding of the clinical task and awareness of the variability in
performance when implemented in actual clinical environments. Presently, a
prevalent method for evaluating the performance of generative models relies on
quantitative benchmarks. Such benchmarks have limitations and may suffer from
train-to-the-test overfitting, optimizing performance for a specified test set
at the cost of generalizability across other task and data distributions.
Evaluation strategies leveraging human expertise and utilizing cost-effective
computational models as evaluators are gaining interest. We discuss current
state-of-the-art methodologies for assessing the performance of GenAI
applications in healthcare and medical devices.

</details>


### [14] [Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning](https://arxiv.org/abs/2509.08089)
*Lucas Fenaux,Zheng Wang,Jacob Yan,Nathan Chung,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的适应性敌手模型和一种名为Hammer and Anvil的防御方案，用于对抗联邦学习中的后门攻击。新的攻出只需要1-2个恶意客户就能突破现有防御，而新的防御方案Krum+能有效抵御这些攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习分布式环境容易受到后门攻击，而现有的防御方案在面对适应性攻击者时效果不佳，需要更加强大的防御策略。

Method: 首先设计了一种新的适应性敌手模型，然后提出Hammer and Anvil防御方案，该方案结合了两种基础原理相互正交的防御技术，形成了一种结合防御系统。

Result: 新的适应性攻出只需要20个客户中的1-2个恶意客户就能突破现有最佳防御方案；新的Krum+防御方案能够成功抵御这些新型攻击和现有的先进攻击。

Conclusion: 论文表明通过结合不同防御原理的方法可以构建更加健壮的联邦学习防御系统，有效对抗适应性后门攻击，为分布式学习安全提供了新的解决方案。

Abstract: Federated Learning is a distributed learning technique in which multiple
clients cooperate to train a machine learning model. Distributed settings
facilitate backdoor attacks by malicious clients, who can embed malicious
behaviors into the model during their participation in the training process.
These malicious behaviors are activated during inference by a specific trigger.
No defense against backdoor attacks has stood the test of time, especially
against adaptive attackers, a powerful but not fully explored category of
attackers. In this work, we first devise a new adaptive adversary that
surpasses existing adversaries in capabilities, yielding attacks that only
require one or two malicious clients out of 20 to break existing
state-of-the-art defenses. Then, we present Hammer and Anvil, a principled
defense approach that combines two defenses orthogonal in their underlying
principle to produce a combined defense that, given the right set of
parameters, must succeed against any attack. We show that our best combined
defense, Krum+, is successful against our new adaptive adversary and
state-of-the-art attacks.

</details>


### [15] [Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography](https://arxiv.org/abs/2509.08116)
*Nooshin Maghsoodi,Sarah Nassar,Paul F R Wilson,Minh Nguyen Nhat To,Sophia Mannina,Shamel Addas,Stephanie Sibley,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: PhysioCLR是一个生理感知的对比学习框架，通过整合ECG生理相似性线索和特定增强技术，显著提升心电图心律失常分类的泛化能力和临床相关性


<details>
  <summary>Details</summary>
Motivation: 基于AI的心电图分析受限于标注数据不足，自监督学习可以利用大规模未标注数据，但需要融入领域特定的生理知识来提升临床相关性

Method: 提出PhysioCLR框架，在对比学习中整合ECG生理相似性线索，使用ECG特异性增强技术保持类别不变性，并采用混合损失函数优化表示质量

Result: 在三个数据集上测试，PhysioCLR相比最强基线平均AUROC提升12%，展现出强大的跨数据集泛化能力

Conclusion: 通过将生理知识嵌入对比学习，PhysioCLR能够学习到具有临床意义且可迁移的心电图特征，为更有效和标签高效的ECG诊断提供了有前景的路径

Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart
conditions; however, the effectiveness of artificial intelligence (AI)-based
ECG analysis is often hindered by the limited availability of labeled data.
Self-supervised learning (SSL) can address this by leveraging large-scale
unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning
Representation for ECG), a physiology-aware contrastive learning framework that
incorporates domain-specific priors to enhance the generalizability and
clinical relevance of ECG-based arrhythmia classification. Methods: During
pretraining, PhysioCLR learns to bring together embeddings of samples that
share similar clinically relevant features while pushing apart those that are
dissimilar. Unlike existing methods, our method integrates ECG physiological
similarity cues into contrastive learning, promoting the learning of clinically
meaningful representations. Additionally, we introduce ECG- specific
augmentations that preserve the ECG category post augmentation and propose a
hybrid loss function to further refine the quality of learned representations.
Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,
for multilabel ECG diagnoses, as well as a private ICU dataset labeled for
binary classification. Across the Chapman, Georgia, and private cohorts,
PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,
underscoring its robust cross-dataset generalization. Conclusion: By embedding
physiological knowledge into contrastive learning, PhysioCLR enables the model
to learn clinically meaningful and transferable ECG eatures. Significance:
PhysioCLR demonstrates the potential of physiology-informed SSL to offer a
promising path toward more effective and label-efficient ECG diagnostics.

</details>


### [16] [Optimization Methods and Software for Federated Learning](https://arxiv.org/abs/2509.08120)
*Konstantin Burlachenko*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习的五个关键挑战并提出新方法，重点解决数据设备异构性、通信问题和隐私保护，旨在弥合理论进展与实际应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分散环境中面临数据设备异构性、通信问题和隐私保护等独特挑战，需要将理论方法转化为实际可用的实现方案。

Method: 识别联邦学习的五个关键挑战，并提出新颖的算法和系统方法来解决这些问题，包括理论到实践的转化和反向的实践到理论的反馈过程。

Result: 提出了能够有效应对联邦学习实际挑战的新方法，促进了理论算法与实际实现的结合，为研究人员提供了实用的指导框架。

Conclusion: 该研究不仅推进了联邦学习算法和系统的发展，更重要的是建立了理论方法与实际实现之间的双向桥梁，揭示了从实践角度重新审视算法机制的新维度。

Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning
paradigm where multiple clients, such as mobile devices, collaborate to solve
machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al.
(2016a,b); McMahan et al. (2017), FL has gained further attention through its
inclusion in the National AI Research and Development Strategic Plan (2023
Update) of the United States (Science and on Artificial Intelligence, 2023).
The FL training process is inherently decentralized and often takes place in
less controlled settings compared to data centers, posing unique challenges
distinct from those in fully controlled environments. In this thesis, we
identify five key challenges in Federated Learning and propose novel approaches
to address them. These challenges arise from the heterogeneity of data and
devices, communication issues, and privacy concerns for clients in FL training.
Moreover, even well-established theoretical advances in FL require diverse
forms of practical implementation to enhance their real-world applicability.
Our contributions advance FL algorithms and systems, bridging theoretical
advancements and practical implementations. More broadly, our work serves as a
guide for researchers navigating the complexities of translating theoretical
methods into efficient real-world implementations and software. Additionally,
it offers insights into the reverse process of adapting practical
implementation aspects back into theoretical algorithm design. This reverse
process is particularly intriguing, as the practical perspective compels us to
examine the underlying mechanics and flexibilities of algorithms more deeply,
often uncovering new dimensions of the algorithms under study.

</details>


### [17] [Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation](https://arxiv.org/abs/2509.08163)
*Ho Ming Lee,Katrien Antonio,Benjamin Avanzi,Lorenzo Marchi,Rui Zhou*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于距离协方差正则化的框架，用于在回归和分类任务中实现多重保护属性的公平性，解决交叉子组的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法主要集中于二元分类，而回归任务和连续保护属性的公平性同样重要。多重保护属性存在时常忽视交叉子组的偏见（公平性分区）。

Method: 提出基于距离协方差的正则化框架，减弱模型预测与保护属性的关联。扩展为两种多变量依赖测度：联合距离协方差（JdCov）和新的链接距离协方差（CCdCov）。

Result: 方法能够有效处理线性和非线性依赖关系，解决交叉子组的公平性问题。在COMPAS再犯数据集和机动车保险答辨数据集上进行了应用验证。

Conclusion: 该框架为多重保护属性和不同类型属性的公平性问题提供了有效解决方案，特别是能够处理交叉子组偏见和连续属性的公平性要求。

Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as
gender or ethnicity) is a critical issue in machine learning. Most existing
literature focuses on binary classification, but achieving fairness in
regression tasks-such as insurance pricing or hiring score assessments-is
equally important. Moreover, anti-discrimination laws also apply to continuous
attributes, such as age, for which many existing methods are not applicable. In
practice, multiple protected attributes can exist simultaneously; however,
methods targeting fairness across several attributes often overlook so-called
"fairness gerrymandering", thereby ignoring disparities among intersectional
subgroups (e.g., African-American women or Hispanic men). In this paper, we
propose a distance covariance regularisation framework that mitigates the
association between model predictions and protected attributes, in line with
the fairness definition of demographic parity, and that captures both linear
and nonlinear dependencies. To enhance applicability in the presence of
multiple protected attributes, we extend our framework by incorporating two
multivariate dependence measures based on distance covariance: the previously
proposed joint distance covariance (JdCov) and our novel concatenated distance
covariance (CCdCov), which effectively address fairness gerrymandering in both
regression and classification tasks involving protected attributes of various
types. We discuss and illustrate how to calibrate regularisation strength,
including a method based on Jensen-Shannon divergence, which quantifies
dissimilarities in prediction distributions across groups. We apply our
framework to the COMPAS recidivism dataset and a large motor insurance claims
dataset.

</details>


### [18] [In-Context Learning Enhanced Credibility Transformer](https://arxiv.org/abs/2509.08122)
*Kishan Padayachy,Ronald Richman,Salvatore Scognamiglio,Mario V. Wüthrich*

Main category: cs.LG

TL;DR: 本文提出了一种基于Credibility Transformer的新范式，通过添加上下文学习机制来增强模型性能，能够处理训练时未见过的新类别特征。


<details>
  <summary>Details</summary>
Motivation: 为了提升Transformer模型的学习和预测性能，特别是在处理新出现的类别特征时，需要一种能够利用相似实例上下文信息的方法。

Method: 扩展Credibility Transformer架构，添加上下文学习机制，通过包含相似实例的上下文批次来增强CLS令牌表示，并进行微调。

Result: 实证验证表明上下文学习通过适应相似风险模式提高了预测准确性，并且能够泛化到训练时未见过的新实例。

Conclusion: 提出的上下文学习机制有效增强了Credibility Transformer的性能，使其能够处理训练数据中未出现的新类别特征，具有很好的实用价值。

Abstract: The starting point of our network architecture is the Credibility Transformer
which extends the classical Transformer architecture by a credibility mechanism
to improve model learning and predictive performance. This Credibility
Transformer learns credibilitized CLS tokens that serve as learned
representations of the original input features. In this paper we present a new
paradigm that augments this architecture by an in-context learning mechanism,
i.e., we increase the information set by a context batch consisting of similar
instances. This allows the model to enhance the CLS token representations of
the instances by additional in-context information and fine-tuning. We
empirically verify that this in-context learning enhances predictive accuracy
by adapting to similar risk patterns. Moreover, this in-context learning also
allows the model to generalize to new instances which, e.g., have feature
levels in the categorical covariates that have not been present when the model
was trained -- for a relevant example, think of a new vehicle model which has
just been developed by a car manufacturer.

</details>


### [19] [Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization](https://arxiv.org/abs/2509.08194)
*Caio de Prospero Iglesias,Kimberly Villalobos Carballo,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出了Prescribe-then-Select框架，通过构建候选策略库并学习元策略来选择最优策略，在上下文随机优化中实现数据驱动的策略选择。


<details>
  <summary>Details</summary>
Motivation: 解决上下文随机优化中的策略选择问题，当多个候选策略在不同协变量区域表现各异且没有单一主导策略时，需要一种方法来选择最优策略。

Method: 采用模块化框架：先构建可行的候选策略库，然后学习元策略来为观测到的协变量选择最佳策略。使用交叉验证训练的Optimal Policy Trees集成实现元策略。

Result: 在两个基准CSO问题（单阶段报童问题和两阶段运输规划）中，PS在协变量空间异质区域始终优于最佳单一策略，在无异质区域收敛到主导策略。

Conclusion: PS框架提供了一种有效的数据驱动方法来解决上下文随机优化中的策略选择问题，能够适应协变量空间的异质性并实现最优性能。

Abstract: We address the problem of policy selection in contextual stochastic
optimization (CSO), where covariates are available as contextual information
and decisions must satisfy hard feasibility constraints. In many CSO settings,
multiple candidate policies--arising from different modeling paradigms--exhibit
heterogeneous performance across the covariate space, with no single policy
uniformly dominating. We propose Prescribe-then-Select (PS), a modular
framework that first constructs a library of feasible candidate policies and
then learns a meta-policy to select the best policy for the observed
covariates. We implement the meta-policy using ensembles of Optimal Policy
Trees trained via cross-validation on the training set, making policy choice
entirely data-driven. Across two benchmark CSO problems--single-stage
newsvendor and two-stage shipment planning--PS consistently outperforms the
best single policy in heterogeneous regimes of the covariate space and
converges to the dominant policy when such heterogeneity is absent. All the
code to reproduce the results can be found at
https://anonymous.4open.science/r/Prescribe-then-Select-TMLR.

</details>


### [20] [torchmil: A PyTorch-based library for deep Multiple Instance Learning](https://arxiv.org/abs/2509.08129)
*Francisco M. Castro-Macías,Francisco J. Sáez-Maldonado,Pablo Morales-Álvarez,Rafael Molina*

Main category: cs.LG

TL;DR: torchmil是一个基于PyTorch的开源Python库，为多示例学习(MIL)提供统一的模块化框架，包含标准数据格式、基准数据集和模型，旨在提高MIL研究的可重复性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 多示例学习领域缺乏标准化的模型开发、评估和比较工具，这阻碍了研究的可重复性和可访问性。

Method: 开发基于PyTorch的torchmil库，提供MIL模型的基础构建模块、标准化数据格式、精选的基准数据集和模型集合，以及全面的文档和教程。

Result: 成功创建了一个开源MIL库，为研究者和实践者提供了统一的开发框架和工具支持。

Conclusion: torchmil将加速MIL领域的进展，降低新用户的入门门槛，促进该领域的标准化发展。

Abstract: Multiple Instance Learning (MIL) is a powerful framework for weakly
supervised learning, particularly useful when fine-grained annotations are
unavailable. Despite growing interest in deep MIL methods, the field lacks
standardized tools for model development, evaluation, and comparison, which
hinders reproducibility and accessibility. To address this, we present
torchmil, an open-source Python library built on PyTorch. torchmil offers a
unified, modular, and extensible framework, featuring basic building blocks for
MIL models, a standardized data format, and a curated collection of benchmark
datasets and models. The library includes comprehensive documentation and
tutorials to support both practitioners and researchers. torchmil aims to
accelerate progress in MIL and lower the entry barrier for new users. Available
at https://torchmil.readthedocs.io.

</details>


### [21] [Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis](https://arxiv.org/abs/2509.08483)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 该论文分析了带有Polyak重球动量(HB)的梯度下降算法，证明了在指数吸引不变流形上，HB算法等价于带有修正损失函数的普通梯度下降，并提供了任意精度的全局近似边界和连续修正方程。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降中重球动量算法的数学特性，理解其在优化过程中的行为机制，为分析其他优化算法提供理论基础。

Method: 基于Kovachki和Stuart(2021)的工作，在指数吸引不变流形上分析HB算法，推导修正损失函数，进行组合数学分析，建立任意阶近似精度的连续修正方程。

Result: 证明了HB算法在小步长条件下等价于修正损失函数的梯度下降，获得了O(h^R)的全局近似边界，发现了包含Eulerian和Narayana多项式的丰富多项式族。

Conclusion: 研究结果深入揭示了重球动量梯度下降的核心特征，为分析其他优化算法提供了路线图，理论结果适用于全批次和小批次HB算法。

Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed
momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory.
Building on Kovachki and Stuart (2021), we prove that on an exponentially
attractive invariant manifold the algorithm is exactly plain gradient descent
with a modified loss, provided that the step size $h$ is small enough. Although
the modified loss does not admit a closed-form expression, we describe it with
arbitrary precision and prove global (finite "time" horizon) approximation
bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a
fine-grained analysis of the combinatorics underlying the memoryless
approximations of HB, in particular, finding a rich family of polynomials in
$\beta$ hidden inside which contains Eulerian and Narayana polynomials. We
derive continuous modified equations of arbitrary approximation order (with
rigorous bounds) and the principal flow that approximates the HB dynamics,
generalizing Rosca et al. (2023). Approximation theorems cover both full-batch
and mini-batch HB. Our theoretical results shed new light on the main features
of gradient descent with heavy-ball momentum, and outline a road-map for
similar analysis of other optimization algorithms.

</details>


### [22] [From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital](https://arxiv.org/abs/2509.08140)
*Mihir Kumar,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Afriyie Kwesi Samuel,Fuat Alican,Yigit Ihlamur*

Main category: cs.LG

TL;DR: 提出结合大语言模型和多模型机器学习的框架，用于预测罕见高影响事件，在风险投资领域实现9.8-11.1倍于随机基线的精确度


<details>
  <summary>Details</summary>
Motivation: 解决风险投资中早期阶段数据有限且嘈杂的情况下，准确预测初创企业成功概率的挑战，同时兼顾模型预测能力和可解释性

Method: 使用LLM进行特征工程从非结构化数据提取复杂信号，构建包含XGBoost、随机森林和线性回归的分层集成模型，首先生成连续成功概率估计，再通过阈值处理得到二元罕见事件预测

Result: 在三个独立测试子集上，模型精确度达到随机分类器基线的9.8-11.1倍；特征敏感性分析显示，初创企业类别列表占预测影响力的15.6%，创始人数量次之，教育水平和领域专业知识也有稳定贡献

Conclusion: 该框架成功整合了LLM的特征工程能力和多模型集成的预测优势，为罕见高影响事件的预测提供了既准确又可解释的解决方案，特别适用于风险投资等数据稀缺领域

Abstract: This paper presents a framework for predicting rare, high-impact outcomes by
integrating large language models (LLMs) with a multi-model machine learning
(ML) architecture. The approach combines the predictive strength of black-box
models with the interpretability required for reliable decision-making. We use
LLM-powered feature engineering to extract and synthesize complex signals from
unstructured data, which are then processed within a layered ensemble of models
including XGBoost, Random Forest, and Linear Regression. The ensemble first
produces a continuous estimate of success likelihood, which is then thresholded
to produce a binary rare-event prediction. We apply this framework to the
domain of Venture Capital (VC), where investors must evaluate startups with
limited and noisy early-stage data. The empirical results show strong
performance: the model achieves precision between 9.8X and 11.1X the random
classifier baseline in three independent test subsets. Feature sensitivity
analysis further reveals interpretable success drivers: the startup's category
list accounts for 15.6% of predictive influence, followed by the number of
founders, while education level and domain expertise contribute smaller yet
consistent effects.

</details>


### [23] [Data-driven generative simulation of SDEs using diffusion models](https://arxiv.org/abs/2509.08731)
*Xuefeng Gao,Jiale Zha,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 使用扩散模型生成未知随机微分方程样本路径的新方法，无需显式漂移和扩散系数，通过数据驱动方式从有限样本生成新路径


<details>
  <summary>Details</summary>
Motivation: 传统蒙特卡洛方法需要显式指定SDE的漂移和扩散系数，限制了在未知SDE情况下的应用。本文旨在开发一种无需模型假设的数据驱动方法

Method: 采用条件扩散模型，基于有限样本路径生成新的合成路径。通过模拟实验与神经SDE等基准方法进行比较验证

Result: 方法有效生成SDE样本路径，在连续时间均值-方差投资组合选择的强化学习应用中提升了算法性能

Conclusion: 扩散模型在金融分析和决策中具有广阔应用前景，为未知SDE的路径生成提供了有效的模型无关解决方案

Abstract: This paper introduces a new approach to generating sample paths of unknown
stochastic differential equations (SDEs) using diffusion models, a class of
generative AI models commonly employed in image and video applications. Unlike
the traditional Monte Carlo methods for simulating SDEs, which require explicit
specifications of the drift and diffusion coefficients, our method takes a
model-free, data-driven approach. Given a finite set of sample paths from an
SDE, we utilize conditional diffusion models to generate new, synthetic paths
of the same SDE. To demonstrate the effectiveness of our approach, we conduct a
simulation experiment to compare our method with alternative benchmark ones
including neural SDEs. Furthermore, in an empirical study we leverage these
synthetically generated sample paths to enhance the performance of
reinforcement learning algorithms for continuous-time mean-variance portfolio
selection, hinting promising applications of diffusion models in financial
analysis and decision-making.

</details>


### [24] [MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs](https://arxiv.org/abs/2509.08156)
*Swati Swati,Arjun Roy,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: mmm-fair是一个开源工具包，使用基于boosting的集成方法动态优化模型权重，同时最小化分类错误和多种公平性违规，支持多目标优化和交叉偏见检测。


<details>
  <summary>Details</summary>
Motivation: 现有公平性工具包对多维公平性和相关权衡的支持有限，而监管和社会对公平AI的需求日益增长，需要能够处理交叉偏见和冲突公平性定义的工具。

Method: 基于boosting的集成方法，动态优化模型权重，联合最小化分类错误和多样公平性违规，支持多目标优化、无代码聊天界面、LLM驱动的解释和交互式Pareto探索。

Result: 能够可靠地发现最先进方法经常遗漏的交叉偏见，使用户能够部署符合其特定上下文需求的模型。

Conclusion: mmm-fair独特地结合了深度多属性公平性、多目标优化、无代码界面等多种功能，是现有公平性工具中罕见的综合解决方案。

Abstract: Fairness-aware classification requires balancing performance and fairness,
often intensified by intersectional biases. Conflicting fairness definitions
further complicate the task, making it difficult to identify universally fair
solutions. Despite growing regulatory and societal demands for equitable AI,
popular toolkits offer limited support for exploring multi-dimensional fairness
and related trade-offs. To address this, we present mmm-fair, an open-source
toolkit leveraging boosting-based ensemble approaches that dynamically
optimizes model weights to jointly minimize classification errors and diverse
fairness violations, enabling flexible multi-objective optimization. The system
empowers users to deploy models that align with their context-specific needs
while reliably uncovering intersectional biases often missed by
state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth
multi-attribute fairness, multi-objective optimization, a no-code, chat-based
interface, LLM-powered explanations, interactive Pareto exploration for model
selection, custom fairness constraint definition, and deployment-ready models
in a single open-source toolkit, a combination rarely found in existing
fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.

</details>


### [25] [MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments](https://arxiv.org/abs/2509.08176)
*Honghui Du,Leandro Minku,Huiyu Zhou*

Main category: cs.LG

TL;DR: MARLINE是一种新颖的多源迁移学习方法，用于处理非平稳环境中的概念漂移问题，即使源域和目标域概念不匹配也能有效利用多源知识。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设至少有一个源模型与目标概念相似，但这在现实场景中往往不成立。需要一种能够在源域和目标域概念不匹配时仍能有效利用多源知识的方法。

Method: 通过将目标概念投影到每个源概念的空间中，使多个源子分类器能够作为集成的一部分共同为目标概念的预测做出贡献。

Result: 在多个合成和真实数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。

Conclusion: MARLINE提供了一种有效的解决方案，能够在非平稳环境中充分利用多源知识，即使源域和目标域概念不完全匹配也能取得良好的性能。

Abstract: Concept drift is a major problem in online learning due to its impact on the
predictive performance of data stream mining systems. Recent studies have
started exploring data streams from different sources as a strategy to tackle
concept drift in a given target domain. These approaches make the assumption
that at least one of the source models represents a concept similar to the
target concept, which may not hold in many real-world scenarios. In this paper,
we propose a novel approach called Multi-source mApping with tRansfer LearnIng
for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge
from multiple data sources in non-stationary environments even when source and
target concepts do not match. This is achieved by projecting the target concept
to the space of each source concept, enabling multiple source sub-classifiers
to contribute towards the prediction of the target concept as part of an
ensemble. Experiments on several synthetic and real-world datasets show that
MARLINE was more accurate than several state-of-the-art data stream learning
approaches.

</details>


### [26] [The Domain Mixed Unit: A New Neural Arithmetic Layer](https://arxiv.org/abs/2509.08180)
*Paul Curry*

Main category: cs.LG

TL;DR: DMU是一种新的神经算术单元，通过单一参数门在log空间和线性空间表示之间混合，支持加法和减法运算，在NALM基准测试中实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经网络在算术运算泛化能力方面的挑战，特别是在乘法和除法运算上的表现，需要开发能够更好地处理不同数值表示空间的神经算术单元。

Method: 提出Domain Mixed Unit (DMU)，使用单一参数门在log空间和线性空间表示之间进行混合，支持加法(DMU add)和减法(DMU sub)运算，并提供了两种初始化方法分别覆盖加法/乘法和减法/除法运算。

Result: 在NALM基准测试中实现了state-of-the-art性能，在乘法和除法运算上获得了所有种子中最高的解决百分比。

Conclusion: DMU单元在算术运算泛化方面表现出色，特别是在乘除运算上，代码已开源并计划提交到NALM基准测试项目中。

Abstract: The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a
single parameter gate that mixes between log-space and linear-space
representations while performing either addition (DMU add) or subtraction (DMU
sub). Two initializations are proposed for the DMU: one covering addition and
multiplication, and another covering subtraction and division. The DMU achieves
state-of-the-art performance on the NALM Benchmark, a dataset designed to test
the ability of neural arithmetic units to generalize arithmetic operations,
specifically performing with the highest percentage solved over all seeds on
multiplication and division. The DMU will be submitted as a pull request to the
open-source NALM benchmark, and its code is available on GitHub at
https://github.com/marict?tab=repositories

</details>


### [27] [Multi-Label Transfer Learning in Non-Stationary Data Streams](https://arxiv.org/abs/2509.08181)
*Honghui Du,Leandro Minku,Aonghus Lawlor,Huiyu Zhou*

Main category: cs.LG

TL;DR: 提出了两种新颖的多标签数据流迁移学习方法：BR-MARLENE利用源流和目标流中不同标签的知识进行多标签分类；BRPW-MARLENE通过显式建模和迁移成对标签依赖关系来提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中的标签概念在非平稳环境中经常发生漂移，现有研究对多标签数据流迁移学习的关注有限，需要在相关标签之间转移知识以加速适应。

Method: BR-MARLENE方法利用源流和目标流中不同标签的知识进行迁移学习；BRPW-MARLENE方法在此基础上显式建模和迁移成对标签依赖关系。

Result: 综合实验表明，两种方法在非平稳环境中都优于最先进的多标签流方法，证明了标签间知识转移对提高预测性能的有效性。

Conclusion: 提出的迁移学习方法能够有效处理多标签数据流中的概念漂移问题，通过标签间知识转移显著提升了多标签分类性能。

Abstract: Label concepts in multi-label data streams often experience drift in
non-stationary environments, either independently or in relation to other
labels. Transferring knowledge between related labels can accelerate
adaptation, yet research on multi-label transfer learning for data streams
remains limited. To address this, we propose two novel transfer learning
methods: BR-MARLENE leverages knowledge from different labels in both source
and target streams for multi-label classification; BRPW-MARLENE builds on this
by explicitly modelling and transferring pairwise label dependencies to enhance
learning performance. Comprehensive experiments show that both methods
outperform state-of-the-art multi-label stream approaches in non-stationary
environments, demonstrating the effectiveness of inter-label knowledge transfer
for improved predictive performance.

</details>


### [28] [Selective Induction Heads: How Transformers Select Causal Structures In Context](https://arxiv.org/abs/2509.08184)
*Francesco D'Angelo,Francesco Croce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文提出了一个新颖框架，展示Transformer动态处理因果结构的能力，通过交错马尔可夫链揭示选择性归纳头的形成机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖固定因果结构的马尔可夫链，无法捕捉自然语言中动态变化的token关系，需要探索Transformer如何动态选择因果结构。

Method: 使用具有不同滞后的交错马尔可夫链来变化因果结构，同时保持转移概率固定，通过3层Transformer实现选择性归纳头机制。

Result: Transformer学会了通过识别正确滞后并从过去复制相应token来预测下一个token，该机制渐近收敛于最大似然解。

Conclusion: 研究推进了对Transformer如何选择因果结构的理解，为其功能和可解释性提供了新见解。

Abstract: Transformers have exhibited exceptional capabilities in sequence modeling
tasks, leveraging self-attention and in-context learning. Critical to this
success are induction heads, attention circuits that enable copying tokens
based on their previous occurrences. In this work, we introduce a novel
framework that showcases transformers' ability to dynamically handle causal
structures. Existing works rely on Markov Chains to study the formation of
induction heads, revealing how transformers capture causal dependencies and
learn transition probabilities in-context. However, they rely on a fixed causal
structure that fails to capture the complexity of natural languages, where the
relationship between tokens dynamically changes with context. To this end, our
framework varies the causal structure through interleaved Markov chains with
different lags while keeping the transition probabilities fixed. This setting
unveils the formation of Selective Induction Heads, a new circuit that endows
transformers with the ability to select the correct causal structure
in-context. We empirically demonstrate that transformers learn this mechanism
to predict the next token by identifying the correct lag and copying the
corresponding token from the past. We provide a detailed construction of a
3-layer transformer to implement the selective induction head, and a
theoretical analysis proving that this mechanism asymptotically converges to
the maximum likelihood solution. Our findings advance the understanding of how
transformers select causal structures, providing new insights into their
functioning and interpretability.

</details>


### [29] [ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis](https://arxiv.org/abs/2509.08188)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 该研究比较了条件WGAN-GP和1D去噪扩散模型在生成真实EEG伪迹片段方面的性能，发现WGAN-GP在频谱对齐方面表现更好，但两种模型在类别条件恢复方面都较弱，限制了数据增强的即时效果。


<details>
  <summary>Details</summary>
Motivation: EEG中的伪迹（肌肉、眼动、电极、咀嚼和颤抖）会干扰自动化分析，但大规模标注成本高昂。研究旨在探索现代生成模型是否能合成真实、标签感知的伪迹片段，用于数据增强和压力测试。

Method: 使用TUH EEG伪迹语料库，整理受试者分割和固定长度多通道窗口。比较条件WGAN-GP（带投影判别器）和1D去噪扩散模型（带分类器自由引导），从三个维度评估：保真度（频谱功率、协方差、自相关和分布指标）、特异性（类别条件恢复）和实用性（数据增强效果）。

Result: WGAN-GP在频谱对齐方面更接近真实数据且MMD更低，但两种模型都表现出较弱的类别条件恢复能力，限制了数据增强的即时收益。

Conclusion: 研究提供了可复现的EEG伪迹合成基线流程，揭示了在更强条件约束和覆盖范围方面的改进机会，为未来工作指明了可操作的失败模式。

Abstract: Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,
chewing, and shiver -- confound automated analysis yet are costly to label at
scale. We study whether modern generative models can synthesize realistic,
label-aware artifact segments suitable for augmentation and stress-testing.
Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and
fixed-length multi-channel windows (e.g., 250 samples) with preprocessing
tailored to each model (per-window min-max for adversarial training;
per-recording/channel $z$-score for diffusion). We compare a conditional
WGAN-GP with a projection discriminator to a 1D denoising diffusion model with
classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch
band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$),
channel-covariance Frobenius distance, autocorrelation $L_2$, and
distributional metrics (MMD/PRD); (ii) specificity via class-conditional
recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation
effects on artifact recognition. In our setting, WGAN-GP achieves closer
spectral alignment and lower MMD to real data, while both models exhibit weak
class-conditional recovery, limiting immediate augmentation gains and revealing
opportunities for stronger conditioning and coverage. We release a reproducible
pipeline -- data manifests, training configurations, and evaluation scripts --
to establish a baseline for EEG artifact synthesis and to surface actionable
failure modes for future work.

</details>


### [30] [Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics](https://arxiv.org/abs/2509.08191)
*Robert Stephany,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种结合灵活高阶有限差分格式和Rollout损失函数的降阶模型方法，用于解决长时间跨度PDE预测精度下降问题，并在2D Burgers方程上验证


<details>
  <summary>Details</summary>
Motivation: 传统降阶模型(ROM)在求解参数化PDE族时，长时间跨度预测精度会显著下降，需要新的方法来保持长期预测准确性

Method: 1) 引入灵活、高阶且计算成本低的有限差分格式；2) 提出Rollout损失函数来训练ROM，使其能够在任意时间跨度上做出准确预测

Result: 在2D Burgers方程上验证了所提方法的有效性，证明了该方法能够显著提升长时间跨度预测精度

Conclusion: 该方法通过改进数值格式和训练策略，有效解决了ROM在长时间预测中的精度退化问题，为复杂PDE的高效求解提供了新思路

Abstract: Solving complex partial differential equations is vital in the physical
sciences, but often requires computationally expensive numerical methods.
Reduced-order models (ROMs) address this by exploiting dimensionality reduction
to create fast approximations. While modern ROMs can solve parameterized
families of PDEs, their predictive power degrades over long time horizons. We
address this by (1) introducing a flexible, high-order, yet inexpensive
finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to
make accurate predictions over arbitrary time horizons. We demonstrate our
approach on the 2D Burgers equation.

</details>


### [31] [Sketched Gaussian Mechanism for Private Federated Learning](https://arxiv.org/abs/2509.08195)
*Qiaobo Li,Zhijie Chen,Arindam Banerjee*

Main category: cs.LG

TL;DR: 提出Sketched Gaussian Mechanism (SGM)，将梯度压缩和差分隐私机制结合，通过联合分析提供比单独分析更灵活和更强的隐私保障，在相同噪声预算下隐私级别与压缩维度平方根成反比。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信成本和隐私保护是两个重要问题。现有研究通常单独分析梯度压缩和差分隐私机制，未能充分利用两者的协同效应。

Method: 提出SGM机制，直接结合梯度压缩（sketching）和高斯机制（GM），使用Rényi-DP工具进行联合隐私分析，并应用于梯度下降或自适应服务器优化的联邦学习。

Result: 理论证明SGM的隐私级别与压缩维度b的平方根成反比，在相同噪声预算下提供比原始GM更强的隐私保障。实验表明在相同隐私级别下，SGM至少与非压缩私有FL方法相当，在某些设置中表现更优。

Conclusion: SGM机制通过联合分析压缩和隐私机制，实现了通信效率和隐私保护的平衡，自适应服务器优化进一步提升了性能同时保持隐私保障。

Abstract: Communication cost and privacy are two major considerations in federated
learning (FL). For communication cost, gradient compression by sketching the
clients' transmitted model updates is often used for reducing per-round
communication. For privacy, the Gaussian mechanism (GM), which consists of
clipping updates and adding Gaussian noise, is commonly used to guarantee
client-level differential privacy. Existing literature on private FL analyzes
privacy of sketching and GM in an isolated manner, illustrating that sketching
provides privacy determined by the sketching dimension and that GM has to
supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which
directly combines sketching and the Gaussian mechanism for privacy. Using
R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy
guarantee, which is significantly more flexible and sharper compared to
isolated analysis of sketching and GM privacy. In particular, we prove that the
privacy level of SGM for a fixed noise magnitude is proportional to
$1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for
moderate $b$) SGM can provide much stronger privacy guarantees than the
original GM under the same noise budget. We demonstrate the application of SGM
to FL with either gradient descent or adaptive server optimizers, and establish
theoretical results on optimization convergence, which exhibits only a
logarithmic dependence on the number of parameters $d$. Experimental results
confirm that at the same privacy level, SGM based FL is at least competitive
with non-sketching private FL variants and outperforms them in some settings.
Moreover, using adaptive optimization at the server improves empirical
performance while maintaining the privacy guarantees.

</details>


### [32] [Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition](https://arxiv.org/abs/2509.08225)
*Matthew Nolan,Lina Yao,Robert Davidson*

Main category: cs.LG

TL;DR: 本文提出了一种基于集成分布蒸馏的自监督学习框架，用于人体活动识别，通过利用未标记数据和部分监督训练策略，提高了预测准确性、不确定性估计和对抗扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人体活动识别在深度学习方法中取得了显著进展，但仍面临数据需求量大、可靠性和鲁棒性不足的挑战。本文旨在通过自监督学习和集成蒸馏技术来解决这些问题。

Method: 采用集成分布蒸馏(EDD)的自监督学习框架，结合创新的数据增强技术和部分监督训练策略，利用未标记数据进行训练。

Result: 在多个公开数据集上的评估表明，该方法显著提高了预测准确性，提供了可靠的uncertainty估计，并大幅增强了对抗扰动的鲁棒性，同时没有增加推理时的计算复杂度。

Conclusion: 提出的自监督EDD框架有效提升了人体活动识别的可靠性和鲁棒性，为实际应用场景提供了更可靠的解决方案，主要贡献包括框架开发、数据增强技术创新和实证验证。

Abstract: Human Activity Recognition (HAR) has seen significant advancements with the
adoption of deep learning techniques, yet challenges remain in terms of data
requirements, reliability and robustness. This paper explores a novel
application of Ensemble Distribution Distillation (EDD) within a
self-supervised learning framework for HAR aimed at overcoming these
challenges. By leveraging unlabeled data and a partially supervised training
strategy, our approach yields an increase in predictive accuracy, robust
estimates of uncertainty, and substantial increases in robustness against
adversarial perturbation; thereby significantly improving reliability in
real-world scenarios without increasing computational complexity at inference.
We demonstrate this with an evaluation on several publicly available datasets.
The contributions of this work include the development of a self-supervised EDD
framework, an innovative data augmentation technique designed for HAR, and
empirical validation of the proposed method's effectiveness in increasing
robustness and reliability.

</details>


### [33] [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)
*Kai Yi*

Main category: cs.LG

TL;DR: 该论文提出了一系列通信高效的分布式和联邦学习策略，包括模型压缩、本地训练个性化、隐私保护剪枝等方法，在保持精度的同时显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 分布式和联邦学习中的通信开销是主要瓶颈，需要开发高效的通信策略来提升训练效率同时保护隐私。

Method: 建立了有偏和无偏压缩算子的统一框架，提出自适应本地训练策略（Scafflix平衡全局和个性化目标），隐私保护剪枝框架（Cohort-Squeeze层次聚合），以及对称后训练剪枝方法SymWanda。

Result: 在基准测试和大规模语言模型上的实验表明，这些方法在IID和非IID设置下都能实现精度、收敛性和通信效率之间的良好权衡。

Conclusion: 该研究为可扩展、高效的分布式学习提供了理论和实践见解，通过模型压缩、个性化训练和剪枝技术有效解决了通信瓶颈问题。

Abstract: Distributed and federated learning are essential paradigms for training
models across decentralized data sources while preserving privacy, yet
communication overhead remains a major bottleneck. This dissertation explores
strategies to improve communication efficiency, focusing on model compression,
local training, and personalization. We establish a unified framework for
biased and unbiased compression operators with convergence guarantees, then
propose adaptive local training strategies that incorporate personalization to
accelerate convergence and mitigate client drift. In particular, Scafflix
balances global and personalized objectives, achieving superior performance
under both IID and non-IID settings. We further introduce privacy-preserving
pruning frameworks that optimize sparsity while minimizing communication costs,
with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device
overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances
robustness under high sparsity and maintains accuracy without retraining.
Extensive experiments on benchmarks and large-scale language models demonstrate
favorable trade-offs among accuracy, convergence, and communication, offering
theoretical and practical insights for scalable, efficient distributed
learning.

</details>


### [34] [The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data](https://arxiv.org/abs/2509.08247)
*Xiaolong Luo,Michael Lingzhi Li*

Main category: cs.LG

TL;DR: CRITICAL数据集提供了大规模多机构重症监护数据，CRISP预处理管道将其转化为ML就绪格式，包含数据质量管理、术语映射标准化、高效处理和基准测试，显著减少研究预处理时间。


<details>
  <summary>Details</summary>
Motivation: 现有重症监护数据集如MIMIC和eICU在规模和多样性上有限，CRITICAL数据集虽然提供了更大规模和地理多样性的数据，但多机构数据的异质性给数据协调带来了巨大挑战，需要系统化的预处理方法。

Method: 开发CRISP预处理管道，通过四个核心步骤：1）透明的数据质量管理与审计追踪；2）异构医学术语映射到统一SNOMED-CT标准，包括去重和单位标准化；3）模块化架构与并行优化，实现高效处理；4）全面的基线模型基准测试。

Result: CRISP能够将原始OMOP CDM数据在标准计算硬件上在1天内完成完整数据集处理，提供了处理管道、基线实现和详细转换文档，为研究人员节省数月预处理时间。

Conclusion: CRISP释放了大规模多机构重症监护数据的全部潜力，使研究人员能够专注于临床AI的推进，促进了健康公平性研究和大规模预测模型的发展。

Abstract: While existing critical care EHR datasets such as MIMIC and eICU have enabled
significant advances in clinical AI research, the CRITICAL dataset opens new
frontiers by providing extensive scale and diversity -- containing 1.95 billion
records from 371,365 patients across four geographically diverse CTSA
institutions. CRITICAL's unique strength lies in capturing full-spectrum
patient journeys, including pre-ICU, ICU, and post-ICU encounters across both
inpatient and outpatient settings. This multi-institutional, longitudinal
perspective creates transformative opportunities for developing generalizable
predictive models and advancing health equity research. However, the richness
of this multi-site resource introduces substantial complexity in data
harmonization, with heterogeneous collection practices and diverse vocabulary
usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource.
CRISP systematically transforms raw Observational Medical Outcomes Partnership
Common Data Model data into ML-ready datasets through: (1) transparent data
quality management with comprehensive audit trails, (2) cross-vocabulary
mapping of heterogeneous medical terminologies to unified SNOMED-CT standards,
with deduplication and unit standardization, (3) modular architecture with
parallel optimization enabling complete dataset processing in $<$1 day even on
standard computing hardware, and (4) comprehensive baseline model benchmarks
spanning multiple clinical prediction tasks to establish reproducible
performance standards. By providing processing pipeline, baseline
implementations, and detailed transformation documentation, CRISP saves
researchers months of preprocessing effort and democratizes access to
large-scale multi-institutional critical care data, enabling them to focus on
advancing clinical AI.

</details>


### [35] [Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning](https://arxiv.org/abs/2509.08255)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.LG

TL;DR: 提出FAPM遗忘感知剪枝指标，通过任务向量与预训练参数的比值量化灾难性遗忘，在保持下游任务99.67%准确率的同时将遗忘率降至0.25%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在微调时面临灾难性遗忘问题，需要平衡遗忘与下游任务性能

Method: 基于任务向量与预训练参数重叠度的发现，将任务向量与预训练参数的比值作为剪枝标准，无需修改训练过程或模型架构

Result: 在8个数据集上实验表明，FAPM将灾难性遗忘限制在0.25%，同时保持下游任务99.67%的准确率

Conclusion: FAPM是一种有效的剪枝方法，能显著减轻灾难性遗忘问题，且无需额外数据或架构修改

Abstract: Recent advancements in large language models (LLMs) have shown impressive
capabilities in various downstream tasks but typically face Catastrophic
Forgetting (CF) during fine-tuning. In this paper, we propose the
Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to
balance CF and downstream task performance. Our investigation reveals that the
degree to which task vectors (i.e., the subtraction of pre-trained weights from
the weights fine-tuned on downstream tasks) overlap with pre-trained model
parameters is a critical factor for CF. Based on this finding, FAPM employs the
ratio of the task vector to pre-trained model parameters as a metric to
quantify CF, integrating this measure into the pruning criteria. Importantly,
FAPM does not necessitate modifications to the training process or model
architecture, nor does it require any auxiliary data. We conducted extensive
experiments across eight datasets, covering natural language inference, General
Q&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results
demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\%
accuracy on downstream tasks. We provide the code to reproduce our results.

</details>


### [36] [Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models](https://arxiv.org/abs/2509.08270)
*Pranav Pawar,Kavish Shah,Akshat Bhalani,Komal Kasat,Dev Mittal,Hadi Gala,Deepali Patil,Nikita Raichada,Monali Deshmukh*

Main category: cs.LG

TL;DR: 提出了一个评估视觉语言模型在2D物理理解能力的新框架，包含400多个问题，测试了四个核心物理领域，发现模型规模与推理能力正相关，但在抽象空间推理方面表现较差。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的发展，需要系统评估其对基础科学原理（特别是物理）的理解能力，当前这一领域尚未得到充分探索。

Method: 开发了一个包含场景生成器的评估框架，涵盖抛体运动、碰撞动力学、力学和流体动力学四个核心领域，共400多个问题，对四个最先进的VLMs进行了全面评估。

Result: 发现模型规模与推理能力呈强相关，最佳模型Qwen2.5-VL-7B总体得分0.815。模型在公式化问题上表现良好，但在需要抽象空间推理的领域表现显著较差。

Conclusion: 该框架旨在普及VLMs科学推理研究，促进对其能力和局限性的深入理解，为未来模型发展提供重要基准。

Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to
perform reasoning is coming under increasing supervision. While they excel at
many tasks, their grasp of fundamental scientific principles, such as physics,
remains an underexplored frontier. To reflect the advancements in these
capabilities, we introduce a novel and accessible framework designed to
rigorously evaluate VLMs on their understanding of 2D physics. Our framework
features a pragmatic scenario generator that creates a diverse testbed of over
400 problems across four core domains: Projectile Motion, Collision Dynamics,
Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four
state-of-the-art VLMs, we demonstrate a strong correlation between model scale
and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving
an overall score of 0.815. We find that while models excel at formulaic
problems, they struggle significantly with domains requiring abstract spatial
reasoning. By designing this framework, we aim to democratize the study of
scientific reasoning in VLMs and foster deeper insights into their capabilities
and limitations.

</details>


### [37] [Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning](https://arxiv.org/abs/2509.08277)
*Dung T. Tran,Huyen Ngoc Huyen,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.LG

TL;DR: 提出基于矩阵轮廓的加权集成框架MPWE，用于越南降雨预测，通过动态捕捉地理模型间的协变依赖和冗余感知加权，在多个流域和预测时域上实现更准确稳定的预测性能。


<details>
  <summary>Details</summary>
Motivation: 越南降雨预测面临气候多样性和地理变异性的挑战，但准确预测对洪水管理、水电运营和灾害准备至关重要。需要一种能够动态整合多个地理模型预测的集成方法。

Method: 提出矩阵轮廓基础的加权集成(MPWE)框架，采用机制切换方法动态捕捉多个地理模型预测间的协变依赖关系，并引入冗余感知加权来平衡各模型的贡献。

Result: 在越南8个主要流域和5个预测时域(1小时及12-84小时累积降雨)上的实验表明，MPWE相比地理模型和集成基线方法，预测误差的均值和标准差均显著降低。

Conclusion: MPWE框架能够有效提升降雨预测的准确性和稳定性，在不同流域和预测时域上均表现出优越性能，为复杂地理条件下的降雨预测提供了有效解决方案。

Abstract: Rainfall forecasting in Vietnam is highly challenging due to its diverse
climatic conditions and strong geographical variability across river basins,
yet accurate and reliable forecasts are vital for flood management, hydropower
operation, and disaster preparedness. In this work, we propose a Matrix
Profile-based Weighted Ensemble (MPWE), a regime-switching framework that
dynamically captures covariant dependencies among multiple geographical model
forecasts while incorporating redundancy-aware weighting to balance
contributions across models. We evaluate MPWE using rainfall forecasts from
eight major basins in Vietnam, spanning five forecast horizons (1 hour and
accumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results
show that MPWE consistently achieves lower mean and standard deviation of
prediction errors compared to geographical models and ensemble baselines,
demonstrating both improved accuracy and stability across basins and horizons.

</details>


### [38] [\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2509.08300)
*Yao Lu,Chunfeng Sun,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: FoQuS是一种基于训练动态的核集选择方法，通过选择原始数据集的子集来近似完整训练效果，显著减少自动调制识别模型的训练开销。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习自动调制识别模型在开发新模型或超参数调优时，使用海量数据进行重复训练所产生的时间和能耗开销问题。

Method: 记录完整数据集训练过程中每个样本的预测轨迹，基于训练动态构建三个重要性指标来选择核集。

Result: 在多个AMR数据集上，仅使用1%-30%的原始数据就能保持高识别精度和良好的跨架构泛化能力。

Conclusion: FoQuS方法能有效减少训练开销，同时保持模型性能，具有良好的实用价值。

Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made
significant progress with the support of large-scale labeled data. However,
when developing new models or performing hyperparameter tuning, the time and
energy consumption associated with repeated training using massive amounts of
data are often unbearable. To address the above challenges, we propose
\emph{FoQuS}, which approximates the effect of full training by selecting a
coreset from the original dataset, thereby significantly reducing training
overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each
sample during full-dataset training and constructs three importance metrics
based on training dynamics. Experiments show that \emph{FoQuS} can maintain
high recognition accuracy and good cross-architecture generalization on
multiple AMR datasets using only 1\%-30\% of the original data.

</details>


### [39] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: EvolKV是一个自适应KV缓存压缩框架，通过进化搜索动态优化各层缓存分配，在保持任务性能的同时显著提升内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存压缩方法依赖启发式策略，忽略了层级特征模式与任务性能之间的关键交互作用，导致泛化性能下降。

Method: 将缓存分配重新表述为多目标优化问题，利用进化搜索动态配置各层预算，直接最大化下游任务性能。

Result: 在11个任务上的实验表明，该方法在长上下文任务中优于所有基线方法，在GSM8K上比启发式基线高出7个百分点，仅使用1.5%原始预算就在代码补全任务上超越完整KV缓存性能。

Conclusion: 学习型压缩策略在KV缓存预算分配方面具有巨大潜力，EvolKV展示了通过动态优化实现内存效率和任务性能双重提升的有效性。

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [40] [Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing](https://arxiv.org/abs/2509.08329)
*Lukas Toral,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 使用预训练大语言模型作为导师，通过学生-教师架构加速强化学习算法的收敛速度，实验表明LLM指导能显著减少训练时间并保持相近的最优性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法在复杂稀疏奖励环境中需要长时间训练，现有加速技术如奖励塑造和课程学习需要专业领域知识且过于特定。研究探索使用预训练LLM作为通用导师来加速RL收敛。

Method: 采用学生-教师架构，使用LLM（Llama、Vicuna、DeepSeek）为RL算法（DQN、PPO、A2C）生成指导建议，并在不同环境（Blackjack、Snake、Connect Four）中进行54种配置的实证研究，探索建议重用机制的效果。

Result: LLM指导显著加速了RL收敛速度，同时保持了可比的最优性能。建议重用机制进一步缩短了训练时间，但导致收敛动态不太稳定。效果对具体任务、RL算法和LLM模型组合敏感。

Conclusion: LLM作为导师能有效加速强化学习训练，但需要根据具体应用场景选择合适的算法和模型组合，建议重用虽能进一步加速但需注意稳定性问题。

Abstract: Reinforcement Learning (RL) algorithms often require long training to become
useful, especially in complex environments with sparse rewards. While
techniques like reward shaping and curriculum learning exist to accelerate
training, these are often extremely specific and require the developer's
professionalism and dedicated expertise in the problem's domain. Tackling this
challenge, in this study, we explore the effectiveness of pre-trained Large
Language Models (LLMs) as tutors in a student-teacher architecture with RL
algorithms, hypothesizing that LLM-generated guidance allows for faster
convergence. In particular, we explore the effectiveness of reusing the LLM's
advice on the RL's convergence dynamics. Through an extensive empirical
examination, which included 54 configurations, varying the RL algorithm (DQN,
PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,
Snake, Connect Four), our results demonstrate that LLM tutoring significantly
accelerates RL convergence while maintaining comparable optimal performance.
Furthermore, the advice reuse mechanism shows a further improvement in training
duration but also results in less stable convergence dynamics. Our findings
suggest that LLM tutoring generally improves convergence, and its effectiveness
is sensitive to the specific task, RL algorithm, and LLM model combination.

</details>


### [41] [Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism](https://arxiv.org/abs/2509.08342)
*Jiaming Yan,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.LG

TL;DR: MoEpic是一个高效的MoE推理系统，通过专家分割机制和预测预取策略，显著降低GPU内存需求和推理延迟


<details>
  <summary>Details</summary>
Motivation: MoE架构在大语言模型中具有潜力，但大量参数导致GPU内存需求过高，现有CPU卸载方法因缓存命中率低和专家加载延迟大而影响推理速度

Method: 提出专家垂直分割机制（top和bottom段），缓存热点专家的top段以提高缓存命中率；预测下一层激活专家并预取；基于定点迭代的自适应缓存配置算法

Result: 实验表明MoEpic可节省约一半GPU成本，相比基线降低推理延迟37.51%-65.73%

Conclusion: MoEpic通过创新的专家分割和智能缓存管理，有效解决了MoE推理中的内存和延迟问题，为MoE模型的广泛应用提供了可行方案

Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern
large language models (LLMs). However, massive parameters impose heavy GPU
memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.
Offloading the expert parameters to CPU RAM offers an effective way to
alleviate the VRAM requirements for MoE inference. Existing approaches
typically cache a small subset of experts in VRAM and dynamically prefetch
experts from RAM during inference, leading to significant degradation in
inference speed due to the poor cache hit rate and substantial expert loading
latency. In this work, we propose MoEpic, an efficient MoE inference system
with a novel expert split mechanism. Specifically, each expert is vertically
divided into two segments: top and bottom. MoEpic caches the top segment of hot
experts, so that more experts will be stored under the limited VRAM budget,
thereby improving the cache hit rate. During each layer's inference, MoEpic
predicts and prefetches the activated experts for the next layer. Since the top
segments of cached experts are exempt from fetching, the loading time is
reduced, which allows efficient transfer-computation overlap. Nevertheless, the
performance of MoEpic critically depends on the cache configuration (i.e., each
layer's VRAM budget and expert split ratio). To this end, we propose a
divide-and-conquer algorithm based on fixed-point iteration for adaptive cache
configuration. Extensive experiments on popular MoE LLMs demonstrate that
MoEpic can save about half of the GPU cost, while lowering the inference
latency by about 37.51%-65.73% compared to the baselines.

</details>


### [42] [Prediction Loss Guided Decision-Focused Learning](https://arxiv.org/abs/2509.08359)
*Haeun Jeon,Hyunglip Bae,Chanyeong Kim,Yongjae Lee,Woo Chang Kim*

Main category: cs.LG

TL;DR: 提出一种通过预测损失梯度扰动决策损失梯度的方法，结合预测聚焦学习和决策聚焦学习的优势，实现更稳定的训练和更好的决策质量


<details>
  <summary>Details</summary>
Motivation: 传统PFL和DFL方法各有局限：PFL优化稳定但忽视下游决策质量，DFL直接优化决策但训练不稳定。需要结合两者优势

Method: 使用预测损失梯度扰动决策损失梯度来构建更新方向，采用sigmoid类衰减参数让预测损失梯度引导决策损失梯度训练预测模型

Result: 在三个随机优化问题上验证了方法的有效性，相比基线实现了更低的regret和更稳定的训练，在PFL或DFL单独表现不佳的情况下也能取得良好效果

Conclusion: 提出的方法无需额外训练，可与任何DFL求解器集成，在理论和实证上都证明了其有效性，能够平衡优化稳定性和决策质量

Abstract: Decision-making under uncertainty is often considered in two stages:
predicting the unknown parameters, and then optimizing decisions based on
predictions. While traditional prediction-focused learning (PFL) treats these
two stages separately, decision-focused learning (DFL) trains the predictive
model by directly optimizing the decision quality in an end-to-end manner.
However, despite using exact or well-approximated gradients, vanilla DFL often
suffers from unstable convergence due to its flat-and-sharp loss landscapes. In
contrast, PFL yields more stable optimization, but overlooks the downstream
decision quality. To address this, we propose a simple yet effective approach:
perturbing the decision loss gradient using the prediction loss gradient to
construct an update direction. Our method requires no additional training and
can be integrated with any DFL solvers. Using the sigmoid-like decaying
parameter, we let the prediction loss gradient guide the decision loss gradient
to train a predictive model that optimizes decision quality. Also, we provide a
theoretical convergence guarantee to Pareto stationary point under mild
assumptions. Empirically, we demonstrate our method across three stochastic
optimization problems, showing promising results compared to other baselines.
We validate that our approach achieves lower regret with more stable training,
even in situations where either PFL or DFL struggles.

</details>


### [43] [Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models](https://arxiv.org/abs/2509.08372)
*Kosuke Kihara,Junki Mori,Taiki Miyagawa,Akinori F. Ebihara*

Main category: cs.LG

TL;DR: 本文提出在联邦学习中用冻结的视觉基础模型(VFM)替代传统特征提取器，有效解决了类别不平衡、域差异和非IID数据分布问题，显著提升了准确率并降低了计算通信成本。


<details>
  <summary>Details</summary>
Motivation: 针对联邦源自由域适应(FFREEDA)中存在的类别不平衡、域偏移和客户端间非IID数据分布等现实挑战，传统方法效果有限，需要重新思考解决方案。

Method: 使用冻结的视觉基础模型(VFM)作为特征提取器替代FFREEDA中的传统骨干网络，无需大量参数调优，直接利用预训练模型的强大特征表示能力。

Result: 实验结果表明VFM能有效缓解域差异、类别不平衡和非IID数据分布的影响，在保持数据隐私的同时显著提升整体准确率，并大幅降低计算和通信成本。

Conclusion: 在现实联邦学习场景中，强大的特征提取器比复杂的适应方法或联邦聚合算法更为关键，视觉基础模型为解决联邦学习中的实际问题提供了有效途径。

Abstract: Federated Learning (FL) offers a framework for training models
collaboratively while preserving data privacy of each client. Recently,
research has focused on Federated Source-Free Domain Adaptation (FFREEDA), a
more realistic scenario wherein client-held target domain data remains
unlabeled, and the server can access source domain data only during
pre-training. We extend this framework to a more complex and realistic setting:
Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account class
imbalances in both the source and target domains, as well as label shifts
between source and target and among target clients. The replication of existing
methods in our experimental setup lead us to rethink the focus from enhancing
aggregation and domain adaptation methods to improving the feature extractors
within the network itself. We propose replacing the FFREEDA backbone with a
frozen vision foundation model (VFM), thereby improving overall accuracy
without extensive parameter tuning and reducing computational and communication
costs in federated learning. Our experimental results demonstrate that VFMs
effectively mitigate the effects of domain gaps, class imbalances, and even
non-IID-ness among target clients, suggesting that strong feature extractors,
not complex adaptation or FL methods, are key to success in the real-world FL.

</details>


### [44] [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)
*Matan Avitan,Moran Baruch,Nir Drucker,Itamar Zimerman,Yoav Goldberg*

Main category: cs.LG

TL;DR: 提出了cutmax算法和HE兼容的nucleus采样方法，显著提升同态加密下LLM文本生成的效率，比基线快24-35倍


<details>
  <summary>Details</summary>
Motivation: 解决同态加密下神经网络文本生成中非多项式解码操作（如argmax和采样）计算开销大的性能瓶颈问题

Method: 开发HE友好的cutmax argmax算法减少密文操作，并提出首个HE兼容的top-p采样方法，两者都是多项式复杂度且可微分

Result: 在真实LLM输出上评估显示延迟比基线减少24-35倍，实现了实用的加密贪婪解码和随机解码

Conclusion: cutmax和HE兼容采样方法为隐私保护设置下的高效推理提供了可行方案，具有强理论保证和显著性能提升

Abstract: Large language models (LLMs) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires decoding methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy decoding under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic decoding with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-preserving
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic LLM outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.

</details>


### [45] [Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models](https://arxiv.org/abs/2509.08401)
*Xunkai Li,Daohan Su,Sicheng Liu,Ru Zhang,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了MoT（Mixture-of-Tinkers）方法来解决图基础模型中的领域泛化冲突问题，包括模型退化和表示坍塌两个核心挑战，通过信息调整和正则化调整机制显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 图基础模型（GFM）在处理多领域图数据时面临领域泛化冲突问题，具体表现为模型退化（编码器和码本无法捕捉输入多样性）和表示坍塌（隐藏嵌入和码本向量无法保持语义可分性），这些pitfalls共同导致预训练优化困境。

Method: 提出MoT框架，包含两个核心组件：(1)信息调整器（Information Tinker）- 使用边级语义融合策略和具有领域感知路由的混合码本来提升信息容量；(2)正则化调整器（Regularization Tinker）- 通过两个额外的正则化项来改进梯度监督。

Result: 在6个领域22个数据集上的实验表明，MoT在有监督、少样本和零样本场景下均取得了显著改进，优于最先进的基线方法。

Conclusion: MoT通过解决信息瓶颈和正则化赤字问题，有效缓解了图基础模型的领域泛化冲突，提供了一个可扩展且可控的架构，遵循GFM的缩放定律。

Abstract: Graph foundation models, inspired by the success of LLMs, are designed to
learn the optimal embedding from multi-domain TAGs for the downstream
cross-task generalization capability. During our investigation, graph VQ-MAE
stands out among the increasingly diverse landscape of GFM architectures. This
is attributed to its ability to jointly encode topology and textual attributes
from multiple domains into discrete embedding spaces with clear semantic
boundaries. Despite its potential, domain generalization conflicts cause
imperceptible pitfalls. In this paper, we instantiate two of them, and they are
just like two sides of the same GFM optimization coin - Side 1 Model
Degradation: The encoder and codebook fail to capture the diversity of inputs;
Side 2 Representation Collapse: The hidden embedding and codebook vector fail
to preserve semantic separability due to constraints from narrow representation
subspaces. These two pitfalls (sides) collectively impair the decoder and
generate the low-quality reconstructed supervision, causing the GFM
optimization dilemma during pre-training (coin). Through empirical
investigation, we attribute the above challenges to Information Bottleneck and
Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -
(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic
fusion strategy and a mixture-of-codebooks with domain-aware routing to improve
information capacity. (2) Regularization Tinker for Optimization Coin, which
utilizes two additional regularizations to further improve gradient supervision
in our proposed Information Tinker. Notably, as a flexible architecture, MoT
adheres to the scaling laws of GFM, offering a controllable model scale.
Compared to SOTA baselines, experiments on 22 datasets across 6 domains
demonstrate that MoT achieves significant improvements in supervised, few-shot,
and zero-shot scenarios.

</details>


### [46] [Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics](https://arxiv.org/abs/2509.08461)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 本研究探索了视觉语言模型(VLMs)在高能物理实验中的中微子相互作用识别任务中的应用，发现经过微调的LLaMa 3.2变体在分类性能和可解释性方面均优于传统卷积神经网络。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型处理多模态数据的能力，探索视觉语言模型在粒子物理实验数据分类中的潜力，特别是在中微子相互作用识别这一重要物理任务中的应用价值。

Method: 使用经过微调的LLaMa 3.2视觉语言模型，将其应用于像素化探测器数据的中微子事件分类任务，并与NOvA和DUNE实验中使用的先进卷积神经网络架构进行基准比较。

Result: 视觉语言模型在分类性能上超越了卷积神经网络，同时提供了更好的可解释性和基于推理的预测能力，能够更灵活地整合辅助文本或语义信息。

Conclusion: 视觉语言模型作为物理事件分类的通用骨干网络具有巨大潜力，其高性能、可解释性和泛化能力为实验性中微子物理中的多模态推理集成开辟了新途径。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their
remarkable capacity to process and reason over structured and unstructured data
modalities beyond natural language. In this work, we explore the applications
of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa
3.2, to the task of identifying neutrino interactions in pixelated detector
data from high-energy physics (HEP) experiments. We benchmark this model
against a state-of-the-art convolutional neural network (CNN) architecture,
similar to those used in the NOvA and DUNE experiments, which have achieved
high efficiency and purity in classifying electron and muon neutrino events.
Our evaluation considers both the classification performance and
interpretability of the model predictions. We find that VLMs can outperform
CNNs, while also providing greater flexibility in integrating auxiliary textual
or semantic information and offering more interpretable, reasoning-based
predictions. This work highlights the potential of VLMs as a general-purpose
backbone for physics event classification, due to their high performance,
interpretability, and generalizability, which opens new avenues for integrating
multimodal reasoning in experimental neutrino physics.

</details>


### [47] [An Interpretable Deep Learning Model for General Insurance Pricing](https://arxiv.org/abs/2509.08467)
*Patrick J. Laub,Tu Pho,Bernard Wong*

Main category: cs.LG

TL;DR: 提出Actuarial Neural Additive Model，一种用于保险定价的可解释深度学习模型，在保持神经网络强大预测能力的同时提供完全透明的结果


<details>
  <summary>Details</summary>
Motivation: 为了解决保险定价中传统模型预测能力有限而复杂机器学习模型缺乏可解释性的问题，需要开发既具有强大预测能力又完全可解释的模型

Method: 为每个协变量和成对交互项分配专用神经网络（或子网络），通过架构约束实现可解释性（如稀疏性）和实际需求（如平滑性、单调性），并建立了保险背景下可解释性的具体定义和数学框架

Result: 在合成和真实保险数据集上的实验表明，该模型在大多数情况下预测精度优于传统精算方法和最先进机器学习方法，同时提供完全透明的内部逻辑

Conclusion: Actuarial Neural Additive Model成功实现了预测能力和可解释性的平衡，为保险定价提供了既准确又透明的解决方案

Abstract: This paper introduces the Actuarial Neural Additive Model, an inherently
interpretable deep learning model for general insurance pricing that offers
fully transparent and interpretable results while retaining the strong
predictive power of neural networks. This model assigns a dedicated neural
network (or subnetwork) to each individual covariate and pairwise interaction
term to independently learn its impact on the modeled output while implementing
various architectural constraints to allow for essential interpretability (e.g.
sparsity) and practical requirements (e.g. smoothness, monotonicity) in
insurance applications. The development of our model is grounded in a solid
foundation, where we establish a concrete definition of interpretability within
the insurance context, complemented by a rigorous mathematical framework.
Comparisons in terms of prediction accuracy are made with traditional actuarial
and state-of-the-art machine learning methods using both synthetic and real
insurance datasets. The results show that the proposed model outperforms other
methods in most cases while offering complete transparency in its internal
logic, underscoring the strong interpretability and predictive capability.

</details>


### [48] [SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms](https://arxiv.org/abs/2509.08482)
*Andrea Maldonado,Christian M. M. Frey,Sai Anirudh Aryasomayajula,Ludwig Zellner,Stephan A. Fahrenkrog-Petersen,Thomas Seidl*

Main category: cs.LG

TL;DR: SHAining方法首次量化事件日志特征对流程挖掘算法指标的边际贡献，通过分析22,000多个事件日志发现特征值与其影响程度的相关性，评估算法鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在固定的事件日志集上评估算法，缺乏对事件日志特征如何单独影响算法的系统分析，且忽视了特征共现对评估指标的影响

Method: 提出SHAining方法，使用流程发现作为下游任务，分析超过22,000个涵盖广泛特征的事件日志，量化不同事件日志特征对算法指标的边际贡献

Result: 揭示了哪些特征对算法各项指标（如拟合度、精确度、复杂度）影响最大，并提供了关于事件日志特征值与其贡献影响相关性的新见解

Conclusion: SHAining方法能够系统评估事件日志特征对流程挖掘算法性能的影响，为算法选择和鲁棒性评估提供了重要工具

Abstract: Process mining aims to extract and analyze insights from event logs, yet
algorithm metric results vary widely depending on structural event log
characteristics. Existing work often evaluates algorithms on a fixed set of
real-world event logs but lacks a systematic analysis of how event log
characteristics impact algorithms individually. Moreover, since event logs are
generated from processes, where characteristics co-occur, we focus on
associational rather than causal effects to assess how strong the overlapping
individual characteristic affects evaluation metrics without assuming isolated
causal effects, a factor often neglected by prior work. We introduce SHAining,
the first approach to quantify the marginal contribution of varying event log
characteristics to process mining algorithms' metrics. Using process discovery
as a downstream task, we analyze over 22,000 event logs covering a wide span of
characteristics to uncover which affect algorithms across metrics (e.g.,
fitness, precision, complexity) the most. Furthermore, we offer novel insights
about how the value of event log characteristics correlates with their
contributed impact, assessing the algorithm's robustness.

</details>


### [49] [Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks](https://arxiv.org/abs/2509.08499)
*Chisom Chibuike,Adeyinka Ogunsanya*

Main category: cs.LG

TL;DR: 本文比较了10种优化器在心脏病预测任务中的性能，发现RMSProp在收敛速度和关键指标上表现最佳，但稳定性不是最好。建议通过全面评估来选择优化器。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练中优化器选择缺乏系统研究，需要深入探讨如何选择优化器以及决定选择的指标。

Method: 使用Kaggle心脏病数据集训练简单的多层感知机模型，在一致的训练范式下比较10种优化器，评估收敛速度、稳定性以及AUC、精确率、召回率等分类指标。

Result: 不同优化器在收敛速度和稳定性之间存在权衡。RMSProp在关键指标上表现最均衡：精确率0.765、召回率0.827、AUC 0.841，训练时间更快，但稳定性不是最佳。

Conclusion: 推荐在计算资源不受限的环境下，采用这种通过全面评估来选择优化器的方法，以提高深度学习模型训练的科学性和性能。

Abstract: Optimization has been an important factor and topic of interest in training
deep learning models, yet less attention has been given to how we select the
optimizers we use to train these models. Hence, there is a need to dive deeper
into how we select the optimizers we use for training and the metrics that
determine this selection. In this work, we compare the performance of 10
different optimizers in training a simple Multi-layer Perceptron model using a
heart disease dataset from Kaggle. We set up a consistent training paradigm and
evaluate the optimizers based on metrics such as convergence speed and
stability. We also include some other Machine Learning Evaluation metrics such
as AUC, Precision, and Recall, which are central metrics to classification
problems. Our results show that there are trade-offs between convergence speed
and stability, as optimizers like Adagrad and Adadelta, which are more stable,
took longer time to converge. Across all our metrics, we chose RMSProp to be
the most effective optimizer for this heart disease prediction task because it
offered a balanced performance across key metrics. It achieved a precision of
0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.
However, it was not the most stable. We recommend that, in less
compute-constrained environments, this method of choosing optimizers through a
thorough evaluation should be adopted to increase the scientific nature and
performance in training deep learning models.

</details>


### [50] [Variational Rank Reduction Autoencoders for Generative](https://arxiv.org/abs/2509.08515)
*Alicia Tierz,Jad Mounayer,Beatriz Moya,Francisco Chinesta*

Main category: cs.LG

TL;DR: 提出了一种结合变分秩降自编码器(VRRAE)和深度算子网络(DeepONet)的混合框架，用于复杂几何形状的生成式热设计，解决了传统方法在高计算成本和潜在空间不连续性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型如自编码器和变分自编码器在热设计中面临高计算成本和潜在空间不连续的问题，导致设计探索能力受限和物理一致性不足。

Method: 使用VRRAE在潜在空间中引入截断SVD，获得连续可解释的结构化表示；然后利用DeepONet，在其分支网络中使用紧凑潜在编码，在主干网络中使用空间坐标，高效预测温度梯度。

Result: 该方法提高了生成几何形状的质量和梯度预测的准确性，相比传统数值求解器在推理效率上有显著优势。

Conclusion: 研究强调了结构化潜在表示对算子学习的重要性，展示了生成模型与算子网络结合在热设计和其他工程应用中的潜力。

Abstract: Generative thermal design for complex geometries is fundamental in many areas
of engineering, yet it faces two main challenges: the high computational cost
of high-fidelity simulations and the limitations of conventional generative
models. Approaches such as autoencoders (AEs) and variational autoencoders
(VAEs) often produce unstructured latent spaces with discontinuities, which
restricts their capacity to explore designs and generate physically consistent
solutions.
  To address these limitations, we propose a hybrid framework that combines
Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks
(DeepONets). The VRRAE introduces a truncated SVD within the latent space,
leading to continuous, interpretable, and well-structured representations that
mitigate posterior collapse and improve geometric reconstruction. The DeepONet
then exploits this compact latent encoding in its branch network, together with
spatial coordinates in the trunk network, to predict temperature gradients
efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries
and the accuracy of gradient prediction, but also provides a substantial
advantage in inference efficiency compared to traditional numerical solvers.
Overall, the study underscores the importance of structured latent
representations for operator learning and highlights the potential of combining
generative models and operator networks in thermal design and broader
engineering applications.

</details>


### [51] [Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures](https://arxiv.org/abs/2509.08530)
*Wen-Bo Xie,Xun Fu,Bin Chen,Yan-Li Lee,Tao Deng,Tian Zou,Xin Wang,Zhen Liu,Jaideep Srivastavad*

Main category: cs.LG

TL;DR: 提出基于图的高效主动聚类算法，使用两个稀疏图结构降低计算成本、减少标注需求并节省内存，在大规模数据处理中实现更准确的聚类效果


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据处理中基于成对约束的主动聚类的效率和可扩展性问题，降低计算成本、减少用户标注需求并优化内存使用

Method: 基于图的主动聚类算法，使用两个稀疏图：一个表示数据关系的数据骨架图，另一个用于更新数据骨架图，通过细化连通子图形成嵌套聚类

Result: 算法在显著减少用户约束输入的情况下实现更准确的聚类，计算性能和可扩展性优于同类方法，且在不同距离度量下保持鲁棒性

Conclusion: 所提出的图基主动聚类算法有效解决了大规模数据处理的效率问题，在减少计算成本和标注需求的同时保持了聚类精度和鲁棒性

Abstract: In this work, we focus on the efficiency and scalability of pairwise
constraint-based active clustering, crucial for processing large-scale data in
applications such as data mining, knowledge annotation, and AI model
pre-training. Our goals are threefold: (1) to reduce computational costs for
iterative clustering updates; (2) to enhance the impact of user-provided
constraints to minimize annotation requirements for precise clustering; and (3)
to cut down memory usage in practical deployments. To achieve these aims, we
propose a graph-based active clustering algorithm that utilizes two sparse
graphs: one for representing relationships between data (our proposed data
skeleton) and another for updating this data skeleton. These two graphs work in
concert, enabling the refinement of connected subgraphs within the data
skeleton to create nested clusters. Our empirical analysis confirms that the
proposed algorithm consistently facilitates more accurate clustering with
dramatically less input of user-provided constraints, and outperforms its
counterparts in terms of computational performance and scalability, while
maintaining robustness across various distance metrics.

</details>


### [52] [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](https://arxiv.org/abs/2509.08578)
*Hong Liu*

Main category: cs.LG

TL;DR: MAESTRO是一个多模态自适应集成模型，用于流感发病率预测，通过融合监测数据、网络搜索趋势和气象数据，结合频谱-时间架构，实现了0.956的R平方值，达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 及时准确的流感发病率预测对公共卫生决策至关重要，需要开发能够融合多模态数据并处理时间序列复杂特征的鲁棒预测模型。

Method: 采用多模态自适应集成方法，首先将时间序列分解为季节性和趋势成分，然后通过Transformer编码器、Mamba状态空间模型、多尺度时间卷积和频域分析模块的混合特征增强管道进行处理，使用跨通道注意力机制整合不同数据模态。

Result: 在香港11年流感数据上的评估显示，模型具有优异的竞争性能，模型拟合度和相对准确性均表现突出，达到了0.956的state-of-the-art R平方值。

Conclusion: MAESTRO提供了一个强大统一的框架，证明了先进的频谱-时间建模和多模态数据融合在流行病学预测中的关键协同作用，其模块化和可复现的管道有助于在其他地区和病原体上的部署和扩展。

Abstract: Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

</details>


### [53] [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
*Aadit Sengupta,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 论文主张将可解释性特别是机制性方法作为AI对齐的核心设计原则，而非辅助诊断工具，认为这是实现安全可信AI的关键


<details>
  <summary>Details</summary>
Motivation: 大型神经网络在高风险场景中的部署引发了对AI行为是否可靠符合人类价值观的担忧，需要内部透明度来确保对齐

Method: 提倡采用机制性可解释性方法（如电路追踪、激活修补）而非事后解释方法（如LIME、SHAP），以获得对内部故障的因果洞察

Result: 机制性可解释性方法能够揭示行为方法可能忽略的欺骗性或未对齐推理模式，但面临可扩展性、认知不确定性和表示不匹配等挑战

Conclusion: 安全可信AI的进展取决于将可解释性作为AI研发的一等目标，确保系统不仅有效而且可审计、透明且符合人类意图

Abstract: Large neural models are increasingly deployed in high-stakes settings,
raising concerns about whether their behavior reliably aligns with human
values. Interpretability provides a route to internal transparency by revealing
the computations that drive outputs. We argue that interpretability especially
mechanistic approaches should be treated as a design principle for alignment,
not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer
intuitive but correlational explanations, while mechanistic techniques like
circuit tracing or activation patching yield causal insight into internal
failures, including deceptive or misaligned reasoning that behavioral methods
like RLHF, red teaming, or Constitutional AI may overlook. Despite these
advantages, interpretability faces challenges of scalability, epistemic
uncertainty, and mismatches between learned representations and human concepts.
Our position is that progress on safe and trustworthy AI will depend on making
interpretability a first-class objective of AI research and development,
ensuring that systems are not only effective but also auditable, transparent,
and aligned with human intent.

</details>


### [54] [Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques](https://arxiv.org/abs/2509.08606)
*Alireza Sameh,Mehrdad Rostami,Mourad Oussalah,Vahid Farrahi*

Main category: cs.LG

TL;DR: 深度学习算法（LSTM、BiLSTM、GRU）使用原始加速度信号在24小时运动行为分类中达到约85%的准确率，略优于使用手工特征的传统机器学习方法（70-81%准确率）。


<details>
  <summary>Details</summary>
Motivation: 比较深度学习和传统机器学习算法在24小时运动行为（睡眠、久坐、轻度活动、中高强度活动）分类中的性能差异。

Method: 使用151名成年人手腕加速度计数据，提取104个手工特征，分别用原始信号和手工特征训练4种深度学习算法（LSTM、BiLSTM、GRU、1D-CNN）和6种传统机器学习算法（RF、SVM、XGBoost、LR、ANN、DT）。

Result: LSTM、BiLSTM和GRU使用原始信号达到约85%准确率，1D-CNN约80%；使用手工特征时所有算法准确率在70-81%之间。中高强度活动和轻度活动分类混淆度较高。

Conclusion: 使用原始加速度信号的深度学习方法在预测24小时运动行为强度方面仅略优于使用手工特征的深度学习和传统机器学习方法。

Abstract: Purpose: We compared the performance of deep learning (DL) and classical
machine learning (ML) algorithms for the classification of 24-hour movement
behavior into sleep, sedentary, light intensity physical activity (LPA), and
moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access
data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.
Participants were randomly divided into training, validation, and test sets
(121, 15, and 15 participants each). Raw acceleration signals were segmented
into non-overlapping 10-second windows, and then a total of 104 handcrafted
features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),
Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and
One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw
acceleration signals and with handcrafted features extracted from these signals
to predict 24-hour movement behavior categories. The handcrafted features were
also used to train classical ML algorithms, namely Random Forest (RF), Support
Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression
(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying
24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an
overall accuracy of approximately 85% when trained with raw acceleration
signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on
handcrafted features, the overall accuracy for both DL and classical ML
algorithms ranged from 70% to 81%. Overall, there was a higher confusion in
classification of MVPA and LPA, compared to sleep and sedentary categories.
Conclusion: DL methods with raw acceleration signals had only slightly better
performance in predicting 24-hour movement behavior intensities, compared to
when DL and classical ML were trained with handcrafted features.

</details>


### [55] [Towards Interpretable Deep Neural Networks for Tabular Data](https://arxiv.org/abs/2509.08617)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab是一种可解释的神经网络架构，使用稀疏自编码器学习语义特征字典，将预测表示为可解释组件的线性组合，在保持高性能的同时提供完全可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然针对表格数据的DNN在预测性能上表现优异，但它们是黑盒模型，缺乏可解释性，这在金融和医疗等关键应用领域是一个重要问题。

Method: 使用稀疏自编码器(SAE)在潜在空间中学习单语义特征字典，并通过自动化方法为这些特征分配人类可解释的语义，使预测能够表示为语义有意义组件的线性组合。

Result: 实证评估表明，XNNTab在性能上与或超过最先进的黑盒神经网络模型和经典机器学习方法，同时保持完全可解释性。

Conclusion: XNNTab成功解决了表格数据神经网络模型的可解释性问题，在保持高性能的同时提供了完全透明的预测解释能力。

Abstract: Tabular data is the foundation of many applications in fields such as finance
and healthcare. Although DNNs tailored for tabular data achieve competitive
predictive performance, they are blackboxes with little interpretability. We
introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to
learn a dictionary of monosemantic features within the latent space used for
prediction. Using an automated method, we assign human-interpretable semantics
to these features. This allows us to represent predictions as linear
combinations of semantically meaningful components. Empirical evaluations
demonstrate that XNNTab attains performance on par with or exceeding that of
state-of-the-art, black-box neural models and classical machine learning
approaches while being fully interpretable.

</details>


### [56] [An upper bound of the silhouette validation metric for clustering](https://arxiv.org/abs/2509.08625)
*Hugo Sträng,Tai Dinh*

Main category: cs.LG

TL;DR: 本文提出了针对轮廓系数的新上界计算方法，为聚类质量评估提供了数据依赖的紧致上界，解决了传统上界1通常无法达到的问题。


<details>
  <summary>Details</summary>
Motivation: 轮廓系数是常用的聚类质量内部评估指标，但传统的上界1往往无法在实际数据集中达到，且数据集特定的最大轮廓宽度通常是未知的，这限制了轮廓系数的评估效果。

Method: 通过为数据集中的每个数据点推导出其轮廓宽度的尖锐上界，然后聚合这些个体上界，得到数据依赖的ASW上界。

Result: 在合成和真实数据集上的实验表明，所提出的上界在许多情况下被证明是接近紧致的，显著丰富了聚类质量评估的能力。

Conclusion: 该方法能够指示单个数据点是否能够被良好放置，支持基于轮廓的优化循环的早期停止，并帮助回答聚类结果与特定数据上最佳可能结果的接近程度。

Abstract: The silhouette coefficient summarizes, per observation, cohesion versus
separation in [-1, 1]; the average silhouette width (ASW) is a common internal
measure of clustering quality where higher values indicate more coveted
results. However, the dataset-specific maximum of ASW is typically unknown, and
the standard upper limit 1 is often unattainable. In this work, we derive for
each data point in a given dataset a sharp upper bound on its silhouette width.
By aggregating these individual bounds, we present a canonical data-dependent
upper bound on ASW that often assumes values well below 1. The presented bounds
can indicate whether individual data points can ever be well placed, enable
early stopping of silhouette-based optimization loops, and help answer a key
question: How close is my clustering result to the best possible outcome on
this specific data? Across synthetic and real datasets, the bounds are provably
near-tight in many cases and offer significant enrichment of cluster quality
evaluation.

</details>


### [57] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: 提出Generative Data Refinement (GDR)框架，使用预训练生成模型将有不良内容的原始数据集转化为更适合训练的净化数据集，解决数据耗尽问题。


<details>
  <summary>Details</summary>
Motivation: 随着模型参数固定，模型能力主要取决于训练数据质量和数量。当前训练数据集增长速度超过网络新数据索引速度，预计未来十年将面临数据耗尽。大量用户生成内容未被公开索引，但直接使用存在隐私泄露和不良内容风险。

Method: GDR框架通过预训练生成模型，以原始数据集中的每个样本为条件生成合成数据，将含有不良内容的数据集转化为净化后的训练数据集。

Result: 实验显示GDR在数据集匿名化方面优于行业级解决方案，能直接对高度不安全数据集进行去毒处理。生成的合成数据自然匹配网络规模数据集的多样性，避免了通过模型提示生成多样化合成数据的挑战。

Conclusion: GDR的简单性和有效性使其成为扩展前沿模型训练数据总量的强大工具，为解决数据耗尽问题提供了有效方案。

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [58] [Replicable Reinforcement Learning with Linear Function Approximation](https://arxiv.org/abs/2509.08660)
*Eric Eaton,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 该论文针对强化学习中的可复现性问题，提出了首个适用于线性函数逼近的、可证明高效的可复现强化学习算法，包括生成模型和情景设置下的解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域面临实验结果的复现挑战，特别是在强化学习中算法存在不稳定性。虽然表格强化学习已有可复现算法，但扩展到更实用的函数逼近设置一直是个开放问题。

Method: 首先开发了两种高效的可复现随机设计回归和未中心化协方差估计算法，然后利用这些工具为线性马尔可夫决策过程提供了首个可证明高效的可复现强化学习算法。

Result: 提出了在生成模型和情景设置下都可证明高效的可复现强化学习算法，并通过实验验证了这些算法能够激发更一致的神经策略。

Conclusion: 这项工作在强化学习的可复现性方面取得了重要进展，为线性函数逼近提供了首个可证明高效的可复现算法，具有重要的理论和实践意义。

Abstract: Replication of experimental results has been a challenge faced by many
scientific disciplines, including the field of machine learning. Recent work on
the theory of machine learning has formalized replicability as the demand that
an algorithm produce identical outcomes when executed twice on different
samples from the same distribution. Provably replicable algorithms are
especially interesting for reinforcement learning (RL), where algorithms are
known to be unstable in practice. While replicable algorithms exist for tabular
RL settings, extending these guarantees to more practical function
approximation settings has remained an open problem. In this work, we make
progress by developing replicable methods for linear function approximation in
RL. We first introduce two efficient algorithms for replicable random design
regression and uncentered covariance estimation, each of independent interest.
We then leverage these tools to provide the first provably efficient replicable
RL algorithms for linear Markov decision processes in both the generative model
and episodic settings. Finally, we evaluate our algorithms experimentally and
show how they can inspire more consistent neural policies.

</details>


### [59] [Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data](https://arxiv.org/abs/2509.08679)
*Jingya Cheng,Jiazi Tian,Federica Spoto,Alaleh Azhir,Daniel Mork,Hossein Estiri*

Main category: cs.LG

TL;DR: 开发信号保真度指数(SFI)量化痴呆症诊断数据质量，通过SFI感知校准显著提升机器学习模型在不同医疗系统间的泛化性能


<details>
  <summary>Details</summary>
Motivation: 电子健康记录(EHR)训练的机器学习模型在跨医疗系统时因分布偏移而性能下降，诊断信号衰减(诊断质量和一致性差异)是重要但未被充分探索的因素

Method: 构建模拟框架生成2,500个合成数据集，SFI包含六个可解释组件：诊断特异性、时间一致性、熵、上下文一致性、药物对齐和轨迹稳定性。应用SFI感知校准进行乘法调整优化

Result: 在最优参数(α=2.0)下，SFI感知校准显著改善所有指标(p<0.001)，平衡准确率提升10.3%，召回率提升32.5%，精确率提升31.9%，F1分数提升26.1%

Conclusion: 诊断信号衰减是模型泛化的可处理障碍，SFI感知校准为缺乏结果标签的大规模管理数据集提供了实用的无标签策略来增强跨医疗环境的预测能力

Abstract: \textbf{Background:} Machine learning models trained on electronic health
records (EHRs) often degrade across healthcare systems due to distributional
shift. A fundamental but underexplored factor is diagnostic signal decay:
variability in diagnostic quality and consistency across institutions, which
affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying
diagnostic data quality at the patient level in dementia, and to test SFI-aware
calibration for improving model performance across heterogeneous datasets
without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic
datasets, each with 1,000 patients and realistic demographics, encounters, and
coding patterns based on dementia risk factors. The SFI was derived from six
interpretable components: diagnostic specificity, temporal consistency,
entropy, contextual concordance, medication alignment, and trajectory
stability. SFI-aware calibration applied a multiplicative adjustment, optimized
across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware
calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from
10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in
Precision (31.9\%) and F1-score (26.1\%). Performance approached reference
standards, with F1-score and Recall within 1\% and Balanced Accuracy and
Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model
generalization. SFI-aware calibration provides a practical, label-free strategy
to enhance prediction across healthcare contexts, particularly for large-scale
administrative datasets lacking outcome labels.

</details>


### [60] [Perfectly-Private Analog Secure Aggregation in Federated Learning](https://arxiv.org/abs/2509.08683)
*Delio Jaramillo-Velez,Charul Rajput,Ragnar Freij-Hollanti,Camilla Hollanti,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出了一种基于环面而非有限域的新型安全参数聚合方法，在联邦学习中实现完美隐私保护的同时避免精度损失。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通过安全多方计算实现安全聚合，但实数数据无法实现完美隐私，而有限域方法存在精度复杂度权衡问题。

Method: 采用环面上的均匀分布进行安全参数聚合，利用环面的特性既保证完美隐私又避免精度损失。

Result: 实验表明新协议在保持完美隐私的同时，性能接近无安全聚合的模型，在某些情况下显著优于有限域方法。

Conclusion: 基于环面的安全聚合协议是更安全的选择，能在保证完美隐私的同时维持模型精度。

Abstract: In federated learning, multiple parties train models locally and share their
parameters with a central server, which aggregates them to update a global
model. To address the risk of exposing sensitive data through local models,
secure aggregation via secure multiparty computation has been proposed to
enhance privacy. At the same time, perfect privacy can only be achieved by a
uniform distribution of the masked local models to be aggregated. This raises a
problem when working with real valued data, as there is no measure on the reals
that is invariant under the masking operation, and hence information leakage is
bound to occur. Shifting the data to a finite field circumvents this problem,
but as a downside runs into an inherent accuracy complexity tradeoff issue due
to fixed point modular arithmetic as opposed to floating point numbers that can
simultaneously handle numbers of varying magnitudes. In this paper, a novel
secure parameter aggregation method is proposed that employs the torus rather
than a finite field. This approach guarantees perfect privacy for each party's
data by utilizing the uniform distribution on the torus, while avoiding
accuracy losses. Experimental results show that the new protocol performs
similarly to the model without secure aggregation while maintaining perfect
privacy. Compared to the finite field secure aggregation, the torus-based
protocol can in some cases significantly outperform it in terms of model
accuracy and cosine similarity, hence making it a safer choice.

</details>


### [61] [Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective](https://arxiv.org/abs/2509.08697)
*James Gong,Raymond Luo,Emma Wang,Leon Ge,Bruce Li,Felix Marattukalam,Waleed Abdulla*

Main category: cs.LG

TL;DR: FAUST算法通过将Forward-Forward算法与相似性学习框架结合，消除了推理时多次前向传播的需求，显著提高了准确率并接近反向传播的性能。


<details>
  <summary>Details</summary>
Motivation: 反向传播算法存在生物学上不合理的问题，而Forward-Forward算法虽然更符合生物学原理，但在准确率和推理效率方面明显落后于反向传播。

Method: 将Forward-Forward算法与基于相似性的Tuplet损失函数框架相结合，提出FAUST算法，避免了推理时需要多次前向传播的问题。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上的实验表明，FAUST显著提高了准确率。在CIFAR-10上使用简单多层感知机架构达到56.22%的准确率，接近反向传播的57.63%基准。

Conclusion: FAUST算法成功解决了Forward-Forward算法的推理效率问题，同时大幅提升了准确率，使其更接近反向传播的性能表现，为生物合理性学习算法的发展提供了新方向。

Abstract: Backpropagation is the pivotal algorithm underpinning the success of
artificial neural networks, yet it has critical limitations such as
biologically implausible backward locking and global error propagation. To
circumvent these constraints, the Forward-Forward algorithm was proposed as a
more biologically plausible method that replaces the backward pass with an
additional forward pass. Despite this advantage, the Forward-Forward algorithm
significantly trails backpropagation in accuracy, and its optimal form exhibits
low inference efficiency due to multiple forward passes required. In this work,
the Forward-Forward algorithm is reshaped through its integration with
similarity learning frameworks, eliminating the need for multiple forward
passes during inference. This proposed algorithm is named Forward-Forward
Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical
evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST
substantially improves accuracy, narrowing the gap with backpropagation. On
CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron
architecture, approaching the backpropagation benchmark of 57.63\% accuracy.

</details>


### [62] [A layered architecture for log analysis in complex IT systems](https://arxiv.org/abs/2509.08698)
*Thorsten Wittkopp*

Main category: cs.LG

TL;DR: 提出三层架构支持DevOps故障解决：日志调查层实现自动日志标注和异常分类；异常检测层提供灵活检测方法；根因分析层识别关键故障日志。整体架构显著提升IT系统可靠性。


<details>
  <summary>Details</summary>
Motivation: IT系统日益复杂，DevOps团队在实施和维护中面临挑战。日志分析作为AIOps核心，需要为复杂行为和故障提供关键洞察，帮助团队高效解决故障。

Method: 三层架构：1)日志调查层-自动日志标注和异常分类，提出无监督标注方法和三分类异常分类法；2)异常检测层-灵活适应无监督、弱监督和监督训练的异常检测方法；3)根因分析层-通过平衡训练数据和识别关键服务来识别最小故障日志集。

Result: 异常检测在公开和工业数据集上F1分数达0.98-1.0；根因分析在top10候选中检测到90-98%的根因日志行；整体架构为故障缓解提供可操作见解。

Conclusion: 该研究展示了如何设计和优化日志分析方法来帮助DevOps高效解决故障。通过整合三层架构，为团队提供了增强IT系统可靠性的稳健方法。

Abstract: In the evolving IT landscape, stability and reliability of systems are
essential, yet their growing complexity challenges DevOps teams in
implementation and maintenance. Log analysis, a core element of AIOps, provides
critical insights into complex behaviors and failures. This dissertation
introduces a three-layered architecture to support DevOps in failure
resolution. The first layer, Log Investigation, performs autonomous log
labeling and anomaly classification. We propose a method that labels log data
without manual effort, enabling supervised training and precise evaluation of
anomaly detection. Additionally, we define a taxonomy that groups anomalies
into three categories, ensuring appropriate method selection. The second layer,
Anomaly Detection, detects behaviors deviating from the norm. We propose a
flexible Anomaly Detection method adaptable to unsupervised, weakly supervised,
and supervised training. Evaluations on public and industry datasets show
F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third
layer, Root Cause Analysis, identifies minimal log sets describing failures,
their origin, and event sequences. By balancing training data and identifying
key services, our Root Cause Analysis method consistently detects 90-98% of
root cause log lines within the top 10 candidates, providing actionable
insights for mitigation. Our research addresses how log analysis methods can be
designed and optimized to help DevOps resolve failures efficiently. By
integrating these three layers, the architecture equips teams with robust
methods to enhance IT system reliability.

</details>


### [63] [Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping](https://arxiv.org/abs/2509.08703)
*Nikasadat Emami,Amirhossein Khalilian-Gourtani,Jianghao Qian,Antoine Ratouchniak,Xupeng Chen,Yao Wang,Adeen Flinker*

Main category: cs.LG

TL;DR: 开发机器学习模型预测大脑皮层区域是否对语言功能关键，结合神经活动信号、解剖区域标签和功能连接特征，在ECoG数据上达到高准确率


<details>
  <summary>Details</summary>
Motivation: ESM是临床金标准但侵入性强且耗时，需要非侵入性方法在脑手术前准确识别语言关键区域

Method: 分析16名参与者的颅内ECoG数据，整合神经活动信号、解剖区域标签和功能连接特征，使用RBF核SVM和MLP聚合进行电极级别分类

Result: 最佳模型在保留参与者测试中表现优异（ROC-AUC: 0.87, PR-AUC: 0.57），区域和连接特征组合效果最佳

Conclusion: 结合空间和网络信息的非线性建模可显著改善术前功能映射，为安全脑手术提供重要工具

Abstract: Identifying cortical regions critical for speech is essential for safe brain
surgery in or near language areas. While Electrical Stimulation Mapping (ESM)
remains the clinical gold standard, it is invasive and time-consuming. To
address this, we analyzed intracranial electrocorticographic (ECoG) data from
16 participants performing speech tasks and developed machine learning models
to directly predict if the brain region underneath each ECoG electrode is
critical. Ground truth labels indicating speech arrest were derived
independently from Electrical Stimulation Mapping (ESM) and used to train
classification models. Our framework integrates neural activity signals,
anatomical region labels, and functional connectivity features to capture both
local activity and network-level dynamics. We found that models combining
region and connectivity features matched the performance of the full feature
set, and outperformed models using either type alone. To classify each
electrode, trial-level predictions were aggregated using an MLP applied to
histogram-encoded scores. Our best-performing model, a trial-level RBF-kernel
Support Vector Machine together with MLP-based aggregation, achieved strong
accuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings
highlight the value of combining spatial and network information with
non-linear modeling to improve functional mapping in presurgical evaluation.

</details>


### [64] [Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing](https://arxiv.org/abs/2509.08709)
*Shun Takagi,Satoshi Hasegawa*

Main category: cs.LG

TL;DR: 提出了一种恶意安全DP-FTRL方案，通过临时TEE模块实现服务器行为可验证证明，解决了现有半诚实假设下的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有DP-FTRL方法假设半诚实服务器，无法应对客户端退出或被腐化的实际场景，而直接使用TEE会引入分叉攻击和可用性问题。

Method: 在服务器端部署临时TEE模块作为可信计算基，生成服务器行为可验证证明，部分客户端参与审计，保持系统可扩展性和活性。

Result: 形式化证明显示在恶意设置下仍能保证隐私性，实验表明在多种现实设置中仅给客户端带来小的常数开销。

Conclusion: 该方案有效减小了TCB规模，同时维持系统性能，为恶意安全DP-FTRL提供了实用解决方案。

Abstract: In cross-device private federated learning, differentially private
follow-the-regularized-leader (DP-FTRL) has emerged as a promising
privacy-preserving method. However, existing approaches assume a semi-honest
server and have not addressed the challenge of securely removing this
assumption. This is due to its statefulness, which becomes particularly
problematic in practical settings where clients can drop out or be corrupted.
While trusted execution environments (TEEs) might seem like an obvious
solution, a straightforward implementation can introduce forking attacks or
availability issues due to state management. To address this problem, our paper
introduces a novel server extension that acts as a trusted computing base (TCB)
to realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral
TEE module on the server side to produce verifiable proofs of server actions.
Some clients, upon being selected, participate in auditing these proofs with
small additional communication and computational demands. This extension
solution reduces the size of the TCB while maintaining the system's scalability
and liveness. We provide formal proofs based on interactive differential
privacy, demonstrating privacy guarantee in malicious settings. Finally, we
experimentally show that our framework adds small constant overhead to clients
in several realistic settings.

</details>


### [65] [Compressing CNN models for resource-constrained systems by channel and layer pruning](https://arxiv.org/abs/2509.08714)
*Ahmed Sadaqa,Di Liu*

Main category: cs.LG

TL;DR: 提出了一种结合通道剪枝和层剪枝的混合剪枝框架，通过逆向应用EfficientNet的缩放原则来缩减CNN模型复杂度，在保持精度的同时显著降低模型延迟。


<details>
  <summary>Details</summary>
Motivation: CNN模型复杂度不断增加，在边缘设备上部署面临挑战，需要有效的模型压缩技术来减小网络规模和计算需求。

Method: 采用混合剪枝方法，同时进行通道剪枝和层剪枝，逆向应用EfficientNet的缩放原则来缩减网络规模。

Result: 实验显示模型复杂度显著降低，精度损失极小，在NVIDIA JETSON TX2设备上部署时延迟明显减少。

Conclusion: 混合剪枝框架是有效的模型压缩方法，能够在保持模型性能的同时显著提升边缘设备上的部署效率。

Abstract: Convolutional Neural Networks (CNNs) have achieved significant breakthroughs
in various fields. However, these advancements have led to a substantial
increase in the complexity and size of these networks. This poses a challenge
when deploying large and complex networks on edge devices. Consequently, model
compression has emerged as a research field aimed at reducing the size and
complexity of CNNs. One prominent technique in model compression is model
pruning. This paper will present a new technique of pruning that combines both
channel and layer pruning in what is called a "hybrid pruning framework".
Inspired by EfficientNet, a renowned CNN architecture known for scaling up
networks from both channel and layer perspectives, this hybrid approach applies
the same principles but in reverse, where it scales down the network through
pruning. Experiments on the hybrid approach demonstrated a notable decrease in
the overall complexity of the model, with only a minimal reduction in accuracy
compared to the baseline model. This complexity reduction translates into
reduced latency when deploying the pruned models on an NVIDIA JETSON TX2
embedded AI device.

</details>


### [66] [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
*Jeffrey Amico,Gabriel Passamani Andrade,John Donaghy,Ben Fielding,Tristin Forbus,Harry Grieve,Semih Kara,Jari Kolehmainen,Yihua Lou,Christopher Nies,Edward Phillip Flores Nuño,Diogo Ortega,Shikhar Rastogi,Austin Virts,Matthew J. Wright*

Main category: cs.LG

TL;DR: SAPO是一种完全去中心化的异步RL后训练算法，用于语言模型，解决了传统RL训练中的并行化挑战和成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练需要大量并行化推理，带来延迟、内存、可靠性等技术挑战以及高昂成本，需要一种去中心化的解决方案。

Method: SAPO算法在异构计算节点组成的去中心化网络中运行，每个节点管理自己的策略模型并与其他节点共享rollouts，无需假设延迟、模型同质性或硬件要求。

Result: 在控制实验中实现高达94%的累积奖励增益，并在数千个节点的开源演示网络中成功测试。

Conclusion: SAPO有效避免了RL后训练的扩展瓶颈，支持异构硬件和模型，促进了学习过程的引导和知识传播。

Abstract: Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs
requires significant parallelization to scale-up inference, which introduces
non-trivial technical challenges (e.g. latency, memory, and reliability)
alongside ever-growing financial costs. We present Swarm sAmpling Policy
Optimization (SAPO), a fully decentralized and asynchronous RL post-training
algorithm. SAPO is designed for decentralized networks of heterogenous compute
nodes, where each node manages its own policy model(s) while "sharing" rollouts
with others in the network; no explicit assumptions about latency, model
homogeneity, or hardware are required and nodes can operate in silo if desired.
As a result, the algorithm avoids common bottlenecks in scaling RL
post-training while also allowing (and even encouraging) new possibilities. By
sampling rollouts "shared" across the network, it enables "Aha moments" to
propagate, thereby bootstrapping the learning process. In this paper we show
SAPO achieved cumulative reward gains of up to 94% in controlled experiments.
We also share insights from tests on a network with thousands of nodes
contributed by Gensyn community members running the algorithm on diverse
hardware and models during an open-source demo.

</details>


### [67] [DEQuify your force field: More efficient simulations using deep equilibrium models](https://arxiv.org/abs/2509.08734)
*Andreas Burger,Luca Thiede,Alán Aspuru-Guzik,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: 该论文提出将等变基础模型重构为深度平衡模型，通过重用先前时间步的中间神经网络特征来利用分子动力学模拟的连续性，在多个数据集上实现了10%-20%的精度和速度提升。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟具有连续性特征，连续状态极其相似，这一重要的先验信息尚未被充分利用。

Method: 将最先进的等变基础模型重构为深度平衡模型，重用先前时间步的中间神经网络特征。

Result: 在MD17、MD22和OC20 200k数据集上，相比非DEQ基础模型，准确性和速度均提升10%-20%，训练内存效率更高，能够训练更大系统的更富表达力模型。

Conclusion: 利用分子动力学模拟的连续性特征，通过深度平衡模型重用中间特征，可以有效提升机器学习力场的性能和效率。

Abstract: Machine learning force fields show great promise in enabling more accurate
molecular dynamics simulations compared to manually derived ones. Much of the
progress in recent years was driven by exploiting prior knowledge about
physical systems, in particular symmetries under rotation, translation, and
reflections. In this paper, we argue that there is another important piece of
prior information that, thus fa,r hasn't been explored: Simulating a molecular
system is necessarily continuous, and successive states are therefore extremely
similar. Our contribution is to show that we can exploit this information by
recasting a state-of-the-art equivariant base model as a deep equilibrium
model. This allows us to recycle intermediate neural network features from
previous time steps, enabling us to improve both accuracy and speed by
$10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ
base model. The training is also much more memory efficient, allowing us to
train more expressive models on larger systems.

</details>


### [68] [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)
*Dong Han,Zhehong Ai,Pengxiang Cai,Shuzhou Sun,Shanya Lu,Jianpeng Chen,Ben Gao,Lingli Ge,Weida Wang,Xiangxin Zhou,Xihui Liu,Mao Su,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Tao XU,Yuqiang Li,Shufei Zhang*

Main category: cs.LG

TL;DR: ChemBOMAS是一个LLM增强的多智能体系统，通过知识驱动的粗粒度优化和数据驱动的细粒度优化策略，显著提升了贝叶斯优化在化学领域的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决化学领域中贝叶斯优化因实验数据稀疏和反应机制复杂而效率低下的问题。

Method: 结合LLM的智能分解搜索空间能力，采用两阶段优化策略：知识驱动的粗粒度优化识别候选区域，数据驱动的细粒度优化生成伪数据点加速收敛。

Result: 在基准测试中显著优于多种BO算法，湿实验中获得96%的最优目标值，远超领域专家的15%。

Conclusion: ChemBOMAS是加速化学发现的强大工具，具有实际应用价值。

Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by sparse experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by LLMs and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
LLMs intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, LLMs enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.

</details>


### [69] [PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints](https://arxiv.org/abs/2509.08750)
*Yuanchun Guo,Bingyan Liu,Yulong Sha,Zhensheng Xian*

Main category: cs.LG

TL;DR: 构建首个系统平台PracMHBench，在边缘设备实际约束下评估模型异构联邦学习算法，通过大量实验分析不同算法在各种数据场景和指标下的适用性


<details>
  <summary>Details</summary>
Motivation: 现有模型异构联邦学习研究缺乏在实际边缘设备约束下的定量分析，需要重新评估该范式在不同数据场景和指标下的表现

Method: 构建PracMHBench系统平台，对多种模型异构算法进行分类并在多个数据任务和指标上进行测试，进行广泛的实验分析

Result: 通过平台评估了不同边缘约束下各种算法的适用性和对应的异构模式

Conclusion: 该工作填补了模型异构联邦学习在实际边缘设备约束下评估的空白，为算法选择和应用提供了实践指导

Abstract: Federating heterogeneous models on edge devices with diverse resource
constraints has been a notable trend in recent years. Compared to traditional
federated learning (FL) that assumes an identical model architecture to
cooperate, model-heterogeneous FL is more practical and flexible since the
model can be customized to satisfy the deployment requirement. Unfortunately,
no prior work ever dives into the existing model-heterogeneous FL algorithms
under the practical edge device constraints and provides quantitative analysis
on various data scenarios and metrics, which motivates us to rethink and
re-evaluate this paradigm. In our work, we construct the first system platform
\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical
constraints of edge devices, where diverse model heterogeneity algorithms are
classified and tested on multiple data tasks and metrics. Based on the
platform, we perform extensive experiments on these algorithms under the
different edge constraints to observe their applicability and the corresponding
heterogeneity pattern.

</details>


### [70] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出了AgentGym-RL框架和ScalingInter-RL训练方法，通过强化学习训练LLM智能体进行多轮交互决策，在27个任务上达到或超越商业模型性能


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一的交互式强化学习框架来从零开始训练LLM智能体，无需监督微调就能在多样化现实环境中有效学习

Method: 采用模块化解耦架构的AgentGym-RL框架，支持主流RL算法；提出ScalingInter-RL训练方法，早期限制交互次数强调利用，后期扩大交互范围鼓励探索

Result: 在27个多样化环境任务中，训练的智能体达到或超越了商业模型的性能表现

Conclusion: 该框架和方法有效解决了LLM智能体训练中的探索-利用平衡和稳定性问题，将开源完整框架以推动智能体研究发展

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [71] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 开发了一个基于深度强化学习的AI决策支持系统，用于优化大规模伤亡事件中的患者转运分配，通过模拟实验证明AI辅助能显著提升决策质量和一致性，使非专家达到专家水平。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件(MCI)会压垮医疗系统，需要在极端压力下快速准确地做出患者-医院分配决策，传统方法难以应对这种复杂决策需求。

Method: 开发了基于深度强化学习的AI代理，考虑患者危急程度、专科护理需求、医院容量和运输物流等因素；构建了MasTER网络指挥仪表板；通过30名参与者（6名创伤专家和24名非专家）进行对照用户研究，比较三种交互方式（纯人工、人机协作、纯AI）在20和60名患者场景下的表现。

Result: AI参与度增加显著提高决策质量和一致性；AI代理表现优于创伤外科医生(p < 0.001)；AI辅助下非专家能达到专家水平，而未辅助时表现显著较差(p < 0.001)。

Conclusion: AI驱动的决策支持系统有潜力提升大规模伤亡事件的准备培训和实际应急响应管理能力。

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


### [72] [Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning](https://arxiv.org/abs/2509.08759)
*Mominul Rubel,Adam Meyers,Gabriel Nicolosi*

Main category: cs.LG

TL;DR: FLM是一种新型神经网络架构，使用余弦激活函数构建多维非谐波傅里叶级数，能够学习频率、振幅和相移作为可训练参数，在科学计算问题上表现优异


<details>
  <summary>Details</summary>
Motivation: 设计一个能够表示完整可分离多维傅里叶基的标准神经网络架构，解决现有傅里叶启发模型无法在多维空间中表示完整基的问题

Method: 采用前馈结构，使用余弦激活函数，将频率、振幅和相移作为可训练参数，构建问题特定的谱基，适用于周期和非周期函数

Result: FLM在偏微分方程和最优控制问题等科学计算任务中表现与SIREN和标准前馈神经网络相当甚至更优，建立了傅里叶系数与振幅相移之间的一一对应关系

Conclusion: FLM是第一个能够使用标准MLP架构表示完整可分离多维傅里叶基的神经网络，为科学计算问题提供了有效的谱表示方法

Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN)
architecture designed to represent a multidimensional nonharmonic Fourier
series. The FLM uses a simple feedforward structure with cosine activation
functions to learn the frequencies, amplitudes, and phase shifts of the series
as trainable parameters. This design allows the model to create a
problem-specific spectral basis adaptable to both periodic and nonperiodic
functions. Unlike previous Fourier-inspired NN models, the FLM is the first
architecture able to represent a complete, separable Fourier basis in multiple
dimensions using a standard Multilayer Perceptron-like architecture. A
one-to-one correspondence between the Fourier coefficients and amplitudes and
phase-shifts is demonstrated, allowing for the translation between a full,
separable basis form and the cosine phase--shifted one. Additionally, we
evaluate the performance of FLMs on several scientific computing problems,
including benchmark Partial Differential Equations (PDEs) and a family of
Optimal Control Problems (OCPs). Computational experiments show that the
performance of FLMs is comparable, and often superior, to that of established
architectures like SIREN and vanilla feedforward NNs.

</details>


### [73] [ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals](https://arxiv.org/abs/2509.08779)
*Ali Amini,Mohammad Alijanpour,Behnam Latifi,Ali Motie Nasrabadi*

Main category: cs.LG

TL;DR: 本文提出ADHDeepNet深度学习模型，利用EEG信号和时空特征提取，通过数据增强和交叉验证，实现了ADHD诊断99.17%的准确率。


<details>
  <summary>Details</summary>
Motivation: ADHD是一种常见的儿童脑部疾病，早期诊断对患者和社会至关重要，但传统诊断方法耗时耗力。需要开发更精准高效的诊断方法。

Method: 提出ADHDeepNet深度学习模型，整合时空特征提取、注意力机制和可解释性技术。采用10折交叉验证和添加高斯噪声的数据增强策略，使用121名参与者的EEG数据集。

Result: 模型在ADHD/健康对照分类中达到100%敏感性和99.17%准确率。通过权重分析和t-SNE可视化识别了关键脑区和频率波段。

Conclusion: 研究表明深度学习和EEG信号结合能显著提高ADHD诊断的准确性和效率，为临床诊断提供了有前景的新方法。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.

</details>


### [74] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: Merge-of-Thought Distillation (MoT) 是一个轻量级框架，通过交替进行教师特定的监督微调分支和权重空间合并，将多个教师模型的推理能力统一到学生模型中，在仅使用约200个高质量CoT样本的情况下，显著提升了学生模型的数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统的推理蒸馏方法假设只有一个最优教师模型，但实际上存在多个候选教师和不断增长的CoT语料库。研究发现不同学生有不同的"最佳教师"，甚至同一学生在不同数据集上的最佳教师也不同，因此需要一种方法来统一多个教师的推理能力并克服监督冲突。

Method: 提出Merge-of-Thought Distillation (MoT)框架，交替进行教师特定的监督微调分支和权重空间合并。首先针对每个教师进行专门的监督微调，然后将得到的多个学生变体在权重空间进行合并，从而整合不同教师的推理能力。

Result: 在数学竞赛基准测试中，使用仅约200个高质量CoT样本，将MoT应用于Qwen3-14B学生模型，性能超过了DEEPSEEK-R1、QWEN3-30B-A3B、QWEN3-32B和OPENAI-O1等强模型。MoT持续优于最佳单教师蒸馏和朴素多教师联合方法，提高了性能上限并减轻过拟合，对分布偏移和同级教师表现出鲁棒性。

Conclusion: MoT是一种简单、可扩展的方法，能够高效地从多样化的教师模型中蒸馏长链推理能力到紧凑的学生模型中，同时减少灾难性遗忘，提升数学之外的通用推理能力，甚至培养出更好的教师模型，表明共识过滤的推理特征具有广泛的迁移性。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


### [75] [A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management](https://arxiv.org/abs/2509.08822)
*Willy Sucipto,Jianlong Zhou,Ray Seung Min Kwon,Fang Chen*

Main category: cs.LG

TL;DR: 这篇综述论文系统总结了TinyML技术在养蜂业中的应用现状，重点围绕蜂群条件监测、蜜蜂行为识别、病虫害检测和分蜂预测四个功能领域，分析了现有数据集、轻量模型架构和部署策略，并指出了数据稀缺、泛化挑战等关键限制因素。


<details>
  <summary>Details</summary>
Motivation: 传统蜂箱检查方式劳动密集且具有干扰性，而基于云的监控方案在偏远或资源有限的养蜂场不实用。物联网和微型机器学习技术的发展为蜂群实时监测提供了低功耗、非侵入式的替代方案。

Method: 采用文献综述方法，系统梳理了TinyML与养蜂学交叉领域的最新创新，围绕四个核心功能领域进行组织分析，并考察了相关数据集、轻量模型架构和基准测试策略。

Result: 论文整合了当前研究和工程实践，为构建可扩展的AI驱动生态监测系统提供了基础，识别了数据稀缺、泛化能力不足和离网环境部署障碍等关键挑战。

Conclusion: 通过整合研究和工程实践，这项工作为支持可持续传粉媒介管理的可扩展、AI驱动和生态信息化的监测系统奠定了基础，同时指出了超高效推理管道、自适应边缘学习和数据集标准化等新兴机遇。

Abstract: Honey bee colonies are essential for global food security and ecosystem
stability, yet they face escalating threats from pests, diseases, and
environmental stressors. Traditional hive inspections are labor-intensive and
disruptive, while cloud-based monitoring solutions remain impractical for
remote or resource-limited apiaries. Recent advances in Internet of Things
(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring
directly on edge devices, offering scalable and non-invasive alternatives. This
survey synthesizes current innovations at the intersection of TinyML and
apiculture, organized around four key functional areas: monitoring hive
conditions, recognizing bee behaviors, detecting pests and diseases, and
forecasting swarming events. We further examine supporting resources, including
publicly available datasets, lightweight model architectures optimized for
embedded deployment, and benchmarking strategies tailored to field constraints.
Critical limitations such as data scarcity, generalization challenges, and
deployment barriers in off-grid environments are highlighted, alongside
emerging opportunities in ultra-efficient inference pipelines, adaptive edge
learning, and dataset standardization. By consolidating research and
engineering practices, this work provides a foundation for scalable, AI-driven,
and ecologically informed monitoring systems to support sustainable pollinator
management.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [76] [kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions](https://arxiv.org/abs/2509.08366)
*Parastoo Pashmchi,Jerome Benoit,Motonobu Kanagawa*

Main category: stat.ML

TL;DR: kNNSampler是一种缺失值插补方法，通过从k个最相似单元的观测响应中随机采样来估计缺失值的条件分布，而非仅仅估计条件均值，能够量化不确定性并支持多重插补。


<details>
  <summary>Details</summary>
Motivation: 现有kNNImputer等方法只能估计缺失值的条件均值，无法捕捉完整的条件分布和量化不确定性。需要一种能够从分布中采样未知缺失值的方法。

Method: 对于给定单元的缺失响应，从k个最相似单元（基于观测协变量）的观测响应中随机采样进行插补，从而估计条件分布而非仅仅条件均值。

Result: 理论证明kNNSampler能够估计缺失响应给定观测协变量的条件分布，实验证明其在恢复缺失值分布方面的有效性。

Conclusion: kNNSampler提供了一种有效的缺失值插补方法，能够采样未知缺失值、量化不确定性，并支持多重插补，优于仅估计条件均值的传统方法。

Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a
given unit's missing response by randomly sampling from the observed responses
of the $k$ most similar units to the given unit in terms of the observed
covariates. This method can sample unknown missing values from their
distributions, quantify the uncertainties of missing values, and be readily
used for multiple imputation. Unlike popular kNNImputer, which estimates the
conditional mean of a missing response given an observed covariate, kNNSampler
is theoretically shown to estimate the conditional distribution of a missing
response given an observed covariate. Experiments demonstrate its effectiveness
in recovering the distribution of missing values. The code for kNNSampler is
made publicly available (https://github.com/SAP/knn-sampler).

</details>


### [77] [Gaussian Process Regression -- Neural Network Hybrid with Optimized Redundant Coordinates](https://arxiv.org/abs/2509.08457)
*Sergei Manzhos,Manabu Ihara*

Main category: stat.ML

TL;DR: 提出了opt-GPRNN方法，通过蒙特卡洛算法优化冗余坐标，在减少神经元数量的同时获得更低测试误差，并保持避免过拟合的优势


<details>
  <summary>Details</summary>
Motivation: 结合高斯过程回归和神经网络的混合机器学习方法，旨在同时获得神经网络的表达能力和线性回归的鲁棒性，特别是在防止过拟合方面

Method: 使用蒙特卡洛算法优化GPRNN中的冗余坐标，实现维度约简，结合高斯过程回归和神经网络的优势

Result: opt-GPRNN以更少的神经元数量获得最低测试误差，表达能力接近多层神经网络，在某些应用中可替代深度神经网络

Conclusion: 通过坐标优化的GPRNN方法在保持避免过拟合优势的同时，显著提高了表达能力和计算效率，为原子间势能学习和材料信息学提供了有效解决方案

Abstract: Recently, a Gaussian Process Regression - neural network (GPRNN) hybrid
machine learning method was proposed, which is based on additive-kernel GPR in
redundant coordinates constructed by rules [J. Phys. Chem. A 127 (2023) 7823].
The method combined the expressive power of an NN with the robustness of linear
regression, in particular, with respect to overfitting when the number of
neurons is increased beyond optimal. We introduce opt-GPRNN, in which the
redundant coordinates of GPRNN are optimized with a Monte Carlo algorithm and
show that when combined with optimization of redundant coordinates, GPRNN
attains the lowest test set error with much fewer terms / neurons and retains
the advantage of avoiding overfitting when the number of neurons is increased
beyond optimal value. The method, opt-GPRNN possesses an expressive power
closer to that of a multilayer NN and could obviate the need for deep NNs in
some applications. With optimized redundant coordinates, a dimensionality
reduction regime is also possible. Examples of application to machine learning
an interatomic potential and materials informatics are given.

</details>


### [78] [PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research](https://arxiv.org/abs/2509.08553)
*Jessica Gronsbell,Vidul Ayakulangara Panickan,Chris Lin,Thomas Charlon,Chuan Hong,Doudou Zhou,Linshanshan Wang,Jianhui Gao,Shirley Zhou,Yuan Tian,Yaqi Shi,Ziming Gan,Tianxi Cai*

Main category: stat.ML

TL;DR: PEHRT是一个用于多机构电子健康记录数据标准化的高效流水线，包含数据预处理和表示学习两个核心模块，能够在不共享个体数据的情况下生成研究就绪数据集。


<details>
  <summary>Details</summary>
Motivation: 多机构电子健康记录数据的整合分析可以提高转化研究的可靠性和泛化能力，但数据异质性、语义差异和隐私问题给数据协调带来了重大挑战。

Method: PEHRT流水线包括两个核心模块：(1)数据预处理，将EHR数据映射到标准编码系统；(2)表示学习，使用先进机器学习技术生成研究就绪数据集，无需个体级数据共享。该流水线是数据模型无关的，基于实际经验设计。

Result: 研究提供了完整的开源软件套件和用户友好教程，并在多个医疗系统的数据上展示了PEHRT在各种任务中的实用性。

Conclusion: PEHRT为解决多机构EHR数据协调的挑战提供了一个标准化、高效的解决方案，能够促进更可靠和可泛化的转化研究。

Abstract: Integrative analysis of multi-institutional Electronic Health Record (EHR)
data enhances the reliability and generalizability of translational research by
leveraging larger, more diverse patient cohorts and incorporating multiple data
modalities. However, harmonizing EHR data across institutions poses major
challenges due to data heterogeneity, semantic differences, and privacy
concerns. To address these challenges, we introduce $\textit{PEHRT}$, a
standardized pipeline for efficient EHR data harmonization consisting of two
core modules: (1) data pre-processing and (2) representation learning. PEHRT
maps EHR data to standard coding systems and uses advanced machine learning to
generate research-ready datasets without requiring individual-level data
sharing. Our pipeline is also data model agnostic and designed for streamlined
execution across institutions based on our extensive real-world experience. We
provide a complete suite of open source software, accompanied by a
user-friendly tutorial, and demonstrate the utility of PEHRT in a variety of
tasks using data from diverse healthcare systems.

</details>


### [79] [A hierarchical entropy method for the delocalization of bias in high-dimensional Langevin Monte Carlo](https://arxiv.org/abs/2509.08619)
*Daniel Lacker,Fuzhong Zhou*

Main category: stat.ML

TL;DR: 本文改进了Chen等人(2024)关于非调整Langevin算法偏差的研究，在稀疏交互分布中消除了对数因子，使用相对熵度量距离，并放宽了强对数凹性假设，证明了偏差仅与低维边际维度相关而非全维度。


<details>
  <summary>Details</summary>
Motivation: 非调整Langevin算法在高维采样中存在偏差，传统认为偏差随维度线性增长，但Chen等人发现稀疏交互分布中低维边际的偏差仅与低维度相关，本文旨在强化这一发现并扩展适用范围。

Method: 采用边际相对熵的层次分析方法，受作者近期关于混沌传播研究的启发，分析稀疏交互和弱交互分布的偏差特性。

Result: 成功移除了对数因子，使用相对熵度量距离，放宽了强对数凹性假设，并将delocalization现象扩展到弱交互分布类别。

Conclusion: 研究证实了在稀疏和弱交互分布中，非调整Langevin算法的偏差确实仅与低维边际维度相关，这一发现对高维采样算法设计具有重要意义。

Abstract: The unadjusted Langevin algorithm is widely used for sampling from complex
high-dimensional distributions. It is well known to be biased, with the bias
typically scaling linearly with the dimension when measured in squared
Wasserstein distance. However, the recent paper of Chen et al. (2024)
identifies an intriguing new delocalization effect: For a class of
distributions with sparse interactions, the bias between low-dimensional
marginals scales only with the lower dimension, not the full dimension. In this
work, we strengthen the results of Chen et al. (2024) in the sparse interaction
regime by removing a logarithmic factor, measuring distance in relative entropy
(a.k.a. KL-divergence), and relaxing the strong log-concavity assumption. In
addition, we expand the scope of the delocalization phenomenon by showing that
it holds for a class of distributions with weak interactions. Our proofs are
based on a hierarchical analysis of the marginal relative entropies, inspired
by the authors' recent work on propagation of chaos.

</details>
