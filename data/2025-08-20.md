<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 6]
- [cs.LG](#cs.LG) [Total: 75]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [CKM-Assisted Physical-Layer Security for Resilience Against Unknown Eavesdropping Location](https://arxiv.org/abs/2508.13681)
*Ladan Khaloopour,Matthias Hollick,Vahid Jamali*

Main category: eess.SP

TL;DR: 利用频道知识地图(CKM)提升mmWave系统的物理层安全性，通过多束编码和最优资源分配对投放者定位不做假设


<details>
  <summary>Details</summary>
Motivation: 解决传统物理层安全方案依赖于对投放者位置和频道状态信息的假设，实际应用中存在很大局限性

Method: 采用高导向性mmWave传输，将秘密消息在多个束上进行联合编码，利用CKM推导时间和功率分配算法

Result: 在不假设投放者位置和CSI的情况下，能够实现最大绝对保密速率

Conclusion: CKM为物理层安全提供了一种数据驱动的新方案，能够在不依赖于投放者信息的情况下有效提升系统安全性

Abstract: Channel Knowledge Map (CKM) is an emerging data-driven toolbox that captures
our awareness of the wireless channel and enables efficient communication and
resource allocation beyond the state of the art. In this work, we consider CKM
for improving physical-layer security (PLS) in the presence of a passive
eavesdropper (Eve), without making any assumptions about Eve's location or
channel state information (CSI). We employ highly directional mmWave
transmissions, with the confidential message jointly encoded across multiple
beams. By exploiting CKM, we derive an algorithm for time and power allocation
among the beams that maximizes the absolute secrecy rate under the worst-case
scenario for Eve's location.

</details>


### [2] [Airy beams for near-field communications: Fundamentals, potentials, and limitations](https://arxiv.org/abs/2508.13714)
*Donatella Darsena,Francesco Verde,Marco Di Renzo,Vincenzo Galdi*

Main category: eess.SP

TL;DR: 这篇论文研究了近场区域中艾里光束的特性和优势，通过理论分析和数值模拟对比了艾里光束与高斯光束在非视线传播场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 新一代无线网络中，电磁大开口和高频传输扩展了近场区域，这为光束形状控制提供了新的自由度。艾里光束在近场区具有自加速曲线轨迹、自恢复和无行射等优势性能。

Method: 首先分析连续开口场分布辐射自加速光束的基本原理，然后解决有限能量导致的指数衰减和开口截断问题，研究光束在自由空间的传播特性。重点分析非视线场景下的传播行为，并与高斯光束进行对比。

Result: 理论和数值结果显示，在某些非视线通道中，只要艾里光束的关键特性（投物线轨迹自加速和无行射传播）得以保持，它们能够比高斯光束提供更好的性能。这需要传输开口中有足够大的区域保持与接收机的直视线路径。

Conclusion: 艾里光束在近场区非视线传播中具有潜在性能优势，特别是在能够保持其自加速和无行射特性的条件下。这为下一代无线通信系统的光束形成和控制提供了新的可能性。

Abstract: In next-generation wireless networks, the combination of electrically large
radiating apertures and high-frequency transmission extends the radiating
near-field region around the transmitter. In this region, unlike in the far
field, the wavefront is nonplanar, which provides additional degrees of freedom
to shape and steer the transmitted beam in a desired manner. In this paper, we
focus on Airy beams, which may exhibit several highly desirable properties in
the near-field region. Ideally, these beams follow self-accelerating (curved)
trajectories, demonstrate resilience to perturbations through self-healing, and
maintain a consistent intensity profile across all planes perpendicular to the
propagation direction, making them effectively diffraction-free. Specifically,
we first present the underlying principles of self-accelerating beams radiated
by continuous aperture field distributions. We then address several challenges
regarding the generation of Airy beams, including their exponential decay due
to finite energy constraints and spatial truncation of the aperture. Moreover,
we examine their free-space propagation characteristics. The second part of the
paper focuses on the propagation behavior of Airy beams in non-line-of-sight
(NLoS) scenarios. A comparison is also presented between Airy beams and
Gaussian beams. Our theoretical and numerical results show that Airy beams may
offer a performance advantage over Gaussian beams in certain NLoS channels,
provided that their key properties are largely preserved, specifically,
self-acceleration along a parabolic trajectory and diffraction-free
propagation. In the presence of an obstacle, this requires that the portion of
the transmit aperture with a clear line-of-sight to the receiver is
sufficiently large.

</details>


### [3] [Joint AP Selection and Power Allocation for Unicast-Multicast Cell-Free Massive MIMO](https://arxiv.org/abs/2508.13771)
*Mustafa S. Abbas,Zahra Mobini,Hien Quoc Ngo,Hyundong Shin,Michail Matthaiou*

Main category: eess.SP

TL;DR: 本文研究了支持单播和多播传输的无蜂窝大规模MIMO系统，推导了频谱效率的闭式表达式，并提出了基于APG的联合AP选择和功率分配优化算法，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网等无线系统中单播和多播联合传输的重要性日益增加，需要研究能够同时支持这两种传输类型的无蜂窝大规模MIMO系统，以提升频谱效率和系统性能。

Method: 推导了零迫近和最大比预编码下频谱效率的精确闭式表达式，将加权和频谱效率最大化问题重构为可处理结构，开发了基于加速投影梯度(APG)的算法，并实现了基于逐次凸近似(SCA)的基准算法。

Result: 仿真结果表明，所提出的联合优化方法在各种系统设置和预编码策略下显著提升了加权和频谱效率，APG算法在保持竞争性能的同时实现了显著的复杂度降低。

Conclusion: 该研究为大规模实际部署提供了高效的联合优化解决方案，APG算法在复杂度和性能之间取得了良好平衡，适用于大规模无蜂窝MIMO系统的实际应用。

Abstract: Joint unicast and multicast transmissions are becoming increasingly important
in practical wireless systems, such as Internet of Things networks. This paper
investigates a cell-free massive multiple-input multiple-output system that
simultaneously supports both transmission types, with multicast serving
multiple groups. Exact closed-form expressions for the achievable downlink
spectral efficiency (SE) of both unicast and multicast users are derived for
zero-forcing and maximum ratio precoding designs. Accordingly, a weighted sum
SE (SSE) maximization problem is formulated to jointly optimize the access
point (AP) selection and power allocation. The optimization framework accounts
for practical constraints, including the maximum transmit power per AP,
fronthaul capacity limitations between APs and the central processing unit, and
quality-of-service requirements for all users. The resulting non-convex
optimization problem is reformulated into a tractable structure, and an
accelerated projected gradient (APG)-based algorithm is developed to
efficiently obtain near-optimal solutions. As a performance benchmark, a
successive convex approximation (SCA)-based algorithm is also implemented.
Simulation results demonstrate that the proposed joint optimization approach
significantly enhances the SSE across various system setups and precoding
strategies. In particular, the APG-based algorithm achieves substantial
complexity reduction while maintaining competitive performance, making it
well-suited for large-scale practical deployments.

</details>


### [4] [Robust Optimization for Movable Antenna-aided Cell-Free ISAC with Time Synchronization Errors](https://arxiv.org/abs/2508.13818)
*Yue Xiu,Yang Zhao,Ran Yang,Wanting Lyu,Dusit Niyato,Dong In Kim,Guangyi Liu,Ning Wei*

Main category: eess.SP

TL;DR: 提出基于可移动天线的CF-ISAC架构，利用空间分集增强通信速率、保持感知精度并减少时间同步误差影响，通过流形优化和元强化学习实现鲁棒优化


<details>
  <summary>Details</summary>
Motivation: 当前时间同步技术存在限制，时间同步误差已成为CF-ISAC系统发展的重大挑战，需要新的解决方案来充分利用CF-ISAC的潜力

Method: 提出MA-enabled CF-ISAC架构，采用流形优化解决最坏情况感知精度优化问题，设计MA-MetaRL算法联合优化AP波束成形和MA位置

Result: 仿真结果表明，所提鲁棒优化算法显著提高检测精度并对时间同步误差具有强鲁棒性，相比传统固定天线技术获得更高系统容量

Conclusion: 基于可移动天线的CF-ISAC架构能有效解决时间同步误差问题，提升系统性能，验证了该方法的有效性

Abstract: The cell-free integrated sensing and communication (CF-ISAC) system, which
effectively mitigates intra-cell interference and provides precise sensing
accuracy, is a promising technology for future 6G networks. However, to fully
capitalize on the potential of CF-ISAC, accurate time synchronization (TS)
between access points (APs) is critical. Due to the limitations of current
synchronization technologies, TS errors have become a significant challenge in
the development of the CF-ISAC system. In this paper, we propose a novel
CF-ISAC architecture based on movable antennas (MAs), which exploits spatial
diversity to enhance communication rates, maintain sensing accuracy, and reduce
the impact of TS errors. We formulate a worst-case sensing accuracy
optimization problem for TS errors to address this challenge, deriving the
worst-case Cram\'er-Rao lower bound (CRLB). Subsequently, we develop a joint
optimization framework for AP beamforming and MA positions to satisfy
communication rate constraints while improving sensing accuracy. A robust
optimization framework is designed for the highly complex and non-convex
problem. Specifically, we employ manifold optimization (MO) to solve the
worst-case sensing accuracy optimization problem. Then, we propose an
MA-enabled meta-reinforcement learning (MA-MetaRL) to design optimization
variables while satisfying constraints on MA positions, communication rate, and
transmit power, thereby improving sensing accuracy. The simulation results
demonstrate that the proposed robust optimization algorithm significantly
improves the accuracy of the detection and is strong against TS errors.
Moreover, compared to conventional fixed position antenna (FPA) technologies,
the proposed MA-aided CF-ISAC architecture achieves higher system capacity,
thus validating its effectiveness.

</details>


### [5] [Distributed Distortion-Aware Robust Optimization for Movable Antenna-aided Cell-Free ISAC Systems](https://arxiv.org/abs/2508.13839)
*Yue Xiu,Yang Zhao,Ran Yang,Zheng Dong,Wanting Lyu,Zeyuan Zhang,Dusit Niyato,Guangyi Liu,Ning Wei*

Main category: eess.SP

TL;DR: 基于可移动天线的无细网集成感知通信系统，通过分布式夸张惠制透明优化框架和SACGNN算法，有效减少功放非线性夸张对通信和感知性能的影响，显著提升系统稳健性和容量。


<details>
  <summary>Details</summary>
Motivation: 无细网集成感知通信作为6G关键技术虽有频谱效率和普遍覆盖优势，但实际部署中受到功放非线性夸张等硬件缺陷影响，导致通信和感知性能下降。需要有效减少这种夸张影响。

Method: 提出可移动天线辅助的CF-ISAC系统，建立三阶无记忆多项式功放模型，设计分布式夸张惠制透明的最差情况稳健优化框架，采用逐次凸近似估计夸张系数，并通过MA启用的自注意卷积图神经网络算法合作优化波束成型和天线位置。

Result: 模拟实验表明，该方法在存在夸张的情况下显著改善了通信-感知权衡性能，在稳健性和容量方面都超过了固定位置天线的基准方案。

Conclusion: 可移动天线辅助的CF-ISAC系统能够有效应对功放非线性夸张带来的挑战，为6G网络提供了更高稳健性和性能的解决方案，显示了该技术在实际部署中的优势。

Abstract: The cell-free integrated sensing and communication (CF-ISAC) architecture is
a promising enabler for 6G, offering spectrum efficiency and ubiquitous
coverage. However, real deployments suffer from hardware impairments,
especially nonlinear distortion from power amplifiers (PAs), which degrades
both communication and sensing. To address this, we propose a movable antenna
(MA)-aided CF-ISAC system that mitigates distortion and enhances robustness.
The PAs nonlinearities are modeled by a third-order memoryless polynomial,
where the third-order distortion coefficients (3RDCs) vary across access points
(APs) due to hardware differences, aging, and environmental conditions. We
design a distributed distortion-aware worst-case robust optimization framework
that explicitly incorporates uncertainty in 3RDCs. First, we analyze the
worst-case impact of PA distortion on both the Cramer-Rao lower bound (CRLB)
and communication rate. Then, to address the resulting non-convexity, we apply
successive convex approximation (SCA) for estimating the 3RDCs. With these, we
jointly optimize beamforming and MA positions under transmit power and sensing
constraints. To efficiently solve this highly non-convex problem, we develop an
MA-enabled self-attention convolutional graph neural network (SACGNN)
algorithm. Simulations demonstrate that our method substantially enhances the
communication-sensing trade-off under distortion and outperforms fixed-position
antenna baselines in terms of robustness and capacity, thereby highlighting the
advantages of MA-aided CF-ISAC systems.

</details>


### [6] [Evaluating Particle Filtering for RSS-Based Target Localization under Varying Noise Levels and Sensor Geometries](https://arxiv.org/abs/2508.13937)
*Halim Lee,Jongmin Park,Kwansik Park*

Main category: eess.SP

TL;DR: 这篇论文系统分析了基于粒子筛波的RSS目标定位算法，在不同传感器配置和噪声条件下较传统三边测量方法更准确。


<details>
  <summary>Details</summary>
Motivation: 虽然粒子筛波已被用于RSS目标定位，但缺乏对不同传感器几何配置和噪声水平下性能的系统分析。

Method: 设计和评估了一种用于静止目标定位的粒子筛波算法，并与传统RSS三边测量方法进行对比。

Result: 模拟结果显示，粒子筛波在传感器几何配置不利和高RSS噪声场景下提供更准确的定位结果。

Conclusion: 粒子筛波在RSS目标定位中显示出优越性，尤其适用于复杂环境条件。

Abstract: Target localization is a critical task in various applications, such as
search and rescue, surveillance, and wireless sensor networks. When a target
emits a radio frequency (RF) signal, spatially distributed sensors can collect
signal measurements to estimate the target's location. Among various
measurement modalities, received signal strength (RSS) is particularly
attractive due to its low cost, low power consumption, and ease of deployment.
While particle filtering has previously been applied to RSS-based target
localization, few studies have systematically analyzed its performance under
varying sensor geometries and RSS noise levels. This paper addresses this gap
by designing and evaluating a particle filtering algorithm for localizing a
stationary target. The proposed method is compared with a conventional
RSS-based trilateration approach across different sensor configurations and
noise conditions. Simulation results indicate that particle filtering provides
more accurate target localization than trilateration, particularly in scenarios
with unfavorable sensor geometries and high RSS noise.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [BERT-VQA: Visual Question Answering on Plots](https://arxiv.org/abs/2508.13184)
*Tai Vu,Robert Yang*

Main category: cs.LG

TL;DR: 这篇论文研究了基于图表的视觉问题回答任务，开发了BERT-VQA模型，但结果显示跨模态对齐模块并非关键因素，提供了有价值的模型选择导向。


<details>
  <summary>Details</summary>
Motivation: 解决图表视觉问题回答这一具有挑战性的子任务，需要深度学习模型在视觉和语言领域之间进行信息交换。

Method: 开发了BERT-VQA模型，基于VisualBERT架构，配备预训练的ResNet 101图像编码器，并可能添加联合融合机制，与由LSTM、CNN和浅层分类器组成的基准模型进行对比评估。

Result: 最终结果否定了核心假设，证明VisualBERT中的跨模态模块并非对齐图表组件与问题短语的关键因素。

Conclusion: 这项工作为图表问题回答挑战的难度以及解决该问题时不同模型架构的适宜性提供了有价值的见解。

Abstract: Visual question answering has been an exciting challenge in the field of
natural language understanding, as it requires deep learning models to exchange
information from both vision and language domains. In this project, we aim to
tackle a subtask of this problem, namely visual question answering on plots. To
achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with
a pretrained ResNet 101 image encoder, along with a potential addition of joint
fusion. We trained and evaluated this model against a baseline that consisted
of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our
core hypothesis that the cross-modality module in VisualBERT is essential in
aligning plot components with question phrases. Therefore, our work provided
valuable insights into the difficulty of the plot question answering challenge
as well as the appropriateness of different model architectures in solving this
problem.

</details>


### [8] [Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis](https://arxiv.org/abs/2508.13196)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: 基于CNN和LLM的多模态情感分析方法，通过上下文注意力机制融合文本和图像信息，在自然灾害情感分析中实现了显著性能提升


<details>
  <summary>Details</summary>
Motivation: 自然灾害期间社交媒体情感分析对危机管理至关重要，但传统方法对文本和图像模态的分离处理效果有限

Method: 结合CNN图像分析和LLM文本处理，使用GPT和prompt工程提取特征，通过上下文注意力机制模型模态间关系，深度神经网络学习融合特征

Result: 在CrisisMMD数据集上实现了显著性能提升，准确率提高2.43%，F1值提高5.18%，能够更有效分类社交媒体信息

Conclusion: 该方法为多模态情感分析提供了有效解决方案，在灾害管理中具有重要应用价值，为AI驱动的危机管理开启了新方向

Abstract: This paper introduces a novel approach for multimodal sentiment analysis on
social media, particularly in the context of natural disasters, where
understanding public sentiment is crucial for effective crisis management.
Unlike conventional methods that process text and image modalities separately,
our approach seamlessly integrates Convolutional Neural Network (CNN) based
image analysis with Large Language Model (LLM) based text processing,
leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to
extract sentiment relevant features from the CrisisMMD dataset. To effectively
model intermodal relationships, we introduce a contextual attention mechanism
within the fusion process. Leveraging contextual-attention layers, this
mechanism effectively captures intermodality interactions, enhancing the
model's comprehension of complex relationships between textual and visual data.
The deep neural network architecture of our model learns from these fused
features, leading to improved accuracy compared to existing baselines.
Experimental results demonstrate significant advancements in classifying social
media data into informative and noninformative categories across various
natural disasters. Our model achieves a notable 2.43% increase in accuracy and
5.18% in F1-score, highlighting its efficacy in processing complex multimodal
data. Beyond quantitative metrics, our approach provides deeper insight into
the sentiments expressed during crises. The practical implications extend to
real time disaster management, where enhanced sentiment analysis can optimize
the accuracy of emergency interventions. By bridging the gap between multimodal
analysis, LLM powered text understanding, and disaster response, our work
presents a promising direction for Artificial Intelligence (AI) driven crisis
management solutions. Keywords:

</details>


### [9] [Strategies for training point distributions in physics-informed neural networks](https://arxiv.org/abs/2508.13216)
*Santosh Humagain,Toni Schneidereit*

Main category: cs.LG

TL;DR: 本文系统研究了物理信息神经网络中训练点分布对求解微分方程精度的影响，比较了五种训练数据生成策略和浅层网络架构，发现训练点分布与微分方程特性相关。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络作为求解微分方程的新兴方法，其性能受多种因素影响，但训练点分布这一核心组件尚未得到系统研究。

Method: 使用两个常微分方程和两个偏微分方程，测试五种训练数据生成策略（包括新提出的基于正弦的训练点），采用浅层网络架构（1-2个隐藏层），并使用随机和固定种子权重初始化确保可重复性。

Result: 结果显示训练点分布对解精度有显著影响，并且发现训练点分布与微分方程的特性存在关联。

Conclusion: 训练点分布是影响物理信息神经网络性能的关键因素，需要根据微分方程特性选择合适的分布策略。

Abstract: Physics-informed neural networks approach the approximation of differential
equations by directly incorporating their structure and given conditions in a
loss function. This enables conditions like, e.g., invariants to be easily
added during the modelling phase. In addition, the approach can be considered
as mesh free and can be utilised to compute solutions on arbitrary grids after
the training phase. Therefore, physics-informed neural networks are emerging as
a promising alternative to solving differential equations with methods from
numerical mathematics. However, their performance highly depends on a large
variety of factors. In this paper, we systematically investigate and evaluate a
core component of the approach, namely the training point distribution. We test
two ordinary and two partial differential equations with five strategies for
training data generation and shallow network architectures, with one and two
hidden layers. In addition to common distributions, we introduce sine-based
training points, which are motivated by the construction of Chebyshev nodes.
The results are challenged by using certain parameter combinations like, e.g.,
random and fixed-seed weight initialisation for reproducibility. The results
show the impact of the training point distributions on the solution accuracy
and we find evidence that they are connected to the characteristics of the
differential equation.

</details>


### [10] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: DGNPP模型通过结合图神经网络和点过程，同时捕捉时间交互网络的拓扑结构和动态演化，显著提升了事件和时间预测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间交互网络学习视为粗粒度的多序列预测问题，忽略了网络拓扑结构的影响，需要同时建模结构和时间动态性

Method: 提出DGNPP模型，包含节点聚合层（捕捉拓扑结构生成静态表示）和自注意力层（动态更新嵌入），将动态和静态嵌入结合到事件强度函数中，通过最大似然估计优化

Result: 在三个公开数据集上的实验表明，DGNPP在事件预测和时间预测任务上均取得优越性能，高效且显著优于基线模型

Conclusion: DGNPP有效解决了先前方法的局限性，通过整合网络拓扑结构和时间动态性，为时间交互网络学习提供了有效的解决方案

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [11] [MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination](https://arxiv.org/abs/2508.13532)
*Ziyan Wu,Ivan Korolija,Rui Tang*

Main category: cs.LG

TL;DR: 开发了MuFlex平台，一个可扩展的开源多建筑灵活性协调控制策略测试平台，解决了现有平台在物理细节捕捉和标准化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源在电网中渗透率增加，需要建筑群提供协调的需求灵活性。现有开源测试平台大多针对单栋建筑，多建筑平台有限且通常依赖简化模型，无法充分捕捉物理细节和中间变量，限制了控制策略的基准测试能力。

Method: 开发了MuFlex平台，支持EnergyPlus建筑模型间的同步信息交换，遵循最新的OpenAI Gym接口，提供模块化、标准化的强化学习实现。使用Soft Actor-Critic算法对四栋办公楼进行需求灵活性协调案例研究。

Result: 协调四栋建筑的灵活性使得总峰值需求降低到指定阈值以下，同时保持了室内环境质量。

Conclusion: MuFlex平台为多建筑灵活性协调控制策略提供了有效的基准测试工具，展示了在保持室内环境质量的同时降低峰值需求的能力。

Abstract: With the increasing penetration of renewable generation on the power grid,
maintaining system balance requires coordinated demand flexibility from
aggregations of buildings. Reinforcement learning (RL) has been widely explored
for building controls because of its model-free nature. Open-source simulation
testbeds are essential not only for training RL agents but also for fairly
benchmarking control strategies. However, most building-sector testbeds target
single buildings; multi-building platforms are relatively limited and typically
rely on simplified models (e.g., Resistance-Capacitance) or data-driven
approaches, which lack the ability to fully capture the physical intricacies
and intermediate variables necessary for interpreting control performance.
Moreover, these platforms often impose fixed inputs, outputs, and model
formats, restricting their applicability as benchmarking tools across diverse
control scenarios. To address these gaps, MuFlex, a scalable, open-source
platform for benchmarking and testing control strategies for multi-building
flexibility coordination, was developed in this study. MuFlex enables
synchronous information exchange across EnergyPlus building models and adheres
to the latest OpenAI Gym interface, providing a modular, standardized RL
implementation. The platform capabilities were demonstrated in a case study
coordinating demand flexibility across four office buildings using the Soft
Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results
show that aggregating the four buildings flexibility reduced total peak demand
below a specified threshold while maintaining indoor environmental quality.

</details>


### [12] [A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education](https://arxiv.org/abs/2508.13224)
*Mizuki Ohira,Toshimichi Saito*

Main category: cs.LG

TL;DR: 本文提出了一种基于循环神经网络的聚类方法，用于处理教育领域广泛使用的S-P图表的大规模学生数据分类问题


<details>
  <summary>Details</summary>
Motivation: 随着学生数量增加，S-P图表变得难以处理，需要将大型图表分类为更小的子图表

Method: 使用具有多个固定点和吸引盆的循环神经网络，通过网络动态实现聚类，每个吸引盆对应一个小型S-P图表

Result: 通过基础实验验证了方法的有效性，并引入了平均谨慎指数作为评估聚类性能的重要特征量

Conclusion: 基于循环神经网络动态的聚类方法能有效处理大规模S-P图表的分类问题，平均谨慎指数是评估学生答题模式特异性的有效指标

Abstract: This paper studies an application of a recurrent neural network to clustering
method for the S-P chart: a binary data set used widely in education. As the
number of students increases, the S-P chart becomes hard to handle. In order to
classify the large chart into smaller charts, we present a simple clustering
method based on the network dynamics. In the method, the network has multiple
fixed points and basins of attraction give clusters corresponding to small S-P
charts. In order to evaluate the clustering performance, we present an
important feature quantity: average caution index that characterizes
singularity of students answer oatterns. Performing fundamental experiments,
effectiveness of the method is confirmed.

</details>


### [13] [RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning](https://arxiv.org/abs/2508.13229)
*Suhang Hu,Wei Hu,Yuhang Su,Fan Zhang*

Main category: cs.LG

TL;DR: RISE是一个两阶段框架，通过强化学习生成视觉基础、逻辑一致的思维链，然后利用高质量CoT进行监督微调和强化微调，在复杂图像标注任务中超越SFT和Visual-RFT方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂图像标注任务中存在推理能力不足的问题，标准监督微调只关注标注结果而忽略推理过程，Visual-RFT由于缺乏高质量验证过的思维链导致推理不一致。

Method: 两阶段框架：1) RISE-CoT阶段：通过强化学习的"标注-推理-标注"闭环生成视觉基础、逻辑一致的思维链；2) RISE-R1阶段：利用高质量CoT子集进行监督微调，然后进行强化微调。

Result: 在复杂和简单图像标注任务上，RISE训练的Qwen2-VL-2B模型性能优于SFT和Visual-RFT，实现了鲁棒性能和增强的可解释性。

Conclusion: RISE提供了一种无需人工标注思维链的自监督解决方案，可有效提升视觉语言模型的推理能力。

Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks,
such as emotion classification and context-driven object detection, which
demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses
solely on annotation outcomes, ignoring underlying rationales, while Visual
Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought
(CoTs) due to the absence of high-quality, verified CoTs during pre-training.
We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework
to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement
learning-driven "annotation-reasoning-annotation" closed-loop generates
visually grounded, logically consistent CoTs by verifying their ability to
reconstruct original annotations without direct leakage. The Inspire and
Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by
RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement
fine-tuning to produce interpretable reasoning and accurate annotations,
achieving Expertise in complex visual tasks. Evaluated on complex and simple
image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and
Visual-RFT, achieving robust performance and enhanced explainability. RISE
offers a self-supervised solution for advancing VLM reasoning without requiring
manually annotated CoTs.

</details>


### [14] [Data driven feedback linearization of nonlinear control systems via Lie derivatives and stacked regression approach](https://arxiv.org/abs/2508.13241)
*Lakshmi Priya P. K.,Andreas Schwung*

Main category: cs.LG

TL;DR: 一种结合稀疏回归算法和相对阶条件的新方法，用于发现物理系统的真实控制方程并设计反馈线性化控制器


<details>
  <summary>Details</summary>
Motivation: 发现物理系统的控制方程和设计有效反馈控制器是一项极具挑战的研究任务，需要深入理解包括非线性因素在内的系统动态行为

Method: 首先使用稀疏回归算法识别系统，然后通过对输出函数字典应用Lie导数来设计反馈控制器，得到一个增广约束条件以确保没有内部动态被观测到

Result: 该方法能够发玹并反馈线性化物理模型的真实经验方程，不同于之前的相关工作

Conclusion: 本文提出的新方法通过结合堆叠回归算法和相对阶条件，有效地解决了物理系统控制方程发现和反馈线性化控制器设计的挑战

Abstract: Discovering the governing equations of a physical system and designing an
effective feedback controller remains one of the most challenging and intensive
areas of ongoing research. This task demands a deep understanding of the system
behavior, including the nonlinear factors that influence its dynamics. In this
article, we propose a novel methodology for identifying a feedback linearized
physical system based on known prior dynamic behavior. Initially, the system is
identified using a sparse regression algorithm, subsequently a feedback
controller is designed for the discovered system by applying Lie derivatives to
the dictionary of output functions to derive an augmented constraint which
guarantees that no internal dynamics are observed. Unlike the prior related
works, the novel aspect of this article combines the approach of stacked
regression algorithm and relative degree conditions to discover and feedback
linearize the true governing equations of a physical model.

</details>


### [15] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: 通过物理模拟实现物理可行的数据增帽(PPDA)，在传感器基于人体活动识别中比传统信号变换方法提升了3.7百分点的F1分数，并能在训练数据减少60%的情况下达到竞争性能能力。


<details>
  <summary>Details</summary>
Motivation: 解决传感器基于人体活动识别中高质量标签数据稀缺问题，传统的信号变换数据增帽方法存在物理不可行的问题，影响模型性能和泛化能力。

Method: 使用物理模拟技术实现物理可行数据增帽(PPDA)，利用动作捕捉或视频姿态估计的人体运动数据，通过修改身体运动、传感器置位和硬件相关效果来增加现实变化性。

Result: 在三个公开数据集上进行实验，PPDA方法比传统STDA方法平均提升了3.7百分点的宏F1分数(最高提升13百分点)，并能在训练主体数量减少60%的情况下达到竞争性能能力。

Conclusion: 物理可行数据增帽方法在传感器基于人体活动识别中显示出显著优势，物理模拟技术为生成合成IMU数据提供了成本效益高且可扩展的解决方案，有助于解决标注数据稀缺挑战。

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


### [16] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 提出协作匹配系统comatch，通过算法选择最有信心的匹配决策，其余交由人类决策者，实现人机互补，在匹配任务中超越单独使用人类或算法的表现


<details>
  <summary>Details</summary>
Motivation: 现有算法匹配系统无法实现人机互补，人类使用算法系统做出的决策不一定比单独使用人类或算法更好

Method: 提出协作匹配(comatch)系统，算法只选择最有信心的决策，其余决策交由人类完成，优化决策分配以最大化性能

Result: 通过800名参与者的大规模实验验证，comatch产生的匹配结果优于单独使用人类参与者或算法匹配的结果

Conclusion: 协作匹配方法能够有效实现人机互补，在匹配任务中显著提升决策质量

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [17] [Hierarchical Conformal Classification](https://arxiv.org/abs/2508.13288)
*Floris den Hengst,Inès Blin,Majid Mohammadi,Syed Ihtesham Hussain Shah,Taraneh Younesian*

Main category: cs.LG

TL;DR: 本文提出了层次化共形分类（HCC），将类别层次结构融入预测集，在保持覆盖保证的同时提供更有语义意义的预测结果。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测将类别视为平坦结构，忽略了类别间的语义关系和层次结构信息，无法利用领域知识。

Method: 将HCC建模为约束优化问题，通过寻找层次结构中不同层级节点组成的预测集，并证明只需考虑较小且结构良好的候选解子集即可确保覆盖性和最优性。

Result: 在音频、图像和文本三个新基准测试上的实证评估显示该方法优势明显，用户研究表明标注者显著偏好层次化预测集而非平坦预测集。

Conclusion: HCC成功将类别层次结构整合到共形预测框架中，在保持统计保证的同时提供了更具语义意义的预测结果。

Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty
in machine learning models, offering reliable predictions with finite-sample
coverage guarantees. When applied to classification, CP produces a prediction
set of possible labels that is guaranteed to contain the true label with high
probability, regardless of the underlying classifier. However, standard CP
treats classes as flat and unstructured, ignoring domain knowledge such as
semantic relationships or hierarchical structure among class labels. This paper
presents hierarchical conformal classification (HCC), an extension of CP that
incorporates class hierarchies into both the structure and semantics of
prediction sets. We formulate HCC as a constrained optimization problem whose
solutions yield prediction sets composed of nodes at different levels of the
hierarchy, while maintaining coverage guarantees. To address the combinatorial
nature of the problem, we formally show that a much smaller, well-structured
subset of candidate solutions suffices to ensure coverage while upholding
optimality. An empirical evaluation on three new benchmarks consisting of
audio, image, and text data highlights the advantages of our approach, and a
user study shows that annotators significantly prefer hierarchical over flat
prediction sets.

</details>


### [18] [Efficient Constraint-Aware Flow Matching via Randomized Exploration](https://arxiv.org/abs/2508.13316)
*Zhengyan Huan,Jacob Boerma,Li-Ping Liu,Shuchin Aeron*

Main category: cs.LG

TL;DR: 提出两种约束流匹配方法：当约束集可微分时使用惩罚项方法；当只有成员查询oracle时使用随机化学习均值流方法，在保持目标分布的同时显著提高约束满足率


<details>
  <summary>Details</summary>
Motivation: 解决流匹配生成中需要满足额外约束的问题，特别是在只有成员查询oracle（如黑盒分类器）而无法获得梯度信息的实际场景

Method: 对于可微分距离函数的情况，在FM目标中添加约束惩罚项；对于成员查询oracle的情况，使用随机化方法学习均值流，并提出两阶段训练策略提高计算效率

Result: 在多个合成约束生成案例中，所提方法在保持目标分布匹配的同时显著提高了约束满足率，并成功应用于对抗样本生成任务

Conclusion: 该方法为约束流匹配提供了有效的解决方案，特别是在只有oracle查询的复杂约束场景，为实际应用如对抗攻击等开辟了新方向

Abstract: We consider the problem of generating samples via Flow Matching (FM) with an
additional requirement that the generated samples must satisfy given
constraints. We consider two scenarios, viz.: (a) when a differentiable
distance function to the constraint set is given, and (b) when the constraint
set is only available via queries to a membership oracle. For case (a), we
propose a simple adaptation of the FM objective with an additional term that
penalizes the distance between the constraint set and the generated samples.
For case (b), we propose to employ randomization and learn a mean flow that is
numerically shown to have a high likelihood of satisfying the constraints. This
approach deviates significantly from existing works that require simple convex
constraints, knowledge of a barrier function, or a reflection mechanism to
constrain the probability flow. Furthermore, in the proposed setting we show
that a two-stage approach, where both stages approximate the same original flow
but with only the second stage probing the constraints via randomization, is
more computationally efficient. Through several synthetic cases of constrained
generation, we numerically show that the proposed approaches achieve
significant gains in terms of constraint satisfaction while matching the target
distributions. As a showcase for a practical oracle-based constraint, we show
how our approach can be used for training an adversarial example generator,
using queries to a hard-label black-box classifier. We conclude with several
future research directions. Our code is available at
https://github.com/ZhengyanHuan/FM-RE.

</details>


### [19] [Decoding Communications with Partial Information](https://arxiv.org/abs/2508.13326)
*Dylan Cope,Peter McBurney*

Main category: cs.LG

TL;DR: 该论文探讨了在部分可观测性条件下机器语言习得的问题，提出了通过环境知识、行动和消息来推断私有信息的学习算法


<details>
  <summary>Details</summary>
Motivation: 传统语言习得模型假设学习者能够观察到所有相关信息，但现实中存在部分可观测性，即学习者无法直接看到所有相关情境信息，需要从环境、行动和消息中进行推断

Method: 提出了一个学习算法来解码私有信息以促进语言习得，首先在玩具设置中展示问题解决方法，然后形式化分析更一般设置中的挑战

Result: 论文展示了在部分可观测性条件下语言习得问题的几个激励性示例，并证明了在玩具设置中这些问题的可解性

Conclusion: 部分可观测性为语言习得提出了更具挑战性的设置，需要开发新的算法来从有限信息中推断私有情境信息，这对于更真实的语言学习场景具有重要意义

Abstract: Machine language acquisition is often presented as a problem of imitation
learning: there exists a community of language users from which a learner
observes speech acts and attempts to decode the mappings between utterances and
situations. However, an interesting consideration that is typically unaddressed
is partial observability, i.e. the learner is assumed to see all relevant
information. This paper explores relaxing this assumption, thereby posing a
more challenging setting where such information needs to be inferred from
knowledge of the environment, the actions taken, and messages sent. We see
several motivating examples of this problem, demonstrate how they can be solved
in a toy setting, and formally explore challenges that arise in more general
settings. A learning-based algorithm is then presented to perform the decoding
of private information to facilitate language acquisition.

</details>


### [20] [A Dual-Attention Graph Network for fMRI Data Classification](https://arxiv.org/abs/2508.13328)
*Amirali Arbab,Zeinab Davarani,Mehran Safayani*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于动态图构建和空间时间注意力机制的新框架，用于自闭谱系症诊断，通过动态推断大脑功能连接性和空间时间关系建模，在ABIDE数据集上超越了静态图方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的fMRI分类方法主要基于静态功能连接性，无法全面捐描空间时间关系，而理解神经活动动态对神经科学发展至关重要。

Method: 使用基于transformer的注意力机制动态推断每个时间间隔的大脑功能连接性，构建时变图并通过图卷积网络(GCN)和transformer处理，捐描局部交互和全局时间依赖关系。

Result: 在ABIDE数据集子集上达到了63.2%的准确率和60.0%的AUC，显著超过静态图方法(GCN:51.8%)。

Conclusion: 证明了联合建模动态连接性和空间时间上下文对fMRI分类的有效性，核心新颖性在于注意力驱动的动态图创建和通过GCN-transformer融合的层次空间时间特征融合。

Abstract: Understanding the complex neural activity dynamics is crucial for the
development of the field of neuroscience. Although current functional MRI
classification approaches tend to be based on static functional connectivity or
cannot capture spatio-temporal relationships comprehensively, we present a new
framework that leverages dynamic graph creation and spatiotemporal attention
mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in
this research dynamically infers functional brain connectivity in each time
interval using transformer-based attention mechanisms, enabling the model to
selectively focus on crucial brain regions and time segments. By constructing
time-varying graphs that are then processed with Graph Convolutional Networks
(GCNs) and transformers, our method successfully captures both localized
interactions and global temporal dependencies. Evaluated on the subset of ABIDE
dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static
graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint
modeling of dynamic connectivity and spatio-temporal context for fMRI
classification. The core novelty arises from (1) attention-driven dynamic graph
creation that learns temporal brain region interactions and (2) hierarchical
spatio-temporal feature fusion through GCNtransformer fusion.

</details>


### [21] [Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling](https://arxiv.org/abs/2508.13703)
*Nikolai Antonov,Prěmysl Šůcha,Mikoláš Janota,Jan Hůla*

Main category: cs.LG

TL;DR: 基于机器学习的数据驱动调度算法，在单机器调度问题中显著提升了优化性能和适应能力，超越了现有最优算法。


<details>
  <summary>Details</summary>
Motivation: 传统的精确算法在某些问题空间区域性能会昂显下降，而数据驱动方法能够根据特定数据集结构提供更强大的可扩展性能。

Method: 提出了一种新颖的数据驱动调度质图算法，将机器学习与问题特定特征相结合，确保产生可行解。还进行了系统的ML模型选择分析。

Result: 实验结果显示，该方法在优化间隔、最优解数量和适应不同数据场景方面显著超过了现有最优技术。

Conclusion: 该研究为单机器调度问题提供了一种灵活且高效的数据驱动解决方案，具有强大的实际应用潜力。

Abstract: Existing research on single-machine scheduling is largely focused on exact
algorithms, which perform well on typical instances but can significantly
deteriorate on certain regions of the problem space. In contrast, data-driven
approaches provide strong and scalable performance when tailored to the
structure of specific datasets. Leveraging this idea, we focus on a
single-machine scheduling problem where each job is defined by its weight,
duration, due date, and deadline, aiming to minimize the total weight of tardy
jobs. We introduce a novel data-driven scheduling heuristic that combines
machine learning with problem-specific characteristics, ensuring feasible
solutions, which is a common challenge for ML-based algorithms. Experimental
results demonstrate that our approach significantly outperforms the
state-of-the-art in terms of optimality gap, number of optimal solutions, and
adaptability across varied data scenarios, highlighting its flexibility for
practical applications. In addition, we conduct a systematic exploration of ML
models, addressing a common gap in similar studies by offering a detailed model
selection process and providing insights into why the chosen model is the best
fit.

</details>


### [22] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE是一个针对新一代MoE架构的高效训练系统，解决了现有MoE训练在非NVIDIA平台上的性能瓶颈，通过创新技术实现了在AMD GPU上的大规模可扩展训练。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE训练系统主要针对NVIDIA GPU优化，在非NVIDIA平台上性能不佳，且面临激活内存开销大和全互联通信成本高的问题，限制了MoE架构的可扩展性。

Method: 采用无填充MoE训练、跨平台内核、冗余绕过调度以及序列分片MoE块的混合并行等技术，专门针对AMD MI250X GPU等非NVIDIA平台进行优化。

Result: 在Frontier超级计算机上，X-MoE成功将DeepSeek风格的MoE模型扩展到5450亿参数，在1024个GPU上实现了比现有方法大10倍的模型规模，同时保持高训练吞吐量。

Conclusion: X-MoE系统证明了在非NVIDIA硬件平台上实现大规模MoE模型高效训练的可行性，为下一代MoE架构的广泛应用提供了重要的技术基础。

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [23] [Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression](https://arxiv.org/abs/2508.13829)
*Samuel Stocksieker,Denys pommeret,Arthur Charpentier*

Main category: cs.LG

TL;DR: 提出了一种基于解耦VAE和平滑Bootstrap的新方法来解决表格数据中的不平衡回归问题，通过在潜在空间中进行数据生成来改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 不平衡分布学习是预测建模中的常见挑战，现有方法主要针对分类问题，对回归问题的关注有限。不平衡回归(IR)是一个重要但研究不足的领域。

Method: 使用变分自编码器(VAE)建模数据分布的潜在表示，结合解耦VAE和潜在空间中的平滑Bootstrap技术来生成数据，解决标准VAE在不平衡数据上的低效问题。

Result: 在IR基准数据集上通过数值比较验证了该方法的有效性，与竞争方法相比表现出更好的性能。

Conclusion: 该方法为不平衡回归问题提供了一种创新的解决方案，通过潜在空间的数据生成技术有效改善了模型在不平衡数据上的学习能力。

Abstract: Imbalanced distribution learning is a common and significant challenge in
predictive modeling, often reducing the performance of standard algorithms.
Although various approaches address this issue, most are tailored to
classification problems, with a limited focus on regression. This paper
introduces a novel method to improve learning on tabular data within the
Imbalanced Regression (IR) framework, which is a critical problem. We propose
using Variational Autoencoders (VAEs) to model and define a latent
representation of data distributions. However, VAEs can be inefficient with
imbalanced data like other standard approaches. To address this, we develop an
innovative data generation method that combines a disentangled VAE with a
Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of
this method through numerical comparisons with competitors on benchmark
datasets for IR.

</details>


### [24] [Dimension lower bounds for linear approaches to function approximation](https://arxiv.org/abs/2508.13346)
*Daniel Hsu*

Main category: cs.LG

TL;DR: 本文提出了一种线性代数方法来证明L^2函数逼近问题中线性方法的维度下界，并将该方法应用于核方法的样本量下界分析


<details>
  <summary>Details</summary>
Motivation: 为线性方法在L^2函数逼近问题中提供维度下界的严格数学证明，扩展已有的Kolmogorov n-宽度下界结果到核方法领域

Method: 采用线性代数论证方法，基于Barron(1993)等文献中已有的基本论证框架，将其系统化并应用于核方法的样本量分析

Result: 建立了线性方法在L^2函数逼近中的维度下界理论，并成功推导出核方法所需的样本量下界

Conclusion: 该线性代数方法为分析函数逼近问题的计算复杂度提供了有效的理论工具，特别适用于核方法等机器学习算法的理论分析

Abstract: This short note presents a linear algebraic approach to proving dimension
lower bounds for linear methods that solve $L^2$ function approximation
problems. The basic argument has appeared in the literature before (e.g.,
Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The
argument is applied to give sample size lower bounds for kernel methods.

</details>


### [25] [Multi-User Contextual Cascading Bandits for Personalized Recommendation](https://arxiv.org/abs/2508.13981)
*Jiho Park,Huiwen Jia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a Multi-User Contextual Cascading Bandit model, a new
combinatorial bandit framework that captures realistic online advertising
scenarios where multiple users interact with sequentially displayed items
simultaneously. Unlike classical contextual bandits, MCCB integrates three key
structural elements: (i) cascading feedback based on sequential arm exposure,
(ii) parallel context sessions enabling selective exploration, and (iii)
heterogeneous arm-level rewards. We first propose Upper Confidence Bound with
Backward Planning (UCBBP), a UCB-style algorithm tailored to this setting, and
prove that it achieves a regret bound of $\widetilde{O}(\sqrt{THN})$ over $T$
episodes, $H$ session steps, and $N$ contexts per episode. Motivated by the
fact that many users interact with the system simultaneously, we introduce a
second algorithm, termed Active Upper Confidence Bound with Backward Planning
(AUCBBP), which shows a strict efficiency improvement in context scaling, i.e.,
user scaling, with a regret bound of $\widetilde{O}(\sqrt{T+HN})$. We validate
our theoretical findings via numerical experiments, demonstrating the empirical
effectiveness of both algorithms under various settings.

</details>


### [26] [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
*Wenhao Mu,Zhi Cao,Mehmed Uludag,Alexander Rodríguez*

Main category: cs.LG

TL;DR: ODE-Diff是一个时间序列扩散框架，通过结合专家模型的高层信号作为结构化先验，在数据稀缺情况下实现更可靠的反事实分布预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖点估计或纯数据驱动模型，在数据稀缺时表现不佳。需要结合机理模型和数据驱动方法的优势，实现更可靠和可解释的因果推断。

Method: 提出ODE-Diff时间序列扩散框架，从不完美的专家模型中提取高层信号作为结构化先验，用于生成建模，桥接机理方法和数据驱动方法。

Result: 在半合成COVID-19模拟、合成药理学动力学和真实案例研究中，ODE-Diff在点预测和分布准确性方面均优于强基线方法。

Conclusion: ODE-Diff成功结合了专家知识和数据驱动建模，在复杂动力系统的反事实分布预测中表现出色，为科学建模和决策提供了更可靠的工具。

Abstract: Predicting counterfactual distributions in complex dynamical systems is
essential for scientific modeling and decision-making in domains such as public
health and medicine. However, existing methods often rely on point estimates or
purely data-driven models, which tend to falter under data scarcity. We propose
a time series diffusion-based framework that incorporates guidance from
imperfect expert models by extracting high-level signals to serve as structured
priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and
data-driven approaches, enabling more reliable and interpretable causal
inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,
synthetic pharmacological dynamics, and real-world case studies, demonstrating
that it consistently outperforms strong baselines in both point prediction and
distributional accuracy.

</details>


### [27] [Adaptive Conformal Prediction Intervals Over Trajectory Ensembles](https://arxiv.org/abs/2508.13362)
*Ruipu Li,Daniel Menacho,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 提出了基于共形预测的统一框架，将采样轨迹转换为具有理论覆盖保证的校准预测区间，通过在线更新和优化步骤处理时间依赖性，产生更精确的不确定性估计


<details>
  <summary>Details</summary>
Motivation: 未来轨迹预测在自动驾驶、飓风预报和流行病建模等领域很重要，但现有的采样轨迹通常未经校准，缺乏可靠的不确定性量化

Method: 基于共形预测的统一框架，引入在线更新步骤和优化步骤来捕捉步间依赖关系，生成不连续的预测区间

Result: 能够产生具有理论覆盖保证的校准预测区间，自然捕捉时间依赖性，提供更锐利、更自适应的不确定性估计

Conclusion: 该方法为轨迹预测提供了可靠的校准不确定性量化框架，具有理论保证和实际应用价值

Abstract: Future trajectories play an important role across domains such as autonomous
driving, hurricane forecasting, and epidemic modeling, where practitioners
commonly generate ensemble paths by sampling probabilistic models or leveraging
multiple autoregressive predictors. While these trajectories reflect inherent
uncertainty, they are typically uncalibrated. We propose a unified framework
based on conformal prediction that transforms sampled trajectories into
calibrated prediction intervals with theoretical coverage guarantees. By
introducing a novel online update step and an optimization step that captures
inter-step dependencies, our method can produce discontinuous prediction
intervals around each trajectory, naturally capture temporal dependencies, and
yield sharper, more adaptive uncertainty estimates.

</details>


### [28] [Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference](https://arxiv.org/abs/2508.13380)
*Seohyeon Cha,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 这篇论文提出了J3O框架，通过聚合优化模型部署（onloading）和查询路由（offloading），在边缘设备上高效支持多任务并发推理，在约束条件下实现超过97%的最优准确率。


<details>
  <summary>Details</summary>
Motivation: 现有框架主要关注单任务单模型场景，而许多实际应用（如自动驾驶、增强现实）需要并发执行检测、分割、深度估计等多样任务。

Method: 提出J3O统一框架，将问题形式化为混合整数规划问题，通过交替优化算法：(i)利用拉格朗日松弛次模优化谭式选择模型部署，(ii)通过约束线性规划确定最优路由策略，并支持边缘批处理。

Result: 实验结果显示J3O在多任务测试中一贯达到超过97%的最优准确率，而运行时间仅需最优求解器的15%以下。

Conclusion: J3O框架能够在资源受限的边缘环境中高效支持多任务并发推理，通过聚合优化模型部署和查询路由，在保证性能的同时实现了接近最优的推理准确性。

Abstract: The growing demand for intelligent services on resource-constrained edge
devices has spurred the development of collaborative inference systems that
distribute workloads across end devices, edge servers, and the cloud. While
most existing frameworks focus on single-task, single-model scenarios, many
real-world applications (e.g., autonomous driving and augmented reality)
require concurrent execution of diverse tasks including detection,
segmentation, and depth estimation. In this work, we propose a unified
framework to jointly decide which multi-task models to deploy (onload) at
clients and edge servers, and how to route queries across the hierarchy
(offload) to maximize overall inference accuracy under memory, compute, and
communication constraints. We formulate this as a mixed-integer program and
introduce J3O (Joint Optimization of Onloading and Offloading), an alternating
algorithm that (i) greedily selects models to onload via Lagrangian-relaxed
submodular optimization and (ii) determines optimal offloading via constrained
linear programming. We further extend J3O to account for batching at the edge,
maintaining scalability under heterogeneous task loads. Experiments show J3O
consistently achieves over $97\%$ of the optimal accuracy while incurring less
than $15\%$ of the runtime required by the optimal solver across multi-task
benchmarks.

</details>


### [29] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 通过时频分析和空间相关性分析，证实鸣声事件的统计异常检测可以有效定位症狂发作起源区，尤其在手术成功的病人中表现更佳


<details>
  <summary>Details</summary>
Motivation: 开发一种量化框架来评估临床定义的症狂发作起源区与通过时频分析识别的统计异常通道之间的空间一致性

Method: 使用两步方法：(1)无监督异常检测（LOF算法）识别异常通道；(2)空间相关性分析，计算精确匹配指标和加权相似性指标

Result: 在手术成功的病人中表现最佳（索引精度平均0.865），而失败案例的一致性较低（索引精度平均0.460）

Conclusion: 基于鸣声事件的异常检测结合加权空间指标，为症狂发作起源区定位提供了补充方法，尤其适用于手术成功的病人

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [30] [NovoMolGen: Rethinking Molecular Language Model Pretraining](https://arxiv.org/abs/2508.13408)
*Kamran Chitsaz,Roshan Balaji,Quentin Fournier,Nirav Pravinbhai Bhatt,Sarath Chandar*

Main category: cs.LG

TL;DR: NovoMolGen是一个基于Transformer的分子大语言模型，通过在15亿分子上预训练，系统研究了文本表示、分词策略、模型大小和数据集规模对分子生成性能的影响，在无约束和目标导向分子生成任务中达到了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前分子大语言模型在探索巨大化学空间（10^23到10^60个可合成候选分子）方面显示出潜力，但缺乏对标准语言建模实践（如文本表示、分词策略等）如何影响分子生成性能的系统理解。

Method: 引入了NovoMolGen系列基础模型，使用Transformer架构，在15亿个分子上进行预训练，系统分析了不同文本表示方法、分词策略、模型规模和数据集大小对性能的影响。

Result: 发现预训练性能指标与下游实际性能之间存在弱相关性，揭示了分子语言建模与通用NLP训练动态的重要区别。在无约束和目标导向分子生成任务中显著优于之前的分子大语言模型和专用生成模型。

Conclusion: NovoMolGen为推进高效有效的分子建模策略提供了坚实基础，建立了分子生成领域的新基准，系统性地阐明了关键建模因素对性能的影响。

Abstract: Designing de-novo molecules with desired property profiles requires efficient
exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$
possible synthesizable candidates. While various deep generative models have
been developed to design small molecules using diverse input representations,
Molecular Large Language Models (Mol-LLMs) based on string representations have
emerged as a scalable approach capable of exploring billions of molecules.
However, there remains limited understanding regarding how standard language
modeling practices such as textual representations, tokenization strategies,
model size, and dataset scale impact molecular generation performance. In this
work, we systematically investigate these critical aspects by introducing
NovoMolGen, a family of transformer-based foundation models pretrained on 1.5
billion molecules for de-novo molecule generation. Through extensive empirical
analyses, we identify a weak correlation between performance metrics measured
during pretraining and actual downstream performance, revealing important
distinctions between molecular and general NLP training dynamics. NovoMolGen
establishes new state-of-the-art results, substantially outperforming prior
Mol-LLMs and specialized generative models in both unconstrained and
goal-directed molecular generation tasks, thus providing a robust foundation
for advancing efficient and effective molecular modeling strategies.

</details>


### [31] [Decentralized Contextual Bandits with Network Adaptivity](https://arxiv.org/abs/2508.13411)
*Chuyun Deng,Huiwen Jia*

Main category: cs.LG

TL;DR: 网络环境下的上下文线性搏村机问题，提出两种网络感知UCB算法，通过动态网络权重实现适应性信息共享，在降低通信成本的同时提升学习效果


<details>
  <summary>Details</summary>
Motivation: 传统上下文搏村机假设数据要么完全中央化要么完全隔离，网络环境下部分信息共享的情况研究较少，需要开发能够涉及网络结构的适应性算法

Method: 提出NetLinUCB和Net-SGD-UCB两种网络感知UCB算法，通过动态更新网络权重来指导信息共享，将学习分解为全局和本地组件，只共享同质特征的计算摘要

Result: 算法将共享结构的学习复杂度从O(N)降低到次线性O(√N)，通信成本较完全中央化设置更轻；NetLinUCB在低噪声治理下表现优异，Net-SGD-UCB在高维高方差场景中更稳健

Conclusion: 该研究为网络环境下的上下文搏村机问题提供了有效解决方案，通过动态网络权重实现了适应性信息共享，在保持通信效率的同时显著提升了学习性能

Abstract: We consider contextual linear bandits over networks, a class of sequential
decision-making problems where learning occurs simultaneously across multiple
locations and the reward distributions share structural similarities while also
exhibiting local differences. While classical contextual bandits assume either
fully centralized data or entirely isolated learners, much remains unexplored
in networked environments when information is partially shared. In this paper,
we address this gap by developing two network-aware Upper Confidence Bound
(UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information
sharing guided by dynamically updated network weights. Our approach decompose
learning into global and local components and as a result allow agents to
benefit from shared structure without full synchronization. Both algorithms
incur lighter communication costs compared to a fully centralized setting as
agents only share computed summaries regarding the homogeneous features. We
establish regret bounds showing that our methods reduce the learning complexity
associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$,
where $N$ is the size of the network. The two algorithms reveal complementary
strengths: NetLinUCB excels in low-noise regimes with fine-grained
heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance
contexts. We further demonstrate the effectiveness of our methods across
simulated pricing environments compared to standard benchmarks.

</details>


### [32] [MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](https://arxiv.org/abs/2508.13415)
*Jeremy Carleton,Debajoy Mukherjee,Srinivas Shakkottai,Dileep Kalathil*

Main category: cs.LG

TL;DR: MAVIS是一个轻量级的推理时对齐框架，通过价值引导的搜索实现多目标对齐，无需修改基础模型权重，在多个目标间动态平衡LLM行为


<details>
  <summary>Details</summary>
Motivation: LLMs在多样应用中需要平衡多个冲突目标（如帮助性、无害性、幽默性），传统方法需要对每个目标配置进行微调，计算成本高且不灵活

Method: 训练一组小型价值模型对应不同目标，推理时根据用户指定权重组合这些模型，生成倾斜函数来调整基础模型的输出分布

Result: MAVIS在性能上优于为每个目标微调模型并事后组合的基线方法，甚至接近为用户的精确偏好微调模型的理想化设置性能

Conclusion: MAVIS提供了一个计算高效且灵活的多目标对齐解决方案，能够在推理时动态控制LLM行为，避免昂贵的模型微调

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
applications that demand balancing multiple, often conflicting, objectives --
such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific
preferences in such multi-objective settings typically requires fine-tuning
models for each objective or preference configuration, which is computationally
expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via
Value-Guided Inference-Time Search -- a lightweight inference-time alignment
framework that enables dynamic control over LLM behavior without modifying the
base model's weights. MAVIS trains a set of small value models, each
corresponding to a distinct objective. At inference time, these value models
are combined using user-specified weights to produce a tilting function that
adjusts the base model's output distribution toward desired trade-offs. The
value models are trained using a simple iterative algorithm that ensures
monotonic improvement of the KL-regularized policy. We show empirically that
MAVIS outperforms baselines that fine-tune per-objective models and combine
them post hoc, and even approaches the performance of the idealized setting
where models are fine-tuned for a user's exact preferences.

</details>


### [33] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: EventTSF是一个创新的多模态时间序列预测框架，通过自回归扩散和流匹配技术整合文本事件与时间序列数据，解决了跨模态同步、语义不确定性和特征对齐三大挑战，在8个数据集上显著优于12个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法主要依赖单一模态，无法有效利用文本事件等外部信息来改善非平稳动态预测，导致上下文知识有限和模型性能不足。

Method: 提出EventTSF自回归生成框架，使用流匹配自回归扩散技术，通过多模态U形扩散变换器融合时间和文本模态，根据事件语义信号自适应控制流匹配时间步长。

Result: 在8个合成和真实数据集上的实验表明，EventTSF相比12个基线方法在预测准确率上提升10.7%，训练效率提高1.13倍。

Conclusion: EventTSF成功解决了多模态时间序列预测中的关键挑战，证明了整合文本事件信息对提升非平稳时间序列预测性能的重要价值。

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [34] [SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer](https://arxiv.org/abs/2508.13435)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: SVDformer是一种新的方向感知图表征学习框架，结合SVD和Transformer架构，通过关注权重模型入边/出边模式的多尺度交互，显式保留边的方向性。


<details>
  <summary>Details</summary>
Motivation: 现有的有向图神经网络常常无法同时抓取方向语义和全局结构模式，因为其各向同性聚合机制和局部滤波机制。

Method: 首先通过多头自注意力精炼奇异值嵌入，自适应地增强关键的谱组件并压制高频噪声。然后将奇异向量作为方向投影基础，奇异值作为缩放因子，使用Transformer模型多尺度交互。

Result: 在6个有向图标准数据集上的实验表明，SVDformer在节点分类任务上一贵超过最先进的GNN和方向感知基线模型。

Conclusion: SVDformer为有向图上的表征学习建立了一种新的范式，能够有效地聚合方向信息和全局结构特征。

Abstract: Directed graphs are widely used to model asymmetric relationships in
real-world systems. However, existing directed graph neural networks often
struggle to jointly capture directional semantics and global structural
patterns due to their isotropic aggregation mechanisms and localized filtering
mechanisms. To address this limitation, this paper proposes SVDformer, a novel
framework that synergizes SVD and Transformer architecture for direction-aware
graph representation learning. SVDformer first refines singular value
embeddings through multi-head self-attention, adaptively enhancing critical
spectral components while suppressing high-frequency noise. This enables
learnable low-pass/high-pass graph filtering without requiring spectral
kernels. Furthermore, by treating singular vectors as directional projection
bases and singular values as scaling factors, SVDformer uses the Transformer to
model multi-scale interactions between incoming/outgoing edge patterns through
attention weights, thereby explicitly preserving edge directionality during
feature propagation. Extensive experiments on six directed graph benchmarks
demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and
direction-aware baselines on node classification tasks, establishing a new
paradigm for learning representations on directed graphs.

</details>


### [35] [Dynamic Design of Machine Learning Pipelines via Metalearning](https://arxiv.org/abs/2508.13436)
*Edesio Alcobaça,André C. P. L. F. de Carvalho*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于元学习的方法，通过历史元知识动态设计AutoML的搜索空间，可以大幅减少运行时间和搜索空间，而不影响预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML系统面临高计算成本和过拟合的挑战，需要一种更高效的方法来动态优化搜索空间。

Method: 使用历史元知识来选择有前景的搜索空间区域，加速优化过程，包括元特征选择和元模型可解释性分析。

Result: 方法在Random Search中减少89%运行时间，大幅缩减搜索空间（预处理器从13个减到1.8个，分类器从16个减到4.3个），且在Auto-Sklearn中也表现竞争力。

Conclusion: 该元学习方法能够有效解决AutoML系统的计算效率问题，为动态搜索空间设计提供了可行的解决方案。

Abstract: Automated machine learning (AutoML) has democratized the design of machine
learning based systems, by automating model selection, hyperparameter tuning
and feature engineering. However, the high computational cost associated with
traditional search and optimization strategies, such as Random Search, Particle
Swarm Optimization and Bayesian Optimization, remains a significant challenge.
Moreover, AutoML systems typically explore a large search space, which can lead
to overfitting. This paper introduces a metalearning method for dynamically
designing search spaces for AutoML system. The proposed method uses historical
metaknowledge to select promising regions of the search space, accelerating the
optimization process. According to experiments conducted for this study, the
proposed method can reduce runtime by 89\% in Random Search and search space by
(1.8/13 preprocessor and 4.3/16 classifier), without compromising significant
predictive performance. Moreover, the proposed method showed competitive
performance when adapted to Auto-Sklearn, reducing its search space.
Furthermore, this study encompasses insights into meta-feature selection,
meta-model explainability, and the trade-offs inherent in search space
reduction strategies.

</details>


### [36] [ASAP: Unsupervised Post-training with Label Distribution Shift Adaptive Learning Rate](https://arxiv.org/abs/2508.13445)
*Heewon Park,Mugon Joe,Miru Kim,Minhae Kwon*

Main category: cs.LG

TL;DR: ASAP是一种无需标签的自适应学习率调整方法，通过计算当前与先前未标记输出的余弦距离来动态调整学习率，有效应对在线标签偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中机器学习模型面临在线标签偏移问题，标签分布随时间变化。传统方法需要仔细选择学习率，过低会减慢适应速度，过高会导致不稳定。

Method: 提出ASAP方法，通过计算当前和先前未标记输出的余弦距离，并将其映射到有界范围内来自动调整学习率。该方法无需标签、模型集成或历史输入，仅使用先前的softmax输出进行快速轻量级适应。

Result: 在多个数据集和偏移场景下的实验表明，ASAP始终能提高准确性和效率。

Conclusion: ASAP为无监督模型适应提供了一种实用有效的解决方案，能够快速适应在线标签分布变化。

Abstract: In real-world applications, machine learning models face online label shift,
where label distributions change over time. Effective adaptation requires
careful learning rate selection: too low slows adaptation and too high causes
instability. We propose ASAP (Adaptive Shift Aware Post-training), which
dynamically adjusts the learning rate by computing the cosine distance between
current and previous unlabeled outputs and mapping it within a bounded range.
ASAP requires no labels, model ensembles, or past inputs, using only the
previous softmax output for fast, lightweight adaptation. Experiments across
multiple datasets and shift scenarios show ASAP consistently improves accuracy
and efficiency, making it practical for unsupervised model adaptation.

</details>


### [37] [Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification](https://arxiv.org/abs/2508.13452)
*Ruobing Jiang,Mengzhe Liu,Haobing Liu,Yanwei Yu*

Main category: cs.LG

TL;DR: 提出了HCAL分类器，通过原型对比学习和自适应任务加权机制解决层次多标签分类中的结构一致性和损失平衡问题


<details>
  <summary>Details</summary>
Motivation: 层次多标签分类面临结构一致性维护和多任务学习中损失权重平衡的关键挑战

Method: 基于多任务学习整合原型对比学习和自适应任务加权机制，包括语义一致性建模、自适应损失加权和原型扰动机制

Result: 在三个数据集上的广泛实验表明，相比基线模型具有更高的分类准确率和更低的层次违规率

Conclusion: HCAL分类器有效解决了传统多任务学习方法中的优化偏差问题，提高了分类性能和结构一致性

Abstract: Hierarchical Multi-Label Classification (HMC) faces critical challenges in
maintaining structural consistency and balancing loss weighting in Multi-Task
Learning (MTL). In order to address these issues, we propose a classifier
called HCAL based on MTL integrated with prototype contrastive learning and
adaptive task-weighting mechanisms. The most significant advantage of our
classifier is semantic consistency including both prototype with explicitly
modeling label and feature aggregation from child classes to parent classes.
The other important advantage is an adaptive loss-weighting mechanism that
dynamically allocates optimization resources by monitoring task-specific
convergence rates. It effectively resolves the "one-strong-many-weak"
optimization bias inherent in traditional MTL approaches. To further enhance
robustness, a prototype perturbation mechanism is formulated by injecting
controlled noise into prototype to expand decision boundaries. Additionally, we
formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to
evaluate hierarchical consistency and generalization. Extensive experiments
across three datasets demonstrate both the higher classification accuracy and
reduced hierarchical violation rate of the proposed classifier over baseline
models.

</details>


### [38] [Classifying Clinical Outcome of Epilepsy Patients with Ictal Chirp Embeddings](https://arxiv.org/abs/2508.13476)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 该研究开发了一个基于t-SNE的管道，用于可视化不同临床结果场景下的chirp特征，并通过机器学习分类器在嵌入空间中进行分类任务，最高达到88.8%的准确率。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的可视化方法来分析chirp特征与临床结果之间的关系，为临床分层和决策支持提供见解。

Method: 使用t-SNE技术处理chirp的时域、频域和频谱指标，保留局部邻域关系并解决拥挤问题。在2D嵌入空间上构建三个分类任务，使用四种分类器（随机森林、SVM、逻辑回归、k-NN）进行5折交叉验证，并应用SHAP生成特征影响敏感度图。

Result: 随机森林和k-NN分类器表现最佳，在最优病例检测（成功结果且临床难度最小）中达到88.8%的准确率。SHAP图揭示了嵌入空间中空间局部化的特征重要性。

Conclusion: 该集成框架展示了可解释嵌入和局部特征归因在临床分层和决策支持方面的潜力，为理解数据潜在结构提供了新见解。

Abstract: This study presents a pipeline leveraging t-Distributed Stochastic Neighbor
Embedding (t-SNE) for interpretable visualizations of chirp features across
diverse outcome scenarios. The dataset, comprising chirp-based temporal,
spectral, and frequency metrics. Using t-SNE, local neighborhood relationships
were preserved while addressing the crowding problem through Student
t-distribution-based similarity optimization. Three classification tasks were
formulated on the 2D t-SNE embeddings: (1) distinguishing clinical success from
failure/no-resection, (2) separating high-difficulty from low-difficulty cases,
and (3) identifying optimal cases, defined as successful outcomes with minimal
clinical difficulty. Four classifiers, namely, Random Forests, Support Vector
Machines, Logistic Regression, and k-Nearest Neighbors, were trained and
evaluated using stratified 5-fold cross-validation. Across tasks, the Random
Forest and k-NN classifiers demonstrated superior performance, achieving up to
88.8% accuracy in optimal case detection (successful outcomes with minimal
clinical difficulty). Additionally, feature influence sensitivity maps were
generated using SHAP explanations applied to model predicting t-SNE
coordinates, revealing spatially localized feature importance within the
embedding space. These maps highlighted how specific chirp attributes drive
regional clustering and class separation, offering insights into the latent
structure of the data. The integrated framework showcases the potential of
interpretable embeddings and local feature attribution for clinical
stratification and decision support.

</details>


### [39] [DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing](https://arxiv.org/abs/2508.13490)
*Pengyu Lai,Yixiao Chen,Hui Xu*

Main category: cs.LG

TL;DR: DyMixOp是一个新颖的神经算子框架，通过惯性流形理论将无限维非线性PDE动力学转换为有限维潜在空间，采用局部-全局混合变换有效捕捉细节和非线性相互作用，在多个PDE基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络近似非线性偏微分方程系统时的格式转换挑战，特别是在处理不可线性化动力学或需要无限维空间进行线性化的情况下。

Method: 基于惯性流形理论，提出局部-全局混合(LGM)变换，受湍流对流动力学启发，构建动力学知情的架构连接多个LGM层来近似线性和非线性动力学。

Result: 在多样化PDE基准测试中实现最先进性能，显著降低预测误差（在对流主导场景中高达86.7%），同时保持计算效率和可扩展性。

Conclusion: DyMixOp框架成功解决了非线性PDE系统的近似问题，通过理论驱动的变换方法有效捕捉系统动力学特征，在精度和效率方面都表现出色。

Abstract: A primary challenge in using neural networks to approximate nonlinear
dynamical systems governed by partial differential equations (PDEs) is
transforming these systems into a suitable format, especially when dealing with
non-linearizable dynamics or the need for infinite-dimensional spaces for
linearization. This paper introduces DyMixOp, a novel neural operator framework
for PDEs that integrates insights from complex dynamical systems to address
this challenge. Grounded in inertial manifold theory, DyMixOp transforms
infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent
space, establishing a structured foundation that maintains essential nonlinear
interactions and enhances physical interpretability. A key innovation is the
Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in
turbulence. This transformation effectively captures both fine-scale details
and nonlinear interactions, while mitigating spectral bias commonly found in
existing neural operators. The framework is further strengthened by a
dynamics-informed architecture that connects multiple LGM layers to approximate
linear and nonlinear dynamics, reflecting the temporal evolution of dynamical
systems. Experimental results across diverse PDE benchmarks demonstrate that
DyMixOp achieves state-of-the-art performance, significantly reducing
prediction errors, particularly in convection-dominated scenarios reaching up
to 86.7\%, while maintaining computational efficiency and scalability.

</details>


### [40] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: 提出了一种名为uncertainty tube的新型可视化方法，用于表示神经网络预测粒子轨迹时的不确定性，通过超椭圆管状结构直观展示非对称不确定性。


<details>
  <summary>Details</summary>
Motivation: 神经网络在粒子轨迹预测中应用广泛，但缺乏有效的不确定性量化和可视化方法，这限制了模型在可靠性要求高的应用中的可信度。

Method: 结合Deep Ensembles、MC Dropout和SWAG等成熟的不确定性量化技术，设计并实现了超椭圆管状结构来捕获和展示非对称不确定性。

Result: 在合成和仿真数据集上验证了uncertainty tube的实用性和计算效率，能够准确直观地表示预测不确定性。

Conclusion: uncertainty tube为神经网络粒子轨迹预测提供了一种有效的不确定性可视化解决方案，增强了模型在关键应用中的可信度。

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [41] [Explainability of Algorithms](https://arxiv.org/abs/2508.13529)
*Andrés Páez*

Main category: cs.LG

TL;DR: 这章讨论了机器学习算法的不透明性问题，分析了技术复杂性和商业保密导致的两种不透明类型，以及解释性AI面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统的黑盒特性导致的伦理障碍，探讨算法不透明性的根源和影响。

Method: 分析两种不透明性类型：技术复杂性导致的认知无法访问和商业保密导致的意图隐藏，研究解释性AI的方法。

Result: 识别了不同类型不透明性的特征和伦理含义，持续的解释性挑战需要更多研究。

Conclusion: 解释性AI对于避免伦理风险至关重要，但目前仍面临重重困难，需要继续探索有效的解释方法。

Abstract: The opaqueness of many complex machine learning algorithms is often mentioned
as one of the main obstacles to the ethical development of artificial
intelligence (AI). But what does it mean for an algorithm to be opaque? Highly
complex algorithms such as artificial neural networks process enormous volumes
of data in parallel along multiple hidden layers of interconnected nodes,
rendering their inner workings epistemically inaccessible to any human being,
including their designers and developers; they are "black boxes" for all their
stakeholders. But opaqueness is not always the inevitable result of technical
complexity. Sometimes, the way an algorithm works is intentionally hidden from
view for proprietary reasons, especially in commercial automated decision
systems, creating an entirely different type of opaqueness. In the first part
of the chapter, we will examine these two ways of understanding opacity and the
ethical implications that stem from each of them. In the second part, we
explore the different explanatory methods that have been developed in computer
science to overcome an AI system's technical opaqueness. As the analysis shows,
explainable AI (XAI) still faces numerous challenges.

</details>


### [42] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: CALYPSO是一个混合框架，结合神经网络和机制性元种群模型来预测MRSA在医疗和社区环境中的传播动态，相比机器学习基线提高了4.5%的预测性能。


<details>
  <summary>Details</summary>
Motivation: MRSA是医院和长期护理机构的重要公共卫生威胁，现有预测模型缺乏流行病学可解释性且性能有限，机制性流行病模型难以校准且无法有效整合多样化数据集。

Method: 整合神经网络与机制性元种群模型，利用患者级保险索赔数据、通勤数据和医疗转移模式，学习区域和时间特异性参数来捕捉MRSA传播动态。

Result: CALYPSO在州级预测性能上比机器学习基线提高了4.5%以上，能够识别高风险区域和制定成本效益高的感染预防资源分配策略。

Conclusion: 该框架能够在多个空间分辨率（县、医疗机构、区域、州）提供准确、可解释的预测，并支持感染控制政策和暴发风险的反事实分析。

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [43] [Collapsing ROC approach for risk prediction research on both common and rare variants](https://arxiv.org/abs/2508.13552)
*Changshuai Wei,Qing Lu*

Main category: cs.LG

TL;DR: 提出新的CROC方法，通过结合常见和稀有变异来提高疾病风险预测的准确性，在极端情况下显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于常见基因变异的风险预测模型准确性不足，需要结合稀有变异来实现更全面的风险预测策略。

Method: 提出摘叙收盘工作特征方法(CROC)，是之前前向ROC(FROC)方法的扩展，特别添加了处理稀有变异的流程。使用遗传分析工作址17小外显子数据集进行评估。

Result: 基于所有SNP的预测模型(AUC=0.605)比仅基于常见变异的模型(AUC=0.585)更准确。当常见变异数量减少时，CROC方法表现更好，在只有稀有变异的极端情况下，CROC AUC值达0.603，而FROC仅为0.524。

Conclusion: CROC方法能够有效结合常见和稀有变异进行风险预测，显著提高了预测准确性，尤其在稀有变异为主的情况下优势明显。

Abstract: Risk prediction that capitalizes on emerging genetic findings holds great
promise for improving public health and clinical care. However, recent risk
prediction research has shown that predictive tests formed on existing common
genetic loci, including those from genome-wide association studies, have lacked
sufficient accuracy for clinical use. Because most rare variants on the genome
have not yet been studied for their role in risk prediction, future disease
prediction discoveries should shift toward a more comprehensive risk prediction
strategy that takes into account both common and rare variants. We are
proposing a collapsing receiver operating characteristic CROC approach for risk
prediction research on both common and rare variants. The new approach is an
extension of a previously developed forward ROC FROC approach, with additional
procedures for handling rare variants. The approach was evaluated through the
use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the
Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction
model built on all SNPs gained more accuracy AUC = 0.605 than one built on
common variants alone AUC = 0.585. We further evaluated the performance of two
approaches by gradually reducing the number of common variants in the analysis.
We found that the CROC method attained more accuracy than the FROC method when
the number of common variants in the data decreased. In an extreme scenario,
when there are only rare variants in the data, the CROC reached an AUC value of
0.603, whereas the FROC had an AUC value of 0.524.

</details>


### [44] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: 提出GenHAI概率生成模型，用于建模住院患者MRSA检测结果序列，帮助医院管理者降低MRSA感染风险


<details>
  <summary>Details</summary>
Motivation: MRSA被CDC列为严重抗生素耐药威胁，住院患者因多种因素面临高风险，需要有效工具来理解和减轻感染风险

Method: 基于概率编程范式的生成概率模型，能够近似回答预测性、因果性和反事实性问题

Result: 在两个真实数据集上与判别式和生成式机器学习模型比较，证明了模型的有效性

Conclusion: GenHAI模型为医院管理者提供了强大的工具来理解和应对MRSA感染风险

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [45] [A Generalized Learning Framework for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2508.13596)
*Lingyu Si,Jingyao Wang,Wenwen Qiang*

Main category: cs.LG

TL;DR: 本文提出了一个广义自监督对比学习框架(GLF)，将现有方法统一为对齐和约束两部分，并提出了自适应分布校准(ADC)方法来提升特征空间的类内紧凑性和类间分离性。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督对比学习方法(BYOL、Barlow Twins、SwAV等)虽然在下游任务中表现出色，但缺乏统一的理论框架。作者希望建立一个通用框架来分析这些方法，并解决自监督学习中无法使用标签来确保特征空间类结构保持的问题。

Method: 1. 提出广义学习框架(GLF)，包含对齐部分和约束部分
2. 分析三种现有SSCL方法在GLF框架下的统一形式
3. 提出自适应分布校准(ADC)方法，通过迭代捕捉锚点与其他样本的动态关系来诱导类内紧凑性和类间分离性
4. ADC是一个即插即用的模块，确保原始输入空间中靠近/远离锚点的样本在特征空间中也相应靠近/远离

Result: 理论分析和实证评估都证明了ADC方法的优越性。该方法能够有效提升特征空间的质量，使其更好地保持输入数据的类别结构信息。

Conclusion: GLF框架为理解现有自监督对比学习方法提供了统一视角，ADC方法通过动态关系建模解决了自监督学习中类结构保持的挑战，为设计更好的约束部分提供了有效解决方案。

Abstract: Self-supervised contrastive learning (SSCL) has recently demonstrated
superiority in multiple downstream tasks. In this paper, we generalize the
standard SSCL methods to a Generalized Learning Framework (GLF) consisting of
two parts: the aligning part and the constraining part. We analyze three
existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be
unified under GLF with different choices of the constraining part. We further
propose empirical and theoretical analyses providing two insights into
designing the constraining part of GLF: intra-class compactness and inter-class
separability, which measure how well the feature space preserves the class
information of the inputs. However, since SSCL can not use labels, it is
challenging to design a constraining part that satisfies these properties. To
address this issue, we consider inducing intra-class compactness and
inter-class separability by iteratively capturing the dynamic relationship
between anchor and other samples and propose a plug-and-play method called
Adaptive Distribution Calibration (ADC) to ensure that samples that are near or
far from the anchor point in the original input space are closer or further
away from the anchor point in the feature space. Both the theoretical analysis
and the empirical evaluation demonstrate the superiority of ADC.

</details>


### [46] [Approximate Bayesian Inference via Bitstring Representations](https://arxiv.org/abs/2508.13598)
*Aleksanteri Sladek,Martin Trapp,Arno Solin*

Main category: cs.LG

TL;DR: 这篇论文提出在量化的离散参数空间中进行概率推理，通过概率电路学习连续分布，实现了高效的推理效果且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习社区正在研究量化或低精度算术来缩放大模型，需要在离散参数空间中进行概率推理的可扩展方案。

Method: 使用概率电路进行可处理的学习方法，考虑了2D密度和量化神经网络，在量化的离散参数空间中进行概率推理。

Result: 在各种模型上验证了该方法，展示了高效的推理能力且不牺牲准确性，能够管理复杂分布并提供模型行为清晰洞察。

Conclusion: 该工作通过利用离散近似进行概率计算，推进了可扩展、可解释的机器学习发展。

Abstract: The machine learning community has recently put effort into quantized or
low-precision arithmetics to scale large models. This paper proposes performing
probabilistic inference in the quantized, discrete parameter space created by
these representations, effectively enabling us to learn a continuous
distribution using discrete parameters. We consider both 2D densities and
quantized neural networks, where we introduce a tractable learning approach
using probabilistic circuits. This method offers a scalable solution to manage
complex distributions and provides clear insights into model behavior. We
validate our approach with various models, demonstrating inference efficiency
without sacrificing accuracy. This work advances scalable, interpretable
machine learning by utilizing discrete approximations for probabilistic
computations.

</details>


### [47] [Bounding Causal Effects and Counterfactuals](https://arxiv.org/abs/2508.13607)
*Tobias Maringgele*

Main category: cs.LG

TL;DR: 这篇论文系统性比较了多种因果推断的部分识别方法，提出了一个统一的评估框架并开源了Python工具包。


<details>
  <summary>Details</summary>
Motivation: 因果推断常依赖于难以验证的偏强假设，部分识别方法能提供更可靠的因果效应边界，但实际应用中存在方法分散和缺乏实践指南的问题。

Method: 实现、扩展并统一了谜语、优化和信息论方法，特别是将熵边界方法扩展到反事实查询。通过数千次随机模拟评估各方法的边界紧凑性、计算效率和稳健性。

Result: 研究结果细化为实践选择决策树，并训练了机器学习模型来预测最佳方法。所有实现已开源发布为CausalBoundingEngine Python包。

Conclusion: 该研究为部分识别方法提供了系统性的实践指南和工具支持，有助于推进这种更可靠的因果推断方法在实际应用中的使用。

Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured
confounding or perfect compliance - that are rarely satisfied in practice.
Partial identification offers a principled alternative: instead of relying on
unverifiable assumptions to estimate causal effects precisely, it derives
bounds that reflect the uncertainty inherent in the data. Despite its
theoretical appeal, partial identification remains underutilized in applied
work, in part due to the fragmented nature of existing methods and the lack of
practical guidance. This thesis addresses these challenges by systematically
comparing a diverse set of bounding algorithms across multiple causal
scenarios. We implement, extend, and unify state-of-the-art methods - including
symbolic, optimization-based, and information-theoretic approaches - within a
common evaluation framework. In particular, we propose an extension of a
recently introduced entropy-bounded method, making it applicable to
counterfactual queries such as the Probability of Necessity and Sufficiency
(PNS). Our empirical study spans thousands of randomized simulations involving
both discrete and continuous data-generating processes. We assess each method
in terms of bound tightness, computational efficiency, and robustness to
assumption violations. To support practitioners, we distill our findings into a
practical decision tree for algorithm selection and train a machine learning
model to predict the best-performing method based on observable data
characteristics.
  All implementations are released as part of an open-source Python package,
CausalBoundingEngine, which enables users to apply and compare bounding methods
through a unified interface.

</details>


### [48] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: FedOL是一种联邦学习新方法，通过知识蒸馏在单轮通信中构建更强大的服务器模型，使用未标记公共数据集交换预测输出而非模型参数，解决了异构架构和通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习需要统一模型架构和多轮通信，忽视了客户端资源异构性，计算和通信开销大。移动网络中客户端拥有宝贵私有数据但计算资源有限，需要更高效的解决方案。

Method: 采用知识蒸馏技术，客户端在未标记公共数据集上交换预测输出而非模型权重；设计专门的目标函数迭代优化伪标签和服务器模型；提出定制化的伪标签生成和知识蒸馏策略整合多样化知识。

Result: 仿真结果显示FedOL显著优于现有基线方法，为移动网络提供了成本效益高的解决方案。

Conclusion: FedOL通过单轮通信实现高效联邦学习，降低通信开销，支持异构模型架构，有效解决了客户端数据分布偏差和缺乏真实标签的挑战，在资源受限的移动网络中具有重要应用价值。

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [49] [Text2Weight: Bridging Natural Language and Neural Network Weight Spaces](https://arxiv.org/abs/2508.13633)
*Bowen Tian,Wenshuo Chen,Zexi Li,Songning Lai,Jiemin Wu,Yutao Yue*

Main category: cs.LG

TL;DR: T2W是一个基于扩散变换器的框架，能够根据自然语言描述生成特定任务的神经网络权重，在未见任务上表现优异并支持权重增强和文本引导的模型融合等新应用。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络权重生成方法在泛化到未见任务和实际应用探索方面存在困难，需要开发能够根据文本描述生成任务特定权重的实用方法。

Method: 提出T2W框架：将网络参数分层处理为统一块，通过先验注意力机制集成CLIP文本嵌入，采用对抗训练和权重空间增强来提高泛化能力。

Result: 在Cifar100、Caltech256和TinyImageNet上的实验表明，T2W能够为未见任务生成高质量权重，优于基于优化的初始化方法。

Conclusion: 该工作将文本语义与权重空间动态联系起来，通过开源文本-权重对数据集推进生成模型在神经网络参数合成中的实用性。

Abstract: How far are we really from automatically generating neural networks? While
neural network weight generation shows promise, current approaches struggle
with generalization to unseen tasks and practical application exploration. To
address this, we propose T2W, a diffusion transformer framework that generates
task-specific weights conditioned on natural language descriptions. T2W
hierarchically processes network parameters into uniform blocks, integrates
text embeddings from CLIP via a prior attention mechanism, and employs
adversarial training with weight-space augmentation to enhance generalization.
Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability
to produce high-quality weights for unseen tasks, outperforming
optimization-based initialization and enabling novel applications such as
weight enhancement and text-guided model fusion. Our work bridges textual
semantics with weight-space dynamics, supported by an open-source dataset of
text-weight pairs, advancing the practicality of generative models in neural
network parameter synthesis. Our code is available on Github.

</details>


### [50] [Explainable Learning Rate Regimes for Stochastic Optimization](https://arxiv.org/abs/2508.13639)
*Zhuang Yang*

Main category: cs.LG

TL;DR: 通过利用随机梯度的内在变化来自动调整学习率，不需手动调参的解释性学习率调整方法


<details>
  <summary>Details</summary>
Motivation: 现有学习率调整方法复杂且需要手动调整超参数，涉及巨大的计算开销和时间成本

Method: 基于随机二阶算法开发了一种解释性学习率调整方法，根据随机梯度的模长度变化自动增减学习率

Result: 该方法在SGD、SGDM、SIGNSGD等随机算法中展现了高效率、稳健性和扩展性

Conclusion: 通过利用随机梯度的本质特性，开发了一种简单有效无需调参的自动学习率调整方法

Abstract: Modern machine learning is trained by stochastic gradient descent (SGD),
whose performance critically depends on how the learning rate (LR) is adjusted
and decreased over time. Yet existing LR regimes may be intricate, or need to
tune one or more additional hyper-parameters manually whose bottlenecks include
huge computational expenditure, time and power in practice. This work, in a
natural and direct manner, clarifies how LR should be updated automatically
only according to the intrinsic variation of stochastic gradients. An
explainable LR regime by leveraging stochastic second-order algorithms is
developed, behaving a similar pattern to heuristic algorithms but implemented
simply without any parameter tuning requirement, where it is of an automatic
procedure that LR should increase (decrease) as the norm of stochastic
gradients decreases (increases). The resulting LR regime shows its efficiency,
robustness, and scalability in different classical stochastic algorithms,
containing SGD, SGDM, and SIGNSGD, on machine learning tasks.

</details>


### [51] [Personalized Subgraph Federated Learning with Sheaf Collaboration](https://arxiv.org/abs/2508.13642)
*Wenfei Liang,Yanan Zhao,Rui She,Yiming Li,Wee Peng Tay*

Main category: cs.LG

TL;DR: FedSheafHN是一个基于层束协作机制的个性化子图联邦学习框架，通过图级嵌入和层束扩散来增强客户端表示，并使用超网络生成定制化模型，在多个图数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决子图联邦学习中由于本地子图异构性导致的客户端性能差异问题，旨在为每个客户端开发定制化模型来处理不同的数据分布。

Method: 1) 利用图级嵌入将客户端本地子图嵌入到服务器构建的协作图中；2) 在协作图中使用层束扩散来丰富客户端表示；3) 通过服务器优化的超网络生成定制化的客户端模型。

Result: FedSheafHN在多个图数据集上优于现有的个性化子图联邦学习方法，表现出快速的模型收敛性，并能有效泛化到新客户端。

Conclusion: FedSheafHN通过层束协作机制成功解决了子图联邦学习中的异构性问题，为客户端提供了有效的个性化模型生成方案。

Abstract: Graph-structured data is prevalent in many applications. In subgraph
federated learning (FL), this data is distributed across clients, each with a
local subgraph. Personalized subgraph FL aims to develop a customized model for
each client to handle diverse data distributions. However, performance
variation across clients remains a key issue due to the heterogeneity of local
subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework
built on a sheaf collaboration mechanism to unify enhanced client descriptors
with efficient personalized model generation. Specifically, FedSheafHN embeds
each client's local subgraph into a server-constructed collaboration graph by
leveraging graph-level embeddings and employing sheaf diffusion within the
collaboration graph to enrich client representations. Subsequently, FedSheafHN
generates customized client models via a server-optimized hypernetwork.
Empirical evaluations demonstrate that FedSheafHN outperforms existing
personalized subgraph FL methods on various graph datasets. Additionally, it
exhibits fast model convergence and effectively generalizes to new clients.

</details>


### [52] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: GRAFT是一种可扩展的训练中子集选择方法，通过低秩特征表示和Fast MaxVol采样器选择多样性子集，动态调整子集大小，在保持训练轨迹的同时减少计算时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在大型数据集上的训练计算成本和环境成本高昂，需要一种高效的训练方法来减少计算时间、能耗和碳排放。

Method: 提取每批数据的低秩特征表示，应用Fast MaxVol采样器选择覆盖批次主导子集的多样小子集，使用梯度近似准则动态调整子集大小。

Result: 在多个基准测试中，GRAFT在准确性和效率方面匹配或超过现有选择基线，在准确性、效率和排放之间提供了有利的权衡。

Conclusion: GRAFT通过在低秩子空间中操作并训练精心选择的样本而非完整批次，能够保持训练轨迹同时显著减少训练时间、能耗和CO2排放。

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [53] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 输入时间缩放新范式：通过训练测试协同设计，在查询输入时使用元知识精炼输入，达到超过传统数据精细化方法的性能。反转"垃圾进垃圾出"偏见，证明随机选择和混入无关信息的输入策略可获得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要靠数据资源和推理时间缩放，本文提出在输入时间进行缩放的新方法，以补充现有方法的不足。

Method: 在训练和测试阶段都使用元知识精炼输入，采用不同的查询策略。发现训练-测试协同设计的重要性，只在单一阶段应用策略会导致性能严重下降。

Result: 在Qwen2.5-32B-Instruct模型上达到AIME24(76.7%)和AIME25(76.7%)的SOTA性能，多模型投票可达AIME24(76.7%)和AIME25(80%)。从DeepSeek-R1-Distill-Qwen-32B起步最佳结果为AIME24(86.7%)和AIME25(76.7%)。

Conclusion: 突破传统"垃圾进垃圾出"偏见，证明随机选择和混入无关信息的输入策略反而更有效。数据集质量和规模的简单缩放需谨慎考虑，小规模数据集即可激活高级推理能力。

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


### [54] [In-Context Decision Making for Optimizing Complex AutoML Pipelines](https://arxiv.org/abs/2508.13657)
*Amir Rezaei Balef,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 扩展CASH框架以适应现代ML流程，提出PS-PFN方法通过后验采样和上下文学习高效探索和利用机器学习流水线适配


<details>
  <summary>Details</summary>
Motivation: 传统AutoML系统的CASH框架无法满足现代ML工作流对于精调、集成等异质性流水线的需求，需要扩展以适应新的挑战

Method: 提出PS-PFN方法，将后验采样扩展到最大k臂老虎机问题设置，利用上下文学习通过先验数据拟合网络高效估计最大值的后验分布

Result: 在一个新的和两个现有标准测试任务上，PS-PFN表现出超过其他老虎机和AutoML策略的优异性能

Conclusion: PS-PFN为现代AutoML系统提供了一种高效的方法来选择和适配异质性ML流水线，扩展了CASH框架的应用范围

Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been
fundamental to traditional AutoML systems. However, with the advancements of
pre-trained models, modern ML workflows go beyond hyperparameter optimization
and often require fine-tuning, ensembling, and other adaptation techniques.
While the core challenge of identifying the best-performing model for a
downstream task remains, the increasing heterogeneity of ML pipelines demands
novel AutoML approaches. This work extends the CASH framework to select and
adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit
adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed
bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to
efficiently estimate the posterior distribution of the maximal value via
in-context learning. We show how to extend this method to consider varying
costs of pulling arms and to use different PFNs to model reward distributions
individually per arm. Experimental results on one novel and two existing
standard benchmark tasks demonstrate the superior performance of PS-PFN
compared to other bandit and AutoML strategies. We make our code and data
available at https://github.com/amirbalef/CASHPlus.

</details>


### [55] [MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.13661)
*Maciej Wojtala,Bogusz Stefańczyk,Dominik Bogucki,Łukasz Lepak,Jakub Strykowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 基于自注意力机制的可微分多满派道通信模块，能够在SMAC基准上达到状态前沿性能


<details>
  <summary>Details</summary>
Motivation: 现有多满派道通信协议通常复杂且不可微分，需要一种更有效的通信机制来支持集体复杂任务的执行

Method: 提出了一种基于自注意力机制的通信模块，该模块完全可微分，允许满派通过奖励驱动的方式学习生成消息，可以无缝集成到任何动作价值函数分解方法中

Result: 在SMAC基准上的实验结果证明了该方法的有效性，在多个地图上达到了状态前沿性能

Conclusion: 该研究提出了一种简洁、可微分且高效的通信机制，为多满派道强化学习领域提供了新的通信解决方案

Abstract: Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.

</details>


### [56] [Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond](https://arxiv.org/abs/2508.13679)
*Canzhe Zhao,Shinji Ito,Shuai Li*

Main category: cs.LG

TL;DR: 提出了一个对抗性重尾bandit问题的通用框架，通过精心设计的奖励函数实现FTRL算法，在重尾MAB中首次实现BOBW性能，并在线性bandit中扩展到有限臂集情况。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在随机环境下的重尾bandit问题，对抗性环境下的研究很少且仅限于MAB特殊情况，需要开发适用于对抗性重尾bandit的通用框架。

Method: 使用FTRL算法，在损失估计上添加奖励函数，提出HT-SPM数据依赖学习率，结合方差缩减线性损失估计器。

Result: 在重尾MAB中实现O~(T^(1/ε))最坏情况遗憾和O~(log T)间隙依赖遗憾；在线性bandit中获得O~(d^(1/2)T^(1/ε))遗憾，匹配随机环境最佳界限；首次实现重尾线性bandit的BOBW结果。

Conclusion: 提出了对抗性重尾bandit的通用框架，解决了该领域的关键问题，为随机和对抗环境提供了统一的解决方案，在理论和算法设计上都有重要贡献。

Abstract: Heavy-tailed bandits have been extensively studied since the seminal work of
\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,
enabling efficient learning with both a large number of arms and heavy-tailed
noises, have recently attracted significant attention
\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.
However, prior studies focus almost exclusively on stochastic regimes, with few
exceptions limited to the special case of heavy-tailed multi-armed bandits
(MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}.
  In this work, we propose a general framework for adversarial heavy-tailed
bandit problems, which performs follow-the-regularized-leader (FTRL) over the
loss estimates shifted by a bonus function. Via a delicate setup of the bonus
function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm
for heavy-tailed MABs, which does not require the truncated non-negativity
assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$
worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log
T)$ gap-dependent regret in the stochastic regime. We then extend our framework
to the linear case, proposing the first algorithm for adversarial heavy-tailed
linear bandits with finite arm sets. This algorithm achieves an
$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the
best-known worst-case regret bound in stochastic regimes. Moreover, we propose
a general data-dependent learning rate, termed \textit{heavy-tailed noise aware
stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW
regret bounds for general heavy-tailed bandit problems once certain conditions
are satisfied. By using HT-SPM and, in particular, a variance-reduced linear
loss estimator, we obtain the first BOBW result for heavy-tailed linear
bandits.

</details>


### [57] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: 这篇论文提出了一种结合联邦学习和可解释AI技术的Trans-XFed架构，用于供应链信用评估，解决隐私保护、数据异构和模型可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 供应链信用评估面临隐私保护、信息孤岛、类不平衡、非IID数据和模型可解释性等多重挑战，需要一种方案来同时解决这些问题。

Method: 采用基于绩效的客户端选择策略(PBCS)处理类不平衡和非IID数据，使用增强同态加密的FedProx架构作为核心模型，加入变换器编码器提供特征视觉化，并使用集成梯度可解释AI技术来解释决策过程。

Result: 在真实供应链数据集上的实验评估显示，Trans-XFed能够比多个基线方法更准确地进行信用评估，同时保持透明性和隐私保护。

Conclusion: 该研究成功开发了一种能够同时满足准确性、隐私保护和可解释性要求的供应链信用评估方案，为复杂的实际应用场景提供了有效的解决方案。

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


### [58] [DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction](https://arxiv.org/abs/2508.13747)
*Noël Kury,Dmitry Kobak,Sebastian Damrich*

Main category: cs.LG

TL;DR: DREAMS是一种新的降维方法，通过正则化项结合t-SNE的局部结构保持和PCA的全局结构保持，生成在两者之间的嵌入谱，有效平衡多尺度结构保持


<details>
  <summary>Details</summary>
Motivation: 现有降维方法要么专注于保持局部结构（如t-SNE、UMAP），要么专注于保持全局结构（如MDS、PCA），但没有方法能够同时很好地保持这两个方面的结构

Method: 通过简单的正则化项将t-SNE的局部结构保持与PCA的全局结构保持相结合，生成从局部结构良好的t-SNE嵌入到全局结构良好的PCA嵌入之间的谱系嵌入

Result: 在7个真实数据集（包括5个单细胞转录组数据集和1个群体遗传学数据集）上进行基准测试，定性和定量结果都显示DREAMS在多尺度结构保持方面优于先前方法

Conclusion: DREAMS方法能够有效平衡局部和全局结构保持，在多尺度数据可视化方面表现出优越性能

Abstract: Dimensionality reduction techniques are widely used for visualizing
high-dimensional data in two dimensions. Existing methods are typically
designed to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,
PCA) structure of the data, but none of the established methods can represent
both aspects well. In this paper, we present DREAMS (Dimensionality Reduction
Enhanced Across Multiple Scales), a method that combines the local structure
preservation of $t$-SNE with the global structure preservation of PCA via a
simple regularization term. Our approach generates a spectrum of embeddings
between the locally well-structured $t$-SNE embedding and the globally
well-structured PCA embedding, efficiently balancing both local and global
structure preservation. We benchmark DREAMS across seven real-world datasets,
including five from single-cell transcriptomics and one from population
genetics, showcasing qualitatively and quantitatively its superior ability to
preserve structure across multiple scales compared to previous approaches.

</details>


### [59] [Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting](https://arxiv.org/abs/2508.13749)
*Mohammad Taha Shah,Sabrina Khurshid,Gourab Ghatak*

Main category: cs.LG

TL;DR: 本文研究在随机多臂老虎机设置中最大化夏普比率的序列决策问题，提出基于Thompson Sampling的SRTS算法，在理论分析和实证验证方面均取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统老虎机算法主要关注最大化累积奖励，而夏普比率优化需要在获得高回报和控制风险之间取得平衡，这需要对均值和方差进行仔细探索。

Method: 采用Thompson Sampling贝叶斯方法，假设奖励服从未知参数的高斯分布，设计了专门针对夏普比率的遗憾分解方法，提出了SRTS算法。

Result: 理论分析表明SRTS算法实现了对数遗憾，建立了遗憾的上界和下界并证明了阶数最优性。实证模拟显示该算法显著优于现有算法。

Conclusion: Thompson Sampling方法在夏普比率最大化问题上表现优异，既能有效探索均值和方差信息，又能实现理论上的最优性能，为风险调整的序列决策提供了有效解决方案。

Abstract: In this paper, we investigate the problem of sequential decision-making for
Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the
Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its
empirical performance and exploration efficiency, under the assumption of
Gaussian rewards with unknown parameters. Unlike conventional bandit objectives
focusing on maximizing cumulative reward, Sharpe ratio optimization instead
introduces an inherent tradeoff between achieving high returns and controlling
risk, demanding careful exploration of both mean and variance. Our theoretical
contributions include a novel regret decomposition specifically designed for
the Sharpe ratio, highlighting the role of information acquisition about the
reward distribution in driving learning efficiency. Then, we establish
fundamental performance limits for the proposed algorithm \texttt{SRTS} in
terms of an upper bound on regret. We also derive the matching lower bound and
show the order-optimality. Our results show that Thompson Sampling achieves
logarithmic regret over time, with distribution-dependent factors capturing the
difficulty of distinguishing arms based on risk-adjusted performance. Empirical
simulations show that our algorithm significantly outperforms existing
algorithms.

</details>


### [60] [Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration](https://arxiv.org/abs/2508.13755)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Dongchun Xie,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: 本文提出了DARS和DARS-B方法，通过深度自适应采样和广度扩展来解决RLVR中的系统性偏差问题，显著提升了语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在系统性偏差，累积优势函数过度重视中等准确率的样本，而忽视了低准确率但对推动推理边界至关重要的困难问题。同时，训练数据的广度维度也未得到充分探索。

Method: 1. 提出Difficulty Adaptive Rollout Sampling (DARS)：通过目标多阶段rollout重新加权困难问题，增加困难问题的正rollout数量；2. 引入广度扩展：大幅增加批次大小，用多轮全批次更新替代PPO的小批次迭代；3. 结合两者提出DARS-B方法。

Result: DARS在不增加推理成本的情况下持续提升Pass@K性能；广度扩展显著提升Pass@1性能；DARS-B同时提升了Pass@K和Pass@1性能；大广度训练保持了高token级熵，表明持续探索和减少梯度噪声。

Conclusion: 广度和跨深度的自适应探索是RLVR中正交的两个维度，对于释放RLVR的推理能力至关重要。DARS和DARS-B方法有效解决了系统性偏差问题，为语言模型的推理能力提升提供了新方向。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.

</details>


### [61] [PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting](https://arxiv.org/abs/2508.13773)
*Tian Sun,Yuqi Chen,Weiwei Sun*

Main category: cs.LG

TL;DR: 重新考察自注意力机制在时间序列预测中的效果，提出周期嵌套组注意力(PENGUIN)机制，显著提升预测性能


<details>
  <summary>Details</summary>
Motivation: Transformer模型在时间序列预测中的效果存在争议，需要重新考察自注意力机制的适用性

Method: 提出周期嵌套相对注意偏置来抓取周期结构，使用分组注意力机制处理多重共存周期性，每个组使用多查询注意力

Result: 在多个基准测试中，PENGUIN一致超过了MLP基础和Transformer基础模型

Conclusion: 显式建模周期模式和统计相对注意偏置对时间序列建模至关重要，PENGUIN提供了一种简单但有效的解决方案

Abstract: Long-term time series forecasting (LTSF) is a fundamental task with
wide-ranging applications. Although Transformer-based models have made
significant breakthroughs in forecasting, their effectiveness for time series
forecasting remains debatable. In this paper, we revisit the significance of
self-attention and propose a simple yet effective mechanism, Periodic-Nested
Group Attention, namely PENGUIN. Our approach highlights the importance of
explicitly modeling periodic patterns and incorporating relative attention bias
for effective time series modeling. To this end, we introduce a periodic-nested
relative attention bias that captures periodic structures directly. To handle
multiple coexisting periodicities (e.g., daily and weekly cycles), we design a
grouped attention mechanism, where each group targets a specific periodicity
using a multi-query attention mechanism. Extensive experiments across diverse
benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and
Transformer-based models.

</details>


### [62] [Communication-Efficient Federated Learning with Adaptive Number of Participants](https://arxiv.org/abs/2508.13803)
*Sergey Skorik,Vladislav Dorofeev,Gleb Molodtsov,Aram Avetisyan,Dmitry Bylinkin,Daniil Medyakov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 这篇论文提出了一种名为ISP的自适应机制，动态确定联邦学习中每轮训练的最优客户端数量，在保持模型准确性的同时实现了达30%的通信效率提升。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信效率仍是关键瓶颈，尤其在异构和动态客户端参与情况下。现有方法如FedAvg和FedProx等对选择每轮参与客户数量的问题研究不足。

Method: 提出ISP（Intelligent Selection of Participants）机制，通过动态适应的方式确定每轮训练的最优客户端数量，以提升通信效率。

Result: 在多种应用场景下验证，包括视觉Transformer、实际ECG分类以及梯度压缩训练，均实现了达30%的通信节省，且不丢失最终模型质量。

Conclusion: 选择参与客户端数量是联邦学习中一个独立且重要的任务，ISP机制有效解决了这一问题。

Abstract: Rapid scaling of deep learning models has enabled performance gains across
domains, yet it introduced several challenges. Federated Learning (FL) has
emerged as a promising framework to address these concerns by enabling
decentralized training. Nevertheless, communication efficiency remains a key
bottleneck in FL, particularly under heterogeneous and dynamic client
participation. Existing methods, such as FedAvg and FedProx, or other
approaches, including client selection strategies, attempt to mitigate
communication costs. However, the problem of choosing the number of clients in
a training round remains extremely underexplored. We introduce Intelligent
Selection of Participants (ISP), an adaptive mechanism that dynamically
determines the optimal number of clients per round to enhance communication
efficiency without compromising model accuracy. We validate the effectiveness
of ISP across diverse setups, including vision transformers, real-world ECG
classification, and training with gradient compression. Our results show
consistent communication savings of up to 30\% without losing the final
quality. Applying ISP to different real-world ECG classification setups
highlighted the selection of the number of clients as a separate task of
federated learning.

</details>


### [63] [Reinforcement Learning-based Adaptive Path Selection for Programmable Networks](https://arxiv.org/abs/2508.13806)
*José Eduardo Zerna Torres,Marios Avgeris,Chrysa Papagianni,Gergely Pongrácz,István Gódor,Paola Grosso*

Main category: cs.LG

TL;DR: 基于随机学习自动机和带内网络测量的分布式网络内强化学习框架，实现了可程序网络中的自适应路径选择


<details>
  <summary>Details</summary>
Motivation: 解决网络拥塞问题，通过分布式在网络内部做出动态适应的转发决策，提高网络性能和灵活性

Method: 结合随机学习自动机(SLA)和带内网络测量(INT)实时数据，在P4可编程BMv2交换机上实现分布式学习机制

Result: 系统能够在线速度下进行高效进化，进行有效的路径选择，并动态适应网络条件变化

Conclusion: 证明了分布式网络内强化学习框架的可行性，为可程序网络提供了一种自适应路由选择的有效方案

Abstract: This work presents a proof-of-concept implementation of a distributed,
in-network reinforcement learning (IN-RL) framework for adaptive path selection
in programmable networks. By combining Stochastic Learning Automata (SLA) with
real-time telemetry data collected via In-Band Network Telemetry (INT), the
proposed system enables local, data-driven forwarding decisions that adapt
dynamically to congestion conditions. The system is evaluated on a
Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how
our SLA-based mechanism converges to effective path selections and adapts to
shifting network conditions at line rate.

</details>


### [64] [Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias](https://arxiv.org/abs/2508.13813)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Frank Kargl*

Main category: cs.LG

TL;DR: 首个基于主观逻辑的正式框架，用于评估AI训练数据集的可信质量（如偏见），支持不确定性量化和解释性


<details>
  <summary>Details</summary>
Motivation: AI系统越来越依赖训练数据，需要在数据集层面评估偏见等全局属性的可信质量，而现有方法主要关注单个数据点

Method: 基于主观逻辑构建正式框架，支持信任命题和在证据不完整/分布/冲突场景下量化不确定性，并在偏见属性上实现该框架

Result: 在交通标志识别数据集上进行实验评估，结果显示方法能够捐描类别不平衡，在集中和联邦环境下都保持解释性和稳健性

Conclusion: 该框架为AI训练数据集的可信质量评估提供了第一个正式的不确定性意识方法，特别在评估偏见等全局属性方面具有重要价值

Abstract: As AI systems increasingly rely on training data, assessing dataset
trustworthiness has become critical, particularly for properties like fairness
or bias that emerge at the dataset level. Prior work has used Subjective Logic
to assess trustworthiness of individual data, but not to evaluate
trustworthiness properties that emerge only at the level of the dataset as a
whole. This paper introduces the first formal framework for assessing the
trustworthiness of AI training datasets, enabling uncertainty-aware evaluations
of global properties such as bias. Built on Subjective Logic, our approach
supports trust propositions and quantifies uncertainty in scenarios where
evidence is incomplete, distributed, and/or conflicting. We instantiate this
framework on the trustworthiness property of bias, and we experimentally
evaluate it based on a traffic sign recognition dataset. The results
demonstrate that our method captures class imbalance and remains interpretable
and robust in both centralized and federated contexts.

</details>


### [65] [One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression](https://arxiv.org/abs/2508.13836)
*Mikołaj Janusz,Tomasz Wojnar,Yawei Li,Luca Benini,Kamil Adamczewski*

Main category: cs.LG

TL;DR: 本文系统性比较了一次性剪枝和迭代剪枝两种深度学习模型压缩方法，发现两者在不同剪枝比下各有优势，并提出了新的混合剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 虽然迭代剪枝历史上更受欢迎，但这种偏好多为假设而非严格验证。需要系统性比较两种方法的真实效果。

Method: 采用严格定义和标准化测试流程，在结构化和非结构化剪枝设置下对比了两种方法，应用了不同的剪枝准则和模态。

Result: 一次性剪枝在低剪枝比时效果更好，而迭代剪枝在高剪枝比时表现更优。基于此发现提出了耐心基础的剪枝和混合方法。

Conclusion: 研究给实践者提供了根据具体目标和约束来选择剪枝策略的价值见解，混合方法在某些场景下能超过传统方法。

Abstract: Pruning is a core technique for compressing neural networks to improve
computational efficiency. This process is typically approached in two ways:
one-shot pruning, which involves a single pass of training and pruning, and
iterative pruning, where pruning is performed over multiple cycles for
potentially finer network refinement. Although iterative pruning has
historically seen broader adoption, this preference is often assumed rather
than rigorously tested. Our study presents one of the first systematic and
comprehensive comparisons of these methods, providing rigorous definitions,
benchmarking both across structured and unstructured settings, and applying
different pruning criteria and modalities. We find that each method has
specific advantages: one-shot pruning proves more effective at lower pruning
ratios, while iterative pruning performs better at higher ratios. Building on
these findings, we advocate for patience-based pruning and introduce a hybrid
approach that can outperform traditional methods in certain scenarios,
providing valuable insights for practitioners selecting a pruning strategy
tailored to their goals and constraints. Source code is available at
https://github.com/janumiko/pruning-benchmark.

</details>


### [66] [FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks](https://arxiv.org/abs/2508.13853)
*Nicolò Romandini,Cristian Borcea,Rebecca Montanari,Luca Foschini*

Main category: cs.LG

TL;DR: FedUP是一种轻量级的联邦遗忘算法，通过剪枝特定连接来高效消除恶意客户端对全局模型的影响，仅需最后一轮训练权重即可实现，在保持良性数据性能的同时将恶意数据准确率降至与重新训练模型相当的水平。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到模型投毒攻击，传统联邦遗忘假设客户端可信合作，但在恶意客户端可能共谋的场景下，现有方法无法有效工作，需要开发能够在不依赖恶意客户端合作的情况下消除其影响的解决方案。

Method: FedUP通过分析良性客户端和恶意客户端在最后一轮训练中的权重更新差异，选择并清零那些差异最大的高幅度权重，从而剪除恶意影响同时保留良性信息，无需完全重新训练。

Result: 在IID和非IID数据、标签翻转和后门攻击等多种场景下，FedUP能有效降低恶意影响，将恶意数据准确率降至与从头训练模型相当的水平，同时保持良性数据性能，且比现有最优方法更快、更节省存储。

Conclusion: FedUP提供了一种高效、鲁棒的联邦遗忘解决方案，能够在恶意客户端可能共谋的强对抗环境下有效消除恶意影响，为联邦学习安全性提供了重要保障。

Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model
poisoning, where adversaries send malicious local weights to compromise the
global model. Federated Unlearning (FU) is emerging as a solution to address
such vulnerabilities by selectively removing the influence of detected
malicious contributors on the global model without complete retraining.
However, unlike typical FU scenarios where clients are trusted and cooperative,
applying FU with malicious and possibly colluding clients is challenging
because their collaboration in unlearning their data cannot be assumed. This
work presents FedUP, a lightweight FU algorithm designed to efficiently
mitigate malicious clients' influence by pruning specific connections within
the attacked model. Our approach achieves efficiency by relying only on
clients' weights from the last training round before unlearning to identify
which connections to inhibit. Isolating malicious influence is non-trivial due
to overlapping updates from benign and malicious clients. FedUP addresses this
by carefully selecting and zeroing the highest magnitude weights that diverge
the most between the latest updates from benign and malicious clients while
preserving benign information. FedUP is evaluated under a strong adversarial
threat model, where up to 50%-1 of the clients could be malicious and have full
knowledge of the aggregation process. We demonstrate the effectiveness,
robustness, and efficiency of our solution through experiments across IID and
Non-IID data, under label-flipping and backdoor attacks, and by comparing it
with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces
malicious influence, lowering accuracy on malicious data to match that of a
model retrained from scratch while preserving performance on benign data. FedUP
achieves effective unlearning while consistently being faster and saving
storage compared to the SOTA.

</details>


### [67] [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](https://arxiv.org/abs/2508.13874)
*Rouqaiah Al-Refai,Pankaja Priya Ramasamy,Ragini Ramesh,Patricia Arias-Cabarcos,Philipp Terhörst*

Main category: cs.LG

TL;DR: 这篇论文通过对24位生物识别专家的调查，重新评估了生物识别模态的适用性，发现了随着技术进步和新兴漏洞而产生的显著变化，并为生物识别系统提供了更可靠的评估框架。


<details>
  <summary>Details</summary>
Motivation: 目前生物识别系统评估框架主要依靠1998年的比较表格，无法充分反映最新技术发展和新兴漏洞，需要更可靠的评估方法。

Method: 通过对24位生物识别专家的调查，重新评估各种生物识别模态的属性评分，并将专家评估与55个生物识别数据集的数据层面不确定性进行对比分析。

Result: 发现生物识别模态评分出现显著变化：人脸识别因技术进步而提升，指纹识别因漏洞而可靠性下降。专家评估与实验数据在大多数模态上呈现强一致性，专家分歧也揭示了关键挑战。

Conclusion: 研究为生物识别系统提供了更加可靠的评估框架，并指明了将专家见解与实证数据相结合的重要性，为未来研究指明了方向。

Abstract: The rapid advancement of authentication systems and their increasing reliance
on biometrics for faster and more accurate user verification experience,
highlight the critical need for a reliable framework to evaluate the
suitability of biometric modalities for specific applications. Currently, the
most widely known evaluation framework is a comparative table from 1998, which
no longer adequately captures recent technological developments or emerging
vulnerabilities in biometric systems. To address these challenges, this work
revisits the evaluation of biometric modalities through an expert survey
involving 24 biometric specialists. The findings indicate substantial shifts in
property ratings across modalities. For example, face recognition, shows
improved ratings due to technological progress, while fingerprint, shows
decreased reliability because of emerging vulnerabilities and attacks. Further
analysis of expert agreement levels across rated properties highlighted the
consistency of the provided evaluations and ensured the reliability of the
ratings. Finally, expert assessments are compared with dataset-level
uncertainty across 55 biometric datasets, revealing strong alignment in most
modalities and underscoring the importance of integrating empirical evidence
with expert insight. Moreover, the identified expert disagreements reveal key
open challenges and help guide future research toward resolving them.

</details>


### [68] [Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches](https://arxiv.org/abs/2508.13898)
*Yishun Lu,Wesley Armour*

Main category: cs.LG

TL;DR: 提出了Fisher正交投影(FOP)方法，解决大批次训练时二阶优化方法失效的问题，通过利用两个子批次的梯度信息构建方差感知的更新方向


<details>
  <summary>Details</summary>
Motivation: 现代GPU支持大批次训练(数万个样本)，但现有优化器在大批次下表现不佳。一阶方法梯度噪声减少难以逃离局部极小值，二阶方法(如KFAC)需要过高阻尼导致曲率信息丢失

Method: FOP技术利用两个子批次的梯度，在Fisher度量下构建与平均梯度正交的梯度差异分量，增强平均梯度形成方差感知的更新方向

Result: FOP恢复了二阶方法在大批次下的有效性，实现了可扩展训练，具有更好的泛化能力和更快的收敛速度

Conclusion: FOP是一种创新技术，能够有效解决大批次训练中优化方法性能下降的问题，为大规模深度学习训练提供了新的解决方案

Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory,
enabling them to support mini-batch sizes of up to tens of thousands of
training samples. However, most existing optimizers struggle to perform
effectively at such a large batch size. As batch size increases, gradient noise
decreases due to averaging over many samples, limiting the ability of
first-order methods to escape sharp or suboptimal minima and reach the global
minimum. Meanwhile, second-order methods like the natural gradient with
Kronecker-Factored Approximate Curvature (KFAC) often require excessively high
damping to remain stable at large batch sizes. This high damping effectively
washes out the curvature information that gives these methods their advantage,
reducing their performance to that of simple gradient descent. In this paper,
we introduce Fisher-Orthogonal Projection (FOP), a novel technique that
restores the effectiveness of the second-order method at very large batch
sizes, enabling scalable training with improved generalization and faster
convergence. FOP constructs a variance-aware update direction by leveraging
gradients from two sub-batches, enhancing the average gradient with a component
of the gradient difference that is orthogonal to the average under the
Fisher-metric.

</details>


### [69] [Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation](https://arxiv.org/abs/2508.13904)
*Thanh Nguyen,Chang D. Yoo*

Main category: cs.LG

TL;DR: 提出了One-Step Flow Q-Learning (OFQL)框架，通过Flow Matching方法实现单步动作生成，解决了Diffusion Q-Learning多步去噪的低效问题，在保持性能的同时大幅提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在离线强化学习中表现出色，但Diffusion Q-Learning依赖多步去噪生成动作，导致训练和推理效率低下。虽然单步去噪更理想，但直接应用会导致性能急剧下降。

Method: 将DQL重新表述到Flow Matching框架中，学习平均速度场而非传统FM的弯曲生成轨迹，从而实现直接准确的单步动作生成，无需辅助模型、蒸馏或多阶段训练。

Result: 在D4RL基准测试中，OFQL超越了DQL和其他基于扩散的基线方法，同时相比DQL显著减少了训练和推理时间。

Conclusion: OFQL成功解决了扩散强化学习中的效率瓶颈，通过单步生成实现了性能与效率的双重提升，为实际应用提供了更可行的解决方案。

Abstract: The generative power of diffusion models (DMs) has recently enabled
high-performing decision-making algorithms in offline reinforcement learning
(RL), achieving state-of-the-art results across standard benchmarks. Among
them, Diffusion Q-Learning (DQL) stands out as a leading method for its
consistently strong performance. Nevertheless, DQL remains limited in practice
due to its reliance on multi-step denoising for action generation during both
training and inference. Although one-step denoising is desirable, simply
applying it to DQL leads to a drastic performance drop. In this work, we
revisit DQL and identify its core limitations. We then propose One-Step Flow
Q-Learning (OFQL), a novel framework that enables efficient one-step action
generation during both training and inference, without requiring auxiliary
models, distillation, or multi-phase training. Specifically, OFQL reformulates
DQL within the sample-efficient Flow Matching (FM) framework. While
conventional FM induces curved generative trajectories that impede one-step
generation, OFQL instead learns an average velocity field that facilitates
direct, accurate action generation. Collectively, OFQL eliminates the need for
multi-step sampling and recursive gradient updates in DQL, resulting in faster
and more robust training and inference. Extensive experiments on the D4RL
benchmark demonstrate that OFQL outperforms DQL and other diffusion-based
baselines, while substantially reducing both training and inference time
compared to DQL.

</details>


### [70] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于边缘计算的端到端预测框架，通过轻量化Transformer和LSTM模型在FPGA设备上实现高效能的污水溢流池填充水平预测，解决了传统云计算AI模型在通信故障时的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件加剧了老化合流制污系统的污水溢流风险，而传统的云计算AI预测方法在通信故障时存在可靠性问题，需要开发能够在边缘设备上高效运行的本地预测解决方案。

Method: 提出了一种端到端预测框架，集成轻量化Transformer和LSTM模型，通过整数量化压缩技术优化模型性能。使用自动化硬件感知部署管道，在AMD Spartan-7 FPGA上搜索最优模型配置，同时最小化预测误差和能消耗。

Result: 8位Transformer模型在24小时历史数据训练下实现高精度（MSE 0.0376），每次推理能耗0.370 mJ。8位LSTM模型能耗更低（0.009 mJ，降低40倍以上）但精度较差（MSE 0.0432，误差增加14.89%）且训练时间更长。

Conclusion: 工作实现了本地化、能效高的预测能力，为合流污水系统提供了更鲁棒的解决方案。LSTM适合超低能耗场景，Transformer适合高精度预测需求，模型选择应根据部署优先级进行。

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


### [71] [Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control](https://arxiv.org/abs/2508.13922)
*SM Mazharul Islam,Manfred Huber*

Main category: cs.LG

TL;DR: 通过引入分类策略模型多模态行为，解决传统高斯策略在连续控制任务中探索不足的问题


<details>
  <summary>Details</summary>
Motivation: 传统高斯策略只能学习单模态行为，而实际决策问题需要多模态策略来应对稀疏奖励、复杂动力学和环境变化的挑战

Method: 使用中间分类分布模型多模态行为模式，通过采样行为模式来生成输出动作，采用两种可微分采样方案确保离散潜变结构的可优化性

Result: 在DeepMind Control Suite环境中，多模态策略通过更好的探索完成更快的收敛和更高的性能，超过传统高斯策略

Conclusion: 分类分布是连续控制中结构化探索和多模态行为表示的强大工具

Abstract: A policy in deep reinforcement learning (RL), either deterministic or
stochastic, is commonly parameterized as a Gaussian distribution alone,
limiting the learned behavior to be unimodal. However, the nature of many
practical decision-making problems favors a multimodal policy that facilitates
robust exploration of the environment and thus to address learning challenges
arising from sparse rewards, complex dynamics, or the need for strategic
adaptation to varying contexts. This issue is exacerbated in continuous control
domains where exploration usually takes place in the vicinity of the predicted
optimal action, either through an additive Gaussian noise or the sampling
process of a stochastic policy. In this paper, we introduce Categorical
Policies to model multimodal behavior modes with an intermediate categorical
distribution, and then generate output action that is conditioned on the
sampled mode. We explore two sampling schemes that ensure differentiable
discrete latent structure while maintaining efficient gradient-based
optimization. By utilizing a latent categorical distribution to select the
behavior mode, our approach naturally expresses multimodality while remaining
fully differentiable via the sampling tricks. We evaluate our multimodal policy
on a set of DeepMind Control Suite environments, demonstrating that through
better exploration, our learned policies converge faster and outperform
standard Gaussian policies. Our results indicate that the Categorical
distribution serves as a powerful tool for structured exploration and
multimodal behavior representation in continuous control.

</details>


### [72] [How Usable is Automated Feature Engineering for Tabular Data?](https://arxiv.org/abs/2508.13932)
*Bastian Schäfer,Lennart Purucker,Maciej Janowski,Frank Hutter*

Main category: cs.LG

TL;DR: 对53种自动特征工程方法的可用性调查显示，现有方法存在使用困难、缺乏文档和社区支持等问题，需要更可用的自动化方案


<details>
  <summary>Details</summary>
Motivation: 手动特征工程耗时耗力，自动化特征工程(AutoFE)方法虽然很多，但彼此的可用性尚未经过详细评估，需要了解实际应用中的问题

Method: 调查了53种自动特征工程方法，分析其可用性、文档质量、社区活跃度等指标

Result: 现有AutoFE方法普遍存在使用困难、文档缺乏、社区支持不足等问题，且没有方法支持时间和内存约束设置

Conclusion: 自动特征工程领域需要更具可用性、更好工程化的方法，应考虑实际应用场景中的限制条件

Abstract: Tabular data, consisting of rows and columns, is omnipresent across various
machine learning applications. Each column represents a feature, and features
can be combined or transformed to create new, more informative features. Such
feature engineering is essential to achieve peak performance in machine
learning. Since manual feature engineering is expensive and time-consuming, a
substantial effort has been put into automating it. Yet, existing automated
feature engineering (AutoFE) methods have never been investigated regarding
their usability for practitioners. Thus, we investigated 53 AutoFE methods. We
found that these methods are, in general, hard to use, lack documentation, and
have no active communities. Furthermore, no method allows users to set time and
memory constraints, which we see as a necessity for usable automation. Our
survey highlights the need for future work on usable, well-engineered AutoFE
methods.

</details>


### [73] [Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem](https://arxiv.org/abs/2508.13963)
*Soumyajit Guin,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出了随机最短路径(SSP)问题的两种表格算法和一种函数逼近算法，展示了渐进几乎必然收敛性，并在性能上优于其他已知收敛RL算法


<details>
  <summary>Details</summary>
Motivation: 随机最短路径问题是强化学习中的重要问题类别，其他类型的成本标准RL问题都可以在SSP框架下表述，因此需要开发有效的SSP求解算法

Method: 提出了两种表格设置下的算法和一种函数逼近设置下的算法，用于解决随机最短路径问题

Result: 所有算法都实现了渐进几乎必然收敛，表格算法相比其他已知收敛RL算法表现出更优越的性能，函数逼近算法相比其他函数逼近设置算法展现出更可靠的性能

Conclusion: 所提出的SSP算法在收敛性和性能方面都表现出色，为强化学习中的随机最短路径问题提供了有效的解决方案

Abstract: In this paper we propose two algorithms in the tabular setting and an
algorithm for the function approximation setting for the Stochastic Shortest
Path (SSP) problem. SSP problems form an important class of problems in
Reinforcement Learning (RL), as other types of cost-criteria in RL can be
formulated in the setting of SSP. We show asymptotic almost-sure convergence
for all our algorithms. We observe superior performance of our tabular
algorithms compared to other well-known convergent RL algorithms. We further
observe reliable performance of our function approximation algorithm compared
to other algorithms in the function approximation setting.

</details>


### [74] [AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics](https://arxiv.org/abs/2508.13979)
*Yi Yang,Kei Ikemura,Qingwen Zhang,Xiaomeng Zhu,Ci Li,Nazre Batool,Sina Sharif Mansouri,John Folkesson*

Main category: cs.LG

TL;DR: AutoScale是一个基于多任务优化指标自动选择线性标量化权重的两阶段框架，无需昂贵的权重搜索即可实现优异性能


<details>
  <summary>Details</summary>
Motivation: 现有研究发现固定权重的线性标量化方法可以达到复杂多任务优化方法的性能，但最佳权重的选择原理和确定方法尚不明确

Method: 建立线性标量化与多任务优化方法的直接联系，通过大量实验发现性能良好的标量化权重在关键MTO指标上呈现特定趋势，基于此设计AutoScale两阶段框架

Result: AutoScale在包括新的大规模基准测试在内的多样化数据集上始终展现出优越的性能和高效率

Conclusion: 通过MTO指标指导权重选择的AutoScale框架为线性标量化提供了一种简单有效的权重确定方法，避免了昂贵的超参数搜索

Abstract: Recent multi-task learning studies suggest that linear scalarization, when
using well-chosen fixed task weights, can achieve comparable to or even better
performance than complex multi-task optimization (MTO) methods. It remains
unclear why certain weights yield optimal performance and how to determine
these weights without relying on exhaustive hyperparameter search. This paper
establishes a direct connection between linear scalarization and MTO methods,
revealing through extensive experiments that well-performing scalarization
weights exhibit specific trends in key MTO metrics, such as high gradient
magnitude similarity. Building on this insight, we introduce AutoScale, a
simple yet effective two-phase framework that uses these MTO metrics to guide
weight selection for linear scalarization, without expensive weight search.
AutoScale consistently shows superior performance with high efficiency across
diverse datasets including a new large-scale benchmark.

</details>


### [75] [Formal Algorithms for Model Efficiency](https://arxiv.org/abs/2508.14000)
*Naman Tyagi,Srishti Das,Kunal,Vatsal Gupta*

Main category: cs.LG

TL;DR: KMR框架是一个统一的深度学习模型效率技术表示和推理形式化方法，将剪枝、量化、知识蒸馏等技术抽象为可控旋钮、确定性规则和可测量仪表的三元组


<details>
  <summary>Details</summary>
Motivation: 为了解决深度学习模型效率优化方法多样化、缺乏统一表示框架的问题，使不同效率技术能够系统组合和灵活应用

Method: 提出KMR三元组框架（旋钮-仪表-规则），将各种效率技术抽象为统一表示，并开发Budgeted-KMR算法进行迭代预算优化

Result: 成功将多种知名效率方法实例化为KMR三元组，揭示了方法间的内在联系，支持混合流水线构建

Conclusion: KMR为模型效率研究提供了概念和实践工具，为自动化策略学习、动态适应和成本质量权衡的理论分析奠定了基础

Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for
representing and reasoning about model efficiency techniques in deep learning.
By abstracting diverse methods, including pruning, quantization, knowledge
distillation, and parameter-efficient architectures, into a consistent set of
controllable knobs, deterministic rules, and measurable meters, KMR provides a
mathematically precise and modular perspective on efficiency optimization. The
framework enables systematic composition of multiple techniques, flexible
policy-driven application, and iterative budgeted optimization through the
Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be
instantiated as KMR triples and present concise algorithmic templates for each.
The framework highlights underlying relationships between methods, facilitates
hybrid pipelines, and lays the foundation for future research in automated
policy learning, dynamic adaptation, and theoretical analysis of cost-quality
trade-offs. Overall, KMR offers both a conceptual and practical tool for
unifying and advancing model efficiency research.

</details>


### [76] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可差分的端到端量化方法，通过学习比特宽、噪声模量和限定边界来解决量化瓶颈，在极端W1A1设置下仍能保持竞争性的准确性。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络存在容量消失问题，随着比特宽的减小，四舍六入操作会导致每个层的信道容量下降，浮点数检查点设置了最大输入速率。需要找到一种方法来跟踪容量动态并解决量化瓶颈。

Method: 将精调视为平滑的约束优化问题，采用全可微分的直通估计器(STE)，具有可学习的比特宽、噪声模量和限定边界。通过外点罚法强制目标比特宽，使用软化的评估指标（通过知识蕴泽）来稳定训练。

Result: 该方法虽然简单，但在极端的W1A1（1比特激活/1比特权重）设置下仍能达到竞争性的准确性，同时保留了STE的高效率。

Conclusion: 这种基于容量动态分析的可差分量化方法能够有效解决量化瓶颈问题，在极端低比特宽情况下仍能维持良好的性能，为高效量化提供了一种简单但有效的解决方案。

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


### [77] [ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery](https://arxiv.org/abs/2508.14005)
*Mohammad Izadi,Mehran Safayani*

Main category: cs.LG

TL;DR: ASDFormer是一种基于Transformer的架构，通过混合专家池化分类器来捕捉自闭症相关的神经特征，在ABIDE数据集上实现了最先进的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍(ASD)与大脑连接性异常相关，需要有效捕捉功能社区内外的连接模式来改善诊断和生物标志物发现。

Method: 提出ASDFormer架构，整合多个专业专家分支和注意力机制，自适应地强调与自闭症相关的大脑区域和连接模式。

Result: 在ABIDE数据集上达到最先进的诊断准确率，并揭示了与ASD相关的功能连接中断的稳健见解。

Conclusion: ASDFormer具有作为生物标志物发现工具的潜力，既能提高分类性能，又能提供更可解释的疾病相关生物标志物识别。

Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition
marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a
non-invasive window into large-scale neural dynamics by measuring
blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can
be modeled as interactions among Regions of Interest (ROIs), which are grouped
into functional communities based on their underlying roles in brain function.
Emerging evidence suggests that connectivity patterns within and between these
communities are particularly sensitive to ASD-related alterations. Effectively
capturing these patterns and identifying interactions that deviate from typical
development is essential for improving ASD diagnosis and enabling biomarker
discovery. In this work, we introduce ASDFormer, a Transformer-based
architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to
capture neural signatures associated with ASD. By integrating multiple
specialized expert branches with attention mechanisms, ASDFormer adaptively
emphasizes different brain regions and connectivity patterns relevant to
autism. This enables both improved classification performance and more
interpretable identification of disorder-related biomarkers. Applied to the
ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and
reveals robust insights into functional connectivity disruptions linked to ASD,
highlighting its potential as a tool for biomarker discovery.

</details>


### [78] [Typed Topological Structures Of Datasets](https://arxiv.org/abs/2508.14008)
*Wanjun Hu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种类型化拓扑结构方法，通过为数据集赋予特定类型的开集来探索数据的内部结构，包括轨迹、分支和组件排序等


<details>
  <summary>Details</summary>
Motivation: 当前对数据集的研究主要集中在统计方法和代数拓扑方法，需要从一般拓扑学角度提出新方法来研究有限拓扑空间如数据集

Method: 开发了一套特殊类型集和相关的类型化拓扑结构，将R^2空间分解为轨迹，每个轨迹分解为有序的组件，并用整数序列表示，跨轨迹组件形成分支，用类型-II伪树表示关系

Result: 构建了能够揭示数据集内部结构的平台，包括轨迹组织、组件排序和分支关系

Conclusion: 该类型化拓扑方法为计算凸包、洞洞检测、聚类和异常检测等问题提供了新的算法平台

Abstract: A datatset $X$ on $R^2$ is a finite topological space. Current research of a
dataset focuses on statistical methods and the algebraic topological method
\cite{carlsson}. In \cite{hu}, the concept of typed topological space was
introduced and showed to have the potential for studying finite topological
spaces, such as a dataset. It is a new method from the general topology
perspective. A typed topological space is a topological space whose open sets
are assigned types. Topological concepts and methods can be redefined using
open sets of certain types. In this article, we develop a special set of types
and its related typed topology on a dataset $X$. Using it, we can investigate
the inner structure of $X$. In particular, $R^2$ has a natural quotient space,
in which $X$ is organized into tracks, and each track is split into components.
Those components are in a order. Further, they can be represented by an integer
sequence. Components crossing tracks form branches, and the relationship can be
well represented by a type of pseudotree (called typed-II pseudotree). Such
structures provide a platform for new algorithms for problems such as
calculating convex hull, holes, clustering and anomaly detection.

</details>


### [79] [Efficient Knowledge Graph Unlearning with Zeroth-order Information](https://arxiv.org/abs/2508.14013)
*Yang Xiao,Ruimeng Ye,Bohan Liu,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 知识图去学习算法，通过泰勒展开和零阶优化近似逆黑塞矩阵，提高了去学习效率和质量


<details>
  <summary>Details</summary>
Motivation: 满足"被忘记权"法规要求，需要从模型中移除训练数据及其影响，但全量重新训练成本高昂

Method: 定义KG去学习影响函数，使用泰勒展开估算参数变化，用Fisher矩阵和零阶优化近似逆黑塞矩阵向量积，避免计算一阶和二阶导数

Result: 在大规模知识图上，方法在去学习效率和质量方面显著超过其他最先进的图去学习基准方法

Conclusion: 提出的知识图去学习算法能够高效地移除训练数据影响，为满足数据隐私法规要求提供了可行解决方案

Abstract: Due to regulations like the Right to be Forgotten, there is growing demand
for removing training data and its influence from models. Since full retraining
is costly, various machine unlearning methods have been proposed. In this
paper, we firstly present an efficient knowledge graph (KG) unlearning
algorithm. We remark that KG unlearning is nontrivial due to the distinctive
structure of KG and the semantic relations between entities. Also, unlearning
by estimating the influence of removed components incurs significant
computational overhead when applied to large-scale knowledge graphs. To this
end, we define an influence function for KG unlearning and propose to
approximate the model's sensitivity without expensive computation of
first-order and second-order derivatives for parameter updates. Specifically,
we use Taylor expansion to estimate the parameter changes caused by data
removal. Given that the first-order gradients and second-order derivatives
dominate the computational load, we use the Fisher matrices and zeroth-order
optimization to approximate the inverse-Hessian vector product without
constructing the computational graphs. Our experimental results demonstrate
that the proposed method outperforms other state-of-the-art graph unlearning
baselines significantly in terms of unlearning efficiency and unlearning
quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.

</details>


### [80] [BLIPs: Bayesian Learned Interatomic Potentials](https://arxiv.org/abs/2508.14022)
*Dario Coscia,Pim de Haan,Max Welling*

Main category: cs.LG

TL;DR: BLIPs是一种贝叶斯学习原子间势能模型，通过变分dropout框架为机器学习原子间势能提供校准的不确定性估计，在数据稀缺和分布外场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 传统机器学习原子间势能模型在分布外数据和数据稀缺场景下预测不准确，且缺乏不确定性估计，无法指导主动学习和确保模拟精度

Method: 提出BLIPs框架，基于自适应变分dropout的可扩展架构无关贝叶斯方法，可训练或微调MLIPs，提供能量和力的不确定性估计

Result: 在计算化学任务中显示出比标准MLIPs更好的预测准确性，特别是在数据稀缺和分布外场景下提供可信的不确定性估计

Conclusion: BLIPs框架能够为原子间势能预测提供校准的不确定性，在数据稀缺和分布外场景下表现优异，微调预训练模型也能获得一致性能提升

Abstract: Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool
in simulation-based chemistry. However, like most deep learning models, MLIPs
struggle to make accurate predictions on out-of-distribution data or when
trained in a data-scarce regime, both common scenarios in simulation-based
chemistry. Moreover, MLIPs do not provide uncertainty estimates by
construction, which are fundamental to guide active learning pipelines and to
ensure the accuracy of simulation results compared to quantum calculations. To
address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic
Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian
framework for training or fine-tuning MLIPs, built on an adaptive version of
Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and
minimal computational overhead for energy and forces prediction at inference
time, while integrating seamlessly with (equivariant) message-passing
architectures. Empirical results on simulation-based computational chemistry
tasks demonstrate improved predictive accuracy with respect to standard MLIPs,
and trustworthy uncertainty estimates, especially in data-scarse or heavy
out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP
yields consistent performance gains and calibrated uncertainties.

</details>


### [81] [Learning from Preferences and Mixed Demonstrations in General Settings](https://arxiv.org/abs/2508.14027)
*Jason R Brown,Carl Henrik Ek,Robert D Mullins*

Main category: cs.LG

TL;DR: LEOPARD算法通过结合偏好反馈和专家演示来学习奖励函数，在数据有限时显著优于现有基线方法，并证明组合多种反馈类型通常有益。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂任务中难以指定好的奖励函数，现有方法同时利用偏好和演示数据的方式往往是临时的、依赖领域特性或难以扩展。

Method: 提出了reward-rational partial orderings over observations框架，并开发了LEOPARD算法，能够从包括负面演示在内的广泛数据中学习奖励函数。

Result: 当偏好和演示反馈数据有限时，LEOPARD显著优于现有基线方法，且组合多种反馈类型通常比单一反馈类型效果更好。

Conclusion: LEOPARD提供了一个灵活且可扩展的框架，能够有效利用多种人类反馈数据来学习奖励函数，在复杂任务中表现出色。

Abstract: Reinforcement learning is a general method for learning in sequential
settings, but it can often be difficult to specify a good reward function when
the task is complex. In these cases, preference feedback or expert
demonstrations can be used instead. However, existing approaches utilising both
together are often ad-hoc, rely on domain-specific properties, or won't scale.
We develop a new framing for learning from human data, \emph{reward-rational
partial orderings over observations}, designed to be flexible and scalable.
Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated
Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a
broad range of data, including negative demonstrations, to efficiently learn
reward functions across a wide range of domains. We find that when a limited
amount of preference and demonstration feedback is available, LEOPARD
outperforms existing baselines by a significant margin. Furthermore, we use
LEOPARD to investigate learning from many types of feedback compared to just a
single one, and find that combining feedback types is often beneficial.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [82] [Preference Models assume Proportional Hazards of Utilities](https://arxiv.org/abs/2508.13189)
*Chirag Nagpal*

Main category: stat.ML

TL;DR: 本文连接了Plackett-Luce模型与Cox比例风险模型，探讨了这种联系对AI对齐工具（如奖励建模和直接偏好优化）的统计假设的影响。


<details>
  <summary>Details</summary>
Motivation: 现代AI对齐工具基于Plackett-Luce模型的统计假设，但该模型与经典的Cox比例风险模型之间存在联系，需要深入理解这种联系的意义。

Method: 通过理论分析将Plackett-Luce模型与Cox比例风险模型进行连接，探讨两者之间的统计关系。

Result: 建立了Plackett-Luce模型与Cox比例风险模型之间的理论联系，揭示了这种联系对偏好估计统计假设的启示。

Conclusion: Plackett-Luce模型与Cox比例风险模型的连接为理解AI对齐工具的统计基础提供了新的视角，有助于改进偏好估计方法。

Abstract: Approaches for estimating preferences from human annotated data typically
involves inducing a distribution over a ranked list of choices such as the
Plackett-Luce model. Indeed, modern AI alignment tools such as Reward Modelling
and Direct Preference Optimization are based on the statistical assumptions
posed by the Plackett-Luce model. In this paper, I will connect the
Plackett-Luce model to another classical and well known statistical model, the
Cox Proportional Hazards model and attempt to shed some light on the
implications of the connection therein.

</details>


### [83] [Structural Foundations for Leading Digit Laws: Beyond Probabilistic Mixtures](https://arxiv.org/abs/2508.13237)
*Vladimir Berman*

Main category: stat.ML

TL;DR: 这篇论文提出了一种现代确定性框架来研究数值数据中首位数字的分布规律，不依赖传统的概率或混合模型，而是通过数据生成过程的算术、算法和结构性质来解释观察到的首位数字频率分布。


<details>
  <summary>Details</summary>
Motivation: 传统的概率性或混合模型方法在解释数字分布时存在局限性，无法充分解释实际数据中观察到的多样化数字分布模式。需要建立一种更为基础的确定性数学框架来统一理解数字现象。

Method: 采用一种平移不变的函数方程作为核心工具，其通解由明确的阿尔法加周期公式给出。通过系统分析有限和无限数据集中的数字分布，研究确定性序列如素数和递推关系，识别块结构和分形特征的出现。

Result: 该方法能够解释实验和数学数据集中遇到的多样化数字分布，包括与对数或标度不变模式存在显著偏移的情况。建立了一个统一的数学基础来理解数字现象。

Conclusion: 该研究为数字现象建立了一个统一的数学基础，提供了一个多用途的工具集用于建模和分析应用及理论上下文中的数字模式。对概率模型进行了批判性考察，提供了明确的例子和反例，并讨论了限制和待解决的问题。

Abstract: This article presents a modern deterministic framework for the study of
leading significant digit distributions in numerical data. Rather than relying
on traditional probabilistic or mixture-based explanations, we demonstrate that
the observed frequencies of leading digits are determined by the underlying
arithmetic, algorithmic, and structural properties of the data-generating
process. Our approach centers on a shift-invariant functional equation, whose
general solution is given by explicit affine-plus-periodic formulas. This
structural formulation explains the diversity of digit distributions
encountered in both empirical and mathematical datasets, including cases with
pronounced deviations from logarithmic or scale-invariant profiles.
  We systematically analyze digit distributions in finite and infinite
datasets, address deterministic sequences such as prime numbers and recurrence
relations, and highlight the emergence of block-structured and fractal
features. The article provides critical examination of probabilistic models,
explicit examples and counterexamples, and discusses limitations and open
problems for further research. Overall, this work establishes a unified
mathematical foundation for digital phenomena and offers a versatile toolset
for modeling and analyzing digit patterns in applied and theoretical contexts.

</details>


### [84] [Flow Matching-Based Generative Modeling for Efficient and Scalable Data Assimilation](https://arxiv.org/abs/2508.13313)
*Taos Transue,Bohan Chen,So Takao,Bao Wang*

Main category: stat.ML

TL;DR: 基于流匹配的组合流滤波器(EnFF)，通过加速采样和灵活概率路径设计，提供了比现有生成式数据同化方法更好的成本-准确性交换效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成式模型方法(如组合分数滤波器EnSF)在高维非线性数据同化中的缓慢采样问题，提高计算效率。

Method: 提出基于流匹配(FM)的组合流滤波器(EnFF)，使用蒙特卡洛估计器计算边际FM向量场，结合局部导航来同化观测数据，无需训练。

Result: 在高维滤波基准测试中，EnFF展现了更优的成本-准确性交换效果，能够利用更大规模的组合。

Conclusion: 流匹配作为一种可扩展工具，在高维应用中具有很大潜力，能够支持大规模组合的使用。

Abstract: Data assimilation (DA) is the problem of sequentially estimating the state of
a dynamical system from noisy observations. Recent advances in generative
modeling have inspired new approaches to DA in high-dimensional nonlinear
settings, especially the ensemble score filter (EnSF). However, these come at a
significant computational burden due to slow sampling. In this paper, we
introduce a new filtering framework based on flow matching (FM) -- called the
ensemble flow filter (EnFF) -- to accelerate sampling and enable flexible
design of probability paths. EnFF -- a training-free DA approach -- integrates
MC estimators for the marginal FM vector field (VF) and a localized guidance to
assimilate observations. EnFF has faster sampling and more flexibility in VF
design compared to existing generative modeling for DA. Theoretically, we show
that EnFF encompasses classical filtering methods such as the bootstrap
particle filter and the ensemble Kalman filter as special cases. Experiments on
high-dimensional filtering benchmarks demonstrate improved cost-accuracy
tradeoffs and the ability to leverage larger ensembles than prior methods. Our
results highlight the promise of FM as a scalable tool for filtering in
high-dimensional applications that enable the use of large ensembles.

</details>


### [85] [Smooth Flow Matching](https://arxiv.org/abs/2508.13831)
*Jianbin Tan,Anru R. Zhang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Functional data, i.e., smooth random functions observed over a continuous
domain, are increasingly available in areas such as biomedical research, health
informatics, and epidemiology. However, effective statistical analysis for
functional data is often hindered by challenges such as privacy constraints,
sparse and irregular sampling, infinite dimensionality, and non-Gaussian
structures. To address these challenges, we introduce a novel framework named
Smooth Flow Matching (SFM), tailored for generative modeling of functional data
to enable statistical analysis without exposing sensitive real data. Built upon
flow-matching ideas, SFM constructs a semiparametric copula flow to generate
infinite-dimensional functional data, free from Gaussianity or low-rank
assumptions. It is computationally efficient, handles irregular observations,
and guarantees the smoothness of the generated functions, offering a practical
and flexible solution in scenarios where existing deep generative methods are
not applicable. Through extensive simulation studies, we demonstrate the
advantages of SFM in terms of both synthetic data quality and computational
efficiency. We then apply SFM to generate clinical trajectory data from the
MIMIC-IV patient electronic health records (EHR) longitudinal database. Our
analysis showcases the ability of SFM to produce high-quality surrogate data
for downstream statistical tasks, highlighting its potential to boost the
utility of EHR data for clinical applications.

</details>


### [86] [Online Conformal Selection with Accept-to-Reject Changes](https://arxiv.org/abs/2508.13838)
*Kangdao Liu,Huajun Xi,Chi-Man Vong,Hongxin Wei*

Main category: stat.ML

TL;DR: 提出了OCS-ARC方法，将保形选择扩展到在线场景，支持不可逆选择决策，在控制FDR的同时提高选择能力


<details>
  <summary>Details</summary>
Motivation: 传统保形选择在在线场景中允许取消先前选择，这与需要不可逆选择决策的应用（如药物发现）不兼容

Method: 提出在线保形选择与接受-拒绝变化（OCS-ARC）方法，将在线Benjamini-Hochberg程序整合到候选选择过程中

Result: 理论保证在任何时间步控制FDR在名义水平以下，实验显示在合成和真实数据集上显著提高选择能力

Conclusion: OCS-ARC成功解决了在线保形选择的不可逆选择问题，为资源密集型序列过程提供了有效的解决方案

Abstract: Selecting a subset of promising candidates from a large pool is crucial
across various scientific and real-world applications. Conformal selection
offers a distribution-free and model-agnostic framework for candidate selection
with uncertainty quantification. While effective in offline settings, its
application to online scenarios, where data arrives sequentially, poses
challenges. Notably, conformal selection permits the deselection of previously
selected candidates, which is incompatible with applications requiring
irreversible selection decisions. This limitation is particularly evident in
resource-intensive sequential processes, such as drug discovery, where
advancing a compound to subsequent stages renders reversal impractical. To
address this issue, we extend conformal selection to an online Accept-to-Reject
Changes (ARC) procedure: non-selected data points can be reconsidered for
selection later, and once a candidate is selected, the decision is
irreversible. Specifically, we propose a novel conformal selection method,
Online Conformal Selection with Accept-to-Reject Changes (dubbed OCS-ARC),
which incorporates online Benjamini-Hochberg procedure into the candidate
selection process. We provide theoretical guarantees that OCS-ARC controls the
false discovery rate (FDR) at or below the nominal level at any timestep under
both i.i.d. and exchangeable data assumptions. Additionally, we theoretically
show that our approach naturally extends to multivariate response settings.
Extensive experiments on synthetic and real-world datasets demonstrate that
OCS-ARC significantly improves selection power over the baseline while
maintaining valid FDR control across all examined timesteps.

</details>


### [87] [Generalisation and benign over-fitting for linear regression onto random functional covariates](https://arxiv.org/abs/2508.13895)
*Andrew Jones,Nick Whiteley*

Main category: stat.ML

TL;DR: 研究岭回归和无岭最小二乘回归在函数型协变量设置下的理论预测性能，分析协变量噪声在良性过拟合中的作用


<details>
  <summary>Details</summary>
Motivation: 传统回归分析通常假设数据独立同分布，但实际中协变量向量可能来自潜在度量空间上随机函数的评估结果，具有可交换性而非独立性，需要研究这种非标准设置下的预测性能

Method: 在函数型协变量设置下，假设维度间独立、四阶矩存在等正则条件，利用Barzilai和Shamir的最新结果，推导预测超额风险的 probabilistic bounds

Result: 获得了当p相对于n以适当速度增长时的收敛速率，展示了模型各成分在决定收敛行为中的相互作用，以及加性协变量噪声在良性过拟合中的作用

Conclusion: 该研究扩展了回归分析的理论框架，为函数型协变量设置下的预测性能提供了理论保证，揭示了协变量噪声对过拟合行为的调节作用

Abstract: We study theoretical predictive performance of ridge and ridge-less
least-squares regression when covariate vectors arise from evaluating $p$
random, means-square continuous functions over a latent metric space at $n$
random and unobserved locations, subject to additive noise. This leads us away
from the standard assumption of i.i.d. data to a setting in which the $n$
covariate vectors are exchangeable but not independent in general. Under an
assumption of independence across dimensions, $4$-th order moment, and other
regularity conditions, we obtain probabilistic bounds on a notion of predictive
excess risk adapted to our random functional covariate setting, making use of
recent results of Barzilai and Shamir. We derive convergence rates in regimes
where $p$ grows suitably fast relative to $n$, illustrating interplay between
ingredients of the model in determining convergence behaviour and the role of
additive covariate noise in benign-overfitting.

</details>


### [88] [A PC Algorithm for Max-Linear Bayesian Networks](https://arxiv.org/abs/2508.13967)
*Carlos Améndola,Benjamin Hollering,Francesco Nowell*

Main category: stat.ML

TL;DR: PC算法在最大线性贝叶斯网络中保持一致性，并提出新算法PCstar能够识别更多边方向


<details>
  <summary>Details</summary>
Motivation: 最大线性贝叶斯网络(MLBNs)由于重尾分布特性，通常不满足d-分离的忠实性，传统因果发现算法无法准确恢复真实图结构

Method: 研究基于约束的发现算法，使用*-分离准则作为条件独立性测试oracle，证明PC算法的一致性，并提出新的PCstar算法

Result: PC算法在*-分离准则下保持一致性，PCstar算法能够利用C*-分离忠实性来定向更多边

Conclusion: *-分离准则可以替代d-分离用于MLBNs的因果发现，PCstar算法相比传统方法能够识别更多边的方向

Abstract: Max-linear Bayesian networks (MLBNs) are a relatively recent class of
structural equation models which arise when the random variables involved have
heavy-tailed distributions. Unlike most directed graphical models, MLBNs are
typically not faithful to d-separation and thus classical causal discovery
algorithms such as the PC algorithm or greedy equivalence search can not be
used to accurately recover the true graph structure. In this paper, we begin
the study of constraint-based discovery algorithms for MLBNs given an oracle
for testing conditional independence in the true, unknown graph. We show that
if the oracle is given by the $\ast$-separation criteria in the true graph,
then the PC algorithm remains consistent despite the presence of additional CI
statements implied by $\ast$-separation. We also introduce a new causal
discovery algorithm named "PCstar" which assumes faithfulness to
$C^\ast$-separation and is able to orient additional edges which cannot be
oriented with only d- or $\ast$-separation.

</details>


### [89] [Uncertainty-Aware PCA for Arbitrarily Distributed Data Modeled by Gaussian Mixture Models](https://arxiv.org/abs/2508.13990)
*Daniel Klötzl,Ozan Tastekin,David Hägele,Marina Evers,Daniel Weiskopf*

Main category: stat.ML

TL;DR: 提出基于高斯混合模型的多维不确定性数据降维方法，相比UAPCA能更准确地表示分布细节，支持用户自定义权重


<details>
  <summary>Details</summary>
Motivation: 多维数据常伴随非正态分布的不确定性，现有UAPCA方法对此类分布处理不足，需要更有效的降维表示方法

Method: 使用高斯混合模型建模多维分布，推导任意概率密度函数的投影公式，支持用户自定义分布权重

Result: 低维投影结果比UAPCA展现更多分布细节，更忠实表示原始分布，与基于样本的投影结果对比验证有效性

Conclusion: 该方法能更好地处理非正态多维不确定性数据的降维问题，提供更准确的分布可视化表示

Abstract: Multidimensional data is often associated with uncertainties that are not
well-described by normal distributions. In this work, we describe how such
distributions can be projected to a low-dimensional space using
uncertainty-aware principal component analysis (UAPCA). We propose to model
multidimensional distributions using Gaussian mixture models (GMMs) and derive
the projection from a general formulation that allows projecting arbitrary
probability density functions. The low-dimensional projections of the densities
exhibit more details about the distributions and represent them more faithfully
compared to UAPCA mappings. Further, we support including user-defined weights
between the different distributions, which allows for varying the importance of
the multidimensional distributions. We evaluate our approach by comparing the
distributions in low-dimensional space obtained by our method and UAPCA to
those obtained by sample-based projections.

</details>
