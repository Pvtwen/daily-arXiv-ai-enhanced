<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [cs.LG](#cs.LG) [Total: 51]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Communication-Efficient Collaborative LLM Inference via Distributed Speculative Decoding](https://arxiv.org/abs/2509.04576)
*Ce Zheng,Tingting Yang*

Main category: eess.SP

TL;DR: 提出TK-SLT方案，通过仅传输top-K token概率和索引来减少分布式推测解码中的上行通信开销，同时保持推理性能


<details>
  <summary>Details</summary>
Motivation: 在AI-RAN中，现有分布式推测解码需要传输完整词汇表概率分布，导致上行通信开销过大

Method: Top-K稀疏对数传输方案，只传输top-K token的原始概率和对应索引，并推导最优草稿长度以最大化推理吞吐量

Result: 实验验证了方法的效率和有效性，显著减少带宽消耗

Conclusion: TK-SLT方案在保持推理性能的同时显著降低了通信开销，适用于资源受限的AI-RAN环境

Abstract: Speculative decoding is an emerging technique that accelerates large language
model (LLM) inference by allowing a smaller draft model to predict multiple
tokens in advance, which are then verified or corrected by a larger target
model. In AI-native radio access networks (AI-RAN), this paradigm is
well-suited for collaborative inference between resource-constrained end
devices and more capable edge servers or base stations (BSs). However, existing
distributed speculative decoding requires transmitting the full vocabulary
probability distribution from the draft model on the device to the target model
at the BS, which leads to prohibitive uplink communication overhead. To address
this issue, we propose a ``Top-K Sparse Logits Transmission (TK-SLT)`` scheme,
where the draft model transmits only the top-K token raw probabilities and the
corresponding token indices instead of the entire distribution. This approach
significantly reduces bandwidth consumption while maintaining inference
performance. We further derive an analytical expression for the optimal draft
length that maximizes inference throughput, and provide a theoretical analysis
of the achievable speedup ratio under TK-SLT. Experimental results validate
both the efficiency and effectiveness of the proposed method.

</details>


### [2] [Tangential Velocity Estimation Using Near-Field Automotive Radar Model](https://arxiv.org/abs/2509.04692)
*Michael Shifrin,Joseph Tabrikian,Igal Bilik*

Main category: eess.SP

TL;DR: 该论文提出了一种基于近场雷达模型的切向速度估计算法，解决了传统车载雷达无法估计目标切向速度的问题，通过利用目标迁移效应来提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统车载雷达基于远场模型，只能估计目标的距离、径向速度和到达方向，但无法估计切向速度分量，而切向速度对于动态环境的可靠感知至关重要。

Method: 引入近场雷达模型，考虑距离、径向速度和多普勒的迁移效应；进行可识别性分析；提出分离阵列配置和基于最大似然的高效算法，利用目标迁移进行切向速度估计。

Result: 仿真验证了理论可行性，在单目标和多目标场景中都表现出良好的性能，提高了切向速度估计的准确性和可靠性。

Conclusion: 所提出的方法能够有效估计切向速度，同时减轻距离、径向速度和多普勒的似然函数模糊，提升了车载雷达的性能，增强了高级驾驶辅助系统和自动驾驶车辆的态势感知能力。

Abstract: This work investigates the problem of tangential velocity estimation in
automotive radar systems, addressing the limitations of conventionally
considered models. Conventional automotive radars are usually based on
far-field models and estimate the target's range, radial velocity, and
direction-of-arrival (DOA) but are not able to estimate the tangential
component of the target 2-D velocity, which is a critical parameter for
reliable perception of dynamic environments. To address this challenge, we
introduce the near-field radar model, which considers various migration
elements in range, radial velocity, and Doppler along time and space.
Conventionally, these migration effects result in smearing of the likelihood
function for estimating the target parameters. However, if the model is
correctly specified, these migration effects are informative for tangential
velocity estimation. We conduct an identifiability analysis for tangential
velocity estimation using the Cram\'er-Rao bound and ambiguity function. The
insights from this study motivate the use of a separated array configuration
and the development of a computationally efficient maximum likelihood based
algorithm designed to utilize target migrations for tangential velocity
estimation, while maintaining practical computational complexity. In addition
to tangential velocity estimation, the proposed algorithm mitigates likelihood
smearing in range, radial velocity, and Doppler. Simulations validate the
theoretical feasibility study, and evaluate the algorithms' performance in both
single- and multi-target scenarios. The proposed approach improves the accuracy
and reliability of automotive radars, enhancing situational awareness for
advanced driver assistance systems and autonomous vehicles.

</details>


### [3] [Environment-Aware IRS Deployment via Channel Knowledge Map: Joint Sensing-Communications Coverage Optimization](https://arxiv.org/abs/2509.04768)
*Yilong Chen,Zixiang Ren,Jie Xu,Rui Zhang*

Main category: eess.SP

TL;DR: 本文研究IRS辅助ISAC系统的部署优化问题，利用CKM信息优化IRS位置选择、基站波束成形和IRS反射波束成形，以最小化系统成本同时满足感知和通信需求


<details>
  <summary>Details</summary>
Motivation: 智能反射面(IRS)在集成感知与通信(ISAC)系统中能显著提升覆盖性能，但需要优化部署位置来最大化系统效益并降低成本

Method: 基于信道知识图(CKM)获取CSI信息，将问题建模为混合整数非凸优化问题，采用SCA松弛-定界方法求解，包括连续化松弛、SCA收敛求解和二进制化回退

Result: 数值结果表明所提算法能有效降低系统成本，同时满足感知和通信的最低要求

Conclusion: 提出的环境感知IRS部署方案通过利用CKM信息，能够实现ISAC系统的高效部署和波束成形优化，为实际系统部署提供有效解决方案

Abstract: This paper studies the intelligent reflecting surface (IRS) deployment
optimization problem for IRS-enabled integrated sensing and communications
(ISAC) systems, in which multiple IRSs are strategically deployed at candidate
locations to assist a base station (BS) to enhance the coverage of both sensing
and communications. We present an environment-aware IRS deployment design via
exploiting the channel knowledge map (CKM), which provides the channel state
information (CSI) between each candidate IRS location and BS or targeted
sensing/communication points. Based on the obtained CSI from CKM, we optimize
the deployment of IRSs, jointly with the BS's transmit beamforming and IRSs'
reflective beamforming during operation, with the objective of minimizing the
system cost, while guaranteeing the minimum illumination power requirements at
sensing areas and the minimum signal-to-noise ratio (SNR) requirements at
communication areas. In particular, we consider two cases when the IRSs'
reflective beamforming optimization can be implemented dynamically in real time
and quasi-stationarily over the whole operation period, respectively. For both
cases, the joint IRS deployment and transmit/reflective beamforming designs are
formulated as mixed-integer non-convex optimization problems, which are solved
via the successive convex approximation (SCA)-based relax-and-bound method.
Specifically, we first relax the binary IRS deployment indicators into
continuous variables, then find converged solutions via SCA, and finally round
relaxed indicators back to binary values. Numerical results demonstrate the
effectiveness of our proposed algorithms in reducing the system cost while
meeting the sensing and communication requirements.

</details>


### [4] [SREC: Encrypted Semantic Super-Resolution Enhanced Communication](https://arxiv.org/abs/2509.04787)
*Zhidi Zhang,Rui Meng,Song Gao,Haixiao Gao,Xiaodong Xu*

Main category: eess.SP

TL;DR: 提出加密语义超分辨率增强通信(SREC)方法，通过模256加密和超分辨率重建技术保护语义通信安全并提升图像重建质量


<details>
  <summary>Details</summary>
Motivation: 语义通信在提高通信效率的同时面临安全风险，明文传输的语义特征容易被窃听者截获，需要解决语义通信的安全性问题

Method: 使用模256加密方法加密语义特征，并采用超分辨率重建方法来提高图像重建质量

Result: 在加性高斯白噪声信道中，使用不同调制方法时，SREC不仅能稳定保证安全性，还能在低信噪比条件下实现更好的传输性能

Conclusion: SREC方法有效解决了语义通信的安全性问题，同时保持了良好的传输性能，特别是在低信噪比环境下表现优异

Abstract: Semantic communication (SemCom), as a typical paradigm of deep integration
between artificial intelligence (AI) and communication technology,
significantly improves communication efficiency and resource utilization
efficiency. However, the security issues of SemCom are becoming increasingly
prominent. Semantic features transmitted in plaintext over physical channels
are easily intercepted by eavesdroppers. To address this issue, this paper
proposes Encrypted Semantic Super-Resolution Enhanced Communication (SREC) to
secure SemCom. SREC uses the modulo-256 encryption method to encrypt semantic
features, and employs super-resolution reconstruction method to improve the
reconstruction quality of images. The simulation results show that in the
additive Gaussian white noise (AWGN) channel, when different modulation methods
are used, SREC can not only stably guarantee security, but also achieve better
transmission performance under low signal-to-noise ratio (SNR) conditions.

</details>


### [5] [KGRAG-SC: Knowledge Graph RAG-Assisted Semantic Communication](https://arxiv.org/abs/2509.04801)
*Dayu Fan,Rui Meng,Song Gao,Xiaodong Xu*

Main category: eess.SP

TL;DR: KGRAG-SC是一个基于知识图谱的语义通信框架，通过检索增强生成技术解决传统语义通信缺乏可解释性和噪声鲁棒性的问题，在低信噪比条件下实现更高的语义保真度和更低的传输开销。


<details>
  <summary>Details</summary>
Motivation: 现有的语义通信方案主要依赖端到端深度学习框架，缺乏可解释性，且在噪声条件下语义选择和重建的鲁棒性不足。

Method: 提出KGRAG-SC框架，利用多维度知识图谱进行语义提取，通过社区引导的实体链接和GraphRAG辅助处理。发射端构建最小连通子图传输紧凑实体索引，采用基于结构中心性度量的重要性感知自适应传输策略。接收端使用大语言模型基于共享知识图谱进行知识驱动的文本重建。

Result: 实验结果表明，KGRAG-SC在低信噪比条件下实现了优越的语义保真度，同时相比传统通信方法显著降低了传输开销。

Conclusion: 将结构化知识表示与生成式语言模型集成到语义通信系统中具有显著效果，KGRAG-SC框架为解决语义通信的可解释性和鲁棒性问题提供了有效方案。

Abstract: The state-of-the-art semantic communication (SC) schemes typically rely on
end-to-end deep learning frameworks that lack interpretability and struggle
with robust semantic selection and reconstruction under noisy conditions. To
address this issue, this paper presents KGRAG-SC, a knowledge graph-assisted SC
framework that leverages retrieval-augmented generation principles. KGRAG-SC
employs a multi-dimensional knowledge graph, enabling efficient semantic
extraction through community-guided entity linking and GraphRAG-assisted
processing. The transmitter constructs minimal connected subgraphs that capture
essential semantic relationships and transmits only compact entity indices
rather than full text or semantic triples. An importance-aware adaptive
transmission strategy provides unequal error protection based on structural
centrality metrics, prioritizing critical semantic elements under adverse
channel conditions. At the receiver, large language models perform
knowledge-driven text reconstruction using the shared knowledge graph as
structured context, ensuring robust semantic recovery even with partial
information loss. Experimental results demonstrate that KGRAG-SC achieves
superior semantic fidelity in low Signal-to-Noise Ratio (SNR) conditions while
significantly reducing transmission overhead compared to traditional
communication methods, highlighting the effectiveness of integrating structured
knowledge representation with generative language models for SC systems.

</details>


### [6] [SemSteDiff: Generative Diffusion Model-based Coverless Semantic Steganography Communication](https://arxiv.org/abs/2509.04803)
*Song Gao,Rui Meng,Xiaodong Xu,Haixiao Gao,Yiming Liu,Chenyuan Feng,Ping Zhang,Tony Q. S. Quek,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出基于生成扩散模型的语义隐写通信方案SemSteDiff，通过语义相关的公私钥对将秘密图像隐藏到生成的隐写图像中，解决现有方案依赖预选封面图像的局限性。


<details>
  <summary>Details</summary>
Motivation: 语义通信虽然提高了通信效率，但仍面临窃听威胁。现有语义隐写通信方案依赖预选封面图像，限制了通用性。

Method: 使用生成扩散模型将秘密图像隐藏到生成的隐写图像中，通过语义相关的公私钥对确保合法接收者能正确解码，而窃听者无法获取秘密信息。

Result: 在不同JSCC框架下验证了即插即用设计的有效性。在SNR=0dB时，合法接收者的PSNR比窃听者高4.14dB。

Conclusion: SemSteDiff方案有效解决了语义隐写通信的通用性问题，能够有效对抗语义窃听威胁。

Abstract: Semantic communication (SemCom), as a novel paradigm for future communication
systems, has recently attracted much attention due to its superiority in
communication efficiency. However, similar to traditional communication, it
also suffers from eavesdropping threats. Intelligent eavesdroppers could launch
advanced semantic analysis techniques to infer secret semantic information.
Therefore, some researchers have designed Semantic Steganography Communication
(SemSteCom) scheme to confuse semantic eavesdroppers. However, the
state-of-the-art SemSteCom schemes for image transmission rely on the
pre-selected cover image, which limits the universality. To address this issue,
we propose a Generative Diffusion Model-based Coverless Semantic Steganography
Communication (SemSteDiff) scheme to hide secret images into generated stego
images. The semantic related private and public keys enable legitimate receiver
to decode secret images correctly while the eavesdropper without completely
true key-pairs fail to obtain them. Simulation results demonstrate the
effectiveness of the plug-and-play design in different Joint Source-Channel
Coding (JSCC) frameworks. The comparison results under different eavesdroppers'
threats show that, when Signal-to-Noise Ratio (SNR) = 0 dB, the peak
signal-to-noise ratio (PSNR) of the legitimate receiver is 4.14 dB higher than
that of the eavesdropper.

</details>


### [7] [AI-Driven Fronthaul Link Compression in Wireless Communication Systems: Review and Method Design](https://arxiv.org/abs/2509.04805)
*Keqin Zhang*

Main category: eess.SP

TL;DR: 本文综述了AI驱动的无线前传压缩技术，重点分析了CSI反馈和RB粒度预编码压缩两种高压缩率方法，并提出了针对cell-free架构的前传压缩策略。


<details>
  <summary>Details</summary>
Motivation: 现代无线系统前传链路需要在高带宽和低延迟约束下传输高维信号，传统压缩方法在高压缩比下性能急剧下降且难以跨信道调优，需要AI驱动的智能压缩方案。

Method: 采用端到端学习变换、矢量和分层量化、学习熵模型等技术，重点研究CSI反馈端到端学习和RB粒度预编码优化结合压缩两种高压缩率路径。

Result: 提出了针对cell-free架构的前传压缩策略，实现了高压缩比下的可控性能损失，支持RB级速率自适应，并满足下一代网络集中式协作传输的低延迟推理需求。

Conclusion: AI驱动的压缩技术能够有效利用信道状态信息、预编码矩阵等信号结构，为下一代无线网络前传链路提供了高性能的压缩解决方案。

Abstract: Modern fronthaul links in wireless systems must transport high-dimensional
signals under stringent bandwidth and latency constraints, which makes
compression indispensable. Traditional strategies such as compressed sensing,
scalar quantization, and fixed-codec pipelines often rely on restrictive
priors, degrade sharply at high compression ratios, and are hard to tune across
channels and deployments. Recent progress in Artificial Intelligence (AI) has
brought end-to-end learned transforms, vector and hierarchical quantization,
and learned entropy models that better exploit the structure of Channel State
Information(CSI), precoding matrices, I/Q samples, and LLRs. This paper first
surveys AI-driven compression techniques and then provides a focused analysis
of two representative high-compression routes: CSI feedback with end-to-end
learning and Resource Block (RB) granularity precoding optimization combined
with compression. Building on these insights, we propose a fronthaul
compression strategy tailored to cell-free architectures. The design targets
high compression with controlled performance loss, supports RB-level rate
adaptation, and enables low-latency inference suitable for centralized
cooperative transmission in next-generation networks.

</details>


### [8] [Plug-and-Play Latent Diffusion for Electromagnetic Inverse Scattering with Application to Brain Imaging](https://arxiv.org/abs/2509.04860)
*Rui Guo,Yi Zhang,Yhonatan Kvich,Tianyao Huang,Maokun Li,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 提出基于潜在扩散的后验采样方法用于电磁脑成像，通过结合物理前向模型和学习到的先验分布，实现高精度、高保真度的定量重建。


<details>
  <summary>Details</summary>
Motivation: 现有电磁成像方法在平衡可解释性、失真误差和可靠性方面存在困难，无法有效结合复杂先验分布或提供理论保证。

Method: 使用潜在扩散模型学习目标先验分布，然后通过交替采样器执行后验采样，分别强制执行似然和先验分布，最后基于样本进行最小均方误差估计。

Result: 在脑成像实验中实现了最先进的重建精度和结构相似性，同时保持高测量保真度。

Conclusion: 该方法能够灵活地将先验知识整合到基于物理的反演中，无需配对测量-标签数据集，为电磁脑成像提供了可靠的重建解决方案。

Abstract: Electromagnetic (EM) imaging is an important tool for non-invasive sensing
with low-cost and portable devices. One emerging application is EM stroke
imaging, which enables early diagnosis and continuous monitoring of brain
strokes. Quantitative imaging is achieved by solving an inverse scattering
problem (ISP) that reconstructs permittivity and conductivity maps from
measurements. In general, the reconstruction accuracy is limited by its
inherent nonlinearity and ill-posedness. Existing methods, including
learning-free and learning-based approaches, fail to either incorporate
complicated prior distributions or provide theoretical guarantees, posing
difficulties in balancing interpretability, distortion error, and reliability.
To overcome these limitations, we propose a posterior sampling method based on
latent diffusion for quantitative EM brain imaging, adapted from a generative
plug-and-play (PnP) posterior sampling framework. Our approach allows to
flexibly integrate prior knowledge into physics-based inversion without
requiring paired measurement-label datasets. We first learn the prior
distribution of targets from an unlabeled dataset, and then incorporate the
learned prior into posterior sampling. In particular, we train a latent
diffusion model on permittivity and conductivity maps to capture their prior
distribution. Then, given measurements and the forward model describing EM wave
physics, we perform posterior sampling by alternating between two samplers that
respectively enforce the likelihood and prior distributions. Finally, reliable
reconstruction is obtained through minimum mean squared error (MMSE) estimation
based on the samples. Experimental results on brain imaging demonstrate that
our approach achieves state-of-the-art performance in reconstruction accuracy
and structural similarity while maintaining high measurement fidelity.

</details>


### [9] [Rotatable Antenna Aided Mixed Near-Field and Far-Field Communications in the Upper Mid-Band: Interference Analysis and Joint Optimization](https://arxiv.org/abs/2509.04865)
*Yunpu Zhang,Changsheng You,Hing Cheung So,Dusit Niyato*

Main category: eess.SP

TL;DR: 利用可旋转天线改善混合近场远场通信系统性能，通过天线旋转提供新的空间自由度来抑制复杂干扰


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在混合近场远场通信中面临复杂干扰问题，需要新的技术手段来提升通信性能

Method: 提出模块化可旋转天线系统，通过联合优化功率分配和旋转角度来最大化近场用户的和速率，采用双层算法（内层SCA优化功率，外层PSO优化旋转角度）

Result: 理论分析和数值结果均表明可旋转天线能有效抑制近场干扰和混合场干扰，相比传统固定天线系统获得显著性能增益

Conclusion: 可旋转天线为混合近场远场通信系统提供了有效的干扰抑制手段，所提出的联合设计方案具有优异的性能表现

Abstract: In this paper, we propose to leverage rotatable antennas (RAs) for improving
the communication performance in mixed near-field and far-field communication
systems by exploiting a new spatial degree-of-freedom (DoF) offered by antenna
rotation to mitigate complex near-field interference and mixed-field
interference. Specifically, we investigate a modular RA-enabled mixed-field
downlink communication system, where a base station (BS) consisting of multiple
RA subarrays communicates with multiple near-field users in the presence of
several legacy far-field users. We formulate an optimization problem to
maximize the sum-rate of the near-field users by jointly optimizing the power
allocation and rotation angles of all subarrays at the BS. To gain useful
insights into the effect of RAs on mixed-field communications, we first analyze
a special case where all subarrays share the same rotation angle and obtain
closed-form expressions for the rotation-aware normalized near-field
interference and the rotation-aware normalized mixed-field interference using
the Fresnel integrals. We then analytically reveal that array rotation
effectively suppresses both interference types, thereby significantly enhancing
mixed-field communication performance. For the general case involving
subarray-wise rotation, we propose an efficient double-layer algorithm to
obtain a high-quality solution, where the inner layer optimizes power
allocation using the successive convex approximation (SCA) technique, while the
outer layer determines the rotation angles of all subarrays via particle swarm
optimization (PSO). Finally, numerical results highlight the significant
performance gains achieved by RAs over conventional fixed-antenna systems and
demonstrate the effectiveness of our developed joint design compared to
benchmark schemes.

</details>


### [10] [Movable IRS-Aided ISAC Systems: Joint Beamforming and Position Optimization](https://arxiv.org/abs/2509.04873)
*Yue Geng,Tee Hiang Cheng,Kai Zhong,Kah Chan Teh,Qingqing Wu*

Main category: eess.SP

TL;DR: 这篇论文研究了可移动智能反射表面(MIRS)在集成感知与通信(ISAC)系统中的应用，通过优化MIRS元素位置、反射系数、放大器和接收器来最小化功耗，并提出了两种控制方案和基于溢价变换的曲面优化算法。


<details>
  <summary>Details</summary>
Motivation: 驱动智能反射表面(IRS)和可移动天线(MA)技术的发展，可移动IRS(MIRS)能够提高传统IRS的适应性和性能，通过灵活调整反射元素位置来优化集成感知与通信系统的性能。

Method: 提出两种MIRS控制方案：元素级控制和数组级控制，分别控制单个反射元素和数组的位置。使用产品曲面流形优化(PRMO)方法，通过溢价变换和Riemannian BFGS算法在构建的产品曲面流形空间中并行更新变量。

Result: 模拟结果显示，提出的MIRS方案在功耗最小化方面超过传统IRS。元素级控制方案能够实现最优功耗，而数组级控制方案虽然是次优解但具有更高的计算效率。

Conclusion: MIRS技术在ISAC系统中具有重要价值，元素级控制能提供最优性能，数组级控制则在性能与计算复杂度之间取得平衡，为实际应用提供了灵活的选择。

Abstract: Driven by intelligent reflecting surface (IRS) and movable antenna (MA)
technologies, movable IRS (MIRS) has been proposed to improve the adaptability
and performance of conventional IRS, enabling flexible adjustment of the IRS
reflecting element positions. This paper investigates MIRS-aided integrated
sensing and communication (ISAC) systems. The objective is to minimize the
power required for satisfying the quality-of-service (QoS) of sensing and
communication by jointly optimizing the MIRS element positions, IRS reflection
coefficients, transmit beamforming, and receive filters. To balance the
performance-cost trade-off, we proposed two MIRS schemes: element-wise control
and array-wise control, where the positions of individual reflecting elements
and arrays consisting of multiple elements are controllable, respectively. To
address the joint beamforming and position optimization, a product Riemannian
manifold optimization (PRMO) method is proposed, where the variables are
updated over a constructed product Riemannian manifold space (PRMS) in parallel
via penalty-based transformation and Riemannian
Broyden-Fletcher-Goldfarb-Shanno (RBFGS) algorithm. Simulation results
demonstrate that the proposed MIRS outperforms conventional IRS in power
minimization with both element-wise control and array-wise control.
Specifically, with different system parameters, the minimum power is achieved
by the MIRS with the element-wise control scheme, while suboptimal solution and
higher computational efficiency are achieved by the MIRS with array-wise
control scheme.

</details>


### [11] [Coupled tensor models for probability mass function estimation: Part I, Principles and algorithms](https://arxiv.org/abs/2509.04930)
*Philippe Flores,Konstantin Usevich,David Brie*

Main category: eess.SP

TL;DR: 提出PCTF3D方法，通过部分耦合3D边际张量来估计高维概率质量函数，避免维度灾难问题


<details>
  <summary>Details</summary>
Motivation: 解决高维概率质量函数估计中的维度灾难问题，传统方法在维度增加时计算复杂度急剧上升

Method: 使用部分耦合张量分解技术，选择3D边际张量的子集进行耦合，通过超图形式化边际选择策略

Result: 提出了新的算法框架，能够有效处理高维PMF估计问题，并通过数值实验验证了方法的有效性

Conclusion: PCTF3D为高维概率质量函数估计提供了新的解决方案，后续文章将研究该模型的唯一性性质

Abstract: In this article, a Probability Mass Function (PMF) estimation method which
tames the curse of dimensionality is proposed. This method, called Partial
Coupled Tensor Factorization of 3D marginals or PCTF3D, has for principle to
partially couple order-3 data projections -- seen as order-3 tensors -- to
obtain a tensor decomposition of the probability mass tensor. The novelty of
PCTF3D relies on partial coupling which consists in choosing a subset of 3D
marginals. The choice of marginals is then formulated with hypergraphs. After
presenting possible coupling strategies, some numerical experiments and an
application of the method are proposed. This article is the first of a two-part
article. While this first article focuses on a new algorithmic framework for
PMF estimation, the second studies uniqueness properties of the model
introduced in this article.

</details>


### [12] [Coupled tensor models for probability mass function estimation: Part II, Uniqueness of the model](https://arxiv.org/abs/2509.04931)
*Philippe Flores,Konstantin Usevich,David Brie*

Main category: eess.SP

TL;DR: 本文研究了PCTF3D方法中耦合张量模型的唯一性特性，分析了不同耦合策略对唯一性的影响，并提出了最大可恢复秩的Jacobian算法和笛卡尔耦合的可识别性边界。


<details>
  <summary>Details</summary>
Motivation: 张量方法在统计学习中广泛应用，其最大优势是具有强唯一性特性。PCTF3D方法通过耦合3D边际张量来估计概率质量函数，但需要研究其约束耦合低秩模型的唯一性特性。

Method: 使用Jacobian算法提供最大可恢复秩，分析PCTF3D中不同耦合策略的唯一性特性，特别关注笛卡尔耦合的可识别性边界。

Result: 研究表明唯一性高度依赖于PCTF3D中使用的耦合策略，在适当处理概率约束的情况下，不同耦合策略具有不同的唯一性表现。

Conclusion: 提出了笛卡尔耦合的可识别性边界，改进了文献中的充分边界，为PCTF3D方法的实际应用提供了理论保证。

Abstract: In this paper, uniqueness properties of a coupled tensor model are studied.
This new coupled tensor model is used in a new method called Partial Coupled
Tensor Factorization of 3D marginals or PCTF3D. This method performs estimation
of probability mass functions by coupling 3D marginals, seen as order-3
tensors. The core novelty of PCTF3D's approach (detailed in the part I article)
relies on the partial coupling which consists on the choice of 3D marginals to
be coupled. Tensor methods are ubiquitous in many applications of statistical
learning, with their biggest advantage of having strong uniqueness properties.
In this paper, the uniqueness properties of PCTF3D's constrained coupled
low-rank model is assessed. While probabilistic constraints of the coupled
model are handled properly, it is shown that uniqueness highly depends on the
coupling used in PCTF3D. After proposing a Jacobian algorithm providing maximum
recoverable rank, different coupling strategies presented in the Part I article
are examined with respect to their uniqueness properties. Finally, an
identifiability bound is given for a so-called Cartesian coupling which permits
enhancing sufficient bounds of the literature.

</details>


### [13] [ROPE: A Novel Method for Real-Time Phase Estimation of Complex Biological Rhythms](https://arxiv.org/abs/2509.04962)
*Antonio Spallone,Marco Coraggio,Francesco De Lellis,Mario di Bernardo*

Main category: eess.SP

TL;DR: ROPE是首个能够处理任意维度信号并实时运行的相位估计算法，通过识别信号中的重复模式进行分段并高效搜索先前信号片段来分配相位值，在噪声和信号漂移下表现鲁棒，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有相位估计方法通常仅限于离线处理和/或一维信号，无法满足实时分析多维振荡信号的需求，特别是在神经科学、机器人学等领域需要理解协调机制的应用中。

Method: ROPE算法通过识别信号中的重复模式将其分割为（伪）周期，然后通过对先前信号片段进行高效、可处理的搜索来分配相位值，支持任意维度信号和实时操作。

Result: 在混沌动力系统轨迹、人体运动捕捉数据和心电图记录等多种信号类型上的广泛验证表明，ROPE对噪声和信号漂移具有鲁棒性，性能显著优于最先进的相位估计方法。

Conclusion: ROPE算法实现了复杂生物节律的实时分析，为病理节律紊乱的早期诊断和神经心血管疾病中基于节律的治疗干预开辟了新途径。

Abstract: Accurate phase estimation -- the process of assigning phase values between
$0$ and $2\pi$ to repetitive or periodic signals -- is a cornerstone in the
analysis of oscillatory signals across diverse fields, from neuroscience to
robotics, where it is fundamental, e.g., to understanding coordination in
neural networks, cardiorespiratory coupling, and human-robot interaction.
However, existing methods are often limited to offline processing and/or
constrained to one-dimensional signals. In this paper, we introduce ROPE,
which, to the best of our knowledge, is the first phase-estimation algorithm
capable of (i) handling signals of arbitrary dimension and (ii) operating in
real-time, with minimal error. ROPE identifies repetitions within the signal to
segment it into (pseudo-)periods and assigns phase values by performing
efficient, tractable searches over previous signal segments. We extensively
validate the algorithm on a variety of signal types, including trajectories
from chaotic dynamical systems, human motion-capture data, and
electrocardiographic recordings. Our results demonstrate that ROPE is robust
against noise and signal drift, and achieves significantly superior performance
compared to state-of-the-art phase estimation methods. This advancement enables
real-time analysis of complex biological rhythms, opening new pathways, for
example, for early diagnosis of pathological rhythm disruptions and developing
rhythm-based therapeutic interventions in neurological and cardiovascular
disorders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [A Kolmogorov-Arnold Network for Interpretable Cyberattack Detection in AGC Systems](https://arxiv.org/abs/2509.05259)
*Jehad Jilan,Niranjana Naveen Nambiar,Ahmad Mohammad Saber,Alok Paranjape,Amr Youssef,Deepa Kundur*

Main category: cs.LG

TL;DR: 使用Kolmogorov-Arnold Networks (KAN) 检测电力系统AGC中的假数据注入攻击，提供高检测率和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统检测方法无法有效识别AGC系统中潜伏的网络攻击，需要更准确且可解释的检测方法

Method: 采用KAN模型学习AGC测量数据的非线性关系，并提取符号式方程提高可解释性

Result: KAN模型检测率达95.97%，符号式检测率95.9%，误报率低

Conclusion: KAN提供了高准确、可解释的FDIA检测方案，有助于提升电力系统网络安全

Abstract: Automatic Generation Control (AGC) is essential for power grid stability but
remains vulnerable to stealthy cyberattacks, such as False Data Injection
Attacks (FDIAs), which can disturb the system's stability while evading
traditional detection methods. Unlike previous works that relied on blackbox
approaches, this work proposes Kolmogorov-Arnold Networks (KAN) as an
interpretable and accurate method for FDIA detection in AGC systems,
considering the system nonlinearities. KAN models include a method for
extracting symbolic equations, and are thus able to provide more
interpretability than the majority of machine learning models. The proposed KAN
is trained offline to learn the complex nonlinear relationships between the AGC
measurements under different operating scenarios. After training, symbolic
formulas that describe the trained model's behavior can be extracted and
leveraged, greatly enhancing interpretability. Our findings confirm that the
proposed KAN model achieves FDIA detection rates of up to 95.97% and 95.9% for
the initial model and the symbolic formula, respectively, with a low false
alarm rate, offering a reliable approach to enhancing AGC cybersecurity.

</details>


### [15] [Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics](https://arxiv.org/abs/2509.04536)
*Oliver Dunn,Koorosh Aslansefat,Yiannis Papadopoulos*

Main category: cs.LG

TL;DR: Q-SafeML是一种针对量子机器学习的安全监控方法，通过量子中心距离度量来检测概念漂移，提高系统透明度和安全性


<details>
  <summary>Details</summary>
Motivation: 由于量子计算与经典计算的根本差异，现有的经典ML安全监控方法无法直接应用于QML，而QML专用安全机制仍不成熟

Method: 基于SafeML方法，采用量子中心距离度量来评估模型准确性，进行模型依赖的后分类评估，检测操作数据与训练数据之间的距离

Result: 在QCNN和VQC模型上的实验表明，该方法能够实现知情的人类监督，增强系统透明度和安全性

Conclusion: Q-SafeML为量子机器学习提供了专门的安全监控解决方案，适应了量子系统的独特表示约束

Abstract: The rise of machine learning in safety-critical systems has paralleled
advancements in quantum computing, leading to the emerging field of Quantum
Machine Learning (QML). While safety monitoring has progressed in classical ML,
existing methods are not directly applicable to QML due to fundamental
differences in quantum computation. Given the novelty of QML, dedicated safety
mechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety
monitoring approach for QML. The method builds on SafeML, a recent method that
utilizes statistical distance measures to assess model accuracy and provide
confidence in the reasoning of an algorithm. An adapted version of Q-SafeML
incorporates quantum-centric distance measures, aligning with the probabilistic
nature of QML outputs. This shift to a model-dependent, post-classification
evaluation represents a key departure from classical SafeML, which is
dataset-driven and classifier-agnostic. The distinction is motivated by the
unique representational constraints of quantum systems, requiring distance
metrics defined over quantum state spaces. Q-SafeML detects distances between
operational and training data addressing the concept drifts in the context of
QML. Experiments on QCNN and VQC Models show that this enables informed human
oversight, enhancing system transparency and safety.

</details>


### [16] [Finance-Grounded Optimization For Algorithmic Trading](https://arxiv.org/abs/2509.04541)
*Kasymkhan Khubiev,Mikhail Semenov,Irina Podlipnova*

Main category: cs.LG

TL;DR: 该论文提出了基于金融指标的损失函数（夏普比率、盈亏、最大回撤）和换手率正则化方法，在算法交易指标上优于传统的均方误差损失函数。


<details>
  <summary>Details</summary>
Motivation: 深度学习在金融领域的应用面临可解释性挑战，传统方法在自然语言处理、计算机视觉和预测方面表现良好，但不完全适合金融领域，因为金融专家使用不同的指标来评估模型性能。

Method: 提出了基于关键量化金融指标的财务基础损失函数（夏普比率、盈亏、最大回撤），以及换手率正则化方法，用于限制生成头寸的换手率在预定范围内。

Result: 研究结果表明，提出的损失函数与换手率正则化相结合，在算法交易指标评估中优于传统的均方误差损失函数，能够提升交易策略和投资组合优化的预测性能。

Conclusion: 财务基础指标能够显著提升深度学习模型在金融交易和投资组合优化任务中的预测性能，为金融领域的AI应用提供了更合适的评估框架。

Abstract: Deep Learning is evolving fast and integrates into various domains. Finance
is a challenging field for deep learning, especially in the case of
interpretable artificial intelligence (AI). Although classical approaches
perform very well with natural language processing, computer vision, and
forecasting, they are not perfect for the financial world, in which specialists
use different metrics to evaluate model performance.
  We first introduce financially grounded loss functions derived from key
quantitative finance metrics, including the Sharpe ratio, Profit-and-Loss
(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,
a method that inherently constrains the turnover of generated positions within
predefined limits.
  Our findings demonstrate that the proposed loss functions, in conjunction
with turnover regularization, outperform the traditional mean squared error
loss for return prediction tasks when evaluated using algorithmic trading
metrics. The study shows that financially grounded metrics enhance predictive
performance in trading strategies and portfolio optimization.

</details>


### [17] [i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition](https://arxiv.org/abs/2509.04544)
*Ashutosh Kumar Sinha,Ayush Patel,Mitul Dudhat,Pritam Anand,Rahul Mishra*

Main category: cs.LG

TL;DR: i-Mask是一种基于呼气模式的人类活动识别方法，使用定制面罩传感器采集数据，通过噪声过滤和时间序列分解处理，准确率超过95%


<details>
  <summary>Details</summary>
Motivation: 呼吸模式包含重要的生理信号，可用于预测人类行为、健康趋势和生命参数，对人类活动识别和实时健康监测具有重要意义

Method: 开发定制面罩配备集成传感器采集呼气模式数据，进行噪声过滤、时间序列分解和标注，训练预测模型

Result: 实验验证了方法的有效性，准确率超过95%

Conclusion: 该方法在医疗健康和健身应用方面具有巨大潜力

Abstract: The patterns of inhalation and exhalation contain important physiological
signals that can be used to anticipate human behavior, health trends, and vital
parameters. Human activity recognition (HAR) is fundamentally connected to
these vital signs, providing deeper insights into well-being and enabling
real-time health monitoring. This work presents i-Mask, a novel HAR approach
that leverages exhaled breath patterns captured using a custom-developed mask
equipped with integrated sensors. Data collected from volunteers wearing the
mask undergoes noise filtering, time-series decomposition, and labeling to
train predictive models. Our experimental results validate the effectiveness of
the approach, achieving over 95\% accuracy and highlighting its potential in
healthcare and fitness applications.

</details>


### [18] [Bootstrapping Task Spaces for Self-Improvement](https://arxiv.org/abs/2509.04575)
*Minqi Jiang,Andrei Lupu,Yoram Bachrach*

Main category: cs.LG

TL;DR: Exploratory Iteration (ExIt)是一种自课程强化学习方法，通过选择性采样信息量最大的中间状态来训练LLM进行多步自我改进推理，无需固定最大迭代深度。


<details>
  <summary>Details</summary>
Motivation: 解决传统RL方法在自我改进任务中需要预设固定最大迭代深度的问题，这种方法既昂贵又任意，无法充分利用自我改进的递归结构特性。

Method: ExIt方法通过选择性采样训练过程中遇到的最具信息量的中间部分历史状态，将这些起点作为新的自我迭代任务实例来训练自我改进策略，并可结合显式探索机制维持任务多样性。

Result: 在数学竞赛、多轮工具使用和机器学习工程等多个领域，ExIt策略能够产生在测试任务实例上表现出强大推理时自我改进能力的策略，且迭代步数可超出训练时的平均迭代深度。

Conclusion: ExIt方法成功利用了自我改进任务的递归结构，实现了有效的多步自我改进推理训练，为强化学习在迭代改进任务中的应用提供了新思路。

Abstract: Progress in many task domains emerges from repeated revisions to previous
solution attempts. Training agents that can reliably self-improve over such
sequences at inference-time is a natural target for reinforcement learning
(RL), yet the naive approach assumes a fixed maximum iteration depth, which can
be both costly and arbitrary. We present Exploratory Iteration (ExIt), a family
of autocurriculum RL methods that directly exploits the recurrent structure of
self-improvement tasks to train LLMs to perform multi-step self-improvement at
inference-time while only training on the most informative single-step
iterations. ExIt grows a task space by selectively sampling the most
informative intermediate, partial histories encountered during an episode for
continued iteration, treating these starting points as new self-iteration task
instances to train a self-improvement policy. ExIt can further pair with
explicit exploration mechanisms to sustain greater task diversity. Across
several domains, encompassing competition math, multi-turn tool-use, and
machine learning engineering, we demonstrate that ExIt strategies, starting
from either a single or many task instances, can produce policies exhibiting
strong inference-time self-improvement on held-out task instances, and the
ability to iterate towards higher performance over a step budget extending
beyond the average iteration depth encountered during training.

</details>


### [19] [Fundamental bounds on efficiency-confidence trade-off for transductive conformal prediction](https://arxiv.org/abs/2509.04631)
*Arash Behboodi,Alvaro H. C. Correia,Fabio Valerio Massoli,Christos Louizos*

Main category: cs.LG

TL;DR: 本文证明了转导共形预测中存在置信度与效率（预测集大小）的基本权衡，推导出严格有限样本边界，显示任何非平凡置信度都会导致预测集大小随样本数指数增长，增长指数与数据条件熵成正比。


<details>
  <summary>Details</summary>
Motivation: 研究转导共形预测中同时预测多个数据点时置信度与预测效率之间的基本关系，揭示内在的不确定性如何影响预测集大小。

Method: 推导严格的有限样本边界，分析预测集大小与置信水平、样本数量、条件熵和分散度（对数条件概率分布的方差）的数学关系，并在理想化设置中证明边界的可达性。

Result: 发现任何非平凡置信水平都会导致预测集大小呈指数增长，增长指数与样本数量线性相关且与条件熵成正比。分散度作为二阶项影响边界。在特殊同标签情况下，问题可简化为假设检验并提供了渐近最优置信预测器。

Conclusion: 转导共形预测中存在置信度与效率的根本权衡，数据的内在不确定性决定了预测集大小的指数增长特性，这一理论边界为实际应用提供了重要指导。

Abstract: Transductive conformal prediction addresses the simultaneous prediction for
multiple data points. Given a desired confidence level, the objective is to
construct a prediction set that includes the true outcomes with the prescribed
confidence. We demonstrate a fundamental trade-off between confidence and
efficiency in transductive methods, where efficiency is measured by the size of
the prediction sets. Specifically, we derive a strict finite-sample bound
showing that any non-trivial confidence level leads to exponential growth in
prediction set size for data with inherent uncertainty. The exponent scales
linearly with the number of samples and is proportional to the conditional
entropy of the data. Additionally, the bound includes a second-order term,
dispersion, defined as the variance of the log conditional probability
distribution. We show that this bound is achievable in an idealized setting.
Finally, we examine a special case of transductive prediction where all test
data points share the same label. We show that this scenario reduces to the
hypothesis testing problem with empirically observed statistics and provide an
asymptotically optimal confidence predictor, along with an analysis of the
error exponent.

</details>


### [20] [Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions](https://arxiv.org/abs/2509.04583)
*Jiequn Han,Kui Ren,Nathan Soedjak*

Main category: cs.LG

TL;DR: 提出一种基于测试实例的适应性采样框架，通过迭代精细训练数据集来构建简洁且信息丰富的逆问题解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统学习方法需要大量训练样本，特别是当先验分布高维或需高精度时，数据收集成本明显。本文方法通过根据具体测试实例动态分配采样努力，提高样本效率。

Method: 迭代精细训练数据集，根据最新预测条件化调整采样策略，使数据集适配于每个测试实例周围的逆映射几何结构。在逆向散射问题中进行实验验证。

Result: 适应性方法在更复杂的先验分布或更高精度要求的场景下优势更加明显，显著提高了样本效率。

Conclusion: 该适应性采样策略具有广泛适用性，可扩展到其他逆问题，为传统固定数据集训练模式提供了可扩展且实用的替代方案。

Abstract: We propose an instance-wise adaptive sampling framework for constructing
compact and informative training datasets for supervised learning of inverse
problem solutions. Typical learning-based approaches aim to learn a
general-purpose inverse map from datasets drawn from a prior distribution, with
the training process independent of the specific test instance. When the prior
has a high intrinsic dimension or when high accuracy of the learned solution is
required, a large number of training samples may be needed, resulting in
substantial data collection costs. In contrast, our method dynamically
allocates sampling effort based on the specific test instance, enabling
significant gains in sample efficiency. By iteratively refining the training
dataset conditioned on the latest prediction, the proposed strategy tailors the
dataset to the geometry of the inverse map around each test instance. We
demonstrate the effectiveness of our approach in the inverse scattering problem
under two types of structured priors. Our results show that the advantage of
the adaptive method becomes more pronounced in settings with more complex
priors or higher accuracy requirements. While our experiments focus on a
particular inverse problem, the adaptive sampling strategy is broadly
applicable and readily extends to other inverse problems, offering a scalable
and practical alternative to conventional fixed-dataset training regimes.

</details>


### [21] [Toward Faithfulness-guided Ensemble Interpretation of Neural Network](https://arxiv.org/abs/2509.04588)
*Siyu Zhang,Kenneth Mcmillan*

Main category: cs.LG

TL;DR: FEI框架通过忠实度引导的集成解释方法，提升神经网络解释的可视化效果和定量忠实度评分，在质量和数量上都超越了现有方法


<details>
  <summary>Details</summary>
Motivation: 需要为特定神经推理提供可解释且忠实的解释，以理解和评估模型行为，现有方法在忠实度广度和有效性方面有待提升

Method: 提出FEI框架，使用平滑近似技术提升定量忠实度评分，针对隐藏层编码设计多种变体来增强忠实度，并提出新的定性度量标准评估隐藏层忠实度

Result: 在大量实验中，FEI在定性可视化和定量忠实度评分方面都显著超越了现有方法，实现了实质性进展

Conclusion: 研究建立了一个提升神经网络解释忠实度的综合框架，强调了广度和精度的双重重要性

Abstract: Interpretable and faithful explanations for specific neural inferences are
crucial for understanding and evaluating model behavior. Our work introduces
\textbf{F}aithfulness-guided \textbf{E}nsemble \textbf{I}nterpretation
(\textbf{FEI}), an innovative framework that enhances the breadth and
effectiveness of faithfulness, advancing interpretability by providing superior
visualization. Through an analysis of existing evaluation benchmarks,
\textbf{FEI} employs a smooth approximation to elevate quantitative
faithfulness scores. Diverse variations of \textbf{FEI} target enhanced
faithfulness in hidden layer encodings, expanding interpretability.
Additionally, we propose a novel qualitative metric that assesses hidden layer
faithfulness. In extensive experiments, \textbf{FEI} surpasses existing
methods, demonstrating substantial advances in qualitative visualization and
quantitative faithfulness scores. Our research establishes a comprehensive
framework for elevating faithfulness in neural network explanations,
emphasizing both breadth and precision

</details>


### [22] [Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction](https://arxiv.org/abs/2509.04601)
*Han Zhang,Fengji Ma,Jiamin Su,Xinyue Yang,Lei Wang,Wen-Cai Ye,Li Liu*

Main category: cs.LG

TL;DR: 重新设计的量子增强多任务学习框架QW-MTL，通过量子化学描述符和动态任务权重调节，在13个ADMET分类任务上显著提升了预测性能


<details>
  <summary>Details</summary>
Motivation: 现有单任务学习方法无法充分利用任务间的互补性，且需要更多计算资源，需要一种更高效的多任务学习框架来处理ADMET预测问题

Method: 基于Chemprop-RDKit背景，采用量子化学描述符丰富分子表征，并介绍了新的指数任务权重方案，结合数据集规模先验和可学习参数实现动态损失平衡

Result: 在13个TDC分类标准测试中，QW-MTL在12个任务上显著超过单任务基线，达到了高预测性能且模型复杂度最小，推理速度快

Conclusion: QW-MTL框架通过量子信息特征和适应性任务权重，证明了多任务分子学习在ADMET预测中的有效性和效率，为药物发现提供了更优科学的解决方案

Abstract: Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and
Toxicity) plays a crucial role in drug discovery and development, accelerating
the screening and optimization of new drugs. Existing methods primarily rely on
single-task learning (STL), which often fails to fully exploit the
complementarities between tasks. Besides, it requires more computational
resources while training and inference of each task independently. To address
these issues, we propose a new unified Quantum-enhanced and task-Weighted
Multi-Task Learning (QW-MTL) framework, specifically designed for ADMET
classification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts
quantum chemical descriptors to enrich molecular representations with
additional information about the electronic structure and interactions.
Meanwhile, it introduces a novel exponential task weighting scheme that
combines dataset-scale priors with learnable parameters to achieve dynamic loss
balancing across tasks. To the best of our knowledge, this is the first work to
systematically conduct joint multi-task training across all 13 Therapeutics
Data Commons (TDC) classification benchmarks, using leaderboard-style data
splits to ensure a standardized and realistic evaluation setting. Extensive
experimental results show that QW-MTL significantly outperforms single-task
baselines on 12 out of 13 tasks, achieving high predictive performance with
minimal model complexity and fast inference, demonstrating the effectiveness
and efficiency of multi-task molecular learning enhanced by quantum-informed
features and adaptive task weighting.

</details>


### [23] [Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families](https://arxiv.org/abs/2509.04622)
*Jialin Wu,Shreya Saha,Yiqing Bo,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 代表性相似性指标评估框架，系统比较不同模型家族的区分能力，发现严格对齐约束的指标更效。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对不同代表性相似性指标在区分不同模型家族方面的系统性比较研究，需要建立量化评估框架。

Method: 使用信号检测d理论dprime、过度系数和ROC-AUC三种补充性区分度量，在不同网络架构和训练方式下评估RSA、线性预测性、Procrustes和软匹配等常用相似性指标。

Result: 区分能力随着对齐约束的严格程度而提高；软匹配方法区分效果最佳，其次是Procrustes对齐和线性预测性；RSA等非拟合方法也有良好区分效果。

Conclusion: 该研究首次通过区分性视角系统比较相似性指标，明确了各指标的相对敏感性，为大规模模型和脑科学比较提供了指导。

Abstract: Representational similarity metrics are fundamental tools in neuroscience and
AI, yet we lack systematic comparisons of their discriminative power across
model families. We introduce a quantitative framework to evaluate
representational similarity measures based on their ability to separate model
families-across architectures (CNNs, Vision Transformers, Swin Transformers,
ConvNeXt) and training regimes (supervised vs. self-supervised). Using three
complementary separability measures-dprime from signal detection theory,
silhouette coefficients and ROC-AUC, we systematically assess the
discriminative capacity of commonly used metrics including RSA, linear
predictivity, Procrustes, and soft matching. We show that separability
systematically increases as metrics impose more stringent alignment
constraints. Among mapping-based approaches, soft-matching achieves the highest
separability, followed by Procrustes alignment and linear predictivity.
Non-fitting methods such as RSA also yield strong separability across families.
These results provide the first systematic comparison of similarity metrics
through a separability lens, clarifying their relative sensitivity and guiding
metric choice for large-scale model and brain comparisons.

</details>


### [24] [Split Conformal Prediction in the Function Space with Neural Operators](https://arxiv.org/abs/2509.04623)
*David Millard,Lars Lindemann,Ali Baheri*

Main category: cs.LG

TL;DR: 该论文将分割共形预测扩展到函数空间，为神经算子提供有限样本覆盖保证，通过离散化映射和渐进收敛方法解决无限维设置中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 神经算子在无限维设置中的不确定性量化缺乏有限样本覆盖保证，现有方法需要强分布假设或产生保守覆盖，需要一种能够处理函数值输出的共形预测方法。

Method: 采用两步法：首先在有限维空间建立覆盖保证，然后通过离散化细化将保证提升到函数空间；提出回归校正方法在分辨率间传递校准，并引入两个诊断指标量化自回归设置中的预测退化。

Result: 经验结果表明，该方法在分辨率变化下保持校准覆盖且变化较小，在超分辨率任务中实现更好的覆盖性能。

Conclusion: 该方法成功将共形预测扩展到函数空间，为神经算子提供了有效的有限样本不确定性量化框架，解决了无限维设置中的覆盖保证问题。

Abstract: Uncertainty quantification for neural operators remains an open problem in
the infinite-dimensional setting due to the lack of finite-sample coverage
guarantees over functional outputs. While conformal prediction offers
finite-sample guarantees in finite-dimensional spaces, it does not directly
extend to function-valued outputs. Existing approaches (Gaussian processes,
Bayesian neural networks, and quantile-based operators) require strong
distributional assumptions or yield conservative coverage. This work extends
split conformal prediction to function spaces following a two step method. We
first establish finite-sample coverage guarantees in a finite-dimensional space
using a discretization map in the output function space. Then these guarantees
are lifted to the function-space by considering the asymptotic convergence as
the discretization is refined. To characterize the effect of resolution, we
decompose the conformal radius into discretization, calibration, and
misspecification components. This decomposition motivates a regression-based
correction to transfer calibration across resolutions. Additionally, we propose
two diagnostic metrics (conformal ensemble score and internal agreement) to
quantify forecast degradation in autoregressive settings. Empirical results
show that our method maintains calibrated coverage with less variation under
resolution shifts and achieves better coverage in super-resolution tasks.

</details>


### [25] [CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals](https://arxiv.org/abs/2509.04699)
*Wenhui Cui,Christopher Sandino,Hadi Pouransari,Ran Liu,Juri Minxha,Ellen L. Zippi,Aman Verma,Anna Sedlackova,Behrooz Mahasseni,Erdrin Azemi*

Main category: cs.LG

TL;DR: 通过对比预训练框架将EMG信号与手部姿态表征对齐，提升了手势分类性能，特别是在零样本分类中显著收益


<details>
  <summary>Details</summary>
Motivation: 利用低耗能、成本效益好的生物信号（如表面电机图EMG）在可穿戴设备上实现持续手势预测，但需要充分利用高质量结构化数据来提升表征质量

Method: 提出对比姿态-EMG预训练（CPEP）框架，学习一个EMG编码器，使其产生高质量且包含姿态信息的表征

Result: 模型在分布内手势分类上超过emg2pose基准模型达21%，在未见手势分类上超迈72%

Conclusion: 通过将弱模态数据与高质量结构化数据的表征对齐，可以显著提升EMG表征质量并实现零样本分类

Abstract: Hand gesture classification using high-quality structured data such as
videos, images, and hand skeletons is a well-explored problem in computer
vision. Leveraging low-power, cost-effective biosignals, e.g. surface
electromyography (sEMG), allows for continuous gesture prediction on wearables.
In this paper, we demonstrate that learning representations from weak-modality
data that are aligned with those from structured, high-quality data can improve
representation quality and enables zero-shot classification. Specifically, we
propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and
pose representations, where we learn an EMG encoder that produces high-quality
and pose-informative representations. We assess the gesture classification
performance of our model through linear probing and zero-shot setups. Our model
outperforms emg2pose benchmark models by up to 21% on in-distribution gesture
classification and 72% on unseen (out-of-distribution) gesture classification.

</details>


### [26] [Interpreting Transformer Architectures as Implicit Multinomial Regression](https://arxiv.org/abs/2509.04653)
*Jonas A. Actor,Anthony Gruber,Eric C. Cyr*

Main category: cs.LG

TL;DR: 论文建立了注意力机制与多项式回归之间的新联系，证明在固定多项式回归设置中，优化潜在特征会产生与注意力块诱导动态一致的解。


<details>
  <summary>Details</summary>
Motivation: 尽管注意力机制在Transformer模型中起核心作用，但其数学基础以及与特征多义性、叠加和模型性能等概念的关系仍不清楚，需要更好的理论理解。

Method: 在固定多项式回归设置中优化潜在特征，分析最优解与注意力块诱导动态的一致性，将Transformer中的表示演化解释为恢复最优分类特征的轨迹。

Result: 研究发现注意力机制的最优解与多项式回归中的最优特征恢复过程具有数学上的一致性，为理解注意力提供了新的理论框架。

Conclusion: 该研究建立了注意力机制与多项式回归之间的理论联系，为理解Transformer内部表示演化提供了新的解释视角，有助于提升对注意力机制的理论认识。

Abstract: Mechanistic interpretability aims to understand how internal components of
modern machine learning models, such as weights, activations, and layers, give
rise to the model's overall behavior. One particularly opaque mechanism is
attention: despite its central role in transformer models, its mathematical
underpinnings and relationship to concepts like feature polysemanticity,
superposition, and model performance remain poorly understood. This paper
establishes a novel connection between attention mechanisms and multinomial
regression. Specifically, we show that in a fixed multinomial regression
setting, optimizing over latent features yields optimal solutions that align
with the dynamics induced by attention blocks. In other words, the evolution of
representations through a transformer can be interpreted as a trajectory that
recovers the optimal features for classification.

</details>


### [27] [Flexible inference of learning rules from de novo learning data using neural networks](https://arxiv.org/abs/2509.04661)
*Yuhan Helena Liu,Victor Geadah,Jonathan Pillow*

Main category: cs.LG

TL;DR: 通过深度神经网络框架从动物决策数据中推断学习规则，发现了正误试验的不对称更新和历史依赖性


<details>
  <summary>Details</summary>
Motivation: 动物学习机制研究对神经科学和AI发展重要，但现有方法多假设特定参数化学习规则或限于简化任务，与动物实际学习新行为的复杂性不符

Method: 提出非参数化框架，用深度神经网络(DNN)定义每次试验的策略权重更新，并扩展到通过递归神经网络(RNN)捕捉非马尔可夫动态性

Result: 在模拟中恢复了真实学习规则，在鼠类感觉决策任务数据上提升了预测性能，推断出了正误试验的不对称更新和历史依赖特征

Conclusion: 该框架为从生物行为数据推断学习规则提供了灵活方法，为实验训练协议和行为数字双胞开发提供了见解

Abstract: Understanding how animals learn is a central challenge in neuroscience, with
growing relevance to the development of animal- or human-aligned artificial
intelligence. However, most existing approaches assume specific parametric
forms for the learning rule (e.g., Q-learning, policy gradient) or are limited
to simplified settings like bandit tasks, which do not involve learning a new
input-output mapping from scratch. In contrast, animals must often learn new
behaviors de novo, which poses a rich challenge for learning-rule inference. We
target this problem by inferring learning rules directly from animal
decision-making data during de novo task learning, a setting that requires
models flexible enough to capture suboptimality, history dependence, and rich
external stimulus integration without strong structural priors. We first
propose a nonparametric framework that parameterizes the per-trial update of
policy weights with a deep neural network (DNN), and validate it by recovering
ground-truth rules in simulation. We then extend to a recurrent variant (RNN)
that captures non-Markovian dynamics by allowing updates to depend on trial
history. Applied to a large behavioral dataset of mice learning a sensory
decision-making task over multiple weeks, our models improved predictions on
held-out data. The inferred rules revealed asymmetric updates after correct
versus error trials and history dependence, consistent with non-Markovian
learning. Overall, these results introduce a flexible framework for inferring
biological learning rules from behavioral data in de novo learning tasks,
providing insights to inform experimental training protocols and the
development of behavioral digital twins.

</details>


### [28] [Beyond Ordinary Lipschitz Constraints: Differentially Private Stochastic Optimization with Tsybakov Noise Condition](https://arxiv.org/abs/2509.04668)
*Difei Xu,Meng Ding,Zihang Xiang,Jinhui Xu,Di Wang*

Main category: cs.LG

TL;DR: 本文研究差分隐私模型下的随机凸优化问题，在Tsybakov噪声条件下提出了新的算法，获得了与Lipschitz常数无关的效用上界，并建立了相应的下界。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私优化算法通常假设损失函数是Lipschitz连续的，但在实际应用中Lipschitz常数可能非常大甚至无界。本文旨在解决在梯度有界矩条件下，Tsybakov噪声条件下的差分隐私优化问题。

Method: 针对Lipschitz情况(θ≥2)提出了(ε,δ)-DP算法，然后扩展到θ≥θ̄>1的情况。在隐私预算ε足够小时，即使损失函数非Lipschitz也能获得上界。同时建立了ρ-zCDP下的下界。

Result: 获得了与Lipschitz常数无关的效用上界：Õ((ṽ₂ₖ(1/√n + (√d/nε))^(k-1)/k)^(θ/(θ-1)))，并证明了相应的下界Ω((ṽₖ(1/√n + (√d/n√ρ))^(k-1)/k)^(θ/(θ-1)))。

Conclusion: 本文在Tsybakov噪声条件和梯度有界矩假设下，为差分隐私凸优化问题建立了紧致的上下界，突破了传统Lipschitz假设的限制，为更广泛的优化问题提供了隐私保护解决方案。

Abstract: We study Stochastic Convex Optimization in the Differential Privacy model
(DP-SCO). Unlike previous studies, here we assume the population risk function
satisfies the Tsybakov Noise Condition (TNC) with some parameter $\theta>1$,
where the Lipschitz constant of the loss could be extremely large or even
unbounded, but the $\ell_2$-norm gradient of the loss has bounded $k$-th moment
with $k\geq 2$. For the Lipschitz case with $\theta\geq 2$, we first propose an
$(\varepsilon, \delta)$-DP algorithm whose utility bound is
$\Tilde{O}\left(\left(\tilde{r}_{2k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
in high probability, where $n$ is the sample size, $d$ is the model dimension,
and $\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the
gradient. It is notable that such an upper bound is independent of the
Lipschitz constant. We then extend to the case where
  $\theta\geq \bar{\theta}> 1$ for some known constant $\bar{\theta}$.
Moreover, when the privacy budget $\varepsilon$ is small enough, we show an
upper bound of
$\tilde{O}\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\varepsilon}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$
even if the loss function is not Lipschitz. For the lower bound, we show that
for any $\theta\geq 2$, the private minimax rate for $\rho$-zero Concentrated
Differential Privacy is lower bounded by
$\Omega\left(\left(\tilde{r}_{k}(\frac{1}{\sqrt{n}}+(\frac{\sqrt{d}}{n\sqrt{\rho}}))^\frac{k-1}{k}\right)^\frac{\theta}{\theta-1}\right)$.

</details>


### [29] [Echoes Before Collapse: Deep Learning Detection of Flickering in Complex Systems](https://arxiv.org/abs/2509.04683)
*Yazdan Babazadeh Maghsoodlo,Madhur Anand,Chris T. Bauch*

Main category: cs.LG

TL;DR: 深度学习模型能够从合成时间序列中学习并准确识别复杂系统中的闪烁模式（flickering），这种噪声驱动的状态切换是系统韧性降低的早期预警信号，模型在多种随机系统和实际数据中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 闪烁现象是气候系统、生态系统、金融市场等复杂系统中韧性降低的标志性特征，可能预示着难以预测的关键体制转变。虽然深度学习在预测临界点方面显示出强大能力，但其在检测闪烁模式方面的潜力尚未被探索。

Method: 使用卷积长短期记忆（CNN LSTM）模型，在由简单多项式函数加噪声生成的合成时间序列上进行训练，学习识别闪烁模式。

Result: 尽管在简化动力学上训练，模型能够泛化到多种随机系统，并在经验数据集中可靠地检测闪烁现象，包括睡鼠体温记录和非洲湿润期古气候代理数据。

Conclusion: 深度学习能够从噪声非线性时间序列中提取早期预警信号，为识别广泛动力系统中的不稳定性提供了一个灵活框架。

Abstract: Deep learning offers powerful tools for anticipating tipping points in
complex systems, yet its potential for detecting flickering (noise-driven
switching between coexisting stable states) remains unexplored. Flickering is a
hallmark of reduced resilience in climate systems, ecosystems, financial
markets, and other systems. It can precede critical regime shifts that are
highly impactful but difficult to predict. Here we show that convolutional long
short-term memory (CNN LSTM) models, trained on synthetic time series generated
from simple polynomial functions with additive noise, can accurately identify
flickering patterns. Despite being trained on simplified dynamics, our models
generalize to diverse stochastic systems and reliably detect flickering in
empirical datasets, including dormouse body temperature records and
palaeoclimate proxies from the African Humid Period. These findings demonstrate
that deep learning can extract early warning signals from noisy, nonlinear time
series, providing a flexible framework for identifying instability across a
wide range of dynamical systems.

</details>


### [30] [KRAFT: A Knowledge Graph-Based Framework for Automated Map Conflation](https://arxiv.org/abs/2509.04684)
*Farnoosh Hashemi,Laks V. S. Lakshmanan*

Main category: cs.LG

TL;DR: KRaft是一种基于学习的数字地图融合方法，通过知识图谱构建、地图匹配和地图合并三个模块，解决了传统方法只能处理线性对象和基于启发式规则的局限性，在性能上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字地图在导航、车队管理等应用中至关重要，但现有地理空间数据库存在区域覆盖不全和实体缺失问题。传统地图融合方法只能处理线性对象且基于预定义规则，无法以数据驱动方式学习实体匹配。

Method: KRaft包含三个部分：(1)知识图谱构建-将每个地理空间数据库表示为知识图谱；(2)地图匹配-使用知识图谱对齐方法和地理空间特征编码器匹配实体；(3)地图合并-通过混合整数线性规划公式一致地合并匹配实体。

Result: 实验评估表明，KRaft不仅在地图融合任务中相比最先进方法和基线方法表现出色，其各个模块（如地图匹配和地图合并）也分别优于传统的匹配和合并方法。

Conclusion: KRaft成功解决了传统地图融合方法的局限性，通过基于学习的方法实现了更全面和准确的地图数据融合，为数字地图的及时更新提供了有效解决方案。

Abstract: Digital maps play a crucial role in various applications such as navigation,
fleet management, and ride-sharing, necessitating their accuracy and currency,
which require timely updates. While the majority of geospatial databases (GDBs)
provide high-quality information, their data is (i) limited to specific regions
and/or (ii) missing some entities, even in their covered areas. Map conflation
is the process of augmentation of a GDB using another GDB to conflate missing
spatial features. Existing map conflation methods suffer from two main
limitations: (1) They are designed for the conflation of linear objects (e.g.,
road networks) and cannot simply be extended to non-linear objects, thus
missing information about most entities in the map. (2) They are heuristic
algorithmic approaches that are based on pre-defined rules, unable to learn
entities matching in a data-driven manner. To address these limitations, we
design KRAFT, a learning based approach consisting of three parts: (1)
Knowledge Graph Construction - where each GDB is represented by a knowledge
graph, (2) Map Matching - where we use a knowledge graph alignment method as
well as a geospatial feature encoder to match entities in obtained knowledge
graphs, and (3) Map Merging - where we merge matched entities in the previous
modules in a consistent manner, using a mixed integer linear programming
formulation that fully merges the GDBs without adding any inconsistencies. Our
experimental evaluation shows that not only does KRAFT achieve outstanding
performance compared to state-of-the-art and baseline methods in map conflation
tasks, but each of its modules (e.g., Map Matching and Map Merging) also
separately outperforms traditional matching and merging methods.

</details>


### [31] [Natural Spectral Fusion: p-Exponent Cyclic Scheduling and Early Decision-Boundary Alignment in First-Order Optimization](https://arxiv.org/abs/2509.04713)
*Gongyue Zhang,Honghai Liu*

Main category: cs.LG

TL;DR: 论文提出了Natural Spectral Fusion (NSF)方法，将优化器视为频谱控制器，通过p指数扩展和循环调度来动态平衡高低频信息，提高优化效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究广泛讨论了机器学习中的频谱行为，但优化器自身的频谱偏好仍不明确。作者认为一阶优化器具有内在的频率偏好，显著影响优化路径。

Method: 提出NSF方法：1) 将优化器视为频谱控制器，动态平衡高低频信息；2) 通过p指数扩展二阶矩项，支持正负指数；3) 采用循环调度定期重新加权频率带，不改变模型、数据或训练流程。

Result: 理论分析和实验表明：自适应方法强调低频信息，SGD接近中性，负指数放大高频信息。循环调度拓宽频谱覆盖，改善跨频带融合，诱导早期决策边界对齐。在多个基准测试中，p指数循环调度持续降低测试误差，在某些任务上仅用1/4训练成本即可达到基线精度。

Conclusion: NSF揭示了优化器作为主动频谱控制器的作用，为一阶优化提供了统一、可控且高效的框架。

Abstract: Spectral behaviors have been widely discussed in machine learning, yet the
optimizer's own spectral bias remains unclear. We argue that first-order
optimizers exhibit an intrinsic frequency preference that significantly
reshapes the optimization path. To address this, we propose Natural Spectral
Fusion (NSF): reframing training as controllable spectral coverage and
information fusion rather than merely scaling step sizes. NSF has two core
principles: treating the optimizer as a spectral controller that dynamically
balances low- and high-frequency information; and periodically reweighting
frequency bands at negligible cost, without modifying the model, data, or
training pipeline. We realize NSF via a p-exponent extension of the
second-moment term, enabling both positive and negative exponents, and
implement it through cyclic scheduling. Theory and experiments show that
adaptive methods emphasize low frequencies, SGD is near-neutral, and negative
exponents amplify high-frequency information. Cyclic scheduling broadens
spectral coverage, improves cross-band fusion, and induces early
decision-boundary alignment, where accuracy improves even while loss remains
high. Across multiple benchmarks, with identical learning-rate strategies and
fixed hyperparameters, p-exponent cyclic scheduling consistently reduces test
error and demonstrates distinct convergence behavior; on some tasks, it matches
baseline accuracy with only one-quarter of the training cost. Overall, NSF
reveals the optimizer's role as an active spectral controller and provides a
unified, controllable, and efficient framework for first-order optimization.

</details>


### [32] [CoVeR: Conformal Calibration for Versatile and Reliable Autoregressive Next-Token Prediction](https://arxiv.org/abs/2509.04733)
*Yuzhu Chen,Yingjie Wang,Shunyu Liu,Yongcheng Jing,Dacheng Tao*

Main category: cs.LG

TL;DR: CoVeR是一种基于共形预测框架的无模型解码策略，能够在保持紧凑搜索空间的同时确保对理想轨迹的高覆盖概率，并提供理论上的覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有自回归预训练模型的解码策略（如beam search）缺乏可证明的覆盖保证，难以在搜索效率和多样化轨迹需求之间取得平衡，特别是在需要长尾序列的真实应用中。

Method: 提出CoVeR解码策略，基于共形预测框架，通过PAC风格的泛化边界理论保证，确保至少1-α的覆盖概率。

Result: 理论分析表明CoVeR能够渐近地实现至少1-α的覆盖率，其中α∈(0,1)是任意目标水平。

Conclusion: CoVeR为解决现有解码策略的覆盖保证问题提供了一种有效的模型无关解决方案，具有理论保证和实际应用价值。

Abstract: Autoregressive pre-trained models combined with decoding methods have
achieved impressive performance on complex reasoning tasks. While mainstream
decoding strategies such as beam search can generate plausible candidate sets,
they often lack provable coverage guarantees, and struggle to effectively
balance search efficiency with the need for versatile trajectories,
particularly those involving long-tail sequences that are essential in certain
real-world applications. To address these limitations, we propose
\textsc{CoVeR}, a novel model-free decoding strategy wihtin the conformal
prediction framework that simultaneously maintains a compact search space and
ensures high coverage probability over desirable trajectories. Theoretically,
we establish a PAC-style generalization bound, guaranteeing that \textsc{CoVeR}
asymptotically achieves a coverage rate of at least $1 - \alpha$ for any target
level $\alpha \in (0,1)$.

</details>


### [33] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: Beyond I-Con框架通过探索替代统计散度和相似性核，系统性地发现新的损失函数，在无监督聚类、监督对比学习和降维任务中均取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: I-Con框架发现超过23种表示学习方法隐式最小化数据和学得分布之间的KL散度，但KL散度的不对称性和无界性可能导致优化问题，且可能与真实目标不一致。

Method: 提出Beyond I-Con框架，通过探索不同的统计散度（如总变差距离、有界f-散度）和相似性核（如距离核替代角度核）来系统发现新的损失函数。

Result: 在无监督聚类DINO-ViT嵌入上获得SOTA结果；在监督对比学习上超越标准方法；在降维任务中获得更好的定性结果和下游任务性能。

Conclusion: 统计散度和相似性核的选择对表示学习优化至关重要，Beyond I-Con框架为系统探索这些选择提供了有效途径。

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>


### [34] [VARMA-Enhanced Transformer for Time Series Forecasting](https://arxiv.org/abs/2509.04782)
*Jiajun Song,Xiaoou Liu*

Main category: cs.LG

TL;DR: VARMAformer是一个新颖的时间序列预测架构，将经典VARMA统计模型的局部时序依赖建模能力与仅交叉注意力的Transformer效率相结合，通过VFE特征提取器和VE-atten注意力机制实现全局和局部模式的协同捕获。


<details>
  <summary>Details</summary>
Motivation: 现有的仅交叉注意力Transformer模型虽然高效，但可能忽略了经典统计模型（如VARMA）能够有效捕捉的细粒度局部时序依赖关系。

Method: 提出VARMAformer架构，包含两个关键创新：1）VARMA启发的特征提取器（VFE）在patch级别显式建模自回归和移动平均模式；2）VARMA增强注意力机制（VE-atten）使用时序门使查询更具上下文感知能力。

Result: 在广泛使用的基准数据集上进行大量实验，证明该模型始终优于现有的最先进方法。

Conclusion: 这项工作验证了将经典统计洞察融入现代深度学习框架对时间序列预测的显著益处。

Abstract: Transformer-based models have significantly advanced time series forecasting.
Recent work, like the Cross-Attention-only Time Series transformer (CATS),
shows that removing self-attention can make the model more accurate and
efficient. However, these streamlined architectures may overlook the
fine-grained, local temporal dependencies effectively captured by classical
statistical models like Vector AutoRegressive Moving Average model (VARMA). To
address this gap, we propose VARMAformer, a novel architecture that synergizes
the efficiency of a cross-attention-only framework with the principles of
classical time series analysis. Our model introduces two key innovations: (1) a
dedicated VARMA-inspired Feature Extractor (VFE) that explicitly models
autoregressive (AR) and moving-average (MA) patterns at the patch level, and
(2) a VARMA-Enhanced Attention (VE-atten) mechanism that employs a temporal
gate to make queries more context-aware. By fusing these classical insights
into a modern backbone, VARMAformer captures both global, long-range
dependencies and local, statistical structures. Through extensive experiments
on widely-used benchmark datasets, we demonstrate that our model consistently
outperforms existing state-of-the-art methods. Our work validates the
significant benefit of integrating classical statistical insights into modern
deep learning frameworks for time series forecasting.

</details>


### [35] [Graph Unlearning: Efficient Node Removal in Graph Neural Networks](https://arxiv.org/abs/2509.04785)
*Faqian Guan,Tianqing Zhu,Zhoutian Wang,Wei Ren,Wanlei Zhou*

Main category: cs.LG

TL;DR: 这篇论文提出了三种新的节点反学习方法，通过利用图的拓扑特征来高效移除GNN模型中的敏感训练节点信息，以保护隐私并提高模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有节点反学习方法存在限制GNN结构、未充分利用图拓扑、或破坏图拓扑等问题，影响了性能复杂度的平衡。需要开发更高效的方法来移除敏感训练数据并保护隐私。

Method: 提出三种新方法：1)基于类别的标签替换；2)拓扑引导的邻居平均后验概率；3)类一致性邻居节点过滤。后两种方法特别重点利用了图的拓扑特征。

Result: 在三个标准数据集上进行实验，从模型效用性、反学习效用性和反学习效率三个维度评估。实验结果表明提出方法在效用性和效率方面都显著优于现有最先进方法。

Conclusion: 提出的方法能够高效移除GNN中的敏感训练节点，保护隐私信息。这些发现有助于提升GNN模型的隐私安全性，并为节点反学习领域提供了有价值的见解。

Abstract: With increasing concerns about privacy attacks and potential sensitive
information leakage, researchers have actively explored methods to efficiently
remove sensitive training data and reduce privacy risks in graph neural network
(GNN) models. Node unlearning has emerged as a promising technique for
protecting the privacy of sensitive nodes by efficiently removing specific
training node information from GNN models. However, existing node unlearning
methods either impose restrictions on the GNN structure or do not effectively
utilize the graph topology for node unlearning. Some methods even compromise
the graph's topology, making it challenging to achieve a satisfactory
performance-complexity trade-off. To address these issues and achieve efficient
unlearning for training node removal in GNNs, we propose three novel node
unlearning methods: Class-based Label Replacement, Topology-guided Neighbor
Mean Posterior Probability, and Class-consistent Neighbor Node Filtering. Among
these methods, Topology-guided Neighbor Mean Posterior Probability and
Class-consistent Neighbor Node Filtering effectively leverage the topological
features of the graph, resulting in more effective node unlearning. To validate
the superiority of our proposed methods in node unlearning, we conducted
experiments on three benchmark datasets. The evaluation criteria included model
utility, unlearning utility, and unlearning efficiency. The experimental
results demonstrate the utility and efficiency of the proposed methods and
illustrate their superiority compared to state-of-the-art node unlearning
methods. Overall, the proposed methods efficiently remove sensitive training
nodes and protect the privacy information of sensitive nodes in GNNs. The
findings contribute to enhancing the privacy and security of GNN models and
provide valuable insights into the field of node unlearning.

</details>


### [36] [An Arbitration Control for an Ensemble of Diversified DQN variants in Continual Reinforcement Learning](https://arxiv.org/abs/2509.04815)
*Wonseo Jang,Dongjae Kim*

Main category: cs.LG

TL;DR: 提出ACED-DQN框架，通过仲裁控制机制管理多样化DQN集成，解决深度强化学习在持续学习中的灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在静态环境中表现优秀，但在持续强化学习场景中容易发生灾难性遗忘，导致性能下降。受人类前额叶皮层决策机制启发，需要开发能够持续学习的RL框架

Method: 集成多样化DQN变体（具有不同价值函数），并设计仲裁控制机制，优先选择最近试验中错误率较低（可靠性更高）的智能体

Result: 在静态和持续环境中都显示出显著的性能提升，实证证据表明仲裁控制对多样化DQN在训练期间的有效性

Conclusion: 提出了一个受人类大脑启发的框架，使RL智能体能够实现持续学习，有效解决了灾难性遗忘问题

Abstract: Deep reinforcement learning (RL) models, despite their efficiency in learning
an optimal policy in static environments, easily loses previously learned
knowledge (i.e., catastrophic forgetting). It leads RL models to poor
performance in continual reinforcement learning (CRL) scenarios. To address
this, we present an arbitration control mechanism over an ensemble of RL
agents. It is motivated by and closely aligned with how humans make decisions
in a CRL context using an arbitration control of multiple RL agents in parallel
as observed in the prefrontal cortex. We integrated two key ideas into our
model: (1) an ensemble of RLs (i.e., DQN variants) explicitly trained to have
diverse value functions and (2) an arbitration control that prioritizes agents
with higher reliability (i.e., less error) in recent trials. We propose a
framework for CRL, an Arbitration Control for an Ensemble of Diversified DQN
variants (ACED-DQN). We demonstrate significant performance improvements in
both static and continual environments, supported by empirical evidence showing
the effectiveness of arbitration control over diversified DQNs during training.
In this work, we introduced a framework that enables RL agents to continuously
learn, with inspiration from the human brain.

</details>


### [37] [Revolution or Hype? Seeking the Limits of Large Models in Hardware Design](https://arxiv.org/abs/2509.04905)
*Qiang Xu,Leon Stok,Rolf Drechsler,Xi Wang,Grace Li Zhang,Igor L. Markov*

Main category: cs.LG

TL;DR: 大型语言模型和大型电路模型在EDA领域的影响与争议分析，评估其实际能力、限制和未来前景


<details>
  <summary>Details</summary>
Motivation: 应对LLMs和LCMs在电子设计自动化领域带来的激动与疑虑，承诺与过度期待并存的状况

Method: 聚集学术界和产业界领军专家视角，批判性审视大型AI模型在硬件设计中的实际应用

Result: 形成关于可靠性、可扩展性和可解释性的核心论迷综述，提供权威视角

Conclusion: 为ICCAD 2025论坛提供基础文本，对当今最争议的技术趋势提供新见解

Abstract: Recent breakthroughs in Large Language Models (LLMs) and Large Circuit Models
(LCMs) have sparked excitement across the electronic design automation (EDA)
community, promising a revolution in circuit design and optimization. Yet, this
excitement is met with significant skepticism: Are these AI models a genuine
revolution in circuit design, or a temporary wave of inflated expectations?
This paper serves as a foundational text for the corresponding ICCAD 2025
panel, bringing together perspectives from leading experts in academia and
industry. It critically examines the practical capabilities, fundamental
limitations, and future prospects of large AI models in hardware design. The
paper synthesizes the core arguments surrounding reliability, scalability, and
interpretability, framing the debate on whether these models can meaningfully
outperform or complement traditional EDA methods. The result is an
authoritative overview offering fresh insights into one of today's most
contentious and impactful technology trends.

</details>


### [38] [Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series](https://arxiv.org/abs/2509.04921)
*Yuki Takemoto*

Main category: cs.LG

TL;DR: 提出了一种通过生成人工混沌时间序列和重采样技术来模拟金融时间序列数据的方法，使用100亿训练样本进行大规模预训练，在比特币交易数据上实现了零样本预测，交易策略收益显著优于自相关模型。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测具有挑战性，现有时间序列基础模型需要适应各种预测任务，而真实世界时间序列具有混沌特性，需要开发能够处理混沌时间序列的预测方法。

Method: 生成人工混沌时间序列，应用重采样技术模拟金融时间序列数据作为训练样本，通过增加重采样间隔扩展预测范围，使用100亿训练样本进行大规模预训练，在真实比特币交易数据上进行零样本预测测试。

Result: 基于预测结果的简单交易策略在盈利能力评估中显示出比自相关模型显著的性能提升，在大规模预训练过程中观察到类似缩放定律的现象。

Conclusion: 通过指数级增加训练样本数量，可以在扩展预测范围的情况下实现一定水平的预测性能，如果缩放定律稳健且适用于各种混沌模型，则表明通过投入大量计算资源有可能预测近期事件。

Abstract: Time series forecasting plays a critical role in decision-making processes
across diverse fields including meteorology, traffic, electricity, economics,
finance, and so on. Especially, predicting returns on financial instruments is
a challenging problem. Some researchers have proposed time series foundation
models applicable to various forecasting tasks. Simultaneously, based on the
recognition that real-world time series exhibit chaotic properties, methods
have been developed to artificially generate synthetic chaotic time series,
construct diverse datasets and train models. In this study, we propose a
methodology for modeling financial time series by generating artificial chaotic
time series and applying resampling techniques to simulate financial time
series data, which we then use as training samples. Increasing the resampling
interval to extend predictive horizons, we conducted large-scale pre-training
using 10 billion training samples for each case. We subsequently created test
datasets for multiple timeframes using actual Bitcoin trade data and performed
zero-shot prediction without re-training the pre-trained model. The results of
evaluating the profitability of a simple trading strategy based on these
predictions demonstrated significant performance improvements over
autocorrelation models. During the large-scale pre-training process, we
observed a scaling law-like phenomenon that we can achieve predictive
performance at a certain level with extended predictive horizons for chaotic
time series by increasing the number of training samples exponentially. If this
scaling law proves robust and holds true across various chaotic models, it
suggests the potential to predict near-future events by investing substantial
computational resources. Future research should focus on further large-scale
training and verifying the applicability of this scaling law to diverse chaotic
models.

</details>


### [39] [A transformer-BiGRU-based framework with data augmentation and confident learning for network intrusion detection](https://arxiv.org/abs/2509.04925)
*Jiale Zhang,Pengfei He,Fei Li,Kewei Li,Yan Wang,Lan Huang,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: TrailGate是一个结合机器学习和深度学习的网络入侵检测框架，使用Transformer和BiGRU架构，通过高级特征选择和数据增强技术来检测常见攻击和新兴威胁。


<details>
  <summary>Details</summary>
Motivation: 当前网络流量数据激增，传统机器学习方法难以处理复杂模式、数据稀缺和类别不平衡问题，需要更强大的入侵检测解决方案。

Method: 集成Transformer和双向门控循环单元(BiGRU)架构，采用高级特征选择策略和数据增强技术，结合机器学习和深度学习技术。

Result: TrailGate能够有效识别常见攻击类型，并在检测和缓解新兴威胁方面表现出色，具有快速识别和中和来自现有范式的新威胁的独特能力。

Conclusion: 通过融合机器学习和深度学习技术，TrailGate框架为网络入侵检测提供了更强大和精确的解决方案，特别是在处理数据稀缺和类别不平衡问题上表现出优势。

Abstract: In today's fast-paced digital communication, the surge in network traffic
data and frequency demands robust and precise network intrusion solutions.
Conventional machine learning methods struggle to grapple with complex patterns
within the vast network intrusion datasets, which suffer from data scarcity and
class imbalance. As a result, we have integrated machine learning and deep
learning techniques within the network intrusion detection system to bridge
this gap. This study has developed TrailGate, a novel framework that combines
machine learning and deep learning techniques. By integrating Transformer and
Bidirectional Gated Recurrent Unit (BiGRU) architectures with advanced feature
selection strategies and supplemented by data augmentation techniques,
TrailGate can identifies common attack types and excels at detecting and
mitigating emerging threats. This algorithmic fusion excels at detecting common
and well-understood attack types and has the unique ability to swiftly identify
and neutralize emerging threats that stem from existing paradigms.

</details>


### [40] [Ontology-Aligned Embeddings for Data-Driven Labour Market Analytics](https://arxiv.org/abs/2509.04942)
*Heinke Hihn,Dennis A. V. Dittrich,Carl Jeske,Cayo Costa Sobral,Helio Pais,Timm Lochmann*

Main category: cs.LG

TL;DR: 使用Sentence-BERT模型将德语自由格式职位名称与标准职业分类体系进行语义对齐，通过近似最近邻搜索实现职业分类的自动化方法


<details>
  <summary>Details</summary>
Motivation: 解决不同来源职业数据难以互操作的长期瓶颈问题，传统手工构建的本体方法计算成本高且需要专家维护，需要更可扩展的解决方案

Method: 利用德国联邦就业局公开数据构建数据集，微调Sentence-BERT模型学习本体结构，构建相似度图结构进行近似最近邻搜索，将分类过程转化为语义搜索问题

Result: 开发了一种基于嵌入的对齐流程，能够将任意自由格式德语职位名称链接到德国职业分类(KldB)和国际教育标准分类(ISCED)两个既定本体

Conclusion: 该方法提供了更大的灵活性（如添加更多类别），并正在扩展到其他本体和多语言标题，为劳动力市场分析提供了可扩展的语义对齐解决方案

Abstract: The limited ability to reason across occupational data from different sources
is a long-standing bottleneck for data-driven labour market analytics. Previous
research has relied on hand-crafted ontologies that allow such reasoning but
are computationally expensive and require careful maintenance by human experts.
The rise of language processing machine learning models offers a scalable
alternative by learning shared semantic spaces that bridge diverse occupational
vocabularies without extensive human curation. We present an embedding-based
alignment process that links any free-form German job title to two established
ontologies - the German Klassifikation der Berufe and the International
Standard Classification of Education. Using publicly available data from the
German Federal Employment Agency, we construct a dataset to fine-tune a
Sentence-BERT model to learn the structure imposed by the ontologies. The
enriched pairs (job title, embedding) define a similarity graph structure that
we can use for efficient approximate nearest-neighbour search, allowing us to
frame the classification process as a semantic search problem. This allows for
greater flexibility, e.g., adding more classes. We discuss design decisions,
open challenges, and outline ongoing work on extending the graph with other
ontologies and multilingual titles.

</details>


### [41] [Detecting Blinks in Healthy and Parkinson's EEG: A Deep Learning Perspective](https://arxiv.org/abs/2509.04951)
*Artem Lensky,Yiding Qiu*

Main category: cs.LG

TL;DR: 这篇论文评估了多种深度学习模型在EEG信号眩眼检测中的性能，发现CNN-RNN混合模型表现最佳，在健康人群中达到95.8%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 眩眼流率及其变异性是监测认知负荷、注意力和神经疾病的重要生理标记，需要准确的眩眼检测方法。

Method: 使用1、3或5个前额叶EEG电极构建眩眼检测流程，将问题形式化为序列到序列任务，测试包括RNN、CNN、TCN、Transformer和混合模型等多种深度学习架构。

Result: CNN-RNN混合模型表现最佳，在健康人群中1、3、5额叶的准确率分别为93.8%、95.4%、95.8%，在帕金森病患者中为73.8%、75.4%、75.8%。

Conclusion: 研究证明了深度学习模型在EEG眩眼检测中的有效性，CNN-RNN混合模型为最优解决方案，为认知功能监测提供了可靠的技术支持。

Abstract: Blinks in electroencephalography (EEG) are often treated as unwanted
artifacts. However, recent studies have demonstrated that blink rate and its
variability are important physiological markers to monitor cognitive load,
attention, and potential neurological disorders. This paper addresses the
critical task of accurate blink detection by evaluating various deep learning
models for segmenting EEG signals into involuntary blinks and non-blinks. We
present a pipeline for blink detection using 1, 3, or 5 frontal EEG electrodes.
The problem is formulated as a sequence-to-sequence task and tested on various
deep learning architectures including standard recurrent neural networks,
convolutional neural networks (both standard and depth-wise), temporal
convolutional networks (TCN), transformer-based models, and hybrid
architectures. The models were trained on raw EEG signals with minimal
pre-processing. Training and testing was carried out on a public dataset of 31
subjects collected at UCSD. This dataset consisted of 15 healthy participants
and 16 patients with Parkinson's disease allowing us to verify the model's
robustness to tremor. Out of all models, CNN-RNN hybrid model consistently
outperformed other models and achieved the best blink detection accuracy of
93.8%, 95.4% and 95.8% with 1, 3, and 5 channels in the healthy cohort and
correspondingly 73.8%, 75.4% and 75.8% in patients with PD. The paper compares
neural networks for the task of segmenting EEG recordings to involuntary blinks
and no blinks allowing for computing blink rate and other statistics.

</details>


### [42] [On the Normalization of Confusion Matrices: Methods and Geometric Interpretations](https://arxiv.org/abs/2509.04959)
*Johan Erbani,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Sonia Ben Mokhtar,Diana Nurbakova*

Main category: cs.LG

TL;DR: 本文提出使用双随机归一化方法来分离混淆矩阵中的类别相似性和分布偏差因素，从而更准确地诊断模型行为。


<details>
  <summary>Details</summary>
Motivation: 混淆矩阵值同时受到类别相似性（模型混淆程度）和分布偏差（训练测试集分布不均衡）的影响，难以区分两者的独立贡献。

Method: 采用迭代比例拟合的双随机归一化方法，这是行和列归一化的泛化，能够恢复类别相似性的底层结构。

Result: 该方法成功分离了误差来源，支持更有针对性的模型改进，并在模型的内部类别表示空间中提供了几何解释。

Conclusion: 双随机归一化提供了一种有效工具来理解混淆矩阵背后的因素，为分类器诊断和改进提供了更深入的见解。

Abstract: The confusion matrix is a standard tool for evaluating classifiers by
providing insights into class-level errors. In heterogeneous settings, its
values are shaped by two main factors: class similarity -- how easily the model
confuses two classes -- and distribution bias, arising from skewed
distributions in the training and test sets. However, confusion matrix values
reflect a mix of both factors, making it difficult to disentangle their
individual contributions. To address this, we introduce bistochastic
normalization using Iterative Proportional Fitting, a generalization of row and
column normalization. Unlike standard normalizations, this method recovers the
underlying structure of class similarity. By disentangling error sources, it
enables more accurate diagnosis of model behavior and supports more targeted
improvements. We also show a correspondence between confusion matrix
normalizations and the model's internal class representations. Both standard
and bistochastic normalizations can be interpreted geometrically in this space,
offering a deeper understanding of what normalization reveals about a
classifier.

</details>


### [43] [Neuro-Spectral Architectures for Causal Physics-Informed Networks](https://arxiv.org/abs/2509.04966)
*Arthur Bizzi,Leonardo M. Moreira,Márcio Marques,Leonardo Mendonça,Christian Júnior de Oliveira,Vitor Balestro,Lucas dos Santos Fernandez,Daniel Yukimura,Pavel Petrov,João M. Pereira,Tiago Novello,Lucas Nissenbaum*

Main category: cs.LG

TL;DR: NeuSA是一种基于谱方法的物理信息神经网络，通过谱基投影和神经ODE集成来解决传统PINNs在处理复杂初值问题时的收敛困难、因果性违反和频谱偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于MLP的PINNs在处理复杂初值问题时经常无法收敛，导致解违反因果性并存在对低频分量的频谱偏差。

Method: NeuSA学习PDE在谱基上的投影，获得动力学的有限维表示，然后与适应的神经ODE集成，利用谱表示的高频分量克服频谱偏差，继承NODE的因果结构来强制因果性，并通过基于经典方法的初始化方案在目标解附近开始训练。

Result: 在线性和非线性波动方程的基准测试中，NeuSA相比其他架构表现出更强的性能，具有更快的收敛速度、改进的时间一致性和更高的预测精度。

Conclusion: NeuSA通过结合谱方法和神经ODE的优势，有效解决了传统PINNs的关键问题，为复杂PDE求解提供了更可靠的神经网络框架。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful neural
framework for solving partial differential equations (PDEs). However, standard
MLP-based PINNs often fail to converge when dealing with complex initial-value
problems, leading to solutions that violate causality and suffer from a
spectral bias towards low-frequency components. To address these issues, we
introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired
by classical spectral methods, designed to solve linear and nonlinear PDEs with
variable coefficients. NeuSA learns a projection of the underlying PDE onto a
spectral basis, leading to a finite-dimensional representation of the dynamics
which is then integrated with an adapted Neural ODE (NODE). This allows us to
overcome spectral bias, by leveraging the high-frequency components enabled by
the spectral representation; to enforce causality, by inheriting the causal
structure of NODEs, and to start training near the target solution, by means of
an initialization scheme based on classical methods. We validate NeuSA on
canonical benchmarks for linear and nonlinear wave equations, demonstrating
strong performance as compared to other architectures, with faster convergence,
improved temporal consistency and superior predictive accuracy. Code and
pretrained models will be released.

</details>


### [44] [Topology-Aware Graph Reinforcement Learning for Dynamic Routing in Cloud Networks](https://arxiv.org/abs/2509.04973)
*Yuxi Wang,Heyao Liu,Guanzi Yao,Nyutian Long,Yue Kang*

Main category: cs.LG

TL;DR: 提出拓扑感知图强化学习方法解决云服务器路由策略优化问题，通过结构感知状态编码和策略自适应图更新机制，在动态拓扑下实现高效稳定的路由决策。


<details>
  <summary>Details</summary>
Motivation: 解决云服务器环境中动态拓扑下的路由策略优化问题，传统方法存在决策不稳定和结构感知不足的挑战，需要能够适应拓扑变化的高效路由方案。

Method: 构建统一框架，包含结构感知状态编码(SASE)模块和多层图卷积与结构位置嵌入建模节点状态，以及策略自适应图更新(PAGU)模块根据策略行为变化和奖励反馈调整图结构。

Result: 在真实GEANT拓扑数据集上实验表明，该方法在吞吐量、延迟控制和链路平衡等多个性能指标上优于现有图强化学习模型，实现了动态复杂云网络中的高效鲁棒路由。

Conclusion: 该方法通过拓扑感知的图强化学习框架，有效解决了动态云网络环境中的路由优化问题，具有优越的性能和鲁棒性。

Abstract: This paper proposes a topology-aware graph reinforcement learning approach to
address the routing policy optimization problem in cloud server environments.
The method builds a unified framework for state representation and structural
evolution by integrating a Structure-Aware State Encoding (SASE) module and a
Policy-Adaptive Graph Update (PAGU) mechanism. It aims to tackle the challenges
of decision instability and insufficient structural awareness under dynamic
topologies. The SASE module models node states through multi-layer graph
convolution and structural positional embeddings, capturing high-order
dependencies in the communication topology and enhancing the expressiveness of
state representations. The PAGU module adjusts the graph structure based on
policy behavior shifts and reward feedback, enabling adaptive structural
updates in dynamic environments. Experiments are conducted on the real-world
GEANT topology dataset, where the model is systematically evaluated against
several representative baselines in terms of throughput, latency control, and
link balance. Additional experiments, including hyperparameter sensitivity,
graph sparsity perturbation, and node feature dimensionality variation, further
explore the impact of structure modeling and graph updates on model stability
and decision quality. Results show that the proposed method outperforms
existing graph reinforcement learning models across multiple performance
metrics, achieving efficient and robust routing in dynamic and complex cloud
networks.

</details>


### [45] [Adapt in the Wild: Test-Time Entropy Minimization with Sharpness and Feature Regularization](https://arxiv.org/abs/2509.04977)
*Shuaicheng Niu,Guohao Chen,Deyu Chen,Yifan Zhang,Jiaxiang Wu,Zhiquan Wen,Yaofo Chen,Peilin Zhao,Chunyan Miao,Mingkui Tan*

Main category: cs.LG

TL;DR: 这篇论文研究测试时适配(TTA)的稳定性问题，发现Batch Norm层是影响稳定性的关键因素，并提出SAR和SAR^2方法来解决模型崩溃问题，在各种野外测试场景下实现更稳定的适配性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在真实部署中遇到的主要问题：在混合分布偏移、小批次大小和在线不平衡标签分布偏移的情况下，TTA可能失效或甚至对模型性能造成负面影响。

Method: 首先分析发现Batch Norm层是影响TTA稳定性的关键因素，使用批次无关的正规化层更稳定。提出SAR方法：1)删除具有大梯度的噪声样本 2)鼓励模型权重进入平坦最小值。在SAR基础上提出SAR^2：1)重复性正则化器减少特征相关性 2)不公平正则化器减少分类偏见。

Result: 方法在各种野外测试场景下表现出更好的稳定性和计算效率，较优于现有方法。

Conclusion: 论文通过分析TTA不稳定的根本原因，提出了有效的解决方案SAR和SAR^2，能够在复杂的真实测试环境下实现稳定的测试时适配，为TTA方法的实际部署提供了重要支撑。

Abstract: Test-time adaptation (TTA) may fail to improve or even harm the model
performance when test data have: 1) mixed distribution shifts, 2) small batch
sizes, 3) online imbalanced label distribution shifts. This is often a key
obstacle preventing existing TTA methods from being deployed in the real world.
In this paper, we investigate the unstable reasons and find that the batch norm
layer is a crucial factor hindering TTA stability. Conversely, TTA can perform
more stably with batch-agnostic norm layers, i.e., group or layer norm.
However, we observe that TTA with group and layer norms does not always succeed
and still suffers many failure cases, i.e., the model collapses into trivial
solutions by assigning the same class label for all samples. By digging into
this, we find that, during the collapse process: 1) the model gradients often
undergo an initial explosion followed by rapid degradation, suggesting that
certain noisy test samples with large gradients may disrupt adaptation; and 2)
the model representations tend to exhibit high correlations and classification
bias. To address this, we first propose a sharpness-aware and reliable entropy
minimization method, called SAR, for stabilizing TTA from two aspects: 1)
remove partial noisy samples with large gradients, 2) encourage model weights
to go to a flat minimum so that the model is robust to the remaining noisy
samples. Based on SAR, we further introduce SAR^2 to prevent representation
collapse with two regularizers: 1) a redundancy regularizer to reduce
inter-dimensional correlations among centroid-invariant features; and 2) an
inequity regularizer to maximize the prediction entropy of a prototype
centroid, thereby penalizing biased representations toward any specific class.
Promising results demonstrate that our methods perform more stably over prior
methods and are computationally efficient under the above wild test scenarios.

</details>


### [46] [Directed Evolution of Proteins via Bayesian Optimization in Embedding Space](https://arxiv.org/abs/2509.04998)
*Matouš Soldát,Jiří Kléma*

Main category: cs.LG

TL;DR: 一种结合贝叶斯优化和预训练蛋白语言模型的新题蛋白导向进化方法，通过更好的序列嵌入表示提高选择效率，在相同次数的细胞屏幕中获得更优的结果


<details>
  <summary>Details</summary>
Motivation: 导向进化是一种迭代的实验室过程，需要细胞屏幕评估蛋白变异体的性能，这个过程费时费力。机器学习方法可以帮助选择信息量丰富或有前景的变异体进行屏幕，提高效率和减少屏幕次数

Method: 结合贝叶斯优化与预训练蛋白语言模型提取的信息化蛋白变异体表示，利用序列嵌入表示来改善选择效果

Result: 新的序列嵌入表示显著提高了贝叶斯优化的性能，在相同次数的屏幕中获得更好的结果，超越了现有的以回归为目标的机器学习辅助导向进化方法

Conclusion: 该方法通过结合贝叶斯优化和蛋白语言模型的嵌入表示，有效提高了导向进化的效率和效果，为蛋白工程提供了更优科学的工具

Abstract: Directed evolution is an iterative laboratory process of designing proteins
with improved function by iteratively synthesizing new protein variants and
evaluating their desired property with expensive and time-consuming biochemical
screening. Machine learning methods can help select informative or promising
variants for screening to increase their quality and reduce the amount of
necessary screening. In this paper, we present a novel method for
machine-learning-assisted directed evolution of proteins which combines
Bayesian optimization with informative representation of protein variants
extracted from a pre-trained protein language model. We demonstrate that the
new representation based on the sequence embeddings significantly improves the
performance of Bayesian optimization yielding better results with the same
number of conducted screening in total. At the same time, our method
outperforms the state-of-the-art machine-learning-assisted directed evolution
methods with regression objective.

</details>


### [47] [Depth-Aware Initialization for Stable and Efficient Neural Network Training](https://arxiv.org/abs/2509.05018)
*Vijay Pandey*

Main category: cs.LG

TL;DR: 这篇论文对深度网络初始化方法进行了综合研究，提出了一种新的初始化方案，能够根据每个层的深度信息灵活增加网络的方差，在深度网络中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的初始化方法如Glorot、He、正交矩阵等方案中，有些没有考虑网络深度信息，而在深度网络中保持激活和梯度传播的单位方差假设并不能进行良好的初始化。

Method: 提出了一种新的初始化方案，结合了每个层的深度信息以及整个网络的深度，能够灵活地增加网络从第一层到最后一层激活的方差。

Result: 实验结果显示，所提出的方法在深度网络中表现出更好的性能，超越了现有的初始化方案。

Conclusion: 对于深度网络，需要根据每个层的深度信息灵活调整方差，而不是简单地保持单位方差。提出的方法能够有效地提高深度网络的初始化效果。

Abstract: In past few years, various initialization schemes have been proposed. These
schemes are glorot initialization, He initialization, initialization using
orthogonal matrix, random walk method for initialization. Some of these methods
stress on keeping unit variance of activation and gradient propagation through
the network layer. Few of these methods are independent of the depth
information while some methods has considered the total network depth for
better initialization. In this paper, comprehensive study has been done where
depth information of each layer as well as total network is incorporated for
better initialization scheme. It has also been studied that for deeper networks
theoretical assumption of unit variance throughout the network does not perform
well. It requires the need to increase the variance of the network from first
layer activation to last layer activation. We proposed a novel way to increase
the variance of the network in flexible manner, which incorporates the
information of each layer depth. Experiments shows that proposed method
performs better than the existing initialization scheme.

</details>


### [48] [MultiSurv: A Multimodal Deep Survival Framework for Prostrate and Bladder Cancer](https://arxiv.org/abs/2509.05037)
*Noorul Wahab,Ethar Alzaid,Jiaqi Lv,Adam Shephard,Shan E Ahmed Raza*

Main category: cs.LG

TL;DR: MultiSurv是一个多模态深度生存模型，整合临床、MRI、RNA-seq和病理数据，用于前列腺癌和膀胱癌的复发时间预测，在CHIMERA挑战赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确预测癌症患者的时间-事件结果对治疗规划和患者管理至关重要，需要整合多模态异构数据来捕捉互补的预后信号。

Method: 使用DeepHit结合投影层和跨模态注意力机制的多模态深度生存模型，整合临床、MRI、RNA-seq和全切片病理特征。

Result: 在前列腺癌任务中C-index达0.843（交叉验证）和0.818（开发集）；在膀胱癌任务中C-index为0.662（交叉验证）和0.457（开发集）。

Conclusion: 多模态整合与深度生存学习为个性化风险分层提供了有前景的途径，该框架可广泛应用于涉及异构生物医学数据的生存预测任务。

Abstract: Accurate prediction of time-to-event outcomes is a central challenge in
oncology, with significant implications for treatment planning and patient
management. In this work, we present MultiSurv, a multimodal deep survival
model utilising DeepHit with a projection layer and inter-modality
cross-attention, which integrates heterogeneous patient data, including
clinical, MRI, RNA-seq and whole-slide pathology features. The model is
designed to capture complementary prognostic signals across modalities and
estimate individualised time-to-biochemical recurrence in prostate cancer and
time-to-cancer recurrence in bladder cancer. Our approach was evaluated in the
context of the CHIMERA Grand Challenge, across two of the three provided tasks.
For Task 1 (prostate cancer bio-chemical recurrence prediction), the proposed
framework achieved a concordance index (C-index) of 0.843 on 5-folds
cross-validation and 0.818 on CHIMERA development set, demonstrating robust
discriminatory ability. For Task 3 (bladder cancer recurrence prediction), the
model obtained a C-index of 0.662 on 5-folds cross-validation and 0.457 on
development set, highlighting its adaptability and potential for clinical
translation. These results suggest that leveraging multimodal integration with
deep survival learning provides a promising pathway toward personalised risk
stratification in prostate and bladder cancer. Beyond the challenge setting,
our framework is broadly applicable to survival prediction tasks involving
heterogeneous biomedical data.

</details>


### [49] [Recurrent State Encoders for Efficient Neural Combinatorial Optimization](https://arxiv.org/abs/2509.05084)
*Tim Dernedde,Daniela Thyssens,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: 通过训练递归编码器来重用之前计算的状态嵌入，提高神经组合优化的计算效率，在保持或提升性能的同时显著减少延迟。


<details>
  <summary>Details</summary>
Motivation: 直观到神经组合优化中状态在连续步骤间的变化很小，应该能够重用之前步骤的计算结果来提高效率。

Method: 训练一个递归编码器，计算状态嵌入时不仅基于当前状态，还考虑前一步的嵌入结果，通过减少网络层数来提高效率。

Result: 递归编码器在旅行商问题、容量车辆路径问题和定向掛问题上都能达到等效或更好的性能，尽管网络层数减少3倍，延迟显著改善。

Conclusion: 递归编码器通过重用计算能够在保持性能的同时大幅提高效率，具有实际应用价值。

Abstract: The primary paradigm in Neural Combinatorial Optimization (NCO) are
construction methods, where a neural network is trained to sequentially add one
solution component at a time until a complete solution is constructed. We
observe that the typical changes to the state between two steps are small,
since usually only the node that gets added to the solution is removed from the
state. An efficient model should be able to reuse computation done in prior
steps. To that end, we propose to train a recurrent encoder that computes the
state embeddings not only based on the state but also the embeddings of the
step before. We show that the recurrent encoder can achieve equivalent or
better performance than a non-recurrent encoder even if it consists of
$3\times$ fewer layers, thus significantly improving on latency. We demonstrate
our findings on three different problems: the Traveling Salesman Problem (TSP),
the Capacitated Vehicle Routing Problem (CVRP), and the Orienteering Problem
(OP) and integrate the models into a large neighborhood search algorithm, to
showcase the practical relevance of our findings.

</details>


### [50] [HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions](https://arxiv.org/abs/2509.05117)
*Rafael Bischof,Michal Piovarči,Michael A. Kraus,Siddhartha Mishra,Bernd Bickel*

Main category: cs.LG

TL;DR: HyPINO是一个多物理神经算子，通过超网络和混合监督实现参数化PDE的零样本泛化，无需任务特定微调，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要针对每个PDE问题进行特定微调的问题，开发一个能够零样本泛化到广泛参数化PDE类的通用神经算子。

Method: 结合Swin Transformer超网络和混合监督：使用MMS生成的解析解作为标注数据，以及物理信息目标优化的无标注样本。模型将PDE参数映射到目标PINNs，并引入迭代精炼过程生成集成解。

Result: 在7个PINN文献基准问题上实现强零样本精度，优于U-Nets、Poseidon和PINO。迭代精炼过程在6个基准上实现误差逐步降低，最佳情况下平均L2损失提升100倍以上。HyPINO初始化的PINNs在微调时收敛更快且误差更低。

Conclusion: 该方法为扩展神经算子解决更复杂、非线性和高维PDE问题提供了可扩展的基础，显著提高了精度并降低了计算成本。

Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot
generalization across a broad class of parametric PDEs without requiring
task-specific fine-tuning. Our approach combines a Swin Transformer-based
hypernetwork with mixed supervision: (i) labeled data from analytical solutions
generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled
samples optimized using physics-informed objectives. The model maps PDE
parametrizations to target Physics-Informed Neural Networks (PINNs) and can
handle linear elliptic, hyperbolic, and parabolic equations in two dimensions
with varying source terms, geometries, and mixed Dirichlet/Neumann boundary
conditions, including interior boundaries. HyPINO achieves strong zero-shot
accuracy on seven benchmark problems from PINN literature, outperforming
U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we
introduce an iterative refinement procedure that compares the physics of the
generated PINN to the requested PDE and uses the discrepancy to generate a
"delta" PINN. Summing their contributions and repeating this process forms an
ensemble whose combined solution progressively reduces the error on six
benchmarks and achieves over 100x gain in average $L_2$ loss in the best case,
while retaining forward-only inference. Additionally, we evaluate the
fine-tuning behavior of PINNs initialized by HyPINO and show that they converge
faster and to lower final error than both randomly initialized and
Reptile-meta-learned PINNs on five benchmarks, performing on par on the
remaining two. Our results highlight the potential of this scalable approach as
a foundation for extending neural operators toward solving increasingly
complex, nonlinear, and high-dimensional PDE problems with significantly
improved accuracy and reduced computational cost.

</details>


### [51] [Should We Always Train Models on Fine-Grained Classes?](https://arxiv.org/abs/2509.05130)
*Davide Pirovano,Federico Milanesio,Michele Caselle,Piero Fariselli,Matteo Osella*

Main category: cs.LG

TL;DR: 细粒度标签训练并不总能提升分类性能，其效果取决于数据几何结构、标签层次关系、数据集大小和模型容量等因素


<details>
  <summary>Details</summary>
Motivation: 虽然经验证据表明使用更细粒度的标签进行训练可以提升分类性能，但这种现象的普遍性和根本原因尚不清楚，需要系统研究

Method: 使用真实和合成数据集，分析数据几何结构、标签层次关系与细粒度训练效果之间的关系，并考察数据集大小和模型容量的影响

Result: 细粒度标签训练并非普遍有效，其效果高度依赖于数据几何结构与标签层次的关系，以及数据集大小和模型容量等条件

Conclusion: 细粒度标签训练的效果是有条件的，需要根据具体的数据特征和任务要求来评估是否采用这种策略，不能盲目应用

Abstract: In classification problems, models must predict a class label based on the
input data features. However, class labels are organized hierarchically in many
datasets. While a classification task is often defined at a specific level of
this hierarchy, training can utilize a finer granularity of labels. Empirical
evidence suggests that such fine-grained training can enhance performance. In
this work, we investigate the generality of this observation and explore its
underlying causes using both real and synthetic datasets. We show that training
on fine-grained labels does not universally improve classification accuracy.
Instead, the effectiveness of this strategy depends critically on the geometric
structure of the data and its relations with the label hierarchy. Additionally,
factors such as dataset size and model capacity significantly influence whether
fine-grained labels provide a performance benefit.

</details>


### [52] [On the Learnability of Distribution Classes with Adaptive Adversaries](https://arxiv.org/abs/2509.05137)
*Tosca Lechner,Alex Bie,Gautam Kamath*

Main category: cs.LG

TL;DR: 本文研究了在自适应对手存在下的分布类可学习性问题，证明了相对于加性自适应对手的可学习性比相对于加性非自适应对手的可学习性条件更严格。


<details>
  <summary>Details</summary>
Motivation: 研究在自适应对手（能够拦截学习者请求的样本并基于完整知识进行修改）存在下的学习问题，与传统的非自适应对手（只能修改底层分布但保持样本的i.i.d.性质）形成对比。

Method: 提出了针对自适应对手的可学习性的一般概念，考虑了对手的预算限制，并通过理论分析比较了不同类型对手下的学习条件。

Result: 证明了相对于加性自适应对手的可学习性是一个比相对于加性非自适应对手的可学习性更强的条件，表明自适应对手对学习过程构成更大的挑战。

Conclusion: 自适应对手的存在显著改变了分布学习的理论框架，需要开发新的学习算法和分析技术来应对这种更强大的对抗性设置。

Abstract: We consider the question of learnability of distribution classes in the
presence of adaptive adversaries -- that is, adversaries capable of
intercepting the samples requested by a learner and applying manipulations with
full knowledge of the samples before passing it on to the learner. This stands
in contrast to oblivious adversaries, who can only modify the underlying
distribution the samples come from but not their i.i.d.\ nature. We formulate a
general notion of learnability with respect to adaptive adversaries, taking
into account the budget of the adversary. We show that learnability with
respect to additive adaptive adversaries is a strictly stronger condition than
learnability with respect to additive oblivious adversaries.

</details>


### [53] [Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights](https://arxiv.org/abs/2509.05142)
*Cosmin-Andrei Hatfaludi,Alex Serban*

Main category: cs.LG

TL;DR: 这是一份关于联邦学习与基础模型融合的综述性论文，通过新分类法对技术方法进行系统分类和对比分析，以医疗健康领域为案例研究。


<details>
  <summary>Details</summary>
Motivation: 解决复杂基础模型训练资源扩展和私有数据集成的需求，探索联邦学习与基础模型的融合可能性。

Method: 采用文献调研方法，从4,200多篇文章中筛选出250篇详细评估，构建以发展生命周期阶段为基础的新分类法，对42种方法进行复杂度、效率和可扩展性对比。

Result: 形成了一个自含的领域概览，提供了技术方法的系统化分类和实践指南，特别是在医疗健康领域的应用演示。

Conclusion: 该研究不仅总结了联邦学习与基础模型融合的现状，还为实践应用提供了有价值的见解和指导。

Abstract: Federated learning has the potential to unlock siloed data and distributed
resources by enabling collaborative model training without sharing private
data. As more complex foundational models gain widespread use, the need to
expand training resources and integrate privately owned data grows as well. In
this article, we explore the intersection of federated learning and
foundational models, aiming to identify, categorize, and characterize technical
methods that integrate the two paradigms. As a unified survey is currently
unavailable, we present a literature survey structured around a novel taxonomy
that follows the development life-cycle stages, along with a technical
comparison of available methods. Additionally, we provide practical insights
and guidelines for implementing and evolving these methods, with a specific
focus on the healthcare domain as a case study, where the potential impact of
federated learning and foundational models is considered significant. Our
survey covers multiple intersecting topics, including but not limited to
federated learning, self-supervised learning, fine-tuning, distillation, and
transfer learning. Initially, we retrieved and reviewed a set of over 4,200
articles. This collection was narrowed to more than 250 thoroughly reviewed
articles through inclusion criteria, featuring 42 unique methods. The methods
were used to construct the taxonomy and enabled their comparison based on
complexity, efficiency, and scalability. We present these results as a
self-contained overview that not only summarizes the state of the field but
also provides insights into the practical aspects of adopting, evolving, and
integrating foundational models with federated learning.

</details>


### [54] [KVCompose: Efficient Structured KV Cache Compression with Composite Tokens](https://arxiv.org/abs/2509.05165)
*Dmitry Akulov,Mohamed Sana,Antonio De Domenico,Tareq Si Salem,Nicola Piovesan,Fadhel Ayed*

Main category: cs.LG

TL;DR: 基于注意力导向的层适应性复合token缩减KV缓存方法，在保持准确性的同时大幅减少内存占用，且与标准推理引擎完全兼容


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中，KV缓存大小线性增长，成为主要性能瓶颈。现有压缩方法或过于缺乏灵活性，或打破张量布局，或需要专门计算内核

Method: 通过注意力分数评估token重要性，独立选择每个注意头的关键token，将其对齐为遵循统一缓存结构的复合token。全局分配机制根据层次调整保留定额，为含有信息token的层分配更多容量

Result: 方法实现了显著的内存减少，同时保持了准确性，在结构化和半结构化方法中都表现更优

Conclusion: 该方法提供了一种简单有效的KV缓存压缩解决方案，在保持高准确性的同时大幅降低内存占用，且完全兼容现有标准推理流程，为高效长上下文LLM部署提供了实用可扩展的解决方案

Abstract: Large language models (LLMs) rely on key-value (KV) caches for efficient
autoregressive decoding; however, cache size grows linearly with context length
and model depth, becoming a major bottleneck in long-context inference. Prior
KV cache compression methods either enforce rigid heuristics, disrupt tensor
layouts with per-attention-head variability, or require specialized compute
kernels.
  We propose a simple, yet effective, KV cache compression framework based on
attention-guided, layer-adaptive composite tokens. Our method aggregates
attention scores to estimate token importance, selects head-specific tokens
independently, and aligns them into composite tokens that respect the uniform
cache structure required by existing inference engines. A global allocation
mechanism further adapts retention budgets across layers, assigning more
capacity to layers with informative tokens. This approach achieves significant
memory reduction while preserving accuracy, consistently outperforming prior
structured and semi-structured methods. Crucially, our approach remains fully
compatible with standard inference pipelines, offering a practical and scalable
solution for efficient long-context LLM deployment.

</details>


### [55] [Accuracy-Constrained CNN Pruning for Efficient and Reliable EEG-Based Seizure Detection](https://arxiv.org/abs/2509.05190)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: 该研究提出了一种轻量化的1D CNN模型，通过结构化剪枝技术去除50%的卷积核，在保持预测能力的同时提高了EEG癫痫检测的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在生物医学信号处理中表现优异，但其计算资源需求大，在实时检测或资源受限环境中面临挑战。研究旨在开发轻量化模型以适应资源有限的环境。

Method: 使用结构化剪枝方法，基于卷积核对模型预测的重要性，移除50%的卷积核。同时采用温和的早停策略来防止过拟合。

Result: 剪枝后模型权重和内存减少50%，但准确率从92.78%略微提升至92.87%，macro-F1分数从0.8686提升至0.8707，保持了预测能力。

Conclusion: 结构化剪枝能有效去除冗余，提高泛化能力，结合温和早停策略，为资源受限环境下的癫痫检测提供了高效可靠的解决方案。

Abstract: Deep learning models, especially convolutional neural networks (CNNs), have
shown considerable promise for biomedical signals such as EEG-based seizure
detection. However, these models come with challenges, primarily due to their
size and compute requirements in environments where real-time detection or
limited resources are available. In this study, we present a lightweight
one-dimensional CNN model with structured pruning to improve efficiency and
reliability. The model was trained with mild early stopping to address possible
overfitting, achieving an accuracy of 92.78% and a macro-F1 score of 0.8686.
Structured pruning of the baseline CNN involved removing 50% of the
convolutional kernels based on their importance to model predictions.
Surprisingly, after pruning the weights and memory by 50%, the new network was
still able to maintain predictive capabilities, while modestly increasing
precision to 92.87% and improving the macro-F1 score to 0.8707. Overall, we
present a convincing case that structured pruning removes redundancy, improves
generalization, and, in combination with mild early stopping, achieves a
promising way forward to improve seizure detection efficiency and reliability,
which is clear motivation for resource-limited settings.

</details>


### [56] [Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning](https://arxiv.org/abs/2509.05193)
*Bastien Dubail,Stefan Stojanovic,Alexandre Proutière*

Main category: cs.LG

TL;DR: 本文挑战了强化学习中后继度量具有低秩结构的假设，发现移位后继度量才具有低秩特性，并提供了有限样本性能保证和理论分析。


<details>
  <summary>Details</summary>
Motivation: 现代强化学习算法通常假设后继度量具有低秩结构，但本文发现这种假设不成立，需要研究移位后的后继度量才能获得有效的低秩表示。

Method: 提出移位后继度量的概念，推导Type II Poincaré不等式来量化所需移位量，建立移位量与动力系统局部混合特性的联系，并通过实验验证理论发现。

Result: 证明了移位后继度量具有低秩结构，提供了有限样本估计的性能保证，发现所需移位量通常很小且取决于高阶奇异值的衰减。

Conclusion: 移位操作能有效改善目标条件强化学习的性能，为低秩近似提供了理论基础和实践指导，揭示了谱可恢复性在估计误差中的关键作用。

Abstract: Low-rank structure is a common implicit assumption in many modern
reinforcement learning (RL) algorithms. For instance, reward-free and
goal-conditioned RL methods often presume that the successor measure admits a
low-rank representation. In this work, we challenge this assumption by first
remarking that the successor measure itself is not low-rank. Instead, we
demonstrate that a low-rank structure naturally emerges in the shifted
successor measure, which captures the system dynamics after bypassing a few
initial transitions. We provide finite-sample performance guarantees for the
entry-wise estimation of a low-rank approximation of the shifted successor
measure from sampled entries. Our analysis reveals that both the approximation
and estimation errors are primarily governed by the so-called spectral
recoverability of the corresponding matrix. To bound this parameter, we derive
a new class of functional inequalities for Markov chains that we call Type II
Poincar\'e inequalities and from which we can quantify the amount of shift
needed for effective low-rank approximation and estimation. This analysis shows
in particular that the required shift depends on decay of the high-order
singular values of the shifted successor measure and is hence typically small
in practice. Additionally, we establish a connection between the necessary
shift and the local mixing properties of the underlying dynamical system, which
provides a natural way of selecting the shift. Finally, we validate our
theoretical findings with experiments, and demonstrate that shifting the
successor measure indeed leads to improved performance in goal-conditioned RL.

</details>


### [57] [RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks](https://arxiv.org/abs/2509.05207)
*Arefin Niam,Tevfik Kosar,M S Q Zulkar Nine*

Main category: cs.LG

TL;DR: RapidGNN是一个分布式GNN训练框架，通过确定性采样调度实现高效缓存和远程特征预取，相比基线方法提升训练吞吐量2.46-3倍，减少远程特征获取9.7-15.4倍，并实现近线性扩展和更高能效。


<details>
  <summary>Details</summary>
Motivation: 大规模图上的分布式GNN训练面临通信开销大的挑战，传统采样方法虽然减轻了计算负载但通信问题仍未解决。

Method: 采用确定性采样调度机制，支持高效的缓存构建和远程特征预取。

Result: 在基准图数据集上，训练吞吐量平均提升2.46-3倍，远程特征获取减少9.7-15.4倍，实现近线性扩展，CPU和GPU能效分别提升44%和32%。

Conclusion: RapidGNN有效解决了分布式GNN训练中的通信瓶颈问题，实现了显著的性能提升和能效改进。

Abstract: Graph Neural Networks (GNNs) have become popular across a diverse set of
tasks in exploring structural relationships between entities. However, due to
the highly connected structure of the datasets, distributed training of GNNs on
large-scale graphs poses significant challenges. Traditional sampling-based
approaches mitigate the computational loads, yet the communication overhead
remains a challenge. This paper presents RapidGNN, a distributed GNN training
framework with deterministic sampling-based scheduling to enable efficient
cache construction and prefetching of remote features. Evaluation on benchmark
graph datasets demonstrates RapidGNN's effectiveness across different scales
and topologies. RapidGNN improves end-to-end training throughput by 2.46x to
3.00x on average over baseline methods across the benchmark datasets, while
cutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further
demonstrates near-linear scalability with an increasing number of computing
units efficiently. Furthermore, it achieves increased energy efficiency over
the baseline methods for both CPU and GPU by 44% and 32%, respectively.

</details>


### [58] [An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data](https://arxiv.org/abs/2509.05213)
*Jiaojiao Zhang,Yuqi Xu,Kun Yuan*

Main category: cs.LG

TL;DR: FedSub是一种高效的子空间联邦学习算法，通过子空间投影和低维对偶变量减少客户偏移，同时降低通信、计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在大规模深度神经网络中的关键挑战，包括客户数据异质性导致的客户偏移问题，以及高通信、计算和内存成本。

Method: 使用子空间投影技术，保证每个客户的本地更新都在低维子空间内进行，并结合低维对偶变量来减缓客户偏移效应。

Result: 提供了收敛性分析，揭示了步长和子空间投影矩阵等关键因素对收敛的影响，实验结果证明了算法的高效性。

Conclusion: FedSub算法能够有效解决联邦学习中的客户偏移问题，同时显著降低了通信、计算和内存成本，适用于大规模深度神经网络的联邦学习场景。

Abstract: This work addresses the key challenges of applying federated learning to
large-scale deep neural networks, particularly the issue of client drift due to
data heterogeneity across clients and the high costs of communication,
computation, and memory. We propose FedSub, an efficient subspace algorithm for
federated learning on heterogeneous data. Specifically, FedSub utilizes
subspace projection to guarantee local updates of each client within
low-dimensional subspaces, thereby reducing communication, computation, and
memory costs. Additionally, it incorporates low-dimensional dual variables to
mitigate client drift. We provide convergence analysis that reveals the impact
of key factors such as step size and subspace projection matrices on
convergence. Experimental results demonstrate its efficiency.

</details>


### [59] [Deep Learning-Enhanced for Amine Emission Monitoring and Performance Analysis in Industrial Carbon Capture Plants](https://arxiv.org/abs/2509.05241)
*Lokendra Poudel,David Tincher,Duy-Nhat Phan,Rahul Bhowmik*

Main category: cs.LG

TL;DR: 本文使用四种深度学习模型（LSTM、堆叠LSTM、双向LSTM、卷积LSTM）预测胺基炳捕获系统中的胺排放和关键性能参数，预测准确度超过99%，并通过因果分析确定了优化操作的关键参数。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够预测和监控胺基炳捕获系统中胺排放物和系统性能的数据驱动深度学习模型，以支持炳捕获操作的优化和环境影响减缓。

Method: 使用蒙格斯特技术中心的CESAR1溶剂运行数据，开发四种LSTM类深度学习模型来捕捉时间依赖性过程行为，包括AMP和喵喵唑排放预测以及四个关键性能参数的预测。进行因果影响分析，系统性批勘8个输入变量来评估对排放和系统性能的影响。

Result: 模型实现了超过99%的高预测准确度，能够有效跟踪稳态趋势和突变波动。因果分析显示，调整稀溶剂温度和水洗条件等特定操作参数可显著降低胺排放并提升系统性能。

Conclusion: 这项研究展示了机器学习不仅作为预测工具，而是作为炳捕获操作的决策支持系统。开发的ML框架为实时监控、场景测试和操作优化提供了实用途径，是向智能化、数据驱动控制策略的重要一步，有助于提高炳捕获和存储技术的效率、稳定性和可持续性。

Abstract: We present data driven deep learning models for forecasting and monitoring
amine emissions and key performance parameters in amine-based post-combustion
carbon capture systems. Using operational data from the CESAR1 solvent campaign
at Technology Center Mongstad, four DL architectures such as Basic Long
Short-Term Memory (LSTM), Stacked LSTM, Bi-directional LSTM, and Convolutional
LSTM were developed to capture time-dependent process behavior. For emission
prediction, models were designed for 2-amino-2-methyl-1-propanol (AMP) and
Piperazine emissions measured via FTIR and IMR-MS methods. System performance
models target four critical parameters: CO$_2$ product flow, absorber outlet
temperature, depleted flue gas outlet temperature, and RFCC stripper bottom
temperature. These models achieved high predictive accuracy exceeding 99% and
effectively tracked both steady trends and abrupt fluctuations. Additionally,
we conducted causal impact analysis to evaluate how operational variables
influence emissions and system performance. Eight input variables were
systematically perturbed within $\pm$20% of nominal values to simulate
deviations and assess their impact. This analysis revealed that adjusting
specific operational parameters, such as lean solvent temperature and water
wash conditions, can significantly reduce amine emissions and enhance system
performance. This study highlights ML not only as a predictive tool but also as
a decision support system for optimizing carbon capture operations under steady
state and dynamic conditions. By enabling real time monitoring, scenario
testing, and operational optimization, the developed ML framework offers a
practical pathway for mitigating environmental impacts. This work represents a
step toward intelligent, data-driven control strategies that enhance the
efficiency, stability, and sustainability of carbon capture and storage
technologies.

</details>


### [60] [Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks](https://arxiv.org/abs/2509.05273)
*Jason Gardner,Ayan Dutta,Swapnoneel Roy,O. Patrick Kreidl,Ladislau Boloni*

Main category: cs.LG

TL;DR: 本文系统性地比较了7种主流深度强化学习算法的能耗、碳排放和成本，发现不同算法间存在显著差异，最高可节省24%能耗和68%成本，为可持续DRL发展提供实践指导


<details>
  <summary>Details</summary>
Motivation: 深度强化学习计算需求激增带来环境和经济成本问题，但现有研究主要关注学习性能，对能耗、碳排放和成本等可持续性指标缺乏系统评估

Method: 使用Stable Baselines实现7种SOTA DRL算法（DQN、TRPO、A2C、ARS、PPO、RecurrentPPO、QR-DQN），在10个Atari 2600游戏上各训练100万步，实时测量功耗并估算能耗、CO2排放和电力成本

Result: 算法间能效和训练成本差异显著：ARS比DQN节能24%；QR-DQN比RecurrentPPO减少近68% CO2排放和成本；在保持学习性能的同时可实现环境和经济影响的优化

Conclusion: 研究为开发能源意识和成本高效的DRL实践提供可行见解，为未来算法设计和评估纳入可持续性考量奠定基础，强调算法选择可在不牺牲性能的情况下减少环境影响

Abstract: The growing computational demands of deep reinforcement learning (DRL) have
raised concerns about the environmental and economic costs of training
large-scale models. While algorithmic efficiency in terms of learning
performance has been extensively studied, the energy requirements, greenhouse
gas emissions, and monetary costs of DRL algorithms remain largely unexplored.
In this work, we present a systematic benchmarking study of the energy
consumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,
ARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each
algorithm was trained for one million steps each on ten Atari 2600 games, and
power consumption was measured in real-time to estimate total energy usage,
CO2-Equivalent emissions, and electricity cost based on the U.S. national
average electricity price. Our results reveal substantial variation in energy
efficiency and training cost across algorithms, with some achieving comparable
performance while consuming up to 24% less energy (ARS vs. DQN), emitting
nearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.
RecurrentPPO) than less efficient counterparts. We further analyze the
trade-offs between learning performance, training time, energy use, and
financial cost, highlighting cases where algorithmic choices can mitigate
environmental and economic impact without sacrificing learning performance.
This study provides actionable insights for developing energy-aware and
cost-efficient DRL practices and establishes a foundation for incorporating
sustainability considerations into future algorithmic design and evaluation.

</details>


### [61] [SpikingBrain Technical Report: Spiking Brain-inspired Large Models](https://arxiv.org/abs/2509.05276)
*Yuqi Pan,Yupeng Feng,Jinghao Zhuang,Siyu Ding,Zehao Liu,Bohan Sun,Yuhong Chou,Han Xu,Xuerui Qiu,Anlin Deng,Anjie Hu,Peng Zhou,Man Yao,Jibin Wu,Jian Yang,Guoliang Sun,Bo Xu,Guoqi Li*

Main category: cs.LG

TL;DR: SpikingBrain是一个基于脉冲神经元的脑启发模型家族，通过线性/混合线性注意力架构、算法优化和系统工程技术，在非NVIDIA平台上实现了高效的长上下文训练和推理，性能媲美Transformer模型但计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决主流Transformer模型在长序列处理时训练计算量二次增长、推理内存线性增长的问题，以及在非NVIDIA平台上构建大模型的稳定高效训练挑战。

Method: 采用线性/混合线性注意力架构配合自适应脉冲神经元；开发高效的基于转换的训练流水线和专用脉冲编码框架；针对MetaX硬件定制训练框架、算子库和并行策略。

Result: 开发了7B和76B两个模型，在150B tokens上持续预训练即达到开源Transformer基线性能；4M token序列首token生成速度提升100倍以上；训练稳定，7B模型达到23.4%的MFU；脉冲方案实现69.15%稀疏度。

Conclusion: 这项工作证明了脑启发机制在推动下一代高效可扩展大模型设计方面的潜力，特别是在非NVIDIA平台上的可行性。

Abstract: Mainstream Transformer-based large language models face major efficiency
bottlenecks: training computation scales quadratically with sequence length,
and inference memory grows linearly, limiting long-context processing. Building
large models on non-NVIDIA platforms also poses challenges for stable and
efficient training. To address this, we introduce SpikingBrain, a family of
brain-inspired models designed for efficient long-context training and
inference. SpikingBrain leverages the MetaX GPU cluster and focuses on three
aspects: (1) Model Architecture: linear and hybrid-linear attention
architectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an
efficient, conversion-based training pipeline and a dedicated spike coding
framework; (3) System Engineering: customized training frameworks, operator
libraries, and parallelism strategies tailored to MetaX hardware.
  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,
and SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the
feasibility of large-scale LLM development on non-NVIDIA platforms.
SpikingBrain achieves performance comparable to open-source Transformer
baselines while using only about 150B tokens for continual pre-training. Our
models significantly improve long-sequence training efficiency and deliver
inference with (partially) constant memory and event-driven spiking behavior.
For example, SpikingBrain-7B attains over 100x speedup in Time to First Token
for 4M-token sequences. Training remains stable for weeks on hundreds of MetaX
C550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4
percent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling
low-power operation. Overall, this work demonstrates the potential of
brain-inspired mechanisms to drive the next generation of efficient and
scalable large model design.

</details>


### [62] [Dual-Branch Convolutional Framework for Spatial and Frequency-Based Image Forgery Detection](https://arxiv.org/abs/2509.05281)
*Naman Tyagi*

Main category: cs.LG

TL;DR: 通过结合空间和频域特征的双支深度网络，提出了一种高效的图像伪造检测方法，在CASIA 2.0数据集上达到77.9%的准确率


<details>
  <summary>Details</summary>
Motivation: 深度伪造和数字图像伪造技术快速发展，图像真实性验证面临无比挑战，需要可靠的检测方法

Method: 设计双支卷积神经网络，同时处理空间域和频域特征，通过Siamese网络融合并比较特征，生成64维嵌入进行分类

Result: 在CASIA 2.0数据集上达到77.9%的准确率，超过传统统计方法，虽然比大型复杂流水线表现略差，但在计算复杂度和检测可靠性之间取得平衡

Conclusion: 该方法为数字图像史证审查提供了强大方法，在媒体验证、执法和数字内容可靠性方面推进了视觉史证学的发展

Abstract: With a very rapid increase in deepfakes and digital image forgeries, ensuring
the authenticity of images is becoming increasingly challenging. This report
introduces a forgery detection framework that combines spatial and
frequency-based features for detecting forgeries. We propose a dual branch
convolution neural network that operates on features extracted from spatial and
frequency domains. Features from both branches are fused and compared within a
Siamese network, yielding 64 dimensional embeddings for classification. When
benchmarked on CASIA 2.0 dataset, our method achieves an accuracy of 77.9%,
outperforming traditional statistical methods. Despite its relatively weaker
performance compared to larger, more complex forgery detection pipelines, our
approach balances computational complexity and detection reliability, making it
ready for practical deployment. It provides a strong methodology for forensic
scrutiny of digital images. In a broader sense, it advances the state of the
art in visual forensics, addressing an urgent requirement in media
verification, law enforcement and digital content reliability.

</details>


### [63] [Learning to accelerate distributed ADMM using graph neural networks](https://arxiv.org/abs/2509.05288)
*Henri Doerks,Paul Häusner,Daniel Hernández Escobar,Jens Sjölund*

Main category: cs.LG

TL;DR: 通过将分布式ADMM与图神经网结合，学习适应性步长和通信权重，提高收敛速度和解决方案质量


<details>
  <summary>Details</summary>
Motivation: ADMM在分布式优化中应用广泛，但存在收敛慢、对超参数敏感等问题，需要提高其性能

Method: 将ADMM迭代表示为图神经网的消息传递框架，通过展开固定迭代次数训练GNN预测适应性步长和通信权重，以最小化最终错误

Result: 数值实验证明学习版本在收敛速度和解决方案质量方面均明显优于标准ADMM

Conclusion: 通过深度学习与传统优化算法的结合，可以有效提升分布式ADMM的性能，同时保持算法的收敛性质

Abstract: Distributed optimization is fundamental in large-scale machine learning and
control applications. Among existing methods, the Alternating Direction Method
of Multipliers (ADMM) has gained popularity due to its strong convergence
guarantees and suitability for decentralized computation. However, ADMM often
suffers from slow convergence and sensitivity to hyperparameter choices. In
this work, we show that distributed ADMM iterations can be naturally
represented within the message-passing framework of graph neural networks
(GNNs). Building on this connection, we propose to learn adaptive step sizes
and communication weights by a graph neural network that predicts the
hyperparameters based on the iterates. By unrolling ADMM for a fixed number of
iterations, we train the network parameters end-to-end to minimize the final
iterates error for a given problem class, while preserving the algorithm's
convergence properties. Numerical experiments demonstrate that our learned
variant consistently improves convergence speed and solution quality compared
to standard ADMM. The code is available at
https://github.com/paulhausner/learning-distributed-admm.

</details>


### [64] [Deep Reinforcement Learning for Ranking Utility Tuning in the Ad Recommender System at Pinterest](https://arxiv.org/abs/2509.05292)
*Xiao Yang,Mehdi Ben Ayed,Longyu Zhao,Fan Zhou,Yuchen Shen,Abe Engle,Jinfeng Zhuang,Ling Leng,Jiajing Xu,Charles Rosenberg,Prathibha Deshikachar*

Main category: cs.LG

TL;DR: 使用深度强化学习框架实现个性化广告推荐系统的多目标优化，提高点击率和长点击率


<details>
  <summary>Details</summary>
Motivation: 传统手动调整广告推荐系统的排名质量函数存在调整目标不理惵、参数组合过多、缺乏个性化和季节性适应性等问题，导致次优结果

Method: 提出DRL-PUT框架：1)将问题形式化为强化学习任务，根据广告请求状态预测最优超参数；2)直接从在线服务日志学习最优策略模型，避免估计价值函数的挑战

Result: 在Pinterest广告推荐系统中的在线A/B实验显示：与基准手动调整方法相比，DRL-PUT在处理段提高点击率9.7%、长点击率7.7%

Conclusion: DRL-PUT框架能够有效解决广告推荐系统中的多目标优化挑战，实现了更优的性能和个性化调整

Abstract: The ranking utility function in an ad recommender system, which linearly
combines predictions of various business goals, plays a central role in
balancing values across the platform, advertisers, and users. Traditional
manual tuning, while offering simplicity and interpretability, often yields
suboptimal results due to its unprincipled tuning objectives, the vast amount
of parameter combinations, and its lack of personalization and adaptability to
seasonality. In this work, we propose a general Deep Reinforcement Learning
framework for Personalized Utility Tuning (DRL-PUT) to address the challenges
of multi-objective optimization within ad recommender systems. Our key
contributions include: 1) Formulating the problem as a reinforcement learning
task: given the state of an ad request, we predict the optimal hyperparameters
to maximize a pre-defined reward. 2) Developing an approach to directly learn
an optimal policy model using online serving logs, avoiding the need to
estimate a value function, which is inherently challenging due to the high
variance and unbalanced distribution of immediate rewards. We evaluated DRL-PUT
through an online A/B experiment in Pinterest's ad recommender system. Compared
to the baseline manual utility tuning approach, DRL-PUT improved the
click-through rate by 9.7% and the long click-through rate by 7.7% on the
treated segment. We conducted a detailed ablation study on the impact of
different reward definitions and analyzed the personalization aspect of the
learned policy model.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [65] [Any-Step Density Ratio Estimation via Interval-Annealed Secant Alignment](https://arxiv.org/abs/2509.04852)
*Wei Chen,Shigui Li,Jiacheng Li,Jian Xu,Zhiqi Lin,Junmei Yang,Delu Zeng,John Paisley,Qibin Zhao*

Main category: stat.ML

TL;DR: ISA-DRE是一种新的密度比估计框架，通过全局割线函数替代传统切线建模，无需数值积分即可实现精确的多步估计，具有更低的方差和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有密度比估计方法往往在准确性和效率之间进行权衡，需要数值积分且计算成本高，限制了在实时和交互应用中的使用。

Method: 提出Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)框架，学习全局割线函数而不是无穷小切线；引入Secant Alignment Identity自一致性条件；采用Contraction Interval Annealing课程策略逐步扩大对齐区间。

Result: ISA-DRE在显著减少函数评估次数的情况下达到竞争性精度，推理速度大幅提升，适合实时和交互应用。

Conclusion: ISA-DRE通过割线对齐和区间退火策略，提供了一种准确、高效且稳定的密度比估计方法，解决了传统方法在准确性和效率之间的权衡问题。

Abstract: Estimating density ratios is a fundamental problem in machine learning, but
existing methods often trade off accuracy for efficiency. We propose
\textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)},
a framework that enables accurate, any-step estimation without numerical
integration.
  Instead of modeling infinitesimal tangents as in prior methods, ISA-DRE
learns a global secant function, defined as the expectation of all tangents
over an interval, with provably lower variance, making it more suitable for
neural approximation. This is made possible by the \emph{Secant Alignment
Identity}, a self-consistency condition that formally connects the secant with
its underlying tangent representations.
  To mitigate instability during early training, we introduce \emph{Contraction
Interval Annealing}, a curriculum strategy that gradually expands the alignment
interval during training. This process induces a contraction mapping, which
improves convergence and training stability.
  Empirically, ISA-DRE achieves competitive accuracy with significantly fewer
function evaluations compared to prior methods, resulting in much faster
inference and making it well suited for real-time and interactive applications.

</details>


### [66] [Optimal Variance and Covariance Estimation under Differential Privacy in the Add-Remove Model and Beyond](https://arxiv.org/abs/2509.04919)
*Shokichi Takakura,Seng Pei Liew,Satoshi Hasegawa*

Main category: stat.ML

TL;DR: 本文基于Bézier机制提出了差分隐私加删模型下的方差和协方差估计方法，证明了在高隐私机制下的极小极大最优性，并展示了优于其他机制的实例级效用。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交换模型下的差分隐私估计，而加删模型由于需要保持数据集大小隐私而更具挑战性，研究相对较少。

Method: 基于Bézier机制（一种利用Bernstein基的新型矩释放框架）开发高效的方差和协方差估计机制。

Result: 证明了所提机制在高隐私机制下达到极小极大最优，实例级分析显示Bézier估计器始终优于其他机制，且该机制可扩展到其他统计任务。

Conclusion: Bézier机制为加删模型下的差分隐私统计估计提供了有效的解决方案，具有最优的理论保证和优越的实际性能。

Abstract: In this paper, we study the problem of estimating the variance and covariance
of datasets under differential privacy in the add-remove model. While
estimation in the swap model has been extensively studied in the literature,
the add-remove model remains less explored and more challenging, as the dataset
size must also be kept private. To address this issue, we develop efficient
mechanisms for variance and covariance estimation based on the \emph{B\'{e}zier
mechanism}, a novel moment-release framework that leverages Bernstein bases. We
prove that our proposed mechanisms are minimax optimal in the high-privacy
regime by establishing new minimax lower bounds. Moreover, beyond worst-case
scenarios, we analyze instance-wise utility and show that the B\'{e}zier-based
estimator consistently achieves better utility compared to alternative
mechanisms. Finally, we demonstrate the effectiveness of the B\'{e}zier
mechanism beyond variance and covariance estimation, showcasing its
applicability to other statistical tasks.

</details>


### [67] [Spectral Algorithms in Misspecified Regression: Convergence under Covariate Shift](https://arxiv.org/abs/2509.05106)
*Ren-Rui Liu,Zheng-Chu Guo*

Main category: stat.ML

TL;DR: 该论文研究了协变量偏移下谱算法的收敛性，通过引入重要性权重处理分布不匹配问题，在再生核希尔伯特空间中建立了加权谱算法，并针对模型误设情况提供了全面的理论分析。


<details>
  <summary>Details</summary>
Motivation: 解决协变量偏移问题，即源域和目标域的输入边际分布不同但条件分布相同的情况，同时处理模型误设这一更具挑战性的场景，扩展经典核学习理论到更实际的场景。

Method: 引入重要性权重（目标密度与源密度之比）到学习框架中，在RKHS中构建加权谱算法。对于无界重要性权重情况，提出了新颖的截断技术。

Result: 在重要性权重有界的情况下，当目标函数属于RKHS时获得了极小极大最优收敛率；对于无界权重情况，在温和正则条件下获得了接近最优的收敛率，并将结果扩展到误设机制。

Conclusion: 该工作通过同时解决协变量偏移和模型误设的挑战，为理解它们的相互作用提供了系统框架，扩展了经典核学习理论到更实用的场景。

Abstract: This paper investigates the convergence properties of spectral algorithms --
a class of regularization methods originating from inverse problems -- under
covariate shift. In this setting, the marginal distributions of inputs differ
between source and target domains, while the conditional distribution of
outputs given inputs remains unchanged. To address this distributional
mismatch, we incorporate importance weights, defined as the ratio of target to
source densities, into the learning framework. This leads to a weighted
spectral algorithm within a nonparametric regression setting in a reproducing
kernel Hilbert space (RKHS). More importantly, in contrast to prior work that
largely focuses on the well-specified setting, we provide a comprehensive
theoretical analysis of the more challenging misspecified case, in which the
target function does not belong to the RKHS. Under the assumption of uniformly
bounded density ratios, we establish minimax-optimal convergence rates when the
target function lies within the RKHS. For scenarios involving unbounded
importance weights, we introduce a novel truncation technique that attains
near-optimal convergence rates under mild regularity conditions, and we further
extend these results to the misspecified regime. By addressing the intertwined
challenges of covariate shift and model misspecification, this work extends
classical kernel learning theory to more practical scenarios, providing a
systematic framework for understanding their interaction.

</details>


### [68] [Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations](https://arxiv.org/abs/2509.05186)
*Benjamin J. Zhang,Siting Liu,Stanley J. Osher,Markos A. Katsoulakis*

Main category: stat.ML

TL;DR: ICON是一种基于基础模型的算子学习方法，通过上下文示例学习微分方程的解算子。本文提出了概率框架，将ICON解释为执行贝叶斯推断，并扩展为生成式版本GenICON，能够对解算子进行不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有的ICON方法缺乏对不确定性建模的能力，无法量化解算子的预测不确定性。需要建立概率框架来理解ICON的工作原理并扩展其生成能力。

Method: 提出随机微分方程的概率框架，将ICON重新解释为计算后验预测分布的均值。基于此发展生成式ICON(GenICON)，能够从后验预测分布中采样解算子。

Result: 建立了ICON的贝叶斯推断解释框架，证明了ICON隐式地计算后验预测均值。GenICON能够生成解算子的不确定性样本，实现原则性的不确定性量化。

Conclusion: 概率框架为理解ICON和多算子学习方法提供了理论基础，GenICON扩展了ICON的生成能力，使算子学习能够进行不确定性量化，提升了方法的可靠性和实用性。

Abstract: In-context operator networks (ICON) are a class of operator learning methods
based on the novel architectures of foundation models. Trained on a diverse set
of datasets of initial and boundary conditions paired with corresponding
solutions to ordinary and partial differential equations (ODEs and PDEs), ICON
learns to map example condition-solution pairs of a given differential equation
to an approximation of its solution operator. Here, we present a probabilistic
framework that reveals ICON as implicitly performing Bayesian inference, where
it computes the mean of the posterior predictive distribution over solution
operators conditioned on the provided context, i.e., example condition-solution
pairs. The formalism of random differential equations provides the
probabilistic framework for describing the tasks ICON accomplishes while also
providing a basis for understanding other multi-operator learning methods. This
probabilistic perspective provides a basis for extending ICON to
\emph{generative} settings, where one can sample from the posterior predictive
distribution of solution operators. The generative formulation of ICON
(GenICON) captures the underlying uncertainty in the solution operator, which
enables principled uncertainty quantification in the solution predictions in
operator learning.

</details>
