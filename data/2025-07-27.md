<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.LG](#cs.LG) [Total: 62]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Time and Frequency Synchronization for Multiuser OTFS in Uplink](https://arxiv.org/abs/2507.17966)
*Mohsen Bayat,Sanoopkumar P. S.,Arman Farhang*

Main category: eess.SP

TL;DR: 提出了一种针对高移动性场景下上行多用户OTFS系统的时间和频率同步技术，重点解决定时偏移和载波频率偏移的估计与校正问题。


<details>
  <summary>Details</summary>
Motivation: 在高移动性场景中，准确估计和校正定时偏移和载波频率偏移对提升系统性能至关重要。

Method: 1. 提出基于单用户启发的PCP（SU-PCP）替换传统IMP导频，利用ZC序列实现导频分离；2. 提出多用户共享导频区域的MU-PCP导频模式；3. 引入基于相关性的定时偏移估计技术；4. 利用Chebyshev多项式基扩展模型（CPF-BEM）简化CFO估计问题。

Result: 提出的技术能够有效分离多用户信号并准确估计定时偏移和载波频率偏移。

Conclusion: 所提出的同步技术显著提升了高移动性场景下多用户OTFS系统的性能。

Abstract: In this paper, we propose time and frequency synchronization techniques for
uplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work
focuses on accurately estimating and correcting timing offsets (TOs) and
carrier frequency offsets (CFOs). Specifically, TO estimation is essential for
locating users' pilots on the delay-time plane, while CFO estimation enhances
channel estimation accuracy. First, we propose a TO estimation technique for an
existing multiuser pilot structure in MU-OTFS. We replace the impulse pilot
(IMP) in this pilot structure with a more practical pilot with a cyclic prefix
(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs
different Zadoff-Chu (ZC) sequences, which enables pilot separation via
correlation at the receiver side. Consequently, we introduce a
correlation-based TO estimation technique for uplink MU-OTFS using this pilot
structure. Next, a spectrally efficient and practical pilot pattern is
proposed, where each user transmits a PCP within a shared pilot region on the
delay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO
estimation technique utilizes a bank of filters to separate different users'
signals and accurately estimate their TOs. Then, we derive a mathematical
threshold range to enhance TO estimation accuracy by finding the first major
peak in the correlation function rather than relying solely on the highest
peak. After locating the received users' pilot signals using one of the
proposed TO estimation techniques, our proposed CFO estimation technique
reduces the multi-dimensional maximum likelihood (ML) search problem into
multiple one-dimensional search problems. In this technique, we apply the
Chebyshev polynomials of the first kind basis expansion model (CPF-BEM) to
effectively handle the time-variations of the channel in obtaining the CFO
estimates for all the users.

</details>


### [2] [Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model](https://arxiv.org/abs/2507.17982)
*Pablo Ramírez-Espinosa,Cleofás Segura-Gómez,Ángel Palomares-Caballero,F. Javier López-Martínez,David Morales-Jiménez*

Main category: eess.SP

TL;DR: 本文提出了一种基于超表面的流体天线系统（FAS）的完整分析模型，利用动态超表面天线（DMA）实现FAS概念，并通过电路理论重新建模信号，验证了其性能接近理想化实现。


<details>
  <summary>Details</summary>
Motivation: 由于电子可重构天线在分析建模上的挑战，以及FAS在无线通信中的潜力，本文旨在提出一种实用的分析模型。

Method: 利用动态超表面天线（DMA）实现FAS，并通过电路理论建模信号，考虑超表面的电磁效应。

Result: 模型通过全波仿真验证，性能接近理想化实现，并提供了关键指标的闭式表达式。

Conclusion: DMA-based FAS在性能上接近理想化实现，为系统设计提供了理论支持。

Abstract: Fluid antenna systems (FASs) have become a popular topic in the wireless
community as an effective yet simple means of exploiting spatial diversity. Due
to the limitations of physically moving radiating elements, electronically
reconfigurable antennas are emerging as practical implementations of FASs,
since changing the radiation pattern is functionally equivalent to physically
moving the device. However, electronically reconfigurable antennas pose a
challenge in terms of analytical modeling, often requiring full-wave
simulations or measurements for their characterization; this severely limits
the extraction of theoretical insights useful for system design. Motivated by
these difficulties and the growing interest in FASs, we propose in this paper a
complete analytical model for metasurface-based embodiments of FASs.
Specifically, we advocate for the implementation of the FAS concept through
dynamic metasurface antennas (DMAs), hitherto proposed as array replacements in
multiple-input multiple-output (MIMO) systems. We leverage circuit theory to
rewrite the conventional signal model of FASs in terms of admittance matrices
accounting for the electromagnetic effects inherent to metasurfaces. The model
is validated with full-wave simulations, showing good agreement. We further
illustrate how to apply the model for standard performance analysis, and
provide closed-form expressions for key metrics, including the resulting signal
covariance matrix. Results confirm that practical DMA-based FASs can achieve
similar performance to that of idealized implementations of position-flexible
antennas.

</details>


### [3] [Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming](https://arxiv.org/abs/2507.18035)
*Hyeonho Noh,Hyeonsu Lyu,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 论文研究了多活动STAR-RIS支持的ISAC网络，通过联合优化BS波束成形和STAR-RIS反射/传输系数，最大化通信总速率，同时满足感知、安全和功率约束。


<details>
  <summary>Details</summary>
Motivation: 探索如何在多用户通信和感知任务中，利用STAR-RIS技术提升系统性能，同时满足严格的感知和安全要求。

Method: 采用交替优化框架，将问题分解为子问题，分别通过KKT条件和SCA方法优化BS波束成形和STAR-RIS参数。

Result: 仿真显示，所提算法在通信总速率上显著优于被动RIS和单STAR-RIS基准，同时满足感知和安全约束。

Conclusion: 多活动STAR-RIS支持的ISAC网络在提升通信性能的同时，能有效满足感知和安全需求，具有实际应用潜力。

Abstract: This paper explores an integrated sensing and communication (ISAC) network
empowered by multiple active simultaneously transmitting and reflecting
reconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes
downlink communication to multiple users while concurrently interrogating a
sensing target. We jointly optimize the BS transmit beamformer and the
reflection/transmission coefficients of every active STAR-RIS in order to
maximize the aggregate communication sum-rate, subject to (i) a stringent
sensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an
upper bound on the leakage of confidential information, and (iii) individual
hardware and total power constraints at both the BS and the STAR-RISs. The
resulting highly non-convex program is tackled with an efficient alternating
optimization (AO) framework. First, the original formulation is reformulated
into an equivalent yet more tractable representation and partitioned into
subproblems. The BS beamformer is updated in closed form via the
Karush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and
transmission vectors are refined through successive convex approximation (SCA),
yielding a semidefinite program that is then solved via semidefinite
relaxation. Comprehensive simulations demonstrate that the proposed algorithm
delivers substantial sum-rate gains over passive-RIS and single STAR-RIS
baselines, all the while rigorously meeting the prescribed sensing and security
constraints.

</details>


### [4] [Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation](https://arxiv.org/abs/2507.18096)
*Jihong Huang,Rong Yang,Wei Gao,Xingqun Zhan,Zheng Yao*

Main category: eess.SP

TL;DR: 本文通过几何分析扩展了直接位置估计（DPE）的理论框架，提出了卫星圆形多径偏差（SCMB）模型，量化了多径误差对CAF和PVT解的影响，并通过模拟和测试验证了模型的正确性。


<details>
  <summary>Details</summary>
Motivation: 尽管DPE方法在GNSS信号处理中表现出色，但多径误差的理论特性尚未明确，本文旨在填补这一空白。

Method: 通过几何分析量化多径误差对CAF和PVT解的影响，提出SCMB模型，并通过蒙特卡洛模拟和城市峡谷测试验证。

Result: 研究发现最大PVT偏差取决于多径误差的最大值，且PVT偏差随卫星仰角增加而增大。

Conclusion: 研究结果为从几何角度选择DPE卫星提供了参考，强调了高低仰角卫星组合的重要性。

Abstract: Direct Position Estimation (DPE) is a method that directly estimate position,
velocity, and time (PVT) information from cross ambiguity function (CAF) of the
GNSS signals, significantly enhancing receiver robustness in urban
environments. However, there is still a lack of theoretical characterization on
multipath errors in the context of DPE theory. Geometric observations highlight
the unique characteristics of DPE errors stemming from multipath and thermal
noise as estimation bias and variance respectively. Expanding upon the
theoretical framework of DPE noise variance through geometric analysis, this
paper focuses on a geometric representation of multipath errors by quantifying
the deviations in CAF and PVT solutions caused by off-centering bias relative
to the azimuth and elevation angles. A satellite circular multipath bias (SCMB)
model is introduced, amalgamating CAF and PVT errors from multiple satellite
channels. The boundaries for maximum or minimum PVT bias are established
through discussions encompassing various multipath conditions. The correctness
of the multipath geometrical portrait is confirmed through both Monte Carlo
simulations and urban canyon tests. The findings indicate that the maximum PVT
bias depends on the largest multipath errors observed across various satellite
channels. Additionally, the PVT bias increases with satellite elevation angles,
influenced by the CAF multipath bias projection. This serves as a reference for
selecting DPE satellites from a geometric standpoint, underscoring the
importance of choosing a balanced combination of high and low elevation angles
to achieve an optimal satellite geometry configuration.

</details>


### [5] [Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems](https://arxiv.org/abs/2507.18149)
*Dongdong Zou,Wei Wang,Jiawen Yao,Zhongxing Tian,Zeyu Feng,Huan Huang,Fan Li,Gordon Ning Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: 提出了一种针对峰值功率受限IM-DD系统的间接概率整形方案，通过动态选择性映射和修改的M-BCJR算法，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 由于IM-DD系统的独特模型和固有约束，概率整形技术的有效应用仍是一个开放性问题，尤其是在存在记忆效应的系统中。

Method: 提出了一种动态选择性映射（DSLM）机制，结合修改的M-BCJR算法的turbo均衡器，以控制信号包络并恢复模糊比特。

Result: 在56GBaud PAM8系统中实验验证，接收灵敏度提升了1dB，且与典型概率幅度整形架构兼容。

Conclusion: 该方案为概率整形技术在具有记忆效应的IM-DD系统中的应用提供了新思路。

Abstract: Probabilistic shaping (PS) has attracted significant attention in
intensity-modulation and direct-detection (IM-DD) systems. However, due to the
unique system model and inherent constraints, the effective application of the
PS technique is still an open question in IM-DD systems, particularly in
systems with memory effects. In this paper, a novel indirect PS scheme tailored
for peak power constrained (PPC) IM-DD systems is proposed. The key idea lies
in strategically controlling the signal envelope to mitigate memory-induced
impairments, such as nonlinearity, overshoot, peak-to-average power ratio
enhancement, etc. The proposed scheme incorporates a dynamic selective mapping
(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol
mapping in which the current symbol is not only determined by the current bits
pattern but also by previously generated symbols within a specified memory
length. At the receiver side, a turbo equalizer with a modified M-BCJR
algorithm is proposed to achieve the recovery of ambiguous bits induced by
DSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the
proposed scheme exhibits 1dB receiver sensitivity improvement over 2km
single-mode fiber transmission. In addition, the proposed scheme has also been
demonstrated to be compatible with the typical probabilistic amplitude shaping
architecture, enabling a simple and fine-granularity rate adaptation
capability. To the best of our knowledge, this work opens a new sight for the
application of the PS technique in PPC IM-DD systems with memory effects.

</details>


### [6] [GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing](https://arxiv.org/abs/2507.18166)
*Jonas Elmiger,Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: SCHIEBER是一种多天线GNSS接收器方法，无需先验知识即可缓解干扰和欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: GNSS信号易受干扰和欺骗攻击，影响定位准确性。

Method: 通过自适应空间滤波技术缓解干扰，通过比较信号到达方向和伪距估计识别欺骗信号。

Result: 在GPS L1 C/A系统模拟中验证了方法的有效性。

Conclusion: SCHIEBER能有效应对干扰和欺骗攻击，提高GNSS定位安全性。

Abstract: Modern positioning relies on radio signals from global navigation satellite
systems (GNSS). Their low receive power renders these radio signals susceptible
to jamming attacks, in which malicious transmitters emit strong interference to
disrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,
in which malicious transmitters mimic legitimate satellites by transmitting
spurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna
GNSS receivers that mitigates jammers as well as spoofers without requiring any
prior knowledge of the receiver position or attack type: Jammers are mitigated
during signal acquisition using a recently developed adaptive spatial filtering
technique. Spoofers are identified and rejected after signal acquisition using
a novel approach that tests the consistency of acquired signals by comparing
their respective direction of arrival (DoA) and pseudorange estimates in a test
that is invariant with respect to the unknown receiver position. We demonstrate
the efficacy of our method using extensive simulations of a GPS L1 C/A system
under spoofing and jamming attacks.

</details>


### [7] [ICWLM: A Multi-Task Wireless Large Model via In-Context Learning](https://arxiv.org/abs/2507.18167)
*Yuxuan Wen,Xiaoming Chen,Maojun Zhang,Zhaoyang Zhang*

Main category: eess.SP

TL;DR: 提出了一种名为ICWLM的新型无线基础模型，通过多任务学习和上下文学习解决无线通信中的复杂问题，具有优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 无线通信技术快速发展带来复杂性和计算需求，传统深度学习方法任务单一且泛化能力不足。

Method: ICWLM直接在大规模混合无线数据集上训练，利用上下文学习和动态权重平均算法实现多任务学习。

Result: ICWLM在多项任务中表现优异，泛化能力显著，适用于未见过的系统配置。

Conclusion: ICWLM为未来无线网络提供了统一且自适应的AI模型范式，降低部署复杂性并提升资源管理智能性。

Abstract: The rapid evolution of wireless communication technologies, particularly
massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),
introduces significant network complexity and computational demands.
Significant research efforts have been made to improve physical layer
performance by resorting to deep learning (DL) methods, which, however, are
usually task-specific and struggle with data scarcity and generalization. To
address these challenges, we propose a novel In-Context Wireless Large Model
(ICWLM), a wireless-native foundation model designed for simultaneous
multi-task learning at the physical layer. Unlike conventional methods that
adapt wireless data to pre-trained large language models (LLMs), ICWLM is
trained directly on large-scale, mixed wireless datasets from scratch. It
jointly solves multiple classical physical layer problems, including multi-user
precoding (sum-rate maximization and max-min SINR) and channel prediction. A
key innovation of ICWLM is its utilization of in-context learning (ICL),
enabling the model to adapt to varying system configurations and channel
conditions with minimal demonstration pairs, eliminating the need for extensive
retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm
to dynamically balance the individual task losses during multi-task training,
ensuring efficient and stable learning across diverse objectives. Extensive
simulation results demonstrate that ICWLM achieves competitive performance
compared to task-specific methods while exhibiting remarkable generalization
capabilities to unseen system configurations. This work offers a promising
paradigm for developing unified and adaptive AI models for future wireless
networks, potentially reducing deployment complexity and enhancing intelligent
resource management.

</details>


### [8] [Quantized Signal Recovery with Interference via Parametrized Look-Up Tables](https://arxiv.org/abs/2507.18370)
*Morriel Kasher,Michael Tinston,Predrag Spasojevic*

Main category: eess.SP

TL;DR: 通过参数化查找表（LUT）优化低分辨率模数转换器的后校正性能，提出三种分析估计器，并验证其在实时信号恢复中的高精度和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率、非线性或宽带量化器的信号恢复问题，尤其是在存在高功率带外干扰时。

Method: 结合参数化LUT和三种分析估计器，提出针对特定信号类型的近似方法以提高计算可行性。

Result: 在3位量化信号和固定12抽头模型下，均方误差改善>10 dB，无杂散动态范围改善>20 dBc。

Conclusion: 该方法显著优于传统线性滤波技术，对输入参数变化和非线性量化器具有鲁棒性。

Abstract: Efficient all-digital post-correction of low-resolution analog-to-digital
converters can be achieved by using Look-Up Tables (LUTs). The performance of a
LUT can be optimized by incorporating a parametric model for the expected input
signal, noise level, and interference signals. We evaluate three analytical
estimators for integration with parametrized LUTs, especially with applications
to low-resolution, non-linear, or wideband quantizers. We also propose several
approximations to improve tractability of the estimation problem for
Phase-Shift Keyed input signals and Linear Frequency Modulated interference
signals. Simulated results validate the ability of our estimator to recover the
instantaneous value of the desired input signal in real-time with a high degree
of accuracy. This includes cancellation of harmonic distortion that aliases
into the desired signal bandwidth from front-end saturation due to high-power
out-of-band interference. Our estimators are shown to achieve a significant
gain over conventional linear-filtering techniques while also being robust to
changes in input parameters, non-linear quantizers, and time-variant
interference sources. For a tone input quantized to 3 bits and estimated with a
fixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error
and $>$20 dBc improvement in Spurious-Free Dynamic Range.

</details>


### [9] [A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff](https://arxiv.org/abs/2507.18587)
*Jérôme Emery,Ali Hasanzadeh Karkan,Jean-François Frigon,François Leduc-Primeau*

Main category: eess.SP

TL;DR: 提出了一种基于Transformer的基础模型，用于大规模MIMO预编码，旨在降低发射机能耗并动态适应用户速率需求，同时在数据稀缺环境下通过数据增强方法提升模型适应性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO系统中深度学习预编码模型对高质量本地数据集的依赖问题，同时降低训练复杂度。

Method: 使用Transformer基础模型进行预编码，引入数据增强方法，通过预训练特征提取器的余弦相似性找到与目标分布相似的训练样本。

Result: 在相同能耗下，零样本部署的模型性能显著优于零强迫方法，接近加权最小均方误差性能，且复杂度降低8倍。

Conclusion: 该工作通过解决数据可用性和训练复杂性问题，实现了基于深度学习的预编码方案在实际中的部署，并支持动态配置用户速率需求。

Abstract: Deep learning (DL) has emerged as a solution for precoding in massive
multiple-input multiple-output (mMIMO) systems due to its capacity to learn the
characteristics of the propagation environment. However, training such a model
requires high-quality, local datasets at the deployment site, which are often
difficult to collect. We propose a transformer-based foundation model for mMIMO
precoding that seeks to minimize the energy consumption of the transmitter
while dynamically adapting to per-user rate requirements. At equal energy
consumption, zero-shot deployment of the proposed foundation model
significantly outperforms zero forcing, and approaches weighted minimum mean
squared error performance with 8x less complexity. To address model adaptation
in data-scarce settings, we introduce a data augmentation method that finds
training samples similar to the target distribution by computing the cosine
similarity between the outputs of the pre-trained feature extractor. Our work
enables the implementation of DL-based solutions in practice by addressing
challenges of data availability and training complexity. Moreover, the ability
to dynamically configure per-user rate requirements can be leveraged by higher
level resource allocation and scheduling algorithms for greater control over
energy efficiency, spectral efficiency and fairness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: QuaRC框架通过核心集选择和层校正策略，在边缘设备上高效进行量化感知训练，显著提升低比特量化模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上敏感数据处理需求增加，但传统量化感知训练计算成本高且性能受限。

Method: 提出QuaRC框架，包括基于相对熵评分的核心集选择和级联层校正策略。

Result: 在1%数据子集上，QuaRC将ResNet-18的2-bit量化Top-1准确率提升5.72%。

Conclusion: QuaRC有效减少量化误差，提升边缘设备上量化模型的性能。

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [11] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种低复杂度、通用的AI模型，利用因果不变学习改进语义通信中信道解码器的数据重建。


<details>
  <summary>Details</summary>
Motivation: 解决用户数据多样化和知识分散导致的语义通信中数据重建问题。

Method: 采用生成对抗网络，结合因果不变学习提取数据的因果和非因果表示，并设计稀疏更新协议。

Result: 因果不变知识确保跨设备一致性，分类任务表现优异，数据重建性能超越现有方法。

Conclusion: 该方法在语义通信中表现出高效性和鲁棒性，适用于多样化数据场景。

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [12] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于隐藏层神经元输出特征的复杂网络建模方法，研究不同隐藏层构建的特征网络的自相似性，并通过调整自相似性提升深度神经网络的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未定量分析隐藏空间几何自相似性对模型权重优化的影响，也不清楚内部神经元的动态行为。

Method: 基于隐藏层神经元输出特征构建复杂网络模型，研究不同隐藏层的特征网络自相似性，并调整自相似性以优化模型性能。

Result: 实验表明，不同模型架构的特征网络自相似性程度不同，嵌入自相似性约束可提升自相似深度神经网络（MLP和注意力架构）性能达6个百分点。

Conclusion: 通过调整特征网络的自相似性，可以有效提升深度神经网络的分类性能。

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [13] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: 本文提出了一种基于强化学习的自适应优化算法，用于降维的气动形状优化，通过代理模型和MCMC方法减少计算量并解释优化结果。


<details>
  <summary>Details</summary>
Motivation: 目标是减少计算成本，并通过优化结果解释极值点对实现目标流场的作用。

Method: 采用基于代理模型的actor-critic策略评估MCMC方法，允许部分参数临时“冻结”，通过局部优化参数变化加速全局优化。

Result: 在简单流体动力学问题上，该方法能够实现特征重要性评分的解释。

Conclusion: 该方法在计算效率和结果解释性方面表现出潜力。

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [14] [Gait Recognition Based on Tiny ML and IMU Sensors](https://arxiv.org/abs/2507.18627)
*Jiahang Zhang,Mingtong Chen,Zhengbao Yang*

Main category: cs.LG

TL;DR: 开发基于Tiny ML和IMU传感器的步态识别系统，通过XIAO-nRF52840 Sense微控制器和LSM6DS3 IMU传感器采集运动数据，利用Edge Impulse平台训练DNN分类器，实现实时活动分类，准确率超80%。


<details>
  <summary>Details</summary>
Motivation: 利用Tiny ML和IMU传感器开发低功耗的实时步态识别系统，适用于电池供电或能量收集设备。

Method: 通过IMU传感器采集运动数据，使用Edge Impulse平台进行数据预处理（滑动窗口、归一化）并训练DNN分类器。

Result: 模型在测试集上准确率超过80%，能有效分类四种活动，并支持异常检测。

Conclusion: 系统展示了Tiny ML在低功耗设备中实现高效步态识别的潜力。

Abstract: This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.

</details>


### [15] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 论文探讨了基础模型的局限性，并提出双曲几何作为改进方向，综述了双曲神经网络及其在基础模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 基础模型在预训练和下游任务中表现出色，但仍存在表示能力有限、适应性低和可扩展性不足的问题。研究双曲几何是否能更好地匹配真实数据的结构。

Method: 利用双曲空间（非欧几里得几何）的低维嵌入特性，改进基础模型，如语言模型和视觉语言模型。

Result: 双曲几何能有效提升模型的复杂推理能力、零样本泛化和跨模态语义对齐，同时保持参数效率。

Conclusion: 双曲神经网络为基础模型提供了新的研究方向，但仍需解决关键挑战以推动领域发展。

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [16] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: 论文研究了LLMs在排序和评估任务中的位置偏差和重复不一致性，提出了一种动态早停方法以减少计算成本，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在排序和评估任务中存在位置偏差和重复不一致性，现有方法通过多次重复和多数投票解决，但计算成本高。

Method: 提出动态早停方法，根据实例自适应确定重复次数，并引入基于置信度的改进。

Result: 动态策略平均减少81%的LLM调用，改进后减少87%，准确性损失较小。

Conclusion: 动态早停方法有效降低计算成本，适用于不同LLMs和任务。

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [17] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix是一个开源的、基于Python的软件框架，旨在为表格数据提供可重复和可解释的机器学习工作流。


<details>
  <summary>Details</summary>
Motivation: 解决透明实验数据分析的需求，确保整个分析过程（包括数据转换和方法选择）可记录、可访问、可重复且易于理解。

Method: 提供标准化数据预处理、可视化、模型训练、评估、解释、结果检查和预测的模块，并配备用户友好界面。

Result: Helix支持社区驱动开发，遵循FAIR原则，帮助非数据科学背景的研究人员获得可操作的见解。

Conclusion: Helix是一个功能全面且易于使用的工具，旨在提升机器学习的透明度和可解释性。

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [18] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: 论文提出了一种名为CICME的三步方法，用于从多领域异构数据中推断因果机制，结合因果迁移学习（CTL）原理，可靠地检测领域不变的因果机制。


<details>
  <summary>Details</summary>
Motivation: 通过因果性视角深入理解复杂传感器系统，解决多领域异构数据中的因果机制推断问题。

Method: 采用三步法CICME，结合CTL原理，先识别领域不变的因果机制，再指导各领域剩余因果机制的估计。

Result: 在线性高斯模型和制造过程场景下，CICME表现优于基线方法，尤其在特定场景下效果显著。

Conclusion: CICME方法在多领域因果机制推断中具有优势，能够有效结合全局和个体领域数据，提升推断准确性。

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [19] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种结合扩散模型和大型语言模型（LLM）的时空扩散模型（LSDM），用于提升个人用户服务级移动流量预测的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有流量预测方法在不同城市环境中的适应性不足，且因个人流量模式的高不确定性和缺乏环境上下文导致预测不准确。

Method: LSDM结合扩散模型的生成能力和Transformer的自适应学习能力，并利用LLM捕捉多模态环境信息。

Result: 实验表明，LSDM在流量预测中表现优异，泛化能力强，结合LLM后性能提升至少2.83%（决定系数），且均方根误差比同类模型（如CSDI）降低至少8.29%。

Conclusion: LSDM通过整合扩散模型和LLM，显著提升了服务级流量预测的准确性和适应性。

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [20] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: 提出了一种结合生成式AI和copula建模的新框架CoCAI，用于多变量时间序列的预测和异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列分析中的准确预测和鲁棒异常检测两大挑战。

Method: 利用扩散模型捕捉数据复杂依赖关系，结合保形预测技术校准预测区域，并通过降维和copula建模进行异常检测。

Result: 在真实水务系统数据上验证了CoCAI的预测准确性和异常检测能力。

Conclusion: CoCAI在理论和实践中均表现出色，适用于复杂时间序列分析。

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [21] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: GenSelect利用LLM的长推理能力从N个候选方案中选择最佳解决方案，优于现有的点对点和成对评分方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用LLM的比较能力或扩展性方面存在不足，GenSelect旨在解决这些问题。

Method: GenSelect通过长推理从并行采样的候选方案中选择最佳解，适用于数学推理任务。

Result: GenSelect在数学推理任务中表现优于现有评分方法，如QwQ和DeepSeek-R1-0528。

Conclusion: GenSelect有效结合了LLM的比较能力和高效扩展性，为推理任务提供了新思路。

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [22] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 使用Wasserstein生成对抗网络（WGAN）进行降水降尺度，提升视觉真实性和数据质量评估。


<details>
  <summary>Details</summary>
Motivation: 高分辨率降水预测对减少局部强降雨灾害至关重要，但传统数值模型难以实现高分辨率预报。

Method: 采用WGAN进行降水降尺度，利用最优传输成本生成视觉真实的降水场。

Result: WGAN生成的降水场具有精细结构，尽管在传统评估指标上略逊，但视觉真实性与人类感知一致。

Conclusion: WGAN框架不仅提升了降水降尺度的视觉真实性，还为降水数据集的质量控制提供了新视角。

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [23] [Efficient Uncertainty in LLMs through Evidential Knowledge Distillation](https://arxiv.org/abs/2507.18366)
*Lakshmana Sri Harsha Nemani,P. K. Srijith,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: 提出了一种通过蒸馏方法在LLMs中高效估计不确定性的新方法，避免了传统贝叶斯和集成方法的高计算成本。


<details>
  <summary>Details</summary>
Motivation: 标准LLMs在不确定性量化方面存在挑战，传统方法计算成本高。

Method: 通过蒸馏不确定性感知的教师模型到学生模型，使用LoRA微调，比较了两种策略：传统softmax输出和Dirichlet分布输出。

Result: 学生模型在分类数据集上表现与教师模型相当或更优，且仅需单次前向传播。

Conclusion: 首次证明通过证据蒸馏可以在LLMs中实现即时且稳健的不确定性量化。

Abstract: Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.

</details>


### [24] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: GraphEXT是一个基于合作博弈论和社会外部性的GNN可解释性框架，通过分解图节点为联盟并量化节点重要性，显著提升GNN的可解释性。


<details>
  <summary>Details</summary>
Motivation: GNN的'黑盒'特性使其可解释性面临挑战，现有方法难以有效捕捉节点间的复杂交互模式。

Method: GraphEXT利用合作博弈论和社会外部性，将图节点划分为联盟，并通过Shapley值量化节点对GNN预测的边际贡献。

Result: 在合成和真实数据集上，GraphEXT在多种GNN架构中的保真度优于现有基线方法。

Conclusion: GraphEXT通过强调节点交互和结构变化的影响，显著提升了GNN模型的可解释性。

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [25] [Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2507.18555)
*Jun'ichi Takeuchia,Yoshinari Takeishia,Noboru Muratab,Kazushi Mimurac,Ka Long Keith Hod,Hiroshi Nagaoka*

Main category: cs.LG

TL;DR: 讨论了2层ReLU网络的Fisher信息矩阵和神经正切核（NTK）的关系，并展示了NTK的谱分解及其主要特征值的具体形式。


<details>
  <summary>Details</summary>
Motivation: 研究2层ReLU网络的Fisher信息矩阵和NTK之间的关系，以揭示其数学结构。

Method: 通过线性变换分析Fisher信息矩阵和NTK的关系，并给出NTK的谱分解及其特征函数的具体形式。

Result: 获得了2层神经网络表示函数的近似公式，并展示了NTK的主要特征值形式。

Conclusion: 揭示了Fisher信息矩阵和NTK之间的线性关系，为理解神经网络的理论基础提供了新的视角。

Abstract: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.

</details>


### [26] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: 提出分子任务算术方法，通过负样本训练模型学习属性方向，无需正样本即可生成理想分子，实验证明其多样性和性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决理想分子稀缺问题，避免依赖正样本数据，提出一种更高效的数据驱动方法。

Method: 利用负样本训练模型学习属性方向，通过反向移动生成正分子，应用于零样本、双目标和少样本设计任务。

Result: 在20个零样本实验中，生成的分子更多样且成功率高；在双目标和少样本任务中，保持理想属性的同时增加多样性。

Conclusion: 分子任务算术因其简单性、数据效率和性能，有望成为分子设计的默认迁移学习策略。

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [27] [Beyond Internal Data: Constructing Complete Datasets for Fairness Testing](https://arxiv.org/abs/2507.18561)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 论文提出了一种利用重叠数据集生成合成数据的方法，以解决公平性测试中真实数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的广泛应用，公平性测试变得至关重要，但获取包含人口统计数据的真实数据存在法律和隐私挑战。

Method: 通过重叠数据集构建包含人口统计信息的合成数据，并验证其与真实数据的一致性。

Result: 实验证明，合成数据得出的公平性指标与真实数据一致。

Conclusion: 该方法为公平性测试提供了一种替代方案，解决了真实数据稀缺的问题。

Abstract: As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.

</details>


### [28] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: MFNO是一种基于镜像填充的傅里叶神经算子，用于学习随机系统的动态，能够处理非周期性输入，并在理论和实验上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准傅里叶神经算子（FNO）无法处理非周期性输入的问题，同时提升随机系统动态学习的准确性和效率。

Method: 通过引入镜像填充扩展FNO，结合Wong-Zakai类型定理和近似技术，理论证明MFNO能高精度逼近路径依赖随机微分方程的解。

Result: MFNO在分辨率泛化上表现优异，性能优于LSTMs、TCNs和DeepONet，且样本路径生成速度显著快于传统数值方法。

Conclusion: MFNO是一种高效且通用的方法，适用于随机系统动态建模，具有理论和实际应用的双重优势。

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [29] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: 论文探讨了在存在显著分布偏移的情况下，公共数据与私有数据结合的差分隐私机器学习算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究在公共数据和私有数据分布存在显著偏移时，结合两者的有效性，扩展了已知的下界结果。

Method: 通过高斯均值估计和高斯线性回归，分析两种分布偏移情况下的数据结合效果。

Result: 发现偏移较小时，需充足数据估计私有参数；偏移较大时，公共数据无益。

Conclusion: 公共数据在分布偏移较大时无助于私有参数估计，偏移较小时需充足数据。

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [30] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 联邦学习（FL）是一种新兴的分布式机器学习范式，通过动态设备协作训练模型，无需共享私有数据。本文探讨了FL与云端机器人操作的关联，并展望了其在大规模应用中的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习需要数据集中存储，而FL通过分布式设备训练模型，保护隐私。云端机器人操作受限于计算资源，FL为其提供了灵活性和可靠性。

Method: 介绍了FL的基本概念及其在云端机器人操作中的应用，探讨了集中式与分散式FL模型的设计与验证。

Result: FL为云端机器人操作提供了分布式计算优势，但也面临效率与可靠性的挑战。

Conclusion: FL在云端机器人操作中具有潜力，但需进一步研究以解决规模化应用中的问题。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [31] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: 该研究提出了一种基于深度学习的生成框架（pVAE），用于逆向设计具有定制水力特性的多孔超材料，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索多孔超材料的逆向设计，以生成具有特定水力特性（如孔隙率和渗透性）的结构。

Method: 开发了pVAE（变分自编码器增强回归器），结合CNN预测水力特性，并使用LBM生成数据。

Result: pVAE框架成功生成了具有目标特性的新材料，并提供了潜在空间的详细分析。

Conclusion: 该方法为高效设计和生成多孔超材料提供了新途径，相关数据和代码将开源。

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [32] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: SETOL理论解释了SOTA神经网络的性能，提出了新的学习预条件ERG，并通过实验验证了其与HTSR理论的一致性。


<details>
  <summary>Details</summary>
Motivation: 解释SOTA神经网络的性能，并验证HTSR理论中的关键指标alpha和alpha-hat的起源。

Method: 结合统计力学、随机矩阵理论和量子化学方法，推导SETOL理论，并通过3层MLP和SOTA神经网络验证。

Result: SETOL理论预测与实验数据高度一致，ERG指标与HTSR的alpha指标表现一致。

Conclusion: SETOL为理解神经网络性能提供了新视角，ERG指标具有潜在应用价值。

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [33] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: Seed2Harvest是一种混合红队方法，结合人类和机器生成对抗性提示的优势，用于扩展文化多样性的人类对抗性提示种子。


<details>
  <summary>Details</summary>
Motivation: 当前对抗性提示生成方法存在人类生成规模小、合成生成缺乏真实性的问题，需要结合两者优势以全面评估T2I模型的鲁棒性。

Method: 提出Seed2Harvest方法，通过混合红队技术扩展人类对抗性提示种子，保留人类提示特征和攻击模式。

Result: 扩展后的数据集在攻击成功率（0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16）、地理多样性（535个地点）和熵（7.48）上显著提升。

Conclusion: 人机协作在T2I模型安全性评估中至关重要，Seed2Harvest展示了其高效性和可扩展性。

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [34] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: UrbanPulse是一个可扩展的深度学习框架，用于超细粒度的城市范围OD流量预测，通过将每个POI视为独立节点，结合时空图卷积编码器和基于Transformer的解码器，实现多尺度时空依赖建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人口流动预测中存在局限性：传统模型依赖静态空间假设，深度学习模型难以跨城市泛化，LLMs计算成本高且无法捕捉空间结构。此外，许多方法通过聚类POIs或限制覆盖范围牺牲分辨率。

Method: UrbanPulse采用三阶段迁移学习策略：大规模城市图预训练、冷启动适应和强化学习微调，结合时空图卷积编码器和Transformer解码器。

Result: 在加州三个大都市区的1.03亿条GPS记录上评估，UrbanPulse实现了最先进的准确性和可扩展性。

Conclusion: UrbanPulse通过高效迁移学习，为高分辨率AI城市预测的跨城市实际部署迈出关键一步。

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [35] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: 论文提出MFTRR框架，通过多模态细粒度主题-帖子关系推理改进帖子质量评估，解决了现有方法的三个主要限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在帖子质量评估中存在三个问题：单模态分类无法利用多模态线索、多模态融合引入噪声、缺乏捕捉复杂语义关系的能力。

Method: MFTRR框架将任务重新定义为排序任务，包含局部-全局语义关联推理模块和多层次证据关系推理模块。

Result: 在三个新构建的多模态数据集和公开数据集上，MFTRR显著优于现有方法，NDCG@3最高提升9.52%。

Conclusion: MFTRR通过模仿人类认知过程，有效提升了帖子质量评估的准确性和鲁棒性。

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [36] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE是一种两阶段Transformer模型，融合多模态视频、音频和文本特征预测fMRI活动，在Algonauts 2025挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通过融合多模态特征（视频、音频、文本）来更准确地预测fMRI活动，提升模型在分布内和分布外数据上的表现。

Method: 使用开源模型提取多模态特征，通过模态融合Transformer和时间预测Transformer（带旋转嵌入）进行解码，并在CNeuroMod数据集上训练。

Result: 在分布内数据上平均Parcel-wise Pearson相关性为32.25，分布外数据上为21.25，优于早期版本。

Conclusion: VIBE在多模态fMRI预测任务中表现出色，验证了其架构的有效性。

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


### [37] [Improving the Computational Efficiency and Explainability of GeoAggregator](https://arxiv.org/abs/2507.17977)
*Rui Deng,Ziqi Li,Mingshu Wang*

Main category: cs.LG

TL;DR: 本文改进了GeoAggregator（GA）模型，通过优化数据加载流程和引入模型集成策略及解释功能，提升了计算效率和模型可解释性。实验证明改进后的GA在预测精度和推理速度上优于原版。


<details>
  <summary>Details</summary>
Motivation: 准确建模和解释地理空间表格数据（GTD）对理解地理现象及其底层过程至关重要。

Method: 1）优化数据加载流程和模型前向传播以提高计算效率；2）引入模型集成策略和基于GeoShapley框架的后验解释功能。

Result: 改进后的GA在合成数据集上表现出更高的预测精度和推理速度，并能有效捕捉空间效应。

Conclusion: 改进后的GA模型在功能和效率上均有显著提升，且已开源供社区使用。

Abstract: Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).

</details>


### [38] [SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning](https://arxiv.org/abs/2507.17979)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.LG

TL;DR: SIFOTL是一种隐私保护的数据分析方法，通过提取隐私合规的统计摘要、利用XGBoost模型和LLM分离信号与噪声，并通过决策树识别数据偏移的驱动因素，在医疗数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗数据中的隐私限制和复杂噪声导致数据偏移分析困难，需要一种既能保护隐私又能有效处理噪声的方法。

Method: SIFOTL结合隐私合规的统计摘要、XGBoost模型和LLM，通过Pareto加权决策树识别数据偏移的驱动因素。

Result: 在MEPS和Synthea ABM数据集上，SIFOTL的F1得分显著优于基线方法，且在噪声环境下表现稳健。

Conclusion: SIFOTL提供了一种可解释、隐私保护且对噪声鲁棒的工作流程，适用于医疗数据分析。

Abstract: Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.

</details>


### [39] [Machine Unlearning of Traffic State Estimation and Prediction](https://arxiv.org/abs/2507.17984)
*Xin Wang,R. Tyrrell Rockafellar,Xuegang,Ban*

Main category: cs.LG

TL;DR: 论文提出了一种新的学习范式TSEP-Machine Unlearning，旨在解决数据驱动的交通状态估计与预测中的隐私、安全和数据新鲜度问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的交通状态估计与预测依赖敏感数据，引发隐私、网络安全和数据新鲜度问题，影响公众对智能交通系统的信任。

Method: 引入TSEP-Machine Unlearning范式，使训练后的TSEP模型能够选择性遗忘隐私敏感、被污染或过时的数据。

Result: 通过“遗忘”机制，提升了数据驱动交通TSEP的可信度和可靠性。

Conclusion: TSEP-Machine Unlearning为解决隐私和数据安全问题提供了有效方法，增强了智能交通系统的信任度。

Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.

</details>


### [40] [Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models](https://arxiv.org/abs/2507.18014)
*Datta Nimmaturi,Vaishnavi Bhargava,Rajat Ghosh,Johnu George,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出一种预测框架，优化GRPO微调的计算资源使用，通过实验总结出训练动态的规律。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型（LLM）在推理任务中微调的计算成本高问题，提出资源优化方案。

Method: 基于Llama和Qwen模型实验，建立模型大小、初始性能和训练进度的经验缩放规律。

Result: 发现训练分为三个阶段（缓慢启动、快速提升、平台期），并确定提前停止可节省计算资源。

Conclusion: 该框架为GRPO微调提供了高效实用的指导，适用于多种模型。

Abstract: Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.

</details>


### [41] [Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents](https://arxiv.org/abs/2507.18067)
*Abdessamad El-Kabid,Loubna Benabbou,Redouane Lguensat,Alex Hernández-García*

Main category: cs.LG

TL;DR: 提出了一种基于神经算子的深度学习框架，用于求解偏微分方程并提供任意分辨率解，同时应用于Copernicus海洋数据的降尺度建模。


<details>
  <summary>Details</summary>
Motivation: 解决海洋学中高分辨率洋流数据不足的问题，以支持海岸管理、环境监测和海上安全。

Method: 采用监督深度学习框架和神经算子，构建降尺度模型，可生成任意分辨率的解。

Result: 在Copernicus洋流数据和合成Navier-Stokes数据集上验证了模型的有效性。

Conclusion: 该方法能够为海洋学提供高分辨率数据支持，并具有广泛的应用潜力。

Abstract: Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.

</details>


### [42] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: GSPO是一种稳定、高效且性能优越的强化学习算法，用于训练大语言模型，通过序列级优化显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法采用词级重要性比率，存在局限性，GSPO旨在通过序列级优化改进训练效率和稳定性。

Method: GSPO基于序列似然定义重要性比率，并进行序列级裁剪、奖励和优化。

Result: GSPO在训练效率和性能上优于GRPO算法，显著稳定了MoE RL训练，并简化了RL基础设施设计。

Conclusion: GSPO的成功应用推动了最新Qwen3模型的显著改进。

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


### [43] [C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams](https://arxiv.org/abs/2507.18072)
*Ryusei Fujimoto,Yugo Nakamura,Yutaka Arakawa*

Main category: cs.LG

TL;DR: C-AAE结合匿名自编码器和自适应差分脉冲编码调制，有效保护用户隐私并减少数据量，同时保持活动识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备的行为数据可能泄露用户身份，隐私保护在医疗应用中至关重要。

Method: C-AAE结合匿名自编码器（AAE）和自适应差分脉冲编码调制（ADPCM），先提取活动特征并抑制身份信息，再进一步压缩数据。

Result: 实验显示，C-AAE将用户重识别F1分数降低10-15个百分点，数据量减少75%，活动识别F1分数仅下降5个百分点。

Conclusion: C-AAE在医疗传感器数据中实现了隐私保护与实用性的平衡。

Abstract: Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.

</details>


### [44] [Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method](https://arxiv.org/abs/2507.18073)
*Qingcheng Zhu,Yangyang Ren,Linlin Yang,Mingbao Lin,Yanjing Li,Sheng Xu,Zichao Feng,Haodong Zhu,Yuguang Yang,Juan Zhang,Runqi Wang,Baochang Zhang*

Main category: cs.LG

TL;DR: Squeeze10-LLM 是一种分阶段混合精度的后训练量化框架，通过将 80% 的权重量化为 1 比特，20% 量化为 4 比特，实现平均 1.6 比特/权重的压缩，显著提升低比特量化性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）部署面临参数庞大和计算成本高的问题，超低比特量化虽能减少存储和加速推理，但极端压缩（如平均比特宽度 ≤ 2）会导致性能严重下降。

Method: 提出 Squeeze10-LLM，采用分阶段混合精度后训练量化（PTQ）框架，并引入两项关键技术：后二值化激活鲁棒性（PBAR）和全信息激活监督（FIAS）。PBAR 改进权重重要性度量，FIAS 保留激活信息以减少误差累积。

Result: 在 LLaMA 和 LLaMA2 上的实验表明，Squeeze10-LLM 在亚 2 比特权重量化中达到最优性能，零样本分类任务准确率从 43% 提升至 56%。

Conclusion: Squeeze10-LLM 通过创新技术显著提升了低比特量化的性能，为 LLMs 的高效部署提供了有效解决方案。

Abstract: Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.

</details>


### [45] [Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes](https://arxiv.org/abs/2507.18098)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，通过将硬标签和额外监督信息结合为概率分布，分析其对分类模型性能的影响。研究发现，额外监督的关键在于非硬标签类的分布信息，而非硬标签的置信度。实验验证了该理论的有效性。


<details>
  <summary>Details</summary>
Motivation: 在训练数据有限的情况下，如何利用额外的监督信息（如置信度）提升分类模型的准确性是一个关键问题。

Method: 提出理论框架，将硬标签和额外监督视为概率分布，并通过仿射组合构建软标签。分析额外监督和混合系数对软标签的互补作用。

Result: 理论分析表明，额外监督的关键是非硬标签类的分布信息，混合系数控制调整步长。实验验证了该理论能提升分类准确性。

Conclusion: 额外监督和非硬标签分布信息对提升分类模型性能至关重要，理论框架为设计更有效的监督方式提供了指导。

Abstract: In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.

</details>


### [46] [Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN](https://arxiv.org/abs/2507.18111)
*Peyman Tehrani,Anas Alsoliman*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度强化学习的RAN切片优化方法PDA-DRL，显著降低延迟并优化资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决O-RAN架构下多MVNO竞争PRB资源时的延迟约束问题，同时最小化资源使用。

Method: 基于大数定律设计奖励函数，提出PDA-DRL算法，并引入基于性能的模型权重共享机制。

Result: PDA-DRL比基线方法平均延迟降低38%，模型权重共享方法优于传统聚合策略。

Conclusion: PDA-DRL和个性化模型权重共享方法在O-RAN中有效优化延迟和资源利用率。

Abstract: In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.

</details>


### [47] [Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification](https://arxiv.org/abs/2507.18113)
*Junyong Jiang,Buwei Tian,Chenxing Xu,Songze Li,Lu Dong*

Main category: cs.LG

TL;DR: 提出一种对抗攻击方法，利用环境中现有代理引导目标策略输出次优动作，无需修改环境。结合LLMs生成针对性对抗奖励，并通过关键状态识别算法提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常依赖修改环境或策略，实用性受限。本文旨在提出一种更实用的方法，利用环境中现有代理引导目标策略输出次优动作。

Method: 提出奖励迭代优化框架，利用LLMs生成针对性对抗奖励；设计关键状态识别算法，识别目标代理最脆弱状态。

Result: 在多样化环境中的实验结果表明，该方法优于现有方法。

Conclusion: 该方法通过针对性对抗奖励和关键状态识别，有效诱导目标代理做出次优决策，且无需修改环境。

Abstract: Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.

</details>


### [48] [Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning](https://arxiv.org/abs/2507.18122)
*Matthias Otth,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: 语言模型通过最大化自身预测置信度实现自我改进，无需外部验证。研究发现，在数学推理任务中，利用模型前缀置信度选择最有希望的尝试可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在数学推理任务中如何通过自身置信度提升性能，减少对外部验证的依赖。

Method: 使用模型前缀置信度选择最有希望的尝试，并在五个数学推理数据集上系统评估。

Result: 前缀置信度缩放（32个标记）比多数投票更高效，且对长度偏差的敏感性较低。测试时训练表现优于基础模型，但未超越前缀置信度缩放。

Conclusion: 前缀置信度缩放是一种高效的方法，适用于数学推理任务，优于传统方法。

Abstract: Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.

</details>


### [49] [Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2507.18139)
*Alberto Marchisio,Muhammad Shafique*

Main category: cs.LG

TL;DR: 本文综述了神经形态计算在自主系统中的进展，重点关注算法、硬件和优化策略，以及事件动态视觉传感器的作用。


<details>
  <summary>Details</summary>
Motivation: 满足对智能、自适应和节能自主系统日益增长的需求，如机器人、无人机和自动驾驶车辆。

Method: 通过整合脉冲神经网络和跨层优化策略，提升感知、决策和响应能力。

Result: 展示了神经形态方法在提高能效、鲁棒性和适应性方面的潜力。

Conclusion: 探讨了实时决策、持续学习和安全系统等新兴趋势和挑战。

Abstract: The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.

</details>


### [50] [When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label](https://arxiv.org/abs/2507.18153)
*Riting Xia,Rucong Wang,Yulin Liu,Anchen Li,Xueyan Liu,Yan Zhang*

Main category: cs.LG

TL;DR: 论文提出GraphALP框架，结合大语言模型（LLMs）和伪标签技术，解决带噪声标签的类别不平衡图节点分类问题。


<details>
  <summary>Details</summary>
Motivation: 现实图中的标签通常存在噪声，而现有方法假设标签干净可靠，与实际不符。

Method: 设计基于LLM的过采样生成少数类节点，动态加权伪标签减少噪声，二次LLM过采样缓解分布偏差。

Result: 实验表明GraphALP在带噪声标签的类别不平衡图上优于现有方法。

Conclusion: GraphALP为类别不平衡图节点分类提供了一种鲁棒且有效的解决方案。

Abstract: Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.

</details>


### [51] [ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory](https://arxiv.org/abs/2507.18183)
*Jianchao Wang,Qingfeng Li,Pengcheng Zheng,Xiaorong Pu,Yazhou Ren*

Main category: cs.LG

TL;DR: ChronoSelect是一种新颖的框架，通过四阶段内存架构和滑动更新机制，利用学习动态的丰富时间信息，有效处理噪声标签问题。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中噪声标签的存在会损害深度神经网络的泛化性能，现有方法未能充分利用学习动态的时间信息。

Method: 提出ChronoSelect框架，采用四阶段内存架构和滑动更新机制，通过时间轨迹分析和双分支一致性对样本进行三分类。

Result: 理论证明框架在噪声条件下的收敛性和稳定性，实验验证其在合成和真实数据集上的先进性能。

Conclusion: ChronoSelect通过时间动态分析显著提升了噪声标签下的学习效果。

Abstract: Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.

</details>


### [52] [Goal-based Trajectory Prediction for improved Cross-Dataset Generalization](https://arxiv.org/abs/2507.18196)
*Daniel Grimm,Ahmed Abouelazm,J. Marius Zöllner*

Main category: cs.LG

TL;DR: 提出了一种基于异构图神经网络的自动驾驶轨迹预测模型，通过多阶段目标分类提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在未见区域泛化能力不足的问题。

Method: 使用包含交通参与者和矢量化路网的异构图神经网络，多阶段分类目标点。

Result: 通过跨数据集评估（Argoverse2训练，NuScenes测试）验证了目标选择的有效性。

Conclusion: 该方法显著提升了模型在未见场景中的泛化性能。

Abstract: To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.

</details>


### [53] [FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting](https://arxiv.org/abs/2507.18219)
*Zhongzheng Yuan,Lianshuai Guo,Xunkai Li,Yinlin Zhu,Wenyu Wang,Meixia Qu*

Main category: cs.LG

TL;DR: FedSA-GCL是一种半异步联邦图学习框架，通过ClusterCast机制解决现有同步和异步方法的不足，提升训练效率和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法依赖同步通信效率低，异步方法未考虑图数据的拓扑特性，可能导致语义漂移和表示不一致。

Method: 提出FedSA-GCL框架，结合客户端标签分布差异和图拓扑特性，采用ClusterCast机制进行高效训练。

Result: 在多个真实图数据集上验证，FedSA-GCL平均性能优于基线2.92%（Louvain）和3.4%（Metis）。

Conclusion: FedSA-GCL在效率和鲁棒性上表现优异，为联邦图学习提供了新思路。

Abstract: Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.

</details>


### [54] [Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective](https://arxiv.org/abs/2507.18220)
*Ansei Yonezawa,Heisei Yonezawa,Shuichi Yahagi,Itsuro Kajiwara,Shinya Kijimoto,Hikaru Taniuchi,Kentaro Murakami*

Main category: cs.LG

TL;DR: SINDy-LOM结合稀疏回归技术和库优化机制，通过参数化基函数和双层优化架构，提高了非线性动力学系统建模的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统SINDy方法在库设计上存在困难，难以适用于复杂动力学系统。

Method: 提出SINDy-LOM，通过参数化基函数和双层优化（内层稀疏回归，外层基于RLT预测精度的库优化）改进库设计。

Result: SINDy-LOM提高了模型的解释性和可靠性，减少了用户负担，并在柴油发动机空气路径系统上验证了有效性。

Conclusion: SINDy-LOM通过优化库设计和RLT视角，显著提升了传统SINDy方法的性能。

Abstract: The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.

</details>


### [55] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: 论文首次大规模实验研究了六种基于线性规划的完全修正提升方法，包括两种新方法NM-Boost和QRLP-Boost，在20个数据集上的表现。结果表明，这些方法在使用浅层树时能优于或匹配XGBoost和LightGBM等先进方法，并生成更稀疏的集成模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于线性规划的完全修正提升方法在理论上具有吸引力，但缺乏大规模的实证研究。本文旨在填补这一空白。

Method: 研究六种LP-based提升方法，包括两种新方法NM-Boost和QRLP-Boost，使用启发式和最优基学习器，评估准确性、集成稀疏性、边际分布、实时性能和超参数敏感性。

Result: 完全修正方法在使用浅层树时能优于或匹配XGBoost和LightGBM，同时生成更稀疏的集成模型，且能在不牺牲性能的情况下精简预训练集成。

Conclusion: 完全修正提升方法在特定条件下具有竞争力，并展示了使用最优决策树的优势和局限性。

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [56] [Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring](https://arxiv.org/abs/2507.18293)
*Sjoerd van Straten,Alessandro Padella,Marwan Hassani*

Main category: cs.LG

TL;DR: SiamSA-PPM是一种结合Siamese学习和统计增强的自监督学习框架，用于预测性过程监控，通过生成语义有效的新轨迹变体提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界事件日志的低变异性和小规模问题，提升预测性过程监控的性能。

Method: 采用三种基于统计的转换方法生成新轨迹变体，结合Siamese学习框架学习过程前缀的通用表示。

Result: 在真实事件日志上表现优于现有方法，统计增强显著优于随机转换。

Conclusion: SiamSA-PPM为过程预测中的数据增强提供了有前景的方向。

Abstract: Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.

</details>


### [57] [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://arxiv.org/abs/2507.18297)
*Sergei Shumilin,Alexander Ryabov,Nikolay Yavich,Evgeny Burnaev,Vladimir Vanovskiy*

Main category: cs.LG

TL;DR: 提出一种基于可微分物理的算法，通过k-means聚类和随机优化减少非结构化网格的计算量，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现代数值模拟计算量大，需要减少离散问题规模但保持合理精度。

Method: 使用k-means聚类、自动微分和随机优化算法设计网格粗化算法。

Result: 在两种PDE测试中，网格点减少10倍，同时保留关键点变量动态。

Conclusion: 该算法适用于任意由演化偏微分方程描述的系统。

Abstract: Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.

</details>


### [58] [Regression-aware Continual Learning for Android Malware Detection](https://arxiv.org/abs/2507.18313)
*Daniele Ghiani,Daniele Angioni,Giorgio Piras,Angelo Sotgiu,Luca Minnei,Srishti Gupta,Maura Pintor,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 论文提出了一种解决持续学习中安全回归问题的方法，通过回归感知惩罚和PCT技术，有效减少模型更新后的有害预测变化。


<details>
  <summary>Details</summary>
Motivation: 恶意软件快速演变，传统ML检测器需持续更新，但完全重新训练不现实。持续学习虽能增量更新，但可能引发安全回归问题，即已检测到的恶意软件在更新后被误判为无害，这对安全应用构成严重风险。

Method: 论文形式化并量化了安全回归问题，提出回归感知惩罚，并采用PCT技术（Positive Congruent Training）在持续学习环境中保持模型预测行为。

Result: 在ELSA、Tesseract和AZ-Class数据集上的实验表明，该方法能有效减少安全回归，同时保持检测性能。

Conclusion: 该方法为持续学习中的安全回归问题提供了有效解决方案，增强了模型更新的可靠性。

Abstract: Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.

</details>


### [59] [State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer](https://arxiv.org/abs/2507.18320)
*Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出了一种名为TIDSIT的新架构，用于处理电池健康监测中的不规则时间序列数据，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 电池健康监测对安全和效率至关重要，但现有模型难以处理实际测量中的不规则性。

Method: TIDSIT结合连续时间嵌入和时序注意力机制，有效处理非均匀采样和变长输入。

Result: 在NASA电池数据集上，TIDSIT将预测误差降低50%以上，误差率低于0.58%。

Conclusion: TIDSIT在电池健康监测中表现优异，且适用于其他不规则时间序列任务。

Abstract: The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.

</details>


### [60] [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333)
*Kale-ab Abebe Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 论文指出当前多智能体强化学习（MARL）算法依赖简单约定而非有效推理，提出需设计新环境以测试核心能力。


<details>
  <summary>Details</summary>
Motivation: 探讨当前MARL算法在Dec-POMDP框架下的局限性，揭示其成功依赖简单约定而非环境观察和记忆。

Method: 通过案例研究分析智能体行为，比较适应性与非适应性合作的表现。

Result: 发现智能体易学习脆弱约定，任务设计不足导致模型未能发挥潜力。

Conclusion: 呼吁设计新环境，强调基于观察和记忆的推理能力，避免依赖脆弱约定。

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.

</details>


### [61] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets通过低秩适应（LoRA）降低HyperDeepONets的复杂度，减少参数数量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决HyperDeepONets因高参数数量导致的高内存和计算成本问题。

Method: 在物理信息机器学习中，利用LoRA将超网络的输出层权重矩阵分解为两个低秩矩阵。

Result: 在常微分和偏微分方程实验中，参数减少70%，预测精度和泛化能力优于常规HyperDeepONets。

Conclusion: PI-LoRA-HyperDeepONets在减少参数的同时提升了性能，适用于物理信息机器学习。

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


### [62] [A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges](https://arxiv.org/abs/2507.18376)
*Xing Hua,Haodong Chen,Qianqian Duan,Danfeng Hong,Ruijiao Li,Huiliang Shang,Linghua Jiang,Haima Yang,Dawei Zhang*

Main category: cs.LG

TL;DR: 论文探讨了扩散模型在智能农业中的应用，特别是在作物病虫害检测、遥感图像增强和数据增强方面的潜力，展示了其优于传统GAN模型的性能。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长和耕地资源稀缺促使智能农业和精准农业成为未来发展方向，扩散模型因其稳定性和高质量生成能力成为解决农业数据不足问题的关键。

Method: 综述了扩散模型在农业中的最新应用进展，包括作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理。

Result: 实验表明扩散模型在数据增强、图像生成和去噪任务中显著提升了模型准确性和鲁棒性，尤其在复杂环境中表现突出。

Conclusion: 尽管存在计算效率和泛化能力的挑战，但随着技术进步，扩散模型有望在智能农业中发挥更大作用，支持全球农业可持续发展。

Abstract: With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.

</details>


### [63] [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](https://arxiv.org/abs/2507.18423)
*Mizuki Funato,Yohei Sawada*

Main category: cs.LG

TL;DR: HYPER是一种结合多模型集成和储层计算的新方法，用于在数据稀缺条件下提高洪水预测和水管理的准确性。


<details>
  <summary>Details</summary>
Motivation: 许多地区缺乏足够的河流流量观测数据，限制了降雨-径流分析的准确性，现有模型在数据稀缺条件下难以同时满足高精度、可解释性和计算效率。

Method: HYPER方法首先对43个未校准的流域概念水文模型应用贝叶斯模型平均（BMA），然后通过储层计算（RC）模型进行误差校正，利用线性回归实现高效计算。对于无测站流域，通过关联测站流域的属性推断权重。

Result: 在数据丰富和稀缺情景下，HYPER的性能均优于基准LSTM模型，计算效率更高且不确定性更低。

Conclusion: HYPER为无测站流域提供了一种高效、稳健且可推广的流量预测解决方案。

Abstract: Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.

</details>


### [64] [Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning](https://arxiv.org/abs/2507.18519)
*Leiji Zhang,Zeyu Wang,Xin Li,Yao-Hui Li*

Main category: cs.LG

TL;DR: 论文提出了改进的双模拟度量方法，解决了传统方法中无法区分特定场景和依赖预定义权重的问题，通过引入状态-动作对测量和自适应系数更新算子，提升了表示能力。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量存在两个主要问题：无法表示某些独特场景，以及依赖预定义权重。这些问题源于奖励差距定义不精确和对不同训练阶段及任务设置中奖励差异和状态差异重要性变化的忽视。

Method: 提出了一种改进的双模拟度量方法，包括更精确的奖励差距定义和带有自适应系数的更新算子，并提供了收敛性和表示独特性的理论保证。

Result: 在DeepMind Control和Meta-World两个代表性基准上进行了广泛实验，验证了方法的有效性。

Conclusion: 改进的双模拟度量方法在理论和实验上均表现出优越性，解决了传统方法的局限性。

Abstract: Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.

</details>


### [65] [GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning](https://arxiv.org/abs/2507.18521)
*Zhongtian Sun,Anoushka Harit,Alexandra Cristea,Christl A. Donnelly,Pietro Liò*

Main category: cs.LG

TL;DR: GLANCE框架通过逻辑引导推理、动态图优化和自适应聚类，解决了GNN在异质图上的性能问题。


<details>
  <summary>Details</summary>
Motivation: GNN在异质图上表现不佳，主要由于邻居聚合的盲目性和高阶结构模式的缺失。

Method: GLANCE结合逻辑层、多头注意力边剪枝和聚类机制，提升图表示学习。

Result: 在Cornell等基准数据集上，GLANCE表现优异，提供鲁棒且可解释的解决方案。

Conclusion: GLANCE轻量、适应性强，特别适合异质图场景。

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.

</details>


### [66] [C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation](https://arxiv.org/abs/2507.18533)
*Magnus Bengtsson,Kenneth Östberg*

Main category: cs.LG

TL;DR: C2G-KD是一种无需真实数据、基于类别条件生成器和PCA几何约束的知识蒸馏框架，通过激活教师模型的输出来生成合成样本。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索无需真实训练数据，仅依赖教师模型和少量类别结构信息的知识蒸馏方法。

Method: 方法包括训练类别条件生成器，结合语义和结构损失，利用PCA子空间约束生成样本的拓扑一致性。

Result: 在MNIST上的实验表明，即使每类仅需两个真实样本，也能生成有效的合成训练数据。

Conclusion: 结论是该方法通过几何约束和教师模型引导，实现了无需真实数据的高效知识蒸馏。

Abstract: We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.

</details>


### [67] [The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection](https://arxiv.org/abs/2507.18549)
*Steven A. Frank*

Main category: cs.LG

TL;DR: 论文通过Price方程提出了一种通用的力-度量-偏置（FMB）定律，统一了多种学习算法和自然选择。


<details>
  <summary>Details</summary>
Motivation: 揭示不同学习算法、优化方法和自然选择背后的共同数学结构。

Method: 使用Price方程对变化进行简单的符号划分，提出FMB定律。

Result: FMB定律统一了自然选择、贝叶斯更新、牛顿法、随机梯度下降等多种算法。

Conclusion: FMB定律为理解和设计跨学科学习算法提供了理论基础。

Abstract: Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.

</details>


### [68] [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)
*Jiale Chen,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文揭示了GPTQ量化方法与Babai最近平面算法在数学上的等价性，为其提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究GPTQ量化方法的内在几何意义和理论保证，以改进大规模语言模型的量化部署。

Method: 通过数学论证，证明GPTQ与Babai最近平面算法在特定条件下的等价性。

Result: GPTQ的误差传播具有几何解释，并在无裁剪条件下继承了Babai算法的误差上界。

Conclusion: 研究为GPTQ提供了理论依据，并启发了未来量化算法的设计。

Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.

</details>


### [69] [Linear Memory SE(2) Invariant Attention](https://arxiv.org/abs/2507.18597)
*Ethan Pronovost,Neha Boloor,Peter Schleede,Noureldin Hendy,Andres Morales,Nicholas Roy*

Main category: cs.LG

TL;DR: 提出了一种线性内存需求的SE(2)不变性注意力机制，用于自动驾驶中的空间数据处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算所有对象对的相对位姿，内存需求为二次方，效率低。

Method: 设计了SE(2)不变性的缩放点积注意力机制，内存需求线性增长。

Result: 实验证明该方法实现可行且性能优于非不变性架构。

Conclusion: 该方法为自动驾驶任务提供了高效且性能优越的解决方案。

Abstract: Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.

</details>


### [70] [Demystify Protein Generation with Hierarchical Conditional Diffusion Models](https://arxiv.org/abs/2507.18603)
*Zinan Ling,Yi Shi,Da Yan,Yang Zhou,Bo Hui*

Main category: cs.LG

TL;DR: 提出了一种多级条件扩散模型，结合序列和结构信息进行蛋白质设计，并引入新的评估指标Protein-MMD。


<details>
  <summary>Details</summary>
Motivation: 蛋白质生成在生物学中应用广泛，但现有条件扩散模型在可靠生成蛋白质方面仍有挑战。

Method: 提出多级条件扩散模型，整合序列和结构信息，同时生成多级表示，并设计Protein-MMD评估指标。

Result: 实验表明，该框架和评估指标在条件蛋白质生成任务中表现优异。

Conclusion: 多级条件扩散模型和Protein-MMD为蛋白质设计提供了高效且可靠的解决方案。

Abstract: Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.

</details>


### [71] [Moving Out: Physically-grounded Human-AI Collaboration](https://arxiv.org/abs/2507.18623)
*Xuhui Kang,Sung-Wook Lee,Haolin Liu,Yuyan Wang,Yen-Ling Kuo*

Main category: cs.LG

TL;DR: 论文提出了一个名为Moving Out的新基准，用于评估人机协作中物理约束下的适应性，并提出了BASS方法以增强AI的行为多样性和动作理解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决物理环境中人机协作的复杂性问题，如连续状态-动作空间和约束动态。

Method: 提出了BASS方法（行为增强、模拟和选择），通过增强行为多样性和动作结果理解来应对挑战。

Result: 实验表明，BASS在AI-AI和人机协作中优于现有模型。

Conclusion: Moving Out基准和BASS方法为物理约束下的人机协作提供了有效解决方案。

Abstract: The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [72] [Sliding Window Informative Canonical Correlation Analysis](https://arxiv.org/abs/2507.17921)
*Arvind Prasadan*

Main category: stat.ML

TL;DR: 提出了一种在线流数据环境下的CCA扩展方法SWICCA，结合流式PCA和小滑动窗口样本实时估计CCA组件。


<details>
  <summary>Details</summary>
Motivation: 解决传统CCA无法处理流式数据的问题，适应高维实时分析需求。

Method: 使用流式PCA算法作为后端，结合滑动窗口样本实时估计CCA组件。

Result: 通过数值模拟验证性能，提供理论性能保证，并展示高维数据的适用性。

Conclusion: SWICCA方法适用于高维流式数据，具有实时性和可扩展性。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlated
sets of features between two datasets. In this paper, we propose a novel
extension of CCA to the online, streaming data setting: Sliding Window
Informative Canonical Correlation Analysis (SWICCA). Our method uses a
streaming principal component analysis (PCA) algorithm as a backend and uses
these outputs combined with a small sliding window of samples to estimate the
CCA components in real time. We motivate and describe our algorithm, provide
numerical simulations to characterize its performance, and provide a
theoretical performance guarantee. The SWICCA method is applicable and scalable
to extremely high dimensions, and we provide a real-data example that
demonstrates this capability.

</details>


### [73] [A Two-armed Bandit Framework for A/B Testing](https://arxiv.org/abs/2507.18118)
*Jinjuan Wang,Qianglin Wen,Yu Zhang,Xiaodong Yan,Chengchun Shi*

Main category: stat.ML

TL;DR: 本文提出了一种基于两臂老虎机框架的A/B测试方法，通过双重稳健估计、构建测试统计量和置换法计算p值，显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: A/B测试在科技公司中广泛应用，但现有方法在效率和准确性上仍有改进空间。本文旨在通过结合因果推断和强化学习方法，提升A/B测试的统计功效。

Method: 方法包括三步：(1)使用双重稳健估计生成伪结果，(2)在两臂老虎机框架下构建测试统计量，(3)通过置换法计算p值。

Result: 通过理论分析、数值实验和实际数据验证，该方法在性能上优于现有方法。

Conclusion: 提出的两臂老虎机框架显著提升了A/B测试的统计功效，适用于实际应用场景。

Abstract: A/B testing is widely used in modern technology companies for policy
evaluation and product deployment, with the goal of comparing the outcomes
under a newly-developed policy against a standard control. Various causal
inference and reinforcement learning methods developed in the literature are
applicable to A/B testing. This paper introduces a two-armed bandit framework
designed to improve the power of existing approaches. The proposed procedure
consists of three main steps: (i) employing doubly robust estimation to
generate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to
construct the test statistic, and (iii) applying a permutation-based method to
compute the $p$-value. We demonstrate the efficacy of the proposed method
through asymptotic theories, numerical experiments and real-world data from a
ridesharing company, showing its superior performance in comparison to existing
methods.

</details>


### [74] [Learning graphons from data: Random walks, transfer operators, and spectral clustering](https://arxiv.org/abs/2507.18147)
*Stefan Klus,Jason J. Bramburger*

Main category: stat.ML

TL;DR: 论文提出了一种将信号随机过程与图随机游走联系的方法，利用图论中的图极限概念（graphon）和转移算子（如Koopman和Perron-Frobenius算子）进行信号分析和聚类。


<details>
  <summary>Details</summary>
Motivation: 研究信号随时间演化的随机过程，探索其与图随机游走的联系，以扩展传统谱聚类方法到图极限（graphon）领域。

Method: 引入图随机游走的转移算子，从信号数据估计这些算子，并利用其特征值和特征函数进行聚类分析。

Result: 展示了如何从信号中重构转移概率密度和可逆随机游走的图极限（graphon），并在合成和真实信号（如温度和股票数据）中验证了方法的有效性。

Conclusion: 该方法成功将传统谱聚类扩展到图极限，为信号分析提供了新的工具。

Abstract: Many signals evolve in time as a stochastic process, randomly switching
between states over discretely sampled time points. Here we make an explicit
link between the underlying stochastic process of a signal that can take on a
bounded continuum of values and a random walk process on a graphon. Graphons
are infinite-dimensional objects that represent the limit of convergent
sequences of graphs whose size tends to infinity. We introduce transfer
operators, such as the Koopman and Perron--Frobenius operators, associated with
random walk processes on graphons and then illustrate how these operators can
be estimated from signal data and how their eigenvalues and eigenfunctions can
be used for detecting clusters, thereby extending conventional spectral
clustering methods from graphs to graphons. Furthermore, we show that it is
also possible to reconstruct transition probability densities and, if the
random walk process is reversible, the graphon itself using only the signal.
The resulting data-driven methods are applied to a variety of synthetic and
real-world signals, including daily average temperatures and stock index
values.

</details>


### [75] [On Reconstructing Training Data From Bayesian Posteriors and Trained Models](https://arxiv.org/abs/2507.18372)
*George Wynne*

Main category: stat.ML

TL;DR: 论文提出了一种数学框架来分析训练数据重构攻击，并首次在文献中实现了贝叶斯模型的数据重构。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法存在训练数据重构攻击的漏洞，公开模型参数可能导致隐私泄露。

Method: 建立了数学框架，通过最大均值差异等价性描述易受攻击的数据特征，并提出了基于分数匹配的重构方法。

Result: 实现了贝叶斯和非贝叶斯模型中的数据重构，填补了文献空白。

Conclusion: 该研究揭示了机器学习模型的隐私风险，并提供了新的攻击方法。

Abstract: Publicly releasing the specification of a model with its trained parameters
means an adversary can attempt to reconstruct information about the training
data via training data reconstruction attacks, a major vulnerability of modern
machine learning methods. This paper makes three primary contributions:
establishing a mathematical framework to express the problem, characterising
the features of the training data that are vulnerable via a maximum mean
discrepancy equivalance and outlining a score matching framework for
reconstructing data in both Bayesian and non-Bayesian models, the former is a
first in the literature.

</details>


### [76] [DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts](https://arxiv.org/abs/2507.18464)
*Miguel Aspis,Sebastián A. Cajas Ordónez,Andrés L. Suárez-Cetrulo,Ricardo Simón Carbajo*

Main category: stat.ML

TL;DR: DriftMoE是一种在线混合专家架构，通过协同训练框架解决现有自适应集成方法的局限性，实现高效的概念漂移适应。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳数据流中的概念漂移问题，优化现有自适应集成方法的粗粒度机制和简单投票方案。

Method: 采用紧凑的神经网络路由器与增量Hoeffding树专家池协同训练，通过共生学习循环实现专家专业化。

Result: 在九种最先进的数据流学习基准测试中表现优异，与现有自适应集成方法竞争。

Conclusion: DriftMoE提供了一种高效且原则性的概念漂移适应方法，代码和数据公开可用。

Abstract: Learning from non-stationary data streams subject to concept drift requires
models that can adapt on-the-fly while remaining resource-efficient. Existing
adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or
simple voting schemes that fail to optimally leverage specialized knowledge.
This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture
that addresses these limitations through a novel co-training framework.
DriftMoE features a compact neural router that is co-trained alongside a pool
of incremental Hoeffding tree experts. The key innovation lies in a symbiotic
learning loop that enables expert specialization: the router selects the most
suitable expert for prediction, the relevant experts update incrementally with
the true label, and the router refines its parameters using a multi-hot
correctness mask that reinforces every accurate expert. This feedback loop
provides the router with a clear training signal while accelerating expert
specialization. We evaluate DriftMoE's performance across nine state-of-the-art
data stream learning benchmarks spanning abrupt, gradual, and real-world drifts
testing two distinct configurations: one where experts specialize on data
regimes (multi-class variant), and another where they focus on single-class
specialization (task-based variant). Our results demonstrate that DriftMoE
achieves competitive results with state-of-the-art stream learning adaptive
ensembles, offering a principled and efficient approach to concept drift
adaptation. All code, data pipelines, and reproducibility scripts are available
in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.

</details>


### [77] [Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise](https://arxiv.org/abs/2507.18520)
*Keyi Li,Yuval Kluger,Boris Landa*

Main category: stat.ML

TL;DR: 该论文提出了一种无超参数的方法，用于在高维数据中估计异方差噪声的幅度并校正欧氏距离，具有理论保证和实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习算法依赖于欧氏距离计算，但异方差噪声会扭曲这些距离，导致数据几何结构的错误表示。本文旨在解决这一问题。

Method: 开发了一种联合估计噪声幅度和校正距离的无超参数方法，适用于高维数据，无需对干净数据结构或噪声分布的先验知识。

Result: 理论证明了噪声幅度和距离估计的误差概率界限，实验验证了方法在合成数据和单细胞RNA测序数据中的有效性。

Conclusion: 该方法能可靠地估计噪声并校正距离，显著提高了基于距离的计算的鲁棒性，适用于实际数据分析。

Abstract: Pairwise Euclidean distance calculation is a fundamental step in many machine
learning and data analysis algorithms. In real-world applications, however,
these distances are frequently distorted by heteroskedastic
noise$\unicode{x2014}$a prevalent form of inhomogeneous corruption
characterized by variable noise magnitudes across data observations. Such noise
inflates the computed distances in a nontrivial way, leading to
misrepresentations of the underlying data geometry. In this work, we address
the tasks of estimating the noise magnitudes per observation and correcting the
pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,
we show that in general high-dimensional settings and without assuming prior
knowledge on the clean data structure or noise distribution, both tasks can be
performed reliably, even when the noise levels vary considerably. Specifically,
we develop a principled, hyperparameter-free approach that jointly estimates
the noise magnitudes and corrects the distances. We provide theoretical
guarantees for our approach, establishing probabilistic bounds on the
estimation errors of both noise magnitudes and distances. These bounds,
measured in the normalized $\ell_1$ norm, converge to zero at polynomial rates
as both feature dimension and dataset size increase. Experiments on synthetic
datasets demonstrate that our method accurately estimates distances in
challenging regimes, significantly improving the robustness of subsequent
distance-based computations. Notably, when applied to single-cell RNA
sequencing data, our method yields noise magnitude estimates consistent with an
established prototypical model, enabling accurate nearest neighbor
identification that is fundamental to many downstream analyses.

</details>
