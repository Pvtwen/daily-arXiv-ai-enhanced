{"id": "2601.11713", "categories": ["eess.SP", "cs.AI", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.11713", "abs": "https://arxiv.org/abs/2601.11713", "authors": ["Rodney Martinez Alonso", "Cel Thys", "Cedric Dehos", "Yuneisy Esthela Garcia Guzman", "Sofie Pollin"], "title": "Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain Wireless Autoencoding", "comment": "This preprint was submitted to The 2026 EuCNC & 6G Summit", "summary": "This paper proposes a novel technique for rejecting partial-in-band inter-cell interference (ICI) in ultrawideband communication systems. We present the design of an end-to-end wireless autoencoder architecture that jointly optimizes the transmitter and receiver encoding/decoding in the Walsh domain to mitigate interference from coexisting narrower-band 5G base stations. By exploiting the orthogonality and self-inverse properties of Walsh functions, the system distributes and learns to encode bit-words across parallel Walsh branches. Through analytical modeling and simulation, we characterize how 5G CPOFDM interference maps into the Walsh domain and identify optimal ratios of transmission frequencies and sampling rate where the end-to-end autoencoder achieves the highest rejection. Experimental results show that the proposed autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block error rate (BLER) for the same baseline channel noise, i.e., baseline Signal-to-Noise-Ratio (SNR) without the interference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u6c83\u5c14\u4ec0\u57df\u4e2d\u8054\u5408\u4f18\u5316\u53d1\u5c04\u673a\u548c\u63a5\u6536\u673a\u7684\u7aef\u5230\u7aef\u65e0\u7ebf\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u6291\u5236\u8d85\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u6765\u81ea5G\u57fa\u7ad9\u7684\u5e26\u5185\u90e8\u5206\u5e72\u6270\uff0c\u5b9e\u73b0\u9ad8\u8fbe12dB\u7684\u5e72\u6270\u6291\u5236\u3002", "motivation": "\u8d85\u5bbd\u5e26\u901a\u4fe1\u7cfb\u7edf\u9762\u4e34\u6765\u81ea\u5171\u5b58\u7a84\u5e265G\u57fa\u7ad9\u7684\u5e26\u5185\u90e8\u5206\u5e72\u6270\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6291\u5236\u8fd9\u79cd\u5e72\u6270\uff0c\u9700\u8981\u65b0\u7684\u5e72\u6270\u6291\u5236\u6280\u672f\u3002", "method": "\u8bbe\u8ba1\u7aef\u5230\u7aef\u65e0\u7ebf\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5728\u6c83\u5c14\u4ec0\u57df\u4e2d\u8054\u5408\u4f18\u5316\u53d1\u5c04\u673a\u548c\u63a5\u6536\u673a\u7684\u7f16\u7801/\u89e3\u7801\u3002\u5229\u7528\u6c83\u5c14\u4ec0\u51fd\u6570\u7684\u6b63\u4ea4\u6027\u548c\u81ea\u9006\u7279\u6027\uff0c\u5c06\u6bd4\u7279\u5b57\u5206\u5e03\u5e76\u5b66\u4e60\u7f16\u7801\u5230\u5e76\u884c\u6c83\u5c14\u4ec0\u5206\u652f\u4e2d\u3002", "result": "\u901a\u8fc7\u5206\u6790\u548c\u4eff\u771f\uff0c\u8868\u5f81\u4e865G CPOFDM\u5e72\u6270\u5728\u6c83\u5c14\u4ec0\u57df\u4e2d\u7684\u6620\u5c04\uff0c\u786e\u5b9a\u4e86\u4f20\u8f93\u9891\u7387\u4e0e\u91c7\u6837\u7387\u7684\u6700\u4f73\u6bd4\u4f8b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u81ea\u52a8\u7f16\u7801\u5668\u5728\u76f8\u540c\u57fa\u7ebf\u4fe1\u9053\u566a\u58f0\u4e0b\u5b9e\u73b0\u9ad8\u8fbe12dB\u7684\u5e72\u6270\u6291\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5757\u9519\u8bef\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u6c83\u5c14\u4ec0\u57df\u7aef\u5230\u7aef\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\u80fd\u6709\u6548\u6291\u5236\u8d85\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u76845G\u5e26\u5185\u90e8\u5206\u5e72\u6270\uff0c\u4e3a\u5171\u5b58\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5e72\u6270\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11720", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11720", "abs": "https://arxiv.org/abs/2601.11720", "authors": ["Hasan M. Boudi", "Taissir Y. Elganimi"], "title": "Sparsity Realization in User-Side Multilayer RIS", "comment": null, "summary": "User-side reconfigurable intelligent surface (US-RIS)-aided communication has recently emerged as a promising solution to overcome the high hardware cost and physical size limitations of large-scale user side antenna arrays. This letter proposes, for the first time, a framework that realizes sparsity in multilayer US-RIS using two strategies, namely element-wise sparsity and geometric sparsity. The element-wise approach distributes a limited number of active elements irregularly across multiple layers, thereby exploiting additional spatial degrees of freedom and boosting the achievable rate. For further performance enhancement, a novel foldable RIS architecture leveraging geometric sparsity is proposed, achieving additional gains by optimizing the folding topology of its multilayer structure. Simulation results show that the proposed sparse architectures provide consistently higher achievable rates than existing designs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c42\u7528\u6237\u4fa7\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08US-RIS\uff09\u7684\u7a00\u758f\u67b6\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u7d20\u7ea7\u7a00\u758f\u548c\u51e0\u4f55\u7a00\u758f\u4e24\u79cd\u7b56\u7565\u63d0\u5347\u7cfb\u7edf\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u7528\u6237\u4fa7\u5929\u7ebf\u9635\u5217\u7684\u9ad8\u786c\u4ef6\u6210\u672c\u548c\u7269\u7406\u5c3a\u5bf8\u9650\u5236\u95ee\u9898\uff0cUS-RIS\u901a\u4fe1\u6210\u4e3a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u8bbe\u8ba1", "method": "\u63d0\u51fa\u4e24\u79cd\u7a00\u758f\u7b56\u7565\uff1a1\uff09\u5143\u7d20\u7ea7\u7a00\u758f - \u5728\u591a\u5c42\u7ed3\u6784\u4e2d\u4e0d\u89c4\u5219\u5206\u5e03\u6709\u9650\u6570\u91cf\u7684\u6709\u6e90\u5143\u4ef6\uff1b2\uff09\u51e0\u4f55\u7a00\u758f - \u63d0\u51fa\u53ef\u6298\u53e0RIS\u67b6\u6784\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u5c42\u7ed3\u6784\u7684\u6298\u53e0\u62d3\u6251\u5b9e\u73b0\u6027\u80fd\u63d0\u5347", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7a00\u758f\u67b6\u6784\u76f8\u6bd4\u73b0\u6709\u8bbe\u8ba1\u80fd\u591f\u6301\u7eed\u63d0\u4f9b\u66f4\u9ad8\u7684\u53ef\u8fbe\u901f\u7387", "conclusion": "\u9996\u6b21\u63d0\u51fa\u7684\u591a\u5c42US-RIS\u7a00\u758f\u6846\u67b6\u901a\u8fc7\u5143\u7d20\u7ea7\u548c\u51e0\u4f55\u7a00\u758f\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3aUS-RIS\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bbe\u8ba1\u601d\u8def"}}
{"id": "2601.11734", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11734", "abs": "https://arxiv.org/abs/2601.11734", "authors": ["Hao Guo", "Ruoyu Sun", "Amir Hossein Fahim Raouf", "Rahil Gandotra", "Jiayu Mao", "Mark Poletti"], "title": "LarS-Net: A Large-Scale Framework for Network-Level Spectrum Sensing", "comment": "7 pages, 5 figures, this paper is under review at an IEEE conference", "summary": "As the demand of wireless communication continues to rise, the radio spectrum (a finite resource) requires increasingly efficient utilization. This trend is driving the evolution from static, stand-alone spectrum allocation toward spectrum sharing and dynamic spectrum sharing. A critical element of this transition is spectrum sensing, which facilitates informed decision-making in shared environments. Previous studies on spectrum sensing and cognitive radio have been largely limited to individual sensors or small sensor groups. In this work, a large-scale spectrum sensing network (LarS-Net) is designed in a cost-effective manner. Spectrum sensors are either co-located with base stations (BSs) to share the tower, backhaul, and power infrastructure, or integrated directly into BSs as a new feature leveraging active BS antenna systems. As an example incumbent system, fixed service microwave link operating in the lower-7 GHz band is investigated. This band is a primary candidate for 6G, being considered by the WRC-23, ITU, and FCC. Based on Monte Carlo simulations, we determine the minimum subset of BSs equipped with sensing capability to guarantee a target incumbent detection probability. The simulations account for various sensor antenna configurations, propagation channel models, and duty cycles for both incumbent transmissions and sensing operations. Building on this framework, we introduce three network-level sensing performance metrics: Emission Detection Probability (EDP), Temporal Detection Probability (TDP), and Temporal Mis-detection Probability (TMP), which jointly capture spatial coverage, temporal detectability, and multi-node diversity effects. Using these metrics, we analyze the impact of LarS-Net inter-site distance, noise uncertainty, and sensing duty-cycle on large-scale sensing performance.", "AI": {"tldr": "\u8bbe\u8ba1\u5927\u89c4\u6a21\u9891\u8c31\u611f\u77e5\u7f51\u7edc(LarS-Net)\uff0c\u901a\u8fc7\u57fa\u7ad9\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u964d\u4f4e\u6210\u672c\uff0c\u5206\u6790\u5fae\u6ce2\u94fe\u8def\u68c0\u6d4b\u6027\u80fd\uff0c\u63d0\u51fa\u4e09\u79cd\u7f51\u7edc\u7ea7\u611f\u77e5\u6307\u6807\u8bc4\u4f30\u7a7a\u95f4\u8986\u76d6\u548c\u65f6\u95f4\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u901a\u4fe1\u9700\u6c42\u589e\u957f\uff0c\u9891\u8c31\u8d44\u6e90\u9700\u8981\u66f4\u9ad8\u6548\u5229\u7528\uff0c\u63a8\u52a8\u4ece\u9759\u6001\u9891\u8c31\u5206\u914d\u5411\u52a8\u6001\u9891\u8c31\u5171\u4eab\u6f14\u8fdb\u3002\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u4e2a\u6216\u5c0f\u578b\u4f20\u611f\u5668\uff0c\u9700\u8981\u5927\u89c4\u6a21\u3001\u7ecf\u6d4e\u6709\u6548\u7684\u9891\u8c31\u611f\u77e5\u7f51\u7edc\u6765\u652f\u6301\u667a\u80fd\u51b3\u7b56\u3002", "method": "\u8bbe\u8ba1\u5927\u89c4\u6a21\u9891\u8c31\u611f\u77e5\u7f51\u7edc(LarS-Net)\uff0c\u4f20\u611f\u5668\u4e0e\u57fa\u7ad9\u5171\u5740\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\uff0c\u6216\u96c6\u6210\u5230\u57fa\u7ad9\u5929\u7ebf\u7cfb\u7edf\u4e2d\u3002\u4ee57GHz\u4ee5\u4e0b\u9891\u6bb5\u7684\u56fa\u5b9a\u670d\u52a1\u5fae\u6ce2\u94fe\u8def\u4e3a\u4f8b\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u786e\u5b9a\u6ee1\u8db3\u76ee\u6807\u68c0\u6d4b\u6982\u7387\u6240\u9700\u7684\u6700\u5c0f\u4f20\u611f\u5668\u57fa\u7ad9\u5b50\u96c6\u3002\u8003\u8651\u591a\u79cd\u5929\u7ebf\u914d\u7f6e\u3001\u4f20\u64ad\u4fe1\u9053\u6a21\u578b\u548c\u5360\u7a7a\u6bd4\u3002", "result": "\u63d0\u51fa\u4e09\u79cd\u7f51\u7edc\u7ea7\u611f\u77e5\u6027\u80fd\u6307\u6807\uff1a\u53d1\u5c04\u68c0\u6d4b\u6982\u7387(EDP)\u3001\u65f6\u95f4\u68c0\u6d4b\u6982\u7387(TDP)\u548c\u65f6\u95f4\u8bef\u68c0\u6982\u7387(TMP)\uff0c\u7528\u4e8e\u8054\u5408\u8bc4\u4f30\u7a7a\u95f4\u8986\u76d6\u3001\u65f6\u95f4\u53ef\u68c0\u6d4b\u6027\u548c\u591a\u8282\u70b9\u591a\u6837\u6027\u6548\u5e94\u3002\u5206\u6790\u4e86\u7ad9\u70b9\u95f4\u8ddd\u3001\u566a\u58f0\u4e0d\u786e\u5b9a\u6027\u548c\u611f\u77e5\u5360\u7a7a\u6bd4\u5bf9\u5927\u89c4\u6a21\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "LarS-Net\u4e3a\u5927\u89c4\u6a21\u9891\u8c31\u611f\u77e5\u63d0\u4f9b\u7ecf\u6d4e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u51fa\u7684\u6027\u80fd\u6307\u6807\u80fd\u5168\u9762\u8bc4\u4f30\u7f51\u7edc\u7ea7\u611f\u77e5\u80fd\u529b\uff0c\u4e3a6G\u9891\u8c31\u5171\u4eab\u548c\u52a8\u6001\u9891\u8c31\u7ba1\u7406\u63d0\u4f9b\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.11741", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11741", "abs": "https://arxiv.org/abs/2601.11741", "authors": ["Oliver Kirkpatrick", "Santiago Ozafrain", "Christopher Gilliam", "Beth Jelfs"], "title": "MIMO Array Calibration in Non-stationary Channels with Residual Surfaces and Slepian Spherical Harmonics", "comment": "5 pages, 3 figures, 1 table", "summary": "The fundamental mechanism driving MIMO beamforming is the relative phases of signals departing the transmit array and arriving at the receive array. If a propagation channel affects all transmitted signals equally, the relative phases are a function of the directions of departure and arrival, as well as the transmit and receive hardware. In a non-stationary channel, the amplitudes and phases of arriving signals may vary significantly over time, making it infeasible to directly measure the influence of hardware. In this paper, we present a calibration method for achieving indirect measurement and compensation of hardware influences in non-stationary channels. Our method characterizes the patterns of array elements relative to a reference element and estimates these relative patterns, termed residual surfaces, using a Slepian spherical harmonic basis. Using simulations, we demonstrate that our calibration method achieves beamforming gains that closely match theoretical optimums. Our results also show a reduction in the error in estimating the target direction, lower side lobes, and improve null-steering capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u975e\u5e73\u7a33\u4fe1\u9053\u4e2d\u901a\u8fc7\u95f4\u63a5\u6d4b\u91cf\u548c\u8865\u507f\u786c\u4ef6\u5f71\u54cd\u6765\u5b9e\u73b0MIMO\u6ce2\u675f\u6210\u5f62\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u4f7f\u7528Slepian\u7403\u8c10\u57fa\u8868\u5f81\u9635\u5217\u5143\u7d20\u76f8\u5bf9\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6ce2\u675f\u6210\u5f62\u6027\u80fd\u3002", "motivation": "\u5728\u975e\u5e73\u7a33\u4fe1\u9053\u4e2d\uff0c\u5230\u8fbe\u4fe1\u53f7\u7684\u5e45\u5ea6\u548c\u76f8\u4f4d\u968f\u65f6\u95f4\u663e\u8457\u53d8\u5316\uff0c\u4f7f\u5f97\u76f4\u63a5\u6d4b\u91cf\u786c\u4ef6\u5f71\u54cd\u53d8\u5f97\u4e0d\u53ef\u884c\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u52a8\u6001\u4fe1\u9053\u6761\u4ef6\u4e0b\u95f4\u63a5\u6d4b\u91cf\u548c\u8865\u507f\u786c\u4ef6\u5f71\u54cd\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8868\u5f81\u9635\u5217\u5143\u7d20\u76f8\u5bf9\u4e8e\u53c2\u8003\u5143\u7d20\u7684\u6a21\u5f0f\uff08\u79f0\u4e3a\u6b8b\u5dee\u8868\u9762\uff09\uff0c\u5e76\u4f7f\u7528Slepian\u7403\u8c10\u57fa\u6765\u4f30\u8ba1\u8fd9\u4e9b\u76f8\u5bf9\u6a21\u5f0f\uff0c\u4ece\u800c\u95f4\u63a5\u6d4b\u91cf\u548c\u8865\u507f\u786c\u4ef6\u5f71\u54cd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6821\u51c6\u65b9\u6cd5\u5b9e\u73b0\u7684\u6ce2\u675f\u6210\u5f62\u589e\u76ca\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u503c\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u76ee\u6807\u65b9\u5411\u4f30\u8ba1\u8bef\u5dee\uff0c\u964d\u4f4e\u4e86\u65c1\u74e3\u6c34\u5e73\uff0c\u5e76\u6539\u5584\u4e86\u96f6\u70b9\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5e73\u7a33\u4fe1\u9053\u4e2d\u786c\u4ef6\u5f71\u54cd\u96be\u4ee5\u76f4\u63a5\u6d4b\u91cf\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u95f4\u63a5\u6821\u51c6\u663e\u8457\u63d0\u5347\u4e86MIMO\u6ce2\u675f\u6210\u5f62\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u52a8\u6001\u4fe1\u9053\u73af\u5883\u4e0b\u7684\u9635\u5217\u6821\u51c6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.11790", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.11790", "abs": "https://arxiv.org/abs/2601.11790", "authors": ["Guerlain Lambert", "C\u00e9line Helbert", "Claire Lauvernet"], "title": "Gradient-based Active Learning with Gaussian Processes for Global Sensitivity Analysis", "comment": null, "summary": "Global sensitivity analysis of complex numerical simulators is often limited by the small number of model evaluations that can be afforded. In such settings, surrogate models built from a limited set of simulations can substantially reduce the computational burden, provided that the design of computer experiments is enriched efficiently. In this context, we propose an active learning approach that, for a fixed evaluation budget, targets the most informative regions of the input space to improve sensitivity analysis accuracy. More specifically, our method builds on recent advances in active learning for sensitivity analysis (Sobol' indices and derivative-based global sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian process (GP) surrogate. By leveraging the joint posterior distribution of the GP gradient, we develop acquisition functions that better account for correlations between partial derivatives and their impact on the response surface, leading to a more comprehensive and robust methodology than existing DGSM-oriented criteria. The proposed approach is first compared to state-of-the-art methods on standard benchmark functions, and is then applied to a real environmental model of pesticide transfers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u7684\u9ad8\u6548\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u548c\u68af\u5ea6\u4fe1\u606f\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u91c7\u96c6\u51fd\u6570\u4f18\u5316\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u63d0\u9ad8\u654f\u611f\u6027\u5206\u6790\u7cbe\u5ea6\u3002", "motivation": "\u590d\u6742\u6570\u503c\u6a21\u62df\u5668\u7684\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u901a\u5e38\u53d7\u9650\u4e8e\u53ef\u8d1f\u62c5\u7684\u6a21\u578b\u8bc4\u4f30\u6b21\u6570\u3002\u5728\u6709\u9650\u6a21\u62df\u6b21\u6570\u4e0b\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u6765\u6784\u5efa\u51c6\u786e\u7684\u4ee3\u7406\u6a21\u578b\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u8d1f\u62c5\u5e76\u63d0\u9ad8\u654f\u611f\u6027\u5206\u6790\u7cbe\u5ea6\u3002", "method": "\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528GP\u68af\u5ea6\u7684\u8054\u5408\u540e\u9a8c\u5206\u5e03\uff0c\u5f00\u53d1\u65b0\u7684\u91c7\u96c6\u51fd\u6570\u6765\u8003\u8651\u504f\u5bfc\u6570\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u53ca\u5176\u5bf9\u54cd\u5e94\u9762\u7684\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u9488\u5bf9Sobol\u6307\u6570\u548c\u57fa\u4e8e\u5bfc\u6570\u7684\u5168\u5c40\u654f\u611f\u6027\u5ea6\u91cf\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u51fd\u6570\u4e0a\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u5728\u519c\u836f\u8fc1\u79fb\u7684\u5b9e\u9645\u73af\u5883\u6a21\u578b\u4e2d\u5e94\u7528\u3002\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u7684DGSM\u5bfc\u5411\u51c6\u5219\u66f4\u5168\u9762\u548c\u7a33\u5065\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u5229\u7528\u6709\u9650\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u901a\u8fc7\u9488\u5bf9\u8f93\u5165\u7a7a\u95f4\u4e2d\u6700\u5177\u4fe1\u606f\u91cf\u7684\u533a\u57df\u8fdb\u884c\u91c7\u6837\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u590d\u6742\u6570\u503c\u6a21\u62df\u5668\u7684\u654f\u611f\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11556", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.11556", "abs": "https://arxiv.org/abs/2601.11556", "authors": ["Boyang Wang", "Yash Vishe", "Xin Xu", "Zachary Novack", "Julian McAuley", "Junda Wu"], "title": "CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool Integration", "comment": null, "summary": "Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet existing benchmarks emphasize isolated knowledge or atomic analyses rather than the integrative compositional reasoning needed to connect musical structures. To address this, we present the Compositional Symbolic Music Reasoning Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions derived from expert forums and professional examinations. Each item involves combining several atomic analyses to arrive at the final answer. Furthermore, we introduce a tool-augmented agent framework that leverages symbolic music analysis tools from the music21 library to address the challenges posed by CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial challenge across both community-sourced and exam-style questions, while our tool-augmented agent consistently outperforms all baselines, achieving 5-7% absolute accuracy gains.", "AI": {"tldr": "CSyMR-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u7b26\u53f7\u97f3\u4e50\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b126\u4e2a\u9700\u8981\u7ec4\u5408\u5206\u6790\u7684\u591a\u9009\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8emusic21\u5de5\u5177\u589e\u5f3a\u7684\u4ee3\u7406\u6846\u67b6\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u97f3\u4e50\u63a8\u7406\u65b9\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u7684\u77e5\u8bc6\u6216\u539f\u5b50\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u8fde\u63a5\u97f3\u4e50\u7ed3\u6784\u7684\u7ec4\u5408\u6027\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u800c\u8fd9\u662f\u97f3\u4e50\u521b\u4f5c\u548c\u7406\u89e3\u7684\u5173\u952e\u3002", "method": "1) \u521b\u5efaCSyMR-Bench\u6570\u636e\u96c6\uff1a\u4ece\u4e13\u5bb6\u8bba\u575b\u548c\u4e13\u4e1a\u8003\u8bd5\u4e2d\u6536\u96c6126\u4e2a\u9700\u8981\u7ec4\u5408\u591a\u4e2a\u539f\u5b50\u5206\u6790\u7684\u591a\u9009\u9898\uff1b2) \u63d0\u51fa\u5de5\u5177\u589e\u5f3a\u7684\u4ee3\u7406\u6846\u67b6\uff1a\u5229\u7528music21\u5e93\u4e2d\u7684\u7b26\u53f7\u97f3\u4e50\u5206\u6790\u5de5\u5177\u6765\u8f85\u52a9\u63a8\u7406\u3002", "result": "CSyMR-Bench\u5bf9\u73b0\u6709\u6a21\u578b\u6784\u6210\u975e\u5e73\u51e1\u6311\u6218\uff0c\u800c\u63d0\u51fa\u7684\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\u5728\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5b9e\u73b0\u4e865-7%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u7b26\u53f7\u97f3\u4e50\u7ec4\u5408\u63a8\u7406\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u5de5\u5177\u589e\u5f3a\u65b9\u6cd5\u5728\u590d\u6742\u97f3\u4e50\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u97f3\u4e50AI\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12341", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.HC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.12341", "abs": "https://arxiv.org/abs/2601.12341", "authors": ["Rezky Kam", "Coddy N. Siswanto"], "title": "Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs", "comment": null, "summary": "This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u8ba9LLM\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u60c5\u611f\u52a8\u6001\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5bf9\u8bdd\u5efa\u6a21", "motivation": "\u5f53\u524dLLM\u5728\u60c5\u611f\u7406\u89e3\u548c\u5bf9\u8bdd\u5efa\u6a21\u4e2d\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u60c5\u611f\u52a8\u6001\u7684\u6a21\u62df\u80fd\u529b\uff0c\u4e14\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6355\u6349\u60c5\u611f\u968f\u65f6\u95f4\u6f14\u5316\u7684\u590d\u6742\u6a21\u5f0f", "method": "\u6784\u5efa\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u901a\u8fc7\u7269\u7406\u539f\u7406\u7ea6\u675f\u60c5\u611f\u52a8\u6001\u5efa\u6a21\uff0c\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u60c5\u611f\u6f14\u5316\u6a21\u62df", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u60c5\u611f\u52a8\u6001\u7684\u6846\u67b6\uff0c\u4e3aLLM\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u60c5\u611f\u6f14\u5316\u5efa\u6a21\u80fd\u529b\uff0c\u5f00\u542f\u4e86\u53ef\u89e3\u91ca\u5bf9\u8bdd\u5efa\u6a21\u7684\u65b0\u53ef\u80fd\u6027", "conclusion": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7ed3\u5408\u4e3aLLM\u60c5\u611f\u52a8\u6001\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u589e\u5f3a\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u771f\u5b9e\u611f"}}
{"id": "2601.11742", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11742", "abs": "https://arxiv.org/abs/2601.11742", "authors": ["Jiayu Mao", "Ruoyu Sun", "Mark Poletti", "Rahil Gandotra", "Hao Guo", "Aylin Yener"], "title": "AI-Driven Spectrum Occupancy Prediction Using Real-World Spectrum Measurements", "comment": "8 pages, 7 figures. This paper is under review at an IEEE conference", "summary": "Spectrum occupancy prediction is a critical enabler for real-time and proactive dynamic spectrum sharing (DSS), as it can provide short-term channel availability information to support more efficient spectrum access decisions in wireless communication systems. Instead of relying on open-source datasets or simulated data, commonly used in the literature, this paper investigates short-horizon spectrum occupancy prediction using mid-band, 24X7 real-world spectrum measurement data collected in the United States. We construct a multi-band channel occupancy dataset through analyzing 61 days of empirical data and formulate a next-minute channel occupancy prediction task across all frequency channels. This study focuses on AI-driven prediction methods, including Random Forest, Extreme Gradient Boosting (XGBoost), and a Long Short-Term Memory (LSTM) network, and compares their performance against a conventional Markov chain-based statistical baseline. Numerical results show that learning-based methods outperform the statistical baseline on dynamic channels, particularly under fixed false-alarm constraints. These results demonstrate the effectiveness of AI-driven spectrum occupancy prediction, indicating that lightweight learning models can effectively support future deployment-oriented DSS systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4f7f\u7528\u7f8e\u56fd\u771f\u5b9e\u9891\u8c31\u6d4b\u91cf\u6570\u636e\uff0c\u7814\u7a76\u77ed\u65f6\u9891\u8c31\u5360\u7528\u9884\u6d4b\uff0c\u6bd4\u8f83AI\u65b9\u6cd5\u4e0e\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u52a8\u6001\u4fe1\u9053\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u9891\u8c31\u5360\u7528\u9884\u6d4b\u662f\u5b9e\u65f6\u52a8\u6001\u9891\u8c31\u5171\u4eab\u7684\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u5f00\u6e90\u6570\u636e\u96c6\u6216\u6a21\u62df\u6570\u636e\uff0c\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u9a8c\u8bc1\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u4e2d\u9891\u6bb524X7\u771f\u5b9e\u9891\u8c31\u6d4b\u91cf\u6570\u636e\u6784\u5efa\u591a\u9891\u6bb5\u4fe1\u9053\u5360\u7528\u6570\u636e\u96c6\uff0c\u5206\u679061\u5929\u7ecf\u9a8c\u6570\u636e\uff0c\u63d0\u51fa\u4e0b\u4e00\u5206\u949f\u4fe1\u9053\u5360\u7528\u9884\u6d4b\u4efb\u52a1\uff0c\u6bd4\u8f83\u968f\u673a\u68ee\u6797\u3001XGBoost\u3001LSTM\u7b49AI\u65b9\u6cd5\u4e0e\u9a6c\u5c14\u53ef\u592b\u94fe\u7edf\u8ba1\u57fa\u7ebf\u3002", "result": "\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u52a8\u6001\u4fe1\u9053\u4e2d\u4f18\u4e8e\u7edf\u8ba1\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u56fa\u5b9a\u8bef\u62a5\u7ea6\u675f\u4e0b\u3002\u8f7b\u91cf\u7ea7\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u652f\u6301\u672a\u6765\u90e8\u7f72\u5bfc\u5411\u7684\u52a8\u6001\u9891\u8c31\u5171\u4eab\u7cfb\u7edf\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u9891\u8c31\u5360\u7528\u9884\u6d4b\u662f\u6709\u6548\u7684\uff0c\u8f7b\u91cf\u7ea7\u5b66\u4e60\u6a21\u578b\u53ef\u4e3a\u672a\u6765\u52a8\u6001\u9891\u8c31\u5171\u4eab\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u652f\u6301\uff0c\u9a8c\u8bc1\u4e86\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0aAI\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.12023", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12023", "abs": "https://arxiv.org/abs/2601.12023", "authors": ["Longlin Yu", "Ziheng Cheng", "Shiyue Zhang", "Cheng Zhang"], "title": "A Kernel Approach for Semi-implicit Variational Inference", "comment": "40 pages, 15 figures. arXiv admin note: substantial text overlap with arXiv:2405.18997", "summary": "Semi-implicit variational inference (SIVI) enhances the expressiveness of variational families through hierarchical semi-implicit distributions, but the intractability of their densities makes standard ELBO-based optimization biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue via a minimax formulation, at the expense of an additional lower-level optimization problem. In this paper, we propose kernel semi-implicit variational inference (KSIVI), a principled and tractable alternative that eliminates the lower-level optimization by leveraging kernel methods. We show that when optimizing over a reproducing kernel Hilbert space, the lower-level problem admits an explicit solution, reducing the objective to the kernel Stein discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit distributions, the resulting KSD objective can be efficiently optimized using stochastic gradient methods. We establish optimization guarantees via variance bounds on Monte Carlo gradient estimators and derive statistical generalization bounds of order $\\tilde{\\mathcal{O}}(1/\\sqrt{n})$. We further introduce a multi-layer hierarchical extension that improves expressiveness while preserving tractability. Empirical results on synthetic and real-world Bayesian inference tasks demonstrate the effectiveness of KSIVI.", "AI": {"tldr": "KSIVI\u63d0\u51fa\u57fa\u4e8e\u6838\u65b9\u6cd5\u7684\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\uff0c\u901a\u8fc7\u6838Stein\u6563\u5ea6\u6d88\u9664\u989d\u5916\u4f18\u5316\u5c42\uff0c\u5728\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edfSIVI\u4f7f\u7528ELBO\u4f18\u5316\u5b58\u5728\u504f\u5dee\uff0c\u800cSIVI-SM\u867d\u7136\u89e3\u51b3\u4e86\u504f\u5dee\u95ee\u9898\u4f46\u5f15\u5165\u4e86\u989d\u5916\u7684\u4e0b\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u590d\u6742\u3002\u9700\u8981\u4e00\u79cd\u65e2\u65e0\u504f\u5dee\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faKSIVI\u65b9\u6cd5\uff0c\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4f18\u5316\uff0c\u5c06\u4e0b\u5c42\u95ee\u9898\u8f6c\u5316\u4e3a\u663e\u5f0f\u89e3\uff0c\u76ee\u6807\u51fd\u6570\u7b80\u5316\u4e3a\u6838Stein\u6563\u5ea6\u3002\u5229\u7528\u534a\u9690\u5f0f\u5206\u5e03\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u968f\u673a\u68af\u5ea6\u65b9\u6cd5\u9ad8\u6548\u4f18\u5316\u3002\u8fd8\u5f15\u5165\u4e86\u591a\u5c42\u5c42\u6b21\u6269\u5c55\u4ee5\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5efa\u7acb\u4e86\u8499\u7279\u5361\u6d1b\u68af\u5ea6\u4f30\u8ba1\u5668\u7684\u65b9\u5dee\u754c\u9650\u4f18\u5316\u4fdd\u8bc1\uff0c\u63a8\u5bfc\u4e86\u7edf\u8ba1\u6cdb\u5316\u754c\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u8d1d\u53f6\u65af\u63a8\u65ad\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "KSIVI\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u53ef\u5904\u7406\u7684\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u989d\u5916\u4f18\u5316\u5c42\uff0c\u5728\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2601.11568", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11568", "abs": "https://arxiv.org/abs/2601.11568", "authors": ["Quang-Hung Bui", "Anh Son Ta"], "title": "AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control", "comment": null, "summary": "Training Large Language Models (LLMs) is highly memory-intensive due to optimizer state overhead. The FRUGAL framework mitigates this with gradient splitting, but its static hyperparameters -- the subspace ratio ($\u03c1$) and update frequency ($T$) -- require costly manual tuning, limiting adaptability. We present AdaFRUGAL, which automates this process by introducing two dynamic controls: (i) a linear decay for $\u03c1$ to progressively reduce memory, and (ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments across large-scale pre-training (English C4, Vietnamese VietVault) and fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off. It maintains competitive performance against AdamW and static FRUGAL while significantly reducing both GPU memory and training time, offering a more practical, autonomous solution for resource-constrained LLM training.", "AI": {"tldr": "AdaFRUGAL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u68af\u5ea6\u5206\u5272\u7684\u8d85\u53c2\u6570\uff08\u5b50\u7a7a\u95f4\u6bd4\u4f8b\u03c1\u548c\u66f4\u65b0\u9891\u7387T\uff09\uff0c\u81ea\u52a8\u4f18\u5316\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11LLM\u8bad\u7ec3\u7684\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u73b0\u6709FRUGAL\u6846\u67b6\u867d\u7136\u901a\u8fc7\u68af\u5ea6\u5206\u5272\u51cf\u5c11LLM\u8bad\u7ec3\u7684\u5185\u5b58\u5360\u7528\uff0c\u4f46\u5176\u9759\u6001\u8d85\u53c2\u6570\u9700\u8981\u624b\u52a8\u8c03\u4f18\uff0c\u6210\u672c\u9ad8\u6602\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faAdaFRUGAL\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u79cd\u52a8\u6001\u63a7\u5236\u673a\u5236\uff1a1) \u7ebf\u6027\u8870\u51cf\u5b50\u7a7a\u95f4\u6bd4\u4f8b\u03c1\u4ee5\u9010\u6b65\u51cf\u5c11\u5185\u5b58\u5360\u7528\uff1b2) \u57fa\u4e8e\u635f\u5931\u611f\u77e5\u7684\u66f4\u65b0\u9891\u7387T\u8c03\u5ea6\u4ee5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u82f1\u6587C4\u3001\u8d8a\u5357\u8bedVietVault\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548cGLUE\u5fae\u8c03\u5b9e\u9a8c\u4e2d\uff0cAdaFRUGAL\u5728\u4fdd\u6301\u4e0eAdamW\u548c\u9759\u6001FRUGAL\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86GPU\u5185\u5b58\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "AdaFRUGAL\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u3001\u81ea\u4e3b\u7684\u8d44\u6e90\u53d7\u9650LLM\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2601.12562", "categories": ["eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.12562", "abs": "https://arxiv.org/abs/2601.12562", "authors": ["Maaz Qureshi", "Mohammad Omid Bagheri", "Abdelrahman Elbadrawy", "William Melek", "George Shaker"], "title": "Automated Angular Received-Power Characterization of Embedded mmWave Transmitters Using Geometry-Calibrated Spatial Sampling", "comment": null, "summary": "This paper presents an automated measurement methodology for angular received-power characterization of embedded millimeter-wave transmitters using geometry-calibrated spatial sampling. Characterization of integrated mmWave transmitters remains challenging due to limited angular coverage and alignment variability in conventional probe-station techniques, as well as the impracticality of anechoic-chamber testing for platform-mounted active modules. To address these challenges, we introduce RAPTAR, an autonomous measurement system for angular received-power acquisition under realistic installation constraints. A collaborative robot executes geometry-calibrated, collision-aware hemispherical trajectories while carrying a calibrated receive probe, enabling controlled and repeatable spatial positioning around a fixed device under test. A spectrum-analyzer-based receiver chain acquires amplitude-only received power as a function of angle and distance following quasi-static pose stabilization. The proposed framework enables repeatable angular received-power mapping and power-domain comparison against idealized free-space references derived from full-wave simulation. Experimental results for a 60-GHz radar module demonstrate a mean absolute received-power error below 2 dB relative to simulation-derived references and a 36.5 % reduction in error compared to manual probe-station measurements, attributed primarily to reduced alignment variability and consistent spatial sampling. The proposed method eliminates the need for coherent field measurements and near-field transformations, enabling practical power-domain characterization of embedded mmWave modules. It is well suited for angular validation in real-world platforms where conventional anechoic measurements are impractical.", "AI": {"tldr": "RAPTAR\uff1a\u4e00\u79cd\u7528\u4e8e\u5d4c\u5165\u5f0f\u6beb\u7c73\u6ce2\u53d1\u5c04\u673a\u89d2\u5ea6\u63a5\u6536\u529f\u7387\u81ea\u52a8\u6d4b\u91cf\u7684\u51e0\u4f55\u6821\u51c6\u7a7a\u95f4\u91c7\u6837\u7cfb\u7edf\uff0c\u76f8\u6bd4\u4f20\u7edf\u63a2\u9488\u53f0\u6d4b\u91cf\u8bef\u5dee\u964d\u4f4e36.5%", "motivation": "\u4f20\u7edf\u63a2\u9488\u53f0\u6280\u672f\u89d2\u5ea6\u8986\u76d6\u6709\u9650\u4e14\u5bf9\u51c6\u53ef\u53d8\u6027\u5927\uff0c\u800c\u6697\u5ba4\u6d4b\u8bd5\u5bf9\u5e73\u53f0\u5b89\u88c5\u7684\u4e3b\u52a8\u6a21\u5757\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5728\u5b9e\u9645\u5b89\u88c5\u7ea6\u675f\u4e0b\u8fdb\u884c\u89d2\u5ea6\u63a5\u6536\u529f\u7387\u6d4b\u91cf\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528\u534f\u4f5c\u673a\u5668\u4eba\u6267\u884c\u51e0\u4f55\u6821\u51c6\u3001\u78b0\u649e\u611f\u77e5\u7684\u534a\u7403\u5f62\u8f68\u8ff9\uff0c\u643a\u5e26\u6821\u51c6\u63a5\u6536\u63a2\u9488\u56f4\u7ed5\u56fa\u5b9a\u88ab\u6d4b\u8bbe\u5907\u8fdb\u884c\u53ef\u63a7\u53ef\u91cd\u590d\u7684\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u901a\u8fc7\u9891\u8c31\u5206\u6790\u4eea\u63a5\u6536\u94fe\u83b7\u53d6\u89d2\u5ea6\u548c\u8ddd\u79bb\u51fd\u6570\u7684\u5e45\u5ea6\u63a5\u6536\u529f\u7387", "result": "60GHz\u96f7\u8fbe\u6a21\u5757\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u5bf9\u4e8e\u4eff\u771f\u53c2\u8003\u7684\u5e73\u5747\u7edd\u5bf9\u63a5\u6536\u529f\u7387\u8bef\u5dee\u4f4e\u4e8e2dB\uff0c\u76f8\u6bd4\u624b\u52a8\u63a2\u9488\u53f0\u6d4b\u91cf\u8bef\u5dee\u964d\u4f4e36.5%\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u51cf\u5c11\u4e86\u5bf9\u51c6\u53ef\u53d8\u6027\u548c\u4e00\u81f4\u7684\u7a7a\u95f4\u91c7\u6837", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u76f8\u5e72\u573a\u6d4b\u91cf\u548c\u8fd1\u573a\u53d8\u6362\uff0c\u5b9e\u73b0\u4e86\u5d4c\u5165\u5f0f\u6beb\u7c73\u6ce2\u6a21\u5757\u7684\u5b9e\u9645\u529f\u7387\u57df\u8868\u5f81\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f20\u7edf\u6697\u5ba4\u6d4b\u91cf\u4e0d\u5b9e\u7528\u7684\u771f\u5b9e\u4e16\u754c\u5e73\u53f0\u7684\u89d2\u5ea6\u9a8c\u8bc1"}}
{"id": "2601.11748", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11748", "abs": "https://arxiv.org/abs/2601.11748", "authors": ["Rahil Gandotra", "Ruoyu Sun", "Mark Poletti", "Jiayu Mao", "Hao Guo"], "title": "Automated Spectrum Sensing and Analysis Framework", "comment": "This paper is under review at an IEEE conference", "summary": "Spectrum sensing and analysis is crucial for a variety of reasons, including regulatory compliance, interference detection and mitigation, and spectrum resource planning and optimization. Effective, real-time spectrum analysis remains a challenge, stemming from the need to analyse an increasingly complex and dynamic environment with limited resources. The vast amount of data generated from sensing the spectrum at multiple sites requires sophisticated data analysis and processing techniques, which can be technically demanding and expensive. This paper presents a novel, holistic framework developed and deployed at multiple locations across the USA for spectrum analysis and describes the different parts of the end-to-end pipeline. The details of each of the modules of the pipeline, data collection and pre-processing at remote locations, transfer to a centralized location, post-processing analysis, visualization, and long-term storage, are reported. The motivation behind this work is to develop a robust spectrum analysis framework that can help gain greater insights into the spectrum usage across the country and augment additional use cases such as dynamic spectrum sharing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7f8e\u56fd\u591a\u5730\u90e8\u7f72\u7684\u7aef\u5230\u7aef\u9891\u8c31\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u9891\u8c31\u611f\u77e5\u3001\u6570\u636e\u5904\u7406\u548c\u53ef\u89c6\u5316\uff0c\u4ee5\u652f\u6301\u9891\u8c31\u8d44\u6e90\u7ba1\u7406\u548c\u52a8\u6001\u9891\u8c31\u5171\u4eab\u3002", "motivation": "\u9891\u8c31\u611f\u77e5\u4e0e\u5206\u6790\u5bf9\u4e8e\u76d1\u7ba1\u5408\u89c4\u3001\u5e72\u6270\u68c0\u6d4b\u4e0e\u7f13\u89e3\u3001\u9891\u8c31\u8d44\u6e90\u89c4\u5212\u4e0e\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5b9e\u65f6\u9891\u8c31\u5206\u6790\u9762\u4e34\u6311\u6218\uff1a\u9700\u8981\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5206\u6790\u65e5\u76ca\u590d\u6742\u52a8\u6001\u7684\u73af\u5883\uff0c\u6d77\u91cf\u9891\u8c31\u6570\u636e\u9700\u8981\u590d\u6742\u7684\u6570\u636e\u5206\u6790\u5904\u7406\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u65e2\u8981\u6c42\u9ad8\u53c8\u6210\u672c\u6602\u8d35\u3002", "method": "\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u9891\u8c31\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ec\u591a\u4e2a\u6a21\u5757\uff1a\u8fdc\u7a0b\u4f4d\u7f6e\u7684\u6570\u636e\u6536\u96c6\u4e0e\u9884\u5904\u7406\u3001\u6570\u636e\u4f20\u8f93\u5230\u96c6\u4e2d\u4f4d\u7f6e\u3001\u540e\u5904\u7406\u5206\u6790\u3001\u53ef\u89c6\u5316\u4ee5\u53ca\u957f\u671f\u5b58\u50a8\u3002\u8be5\u6846\u67b6\u5728\u7f8e\u56fd\u591a\u4e2a\u5730\u70b9\u90e8\u7f72\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u9891\u8c31\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4ece\u6570\u636e\u6536\u96c6\u5230\u957f\u671f\u5b58\u50a8\u7684\u5168\u6d41\u7a0b\uff0c\u4e3a\u5168\u56fd\u8303\u56f4\u5185\u7684\u9891\u8c31\u4f7f\u7528\u63d0\u4f9b\u6df1\u5165\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9891\u8c31\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u5168\u56fd\u9891\u8c31\u4f7f\u7528\u60c5\u51b5\uff0c\u5e76\u652f\u6301\u52a8\u6001\u9891\u8c31\u5171\u4eab\u7b49\u9644\u52a0\u7528\u4f8b\u3002"}}
{"id": "2601.12238", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.12238", "abs": "https://arxiv.org/abs/2601.12238", "authors": ["Sharan Sahu", "Cameron J. Hogan", "Martin T. Wells"], "title": "On the Provable Suboptimality of Momentum SGD in Nonstationary Stochastic Optimization", "comment": "70 pages, 4 figures, 2 tables", "summary": "While momentum-based acceleration has been studied extensively in deterministic optimization problems, its behavior in nonstationary environments -- where the data distribution and optimal parameters drift over time -- remains underexplored. We analyze the tracking performance of Stochastic Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and Nesterov) under uniform strong convexity and smoothness in varying stepsize regimes. We derive finite-time bounds in expectation and with high probability for the tracking error, establishing a sharp decomposition into three components: a transient initialization term, a noise-induced variance term, and a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental trade-off: while momentum can suppress gradient noise, it incurs an explicit penalty on the tracking capability. We show that momentum can substantially amplify drift-induced tracking error, with amplification that becomes unbounded as the momentum parameter approaches one, formalizing the intuition that using 'stale' gradients hinders adaptation to rapid regime shifts. Complementing these upper bounds, we establish minimax lower bounds for dynamic regret under gradient-variation constraints. These lower bounds prove that the inertia-induced penalty is not an artifact of analysis but an information-theoretic barrier: in drift-dominated regimes, momentum creates an unavoidable 'inertia window' that fundamentally degrades performance. Collectively, these results provide a definitive theoretical grounding for the empirical instability of momentum in dynamic environments and delineate the precise regime boundaries where SGD provably outperforms its accelerated counterparts.", "AI": {"tldr": "\u52a8\u91cf\u4f18\u5316\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u5206\u6790\uff1a\u52a8\u91cf\u867d\u80fd\u6291\u5236\u68af\u5ea6\u566a\u58f0\uff0c\u4f46\u4f1a\u663e\u8457\u653e\u5927\u6f02\u79fb\u5bfc\u81f4\u7684\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5728\u5feb\u901f\u53d8\u5316\u73af\u5883\u4e2d\u53ef\u80fd\u52a3\u4e8e\u666e\u901aSGD\u3002", "motivation": "\u52a8\u91cf\u52a0\u901f\u65b9\u6cd5\u5728\u786e\u5b9a\u6027\u4f18\u5316\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u975e\u5e73\u7a33\u73af\u5883\uff08\u6570\u636e\u5206\u5e03\u548c\u6700\u4f18\u53c2\u6570\u968f\u65f6\u95f4\u6f02\u79fb\uff09\u4e2d\u7684\u884c\u4e3a\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u7406\u89e3SGD\u53ca\u5176\u52a8\u91cf\u53d8\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "\u5728\u5747\u5300\u5f3a\u51f8\u6027\u548c\u5e73\u6ed1\u6027\u5047\u8bbe\u4e0b\uff0c\u5206\u6790SGD\u3001Polyak heavy-ball\u548cNesterov\u52a8\u91cf\u65b9\u6cd5\u5728\u4e0d\u540c\u6b65\u957f\u673a\u5236\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\u3002\u63a8\u5bfc\u6709\u9650\u65f6\u95f4\u671f\u671b\u548c\u9ad8\u6982\u7387\u8ddf\u8e2a\u8bef\u5dee\u754c\uff0c\u5efa\u7acb\u5305\u542b\u521d\u59cb\u5316\u77ac\u6001\u9879\u3001\u566a\u58f0\u65b9\u5dee\u9879\u548c\u6f02\u79fb\u8ddf\u8e2a\u6ede\u540e\u9879\u7684\u4e09\u90e8\u5206\u5206\u89e3\u3002", "result": "\u53d1\u73b0\u52a8\u91cf\u65b9\u6cd5\u5b58\u5728\u57fa\u672c\u6743\u8861\uff1a\u867d\u7136\u80fd\u6291\u5236\u68af\u5ea6\u566a\u58f0\uff0c\u4f46\u4f1a\u663e\u8457\u589e\u52a0\u8ddf\u8e2a\u6ede\u540e\u60e9\u7f5a\u3002\u52a8\u91cf\u4f1a\u653e\u5927\u6f02\u79fb\u5bfc\u81f4\u7684\u8ddf\u8e2a\u8bef\u5dee\uff0c\u4e14\u5f53\u52a8\u91cf\u53c2\u6570\u63a5\u8fd11\u65f6\u653e\u5927\u6548\u5e94\u65e0\u754c\u3002\u5efa\u7acb\u4e86\u68af\u5ea6\u53d8\u5316\u7ea6\u675f\u4e0b\u7684\u52a8\u6001\u9057\u61be\u6781\u5c0f\u6781\u5927\u4e0b\u754c\uff0c\u8bc1\u660e\u60ef\u6027\u60e9\u7f5a\u662f\u4fe1\u606f\u8bba\u969c\u788d\u800c\u975e\u5206\u6790\u5047\u8c61\u3002", "conclusion": "\u52a8\u91cf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\"\u60ef\u6027\u7a97\u53e3\"\uff0c\u5728\u6f02\u79fb\u4e3b\u5bfc\u673a\u5236\u4e0b\u4f1a\u4ece\u6839\u672c\u4e0a\u964d\u4f4e\u6027\u80fd\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u52a8\u91cf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7ecf\u9a8c\u4e0d\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u5212\u5b9a\u4e86SGD\u4f18\u4e8e\u52a0\u901f\u53d8\u4f53\u7684\u7cbe\u786e\u673a\u5236\u8fb9\u754c\u3002"}}
{"id": "2601.11572", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11572", "abs": "https://arxiv.org/abs/2601.11572", "authors": ["Timo Aukusti Laine"], "title": "Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding Spaces", "comment": "23 pages, 5 figures", "summary": "We investigate the structure of Large Language Model (LLM) embedding spaces using mathematical concepts, particularly linear algebra and the Hamiltonian formalism, drawing inspiration from analogies with quantum mechanical systems. Motivated by the observation that LLM embeddings exhibit distinct states, suggesting discrete semantic representations, we explore the application of these mathematical tools to analyze semantic relationships. We demonstrate that the L2 normalization constraint, a characteristic of many LLM architectures, results in a structured embedding space suitable for analysis using a Hamiltonian formalism. We derive relationships between cosine similarity and perturbations of embedding vectors, and explore direct and indirect semantic transitions. Furthermore, we explore a quantum-inspired perspective, deriving an analogue of zero-point energy and discussing potential connections to Koopman-von Neumann mechanics. While the interpretation warrants careful consideration, our results suggest that this approach offers a promising avenue for gaining deeper insights into LLMs and potentially informing new methods for mitigating hallucinations.", "AI": {"tldr": "\u4f7f\u7528\u7ebf\u6027\u4ee3\u6570\u548c\u54c8\u5bc6\u987f\u5f62\u5f0f\u5206\u6790LLM\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u53d1\u73b0L2\u5f52\u4e00\u5316\u7ea6\u675f\u4f7f\u5d4c\u5165\u7a7a\u95f4\u9002\u5408\u54c8\u5bc6\u987f\u5206\u6790\uff0c\u63a8\u5bfc\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u5173\u7cfb\uff0c\u63a2\u7d22\u91cf\u5b50\u529b\u5b66\u7c7b\u6bd4", "motivation": "\u89c2\u5bdf\u5230LLM\u5d4c\u5165\u5448\u73b0\u79bb\u6563\u8bed\u4e49\u72b6\u6001\uff0c\u9700\u8981\u6570\u5b66\u5de5\u5177\u5206\u6790\u8bed\u4e49\u5173\u7cfb\uff0c\u7279\u522b\u662f\u91cf\u5b50\u529b\u5b66\u7cfb\u7edf\u7c7b\u6bd4\u53ef\u80fd\u63d0\u4f9b\u65b0\u89c6\u89d2", "method": "\u5e94\u7528\u7ebf\u6027\u4ee3\u6570\u548c\u54c8\u5bc6\u987f\u5f62\u5f0f\u5206\u6790LLM\u5d4c\u5165\u7a7a\u95f4\uff0c\u63a8\u5bfcL2\u5f52\u4e00\u5316\u7ea6\u675f\u4e0b\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u63a2\u7d22\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u76f4\u63a5\u548c\u95f4\u63a5\u8bed\u4e49\u8f6c\u6362", "result": "\u53d1\u73b0L2\u5f52\u4e00\u5316\u7ea6\u675f\u4f7f\u5d4c\u5165\u7a7a\u95f4\u9002\u5408\u54c8\u5bc6\u987f\u5f62\u5f0f\u5206\u6790\uff0c\u63a8\u5bfc\u51fa\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4e0e\u5411\u91cf\u6270\u52a8\u7684\u6570\u5b66\u5173\u7cfb\uff0c\u5efa\u7acb\u4e86\u91cf\u5b50\u529b\u5b66\u7c7b\u6bd4\uff08\u5982\u96f6\u70b9\u80fd\u91cf\uff09\uff0c\u5e76\u63a2\u7d22\u4e86\u4e0eKoopman-von Neumann\u529b\u5b66\u7684\u6f5c\u5728\u8054\u7cfb", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5165\u7406\u89e3LLM\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\uff0c\u53ef\u80fd\u6709\u52a9\u4e8e\u5f00\u53d1\u51cf\u8f7b\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u4f46\u89e3\u91ca\u9700\u8981\u8c28\u614e\u8003\u8651"}}
{"id": "2601.12699", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.12699", "abs": "https://arxiv.org/abs/2601.12699", "authors": ["Arkaprava Gupta", "Nicholas Carter", "William Zellers", "Prateek Ganguli", "Benedikt Dietrich", "Vibhor Krishna", "Parasara Sridhar Duggirala", "Samarjit Chakraborty"], "title": "Resource-Conscious RL Algorithms for Deep Brain Stimulation", "comment": null, "summary": "Deep Brain Stimulation (DBS) has proven to be a promising treatment of Parkinson's Disease (PD). DBS involves stimulating specific regions of the brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD such as tremors, rigidity, and bradykinesia. Although most clinical DBS approaches today use a fixed frequency and amplitude, they suffer from side effects (such as slurring of speech) and shortened battery life of the implant. Reinforcement learning (RL) approaches have been used in recent research to perform DBS in a more adaptive manner to improve overall patient outcome. These RL algorithms are, however, too complex to be trained in vivo due to their long convergence time and requirement of high computational resources.\n  We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL approach for DBS that is more effective than existing algorithms. Further, our T3P agent is lightweight enough to be deployed in the implant, unlike current deep-RL strategies, and even forgoes the need for an offline training phase. Additionally, most existing RL approaches have focused on modulating only frequency or amplitude, and the possibility of tuning them together remains greatly unexplored in the literature. Our RL agent can tune both frequency and amplitude of DBS signals to the brain with better sample efficiency and requires minimal time to converge. We implement an MAB agent for DBS for the first time on hardware to report energy measurements and prove its suitability for resource-constrained platforms. Our T3P MAB algorithm is deployed on a variety of microcontroller unit (MCU) setups to show its efficiency in terms of power consumption as opposed to other existing RL approaches used in recent work.", "AI": {"tldr": "\u63d0\u51faT3P MAB\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7528\u4e8e\u5e15\u91d1\u68ee\u75c5\u6df1\u90e8\u8111\u523a\u6fc0\uff0c\u80fd\u540c\u65f6\u8c03\u8282\u9891\u7387\u548c\u632f\u5e45\uff0c\u8ba1\u7b97\u8f7b\u91cf\u9002\u5408\u690d\u5165\u8bbe\u5907\uff0c\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edfDBS\u4f7f\u7528\u56fa\u5b9a\u9891\u7387\u548c\u632f\u5e45\u5b58\u5728\u526f\u4f5c\u7528\u548c\u7535\u6c60\u5bff\u547d\u77ed\u7684\u95ee\u9898\uff0c\u73b0\u6709RL\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u96be\u4ee5\u5728\u4f53\u5185\u8bad\u7ec3\uff0c\u4e14\u5927\u591a\u53ea\u8c03\u8282\u5355\u4e00\u53c2\u6570\u3002", "method": "\u63d0\u51fa\u65f6\u95f4\u4e0e\u9608\u503c\u89e6\u53d1\u7684\u591a\u81c2\u8001\u864e\u673a\uff08T3P MAB\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u540c\u65f6\u8c03\u8282DBS\u7684\u9891\u7387\u548c\u632f\u5e45\uff0c\u8ba1\u7b97\u8f7b\u91cf\u9002\u5408\u5fae\u63a7\u5236\u5668\u90e8\u7f72\u3002", "result": "T3P MAB\u7b97\u6cd5\u5728\u591a\u79cdMCU\u5e73\u53f0\u4e0a\u5b9e\u73b0\uff0c\u80fd\u8017\u6d4b\u91cf\u663e\u793a\u9002\u5408\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\uff0c\u6bd4\u73b0\u6709RL\u65b9\u6cd5\u6837\u672c\u6548\u7387\u66f4\u9ad8\u3001\u6536\u655b\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "T3P MAB\u4e3aDBS\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u3001\u81ea\u9002\u5e94\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u540c\u65f6\u4f18\u5316\u9891\u7387\u548c\u632f\u5e45\uff0c\u9002\u5408\u690d\u5165\u5f0f\u8bbe\u5907\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8bad\u7ec3\u56f0\u96be\u95ee\u9898\u3002"}}
{"id": "2601.11844", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.11844", "abs": "https://arxiv.org/abs/2601.11844", "authors": ["Yue Bi", "Mich\u00e8le Wigger"], "title": "Necessity of Cooperative Transmissions for Wireless MapReduce", "comment": null, "summary": "The paper presents an improved upper bound (achievability result) on the optimal tradeoff between Normalized Delivery Time (NDT) and computation load for distributed computing MapReduce systems in certain ranges of the parameters. The upper bound is based on interference alignment combined with zero-forcing. The paper further provides a lower bound (converse) on the optimal NDT-computation tradeoff that can be achieved when IVAs are partitioned into sub-IVAs, and these sub-IVAs are then transmitted (in an arbitrary form) by a single node, without cooperation among nodes. For appropriate linear functions (e.g., XORs), such non-cooperative schemes can achieve some of the best NDT-computation tradeoff points so far obtained in the literature. However, as our lower bound shows, any non-cooperative scheme achieves a worse NDT-computation tradeoff than our new proposed scheme for certain parameters, thus proving the necessity of cooperative schemes like zero-forcing to attain the optimal NDT-computation tradeoff.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86\u5206\u5e03\u5f0fMapReduce\u7cfb\u7edf\u4e2dNDT\u4e0e\u8ba1\u7b97\u8d1f\u8f7d\u6743\u8861\u7684\u4e0a\u754c\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5e72\u6270\u5bf9\u9f50\u548c\u8feb\u96f6\u7684\u534f\u4f5c\u65b9\u6848\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u67d0\u4e9b\u53c2\u6570\u4e0b\u975e\u534f\u4f5c\u65b9\u6848\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u8ba1\u7b97MapReduce\u7cfb\u7edf\u4e2d\u5f52\u4e00\u5316\u4ea4\u4ed8\u65f6\u95f4(NDT)\u4e0e\u8ba1\u7b97\u8d1f\u8f7d\u4e4b\u95f4\u7684\u6700\u4f18\u6743\u8861\uff0c\u63a2\u7d22\u534f\u4f5c\u4f20\u8f93\u65b9\u6848\u76f8\u5bf9\u4e8e\u975e\u534f\u4f5c\u65b9\u6848\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u5e72\u6270\u5bf9\u9f50\u4e0e\u8feb\u96f6\u6280\u672f\u76f8\u7ed3\u5408\u7684\u534f\u4f5c\u4f20\u8f93\u65b9\u6848\uff0c\u540c\u65f6\u5206\u6790\u4e86\u975e\u534f\u4f5c\u65b9\u6848\uff08\u5355\u4e2a\u8282\u70b9\u4f20\u8f93\u5b50\u4e2d\u95f4\u503c\uff09\u7684\u6027\u80fd\u4e0b\u754c\u3002", "result": "\u63d0\u51fa\u4e86\u6539\u8fdb\u7684NDT-\u8ba1\u7b97\u6743\u8861\u4e0a\u754c\uff0c\u8bc1\u660e\u4e86\u5728\u67d0\u4e9b\u53c2\u6570\u8303\u56f4\u5185\uff0c\u534f\u4f5c\u65b9\u6848\u4f18\u4e8e\u975e\u534f\u4f5c\u65b9\u6848\uff0c\u4e14\u975e\u534f\u4f5c\u65b9\u6848\u65e0\u6cd5\u8fbe\u5230\u6700\u4f18NDT-\u8ba1\u7b97\u6743\u8861\u3002", "conclusion": "\u5bf9\u4e8e\u67d0\u4e9b\u53c2\u6570\u914d\u7f6e\uff0c\u5fc5\u987b\u91c7\u7528\u534f\u4f5c\u4f20\u8f93\u65b9\u6848\uff08\u5982\u8feb\u96f6\uff09\u624d\u80fd\u8fbe\u5230\u6700\u4f18NDT-\u8ba1\u7b97\u6743\u8861\uff0c\u800c\u975e\u534f\u4f5c\u65b9\u6848\u5b58\u5728\u6027\u80fd\u9650\u5236\u3002"}}
{"id": "2601.12587", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12587", "abs": "https://arxiv.org/abs/2601.12587", "authors": ["Frank Cole", "Yulong Lu", "Shaurya Sehgal"], "title": "A Theory of Diversity for Random Matrices with Applications to In-Context Learning of Schr\u00f6dinger Equations", "comment": null, "summary": "We address the following question: given a collection $\\{\\mathbf{A}^{(1)}, \\dots, \\mathbf{A}^{(N)}\\}$ of independent $d \\times d$ random matrices drawn from a common distribution $\\mathbb{P}$, what is the probability that the centralizer of $\\{\\mathbf{A}^{(1)}, \\dots, \\mathbf{A}^{(N)}\\}$ is trivial? We provide lower bounds on this probability in terms of the sample size $N$ and the dimension $d$ for several families of random matrices which arise from the discretization of linear Schr\u00f6dinger operators with random potentials. When combined with recent work on machine learning theory, our results provide guarantees on the generalization ability of transformer-based neural networks for in-context learning of Schr\u00f6dinger equations.", "AI": {"tldr": "\u7814\u7a76\u968f\u673a\u77e9\u9635\u96c6\u5408\u4e2d\u5fc3\u5316\u5b50\u4e3a\u5e73\u51e1\u7684\u6982\u7387\u4e0b\u754c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eTransformer\u7f51\u7edc\u5728\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4fdd\u8bc1", "motivation": "\u7814\u7a76\u968f\u673a\u77e9\u9635\u96c6\u5408\u4e2d\u5fc3\u5316\u5b50\u4e3a\u5e73\u51e1\u7684\u6982\u7387\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8e\u968f\u673a\u52bf\u80fd\u7ebf\u6027\u859b\u5b9a\u8c14\u7b97\u5b50\u79bb\u6563\u5316\uff0c\u5e76\u4e0e\u673a\u5668\u5b66\u4e60\u7406\u8bba\u7ed3\u5408\uff0c\u4e3aTransformer\u7f51\u7edc\u5728\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1", "method": "\u9488\u5bf9\u4ece\u5171\u540c\u5206\u5e03\u4e2d\u62bd\u53d6\u7684\u72ec\u7acb\u968f\u673a\u77e9\u9635\u96c6\u5408\uff0c\u57fa\u4e8e\u6837\u672c\u5927\u5c0fN\u548c\u7ef4\u5ea6d\u63d0\u4f9b\u4e2d\u5fc3\u5316\u5b50\u4e3a\u5e73\u51e1\u7684\u6982\u7387\u4e0b\u754c\uff0c\u7279\u522b\u5173\u6ce8\u968f\u673a\u52bf\u80fd\u859b\u5b9a\u8c14\u7b97\u5b50\u79bb\u6563\u5316\u4ea7\u751f\u7684\u968f\u673a\u77e9\u9635\u65cf", "result": "\u4e3a\u591a\u4e2a\u968f\u673a\u77e9\u9635\u65cf\u63d0\u4f9b\u4e86\u4e2d\u5fc3\u5316\u5b50\u4e3a\u5e73\u51e1\u7684\u6982\u7387\u4e0b\u754c\uff0c\u8fd9\u4e9b\u4e0b\u754c\u4e0e\u6837\u672c\u5927\u5c0fN\u548c\u7ef4\u5ea6d\u76f8\u5173\uff0c\u5f53\u4e0e\u8fd1\u671f\u673a\u5668\u5b66\u4e60\u7406\u8bba\u7ed3\u5408\u65f6\uff0c\u80fd\u4e3aTransformer\u7f51\u7edc\u5728\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u6cdb\u5316\u80fd\u529b\u4fdd\u8bc1", "conclusion": "\u968f\u673a\u77e9\u9635\u4e2d\u5fc3\u5316\u5b50\u5e73\u51e1\u6027\u7684\u6982\u7387\u4e0b\u754c\u5206\u6790\u4e3aTransformer\u7f51\u7edc\u5728\u859b\u5b9a\u8c14\u65b9\u7a0b\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u8fde\u63a5\u4e86\u968f\u673a\u77e9\u9635\u7406\u8bba\u4e0e\u673a\u5668\u5b66\u4e60\u5e94\u7528"}}
{"id": "2601.11574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11574", "abs": "https://arxiv.org/abs/2601.11574", "authors": ["Lukas Abrie Nel"], "title": "GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment", "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) has become the dominant paradigm for aligning large language models with human preferences. However, policy gradient methods such as PPO suffer from high variance gradient estimates, requiring careful hyperparameter tuning and extensive computational resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via Differentiable Estimation), a method that replaces high-variance policy gradient estimation with direct backpropagation through a differentiable relaxation of the discrete token sampling process. Using the Gumbel-Softmax reparameterization with straight-through estimation (GRADE-STE), we enable end-to-end gradient flow from reward signals through generated tokens to model parameters. On sentiment-controlled text generation using the IMDB dataset, GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +- 0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower than REINFORCE and maintains stable training dynamics throughout optimization. Our rigorous evaluation with proper train/validation/test splits demonstrates that these improvements generalize to held-out data, with GRADE-STE showing the best generalization characteristics among all methods tested. GRADE offers a simpler, more stable, and more effective alternative to reinforcement learning for LLM alignment.", "AI": {"tldr": "GRADE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGumbel-Softmax\u91cd\u53c2\u6570\u5316\u7684\u53ef\u5fae\u5bf9\u9f50\u65b9\u6cd5\uff0c\u66ff\u4ee3\u9ad8\u65b9\u5dee\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5956\u52b1\u6027\u80fd\u548c\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u7684RLHF\u65b9\u6cd5\u5982PPO\u5b58\u5728\u68af\u5ea6\u4f30\u8ba1\u65b9\u5dee\u9ad8\u3001\u9700\u8981\u7cbe\u7ec6\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "GRADE\u4f7f\u7528Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u914d\u5408\u76f4\u901a\u4f30\u8ba1\uff08GRADE-STE\uff09\uff0c\u901a\u8fc7\u79bb\u6563token\u91c7\u6837\u8fc7\u7a0b\u7684\u53ef\u5fae\u677e\u5f1b\u5b9e\u73b0\u7aef\u5230\u7aef\u68af\u5ea6\u4f20\u64ad\uff0c\u66ff\u4ee3\u9ad8\u65b9\u5dee\u7684\u7b56\u7565\u68af\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728IMDB\u6570\u636e\u96c6\u7684\u60c5\u611f\u63a7\u5236\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\uff0cGRADE-STE\u83b7\u5f970.763\u00b10.344\u7684\u6d4b\u8bd5\u5956\u52b1\uff0c\u4f18\u4e8ePPO\u76840.510\u00b10.313\u548cREINFORCE\u76840.617\u00b10.378\uff0c\u68af\u5ea6\u65b9\u5dee\u6bd4REINFORCE\u4f4e14\u500d\u4ee5\u4e0a\uff0c\u8bad\u7ec3\u52a8\u6001\u66f4\u7a33\u5b9a\u3002", "conclusion": "GRADE\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6d4b\u8bd5\u96c6\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u590d\u6742\u6027\u3002"}}
{"id": "2601.13357", "categories": ["cs.LG", "cs.CL", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.13357", "abs": "https://arxiv.org/abs/2601.13357", "authors": ["Aydin Ghojogh", "M. Hadi Sepanj", "Benyamin Ghojogh"], "title": "On the Relation of State Space Models and Hidden Markov Models", "comment": null, "summary": "State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational frameworks for modeling sequential data with latent variables and are widely used in signal processing, control theory, and machine learning. Despite their shared temporal structure, they differ fundamentally in the nature of their latent states, probabilistic assumptions, inference procedures, and training paradigms. Recently, deterministic state space models have re-emerged in natural language processing through architectures such as S4 and Mamba, raising new questions about the relationship between classical probabilistic SSMs, HMMs, and modern neural sequence models.\n  In this paper, we present a unified and systematic comparison of HMMs, linear Gaussian state space models, Kalman filtering, and contemporary NLP state space models. We analyze their formulations through the lens of probabilistic graphical models, examine their inference algorithms -- including forward-backward inference and Kalman filtering -- and contrast their learning procedures via Expectation-Maximization and gradient-based optimization. By highlighting both structural similarities and semantic differences, we clarify when these models are equivalent, when they fundamentally diverge, and how modern NLP SSMs relate to classical probabilistic models. Our analysis bridges perspectives from control theory, probabilistic modeling, and modern deep learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86HMM\u3001\u7ebf\u6027\u9ad8\u65af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e0e\u5f53\u4ee3NLP\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5206\u6790\u5b83\u4eec\u5728\u6982\u7387\u56fe\u6a21\u578b\u3001\u63a8\u7406\u7b97\u6cd5\u548c\u5b66\u4e60\u65b9\u6cd5\u4e0a\u7684\u5f02\u540c\uff0c\u5e76\u63a2\u8ba8\u73b0\u4ee3NLP SSM\u4e0e\u7ecf\u5178\u6982\u7387\u6a21\u578b\u7684\u5173\u7cfb\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u548c\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMMs\uff09\u662f\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u7684\u57fa\u7840\u6846\u67b6\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4fe1\u53f7\u5904\u7406\u3001\u63a7\u5236\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u3002\u5c3d\u7ba1\u5b83\u4eec\u5177\u6709\u76f8\u4f3c\u7684\u65f6\u95f4\u7ed3\u6784\uff0c\u4f46\u5728\u6f5c\u5728\u72b6\u6001\u6027\u8d28\u3001\u6982\u7387\u5047\u8bbe\u3001\u63a8\u7406\u8fc7\u7a0b\u548c\u5b66\u4e60\u8303\u5f0f\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\u3002\u8fd1\u5e74\u6765\uff0c\u786e\u5b9a\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u901a\u8fc7S4\u548cMamba\u7b49\u67b6\u6784\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u91cd\u65b0\u51fa\u73b0\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u7ecf\u5178\u6982\u7387SSMs\u3001HMMs\u4e0e\u73b0\u4ee3\u795e\u7ecf\u5e8f\u5217\u6a21\u578b\u5173\u7cfb\u7684\u65b0\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6982\u7387\u56fe\u6a21\u578b\u7684\u89c6\u89d2\u5206\u6790\u8fd9\u4e9b\u6a21\u578b\u7684\u6570\u5b66\u8868\u8ff0\uff0c\u6bd4\u8f83\u5b83\u4eec\u7684\u63a8\u7406\u7b97\u6cd5\uff08\u5305\u62ec\u524d\u5411-\u540e\u5411\u63a8\u7406\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\uff09\uff0c\u5e76\u5bf9\u6bd4\u5b83\u4eec\u7684\u5b66\u4e60\u65b9\u6cd5\uff08\u671f\u671b\u6700\u5927\u5316\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\uff09\u3002\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83HMMs\u3001\u7ebf\u6027\u9ad8\u65af\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u5f53\u4ee3NLP\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u9610\u660e\u4e86\u8fd9\u4e9b\u6a21\u578b\u4f55\u65f6\u7b49\u4ef7\u3001\u4f55\u65f6\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u4ee5\u53ca\u73b0\u4ee3NLP SSMs\u5982\u4f55\u4e0e\u7ecf\u5178\u6982\u7387\u6a21\u578b\u76f8\u5173\u8054\u3002\u901a\u8fc7\u7a81\u51fa\u7ed3\u6784\u76f8\u4f3c\u6027\u548c\u8bed\u4e49\u5dee\u5f02\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6846\u67b6\u3002", "conclusion": "\u8be5\u5206\u6790\u8fde\u63a5\u4e86\u63a7\u5236\u7406\u8bba\u3001\u6982\u7387\u5efa\u6a21\u548c\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u7684\u89c6\u89d2\uff0c\u4e3a\u7406\u89e3\u7ecf\u5178\u6982\u7387\u6a21\u578b\u4e0e\u73b0\u4ee3\u795e\u7ecf\u5e8f\u5217\u6a21\u578b\u4e4b\u95f4\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u6f84\u6e05\u8fd9\u4e9b\u6a21\u578b\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u5e94\u7528\u4e2d\u7684\u5f02\u540c\u3002"}}
{"id": "2601.11869", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11869", "abs": "https://arxiv.org/abs/2601.11869", "authors": ["Zekun Hong", "Shinya Sugiura", "Chao Xu", "Lajos Hanzo"], "title": "Delay-Doppler-Domain Channel Estimation and Reduced-Complexity Detection of Faster-than-Nyquist Signaling Aided OTFS", "comment": "16 pages, 18 figures, 3 tables", "summary": "We conceive a novel channel estimation and data detection scheme for OTFS-modulated faster-than-Nyquist (FTN) transmission over doubly selective fading channels, aiming for enhancing the spectral efficiency and Doppler resilience. The delay-Doppler (DD) domain's input-output relationship of OTFS-FTN signaling is derived by employing a root-raised cosine (RRC) shaping filter. More specifically, we design our DD-domain channel estimator for FTN-based pilot transmission, where the pilot symbol interval is lower than that defined by the classic Nyquist criterion. Moreover, we propose a reduced-complexity linear minimum mean square error equalizer, supporting noise whitening, where the FTN-induced inter-symbol interference (ISI) matrix is approximated by a sparse one. Our performance results demonstrate that the proposed OTFS-FTN scheme is capable of enhancing the achievable information rate, while attaining a comparable BER performance to both that of its Nyquist-based OTFS counterpart and to other FTN transmission schemes, which employ the same RRC shaping filter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eOTFS-FTN\u4f20\u8f93\u7684\u65b0\u578b\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u65b9\u6848\uff0c\u65e8\u5728\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u548c\u6297\u591a\u666e\u52d2\u80fd\u529b\uff0c\u5728DD\u57df\u8bbe\u8ba1FTN\u5bfc\u9891\u4f20\u8f93\u548c\u4f4e\u590d\u6742\u5ea6\u5747\u8861\u5668\u3002", "motivation": "\u4f20\u7edfOTFS\u7cfb\u7edf\u5728\u9891\u8c31\u6548\u7387\u548c\u6297\u591a\u666e\u52d2\u6027\u80fd\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0cFTN\uff08\u8d85\u5948\u594e\u65af\u7279\uff09\u4f20\u8f93\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9891\u8c31\u6548\u7387\uff0c\u4f46\u9700\u8981\u89e3\u51b3FTN\u5f15\u5165\u7684\u7b26\u53f7\u95f4\u5e72\u6270\u95ee\u9898\u3002", "method": "1. \u63a8\u5bfcOTFS-FTN\u4fe1\u53f7\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff1b2. \u8bbe\u8ba1DD\u57df\u4fe1\u9053\u4f30\u8ba1\u5668\uff0c\u4f7f\u7528FTN\u5bfc\u9891\u4f20\u8f93\uff1b3. \u63d0\u51fa\u652f\u6301\u566a\u58f0\u767d\u5316\u7684\u4f4e\u590d\u6742\u5ea6\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u5747\u8861\u5668\uff0c\u5c06FTN\u5f15\u8d77\u7684ISI\u77e9\u9635\u8fd1\u4f3c\u4e3a\u7a00\u758f\u77e9\u9635\u3002", "result": "\u63d0\u51fa\u7684OTFS-FTN\u65b9\u6848\u80fd\u591f\u63d0\u9ad8\u53ef\u8fbe\u4fe1\u606f\u7387\uff0c\u540c\u65f6\u83b7\u5f97\u4e0e\u5948\u594e\u65af\u7279OTFS\u65b9\u6848\u76f8\u5f53\u7684BER\u6027\u80fd\uff0c\u4e14\u4f18\u4e8e\u5176\u4ed6\u4f7f\u7528\u76f8\u540cRRC\u6574\u5f62\u6ee4\u6ce2\u5668\u7684FTN\u4f20\u8f93\u65b9\u6848\u3002", "conclusion": "OTFS-FTN\u65b9\u6848\u6210\u529f\u7ed3\u5408\u4e86OTFS\u7684\u6297\u591a\u666e\u52d2\u80fd\u529b\u548cFTN\u7684\u9ad8\u9891\u8c31\u6548\u7387\u4f18\u52bf\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4fe1\u9053\u4f30\u8ba1\u548c\u5747\u8861\u5668\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2601.13102", "categories": ["stat.ML", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.13102", "abs": "https://arxiv.org/abs/2601.13102", "authors": ["Davidson Lova Razafindrakoto", "Alain Celisse", "J\u00e9r\u00f4me Lacaille"], "title": "Approximate full conformal prediction in RKHS", "comment": null, "summary": "Full conformal prediction is a framework that implicitly formulates distribution-free confidence prediction regions for a wide range of estimators. However, a classical limitation of the full conformal framework is the computation of the confidence prediction regions, which is usually impossible since it requires training infinitely many estimators (for real-valued prediction for instance). The main purpose of the present work is to describe a generic strategy for designing a tight approximation to the full conformal prediction region that can be efficiently computed. Along with this approximate confidence region, a theoretical quantification of the tightness of this approximation is developed, depending on the smoothness assumptions on the loss and score functions. The new notion of thickness is introduced for quantifying the discrepancy between the approximate confidence region and the full conformal one.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u5168\u4fdd\u5f62\u9884\u6d4b\u7f6e\u4fe1\u533a\u57df\u7684\u7d27\u81f4\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u539a\u5ea6\u6982\u5ff5\u91cf\u5316\u8fd1\u4f3c\u8bef\u5dee", "motivation": "\u5168\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\u867d\u7136\u80fd\u6784\u5efa\u5206\u5e03\u65e0\u5173\u7684\u7f6e\u4fe1\u9884\u6d4b\u533a\u57df\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u65e0\u9650\u591a\u4e2a\u4f30\u8ba1\u5668\uff0c\u8ba1\u7b97\u4e0a\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u8ba1\u7b97\u7684\u8fd1\u4f3c\u65b9\u6cd5", "method": "\u63d0\u51fa\u901a\u7528\u7b56\u7565\u8bbe\u8ba1\u5168\u4fdd\u5f62\u9884\u6d4b\u533a\u57df\u7684\u7d27\u81f4\u8fd1\u4f3c\uff0c\u5f15\u5165\u539a\u5ea6\u6982\u5ff5\u91cf\u5316\u8fd1\u4f3c\u4e0e\u5168\u4fdd\u5f62\u533a\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u7406\u8bba\u5206\u6790\u4f9d\u8d56\u4e8e\u635f\u5931\u51fd\u6570\u548c\u8bc4\u5206\u51fd\u6570\u7684\u5e73\u6ed1\u6027\u5047\u8bbe", "result": "\u5f00\u53d1\u4e86\u53ef\u9ad8\u6548\u8ba1\u7b97\u7684\u8fd1\u4f3c\u7f6e\u4fe1\u533a\u57df\uff0c\u5e76\u5efa\u7acb\u4e86\u7406\u8bba\u6846\u67b6\u91cf\u5316\u8be5\u8fd1\u4f3c\u7684\u7d27\u81f4\u7a0b\u5ea6", "conclusion": "\u8be5\u5de5\u4f5c\u89e3\u51b3\u4e86\u5168\u4fdd\u5f62\u9884\u6d4b\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u539a\u5ea6\u6982\u5ff5\u4e3a\u8bc4\u4f30\u8fd1\u4f3c\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177"}}
{"id": "2601.11604", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11604", "abs": "https://arxiv.org/abs/2601.11604", "authors": ["Jonaid Shianifar", "Michael Schukat", "Karl Mason"], "title": "Hindsight Preference Replay Improves Preference-Conditioned Multi-Objective Reinforcement Learning", "comment": null, "summary": "Multi-objective reinforcement learning (MORL) enables agents to optimize vector-valued rewards while respecting user preferences. CAPQL, a preference-conditioned actor-critic method, achieves this by conditioning on weight vectors w and restricts data usage to the specific preferences under which it was collected, leaving off-policy data from other preferences unused. We introduce Hindsight Preference Replay (HPR), a simple and general replay augmentation strategy that retroactively relabels stored transitions with alternative preferences. This densifies supervision across the preference simplex without altering the CAPQL architecture or loss functions. Evaluated on six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM rises from $323\\!\\pm\\!125$ to $1613\\!\\pm\\!464$ and HV from 0.52M to 9.63M, with strong statistical support. mo-halfcheetah-v5 remains a challenging exception where CAPQL attains higher HV at comparable EUM. We report final summaries and Pareto-front visualizations across all tasks.", "AI": {"tldr": "Hindsight Preference Replay (HPR) \u662f\u4e00\u79cd\u7b80\u5355\u7684\u56de\u653e\u589e\u5f3a\u7b56\u7565\uff0c\u901a\u8fc7\u91cd\u65b0\u6807\u8bb0\u5b58\u50a8\u7684\u8f6c\u79fb\u6570\u636e\u6765\u5229\u7528\u5176\u4ed6\u504f\u597d\u7684\u79bb\u7ebf\u6570\u636e\uff0c\u4ece\u800c\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "CAPQL\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4e2d\u53ea\u4f7f\u7528\u7279\u5b9a\u504f\u597d\u4e0b\u6536\u96c6\u7684\u6570\u636e\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u504f\u597d\u7684\u79bb\u7ebf\u6570\u636e\uff0c\u5bfc\u81f4\u6570\u636e\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faHindsight Preference Replay (HPR)\u7b56\u7565\uff0c\u5bf9\u5b58\u50a8\u7684\u8f6c\u79fb\u6570\u636e\u8fdb\u884c\u91cd\u65b0\u6807\u8bb0\uff0c\u4f7f\u7528\u66ff\u4ee3\u504f\u597d\u6765\u589e\u5f3a\u76d1\u7763\u4fe1\u53f7\uff0c\u65e0\u9700\u6539\u53d8CAPQL\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u3002", "result": "\u57286\u4e2aMO-Gymnasium\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0cHPR-CAPQL\u57285\u4e2a\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u8d85\u4f53\u79ef(HV)\uff0c\u57284\u4e2a\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u671f\u671b\u6548\u7528(EUM)\u3002\u7279\u522b\u662f\u5728mo-humanoid-v5\u4e2d\uff0cEUM\u4ece323\u00b1125\u63d0\u5347\u52301613\u00b1464\uff0cHV\u4ece0.52M\u63d0\u5347\u52309.63M\u3002", "conclusion": "HPR\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u56de\u653e\u589e\u5f3a\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u66f4\u597d\u5730\u5229\u7528\u79bb\u7ebf\u6570\u636e\u6765\u589e\u5f3a\u504f\u597d\u7a7a\u95f4\u4e2d\u7684\u76d1\u7763\u4fe1\u53f7\u3002"}}
{"id": "2601.13436", "categories": ["stat.ML", "cs.LG", "eess.SP", "eess.SY", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.13436", "abs": "https://arxiv.org/abs/2601.13436", "authors": ["Szabolcs Szentp\u00e9teri", "Bal\u00e1zs Csan\u00e1d Cs\u00e1ji"], "title": "Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC Bounds", "comment": null, "summary": "Linearly parametrized models are widely used in control and signal processing, with the least-squares (LS) estimate being the archetypical solution. When the input is insufficiently exciting, the LS problem may be unsolvable or numerically unstable. This issue can be resolved through regularization, typically with ridge regression. Although regularized estimators reduce the variance error, it remains important to quantify their estimation uncertainty. A possible approach for linear regression is to construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic confidence ellipsoids under the assumption that the noises are independent and symmetric about zero. This paper introduces an extension of the SPS EOA algorithm to ridge regression, and derives probably approximately correct (PAC) upper bounds for the resulting region sizes. Compared with previous analyses, our result explicitly show how the regularization parameter affects the region sizes, and provide tighter bounds under weaker excitation assumptions. Finally, the practical effect of regularization is also demonstrated via simulation experiments.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86SPS EOA\u7b97\u6cd5\u5230\u5cad\u56de\u5f52\uff0c\u63a8\u5bfc\u4e86\u7f6e\u4fe1\u692d\u7403\u5927\u5c0f\u7684PAC\u4e0a\u754c\uff0c\u63ed\u793a\u4e86\u6b63\u5219\u5316\u53c2\u6570\u5bf9\u533a\u57df\u5927\u5c0f\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u8f83\u5f31\u6fc0\u52b1\u6761\u4ef6\u4e0b\u63d0\u4f9b\u4e86\u66f4\u7d27\u7684\u754c\u3002", "motivation": "\u7ebf\u6027\u53c2\u6570\u5316\u6a21\u578b\u4e2d\u7684\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5728\u8f93\u5165\u6fc0\u52b1\u4e0d\u8db3\u65f6\u53ef\u80fd\u65e0\u6cd5\u6c42\u89e3\u6216\u4e0d\u7a33\u5b9a\uff0c\u6b63\u5219\u5316\uff08\u5982\u5cad\u56de\u5f52\uff09\u53ef\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u9700\u8981\u91cf\u5316\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u7684SPS EOA\u7b97\u6cd5\u53ea\u80fd\u7528\u4e8e\u7ebf\u6027\u56de\u5f52\uff0c\u9700\u8981\u6269\u5c55\u5230\u5cad\u56de\u5f52\u573a\u666f\u3002", "method": "\u6269\u5c55SPS EOA\u7b97\u6cd5\u5230\u5cad\u56de\u5f52\uff0c\u63a8\u5bfc\u6982\u7387\u8fd1\u4f3c\u6b63\u786e\uff08PAC\uff09\u4e0a\u754c\u6765\u5206\u6790\u6b63\u5219\u5316\u53c2\u6570\u5bf9\u7f6e\u4fe1\u692d\u7403\u5927\u5c0f\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u8f83\u5f31\u6fc0\u52b1\u6761\u4ef6\u4e0b\u63d0\u4f9b\u66f4\u7d27\u7684\u754c\u3002", "result": "\u6210\u529f\u5c06SPS EOA\u6269\u5c55\u5230\u5cad\u56de\u5f52\uff0c\u63a8\u5bfc\u7684PAC\u4e0a\u754c\u660e\u786e\u663e\u793a\u4e86\u6b63\u5219\u5316\u53c2\u6570\u5982\u4f55\u5f71\u54cd\u533a\u57df\u5927\u5c0f\uff0c\u5728\u8f83\u5f31\u6fc0\u52b1\u6761\u4ef6\u4e0b\u83b7\u5f97\u4e86\u66f4\u7d27\u7684\u754c\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6b63\u5219\u5316\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5cad\u56de\u5f52\u63d0\u4f9b\u4e86\u975e\u6e10\u8fd1\u7f6e\u4fe1\u692d\u7403\u6784\u5efa\u5de5\u5177\uff0c\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u6b63\u5219\u5316\u53c2\u6570\u4e0e\u7f6e\u4fe1\u533a\u57df\u5927\u5c0f\u7684\u660e\u786e\u5173\u7cfb\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6b63\u5219\u5316\u53c2\u6570\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2601.11878", "categories": ["eess.SP", "cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.11878", "abs": "https://arxiv.org/abs/2601.11878", "authors": ["Xi Peng"], "title": "Accelerated MR Elastography Using Learned Neural Network Representation", "comment": null, "summary": "To develop a deep-learning method for achieving fast high-resolution MR elastography from highly undersampled data without the need of high-quality training dataset. We first framed the deep neural network representation as a nonlinear extension of the linear subspace model, then used it to represent and reconstruct MRE image repetitions from undersampled k-space data. The network weights were learned using a multi-level k-space consistent loss in a self-supervised manner. To further enhance reconstruction quality, phase-contrast specific magnitude and phase priors were incorporated, including the similarity of anatomical structures and smoothness of wave-induced harmonic displacement. Experiments were conducted using both 3D gradient-echo spiral and multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear subspace-based approaches, the nonlinear network representation method was able to produce superior image reconstruction with suppressed noise and artifacts from a single in-plane spiral arm per MRE repetition (e.g., total R=10), yielding comparable stiffness estimation to the fully sampled data. This work demonstrated the feasibility of using deep network representations to model and reconstruct MRE images from highly-undersampled data, a nonlinear extension of the subspace-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u9ad8\u5ea6\u6b20\u91c7\u6837\u6570\u636e\u5b9e\u73b0\u5feb\u901f\u9ad8\u5206\u8fa8\u7387\u78c1\u5171\u632f\u5f39\u6027\u6210\u50cf", "motivation": "\u4f20\u7edf\u7ebf\u6027\u5b50\u7a7a\u95f4\u65b9\u6cd5\u5728\u9ad8\u5ea6\u6b20\u91c7\u6837\u60c5\u51b5\u4e0b\u91cd\u5efa\u8d28\u91cf\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u975e\u7ebf\u6027\u65b9\u6cd5\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u907f\u514d\u5bf9\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56", "method": "\u5c06\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u4e3a\u7ebf\u6027\u5b50\u7a7a\u95f4\u6a21\u578b\u7684\u975e\u7ebf\u6027\u6269\u5c55\uff0c\u91c7\u7528\u591a\u7ea7k\u7a7a\u95f4\u4e00\u81f4\u6027\u635f\u5931\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u6574\u5408\u5e45\u5ea6\u548c\u76f8\u4f4d\u5148\u9a8c\u4fe1\u606f", "result": "\u76f8\u6bd4\u4f20\u7edf\u7ebf\u6027\u5b50\u7a7a\u95f4\u65b9\u6cd5\uff0c\u975e\u7ebf\u6027\u7f51\u7edc\u8868\u793a\u80fd\u4ece\u5355\u4e2a\u5e73\u9762\u87ba\u65cb\u81c2\u91cd\u5efa\u51fa\u566a\u58f0\u548c\u4f2a\u5f71\u6291\u5236\u7684\u4f18\u8d28\u56fe\u50cf\uff0c\u521a\u5ea6\u4f30\u8ba1\u4e0e\u5168\u91c7\u6837\u6570\u636e\u76f8\u5f53", "conclusion": "\u6df1\u5ea6\u7f51\u7edc\u8868\u793a\u53ef\u4f5c\u4e3a\u5b50\u7a7a\u95f4\u65b9\u6cd5\u7684\u975e\u7ebf\u6027\u6269\u5c55\uff0c\u6709\u6548\u5efa\u6a21\u548c\u91cd\u5efa\u9ad8\u5ea6\u6b20\u91c7\u6837\u7684MRE\u6570\u636e\uff0c\u65e0\u9700\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u96c6"}}
{"id": "2601.13191", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13191", "abs": "https://arxiv.org/abs/2601.13191", "authors": ["Francisco Daunas", "I\u00f1aki Esnaola", "Samir M. Perlaza", "H. Vincent Poor"], "title": "Empirical Risk Minimization with $f$-Divergence Regularization", "comment": "Submitted to IEEE Transactions on Information Theory. arXiv admin note: substantial text overlap with arXiv:2502.14544, arXiv:2508.03314", "summary": "In this paper, the solution to the empirical risk minimization problem with $f$-divergence regularization (ERM-$f$DR) is presented and conditions under which the solution also serves as the solution to the minimization of the expected empirical risk subject to an $f$-divergence constraint are established. The proposed approach extends applicability to a broader class of $f$-divergences than previously reported and yields theoretical results that recover previously known results. Additionally, the difference between the expected empirical risk of the ERM-$f$DR solution and that of its reference measure is characterized, providing insights into previously studied cases of $f$-divergences. A central contribution is the introduction of the normalization function, a mathematical object that is critical in both the dual formulation and practical computation of the ERM-$f$DR solution. This work presents an implicit characterization of the normalization function as a nonlinear ordinary differential equation (ODE), establishes its key properties, and subsequently leverages them to construct a numerical algorithm for approximating the normalization factor under mild assumptions. Further analysis demonstrates structural equivalences between ERM-$f$DR problems with different $f$-divergences via transformations of the empirical risk. Finally, the proposed algorithm is used to compute the training and test risks of ERM-$f$DR solutions under different $f$-divergence regularizers. This numerical example highlights the practical implications of choosing different functions $f$ in ERM-$f$DR problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4e0ef-\u6563\u5ea6\u6b63\u5219\u5316\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u7acb\u4e86\u4e0e\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u7684\u7b49\u4ef7\u6761\u4ef6\uff0c\u5f15\u5165\u4e86\u5f52\u4e00\u5316\u51fd\u6570\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86\u6570\u503c\u7b97\u6cd5\u8ba1\u7b97f-\u6563\u5ea6\u6b63\u5219\u5316\u89e3\u3002", "motivation": "\u89e3\u51b3\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4e0ef-\u6563\u5ea6\u6b63\u5219\u5316\u95ee\u9898\uff0c\u6269\u5c55\u9002\u7528f-\u6563\u5ea6\u7c7b\u522b\uff0c\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u8fde\u63a5\u6b63\u5219\u5316\u4e0e\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u8ba1\u7b97\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u5f52\u4e00\u5316\u51fd\u6570\u6982\u5ff5\uff0c\u5efa\u7acb\u5176\u975e\u7ebf\u6027\u5e38\u5fae\u5206\u65b9\u7a0b\u8868\u5f81\uff0c\u5f00\u53d1\u6570\u503c\u7b97\u6cd5\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5206\u6790\u4e0d\u540cf-\u6563\u5ea6\u95ee\u9898\u7684\u7ed3\u6784\u7b49\u4ef7\u6027\uff0c\u901a\u8fc7\u53d8\u6362\u7ecf\u9a8c\u98ce\u9669\u5efa\u7acb\u8054\u7cfb\u3002", "result": "\u6269\u5c55\u4e86f-\u6563\u5ea6\u6b63\u5219\u5316\u7684\u9002\u7528\u8303\u56f4\uff0c\u5efa\u7acb\u4e86\u6b63\u5219\u5316\u4e0e\u7ea6\u675f\u95ee\u9898\u7684\u7b49\u4ef7\u6761\u4ef6\uff0c\u5f00\u53d1\u4e86\u6709\u6548\u7684\u6570\u503c\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540cf-\u6563\u5ea6\u95ee\u9898\u7684\u7ed3\u6784\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u4e0d\u540cf\u51fd\u6570\u9009\u62e9\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3af-\u6563\u5ea6\u6b63\u5219\u5316\u7684\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7406\u8bba\u6846\u67b6\u548c\u8ba1\u7b97\u5de5\u5177\uff0c\u7edf\u4e00\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u7ed3\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6b63\u5219\u5316\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u548c\u6570\u503c\u652f\u6301\u3002"}}
{"id": "2601.11606", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11606", "abs": "https://arxiv.org/abs/2601.11606", "authors": ["Farzana Islam Adiba", "Varsha Danduri", "Fahmida Liza Piya", "Ali Abbasi", "Mehak Gupta", "Rahmatollah Beheshti"], "title": "A Multimodal Data Processing Pipeline for MIMIC-IV Dataset", "comment": null, "summary": "The MIMIC-IV dataset is a large, publicly available electronic health record (EHR) resource widely used for clinical machine learning research. It comprises multiple modalities, including structured data, clinical notes, waveforms, and imaging data. Working with these disjointed modalities requires an extensive manual effort to preprocess and align them for downstream analysis. While several pipelines for MIMIC-IV data extraction are available, they target a small subset of modalities or do not fully support arbitrary downstream applications. In this work, we greatly expand our prior popular unimodal pipeline and present a comprehensive and customizable multimodal pipeline that can significantly reduce multimodal processing time and enhance the reproducibility of MIMIC-based studies. Our pipeline systematically integrates the listed modalities, enabling automated cohort selection, temporal alignment across modalities, and standardized multimodal output formats suitable for arbitrary static and time-series downstream applications. We release the code, a simple UI, and a Python package for selective integration (with embedding) at https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u591a\u6a21\u6001MIMIC-IV\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u53ef\u81ea\u52a8\u5316\u6574\u5408\u7ed3\u6784\u5316\u6570\u636e\u3001\u4e34\u5e8a\u7b14\u8bb0\u3001\u6ce2\u5f62\u548c\u5f71\u50cf\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u5e76\u63d0\u9ad8\u7814\u7a76\u53ef\u91cd\u590d\u6027\u3002", "motivation": "MIMIC-IV\u6570\u636e\u96c6\u5305\u542b\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u4f46\u73b0\u6709\u5904\u7406\u5de5\u5177\u8981\u4e48\u53ea\u652f\u6301\u90e8\u5206\u6a21\u6001\uff0c\u8981\u4e48\u65e0\u6cd5\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u4e0b\u6e38\u5e94\u7528\uff0c\u9700\u8981\u5927\u91cf\u624b\u52a8\u9884\u5904\u7406\u5de5\u4f5c\u3002", "method": "\u6269\u5c55\u4e86\u4e4b\u524d\u6d41\u884c\u7684\u5355\u6a21\u6001\u7ba1\u9053\uff0c\u5f00\u53d1\u4e86\u53ef\u5b9a\u5236\u7684\u591a\u6a21\u6001\u7ba1\u9053\uff0c\u652f\u6301\u81ea\u52a8\u5316\u961f\u5217\u9009\u62e9\u3001\u8de8\u6a21\u6001\u65f6\u95f4\u5bf9\u9f50\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u591a\u6a21\u6001\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\u53d1\u5e03\u4e86\u5305\u542b\u4ee3\u7801\u3001\u7b80\u5355UI\u548cPython\u5305\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u9009\u62e9\u6027\u96c6\u6210\u548c\u5d4c\u5165\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u7ba1\u9053\u80fd\u591f\u663e\u8457\u63d0\u9ad8MIMIC-IV\u6570\u636e\u5904\u7406\u7684\u6548\u7387\u548c\u53ef\u91cd\u590d\u6027\uff0c\u652f\u6301\u4efb\u610f\u7684\u9759\u6001\u548c\u65f6\u95f4\u5e8f\u5217\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2601.13962", "categories": ["eess.SP", "eess.SY", "q-bio.NC", "stat.ME"], "pdf": "https://arxiv.org/pdf/2601.13962", "abs": "https://arxiv.org/abs/2601.13962", "authors": ["Eike Osmers", "Dorothea Kolossa"], "title": "Optimal Calibration of the endpoint-corrected Hilbert Transform", "comment": null, "summary": "Accurate, low-latency estimates of the instantaneous phase of oscillations are essential for closed-loop sensing and actuation, including (but not limited to) phase-locked neurostimulation and other real-time applications. The endpoint-corrected Hilbert transform (ecHT) reduces boundary artefacts of the Hilbert transform by applying a causal narrow-band filter to the analytic spectrum. This improves the phase estimate at the most recent sample. Despite its widespread empirical use, the systematic endpoint distortions of ecHT have lacked a principled, closed-form analysis. In this study, we derive the ecHT endpoint operator analytically and demonstrate that its output can be decomposed into a desired positive-frequency term (a deterministic complex gain that induces a calibratable amplitude/phase bias) and a residual leakage term setting an irreducible variance floor. This yields (i) an explicit characterisation and bounds for endpoint phase/amplitude error, (ii) a mean-squared-error-optimal scalar calibration (c-ecHT), and (iii) practical design rules relating window length, bandwidth/order, and centre-frequency mismatch to residual bias via an endpoint group delay. The resulting calibrated ecHT achieves near-zero mean phase error and remains computationally compatible with real-time pipelines. Code and analyses are provided at https://github.com/eosmers/cecHT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u7aef\u70b9\u6821\u6b63\u5e0c\u5c14\u4f2f\u7279\u53d8\u6362(ecHT)\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u4e86\u5176\u7aef\u70b9\u7b97\u5b50\uff0c\u63d0\u51fa\u4e86\u6821\u51c6\u65b9\u6cd5(c-ecHT)\u548c\u8bbe\u8ba1\u51c6\u5219\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u96f6\u5e73\u5747\u76f8\u4f4d\u8bef\u5dee\u7684\u5b9e\u65f6\u76f8\u4f4d\u4f30\u8ba1\u3002", "motivation": "\u5728\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u7b49\u5b9e\u65f6\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u51c6\u786e\u3001\u4f4e\u5ef6\u8fdf\u7684\u77ac\u65f6\u76f8\u4f4d\u4f30\u8ba1\u3002ecHT\u867d\u7136\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u7aef\u70b9\u5931\u771f\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u901a\u8fc7\u89e3\u6790\u63a8\u5bfcecHT\u7aef\u70b9\u7b97\u5b50\uff0c\u5c06\u5176\u8f93\u51fa\u5206\u89e3\u4e3a\u671f\u671b\u7684\u6b63\u9891\u7387\u9879\uff08\u53ef\u6821\u51c6\u7684\u786e\u5b9a\u6027\u590d\u589e\u76ca\uff09\u548c\u6b8b\u4f59\u6cc4\u6f0f\u9879\uff08\u8bbe\u5b9a\u65b9\u5dee\u4e0b\u9650\uff09\uff0c\u63d0\u51fa\u4e86\u5747\u65b9\u8bef\u5dee\u6700\u4f18\u7684\u6807\u91cf\u6821\u51c6\u65b9\u6cd5(c-ecHT)\u3002", "result": "\u83b7\u5f97\u4e86\u7aef\u70b9\u76f8\u4f4d/\u5e45\u5ea6\u8bef\u5dee\u7684\u660e\u786e\u8868\u5f81\u548c\u8fb9\u754c\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u96f6\u5e73\u5747\u76f8\u4f4d\u8bef\u5dee\u7684\u6821\u51c6ecHT\uff0c\u5e76\u5efa\u7acb\u4e86\u7a97\u53e3\u957f\u5ea6\u3001\u5e26\u5bbd/\u9636\u6570\u3001\u4e2d\u5fc3\u9891\u7387\u5931\u914d\u4e0e\u6b8b\u4f59\u504f\u5dee\u4e4b\u95f4\u7684\u8bbe\u8ba1\u89c4\u5219\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790ecHT\u7aef\u70b9\u5931\u771f\uff0c\u63d0\u51fa\u7684\u6821\u51c6\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u4f4d\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u8ba1\u7b97\u517c\u5bb9\u6027\uff0c\u4e3a\u5b9e\u65f6\u76f8\u4f4d\u4f30\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.11894", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11894", "abs": "https://arxiv.org/abs/2601.11894", "authors": ["Haotian Liu", "Zhiqing Wei", "Yucong Du", "Jiachen Wei", "Xingwang Li", "Zhiyong Feng"], "title": "Beyond Target-Level: ISAC-Enabled Event-Level Sensing for Behavioral Intention Prediction", "comment": "5 pages, 5 figures, 1 table, and 15 references. Reviewed by IEEE WCL", "summary": "Integrated Sensing and Communication (ISAC) holds great promise for enabling event-level sensing, such as behavioral intention prediction (BIP) in autonomous driving, particularly under non-line-of-sight (NLoS) or adverse weather conditions where conventional sensors degrade. However, as a key instance of event-level sensing, ISAC-based BIP remains unexplored. To address this gap, we propose an ISAC-enabled BIP framework and validate its feasibility and effectiveness through extensive simulations. Our framework achieves robust performance in safety-critical scenarios, improving the F1-score by 11.4% over sensor-based baselines in adverse weather, thereby demonstrating ISAC's potential for intelligent event-level sensing.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8eISAC\u7684\u884c\u4e3a\u610f\u56fe\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u6bd4\u4f20\u7edf\u4f20\u611f\u5668\u65b9\u6cd5F1\u5206\u6570\u63d0\u534711.4%", "motivation": "ISAC\u5728\u4e8b\u4ef6\u7ea7\u611f\u77e5\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u884c\u4e3a\u610f\u56fe\u9884\u6d4b\uff09\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728NLoS\u6216\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u4f20\u7edf\u4f20\u611f\u5668\u6027\u80fd\u4e0b\u964d\u65f6\u3002\u7136\u800c\uff0cISAC\u5728\u884c\u4e3a\u610f\u56fe\u9884\u6d4b\u8fd9\u4e00\u5173\u952e\u5e94\u7528\u4e0a\u5c1a\u672a\u88ab\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aISAC\u542f\u7528\u7684\u884c\u4e3a\u610f\u56fe\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027", "result": "\u6846\u67b6\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u6bd4\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u57fa\u7ebf\u65b9\u6cd5F1\u5206\u6570\u63d0\u534711.4%", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86ISAC\u5728\u667a\u80fd\u4e8b\u4ef6\u7ea7\u611f\u77e5\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u4f20\u7edf\u4f20\u611f\u5668\u6027\u80fd\u53d7\u9650\u7684\u573a\u666f\u4e0b"}}
{"id": "2601.11609", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11609", "abs": "https://arxiv.org/abs/2601.11609", "authors": ["Weinuo Ou"], "title": "Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory Storage Model Based on Invertible Compression and Learnable Prediction", "comment": "9 pages, 7 figures", "summary": "Current large language models (LLMs) generally lack an effective runtime memory mechanism,making it difficult to adapt to dynamic and personalized interaction requirements. To address this issue, this paper proposes a novel neural memory storage architecture--the Auxiliary Prediction Compression Memory Model (ApCM Model).", "AI": {"tldr": "\u63d0\u51faApCM\u6a21\u578b\uff0c\u901a\u8fc7\u8f85\u52a9\u9884\u6d4b\u538b\u7f29\u8bb0\u5fc6\u67b6\u6784\u89e3\u51b3LLMs\u7f3a\u4e4f\u6709\u6548\u8fd0\u884c\u65f6\u8bb0\u5fc6\u673a\u5236\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u8fd0\u884c\u65f6\u8bb0\u5fc6\u673a\u5236\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u548c\u4e2a\u6027\u5316\u7684\u4ea4\u4e92\u9700\u6c42", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u795e\u7ecf\u8bb0\u5fc6\u5b58\u50a8\u67b6\u6784\u2014\u2014\u8f85\u52a9\u9884\u6d4b\u538b\u7f29\u8bb0\u5fc6\u6a21\u578b\uff08ApCM\u6a21\u578b\uff09", "result": "\u4ece\u6458\u8981\u4e2d\u65e0\u6cd5\u5f97\u77e5\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u8be5\u6a21\u578b\u65e8\u5728\u89e3\u51b3LLMs\u7684\u8bb0\u5fc6\u673a\u5236\u95ee\u9898\uff0c\u63d0\u5347\u5bf9\u52a8\u6001\u4e2a\u6027\u5316\u4ea4\u4e92\u7684\u9002\u5e94\u80fd\u529b"}}
{"id": "2601.11938", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11938", "abs": "https://arxiv.org/abs/2601.11938", "authors": ["Sebastian Ratto", "Huy Trinh", "Ahmed N. Sayed", "Abdelrahman Elbadrawy", "Arien Sligar", "George Shaker"], "title": "Radar-Based Fall Detection for Assisted Living: A Digital-Twin Representation Case Study", "comment": "6 pages, 8 figures", "summary": "Obtaining data on high-impact falls from older adults is ethically difficult, yet these rare events cause many fall-related health problems. As a result, most radar-based fall detectors are trained on staged falls from young volunteers, and representation choices are rarely tested against the radar signals from dangerous falls. This paper uses a frequency-modulated continuous-wave (FMCW) radar digital twin as a single simulated room testbed to study how representation choice affects fall/non-fall discrimination. From the same simulated range-Doppler sequence, Doppler-time spectrograms, three-channel per-receiver spectrogram stacks, and time-pooled range-Doppler maps (RDMs) are derived and fed to an identical compact CNN under matched training on a balanced fall/non-fall dataset. In this twin, temporal spectrograms reach 98-99% test accuracy with similar precision and recall for both classes, while static RDMs reach 89.4% and show more variable training despite using the same backbone. A qualitative comparison between synthetic and measured fall spectrograms suggests that the twin captures gross Doppler-time structure, but amplitude histograms reveal differences in the distributions of amplitude values consistent with receiver processing not modeled in the twin. Because the twin omits noise and hardware impairments and is only qualitatively compared to a single measured example, these results provide representation-level guidance under controlled synthetic conditions rather than ready-to-use clinical performance in real settings.", "AI": {"tldr": "\u4f7f\u7528FMCW\u96f7\u8fbe\u6570\u5b57\u5b6a\u751f\u6d4b\u8bd5\u4e0d\u540c\u8868\u793a\u65b9\u6cd5\u5bf9\u8dcc\u5012\u68c0\u6d4b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65f6\u9891\u8c31\u56fe\u6bd4\u9759\u6001\u8ddd\u79bb\u591a\u666e\u52d2\u56fe\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6a21\u62df\u7ed3\u679c\u4e0e\u771f\u5b9e\u6d4b\u91cf\u5b58\u5728\u5dee\u5f02", "motivation": "\u83b7\u53d6\u8001\u5e74\u4eba\u9ad8\u5371\u8dcc\u5012\u6570\u636e\u5b58\u5728\u4f26\u7406\u56f0\u96be\uff0c\u800c\u73b0\u6709\u96f7\u8fbe\u8dcc\u5012\u68c0\u6d4b\u5668\u5927\u591a\u57fa\u4e8e\u5e74\u8f7b\u4eba\u6a21\u62df\u8dcc\u5012\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u5371\u9669\u8dcc\u5012\u96f7\u8fbe\u4fe1\u53f7\u7684\u9a8c\u8bc1\u3002\u9700\u8981\u7814\u7a76\u8868\u793a\u65b9\u6cd5\u9009\u62e9\u5bf9\u8dcc\u5012/\u975e\u8dcc\u5012\u533a\u5206\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u9891\u7387\u8c03\u5236\u8fde\u7eed\u6ce2\u96f7\u8fbe\u6570\u5b57\u5b6a\u751f\u4f5c\u4e3a\u6a21\u62df\u623f\u95f4\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4ece\u76f8\u540c\u7684\u6a21\u62df\u8ddd\u79bb-\u591a\u666e\u52d2\u5e8f\u5217\u4e2d\u63d0\u53d6\u4e09\u79cd\u8868\u793a\uff1a\u591a\u666e\u52d2-\u65f6\u95f4\u9891\u8c31\u56fe\u3001\u4e09\u901a\u9053\u63a5\u6536\u673a\u9891\u8c31\u56fe\u5806\u6808\u548c\u65f6\u95f4\u6c60\u5316\u8ddd\u79bb-\u591a\u666e\u52d2\u56fe\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u7d27\u51d1CNN\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6bd4\u8f83\u3002", "result": "\u65f6\u9891\u8c31\u56fe\u8fbe\u523098-99%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4e24\u7c7b\u5177\u6709\u76f8\u4f3c\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff1b\u9759\u6001\u8ddd\u79bb-\u591a\u666e\u52d2\u56fe\u8fbe\u523089.4%\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u4e0d\u7a33\u5b9a\u3002\u6a21\u62df\u4e0e\u5b9e\u6d4b\u9891\u8c31\u56fe\u7684\u5b9a\u6027\u6bd4\u8f83\u663e\u793a\u6355\u83b7\u4e86\u4e3b\u8981\u591a\u666e\u52d2-\u65f6\u95f4\u7ed3\u6784\uff0c\u4f46\u5e45\u5ea6\u76f4\u65b9\u56fe\u5206\u5e03\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u53d7\u63a7\u5408\u6210\u6761\u4ef6\u4e0b\u7684\u8868\u793a\u5c42\u9762\u6307\u5bfc\uff0c\u800c\u975e\u53ef\u76f4\u63a5\u7528\u4e8e\u4e34\u5e8a\u7684\u5b9e\u9645\u6027\u80fd\u3002\u6a21\u62df\u5ffd\u7565\u4e86\u566a\u58f0\u548c\u786c\u4ef6\u635f\u4f24\uff0c\u4ec5\u4e0e\u5355\u4e2a\u5b9e\u6d4b\u793a\u4f8b\u8fdb\u884c\u5b9a\u6027\u6bd4\u8f83\uff0c\u7ed3\u679c\u9700\u8c28\u614e\u89e3\u8bfb\u3002"}}
{"id": "2601.13458", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2601.13458", "abs": "https://arxiv.org/abs/2601.13458", "authors": ["Zihan Dong", "Ruijia Wu", "Linjun Zhang"], "title": "Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs", "comment": null, "summary": "The increasing reliance on human preference feedback to judge AI-generated pseudo labels has created a pressing need for principled, budget-conscious data acquisition strategies. We address the crucial question of how to optimally allocate a fixed annotation budget between ground-truth labels and pairwise preferences in AI. Our solution, grounded in semi-parametric inference, casts the budget allocation problem as a monotone missing data framework. Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator for functionals of the data distribution. Theoretically, we prove the asymptotic optimality of our PCAL estimator and establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models. Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution. This work provides a principled and statistically efficient approach for budget-constrained learning in modern AI. Simulations and real-data analysis demonstrate the practical benefits and superior performance of our proposed method.", "AI": {"tldr": "\u63d0\u51faPCAL\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u53c2\u6570\u63a8\u65ad\u5c06\u6807\u6ce8\u9884\u7b97\u5206\u914d\u95ee\u9898\u8f6c\u5316\u4e3a\u5355\u8c03\u7f3a\u5931\u6570\u636e\u6846\u67b6\uff0c\u4f18\u5316\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u548c\u6210\u5bf9\u504f\u597d\u7684\u9884\u7b97\u5206\u914d\uff0c\u5b9e\u73b0\u7edf\u8ba1\u9ad8\u6548\u4f30\u8ba1\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u4f2a\u6807\u7b7e\u8d8a\u6765\u8d8a\u4f9d\u8d56\u4eba\u7c7b\u504f\u597d\u53cd\u9988\uff0c\u9700\u8981\u539f\u5219\u6027\u7684\u3001\u9884\u7b97\u654f\u611f\u7684\u6570\u636e\u83b7\u53d6\u7b56\u7565\u3002\u5173\u952e\u95ee\u9898\u662f\u5982\u4f55\u5728\u6709\u9650\u6807\u6ce8\u9884\u7b97\u4e0b\uff0c\u6700\u4f18\u5206\u914d\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u548c\u6210\u5bf9\u504f\u597d\u7684\u6807\u6ce8\u8d44\u6e90\u3002", "method": "\u57fa\u4e8e\u534a\u53c2\u6570\u63a8\u65ad\uff0c\u5c06\u9884\u7b97\u5206\u914d\u95ee\u9898\u5efa\u6a21\u4e3a\u5355\u8c03\u7f3a\u5931\u6570\u636e\u6846\u67b6\u3002\u63d0\u51fa\u504f\u597d\u6821\u51c6\u4e3b\u52a8\u5b66\u4e60\uff08PCAL\uff09\u65b9\u6cd5\uff0c\u5b66\u4e60\u6700\u4f18\u6570\u636e\u83b7\u53d6\u7b56\u7565\uff0c\u5e76\u5f00\u53d1\u7edf\u8ba1\u9ad8\u6548\u7684\u6570\u636e\u5206\u5e03\u6cdb\u51fd\u4f30\u8ba1\u5668\u3002\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u4f30\u8ba1\u5668\u65b9\u5dee\u800c\u975e\u8981\u6c42\u95ed\u5f0f\u89e3\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u95ee\u9898\u7c7b\u522b\u3002", "result": "\u7406\u8bba\u8bc1\u660ePCAL\u4f30\u8ba1\u5668\u7684\u6e10\u8fd1\u6700\u4f18\u6027\uff0c\u5e76\u5efa\u7acb\u5173\u952e\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u5373\u4f7f\u5728\u8f85\u52a9\u6a21\u578b\u4f30\u8ba1\u4e0d\u4f73\u65f6\u4e5f\u80fd\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u5206\u6790\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u4f18\u52bf\u548c\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u4e3a\u73b0\u4ee3AI\u4e2d\u7684\u9884\u7b97\u7ea6\u675f\u5b66\u4e60\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u548c\u7edf\u8ba1\u9ad8\u6548\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u548c\u504f\u597d\u53cd\u9988\u4e4b\u95f4\u7684\u6700\u4f18\u9884\u7b97\u5206\u914d\u95ee\u9898\u3002"}}
{"id": "2601.11611", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11611", "abs": "https://arxiv.org/abs/2601.11611", "authors": ["Marina Vicini", "Martin Rudorfer", "Zhuangzhuang Dai", "Luis J. Manso"], "title": "Integrating Temporal Context into Streaming Data for Human Activity Recognition in Smart Home", "comment": "Accepted to International Conference on Ubiquitous Computing and Ambient Intelligence (UCAmI) 2024", "summary": "With the global population ageing, it is crucial to enable individuals to live independently and safely in their homes. Using ubiquitous sensors such as Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest for monitoring daily activities and facilitating preventative healthcare interventions for the elderly. Human Activity Recognition (HAR) from passive sensors mostly relies on traditional machine learning and includes data segmentation, feature extraction, and classification. While techniques like Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature vector, effectively leveraging temporal information remains a challenge. We tackle this by clustering activities into morning, afternoon, and night, and encoding them into the feature weighting method calculating distinct mutual information matrices. We further propose to extend the feature vector by incorporating time of day and day of week as cyclical temporal features, as well as adding a feature to track the user's location. The experiments show improved accuracy and F1-score over existing state-of-the-art methods in three out of four real-world datasets, with highest gains in a low-data regime. These results highlight the potential of our approach for developing effective smart home solutions to support ageing in place.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u57fa\u4e8e\u88ab\u52a8\u4f20\u611f\u5668\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b\u6d3b\u52a8\u65f6\u95f4\uff08\u65e9\u3001\u4e2d\u3001\u665a\uff09\u5e76\u7f16\u7801\u5230\u7279\u5f81\u52a0\u6743\u4e2d\uff0c\u540c\u65f6\u52a0\u5165\u5faa\u73af\u65f6\u95f4\u7279\u5f81\u548c\u7528\u6237\u4f4d\u7f6e\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5168\u7403\u4eba\u53e3\u8001\u9f84\u5316\uff0c\u9700\u8981\u8ba9\u8001\u5e74\u4eba\u80fd\u591f\u72ec\u7acb\u5b89\u5168\u5730\u5728\u5bb6\u4e2d\u751f\u6d3b\u3002\u4f7f\u7528PIR\u548c\u95e8\u4f20\u611f\u5668\u7b49\u88ab\u52a8\u4f20\u611f\u5668\u76d1\u6d4b\u65e5\u5e38\u6d3b\u52a8\u5e76\u4fc3\u8fdb\u9884\u9632\u6027\u533b\u7597\u5e72\u9884\u8d8a\u6765\u8d8a\u53d7\u5173\u6ce8\u3002\u4f20\u7edfHAR\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u4f46\u6709\u6548\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "1. \u5c06\u6d3b\u52a8\u805a\u7c7b\u4e3a\u65e9\u6668\u3001\u4e0b\u5348\u548c\u591c\u665a\u4e09\u4e2a\u65f6\u95f4\u6bb5\uff1b2. \u5c06\u8fd9\u4e9b\u65f6\u95f4\u805a\u7c7b\u7f16\u7801\u5230\u7279\u5f81\u52a0\u6743\u65b9\u6cd5\u4e2d\uff0c\u8ba1\u7b97\u4e0d\u540c\u7684\u4e92\u4fe1\u606f\u77e9\u9635\uff1b3. \u6269\u5c55\u7279\u5f81\u5411\u91cf\uff0c\u52a0\u5165\u4e00\u5929\u4e2d\u7684\u65f6\u95f4\u548c\u4e00\u5468\u4e2d\u7684\u5929\u4f5c\u4e3a\u5faa\u73af\u65f6\u95f4\u7279\u5f81\uff1b4. \u6dfb\u52a0\u8ddf\u8e2a\u7528\u6237\u4f4d\u7f6e\u7684\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u4e09\u4e2a\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u6570\u636e\u91cf\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u6700\u9ad8\u589e\u76ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5f00\u53d1\u6709\u6548\u667a\u80fd\u5bb6\u5c45\u89e3\u51b3\u65b9\u6848\u4ee5\u652f\u6301\u8001\u5e74\u4eba\u539f\u5730\u517b\u8001\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u66f4\u597d\u5730\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u63d0\u9ad8\u4e86\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2601.11971", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.11971", "abs": "https://arxiv.org/abs/2601.11971", "authors": ["Duc Viet Nguyen", "Haiquan Zhao", "Jinhui Hu", "Xiaoli Li"], "title": "Robust distributed extended Kalman filter based on adaptive multi-kernel mixture maximum correntropy for non-Gaussian systems", "comment": "15 pages, 16 figures,", "summary": "As one of the most advanced variants in the correntropy family, the multi-kernel correntropy criterion demonstrates superior accuracy in handling non-Gaussian noise, particularly with multimodal distributions. However, current approaches suffer from key limitations-namely, reliance on a single type of sensitive Gaussian kernel and the manual selection of free parameters. To address these issues and further boost robustness, this paper introduces the concept of multi-kernel mixture correntropy (MKMC), along with its key properties. MKMC employs a flexible kernel function composed of a mixture of two Students t-Cauchy functions with adjustable (non-zero) means. Building on this criterion within multi-sensor networks, we propose a robust distributed extended Kalman filter-AMKMMC-RDEKF based on adaptive multi-kernel mixture maximum correntropy. To reduce communication overhead, a consensus averaging strategy is incorporated. Furthermore, an adaptive mechanism is introduced to mitigate the impact of manually tuned free parameters. At the same time, the computational complexity and convergence ability of the proposed algorithm are analyzed. The effectiveness of the proposed algorithm is validated through challenging scenarios involving power system and land vehicle state estimation.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6838\u6df7\u5408\u76f8\u5173\u71b5(MKMC)\u6982\u5ff5\uff0c\u57fa\u4e8e\u5b66\u751ft-\u67ef\u897f\u6df7\u5408\u6838\u51fd\u6570\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u591a\u6838\u6df7\u5408\u6700\u5927\u76f8\u5173\u71b5\u9c81\u68d2\u5206\u5e03\u5f0f\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(AMKMMC-RDEKF)\uff0c\u7528\u4e8e\u591a\u4f20\u611f\u5668\u7f51\u7edc\u72b6\u6001\u4f30\u8ba1", "motivation": "\u73b0\u6709\u591a\u6838\u76f8\u5173\u71b5\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4f9d\u8d56\u5355\u4e00\u7c7b\u578b\u7684\u9ad8\u65af\u6838\u51fd\u6570\uff0c\u5bf9\u53c2\u6570\u654f\u611f\uff1b2) \u9700\u8981\u624b\u52a8\u9009\u62e9\u81ea\u7531\u53c2\u6570\u3002\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\uff08\u7279\u522b\u662f\u591a\u6a21\u6001\u5206\u5e03\uff09\u65f6\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "1) \u63d0\u51fa\u591a\u6838\u6df7\u5408\u76f8\u5173\u71b5(MKMC)\u6982\u5ff5\uff0c\u4f7f\u7528\u4e24\u4e2a\u5b66\u751ft-\u67ef\u897f\u51fd\u6570\u7684\u6df7\u5408\u6838\u51fd\u6570\uff0c\u5177\u6709\u53ef\u8c03\u975e\u96f6\u5747\u503c\uff1b2) \u57fa\u4e8eMKMC\u5f00\u53d1\u81ea\u9002\u5e94\u591a\u6838\u6df7\u5408\u6700\u5927\u76f8\u5173\u71b5\u9c81\u68d2\u5206\u5e03\u5f0f\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(AMKMMC-RDEKF)\uff1b3) \u5f15\u5165\u5171\u8bc6\u5e73\u5747\u7b56\u7565\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\uff1b4) \u8bbe\u8ba1\u81ea\u9002\u5e94\u673a\u5236\u51cf\u5c11\u624b\u52a8\u8c03\u53c2\u5f71\u54cd", "result": "1) \u5206\u6790\u4e86\u7b97\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6536\u655b\u80fd\u529b\uff1b2) \u5728\u7535\u529b\u7cfb\u7edf\u548c\u9646\u5730\u8f66\u8f86\u72b6\u6001\u4f30\u8ba1\u7684\u6311\u6218\u6027\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u975e\u9ad8\u65af\u566a\u58f0\u73af\u5883\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684MKMC\u6846\u67b6\u548cAMKMMC-RDEKF\u7b97\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u591a\u6838\u76f8\u5173\u71b5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6df7\u5408\u6838\u51fd\u6570\u548c\u81ea\u9002\u5e94\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u5728\u590d\u6742\u975e\u9ad8\u65af\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd"}}
{"id": "2601.13519", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.13519", "abs": "https://arxiv.org/abs/2601.13519", "authors": ["Wenzhi Gao", "Chang He", "Madeleine Udell"], "title": "Small Gradient Norm Regret for Online Convex Optimization", "comment": null, "summary": "This paper introduces a new problem-dependent regret measure for online convex optimization with smooth losses. The notion, which we call the $G^\\star$ regret, depends on the cumulative squared gradient norm evaluated at the decision in hindsight $\\sum_{t=1}^T \\|\\nabla \\ell(x^\\star)\\|^2$. We show that the $G^\\star$ regret strictly refines the existing $L^\\star$ (small loss) regret, and that it can be arbitrarily sharper when the losses have vanishing curvature around the hindsight decision. We establish upper and lower bounds on the $G^\\star$ regret and extend our results to dynamic regret and bandit settings. As a byproduct, we refine the existing convergence analysis of stochastic optimization algorithms in the interpolation regime. Some experiments validate our theoretical findings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u51f8\u4f18\u5316\u95ee\u9898\u4f9d\u8d56\u7684\u9057\u61be\u5ea6\u91cf$G^\\star$\u9057\u61be\uff0c\u57fa\u4e8e\u7d2f\u79ef\u5e73\u65b9\u68af\u5ea6\u8303\u6570\uff0c\u6bd4\u73b0\u6709\u7684$L^\\star$\u9057\u61be\u66f4\u7cbe\u7ec6\uff0c\u5728\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684$L^\\star$\u9057\u61be\u5ea6\u91cf\u5728\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u65f6\u53ef\u80fd\u4e0d\u591f\u7cbe\u786e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u9057\u61be\u5ea6\u91cf\u6765\u66f4\u597d\u5730\u8861\u91cf\u5728\u7ebf\u51f8\u4f18\u5316\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa$G^\\star$\u9057\u61be\u5ea6\u91cf\uff0c\u5b9a\u4e49\u4e3a$\\sum_{t=1}^T \\|\\nabla \\ell(x^\\star)\\|^2$\uff0c\u5373\u7d2f\u79ef\u5e73\u65b9\u68af\u5ea6\u8303\u6570\u5728\u4e8b\u540e\u6700\u4f18\u51b3\u7b56\u5904\u7684\u8bc4\u4f30\u3002\u5efa\u7acb\u8be5\u5ea6\u91cf\u7684\u4e0a\u4e0b\u754c\uff0c\u5e76\u6269\u5c55\u5230\u52a8\u6001\u9057\u61be\u548c\u8d4c\u535a\u673a\u8bbe\u7f6e\u3002", "result": "\u8bc1\u660e$G^\\star$\u9057\u61be\u4e25\u683c\u7ec6\u5316\u4e86$L^\\star$\u9057\u61be\uff0c\u5f53\u635f\u5931\u51fd\u6570\u5728\u6700\u4f18\u89e3\u9644\u8fd1\u66f2\u7387\u6d88\u5931\u65f6\u53ef\u4ee5\u4efb\u610f\u66f4\u5c16\u9510\u3002\u5efa\u7acb\u4e86\u4e0a\u4e0b\u754c\uff0c\u5e76\u6539\u8fdb\u4e86\u63d2\u503c\u673a\u5236\u4e0b\u968f\u673a\u4f18\u5316\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\u3002", "conclusion": "$G^\\star$\u9057\u61be\u662f\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u95ee\u9898\u4f9d\u8d56\u9057\u61be\u5ea6\u91cf\uff0c\u80fd\u66f4\u597d\u5730\u8861\u91cf\u5728\u7ebf\u51f8\u4f18\u5316\u7b97\u6cd5\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u635f\u5931\u51fd\u6570\u66f2\u7387\u6d88\u5931\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u3002"}}
{"id": "2601.11615", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11615", "abs": "https://arxiv.org/abs/2601.11615", "authors": ["Beyza Cinar", "Louisa van den Boom", "Maria Maleshkova"], "title": "A Review on Machine Learning Approaches for the Prediction of Glucose Levels and Hypogylcemia", "comment": null, "summary": "Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin insufficiency. Thus, patients require lifelong insulin therapy, which has a side effect of hypoglycemia. Hypoglycemia is a critical state of decreased blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk of mortality. Machine learning (ML) models can improve diabetes management by predicting hypoglycemia and providing optimal prevention methods. ML models are classified into regression and classification based, that forecast glucose levels and identify events based on defined labels, respectively. This review investigates state-of-the-art models trained on data of continuous glucose monitoring (CGM) devices from patients with T1D. We compare the models' performance across short-term (15 to 120 min) and long term (3 to more than 24 hours) prediction horizons (PHs). Particularly, we explore: 1) How much in advance can glucose values or a hypoglycemic event be accurately predicted? 2) Which models have the best performance? 3) Which factors impact the performance? and 4) Does personalization increase performance? The results show that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML methods yield the best results for classification and DL for regression. A single model cannot adequately classify across multiple PHs. 3) The model performance is influenced by multivariate datasets and the input sequence length (ISL). 4) Personal data enhances performance but due to limited data quality population-based models are preferred.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u57281\u578b\u7cd6\u5c3f\u75c5\u4f4e\u8840\u7cd6\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u9700\u8981\u7ec8\u8eab\u80f0\u5c9b\u7d20\u6cbb\u7597\uff0c\u4f46\u80f0\u5c9b\u7d20\u6cbb\u7597\u6709\u5bfc\u81f4\u4f4e\u8840\u7cd6\u7684\u526f\u4f5c\u7528\u3002\u4f4e\u8840\u7cd6\u4f1a\u589e\u52a0\u6b7b\u4ea1\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u51c6\u786e\u9884\u6d4b\u4f4e\u8840\u7cd6\u4e8b\u4ef6\u4ee5\u6539\u5584\u7cd6\u5c3f\u75c5\u7ba1\u7406\u3002", "method": "\u7efc\u8ff0\u5206\u6790\u4e86\u57fa\u4e8e\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u56de\u5f52\u6a21\u578b\uff08\u9884\u6d4b\u8840\u7cd6\u503c\uff09\u548c\u5206\u7c7b\u6a21\u578b\uff08\u8bc6\u522b\u4f4e\u8840\u7cd6\u4e8b\u4ef6\uff09\uff0c\u6bd4\u8f83\u4e86\u77ed\u671f\uff0815-120\u5206\u949f\uff09\u548c\u957f\u671f\uff083-24\u5c0f\u65f6\u4ee5\u4e0a\uff09\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u4e0b\u7684\u6027\u80fd\u3002", "result": "1\uff091\u5c0f\u65f6\u5185\u7684\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u6548\u679c\u6700\u4f73\uff1b2\uff09\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u597d\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u597d\uff1b3\uff09\u6a21\u578b\u6027\u80fd\u53d7\u591a\u53d8\u91cf\u6570\u636e\u96c6\u548c\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u5f71\u54cd\uff1b4\uff09\u4e2a\u6027\u5316\u6570\u636e\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u8d28\u91cf\u9650\u5236\uff0c\u57fa\u4e8e\u4eba\u7fa4\u7684\u6a21\u578b\u66f4\u53d7\u9752\u7750\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u9884\u6d4b1\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u7684\u4f4e\u8840\u7cd6\u4e8b\u4ef6\uff0c\u4f46\u9700\u8981\u6839\u636e\u9884\u6d4b\u65f6\u95f4\u7a97\u53e3\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u7c7b\u578b\uff0c\u540c\u65f6\u9700\u8981\u8003\u8651\u6570\u636e\u8d28\u91cf\u548c\u4e2a\u6027\u5316\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.12110", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12110", "abs": "https://arxiv.org/abs/2601.12110", "authors": ["David. Casillas-P\u00e9rez", "Daniel. Merino-P\u00e9rez", "Silvia. Jim\u00e9nez-Fern\u00e1ndez", "J. Antonio. Portilla-Figueras", "Sancho. Salcedo-Sanz"], "title": "Extended Weighted ABG: A Robust Non-Linear ABG-Based Approach for Optimal Combination of ABG Path-Loss Propagation Models", "comment": null, "summary": "This paper proposes a robust non-linear generalized path-loss propagation model, the Extended Weighted ABG (EWABG), which efficiently allows generating a path-loss propagation model by combining several available path-loss datasets (from measurements campaigns) and other previously proposed state-of-the-art 5G path-loss propagation models. The EWABG model works by integrating individual path-loss models into one single model in the least-squares sense, allowing to extend knowledge from frequencies and distances covered by path-loss datasets or path-loss propagation models. The proposed EWABG model is the first non-linear extension of the common ABG-based approach, which surpasses the non-uniformity problem between the low and high 5G frequencies (as most measurements campaigns have taken place in low frequencies). The EWABG also addresses the problem of removing outlier measurements, a step not included in previous propagation path-loss models. In this case, we have compared the most recent techniques for avoiding outliers, and we have adopted the Theil-Sen method, due to its strong robustness demonstrated in the experiments carried out. In addition, the proposed model specifically considers non-linear attenuation by atmospheric gases, in order to improve its estimations. The good performance of the proposed EWABG model has been tested and compared against recent 5G propagation path-loss models including the ABG and WABG models. The exhaustive experimentation carried out includes the 5G non-line-of-sight environment in different 5G scenarios, UMiSC, UMiOS and UMa. The proposed EWABG obtains the best accuracy, specially in noisy environments with outliers, reporting negligible increment error rates (with respect to the non-outliers situation), lower than 1%, compared to the ABG and WABG.", "AI": {"tldr": "\u63d0\u51faEWABG\u6a21\u578b\uff0c\u9996\u4e2a\u975e\u7ebf\u6027\u6269\u5c55\u7684ABG\u8def\u5f84\u635f\u8017\u6a21\u578b\uff0c\u80fd\u6574\u5408\u591a\u4e2a\u6570\u636e\u96c6\u548c\u73b0\u6709\u6a21\u578b\uff0c\u89e3\u51b35G\u9ad8\u4f4e\u9891\u4e0d\u5747\u5300\u95ee\u9898\u548c\u5f02\u5e38\u503c\u5904\u7406", "motivation": "\u73b0\u67095G\u8def\u5f84\u635f\u8017\u6a21\u578b\u5b58\u5728\u9ad8\u4f4e\u9891\u6d4b\u91cf\u4e0d\u5747\u5300\u95ee\u9898\uff08\u591a\u6570\u6d4b\u91cf\u96c6\u4e2d\u5728\u4f4e\u9891\uff09\uff0c\u7f3a\u4e4f\u5f02\u5e38\u503c\u5904\u7406\u673a\u5236\uff0c\u4e14\u9700\u8981\u6574\u5408\u591a\u4e2a\u6570\u636e\u96c6\u548c\u73b0\u6709\u6a21\u578b\u6765\u6269\u5c55\u9891\u7387\u548c\u8ddd\u79bb\u8986\u76d6\u8303\u56f4", "method": "\u63d0\u51fa\u6269\u5c55\u52a0\u6743ABG\uff08EWABG\uff09\u6a21\u578b\uff0c\u91c7\u7528\u975e\u7ebf\u6027\u6269\u5c55\u7684ABG\u65b9\u6cd5\uff0c\u4f7f\u7528Theil-Sen\u65b9\u6cd5\u5904\u7406\u5f02\u5e38\u503c\uff0c\u8003\u8651\u5927\u6c14\u6c14\u4f53\u975e\u7ebf\u6027\u8870\u51cf\uff0c\u5728\u6700\u5c0f\u4e8c\u4e58\u610f\u4e49\u4e0b\u6574\u5408\u591a\u4e2a\u8def\u5f84\u635f\u8017\u6570\u636e\u96c6\u548c\u6a21\u578b", "result": "EWABG\u57285G\u975e\u89c6\u8ddd\u73af\u5883\uff08UMiSC\u3001UMiOS\u3001UMa\u573a\u666f\uff09\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u6709\u5f02\u5e38\u503c\u7684\u566a\u58f0\u73af\u5883\u4e2d\uff0c\u8bef\u5dee\u7387\u589e\u52a0\u53ef\u5ffd\u7565\uff08\u4f4e\u4e8e1%\uff09\uff0c\u4f18\u4e8eABG\u548cWABG\u6a21\u578b", "conclusion": "EWABG\u662f\u9996\u4e2a\u975e\u7ebf\u6027\u6269\u5c55\u7684ABG\u8def\u5f84\u635f\u8017\u6a21\u578b\uff0c\u80fd\u6709\u6548\u6574\u5408\u591a\u4e2a\u6570\u636e\u96c6\uff0c\u89e3\u51b35G\u9ad8\u4f4e\u9891\u4e0d\u5747\u5300\u95ee\u9898\uff0c\u5e76\u5305\u542b\u5f02\u5e38\u503c\u5904\u7406\u673a\u5236\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02"}}
{"id": "2601.13642", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13642", "abs": "https://arxiv.org/abs/2601.13642", "authors": ["Yuchen Jiao", "Jiin Woo", "Gen Li", "Gauri Joshi", "Yuejie Chi"], "title": "Sample Complexity of Average-Reward Q-Learning: From Single-agent to Federated Reinforcement Learning", "comment": null, "summary": "Average-reward reinforcement learning offers a principled framework for long-term decision-making by maximizing the mean reward per time step. Although Q-learning is a widely used model-free algorithm with established sample complexity in discounted and finite-horizon Markov decision processes (MDPs), its theoretical guarantees for average-reward settings remain limited. This work studies a simple but effective Q-learning algorithm for average-reward MDPs with finite state and action spaces under the weakly communicating assumption, covering both single-agent and federated scenarios. For the single-agent case, we show that Q-learning with carefully chosen parameters achieves sample complexity $\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{\\varepsilon^3}\\right)$, where $\\|h^{\\star}\\|_{\\mathsf{sp}}$ is the span norm of the bias function, improving previous results by at least a factor of $\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}^2}{\\varepsilon^2}$. In the federated setting with $M$ agents, we prove that collaboration reduces the per-agent sample complexity to $\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{M\\varepsilon^3}\\right)$, with only $\\widetilde{O}\\left(\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}}{\\varepsilon}\\right)$ communication rounds required. These results establish the first federated Q-learning algorithm for average-reward MDPs, with provable efficiency in both sample and communication complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5e73\u5747\u5956\u52b1MDPs\u7684Q-learning\u7b97\u6cd5\uff0c\u5728\u5355\u667a\u80fd\u4f53\u548c\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u9996\u6b21\u5efa\u7acb\u4e86\u8054\u90a6Q-learning\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u5e73\u5747\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e3a\u957f\u671f\u51b3\u7b56\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u4f46Q-learning\u5728\u5e73\u5747\u5956\u52b1\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u4fdd\u8bc1\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u5728\u6837\u672c\u590d\u6742\u5ea6\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e14\u7f3a\u4e4f\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u7406\u8bba\u5206\u6790\u3002", "method": "\u7814\u7a76\u6709\u9650\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u5f31\u901a\u4fe1\u5e73\u5747\u5956\u52b1MDPs\uff0c\u8bbe\u8ba1\u53c2\u6570\u7cbe\u5fc3\u9009\u62e9\u7684Q-learning\u7b97\u6cd5\u3002\u5206\u6790\u5355\u667a\u80fd\u4f53\u573a\u666f\uff0c\u5e76\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5176\u4e2d\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u5b66\u4e60\u3002", "result": "\u5355\u667a\u80fd\u4f53\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{\\varepsilon^3}\\right)$\uff0c\u6bd4\u5148\u524d\u7ed3\u679c\u6539\u8fdb\u81f3\u5c11$\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}^2}{\\varepsilon^2}$\u500d\u3002\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u6837\u672c\u590d\u6742\u5ea6\u964d\u81f3$\\widetilde{O}\\left(\\frac{|\\mathcal{S}||\\mathcal{A}|\\|h^{\\star}\\|_{\\mathsf{sp}}^3}{M\\varepsilon^3}\\right)$\uff0c\u4ec5\u9700$\\widetilde{O}\\left(\\frac{\\|h^{\\star}\\|_{\\mathsf{sp}}}{\\varepsilon}\\right)$\u901a\u4fe1\u8f6e\u6b21\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u5efa\u7acb\u4e86\u5e73\u5747\u5956\u52b1MDPs\u7684\u8054\u90a6Q-learning\u7b97\u6cd5\uff0c\u5728\u6837\u672c\u590d\u6742\u5ea6\u548c\u901a\u4fe1\u590d\u6742\u5ea6\u65b9\u9762\u90fd\u5177\u6709\u53ef\u8bc1\u660e\u7684\u6548\u7387\uff0c\u4e3a\u957f\u671f\u51b3\u7b56\u7684\u5206\u5e03\u5f0f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.11616", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11616", "abs": "https://arxiv.org/abs/2601.11616", "authors": ["Feilong Liu"], "title": "Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly characterized. In this work, we study MoEs through a geometric lens, interpreting routing as a form of soft partitioning of the representation space into overlapping local charts. We introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local function geometry via Jacobian singular-value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting that permits exact Jacobian computation, we compare dense, Top-k, and fully-soft routing architectures under matched capacity. Across random seeds, we observe that MoE routing consistently reduces local sensitivity, with expert-local Jacobians exhibiting smaller leading singular values and faster spectral decay than dense baselines. At the same time, weighted PCA reveals that expert-local representations distribute variance across a larger number of principal directions, indicating higher effective rank under identical input distributions. We further find that average expert Jacobians are nearly orthogonal, suggesting a decomposition of the transformation into low-overlap expert-specific subspaces rather than scaled variants of a shared map. We analyze how routing sharpness modulates these effects, showing that Top-k routing produces lower-rank, more concentrated expert-local structure, while fully-soft routing yields broader, higher-rank representations. Together, these results support a geometric interpretation of MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance.", "AI": {"tldr": "MoE\u67b6\u6784\u901a\u8fc7\u8f6f\u5206\u533a\u8868\u793a\u7a7a\u95f4\uff0c\u964d\u4f4e\u5c40\u90e8\u654f\u611f\u6027\u548c\u66f2\u7387\uff0c\u540c\u65f6\u589e\u52a0\u8868\u793a\u7684\u6709\u6548\u79e9\uff0c\u4e13\u5bb6\u95f4\u53d8\u6362\u8fd1\u4e4e\u6b63\u4ea4\u3002", "motivation": "\u7814\u7a76MoE\u67b6\u6784\u5bf9\u5b66\u4e60\u51fd\u6570\u548c\u8868\u793a\u51e0\u4f55\u7279\u6027\u7684\u5f71\u54cd\uff0c\u7406\u89e3\u8def\u7531\u673a\u5236\u5982\u4f55\u5851\u9020\u8868\u793a\u7a7a\u95f4\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u53cc\u96c5\u53ef\u6bd4-PCA\u8c31\u51e0\u4f55\u63a2\u9488\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u5947\u5f02\u503c\u8c31\u5206\u6790\u5c40\u90e8\u51fd\u6570\u51e0\u4f55\uff0c\u901a\u8fc7\u52a0\u6743PCA\u5206\u6790\u8def\u7531\u9690\u85cf\u72b6\u6001\u7684\u8868\u793a\u51e0\u4f55\uff0c\u5728\u53ef\u63a7MLP-MoE\u8bbe\u7f6e\u4e0b\u6bd4\u8f83\u5bc6\u96c6\u3001Top-k\u548c\u5168\u8f6f\u8def\u7531\u67b6\u6784\u3002", "result": "MoE\u8def\u7531\u4e00\u81f4\u964d\u4f4e\u5c40\u90e8\u654f\u611f\u6027\uff0c\u4e13\u5bb6\u5c40\u90e8\u96c5\u53ef\u6bd4\u77e9\u9635\u5177\u6709\u66f4\u5c0f\u7684\u4e3b\u5bfc\u5947\u5f02\u503c\u548c\u66f4\u5feb\u7684\u8c31\u8870\u51cf\uff1b\u52a0\u6743PCA\u663e\u793a\u4e13\u5bb6\u5c40\u90e8\u8868\u793a\u5728\u66f4\u591a\u4e3b\u65b9\u5411\u4e0a\u5206\u5e03\u65b9\u5dee\uff0c\u8868\u660e\u66f4\u9ad8\u6709\u6548\u79e9\uff1b\u5e73\u5747\u4e13\u5bb6\u96c5\u53ef\u6bd4\u77e9\u9635\u8fd1\u4e4e\u6b63\u4ea4\uff1bTop-k\u8def\u7531\u4ea7\u751f\u66f4\u4f4e\u79e9\u3001\u66f4\u96c6\u4e2d\u7684\u4e13\u5bb6\u5c40\u90e8\u7ed3\u6784\uff0c\u5168\u8f6f\u8def\u7531\u4ea7\u751f\u66f4\u5bbd\u3001\u66f4\u9ad8\u79e9\u7684\u8868\u793a\u3002", "conclusion": "MoE\u53ef\u89e3\u91ca\u4e3a\u51fd\u6570\u7a7a\u95f4\u7684\u8f6f\u5206\u533a\uff0c\u80fd\u591f\u5e73\u5766\u5316\u5c40\u90e8\u66f2\u7387\u540c\u65f6\u91cd\u65b0\u5206\u5e03\u8868\u793a\u65b9\u5dee\uff0c\u4e3a\u7406\u89e3MoE\u67b6\u6784\u7684\u51e0\u4f55\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.12171", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12171", "abs": "https://arxiv.org/abs/2601.12171", "authors": ["Jeffrey W. Utley", "Gregery T. Buzzard", "Charles A. Bouman", "Matthew R. Kemnetz"], "title": "Boiling flow estimation for aero-optic phase screen generation", "comment": "Submitted to the Unconventional Imaging, Sensing, and Adaptive Optics special session of Optical Engineering", "summary": "Aero-optic effects due to turbulence can reduce the effectiveness of transmitting light waves to a distant target. Methods to compensate for turbulence typically rely on realistic turbulence data, which can be generated by i) experiment, ii) high-fidelity CFD, iii) low-fidelity CFD, and iv) autoregressive methods. However, each of these methods has significant drawbacks, including monetary and/or computational expense, limited quantity, inaccurate statistics, and overall complexity. In contrast, the boiling flow algorithm is a simple, computationally efficient model that can generate atmospheric phase screen data with only a handful of parameters. However, boiling flow has not been widely used in aero-optic applications, at least in part because some of these parameters, such as r0, are not clearly defined for aero-optic data. In this paper, we demonstrate a method to use the boiling flow algorithm to generate arbitrary length synthetic data to match the statistics of measured aero-optic data. Importantly, we modify the standard boiling flow method to generate anisotropic phase screens. While this model does not fully capture all statistics, it can be used to generate data that matches the temporal power spectrum or the anisotropic 2D structure function, with the ability to trade fidelity to one for fidelity to the other.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u6cb8\u817e\u6d41\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5339\u914d\u5b9e\u6d4b\u6c14\u52a8\u5149\u5b66\u6570\u636e\u7edf\u8ba1\u7279\u6027\u7684\u4efb\u610f\u957f\u5ea6\u5408\u6210\u6570\u636e\uff0c\u7279\u522b\u652f\u6301\u5404\u5411\u5f02\u6027\u76f8\u4f4d\u5c4f\u751f\u6210\u3002", "motivation": "\u6e4d\u6d41\u5f15\u8d77\u7684\u6c14\u52a8\u5149\u5b66\u6548\u5e94\u4f1a\u964d\u4f4e\u5149\u6ce2\u4f20\u8f93\u6548\u7387\uff0c\u73b0\u6709\u6e4d\u6d41\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff08\u5b9e\u9a8c\u3001\u9ad8/\u4f4e\u7cbe\u5ea6CFD\u3001\u81ea\u56de\u5f52\u65b9\u6cd5\uff09\u5b58\u5728\u6210\u672c\u9ad8\u3001\u6570\u636e\u91cf\u6709\u9650\u3001\u7edf\u8ba1\u4e0d\u51c6\u786e\u6216\u590d\u6742\u5ea6\u9ad8\u7b49\u95ee\u9898\u3002\u6cb8\u817e\u6d41\u7b97\u6cd5\u867d\u7136\u7b80\u5355\u9ad8\u6548\uff0c\u4f46\u53c2\u6570\u5b9a\u4e49\u4e0d\u660e\u786e\u4e14\u672a\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6c14\u52a8\u5149\u5b66\u9886\u57df\u3002", "method": "\u6539\u8fdb\u6807\u51c6\u6cb8\u817e\u6d41\u7b97\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u5404\u5411\u5f02\u6027\u76f8\u4f4d\u5c4f\uff0c\u901a\u8fc7\u8c03\u6574\u53c2\u6570\u5339\u914d\u5b9e\u6d4b\u6c14\u52a8\u5149\u5b66\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u7279\u522b\u662f\u65f6\u95f4\u529f\u7387\u8c31\u6216\u5404\u5411\u5f02\u6027\u4e8c\u7ef4\u7ed3\u6784\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u4efb\u610f\u957f\u5ea6\u7684\u5408\u6210\u6570\u636e\uff0c\u5339\u914d\u5b9e\u6d4b\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5728\u65f6\u95f4\u529f\u7387\u8c31\u548c\u5404\u5411\u5f02\u6027\u4e8c\u7ef4\u7ed3\u6784\u51fd\u6570\u7684\u4fdd\u771f\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u6743\u8861\u3002", "conclusion": "\u6539\u8fdb\u7684\u6cb8\u817e\u6d41\u7b97\u6cd5\u4e3a\u6c14\u52a8\u5149\u5b66\u6e4d\u6d41\u8865\u507f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u867d\u7136\u4e0d\u80fd\u5b8c\u5168\u6355\u83b7\u6240\u6709\u7edf\u8ba1\u7279\u6027\uff0c\u4f46\u5728\u5173\u952e\u7edf\u8ba1\u6307\u6807\u4e0a\u80fd\u591f\u5339\u914d\u5b9e\u6d4b\u6570\u636e\u3002"}}
{"id": "2601.13874", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13874", "abs": "https://arxiv.org/abs/2601.13874", "authors": ["Shijie Zhong", "Jiangfeng Fu", "Yikun Yang"], "title": "Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses", "comment": null, "summary": "The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\\mathcal O(n^2)$ to $\\mathcal O(n \\log n)$.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7edf\u8ba1\u91cf\u7684\u65b9\u5dee\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u6709\u9650\u6837\u672c\u65b9\u5dee\u8868\u5f81\u65b9\u6cd5\uff0c\u5e76\u5728\u62c9\u666e\u62c9\u65af\u6838\u4e0b\u5f00\u53d1\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u81f3O(n log n)\u7684\u7cbe\u786e\u52a0\u901f\u7b97\u6cd5\u3002", "motivation": "MMD\u4f5c\u4e3a\u57fa\u4e8e\u6838\u7684\u975e\u53c2\u6570\u53cc\u6837\u672c\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u5176\u63a8\u65ad\u51c6\u786e\u6027\u4e25\u91cd\u4f9d\u8d56\u4e8e\u65b9\u5dee\u8868\u5f81\u3002\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5404\u79cd\u6709\u9650\u6837\u672c\u7684MMD\u65b9\u5dee\u4f30\u8ba1\u5668\uff0c\u4f46\u8fd9\u4e9b\u4f30\u8ba1\u5668\u5728\u96f6\u5047\u8bbe\u4e0e\u5907\u62e9\u5047\u8bbe\u4e0b\u3001\u5e73\u8861\u6216\u4e0d\u5e73\u8861\u62bd\u6837\u65b9\u6848\u4e2d\u5f80\u5f80\u5b58\u5728\u5dee\u5f02\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u901a\u8fc7U\u7edf\u8ba1\u91cf\u8868\u793a\u548cHoeffding\u5206\u89e3\u7814\u7a76MMD\u7edf\u8ba1\u91cf\u7684\u65b9\u5dee\u7279\u6027\uff0c\u5efa\u7acb\u4e86\u8986\u76d6\u4e0d\u540c\u5047\u8bbe\u548c\u6837\u672c\u914d\u7f6e\u7684\u7edf\u4e00\u6709\u9650\u6837\u672c\u8868\u5f81\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u9488\u5bf9\u62c9\u666e\u62c9\u65af\u6838\u4e0b\u7684\u5355\u53d8\u91cf\u60c5\u51b5\uff0c\u63d0\u51fa\u4e86\u7cbe\u786e\u52a0\u901f\u65b9\u6cd5\uff0c\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4f4e\u5230O(n log n)\u3002", "result": "\u5efa\u7acb\u4e86MMD\u65b9\u5dee\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5047\u8bbe\u548c\u6837\u672c\u914d\u7f6e\u3002\u5728\u62c9\u666e\u62c9\u65af\u6838\u7684\u5355\u53d8\u91cf\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\uff0c\u4ece\u4e8c\u6b21\u590d\u6742\u5ea6\u964d\u4f4e\u5230\u63a5\u8fd1\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aMMD\u65b9\u5dee\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7279\u5b9a\u6838\u51fd\u6570\u4e0b\u5b9e\u73b0\u8ba1\u7b97\u52a0\u901f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u53cc\u6837\u672c\u68c0\u9a8c\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u65b9\u6cd5\u6539\u8fdb\u3002"}}
{"id": "2601.11618", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11618", "abs": "https://arxiv.org/abs/2601.11618", "authors": ["Luis Rosario Freytes"], "title": "Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention", "comment": "57 pages", "summary": "Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.", "AI": {"tldr": "\u51e0\u4f55\u6ce8\u610f\u529b\uff08GA\uff09\u901a\u8fc7\u56db\u4e2a\u72ec\u7acb\u8f93\u5165\u5b9a\u4e49\u6ce8\u610f\u529b\u5c42\uff1a\u8f7d\u4f53\u3001\u8bc1\u636e\u6838\u89c4\u5219\u3001\u63a2\u9488\u65cf\u548c\u951a\u5b9a/\u66f4\u65b0\u89c4\u5219\uff0c\u5c06\u4e0d\u53d8\u7ed3\u6784\u4e0e\u5efa\u6a21\u9009\u62e9\u5206\u79bb\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982Transformer\u4e2d\u7684softmax\uff09\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u96be\u4ee5\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u548c\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u51e0\u4f55\u6ce8\u610f\u529b\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u5206\u79bb\u4e0d\u53d8\u7ed3\u6784\u4e0e\u5efa\u6a21\u9009\u62e9\uff0c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u7ec4\u4ef6\u5b9a\u4e49\uff1a\u6709\u9650\u8f7d\u4f53\uff08\u53ef\u5bfb\u5740\u7d22\u5f15\uff09\u3001\u8bc1\u636e\u6838\u89c4\u5219\uff08\u63a9\u7801\u539f\u59cb\u5206\u6570\u548c\u94fe\u63a5\u4ea7\u751f\u975e\u8d1f\u6743\u91cd\uff09\u3001\u63a2\u9488\u65cf\uff08\u53ef\u89c2\u6d4b\u91cf\u7684\u96c6\u5408\uff09\u3001\u951a\u5b9a/\u66f4\u65b0\u89c4\u5219\uff08\u9009\u62e9\u548c\u5e94\u7528\u4ee3\u8868\u6027\u6838\uff09\u3002\u5728\u6807\u91cf\u5173\u7cfb\u5de5\u4f5c\u8868\u793a\u548c\u8bc1\u636e\u4e58\u6cd5\u7ec4\u5408\u5f8b\u4e0b\uff0c\u63a8\u5bfc\u51fa\u6307\u6570\u94fe\u63a5\u65cf\uff08Gibbs\u6743\u91cd\uff09\uff0c\u5305\u542bsoftmax\u6838\u4f5c\u4e3a\u5b50\u60c5\u51b5\u3002\u901a\u8fc7\u5546\u5316\u4e00\u5143\u884c/\u5217\u5206\u6570\u573a\uff0c\u5f97\u5230\u89c4\u8303\u79e9r\u8303\u5f0f\uff08Eckart-Young/SVD\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u51e0\u4f55\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u5c06\u6807\u51c6Transformer\u6ce8\u610f\u529b\u3001\u81ea\u9002\u5e94\u8f7d\u4f53\u3001\u591a\u6838\u6ce8\u610f\u529b\u3001\u57fa\u4e8e\u8ba1\u5212\u7684\u951a\u5b9a\uff08\u5982\u71b5\u6700\u4f18\u4f20\u8f93/Sinkhorn\uff09\u7b49\u4f5c\u4e3a\u7279\u4f8b\u5305\u542b\u5176\u4e2d\u3002\u8bc1\u660e\u4e86\u70b9\u79ef\u5206\u6570\u56fe\u5b9e\u73b0\u4e86\u76f8\u5e94\u7684\u4f4e\u79e9\u4ea4\u4e92\u673a\u5236\u3002", "conclusion": "\u51e0\u4f55\u6ce8\u610f\u529b\u6846\u67b6\u5206\u79bb\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e0d\u53d8\u7ed3\u6784\u4e0e\u5efa\u6a21\u9009\u62e9\uff0c\u4e3a\u7cfb\u7edf\u6bd4\u8f83\u548c\u6269\u5c55\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u652f\u6301\u591a\u79cd\u6ce8\u610f\u529b\u53d8\u4f53\uff08\u5305\u62ec\u591a\u6838\u3001\u81ea\u9002\u5e94\u8f7d\u4f53\u3001\u57fa\u4e8e\u8ba1\u5212\u7684\u951a\u5b9a\u7b49\uff09\u7684\u7edf\u4e00\u63cf\u8ff0\u3002"}}
{"id": "2601.12278", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12278", "abs": "https://arxiv.org/abs/2601.12278", "authors": ["Yingquan Li", "Jiajie Xu", "Bodhibrata Mukhopadhyay", "Mohamed-Slim Alouini"], "title": "Low-Complexity RSS-based Underwater Localization with Unknown Transmit Power", "comment": null, "summary": "Underwater wireless sensor networks (UWSNs) have received significant attention due to their various applications, with underwater target localization playing a vital role in enhancing network performance. Given the challenges and high costs associated with UWSN deployments, Received Signal Strength (RSS)-based localization offers a viable solution due to its minimal hardware requirements and cost-effectiveness. In this paper, we assign distance-based weights to RSS measurements, providing higher reliability to closer anchor nodes. Using the weighted RSS measurements and generalized trust region subproblem (GTRS), we propose the GTRS-based localization technique with Unknown Transmit Power (GUTP), which can be solved by a simple bisection method. Unlike conventional localization methods that require prior knowledge of the target node's transmit power, GUTP jointly estimates both the location and transmit power of the target node, broadening its practical use. Additionally, we derive the Cramer-Rao lower bounds (CRLBs) for RSS-based underwater localization with known and unknown transmit power, respectively. Extensive simulations demonstrate that GUTP achieves enhanced accuracy and significantly lower computational complexity in estimating the target node's location and transmit power compared to existing semidefinite programming (SDP)-based techniques.", "AI": {"tldr": "\u63d0\u51faGUTP\u65b9\u6cd5\uff0c\u5229\u7528\u52a0\u6743RSS\u6d4b\u91cf\u548c\u5e7f\u4e49\u4fe1\u4efb\u57df\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u4e8c\u5206\u6cd5\u8054\u5408\u4f30\u8ba1\u6c34\u4e0b\u76ee\u6807\u4f4d\u7f6e\u548c\u53d1\u5c04\u529f\u7387\uff0c\u65e0\u9700\u5148\u9a8c\u529f\u7387\u77e5\u8bc6\uff0c\u76f8\u6bd4\u73b0\u6709SDP\u65b9\u6cd5\u7cbe\u5ea6\u66f4\u9ad8\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "motivation": "\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u6311\u6218\u5927\uff0c\u57fa\u4e8e\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u7684\u5b9a\u4f4d\u65b9\u6cd5\u786c\u4ef6\u9700\u6c42\u4f4e\u3001\u6210\u672c\u6548\u76ca\u9ad8\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u76ee\u6807\u8282\u70b9\u53d1\u5c04\u529f\u7387\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faGUTP\u65b9\u6cd5\uff1a1) \u5bf9RSS\u6d4b\u91cf\u503c\u5206\u914d\u57fa\u4e8e\u8ddd\u79bb\u7684\u6743\u91cd\uff0c\u4f7f\u66f4\u8fd1\u7684\u951a\u8282\u70b9\u5177\u6709\u66f4\u9ad8\u53ef\u9760\u6027\uff1b2) \u5229\u7528\u52a0\u6743RSS\u6d4b\u91cf\u548c\u5e7f\u4e49\u4fe1\u4efb\u57df\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u7b80\u5355\u4e8c\u5206\u6cd5\u6c42\u89e3\uff1b3) \u8054\u5408\u4f30\u8ba1\u76ee\u6807\u8282\u70b9\u4f4d\u7f6e\u548c\u53d1\u5c04\u529f\u7387\uff0c\u65e0\u9700\u5148\u9a8c\u529f\u7387\u77e5\u8bc6\u3002", "result": "\u63a8\u5bfc\u4e86\u5df2\u77e5\u548c\u672a\u77e5\u53d1\u5c04\u529f\u7387\u60c5\u51b5\u4e0bRSS\u6c34\u4e0b\u5b9a\u4f4d\u7684Cramer-Rao\u4e0b\u754c\u3002\u4eff\u771f\u8868\u660e\uff0cGUTP\u76f8\u6bd4\u73b0\u6709SDP\u65b9\u6cd5\u5728\u4f30\u8ba1\u76ee\u6807\u8282\u70b9\u4f4d\u7f6e\u548c\u53d1\u5c04\u529f\u7387\u65b9\u9762\u7cbe\u5ea6\u66f4\u9ad8\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "GUTP\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u4f4d\u7f6e\u548c\u53d1\u5c04\u529f\u7387\uff0c\u65e0\u9700\u5148\u9a8c\u529f\u7387\u77e5\u8bc6\uff0c\u6269\u5c55\u4e86\u5b9e\u9645\u5e94\u7528\u8303\u56f4\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6\u548c\u66f4\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u6c34\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14031", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14031", "abs": "https://arxiv.org/abs/2601.14031", "authors": ["Stefano Damato", "Nicol\u00f2 Rubattu", "Dario Azzimonti", "Giorgio Corani"], "title": "Intermittent time series forecasting: local vs global models", "comment": "Submitted to Data Mining and Knowledge Discovery", "summary": "Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5c40\u90e8\u6a21\u578b\uff08iETS\u3001TweedieGP\uff09\u548c\u5168\u5c40\u6a21\u578b\uff08D-Linear\u3001DeepAR\u3001Transformers\uff09\u5728\u95f4\u6b47\u6027\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0D-Linear\u5728\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u4f18\u4e8e\u5c40\u90e8\u6a21\u578b\u3002", "motivation": "\u95f4\u6b47\u6027\u65f6\u95f4\u5e8f\u5217\uff08\u5305\u542b\u5927\u91cf\u96f6\u503c\uff09\u5728\u4f9b\u5e94\u94fe\u5e93\u5b58\u4e2d\u5360\u6bd4\u5f88\u5927\uff0c\u9700\u8981\u6982\u7387\u9884\u6d4b\u6765\u89c4\u5212\u5e93\u5b58\u6c34\u5e73\u3002\u867d\u7136\u5168\u5c40\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u4f46\u5c1a\u672a\u5728\u95f4\u6b47\u6027\u65f6\u95f4\u5e8f\u5217\u4e0a\u5f97\u5230\u5145\u5206\u6d4b\u8bd5\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u5c40\u90e8\u6a21\u578b\uff08iETS\u3001TweedieGP\uff09\u548c\u5168\u5c40\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff08D-Linear\u3001DeepAR\u3001Transformers\uff09\u3002\u5bf9\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u9002\u5408\u95f4\u6b47\u6027\u5e8f\u5217\u7684\u5206\u5e03\u5934\uff1a\u8d1f\u4e8c\u9879\u5206\u5e03\u3001\u969c\u788d\u504f\u79fb\u8d1f\u4e8c\u9879\u5206\u5e03\u548cTweedie\u5206\u5e03\u3002\u5728\u5305\u542b\u8d85\u8fc740,000\u4e2a\u771f\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u7684\u4e94\u4e2a\u5927\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\uff0cD-Linear\u63d0\u4f9b\u6700\u4f73\u51c6\u786e\u6027\uff0c\u4e14\u59cb\u7ec8\u4f18\u4e8e\u5c40\u90e8\u6a21\u578b\uff0c\u540c\u65f6\u8ba1\u7b97\u9700\u6c42\u8f83\u4f4e\u3002\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u8ba1\u7b97\u9700\u6c42\u66f4\u9ad8\u4e14\u51c6\u786e\u6027\u8f83\u5dee\u3002\u5728\u5206\u5e03\u5934\u4e2d\uff0cTweedie\u5bf9\u6700\u9ad8\u5206\u4f4d\u6570\u4f30\u8ba1\u6700\u597d\uff0c\u800c\u8d1f\u4e8c\u9879\u5206\u5e03\u6574\u4f53\u6027\u80fd\u6700\u4f73\u3002", "conclusion": "D-Linear\u662f\u95f4\u6b47\u6027\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6709\u6548\u9009\u62e9\uff0c\u7ed3\u5408\u4e86\u826f\u597d\u7684\u51c6\u786e\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u4f18\u4e8e\u4f20\u7edf\u5c40\u90e8\u6a21\u578b\u7684\u6027\u80fd\u3002Tweedie\u5206\u5e03\u5934\u5728\u9ad8\u5206\u4f4d\u6570\u4f30\u8ba1\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u800c\u8d1f\u4e8c\u9879\u5206\u5e03\u6574\u4f53\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2601.11619", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11619", "abs": "https://arxiv.org/abs/2601.11619", "authors": ["Phani Kumar", "Nyshadham", "Jyothendra Varma", "Polisetty V R K", "Aditya Rathore"], "title": "NoiseFormer -- Noise Diffused Symmetric Attention Transformer", "comment": null, "summary": "Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.", "AI": {"tldr": "\u63d0\u51faNoise Diffused Symmetric Attention Transformer\uff0c\u5728\u4fdd\u6301Symmetric Attention\u5185\u5b58\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5fae\u5c0f\u53c2\u6570\u548c\u8ba1\u7b97\u5f00\u9500\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4ecb\u4e8e\u539f\u59cbSymmetric Attention\u548cGPT2\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u3002", "motivation": "\u968f\u7740Transformer\u6a21\u578b\u89c4\u6a21\u6025\u5267\u589e\u5927\uff0c\u5185\u5b58\u5360\u7528\u95ee\u9898\u5bfc\u81f4\u96be\u4ee5\u5728\u5355\u4e2aGPU/AI\u52a0\u901f\u5668\u4e0a\u90e8\u7f72\uff0c\u9700\u8981\u591a\u8bbe\u5907\u8ba1\u7b97\u4ece\u800c\u589e\u52a0\u6210\u672c\u3002\u8fd9\u4fc3\u4f7f\u9700\u8981\u7a00\u758f\u6ce8\u610f\u529b\u7b49\u53c2\u6570\u51cf\u5c11\u6280\u672f\u6765\u964d\u4f4e\u6a21\u578b\u5927\u5c0f\u3002", "method": "\u5206\u6790Symmetric Dot-Product Attention\uff08\u5bf9\u79f0\u6ce8\u610f\u529b\uff09\u6280\u672f\uff0c\u63d0\u51faNoise Diffused Symmetric Attention Transformer\u7edf\u4e00\u67b6\u6784\u3002\u5728\u4fdd\u6301\u5bf9\u79f0\u6ce8\u610f\u529b\u5185\u5b58\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u6dfb\u52a0\u5fae\u5c0f\u566a\u58f0\u6269\u6563\u673a\u5236\u6765\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u5728GPT2\u57fa\u7840\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u5728\u591a\u79cdGLUE\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u8868\u73b0\u4ecb\u4e8e\u539f\u59cb\u5bf9\u79f0\u6ce8\u610f\u529b\u548cGPT2\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8e\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6a21\u578b\u5927\u5c0f\u7f29\u51cf\u3002", "conclusion": "\u63d0\u51fa\u7684Noise Diffused Symmetric Attention Transformer\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5fae\u5c0f\u5f00\u9500\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12281", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12281", "abs": "https://arxiv.org/abs/2601.12281", "authors": ["Lingyi Zhu", "Zhongxiang Wei", "Fan Liu", "Jianjun Wu", "Xiao-Wei Tang", "Christos Masouros", "Shanpu Shen"], "title": "Overcoming BS Down-Tilt for Air-Ground ISAC Coverage: Antenna Design, Beamforming and User Scheduling", "comment": null, "summary": "Integrated sensing and communication holds great promise for low-altitude economy applications. However, conventional downtilted base stations primarily provide sectorized forward lobes for ground services, failing to sense air targets due to backward blind zones. In this paper, a novel antenna structure is proposed to enable air-ground beam steering, facilitating simultaneous full-space sensing and communication (S&C). Specifically, instead of inserting a reflector behind the antenna array for backlobe mitigation, an omni-steering plate is introduced to collaborate with the active array for omnidirectional beamforming. Building on this hardware innovation, sum S&C mutual information (MI) is maximized, jointly optimizing user scheduling, passive coefficients of the omni-steering plate, and beamforming of the active array. The problem is decomposed into two subproblems: one for optimizing passive coefficients via Riemannian gradient on the manifold, and the other for optimizing user scheduling and active array beamforming. Exploiting relationships among S&C MI, data decoding MMSE, and parameter estimation MMSE, the original subproblem is equivalently transformed into a sum weighted MMSE problem, rigorously established via the Lagrangian and first-order optimality conditions. Simulations show that the proposed algorithm outperforms baselines in sum-MI and MSE, while providing 360 sensing coverage. Beampattern analysis further demonstrates effective user scheduling and accurate target alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u5929\u7ebf\u7ed3\u6784\uff0c\u901a\u8fc7\u5168\u5411\u8f6c\u5411\u677f\u4e0e\u6709\u6e90\u9635\u5217\u534f\u4f5c\u5b9e\u73b0\u5168\u7a7a\u95f4\u6ce2\u675f\u6210\u5f62\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u7ad9\u65e0\u6cd5\u611f\u77e5\u7a7a\u4e2d\u76ee\u6807\u7684\u95ee\u9898\uff0c\u5e76\u4f18\u5316\u4e86\u7528\u6237\u8c03\u5ea6\u3001\u88ab\u52a8\u7cfb\u6570\u548c\u6ce2\u675f\u6210\u5f62\u4ee5\u6700\u5927\u5316\u611f\u77e5\u901a\u4fe1\u4e92\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u4e0b\u503e\u57fa\u7ad9\u4e3b\u8981\u4e3a\u5730\u9762\u670d\u52a1\u63d0\u4f9b\u6247\u533a\u524d\u5411\u6ce2\u675f\uff0c\u4f46\u7531\u4e8e\u540e\u5411\u76f2\u533a\u65e0\u6cd5\u611f\u77e5\u7a7a\u4e2d\u76ee\u6807\uff0c\u8fd9\u9650\u5236\u4e86\u4f4e\u7a7a\u7ecf\u6d4e\u5e94\u7528\u4e2d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u65b0\u578b\u5929\u7ebf\u7ed3\u6784\uff1a\u7528\u5168\u5411\u8f6c\u5411\u677f\u66ff\u4ee3\u4f20\u7edf\u53cd\u5c04\u5668\uff0c\u4e0e\u6709\u6e90\u9635\u5217\u534f\u4f5c\u5b9e\u73b0\u5168\u5411\u6ce2\u675f\u6210\u5f62\u3002\u5c06\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u95ee\u9898\uff1a1) \u5728\u6d41\u5f62\u4e0a\u901a\u8fc7\u9ece\u66fc\u68af\u5ea6\u4f18\u5316\u88ab\u52a8\u7cfb\u6570\uff1b2) \u4f18\u5316\u7528\u6237\u8c03\u5ea6\u548c\u6709\u6e90\u9635\u5217\u6ce2\u675f\u6210\u5f62\u3002\u5229\u7528\u611f\u77e5\u901a\u4fe1\u4e92\u4fe1\u606f\u3001\u6570\u636e\u89e3\u7801MMSE\u548c\u53c2\u6570\u4f30\u8ba1MMSE\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5c06\u539f\u95ee\u9898\u7b49\u4ef7\u8f6c\u5316\u4e3a\u52a0\u6743MMSE\u95ee\u9898\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u603b\u548c\u4e92\u4fe1\u606f\u548c\u5747\u65b9\u8bef\u5dee\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b360\u5ea6\u611f\u77e5\u8986\u76d6\u3002\u6ce2\u675f\u56fe\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6709\u6548\u7684\u7528\u6237\u8c03\u5ea6\u548c\u51c6\u786e\u7684\u76ee\u6807\u5bf9\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5929\u7ebf\u7ed3\u6784\u548c\u4f18\u5316\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u7ad9\u65e0\u6cd5\u540c\u65f6\u8fdb\u884c\u5168\u7a7a\u95f4\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u95ee\u9898\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11638", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.11638", "abs": "https://arxiv.org/abs/2601.11638", "authors": ["Josafat Ribeiro Leal Filho", "Ant\u00f4nio Augusto Fr\u00f6hlich"], "title": "Verifying Physics-Informed Neural Network Fidelity using Classical Fisher Information from Differentiable Dynamical System", "comment": "This paper has been submitted and is currently under review at IEEE Transactions on Neural Networks and Learning Systems (TNNLS)", "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations and modeling physical systems by embedding physical laws into the learning process. However, rigorously quantifying how well a PINN captures the complete dynamical behavior of the system, beyond simple trajectory prediction, remains a challenge. This paper proposes a novel experimental framework to address this by employing Fisher information for differentiable dynamical systems, denoted $g_F^C$. This Fisher information, distinct from its statistical counterpart, measures inherent uncertainties in deterministic systems, such as sensitivity to initial conditions, and is related to the phase space curvature and the net stretching action of the state space evolution. We hypothesize that if a PINN accurately learns the underlying dynamics of a physical system, then the Fisher information landscape derived from the PINN's learned equations of motion will closely match that of the original analytical model. This match would signify that the PINN has achieved comprehensive fidelity capturing not only the state evolution but also crucial geometric and stability properties. We outline an experimental methodology using the dynamical model of a car to compute and compare $g_F^C$ for both the analytical model and a trained PINN. The comparison, based on the Jacobians of the respective system dynamics, provides a quantitative measure of the PINN's fidelity in representing the system's intricate dynamical characteristics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528Fisher\u4fe1\u606f$g_F^C$\u6765\u91cf\u5316\u8bc4\u4f30PINNs\u5bf9\u7269\u7406\u7cfb\u7edf\u52a8\u6001\u884c\u4e3a\u7684\u6355\u83b7\u7a0b\u5ea6\uff0c\u901a\u8fc7\u6bd4\u8f83PINN\u5b66\u4e60\u6a21\u578b\u4e0e\u539f\u59cb\u89e3\u6790\u6a21\u578b\u7684Fisher\u4fe1\u606f\u666f\u89c2\u6765\u9a8c\u8bc1PINN\u7684\u5168\u9762\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f53\u524dPINNs\u5728\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u548c\u5efa\u6a21\u7269\u7406\u7cfb\u7edf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u4e25\u683c\u7684\u65b9\u6cd5\u6765\u91cf\u5316PINN\u662f\u5426\u5b8c\u6574\u6355\u83b7\u4e86\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\uff08\u800c\u4e0d\u4ec5\u4ec5\u662f\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u9700\u8981\u4e00\u79cd\u8bc4\u4f30PINN\u5bf9\u7cfb\u7edf\u51e0\u4f55\u548c\u7a33\u5b9a\u6027\u7279\u6027\u4fdd\u771f\u5ea6\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4f7f\u7528Fisher\u4fe1\u606f$g_F^C$\uff08\u9488\u5bf9\u53ef\u5fae\u52a8\u6001\u7cfb\u7edf\uff09\u6765\u8bc4\u4f30PINN\u7684\u4fdd\u771f\u5ea6\u3002\u8be5\u65b9\u6cd5\u8ba1\u7b97\u5e76\u6bd4\u8f83\u539f\u59cb\u89e3\u6790\u6a21\u578b\u548cPINN\u5b66\u4e60\u6a21\u578b\u7684Fisher\u4fe1\u606f\u666f\u89c2\uff0c\u901a\u8fc7\u5206\u6790\u5404\u81ea\u7cfb\u7edf\u52a8\u6001\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u6765\u91cf\u5316PINN\u5bf9\u7cfb\u7edf\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u3001\u76f8\u7a7a\u95f4\u66f2\u7387\u548c\u72b6\u6001\u7a7a\u95f4\u6f14\u5316\u62c9\u4f38\u6548\u5e94\u7684\u6355\u83b7\u7a0b\u5ea6\u3002", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u6846\u67b6\uff0c\u4f7f\u7528\u6c7d\u8f66\u52a8\u6001\u6a21\u578b\u4f5c\u4e3a\u6848\u4f8b\uff0c\u8ba1\u7b97\u548c\u6bd4\u8f83\u89e3\u6790\u6a21\u578b\u4e0e\u8bad\u7ec3\u540ePINN\u7684$g_F^C$\u3002\u8fd9\u79cd\u6bd4\u8f83\u63d0\u4f9b\u4e86PINN\u5728\u8868\u793a\u7cfb\u7edf\u590d\u6742\u52a8\u6001\u7279\u6027\u65b9\u9762\u4fdd\u771f\u5ea6\u7684\u5b9a\u91cf\u5ea6\u91cf\u3002", "conclusion": "\u901a\u8fc7Fisher\u4fe1\u606f$g_F^C$\u7684\u6bd4\u8f83\uff0c\u53ef\u4ee5\u8bc4\u4f30PINN\u662f\u5426\u4e0d\u4ec5\u51c6\u786e\u9884\u6d4b\u72b6\u6001\u6f14\u5316\uff0c\u800c\u4e14\u5b8c\u6574\u6355\u83b7\u4e86\u7269\u7406\u7cfb\u7edf\u7684\u51e0\u4f55\u548c\u7a33\u5b9a\u6027\u7279\u6027\uff0c\u4e3aPINN\u7684\u5168\u9762\u4fdd\u771f\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.12403", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12403", "abs": "https://arxiv.org/abs/2601.12403", "authors": ["Shu Cai", "Ya-Feng Liu", "Jun Zhan", "Qi Zhang"], "title": "RIS-Enhanced Information-Decoupled Symbiotic Radio Over Broadcasting Signals", "comment": "Accepted to ICASSP 2026. \u00a92026 IEEE. Personal use of this material is permitted", "summary": "This paper studies a reconfigurable intelligent surface (RIS)-enhanced decoupled symbiotic radio (SR) system in which a primary transmitter delivers common data to multiple primary receivers (PRs), while a RIS-based backscatter device sends secondary data to a backscatter receiver (BRx). Unlike conventional SR, the BRx performs energy detection and never decodes the primary signal, thereby removing ambiguity and preventing exposure of the primary payload to unintended receivers. In this paper, we formulate the problem as the minimization of the transmit power subject to a common broadcast rate constraint across all PRs and a bit error rate (BER) constraint at the BRx. The problem is nonconvex due to the unit-modulus RIS constraint and coupled quadratic forms. Leveraging a rate-balanced reformulation and a monotonic BER ratio characterization, we develop a low-complexity penalty-based block coordinate descent algorithm with closed-form updates. Numerical results show fast convergence of the proposed algorithm and reduced power consumption of the considered RIS-enhanced information-decoupled SR system over conventional SR baselines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RIS\u589e\u5f3a\u7684\u89e3\u8026\u5171\u751f\u65e0\u7ebf\u7535\u7cfb\u7edf\uff0c\u901a\u8fc7\u80fd\u91cf\u68c0\u6d4b\u800c\u975e\u89e3\u7801\u4e3b\u4fe1\u53f7\u6765\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u6765\u6700\u5c0f\u5316\u53d1\u5c04\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u5171\u751f\u65e0\u7ebf\u7535\u7cfb\u7edf\u4e2d\uff0c\u6b21\u63a5\u6536\u5668\u9700\u8981\u89e3\u7801\u4e3b\u4fe1\u53f7\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e3b\u6570\u636e\u66b4\u9732\u7ed9\u975e\u6388\u6743\u63a5\u6536\u5668\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u6d88\u9664\u8fd9\u79cd\u6a21\u7cca\u6027\uff0c\u63d0\u9ad8\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528RIS\u589e\u5f3a\u7684\u89e3\u8026\u5171\u751f\u65e0\u7ebf\u7535\u67b6\u6784\uff0c\u6b21\u63a5\u6536\u5668\u4ec5\u8fdb\u884c\u80fd\u91cf\u68c0\u6d4b\u800c\u4e0d\u89e3\u7801\u4e3b\u4fe1\u53f7\u3002\u901a\u8fc7\u901f\u7387\u5e73\u8861\u91cd\u6784\u548c\u5355\u8c03BER\u6bd4\u8868\u5f81\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u60e9\u7f5a\u7684\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\uff0c\u5177\u6709\u95ed\u5f0f\u66f4\u65b0\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u4e0e\u4f20\u7edf\u5171\u751f\u65e0\u7ebf\u7535\u57fa\u7ebf\u76f8\u6bd4\uff0cRIS\u589e\u5f3a\u7684\u4fe1\u606f\u89e3\u8026\u5171\u751f\u65e0\u7ebf\u7535\u7cfb\u7edf\u663e\u8457\u964d\u4f4e\u4e86\u529f\u8017\u3002", "conclusion": "RIS\u589e\u5f3a\u7684\u89e3\u8026\u5171\u751f\u65e0\u7ebf\u7535\u7cfb\u7edf\u901a\u8fc7\u80fd\u91cf\u68c0\u6d4b\u800c\u975e\u89e3\u7801\u4e3b\u4fe1\u53f7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u5171\u751f\u65e0\u7ebf\u7535\u7684\u5b89\u5168\u9690\u60a3\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0c\u964d\u4f4e\u529f\u8017\u3002"}}
{"id": "2601.11897", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.11897", "abs": "https://arxiv.org/abs/2601.11897", "authors": ["Jinwon Sohn", "Guang Lin", "Qifan Song"], "title": "Task-tailored Pre-processing: Fair Downstream Supervised Learning", "comment": null, "summary": "Fairness-aware machine learning has recently attracted various communities to mitigate discrimination against certain societal groups in data-driven tasks. For fair supervised learning, particularly in pre-processing, there have been two main categories: data fairness and task-tailored fairness. The former directly finds an intermediate distribution among the groups, independent of the type of the downstream model, so a learned downstream classification/regression model returns similar predictive scores to individuals inputting the same covariates irrespective of their sensitive attributes. The latter explicitly takes the supervised learning task into account when constructing the pre-processing map. In this work, we study algorithmic fairness for supervised learning and argue that the data fairness approaches impose overly strong regularization from the perspective of the HGR correlation. This motivates us to devise a novel pre-processing approach tailored to supervised learning. We account for the trade-off between fairness and utility in obtaining the pre-processing map. Then we study the behavior of arbitrary downstream supervised models learned on the transformed data to find sufficient conditions to guarantee their fairness improvement and utility preservation. To our knowledge, no prior work in the branch of task-tailored methods has theoretically investigated downstream guarantees when using pre-processed data. We further evaluate our framework through comparison studies based on tabular and image data sets, showing the superiority of our framework which preserves consistent trade-offs among multiple downstream models compared to recent competing models. Particularly for computer vision data, we see our method alters only necessary semantic features related to the central machine learning task to achieve fairness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u76d1\u7763\u5b66\u4e60\u7684\u516c\u5e73\u6027\u9884\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7HGR\u76f8\u5173\u6027\u5206\u6790\u53d1\u73b0\u73b0\u6709\u6570\u636e\u516c\u5e73\u65b9\u6cd5\u6b63\u5219\u5316\u8fc7\u5f3a\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8003\u8651\u516c\u5e73-\u6548\u7528\u6743\u8861\u7684\u4efb\u52a1\u5b9a\u5236\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u5e76\u7406\u8bba\u5206\u6790\u4e86\u4e0b\u6e38\u6a21\u578b\u7684\u516c\u5e73\u6027\u6539\u8fdb\u548c\u6548\u7528\u4fdd\u6301\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u9884\u5904\u7406\u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b\uff1a\u6570\u636e\u516c\u5e73\u6027\uff08\u72ec\u7acb\u4e8e\u4e0b\u6e38\u6a21\u578b\uff09\u548c\u4efb\u52a1\u5b9a\u5236\u516c\u5e73\u6027\uff08\u8003\u8651\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff09\u3002\u4f5c\u8005\u8ba4\u4e3a\u6570\u636e\u516c\u5e73\u6027\u65b9\u6cd5\u4eceHGR\u76f8\u5173\u6027\u89d2\u5ea6\u770b\u65bd\u52a0\u4e86\u8fc7\u5f3a\u7684\u6b63\u5219\u5316\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u9002\u5408\u76d1\u7763\u5b66\u4e60\u7684\u9884\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u5b9a\u5236\u9884\u5904\u7406\u6846\u67b6\uff1a1\uff09\u8003\u8651\u516c\u5e73\u6027\u548c\u6548\u7528\u4e4b\u95f4\u7684\u6743\u8861\u6765\u83b7\u5f97\u9884\u5904\u7406\u6620\u5c04\uff1b2\uff09\u7406\u8bba\u5206\u6790\u5728\u53d8\u6362\u6570\u636e\u4e0a\u5b66\u4e60\u7684\u4efb\u610f\u4e0b\u6e38\u76d1\u7763\u6a21\u578b\uff0c\u627e\u5230\u4fdd\u8bc1\u5176\u516c\u5e73\u6027\u6539\u8fdb\u548c\u6548\u7528\u4fdd\u6301\u7684\u5145\u5206\u6761\u4ef6\uff1b3\uff09\u5728\u8868\u683c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u4e0b\u6e38\u6a21\u578b\u4e2d\u4fdd\u6301\u4e00\u81f4\u7684\u516c\u5e73-\u6548\u7528\u6743\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u7ade\u4e89\u6a21\u578b\u3002\u7279\u522b\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4ec5\u6539\u53d8\u4e0e\u6838\u5fc3\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u5fc5\u8981\u8bed\u4e49\u7279\u5f81\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4efb\u52a1\u5b9a\u5236\u9884\u5904\u7406\u6846\u67b6\u5728\u7406\u8bba\u4e0a\u4e3a\u4e0b\u6e38\u6a21\u578b\u63d0\u4f9b\u516c\u5e73\u6027\u4fdd\u8bc1\uff0c\u5728\u5b9e\u8df5\u4e2d\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u80fd\u591f\u7cbe\u786e\u5730\u8c03\u6574\u76f8\u5173\u8bed\u4e49\u7279\u5f81\u6765\u5b9e\u73b0\u516c\u5e73\u6027\u3002"}}
{"id": "2601.11639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11639", "abs": "https://arxiv.org/abs/2601.11639", "authors": ["Ming Li"], "title": "Global Optimization By Gradient from Hierarchical Score-Matching Spaces", "comment": null, "summary": "Gradient descent is the most commonly used optimization method, but limited to local optimality, and confined to the field of continuous differentiable problems with simple convex constraints. This work solve these limitations and restrictions by unifying all optimization problems with various complex constraints as a general hierarchical optimization objective without constraints, which is optimized by gradient obtained through score matching. By this way, global optimization by deterministic method using strict gradient is achieved for the first time, and verified through simple-constructed and complex-practical experiments. Even more importantly, it reveals the profound connection between global optimization and diffusion based generative modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6570\u5339\u914d\u83b7\u53d6\u68af\u5ea6\uff0c\u5c06\u5e26\u590d\u6742\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u7edf\u4e00\u4e3a\u65e0\u7ea6\u675f\u5206\u5c42\u4f18\u5316\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e25\u683c\u68af\u5ea6\u7684\u786e\u5b9a\u6027\u5168\u5c40\u4f18\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u5168\u5c40\u4f18\u5316\u4e0e\u6269\u6563\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u53ea\u80fd\u627e\u5230\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u8fde\u7eed\u53ef\u5fae\u95ee\u9898\u548c\u7b80\u5355\u51f8\u7ea6\u675f\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5904\u7406\u5404\u79cd\u590d\u6742\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5c06\u5e26\u590d\u6742\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u7edf\u4e00\u4e3a\u65e0\u7ea6\u675f\u7684\u5206\u5c42\u4f18\u5316\u76ee\u6807\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u6280\u672f\u83b7\u53d6\u68af\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e25\u683c\u68af\u5ea6\u7684\u786e\u5b9a\u6027\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u7b80\u5355\u6784\u9020\u548c\u590d\u6742\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u4f7f\u7528\u4e25\u683c\u68af\u5ea6\u7684\u786e\u5b9a\u6027\u5168\u5c40\u4f18\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u5168\u5c40\u4f18\u5316\u4e0e\u6269\u6563\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u68af\u5ea6\u4e0b\u964d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5904\u7406\u590d\u6742\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5efa\u7acb\u4e86\u5168\u5c40\u4f18\u5316\u4e0e\u6269\u6563\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.12433", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12433", "abs": "https://arxiv.org/abs/2601.12433", "authors": ["Amanda Nyholm", "Yessica Arellano", "Jinyu Liu", "Damian Krakowiak", "Pierluigi Salvo Rossi"], "title": "Temporal Data and Short-Time Averages Improve Multiphase Mass Flow Metering", "comment": "9 pages, 6 figures", "summary": "Reliable flow measurements are essential in many industries, but current instruments often fail to accurately estimate multiphase flows, which are frequently encountered in real-world operations. Combining machine learning (ML) algorithms with accurate single-phase flowmeters has therefore received extensive research attention in recent years. The Coriolis mass flowmeter is a widely used single-phase meter that provides direct mass flow measurements, which ML models can be trained to correct, thereby reducing measurement errors in multiphase conditions. This paper demonstrates that preserving temporal information significantly improves model performance in such scenarios. We compare a multilayer perceptron, a windowed multilayer perceptron, and a convolutional neural network (CNN) on three-phase air-water-oil flow data from 342 experiments. Whereas prior work typically compresses each experiment into a single averaged sample, we instead compute short-time averages from within each experiment and train models that preserve temporal information at several downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95 % of relative errors below 13 %, a normalized root mean squared error of 0.03, and a mean absolute percentage error of approximately 4.3 %, clearly outperforming the best single-averaged model and demonstrating that short-time averaging within individual experiments is preferable. Results are consistent across multiple data splits and random seeds, demonstrating robustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc1\u660e\u5728\u5229\u7528\u673a\u5668\u5b66\u4e60\u6821\u6b63\u79d1\u91cc\u5965\u5229\u8d28\u91cf\u6d41\u91cf\u8ba1\u591a\u76f8\u6d41\u6d4b\u91cf\u8bef\u5dee\u65f6\uff0c\u4fdd\u7559\u65f6\u95f4\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5176\u4e2dCNN\u57280.25Hz\u91c7\u6837\u9891\u7387\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u6d41\u91cf\u6d4b\u91cf\u4eea\u5668\u5728\u591a\u76f8\u6d41\u6761\u4ef6\u4e0b\u6d4b\u91cf\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u79d1\u91cc\u5965\u5229\u8d28\u91cf\u6d41\u91cf\u8ba1\u4f5c\u4e3a\u5e7f\u6cdb\u4f7f\u7528\u7684\u5355\u76f8\u6d41\u91cf\u8ba1\uff0c\u5176\u6d4b\u91cf\u7ed3\u679c\u53ef\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6821\u6b63\u6765\u51cf\u5c11\u591a\u76f8\u6d41\u6761\u4ef6\u4e0b\u7684\u8bef\u5dee\u3002", "method": "\u6bd4\u8f83\u591a\u5c42\u611f\u77e5\u673a\u3001\u7a97\u53e3\u591a\u5c42\u611f\u77e5\u673a\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728342\u4e2a\u4e09\u76f8\u6c14-\u6c34-\u6cb9\u6d41\u5b9e\u9a8c\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002\u4e0d\u540c\u4e8e\u4ee5\u5f80\u5c06\u6bcf\u4e2a\u5b9e\u9a8c\u538b\u7f29\u4e3a\u5355\u4e00\u5e73\u5747\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u672c\u7814\u7a76\u8ba1\u7b97\u5b9e\u9a8c\u5185\u7684\u77ed\u65f6\u5e73\u5747\u503c\uff0c\u5728\u591a\u4e2a\u4e0b\u91c7\u6837\u95f4\u9694\u4e0b\u4fdd\u7559\u65f6\u95f4\u4fe1\u606f\u8bad\u7ec3\u6a21\u578b\u3002", "result": "CNN\u57280.25Hz\u9891\u7387\u4e0b\u8868\u73b0\u6700\u4f73\uff1a\u7ea695%\u7684\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e13%\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.03\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u7ea64.3%\u3002\u660e\u663e\u4f18\u4e8e\u6700\u4f73\u5355\u5e73\u5747\u6a21\u578b\uff0c\u4e14\u7ed3\u679c\u5728\u4e0d\u540c\u6570\u636e\u5206\u5272\u548c\u968f\u673a\u79cd\u5b50\u4e0b\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u5728\u4e2a\u4f53\u5b9e\u9a8c\u5185\u8fdb\u884c\u77ed\u65f6\u5e73\u5747\u4f18\u4e8e\u5355\u4e00\u5b9e\u9a8c\u5e73\u5747\uff0c\u4fdd\u7559\u65f6\u95f4\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u591a\u76f8\u6d41\u6d4b\u91cf\u6821\u6b63\u6a21\u578b\u7684\u6027\u80fd\uff0cCNN\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2601.12178", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12178", "abs": "https://arxiv.org/abs/2601.12178", "authors": ["Fallou Niakh"], "title": "Federated Learning for the Design of Parametric Insurance Indices under Heterogeneous Renewable Production Losses", "comment": null, "summary": "We propose a federated learning framework for the calibration of parametric insurance indices under heterogeneous renewable energy production losses. Producers locally model their losses using Tweedie generalized linear models and private data, while a common index is learned through federated optimization without sharing raw observations. The approach accommodates heterogeneity in variance and link functions and directly minimizes a global deviance objective in a distributed setting. We implement and compare FedAvg, FedProx and FedOpt, and benchmark them against an existing approximation-based aggregation method. An empirical application to solar power production in Germany shows that federated learning recovers comparable index coefficients under moderate heterogeneity, while providing a more general and scalable framework.", "AI": {"tldr": "\u63d0\u51fa\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u53c2\u6570\u5316\u4fdd\u9669\u6307\u6570\u6821\u51c6\uff0c\u5904\u7406\u53ef\u518d\u751f\u80fd\u6e90\u751f\u4ea7\u635f\u5931\u7684\u5f02\u8d28\u6027\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f18\u5316\u5b66\u4e60\u5171\u540c\u6307\u6570\u800c\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u751f\u4ea7\u635f\u5931\u5b58\u5728\u5f02\u8d28\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5171\u4eab\u654f\u611f\u6570\u636e\uff0c\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u8fdb\u884c\u53c2\u6570\u5316\u4fdd\u9669\u6307\u6570\u6821\u51c6\u3002", "method": "\u751f\u4ea7\u8005\u4f7f\u7528Tweedie\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\u672c\u5730\u5efa\u6a21\u635f\u5931\uff0c\u901a\u8fc7\u8054\u90a6\u4f18\u5316\uff08FedAvg\u3001FedProx\u3001FedOpt\uff09\u5b66\u4e60\u5171\u540c\u6307\u6570\uff0c\u5904\u7406\u65b9\u5dee\u548c\u94fe\u63a5\u51fd\u6570\u7684\u5f02\u8d28\u6027\u3002", "result": "\u5728\u5fb7\u56fd\u592a\u9633\u80fd\u53d1\u7535\u7684\u5b9e\u8bc1\u5e94\u7528\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\u5728\u9002\u5ea6\u5f02\u8d28\u6027\u4e0b\u6062\u590d\u53ef\u6bd4\u8f83\u7684\u6307\u6570\u7cfb\u6570\uff0c\u76f8\u6bd4\u73b0\u6709\u8fd1\u4f3c\u805a\u5408\u65b9\u6cd5\u63d0\u4f9b\u66f4\u901a\u7528\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u53c2\u6570\u5316\u4fdd\u9669\u6307\u6570\u6821\u51c6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\uff0c\u53c8\u80fd\u5904\u7406\u751f\u4ea7\u635f\u5931\u7684\u5f02\u8d28\u6027\u3002"}}
{"id": "2601.11657", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11657", "abs": "https://arxiv.org/abs/2601.11657", "authors": ["Jack T. Beerman", "Shobhan Roy", "H. S. Udaykumar", "Stephen S. Baek"], "title": "Size is Not the Solution: Deformable Convolutions for Effective Physics Aware Deep Learning", "comment": null, "summary": "Physics-aware deep learning (PADL) enables rapid prediction of complex physical systems, yet current convolutional neural network (CNN) architectures struggle with highly nonlinear flows. While scaling model size addresses complexity in broader AI, this approach yields diminishing returns for physics modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical methods, we introduce deformable physics-aware recurrent convolutions (D-PARC) to overcome the rigidity of CNNs. Across Burgers' equation, Navier-Stokes, and reactive flows, D-PARC achieves superior fidelity compared to substantially larger architectures. Analysis reveals that kernels display anti-clustering behavior, evolving into a learned \"active filtration\" strategy distinct from traditional h- or p-adaptivity. Effective receptive field analysis confirms that D-PARC autonomously concentrates resources in high-strain regions while coarsening focus elsewhere, mirroring adaptive refinement in computational mechanics. This demonstrates that physically intuitive architectural design can outperform parameter scaling, establishing that strategic learning in lean networks offers a more effective path forward for PADL than indiscriminate network expansion.", "AI": {"tldr": "D-PARC\u67b6\u6784\u901a\u8fc7\u53ef\u53d8\u5f62\u7269\u7406\u611f\u77e5\u5faa\u73af\u5377\u79ef\uff0c\u5728\u4fdd\u6301\u7f51\u7edc\u7cbe\u7b80\u7684\u540c\u65f6\uff0c\u6bd4\u5927\u89c4\u6a21CNN\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u9884\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c55\u793a\u4e86\u7269\u7406\u76f4\u89c9\u67b6\u6784\u8bbe\u8ba1\u4f18\u4e8e\u53c2\u6570\u6269\u5c55\u7684\u7b56\u7565\u3002", "motivation": "\u5f53\u524d\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u9ad8\u5ea6\u975e\u7ebf\u6027\u7269\u7406\u6d41\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5355\u7eaf\u6269\u5927\u6a21\u578b\u89c4\u6a21\u5bf9\u7269\u7406\u5efa\u6a21\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u501f\u9274\u6df7\u5408\u62c9\u683c\u6717\u65e5-\u6b27\u62c9\u6570\u503c\u65b9\u6cd5\u7684\u7269\u7406\u76f4\u89c9\u6765\u6539\u8fdb\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u53ef\u53d8\u5f62\u7269\u7406\u611f\u77e5\u5faa\u73af\u5377\u79ef\uff08D-PARC\uff09\uff0c\u501f\u9274\u6df7\u5408\u62c9\u683c\u6717\u65e5-\u6b27\u62c9\u65b9\u6cd5\uff0c\u514b\u670dCNN\u7684\u521a\u6027\u9650\u5236\uff0c\u4f7f\u5377\u79ef\u6838\u80fd\u591f\u81ea\u9002\u5e94\u53d8\u5f62\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7269\u7406\u7279\u5f81\u3002", "result": "\u5728Burgers\u65b9\u7a0b\u3001Navier-Stokes\u548c\u53cd\u5e94\u6d41\u7b49\u591a\u4e2a\u7269\u7406\u7cfb\u7edf\u4e2d\uff0cD-PARC\u76f8\u6bd4\u89c4\u6a21\u5927\u5f97\u591a\u7684\u67b6\u6784\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5377\u79ef\u6838\u8868\u73b0\u51fa\u53cd\u805a\u7c7b\u884c\u4e3a\uff0c\u5f62\u6210\u72ec\u7279\u7684\"\u4e3b\u52a8\u8fc7\u6ee4\"\u7b56\u7565\u3002", "conclusion": "\u7269\u7406\u76f4\u89c9\u7684\u67b6\u6784\u8bbe\u8ba1\u6bd4\u5355\u7eaf\u6269\u5927\u7f51\u7edc\u89c4\u6a21\u66f4\u6709\u6548\uff0c\u7cbe\u7b80\u7f51\u7edc\u7684\u7b56\u7565\u6027\u5b66\u4e60\u4e3a\u7269\u7406\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u53d1\u5c55\u8def\u5f84\uff0cD-PARC\u80fd\u591f\u81ea\u4e3b\u96c6\u4e2d\u8d44\u6e90\u4e8e\u9ad8\u5e94\u53d8\u533a\u57df\uff0c\u7c7b\u4f3c\u4e8e\u8ba1\u7b97\u529b\u5b66\u4e2d\u7684\u81ea\u9002\u5e94\u7ec6\u5316\u3002"}}
{"id": "2601.12482", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12482", "abs": "https://arxiv.org/abs/2601.12482", "authors": ["Minhua Ding", "Prathapasinghe Dharmawansa", "Italo Atzeni", "Antti T\u00f6lli"], "title": "The Effect of Noise Correlation on MMSE Channel Estimation in One-Bit Quantized Systems", "comment": null, "summary": "This paper analyzes the impact of spatially correlated additive noise on the minimum mean-square error (MMSE) estimation of multiple-input multiple-output (MIMO) channels from one-bit quantized observations. Although additive noise can be correlated in practical scenarios, e.g., due to jamming, clutter, or other external disturbances, the effect of such correlation on the MMSE channel estimator in this setting remains unexplored in prior work. Against this backdrop, we derive a novel analytical expression for the general MIMO MMSE channel estimator, which is inherently nonlinear in one-bit observations, and accommodates arbitrary channel and noise correlation structures. To further characterize the impact of noise correlation, we subsequently specialize the general MMSE expression to certain tractable multi antenna configurations in which both the channel and the noise assume single-parameter constant correlation structures. Our analyses reveal nontrivial, noise-correlation-induced scenarios in which the estimator remains linear despite non-zero channel and noise correlation parameters. Moreover, the results indicate that, at low-to-medium signal-to-noise ratio, noise correlation improves the MMSE performance when channels are uncorrelated, but degrades performance when channels are strongly correlated.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7a7a\u95f4\u76f8\u5173\u52a0\u6027\u566a\u58f0\u5bf9\u57fa\u4e8e1\u6bd4\u7279\u91cf\u5316\u89c2\u6d4b\u7684MIMO\u4fe1\u9053MMSE\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u63a8\u5bfc\u4e86\u9002\u7528\u4e8e\u4efb\u610f\u4fe1\u9053\u548c\u566a\u58f0\u76f8\u5173\u7ed3\u6784\u7684\u901a\u7528MMSE\u4f30\u8ba1\u5668\u8868\u8fbe\u5f0f\uff0c\u5e76\u63ed\u793a\u4e86\u566a\u58f0\u76f8\u5173\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5bf9\u4f30\u8ba1\u6027\u80fd\u7684\u590d\u6742\u5f71\u54cd\u3002", "motivation": "\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u5e72\u6270\u3001\u6742\u6ce2\u6216\u5176\u4ed6\u5916\u90e8\u6270\u52a8\uff0c\u52a0\u6027\u566a\u58f0\u53ef\u80fd\u5177\u6709\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5c1a\u672a\u63a2\u7d22\u8fd9\u79cd\u76f8\u5173\u6027\u5bf9\u57fa\u4e8e1\u6bd4\u7279\u91cf\u5316\u89c2\u6d4b\u7684MIMO\u4fe1\u9053MMSE\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u63a8\u5bfc\u4e86\u9002\u7528\u4e8e\u4efb\u610f\u4fe1\u9053\u548c\u566a\u58f0\u76f8\u5173\u7ed3\u6784\u7684\u901a\u7528MIMO MMSE\u4fe1\u9053\u4f30\u8ba1\u5668\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u8be5\u4f30\u8ba1\u5668\u57281\u6bd4\u7279\u89c2\u6d4b\u4e2d\u672c\u8d28\u4e0a\u662f\u975e\u7ebf\u6027\u7684\u3002\u968f\u540e\u5c06\u901a\u7528\u8868\u8fbe\u5f0f\u7279\u5316\u4e3a\u5177\u6709\u5355\u53c2\u6570\u6052\u5b9a\u76f8\u5173\u7ed3\u6784\u7684\u53ef\u5904\u7406\u591a\u5929\u7ebf\u914d\u7f6e\uff0c\u4ee5\u8fdb\u4e00\u6b65\u5206\u6790\u566a\u58f0\u76f8\u5173\u7684\u5f71\u54cd\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u975e\u5e73\u51e1\u7684\u566a\u58f0\u76f8\u5173\u8bf1\u5bfc\u573a\u666f\uff1a\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u5373\u4f7f\u4fe1\u9053\u548c\u566a\u58f0\u76f8\u5173\u53c2\u6570\u975e\u96f6\uff0c\u4f30\u8ba1\u5668\u4ecd\u4fdd\u6301\u7ebf\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f4e\u5230\u4e2d\u4fe1\u566a\u6bd4\u4e0b\uff0c\u5f53\u4fe1\u9053\u4e0d\u76f8\u5173\u65f6\uff0c\u566a\u58f0\u76f8\u5173\u4f1a\u6539\u5584MMSE\u6027\u80fd\uff1b\u4f46\u5f53\u4fe1\u9053\u5f3a\u76f8\u5173\u65f6\uff0c\u566a\u58f0\u76f8\u5173\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u5206\u6790\u4e86\u7a7a\u95f4\u76f8\u5173\u566a\u58f0\u5bf91\u6bd4\u7279\u91cf\u5316MIMO\u4fe1\u9053MMSE\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u63a8\u5bfc\u4e86\u901a\u7528\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u63ed\u793a\u4e86\u566a\u58f0\u76f8\u5173\u4e0e\u4fe1\u9053\u76f8\u5173\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u5bf9\u4f30\u8ba1\u6027\u80fd\u7684\u5f71\u54cd\u673a\u5236\u3002"}}
{"id": "2601.12213", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12213", "abs": "https://arxiv.org/abs/2601.12213", "authors": ["Hongyang R. Zhang", "Zhenshuo Zhang", "Huy L. Nguyen", "Guanghui Lan"], "title": "One-Sided Matrix Completion from Ultra-Sparse Samples", "comment": "41 pages", "summary": "Matrix completion is a classical problem that has received recurring interest across a wide range of fields. In this paper, we revisit this problem in an ultra-sparse sampling regime, where each entry of an unknown, $n\\times d$ matrix $M$ (with $n \\ge d$) is observed independently with probability $p = C / d$, for a fixed integer $C \\ge 2$. This setting is motivated by applications involving large, sparse panel datasets, where the number of rows far exceeds the number of columns. When each row contains only $C$ entries -- fewer than the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\\top} M / n$.\n  The empirical second-moment matrix computed from observed entries exhibits non-random and sparse missingness. We propose an unbiased estimator that normalizes each nonzero entry of the second moment by its observed frequency, followed by gradient descent to impute the missing entries of $T$. The normalization divides a weighted sum of $n$ binomial random variables by the total number of ones. We show that the estimator is unbiased for any $p$ and enjoys low variance. When the row vectors of $M$ are drawn uniformly from a rank-$r$ factor model satisfying an incoherence condition, we prove that if $n \\ge O({d r^5 \u03b5^{-2} C^{-2} \\log d})$, any local minimum of the gradient-descent objective is approximately global and recovers $T$ with error at most $\u03b5^2$.\n  Experiments on both synthetic and real-world data validate our approach. On three MovieLens datasets, our algorithm reduces bias by $88\\%$ relative to baseline estimators. We also empirically validate the linear sampling complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$ by $59\\%$ and $M$ by $38\\%$ compared to baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u8d85\u7a00\u758f\u91c7\u6837\u4e0b\u4f30\u8ba1\u77e9\u9635\u884c\u7a7a\u95f4\u6216\u4e8c\u9636\u77e9\u77e9\u9635\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u89c2\u6d4b\u9891\u7387\u548c\u68af\u5ea6\u4e0b\u964d\u6765\u6062\u590d\u7f3a\u5931\u7684\u4e8c\u9636\u77e9\u4fe1\u606f\u3002", "motivation": "\u9488\u5bf9\u5927\u578b\u7a00\u758f\u9762\u677f\u6570\u636e\u96c6\uff0c\u5f53\u6bcf\u884c\u89c2\u6d4b\u6761\u76ee\u6570\u5c11\u4e8e\u77e9\u9635\u79e9\u65f6\uff0c\u4f20\u7edf\u77e9\u9635\u8865\u5168\u65b9\u6cd5\u5931\u6548\uff0c\u9700\u8981\u4f30\u8ba1\u77e9\u9635\u7684\u884c\u7a7a\u95f4\u6216\u4e8c\u9636\u77e9\u77e9\u9635\u3002", "method": "\u63d0\u51fa\u65e0\u504f\u4f30\u8ba1\u5668\uff1a\u5bf9\u4e8c\u9636\u77e9\u77e9\u9635\u7684\u975e\u96f6\u6761\u76ee\u6309\u89c2\u6d4b\u9891\u7387\u5f52\u4e00\u5316\uff0c\u7136\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u8865\u5168\u7f3a\u5931\u7684\u4e8c\u9636\u77e9\u77e9\u9635\u6761\u76ee\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5f53n\u2265O(dr\u2075\u03b5\u207b\u00b2C\u207b\u00b2log d)\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u8fd1\u4f3c\u5168\u5c40\u6700\u4f18\uff0c\u80fd\u4ee5\u03b5\u00b2\u8bef\u5dee\u6062\u590dT\u3002\u5b9e\u9a8c\u663e\u793a\u5728MovieLens\u6570\u636e\u96c6\u4e0a\u51cf\u5c1188%\u504f\u5dee\uff0c\u5728Amazon\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u51cf\u5c1159%\u7684T\u6062\u590d\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d85\u7a00\u758f\u91c7\u6837\u4e0b\u6709\u6548\u4f30\u8ba1\u4e8c\u9636\u77e9\u77e9\u9635\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7a00\u758f\u9762\u677f\u6570\u636e\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2601.11661", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2601.11661", "abs": "https://arxiv.org/abs/2601.11661", "authors": ["Mohammad Mohammadzadeh Sanandaji", "Danial Ebrahimzadeh", "Mohammad Ikram Haider", "Yaser Mike Banad", "Aleksandar Poleksic", "Hongtao Ding"], "title": "Machine learning model for predicting surface wettability in laser-textured metal alloys", "comment": "This manuscript has 9 figures and contains 16 pages two column. submitted to journal of laser applications. Under review", "summary": "Surface wettability, governed by both topography and chemistry, plays a critical role in applications such as heat transfer, lubrication, microfluidics, and surface coatings. In this study, we present a machine learning (ML) framework capable of accurately predicting the wettability of laser-textured metal alloys using experimentally derived morphological and chemical features. Superhydrophilic and superhydrophobic surfaces were fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing followed by chemical immersion treatments. Surface morphology was quantified using the Laws texture energy method and profilometry, while surface chemistry was characterized through X-ray photoelectron spectroscopy (XPS), extracting features such as functional group polarity, molecular volume, and peak area fraction. These features were used to train an ensemble neural network model incorporating residual connections, batch normalization, and dropout regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE = 13.896), outperforming previous approaches. Feature importance analysis revealed that surface chemistry had the strongest influence on contact angle prediction, with topographical features also contributing significantly. This work demonstrates the potential of artificial intelligence to model and predict wetting behavior by capturing the complex interplay of surface characteristics, offering a data-driven pathway for designing tailored functional surfaces.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u6846\u67b6\u51c6\u786e\u9884\u6d4b\u6fc0\u5149\u7eb9\u7406\u91d1\u5c5e\u5408\u91d1\u7684\u6da6\u6e7f\u6027\uff0c\u901a\u8fc7\u5f62\u6001\u548c\u5316\u5b66\u7279\u5f81\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff08R\u00b2=0.942\uff09", "motivation": "\u8868\u9762\u6da6\u6e7f\u6027\u5728\u70ed\u4f20\u9012\u3001\u6da6\u6ed1\u3001\u5fae\u6d41\u4f53\u548c\u8868\u9762\u6d82\u5c42\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u5f62\u8c8c\u548c\u5316\u5b66\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u5f71\u54cd\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u8fd9\u79cd\u590d\u6742\u5173\u7cfb\uff0c\u9700\u8981\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u4f18\u5316\u529f\u80fd\u8868\u9762\u3002", "method": "1. \u5728AA6061\u548cAISI 4130\u5408\u91d1\u4e0a\u901a\u8fc7\u7eb3\u79d2\u6fc0\u5149\u7eb9\u7406\u5316\u548c\u5316\u5b66\u6d78\u6e0d\u5904\u7406\u5236\u5907\u8d85\u4eb2\u6c34\u548c\u8d85\u758f\u6c34\u8868\u9762\uff1b2. \u4f7f\u7528Laws\u7eb9\u7406\u80fd\u91cf\u6cd5\u548c\u8f6e\u5ed3\u6d4b\u91cf\u6cd5\u5b9a\u91cf\u8868\u9762\u5f62\u6001\uff1b3. \u901a\u8fc7XPS\u8868\u5f81\u8868\u9762\u5316\u5b66\uff0c\u63d0\u53d6\u5b98\u80fd\u56e2\u6781\u6027\u3001\u5206\u5b50\u4f53\u79ef\u548c\u5cf0\u9762\u79ef\u5206\u6570\u7b49\u7279\u5f81\uff1b4. \u8bad\u7ec3\u5305\u542b\u6b8b\u5dee\u8fde\u63a5\u3001\u6279\u91cf\u5f52\u4e00\u5316\u548cdropout\u6b63\u5219\u5316\u7684\u96c6\u6210\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff08R\u00b2=0.942\uff0cRMSE=13.896\uff09\uff0c\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\u8868\u9762\u5316\u5b66\u5bf9\u63a5\u89e6\u89d2\u9884\u6d4b\u5f71\u54cd\u6700\u5927\uff0c\u5f62\u8c8c\u7279\u5f81\u4e5f\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4eba\u5de5\u667a\u80fd\u901a\u8fc7\u6355\u6349\u8868\u9762\u7279\u6027\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u6765\u5efa\u6a21\u548c\u9884\u6d4b\u6da6\u6e7f\u884c\u4e3a\u7684\u6f5c\u529b\uff0c\u4e3a\u8bbe\u8ba1\u5b9a\u5236\u529f\u80fd\u8868\u9762\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u9014\u5f84\u3002"}}
{"id": "2601.12380", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12380", "abs": "https://arxiv.org/abs/2601.12380", "authors": ["Ou Deng", "Shoji Nishimura", "Atsushi Ogihara", "Qun Jin"], "title": "Statistical-Neural Interaction Networks for Interpretable Mixed-Type Data Imputation", "comment": null, "summary": "Real-world tabular databases routinely combine continuous measurements and categorical records, yet missing entries are pervasive and can distort downstream analysis. We propose Statistical-Neural Interaction (SNI), an interpretable mixed-type imputation framework that couples correlation-derived statistical priors with neural feature attention through a Controllable-Prior Feature Attention (CPFA) module. CPFA learns head-wise prior-strength coefficients $\\{\u03bb_h\\}$ that softly regularize attention toward the prior while allowing data-driven deviations when nonlinear patterns appear to be present in the data. Beyond imputation, SNI aggregates attention maps into a directed feature-dependency matrix that summarizes which variables the imputer relied on, without requiring post-hoc explainers. We evaluate SNI against six baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets spanning ICU monitoring, population surveys, socio-economic statistics, and engineering applications. Under MCAR/strict-MAR at 30\\% missingness, SNI is generally competitive on continuous metrics but is often outperformed by accuracy-first baselines (MissForest, MIWAE) on categorical variables; in return, it provides intrinsic dependency diagnostics and explicit statistical-neural trade-off parameters. We additionally report MNAR stress tests (with a mask-aware variant) and discuss computational cost, limitations -- particularly for severely imbalanced categorical targets -- and deployment scenarios where interpretability may justify the trade-off.", "AI": {"tldr": "SNI\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6df7\u5408\u7c7b\u578b\u6570\u636e\u586b\u8865\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u63a7\u5148\u9a8c\u7279\u5f81\u6ce8\u610f\u529b\u6a21\u5757\u7ed3\u5408\u7edf\u8ba1\u5148\u9a8c\u548c\u795e\u7ecf\u6ce8\u610f\u529b\uff0c\u63d0\u4f9b\u5185\u5728\u7684\u7279\u5f81\u4f9d\u8d56\u8bca\u65ad\u548c\u7edf\u8ba1-\u795e\u7ecf\u6743\u8861\u53c2\u6570\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u901a\u5e38\u5305\u542b\u8fde\u7eed\u6d4b\u91cf\u548c\u5206\u7c7b\u8bb0\u5f55\uff0c\u4f46\u7f3a\u5931\u503c\u666e\u904d\u5b58\u5728\u4e14\u4f1a\u626d\u66f2\u4e0b\u6e38\u5206\u6790\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6df7\u5408\u6570\u636e\u7c7b\u578b\u3002", "method": "\u63d0\u51fa\u7edf\u8ba1-\u795e\u7ecf\u4ea4\u4e92\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u63a7\u5148\u9a8c\u7279\u5f81\u6ce8\u610f\u529b\u6a21\u5757\u5b66\u4e60\u5934\u7ea7\u5148\u9a8c\u5f3a\u5ea6\u7cfb\u6570\uff0c\u8f6f\u6027\u6b63\u5219\u5316\u6ce8\u610f\u529b\u671d\u5411\u7edf\u8ba1\u5148\u9a8c\uff0c\u540c\u65f6\u5141\u8bb8\u6570\u636e\u9a71\u52a8\u7684\u975e\u7ebf\u6027\u6a21\u5f0f\u504f\u5dee\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c30%MCAR/\u4e25\u683cMAR\u7f3a\u5931\u4e0b\uff0cSNI\u5728\u8fde\u7eed\u53d8\u91cf\u6307\u6807\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4f46\u5728\u5206\u7c7b\u53d8\u91cf\u4e0a\u5e38\u88abMissForest\u548cMIWAE\u8d85\u8d8a\uff1b\u63d0\u4f9b\u5185\u5728\u4f9d\u8d56\u8bca\u65ad\u548c\u663e\u5f0f\u6743\u8861\u53c2\u6570\u3002", "conclusion": "SNI\u5728\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\u95f4\u63d0\u4f9b\u6743\u8861\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u4f9d\u8d56\u8bca\u65ad\u548c\u7edf\u8ba1-\u795e\u7ecf\u6743\u8861\u53c2\u6570\u7684\u5e94\u7528\u573a\u666f\uff0c\u4f46\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\u5206\u7c7b\u76ee\u6807\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2601.11663", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11663", "abs": "https://arxiv.org/abs/2601.11663", "authors": ["Bruce Changlong Xu"], "title": "Activation Sensitivity as a Unifying Principle for Post-Training Quantization", "comment": null, "summary": "Post-training quantization (PTQ) methods for large language models rely on heuristics that implicitly estimate which weight channels most strongly influence model behavior. Two dominant paradigms have emerged: activation-aware methods such as AWQ prioritize channels with large activation magnitudes, while second-order methods such as GPTQ allocate quantization error according to input covariance structure. Despite strong empirical performance, these approaches remain conceptually fragmented, and it is unclear what underlying quantity they are approximating. In this work, we present a unified theoretical framework for PTQ by formalizing activation sensitivity, defined as the expected impact of channel-wise perturbations on the loss. Using a first-order Taylor expansion, we show that sensitivity naturally arises as the squared norm of gradient-weighted activations, yielding a principled measure of channel importance that captures both activation magnitude and downstream error propagation. Within this framework, AWQ and GPTQ can be interpreted as complementary approximations that recover sensitivity under distinct simplifying assumptions. We analyze the design space of sensitivity metrics, connect gradient-based saliency, Fisher information, and Hessian-based criteria, and clarify their relationships to classical pruning methods such as Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new quantization algorithm, this work provides a conceptual foundation for understanding and comparing post-training quantization methods through the lens of sensitivity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u6fc0\u6d3b\u611f\u77e5\u91cf\u5316\uff08\u5982AWQ\uff09\u548c\u4e8c\u9636\u91cf\u5316\uff08\u5982GPTQ\uff09\u65b9\u6cd5\u7edf\u4e00\u4e3a\u5bf9\"\u6fc0\u6d3b\u654f\u611f\u6027\"\u7684\u4e0d\u540c\u8fd1\u4f3c\uff0c\u4e3a\u540e\u8bad\u7ec3\u91cf\u5316\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff08AWQ\u548cGPTQ\uff09\u867d\u7136\u8868\u73b0\u826f\u597d\uff0c\u4f46\u6982\u5ff5\u4e0a\u5206\u6563\uff0c\u4e0d\u6e05\u695a\u5b83\u4eec\u8fd1\u4f3c\u7684\u662f\u4ec0\u4e48\u5e95\u5c42\u91cf\u3002\u9700\u8981\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49\"\u6fc0\u6d3b\u654f\u611f\u6027\"\uff08\u901a\u9053\u6270\u52a8\u5bf9\u635f\u5931\u7684\u671f\u671b\u5f71\u54cd\uff09\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u68af\u5ea6\u52a0\u6743\u6fc0\u6d3b\u7684\u5e73\u65b9\u8303\u6570\uff0c\u4ece\u800c\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002", "result": "\u5728\u8be5\u6846\u67b6\u4e0b\uff0cAWQ\u548cGPTQ\u53ef\u89e3\u91ca\u4e3a\u5728\u7279\u5b9a\u7b80\u5316\u5047\u8bbe\u4e0b\u5bf9\u654f\u611f\u6027\u7684\u4e92\u8865\u8fd1\u4f3c\u3002\u5206\u6790\u4e86\u654f\u611f\u6027\u5ea6\u91cf\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u8fde\u63a5\u4e86\u68af\u5ea6\u663e\u8457\u6027\u3001Fisher\u4fe1\u606f\u548cHessian\u51c6\u5219\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3\u548c\u6bd4\u8f83\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u901a\u8fc7\u654f\u611f\u6027\u89c6\u89d2\u7edf\u4e00\u4e86\u4e0d\u540c\u91cf\u5316\u65b9\u6cd5\uff0c\u800c\u975e\u63d0\u51fa\u65b0\u7b97\u6cd5\u3002"}}
{"id": "2601.12629", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12629", "abs": "https://arxiv.org/abs/2601.12629", "authors": ["Mohammad Omid Bagheri", "Justin Chow", "Josh Visser", "Veronica Leong", "George Shaker"], "title": "Millimeter-Wave Multi-Radar Tracking System Enabled by a Modified GRIN Luneburg Lens for Real-Time Healthcare Monitoring", "comment": null, "summary": "Multi-beam radar sensing systems are emerging as powerful tools for non-contact motion tracking and vital-sign monitoring in healthcare environments. This paper presents the design and experimental validation of a synchronized millimeter-wave multi-radar tracking system enhanced by a modified spherical gradient-index (GRIN) Luneburg lens. Five commercial FMCW radar modules operating in the 58--63 GHz band are arranged in a semi-circular configuration around the lens, whose tailored refractive-index profile accommodates bistatic radar modules with co-located transmit (TX) and receive (RX) antennas. The resulting architecture generates multiple fixed high-gain beams with improved angular resolution and minimal mutual interference. Each radar operates independently but is temporally synchronized through a centralized Python-based acquisition framework to enable parallel data collection and low-latency motion tracking. A 10-cm-diameter 3D-printed prototype demonstrates a measured gain enhancement of approximately 12 dB for each module, corresponding to a substantial improvement in detection range. Full-wave simulations and measurements confirm effective non-contact, privacy-preserving short-range human-motion detection across five 28-degree sectors, providing 140-degree total angular coverage. Fall-detection experiments further validate reliable wide-angle performance and continuous spatial tracking. The proposed system offers a compact, low-cost, and scalable platform for millimeter-wave sensing in ambient healthcare and smart-environment applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u7403\u5f62\u68af\u5ea6\u6298\u5c04\u7387Luneburg\u900f\u955c\u7684\u540c\u6b65\u6beb\u7c73\u6ce2\u591a\u96f7\u8fbe\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7528\u4e8e\u975e\u63a5\u89e6\u5f0f\u4eba\u4f53\u8fd0\u52a8\u68c0\u6d4b\u548c\u8dcc\u5012\u76d1\u6d4b\uff0c\u5177\u6709140\u5ea6\u8986\u76d6\u8303\u56f4\u548c12dB\u589e\u76ca\u63d0\u5347\u3002", "motivation": "\u5728\u533b\u7597\u4fdd\u5065\u73af\u5883\u4e2d\uff0c\u975e\u63a5\u89e6\u5f0f\u8fd0\u52a8\u8ddf\u8e2a\u548c\u751f\u547d\u4f53\u5f81\u76d1\u6d4b\u9700\u8981\u9690\u79c1\u4fdd\u62a4\u3001\u5bbd\u89d2\u5ea6\u8986\u76d6\u548c\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002\u4f20\u7edf\u96f7\u8fbe\u7cfb\u7edf\u5b58\u5728\u89d2\u5ea6\u5206\u8fa8\u7387\u6709\u9650\u3001\u76f8\u4e92\u5e72\u6270\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u7403\u5f62\u68af\u5ea6\u6298\u5c04\u7387Luneburg\u900f\u955c\uff0c\u5c065\u4e2a\u5de5\u4f5c\u572858-63GHz\u9891\u6bb5\u7684FMCW\u96f7\u8fbe\u6a21\u5757\u5448\u534a\u5706\u5f62\u6392\u5217\u3002\u900f\u955c\u7684\u5b9a\u5236\u6298\u5c04\u7387\u5206\u5e03\u652f\u6301\u53cc\u9759\u6001\u96f7\u8fbe\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u4e2a\u56fa\u5b9a\u9ad8\u589e\u76ca\u6ce2\u675f\u3002\u901a\u8fc7\u57fa\u4e8ePython\u7684\u96c6\u4e2d\u91c7\u96c6\u6846\u67b6\u5b9e\u73b0\u65f6\u95f4\u540c\u6b65\u548c\u5e76\u884c\u6570\u636e\u91c7\u96c6\u3002", "result": "10\u5398\u7c73\u76f4\u5f843D\u6253\u5370\u539f\u578b\u663e\u793a\u6bcf\u4e2a\u6a21\u5757\u589e\u76ca\u63d0\u5347\u7ea612dB\uff0c\u68c0\u6d4b\u8303\u56f4\u663e\u8457\u6539\u5584\u3002\u5168\u6ce2\u4eff\u771f\u548c\u6d4b\u91cf\u8bc1\u5b9e\u4e86\u5728\u4e94\u4e2a28\u5ea6\u6247\u533a\uff08\u603b\u8ba1140\u5ea6\u8986\u76d6\uff09\u5185\u6709\u6548\u7684\u975e\u63a5\u89e6\u5f0f\u4eba\u4f53\u8fd0\u52a8\u68c0\u6d4b\u3002\u8dcc\u5012\u68c0\u6d4b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53ef\u9760\u7684\u5bbd\u89d2\u5ea6\u6027\u80fd\u548c\u8fde\u7eed\u7a7a\u95f4\u8ddf\u8e2a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6beb\u7c73\u6ce2\u4f20\u611f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u73af\u5883\u533b\u7597\u4fdd\u5065\u548c\u667a\u80fd\u73af\u5883\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u3001\u5bbd\u89d2\u5ea6\u8986\u76d6\u548c\u9ad8\u6027\u80fd\u8fd0\u52a8\u8ddf\u8e2a\u3002"}}
{"id": "2601.12518", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12518", "abs": "https://arxiv.org/abs/2601.12518", "authors": ["Nuoya Xiong", "Aarti Singh"], "title": "Cooperative Multi-agent RL with Communication Constraints", "comment": "33 pages", "summary": "Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\\varepsilon$-Nash equilibrium in potential games with only $O(\\varepsilon^{-3/4})$ communication rounds and $O(poly(\\max_i |A_i|)\\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u7b56\u7565\u9884\u6d4b\u6280\u672f\uff0c\u901a\u8fc7\u65e7\u68af\u5ea6\u9884\u6d4b\u7b56\u7565\u66f4\u65b0\uff0c\u51cf\u5c11\u57fa\u7b56\u7565\u4e0e\u5f53\u524d\u7b56\u7565\u5dee\u8ddd\uff0c\u5728\u6709\u9650\u901a\u4fe1\u4e0b\u5b9e\u73b0\u9ad8\u6548MARL\u5b66\u4e60", "motivation": "\u4f20\u7edf\u5408\u4f5cMARL\u901a\u5e38\u5047\u8bbe\u80fd\u9891\u7e41\u8bbf\u95ee\u5168\u5c40\u4fe1\u606f\uff08\u5982\u56e2\u961f\u5956\u52b1\u3001\u5176\u4ed6\u667a\u80fd\u4f53\u52a8\u4f5c\uff09\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u4e2d\u7531\u4e8e\u9ad8\u901a\u4fe1\u6210\u672c\u4e0d\u73b0\u5b9e\u3002\u5f53\u901a\u4fe1\u53d7\u9650\u65f6\uff0c\u667a\u80fd\u4f53\u5fc5\u987b\u4f9d\u8d56\u8fc7\u65f6\u4fe1\u606f\u4f30\u8ba1\u68af\u5ea6\u548c\u66f4\u65b0\u7b56\u7565\uff0c\u800c\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u5728\u901a\u4fe1\u53d7\u9650\u65f6\uff08\u7f3a\u5931\u6570\u636e\u6982\u7387\u9ad8\uff09\u4f1a\u53d8\u5f97\u4e0d\u7a33\u5b9a", "method": "\u63d0\u51fa\u57fa\u7b56\u7565\u9884\u6d4b\u6280\u672f\uff0c\u5229\u7528\u65e7\u68af\u5ea6\u9884\u6d4b\u7b56\u7565\u66f4\u65b0\uff0c\u4e3a\u4e00\u7cfb\u5217\u57fa\u7b56\u7565\u6536\u96c6\u6837\u672c\uff0c\u51cf\u5c11\u57fa\u7b56\u7565\u4e0e\u5f53\u524d\u7b56\u7565\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u8f6e\u901a\u4fe1\u6536\u96c6\u9884\u6d4b\u57fa\u7b56\u7565\u7684\u6837\u672c\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u8f6e\u6570", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u7b97\u6cd5\u5728\u52bf\u535a\u5f08\u4e2d\u6536\u655b\u5230\u03b5-\u7eb3\u4ec0\u5747\u8861\uff0c\u4ec5\u9700O(\u03b5^{-3/4})\u901a\u4fe1\u8f6e\u6570\u548cO(poly(max_i |A_i|)\u03b5^{-11/4})\u6837\u672c\uff0c\u5728\u901a\u4fe1\u6210\u672c\u548c\u6837\u672c\u590d\u6742\u5ea6\u4e0a\u6539\u8fdb\u73b0\u6709\u6700\u4f18\u7ed3\u679c\uff0c\u907f\u514d\u4e86\u5bf9\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u5927\u5c0f\u7684\u6307\u6570\u4f9d\u8d56\u3002\u6269\u5c55\u5230\u4e00\u822c\u9a6c\u5c14\u53ef\u592b\u5408\u4f5c\u535a\u5f08\u5bfb\u627e\u667a\u80fd\u4f53\u5c40\u90e8\u6700\u4f18", "conclusion": "\u57fa\u7b56\u7565\u9884\u6d4b\u7b97\u6cd5\u5728\u6709\u9650\u901a\u4fe1\u4e0b\u6709\u6548\uff0c\u5728\u6a21\u62df\u6e38\u620f\u548c\u590d\u6742\u73af\u5883\u7684MAPPO\u4e2d\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u9700\u6c42\u540c\u65f6\u4fdd\u6301\u5b66\u4e60\u6548\u679c"}}
{"id": "2601.11667", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11667", "abs": "https://arxiv.org/abs/2601.11667", "authors": ["Xiaojie Xia", "Huigang Zhang", "Chaoliang Zhong", "Jun Sun", "Yusuke Oishi"], "title": "Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model Construction", "comment": null, "summary": "Transformer architectures deliver state-of-the-art accuracy via dense full-attention, but their quadratic time and memory complexity with respect to sequence length limits practical deployment. Linear attention mechanisms offer linear or near-linear scaling yet often incur performance degradation. Hybrid models that integrate full and linear attention layers promise a balance between efficiency and expressiveness, but face two major challenges: training such hybrid models from scratch is computationally expensive, and manually designing the optimal placement of attention types is highly nontrivial. We address both issues by first transferring weights from the pretrained full-attention modules to its linear attention counterparts through blockwise local distillation, and second, introducing a greedy layer replacement strategy that iteratively substitutes full attention blocks with linear ones while monitoring validation performance on the target task. This yields a task-specific hybrid model in a single efficient pass, without costly re-training or neural architecture search, and can be applied to any pretrained full-attention backbone for diverse downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u7ea7\u5c40\u90e8\u84b8\u998f\u548c\u8d2a\u5a6a\u5c42\u66ff\u6362\u7b56\u7565\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u5168\u6ce8\u610f\u529b\u6a21\u578b\u8f6c\u6362\u4e3a\u4efb\u52a1\u7279\u5b9a\u7684\u6df7\u5408\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u5e73\u8861\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "Transformer\u7684\u5168\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\uff1b\u7ebf\u6027\u6ce8\u610f\u529b\u867d\u7136\u6548\u7387\u9ad8\u4f46\u6027\u80fd\u4e0b\u964d\uff1b\u6df7\u5408\u6a21\u578b\u9700\u8981\u6602\u8d35\u8bad\u7ec3\u4e14\u8bbe\u8ba1\u56f0\u96be\u3002", "method": "1) \u901a\u8fc7\u5757\u7ea7\u5c40\u90e8\u84b8\u998f\u5c06\u9884\u8bad\u7ec3\u5168\u6ce8\u610f\u529b\u6a21\u5757\u6743\u91cd\u8f6c\u79fb\u5230\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff1b2) \u91c7\u7528\u8d2a\u5a6a\u5c42\u66ff\u6362\u7b56\u7565\uff0c\u8fed\u4ee3\u66ff\u6362\u5168\u6ce8\u610f\u529b\u5757\u4e3a\u7ebf\u6027\u6ce8\u610f\u529b\u5757\uff0c\u540c\u65f6\u76d1\u63a7\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u5728\u5355\u6b21\u9ad8\u6548\u8fc7\u7a0b\u4e2d\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u6df7\u5408\u6a21\u578b\uff0c\u65e0\u9700\u6602\u8d35\u91cd\u8bad\u7ec3\u6216\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u9884\u8bad\u7ec3\u5168\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\u7edc\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u6ce8\u610f\u529b\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u548c\u8bbe\u8ba1\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u548c\u8868\u8fbe\u80fd\u529b\u7684\u5e73\u8861\u3002"}}
{"id": "2601.12659", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12659", "abs": "https://arxiv.org/abs/2601.12659", "authors": ["Ruiqi Wang", "Essra M. Ghoura", "Omar Alhussein", "Yuzhi Yang", "Yuhang Sheng", "Jing Ren", "Shizhong Xu", "Sami Muhaidat"], "title": "Two-Layer Reinforcement Learning-Assisted Joint Beamforming and Trajectory Optimization for Multi-UAV Downlink Communications", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) are pivotal for future 6G non-terrestrial networks, yet their high mobility creates a complex coupled optimization problem for beamforming and trajectory design. Existing numerical methods suffer from prohibitive latency, while standard deep learning often ignores dynamic interference topology, limiting scalability. To address these issues, this paper proposes a hierarchically decoupled framework synergizing graph neural networks (GNNs) with multi-agent reinforcement learning. Specifically, on the short timescale, we develop a topology-aware GNN beamformer by incorporating GraphNorm. By modeling the dynamic UAV-user association as a time-varying heterogeneous graph, this method explicitly extracts interference patterns to achieve sub-millisecond inference. On the long timescale, trajectory planning is modeled as a decentralized partially observable Markov decision process and solved via the multi-agent proximal policy optimization algorithm under the centralized training with decentralized execution paradigm, facilitating cooperative behaviors. Extensive simulation results demonstrate that the proposed framework significantly outperforms state-of-the-art optimization heuristics and deep learning baselines in terms of system sum rate, convergence speed, and generalization capability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89e3\u8026\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u901a\u4fe1\u7f51\u7edc\u4e2d\u6ce2\u675f\u6210\u5f62\u548c\u8f68\u8ff9\u89c4\u5212\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u57286G\u975e\u5730\u9762\u7f51\u7edc\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9ad8\u79fb\u52a8\u6027\u5bfc\u81f4\u6ce2\u675f\u6210\u5f62\u548c\u8f68\u8ff9\u8bbe\u8ba1\u5f62\u6210\u590d\u6742\u7684\u8026\u5408\u4f18\u5316\u95ee\u9898\u3002\u73b0\u6709\u6570\u503c\u65b9\u6cd5\u5ef6\u8fdf\u8fc7\u9ad8\uff0c\u800c\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u52a8\u6001\u5e72\u6270\u62d3\u6251\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u89e3\u8026\u6846\u67b6\uff1a\u77ed\u65f6\u95f4\u5c3a\u5ea6\u4e0a\uff0c\u4f7f\u7528\u5305\u542bGraphNorm\u7684\u62d3\u6251\u611f\u77e5\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6ce2\u675f\u6210\u5f62\uff0c\u5c06\u52a8\u6001\u65e0\u4eba\u673a-\u7528\u6237\u5173\u8054\u5efa\u6a21\u4e3a\u65f6\u53d8\u5f02\u6784\u56fe\uff1b\u957f\u65f6\u95f4\u5c3a\u5ea6\u4e0a\uff0c\u5c06\u8f68\u8ff9\u89c4\u5212\u5efa\u6a21\u4e3a\u53bb\u4e2d\u5fc3\u5316\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u8303\u5f0f\u4e0b\u5b9e\u73b0\u5408\u4f5c\u884c\u4e3a\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728\u7cfb\u7edf\u603b\u901f\u7387\u3001\u6536\u655b\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4f18\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u5206\u5c42\u89e3\u8026\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u901a\u4fe1\u7f51\u7edc\u4e2d\u6ce2\u675f\u6210\u5f62\u548c\u8f68\u8ff9\u89c4\u5212\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3001\u4f4e\u5ef6\u8fdf\u548c\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.12612", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12612", "abs": "https://arxiv.org/abs/2601.12612", "authors": ["Piyush Sao"], "title": "What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators, Certificates, and Failure Modes", "comment": null, "summary": "Computing $\\log\\det(A)$ for large symmetric positive definite matrices arises in Gaussian process inference and Bayesian model comparison. Standard methods combine matrix-vector products with polynomial approximations. We study a different model: access to trace powers $p_k = \\tr(A^k)$, natural when matrix powers are available.\n  Classical moment-based approximations Taylor-expand $\\log(\u03bb)$ around the arithmetic mean. This requires $|\u03bb- \\AM| < \\AM$ and diverges when $\u03ba> 4$. We work instead with the moment-generating function $M(t) = \\E[X^t]$ for normalized eigenvalues $X = \u03bb/\\AM$. Since $M'(0) = \\E[\\log X]$, the log-determinant becomes $\\log\\det(A) = n(\\log \\AM + M'(0))$ -- the problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$ at positive integers, but interpolating $M(t)$ directly is ill-conditioned due to exponential growth. The transform $K(t) = \\log M(t)$ compresses this range. Normalization by $\\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we interpolate $K$ through $m+1$ consecutive integers and differentiate to estimate $K'(0)$. However, this local interpolation cannot capture arbitrary spectral features.\n  We prove a fundamental limit: no continuous estimator using finitely many positive moments can be uniformly accurate over unbounded conditioning. Positive moments downweight the spectral tail; $K'(0) = \\E[\\log X]$ is tail-sensitive. This motivates guaranteed bounds. From the same traces we derive upper bounds on $(\\det A)^{1/n}$. Given a spectral floor $r \\leq \u03bb_{\\min}$, we obtain moment-constrained lower bounds, yielding a provable interval for $\\log\\det(A)$. A gap diagnostic indicates when to trust the point estimate and when to report bounds. All estimators and bounds cost $O(m)$, independent of $n$. For $m \\in \\{4, \\ldots, 8\\}$, this is effectively constant time.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e9\u9635\u8ff9\u5e42\u8ba1\u7b97\u5bf9\u6570\u884c\u5217\u5f0f\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u751f\u6210\u51fd\u6570\u53d8\u6362\u548c\u63d2\u503c\u6280\u672f\uff0c\u5728\u5e38\u6570\u65f6\u95f4\u5185\u63d0\u4f9b\u5bf9\u6570\u884c\u5217\u5f0f\u7684\u70b9\u4f30\u8ba1\u548c\u53ef\u8bc1\u660e\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u63a8\u65ad\u548c\u8d1d\u53f6\u65af\u6a21\u578b\u6bd4\u8f83\u4e2d\u9700\u8981\u8ba1\u7b97\u5927\u578b\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684\u5bf9\u6570\u884c\u5217\u5f0f\u3002\u4f20\u7edf\u65b9\u6cd5\u7ed3\u5408\u77e9\u9635\u5411\u91cf\u4e58\u79ef\u548c\u591a\u9879\u5f0f\u8fd1\u4f3c\uff0c\u4f46\u672c\u6587\u7814\u7a76\u5f53\u77e9\u9635\u5e42\u53ef\u7528\u65f6\uff0c\u901a\u8fc7\u8ff9\u5e42\u8ba1\u7b97\u5bf9\u6570\u884c\u5217\u5f0f\u7684\u4e0d\u540c\u6a21\u578b\u3002", "method": "1. \u4f7f\u7528\u5f52\u4e00\u5316\u7279\u5f81\u503c\u7684\u77e9\u751f\u6210\u51fd\u6570M(t)\uff0c\u5c06\u5bf9\u6570\u884c\u5217\u5f0f\u8f6c\u5316\u4e3a\u5728t=0\u5904\u7684\u5bfc\u6570\u4f30\u8ba1\u95ee\u9898\n2. \u901a\u8fc7\u53d8\u6362K(t)=log M(t)\u538b\u7f29\u6570\u503c\u8303\u56f4\uff0c\u5229\u7528K(0)=K(1)=0\u7684\u951a\u70b9\u8fdb\u884c\u63d2\u503c\n3. \u4ece\u76f8\u540c\u7684\u8ff9\u4fe1\u606f\u63a8\u5bfc\u51fa\u5bf9\u6570\u884c\u5217\u5f0f\u7684\u4e0a\u4e0b\u754c\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u533a\u95f4\u4f30\u8ba1\n4. \u6240\u6709\u4f30\u8ba1\u5668\u548c\u8fb9\u754c\u8ba1\u7b97\u6210\u672c\u4e3aO(m)\uff0c\u5bf9\u4e8em\u2208{4,...,8}\u57fa\u672c\u4e0a\u662f\u5e38\u6570\u65f6\u95f4", "result": "1. \u8bc1\u660e\u4e86\u4f7f\u7528\u6709\u9650\u6b63\u77e9\u7684\u8fde\u7eed\u4f30\u8ba1\u5668\u5728\u65e0\u754c\u6761\u4ef6\u6570\u4e0b\u65e0\u6cd5\u8fbe\u5230\u4e00\u81f4\u7cbe\u5ea6\u7684\u57fa\u672c\u9650\u5236\n2. \u5f00\u53d1\u4e86\u57fa\u4e8e\u8c31\u4e0b\u754cr\u2264\u03bb_min\u7684\u77e9\u7ea6\u675f\u4e0b\u754c\u65b9\u6cd5\n3. \u63d0\u4f9b\u4e86\u95f4\u9699\u8bca\u65ad\u6307\u6807\uff0c\u7528\u4e8e\u5224\u65ad\u4f55\u65f6\u4fe1\u4efb\u70b9\u4f30\u8ba1\u3001\u4f55\u65f6\u62a5\u544a\u8fb9\u754c\n4. \u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u9ad8\u6548\uff0c\u72ec\u7acb\u4e8e\u77e9\u9635\u7ef4\u5ea6n", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6570\u884c\u5217\u5f0f\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e9\u751f\u6210\u51fd\u6570\u53d8\u6362\u548c\u63d2\u503c\u6280\u672f\uff0c\u5728\u4ec5\u9700\u77e9\u9635\u8ff9\u5e42\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u70b9\u4f30\u8ba1\u548c\u53ef\u8bc1\u660e\u7684\u8fb9\u754c\u3002\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u77e9\u9635\u5e42\u53ef\u7528\u7684\u573a\u666f\uff0c\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u63a8\u65ad\u548c\u8d1d\u53f6\u65af\u6a21\u578b\u6bd4\u8f83\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2601.11669", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.11669", "abs": "https://arxiv.org/abs/2601.11669", "authors": ["Wenwen Liao", "Hang Ruan", "Jianbo Yu", "Xiaofeng Yang", "Qingchao Jiang", "Xuefeng Yan"], "title": "IPEC: Test-Time Incremental Prototype Enhancement Classifier for Few-Shot Learning", "comment": null, "summary": "Metric-based few-shot approaches have gained significant popularity due to their relatively straightforward implementation, high interpret ability, and computational efficiency. However, stemming from the batch-independence assumption during testing, which prevents the model from leveraging valuable knowledge accumulated from previous batches. To address these challenges, we propose a novel test-time method called Incremental Prototype Enhancement Classifier (IPEC), a test-time method that optimizes prototype estimation by leveraging information from previous query samples. IPEC maintains a dynamic auxiliary set by selectively incorporating query samples that are classified with high confidence. To ensure sample quality, we design a robust dual-filtering mechanism that assesses each query sample based on both global prediction confidence and local discriminative ability. By aggregating this auxiliary set with the support set in subsequent tasks, IPEC builds progressively more stable and representative prototypes, effectively reducing its reliance on the initial support set. We ground this approach in a Bayesian interpretation, conceptualizing the support set as a prior and the auxiliary set as a data-driven posterior, which in turn motivates the design of a practical \"warm-up and test\" two-stage inference protocol. Extensive empirical results validate the superior performance of our proposed method across multiple few-shot classification tasks.", "AI": {"tldr": "IPEC\u662f\u4e00\u79cd\u6d4b\u8bd5\u65f6\u589e\u91cf\u539f\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5148\u524d\u67e5\u8be2\u6837\u672c\u7684\u4fe1\u606f\u4f18\u5316\u539f\u578b\u4f30\u8ba1\uff0c\u63d0\u5347\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5ea6\u91cf\u7684\u5c0f\u6837\u672c\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u5047\u8bbe\u6279\u6b21\u72ec\u7acb\uff0c\u65e0\u6cd5\u5229\u7528\u5148\u524d\u6279\u6b21\u79ef\u7d2f\u7684\u5b9d\u8d35\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u589e\u91cf\u539f\u578b\u589e\u5f3a\u5206\u7c7b\u5668(IPEC)\uff0c\u7ef4\u62a4\u52a8\u6001\u8f85\u52a9\u96c6\uff0c\u901a\u8fc7\u53cc\u91cd\u8fc7\u6ee4\u673a\u5236\u9009\u62e9\u9ad8\u7f6e\u4fe1\u5ea6\u67e5\u8be2\u6837\u672c\uff0c\u4e0e\u652f\u6301\u96c6\u805a\u5408\u6784\u5efa\u66f4\u7a33\u5b9a\u7684\u539f\u578b\uff0c\u91c7\u7528\u8d1d\u53f6\u65af\u89e3\u91ca\u548c\"\u9884\u70ed-\u6d4b\u8bd5\"\u4e24\u9636\u6bb5\u63a8\u7406\u534f\u8bae\u3002", "result": "\u5728\u591a\u4e2a\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86IPEC\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "IPEC\u901a\u8fc7\u6d4b\u8bd5\u65f6\u589e\u91cf\u5b66\u4e60\u6709\u6548\u51cf\u5c11\u5bf9\u521d\u59cb\u652f\u6301\u96c6\u7684\u4f9d\u8d56\uff0c\u6784\u5efa\u66f4\u7a33\u5b9a\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u539f\u578b\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2601.12663", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12663", "abs": "https://arxiv.org/abs/2601.12663", "authors": ["Yan-Chen Chen", "Wei-Yu Chiu", "Qun-Yu Wang", "Jing-Wei Chen", "Hao-Ting Zhao"], "title": "Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy and Data Efficiency With Ensemble Deep Transfer Learning", "comment": "26 pages, 11 figures", "summary": "Traditional textile factories consume substantial energy, making energy-efficient production optimization crucial for sustainability and cost reduction. Meanwhile, deep neural networks (DNNs), which are effective for factory output prediction and operational optimization, require extensive historical data, posing challenges due to high sensor deployment and data collection costs. To address this, we propose Ensemble Deep Transfer Learning (EDTL), a novel framework that enhances prediction accuracy and data efficiency by integrating transfer learning with an ensemble strategy and a feature alignment layer. EDTL pretrains DNN models on data-rich production lines (source domain) and adapts them to data-limited lines (target domain), reducing dependency on large datasets. Experiments on real-world textile factory datasets show that EDTL improves prediction accuracy by 5.66% and enhances model robustness by 3.96% compared to conventional DNNs, particularly in data-limited scenarios (20%-40% data availability). This research contributes to energy-efficient textile manufacturing by enabling accurate predictions with fewer data requirements, providing a scalable and cost-effective solution for smart production systems.", "AI": {"tldr": "\u63d0\u51faEDTL\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u7eba\u7ec7\u5de5\u5382\u80fd\u8017\u9884\u6d4b\u7cbe\u5ea6\u548c\u6570\u636e\u6548\u7387\uff0c\u5728\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u7eba\u7ec7\u5de5\u5382\u80fd\u8017\u5927\uff0c\u9700\u8981\u8282\u80fd\u4f18\u5316\uff1b\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5927\u91cf\u5386\u53f2\u6570\u636e\uff0c\u4f46\u4f20\u611f\u5668\u90e8\u7f72\u548c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\uff0c\u6570\u636e\u83b7\u53d6\u56f0\u96be", "method": "\u63d0\u51fa\u96c6\u6210\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60(EDTL)\u6846\u67b6\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u96c6\u6210\u7b56\u7565\uff0c\u5305\u542b\u7279\u5f81\u5bf9\u9f50\u5c42\uff1b\u5148\u5728\u6570\u636e\u4e30\u5bcc\u7684\u4ea7\u7ebf\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u518d\u8fc1\u79fb\u5230\u6570\u636e\u6709\u9650\u7684\u4ea7\u7ebf", "result": "\u5728\u771f\u5b9e\u7eba\u7ec7\u5de5\u5382\u6570\u636e\u96c6\u4e0a\uff0cEDTL\u6bd4\u4f20\u7edfDNN\u9884\u6d4b\u7cbe\u5ea6\u63d0\u53475.66%\uff0c\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u53473.96%\uff0c\u572820%-40%\u6570\u636e\u53ef\u7528\u6027\u573a\u666f\u4e0b\u8868\u73b0\u5c24\u5176\u7a81\u51fa", "conclusion": "EDTL\u4e3a\u8282\u80fd\u7eba\u7ec7\u5236\u9020\u63d0\u4f9b\u4e86\u51c6\u786e\u9884\u6d4b\u4e14\u6570\u636e\u9700\u6c42\u5c11\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u667a\u80fd\u751f\u4ea7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u6709\u6548\u7684\u65b9\u6848"}}
{"id": "2601.12707", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12707", "abs": "https://arxiv.org/abs/2601.12707", "authors": ["Junyi Liao", "Zihan Zhu", "Ethan Fang", "Zhuoran Yang", "Vahid Tarokh"], "title": "Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy Regularization", "comment": "Extended journal version of ICML 2025 paper. Submitted to Operations Research", "summary": "Estimating the unknown reward functions driving agents' behaviors is of central interest in inverse reinforcement learning and game theory. To tackle this problem, we develop a unified framework for reward function recovery in two-player zero-sum matrix games and Markov games with entropy regularization, where we aim to reconstruct the underlying reward functions given observed players' strategies and actions. This task is challenging due to the inherent ambiguity of inverse problems, the non-uniqueness of feasible rewards, and limited observational data coverage. To address these challenges, we establish the reward function's identifiability using the quantal response equilibrium (QRE) under linear assumptions. Building upon this theoretical foundation, we propose a novel algorithm to learn reward functions from observed actions. Our algorithm works in both static and dynamic settings and is adaptable to incorporate different methods, such as Maximum Likelihood Estimation (MLE). We provide strong theoretical guarantees for the reliability and sample efficiency of our algorithm. Further, we conduct extensive numerical studies to demonstrate the practical effectiveness of the proposed framework, offering new insights into decision-making in competitive environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6b63\u5219\u5316\u71b5\u7684\u96f6\u548c\u77e9\u9635\u535a\u5f08\u548c\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u6062\u590d\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7QRE\u5efa\u7acb\u53ef\u8bc6\u522b\u6027\uff0c\u5f00\u53d1\u65b0\u7b97\u6cd5\u4ece\u89c2\u5bdf\u5230\u7684\u884c\u52a8\u4e2d\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u5728\u9006\u5f3a\u5316\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u4e2d\uff0c\u4f30\u8ba1\u9a71\u52a8\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u672a\u77e5\u5956\u52b1\u51fd\u6570\u662f\u6838\u5fc3\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u9006\u95ee\u9898\u7684\u56fa\u6709\u6a21\u7cca\u6027\u3001\u53ef\u884c\u5956\u52b1\u7684\u975e\u552f\u4e00\u6027\u4ee5\u53ca\u89c2\u6d4b\u6570\u636e\u8986\u76d6\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "\u5efa\u7acb\u57fa\u4e8e\u91cf\u5316\u54cd\u5e94\u5747\u8861(QRE)\u7684\u5956\u52b1\u51fd\u6570\u53ef\u8bc6\u522b\u6027\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u8bbe\u7f6e\u7684\u65b0\u7b97\u6cd5\uff0c\u53ef\u7ed3\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7b49\u65b9\u6cd5\uff0c\u4ece\u89c2\u5bdf\u5230\u7684\u73a9\u5bb6\u7b56\u7565\u548c\u884c\u52a8\u4e2d\u6062\u590d\u5956\u52b1\u51fd\u6570\u3002", "result": "\u63d0\u4f9b\u4e86\u7b97\u6cd5\u7684\u53ef\u9760\u6027\u548c\u6837\u672c\u6548\u7387\u7684\u5f3a\u7406\u8bba\u4fdd\u8bc1\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u6570\u503c\u7814\u7a76\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u5b9e\u9645\u6709\u6548\u6027\uff0c\u4e3a\u7ade\u4e89\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9006\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u51fd\u6570\u6062\u590d\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u548c\u7b97\u6cd5\u6846\u67b6\uff0c\u5728\u96f6\u548c\u535a\u5f08\u8bbe\u7f6e\u4e0b\u89e3\u51b3\u4e86\u53ef\u8bc6\u522b\u6027\u548c\u5b66\u4e60\u6548\u7387\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2601.11670", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11670", "abs": "https://arxiv.org/abs/2601.11670", "authors": ["Jinshi Liu", "Pan Liu"], "title": "A Confidence-Variance Theory for Pseudo-Label Selection in Semi-Supervised Learning", "comment": null, "summary": "Most pseudo-label selection strategies in semi-supervised learning rely on fixed confidence thresholds, implicitly assuming that prediction confidence reliably indicates correctness. In practice, deep networks are often overconfident: high-confidence predictions can still be wrong, while informative low-confidence samples near decision boundaries are discarded. This paper introduces a Confidence-Variance (CoVar) theory framework that provides a principled joint reliability criterion for pseudo-label selection. Starting from the entropy minimization principle, we derive a reliability measure that combines maximum confidence (MC) with residual-class variance (RCV), which characterizes how probability mass is distributed over non-maximum classes. The derivation shows that reliable pseudo-labels should have both high MC and low RCV, and that the influence of RCV increases as confidence grows, thereby correcting overconfident but unstable predictions. From this perspective, we cast pseudo-label selection as a spectral relaxation problem that maximizes separability in a confidence-variance feature space, and design a threshold-free selection mechanism to distinguish high- from low-reliability predictions. We integrate CoVar as a plug-in module into representative semi-supervised semantic segmentation and image classification methods. Across PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label ratios and backbones, it consistently improves over strong baselines, indicating that combining confidence with residual-class variance provides a more reliable basis for pseudo-label selection than fixed confidence thresholds. (Code: https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)", "AI": {"tldr": "\u63d0\u51faCoVar\u7406\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u6700\u5927\u7f6e\u4fe1\u5ea6\u548c\u6b8b\u5dee\u7c7b\u522b\u65b9\u5dee\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u9009\u62e9\u51c6\u5219\uff0c\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u9608\u503c\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4f2a\u6807\u7b7e\u9009\u62e9\u4f9d\u8d56\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u9608\u503c\uff0c\u4f46\u6df1\u5ea6\u7f51\u7edc\u5e38\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff1a\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u53ef\u80fd\u9519\u8bef\uff0c\u800c\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u4fe1\u606f\u4e30\u5bcc\u7684\u4f4e\u7f6e\u4fe1\u5ea6\u6837\u672c\u5374\u88ab\u4e22\u5f03", "method": "\u4ece\u71b5\u6700\u5c0f\u5316\u539f\u7406\u51fa\u53d1\uff0c\u63a8\u5bfc\u51fa\u7ed3\u5408\u6700\u5927\u7f6e\u4fe1\u5ea6(MC)\u548c\u6b8b\u5dee\u7c7b\u522b\u65b9\u5dee(RCV)\u7684\u53ef\u9760\u6027\u5ea6\u91cf\uff0c\u5c06\u4f2a\u6807\u7b7e\u9009\u62e9\u8f6c\u5316\u4e3a\u7f6e\u4fe1\u5ea6-\u65b9\u5dee\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u8c31\u677e\u5f1b\u95ee\u9898\uff0c\u8bbe\u8ba1\u65e0\u9608\u503c\u9009\u62e9\u673a\u5236", "result": "\u5728PASCAL VOC 2012\u3001Cityscapes\u3001CIFAR-10\u548cMini-ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e0d\u540c\u6807\u7b7e\u6bd4\u4f8b\u548c\u9aa8\u5e72\u7f51\u7edc\uff0cCoVar\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u6301\u7eed\u6539\u8fdb\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u548c\u6b8b\u5dee\u7c7b\u522b\u65b9\u5dee\u4e3a\u4f2a\u6807\u7b7e\u9009\u62e9\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u9608\u503c\u66f4\u53ef\u9760\u7684\u57fa\u7840\uff0cCoVar\u6846\u67b6\u80fd\u6709\u6548\u7ea0\u6b63\u8fc7\u5ea6\u81ea\u4fe1\u4f46\u4e0d\u7a33\u5b9a\u7684\u9884\u6d4b"}}
{"id": "2601.12708", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12708", "abs": "https://arxiv.org/abs/2601.12708", "authors": ["Yuxi Zhao", "Vicente Casares-Giner", "Vicent Pla", "Luis Guijarro", "Iztok Humar", "Yi Zhong", "Xiaohu Ge"], "title": "Energy-Based Cell Association in Nonuniform Renewable Energy-Powered Cellular Networks: Analysis and Optimization of Carbon Efficiency", "comment": null, "summary": "The increasing global push for carbon reduction highlights the importance of integrating renewable energy into the supply chain of cellular networks. However, due to the stochastic nature of renewable energy generation and the uneven load distribution across base stations, the utilization rate of renewable energy remains low. To address these challenges, this paper investigates the trade-off between carbon emissions and downlink throughput in cellular networks, offering insights into optimizing both network performance and sustainability. The renewable energy state of base station batteries and the number of occupied channels are modeled as a quasi-birth-death process. We construct models for the probability of channel blocking, average successful transmission probability for users, downlink throughput, carbon emissions, and carbon efficiency based on stochastic geometry. Based on these analyses, an energy-based cell association scheme is proposed to optimize the carbon efficiency of cellular networks. The results show that, compared to the closest cell association scheme, the energy-based cell association scheme is capable of reducing the carbon emissions of the network by 13.0% and improving the carbon efficiency by 11.3%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u8702\u7a9d\u5173\u8054\u65b9\u6848\uff0c\u901a\u8fc7\u5efa\u6a21\u53ef\u518d\u751f\u80fd\u6e90\u72b6\u6001\u548c\u4fe1\u9053\u5360\u7528\u60c5\u51b5\uff0c\u4f18\u5316\u8702\u7a9d\u7f51\u7edc\u7684\u78b3\u6548\u7387\uff0c\u76f8\u6bd4\u6700\u8fd1\u5173\u8054\u65b9\u6848\u53ef\u51cf\u5c1113.0%\u78b3\u6392\u653e\u5e76\u63d0\u9ad811.3%\u78b3\u6548\u7387\u3002", "motivation": "\u5168\u7403\u78b3\u51cf\u6392\u538b\u529b\u4e0b\uff0c\u9700\u8981\u5c06\u53ef\u518d\u751f\u80fd\u6e90\u6574\u5408\u5230\u8702\u7a9d\u7f51\u7edc\u4f9b\u5e94\u94fe\u4e2d\u3002\u4f46\u7531\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u7684\u968f\u673a\u6027\u548c\u57fa\u7ad9\u95f4\u8d1f\u8f7d\u5206\u5e03\u4e0d\u5747\uff0c\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\u4ecd\u7136\u8f83\u4f4e\uff0c\u9700\u8981\u5728\u78b3\u6392\u653e\u548c\u4e0b\u884c\u541e\u5410\u91cf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u4f18\u5316\u3002", "method": "\u5c06\u57fa\u7ad9\u7535\u6c60\u7684\u53ef\u518d\u751f\u80fd\u6e90\u72b6\u6001\u548c\u5360\u7528\u4fe1\u9053\u6570\u5efa\u6a21\u4e3a\u51c6\u751f\u706d\u8fc7\u7a0b\uff0c\u57fa\u4e8e\u968f\u673a\u51e0\u4f55\u6784\u5efa\u4fe1\u9053\u963b\u585e\u6982\u7387\u3001\u7528\u6237\u5e73\u5747\u6210\u529f\u4f20\u8f93\u6982\u7387\u3001\u4e0b\u884c\u541e\u5410\u91cf\u3001\u78b3\u6392\u653e\u548c\u78b3\u6548\u7387\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u7684\u8702\u7a9d\u5173\u8054\u65b9\u6848\u6765\u4f18\u5316\u7f51\u7edc\u78b3\u6548\u7387\u3002", "result": "\u76f8\u6bd4\u6700\u8fd1\u8702\u7a9d\u5173\u8054\u65b9\u6848\uff0c\u57fa\u4e8e\u80fd\u91cf\u7684\u8702\u7a9d\u5173\u8054\u65b9\u6848\u80fd\u591f\u5c06\u7f51\u7edc\u78b3\u6392\u653e\u51cf\u5c1113.0%\uff0c\u5e76\u5c06\u78b3\u6548\u7387\u63d0\u9ad811.3%\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u80fd\u91cf\u7684\u8702\u7a9d\u5173\u8054\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u518d\u751f\u80fd\u6e90\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u7f51\u7edc\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8702\u7a9d\u7f51\u7edc\u7684\u78b3\u6548\u7387\uff0c\u4e3a\u5b9e\u73b0\u53ef\u6301\u7eed\u8702\u7a9d\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12931", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.12931", "abs": "https://arxiv.org/abs/2601.12931", "authors": ["Edoardo Urettini", "Daniele Atzeni", "Ioanna-Yvonni Tsaknaki", "Antonio Carta"], "title": "Online Continual Learning for Time Series: a Natural Score-driven Approach", "comment": null, "summary": "Online continual learning (OCL) methods adapt to changing environments without forgetting past knowledge. Similarly, online time series forecasting (OTSF) is a real-world problem where data evolve in time and success depends on both rapid adaptation and long-term memory. Indeed, time-varying and regime-switching forecasting models have been extensively studied, offering a strong justification for the use of OCL in these settings. Building on recent work that applies OCL to OTSF, this paper aims to strengthen the theoretical and practical connections between time series methods and OCL. First, we reframe neural network optimization as a parameter filtering problem, showing that natural gradient descent is a score-driven method and proving its information-theoretic optimality. Then, we show that using a Student's t likelihood in addition to natural gradient induces a bounded update, which improves robustness to outliers. Finally, we introduce Natural Score-driven Replay (NatSR), which combines our robust optimizer with a replay buffer and a dynamic scale heuristic that improves fast adaptation at regime drifts. Empirical results demonstrate that NatSR achieves stronger forecasting performance than more complex state-of-the-art methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faNatSR\u65b9\u6cd5\uff0c\u5c06\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u4e0et\u5206\u5e03\u4f3c\u7136\u7ed3\u5408\u5b9e\u73b0\u9c81\u68d2\u4f18\u5316\uff0c\u7ed3\u5408\u56de\u653e\u7f13\u51b2\u548c\u52a8\u6001\u5c3a\u5ea6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u590d\u6742\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9700\u8981\u5feb\u901f\u9002\u5e94\u73af\u5883\u53d8\u5316\u540c\u65f6\u4fdd\u6301\u957f\u671f\u8bb0\u5fc6\uff0c\u8fd9\u4e0e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u7684\u76ee\u6807\u9ad8\u5ea6\u4e00\u81f4\u3002\u73b0\u6709\u7814\u7a76\u5df2\u521d\u6b65\u5c06OCL\u5e94\u7528\u4e8eOTSF\uff0c\u4f46\u7406\u8bba\u548c\u5b9e\u8df5\u8fde\u63a5\u4ecd\u9700\u52a0\u5f3a\u3002", "method": "1. \u5c06\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u91cd\u6784\u4e3a\u53c2\u6570\u6ee4\u6ce2\u95ee\u9898\uff0c\u8bc1\u660e\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u662f\u4e00\u79cd\u5206\u6570\u9a71\u52a8\u65b9\u6cd5\u5e76\u8bc1\u660e\u5176\u4fe1\u606f\u7406\u8bba\u6700\u4f18\u6027\uff1b2. \u4f7f\u7528Student's t\u5206\u5e03\u4f3c\u7136\u7ed3\u5408\u81ea\u7136\u68af\u5ea6\u5b9e\u73b0\u6709\u754c\u66f4\u65b0\uff0c\u63d0\u9ad8\u5bf9\u5f02\u5e38\u503c\u7684\u9c81\u68d2\u6027\uff1b3. \u63d0\u51faNatSR\u65b9\u6cd5\uff0c\u7ed3\u5408\u9c81\u68d2\u4f18\u5316\u5668\u3001\u56de\u653e\u7f13\u51b2\u548c\u52a8\u6001\u5c3a\u5ea6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u5236\u5ea6\u6f02\u79fb\u65f6\u5feb\u901f\u9002\u5e94\u3002", "result": "NatSR\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u66f4\u590d\u6742\u7684\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u5316\u4e86\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u4e0e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u4e4b\u95f4\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u8fde\u63a5\uff0c\u63d0\u51fa\u7684NatSR\u65b9\u6cd5\u5728\u5728\u7ebf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11686", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11686", "abs": "https://arxiv.org/abs/2601.11686", "authors": ["Nicolas Caron", "Christophe Guyeux", "Hassan Noura", "Benjamin Aynes"], "title": "Proof of Concept: Multi-Target Wildfire Risk Prediction and Large Language Model Synthesis", "comment": null, "summary": "Current state-of-the-art approaches to wildfire risk assessment often overlook operational needs, limiting their practical value for first responders and firefighting services. Effective wildfire management requires a multi-target analysis that captures the diverse dimensions of wildfire risk, including meteorological danger, ignition activity, intervention complexity, and resource mobilization, rather than relying on a single predictive indicator. In this proof of concept, we propose the development of a hybrid framework that combines predictive models for each risk dimension with large language models (LLMs) to synthesize heterogeneous outputs into structured, actionable reports.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u9884\u6d4b\u6a21\u578b\u4e0eLLM\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4e3a\u91ce\u706b\u98ce\u9669\u7ba1\u7406\u751f\u6210\u7ed3\u6784\u5316\u53ef\u64cd\u4f5c\u62a5\u544a", "motivation": "\u73b0\u6709\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u5b9e\u9645\u8fd0\u8425\u9700\u6c42\uff0c\u7f3a\u4e4f\u5bf9\u591a\u7ef4\u5ea6\u98ce\u9669\u7684\u7efc\u5408\u5206\u6790\uff0c\u9650\u5236\u4e86\u5e94\u6025\u54cd\u5e94\u548c\u6d88\u9632\u670d\u52a1\u7684\u5b9e\u7528\u4ef7\u503c", "method": "\u5f00\u53d1\u6df7\u5408\u6846\u67b6\uff1a\u7ed3\u5408\u591a\u4e2a\u9884\u6d4b\u6a21\u578b\u5206\u522b\u8bc4\u4f30\u6c14\u8c61\u5371\u9669\u3001\u70b9\u706b\u6d3b\u52a8\u3001\u5e72\u9884\u590d\u6742\u6027\u548c\u8d44\u6e90\u8c03\u52a8\u7b49\u98ce\u9669\u7ef4\u5ea6\uff0c\u518d\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u5c06\u5f02\u8d28\u8f93\u51fa\u5408\u6210\u4e3a\u7ed3\u6784\u5316\u53ef\u64cd\u4f5c\u62a5\u544a", "result": "\u8bba\u6587\u63d0\u51fa\u7684\u662f\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u65b9\u6848\uff0c\u5c1a\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u5c55\u793a\u4e86\u5c06\u9884\u6d4b\u6a21\u578b\u4e0eLLM\u7ed3\u5408\u89e3\u51b3\u91ce\u706b\u98ce\u9669\u7ba1\u7406\u95ee\u9898\u7684\u521b\u65b0\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u6574\u5408\u591a\u7ef4\u5ea6\u9884\u6d4b\u6a21\u578b\u548cLLM\u7684\u5408\u6210\u80fd\u529b\uff0c\u53ef\u4ee5\u521b\u5efa\u66f4\u5b9e\u7528\u3001\u66f4\u5168\u9762\u7684\u91ce\u706b\u98ce\u9669\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4e3a\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u7ed3\u6784\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u4fe1\u606f\u652f\u6301"}}
{"id": "2601.12725", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12725", "abs": "https://arxiv.org/abs/2601.12725", "authors": ["Chaedam Son", "Si-Hyeon Lee"], "title": "Robust Beamforming and Time Allocation for Time-Division Cell-Free Near-Field ISAC", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we propose a time-division near-field integrated sensing and communication (ISAC) framework for cell-free multiple-input multiple-output (MIMO), where sensing and downlink communication are separated in time. During the sensing phase, user locations are estimated and used to construct location-aware channels, which are then exploited in the subsequent communication phase. By explicitly modeling the coupling between sensing-induced localization errors and channel-estimation errors, we capture the tradeoff between sensing accuracy and communication throughput. Based on this model, we jointly optimize the time-allocation ratio, sensing covariance matrix, and robust downlink beamforming under imperfect channel state information (CSI). The resulting non-convex problem is addressed via a semidefinite programming (SDP)-based reformulation within an alternating-optimization framework. To further reduce computational complexity, we also propose two low-complexity suboptimal designs: an error-ignorant scheme and a maximum ratio transmission (MRT)-based scheme. Simulation results show that the proposed scheme significantly improves localization accuracy over far-field and monostatic setups, thereby reducing channel estimation errors and ultimately enhancing the achievable rate. Moreover, the error-ignorant scheme performs well under stringent sensing requirements, whereas the MRT-based scheme remains robust over a wide range of sensing requirements by adapting the time-allocation ratio, albeit with some beamforming loss.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u8702\u7a9dMIMO\u7684\u65f6\u5206\u8fd1\u573a\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u9636\u6bb5\u4f30\u8ba1\u7528\u6237\u4f4d\u7f6e\u6765\u6784\u5efa\u4f4d\u7f6e\u611f\u77e5\u4fe1\u9053\uff0c\u4f18\u5316\u65f6\u95f4\u5206\u914d\u6bd4\u3001\u611f\u77e5\u534f\u65b9\u5dee\u77e9\u9635\u548c\u9c81\u68d2\u4e0b\u884c\u6ce2\u675f\u6210\u5f62\uff0c\u4ee5\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u901a\u4fe1\u901f\u7387\u3002", "motivation": "\u5728\u65e0\u8702\u7a9dMIMO\u7cfb\u7edf\u4e2d\uff0c\u9700\u8981\u89e3\u51b3\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u611f\u77e5\u5f15\u8d77\u7684\u5b9a\u4f4d\u8bef\u5dee\u4e0e\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u4e4b\u95f4\u7684\u8026\u5408\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u65f6\u5206\u8fd1\u573aISAC\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u548c\u4e0b\u884c\u901a\u4fe1\u5728\u65f6\u95f4\u4e0a\u5206\u79bb\u3002\u611f\u77e5\u9636\u6bb5\u4f30\u8ba1\u7528\u6237\u4f4d\u7f6e\u5e76\u6784\u5efa\u4f4d\u7f6e\u611f\u77e5\u4fe1\u9053\uff0c\u901a\u4fe1\u9636\u6bb5\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u65f6\u95f4\u5206\u914d\u6bd4\u3001\u611f\u77e5\u534f\u65b9\u5dee\u77e9\u9635\u548c\u9c81\u68d2\u4e0b\u884c\u6ce2\u675f\u6210\u5f62\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eSDP\u7684\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\u3002\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cd\u4f4e\u590d\u6742\u5ea6\u6b21\u4f18\u65b9\u6848\uff1a\u8bef\u5dee\u5ffd\u7565\u65b9\u6848\u548c\u57fa\u4e8eMRT\u7684\u65b9\u6848\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u76f8\u6bd4\u8fdc\u573a\u548c\u5355\u7ad9\u8bbe\u7f6e\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u5e76\u6700\u7ec8\u63d0\u9ad8\u4e86\u53ef\u8fbe\u901f\u7387\u3002\u8bef\u5dee\u5ffd\u7565\u65b9\u6848\u5728\u4e25\u683c\u611f\u77e5\u8981\u6c42\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u800c\u57fa\u4e8eMRT\u7684\u65b9\u6848\u901a\u8fc7\u8c03\u6574\u65f6\u95f4\u5206\u914d\u6bd4\uff0c\u5728\u5404\u79cd\u611f\u77e5\u8981\u6c42\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff08\u5c3d\u7ba1\u6709\u6ce2\u675f\u6210\u5f62\u635f\u5931\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u8702\u7a9dMIMO\u7cfb\u7edf\u4e2d\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13272", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13272", "abs": "https://arxiv.org/abs/2601.13272", "authors": ["Aaron Pim", "Tristan Pryer"], "title": "Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification", "comment": "26 pages, 11 figures", "summary": "We develop a multilevel Monte Carlo (MLMC) framework for uncertainty quantification with Monte Carlo dropout. Treating dropout masks as a source of epistemic randomness, we define a fidelity hierarchy by the number of stochastic forward passes used to estimate predictive moments. We construct coupled coarse--fine estimators by reusing dropout masks across fidelities, yielding telescoping MLMC estimators for both predictive means and predictive variances that remain unbiased for the corresponding dropout-induced quantities while reducing sampling variance at fixed evaluation budget. We derive explicit bias, variance and effective cost expressions, together with sample-allocation rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa benchmarks confirm the predicted variance rates and demonstrate efficiency gains over single-level MC-dropout at matched cost.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u7684dropout\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528dropout\u63a9\u7801\u6784\u5efa\u8026\u5408\u7c97-\u7ec6\u4f30\u8ba1\u5668\uff0c\u964d\u4f4e\u65b9\u5dee\u5e76\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u8499\u7279\u5361\u6d1bdropout\u662f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5e38\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u5927\u91cf\u524d\u5411\u4f20\u64ad\u6765\u4f30\u8ba1\u9884\u6d4b\u77e9\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u964d\u4f4e\u65b9\u5dee\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u5c06dropout\u63a9\u7801\u89c6\u4e3a\u8ba4\u77e5\u968f\u673a\u6e90\uff0c\u901a\u8fc7\u524d\u5411\u4f20\u64ad\u6b21\u6570\u5b9a\u4e49\u4fdd\u771f\u5ea6\u5c42\u7ea7\u3002\u91cd\u7528dropout\u63a9\u7801\u6784\u5efa\u8026\u5408\u7684\u7c97-\u7ec6\u4f30\u8ba1\u5668\uff0c\u5f62\u6210\u7528\u4e8e\u9884\u6d4b\u5747\u503c\u548c\u65b9\u5dee\u7684\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u4f38\u7f29\u4f30\u8ba1\u5668\u3002\u63a8\u5bfc\u4e86\u504f\u5dee\u3001\u65b9\u5dee\u548c\u6709\u6548\u6210\u672c\u8868\u8fbe\u5f0f\uff0c\u5e76\u5236\u5b9a\u4e86\u5c42\u7ea7\u95f4\u7684\u6837\u672c\u5206\u914d\u89c4\u5219\u3002", "result": "\u5728\u6b63\u5411\u548c\u9006\u5411PINNs-Uzawa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u9884\u6d4b\u7684\u65b9\u5dee\u7387\uff0c\u8bc1\u660e\u5728\u76f8\u540c\u8ba1\u7b97\u6210\u672c\u4e0b\uff0c\u591a\u7ea7\u8499\u7279\u5361\u6d1bdropout\u6bd4\u5355\u7ea7\u8499\u7279\u5361\u6d1bdropout\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u589e\u76ca\u3002", "conclusion": "\u591a\u7ea7\u8499\u7279\u5361\u6d1b\u6846\u67b6\u4e3adropout\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528dropout\u63a9\u7801\u548c\u4f18\u5316\u6837\u672c\u5206\u914d\uff0c\u5728\u4fdd\u6301\u65e0\u504f\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.11719", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2601.11719", "abs": "https://arxiv.org/abs/2601.11719", "authors": ["Ho Fung Tsoi", "Dylan Rankin"], "title": "jBOT: Semantic Jet Representation Clustering Emerges from Self-Distillation", "comment": "Under review", "summary": "Self-supervised learning is a powerful pre-training method for learning feature representations without labels, which often capture generic underlying semantics from the data and can later be fine-tuned for downstream tasks. In this work, we introduce jBOT, a pre-training method based on self-distillation for jet data from the CERN Large Hadron Collider, which combines local particle-level distillation with global jet-level distillation to learn jet representations that support downstream tasks such as anomaly detection and classification. We observe that pre-training on unlabeled jets leads to emergent semantic class clustering in the representation space. The clustering in the frozen embedding, when pre-trained on background jets only, enables anomaly detection via simple distance-based metrics, and the learned embedding can be fine-tuned for classification with improved performance compared to supervised models trained from scratch.", "AI": {"tldr": "jBOT\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u84b8\u998f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406CERN\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7684\u55b7\u6ce8\u6570\u636e\uff0c\u7ed3\u5408\u5c40\u90e8\u7c92\u5b50\u7ea7\u548c\u5168\u5c40\u55b7\u6ce8\u7ea7\u84b8\u998f\uff0c\u5b66\u4e60\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u55b7\u6ce8\u8868\u793a\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u662f\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u5373\u53ef\u5b66\u4e60\u7279\u5f81\u8868\u793a\u7684\u5f3a\u5927\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u5e38\u80fd\u6355\u6349\u6570\u636e\u7684\u901a\u7528\u5e95\u5c42\u8bed\u4e49\uff0c\u5e76\u53ef\u5728\u540e\u7eed\u5fae\u8c03\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3aLHC\u55b7\u6ce8\u6570\u636e\u5f00\u53d1\u6709\u6548\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fajBOT\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u57fa\u4e8e\u81ea\u84b8\u998f\u6280\u672f\uff0c\u7ed3\u5408\u5c40\u90e8\u7c92\u5b50\u7ea7\u84b8\u998f\u548c\u5168\u5c40\u55b7\u6ce8\u7ea7\u84b8\u998f\uff0c\u5b66\u4e60\u55b7\u6ce8\u7684\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728\u65e0\u6807\u7b7e\u55b7\u6ce8\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u65e0\u6807\u7b7e\u55b7\u6ce8\u4e0a\u9884\u8bad\u7ec3\u5bfc\u81f4\u8868\u793a\u7a7a\u95f4\u4e2d\u51fa\u73b0\u8bed\u4e49\u7c7b\u522b\u805a\u7c7b\u73b0\u8c61\u3002\u5f53\u4ec5\u5728\u80cc\u666f\u55b7\u6ce8\u4e0a\u9884\u8bad\u7ec3\u65f6\uff0c\u51bb\u7ed3\u5d4c\u5165\u4e2d\u7684\u805a\u7c7b\u53ef\u901a\u8fc7\u7b80\u5355\u8ddd\u79bb\u5ea6\u91cf\u5b9e\u73b0\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e14\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u5fae\u8c03\u540e\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "jBOT\u65b9\u6cd5\u6210\u529f\u5730\u4e3aLHC\u55b7\u6ce8\u6570\u636e\u5f00\u53d1\u4e86\u6709\u6548\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u5728\u8868\u793a\u7a7a\u95f4\u5f62\u6210\u8bed\u4e49\u805a\u7c7b\uff0c\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2601.12788", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12788", "abs": "https://arxiv.org/abs/2601.12788", "authors": ["Kaihe Wang", "Ran Yang", "Lipeng Zhu", "Rongyan Xi", "Yue Xiu", "Zhongpei Zhang"], "title": "Movable Antenna Enhanced MIMO Communications with Spatial Modulation", "comment": null, "summary": "Movable antenna (MA) has demonstrated great potential in enhancing wireless communication performance. In this paper, we investigate an MA-enabled multiple-input multiple-output (MIMO) communication system with spatial modulation (SM), which improves communication performance by utilizing flexible MA placement while reducing the cost of RF chains. To this end, we propose a joint transceiver design framework aimed at minimizing the bit error rate (BER) based on the maximum minimum distance (MMD) criterion. To address the intractable problem, we develop an efficient iterative algorithm based on alternating optimization (AO) and successive convex approximation (SCA) techniques. Simulation results demonstrate that the proposed algorithm achieves rapid convergence performance and significantly outperforms the existing benchmark schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7684MIMO\u7a7a\u95f4\u8c03\u5236\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u6536\u53d1\u5668\u8bbe\u8ba1\u6700\u5c0f\u5316\u8bef\u7801\u7387\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u548c\u9010\u6b21\u51f8\u903c\u8fd1\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "motivation": "\u53ef\u79fb\u52a8\u5929\u7ebf\u80fd\u589e\u5f3a\u65e0\u7ebf\u901a\u4fe1\u6027\u80fd\uff0c\u7ed3\u5408\u7a7a\u95f4\u8c03\u5236\u53ef\u4ee5\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u5c04\u9891\u94fe\u6210\u672c\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u8054\u5408\u6536\u53d1\u5668\u8bbe\u8ba1\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u6700\u5c0f\u8ddd\u79bb\u51c6\u5219\u7684\u8054\u5408\u6536\u53d1\u5668\u8bbe\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u548c\u9010\u6b21\u51f8\u903c\u8fd1\uff08SCA\uff09\u6280\u672f\u5f00\u53d1\u9ad8\u6548\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u6240\u63d0\u7b97\u6cd5\u5177\u6709\u5feb\u901f\u6536\u655b\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "MA-enabled MIMO\u7a7a\u95f4\u8c03\u5236\u7cfb\u7edf\u901a\u8fc7\u63d0\u51fa\u7684\u8054\u5408\u6536\u53d1\u5668\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u901a\u4fe1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.13448", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13448", "abs": "https://arxiv.org/abs/2601.13448", "authors": ["Sofiane Tanji", "Samuel Vaiter", "Yassine Laguel"], "title": "Fairness-informed Pareto Optimization : An Efficient Bilevel Framework", "comment": null, "summary": "Despite their promise, fair machine learning methods often yield Pareto-inefficient models, in which the performance of certain groups can be improved without degrading that of others. This issue arises frequently in traditional in-processing approaches such as fairness-through-regularization. In contrast, existing Pareto-efficient approaches are biased towards a certain perspective on fairness and fail to adapt to the broad range of fairness metrics studied in the literature. In this paper, we present BADR, a simple framework to recover the optimal Pareto-efficient model for any fairness metric. Our framework recovers its models through a Bilevel Adaptive Rescalarisation procedure. The lower level is a weighted empirical risk minimization task where the weights are a convex combination of the groups, while the upper level optimizes the chosen fairness objective. We equip our framework with two novel large-scale, single-loop algorithms, BADR-GD and BADR-SGD, and establish their convergence guarantees. We release badr, an open-source Python toolbox implementing our framework for a variety of learning tasks and fairness metrics. Finally, we conduct extensive numerical experiments demonstrating the advantages of BADR over existing Pareto-efficient approaches to fairness.", "AI": {"tldr": "BADR\u662f\u4e00\u4e2a\u53cc\u5c42\u81ea\u9002\u5e94\u91cd\u65b0\u6807\u91cf\u5316\u6846\u67b6\uff0c\u53ef\u4e3a\u4efb\u4f55\u516c\u5e73\u6027\u6307\u6807\u6062\u590d\u6700\u4f18\u5e15\u7d2f\u6258\u6548\u7387\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u504f\u5411\u7279\u5b9a\u516c\u5e73\u89c6\u89d2\u4e14\u65e0\u6cd5\u9002\u5e94\u5e7f\u6cdb\u516c\u5e73\u6307\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7ecf\u5e38\u4ea7\u751f\u5e15\u7d2f\u6258\u4f4e\u6548\u6a21\u578b\uff0c\u67d0\u4e9b\u7fa4\u4f53\u7684\u6027\u80fd\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u5176\u4ed6\u7fa4\u4f53\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u6539\u8fdb\u3002\u4f20\u7edf\u5904\u7406\u65b9\u6cd5\uff08\u5982\u901a\u8fc7\u6b63\u5219\u5316\u7684\u516c\u5e73\u6027\uff09\u5b58\u5728\u6b64\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5e15\u7d2f\u6258\u6548\u7387\u65b9\u6cd5\u504f\u5411\u7279\u5b9a\u516c\u5e73\u89c6\u89d2\uff0c\u65e0\u6cd5\u9002\u5e94\u6587\u732e\u4e2d\u7814\u7a76\u7684\u5e7f\u6cdb\u516c\u5e73\u6307\u6807\u3002", "method": "\u63d0\u51faBADR\uff08Bilevel Adaptive Rescalarisation\uff09\u6846\u67b6\uff1a\u4e0b\u5c42\u662f\u52a0\u6743\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u4efb\u52a1\uff0c\u6743\u91cd\u662f\u5404\u7ec4\u7684\u51f8\u7ec4\u5408\uff1b\u4e0a\u5c42\u4f18\u5316\u9009\u62e9\u7684\u516c\u5e73\u6027\u76ee\u6807\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u5927\u89c4\u6a21\u5355\u5faa\u73af\u7b97\u6cd5BADR-GD\u548cBADR-SGD\uff0c\u5e76\u5efa\u7acb\u4e86\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u53d1\u5e03\u4e86badr\u5f00\u6e90Python\u5de5\u5177\u7bb1\uff0c\u652f\u6301\u591a\u79cd\u5b66\u4e60\u4efb\u52a1\u548c\u516c\u5e73\u6027\u6307\u6807\u3002\u901a\u8fc7\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8bc1\u660eBADR\u76f8\u6bd4\u73b0\u6709\u5e15\u7d2f\u6258\u6548\u7387\u516c\u5e73\u6027\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "BADR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6846\u67b6\uff0c\u53ef\u4e3a\u4efb\u4f55\u516c\u5e73\u6027\u6307\u6807\u6062\u590d\u6700\u4f18\u5e15\u7d2f\u6258\u6548\u7387\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u548c\u6536\u655b\u4fdd\u8bc1\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2601.11789", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11789", "abs": "https://arxiv.org/abs/2601.11789", "authors": ["Shenyang Deng", "Boyao Liao", "Zhuoli Ouyang", "Tianyu Pang", "Minhak Song", "Yaoqing Yang"], "title": "Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis", "comment": "The 37th International Conference on Algorithmic Learning Theory", "summary": "This paper explores the suspicious alignment phenomenon in stochastic gradient descent (SGD) under ill-conditioned optimization, where the Hessian spectrum splits into dominant and bulk subspaces. This phenomenon describes the behavior of gradient alignment in SGD updates. Specifically, during the initial phase of SGD updates, the alignment between the gradient and the dominant subspace tends to decrease. Subsequently, it enters a rising phase and eventually stabilizes in a high-alignment phase. The alignment is considered ``suspicious'' because, paradoxically, the projected gradient update along this highly-aligned dominant subspace proves ineffective at reducing the loss. The focus of this work is to give a fine-grained analysis in a high-dimensional quadratic setup about how step size selection produces this phenomenon. Our main contribution can be summarized as follows: We propose a step-size condition revealing that in low-alignment regimes, an adaptive critical step size $\u03b7_t^*$ separates alignment-decreasing ($\u03b7_t < \u03b7_t^*$) from alignment-increasing ($\u03b7_t > \u03b7_t^*$) regimes, whereas in high-alignment regimes, the alignment is self-correcting and decreases regardless of the step size. We further show that under sufficient ill-conditioning, a step size interval exists where projecting the SGD updates to the bulk space decreases the loss while projecting them to the dominant space increases the loss, which explains a recent empirical observation that projecting gradient updates to the dominant subspace is ineffective. Finally, based on this adaptive step-size theory, we prove that for a constant step size and large initialization, SGD exhibits this distinct two-phase behavior: an initial alignment-decreasing phase, followed by stabilization at high alignment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u4e0e\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u5bf9\u9f50\u7684\u52a8\u6001\u53d8\u5316\u89c4\u5f8b\uff0c\u5e76\u5206\u6790\u4e86\u6b65\u957f\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u89c2\u5bdf\u5230\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\uff1a\u5728\u75c5\u6001\u4f18\u5316\u4e2d\uff0c\u68af\u5ea6\u4e0e\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u7684\u5bf9\u9f50\u4f1a\u7ecf\u5386\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u6700\u7ec8\u7a33\u5b9a\u7684\u52a8\u6001\u8fc7\u7a0b\uff0c\u4f46\u4ee4\u4eba\u56f0\u60d1\u7684\u662f\uff0c\u8fd9\u79cd\u9ad8\u5ea6\u5bf9\u9f50\u7684\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u6295\u5f71\u66f4\u65b0\u5374\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u635f\u5931\u3002\u9700\u8981\u6df1\u5165\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u80cc\u540e\u7684\u673a\u5236\u3002", "method": "\u5728\u9ad8\u7ef4\u4e8c\u6b21\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u63d0\u51fa\u6b65\u957f\u6761\u4ef6\u7406\u8bba\u3002\u901a\u8fc7\u5206\u6790\u81ea\u9002\u5e94\u4e34\u754c\u6b65\u957f\u03b7_t^*\uff0c\u533a\u5206\u5bf9\u9f50\u4e0b\u964d\u548c\u5bf9\u9f50\u4e0a\u5347\u673a\u5236\u3002\u7814\u7a76\u5728\u75c5\u6001\u6761\u4ef6\u4e0b\uff0c\u6b65\u957f\u533a\u95f4\u5982\u4f55\u5bfc\u81f4\u6279\u91cf\u5b50\u7a7a\u95f4\u6295\u5f71\u964d\u4f4e\u635f\u5931\u800c\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u6295\u5f71\u589e\u52a0\u635f\u5931\u3002", "result": "\u53d1\u73b0\uff1a1\uff09\u5728\u4f4e\u5bf9\u9f50\u673a\u5236\u4e2d\uff0c\u81ea\u9002\u5e94\u4e34\u754c\u6b65\u957f\u03b7_t^*\u5206\u79bb\u5bf9\u9f50\u4e0b\u964d\u548c\u5bf9\u9f50\u4e0a\u5347\u673a\u5236\uff1b2\uff09\u5728\u9ad8\u5bf9\u9f50\u673a\u5236\u4e2d\uff0c\u5bf9\u9f50\u5177\u6709\u81ea\u6821\u6b63\u7279\u6027\uff0c\u65e0\u8bba\u6b65\u957f\u5982\u4f55\u90fd\u4f1a\u4e0b\u964d\uff1b3\uff09\u5728\u8db3\u591f\u75c5\u6001\u6761\u4ef6\u4e0b\uff0c\u5b58\u5728\u6b65\u957f\u533a\u95f4\u4f7f\u5f97\u6279\u91cf\u5b50\u7a7a\u95f4\u6295\u5f71\u964d\u4f4e\u635f\u5931\u800c\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u6295\u5f71\u589e\u52a0\u635f\u5931\uff1b4\uff09\u5bf9\u4e8e\u6052\u5b9a\u6b65\u957f\u548c\u5927\u521d\u59cb\u5316\uff0cSGD\u786e\u5b9e\u8868\u73b0\u51fa\u4e24\u9636\u6bb5\u884c\u4e3a\uff1a\u521d\u59cb\u5bf9\u9f50\u4e0b\u964d\u9636\u6bb5\uff0c\u968f\u540e\u7a33\u5b9a\u5728\u9ad8\u5bf9\u9f50\u72b6\u6001\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86SGD\u5728\u75c5\u6001\u4f18\u5316\u4e2d\u7684\"\u53ef\u7591\u5bf9\u9f50\"\u73b0\u8c61\u673a\u5236\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u4e3b\u5bfc\u5b50\u7a7a\u95f4\u6295\u5f71\u66f4\u65b0\u65e0\u6548\uff0c\u5e76\u5efa\u7acb\u4e86\u6b65\u957f\u9009\u62e9\u4e0e\u5bf9\u9f50\u52a8\u6001\u4e4b\u95f4\u7684\u5b9a\u91cf\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3SGD\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.12827", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12827", "abs": "https://arxiv.org/abs/2601.12827", "authors": ["Haotian Wang", "Dan Wang", "Xiaodong Xu", "Chuan Huang", "Hao Chen", "Nan Ma"], "title": "Integrated Sensing and Semantic Communication with Adaptive Source-Channel Coding", "comment": null, "summary": "Semantic communication has emerged as a new paradigm to facilitate the performance of integrated sensing and communication systems in 6G. However, most of the existing works mainly focus on sensing data compression to reduce the subsequent communication overheads, without considering the integrated transmission framework for both the SemCom and sensing tasks. This paper proposes an adaptive source-channel coding and beamforming design framework for integrated sensing and SemCom systems by jointly optimizing the coding rate for SemCom task and the transmit beamforming for both the SemCom and sensing tasks. Specifically, an end-to-end semantic distortion function is approximated by deriving an upper bound composing of source and channel coding induced components, and then a hybrid Cram\u00e9r-Rao bound (HCRB) is also derived for target position under imperfect time synchronization. To facilitate the joint optimization, a distortion minimization problem is formulated by considering the HCRB threshold, channel uses, and power budget. Subsequently, an alternative optimization algorithm composed of successive convex approximation and fractional programming is proposed to address this problem by decoupling it into two subproblems for coding rate and beamforming designs, respectively. Simulation results demonstrate that our proposed scheme outperforms the conventional deep joint source-channel coding -water filling-zero forcing benchmark.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u96c6\u6210\u611f\u77e5\u4e0e\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u4fe1\u6e90\u4fe1\u9053\u7f16\u7801\u4e0e\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8bed\u4e49\u901a\u4fe1\u4efb\u52a1\u7684\u7f16\u7801\u7387\u548c\u611f\u77e5\u4efb\u52a1\u7684\u6ce2\u675f\u6210\u5f62\uff0c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u6570\u636e\u538b\u7f29\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u4f46\u7f3a\u4e4f\u540c\u65f6\u8003\u8651\u8bed\u4e49\u901a\u4fe1\u548c\u611f\u77e5\u4efb\u52a1\u7684\u96c6\u6210\u4f20\u8f93\u6846\u67b6\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u6765\u540c\u65f6\u4f18\u5316\u8bed\u4e49\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u4fe1\u6e90\u4fe1\u9053\u7f16\u7801\u548c\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u6846\u67b6\uff1a1) \u63a8\u5bfc\u7aef\u5230\u7aef\u8bed\u4e49\u5931\u771f\u51fd\u6570\u7684\u4e0a\u754c\uff1b2) \u63a8\u5bfc\u65f6\u95f4\u540c\u6b65\u4e0d\u5b8c\u7f8e\u4e0b\u7684\u76ee\u6807\u4f4d\u7f6e\u6df7\u5408\u514b\u62c9\u7f8e\u7f57\u4e0b\u754c\uff1b3) \u5efa\u7acb\u8003\u8651HCRB\u9608\u503c\u3001\u4fe1\u9053\u4f7f\u7528\u548c\u529f\u7387\u9884\u7b97\u7684\u5931\u771f\u6700\u5c0f\u5316\u95ee\u9898\uff1b4) \u63d0\u51fa\u57fa\u4e8e\u9010\u6b21\u51f8\u903c\u8fd1\u548c\u5206\u5f0f\u89c4\u5212\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u7f16\u7801\u7387\u548c\u6ce2\u675f\u6210\u5f62\u4e24\u4e2a\u5b50\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u4f18\u4e8e\u4f20\u7edf\u7684\u6df1\u5ea6\u8054\u5408\u4fe1\u6e90\u4fe1\u9053\u7f16\u7801-\u6ce8\u6c34-\u8feb\u96f6\u57fa\u51c6\u65b9\u6848\uff0c\u5728\u96c6\u6210\u611f\u77e5\u548c\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a6G\u96c6\u6210\u611f\u77e5\u4e0e\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u81ea\u9002\u5e94\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7f16\u7801\u7387\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u5728\u6ee1\u8db3\u611f\u77e5\u7cbe\u5ea6\u8981\u6c42\u7684\u540c\u65f6\u4f18\u5316\u8bed\u4e49\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.13474", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13474", "abs": "https://arxiv.org/abs/2601.13474", "authors": ["Jianhao Ma", "Yu Huang", "Yuejie Chi", "Yuxin Chen"], "title": "Preconditioning Benefits of Spectral Orthogonalization in Muon", "comment": null, "summary": "The Muon optimizer, a matrix-structured algorithm that leverages spectral orthogonalization of gradients, is a milestone in the pretraining of large language models. However, the underlying mechanisms of Muon -- particularly the role of gradient orthogonalization -- remain poorly understood, with very few works providing end-to-end analyses that rigorously explain its advantages in concrete applications. We take a step by studying the effectiveness of a simplified variant of Muon through two case studies: matrix factorization, and in-context learning of linear transformers. For both problems, we prove that simplified Muon converges linearly with iteration complexities independent of the relevant condition number, provably outperforming gradient descent and Adam. Our analysis reveals that the Muon dynamics decouple into a collection of independent scalar sequences in the spectral domain, each exhibiting similar convergence behavior. Our theory formalizes the preconditioning effect induced by spectral orthogonalization, offering insight into Muon's effectiveness in these matrix optimization problems and potentially beyond.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86Muon\u4f18\u5316\u5668\u7684\u7b80\u5316\u7248\u672c\uff0c\u8bc1\u660e\u4e86\u5728\u77e9\u9635\u5206\u89e3\u548c\u7ebf\u6027Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\uff0c\u7b80\u5316Muon\u5177\u6709\u72ec\u7acb\u4e8e\u6761\u4ef6\u6570\u7684\u7ebf\u6027\u6536\u655b\u6027\uff0c\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u548cAdam\u3002", "motivation": "Muon\u4f18\u5316\u5668\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u91cd\u8981\u7b97\u6cd5\uff0c\u5176\u6838\u5fc3\u673a\u5236\u2014\u2014\u68af\u5ea6\u6b63\u4ea4\u5316\u7684\u4f5c\u7528\u2014\u2014\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684\u7406\u8bba\u5206\u6790\u6765\u89e3\u91ca\u5176\u5728\u5177\u4f53\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u5206\u6790\u7b80\u5316\u7248Muon\u7684\u6709\u6548\u6027\uff1a\u77e9\u9635\u5206\u89e3\u548c\u7ebf\u6027Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u5728\u8c31\u57df\u4e2d\u5c06Muon\u52a8\u6001\u89e3\u8026\u4e3a\u72ec\u7acb\u7684\u6807\u91cf\u5e8f\u5217\uff0c\u5206\u6790\u5176\u6536\u655b\u884c\u4e3a\u3002", "result": "\u8bc1\u660e\u4e86\u7b80\u5316Muon\u5728\u8fd9\u4e24\u4e2a\u95ee\u9898\u4e0a\u90fd\u80fd\u7ebf\u6027\u6536\u655b\uff0c\u4e14\u8fed\u4ee3\u590d\u6742\u5ea6\u4e0e\u76f8\u5173\u6761\u4ef6\u6570\u65e0\u5173\uff0c\u7406\u8bba\u4e0a\u4f18\u4e8e\u68af\u5ea6\u4e0b\u964d\u548cAdam\u3002\u63ed\u793a\u4e86\u8c31\u6b63\u4ea4\u5316\u5e26\u6765\u7684\u9884\u5904\u7406\u6548\u5e94\u3002", "conclusion": "\u8be5\u7406\u8bba\u5206\u6790\u5f62\u5f0f\u5316\u4e86\u8c31\u6b63\u4ea4\u5316\u8bf1\u5bfc\u7684\u9884\u5904\u7406\u6548\u5e94\uff0c\u4e3a\u7406\u89e3Muon\u5728\u77e9\u9635\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u53ef\u80fd\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2601.11794", "categories": ["cs.LG", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.11794", "abs": "https://arxiv.org/abs/2601.11794", "authors": ["Abdelrahman Ramadan", "Zahra Dorbeigi Namaghi", "Emily Taylor", "Lucas Edwards", "Xan Giuliani", "David S. McLagan", "Sidney Givigi", "Melissa Greeff"], "title": "Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV Sensing", "comment": null, "summary": "Wildfire monitoring requires high-resolution atmospheric measurements, yet low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift, cross-sensitivity, and response lag that corrupt concentration estimates. Traditional deep learning denoising approaches demand large datasets impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE, a physics-informed denoising autoencoder that addresses data scarcity by embedding physical constraints directly into the network architecture. Non-negative concentration estimates are enforced via softplus activations and physically plausible temporal smoothing, ensuring outputs are physically admissible by construction rather than relying on loss function penalties. The architecture employs hierarchical decoder heads for Black Carbon, Gas, and CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight data), two orders of magnitude below typical deep learning requirements. PC$^2$DAE-Lean achieves 67.3\\% smoothness improvement and 90.7\\% high-frequency noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net, Transformer, CBDAE, DeSpaWN) produce 15--23\\% negative outputs. The lean variant outperforms wide (+5.6\\% smoothness), suggesting reduced capacity with strong inductive bias prevents overfitting in data-scarce regimes. Training completes in under 65 seconds on consumer hardware.", "AI": {"tldr": "PC\u00b2DAE\uff1a\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u7ea6\u675f\u5d4c\u5165\u7f51\u7edc\u67b6\u6784\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u4f20\u611f\u5668\u6570\u636e\u7a00\u7f3a\u4e0b\u7684\u91ce\u706b\u76d1\u6d4b\u53bb\u566a\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u642d\u8f7d\u7684\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5b58\u5728\u57fa\u7ebf\u6f02\u79fb\u3001\u4ea4\u53c9\u654f\u611f\u6027\u548c\u54cd\u5e94\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u800c\u91ce\u706b\u76d1\u6d4b\u7684\u65e0\u4eba\u673a\u98de\u884c\u6570\u636e\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u63d0\u51faPC\u00b2DAE\u7269\u7406\u4fe1\u606f\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7softplus\u6fc0\u6d3b\u51fd\u6570\u786e\u4fdd\u6d53\u5ea6\u4f30\u8ba1\u975e\u8d1f\uff0c\u7269\u7406\u5408\u7406\u7684\u65f6\u95f4\u5e73\u6ed1\u7ea6\u675f\uff0c\u5206\u5c42\u89e3\u7801\u5668\u5934\u5904\u7406\u4e0d\u540c\u4f20\u611f\u5668\u5bb6\u65cf\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7248\uff0821k\u53c2\u6570\uff09\u548c\u5bbd\u7248\uff08204k\u53c2\u6570\uff09\u4e24\u79cd\u53d8\u4f53\u3002", "result": "\u5728\u4ec57,894\u4e2a\u6837\u672c\uff08\u7ea62.2\u5c0f\u65f6\u98de\u884c\u6570\u636e\uff09\u7684\u5c0f\u6570\u636e\u96c6\u4e0a\uff0cPC\u00b2DAE-Lean\u5b9e\u73b067.3%\u5e73\u6ed1\u5ea6\u63d0\u5347\u548c90.7%\u9ad8\u9891\u566a\u58f0\u51cf\u5c11\uff0c\u96f6\u7269\u7406\u8fdd\u89c4\uff0c\u4f18\u4e8e\u4e94\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec565\u79d2\u3002", "conclusion": "PC\u00b2DAE\u901a\u8fc7\u5c06\u7269\u7406\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u4f20\u611f\u5668\u53bb\u566a\uff0c\u8f7b\u91cf\u7248\u6027\u80fd\u4f18\u4e8e\u5bbd\u7248\uff0c\u8868\u660e\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\uff0c\u5f3a\u5f52\u7eb3\u504f\u7f6e\u6bd4\u6a21\u578b\u5bb9\u91cf\u66f4\u91cd\u8981\u3002"}}
{"id": "2601.12867", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.12867", "abs": "https://arxiv.org/abs/2601.12867", "authors": ["Zixiang Han", "Hanning Wang", "Shiwen Tang", "Yujie Zhang"], "title": "Angular Sensing by Highly Reconfigurable Pixel Antennas with Joint Radiating Aperture and Feeding Ports Reconfiguration", "comment": null, "summary": "Angular sensing capability is realized using highly reconfigurable pixel antenna (HRPA) with joint radiating aperture and feeding ports reconfiguration. Pixel antennas represent a general class of reconfigurable antenna designs in which the radiating surface, regardless of its shape or size, is divided into sub-wavelength elements called pixels. Each pixel is connected to its neighboring elements through radio frequency switches. By controlling pixel connections, the pixel antenna topology can be flexibly adjusted so that the resulting radiation pattern can be reconfigured. However, conventional pixel antennas have only a single, fixed-position feeding port, which is not efficient for angular sensing. Therefore, in this work, we further extend the reconfigurability of pixel antennas by introducing the HRPA, which enables both geometry control of the pixel antenna and switching of its feeding ports. The model of the proposed HRPA, including both circuit and radiation parameters, is derived. A codebook is then defined, consisting of pixel connection states and feeding port positions for each sensing area. Based on this codebook, an efficient optimization approach is developed to minimize the Cram\\acute{\\mathrm{\\mathbf{e}}}r-Rao lower bound (CRLB) and obtain the optimal HRPA geometries for angular sensing within a given area. Numerical results show that the HRPA reduces the angle estimation error by more than 50% across the full three-dimensional sphere when compared with a conventional uniform planar array of the same size. This demonstrates the effectiveness of the proposed approach and highlights the potential of HRPA for integrated sensing and communication systems.", "AI": {"tldr": "\u63d0\u51fa\u9ad8\u5ea6\u53ef\u91cd\u6784\u50cf\u7d20\u5929\u7ebf(HRPA)\uff0c\u901a\u8fc7\u8054\u5408\u91cd\u6784\u8f90\u5c04\u5b54\u5f84\u548c\u9988\u7535\u7aef\u53e3\u5b9e\u73b0\u89d2\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u5e73\u9762\u9635\u5217\u5c06\u89d2\u5ea6\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e50%\u4ee5\u4e0a\u3002", "motivation": "\u4f20\u7edf\u50cf\u7d20\u5929\u7ebf\u53ea\u6709\u56fa\u5b9a\u4f4d\u7f6e\u7684\u5355\u4e00\u9988\u7535\u7aef\u53e3\uff0c\u5bf9\u4e8e\u89d2\u5ea6\u611f\u77e5\u4e0d\u591f\u9ad8\u6548\u3002\u9700\u8981\u6269\u5c55\u50cf\u7d20\u5929\u7ebf\u7684\u53ef\u91cd\u6784\u6027\uff0c\u540c\u65f6\u63a7\u5236\u5929\u7ebf\u51e0\u4f55\u5f62\u72b6\u548c\u9988\u7535\u7aef\u53e3\u5207\u6362\uff0c\u4ee5\u63d0\u5347\u89d2\u5ea6\u611f\u77e5\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u9ad8\u5ea6\u53ef\u91cd\u6784\u50cf\u7d20\u5929\u7ebf(HRPA)\uff0c\u5efa\u7acb\u5305\u542b\u7535\u8def\u548c\u8f90\u5c04\u53c2\u6570\u7684\u6a21\u578b\u3002\u5b9a\u4e49\u5305\u542b\u50cf\u7d20\u8fde\u63a5\u72b6\u6001\u548c\u9988\u7535\u7aef\u53e3\u4f4d\u7f6e\u7684\u7801\u672c\uff0c\u5f00\u53d1\u4f18\u5316\u65b9\u6cd5\u6700\u5c0f\u5316CRLB\uff0c\u83b7\u53d6\u7ed9\u5b9a\u533a\u57df\u5185\u89d2\u5ea6\u611f\u77e5\u7684\u6700\u4f18HRPA\u51e0\u4f55\u5f62\u72b6\u3002", "result": "HRPA\u5728\u5168\u4e09\u7ef4\u7403\u9762\u4e0a\u5c06\u89d2\u5ea6\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u8d85\u8fc750%\uff0c\u76f8\u6bd4\u76f8\u540c\u5c3a\u5bf8\u7684\u4f20\u7edf\u5747\u5300\u5e73\u9762\u9635\u5217\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "HRPA\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89d2\u5ea6\u611f\u77e5\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u96c6\u6210\u611f\u77e5\u548c\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002\u901a\u8fc7\u8054\u5408\u91cd\u6784\u8f90\u5c04\u5b54\u5f84\u548c\u9988\u7535\u7aef\u53e3\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u89d2\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u5347\u3002"}}
{"id": "2601.13698", "categories": ["cs.LG", "cs.AI", "cs.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13698", "abs": "https://arxiv.org/abs/2601.13698", "authors": ["Arjun Nichani", "Hsiang Hsu", "Chun-Fu", "Chen", "Haewon Jeong"], "title": "Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation", "comment": null, "summary": "Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528\u4fe1\u606f\u8bba\u4e2d\u7684\u5207\u5c14\u8bfa\u592b\u4fe1\u606f\u5206\u6790\u516c\u5e73\u6027\u3001\u9690\u79c1\u6027\u548c\u51c6\u786e\u6027\u4e09\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u566a\u58f0\u5207\u5c14\u8bfa\u592b\u5dee\u5f02\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u8fd9\u79cd\u5173\u7cfb\u7684\u6570\u636e\u4f9d\u8d56\u6027\u3002", "motivation": "\u5c3d\u7ba1\u516c\u5e73\u6027\u548c\u9690\u79c1\u6027\u90fd\u662f\u53ef\u4fe1\u673a\u5668\u5b66\u4e60\u7684\u91cd\u8981\u652f\u67f1\uff0c\u4f46\u4e24\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\u7814\u7a76\u76f8\u5bf9\u8f83\u5c11\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u516c\u5e73\u6027\u3001\u9690\u79c1\u6027\u548c\u51c6\u786e\u6027\u4e09\u8005\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5982\u4f55\u76f8\u4e92\u5f71\u54cd\u3002", "method": "1. \u63d0\u51fa\u566a\u58f0\u5207\u5c14\u8bfa\u592b\u5dee\u5f02\u4f5c\u4e3a\u540c\u65f6\u5206\u6790\u516c\u5e73\u6027\u3001\u9690\u79c1\u6027\u548c\u51c6\u786e\u6027\u4e09\u8005\u5173\u7cfb\u7684\u5de5\u5177\n2. \u5728\u5408\u6210\u6570\u636e\u4e0a\u5206\u6790\u8be5\u503c\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u4e09\u79cd\u884c\u4e3a\u6a21\u5f0f\n3. \u63d0\u51fa\u4f30\u8ba1\u672a\u77e5\u5206\u5e03\u6570\u636e\u5207\u5c14\u8bfa\u592b\u4fe1\u606f\u7684\u65b9\u6cd5\n4. \u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u8be5\u6846\u67b6\u5206\u6790\u4e09\u8005\u52a8\u6001\u5173\u7cfb", "result": "1. \u566a\u58f0\u5207\u5c14\u8bfa\u592b\u5dee\u5f02\u5728\u5408\u6210\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u4e09\u79cd\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u53d6\u51b3\u4e8e\u6570\u636e\u5206\u5e03\n2. \u8be5\u503c\u53ef\u4ee5\u4f5c\u4e3a\u516c\u5e73\u6027-\u51c6\u786e\u6027\u66f2\u7ebf\u9661\u5ced\u5ea6\u7684\u4ee3\u7406\u6307\u6807\n3. \u63ed\u793a\u4e86\u516c\u5e73\u6027\u3001\u9690\u79c1\u6027\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u5177\u6709\u6570\u636e\u4f9d\u8d56\u6027\n4. \u6210\u529f\u5c06\u6846\u67b6\u5e94\u7528\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\u5206\u6790", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u5bf9\u516c\u5e73\u6027-\u9690\u79c1\u6027-\u51c6\u786e\u6027\u5173\u7cfb\u7684\u7edf\u4e00\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u8fd9\u79cd\u5173\u7cfb\u7684\u6570\u636e\u4f9d\u8d56\u6027\u3002\u566a\u58f0\u5207\u5c14\u8bfa\u592b\u5dee\u5f02\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u540c\u65f6\u8003\u5bdf\u4e09\u8005\u4e4b\u95f4\u7684\u590d\u6742\u4e92\u52a8\u5173\u7cfb\u3002"}}
{"id": "2601.11821", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11821", "abs": "https://arxiv.org/abs/2601.11821", "authors": ["Shivani Tomar", "Seshu Tirupathi", "Elizabeth Daly", "Ivana Dusparic"], "title": "Shapelets-Enriched Selective Forecasting using Time Series Foundation Models", "comment": "Accepted by the AAAI-26 Workshop on Artificial Intelligence for Time Series Analysis (AI4TS)", "summary": "Time series foundation models have recently gained a lot of attention due to their ability to model complex time series data encompassing different domains including traffic, energy, and weather. Although they exhibit strong average zero-shot performance on forecasting tasks, their predictions on certain critical regions of the data are not always reliable, limiting their usability in real-world applications, especially when data exhibits unique trends. In this paper, we propose a selective forecasting framework to identify these critical segments of time series using shapelets. We learn shapelets using shift-invariant dictionary learning on the validation split of the target domain dataset. Utilizing distance-based similarity to these shapelets, we facilitate the user to selectively discard unreliable predictions and be informed of the model's realistic capabilities. Empirical results on diverse benchmark time series datasets demonstrate that our approach leveraging both zero-shot and full-shot fine-tuned models reduces the overall error by an average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model. Furthermore, our approach using zero-shot and full-shot fine-tuned models, also outperforms its random selection counterparts by up to 21.41% and 21.43% on one of the datasets.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eshapelets\u7684\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u8bc6\u522b\u65f6\u95f4\u5e8f\u5217\u5173\u952e\u533a\u57df\uff0c\u9009\u62e9\u6027\u4e22\u5f03\u4e0d\u53ef\u9760\u9884\u6d4b\uff0c\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u67d0\u4e9b\u5173\u952e\u6570\u636e\u533a\u57df\u7684\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u9650\u5236\u4e86\u5176\u5728\u5177\u6709\u72ec\u7279\u8d8b\u52bf\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\u3002", "method": "\u4f7f\u7528shapelets\u6784\u5efa\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\uff1a\u5728\u76ee\u6807\u57df\u9a8c\u8bc1\u96c6\u4e0a\u901a\u8fc7\u5e73\u79fb\u4e0d\u53d8\u5b57\u5178\u5b66\u4e60\u5b66\u4e60shapelets\uff0c\u5229\u7528\u57fa\u4e8e\u8ddd\u79bb\u7684\u76f8\u4f3c\u6027\u8bc6\u522b\u4e0d\u53ef\u9760\u9884\u6d4b\u533a\u57df\uff0c\u9009\u62e9\u6027\u4e22\u5f03\u4e0d\u53ef\u9760\u9884\u6d4b\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u96f6\u6837\u672c\u6a21\u578b\u5e73\u5747\u51cf\u5c1122.17%\u8bef\u5dee\uff0c\u5168\u6837\u672c\u5fae\u8c03\u6a21\u578b\u51cf\u5c1122.62%\u8bef\u5dee\uff1b\u76f8\u6bd4\u968f\u673a\u9009\u62e9\u65b9\u6cd5\uff0c\u5728\u67d0\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u534721.41%\u548c21.43%\u3002", "conclusion": "\u57fa\u4e8eshapelets\u7684\u9009\u62e9\u6027\u9884\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u65f6\u95f4\u5e8f\u5217\u5173\u952e\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u73b0\u5b9e\u7684\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u3002"}}
{"id": "2601.12924", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12924", "abs": "https://arxiv.org/abs/2601.12924", "authors": ["Ruopeng Xu", "Songling Zhang", "Zhaohui Yang", "Mingzhe Chen", "Zhaoyang Zhang", "Kai-Kit Wong"], "title": "Fluid Antenna Relay (FAR)-assisted Communication with Hybrid Relaying Scheme Selection", "comment": null, "summary": "In this paper, we investigate a fluid antenna relay (FAR)-assisted communication system with hybrid relaying scheme selection. By leveraging statistical channel state information (CSI) and distribution characteristics of fluid antenna system (FAS), we approximate the outage probability (OP) with different relaying schemes utilizing a Gaussian copula-based method. Each relay node follows the OP-minimized principle to choose the forwarding schemes. To reduce self-interference and avoid multi-user interference, half-duplex relays and frequency division multiple access schemes are considered, respectively. On this basis, we formulate a sum-rate maximization problem to mitigate the rate loss introduced by the half-duplex mode. To solve this problem, we first transform the original nonconvex problem into a power control optimization problem by obtaining the closed form of bandwidth allocation and substituting it into the original problem. Then, we solve the power control optimization problem with a low complexity method. Simulation results verify the effectiveness of our proposed algorithm to improve the sum rate of the system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91c7\u7528\u6df7\u5408\u4e2d\u7ee7\u65b9\u6848\u9009\u62e9\u7684\u6d41\u4f53\u5929\u7ebf\u4e2d\u7ee7\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u65afcopula\u65b9\u6cd5\u8fd1\u4f3c\u8ba1\u7b97\u4e0d\u540c\u4e2d\u7ee7\u65b9\u6848\u7684\u4e2d\u65ad\u6982\u7387\uff0c\u5e76\u57fa\u4e8e\u4e2d\u65ad\u6982\u7387\u6700\u5c0f\u5316\u539f\u5219\u9009\u62e9\u8f6c\u53d1\u65b9\u6848\uff0c\u6700\u7ec8\u901a\u8fc7\u529f\u7387\u63a7\u5236\u4f18\u5316\u63d0\u5347\u7cfb\u7edf\u603b\u901f\u7387\u3002", "motivation": "\u4f20\u7edf\u4e2d\u7ee7\u7cfb\u7edf\u5b58\u5728\u81ea\u5e72\u6270\u548c\u591a\u7528\u6237\u5e72\u6270\u95ee\u9898\uff0c\u4e14\u534a\u53cc\u5de5\u6a21\u5f0f\u4f1a\u5f15\u5165\u901f\u7387\u635f\u5931\u3002\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u5177\u6709\u7a7a\u95f4\u5206\u96c6\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u65b9\u6848\u9009\u62e9\u548c\u8d44\u6e90\u5206\u914d\u6765\u6700\u5927\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "1) \u5229\u7528\u7edf\u8ba1CSI\u548cFAS\u5206\u5e03\u7279\u6027\uff0c\u91c7\u7528\u9ad8\u65afcopula\u65b9\u6cd5\u8fd1\u4f3c\u8ba1\u7b97\u4e0d\u540c\u4e2d\u7ee7\u65b9\u6848\u7684\u4e2d\u65ad\u6982\u7387\uff1b2) \u57fa\u4e8e\u4e2d\u65ad\u6982\u7387\u6700\u5c0f\u5316\u539f\u5219\u9009\u62e9\u4e2d\u7ee7\u8f6c\u53d1\u65b9\u6848\uff1b3) \u91c7\u7528\u534a\u53cc\u5de5\u4e2d\u7ee7\u548c\u9891\u5206\u591a\u5740\u907f\u514d\u5e72\u6270\uff1b4) \u5c06\u539f\u59cb\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u529f\u7387\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u95ed\u5f0f\u5e26\u5bbd\u5206\u914d\u548c\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u603b\u901f\u7387\uff0c\u9a8c\u8bc1\u4e86\u6d41\u4f53\u5929\u7ebf\u4e2d\u7ee7\u7cfb\u7edf\u7ed3\u5408\u6df7\u5408\u4e2d\u7ee7\u65b9\u6848\u9009\u62e9\u548c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6d41\u4f53\u5929\u7ebf\u4e2d\u7ee7\u7cfb\u7edf\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e2d\u7ee7\u65b9\u6848\u9009\u62e9\u548c\u529f\u7387\u63a7\u5236\uff0c\u5728\u907f\u514d\u5e72\u6270\u7684\u540c\u65f6\u6700\u5927\u5316\u7cfb\u7edf\u603b\u901f\u7387\uff0c\u4e3a\u672a\u6765\u79fb\u52a8\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13776", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13776", "abs": "https://arxiv.org/abs/2601.13776", "authors": ["Thibaut Boissin", "Franck Mamalet", "Valentin Lafargue", "Mathieu Serrurier"], "title": "Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks", "comment": null, "summary": "Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at https://github.com/deel-ai/orthogonium.", "AI": {"tldr": "Orthogonium\u662f\u4e00\u4e2a\u7edf\u4e00\u7684PyTorch\u5e93\uff0c\u63d0\u4f9b\u6b63\u4ea4\u548c1-Lipschitz\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5b9e\u73b0\u5206\u6563\u3001\u6709\u9650\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u6b63\u4ea4\u548c1-Lipschitz\u795e\u7ecf\u7f51\u7edc\u5c42\u5bf9\u4e8e\u8ba4\u8bc1\u5bf9\u6297\u9c81\u68d2\u6027\u3001\u7a33\u5b9a\u751f\u6210\u6a21\u578b\u548c\u53ef\u9760\u5faa\u73af\u7f51\u7edc\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u5206\u6563\u3001\u6709\u9650\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86\u8fd9\u4e9b\u9c81\u68d2\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u91c7\u7528\u3002", "method": "\u5f00\u53d1\u4e86Orthogonium\u5e93\uff0c\u63d0\u4f9b\u7edf\u4e00\u3001\u9ad8\u6548\u3001\u5168\u9762\u7684\u6b63\u4ea4\u548c1-Lipschitz\u5c42\u5b9e\u73b0\uff0c\u652f\u6301\u6807\u51c6\u5377\u79ef\u7279\u5f81\uff08\u6b65\u957f\u3001\u81a8\u80c0\u3001\u5206\u7ec4\u3001\u8f6c\u7f6e\u7b49\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e25\u683c\u7684\u6570\u5b66\u4fdd\u8bc1\uff0c\u5e76\u4f18\u5316\u5b9e\u73b0\u4ee5\u51cf\u5c11\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f00\u9500\u3002", "result": "\u8be5\u5e93\u663e\u8457\u964d\u4f4e\u4e86\u91c7\u7528\u969c\u788d\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u5b9e\u9a8c\u548c\u96c6\u6210\uff0c\u540c\u65f6\u901a\u8fc7\u4e25\u683c\u6d4b\u8bd5\u53d1\u73b0\u4e86\u73b0\u6709\u5b9e\u73b0\u4e2d\u7684\u5173\u952e\u9519\u8bef\uff0c\u5f3a\u8c03\u4e86\u6807\u51c6\u5316\u53ef\u9760\u5de5\u5177\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Orthogonium\u4e3a\u9700\u8981\u6b63\u4ea4\u6027\u548c\u9c81\u68d2Lipschitz\u7ea6\u675f\u7684\u591a\u6837\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u9ad8\u6548\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9c81\u68d2\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u53d1\u5c55\u548c\u5e94\u7528\u3002"}}
{"id": "2601.11827", "categories": ["cs.LG", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.11827", "abs": "https://arxiv.org/abs/2601.11827", "authors": ["Andrea Rubbi", "Amir Akbarnejad", "Mohammad Vali Sanian", "Aryan Yazdan Parast", "Hesam Asadollahzadeh", "Arian Amani", "Naveed Akhtar", "Sarah Cooper", "Andrew Bassett", "Pietro Li\u00f2", "Lassi Paavolainen", "Sattar Vakili", "Mo Lotfollahi"], "title": "MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution Generalization", "comment": null, "summary": "Achieving robust generalization under distribution shift remains a central challenge in conditional generative modeling, as existing conditional flow-based methods often struggle to extrapolate beyond the training conditions. We introduce MixFlow, a conditional flow-matching framework for descriptor-controlled generation that directly targets this limitation by jointly learning a descriptor-conditioned base distribution and a descriptor-conditioned flow field via shortest-path flow matching. By modeling the base distribution as a learnable, descriptor-dependent mixture, MixFlow enables smooth interpolation and extrapolation to unseen conditions, leading to substantially improved out-of-distribution generalization. We provide analytical insights into the behavior of the proposed framework and empirically demonstrate its effectiveness across multiple domains, including prediction of responses to unseen perturbations in single-cell transcriptomic data and high-content microscopy-based drug screening tasks. Across these diverse settings, MixFlow consistently outperforms standard conditional flow-matching baselines. Overall, MixFlow offers a simple yet powerful approach for achieving robust, generalizable, and controllable generative modeling across heterogeneous domains.", "AI": {"tldr": "MixFlow\uff1a\u4e00\u79cd\u7528\u4e8e\u63cf\u8ff0\u7b26\u63a7\u5236\u751f\u6210\u7684\u6df7\u5408\u6761\u4ef6\u6d41\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u63cf\u8ff0\u7b26\u6761\u4ef6\u7684\u57fa\u7840\u5206\u5e03\u548c\u6d41\u573a\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6761\u4ef6\u6d41\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u96be\u4ee5\u5728\u8bad\u7ec3\u6761\u4ef6\u4e4b\u5916\u8fdb\u884c\u5916\u63a8\uff0c\u8fd9\u9650\u5236\u4e86\u6761\u4ef6\u751f\u6210\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faMixFlow\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u77ed\u8def\u5f84\u6d41\u5339\u914d\u8054\u5408\u5b66\u4e60\u63cf\u8ff0\u7b26\u6761\u4ef6\u7684\u57fa\u7840\u5206\u5e03\uff08\u5efa\u6a21\u4e3a\u53ef\u5b66\u4e60\u7684\u63cf\u8ff0\u7b26\u4f9d\u8d56\u6df7\u5408\u5206\u5e03\uff09\u548c\u63cf\u8ff0\u7b26\u6761\u4ef6\u7684\u6d41\u573a\uff0c\u5b9e\u73b0\u5e73\u6ed1\u63d2\u503c\u548c\u5916\u63a8\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\uff08\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u6570\u636e\u6270\u52a8\u54cd\u5e94\u9884\u6d4b\u3001\u9ad8\u5185\u6db5\u663e\u5fae\u955c\u836f\u7269\u7b5b\u9009\u7b49\uff09\u4e2d\uff0cMixFlow\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u6761\u4ef6\u6d41\u5339\u914d\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MixFlow\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u5f02\u6784\u9886\u57df\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u63a7\u7684\u751f\u6210\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6761\u4ef6\u751f\u6210\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u6311\u6218\u3002"}}
{"id": "2601.12963", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12963", "abs": "https://arxiv.org/abs/2601.12963", "authors": ["Mauro Marchese", "Musa Furkan Keskin", "Pietro Savazzi", "Henk Wymeersch"], "title": "Monostatic ISAC Without Full Buffers: Revisiting Spatial Trade-Offs Under Bursty Traffic", "comment": "Presented at 6th IEEE International Symposium on Joint Communication and Sensing (JC&S) 2026", "summary": "This work investigates the spatial trade-offs arising from the design of the transmit beamformer in a monostatic integrated sensing and communication (ISAC) base station (BS) under bursty traffic, a crucial aspect necessitated by the integration of communication and sensing functionalities in next-generation wireless systems. In this setting, the BS does not always have data available for transmission. This study compares different ISAC policies and reveals the presence of multiple effects influencing ISAC performance: signal-to-noise ratio (SNR) boosting of data-aided strategies compared to pilot-based ones, saturation of the probability of detection in data-aided strategies due to the non-full-buffer assumption, and, finally, directional masking of sensing targets due to the relative position between target and user. Simulation results demonstrate varying impact of these effects on ISAC trade-offs under different operating conditions, thus guiding the design of efficient ISAC transmission strategies.", "AI": {"tldr": "\u7814\u7a76\u5728\u7a81\u53d1\u6d41\u91cf\u4e0b\uff0c\u5355\u57fa\u5730\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u57fa\u7ad9\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u4e2d\u7684\u7a7a\u95f4\u6743\u8861\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u5f71\u54cdISAC\u6027\u80fd\u7684\u591a\u4e2a\u6548\u5e94", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u9700\u8981\u96c6\u6210\u901a\u4fe1\u548c\u611f\u77e5\u529f\u80fd\uff0c\u4f46\u5728\u7a81\u53d1\u6d41\u91cf\u573a\u666f\u4e0b\uff0c\u57fa\u7ad9\u5e76\u4e0d\u603b\u662f\u6709\u6570\u636e\u4f20\u8f93\u53ef\u7528\uff0c\u8fd9\u7ed9ISAC\u8bbe\u8ba1\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218", "method": "\u6bd4\u8f83\u4e0d\u540c\u7684ISAC\u7b56\u7565\uff0c\u5206\u6790\u6570\u636e\u8f85\u52a9\u7b56\u7565\u4e0e\u5bfc\u9891\u7b56\u7565\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u8003\u8651\u975e\u5168\u7f13\u51b2\u5047\u8bbe\u4e0b\u7684\u68c0\u6d4b\u6982\u7387\u9971\u548c\u6548\u5e94\uff0c\u4ee5\u53ca\u76ee\u6807\u4e0e\u7528\u6237\u76f8\u5bf9\u4f4d\u7f6e\u5f15\u8d77\u7684\u65b9\u5411\u6027\u63a9\u853d\u6548\u5e94", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e0d\u540c\u64cd\u4f5c\u6761\u4ef6\u4e0b\uff0c\u8fd9\u4e9b\u6548\u5e94\u5bf9ISAC\u6743\u8861\u4ea7\u751f\u4e0d\u540c\u5f71\u54cd\uff1a\u6570\u636e\u8f85\u52a9\u7b56\u7565\u76f8\u6bd4\u5bfc\u9891\u7b56\u7565\u5177\u6709SNR\u63d0\u5347\u4f18\u52bf\uff0c\u4f46\u5b58\u5728\u68c0\u6d4b\u6982\u7387\u9971\u548c\u95ee\u9898\uff0c\u540c\u65f6\u76ee\u6807\u4e0e\u7528\u6237\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f1a\u5bfc\u81f4\u65b9\u5411\u6027\u63a9\u853d", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7a81\u53d1\u6d41\u91cf\u4e0bISAC\u6027\u80fd\u7684\u591a\u4e2a\u5173\u952e\u6548\u5e94\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u7684ISAC\u4f20\u8f93\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u8868\u660e\u9700\u8981\u6839\u636e\u5177\u4f53\u64cd\u4f5c\u6761\u4ef6\u5e73\u8861\u8fd9\u4e9b\u6548\u5e94"}}
{"id": "2601.13851", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.13851", "abs": "https://arxiv.org/abs/2601.13851", "authors": ["Alessandro Londei", "Matteo Benati", "Denise Lanzieri", "Vittorio Loreto"], "title": "Inverting Self-Organizing Maps: A Unified Activation-Based Framework", "comment": null, "summary": "Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.", "AI": {"tldr": "\u63d0\u51faMUSIC\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u6620\u5c04(SOM)\u7684\u539f\u578b\u8ddd\u79bb\u53cd\u6f14\u7cbe\u786e\u6062\u590d\u8f93\u5165\u6570\u636e\uff0c\u5e76\u5b9e\u73b0\u53ef\u63a7\u7684\u8bed\u4e49\u8f68\u8ff9\u751f\u6210\uff0c\u65e0\u9700\u91c7\u6837\u6216\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002", "motivation": "\u4f20\u7edfSOM\u4e3b\u8981\u7528\u4e8e\u53ef\u89c6\u5316\u3001\u805a\u7c7b\u548c\u5411\u91cf\u91cf\u5316\uff0c\u4f46\u7f3a\u4e4f\u7cbe\u786e\u53cd\u6f14\u548c\u53ef\u63a7\u751f\u6210\u7684\u80fd\u529b\u3002\u4f5c\u8005\u5e0c\u671b\u5229\u7528SOM\u7684\u539f\u578b\u8ddd\u79bb\u51e0\u4f55\u7279\u6027\uff0c\u5b9e\u73b0\u7cbe\u786e\u6570\u636e\u6062\u590d\u548c\u53ef\u63a7\u8bed\u4e49\u53d8\u5316\u3002", "method": "\u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\u51e0\u4f55\u7406\u8bba\uff1aD\u7ef4\u7a7a\u95f4\u4e2d\u7684\u70b9\u53ef\u7531\u5176\u5230D+1\u4e2a\u4eff\u5c04\u72ec\u7acb\u53c2\u8003\u70b9\u7684\u8ddd\u79bb\u552f\u4e00\u786e\u5b9a\u3002\u63d0\u51faMUSIC\u66f4\u65b0\u89c4\u5219\uff0c\u901a\u8fc7\u4fee\u6539\u9009\u5b9a\u539f\u578b\u7684\u5e73\u65b9\u8ddd\u79bb\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8ddd\u79bb\u4e0d\u53d8\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u51e0\u4f55\u6d41\u3002\u4f7f\u7528Tikhonov\u6b63\u5219\u5316\u7a33\u5b9a\u66f4\u65b0\u89c4\u5219\u3002", "result": "\u5728\u5408\u6210\u9ad8\u65af\u6df7\u5408\u3001MNIST\u548cFaces in the Wild\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMUSIC\u80fd\u4ea7\u751f\u5e73\u6ed1\u3001\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\uff0c\u63ed\u793a\u5b66\u4e60\u6d41\u5f62\u7684\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u3002\u65e0\u6270\u52a8\u65f6\u7cbe\u786e\u6062\u590d\u8f93\u5165\uff0c\u6307\u5b9a\u76ee\u6807\u805a\u7c7b\u65f6\u4ea7\u751f\u8fde\u8d2f\u7684\u8bed\u4e49\u53d8\u5316\u3002", "conclusion": "MUSIC\u4e3a\u57fa\u4e8e\u539f\u578b\u51e0\u4f55\u7684\u6570\u636e\u589e\u5f3a\u548c\u53ef\u63a7\u6f5c\u5728\u63a2\u7d22\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86SOM\u53cd\u6f14\u76f8\u6bd4\u65e0\u76d1\u7763\u805a\u7c7b\u7684\u4f18\u52bf\uff0c\u65e0\u9700\u4f9d\u8d56\u91c7\u6837\u3001\u6f5c\u5728\u5148\u9a8c\u6216\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u3002"}}
{"id": "2601.11864", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11864", "abs": "https://arxiv.org/abs/2601.11864", "authors": ["Zhiyuan Li", "Yuan Wu", "Yi Chang"], "title": "AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training", "comment": "13 pages", "summary": "To stabilize the training of Large Language Models (LLMs), gradient clipping is a nearly ubiquitous heuristic used to alleviate exploding gradients. However, traditional global norm clipping erroneously presupposes gradient homogeneity across different functional modules, leading to an adverse \"spill-over\" effect where volatile parameters force unnecessary scaling on stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping (AGGC). AGGC partitions parameters into groups based on functional types and regulates each according to its historical behavior using an Exponential Moving Average (EMA). Specifically, it constructs an adaptive interval to simultaneously mitigate gradient explosion and vanishing, while employing a time-dependent scheduling mechanism to balance exploration and convergence. Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of 72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen 2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC effectively addresses the limitations of traditional gradient clipping methods, particularly in overcoming gradient heterogeneity, by utilizing a modular, adaptive clipping strategy to stabilize the training process. Due to its lightweight design, AGGC can be seamlessly integrated into existing post-training pipelines with negligible overhead.", "AI": {"tldr": "AGGC\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u7ec4\u68af\u5ea6\u88c1\u526a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6309\u529f\u80fd\u7c7b\u578b\u5206\u7ec4\u53c2\u6570\u5e76\u57fa\u4e8e\u5386\u53f2\u884c\u4e3a\u8fdb\u884cEMA\u8c03\u6574\uff0c\u89e3\u51b3\u4f20\u7edf\u5168\u5c40\u88c1\u526a\u4e2d\u7684\u68af\u5ea6\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2aLLM\u5fae\u8c03\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eLoRA\u548c\u5168\u53c2\u6570\u5fae\u8c03\u3002", "motivation": "\u4f20\u7edf\u5168\u5c40\u68af\u5ea6\u88c1\u526a\u5047\u8bbe\u6240\u6709\u53c2\u6570\u68af\u5ea6\u540c\u8d28\u5316\uff0c\u4f46\u5b9e\u9645\u4e0a\u4e0d\u540c\u529f\u80fd\u6a21\u5757\u7684\u68af\u5ea6\u884c\u4e3a\u5dee\u5f02\u5f88\u5927\uff0c\u5bfc\u81f4\"\u6ea2\u51fa\u6548\u5e94\"\u2014\u2014\u6ce2\u52a8\u5927\u7684\u53c2\u6570\u4f1a\u4e0d\u5fc5\u8981\u5730\u7f29\u653e\u7a33\u5b9a\u53c2\u6570\uff0c\u5f71\u54cd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "AGGC\u5c06\u53c2\u6570\u6309\u529f\u80fd\u7c7b\u578b\u5206\u7ec4\uff0c\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747(EMA)\u8ddf\u8e2a\u6bcf\u7ec4\u7684\u5386\u53f2\u68af\u5ea6\u884c\u4e3a\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u533a\u95f4\u540c\u65f6\u7f13\u89e3\u68af\u5ea6\u7206\u70b8\u548c\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u65f6\u95f4\u76f8\u5173\u8c03\u5ea6\u673a\u5236\u5e73\u8861\u63a2\u7d22\u4e0e\u6536\u655b\u3002", "result": "\u5728LLaMA 2-7B\u3001Mistral-7B\u548cGemma-7B\u6a21\u578b\u4e0a\uff0cAGGC\u4e00\u81f4\u4f18\u4e8eLoRA\u5e76\u7ecf\u5e38\u8d85\u8d8a\u5168\u53c2\u6570\u5fae\u8c03\u3002\u5728GSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMistral-7B\u4f7f\u7528AGGC\u8fbe\u523072.93%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7LoRA\u768469.5%\u3002AGGC\u8fd8\u80fd\u6709\u6548\u7a33\u5b9aRLVR\u8bad\u7ec3\uff0c\u63d0\u5347Qwen 2.5\u548cLlama 3.2\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "AGGC\u901a\u8fc7\u6a21\u5757\u5316\u81ea\u9002\u5e94\u88c1\u526a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u68af\u5ea6\u88c1\u526a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u68af\u5ea6\u5f02\u8d28\u6027\u95ee\u9898\u3002\u5176\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u540e\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u5f00\u9500\u6781\u5c0f\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.12970", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12970", "abs": "https://arxiv.org/abs/2601.12970", "authors": ["Mauro Marchese", "Musa Furkan Keskin", "Henk Wymeersch", "Pietro Savazzi"], "title": "6G OFDM Communications with High Mobility Transceivers and Scatterers via Angle-Domain Processing and Deep Learning", "comment": "Accepted for presentation at IEEE International Conference on Communications (ICC) 2026", "summary": "High-mobility communications, which are crucial for next-generation wireless systems, cause the orthogonal frequency division multiplexing (OFDM) waveform to suffer from strong intercarrier interference (ICI) due to the Doppler effect. In this work, we propose a novel receiver architecture for OFDM that leverages the angular domain to separate multipaths. A block-type pilot is sent to estimate direction-of-arrivals (DoAs), propagation delays, and channel gains of the multipaths. Subsequently, a decision-directed (DD) approach is employed to estimate and iteratively refine the Dopplers. Two different approaches are investigated to provide initial Doppler estimates: an error vector magnitude (EVM)-based method and a deep learning (DL)-based method. Simulation results reveal that the DL-based approach allows for constant bit error rate (BER) performance up to the maximum 6G speed of 1000 km/h.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u89d2\u5ea6\u57df\u5206\u79bb\u591a\u5f84\u7684OFDM\u63a5\u6536\u673a\u67b6\u6784\uff0c\u901a\u8fc7\u5757\u72b6\u5bfc\u9891\u4f30\u8ba1\u591a\u5f84\u53c2\u6570\uff0c\u91c7\u7528\u51b3\u7b56\u5bfc\u5411\u65b9\u6cd5\u8fed\u4ee3\u4f18\u5316\u591a\u666e\u52d2\u4f30\u8ba1\uff0cDL\u65b9\u6cd5\u53ef\u57281000km/h\u901f\u5ea6\u4e0b\u4fdd\u6301\u6052\u5b9aBER\u6027\u80fd", "motivation": "\u9ad8\u901f\u79fb\u52a8\u901a\u4fe1\u5bfc\u81f4OFDM\u6ce2\u5f62\u56e0\u591a\u666e\u52d2\u6548\u5e94\u4ea7\u751f\u5f3a\u8f7d\u6ce2\u95f4\u5e72\u6270\uff0c\u9700\u8981\u89e3\u51b3\u9ad8\u901f\u573a\u666f\u4e0b\u7684ICI\u95ee\u9898", "method": "\u63d0\u51fa\u65b0\u9896\u63a5\u6536\u673a\u67b6\u6784\uff1a1) \u53d1\u9001\u5757\u72b6\u5bfc\u9891\u4f30\u8ba1\u591a\u5f84\u7684\u5230\u8fbe\u65b9\u5411\u3001\u4f20\u64ad\u65f6\u5ef6\u548c\u4fe1\u9053\u589e\u76ca\uff1b2) \u91c7\u7528\u51b3\u7b56\u5bfc\u5411\u65b9\u6cd5\u4f30\u8ba1\u5e76\u8fed\u4ee3\u4f18\u5316\u591a\u666e\u52d2\uff1b3) \u7814\u7a76\u4e24\u79cd\u521d\u59cb\u591a\u666e\u52d2\u4f30\u8ba1\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8bef\u5dee\u5411\u91cf\u5e45\u5ea6\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u5728\u6700\u9ad81000km/h\u76846G\u901f\u5ea6\u4e0b\u4fdd\u6301\u6052\u5b9a\u7684\u8bef\u7801\u7387\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u63a5\u6536\u673a\u67b6\u6784\u80fd\u6709\u6548\u89e3\u51b3\u9ad8\u901f\u79fb\u52a8\u901a\u4fe1\u4e2d\u7684ICI\u95ee\u9898\uff0c\u7279\u522b\u662fDL\u65b9\u6cd5\u5728\u6781\u7aef\u9ad8\u901f\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02"}}
{"id": "2601.14173", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14173", "abs": "https://arxiv.org/abs/2601.14173", "authors": ["Paris A. Karakasis", "Nicholas D. Sidiropoulos"], "title": "Penalizing Localized Dirichlet Energies in Low Rank Tensor Products", "comment": "19 pages", "summary": "We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.", "AI": {"tldr": "TPBS\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u901a\u8fc7\u5c40\u90e8Dirichlet\u80fd\u91cf\u6b63\u5219\u5316\u89e3\u51b3\u5168\u5c40\u6b63\u5219\u5316\u5931\u6548\u95ee\u9898\uff0c\u5728\u8fc7\u62df\u5408\u60c5\u51b5\u4e0b\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "\u7814\u7a76\u4f4e\u79e9\u5f20\u91cf\u79efB\u6837\u6761\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u7d22Dirichlet\u80fd\u91cf\u4f5c\u4e3a\u5e73\u6ed1\u5ea6\u5ea6\u91cf\uff0c\u53d1\u73b0\u5168\u5c40Dirichlet\u80fd\u91cf\u6b63\u5219\u5316\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8bad\u7ec3\u70b9\u5468\u56f4\u5c0f\u8d85\u7acb\u65b9\u4f53\u5b9a\u4e49\u7684\u5c40\u90e8Dirichlet\u80fd\u91cf\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684TPBS\u6a21\u578b\uff0c\u5f15\u5165\u4e24\u79cd\u4ece\u4e0d\u5b8c\u6574\u6837\u672c\u8fdb\u884c\u63a8\u65ad\u7684\u4f30\u8ba1\u5668\u3002", "result": "TPBS\u6a21\u578b\u5728\u5927\u591a\u6570\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u8fc7\u62df\u5408\u60c5\u51b5\u4e0b\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\uff0c\u5176\u4ed6\u60c5\u51b5\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002TPBS\u6a21\u578b\u5bf9\u8fc7\u62df\u5408\u66f4\u9c81\u68d2\uff0c\u80fd\u6301\u7eed\u4ece\u6b63\u5219\u5316\u4e2d\u53d7\u76ca\uff0c\u800c\u795e\u7ecf\u7f51\u7edc\u5bf9\u8fc7\u62df\u5408\u66f4\u654f\u611f\uff0c\u5229\u7528\u6b63\u5219\u5316\u7684\u6548\u679c\u8f83\u5dee\u3002", "conclusion": "TPBS\u6a21\u578b\u662f\u56de\u5f52\u4efb\u52a1\u4e2d\u7a33\u5065\u7684\u9009\u62e9\uff0c\u7279\u522b\u662f\u5728\u8fc7\u62df\u5408\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5c40\u90e8Dirichlet\u80fd\u91cf\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5168\u5c40\u6b63\u5219\u5316\u5931\u6548\u7684\u95ee\u9898\u3002"}}
{"id": "2601.11880", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11880", "abs": "https://arxiv.org/abs/2601.11880", "authors": ["Yingxiao Zhang", "Jiaxin Duan", "Junfu Zhang", "Ke Feng"], "title": "TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers for Treasury Futures", "comment": null, "summary": "Diffusion Transformers (DiT) have achieved milestones in synthesizing financial time-series data, such as stock prices and order flows. However, their performance in synthesizing treasury futures data is still underexplored. This work emphasizes the characteristics of treasury futures data, including its low volume, market dependencies, and the grouped correlations among multivariables. To overcome these challenges, we propose TF-CoDiT, the first DiT framework for language-controlled treasury futures synthesis. To facilitate low-data learning, TF-CoDiT adapts the standard DiT by transforming multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient matrices. A U-shape VAE is proposed to encode cross-channel dependencies hierarchically into a latent variable and bridge the latent and DWT spaces through decoding, thereby enabling latent diffusion generation. To derive prompts that cover essential conditions, we introduce the Financial Market Attribute Protocol (FinMAP) - a multi-level description system that standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic indicators from 7/8 perspectives. In our experiments, we gather four types of treasury futures data covering the period from 2015 to 2025, and define data synthesis tasks with durations ranging from one week to four months. Extensive evaluations demonstrate that TF-CoDiT can produce highly authentic data with errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies evidence the robustness of TF-CoDiT across contracts and temporal horizons.", "AI": {"tldr": "TF-CoDiT\uff1a\u9996\u4e2a\u7528\u4e8e\u8bed\u8a00\u63a7\u5236\u56fd\u503a\u671f\u8d27\u5408\u6210\u7684\u6269\u6563Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u548cU\u578bVAE\u5904\u7406\u4f4e\u6570\u636e\u91cf\u5b66\u4e60\uff0c\u5f15\u5165\u91d1\u878d\u5e02\u573a\u5c5e\u6027\u534f\u8bae\u751f\u6210\u63d0\u793a\uff0c\u5728\u56fd\u503a\u671f\u8d27\u6570\u636e\u5408\u6210\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u6269\u6563Transformer\u5728\u80a1\u7968\u4ef7\u683c\u548c\u8ba2\u5355\u6d41\u7b49\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u56fd\u503a\u671f\u8d27\u6570\u636e\u5408\u6210\u65b9\u9762\u7684\u6027\u80fd\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u56fd\u503a\u671f\u8d27\u6570\u636e\u5177\u6709\u4f4e\u4ea4\u6613\u91cf\u3001\u5e02\u573a\u4f9d\u8d56\u6027\u548c\u591a\u53d8\u91cf\u5206\u7ec4\u76f8\u5173\u6027\u7b49\u72ec\u7279\u7279\u5f81\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTF-CoDiT\u6846\u67b6\uff1a1\uff09\u5c06\u591a\u901a\u90531-D\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u7cfb\u6570\u77e9\u9635\u4ee5\u4fc3\u8fdb\u4f4e\u6570\u636e\u5b66\u4e60\uff1b2\uff09\u8bbe\u8ba1U\u578bVAE\u5206\u5c42\u7f16\u7801\u8de8\u901a\u9053\u4f9d\u8d56\u5230\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u89e3\u7801\u6865\u63a5\u6f5c\u5728\u7a7a\u95f4\u548cDWT\u7a7a\u95f4\uff0c\u5b9e\u73b0\u6f5c\u5728\u6269\u6563\u751f\u6210\uff1b3\uff09\u5f15\u5165\u91d1\u878d\u5e02\u573a\u5c5e\u6027\u534f\u8bae\uff08FinMAP\uff09\u4f5c\u4e3a\u591a\u7ea7\u63cf\u8ff0\u7cfb\u7edf\uff0c\u4ece7/8\u4e2a\u89d2\u5ea6\u8bc6\u522b17/23\u4e2a\u7ecf\u6d4e\u6307\u6807\uff0c\u6807\u51c6\u5316\u5e02\u573a\u52a8\u6001\u4ee5\u751f\u6210\u63d0\u793a\u3002", "result": "\u57282015-2025\u5e74\u56db\u79cd\u56fd\u503a\u671f\u8d27\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u5408\u6210\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u4ece\u4e00\u5468\u5230\u56db\u4e2a\u6708\u3002TF-CoDiT\u80fd\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u6570\u636e\uff0c\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u8bef\u5dee\u6700\u591a\u4e3aMSE 0.433\u548cMAE 0.453\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u8bc1\u660eTF-CoDiT\u5728\u4e0d\u540c\u5408\u7ea6\u548c\u65f6\u95f4\u8303\u56f4\u5185\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "TF-CoDiT\u662f\u9996\u4e2a\u7528\u4e8e\u8bed\u8a00\u63a7\u5236\u56fd\u503a\u671f\u8d27\u5408\u6210\u7684\u6269\u6563Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5c0f\u6ce2\u53d8\u6362\u3001U\u578bVAE\u7f16\u7801\u548c\u6807\u51c6\u5316\u5e02\u573a\u63cf\u8ff0\u534f\u8bae\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fd\u503a\u671f\u8d27\u6570\u636e\u5408\u6210\u7684\u72ec\u7279\u6311\u6218\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.12982", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.12982", "abs": "https://arxiv.org/abs/2601.12982", "authors": ["Alexandros I. Papadopoulos", "Maria Anna Pistela", "Dimitrios Tyrovolas", "Antonios Lalas", "Konstantinos Votis", "Sotiris Ioannidis", "George K. Karagiannidis", "Christos Liaskos"], "title": "Physics-Aware RIS Codebook Compilation for Near-Field Beam Focusing under Mutual Coupling and Specular Reflections", "comment": "Accepted for presentation in IEEE International Conference on Communications (IEEE ICC 2026)", "summary": "Next-generation wireless networks are envisioned to achieve reliable, low-latency connectivity within environments characterized by strong multipath and severe channel variability. Programmable wireless environments (PWEs) address this challenge by enabling deterministic control of electromagnetic (EM) propagation through software-defined reconfigurable intelligent surfaces (RISs). However, effectively configuring RISs in real time remains a major bottleneck, particularly under near-field conditions where mutual coupling and specular reflections alter the intended response. To overcome this limitation, this paper introduces MATCH, a physics-based codebook compilation algorithm that explicitly accounts for the EM coupling among RIS unit cells and the reflective interactions with surrounding structures, ensuring that the resulting codebooks remain consistent with the physical characteristics of the environment. Finally, MATCH is evaluated under a full-wave simulation framework incorporating mutual coupling and secondary reflections, demonstrating its ability to concentrate scattered energy within the focal region, confirming that physics-consistent, codebook-based optimization constitutes an effective approach for practical and efficient RIS configuration.", "AI": {"tldr": "MATCH\uff1a\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684RIS\u7801\u672c\u7f16\u8bd1\u7b97\u6cd5\uff0c\u8003\u8651\u5355\u5143\u95f4\u7535\u78c1\u8026\u5408\u548c\u73af\u5883\u53cd\u5c04\uff0c\u5728\u8fd1\u573a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u53ef\u7f16\u7a0b\u65e0\u7ebf\u73af\u5883\u914d\u7f6e\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u9700\u8981\u5728\u5f3a\u591a\u5f84\u548c\u4e25\u91cd\u4fe1\u9053\u53d8\u5316\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u4f4e\u5ef6\u8fdf\u8fde\u63a5\u3002\u53ef\u7f16\u7a0b\u65e0\u7ebf\u73af\u5883\u901a\u8fc7RIS\u5b9e\u73b0\u786e\u5b9a\u6027\u7535\u78c1\u4f20\u64ad\u63a7\u5236\uff0c\u4f46\u5728\u5b9e\u65f6\u914d\u7f6eRIS\u65b9\u9762\u5b58\u5728\u74f6\u9888\uff0c\u7279\u522b\u662f\u5728\u8fd1\u573a\u6761\u4ef6\u4e0b\uff0c\u4e92\u8026\u5408\u548c\u955c\u9762\u53cd\u5c04\u4f1a\u6539\u53d8\u9884\u671f\u54cd\u5e94\u3002", "method": "\u63d0\u51faMATCH\u7b97\u6cd5\uff0c\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u7801\u672c\u7f16\u8bd1\u65b9\u6cd5\uff0c\u660e\u786e\u8003\u8651RIS\u5355\u5143\u95f4\u7684\u7535\u78c1\u8026\u5408\u4ee5\u53ca\u4e0e\u73af\u5883\u7ed3\u6784\u7684\u53cd\u5c04\u76f8\u4e92\u4f5c\u7528\uff0c\u786e\u4fdd\u751f\u6210\u7684\u7801\u672c\u4e0e\u73af\u5883\u7269\u7406\u7279\u6027\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5728\u5168\u6ce2\u4eff\u771f\u6846\u67b6\u4e0b\u8bc4\u4f30MATCH\uff0c\u5305\u542b\u4e92\u8026\u5408\u548c\u4e8c\u6b21\u53cd\u5c04\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u5c06\u6563\u5c04\u80fd\u91cf\u96c6\u4e2d\u5728\u7126\u70b9\u533a\u57df\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u7269\u7406\u4e00\u81f4\u7801\u672c\u7684\u4f18\u5316\u662f\u5b9e\u7528\u9ad8\u6548\u7684RIS\u914d\u7f6e\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u7269\u7406\u7684\u7801\u672c\u7f16\u8bd1\u7b97\u6cd5MATCH\u80fd\u591f\u6709\u6548\u89e3\u51b3RIS\u5728\u8fd1\u573a\u6761\u4ef6\u4e0b\u7684\u914d\u7f6e\u95ee\u9898\uff0c\u4e3a\u53ef\u7f16\u7a0b\u65e0\u7ebf\u73af\u5883\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14234", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14234", "abs": "https://arxiv.org/abs/2601.14234", "authors": ["Qiyang Li", "Sergey Levine"], "title": "Q-learning with Adjoint Matching", "comment": "32 pages, 8 figures, 7 tables", "summary": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.", "AI": {"tldr": "QAM\u662f\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f34\u968f\u5339\u914d\u6280\u672f\u89e3\u51b3\u4e86\u8fde\u7eed\u52a8\u4f5cRL\u4e2d\u6269\u6563/\u6d41\u5339\u914d\u7b56\u7565\u4f18\u5316\u7684\u957f\u671f\u6311\u6218\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u5728\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8fde\u7eed\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5bf9\u8868\u8fbe\u6027\u5f3a\u7684\u6269\u6563\u6216\u6d41\u5339\u914d\u7b56\u7565\u8fdb\u884c\u9ad8\u6548\u4f18\u5316\u4e00\u76f4\u662f\u4e2a\u6311\u6218\u3002\u76f4\u63a5\u901a\u8fc7\u591a\u6b65\u53bb\u566a\u8fc7\u7a0b\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u53ea\u4f7f\u7528\u4ef7\u503c\u4fe1\u606f\u4e22\u5f03\u68af\u5ea6\uff0c\u8981\u4e48\u4f9d\u8d56\u8fd1\u4f3c\u65b9\u6cd5\u727a\u7272\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u6216\u5f15\u5165\u504f\u5dee\u3002", "method": "QAM\u91c7\u7528\u4f34\u968f\u5339\u914d\u6280\u672f\uff0c\u8fd9\u662f\u4e00\u79cd\u6700\u8fd1\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u5b83\u5c06\u8bc4\u8bba\u5bb6\u7684\u52a8\u4f5c\u68af\u5ea6\u8f6c\u6362\u4e3a\u9010\u6b65\u76ee\u6807\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u540c\u65f6\u5728\u6700\u4f18\u89e3\u5904\u63d0\u4f9b\u65e0\u504f\u4e14\u8868\u8fbe\u6027\u5f3a\u7684\u7b56\u7565\u3002\u7ed3\u5408\u8bc4\u8bba\u5bb6\u5b66\u4e60\u7684\u65f6\u95f4\u5dee\u5206\u5907\u4efd\u3002", "result": "QAM\u5728\u56f0\u96be\u7684\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e0a\uff0c\u65e0\u8bba\u662f\u79bb\u7ebfRL\u8fd8\u662f\u79bb\u7ebf\u5230\u5728\u7ebfRL\uff0c\u90fd\u6301\u7eed\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "QAM\u901a\u8fc7\u4f34\u968f\u5339\u914d\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u8fde\u7eed\u52a8\u4f5cRL\u4e2d\u6269\u6563/\u6d41\u5339\u914d\u7b56\u7565\u4f18\u5316\u7684\u957f\u671f\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e2\u4fdd\u6301\u7b56\u7565\u8868\u8fbe\u80fd\u529b\u53c8\u907f\u514d\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11883", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11883", "abs": "https://arxiv.org/abs/2601.11883", "authors": ["Chaoqi Jia", "Longkun Guo", "Kewen Liao", "Zhigang Lu", "Chao Chen", "Jason Xue"], "title": "Approximation Algorithm for Constrained $k$-Center Clustering: A Local Search Approach", "comment": "AAAI-26", "summary": "Clustering is a long-standing research problem and a fundamental tool in AI and data analysis. The traditional k-center problem, a fundamental theoretical challenge in clustering, has a best possible approximation ratio of 2, and any improvement to a ratio of 2 - \u03b5 would imply P = NP. In this work, we study the constrained k-center clustering problem, where instance-level cannot-link (CL) and must-link (ML) constraints are incorporated as background knowledge. Although general CL constraints significantly increase the hardness of approximation, previous work has shown that disjoint CL sets permit constant-factor approximations. However, whether local search can achieve such a guarantee in this setting remains an open question. To this end, we propose a novel local search framework based on a transformation to a dominating matching set problem, achieving the best possible approximation ratio of 2. The experimental results on both real-world and synthetic datasets demonstrate that our algorithm outperforms baselines in solution quality.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u652f\u914d\u5339\u914d\u96c6\u95ee\u9898\u8f6c\u6362\u7684\u65b0\u578b\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u89e3\u51b3\u5e26\u7ea6\u675f\u7684k\u4e2d\u5fc3\u805a\u7c7b\u95ee\u9898\uff0c\u8fbe\u5230\u6700\u4f73\u8fd1\u4f3c\u6bd42", "motivation": "\u4f20\u7edfk\u4e2d\u5fc3\u95ee\u9898\u7684\u6700\u4f73\u8fd1\u4f3c\u6bd4\u4e3a2\uff0c\u4efb\u4f55\u6539\u8fdb\u90fd\u4f1a\u5bfc\u81f4P=NP\u3002\u5728\u52a0\u5165\u5b9e\u4f8b\u7ea7\u4e0d\u80fd\u94fe\u63a5(CL)\u548c\u5fc5\u987b\u94fe\u63a5(ML)\u7ea6\u675f\u540e\uff0c\u867d\u7136\u4e0d\u76f8\u4ea4CL\u96c6\u5141\u8bb8\u5e38\u6570\u56e0\u5b50\u8fd1\u4f3c\uff0c\u4f46\u5c40\u90e8\u641c\u7d22\u80fd\u5426\u8fbe\u5230\u6b64\u4fdd\u8bc1\u4ecd\u662f\u5f00\u653e\u95ee\u9898", "method": "\u63d0\u51fa\u57fa\u4e8e\u652f\u914d\u5339\u914d\u96c6\u95ee\u9898\u8f6c\u6362\u7684\u65b0\u578b\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u5c06\u5e26\u7ea6\u675f\u7684k\u4e2d\u5fc3\u805a\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u652f\u914d\u5339\u914d\u96c6\u95ee\u9898\uff0c\u7136\u540e\u5e94\u7528\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5", "result": "\u7b97\u6cd5\u8fbe\u5230\u4e86\u6700\u4f73\u53ef\u80fd\u7684\u8fd1\u4f3c\u6bd42\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u89e3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5c06\u5e26\u7ea6\u675f\u7684k\u4e2d\u5fc3\u805a\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u652f\u914d\u5339\u914d\u96c6\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8be5\u5f00\u653e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u8fd1\u4f3c\u6bd42\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.13001", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13001", "abs": "https://arxiv.org/abs/2601.13001", "authors": ["Wenrui Yu", "Jaron Skovsted Gundersen", "Richard Heusdens", "Qiongxiu Li"], "title": "When Is Distributed Nonlinear Aggregation Private? Optimality and Information-Theoretical Bounds", "comment": null, "summary": "Nonlinear aggregation is central to modern distributed systems, yet its privacy behavior is far less understood than that of linear aggregation. Unlike linear aggregation where mature mechanisms can often suppress information leakage, nonlinear operators impose inherent structural limits on what privacy guarantees are theoretically achievable when the aggregate must be computed exactly. This paper develops a unified information-theoretic framework to characterize privacy leakage in distributed nonlinear aggregation under a joint adversary that combines passive (honest-but-curious) corruption and eavesdropping over communication channels.\n  We cover two broad classes of nonlinear aggregates: order-based operators (maximum/minimum and top-$K$) and robust aggregation (median/quantiles and trimmed mean). We first derive fundamental lower bounds on leakage that hold without sacrificing accuracy, thereby identifying the minimum unavoidable information revealed by the computation and the transcript. We then propose simple yet effective privacy-preserving distributed algorithms, and show that with appropriate randomized initialization and parameter choices, our proposed approaches can attach the derived optimal bounds for the considered operators. Extensive experiments validate the tightness of the bounds and demonstrate that network topology and key algorithmic parameters (including the stepsize) govern the observed leakage in line with the theoretical analysis, yielding actionable guidelines for privacy-preserving nonlinear aggregation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5efa\u7acb\u4e86\u4fe1\u606f\u8bba\u6846\u67b6\u5206\u6790\u5206\u5e03\u5f0f\u975e\u7ebf\u6027\u805a\u5408\u7684\u9690\u79c1\u6cc4\u9732\uff0c\u63a8\u5bfc\u4e86\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u60c5\u51b5\u4e0b\u7684\u57fa\u672c\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u8fbe\u5230\u6700\u4f18\u754c\u7684\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\u3002", "motivation": "\u975e\u7ebf\u6027\u805a\u5408\u5728\u73b0\u4ee3\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u9690\u79c1\u884c\u4e3a\u8fdc\u4e0d\u5982\u7ebf\u6027\u805a\u5408\u88ab\u5145\u5206\u7406\u89e3\u3002\u975e\u7ebf\u6027\u7b97\u5b50\u5bf9\u53ef\u5b9e\u73b0\u7684\u7406\u8bba\u9690\u79c1\u4fdd\u8bc1\u65bd\u52a0\u4e86\u56fa\u6709\u7684\u7ed3\u6784\u9650\u5236\uff0c\u7279\u522b\u662f\u5f53\u9700\u8981\u7cbe\u786e\u8ba1\u7b97\u805a\u5408\u503c\u65f6\u3002", "method": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u4fe1\u606f\u8bba\u6846\u67b6\u6765\u8868\u5f81\u5206\u5e03\u5f0f\u975e\u7ebf\u6027\u805a\u5408\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\uff0c\u6db5\u76d6\u4e24\u7c7b\u975e\u7ebf\u6027\u805a\u5408\uff1a\u57fa\u4e8e\u987a\u5e8f\u7684\u7b97\u5b50\uff08\u6700\u5927\u503c/\u6700\u5c0f\u503c\u548ctop-K\uff09\u548c\u9c81\u68d2\u805a\u5408\uff08\u4e2d\u4f4d\u6570/\u5206\u4f4d\u6570\u548c\u4fee\u526a\u5747\u503c\uff09\u3002\u63a8\u5bfc\u4e86\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u60c5\u51b5\u4e0b\u7684\u57fa\u672c\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u9690\u79c1\u4fdd\u62a4\u5206\u5e03\u5f0f\u7b97\u6cd5\u3002", "result": "\u63a8\u5bfc\u4e86\u9690\u79c1\u6cc4\u9732\u7684\u57fa\u672c\u4e0b\u754c\uff0c\u786e\u5b9a\u4e86\u8ba1\u7b97\u548c\u4f20\u8f93\u4e2d\u4e0d\u53ef\u907f\u514d\u7684\u6700\u5c0f\u4fe1\u606f\u6cc4\u9732\u3002\u63d0\u51fa\u7684\u7b97\u6cd5\u901a\u8fc7\u9002\u5f53\u7684\u968f\u673a\u521d\u59cb\u5316\u548c\u53c2\u6570\u9009\u62e9\uff0c\u80fd\u591f\u8fbe\u5230\u63a8\u5bfc\u51fa\u7684\u6700\u4f18\u754c\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u754c\u7684\u7d27\u81f4\u6027\uff0c\u5e76\u8868\u660e\u7f51\u7edc\u62d3\u6251\u548c\u5173\u952e\u7b97\u6cd5\u53c2\u6570\uff08\u5305\u62ec\u6b65\u957f\uff09\u6309\u7167\u7406\u8bba\u5206\u6790\u63a7\u5236\u89c2\u5bdf\u5230\u7684\u6cc4\u9732\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u975e\u7ebf\u6027\u805a\u5408\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u8bc6\u522b\u4e86\u4e0d\u53ef\u907f\u514d\u7684\u9690\u79c1\u6cc4\u9732\u4e0b\u754c\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7b97\u6cd5\u8bbe\u8ba1\u8fbe\u5230\u8fd9\u4e9b\u6700\u4f18\u754c\uff0c\u4e3a\u5b9e\u9645\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2601.11890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11890", "abs": "https://arxiv.org/abs/2601.11890", "authors": ["Xihe Gu", "Urbashi Mitra", "Tara Javidi"], "title": "From Relative Entropy to Minimax: A Unified Framework for Coverage in MDPs", "comment": null, "summary": "Targeted and deliberate exploration of state--action pairs is essential in reward-free Markov Decision Problems (MDPs). More precisely, different state-action pairs exhibit different degree of importance or difficulty which must be actively and explicitly built into a controlled exploration strategy. To this end, we propose a weighted and parameterized family of concave coverage objectives, denoted by $U_\u03c1$, defined directly over state--action occupancy measures. This family unifies several widely studied objectives within a single framework, including divergence-based marginal matching, weighted average coverage, and worst-case (minimax) coverage. While the concavity of $U_\u03c1$ captures the diminishing return associated with over-exploration, the simple closed form of the gradient of $U_\u03c1$ enables an explicit control to prioritize under-explored state--action pairs. Leveraging this structure, we develop a gradient-based algorithm that actively steers the induced occupancy toward a desired coverage pattern. Moreover, we show that as $\u03c1$ increases, the resulting exploration strategy increasingly emphasizes the least-explored state--action pairs, recovering worst-case coverage behavior in the limit.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u51f9\u8986\u76d6\u76ee\u6807U_\u03c1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u65e0\u5956\u52b1MDP\u4e2d\u8fdb\u884c\u6709\u76ee\u6807\u7684\u63a2\u7d22\uff0c\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u591a\u79cd\u73b0\u6709\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u7b97\u6cd5\u4e3b\u52a8\u5f15\u5bfc\u72b6\u6001-\u52a8\u4f5c\u5360\u7528\u5206\u5e03\u3002", "motivation": "\u5728\u65e0\u5956\u52b1\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u540c\u72b6\u6001-\u52a8\u4f5c\u5bf9\u5177\u6709\u4e0d\u540c\u7684\u91cd\u8981\u6027\u6216\u96be\u5ea6\uff0c\u9700\u8981\u4e3b\u52a8\u4e14\u6709\u76ee\u6807\u7684\u63a2\u7d22\u7b56\u7565\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u5e73\u8861\u63a2\u7d22\u7684\u5e7f\u5ea6\u548c\u6df1\u5ea6\u3002", "method": "\u63d0\u51fa\u52a0\u6743\u53c2\u6570\u5316\u51f9\u8986\u76d6\u76ee\u6807\u65cfU_\u03c1\uff0c\u5b9a\u4e49\u5728\u72b6\u6001-\u52a8\u4f5c\u5360\u7528\u6d4b\u5ea6\u4e0a\u3002\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u57fa\u4e8e\u6563\u5ea6\u7684\u8fb9\u9645\u5339\u914d\u3001\u52a0\u6743\u5e73\u5747\u8986\u76d6\u548c\u6700\u574f\u60c5\u51b5\u8986\u76d6\u3002\u5229\u7528U_\u03c1\u7684\u51f9\u6027\u548c\u68af\u5ea6\u95ed\u5f0f\u89e3\uff0c\u5f00\u53d1\u68af\u5ea6\u7b97\u6cd5\u4e3b\u52a8\u5f15\u5bfc\u5360\u7528\u5206\u5e03\u3002", "result": "U_\u03c1\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u591a\u79cd\u63a2\u7d22\u76ee\u6807\uff0c\u68af\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u5f15\u5bfc\u63a2\u7d22\u7b56\u7565\u3002\u5f53\u03c1\u589e\u5927\u65f6\uff0c\u63a2\u7d22\u7b56\u7565\u8d8a\u6765\u8d8a\u5173\u6ce8\u6700\u5c11\u63a2\u7d22\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u5728\u6781\u9650\u60c5\u51b5\u4e0b\u6062\u590d\u6700\u574f\u60c5\u51b5\u8986\u76d6\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a0\u6743\u51f9\u8986\u76d6\u76ee\u6807\u65cf\u4e3a\u65e0\u5956\u52b1MDP\u4e2d\u7684\u6709\u76ee\u6807\u63a2\u7d22\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u03c1\u7075\u6d3b\u63a7\u5236\u63a2\u7d22\u7b56\u7565\uff0c\u4ece\u5e73\u5747\u8986\u76d6\u5230\u6700\u574f\u60c5\u51b5\u8986\u76d6\u8fde\u7eed\u53d8\u5316\uff0c\u68af\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u5b9e\u73b0\u6240\u9700\u7684\u63a2\u7d22\u6a21\u5f0f\u3002"}}
{"id": "2601.13065", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13065", "abs": "https://arxiv.org/abs/2601.13065", "authors": ["Davide Bergamasco", "Federico Clazzer", "Paolo Casari"], "title": "OTFS-IDMA: An Unsourced Multiple Access Scheme for Doubly-Dispersive Channels", "comment": "6 pages, 5 figures", "summary": "We present an unsourced multiple access (UMAC) scheme tailored to high-mobility wireless channels. The proposed construction is based on orthogonal time frequency space (OTFS) modulation and sparse interleaver division multiple access (IDMA) in the delay-Doppler (DD) domain. The receiver runs a compressive-sensing joint activity-detection and channel estimation process followed by a single-user decoder which harnesses multipath diversity via the maximal-ratio combining (MRC) principle. Numerical results show the potential of DD-based uncoordinated schemes in the presence of double selectivity, while remarking the design tradeoffs and remaining challenges introduced by the proposed design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u9ad8\u79fb\u52a8\u6027\u65e0\u7ebf\u4fe1\u9053\u7684\u65e0\u6e90\u591a\u5740\u63a5\u5165\u65b9\u6848\uff0c\u57fa\u4e8eOTFS\u8c03\u5236\u548c\u7a00\u758f\u4ea4\u7ec7\u5206\u591a\u5740\uff0c\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u5b9e\u73b0\u8054\u5408\u6d3b\u52a8\u68c0\u6d4b\u548c\u4fe1\u9053\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u9ad8\u79fb\u52a8\u6027\u65e0\u7ebf\u4fe1\u9053\u4e2d\u7684\u65e0\u6e90\u591a\u5740\u63a5\u5165\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6848\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e2d\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u9002\u5e94\u65f6\u9891\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u7279\u6027\u7684\u65b0\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6b63\u4ea4\u65f6\u9891\u7a7a\u95f4\u8c03\u5236\u548c\u7a00\u758f\u4ea4\u7ec7\u5206\u591a\u5740\u6280\u672f\uff0c\u5728\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u8bbe\u8ba1\u63a5\u6536\u673a\uff0c\u901a\u8fc7\u538b\u7f29\u611f\u77e5\u5b9e\u73b0\u8054\u5408\u6d3b\u52a8\u68c0\u6d4b\u548c\u4fe1\u9053\u4f30\u8ba1\uff0c\u7136\u540e\u4f7f\u7528\u6700\u5927\u6bd4\u5408\u5e76\u539f\u7406\u7684\u5355\u7528\u6237\u89e3\u7801\u5668\u5229\u7528\u591a\u5f84\u5206\u96c6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u65e0\u534f\u8c03\u65b9\u6848\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u8bbe\u8ba1\u6743\u8861\u548c\u5269\u4f59\u6311\u6218\u3002", "conclusion": "\u57fa\u4e8e\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u7684\u65e0\u6e90\u591a\u5740\u63a5\u5165\u65b9\u6848\u4e3a\u9ad8\u79fb\u52a8\u6027\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u8bbe\u8ba1\u4e2d\u7684\u6743\u8861\u548c\u6311\u6218\u3002"}}
{"id": "2601.11895", "categories": ["cs.LG", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.11895", "abs": "https://arxiv.org/abs/2601.11895", "authors": ["Pareesa Ameneh Golnari", "Adarsh Kumarappan", "Wen Wen", "Xiaoyu Liu", "Gabriel Ryan", "Yuting Sun", "Shengyu Fu", "Elsie Nallipogu"], "title": "DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models", "comment": null, "summary": "DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development.", "AI": {"tldr": "DevBench\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5f00\u53d1\u8005\u9065\u6d4b\u6570\u636e\u7684\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c6\u4e2a\u4efb\u52a1\u7c7b\u522b\uff0c\u51711800\u4e2a\u8bc4\u4f30\u5b9e\u4f8b\uff0c\u65e8\u5728\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u5f00\u53d1\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u751f\u6001\u6548\u5ea6\uff0c\u5bb9\u6613\u53d7\u5230\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\uff0c\u4e14\u65e0\u6cd5\u63d0\u4f9b\u8be6\u7ec6\u7684\u8bca\u65ad\u4fe1\u606f\u3002\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u5f00\u53d1\u8005\u9065\u6d4b\u6570\u636e\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30LLM\u5728\u5b9e\u9645\u5f00\u53d1\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4ece\u771f\u5b9e\u5f00\u53d1\u8005\u9065\u6d4b\u6570\u636e\u4e2d\u63d0\u53d66\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c6\u4e2a\u4efb\u52a1\u7c7b\u522b\uff08\u5982API\u4f7f\u7528\u3001\u4ee3\u7801\u76ee\u7684\u7406\u89e3\u7b49\uff09\uff0c\u6784\u5efa1800\u4e2a\u8bc4\u4f30\u5b9e\u4f8b\u3002\u91c7\u7528\u529f\u80fd\u6b63\u786e\u6027\u3001\u76f8\u4f3c\u6027\u6307\u6807\u548cLLM\u8bc4\u5224\u76f8\u7ed3\u5408\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u7528\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "result": "\u8bc4\u4f30\u4e869\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u8bed\u6cd5\u7cbe\u5ea6\u3001\u8bed\u4e49\u63a8\u7406\u548c\u5b9e\u9645\u6548\u7528\u65b9\u9762\u7684\u5dee\u5f02\u3002\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u548c\u6539\u8fdb\u3002", "conclusion": "DevBench\u63d0\u4f9b\u4e86\u6bd4\u5176\u4ed6\u57fa\u51c6\u6d4b\u8bd5\u66f4\u8be6\u7ec6\u7684\u8bca\u65ad\u4fe1\u606f\uff0c\u5bf9\u5b9e\u9645\u90e8\u7f72\u548c\u9488\u5bf9\u6027\u6a21\u578b\u5f00\u53d1\u81f3\u5173\u91cd\u8981\u3002\u5b83\u5f3a\u8c03\u4e86\u751f\u6001\u6548\u5ea6\uff0c\u907f\u514d\u4e86\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\uff0c\u80fd\u591f\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u6539\u8fdb\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2601.13157", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13157", "abs": "https://arxiv.org/abs/2601.13157", "authors": ["Hang Zou", "Bohao Wang", "Yu Tian", "Lina Bariah", "Chongwen Huang", "Samson Lasaulce", "M\u00e9rouane Debbah"], "title": "Seeing Radio: From Zero RF Priors to Explainable Modulation Recognition with Vision Language Models", "comment": null, "summary": "The rise of vision language models (VLMs) paves a new path for radio frequency (RF) perception. Rather than designing task-specific neural receivers, we ask if VLMs can learn to recognize modulations when RF waveforms are expressed as images. In this work, we find that they can. In specific, in this paper, we introduce a practical pipeline for converting complex IQ streams into visually interpretable inputs, hence, enabling general-purpose VLMs to classify modulation schemes without changing their underlying design. Building on this, we construct an RF visual question answering (VQA) benchmark framework that covers 57 classes across major families of analog/digital modulations with three complementary image modes, namely, (i) short \\emph{time-series} IQ segments represented as real/imaginary traces, (ii) magnitude-only \\emph{spectrograms}, and (iii) \\emph{joint} representations that pair spectrograms with a synchronized time-series waveforms. We design uniform zero-shot and few-shot prompts for both class-level and family-level evaluations. Our finetuned VLMs with these images achieve competitive accuracy of $90\\%$ compared to $10\\%$ of the base models. Furthermore, the fine-tuned VLMs show robust performance under noise and demonstrate high generalization performance to unseen modulation types, without relying on RF-domain priors or specialized architectures. The obtained results show that combining RF-to-image conversion with promptable VLMs provides a scalable and practical foundation for RF-aware AI systems in future 6G networks.", "AI": {"tldr": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5c04\u9891\u8c03\u5236\u8bc6\u522b\uff0c\u901a\u8fc7\u5c06IQ\u4fe1\u53f7\u8f6c\u6362\u4e3a\u56fe\u50cf\uff0c\u65e0\u9700\u4e13\u95e8\u8bbe\u8ba1\u5c04\u9891\u63a5\u6536\u5668\uff0c\u5b9e\u73b0\u4e8690%\u7684\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u5c04\u9891\u611f\u77e5\u9700\u8981\u8bbe\u8ba1\u4e13\u95e8\u7684\u795e\u7ecf\u7f51\u7edc\u63a5\u6536\u5668\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\u4e3a\u5c04\u9891\u611f\u77e5\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u7814\u7a76\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5c06\u5c04\u9891\u6ce2\u5f62\u8868\u793a\u4e3a\u56fe\u50cf\uff0c\u8ba9\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u8c03\u5236\u65b9\u5f0f", "method": "\u63d0\u51fa\u5c06\u590d\u6742IQ\u6d41\u8f6c\u6362\u4e3a\u89c6\u89c9\u53ef\u89e3\u91ca\u8f93\u5165\u7684\u5b9e\u7528\u7ba1\u9053\uff0c\u6784\u5efa\u5305\u542b57\u7c7b\u6a21\u62df/\u6570\u5b57\u8c03\u5236\u7684RF\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u79cd\u4e92\u8865\u56fe\u50cf\u6a21\u5f0f\uff1a\u65f6\u57dfIQ\u6bb5\u3001\u5e45\u5ea6\u8c31\u56fe\u548c\u8054\u5408\u8868\u793a\u3002\u8bbe\u8ba1\u7edf\u4e00\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u8fdb\u884c\u7c7b\u522b\u7ea7\u548c\u5bb6\u65cf\u7ea7\u8bc4\u4f30", "result": "\u5fae\u8c03\u540e\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fbe\u523090%\u7684\u51c6\u786e\u7387\uff08\u57fa\u7840\u6a21\u578b\u4ec510%\uff09\uff0c\u5728\u566a\u58f0\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u5bf9\u672a\u89c1\u8c03\u5236\u7c7b\u578b\u5177\u6709\u9ad8\u6cdb\u5316\u6027\u80fd\uff0c\u65e0\u9700\u5c04\u9891\u9886\u57df\u5148\u9a8c\u77e5\u8bc6\u6216\u4e13\u95e8\u67b6\u6784", "conclusion": "RF\u5230\u56fe\u50cf\u7684\u8f6c\u6362\u4e0e\u53ef\u63d0\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u4e2d\u7684\u5c04\u9891\u611f\u77e5AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u57fa\u7840"}}
{"id": "2601.13201", "categories": ["eess.SP", "cs.ET", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13201", "abs": "https://arxiv.org/abs/2601.13201", "authors": ["Konstantinos D. Katsanos", "George C. Alexandropoulos"], "title": "Decentralized Cooperative Beamforming for BDRIS-Assisted Cell-Free MIMO OFDM Systems", "comment": "13 pages, 6 figures, submitted to an IEEE Transactions journal", "summary": "In this paper, a wideband cell-free multi-stream multi-user Multiple-Input Multiple-Output (MIMO) Orthogonal Frequency Division Multiplexing (OFDM) system is considered operating within a smart wireless environment enabled by multiple Beyond Diagonal Reconfigurable Intelligent Surfaces (BDRISs). A novel decentralized active and passive beamforming framework, robust to imperfect channel state availability and with minimal cooperation among the system's multiple Base Stations (BSs) for deciding the final configurations of the shared BDRISs, is proposed, which aims to substantially reduce the overhead inherent in centralized solutions necessitating a central processing unit of high computational power. By considering a Dynamic Group-Connected (DGC) BDRIS architecture with frequency-selective responses per unit element, we formulate the system's sum-rate maximization problem with respect to the tunable capacitances and permutation matrices of the BDRISs as well as the precoding matrices of the BSs, which is solved via successive concave approximation and alternating projections as well as consensus-based updates for the BDRISs' design. Through extensive simulation results, it is showcased that the proposed robust decentralized cooperative approach with diverse BDRIS architectures outperforms non-cooperation benchmarks. It is also demonstrated that the considered DGC BDRIS architecture is able to provide sum-rate performance gains sufficiently close to the more complex fully-connected BDRIS structure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5bbd\u5e26\u65e0\u5c0f\u533a\u591a\u6d41\u591a\u7528\u6237MIMO-OFDM\u7cfb\u7edf\u7684\u9c81\u68d2\u53bb\u4e2d\u5fc3\u5316\u4e3b\u52a8\u548c\u88ab\u52a8\u6ce2\u675f\u8d4b\u5f62\u6846\u67b6\uff0c\u91c7\u7528\u52a8\u6001\u7ec4\u8fde\u63a5BDRIS\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f18\u5316\u663e\u8457\u964d\u4f4e\u96c6\u4e2d\u5f0f\u5904\u7406\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u6ce2\u675f\u8d4b\u5f62\u65b9\u6848\u9700\u8981\u9ad8\u8ba1\u7b97\u80fd\u529b\u7684\u4e2d\u5171\u5904\u7406\u5355\u5143\uff0c\u5b58\u5728\u663e\u8457\u7684\u5f00\u9500\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u51cf\u5c11\u57fa\u7ad9\u95f4\u534f\u4f5c\u9700\u6c42\uff0c\u540c\u65f6\u5e94\u5bf9\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0d\u5b8c\u7f8e\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u52a8\u6001\u7ec4\u8fde\u63a5BDRIS\u67b6\u6784\uff0c\u8003\u8651\u9891\u7387\u9009\u62e9\u6027\u54cd\u5e94\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316BDRIS\u7684\u53ef\u8c03\u7535\u5bb9\u3001\u7f6e\u6362\u77e9\u9635\u548c\u57fa\u7ad9\u7684\u9884\u7f16\u7801\u77e9\u9635\uff0c\u4f7f\u7528\u8fde\u7eed\u51f9\u903c\u8fd1\u3001\u4ea4\u66ff\u6295\u5f71\u548c\u57fa\u4e8e\u5171\u8bc6\u7684\u66f4\u65b0\u7b97\u6cd5\u89e3\u51b3\u7cfb\u7edf\u548c\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u9c81\u68d2\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u65b9\u6cd5\u5728\u4e0d\u540cBDRIS\u67b6\u6784\u4e0b\u5747\u4f18\u4e8e\u975e\u534f\u4f5c\u57fa\u51c6\u65b9\u6848\u3002\u52a8\u6001\u7ec4\u8fde\u63a5BDRIS\u67b6\u6784\u80fd\u591f\u63d0\u4f9b\u63a5\u8fd1\u66f4\u590d\u6742\u5168\u8fde\u63a5BDRIS\u7ed3\u6784\u7684\u548c\u901f\u7387\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "\u63d0\u51fa\u7684\u53bb\u4e2d\u5fc3\u5316\u6ce2\u675f\u8d4b\u5f62\u6846\u67b6\u6709\u6548\u964d\u4f4e\u4e86\u7cfb\u7edf\u5f00\u9500\uff0c\u52a8\u6001\u7ec4\u8fde\u63a5BDRIS\u67b6\u6784\u5728\u6027\u80fd\u548c\u590d\u6742\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u667a\u80fd\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u5bbd\u5e26\u65e0\u5c0f\u533aMIMO-OFDM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11924", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11924", "abs": "https://arxiv.org/abs/2601.11924", "authors": ["Ming Shi"], "title": "Communication-Corruption Coupling and Verification in Cooperative Multi-Objective Bandits", "comment": null, "summary": "We study cooperative stochastic multi-armed bandits with vector-valued rewards under adversarial corruption and limited verification. In each of $T$ rounds, each of $N$ agents selects an arm, the environment generates a clean reward vector, and an adversary perturbs the observed feedback subject to a global corruption budget $\u0393$. Performance is measured by team regret under a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $\u03c6$, covering linear, Chebyshev, and smooth monotone utilities. Our main contribution is a communication-corruption coupling: we show that a fixed environment-side budget $\u0393$ can translate into an effective corruption level ranging from $\u0393$ to $N\u0393$, depending on whether agents share raw samples, sufficient statistics, or only arm recommendations. We formalize this via a protocol-induced multiplicity functional and prove regret bounds parameterized by the resulting effective corruption. As corollaries, raw-sample sharing can suffer an $N$-fold larger additive corruption penalty, whereas summary sharing and recommendation-only sharing preserve an unamplified $O(\u0393)$ term and achieve centralized-rate team regret. We further establish information-theoretic limits, including an unavoidable additive $\u03a9(\u0393)$ penalty and a high-corruption regime $\u0393=\u0398(NT)$ where sublinear regret is impossible without clean information. Finally, we characterize how a global budget $\u03bd$ of verified observations restores learnability. That is, verification is necessary in the high-corruption regime, and sufficient once it crosses the identification threshold, with certified sharing enabling the team's regret to become independent of $\u0393$.", "AI": {"tldr": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u8003\u8651\u5411\u91cf\u5956\u52b1\u3001\u5bf9\u6297\u6027\u8150\u8d25\u548c\u6709\u9650\u9a8c\u8bc1\u3002\u63ed\u793a\u4e86\u901a\u4fe1\u534f\u8bae\u5982\u4f55\u5f71\u54cd\u8150\u8d25\u7684\u6709\u6548\u6c34\u5e73\uff0c\u4ece\u0393\u5230N\u0393\u4e0d\u7b49\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5e94\u7684\u9057\u61be\u754c\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1-\u8150\u8d25\u8026\u5408\u95ee\u9898\u3002\u5728\u5bf9\u6297\u6027\u8150\u8d25\u73af\u5883\u4e0b\uff0c\u4e0d\u540c\u7684\u901a\u4fe1\u534f\u8bae\uff08\u5171\u4eab\u539f\u59cb\u6837\u672c\u3001\u7edf\u8ba1\u6458\u8981\u6216\u4ec5\u63a8\u8350\uff09\u4f1a\u5bfc\u81f4\u8150\u8d25\u5f71\u54cd\u88ab\u4e0d\u540c\u7a0b\u5ea6\u5730\u653e\u5927\uff0c\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u5f71\u54cd\u5e76\u8bbe\u8ba1\u6709\u6548\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u534f\u8bae\u8bf1\u5bfc\u7684\u591a\u91cd\u6027\u51fd\u6570\u6765\u5f62\u5f0f\u5316\u901a\u4fe1-\u8150\u8d25\u8026\u5408\u3002\u5206\u6790\u4e09\u79cd\u901a\u4fe1\u534f\u8bae\uff1a\u539f\u59cb\u6837\u672c\u5171\u4eab\u3001\u7edf\u8ba1\u6458\u8981\u5171\u4eab\u548c\u4ec5\u63a8\u8350\u5171\u4eab\u3002\u5efa\u7acb\u53c2\u6570\u5316\u6709\u6548\u8150\u8d25\u6c34\u5e73\u7684\u9057\u61be\u754c\uff0c\u5e76\u7814\u7a76\u9a8c\u8bc1\u89c2\u6d4b\u5982\u4f55\u6062\u590d\u53ef\u5b66\u4e60\u6027\u3002", "result": "\u539f\u59cb\u6837\u672c\u5171\u4eab\u53ef\u80fd\u906d\u53d7N\u500d\u7684\u8150\u8d25\u60e9\u7f5a\uff0c\u800c\u6458\u8981\u5171\u4eab\u548c\u4ec5\u63a8\u8350\u5171\u4eab\u4fdd\u6301O(\u0393)\u7684\u672a\u653e\u5927\u9879\uff0c\u8fbe\u5230\u4e2d\u5fc3\u5316\u901f\u7387\u7684\u56e2\u961f\u9057\u61be\u3002\u5efa\u7acb\u4e86\u4fe1\u606f\u8bba\u6781\u9650\uff1a\u4e0d\u53ef\u907f\u514d\u7684\u03a9(\u0393)\u60e9\u7f5a\uff0c\u4ee5\u53ca\u9ad8\u8150\u8d25\u0393=\u0398(NT)\u4e0b\u6ca1\u6709\u5e72\u51c0\u4fe1\u606f\u5219\u65e0\u6cd5\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002\u9a8c\u8bc1\u89c2\u6d4b\u53ef\u4ee5\u6062\u590d\u53ef\u5b66\u4e60\u6027\u3002", "conclusion": "\u901a\u4fe1\u534f\u8bae\u663e\u8457\u5f71\u54cd\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b66\u4e60\u4e2d\u5bf9\u8150\u8d25\u7684\u9c81\u68d2\u6027\u3002\u6458\u8981\u5171\u4eab\u548c\u4ec5\u63a8\u8350\u5171\u4eab\u6bd4\u539f\u59cb\u6837\u672c\u5171\u4eab\u66f4\u9c81\u68d2\u3002\u5728\u9ad8\u8150\u8d25\u73af\u5883\u4e0b\uff0c\u9a8c\u8bc1\u662f\u5fc5\u8981\u7684\uff0c\u4e00\u65e6\u8d85\u8fc7\u8bc6\u522b\u9608\u503c\uff0c\u9a8c\u8bc1\u5c31\u8db3\u591f\u4e86\uff0c\u901a\u8fc7\u8ba4\u8bc1\u5171\u4eab\u53ef\u4ee5\u4f7f\u56e2\u961f\u9057\u61be\u72ec\u7acb\u4e8e\u0393\u3002"}}
{"id": "2601.13204", "categories": ["eess.SP", "cs.IT", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.13204", "abs": "https://arxiv.org/abs/2601.13204", "authors": ["Yanfeng Zhang", "Xi'an Fan", "Jinkai Zheng", "Xiaoye Jing", "Weiwei Yang", "Xu Zhu"], "title": "Hierarchical Sparse Vector Transmission for Ultra Reliable and Low Latency Communications", "comment": null, "summary": "Sparse vector transmission (SVT) is a promising candidate technology for achieving ultra-reliable low-latency communication (URLLC). In this paper, a hierarchical SVT scheme is proposed for multi-user URLLC scenarios. The hierarchical SVT scheme partitions the transmitted bits into common and private parts. The common information is conveyed by the indices of non-zero sections in a sparse vector, while each user's private information is embedded into non-zero blocks with specific block lengths. At the receiver, the common bits are first recovered from the detected non-zero sections, followed by user-specific private bits decoding based on the corresponding non-zero block indices. Simulation results show the proposed scheme outperforms state-of-the-art SVT schemes in terms of block error rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u7528\u6237URLLC\u573a\u666f\u7684\u5206\u5c42\u7a00\u758f\u5411\u91cf\u4f20\u8f93\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u4f20\u8f93\u6bd4\u7279\u5206\u4e3a\u516c\u5171\u548c\u79c1\u6709\u90e8\u5206\uff0c\u5206\u522b\u7528\u7a00\u758f\u5411\u91cf\u7684\u975e\u96f6\u6bb5\u7d22\u5f15\u548c\u7279\u5b9a\u5757\u957f\u5ea6\u7684\u975e\u96f6\u5757\u5d4c\u5165\u4fe1\u606f\uff0c\u5728\u63a5\u6536\u7aef\u5148\u6062\u590d\u516c\u5171\u6bd4\u7279\u518d\u89e3\u7801\u7528\u6237\u79c1\u6709\u6bd4\u7279\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SVT\u65b9\u6848\u3002", "motivation": "\u7a00\u758f\u5411\u91cf\u4f20\u8f93\u662f\u5b9e\u73b0\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u7684\u6709\u524d\u666f\u6280\u672f\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u5728\u591a\u7528\u6237\u573a\u666f\u4e0b\u7684\u6027\u80fd\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u3002\u672c\u6587\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u9002\u7528\u4e8e\u591a\u7528\u6237URLLC\u573a\u666f\u7684\u6539\u8fdbSVT\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5206\u5c42SVT\u65b9\u6848\uff1a1) \u5c06\u4f20\u8f93\u6bd4\u7279\u5212\u5206\u4e3a\u516c\u5171\u90e8\u5206\u548c\u79c1\u6709\u90e8\u5206\uff1b2) \u516c\u5171\u4fe1\u606f\u901a\u8fc7\u7a00\u758f\u5411\u91cf\u4e2d\u7684\u975e\u96f6\u6bb5\u7d22\u5f15\u4f20\u8f93\uff1b3) \u6bcf\u4e2a\u7528\u6237\u7684\u79c1\u6709\u4fe1\u606f\u5d4c\u5165\u5230\u5177\u6709\u7279\u5b9a\u5757\u957f\u5ea6\u7684\u975e\u96f6\u5757\u4e2d\uff1b4) \u63a5\u6536\u7aef\u5148\u68c0\u6d4b\u975e\u96f6\u6bb5\u6062\u590d\u516c\u5171\u6bd4\u7279\uff0c\u518d\u6839\u636e\u5bf9\u5e94\u7684\u975e\u96f6\u5757\u7d22\u5f15\u89e3\u7801\u7528\u6237\u79c1\u6709\u6bd4\u7279\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5206\u5c42SVT\u65b9\u6848\u5728\u5757\u9519\u8bef\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684SVT\u65b9\u6848\u3002", "conclusion": "\u5206\u5c42SVT\u65b9\u6848\u4e3a\u591a\u7528\u6237URLLC\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u901a\u4fe1\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u79bb\u516c\u5171\u548c\u79c1\u6709\u4fe1\u606f\u4f20\u8f93\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u5728\u5757\u9519\u8bef\u7387\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2601.11942", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.11942", "abs": "https://arxiv.org/abs/2601.11942", "authors": ["Qingyu Meng", "Yangshuai Wang"], "title": "Trainability-Oriented Hybrid Quantum Regression via Geometric Preconditioning and Curriculum Optimization", "comment": null, "summary": "Quantum neural networks (QNNs) have attracted growing interest for scientific machine learning, yet in regression settings they often suffer from limited trainability under noisy gradients and ill-conditioned optimization. We propose a hybrid quantum-classical regression framework designed to mitigate these bottlenecks. Our model prepends a lightweight classical embedding that acts as a learnable geometric preconditioner, reshaping the input representation to better condition a downstream variational quantum circuit. Building on this architecture, we introduce a curriculum optimization protocol that progressively increases circuit depth and transitions from SPSA-based stochastic exploration to Adam-based gradient fine-tuning. We evaluate the approach on PDE-informed regression benchmarks and standard regression datasets under a fixed training budget in a simulator setting. Empirically, the proposed framework consistently improves over pure QNN baselines and yields more stable convergence in data-limited regimes. We further observe reduced structured errors that are visually correlated with oscillatory components on several scientific benchmarks, suggesting that geometric preconditioning combined with curriculum training is a practical approach for stabilizing quantum regression.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u5178\u5d4c\u5165\u5c42\u4f5c\u4e3a\u51e0\u4f55\u9884\u5904\u7406\u5668\u6539\u5584\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u7ed3\u5408\u8bfe\u7a0b\u4f18\u5316\u534f\u8bae\u63d0\u5347\u6027\u80fd", "motivation": "\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u9762\u4e34\u68af\u5ea6\u566a\u58f0\u548c\u4f18\u5316\u6761\u4ef6\u4e0d\u826f\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u65b9\u6cd5", "method": "1) \u8f7b\u91cf\u7ea7\u7ecf\u5178\u5d4c\u5165\u5c42\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7684\u51e0\u4f55\u9884\u5904\u7406\u5668\uff1b2) \u8bfe\u7a0b\u4f18\u5316\u534f\u8bae\uff1a\u9010\u6b65\u589e\u52a0\u7535\u8def\u6df1\u5ea6\uff0c\u4eceSPSA\u968f\u673a\u63a2\u7d22\u8fc7\u6e21\u5230Adam\u68af\u5ea6\u5fae\u8c03", "result": "\u5728PDE\u56de\u5f52\u57fa\u51c6\u548c\u6807\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u7eafQNN\u57fa\u7ebf\u8868\u73b0\u66f4\u597d\uff0c\u6536\u655b\u66f4\u7a33\u5b9a\uff0c\u7ed3\u6784\u5316\u8bef\u5dee\u51cf\u5c11\uff0c\u632f\u8361\u5206\u91cf\u76f8\u5173\u8bef\u5dee\u964d\u4f4e", "conclusion": "\u51e0\u4f55\u9884\u5904\u7406\u7ed3\u5408\u8bfe\u7a0b\u8bad\u7ec3\u662f\u7a33\u5b9a\u91cf\u5b50\u56de\u5f52\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u80fd\u6539\u5584\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6027\u80fd"}}
{"id": "2601.13205", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13205", "abs": "https://arxiv.org/abs/2601.13205", "authors": ["Kadyrzhan Tortayev", "Oliver Falkenberg Damborg", "J\u00f2nas \u00c0 H\u00e0lvm\u00f8rk Joensen", "Jonas Pedesk", "Yifa Li", "Fengchun Zhang", "Zeliang An", "Yubo Wang", "Ming Shen"], "title": "Co-Channel Interference Mitigation Using Deep Learning for Drone-Based Large-Scale Antenna Measurements", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) enable efficient in-situ radiation characterization of large-aperture antennas directly in their deployment environments. In such measurements, a continuous-wave (CW) probe tone is commonly transmitted to characterize the antenna response. However, active co-channel emissions from neighboring antennas often introduce severe in-band interference, where classical FFT-based estimators fail to accurately estimate the CW tone amplitude when the signal-to-interference ratios (SIR) falls below -10 dB. This paper proposes a lightweight deep convolutional neural network (DC-CNN) that estimates the amplitude of the CW tone. The model is trained and evaluated on real 5~GHz measurement bursts spanning an effective SIR range of --33.3 dB to +46.7 dB. Despite its compact size (<20k parameters), the proposed DC-CNN achieves a mean absolute error (MAE) of 7% over the full range, with <1 dB error for SIR >= -30 dB. This robustness and efficiency make DC-CNN suitable for deployment on embedded UAV platforms for interference-resilient antenna pattern characterization.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(DC-CNN)\u7528\u4e8e\u65e0\u4eba\u673a\u5929\u7ebf\u8f90\u5c04\u6d4b\u91cf\u4e2d\uff0c\u5728\u5f3a\u540c\u4fe1\u9053\u5e72\u6270\u4e0b\u51c6\u786e\u4f30\u8ba1\u8fde\u7eed\u6ce2\u4fe1\u53f7\u5e45\u5ea6", "motivation": "\u65e0\u4eba\u673a\u5929\u7ebf\u8f90\u5c04\u6d4b\u91cf\u4e2d\uff0c\u4f20\u7edfFFT\u65b9\u6cd5\u5728\u4fe1\u53f7\u5e72\u6270\u6bd4\u4f4e\u4e8e-10dB\u65f6\u65e0\u6cd5\u51c6\u786e\u4f30\u8ba1\u8fde\u7eed\u6ce2\u4fe1\u53f7\u5e45\u5ea6\uff0c\u9700\u8981\u89e3\u51b3\u5f3a\u540c\u4fe1\u9053\u5e72\u6270\u4e0b\u7684\u6d4b\u91cf\u95ee\u9898", "method": "\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(DC-CNN)\uff0c\u53c2\u6570\u5c11\u4e8e2\u4e07\uff0c\u5728\u771f\u5b9e5GHz\u6d4b\u91cf\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u8986\u76d6-33.3dB\u5230+46.7dB\u7684\u4fe1\u53f7\u5e72\u6270\u6bd4\u8303\u56f4", "result": "DC-CNN\u5728\u5168\u8303\u56f4\u5185\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee7%\uff0c\u5728SIR\u2265-30dB\u65f6\u8bef\u5dee\u5c0f\u4e8e1dB\uff0c\u76f8\u6bd4\u4f20\u7edfFFT\u65b9\u6cd5\u5728SIR<-10dB\u65f6\u5931\u6548\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "DC-CNN\u5177\u6709\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u9002\u5408\u90e8\u7f72\u5728\u5d4c\u5165\u5f0f\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\uff0c\u5b9e\u73b0\u6297\u5e72\u6270\u7684\u5929\u7ebf\u65b9\u5411\u56fe\u6d4b\u91cf"}}
{"id": "2601.11953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11953", "abs": "https://arxiv.org/abs/2601.11953", "authors": ["Shiqing Gao", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "title": "Controlling Underestimation Bias in Constrained Reinforcement Learning for Safe Exploration", "comment": "Published in the 42nd International Conference on Machine Learning (ICML 2025, Oral)", "summary": "Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards while satisfying constraints. However, existing CRL algorithms often encounter significant constraint violations during training, limiting their applicability in safety-critical scenarios. In this paper, we identify the underestimation of the cost value function as a key factor contributing to these violations. To address this issue, we propose the Memory-driven Intrinsic Cost Estimation (MICE) method, which introduces intrinsic costs to mitigate underestimation and control bias to promote safer exploration. Inspired by flashbulb memory, where humans vividly recall dangerous experiences to avoid risks, MICE constructs a memory module that stores previously explored unsafe states to identify high-cost regions. The intrinsic cost is formulated as the pseudo-count of the current state visiting these risk regions. Furthermore, we propose an extrinsic-intrinsic cost value function that incorporates intrinsic costs and adopts a bias correction strategy. Using this function, we formulate an optimization objective within the trust region, along with corresponding optimization methods. Theoretically, we provide convergence guarantees for the proposed cost value function and establish the worst-case constraint violation for the MICE update. Extensive experiments demonstrate that MICE significantly reduces constraint violations while preserving policy performance comparable to baselines.", "AI": {"tldr": "\u63d0\u51faMICE\u65b9\u6cd5\u89e3\u51b3\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7ea6\u675f\u8fdd\u53cd\u95ee\u9898\uff0c\u901a\u8fc7\u8bb0\u5fc6\u6a21\u5757\u5b58\u50a8\u5371\u9669\u72b6\u6001\u5e76\u5f15\u5165\u5185\u5728\u6210\u672c\u6765\u7f13\u89e3\u6210\u672c\u51fd\u6570\u4f4e\u4f30\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7ecf\u5e38\u51fa\u73b0\u4e25\u91cd\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u9650\u5236\u4e86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u7814\u7a76\u53d1\u73b0\u6210\u672c\u4ef7\u503c\u51fd\u6570\u7684\u4f4e\u4f30\u662f\u5bfc\u81f4\u8fd9\u4e9b\u8fdd\u53cd\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51fa\u8bb0\u5fc6\u9a71\u52a8\u7684\u5185\u5728\u6210\u672c\u4f30\u8ba1\uff08MICE\uff09\u65b9\u6cd5\uff1a1\uff09\u6784\u5efa\u8bb0\u5fc6\u6a21\u5757\u5b58\u50a8\u63a2\u7d22\u8fc7\u7684\u4e0d\u5b89\u5168\u72b6\u6001\uff1b2\uff09\u57fa\u4e8e\u5f53\u524d\u72b6\u6001\u8bbf\u95ee\u98ce\u9669\u533a\u57df\u7684\u4f2a\u8ba1\u6570\u5b9a\u4e49\u5185\u5728\u6210\u672c\uff1b3\uff09\u63d0\u51fa\u5305\u542b\u5185\u5728\u6210\u672c\u7684\u5916\u5728-\u5185\u5728\u6210\u672c\u4ef7\u503c\u51fd\u6570\uff0c\u91c7\u7528\u504f\u5dee\u6821\u6b63\u7b56\u7565\uff1b4\uff09\u5728\u4fe1\u4efb\u533a\u57df\u5185\u5236\u5b9a\u4f18\u5316\u76ee\u6807\u3002", "result": "\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u6210\u672c\u4ef7\u503c\u51fd\u6570\u7684\u6536\u655b\u4fdd\u8bc1\u548cMICE\u66f4\u65b0\u7684\u6700\u574f\u60c5\u51b5\u7ea6\u675f\u8fdd\u53cd\u754c\u9650\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660eMICE\u663e\u8457\u51cf\u5c11\u4e86\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "MICE\u901a\u8fc7\u7f13\u89e3\u6210\u672c\u4ef7\u503c\u51fd\u6570\u4f4e\u4f30\u95ee\u9898\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u7684\u63a2\u7d22\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13289", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13289", "abs": "https://arxiv.org/abs/2601.13289", "authors": ["Ruhul Amin Khalil", "Asiya Jehangir", "Hanane Lamaazi", "Sadaf Rubab", "Nasir Saeed"], "title": "Semantic Communication in Underwater IoT Networks for Meaning-Driven Connectivity", "comment": "25 pages, 3 figures and 8 tables", "summary": "The Internet of Underwater Things (IoUT) is revolutionizing marine sensing and environmental monitoring, as well as subaquatic exploration, which are enabled by interconnected and intelligent subsystems. Nevertheless, underwater communication is constrained by narrow bandwidth, high latency, and strict energy constraints, which are the source of efficiency problems in traditional data-centric networks. To tackle these problematic issues, this work provides a survey of recent advances in Semantic Communication (SC) for IoUT, a novel communication paradigm that seeks to harness not raw symbol information but rather its meaning and/or contextual significance. In this paper, we investigate the emerging advanced AI-powered frameworks, including large language models (LLMs), diffusion-based generative encoders, and federated learning (FL), that bridge semantic compression with context-aware prioritization and robust information reconstruction over noisy underwater channels. Hybrid acoustic-optical-RF architectures and edge-intelligent semantic encoders are also considered enablers of sustainable, adaptive operations. Examples in underwater archaeology, marine ecology, and autonomous underwater vehicles (AUVs) coordination are provided as a relief to illustrate the merits of meaning-driven connectivity. The paper concludes with some recommendations, including semantic representations standardization, cross-domain interpolation, and privacy-support schemes. These issues must be addressed in the future before trustworthy SC-enabled IoUT systems can be developed for underwater communication.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u6c34\u4e0b\u7269\u8054\u7f51\u4e2d\u8bed\u4e49\u901a\u4fe1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86AI\u9a71\u52a8\u7684\u8bed\u4e49\u538b\u7f29\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5148\u5904\u7406\u548c\u9c81\u68d2\u4fe1\u606f\u91cd\u5efa\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u6c34\u4e0b\u901a\u4fe1\u7684\u5e26\u5bbd\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u7269\u8054\u7f51\u9762\u4e34\u4f20\u7edf\u6570\u636e\u901a\u4fe1\u7684\u6311\u6218\uff1a\u7a84\u5e26\u5bbd\u3001\u9ad8\u5ef6\u8fdf\u548c\u4e25\u683c\u80fd\u8017\u9650\u5236\u3002\u9700\u8981\u65b0\u7684\u901a\u4fe1\u8303\u5f0f\u6765\u63d0\u5347\u6548\u7387\uff0c\u8bed\u4e49\u901a\u4fe1\u901a\u8fc7\u4f20\u8f93\u4fe1\u606f\u7684\u542b\u4e49\u800c\u975e\u539f\u59cb\u7b26\u53f7\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8c03\u67e5\u4e86AI\u9a71\u52a8\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u7f16\u7801\u5668\u548c\u8054\u90a6\u5b66\u4e60\uff0c\u7ed3\u5408\u8bed\u4e49\u538b\u7f29\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5\u4f18\u5148\u5904\u7406\u3002\u8fd8\u8003\u8651\u4e86\u6df7\u5408\u58f0\u5b66-\u5149\u5b66-RF\u67b6\u6784\u548c\u8fb9\u7f18\u667a\u80fd\u8bed\u4e49\u7f16\u7801\u5668\u3002", "result": "\u63d0\u4f9b\u4e86\u6c34\u4e0b\u8003\u53e4\u3001\u6d77\u6d0b\u751f\u6001\u5b66\u548c\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u534f\u8c03\u7b49\u5e94\u7528\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u4f18\u52bf\u3002\u63d0\u51fa\u4e86\u8bed\u4e49\u8868\u793a\u6807\u51c6\u5316\u3001\u8de8\u57df\u63d2\u503c\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u7b49\u5efa\u8bae\u3002", "conclusion": "\u8bed\u4e49\u901a\u4fe1\u662f\u89e3\u51b3\u6c34\u4e0b\u7269\u8054\u7f51\u901a\u4fe1\u6311\u6218\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u6807\u51c6\u5316\u3001\u8de8\u57df\u96c6\u6210\u548c\u9690\u79c1\u4fdd\u62a4\u7b49\u95ee\u9898\uff0c\u624d\u80fd\u5f00\u53d1\u51fa\u53ef\u4fe1\u8d56\u7684\u8bed\u4e49\u901a\u4fe1\u6c34\u4e0b\u7269\u8054\u7f51\u7cfb\u7edf\u3002"}}
{"id": "2601.11954", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.11954", "abs": "https://arxiv.org/abs/2601.11954", "authors": ["Yufei Peng", "Cheng Yang", "Zhengjie Fan", "Chuan Shi"], "title": "Data-centric Prompt Tuning for Dynamic Graphs", "comment": "CIKM 2025", "summary": "Dynamic graphs have attracted increasing attention due to their ability to model complex and evolving relationships in real-world scenarios. Traditional approaches typically pre-train models using dynamic link prediction and directly apply the resulting node temporal embeddings to specific downstream tasks. However, the significant differences among downstream tasks often lead to performance degradation, especially under few-shot settings. Prompt tuning has emerged as an effective solution to this problem. Existing prompting methods are often strongly coupled with specific model architectures or pretraining tasks, which makes it difficult to adapt to recent or future model designs. Moreover, their exclusive focus on modifying node or temporal features while neglecting spatial structural information leads to limited expressiveness and degraded performance. To address these limitations, we propose DDGPrompt, a data-centric prompting framework designed to effectively refine pre-trained node embeddings at the input data level, enabling better adaptability to diverse downstream tasks. We first define a unified node expression feature matrix that aggregates all relevant temporal and structural information of each node, ensuring compatibility with a wide range of dynamic graph models. Then, we introduce three prompt matrices (temporal bias, edge weight, and feature mask) to adjust the feature matrix completely, achieving task-specific adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot setting on four public dynamic graph datasets. Experimental results demonstrate that our method significantly outperforms traditional methods and prompting approaches in scenarios with limited labels and cold-start conditions.", "AI": {"tldr": "DDGPrompt\uff1a\u4e00\u79cd\u9762\u5411\u52a8\u6001\u56fe\u7684\u6570\u636e\u4e2d\u5fc3\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u8282\u70b9\u7279\u5f81\u77e9\u9635\u548c\u4e09\u79cd\u63d0\u793a\u77e9\u9635\uff08\u65f6\u95f4\u504f\u7f6e\u3001\u8fb9\u6743\u91cd\u3001\u7279\u5f81\u63a9\u7801\uff09\u6765\u4f18\u5316\u9884\u8bad\u7ec3\u8282\u70b9\u5d4c\u5165\uff0c\u63d0\u5347\u5c11\u6837\u672c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u52a8\u6001\u56fe\u65b9\u6cd5\u9884\u8bad\u7ec3\u540e\u76f4\u63a5\u5e94\u7528\u8282\u70b9\u65f6\u95f4\u5d4c\u5165\u5230\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f46\u4efb\u52a1\u5dee\u5f02\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u5c11\u6837\u672c\u573a\u666f\u3002\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u901a\u5e38\u4e0e\u7279\u5b9a\u6a21\u578b\u67b6\u6784\u6216\u9884\u8bad\u7ec3\u4efb\u52a1\u5f3a\u8026\u5408\uff0c\u4e14\u53ea\u5173\u6ce8\u8282\u70b9\u6216\u65f6\u95f4\u7279\u5f81\u800c\u5ffd\u7565\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faDDGPrompt\u6846\u67b6\uff1a1) \u5b9a\u4e49\u7edf\u4e00\u8282\u70b9\u8868\u8fbe\u7279\u5f81\u77e9\u9635\uff0c\u805a\u5408\u8282\u70b9\u6240\u6709\u76f8\u5173\u65f6\u95f4\u548c\u7ed3\u6784\u4fe1\u606f\uff1b2) \u5f15\u5165\u4e09\u79cd\u63d0\u793a\u77e9\u9635\uff08\u65f6\u95f4\u504f\u7f6e\u3001\u8fb9\u6743\u91cd\u3001\u7279\u5f81\u63a9\u7801\uff09\u5728\u8f93\u5165\u6570\u636e\u5c42\u9762\u5bf9\u7279\u5f81\u77e9\u9635\u8fdb\u884c\u5168\u9762\u8c03\u6574\uff0c\u5b9e\u73b0\u8282\u70b9\u5d4c\u5165\u7684\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u52a8\u6001\u56fe\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4e25\u683c\u7684\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6807\u7b7e\u6709\u9650\u548c\u51b7\u542f\u52a8\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "DDGPrompt\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u63d0\u793a\u6846\u67b6\u6709\u6548\u4f18\u5316\u9884\u8bad\u7ec3\u8282\u70b9\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u67b6\u6784\u8026\u5408\u548c\u5ffd\u7565\u7a7a\u95f4\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5728\u5c11\u6837\u672c\u52a8\u6001\u56fe\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2601.13418", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13418", "abs": "https://arxiv.org/abs/2601.13418", "authors": ["Sambrama Hegde", "Venkata Srirama Rohit Kantheti", "Liang C Chu", "Erik Blasch", "Shih-Chun Lin"], "title": "Autonomous Self-Healing UAV Swarms for Robust 6G Non-Terrestrial Networks", "comment": null, "summary": "Recent years have seen an increased interest in the use of Non-terrestrial networks (NTNs), especially the unmanned aerial vehicles (UAVs) to provide cost-effective global connectivity in next-generation wireless networks. We introduce a resilient, adaptive, self-healing network design (RASHND) to optimize signal quality under dynamic interference and adversarial conditions. RASHND leverages inter-node communication and an intelligent algorithm selection process, incorporating combining techniques like distributed-Maximal Ratio Combining (d-MRC), distributed-Linear Minimum Mean Squared Error Estimation(d-LMMSE), and Selection Combining (SC). These algorithms are selected to improve performance by adapting to changing network conditions. To evaluate the effectiveness of the proposed RASHND solutions, a software-defined radio (SDR)-based hardware testbed afforded initial testing and evaluations. Additionally, we present results from UAV tests conducted on the AERPAW testbed to validate our solutions in real-world scenarios. The results demonstrate that RASHND significantly enhances the reliability and interference resilience of UAV networks, making them well-suited for critical communications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u5f39\u6027\u81ea\u9002\u5e94\u81ea\u6108\u7f51\u7edc\u8bbe\u8ba1(RASHND)\uff0c\u901a\u8fc7\u667a\u80fd\u7b97\u6cd5\u9009\u62e9\u548c\u5206\u5e03\u5f0f\u4fe1\u53f7\u5904\u7406\u6280\u672f\u6765\u4f18\u5316\u52a8\u6001\u5e72\u6270\u73af\u5883\u4e0b\u7684\u4fe1\u53f7\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u975e\u5730\u9762\u7f51\u7edc(NTN)\u7279\u522b\u662f\u65e0\u4eba\u673a(UAV)\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u9700\u8981\u89e3\u51b3\u52a8\u6001\u5e72\u6270\u548c\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u7f51\u7edc\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5173\u952e\u901a\u4fe1\u5e94\u7528\u3002", "method": "\u63d0\u51faRASHND\u6846\u67b6\uff0c\u5229\u7528\u8282\u70b9\u95f4\u901a\u4fe1\u548c\u667a\u80fd\u7b97\u6cd5\u9009\u62e9\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u6700\u5927\u6bd4\u5408\u5e76(d-MRC)\u3001\u5206\u5e03\u5f0f\u7ebf\u6027\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u4f30\u8ba1(d-LMMSE)\u548c\u9009\u62e9\u5408\u5e76(SC)\u7b49\u6280\u672f\uff0c\u6839\u636e\u7f51\u7edc\u6761\u4ef6\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535(SDR)\u786c\u4ef6\u6d4b\u8bd5\u5e73\u53f0\u548cAERPAW\u6d4b\u8bd5\u5e8a\u7684\u65e0\u4eba\u673a\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\uff0cRASHND\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u53ef\u9760\u6027\u548c\u5e72\u6270\u6062\u590d\u80fd\u529b\u3002", "conclusion": "RASHND\u80fd\u591f\u6709\u6548\u589e\u5f3a\u65e0\u4eba\u673a\u7f51\u7edc\u5728\u52a8\u6001\u5e72\u6270\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u5173\u952e\u901a\u4fe1\u5e94\u7528\uff0c\u4e3a\u975e\u5730\u9762\u7f51\u7edc\u7684\u53ef\u9760\u8fde\u63a5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11960", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.11960", "abs": "https://arxiv.org/abs/2601.11960", "authors": ["Jingchu Wang", "Bingbing Xu", "Yige Yuan", "Bin Xie", "Xiaoqian Sun", "Huawei Shen"], "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning", "comment": null, "summary": "Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.", "AI": {"tldr": "\u63d0\u51faR\u00b2PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5deerollout\u5934\u6765\u89e3\u8026\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\uff0c\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u5355\u4e00\u7b56\u7565\u540c\u65f6\u8d1f\u8d23\u63a8\u7406\u54cd\u5e94\u548c\u8bad\u7ec3\u4f18\u5316\u8f68\u8ff9\u5bfc\u81f4\u7684\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u540c\u65f6\u751f\u6210\u63a8\u7406\u54cd\u5e94\u548c\u8bad\u7ec3\u4f18\u5316\u8f68\u8ff9\uff0c\u5bfc\u81f4\u751f\u6210\u7a33\u5b9a\u63a8\u7406\u54cd\u5e94\u4e0e\u591a\u6837\u5316\u8bad\u7ec3\u8f68\u8ff9\u4e4b\u95f4\u7684\u76ee\u6807\u51b2\u7a81\uff0c\u9020\u6210\u63a2\u7d22\u4e0d\u8db3\uff0c\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faR\u00b2PO\uff08\u6b8b\u5deerollout\u7b56\u7565\u4f18\u5316\uff09\uff0c\u5728\u7b56\u7565\u4e4b\u4e0a\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b8b\u5deerollout\u5934\uff0c\u5c06\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\u89e3\u8026\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u5b9e\u73b0\u53ef\u63a7\u7684\u8f68\u8ff9\u591a\u6837\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u751f\u6210\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728MATH-500\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.1%\uff0c\u5728APPS\u4e0a\u63d0\u53472.4%\uff0c\u540c\u65f6\u51cf\u5c11\u683c\u5f0f\u9519\u8bef\u5e76\u7f13\u89e3\u957f\u5ea6\u504f\u5dee\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\u3002", "conclusion": "R\u00b2PO\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u8f68\u8ff9\u4e0e\u63a8\u7406\u54cd\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728LLM\u63a8\u7406\u4e2d\u7684\u63a2\u7d22\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.13470", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13470", "abs": "https://arxiv.org/abs/2601.13470", "authors": ["Gabriel Avanzi Ubiali", "Jos\u00e9 Carlos Marinello Filho", "Taufik Abr\u00e3o"], "title": "Joint Subarray Selection, User Scheduling, and Pilot Assignment for XL-MIMO", "comment": "31 pages, 5 figures, full paper", "summary": "Extra-large scale MIMO (XL-MIMO) is a key technology for meeting sixth-generation (6G) requirements for high-rate connectivity and uniform quality of service (QoS); however, its deployment is challenged by the prohibitive complexity of resource management based on instantaneous channel state information (CSI). To address this intractability, this work derives novel closed-form deterministic signal-to-interference-plus-noise ratio (SINR) expressions for both centralized and distributed uplink operations. Valid for Rician fading channels with minimum mean square error (MMSE) receive combining and MMSE channel estimation, these expressions depend exclusively on long-term channel statistics, providing a tractable alternative to computationally expensive instantaneous CSI-driven optimization. Building on these results, we develop statistical-CSI-based algorithms for joint subarray selection, users scheduling, and pilot assignment, leveraging the derived SINR approximations to maximize the minimum spectral efficiency (SE) among scheduled users while preserving computational tractability. The proposed framework exploits the spatial sparsity of user equipment (UE) visibility regions (VRs) to enable more aggressive pilot reuse than is possible in conventional massive MIMO. Numerical results validate the high accuracy of the derived SINR approximations and demonstrate that the proposed algorithms significantly enhance fairness and throughput in crowded scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u8d85\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u8d44\u6e90\u7ba1\u7406\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u7edf\u8ba1CSI\u7684\u786e\u5b9a\u6027SINR\u8868\u8fbe\u5f0f\u548c\u8054\u5408\u8d44\u6e90\u5206\u914d\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u8d85\u5927\u89c4\u6a21MIMO\u662f6G\u5173\u952e\u6280\u672f\uff0c\u4f46\u57fa\u4e8e\u77ac\u65f6CSI\u7684\u8d44\u6e90\u7ba1\u7406\u590d\u6742\u5ea6\u6781\u9ad8\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u957f\u671f\u4fe1\u9053\u7edf\u8ba1\u4fe1\u606f\u7684\u4f4e\u590d\u6742\u5ea6\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63a8\u5bfc\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u4e0a\u884c\u64cd\u4f5c\u7684\u786e\u5b9a\u6027SINR\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u9002\u7528\u4e8eRician\u8870\u843d\u4fe1\u9053\u548cMMSE\u63a5\u6536\u5408\u5e76\u3002\u57fa\u4e8e\u8fd9\u4e9b\u8868\u8fbe\u5f0f\uff0c\u5f00\u53d1\u4e86\u8054\u5408\u5b50\u9635\u5217\u9009\u62e9\u3001\u7528\u6237\u8c03\u5ea6\u548c\u5bfc\u9891\u5206\u914d\u7684\u7edf\u8ba1CSI\u7b97\u6cd5\uff0c\u5229\u7528\u7528\u6237\u53ef\u89c1\u533a\u57df\u7684\u7a7a\u95f4\u7a00\u758f\u6027\u5b9e\u73b0\u66f4\u6fc0\u8fdb\u7684\u5bfc\u9891\u590d\u7528\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u63a8\u5bfc\u7684SINR\u8fd1\u4f3c\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u62e5\u6324\u573a\u666f\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u548c\u541e\u5410\u91cf\uff0c\u76f8\u6bd4\u4f20\u7edf\u5927\u89c4\u6a21MIMO\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5bfc\u9891\u590d\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8d85\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u4e8e\u7edf\u8ba1CSI\u7684\u5b9e\u7528\u8d44\u6e90\u7ba1\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u77ac\u65f6CSI\u4f18\u5316\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a6G\u8d85\u5927\u89c4\u6a21MIMO\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.11977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.11977", "abs": "https://arxiv.org/abs/2601.11977", "authors": ["Ren He", "Yinliang Xu", "Jinfeng Wang", "Jeremy Watson", "Jian Song"], "title": "One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints", "comment": null, "summary": "Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models.", "AI": {"tldr": "\u63d0\u51faMoE-Encoder\u6a21\u5757\uff0c\u901a\u8fc7\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u5c42\u589e\u5f3a\u9884\u8bad\u7ec3\u65f6\u5e8f\u6a21\u578b\uff0c\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u591a\u53d8\u91cf\u9884\u6d4b\u4e2d\u7684\u9690\u79c1\u7ea6\u675f\u548c\u8de8\u533a\u57df\u6cdb\u5316\u95ee\u9898", "motivation": "\u7535\u529b\u7cfb\u7edf\u9884\u6d4b\u9762\u4e34\u591a\u53d8\u91cf\u590d\u6742\u4f9d\u8d56\u3001\u4e25\u683c\u9690\u79c1\u7ea6\u675f\u548c\u8de8\u533a\u57df\u90e8\u7f72\u7684\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u77e5\u8bc6\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u96f6\u6837\u672c\u6027\u80fd\u5728\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u7684tokenization\u548cencoding\u4e4b\u95f4\u6ce8\u5165\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u5c42\uff0c\u5c06\u591a\u53d8\u91cf\u9884\u6d4b\u8f6c\u5316\u4e3a\u4e13\u5bb6\u5f15\u5bfc\u7684\u5355\u53d8\u91cf\u4efb\u52a1\uff0c\u652f\u6301\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e0b\u7684\u672c\u5730\u5316\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u53c2\u6570\u5171\u4eab\u3002", "result": "\u5728\u516c\u5f00\u591a\u53d8\u91cf\u6570\u636e\u96c6\u4e0a\uff0cMoE-Encoder\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002\u8054\u90a6\u73af\u5883\u6a21\u62df\u663e\u793a\uff0c\u4ec5\u4f20\u8f93MoE-Encoder\u53c2\u6570\u5373\u53ef\u9ad8\u6548\u9002\u5e94\u65b0\u533a\u57df\uff0c\u6027\u80fd\u4e0b\u964d\u6700\u5c0f\u3002", "conclusion": "MoE-Encoder\u4e3a\u65f6\u5e8f\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u611f\u77e5\u7684\u6269\u5c55\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5904\u7406\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u4f9d\u8d56\u548c\u9690\u79c1\u7ea6\u675f\u95ee\u9898\u3002"}}
{"id": "2601.13549", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13549", "abs": "https://arxiv.org/abs/2601.13549", "authors": ["Chao Zhou", "Changsheng You", "Cong Zhou", "Chengwen Xing", "Jianhua Zhang"], "title": "Near-field Physical Layer Security: Robust Beamforming under Location Uncertainty", "comment": "13 pages, 11 figures, submitted to IEEE for possible publication", "summary": "In this paper, we study robust beamforming design for near-field physical-layer-security (PLS) systems, where a base station (BS) equipped with an extremely large-scale array (XL-array) serves multiple near-field legitimate users (Bobs) in the presence of multiple near-field eavesdroppers (Eves). Unlike existing works that mostly assume perfect channel state information (CSI) or location information of Eves, we consider a more practical and challenging scenario, where the locations of Bobs are perfectly known, while only imperfect location information of Eves is available at the BS. We first formulate a robust optimization problem to maximize the sum-rate of Bobs while guaranteeing a worst-case limit on the eavesdropping rate under location uncertainty. By transforming Cartesian position errors into the polar domain, we reveal an important near-field angular-error amplification effect: for the same location error, the closer the Eve, the larger the angle error, severely degrading the performance of conventional robust beamforming methods based on imperfect channel state information. To address this issue, we first establish the conditions for which the first-order Taylor approximation of the near-field channel steering vector under location uncertainty is largely accurate. Then, we propose a two-stage robust beamforming method, which first partitions the uncertainty region into multiple fan-shaped sub-regions, followed by the second stage to formulate and solve a refined linear-matrix-inequality (LMI)-based robust beamforming optimization problem. In addition, the proposed method is further extended to scenarios with multiple Bobs and multiple Eves. Finally, numerical results validate that the proposed method achieves a superior trade-off between rate performance and secrecy robustness, hence significantly outperforming existing benchmarks under Eve location uncertainty.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u8fd1\u573a\u7269\u7406\u5c42\u5b89\u5168\u7cfb\u7edf\u7684\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7279\u522b\u5904\u7406\u7a83\u542c\u8005\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fd1\u573a\u7269\u7406\u5c42\u5b89\u5168\u7814\u7a76\u5927\u591a\u5047\u8bbe\u7a83\u542c\u8005\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u6216\u4f4d\u7f6e\u4fe1\u606f\u5b8c\u7f8e\u5df2\u77e5\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7a83\u542c\u8005\u4f4d\u7f6e\u4fe1\u606f\u901a\u5e38\u4e0d\u5b8c\u7f8e\u3002\u672c\u6587\u9488\u5bf9\u8fd9\u4e00\u66f4\u5b9e\u9645\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u7814\u7a76\u5728\u7a83\u542c\u8005\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u3002", "method": "\u9996\u5148\u5c06\u7b1b\u5361\u5c14\u5750\u6807\u4f4d\u7f6e\u8bef\u5dee\u8f6c\u6362\u5230\u6781\u5750\u6807\u57df\uff0c\u63ed\u793a\u8fd1\u573a\u89d2\u5ea6\u8bef\u5dee\u653e\u5927\u6548\u5e94\u3002\u7136\u540e\u63d0\u51fa\u4e24\u9636\u6bb5\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u4e0d\u786e\u5b9a\u533a\u57df\u5212\u5206\u4e3a\u591a\u4e2a\u6247\u5f62\u5b50\u533a\u57df\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5236\u5b9a\u5e76\u6c42\u89e3\u57fa\u4e8e\u7ebf\u6027\u77e9\u9635\u4e0d\u7b49\u5f0f\u7684\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u8fd8\u6269\u5c55\u5230\u591a\u5408\u6cd5\u7528\u6237\u548c\u591a\u7a83\u542c\u8005\u573a\u666f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u901f\u7387\u6027\u80fd\u548c\u4fdd\u5bc6\u9c81\u68d2\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6743\u8861\uff0c\u5728\u7a83\u542c\u8005\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8fd1\u573a\u7269\u7406\u5c42\u5b89\u5168\u7cfb\u7edf\u4e2d\u7a83\u542c\u8005\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5904\u7406\u8fd1\u573a\u89d2\u5ea6\u8bef\u5dee\u653e\u5927\u6548\u5e94\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5b89\u5168\u6027\u80fd\u3002"}}
{"id": "2601.12008", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12008", "abs": "https://arxiv.org/abs/2601.12008", "authors": ["Shiqing Gao", "Yihang Zhou", "Shuai Shao", "Haoyu Luo", "Yiheng Bing", "Jiaxin Ding", "Luoyi Fu", "Xinbing Wang"], "title": "Extreme Value Policy Optimization for Safe Reinforcement Learning", "comment": "Published in the 42nd International Conference on Machine Learning (ICML 2025)", "summary": "Ensuring safety is a critical challenge in applying Reinforcement Learning (RL) to real-world scenarios. Constrained Reinforcement Learning (CRL) addresses this by maximizing returns under predefined constraints, typically formulated as the expected cumulative cost. However, expectation-based constraints overlook rare but high-impact extreme value events in the tail distribution, such as black swan incidents, which can lead to severe constraint violations. To address this issue, we propose the Extreme Value policy Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model and exploit extreme reward and cost samples, reducing constraint violations. EVO introduces an extreme quantile optimization objective to explicitly capture extreme samples in the cost tail distribution. Additionally, we propose an extreme prioritization mechanism during replay, amplifying the learning signal from rare but high-impact extreme samples. Theoretically, we establish upper bounds on expected constraint violations during policy updates, guaranteeing strict constraint satisfaction at a zero-violation quantile level. Further, we demonstrate that EVO achieves a lower probability of constraint violations than expectation-based methods and exhibits lower variance than quantile regression methods. Extensive experiments show that EVO significantly reduces constraint violations during training while maintaining competitive policy performance compared to baselines.", "AI": {"tldr": "\u63d0\u51faEVO\u7b97\u6cd5\uff0c\u5229\u7528\u6781\u503c\u7406\u8bba\u5904\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6781\u7aef\u98ce\u9669\u4e8b\u4ef6\uff0c\u51cf\u5c11\u7ea6\u675f\u8fdd\u53cd", "motivation": "\u73b0\u6709\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u57fa\u4e8e\u671f\u671b\u7ea6\u675f\uff0c\u5ffd\u7565\u4e86\u5c3e\u90e8\u5206\u5e03\u4e2d\u7684\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u7684\u6781\u7aef\u503c\u4e8b\u4ef6\uff08\u5982\u9ed1\u5929\u9e45\u4e8b\u4ef6\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u7684\u7ea6\u675f\u8fdd\u53cd", "method": "\u63d0\u51fa\u6781\u503c\u7b56\u7565\u4f18\u5316\uff08EVO\uff09\u7b97\u6cd5\uff1a1\uff09\u5229\u7528\u6781\u503c\u7406\u8bba\u5efa\u6a21\u6781\u7aef\u5956\u52b1\u548c\u6210\u672c\u6837\u672c\uff1b2\uff09\u5f15\u5165\u6781\u7aef\u5206\u4f4d\u6570\u4f18\u5316\u76ee\u6807\u6355\u83b7\u6210\u672c\u5c3e\u90e8\u5206\u5e03\uff1b3\uff09\u63d0\u51fa\u56de\u653e\u8fc7\u7a0b\u4e2d\u7684\u6781\u7aef\u4f18\u5148\u7ea7\u673a\u5236\uff0c\u653e\u5927\u7f55\u89c1\u4f46\u9ad8\u5f71\u54cd\u6837\u672c\u7684\u5b66\u4e60\u4fe1\u53f7", "result": "\u7406\u8bba\u4e0a\u5efa\u7acb\u4e86\u7b56\u7565\u66f4\u65b0\u671f\u95f4\u9884\u671f\u7ea6\u675f\u8fdd\u53cd\u7684\u4e0a\u754c\uff0c\u4fdd\u8bc1\u5728\u96f6\u8fdd\u53cd\u5206\u4f4d\u6570\u6c34\u5e73\u7684\u4e25\u683c\u7ea6\u675f\u6ee1\u8db3\uff1b\u5b9e\u9a8c\u8868\u660eEVO\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u671f\u95f4\u7684\u7ea6\u675f\u8fdd\u53cd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u7b56\u7565\u6027\u80fd", "conclusion": "EVO\u7b97\u6cd5\u901a\u8fc7\u5efa\u6a21\u6781\u7aef\u4e8b\u4ef6\u6709\u6548\u89e3\u51b3\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5c3e\u90e8\u98ce\u9669\u95ee\u9898\uff0c\u6bd4\u57fa\u4e8e\u671f\u671b\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u7ea6\u675f\u8fdd\u53cd\u6982\u7387\uff0c\u6bd4\u5206\u4f4d\u6570\u56de\u5f52\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u65b9\u5dee"}}
{"id": "2601.13593", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13593", "abs": "https://arxiv.org/abs/2601.13593", "authors": ["Aswin Jose", "Roeland P. J. E. Decorte", "Laurent Locquet"], "title": "Instant Preliminary Cardiac Analysis from Smartphone Auscultation: A Real-World Canine Heart Sound Dataset and Evaluation", "comment": null, "summary": "This study presents a real-world canine heart sound dataset and evaluates SoNUS version 3.2.x, a machine learning algorithm for preliminary cardiac analysis using smartphone microphone recordings. More than one hundred recordings were collected from dogs across four continents, with thirty eight recordings annotated by board certified veterinary cardiologists for quantitative evaluation. SoNUS version 3.2.x employs a multi-stage fallback architecture with quality-aware filtering to ensure reliable output under variable recording conditions. The primary sixty second model achieved mean and median heart rate accuracies of ninety one point six three percent and ninety four point nine five percent, while a fast model optimized for thirty to forty second recordings achieved mean and median accuracies of eighty eight point eight six percent and ninety two point nine eight percent. These results demonstrate the feasibility of extracting clinically relevant cardiac information from opportunistic smartphone recordings, supporting scalable preliminary assessment and telehealth applications in veterinary cardiology.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u72ac\u7c7b\u5fc3\u97f3\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86SoNUS 3.2.x\u7b97\u6cd5\u5728\u667a\u80fd\u624b\u673a\u5f55\u97f3\u4e0a\u8fdb\u884c\u521d\u6b65\u5fc3\u810f\u5206\u6790\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u517d\u533b\u5fc3\u810f\u75c5\u5b66\u4e2d\u53ef\u6269\u5c55\u7684\u521d\u6b65\u8bc4\u4f30\u548c\u8fdc\u7a0b\u533b\u7597\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u5f55\u97f3\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u517d\u533b\u5fc3\u810f\u75c5\u5b66\u7684\u521d\u6b65\u5fc3\u810f\u8bc4\u4f30\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u8fdc\u7a0b\u533b\u7597\u5e94\u7528\uff0c\u89e3\u51b3\u4f20\u7edf\u517d\u533b\u5fc3\u810f\u68c0\u67e5\u7684\u4fbf\u5229\u6027\u548c\u53ef\u53ca\u6027\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u6536\u96c6\u6765\u81ea\u56db\u5927\u6d32100\u591a\u4e2a\u72ac\u7c7b\u5fc3\u97f3\u5f55\u97f3\uff1b2\uff09\u7531\u517d\u533b\u5fc3\u810f\u75c5\u4e13\u5bb6\u5bf938\u4e2a\u5f55\u97f3\u8fdb\u884c\u6807\u6ce8\u7528\u4e8e\u5b9a\u91cf\u8bc4\u4f30\uff1b3\uff09\u4f7f\u7528SoNUS 3.2.x\u7b97\u6cd5\uff0c\u91c7\u7528\u591a\u7ea7\u56de\u9000\u67b6\u6784\u548c\u8d28\u91cf\u611f\u77e5\u8fc7\u6ee4\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u5f55\u97f3\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u8f93\u51fa\uff1b4\uff09\u8bc4\u4f3060\u79d2\u4e3b\u6a21\u578b\u548c30-40\u79d2\u5feb\u901f\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\uff1a60\u79d2\u4e3b\u6a21\u578b\u7684\u5fc3\u7387\u51c6\u786e\u7387\u5747\u503c\u4e3a91.63%\uff0c\u4e2d\u4f4d\u6570\u4e3a94.95%\uff1b30-40\u79d2\u5feb\u901f\u6a21\u578b\u7684\u51c6\u786e\u7387\u5747\u503c\u4e3a88.86%\uff0c\u4e2d\u4f4d\u6570\u4e3a92.98%\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u4ece\u667a\u80fd\u624b\u673a\u5f55\u97f3\u4e2d\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u5fc3\u810f\u4fe1\u606f\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7ed3\u8bba\uff1aSoNUS 3.2.x\u7b97\u6cd5\u80fd\u591f\u4ece\u673a\u4f1a\u6027\u667a\u80fd\u624b\u673a\u5f55\u97f3\u4e2d\u53ef\u9760\u5730\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u5fc3\u810f\u4fe1\u606f\uff0c\u652f\u6301\u517d\u533b\u5fc3\u810f\u75c5\u5b66\u4e2d\u53ef\u6269\u5c55\u7684\u521d\u6b65\u8bc4\u4f30\u548c\u8fdc\u7a0b\u533b\u7597\u5e94\u7528\uff0c\u4e3a\u5ba0\u7269\u5fc3\u810f\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4fbf\u6377\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2601.12011", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12011", "abs": "https://arxiv.org/abs/2601.12011", "authors": ["Yize Zhao", "Christos Thrampoulidis"], "title": "Why Loss Re-weighting Works If You Stop Early: Training Dynamics of Unconstrained Features", "comment": null, "summary": "The application of loss reweighting in modern deep learning presents a nuanced picture. While it fails to alter the terminal learning phase in overparameterized deep neural networks (DNNs) trained on high-dimensional datasets, empirical evidence consistently shows it offers significant benefits early in training. To transparently demonstrate and analyze this phenomenon, we introduce a small-scale model (SSM). This model is specifically designed to abstract the inherent complexities of both the DNN architecture and the input data, while maintaining key information about the structure of imbalance within its spectral components. On the one hand, the SSM reveals how vanilla empirical risk minimization preferentially learns to distinguish majority classes over minorities early in training, consequently delaying minority learning. In stark contrast, reweighting restores balanced learning dynamics, enabling the simultaneous learning of features associated with both majorities and minorities.", "AI": {"tldr": "\u635f\u5931\u91cd\u52a0\u6743\u5728\u8fc7\u53c2\u6570\u5316DNN\u4e2d\u65e0\u6cd5\u6539\u53d8\u6700\u7ec8\u5b66\u4e60\u9636\u6bb5\uff0c\u4f46\u5728\u8bad\u7ec3\u65e9\u671f\u663e\u8457\u6709\u76ca\u3002\u4f5c\u8005\u5f15\u5165\u5c0f\u89c4\u6a21\u6a21\u578b(SSM)\u6765\u900f\u660e\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\uff0c\u53d1\u73b0\u6807\u51c6ERM\u65e9\u671f\u504f\u597d\u5b66\u4e60\u591a\u6570\u7c7b\u7279\u5f81\uff0c\u800c\u91cd\u52a0\u6743\u80fd\u6062\u590d\u5e73\u8861\u5b66\u4e60\u52a8\u6001\u3002", "motivation": "\u635f\u5931\u91cd\u52a0\u6743\u5728\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5448\u73b0\u590d\u6742\u56fe\u666f\uff1a\u867d\u7136\u65e0\u6cd5\u6539\u53d8\u8fc7\u53c2\u6570\u5316DNN\u5728\u8bad\u7ec3\u540e\u671f\u7684\u6700\u7ec8\u5b66\u4e60\u9636\u6bb5\uff0c\u4f46\u5b9e\u8bc1\u8bc1\u636e\u4e00\u81f4\u8868\u660e\u5b83\u5728\u8bad\u7ec3\u65e9\u671f\u63d0\u4f9b\u663e\u8457\u76ca\u5904\u3002\u9700\u8981\u900f\u660e\u5730\u5c55\u793a\u548c\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u5f15\u5165\u5c0f\u89c4\u6a21\u6a21\u578b(SSM)\uff0c\u8be5\u6a21\u578b\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u62bd\u8c61DNN\u67b6\u6784\u548c\u8f93\u5165\u6570\u636e\u7684\u56fa\u6709\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u9891\u8c31\u5206\u91cf\u4e2d\u4e0d\u5e73\u8861\u7ed3\u6784\u7684\u5173\u952e\u4fe1\u606f\u3002\u901a\u8fc7SSM\u5206\u6790\u6807\u51c6\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316(ERM)\u4e0e\u91cd\u52a0\u6743\u65b9\u6cd5\u7684\u5b66\u4e60\u52a8\u6001\u5dee\u5f02\u3002", "result": "SSM\u63ed\u793a\uff1a1) \u6807\u51c6ERM\u5728\u8bad\u7ec3\u65e9\u671f\u4f18\u5148\u5b66\u4e60\u533a\u5206\u591a\u6570\u7c7b\u800c\u975e\u5c11\u6570\u7c7b\uff0c\u4ece\u800c\u5ef6\u8fdf\u5c11\u6570\u7c7b\u5b66\u4e60\uff1b2) \u91cd\u52a0\u6743\u6062\u590d\u5e73\u8861\u5b66\u4e60\u52a8\u6001\uff0c\u4f7f\u4e0e\u591a\u6570\u7c7b\u548c\u5c11\u6570\u7c7b\u76f8\u5173\u7684\u7279\u5f81\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u3002", "conclusion": "\u635f\u5931\u91cd\u52a0\u6743\u901a\u8fc7\u6062\u590d\u8bad\u7ec3\u65e9\u671f\u7684\u5e73\u8861\u5b66\u4e60\u52a8\u6001\u800c\u53d1\u6325\u4f5c\u7528\uff0c\u5c3d\u7ba1\u5728\u8fc7\u53c2\u6570\u5316DNN\u7684\u6700\u7ec8\u5b66\u4e60\u9636\u6bb5\u65e0\u6cd5\u6539\u53d8\u7ed3\u679c\u3002SSM\u4e3a\u7406\u89e3\u8fd9\u4e00\u73b0\u8c61\u63d0\u4f9b\u4e86\u900f\u660e\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2601.13635", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13635", "abs": "https://arxiv.org/abs/2601.13635", "authors": ["Emin Akpinar", "Emir Aslandogan", "Burak Ahmet Ozden", "Haci Ilhan", "Erdogan Aydin"], "title": "Deep Learning-Enabled Signal Detection for MIMO-OTFS-Based 6G and Future Wireless Networks", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Orthogonal time frequency space (OTFS) modulation stands out as a promising waveform for sixth generation (6G) and beyond wireless communication systems, offering superior performance over conventional methods, particularly in high-mobility scenarios and dispersive channel conditions. Recent research has demonstrated that the reduced computational complexity of deep learning (DL)-based signal detection (SD) methods constitutes a compelling alternative to conventional techniques. In this study, low-complexity DL-based SD methods are proposed for a multiple-input multiple-output (MIMO)-OTFS system and examined under Nakagami-$m$ channel conditions. The symbols obtained from the receiver antennas are combined using maximum ratio combining (MRC) and detected with the help of a DL-based detector implemented with multi-layer perceptron (MLP), convolutional neural network (CNN), and residual network (ResNet). Complexity analysis reveals that the MLP architecture offers significantly lower computational complexity compared to CNN, ResNet, and classical methods such as maximum likelihood detection (MLD). Furthermore, numerical analyses have shown that the proposed DL-based detectors, despite their low complexity, achieve comparable bit error rate (BER) performance to that of a high-performance MLD under various system conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528\u4e8eMIMO-OTFS\u7cfb\u7edf\u7684\u4f4e\u590d\u6742\u5ea6\u6df1\u5ea6\u5b66\u4e60\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728Nakagami-m\u4fe1\u9053\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528MLP\u3001CNN\u548cResNet\u67b6\u6784\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9ad8\u6027\u80fdMLD\u76f8\u5f53\u7684\u8bef\u7801\u7387\u6027\u80fd\u3002", "motivation": "OTFS\u8c03\u5236\u4f5c\u4e3a6G\u53ca\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u6709\u524d\u666f\u6ce2\u5f62\uff0c\u5728\u9ad8\u79fb\u52a8\u6027\u548c\u8272\u6563\u4fe1\u9053\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002\u6df1\u5ea6\u5b66\u4e60\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\uff0c\u662f\u4f20\u7edf\u6280\u672f\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3aMIMO-OTFS\u7cfb\u7edf\u8bbe\u8ba1\u4f4e\u590d\u6742\u5ea6\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u9488\u5bf9MIMO-OTFS\u7cfb\u7edf\uff0c\u5728Nakagami-m\u4fe1\u9053\u6761\u4ef6\u4e0b\uff0c\u91c7\u7528\u6700\u5927\u6bd4\u5408\u5e76\u63a5\u6536\u5929\u7ebf\u7b26\u53f7\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u591a\u5c42\u611f\u77e5\u5668\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u6b8b\u5dee\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u5668\u8fdb\u884c\u4fe1\u53f7\u68c0\u6d4b\u3002", "result": "\u590d\u6742\u5ea6\u5206\u6790\u663e\u793aMLP\u67b6\u6784\u76f8\u6bd4CNN\u3001ResNet\u548c\u4f20\u7edf\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u6570\u503c\u5206\u6790\u8868\u660e\uff0c\u5c3d\u7ba1\u590d\u6742\u5ea6\u4f4e\uff0c\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u5668\u5728\u5404\u79cd\u7cfb\u7edf\u6761\u4ef6\u4e0b\u90fd\u80fd\u8fbe\u5230\u4e0e\u9ad8\u6027\u80fdMLD\u76f8\u5f53\u7684\u8bef\u7801\u7387\u6027\u80fd\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4fe1\u53f7\u68c0\u6d4b\u65b9\u6cd5\u4e3aMIMO-OTFS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4f4e\u590d\u6742\u5ea6\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0cMLP\u67b6\u6784\u5728\u590d\u6742\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u6709\u529b\u5019\u9009\u65b9\u6848\u3002"}}
{"id": "2601.12083", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12083", "abs": "https://arxiv.org/abs/2601.12083", "authors": ["Siru Zhong", "Junjie Qiu", "Yangyu Wu", "Yiqiu Liu", "Yuanpeng He", "Zhongwen Rao", "Bin Yang", "Chenjuan Guo", "Hao Xu", "Yuxuan Liang"], "title": "Learning to Factorize and Adapt: A Versatile Approach Toward Universal Spatio-Temporal Foundation Models", "comment": "This is an extended version of the paper presented at NeurIPS 2025. Code available at https://github.com/CityMind-Lab/FactoST", "summary": "Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset generalization, yet joint ST pretraining is computationally expensive and grapples with the heterogeneity of domain-specific spatial patterns. Substantially extending our preliminary conference version, we present FactoST-v2, an enhanced factorized framework redesigned for full weight transfer and arbitrary-length generalization. FactoST-v2 decouples universal temporal learning from domain-specific spatial adaptation. The first stage pretrains a minimalist encoder-only backbone using randomized sequence masking to capture invariant temporal dynamics, enabling probabilistic quantile prediction across variable horizons. The second stage employs a streamlined adapter to rapidly inject spatial awareness via meta adaptive learning and prompting. Comprehensive evaluations across diverse domains demonstrate that FactoST-v2 achieves state-of-the-art accuracy with linear efficiency - significantly outperforming existing foundation models in zero-shot and few-shot scenarios while rivaling domain-specific expert baselines. This factorized paradigm offers a practical, scalable path toward truly universal STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.", "AI": {"tldr": "FactoST-v2\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u901a\u7528\u65f6\u95f4\u5b66\u4e60\u548c\u9886\u57df\u7279\u5b9a\u7a7a\u95f4\u9002\u5e94\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u6743\u91cd\u8fc1\u79fb\u548c\u4efb\u610f\u957f\u5ea6\u6cdb\u5316\u3002", "motivation": "\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u867d\u7136\u5177\u6709\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u6f5c\u529b\uff0c\u4f46\u8054\u5408\u65f6\u7a7a\u9884\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u9886\u57df\u7279\u5b9a\u7684\u7a7a\u95f4\u6a21\u5f0f\u5f02\u8d28\u6027\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u89e3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u9884\u8bad\u7ec3\u4ec5\u7f16\u7801\u5668\u9aa8\u5e72\u7f51\u7edc\uff0c\u4f7f\u7528\u968f\u673a\u5e8f\u5217\u63a9\u7801\u6355\u83b7\u4e0d\u53d8\u65f6\u95f4\u52a8\u6001\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8f7b\u91cf\u9002\u914d\u5668\u6ce8\u5165\u7a7a\u95f4\u611f\u77e5\uff0c\u4f7f\u7528\u5143\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u63d0\u793a\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u8bc4\u4f30\u4e2d\uff0cFactoST-v2\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u7ebf\u6027\u6548\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\uff0c\u5ab2\u7f8e\u9886\u57df\u7279\u5b9a\u4e13\u5bb6\u57fa\u7ebf\u3002", "conclusion": "\u8fd9\u79cd\u5206\u89e3\u8303\u5f0f\u4e3a\u6784\u5efa\u771f\u6b63\u901a\u7528\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u6743\u91cd\u8fc1\u79fb\u548c\u4efb\u610f\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.13680", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13680", "abs": "https://arxiv.org/abs/2601.13680", "authors": ["Shama Siddiqui", "Anwar Ahmed Khan", "Nicola Marchetti"], "title": "TSN-IoT: A Two-Stage NOMA-Enabled Framework for Prioritized Traffic Handling in Dense IoT Networks", "comment": null, "summary": "With the growing applications of the Internet of Things (IoT), a major challenge is to ensure continuous connectivity while providing prioritized access. In dense IoT scenarios, synchronization may be disrupted either by the movement of nodes away from base stations or by the unavailability of reliable Global Navigation Satellite System (GNSS) signals, which can be affected by physical obstructions, multipath fading, or environmental interference, such as such as walls, buildings, moving objects, or electromagnetic noise from surrounding devices. In such contexts, distributed synchronization through Non-Orthogonal Multiple Access (NOMA) offers a promising solution, as it enables simultaneous transmission to multiple users with different power levels, supporting efficient synchronization while minimizing the signaling overhead. Moreover, NOMA also plays a vital role for dynamic priority management in dense and heterogeneous IoT environments. In this article, we proposed a Two-Stage NOMA-Enabled Framework \"TSN-IoT\" that integrates the mechanisms of conventional Precision Time Protocol (PTP) based synchronization, distributed synchronization and data transmission. The framework is designed as a four-tier architecture that facilitates prioritized data delivery from sensor nodes to the central base station. We demonstrated the performance of \"TSN-IoT\" through a healthcare use case, where intermittent connectivity and varying data priority levels present key challenges for reliable communication. Synchronization speed and end-to-end delay were evaluated through a series of simulations implemented in Python. Results show that, compared to priority-based Orthogonal Frequency Division Multiple Access (OFDMA), TSN-IoT achieves significantly better performance by offering improved synchronization opportunities and enabling parallel transmissions over the same sub-carrier.", "AI": {"tldr": "\u63d0\u51faTSN-IoT\u6846\u67b6\uff0c\u5728\u5bc6\u96c6IoT\u73af\u5883\u4e2d\u901a\u8fc7NOMA\u5b9e\u73b0\u5206\u5e03\u5f0f\u540c\u6b65\u548c\u4f18\u5148\u7ea7\u6570\u636e\u4f20\u8f93\uff0c\u76f8\u6bd4OFDMA\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u5bc6\u96c6IoT\u573a\u666f\u4e2d\uff0c\u8282\u70b9\u79fb\u52a8\u548cGNSS\u4fe1\u53f7\u4e0d\u53ef\u9760\u4f1a\u7834\u574f\u540c\u6b65\uff0c\u9700\u8981\u786e\u4fdd\u8fde\u7eed\u8fde\u63a5\u5e76\u63d0\u4f9b\u4f18\u5148\u7ea7\u8bbf\u95ee", "method": "\u63d0\u51faTSN-IoT\u6846\u67b6\uff0c\u6574\u5408\u4f20\u7edfPTP\u540c\u6b65\u3001\u5206\u5e03\u5f0f\u540c\u6b65\u548c\u6570\u636e\u4f20\u8f93\uff0c\u91c7\u7528\u56db\u5c42\u67b6\u6784\u5b9e\u73b0\u4ece\u4f20\u611f\u5668\u8282\u70b9\u5230\u57fa\u7ad9\u7684\u4f18\u5148\u7ea7\u6570\u636e\u4f20\u8f93", "result": "\u901a\u8fc7\u533b\u7597\u7528\u4f8b\u4eff\u771f\uff0c\u76f8\u6bd4\u57fa\u4e8e\u4f18\u5148\u7ea7\u7684OFDMA\uff0cTSN-IoT\u5728\u540c\u6b65\u901f\u5ea6\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u663e\u8457\u66f4\u597d", "conclusion": "TSN-IoT\u6846\u67b6\u901a\u8fc7NOMA\u5b9e\u73b0\u5206\u5e03\u5f0f\u540c\u6b65\u548c\u5e76\u884c\u4f20\u8f93\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6IoT\u73af\u5883\u4e2d\u7684\u8fde\u63a5\u548c\u4f18\u5148\u7ea7\u7ba1\u7406\u6311\u6218"}}
{"id": "2601.12091", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12091", "abs": "https://arxiv.org/abs/2601.12091", "authors": ["Qian Tan", "Lei Jiang", "Yuting Zeng", "Shuoyang Ding", "Xiaohua Xu"], "title": "Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate", "comment": "13 pages", "summary": "Large language models (LLMs) exhibit systematic Western-centric bias, yet whether prompting in non-Western languages (e.g., Chinese) can mitigate this remains understudied. Answering this question requires rigorous evaluation and effective mitigation, but existing approaches fall short on both fronts: evaluation methods force outputs into predefined cultural categories without a neutral option, while mitigation relies on expensive multi-cultural corpora or agent frameworks that use functional roles (e.g., Planner--Critique) lacking explicit cultural representation. To address these gaps, we introduce CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote (MAV), which enables explicit ``no bias'' judgments. Using this framework, we find that Chinese prompting merely shifts bias toward East Asian perspectives rather than eliminating it. To mitigate such persistent bias, we propose Multi-Agent Cultural Debate (MACD), a training-free framework that assigns agents distinct cultural personas and orchestrates deliberation via a \"Seeking Common Ground while Reserving Differences\" strategy. Experiments demonstrate that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and 86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone) on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that explicit cultural representation in agent frameworks is essential for cross-cultural fairness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCEBiasBench\u4e2d\u82f1\u53cc\u8bed\u57fa\u51c6\u548cMulti-Agent Vote\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u4e2d\u6587\u63d0\u793a\u4ec5\u5c06\u504f\u89c1\u8f6c\u5411\u4e1c\u4e9a\u89c6\u89d2\u800c\u975e\u6d88\u9664\u504f\u89c1\uff0c\u5e76\u63d0\u51faMulti-Agent Cultural Debate\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u6765\u7f13\u89e3\u8de8\u6587\u5316\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7cfb\u7edf\u6027\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u4f46\u4f7f\u7528\u975e\u897f\u65b9\u8bed\u8a00\uff08\u5982\u4e2d\u6587\uff09\u63d0\u793a\u662f\u5426\u80fd\u7f13\u89e3\u8fd9\u79cd\u504f\u89c1\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30\u548c\u7f13\u89e3\u4e24\u65b9\u9762\u90fd\u5b58\u5728\u4e0d\u8db3\uff1a\u8bc4\u4f30\u65b9\u6cd5\u5f3a\u5236\u8f93\u51fa\u5230\u9884\u5b9a\u4e49\u6587\u5316\u7c7b\u522b\u800c\u65e0\u4e2d\u7acb\u9009\u9879\uff0c\u7f13\u89e3\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u591a\u6587\u5316\u8bed\u6599\u5e93\u6216\u7f3a\u4e4f\u660e\u786e\u6587\u5316\u8868\u5f81\u7684\u4ee3\u7406\u6846\u67b6\u3002", "method": "1) \u5f15\u5165CEBiasBench\u4e2d\u82f1\u53cc\u8bed\u57fa\u51c6\u548cMulti-Agent Vote\u6846\u67b6\uff0c\u652f\u6301\u660e\u786e\u7684\"\u65e0\u504f\u89c1\"\u5224\u65ad\uff1b2) \u63d0\u51faMulti-Agent Cultural Debate\u6846\u67b6\uff0c\u4e3a\u4ee3\u7406\u5206\u914d\u4e0d\u540c\u6587\u5316\u89d2\u8272\uff0c\u91c7\u7528\"\u6c42\u540c\u5b58\u5f02\"\u7b56\u7565\u8fdb\u884c\u5ba1\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u4e2d\u6587\u63d0\u793a\u4ec5\u5c06\u504f\u89c1\u8f6c\u5411\u4e1c\u4e9a\u89c6\u89d2\u800c\u975e\u6d88\u9664\u504f\u89c1\uff1b2) MACD\u5728CEBiasBench\u4e0a\u8fbe\u523057.6%\u7684\u5e73\u5747\u65e0\u504f\u89c1\u7387\uff08LLM-as-judge\u8bc4\u4f30\uff09\u548c86.0%\uff08MAV\u8bc4\u4f30\uff09\uff0c\u76f8\u6bd4GPT-4o\u57fa\u7ebf\u768447.6%\u548c69.0%\u6709\u663e\u8457\u63d0\u5347\uff1b3) MACD\u80fd\u6cdb\u5316\u5230\u963f\u62c9\u4f2f\u8bedCAMeL\u57fa\u51c6\u3002", "conclusion": "\u4ee3\u7406\u6846\u67b6\u4e2d\u660e\u786e\u7684\u6587\u5316\u8868\u5f81\u5bf9\u4e8e\u8de8\u6587\u5316\u516c\u5e73\u81f3\u5173\u91cd\u8981\uff0cMulti-Agent Cultural Debate\u901a\u8fc7\u8bad\u7ec3\u514d\u8d39\u7684\u65b9\u5f0f\u6709\u6548\u7f13\u89e3\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6587\u5316\u504f\u89c1\u3002"}}
{"id": "2601.13827", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13827", "abs": "https://arxiv.org/abs/2601.13827", "authors": ["Yongqiang Zhang", "Qurrat-Ul-Ain Nadeem"], "title": "Channel Estimation in MIMO Systems Using Flow Matching Models", "comment": "6 pages, 3 figures, accepted by IEEE International Conference on Communications (ICC) 2026", "summary": "Multiple-input multiple-output (MIMO) systems require efficient and accurate channel estimation with low pilot overhead to unlock their full potential for high spectral and energy efficiency. While deep generative models have emerged as a powerful foundation for the channel estimation task, the existing approaches using diffusion-based and score-based models suffer from high computational runtime due to their stochastic and many-step iterative sampling. In this paper, we introduce a flow matching-based channel estimator to overcome this limitation. The proposed channel estimator is based on a deep neural network trained to learn the velocity field of wireless channels, which we then integrate into a plug-and-play proximal gradient descent (PnP-PGD) framework. Simulation results reveal that our formulated approach consistently outperforms existing state-of-the-art (SOTA) generative model-based estimators, achieves up to 49 times faster inference at test time, and reduces up to 20 times peak graphics processing unit (GPU) memory usage. Our code and models are publicly available to support reproducible research.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6d41\u5339\u914d\u7684MIMO\u4fe1\u9053\u4f30\u8ba1\u5668\uff0c\u76f8\u6bd4\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534749\u500d\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120\u500d", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548c\u5206\u6570\u5339\u914d\u7684\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u751f\u6210\u6a21\u578b\u65b9\u6cd5", "method": "\u57fa\u4e8e\u6d41\u5339\u914d\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u65e0\u7ebf\u4fe1\u9053\u7684\u901f\u5ea6\u573a\uff0c\u7136\u540e\u96c6\u6210\u5230\u5373\u63d2\u5373\u7528\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u6846\u67b6\u4e2d", "result": "\u6027\u80fd\u4f18\u4e8e\u73b0\u6709SOTA\u751f\u6210\u6a21\u578b\u4f30\u8ba1\u5668\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534749\u500d\uff0cGPU\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120\u500d", "conclusion": "\u6d41\u5339\u914d\u65b9\u6cd5\u4e3aMIMO\u4fe1\u9053\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500"}}
{"id": "2601.12093", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12093", "abs": "https://arxiv.org/abs/2601.12093", "authors": ["Duarte Alexandrino", "Ben Moseley", "Pavlos Protopapas"], "title": "PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed Neural Networks for Nonlinear Systems", "comment": "51 pages, 14 figures, 7 tables", "summary": "Accurately and efficiently solving nonlinear differential equations is crucial for modeling dynamic behavior across science and engineering. Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution that embeds physical laws in training by enforcing equation residuals. However, these struggle to model nonlinear dynamics, suffering from limited generalization across problems and long training times. To address these limitations, we propose a perturbation-guided transfer learning framework for PINNs (PTL-PINN), which integrates perturbation theory with transfer learning to efficiently solve nonlinear equations. Unlike gradient-based transfer learning, PTL-PINNs solve an approximate linear perturbative system using closed-form expressions, enabling rapid generalization with the time complexity of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy comparable to various Runge-Kutta methods, with computational speeds up to one order of magnitude faster. To benchmark performance, we solve a broad set of problems, including nonlinear oscillators across various damping regimes, the equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we systematically evaluate its practical applicability. This work connects long-standing perturbation methods with PINNs, demonstrating how perturbation theory can guide foundational models to solve nonlinear systems with speeds comparable to those of classical solvers.", "AI": {"tldr": "\u63d0\u51faPTL-PINN\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u6270\u7406\u8bba\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5feb\u901f\u6c42\u89e3\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\uff0c\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u5728\u6c42\u89e3\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u65f6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8bad\u7ec3\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPTL-PINN\u6846\u67b6\uff0c\u5c06\u5fae\u6270\u7406\u8bba\u4e0e\u8fc1\u79fb\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u6c42\u89e3\u8fd1\u4f3c\u7ebf\u6027\u5fae\u6270\u7cfb\u7edf\uff08\u4f7f\u7528\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff09\uff0c\u5b9e\u73b0\u5feb\u901f\u6cdb\u5316\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4ec5\u4e3a\u77e9\u9635\u5411\u91cf\u4e58\u6cd5\u7ea7\u522b\u3002", "result": "PTL-PINNs\u8fbe\u5230\u4e0e\u591a\u79cdRunge-Kutta\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u6210\u529f\u6c42\u89e3\u4e86\u975e\u7ebf\u6027\u632f\u8361\u5668\u3001Lotka-Volterra\u7cfb\u7edf\u3001KPP-Fisher\u65b9\u7a0b\u548c\u6ce2\u52a8\u65b9\u7a0b\u7b49\u591a\u79cd\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u957f\u671f\u5b58\u5728\u7684\u5fae\u6270\u65b9\u6cd5\u4e0ePINNs\u8fde\u63a5\u8d77\u6765\uff0c\u5c55\u793a\u4e86\u5fae\u6270\u7406\u8bba\u5982\u4f55\u6307\u5bfc\u57fa\u7840\u6a21\u578b\u4ee5\u63a5\u8fd1\u7ecf\u5178\u6c42\u89e3\u5668\u7684\u901f\u5ea6\u6c42\u89e3\u975e\u7ebf\u6027\u7cfb\u7edf\uff0c\u4e3a\u9ad8\u6548\u6c42\u89e3\u975e\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.13877", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13877", "abs": "https://arxiv.org/abs/2601.13877", "authors": ["Ignacio Santamaria", "Mohammad Soleymani", "Eduard Jorswieck", "Jesus Gutierrez", "Carlos Beltran"], "title": "Riemannian optimization on the manifold of unitary and symmetric matrices with application to BD-RIS-assisted systems", "comment": "5 pages, 2 figures", "summary": "In this paper, we rigorously characterize for the first time the manifold of unitary and symmetric matrices, deriving its tangent space and its geodesics. The resulting parameterization of the geodesics (through a real and symmetric matrix) allows us to derive a new Riemannian manifold optimization (MO) algorithm whose most remarkable feature is that it does not need to set any adaptation parameter. We apply the proposed MO algorithm to maximize the achievable rate in a multiple-input multiple-output (MIMO) system assisted by a beyond-diagonal reconfigurable intelligent surface (BD-RIS), illustrating the method's performance through simulations. The MO algorithm achieves a significant reduction in computational cost compared to previous alternatives based on Takagi decomposition, while retaining global convergence to a stationary point of the cost function.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4e25\u683c\u523b\u753b\u4e86\u9149\u77e9\u9635\u548c\u5bf9\u79f0\u77e9\u9635\u7684\u6d41\u5f62\uff0c\u63a8\u5bfc\u4e86\u5176\u5207\u7a7a\u95f4\u548c\u6d4b\u5730\u7ebf\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u8c03\u8282\u53c2\u6570\u7684\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8eBD-RIS\u8f85\u52a9\u7684MIMO\u7cfb\u7edf\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u7b97\u6cd5\u901a\u5e38\u9700\u8981\u8bbe\u7f6e\u8c03\u6574\u53c2\u6570\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5e26\u6765\u4e0d\u4fbf\u3002\u540c\u65f6\uff0c\u5bf9\u4e8e\u9149\u77e9\u9635\u548c\u5bf9\u79f0\u77e9\u9635\u6d41\u5f62\u7684\u4e25\u683c\u6570\u5b66\u523b\u753b\u5c1a\u672a\u5b8c\u5168\u5efa\u7acb\uff0c\u9650\u5236\u4e86\u76f8\u5173\u4f18\u5316\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u9996\u5148\u4e25\u683c\u63a8\u5bfc\u4e86\u9149\u77e9\u9635\u548c\u5bf9\u79f0\u77e9\u9635\u6d41\u5f62\u7684\u5207\u7a7a\u95f4\u548c\u6d4b\u5730\u7ebf\uff0c\u901a\u8fc7\u5b9e\u5bf9\u79f0\u77e9\u9635\u53c2\u6570\u5316\u6d4b\u5730\u7ebf\u3002\u57fa\u4e8e\u6b64\u53c2\u6570\u5316\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u9700\u8c03\u8282\u53c2\u6570\u7684\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eBD-RIS\u8f85\u52a9\u7684MIMO\u7cfb\u7edf\u901f\u7387\u6700\u5927\u5316\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684MO\u7b97\u6cd5\u76f8\u6bd4\u57fa\u4e8eTakagi\u5206\u89e3\u7684\u5148\u524d\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5411\u6210\u672c\u51fd\u6570\u9a7b\u70b9\u7684\u5168\u5c40\u6536\u655b\u6027\u3002\u5728BD-RIS\u8f85\u52a9\u7684MIMO\u7cfb\u7edf\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5efa\u7acb\u4e86\u9149\u77e9\u9635\u548c\u5bf9\u79f0\u77e9\u9635\u6d41\u5f62\u7684\u4e25\u683c\u6570\u5b66\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u65e0\u9700\u8c03\u8282\u53c2\u6570\u7684\u9ad8\u6548\u9ece\u66fc\u6d41\u5f62\u4f18\u5316\u7b97\u6cd5\uff0c\u4e3a\u76f8\u5173\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2601.12095", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12095", "abs": "https://arxiv.org/abs/2601.12095", "authors": ["Hamidreza Sadeghi", "Saeedeh Momtazi", "Reza Safabakhsh"], "title": "Neural Isomorphic Fields: A Transformer-based Algebraic Numerical Embedding", "comment": null, "summary": "Neural network models often face challenges when processing very small or very large numbers due to issues such as overflow, underflow, and unstable output variations. To mitigate these problems, we propose using embedding vectors for numbers instead of directly using their raw values. These embeddings aim to retain essential algebraic properties while preventing numerical instabilities. In this paper, we introduce, for the first time, a fixed-length number embedding vector that preserves algebraic operations, including addition, multiplication, and comparison, within the field of rational numbers. We propose a novel Neural Isomorphic Field, a neural abstraction of algebraic structures such as groups and fields. The elements of this neural field are embedding vectors that maintain algebraic structure during computations. Our experiments demonstrate that addition performs exceptionally well, achieving over 95 percent accuracy on key algebraic tests such as identity, closure, and associativity. In contrast, multiplication exhibits challenges, with accuracy ranging from 53 percent to 73 percent across various algebraic properties. These findings highlight the model's strengths in preserving algebraic properties under addition while identifying avenues for further improvement in handling multiplication.", "AI": {"tldr": "\u63d0\u51fa\u56fa\u5b9a\u957f\u5ea6\u6570\u5b57\u5d4c\u5165\u5411\u91cf\uff0c\u4fdd\u7559\u6709\u7406\u6570\u4ee3\u6570\u8fd0\u7b97\u7279\u6027\uff0c\u901a\u8fc7\u795e\u7ecf\u540c\u6784\u573a\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u6781\u503c\u6570\u5b57\u65f6\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u6781\u5c0f\u6216\u6781\u5927\u6570\u503c\u65f6\u9762\u4e34\u6ea2\u51fa\u3001\u4e0b\u6ea2\u548c\u8f93\u51fa\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u4ee3\u6570\u7279\u6027\u540c\u65f6\u907f\u514d\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u6570\u5b57\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56fa\u5b9a\u957f\u5ea6\u7684\u6570\u5b57\u5d4c\u5165\u5411\u91cf\u8868\u793a\uff0c\u5f15\u5165\u795e\u7ecf\u540c\u6784\u573a\u4f5c\u4e3a\u4ee3\u6570\u7ed3\u6784\uff08\u7fa4\u3001\u57df\uff09\u7684\u795e\u7ecf\u62bd\u8c61\uff0c\u5d4c\u5165\u5411\u91cf\u5728\u8ba1\u7b97\u4e2d\u4fdd\u6301\u4ee3\u6570\u7ed3\u6784\u3002", "result": "\u52a0\u6cd5\u8fd0\u7b97\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6052\u7b49\u6027\u3001\u5c01\u95ed\u6027\u3001\u7ed3\u5408\u6027\u7b49\u5173\u952e\u4ee3\u6570\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8d85\u8fc795%\uff1b\u4e58\u6cd5\u8fd0\u7b97\u9762\u4e34\u6311\u6218\uff0c\u5728\u4e0d\u540c\u4ee3\u6570\u7279\u6027\u4e0a\u51c6\u786e\u7387\u572853%-73%\u4e4b\u95f4\u3002", "conclusion": "\u6a21\u578b\u5728\u4fdd\u6301\u52a0\u6cd5\u8fd0\u7b97\u7684\u4ee3\u6570\u7279\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u4e58\u6cd5\u8fd0\u7b97\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u6570\u5b57\u5d4c\u5165\u548c\u4ee3\u6570\u7ed3\u6784\u4fdd\u7559\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2601.13934", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.13934", "abs": "https://arxiv.org/abs/2601.13934", "authors": ["Phuong Nam Tran", "Nhan Thanh Nguyen", "Hien Quoc Ngo", "Markku Juntti"], "title": "Deep Reinforcement Learning-Based Dynamic Resource Allocation in Cell-Free Massive MIMO", "comment": null, "summary": "In this paper, we consider power allocation and antenna activation of cell-free massive multiple-input multiple-output (CFmMIMO) systems. We first derive closed-form expressions for the system spectral efficiency (SE) and energy efficiency (EE) as functions of the power allocation coefficients and the number of active antennas at the access points (APs). Then, we aim to enhance the EE through jointly optimizing antenna activation and power control. This task leads to a non-convex and mixed-integer design problem with high-dimensional design variables. To address this, we propose a novel DRL-based framework, in which the agent learns to map large-scale fading coefficients to AP activation ratio, antenna coefficient, and power coefficient. These coefficients are then employed to determine the number of active antennas per AP and the power factors assigned to users based on closed-form expressions. By optimizing these parameters instead of directly controlling antenna selection and power allocation, the proposed method transforms the intractable optimization into a low-dimensional learning task. Our extensive simulations demonstrate the efficiency and scalability of the proposed scheme. Specifically, in a CFmMIMO system with 40 APs and 20 users, it achieves a 50% EE improvement and 3350 times run time reduction compared to the conventional sequential convex approximation method.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u4f18\u5316\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u5929\u7ebf\u6fc0\u6d3b\u548c\u529f\u7387\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\u3002", "motivation": "\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u9700\u8981\u540c\u65f6\u4f18\u5316\u5929\u7ebf\u6fc0\u6d3b\u548c\u529f\u7387\u5206\u914d\u4ee5\u63d0\u9ad8\u80fd\u91cf\u6548\u7387\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a\u975e\u51f8\u3001\u6df7\u5408\u6574\u6570\u7684\u9ad8\u7ef4\u4f18\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u6846\u67b6\uff0c\u667a\u80fd\u4f53\u5b66\u4e60\u5c06\u5927\u5c3a\u5ea6\u8870\u843d\u7cfb\u6570\u6620\u5c04\u5230AP\u6fc0\u6d3b\u6bd4\u4f8b\u3001\u5929\u7ebf\u7cfb\u6570\u548c\u529f\u7387\u7cfb\u6570\uff0c\u7136\u540e\u901a\u8fc7\u95ed\u5f0f\u8868\u8fbe\u5f0f\u786e\u5b9a\u6bcf\u4e2aAP\u7684\u6fc0\u6d3b\u5929\u7ebf\u6570\u548c\u7528\u6237\u529f\u7387\u5206\u914d\uff0c\u5c06\u9ad8\u7ef4\u4f18\u5316\u8f6c\u5316\u4e3a\u4f4e\u7ef4\u5b66\u4e60\u4efb\u52a1\u3002", "result": "\u572840\u4e2aAP\u548c20\u4e2a\u7528\u6237\u7684CFmMIMO\u7cfb\u7edf\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u987a\u5e8f\u51f8\u903c\u8fd1\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8650%\u7684\u80fd\u91cf\u6548\u7387\u63d0\u5347\u548c3350\u500d\u7684\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684DRL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u5929\u7ebf\u6fc0\u6d3b\u548c\u529f\u7387\u5206\u914d\u7684\u8054\u5408\u4f18\u5316\u96be\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u80fd\u91cf\u6548\u7387\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2601.12124", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12124", "abs": "https://arxiv.org/abs/2601.12124", "authors": ["Bing Hu", "Yixin Li", "Asma Bahamyirou", "Helen Chen"], "title": "SynQP: A Framework and Metrics for Evaluating the Quality and Privacy Risk of Synthetic Data", "comment": "7 Pages, 22nd Annual International Conference on Privacy, Security, and Trust (PST2025), Fredericton, Canada", "summary": "The use of synthetic data in health applications raises privacy concerns, yet the lack of open frameworks for privacy evaluations has slowed its adoption. A major challenge is the absence of accessible benchmark datasets for evaluating privacy risks, due to difficulties in acquiring sensitive data. To address this, we introduce SynQP, an open framework for benchmarking privacy in synthetic data generation (SDG) using simulated sensitive data, ensuring that original data remains confidential. We also highlight the need for privacy metrics that fairly account for the probabilistic nature of machine learning models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new identity disclosure risk metric that offers a more accurate estimation of privacy risks compared to existing approaches. Our work provides a critical tool for improving the transparency and reliability of privacy evaluations, enabling safer use of synthetic data in health-related applications. % In our quality evaluations, non-private models achieved near-perfect machine-learning efficacy \\(\\ge0.97\\). Our privacy assessments (Table II) reveal that DP consistently lowers both identity disclosure risk (SD-IDR) and membership-inference attack risk (SD-MIA), with all DP-augmented models staying below the 0.09 regulatory threshold. Code available at https://github.com/CAN-SYNH/SynQP", "AI": {"tldr": "SynQP\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5408\u6210\u6570\u636e\u9690\u79c1\u98ce\u9669\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u62df\u654f\u611f\u6570\u636e\u907f\u514d\u771f\u5b9e\u6570\u636e\u6cc4\u9732\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u51c6\u786e\u7684\u9690\u79c1\u5ea6\u91cf\u6307\u6807\u3002", "motivation": "\u5065\u5eb7\u5e94\u7528\u4e2d\u5408\u6210\u6570\u636e\u4f7f\u7528\u5b58\u5728\u9690\u79c1\u62c5\u5fe7\uff0c\u4f46\u7f3a\u4e4f\u5f00\u653e\u7684\u9690\u79c1\u8bc4\u4f30\u6846\u67b6\u548c\u53ef\u8bbf\u95ee\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u5176\u91c7\u7528\u3002", "method": "\u5f00\u53d1SynQP\u5f00\u6e90\u6846\u67b6\uff0c\u4f7f\u7528\u6a21\u62df\u654f\u611f\u6570\u636e\u8fdb\u884c\u9690\u79c1\u57fa\u51c6\u6d4b\u8bd5\uff1b\u63d0\u51fa\u65b0\u7684\u8eab\u4efd\u62ab\u9732\u98ce\u9669\u5ea6\u91cf\u6307\u6807\uff0c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u9690\u79c1\u98ce\u9669\uff1b\u4ee5CTGAN\u4e3a\u4f8b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u975e\u79c1\u6709\u6a21\u578b\u5728\u673a\u5668\u5b66\u4e60\u6548\u80fd\u4e0a\u8fbe\u5230\u22650.97\uff1b\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u80fd\u6709\u6548\u964d\u4f4e\u8eab\u4efd\u62ab\u9732\u98ce\u9669\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\u98ce\u9669\uff0c\u6240\u6709DP\u589e\u5f3a\u6a21\u578b\u5747\u4f4e\u4e8e0.09\u7684\u76d1\u7ba1\u9608\u503c\u3002", "conclusion": "SynQP\u4e3a\u9690\u79c1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u4f7f\u5408\u6210\u6570\u636e\u5728\u5065\u5eb7\u76f8\u5173\u5e94\u7528\u4e2d\u66f4\u5b89\u5168\u5730\u4f7f\u7528\u3002"}}
{"id": "2601.12131", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.12131", "abs": "https://arxiv.org/abs/2601.12131", "authors": ["Santosh Chapagain", "MohammadReza EskandariNasab", "Onur Vural", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics", "comment": "This is preliminary work towards a broader SolarGPT framework", "summary": "Solar activity, including solar flares, coronal mass ejections (CMEs), and geomagnetic storms, can significantly impact satellites, aviation, power grids, data centers, and space missions. Extreme solar events can cause substantial economic damage if not predicted in advance, highlighting the importance of accurate forecasting and effective education in space science. Although large language models (LLMs) perform well on general tasks, they often lack domain-specific knowledge and pedagogical capability to clearly explain complex space science concepts.\n  We introduce SolarGPT-QA, a question answering system based on a domain-adapted large language model built on the LLaMA-3 base model. The model is trained using scientific literature and large-scale question-answer data generated with GPT-4 and refined using Grok-3 in a student-friendly storytelling style. Human pairwise evaluations show that SolarGPT-QA outperforms general-purpose models in zero-shot settings and achieves competitive performance compared to instruction-tuned models for educational explanations in space weather and heliophysics. A small pilot student comprehension study further suggests improved clarity and accessibility of the generated explanations. Ablation experiments indicate that combining domain-adaptive pretraining with pedagogical fine-tuning is important for balancing scientific accuracy and educational effectiveness. This work represents an initial step toward a broader SolarGPT framework for space science education and forecasting.", "AI": {"tldr": "SolarGPT-QA\u662f\u57fa\u4e8eLLaMA-3\u6784\u5efa\u7684\u9886\u57df\u81ea\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u95ee\u7b54\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u7a7a\u95f4\u5929\u6c14\u548c\u592a\u9633\u7269\u7406\u5b66\u6559\u80b2\uff0c\u901a\u8fc7\u79d1\u5b66\u6587\u732e\u548cGPT-4\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u592a\u9633\u6d3b\u52a8\uff08\u592a\u9633\u8000\u6591\u3001\u65e5\u5195\u7269\u8d28\u629b\u5c04\u7b49\uff09\u5bf9\u536b\u661f\u3001\u7535\u7f51\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u6781\u7aef\u4e8b\u4ef6\u53ef\u80fd\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\u3002\u5f53\u524d\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u7f3a\u4e4f\u6e05\u6670\u89e3\u91ca\u590d\u6742\u7a7a\u95f4\u79d1\u5b66\u6982\u5ff5\u7684\u6559\u5b66\u80fd\u529b\u3002", "method": "\u57fa\u4e8eLLaMA-3\u57fa\u7840\u6a21\u578b\u6784\u5efaSolarGPT-QA\u95ee\u7b54\u7cfb\u7edf\uff0c\u4f7f\u7528\u79d1\u5b66\u6587\u732e\u548c\u5927\u89c4\u6a21\u95ee\u7b54\u6570\u636e\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff0c\u6570\u636e\u7531GPT-4\u751f\u6210\u5e76\u901a\u8fc7Grok-3\u4ee5\u5b66\u751f\u53cb\u597d\u7684\u6545\u4e8b\u5316\u98ce\u683c\u7cbe\u70bc\u3002\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6559\u5b66\u5fae\u8c03\u3002", "result": "\u4eba\u5de5\u6210\u5bf9\u8bc4\u4f30\u663e\u793a\uff0cSolarGPT-QA\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5728\u6559\u5b66\u89e3\u91ca\u65b9\u9762\u4e0e\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u7ade\u4e89\u3002\u5c0f\u578b\u8bd5\u70b9\u5b66\u751f\u7406\u89e3\u7814\u7a76\u8868\u660e\u751f\u6210\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\u548c\u53ef\u8bbf\u95ee\u6027\u6709\u6240\u6539\u5584\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u548c\u6559\u5b66\u5fae\u8c03\u5bf9\u5e73\u8861\u79d1\u5b66\u51c6\u786e\u6027\u548c\u6559\u5b66\u6548\u679c\u5f88\u91cd\u8981\u3002", "conclusion": "SolarGPT-QA\u5728\u7a7a\u95f4\u5929\u6c14\u548c\u592a\u9633\u7269\u7406\u5b66\u6559\u80b2\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u8fc8\u5411\u66f4\u5e7f\u6cdb\u7684SolarGPT\u7a7a\u95f4\u79d1\u5b66\u6559\u80b2\u548c\u9884\u62a5\u6846\u67b6\u7684\u521d\u6b65\u6b65\u9aa4\uff0c\u5c55\u793a\u4e86\u9886\u57df\u81ea\u9002\u5e94\u4e0e\u6559\u5b66\u5fae\u8c03\u7ed3\u5408\u7684\u4ef7\u503c\u3002"}}
{"id": "2601.13997", "categories": ["eess.SP", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.13997", "abs": "https://arxiv.org/abs/2601.13997", "authors": ["Xuehan Wang", "Jinhong Yuan", "Jintao Wang", "Kehan Huang"], "title": "Achieving Full Multipath Diversity by Random Constellation Rotation: a Theoretical Perspective", "comment": "10 pages, 5 figures. This paper has been accepted for publication in IEEE TSP", "summary": "Diversity is an essential concept associated with communication reliability in multipath channels since it determines the slope of bit error rate performance in the medium to high signal-to-noise ratio regions. However, most of the existing analytical frameworks were developed for specific modulation schemes while the efficient validation of full multipath diversity for general modulation schemes remains an open problem. To fill this research gap, we propose to utilize random constellation rotation to ease the conditions for full-diversity modulation designs. For linearly precoded cyclic-prefix orthogonal frequency division multiplexing (OFDM) systems, we prove that maximum multipath diversity can be attained as long as the spread matrix does not have zero entries, which is a sufficient but easily satisfied condition. Furthermore, we derive the sufficient and necessary condition for general modulation schemes, whose verification can be divided into validation tasks for each column of the modulation matrix. Based on the proposed conditions, maximum diversity order can be attained with the probability of 1 by enabling a randomly generated rotation pattern for both time and doubly dispersive channels. The theoretical analysis in this paper also demonstrates that the diversity evaluation can be concentrated on the pairwise error probability when the number of error symbols is one, which reduces the complexity of diversity-driven design and performance analysis for novel modulation schemes significantly in both time and doubly dispersive channels. Finally, numerical results for various modulation schemes confirm that the theoretical analysis holds in both time and doubly dispersive channels. Furthermore, when employing practical detectors, the random constellation rotation technique consistently enhance the transmission reliability for both coded and uncoded systems.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u968f\u673a\u661f\u5ea7\u65cb\u8f6c\u7b80\u5316\u5168\u5206\u96c6\u8c03\u5236\u8bbe\u8ba1\u6761\u4ef6\uff0c\u8bc1\u660e\u7ebf\u6027\u9884\u7f16\u7801CP-OFDM\u7cfb\u7edf\u53ea\u8981\u6269\u5c55\u77e9\u9635\u65e0\u96f6\u5143\u7d20\u5373\u53ef\u83b7\u5f97\u6700\u5927\u591a\u5f84\u5206\u96c6\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e00\u822c\u8c03\u5236\u65b9\u6848\u7684\u5145\u8981\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u5206\u6790\u6846\u67b6\u5927\u591a\u9488\u5bf9\u7279\u5b9a\u8c03\u5236\u65b9\u6848\uff0c\u800c\u9a8c\u8bc1\u4e00\u822c\u8c03\u5236\u65b9\u6848\u7684\u5168\u591a\u5f84\u5206\u96c6\u6548\u7387\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u7b80\u5316\u5168\u5206\u96c6\u8c03\u5236\u8bbe\u8ba1\u6761\u4ef6\u3002", "method": "\u91c7\u7528\u968f\u673a\u661f\u5ea7\u65cb\u8f6c\u6280\u672f\uff0c\u63a8\u5bfc\u7ebf\u6027\u9884\u7f16\u7801CP-OFDM\u7cfb\u7edf\u7684\u5145\u5206\u6761\u4ef6\uff08\u6269\u5c55\u77e9\u9635\u65e0\u96f6\u5143\u7d20\uff09\uff0c\u5e76\u5efa\u7acb\u4e00\u822c\u8c03\u5236\u65b9\u6848\u7684\u5145\u8981\u6761\u4ef6\u9a8c\u8bc1\u6846\u67b6\uff0c\u5c06\u9a8c\u8bc1\u4efb\u52a1\u5206\u89e3\u4e3a\u8c03\u5236\u77e9\u9635\u5404\u5217\u7684\u72ec\u7acb\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u901a\u8fc7\u542f\u7528\u968f\u673a\u751f\u6210\u7684\u65cb\u8f6c\u6a21\u5f0f\uff0c\u5728\u65f6\u95f4\u548c\u53cc\u5f25\u6563\u4fe1\u9053\u4e2d\u4ee5\u6982\u73871\u83b7\u5f97\u6700\u5927\u5206\u96c6\u9636\u6570\u3002\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u6709\u6548\u6027\uff0c\u968f\u673a\u661f\u5ea7\u65cb\u8f6c\u6280\u672f\u80fd\u6301\u7eed\u63d0\u5347\u7f16\u7801\u548c\u975e\u7f16\u7801\u7cfb\u7edf\u7684\u4f20\u8f93\u53ef\u9760\u6027\u3002", "conclusion": "\u968f\u673a\u661f\u5ea7\u65cb\u8f6c\u7b80\u5316\u4e86\u5168\u5206\u96c6\u8c03\u5236\u8bbe\u8ba1\u6761\u4ef6\uff0c\u5c06\u5206\u96c6\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u5355\u7b26\u53f7\u9519\u8bef\u6982\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65b0\u8c03\u5236\u65b9\u6848\u7684\u5206\u96c6\u9a71\u52a8\u8bbe\u8ba1\u548c\u6027\u80fd\u5206\u6790\u590d\u6742\u5ea6\uff0c\u4e3a\u4e00\u822c\u8c03\u5236\u65b9\u6848\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u96c6\u9a8c\u8bc1\u6846\u67b6\u3002"}}
{"id": "2601.12137", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12137", "abs": "https://arxiv.org/abs/2601.12137", "authors": ["Anzhe Cheng", "Shukai Duan", "Shixuan Li", "Chenzhong Yin", "Mingxi Cheng", "Shahin Nazarian", "Paul Thompson", "Paul Bogdan"], "title": "EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts", "comment": "accepted by ICASSP2026", "summary": "The relentless scaling of deep learning models has led to unsustainable computational demands, positioning Mixture-of-Experts (MoE) architectures as a promising path towards greater efficiency. However, MoE models are plagued by two fundamental challenges: 1) a load imbalance problem known as the``rich get richer\" phenomenon, where a few experts are over-utilized, and 2) an expert homogeneity problem, where experts learn redundant representations, negating their purpose. Current solutions typically employ an auxiliary load-balancing loss that, while mitigating imbalance, often exacerbates homogeneity by enforcing uniform routing at the expense of specialization. To resolve this, we introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE projects input tokens onto this shared eigenbasis and routes them based on their alignment with the principal components of the feature space. This principled, geometric partitioning of data intrinsically promotes both balanced expert utilization and the development of diverse, specialized experts, all without the need for a conflicting auxiliary loss function. Our code is publicly available at https://github.com/Belis0811/EMoE.", "AI": {"tldr": "EMoE\u63d0\u51fa\u57fa\u4e8e\u6b63\u4ea4\u7279\u5f81\u57fa\u7684\u8def\u7531\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u8f93\u5165token\u6295\u5f71\u5230\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u7684\u4e3b\u6210\u5206\u4e0a\uff0c\u5b9e\u73b0\u8d1f\u8f7d\u5747\u8861\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\uff0c\u65e0\u9700\u989d\u5916\u7684\u8d1f\u8f7d\u5747\u8861\u635f\u5931\u51fd\u6570\u3002", "motivation": "MoE\u67b6\u6784\u867d\u7136\u80fd\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a1) \"\u5bcc\u8005\u6108\u5bcc\"\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u5c11\u6570\u4e13\u5bb6\u88ab\u8fc7\u5ea6\u4f7f\u7528\uff1b2) \u4e13\u5bb6\u540c\u8d28\u5316\u95ee\u9898\uff0c\u4e13\u5bb6\u5b66\u4e60\u5197\u4f59\u8868\u793a\uff0c\u5931\u53bb\u4e86\u4e13\u4e1a\u5316\u610f\u4e49\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f7f\u7528\u8f85\u52a9\u8d1f\u8f7d\u5747\u8861\u635f\u5931\uff0c\u4f46\u8fd9\u4f1a\u52a0\u5267\u540c\u8d28\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faEigen-Mixture-of-Experts (EMoE)\u67b6\u6784\uff0c\u91c7\u7528\u57fa\u4e8e\u5b66\u4e60\u6b63\u4ea4\u7279\u5f81\u57fa\u7684\u8def\u7531\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u5c06\u8f93\u5165token\u6295\u5f71\u5230\u5171\u4eab\u7279\u5f81\u7a7a\u95f4\u7684\u6b63\u4ea4\u57fa\u4e0a\uff0c\u6839\u636etoken\u4e0e\u4e3b\u6210\u5206\u7684\u5bf9\u9f50\u7a0b\u5ea6\u8fdb\u884c\u8def\u7531\uff0c\u5b9e\u73b0\u6570\u636e\u7684\u51e0\u4f55\u5212\u5206\u3002", "result": "EMoE\u80fd\u591f\u540c\u65f6\u4fc3\u8fdb\u4e13\u5bb6\u8d1f\u8f7d\u5747\u8861\u548c\u4e13\u4e1a\u5316\u53d1\u5c55\uff0c\u65e0\u9700\u4f7f\u7528\u51b2\u7a81\u7684\u8f85\u52a9\u635f\u5931\u51fd\u6570\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u5728GitHub\u4e0a\u3002", "conclusion": "EMoE\u901a\u8fc7\u57fa\u4e8e\u6b63\u4ea4\u7279\u5f81\u57fa\u7684\u8def\u7531\u673a\u5236\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u4e86MoE\u67b6\u6784\u4e2d\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u4e13\u5bb6\u540c\u8d28\u5316\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14080", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14080", "abs": "https://arxiv.org/abs/2601.14080", "authors": ["Alexander Ihlow", "Marius Schmidt", "Carsten Andrich", "Reiner S. Thom\u00e4"], "title": "Background Subtraction with Drift Correction for Bistatic Radar Reflectivity Measurements", "comment": "20th European Conference on Antennas and Propagation (EuCAP 2026)", "summary": "Fundamental research on bistatic radar reflectivity is highly relevant, e.g., to the upcoming mobile communication standard 6G, which includes integrated sensing and communication (ISAC). We introduce a model for correcting instrumentation drift during bistatic radar measurements in anechoic chambers. Usually, background subtraction is applied with the goal to yield the target reflection signal as best as possible while coherently subtracting all signals which were present in both the foreground and background measurement. However, even slight incoherences between the foreground and background measurement process deteriorate the result. We analyze these effects in real measurements in the frequency range 2-18 GHz, taken with the Bistatic Radar (BIRA) measurement facility at TU Ilmenau. Applying our proposed drift correction model, we demonstrate up to 40 dB improvement for the removal of direct line-of-sight antenna crosstalk over the state of the art.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u6821\u6b63\u65e0\u56de\u6ce2\u5ba4\u53cc\u57fa\u5730\u96f7\u8fbe\u6d4b\u91cf\u4e2d\u4eea\u5668\u6f02\u79fb\u7684\u6a21\u578b\uff0c\u53ef\u663e\u8457\u6539\u5584\u5929\u7ebf\u4e32\u6270\u6d88\u9664\u6548\u679c", "motivation": "\u53cc\u57fa\u5730\u96f7\u8fbe\u53cd\u5c04\u7387\u7814\u7a76\u5bf96G\u79fb\u52a8\u901a\u4fe1\u6807\u51c6\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u81f3\u5173\u91cd\u8981\u3002\u5728\u65e0\u56de\u6ce2\u5ba4\u6d4b\u91cf\u4e2d\uff0c\u80cc\u666f\u51cf\u6cd5\u7684\u6709\u6548\u6027\u53d7\u5230\u524d\u666f\u548c\u80cc\u666f\u6d4b\u91cf\u8fc7\u7a0b\u4e4b\u95f4\u8f7b\u5fae\u975e\u76f8\u5e72\u6027\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u7ed3\u679c\u6076\u5316\u3002", "method": "\u5f15\u5165\u4eea\u5668\u6f02\u79fb\u6821\u6b63\u6a21\u578b\uff0c\u5206\u67902-18GHz\u9891\u6bb5\u7684\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\uff0c\u4f7f\u7528TU Ilmenau\u7684\u53cc\u57fa\u5730\u96f7\u8fbe(BIRA)\u6d4b\u91cf\u8bbe\u65bd\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5e94\u7528\u63d0\u51fa\u7684\u6f02\u79fb\u6821\u6b63\u6a21\u578b\u540e\uff0c\u5728\u6d88\u9664\u89c6\u7ebf\u5929\u7ebf\u4e32\u6270\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u5347\u4e8640dB\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6f02\u79fb\u6821\u6b63\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u53cc\u57fa\u5730\u96f7\u8fbe\u6d4b\u91cf\u4e2d\u7684\u4eea\u5668\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u80cc\u666f\u51cf\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u5bf9ISAC\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.12145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12145", "abs": "https://arxiv.org/abs/2601.12145", "authors": ["Xingyue Huang", "Xueying Ding", "Mingxuan Ju", "Yozen Liu", "Neil Shah", "Tong Zhao"], "title": "Threshold Differential Attention for Sink-Free, Ultra-Sparse, and Non-Dispersive Language Modeling", "comment": null, "summary": "Softmax attention struggles with long contexts due to structural limitations: the strict sum-to-one constraint forces attention sinks on irrelevant tokens, and probability mass disperses as sequence lengths increase. We tackle these problems with Threshold Differential Attention (TDA), a sink-free attention mechanism that achieves ultra-sparsity and improved robustness at longer sequence lengths without the computational overhead of projection methods or the performance degradation caused by noise accumulation of standard rectified attention. TDA applies row-wise extreme-value thresholding with a length-dependent gate, retaining only exceedances. Inspired by the differential transformer, TDA also subtracts an inhibitory view to enhance expressivity. Theoretically, we prove that TDA controls the expected number of spurious survivors per row to $O(1)$ and that consensus spurious matches across independent views vanish as context grows. Empirically, TDA produces $>99\\%$ exact zeros and eliminates attention sinks while maintaining competitive performance on standard and long-context benchmarks.", "AI": {"tldr": "TDA\u662f\u4e00\u79cd\u65e0\u6ce8\u610f\u529b\u6c89\u6ca1\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u9608\u503c\u5dee\u5206\u65b9\u6cd5\u5b9e\u73b0\u8d85\u7a00\u758f\u6027\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ea7\u751f\u8d85\u8fc799%\u7684\u7cbe\u786e\u96f6\u503c\u3002", "motivation": "\u89e3\u51b3Softmax\u6ce8\u610f\u529b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u7ed3\u6784\u9650\u5236\uff1a\u4e25\u683c\u7684\u5f52\u4e00\u5316\u7ea6\u675f\u5bfc\u81f4\u6ce8\u610f\u529b\u6c89\u6ca1\u5728\u65e0\u5173\u4ee4\u724c\u4e0a\uff0c\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0c\u6982\u7387\u8d28\u91cf\u5206\u6563\u3002", "method": "\u63d0\u51fa\u9608\u503c\u5dee\u5206\u6ce8\u610f\u529b\uff08TDA\uff09\uff1a\u91c7\u7528\u884c\u7ea7\u6781\u503c\u9608\u503c\u5316\u914d\u5408\u957f\u5ea6\u76f8\u5173\u95e8\u63a7\uff0c\u53ea\u4fdd\u7559\u8d85\u8fc7\u9608\u503c\u7684\u503c\uff1b\u501f\u9274\u5dee\u5206\u53d8\u6362\u5668\u601d\u60f3\uff0c\u51cf\u53bb\u6291\u5236\u89c6\u56fe\u4ee5\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660eTDA\u6bcf\u884c\u865a\u5047\u5e78\u5b58\u8005\u671f\u671b\u4e3aO(1)\uff0c\u72ec\u7acb\u89c6\u56fe\u95f4\u7684\u865a\u5047\u5339\u914d\u968f\u4e0a\u4e0b\u6587\u589e\u957f\u800c\u6d88\u5931\uff1b\u5b9e\u8bc1\u663e\u793a\u4ea7\u751f>99%\u7cbe\u786e\u96f6\u503c\uff0c\u6d88\u9664\u6ce8\u610f\u529b\u6c89\u6ca1\uff0c\u5728\u6807\u51c6\u53ca\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "TDA\u89e3\u51b3\u4e86Softmax\u6ce8\u610f\u529b\u7684\u957f\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u8d85\u7a00\u758f\u3001\u65e0\u6c89\u6ca1\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2601.14220", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14220", "abs": "https://arxiv.org/abs/2601.14220", "authors": ["Wenyi Yan", "Zeyuan Li", "Lu Gan", "Honqing Liu", "Guoquan Li"], "title": "Bit-Efficient Quantisation for Two-Channel Modulo-Sampling Systems", "comment": null, "summary": "Two-channel modulo analog-to-digital converters (ADCs) enable high-dynamic-range signal sensing at the Nyquist rate per channel, but existing designs quantise both channel outputs independently, incurring redundant bitrate costs. This paper proposes a bit-efficient quantisation scheme that exploits the integer-valued structure of inter-channel differences, transmitting one quantised channel output together with a compact difference index. We prove that this approach requires only 1-2 bits per signal sample overhead relative to conventional ADCs, despite operating with a much smaller per-channel dynamic range. Simulations confirm the theoretical error bounds and bitrate analysis, while hardware experiments demonstrate substantial bitrate savings compared with existing modulo sampling schemes, while maintaining comparable reconstruction accuracy. These results highlight a practical path towards high-resolution, bandwidth-efficient modulo ADCs for bitrate-constrained systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6bd4\u7279\u9ad8\u6548\u7684\u4e24\u901a\u9053\u6a21\u6570ADC\u91cf\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u5229\u7528\u901a\u9053\u95f4\u5dee\u503c\u7684\u6574\u6570\u7ed3\u6784\uff0c\u4f20\u8f93\u4e00\u4e2a\u91cf\u5316\u901a\u9053\u8f93\u51fa\u548c\u7d27\u51d1\u7684\u5dee\u503c\u7d22\u5f15\uff0c\u76f8\u6bd4\u4f20\u7edfADC\u4ec5\u97001-2\u6bd4\u7279/\u6837\u672c\u7684\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u4e24\u901a\u9053\u6a21\u6570ADC\u8bbe\u8ba1\u72ec\u7acb\u91cf\u5316\u4e24\u4e2a\u901a\u9053\u8f93\u51fa\uff0c\u5bfc\u81f4\u5197\u4f59\u6bd4\u7279\u7387\u6210\u672c\u3002\u9700\u8981\u5728\u4fdd\u6301\u9ad8\u52a8\u6001\u8303\u56f4\u4fe1\u53f7\u611f\u77e5\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u6bd4\u7279\u7387\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u6bd4\u7279\u9ad8\u6548\u91cf\u5316\u65b9\u6848\uff0c\u5229\u7528\u901a\u9053\u95f4\u5dee\u503c\u4e3a\u6574\u6570\u7684\u7ed3\u6784\u7279\u6027\uff0c\u4ec5\u4f20\u8f93\u4e00\u4e2a\u91cf\u5316\u901a\u9053\u8f93\u51fa\u548c\u7d27\u51d1\u7684\u5dee\u503c\u7d22\u5f15\uff0c\u800c\u975e\u72ec\u7acb\u91cf\u5316\u4e24\u4e2a\u901a\u9053\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfADC\u4ec5\u97001-2\u6bd4\u7279/\u6837\u672c\u7684\u5f00\u9500\uff0c\u4eff\u771f\u9a8c\u8bc1\u4e86\u7406\u8bba\u8bef\u5dee\u754c\u9650\u548c\u6bd4\u7279\u7387\u5206\u6790\uff0c\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u73b0\u6709\u6a21\u6570\u91c7\u6837\u65b9\u6848\u663e\u8457\u8282\u7701\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u6bd4\u7279\u7387\u53d7\u9650\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u3001\u5e26\u5bbd\u9ad8\u6548\u6a21\u6570ADC\u7684\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.14233", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14233", "abs": "https://arxiv.org/abs/2601.14233", "authors": ["Yekta Demirci", "Guillaume Mantelet", "Stephane Martel", "Jean-Francois Frigon", "Gunes Karabulut Kurt"], "title": "Burst Aware Forecasting of User Traffic Demand in LEO Satellite Networks", "comment": "Accepted by IEEE International Conference on Communications (ICC) 2026", "summary": "In Low Earth Orbit (LEO) satellite networks, Beam Hopping (BH) technology enables the efficient utilization of limited radio resources by adapting to varying user demands and link conditions. Effective BH planning requires prior knowledge of upcoming traffic at the time of scheduling, making forecasting an important sub-task. Forecasting becomes particularly critical under heavy load conditions where an unexpected demand burst combined with link degradation may cause buffer overflows and packet loss. To address this challenge, we propose a burst aware forecasting solution. This challenge may arise in a wide range of wireless networks; therefore, the proposed solution is broadly applicable to settings characterized by bursty traffic patterns where accurate demand forecasting is essential. Our approach introduces three key enhancements to a transformer architecture: (i) a distance from the last burst embedding to capture burst proximity, (ii) two additional linear layers in the decoder to forecast both upcoming bursts and their relative impact, and (iii) use of an asymmetric cost function during model training to better capture burst dynamics. Empirical evaluations in an Earth-fixed cell under high-traffic demand scenario demonstrate that the proposed model reduces prediction error by up to 94% at a one-step horizon and maintains the ability to accurately capture bursts even near the end of longer prediction horizons following Mean Square Error (MSE) metric.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u4f4e\u8f68\u536b\u661f\u7f51\u7edc\u6ce2\u675f\u8df3\u53d8\u8c03\u5ea6\u7684\u7a81\u53d1\u6d41\u91cf\u611f\u77e5\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbTransformer\u67b6\u6784\u6765\u51c6\u786e\u9884\u6d4b\u6d41\u91cf\u7a81\u53d1\uff0c\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u8fbe94%\u3002", "motivation": "\u5728\u4f4e\u8f68\u536b\u661f\u7f51\u7edc\u4e2d\uff0c\u6ce2\u675f\u8df3\u53d8\u6280\u672f\u9700\u8981\u63d0\u524d\u77e5\u9053\u672a\u6765\u6d41\u91cf\u6765\u8fdb\u884c\u8c03\u5ea6\u89c4\u5212\u3002\u5728\u91cd\u8d1f\u8f7d\u6761\u4ef6\u4e0b\uff0c\u610f\u5916\u7684\u6d41\u91cf\u7a81\u53d1\u7ed3\u5408\u94fe\u8def\u8d28\u91cf\u4e0b\u964d\u53ef\u80fd\u5bfc\u81f4\u7f13\u51b2\u533a\u6ea2\u51fa\u548c\u6570\u636e\u5305\u4e22\u5931\uff0c\u56e0\u6b64\u51c6\u786e\u7684\u6d41\u91cf\u9884\u6d4b\u7279\u522b\u662f\u7a81\u53d1\u6d41\u91cf\u9884\u6d4b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7a81\u53d1\u611f\u77e5\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9Transformer\u67b6\u6784\u8fdb\u884c\u4e09\u9879\u5173\u952e\u6539\u8fdb\uff1a(1) \u5f15\u5165\"\u8ddd\u79bb\u4e0a\u6b21\u7a81\u53d1\"\u7684\u5d4c\u5165\u8868\u793a\u6765\u6355\u6349\u7a81\u53d1\u4e34\u8fd1\u6027\uff1b(2) \u5728\u89e3\u7801\u5668\u4e2d\u589e\u52a0\u4e24\u4e2a\u7ebf\u6027\u5c42\uff0c\u5206\u522b\u9884\u6d4b\u5373\u5c06\u5230\u6765\u7684\u7a81\u53d1\u53ca\u5176\u76f8\u5bf9\u5f71\u54cd\uff1b(3) \u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u4f7f\u7528\u975e\u5bf9\u79f0\u635f\u5931\u51fd\u6570\u6765\u66f4\u597d\u5730\u6355\u6349\u7a81\u53d1\u52a8\u6001\u3002", "result": "\u5728\u9ad8\u6d41\u91cf\u9700\u6c42\u573a\u666f\u4e0b\u7684\u5730\u7403\u56fa\u5b9a\u5c0f\u533a\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u6a21\u578b\u5728\u4e00\u6b65\u9884\u6d4b\u8303\u56f4\u5185\u5c06\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e8694%\uff0c\u5e76\u4e14\u5728\u8f83\u957f\u9884\u6d4b\u8303\u56f4\u672b\u7aef\u4ecd\u80fd\u51c6\u786e\u6355\u6349\u7a81\u53d1\u6d41\u91cf\uff08\u57fa\u4e8eMSE\u6307\u6807\uff09\u3002", "conclusion": "\u8be5\u7a81\u53d1\u611f\u77e5\u9884\u6d4b\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u4f4e\u8f68\u536b\u661f\u7f51\u7edc\u4e2d\u6ce2\u675f\u8df3\u53d8\u8c03\u5ea6\u9762\u4e34\u7684\u6d41\u91cf\u7a81\u53d1\u9884\u6d4b\u6311\u6218\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u536b\u661f\u7f51\u7edc\uff0c\u4e5f\u9002\u7528\u4e8e\u5177\u6709\u7a81\u53d1\u6d41\u91cf\u6a21\u5f0f\u4e14\u9700\u8981\u51c6\u786e\u9700\u6c42\u9884\u6d4b\u7684\u5404\u79cd\u65e0\u7ebf\u7f51\u7edc\u573a\u666f\u3002"}}
{"id": "2601.12212", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12212", "abs": "https://arxiv.org/abs/2601.12212", "authors": ["Chenan Wang", "Daniel H. Shi", "Haipeng Chen"], "title": "Speculative Sampling with Reinforcement Learning", "comment": "Accepted to AAAI 2026", "summary": "Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\\times$ speedup over the backbone LLM and up to 1.12$\\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.", "AI": {"tldr": "\u63d0\u51faRe-SpS\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u63a8\u6d4b\u91c7\u6837\u7684\u6811\u7ed3\u6784\u8d85\u53c2\u6570\uff0c\u76f8\u6bd4\u9759\u6001\u8d85\u53c2\u6570\u7684EAGLE-3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f97\u6700\u9ad81.12\u500d\u52a0\u901f", "motivation": "\u73b0\u6709\u63a8\u6d4b\u91c7\u6837\u65b9\u6cd5\uff08\u5982EAGLE-3\uff09\u4f7f\u7528\u9759\u6001\u6811\u7ed3\u6784\u8d85\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u548c\u9886\u57df\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002\u9700\u8981\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\u6765\u5e73\u8861\u63a8\u6d4b\u7684\u6fc0\u8fdb\u7a0b\u5ea6\u4e0e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u6d4b\u91c7\u6837\u6846\u67b6Re-SpS\uff0c\u5229\u7528\u76ee\u6807\u6a21\u578b\u9690\u85cf\u72b6\u6001\u6784\u5efa\u9ad8\u6548\u72b6\u6001\u8868\u793a\uff0c\u5f15\u5165\u591a\u6b65\u52a8\u4f5c\u6301\u4e45\u5316\u4ee5\u66f4\u597d\u5730\u5efa\u6a21\u4e0a\u4e0b\u6587\uff0c\u5b9e\u65f6\u52a8\u6001\u8c03\u6574\u8349\u7a3f\u6811\u8d85\u53c2\u6570\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u9aa8\u5e72LLM\u83b7\u5f97\u6700\u9ad85.45\u500d\u52a0\u901f\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5EAGLE-3\u83b7\u5f97\u6700\u9ad81.12\u500d\u52a0\u901f\uff0c\u4e14\u6ca1\u6709\u8f93\u51fa\u8d28\u91cf\u635f\u5931\u3002", "conclusion": "Re-SpS\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u63a8\u6d4b\u91c7\u6837\u8d85\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14244", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2601.14244", "abs": "https://arxiv.org/abs/2601.14244", "authors": ["Qing Zhang", "Adham Sakhnini", "Robbert Beerten", "Haoqiu Xiong", "Zhuangzhuang Cui", "Yang Miao", "Sofie Pollin"], "title": "Robust Localization in OFDM-Based Massive MIMO through Phase Offset Calibration", "comment": "Accepted to IEEE International Symposium on Joint Communications & Sensing (JC&S) 2026; recipient of the Best Student Paper Award", "summary": "Accurate localization in Orthogonal Frequency Division Multiplexing (OFDM)-based massive Multiple-Input Multiple-Output (MIMO) systems depends critically on phase coherence across subcarriers and antennas. However, practical systems suffer from frequency-dependent and (spatial) antenna-dependent phase offsets, degrading localization accuracy. This paper analytically studies the impact of phase incoherence on localization performance under a static User Equipment (UE) and Line-of-Sight (LoS) scenario. We use two complementary tools. First, we derive the Cram\u00e9r-Rao Lower Bound (CRLB) to quantify the theoretical limits under phase offsets. Then, we develop a Spatial Ambiguity Function (SAF)-based model to characterize ambiguity patterns. Simulation results reveal that spatial phase offsets severely degrade localization performance, while frequency phase offsets have a minor effect in the considered system configuration. To address this, we propose a robust Channel State Information (CSI) calibration framework and validate it using real-world measurements from a practical massive MIMO testbed. The experimental results confirm that the proposed calibration framework significantly improves the localization Root Mean Squared Error (RMSE) from 5 m to 1.2 cm, aligning well with the theoretical predictions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86OFDM\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u76f8\u4f4d\u975e\u76f8\u5e72\u6027\u5bf9\u5b9a\u4f4d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86CSI\u6821\u51c6\u6846\u67b6\uff0c\u5c06\u5b9a\u4f4d\u8bef\u5dee\u4ece5\u7c73\u964d\u4f4e\u52301.2\u5398\u7c73", "motivation": "OFDM\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u7684\u7cbe\u786e\u5b9a\u4f4d\u4f9d\u8d56\u4e8e\u5b50\u8f7d\u6ce2\u548c\u5929\u7ebf\u95f4\u7684\u76f8\u4f4d\u76f8\u5e72\u6027\uff0c\u4f46\u5b9e\u9645\u7cfb\u7edf\u5b58\u5728\u9891\u7387\u76f8\u5173\u548c\u5929\u7ebf\u76f8\u5173\u7684\u76f8\u4f4d\u504f\u79fb\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u9700\u8981\u5206\u6790\u8fd9\u4e9b\u76f8\u4f4d\u975e\u76f8\u5e72\u6027\u7684\u5f71\u54cd\u5e76\u627e\u5230\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u4e92\u8865\u5de5\u5177\uff1a1) \u63a8\u5bfcCram\u00e9r-Rao\u4e0b\u754c(CRLB)\u91cf\u5316\u76f8\u4f4d\u504f\u79fb\u4e0b\u7684\u7406\u8bba\u6781\u9650\uff1b2) \u5f00\u53d1\u7a7a\u95f4\u6a21\u7cca\u51fd\u6570(SAF)\u6a21\u578b\u8868\u5f81\u6a21\u7cca\u6a21\u5f0f\u3002\u63d0\u51fa\u9c81\u68d2\u7684CSI\u6821\u51c6\u6846\u67b6\uff0c\u5e76\u5728\u5b9e\u9645\u5927\u89c4\u6a21MIMO\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7528\u771f\u5b9e\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff1a\u7a7a\u95f4\u76f8\u4f4d\u504f\u79fb\u4e25\u91cd\u964d\u4f4e\u5b9a\u4f4d\u6027\u80fd\uff0c\u800c\u9891\u7387\u76f8\u4f4d\u504f\u79fb\u5728\u8003\u8651\u7684\u7cfb\u7edf\u914d\u7f6e\u4e2d\u5f71\u54cd\u8f83\u5c0f\u3002\u63d0\u51fa\u7684\u6821\u51c6\u6846\u67b6\u5c06\u5b9a\u4f4d\u5747\u65b9\u6839\u8bef\u5dee(RMSE)\u4ece5\u7c73\u663e\u8457\u6539\u5584\u52301.2\u5398\u7c73\uff0c\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\u3002", "conclusion": "\u76f8\u4f4d\u975e\u76f8\u5e72\u6027\u7279\u522b\u662f\u7a7a\u95f4\u76f8\u4f4d\u504f\u79fb\u5bf9\u5927\u89c4\u6a21MIMO\u5b9a\u4f4d\u7cbe\u5ea6\u6709\u4e25\u91cd\u5f71\u54cd\uff0c\u63d0\u51fa\u7684CSI\u6821\u51c6\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u543b\u5408\u3002"}}
{"id": "2601.12662", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.12662", "abs": "https://arxiv.org/abs/2601.12662", "authors": ["Xingran Chen", "Navid NaderiAlizadeh", "Alejandro Ribeiro", "Shirin Saeedi Bidokhti"], "title": "Decentralized Learning Strategies for Estimation Error Minimization with Graph Neural Networks", "comment": null, "summary": "We address real-time sampling and estimation of autoregressive Markovian sources in dynamic yet structurally similar multi-hop wireless networks. Each node caches samples from others and communicates over wireless collision channels, aiming to minimize time-average estimation error via decentralized policies. Due to the high dimensionality of action spaces and complexity of network topologies, deriving optimal policies analytically is intractable. To address this, we propose a graphical multi-agent reinforcement learning framework for policy optimization. Theoretically, we demonstrate that our proposed policies are transferable, allowing a policy trained on one graph to be effectively applied to structurally similar graphs. Numerical experiments demonstrate that (i) our proposed policy outperforms state-of-the-art baselines; (ii) the trained policies are transferable to larger networks, with performance gains increasing with the number of agents; (iii) the graphical training procedure withstands non-stationarity, even when using independent learning techniques; and (iv) recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u65e0\u7ebf\u7f51\u7edc\u4e2d\u81ea\u56de\u5f52\u9a6c\u5c14\u53ef\u592b\u6e90\u7684\u5b9e\u65f6\u91c7\u6837\u4e0e\u4f30\u8ba1\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u4e14\u80fd\u5e94\u5bf9\u975e\u5e73\u7a33\u6027\u3002", "motivation": "\u5728\u591a\u8df3\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u7ef4\u5ea6\u9ad8\u3001\u7f51\u7edc\u62d3\u6251\u590d\u6742\uff0c\u96be\u4ee5\u901a\u8fc7\u89e3\u6790\u65b9\u6cd5\u63a8\u5bfc\u6700\u4f18\u7684\u5b9e\u65f6\u91c7\u6837\u4e0e\u4f30\u8ba1\u7b56\u7565\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u56fe\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u5229\u7528\u7f51\u7edc\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u4f7f\u5728\u4e00\u4e2a\u56fe\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u591f\u8fc1\u79fb\u5230\u5176\u4ed6\u7ed3\u6784\u76f8\u4f3c\u7684\u56fe\u4e0a\u3002", "result": "\u63d0\u51fa\u7684\u7b56\u7565\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff1b\u8bad\u7ec3\u7684\u7b56\u7565\u53ef\u8fc1\u79fb\u5230\u66f4\u5927\u7f51\u7edc\uff0c\u4e14\u6027\u80fd\u589e\u76ca\u968f\u667a\u80fd\u4f53\u6570\u91cf\u589e\u52a0\uff1b\u56fe\u5f62\u8bad\u7ec3\u8fc7\u7a0b\u80fd\u627f\u53d7\u975e\u5e73\u7a33\u6027\uff1b\u5faa\u73af\u673a\u5236\u5bf9\u72ec\u7acb\u5b66\u4e60\u548c\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u90fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u56fe\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5b9e\u65f6\u91c7\u6837\u4e0e\u4f30\u8ba1\u95ee\u9898\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u548c\u5bf9\u975e\u5e73\u7a33\u6027\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5206\u5e03\u5f0f\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.12215", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12215", "abs": "https://arxiv.org/abs/2601.12215", "authors": ["Megha Thukral", "Cyrus Tanade", "Simon A. Lee", "Juhyeon Lee", "Hao Zhou", "Keum San Chun", "Migyeong Gwak", "Viswam Nathan", "Md Mahbubur Rahman", "Li Zhu", "Mehrab Bin Morshed", "Subramaniam Venkatraman", "Sharanya Arcot Desai"], "title": "Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models", "comment": null, "summary": "Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.", "AI": {"tldr": "\u63d0\u51faMMR\u6846\u67b6\uff0c\u901a\u8fc7\u5c0f\u6ce2\u591a\u5206\u8fa8\u7387\u5206\u89e3\u548c\u63a9\u7801\u91cd\u5efa\u4efb\u52a1\uff0c\u4ece\u5927\u89c4\u6a21PPG\u4fe1\u53f7\u4e2d\u5b66\u4e60\u8de8\u65f6\u95f4-\u9891\u7387\u5c3a\u5ea6\u7684\u8868\u5f81\uff0c\u572817/19\u4e2a\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u53ef\u7a7f\u6234\u57fa\u7840\u6a21\u578b\u5927\u591a\u5ffd\u7565PPG\u4fe1\u53f7\u7684\u9891\u8c31\u7ed3\u6784\uff0c\u800c\u8bb8\u591a\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u9700\u8981\u4ece\u7ec6\u7c92\u5ea6\u6ce2\u5f62\u5f62\u6001\u5230\u5168\u5c40\u8282\u5f8b\u52a8\u6001\u7684\u591a\u5206\u8fa8\u7387\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u591a\u5206\u8fa8\u7387\u91cd\u5efa(MMR)\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff1a\u4f7f\u7528\u5c0f\u6ce2\u591a\u5206\u8fa8\u7387\u5206\u89e3PPG\u4fe1\u53f7\uff0c\u968f\u673a\u63a9\u7801\u7cfb\u6570\u540e\u8ba9Transformer\u7f16\u7801\u5668\u91cd\u5efa\uff0c\u5f3a\u5236\u6a21\u578b\u6574\u5408\u8de8\u65f6\u95f4\u548c\u9891\u8c31\u5c3a\u5ea6\u7684\u4fe1\u606f\u3002", "result": "\u4f7f\u7528\u7ea61700\u4e07\u4e2a\u672a\u6807\u8bb0\u768410\u79d2PPG\u7247\u6bb5\uff08\u6765\u81ea\u7ea63.2\u4e07\u667a\u80fd\u624b\u8868\u7528\u6237\uff09\u9884\u8bad\u7ec3\uff0c\u572819\u4e2a\u591a\u6837\u5316\u5065\u5eb7\u76f8\u5173\u4efb\u52a1\u4e2d\u768417\u4e2a\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u6700\u5148\u8fdb\u7684\u5f00\u6e90PPG\u57fa\u7840\u6a21\u578b\u3001\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u548c\u5176\u4ed6\u81ea\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "MMR\u5c55\u793a\u4e86\u4f5c\u4e3a\u901a\u7528PPG\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5c0f\u6ce2\u8868\u793a\u80fd\u6355\u83b7\u7a33\u5065\u4e14\u5177\u6709\u751f\u7406\u57fa\u7840\u7684\u7279\u5f81\uff0c\u4e3a\u53ef\u7a7f\u6234\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u5f81\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2601.12227", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12227", "abs": "https://arxiv.org/abs/2601.12227", "authors": ["Yuanyun Zhang", "Han Zhou", "Li Feng", "Yilin Hong", "Shi Li"], "title": "Learning Longitudinal Health Representations from EHR and Wearable Data", "comment": null, "summary": "Foundation models trained on electronic health records show strong performance on many clinical prediction tasks but are limited by sparse and irregular documentation. Wearable devices provide dense continuous physiological signals but lack semantic grounding. Existing methods usually model these data sources separately or combine them through late fusion. We propose a multimodal foundation model that jointly represents electronic health records and wearable data as a continuous time latent process. The model uses modality specific encoders and a shared temporal backbone pretrained with self supervised and cross modal objectives. This design produces representations that are temporally coherent and clinically grounded. Across forecasting physiological and risk modeling tasks the model outperforms strong electronic health record only and wearable only baselines especially at long horizons and under missing data. These results show that joint electronic health record and wearable pretraining yields more faithful representations of longitudinal health.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u8054\u5408\u8868\u793a\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u6570\u636e\u4f5c\u4e3a\u8fde\u7eed\u65f6\u95f4\u6f5c\u5728\u8fc7\u7a0b\uff0c\u5728\u751f\u7406\u9884\u6d4b\u548c\u98ce\u9669\u5efa\u6a21\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5355\u6a21\u6001\u57fa\u7ebf", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u89c4\u5219\uff0c\u53ef\u7a7f\u6234\u8bbe\u5907\u63d0\u4f9b\u5bc6\u96c6\u8fde\u7eed\u751f\u7406\u4fe1\u53f7\u4f46\u7f3a\u4e4f\u8bed\u4e49\u57fa\u7840\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5efa\u6a21\u6216\u901a\u8fc7\u540e\u671f\u878d\u5408\u7ec4\u5408\u8fd9\u4e9b\u6570\u636e\u6e90", "method": "\u4f7f\u7528\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u548c\u5171\u4eab\u65f6\u95f4\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u8de8\u6a21\u6001\u76ee\u6807\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5c06\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u53ef\u7a7f\u6234\u6570\u636e\u8054\u5408\u8868\u793a\u4e3a\u8fde\u7eed\u65f6\u95f4\u6f5c\u5728\u8fc7\u7a0b", "result": "\u5728\u9884\u6d4b\u751f\u7406\u548c\u98ce\u9669\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u4f18\u4e8e\u4ec5\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6216\u4ec5\u4f7f\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u957f\u65f6\u7a0b\u9884\u6d4b\u548c\u7f3a\u5931\u6570\u636e\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d", "conclusion": "\u8054\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u9884\u8bad\u7ec3\u80fd\u591f\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u7eb5\u5411\u5065\u5eb7\u8868\u793a\uff0c\u8bc1\u660e\u591a\u6a21\u6001\u6574\u5408\u7684\u4f18\u52bf"}}
{"id": "2601.12231", "categories": ["cs.LG", "cs.CR", "stat.CO"], "pdf": "https://arxiv.org/pdf/2601.12231", "abs": "https://arxiv.org/abs/2601.12231", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Shijie Xu", "Guanggang Geng"], "title": "Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation Modulation and Resolution-Adaptive Attention", "comment": "Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "Insider threat detection is a key challenge in enterprise security, relying on user activity logs that capture rich and complex behavioral patterns. These logs are often multi-channel, non-stationary, and anomalies are rare, making anomaly detection challenging. To address these issues, we propose a novel framework that integrates wavelet-aware modulation, multi-resolution wavelet decomposition, and resolution-adaptive attention for robust anomaly detection. Our approach first applies a deviation-aware modulation scheme to suppress routine behaviors while amplifying anomalous deviations. Next, discrete wavelet transform (DWT) decomposes the log signals into multi-resolution representations, capturing both long-term trends and short-term anomalies. Finally, a learnable attention mechanism dynamically reweights the most discriminative frequency bands for detection. On the CERT r4.2 benchmark, our approach consistently outperforms existing baselines in precision, recall, and F1 score across various time granularities and scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5c0f\u6ce2\u611f\u77e5\u8c03\u5236\u3001\u591a\u5206\u8fa8\u7387\u5c0f\u6ce2\u5206\u89e3\u548c\u5206\u8fa8\u7387\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4f01\u4e1a\u5b89\u5168\u4e2d\u7684\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\uff0c\u5728CERT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u9762\u4e34\u591a\u901a\u9053\u3001\u975e\u5e73\u7a33\u7684\u7528\u6237\u6d3b\u52a8\u65e5\u5fd7\u6570\u636e\uff0c\u4e14\u5f02\u5e38\u4e8b\u4ef6\u7a00\u5c11\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "1) \u504f\u5dee\u611f\u77e5\u8c03\u5236\u65b9\u6848\u6291\u5236\u5e38\u89c4\u884c\u4e3a\u5e76\u653e\u5927\u5f02\u5e38\u504f\u5dee\uff1b2) \u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5c06\u65e5\u5fd7\u4fe1\u53f7\u5206\u89e3\u4e3a\u591a\u5206\u8fa8\u7387\u8868\u793a\uff1b3) \u53ef\u5b66\u4e60\u7684\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u91cd\u52a0\u6743\u6700\u5177\u533a\u5206\u6027\u7684\u9891\u5e26\u3002", "result": "\u5728CERT r4.2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u65f6\u95f4\u7c92\u5ea6\u548c\u573a\u666f\u4e0b\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u5c0f\u6ce2\u611f\u77e5\u8c03\u5236\u3001\u591a\u5206\u8fa8\u7387\u5206\u89e3\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u4f01\u4e1a\u5b89\u5168\u65e5\u5fd7\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2601.12288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12288", "abs": "https://arxiv.org/abs/2601.12288", "authors": ["Lei Liu", "Tengyuan Liu", "Hongwei Zhao", "Jiahui Huang", "Ruibo Guo", "Bin Li"], "title": "TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization", "comment": null, "summary": "Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\\% in CRPS and 21.23\\% in NMAE.", "AI": {"tldr": "TimeGMM\uff1a\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7GRIN\u6a21\u5757\u52a8\u6001\u9002\u5e94\u65f6\u95f4-\u6982\u7387\u5206\u5e03\u6f02\u79fb\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u6355\u6349\u590d\u6742\u672a\u6765\u5206\u5e03", "motivation": "\u73b0\u6709\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u91c7\u6837\u65b9\u6cd5\uff1b2\uff09\u4f7f\u7528\u9650\u5236\u6027\u53c2\u6570\u5047\u8bbe\u6765\u8868\u5f81\u672a\u6765\u5206\u5e03\uff0c\u8fd9\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u5e76\u5bfc\u81f4\u5206\u5e03\u4e0d\u5339\u914d", "method": "\u63d0\u51faTimeGMM\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u5305\u542bGRIN\u6a21\u5757\uff08GMM\u9002\u5e94\u7684\u53ef\u9006\u5b9e\u4f8b\u5f52\u4e00\u5316\uff09\u52a8\u6001\u9002\u5e94\u5206\u5e03\u6f02\u79fb\uff0c\u96c6\u6210\u65f6\u95f4\u7f16\u7801\u5668\uff08TE-Module\uff09\u548c\u6761\u4ef6\u65f6\u95f4-\u6982\u7387\u89e3\u7801\u5668\uff08CTPD-Module\uff09\u8054\u5408\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u6df7\u5408\u5206\u5e03\u53c2\u6570", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cTimeGMM\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728CRPS\u6307\u6807\u4e0a\u6700\u5927\u63d0\u534722.48%\uff0c\u5728NMAE\u6307\u6807\u4e0a\u6700\u5927\u63d0\u534721.23%", "conclusion": "TimeGMM\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u7387\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u6355\u6349\u590d\u6742\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd"}}
{"id": "2601.12296", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12296", "abs": "https://arxiv.org/abs/2601.12296", "authors": ["Hong Zheng", "Fei Teng"], "title": "Distribution Shift Is Key to Learning Invariant Prediction", "comment": null, "summary": "An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes outperforms methods specifically designed for out-of-distribution tasks. This motivates an investigation into the reasons behind such behavior beyond algorithmic design. In this study, we find that one such reason lies in the distribution shift across training domains. A large degree of distribution shift can lead to better performance even under ERM. Specifically, we derive several theoretical and empirical findings demonstrating that distribution shift plays a crucial role in model learning and benefits learning invariant prediction. Firstly, the proposed upper bounds indicate that the degree of distribution shift directly affects the prediction ability of the learned models. If it is large, the models' ability can increase, approximating invariant prediction models that make stable predictions under arbitrary known or unseen domains; and vice versa. We also prove that, under certain data conditions, ERM solutions can achieve performance comparable to that of invariant prediction models. Secondly, the empirical validation results demonstrated that the predictions of learned models approximate those of Oracle or Optimal models, provided that the degree of distribution shift in the training data increases.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u8d8a\u5927\uff0c\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u7684\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u65b9\u6cd5\uff0c\u4e5f\u80fd\u83b7\u5f97\u63a5\u8fd1\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u6709\u65f6\u751a\u81f3\u4f18\u4e8e\u4e13\u95e8\u8bbe\u8ba1\u7684OOD\u65b9\u6cd5\u3002", "motivation": "\u89c2\u5bdf\u5230\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\uff08ERM\uff09\u6709\u65f6\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e13\u95e8\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7a76\u7b97\u6cd5\u8bbe\u8ba1\u4e4b\u5916\u7684\u539f\u56e0\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u57df\u95f4\u7684\u5206\u5e03\u504f\u79fb\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff1a1\uff09\u63d0\u51fa\u7406\u8bba\u4e0a\u754c\u8868\u660e\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u80fd\u529b\uff1b2\uff09\u8bc1\u660e\u5728\u7279\u5b9a\u6570\u636e\u6761\u4ef6\u4e0b\uff0cERM\u89e3\u80fd\u8fbe\u5230\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff1b3\uff09\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5f53\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u504f\u79fb\u589e\u52a0\u65f6\uff0c\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u63a5\u8fd1Oracle\u6216\u6700\u4f18\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5206\u5e03\u504f\u79fb\u7a0b\u5ea6\u8d8a\u5927\uff0c\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u8d8a\u5f3a\uff0c\u8d8a\u80fd\u8fd1\u4f3c\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\uff1b2\uff09\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cERM\u80fd\u8fbe\u5230\u4e0e\u4e0d\u53d8\u9884\u6d4b\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff1b3\uff09\u5b9e\u8bc1\u8868\u660e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u504f\u79fb\u589e\u52a0\u65f6\uff0c\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u786e\u5b9e\u63a5\u8fd1Oracle\u6216\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "\u8bad\u7ec3\u57df\u95f4\u7684\u5206\u5e03\u504f\u79fb\u662f\u5f71\u54cd\u6a21\u578b\u5b66\u4e60\u4e0d\u53d8\u9884\u6d4b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u8f83\u5927\u7684\u5206\u5e03\u504f\u79fb\u80fd\u4f7fERM\u7b49\u7b80\u5355\u65b9\u6cd5\u83b7\u5f97\u63a5\u8fd1\u4e13\u95e8\u8bbe\u8ba1\u7684OOD\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8fd9\u4e3a\u7406\u89e3\u7b97\u6cd5\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.12305", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12305", "abs": "https://arxiv.org/abs/2601.12305", "authors": ["Deepak Kanneganti", "Sajib Mistry", "Sheik Fattah", "Joshua Boland", "Aneesh Krishna"], "title": "Machine Learning as a Service (MLaaS) Dataset Generator Framework for IoT Environments", "comment": null, "summary": "We propose a novel MLaaS Dataset Generator (MDG) framework that creates configurable and reproducible datasets for evaluating Machine Learning as a Service (MLaaS) selection and composition. MDG simulates realistic MLaaS behaviour by training and evaluating diverse model families across multiple real-world datasets and data distribution settings. It records detailed functional attributes, quality of service metrics, and composition-specific indicators, enabling systematic analysis of service performance and cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS service instances and construct a large-scale benchmark dataset suitable for downstream evaluation. We also implement a built-in composition mechanism that models how services interact under varied Internet of Things conditions. Experiments demonstrate that datasets generated by MDG enhance selection accuracy and composition quality compared to existing baselines. MDG provides a practical and extensible foundation for advancing data-driven research on MLaaS selection and composition", "AI": {"tldr": "\u63d0\u51faMLaaS\u6570\u636e\u96c6\u751f\u6210\u5668(MDG)\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u53ef\u914d\u7f6e\u3001\u53ef\u590d\u73b0\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30MLaaS\u670d\u52a1\u9009\u62e9\u548c\u7ec4\u5408\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9eMLaaS\u884c\u4e3a\u751f\u6210\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u3002", "motivation": "\u9700\u8981\u7cfb\u7edf\u5316\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5373\u670d\u52a1(MLaaS)\u9009\u62e9\u548c\u7ec4\u5408\u7684\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u914d\u7f6e\u3001\u53ef\u590d\u73b0\u7684\u6570\u636e\u96c6\u6765\u6a21\u62df\u771f\u5b9eMLaaS\u884c\u4e3a\u548c\u670d\u52a1\u4ea4\u4e92\u3002", "method": "MDG\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\uff0c\u8de8\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u6570\u636e\u5206\u5e03\u8bbe\u7f6e\u6765\u6a21\u62dfMLaaS\u884c\u4e3a\uff0c\u8bb0\u5f55\u529f\u80fd\u5c5e\u6027\u3001\u670d\u52a1\u8d28\u91cf\u6307\u6807\u548c\u7ec4\u5408\u7279\u5b9a\u6307\u6807\uff0c\u5e76\u5185\u7f6e\u7ec4\u5408\u673a\u5236\u6a21\u62df\u7269\u8054\u7f51\u6761\u4ef6\u4e0b\u7684\u670d\u52a1\u4ea4\u4e92\u3002", "result": "\u751f\u6210\u4e86\u8d85\u8fc7\u4e00\u4e07\u4e2aMLaaS\u670d\u52a1\u5b9e\u4f8b\uff0c\u6784\u5efa\u4e86\u9002\u7528\u4e8e\u4e0b\u6e38\u8bc4\u4f30\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8868\u660eMDG\u751f\u6210\u7684\u6570\u636e\u96c6\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u9ad8\u4e86\u9009\u62e9\u51c6\u786e\u6027\u548c\u7ec4\u5408\u8d28\u91cf\u3002", "conclusion": "MDG\u4e3a\u63a8\u8fdbMLaaS\u9009\u62e9\u548c\u7ec4\u5408\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u914d\u7f6e\u7684\u6570\u636e\u96c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u670d\u52a1\u6027\u80fd\u3002"}}
{"id": "2601.12317", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12317", "abs": "https://arxiv.org/abs/2601.12317", "authors": ["Yiming Huang"], "title": "Explanova: Automatically Discover Data Insights in N \\times M Table via XAI Combined LLM Workflow", "comment": null, "summary": "Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.", "AI": {"tldr": "Explanova\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bbeAutoML\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u4f7f\u7528\u672c\u5730\u5c0f\u578bLLM\u964d\u4f4e\u6210\u672c", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u4ee3\u7406\u5de5\u5177\u8c03\u7528\u7684\u6570\u636e\u5206\u6790\u6846\u67b6\uff08\u5982DeepAnalyze\u3001DataSage\u3001Datawise\uff09\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u6210\u672c\u8f83\u9ad8\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u57fa\u4e8e\u9884\u8bbeAutoML\u5de5\u4f5c\u6d41\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684\u5206\u6790\u8def\u5f84\uff08\u5982\u53d8\u91cf\u81ea\u8eab\u7edf\u8ba1\u3001\u53d8\u91cf\u95f4\u5173\u7cfb\u3001\u53d8\u91cf\u4e0e\u6240\u6709\u5176\u4ed6\u53d8\u91cf\u7684\u5173\u7cfb\u3001\u6700\u7ec8\u89e3\u91ca\uff09\u6765\u5b9e\u73b0\u81ea\u52a8\u5316\u5206\u6790\u3002", "method": "Explanova\u91c7\u7528\u9884\u8bbe\u7684AutoML\u5f0f\u5de5\u4f5c\u6d41\uff0c\u7cfb\u7edf\u6027\u5730\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684\u5206\u6790\u63a2\u7d22\u8def\u5f84\uff0c\u5305\u62ec\uff1a\u53d8\u91cf\u81ea\u8eab\u7edf\u8ba1\u7279\u5f81\u5206\u6790\u3001\u53d8\u91cf\u95f4\u5173\u7cfb\u5206\u6790\u3001\u53d8\u91cf\u4e0e\u6240\u6709\u5176\u4ed6\u53d8\u91cf\u7684\u5173\u7cfb\u5206\u6790\uff0c\u6700\u7ec8\u751f\u6210\u89e3\u91ca\u3002\u5173\u952e\u521b\u65b0\u662f\u4f7f\u7528\u672c\u5730\u5c0f\u578bLLM\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "Explanova\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\uff0c\u76f8\u6bd4\u73b0\u6709\u7684LLM\u4ee3\u7406\u6846\u67b6\uff0c\u6210\u672c\u663e\u8457\u964d\u4f4e\uff08\u7531\u4e8e\u4f7f\u7528\u672c\u5730\u5c0f\u578bLLM\uff09\uff0c\u540c\u65f6\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5de5\u4f5c\u6d41\u786e\u4fdd\u4e86\u5206\u6790\u7684\u5168\u9762\u6027\u3002", "conclusion": "\u57fa\u4e8e\u9884\u8bbeAutoML\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u5206\u6790\u6846\u67b6\u662f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u81ea\u52a8\u5316\u6570\u636e\u5206\u6790\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.12322", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12322", "abs": "https://arxiv.org/abs/2601.12322", "authors": ["Chang-Wei Shi", "Shi-Shang Wang", "Wu-Jun Li"], "title": "Ordered Local Momentum for Asynchronous Distributed Learning under Arbitrary Delays", "comment": null, "summary": "Momentum SGD (MSGD) serves as a foundational optimizer in training deep models due to momentum's key role in accelerating convergence and enhancing generalization. Meanwhile, asynchronous distributed learning is crucial for training large-scale deep models, especially when the computing capabilities of the workers in the cluster are heterogeneous. To reduce communication frequency, local updates are widely adopted in distributed learning. However, how to implement asynchronous distributed MSGD with local updates remains unexplored. To solve this problem, we propose a novel method, called \\underline{or}dered \\underline{lo}cal \\underline{mo}mentum (OrLoMo), for asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally. Then the local momentum from each worker will be aggregated by the server in order based on its global iteration index. To the best of our knowledge, OrLoMo is the first method to implement asynchronous distributed MSGD with local updates. We prove the convergence of OrLoMo for non-convex problems under arbitrary delays. Experiments validate that OrLoMo can outperform its synchronous counterpart and other asynchronous methods.", "AI": {"tldr": "\u63d0\u51faOrLoMo\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u5e26\u672c\u5730\u66f4\u65b0\u7684\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\uff0c\u901a\u8fc7\u6709\u5e8f\u805a\u5408\u672c\u5730\u52a8\u91cf\u6765\u52a0\u901f\u5f02\u6784\u96c6\u7fa4\u8bad\u7ec3", "motivation": "\u52a8\u91cfSGD\u662f\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7684\u57fa\u7840\u4f18\u5316\u5668\uff0c\u5f02\u6b65\u5206\u5e03\u5f0f\u5b66\u4e60\u5bf9\u8bad\u7ec3\u5927\u89c4\u6a21\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5f02\u6784\u8ba1\u7b97\u96c6\u7fa4\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\u4e0e\u672c\u5730\u66f4\u65b0\u7684\u7ed3\u5408\u65b9\u6848\u3002", "method": "\u63d0\u51faOrLoMo\uff08\u6709\u5e8f\u672c\u5730\u52a8\u91cf\uff09\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u5de5\u4f5c\u8282\u70b9\u672c\u5730\u8fd0\u884c\u52a8\u91cfSGD\uff0c\u670d\u52a1\u5668\u6839\u636e\u5168\u5c40\u8fed\u4ee3\u7d22\u5f15\u6709\u5e8f\u805a\u5408\u5404\u5de5\u4f5c\u8282\u70b9\u7684\u672c\u5730\u52a8\u91cf\u3002", "result": "\u8bc1\u660e\u4e86OrLoMo\u5728\u4efb\u610f\u5ef6\u8fdf\u4e0b\u7684\u975e\u51f8\u95ee\u9898\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u540c\u6b65\u65b9\u6cd5\u548c\u5176\u4ed6\u5f02\u6b65\u65b9\u6cd5\u3002", "conclusion": "OrLoMo\u662f\u9996\u4e2a\u5b9e\u73b0\u5f02\u6b65\u5206\u5e03\u5f0f\u52a8\u91cfSGD\u4e0e\u672c\u5730\u66f4\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5904\u7406\u5f02\u6784\u96c6\u7fa4\u8bad\u7ec3\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2601.12330", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12330", "abs": "https://arxiv.org/abs/2601.12330", "authors": ["Zuha Fatima", "Muhammad Anser Sohaib", "Muhammad Talha", "Ayesha Kanwal", "Sidra Sultana", "Nazia Perwaiz"], "title": "IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning", "comment": null, "summary": "Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.", "AI": {"tldr": "\u63d0\u51faIceWatch\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u89c6\u89c9\u548c\u65f6\u5e8f\u7269\u7406\u52a8\u6001\uff0c\u5b9e\u73b0\u51b0\u5ddd\u6e56\u6e83\u51b3\u6d2a\u6c34(GLOF)\u7684\u81ea\u52a8\u9884\u6d4b\u4e0e\u9884\u8b66", "motivation": "\u4f20\u7edfGLOF\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6c34\u6587\u6a21\u578b\u3001\u9608\u503c\u76d1\u6d4b\u548c\u4eba\u5de5\u536b\u661f\u56fe\u50cf\u5206\u6790\uff0c\u5b58\u5728\u66f4\u65b0\u6162\u3001\u4f9d\u8d56\u4eba\u5de5\u3001\u4e91\u5e72\u6270\u548c\u73b0\u573a\u6570\u636e\u7f3a\u4e4f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u81ea\u52a8\u5316\u548c\u53ef\u9760\u7684\u9884\u6d4b\u7cfb\u7edf", "method": "\u5f00\u53d1IceWatch\u6846\u67b6\uff0c\u5305\u542b\uff1a1) RiskFlow\u89c6\u89c9\u7ec4\u4ef6\uff1a\u57fa\u4e8eCNN\u5206\u6790Sentinel-2\u591a\u5149\u8c31\u536b\u661f\u56fe\u50cf\uff0c\u8bc6\u522b\u51b0\u96ea\u878d\u6c34\u7a7a\u95f4\u6a21\u5f0f\uff1b2) TerraFlow\uff1a\u4eceNASA ITS_LIVE\u65f6\u5e8f\u6570\u636e\u5efa\u6a21\u51b0\u5ddd\u6d41\u901f\uff1b3) TempFlow\uff1a\u4eceMODIS LST\u8bb0\u5f55\u9884\u6d4b\u8fd1\u5730\u8868\u6e29\u5ea6\uff1b\u901a\u8fc7\u534f\u8c03\u9884\u5904\u7406\u548c\u540c\u6b65\u5b9e\u73b0\u591a\u6a21\u6001\u7269\u7406\u4fe1\u606f\u878d\u5408", "result": "\u7cfb\u7edf\u63d0\u4f9b\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u63d0\u9ad8GLOF\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u786e\u4fdd\u5f3a\u9884\u6d4b\u6027\u80fd\u3001\u5b9e\u65f6\u6570\u636e\u5904\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u5bf9\u566a\u58f0\u548c\u7f3a\u5931\u4fe1\u606f\u7684\u9c81\u68d2\u6027", "conclusion": "IceWatch\u4e3a\u81ea\u52a8\u3001\u53ef\u6269\u5c55\u7684GLOF\u9884\u8b66\u7cfb\u7edf\u94fa\u5e73\u9053\u8def\uff0c\u5177\u6709\u4e0e\u591a\u79cd\u4f20\u611f\u5668\u8f93\u5165\u548c\u5168\u7403\u51b0\u5ddd\u76d1\u6d4b\u6d3b\u52a8\u96c6\u6210\u7684\u6f5c\u529b"}}
{"id": "2601.12355", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12355", "abs": "https://arxiv.org/abs/2601.12355", "authors": ["Beicheng Xu", "Weitong Qian", "Lingching Tung", "Yupeng Lu", "Bin Cui"], "title": "LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for Efficient CASH", "comment": null, "summary": "To lower the expertise barrier in machine learning, the AutoML community has focused on the CASH problem, a fundamental challenge that automates the process of algorithm selection and hyperparameter tuning. While traditional methods like Bayesian Optimization (BO) struggle with cold-start issues, Large Language Models (LLMs) can mitigate these via semantic priors. However, existing LLM-based optimizers generalize poorly to the high-dimensional, structured CASH space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning Memory (STM) and explicit exploration-exploitation trade-off. It combines the strengths of both paradigms by dynamically shifting from LLM-driven to BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets demonstrate the superiority of LB-MCTS over the competitive baselines.", "AI": {"tldr": "LB-MCTS\uff1a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7684AutoML\u6846\u67b6\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u89e3\u51b3CASH\u95ee\u9898\uff0c\u5728104\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u964d\u4f4e\u673a\u5668\u5b66\u4e60\u95e8\u69db\uff0c\u89e3\u51b3\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u5728CASH\u95ee\u9898\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4f18\u5316\u5668\u5728\u9ad8\u7ef4\u7ed3\u6784\u5316CASH\u7a7a\u95f4\u4e2d\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898", "method": "\u63d0\u51faLB-MCTS\u6846\u67b6\uff0c\u5c06LLM\u548cBO\u7ed3\u5408\u5728\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7ed3\u6784\u4e2d\uff0c\u4f7f\u7528\u9009\u62e9\u6027\u8c03\u4f18\u8bb0\u5fc6\uff08STM\uff09\u6700\u5927\u5316LLM\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u663e\u5f0f\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u968f\u7740\u6570\u636e\u79ef\u7d2f\u52a8\u6001\u4eceLLM\u9a71\u52a8\u8f6c\u5411BO\u9a71\u52a8\u63d0\u6848", "result": "\u5728104\u4e2aAMLB\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLB-MCTS\u4f18\u4e8e\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "LB-MCTS\u6210\u529f\u7ed3\u5408\u4e86LLM\u548cBO\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u52a8\u6001\u5207\u6362\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86CASH\u95ee\u9898\uff0c\u4e3aAutoML\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u4f18\u5316\u6846\u67b6"}}
{"id": "2601.12362", "categories": ["cs.LG", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2601.12362", "abs": "https://arxiv.org/abs/2601.12362", "authors": ["Natthapong Promsricha", "Chotirawee Chatpattanasiri", "Nuttavut Kerdgongsup", "Stavroula Balabani"], "title": "Machine Learning-Based Framework for Real Time Detection and Early Prediction of Control Valve Stiction in Industrial Control Systems", "comment": null, "summary": "Control valve stiction, a friction that prevents smooth valve movement, is a common fault in industrial process systems that causes instability, equipment wear, and higher maintenance costs. Many plants still operate with conventional valves that lack real time monitoring, making early predictions challenging. This study presents a machine learning (ML) framework for detecting and predicting stiction using only routinely collected process signals: the controller output (OP) from control systems and the process variable (PV), such as flow rate. Three deep learning models were developed and compared: a Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine (CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models, a data-driven labeling method based on slope ratio analysis was applied to a real oil and gas refinery dataset. The LSTM model achieved the highest accuracy and was able to predict stiction up to four hours in advance. To the best of the authors' knowledge, this is the first study to demonstrate ML based early prediction of control valve stiction from real industry data. The proposed framework can be integrated into existing control systems to support predictive maintenance, reduce downtime, and avoid unnecessary hardware replacement.", "AI": {"tldr": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u63a7\u5236\u9600\u7c98\u6ede\u6545\u969c\u65e9\u671f\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u5e38\u89c4\u8fc7\u7a0b\u4fe1\u53f7\uff08\u63a7\u5236\u5668\u8f93\u51fa\u548c\u8fc7\u7a0b\u53d8\u91cf\uff09\uff0cLSTM\u6a21\u578b\u5728\u771f\u5b9e\u70bc\u6cb9\u5382\u6570\u636e\u4e0a\u5b9e\u73b0\u6700\u9ad8\u7cbe\u5ea6\uff0c\u53ef\u63d0\u524d4\u5c0f\u65f6\u9884\u6d4b\u7c98\u6ede\u6545\u969c\u3002", "motivation": "\u63a7\u5236\u9600\u7c98\u6ede\u662f\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u5e38\u89c1\u6545\u969c\uff0c\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u3001\u8bbe\u5907\u78e8\u635f\u548c\u7ef4\u62a4\u6210\u672c\u589e\u52a0\u3002\u8bb8\u591a\u5de5\u5382\u4ecd\u4f7f\u7528\u7f3a\u4e4f\u5b9e\u65f6\u76d1\u63a7\u7684\u4f20\u7edf\u9600\u95e8\uff0c\u4f7f\u5f97\u65e9\u671f\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1aCNN\u3001CNN-SVM\u6df7\u5408\u6a21\u578b\u548cLSTM\u7f51\u7edc\u3002\u4f7f\u7528\u57fa\u4e8e\u659c\u7387\u6bd4\u5206\u6790\u7684\u6570\u636e\u9a71\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u77f3\u6cb9\u548c\u5929\u7136\u6c14\u70bc\u6cb9\u5382\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "LSTM\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u80fd\u591f\u63d0\u524d4\u5c0f\u65f6\u9884\u6d4b\u63a7\u5236\u9600\u7c98\u6ede\u6545\u969c\u3002\u8fd9\u662f\u9996\u6b21\u57fa\u4e8e\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u5c55\u793a\u673a\u5668\u5b66\u4e60\u65e9\u671f\u9884\u6d4b\u63a7\u5236\u9600\u7c98\u6ede\u7684\u7814\u7a76\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u96c6\u6210\u5230\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\u4e2d\uff0c\u652f\u6301\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u51cf\u5c11\u505c\u673a\u65f6\u95f4\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u786c\u4ef6\u66f4\u6362\u3002"}}
{"id": "2601.12401", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12401", "abs": "https://arxiv.org/abs/2601.12401", "authors": ["Jinmei Liu", "Haoru Li", "Zhenhong Sun", "Chaofeng Chen", "Yatao Bian", "Bo Wang", "Daoyi Dong", "Chunlin Chen", "Zhi Wang"], "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation", "comment": null, "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.", "AI": {"tldr": "DRIFT\u6846\u67b6\u901a\u8fc7\u91c7\u6837\u3001\u63d0\u793a\u548c\u4f18\u5316\u4e09\u4e2a\u89d2\u5ea6\u89e3\u51b3RL\u5fae\u8c03\u751f\u6210\u6a21\u578b\u65f6\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u5bf9\u9f50\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u5fae\u8c03\u5927\u89c4\u6a21\u751f\u6210\u6a21\u578b\u65f6\u5b58\u5728\"\u591a\u6837\u6027\u5d29\u6e83\u8bc5\u5492\"\uff0c\u5373\u4f18\u5316\u8fc7\u7a0b\u4f1a\u4f7f\u7b56\u7565\u6536\u655b\u5230\u72c4\u62c9\u514b\u03b4\u5206\u5e03\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u9700\u8981\u591a\u6837\u5316\u5019\u9009\u751f\u6210\u7684\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faDRIFT\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u89d2\u5ea6\u7cfb\u7edf\u6027\u5730\u6fc0\u52b1\u8f93\u51fa\u591a\u6837\u6027\uff1a1) \u91c7\u6837\u5956\u52b1\u96c6\u4e2d\u5b50\u96c6\uff0c\u8fc7\u6ee4\u5956\u52b1\u5f02\u5e38\u503c\u9632\u6b62\u8fc7\u65e9\u5d29\u6e83\uff1b2) \u4f7f\u7528\u968f\u673a\u53d8\u4f53\u63d0\u793a\u6269\u5c55\u6761\u4ef6\u7a7a\u95f4\uff1b3) \u901a\u8fc7\u57fa\u4e8e\u52bf\u80fd\u7684\u5956\u52b1\u5851\u9020\u673a\u5236\u4f18\u5316\u7ec4\u5185\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDRIFT\u5728\u4efb\u52a1\u5bf9\u9f50\u548c\u751f\u6210\u591a\u6837\u6027\u65b9\u9762\u5b9e\u73b0\u5e15\u7d2f\u6258\u4f18\u52bf\uff1a\u5728\u76f8\u540c\u5bf9\u9f50\u6c34\u5e73\u4e0b\u591a\u6837\u6027\u63d0\u53479.08%~43.46%\uff0c\u5728\u76f8\u540c\u591a\u6837\u6027\u6c34\u5e73\u4e0b\u5bf9\u9f50\u5ea6\u63d0\u534759.65%~65.86%\u3002", "conclusion": "DRIFT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RL\u5fae\u8c03\u751f\u6210\u6a21\u578b\u65f6\u7684\u591a\u6837\u6027\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5f3a\u4efb\u52a1\u5bf9\u9f50\u4e0e\u9ad8\u751f\u6210\u591a\u6837\u6027\u7684\u5e73\u8861\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u9700\u8981\u591a\u6837\u5316\u751f\u6210\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.12405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12405", "abs": "https://arxiv.org/abs/2601.12405", "authors": ["Manasi Kanade", "Abhi Thakkar", "Gabriela Fernandes"], "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants", "comment": null, "summary": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.\n  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.\n  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.\n  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.\n  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u513f\u79d1\u7259\u79d1\u98ce\u9669\u5206\u5c42\uff0c\u5f3a\u8c03\u53ef\u89e3\u91ca\u6027\u548c\u4f26\u7406\u90e8\u7f72\u800c\u975e\u6700\u5927\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u513f\u79d1\u7259\u79d1\u75be\u75c5\u662f\u5168\u7403\u6700\u666e\u904d\u4e14\u4e0d\u516c\u5e73\u7684\u6162\u6027\u5065\u5eb7\u95ee\u9898\u4e4b\u4e00\u3002\u867d\u7136\u6d41\u884c\u75c5\u5b66\u8bc1\u636e\u663e\u793a\u53e3\u8154\u5065\u5eb7\u7ed3\u679c\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u56e0\u7d20\u76f8\u5173\uff0c\u4f46\u5f53\u524d\u7259\u79d1AI\u5e94\u7528\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u56fe\u50cf\u7684\u8bca\u65ad\u548c\u9ed1\u76d2\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u513f\u79d1\u4eba\u7fa4\u4e2d\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u4f26\u7406\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u4eba\u53e3\u6c34\u5e73\u7684\u513f\u79d1\u6570\u636e\uff08\u5305\u62ec\u5e74\u9f84\u3001\u6536\u5165\u8d2b\u56f0\u6bd4\u3001\u79cd\u65cf/\u6c11\u65cf\u3001\u6027\u522b\u548c\u75c5\u53f2\uff09\u8bad\u7ec3\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u901a\u8fc7ROC\u5206\u6790\u548c\u6821\u51c6\u66f2\u7ebf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u7528SHAP\u65b9\u6cd5\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u4f9b\u5168\u5c40\u548c\u4e2a\u4f53\u5c42\u9762\u7684\u9884\u6d4b\u89e3\u91ca\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u9002\u5ea6\u7684\u533a\u5206\u80fd\u529b\uff08AUC=0.61\uff09\uff0c\u5177\u6709\u4fdd\u5b88\u7684\u6821\u51c6\u7279\u6027\uff0c\u5728\u9ad8\u6982\u7387\u6c34\u5e73\u4e0b\u4f4e\u4f30\u98ce\u9669\u3002SHAP\u5206\u6790\u663e\u793a\u5e74\u9f84\u548c\u6536\u5165\u8d2b\u56f0\u6bd4\u5bf9\u9884\u6d4b\u98ce\u9669\u7684\u8d21\u732e\u6700\u5927\uff0c\u5176\u6b21\u662f\u79cd\u65cf/\u6c11\u65cf\u548c\u6027\u522b\u3002", "conclusion": "\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u900f\u660e\u7684\u3001\u9884\u9632\u5bfc\u5411\u7684\u513f\u79d1\u7259\u79d1\u98ce\u9669\u5206\u5c42\uff0c\u652f\u6301\u4eba\u7fa4\u7b5b\u67e5\u548c\u516c\u5e73\u7684\u8d44\u6e90\u5206\u914d\uff0c\u800c\u4e0d\u662f\u7528\u4e8e\u8bca\u65ad\u51b3\u7b56\u3002"}}
{"id": "2601.12415", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12415", "abs": "https://arxiv.org/abs/2601.12415", "authors": ["Wang Zixian"], "title": "Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF", "comment": null, "summary": "Recent alignment methods for large language models, including PPO, DPO, and IPO, are often presented as distinct algorithms. In this work, we show that many of these approaches implicitly conflate two fundamental and independent design choices: (i) the sampling geometry, which determines which samples dominate the gradient signal, and (ii) the optimization geometry, which determines how deviations in value are penalized. We formalize this observation by expressing alignment as the minimization of a generalized distance between policy energy and target energy, parameterized by an alpha-divergence-based sampling weight and a Bregman-divergence-based value metric. We demonstrate that the commonly used KL divergence induces an exponential penalty on unbounded value signals, leading to numerical instability and vanishing gradients in high-confidence regimes. To address this issue, we propose Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples sampling geometry from optimization geometry. By combining alpha-weighted importance sampling with a chi-square-induced quadratic regularization in ratio coordinates, OPO yields a simple and well-conditioned objective with linear gradient dynamics. This formulation maintains stable optimization while preserving peak-seeking behavior and avoids gradient saturation even when model confidence is high. Our analysis positions OPO as a unifying perspective on existing alignment methods and provides a principled foundation for robust reasoning-oriented training.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6b63\u4ea4\u5316\u7b56\u7565\u4f18\u5316\uff08OPO\uff09\u6846\u67b6\uff0c\u5c06\u5bf9\u9f50\u65b9\u6cd5\u89e3\u8026\u4e3a\u91c7\u6837\u51e0\u4f55\u548c\u4f18\u5316\u51e0\u4f55\u4e24\u4e2a\u72ec\u7acb\u8bbe\u8ba1\u9009\u62e9\uff0c\u89e3\u51b3KL\u6563\u5ea6\u5bfc\u81f4\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982PPO\u3001DPO\u3001IPO\uff09\u9690\u542b\u5730\u5c06\u91c7\u6837\u51e0\u4f55\u548c\u4f18\u5316\u51e0\u4f55\u6df7\u4e3a\u4e00\u8c08\uff0c\u5bfc\u81f4KL\u6563\u5ea6\u5bf9\u65e0\u754c\u503c\u4fe1\u53f7\u65bd\u52a0\u6307\u6570\u60e9\u7f5a\uff0c\u9020\u6210\u6570\u503c\u4e0d\u7a33\u5b9a\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4e0b\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5316\u7b56\u7565\u4f18\u5316\uff08OPO\uff09\u6846\u67b6\uff0c\u5c06\u91c7\u6837\u51e0\u4f55\uff08\u03b1\u6563\u5ea6\u52a0\u6743\u7684\u91cd\u91c7\u6837\uff09\u4e0e\u4f18\u5316\u51e0\u4f55\uff08\u03c7\u00b2\u8bf1\u5bfc\u7684\u4e8c\u6b21\u6b63\u5219\u5316\uff09\u663e\u5f0f\u89e3\u8026\uff0c\u5728\u6bd4\u7387\u5750\u6807\u4e2d\u4f7f\u7528\u7ebf\u6027\u68af\u5ea6\u52a8\u6001\u7684\u7b80\u5355\u76ee\u6807\u51fd\u6570\u3002", "result": "OPO\u5728\u4fdd\u6301\u5cf0\u503c\u5bfb\u6c42\u884c\u4e3a\u7684\u540c\u65f6\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\uff0c\u907f\u514d\u68af\u5ea6\u9971\u548c\uff0c\u5373\u4f7f\u6a21\u578b\u7f6e\u4fe1\u5ea6\u9ad8\u65f6\u4e5f\u80fd\u7ef4\u6301\u826f\u597d\u6761\u4ef6\u7684\u76ee\u6807\u51fd\u6570\u3002", "conclusion": "OPO\u4e3a\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u89c6\u89d2\uff0c\u4e3a\u7a33\u5065\u7684\u63a8\u7406\u5bfc\u5411\u8bad\u7ec3\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u89e3\u8026\u91c7\u6837\u548c\u4f18\u5316\u51e0\u4f55\u89e3\u51b3\u4e86\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002"}}
{"id": "2601.12426", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.12426", "abs": "https://arxiv.org/abs/2601.12426", "authors": ["Mohammadhossein Homaei", "Iman Khazrak", "Ruben Molano", "Andres Caro", "Mar Avila"], "title": "Graph Attention Networks with Physical Constraints for Anomaly Detection", "comment": "7 Pages, 4 Figures, 5 Tables", "summary": "Water distribution systems (WDSs) face increasing cyber-physical risks, which make reliable anomaly detection essential. Many data-driven models ignore network topology and are hard to interpret, while model-based ones depend strongly on parameter accuracy. This work proposes a hydraulic-aware graph attention network using normalized conservation law violations as features. It combines mass and energy balance residuals with graph attention and bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module aggregates detection scores from node to network level. On the BATADAL dataset, it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\\%$ parameter noise.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6c34\u529b\u611f\u77e5\u7684\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u5229\u7528\u5f52\u4e00\u5316\u5b88\u6052\u5b9a\u5f8b\u8fdd\u89c4\u4f5c\u4e3a\u7279\u5f81\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u548c\u53cc\u5411LSTM\u5b66\u4e60\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5b9e\u73b0\u6c34\u5206\u914d\u7cfb\u7edf\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u6c34\u5206\u914d\u7cfb\u7edf\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u7f51\u7edc\u7269\u7406\u98ce\u9669\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5ffd\u7565\u7f51\u7edc\u62d3\u6251\u4e14\u96be\u4ee5\u89e3\u91ca\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u53c2\u6570\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u5f52\u4e00\u5316\u5b88\u6052\u5b9a\u5f8b\u8fdd\u89c4\u4f5c\u4e3a\u7279\u5f81\uff0c\u7ed3\u5408\u8d28\u91cf\u4e0e\u80fd\u91cf\u5e73\u8861\u6b8b\u5dee\u3001\u56fe\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cc\u5411LSTM\u5b66\u4e60\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u591a\u5c3a\u5ea6\u6a21\u5757\u4ece\u8282\u70b9\u5230\u7f51\u7edc\u5c42\u9762\u805a\u5408\u68c0\u6d4b\u5206\u6570\u3002", "result": "\u5728BATADAL\u6570\u636e\u96c6\u4e0a\u8fbe\u5230F1=0.979\uff0c\u76f8\u6bd4\u57fa\u7ebf\u63d0\u53473.3\u4e2a\u767e\u5206\u70b9\uff0c\u572815%\u53c2\u6570\u566a\u58f0\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u7269\u7406\u7ea6\u675f\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u9c81\u68d2\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u6c34\u5206\u914d\u7cfb\u7edf\u7684\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12442", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12442", "abs": "https://arxiv.org/abs/2601.12442", "authors": ["Shahnawaz Alam", "Mohammed Mudassir Uddin", "Mohammed Kaif Pasha"], "title": "Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery", "comment": null, "summary": "Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.", "AI": {"tldr": "CANUF\u6846\u67b6\u5c06\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u4e0e\u53ef\u5fae\u5206\u7b26\u53f7\u63a8\u7406\u7ed3\u5408\uff0c\u5728\u79d1\u5b66AI\u4e2d\u540c\u65f6\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u9886\u57df\u7ea6\u675f\u6ee1\u8db3\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7f3a\u4e4f\u6574\u5408\u7b26\u53f7\u79d1\u5b66\u77e5\u8bc6\u7684\u673a\u5236\uff0c\u800c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u3002\u79d1\u5b66AI\u9700\u8981\u65e2\u63d0\u4f9b\u53ef\u4fe1\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53c8\u5c0a\u91cd\u9886\u57df\u7ea6\u675f\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u7ea6\u675f\u611f\u77e5\u795e\u7ecf\u7b26\u53f7\u4e0d\u786e\u5b9a\u6027\u6846\u67b6(CANUF)\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u4ece\u79d1\u5b66\u6587\u732e\u81ea\u52a8\u63d0\u53d6\u7ea6\u675f\u3001\u5177\u6709\u53d8\u5206\u63a8\u7406\u7684\u6982\u7387\u795e\u7ecf\u9aa8\u5e72\u3001\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u7684\u53ef\u5fae\u5206\u7ea6\u675f\u6ee1\u8db3\u5c42\u3002", "result": "\u5728Materials Project\u3001QM9\u5206\u5b50\u5c5e\u6027\u548c\u6c14\u5019\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCANUF\u76f8\u6bd4\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u5c06\u671f\u671b\u6821\u51c6\u8bef\u5dee\u964d\u4f4e34.7%\uff0c\u540c\u65f6\u4fdd\u630199.2%\u7684\u7ea6\u675f\u6ee1\u8db3\u7387\u3002\u7ea6\u675f\u5f15\u5bfc\u7684\u91cd\u65b0\u6821\u51c6\u8d21\u732e18.3%\u6027\u80fd\u589e\u76ca\uff0c\u7ea6\u675f\u63d0\u53d6\u7cbe\u5ea6\u8fbe91.4%\u3002", "conclusion": "CANUF\u63d0\u4f9b\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u7ba1\u9053\uff0c\u540c\u65f6\u89e3\u51b3\u79d1\u5b66\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u7ea6\u675f\u6ee1\u8db3\u548c\u53ef\u89e3\u91ca\u6027\u89e3\u91ca\u95ee\u9898\u3002"}}
{"id": "2601.12467", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12467", "abs": "https://arxiv.org/abs/2601.12467", "authors": ["Saurish Nagrath"], "title": "Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting", "comment": "6 pages, 2 figures, 3 tables", "summary": "Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data. This work proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the first stage, a convolutional neural network (CNN) operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is subsequently applied during representation learning to refine these embeddings by enabling interactions across temporal patches. In the second stage, a Transformer encoder processes the resulting token sequence to model inter-patch temporal dependencies and generate per-patch forecasts. Experiments conducted on synthetic multivariate time-series data with controlled static and dynamic factors demonstrate that the proposed patch-based tokenization strategy achieves competitive forecasting performance compared to convolutional and patch-based Transformer baselines. The results highlight the importance of structured temporal representations and show that decoupling local temporal encoding from global attention-based modelling yields more effective and stable time-series forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528CNN\u63d0\u53d6\u5c40\u90e8\u65f6\u95f4\u52a8\u6001\u7279\u5f81\u5e76\u751f\u6210\u8865\u4e01\u7ea7token\u5d4c\u5165\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528Transformer\u7f16\u7801\u5668\u5efa\u6a21\u8865\u4e01\u95f4\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u9884\u6d4b\u3002", "motivation": "Transformer\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u6548\u679c\u4e25\u91cd\u4f9d\u8d56\u4e8e\u4ece\u539f\u59cb\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u8f93\u5165\u8868\u793a\u7684\u8d28\u91cf\u548c\u7ed3\u6784\u3002\u9700\u8981\u66f4\u597d\u7684\u65f6\u95f4\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5c40\u90e8\u65f6\u95f4\u8868\u793a\u5b66\u4e60\uff1aCNN\u5728\u56fa\u5b9a\u957f\u5ea6\u65f6\u95f4\u8865\u4e01\u4e0a\u63d0\u53d6\u77ed\u7a0b\u65f6\u95f4\u52a8\u6001\u548c\u975e\u7ebf\u6027\u7279\u5f81\u4ea4\u4e92\uff0c\u751f\u6210\u8865\u4e01\u7ea7token\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528token\u7ea7\u81ea\u6ce8\u610f\u529b\u4f18\u5316\uff1b2) \u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\uff1aTransformer\u7f16\u7801\u5668\u5904\u7406token\u5e8f\u5217\uff0c\u5efa\u6a21\u8865\u4e01\u95f4\u65f6\u95f4\u4f9d\u8d56\u5e76\u751f\u6210\u9884\u6d4b\u3002", "result": "\u5728\u5177\u6709\u53d7\u63a7\u9759\u6001\u548c\u52a8\u6001\u56e0\u7d20\u7684\u5408\u6210\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u8865\u4e01\u7684token\u5316\u7b56\u7565\u76f8\u6bd4\u5377\u79ef\u548c\u57fa\u4e8e\u8865\u4e01\u7684Transformer\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u65f6\u95f4\u8868\u793a\u7684\u91cd\u8981\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c06\u5c40\u90e8\u65f6\u95f4\u7f16\u7801\u4e0e\u57fa\u4e8e\u5168\u5c40\u6ce8\u610f\u529b\u7684\u5efa\u6a21\u89e3\u8026\u80fd\u591f\u4ea7\u751f\u66f4\u6709\u6548\u548c\u7a33\u5b9a\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002"}}
{"id": "2601.12502", "categories": ["cs.LG", "math.NA", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.12502", "abs": "https://arxiv.org/abs/2601.12502", "authors": ["Mikhail Gennadievich Belov", "Victor Victorovich Dubov", "Vadim Konstantinovich Ivanov", "Alexander Yurievich Maslov", "Olga Vladimirovna Proshina", "Vladislav Gennadievich Malyshkin"], "title": "Semidefinite Programming for Quantum Channel Learning", "comment": null, "summary": "The problem of reconstructing a quantum channel from a sample of classical data is considered. When the total fidelity can be represented as a ratio of two quadratic forms (e.g., in the case of mapping a mixed state to a pure state, projective operators, unitary learning, and others), Semidefinite Programming (SDP) can be applied to solve the fidelity optimization problem with respect to the Choi matrix. A remarkable feature of SDP is that the optimization is convex, which allows the problem to be efficiently solved by a variety of numerical algorithms. We have tested several commercially available SDP solvers, all of which allowed for the reconstruction of quantum channels of different forms. A notable feature is that the Kraus rank of the obtained quantum channel typically comprises less than a few percent of its maximal possible value. This suggests that a relatively small Kraus rank quantum channel is typically sufficient to describe experimentally observed classical data. The theory was also applied to the problem of reconstructing projective operators from data. Finally, we discuss a classical computational model based on quantum channel transformation, performed and calculated on a classical computer, possibly hardware-optimized.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u534a\u6b63\u5b9a\u89c4\u5212\uff08SDP\uff09\u4ece\u7ecf\u5178\u6570\u636e\u91cd\u5efa\u91cf\u5b50\u901a\u9053\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4fdd\u771f\u5ea6\u53ef\u8868\u793a\u4e3a\u4e24\u4e2a\u4e8c\u6b21\u578b\u6bd4\u503c\u7684\u60c5\u51b5\uff0c\u5e76\u53d1\u73b0\u91cd\u5efa\u7684\u91cf\u5b50\u901a\u9053\u901a\u5e38\u5177\u6709\u8f83\u5c0f\u7684Kraus\u79e9\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4ece\u7ecf\u5178\u5b9e\u9a8c\u6570\u636e\u4e2d\u91cd\u5efa\u91cf\u5b50\u901a\u9053\uff0c\u89e3\u51b3\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u901a\u9053\u8868\u5f81\u548c\u4f18\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u534a\u6b63\u5b9a\u89c4\u5212\uff08SDP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316Choi\u77e9\u9635\u6765\u6700\u5927\u5316\u4fdd\u771f\u5ea6\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4fdd\u771f\u5ea6\u53ef\u8868\u793a\u4e3a\u4e24\u4e2a\u4e8c\u6b21\u578b\u6bd4\u503c\u7684\u60c5\u51b5\uff08\u5982\u6df7\u5408\u6001\u5230\u7eaf\u6001\u6620\u5c04\u3001\u6295\u5f71\u7b97\u5b50\u3001\u9149\u5b66\u4e60\u7b49\uff09\u3002", "result": "\u6d4b\u8bd5\u4e86\u591a\u4e2a\u5546\u4e1aSDP\u6c42\u89e3\u5668\uff0c\u90fd\u80fd\u6210\u529f\u91cd\u5efa\u4e0d\u540c\u5f62\u5f0f\u7684\u91cf\u5b50\u901a\u9053\u3002\u91cd\u5efa\u7684\u91cf\u5b50\u901a\u9053\u901a\u5e38\u5177\u6709\u8f83\u5c0f\u7684Kraus\u79e9\uff08\u901a\u5e38\u5c0f\u4e8e\u6700\u5927\u53ef\u80fd\u503c\u7684\u51e0\u4e2a\u767e\u5206\u6bd4\uff09\uff0c\u8868\u660e\u7528\u76f8\u5bf9\u7b80\u5355\u7684\u91cf\u5b50\u901a\u9053\u5c31\u80fd\u63cf\u8ff0\u5b9e\u9a8c\u89c2\u6d4b\u6570\u636e\u3002", "conclusion": "SDP\u662f\u91cd\u5efa\u91cf\u5b50\u901a\u9053\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u91cd\u5efa\u7684\u901a\u9053\u901a\u5e38\u5177\u6709\u4f4eKraus\u79e9\uff0c\u8be5\u65b9\u6cd5\u8fd8\u53ef\u5e94\u7528\u4e8e\u91cd\u5efa\u6295\u5f71\u7b97\u5b50\uff0c\u5e76\u8ba8\u8bba\u4e86\u57fa\u4e8e\u91cf\u5b50\u901a\u9053\u53d8\u6362\u7684\u7ecf\u5178\u8ba1\u7b97\u6a21\u578b\u3002"}}
{"id": "2601.12519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12519", "abs": "https://arxiv.org/abs/2601.12519", "authors": ["Abdullah Umut Hamzaogullari", "Arkadas Ozakin"], "title": "Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized Lagrangian Neural Networks", "comment": "21 pages", "summary": "Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from trajectory data, but their unusual optimization objective leads to significant training instabilities that limit their application to complex systems. We propose several improvements that address these fundamental challenges, namely, a Hessian regularization scheme that penalizes unphysical signatures in the Lagrangian's second derivatives with respect to velocities, preventing the network from learning unstable dynamics, activation functions that are better suited to the problem of learning Lagrangians, and a physics-aware coordinate scaling that improves stability. We systematically evaluate these techniques alongside previously proposed methods for improving stability. Our improved architecture successfully trains on systems of unprecedented complexity, including triple pendulums, and achieved 96.6\\% lower validation loss value and 90.68\\% better stability than baseline LNNs in double pendulum systems. With the improved framework, we show that our LNNs can learn Lagrangians representing geodesic motion in both non-relativistic and general relativistic settings. To deal with the relativistic setting, we extended our regularization to penalize violations of Lorentzian signatures, which allowed us to predict a geodesic Lagrangian under AdS\\textsubscript{4} spacetime metric directly from trajectory data, which to our knowledge has not been done in the literature before. This opens new possibilities for automated discovery of geometric structures in physics, including extraction of spacetime metric tensor components from geodesic trajectories. While our approach inherits some limitations of the original LNN framework, particularly the requirement for invertible Hessians, it significantly expands the practical applicability of LNNs for scientific discovery tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6539\u8fdb\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\uff08LNNs\uff09\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u5305\u62ecHessian\u6b63\u5219\u5316\u3001\u6539\u8fdb\u7684\u6fc0\u6d3b\u51fd\u6570\u548c\u7269\u7406\u611f\u77e5\u5750\u6807\u7f29\u653e\uff0c\u4f7fLNNs\u80fd\u5904\u7406\u66f4\u590d\u6742\u7cfb\u7edf\uff08\u5982\u4e09\u6446\uff09\u5e76\u6269\u5c55\u5230\u76f8\u5bf9\u8bba\u573a\u666f\u3002", "motivation": "\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\u80fd\u4ece\u8f68\u8ff9\u6570\u636e\u5b66\u4e60\u4efb\u610f\u62c9\u683c\u6717\u65e5\u91cf\uff0c\u4f46\u5176\u4e0d\u5bfb\u5e38\u7684\u4f18\u5316\u76ee\u6807\u5bfc\u81f4\u663e\u8457\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6539\u8fdb\uff1a1) Hessian\u6b63\u5219\u5316\u65b9\u6848\uff0c\u60e9\u7f5a\u62c9\u683c\u6717\u65e5\u91cf\u5bf9\u901f\u5ea6\u4e8c\u9636\u5bfc\u6570\u4e2d\u7684\u975e\u7269\u7406\u7279\u5f81\uff1b2) \u66f4\u9002\u5408\u5b66\u4e60\u62c9\u683c\u6717\u65e5\u91cf\u7684\u6fc0\u6d3b\u51fd\u6570\uff1b3) \u7269\u7406\u611f\u77e5\u5750\u6807\u7f29\u653e\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002\u8fd8\u6269\u5c55\u6b63\u5219\u5316\u4ee5\u5904\u7406\u76f8\u5bf9\u8bba\u573a\u666f\uff0c\u60e9\u7f5a\u6d1b\u4f26\u5179\u7279\u5f81\u8fdd\u53cd\u3002", "result": "\u6539\u8fdb\u67b6\u6784\u6210\u529f\u8bad\u7ec3\u524d\u6240\u672a\u6709\u7684\u590d\u6742\u7cfb\u7edf\uff08\u5305\u62ec\u4e09\u6446\uff09\uff0c\u5728\u53cc\u6446\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e96.6%\uff0c\u7a33\u5b9a\u6027\u63d0\u9ad890.68%\u3002\u9996\u6b21\u4ece\u8f68\u8ff9\u6570\u636e\u9884\u6d4bAdS\u2084\u65f6\u7a7a\u5ea6\u91cf\u4e0b\u7684\u6d4b\u5730\u7ebf\u62c9\u683c\u6717\u65e5\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u6269\u5c55\u4e86LNNs\u5728\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u7269\u7406\u4e2d\u51e0\u4f55\u7ed3\u6784\u7684\u81ea\u52a8\u53d1\u73b0\uff08\u5305\u62ec\u4ece\u6d4b\u5730\u7ebf\u8f68\u8ff9\u63d0\u53d6\u65f6\u7a7a\u5ea6\u91cf\u5f20\u91cf\u5206\u91cf\uff09\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2601.12525", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2601.12525", "abs": "https://arxiv.org/abs/2601.12525", "authors": ["Nikolaj Tatti"], "title": "Approximating splits for decision trees quickly in sparse data streams", "comment": null, "summary": "Decision trees are one of the most popular classifiers in the machine learning literature. While the most common decision tree learning algorithms treat data as a batch, numerous algorithms have been proposed to construct decision trees from a data stream. A standard training strategy involves augmenting the current tree by changing a leaf node into a split. Here we typically maintain counters in each leaf which allow us to determine the optimal split, and whether the split should be done. In this paper we focus on how to speed up the search for the optimal split when dealing with sparse binary features and a binary class. We focus on finding splits that have the approximately optimal information gain or Gini index. In both cases finding the optimal split can be done in $O(d)$ time, where $d$ is the number of features. We propose an algorithm that yields $(1 + \u03b1)$ approximation when using conditional entropy in amortized $O(\u03b1^{-1}(1 + m\\log d) \\log \\log n)$ time, where $m$ is the number of 1s in a data point, and $n$ is the number of data points. Similarly, for Gini index, we achieve $(1 + \u03b1)$ approximation in amortized $O(\u03b1^{-1} + m \\log d)$ time. Our approach is beneficial for sparse data where $m \\ll d$. In our experiments we find almost-optimal splits efficiently, faster than the baseline, overperforming the theoretical approximation guarantees.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u51b3\u7b56\u6811\u5206\u88c2\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u4fe1\u606f\u589e\u76ca\u548c\u57fa\u5c3c\u6307\u6570\u4e0a\u5b9e\u73b0(1+\u03b1)\u8fd1\u4f3c\uff0c\u65f6\u95f4\u590d\u6742\u5ea6\u4f18\u4e8e\u4f20\u7edfO(d)\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a00\u758f\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u51b3\u7b56\u6811\u5b66\u4e60\u7b97\u6cd5\u5728\u5904\u7406\u6570\u636e\u6d41\u65f6\uff0c\u9700\u8981\u5728\u6bcf\u4e2a\u53f6\u5b50\u8282\u70b9\u7ef4\u62a4\u8ba1\u6570\u5668\u6765\u786e\u5b9a\u6700\u4f18\u5206\u88c2\uff0c\u4f46\u641c\u7d22\u6700\u4f18\u5206\u88c2\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(d)\uff0c\u5176\u4e2dd\u662f\u7279\u5f81\u6570\u91cf\u3002\u5bf9\u4e8e\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u548c\u4e8c\u5143\u5206\u7c7b\u573a\u666f\uff0c\u5f53d\u5f88\u5927\u4f46\u6570\u636e\u7a00\u758f\u65f6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7684\u6548\u7387\u8f83\u4f4e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u8fd1\u4f3c\u5206\u88c2\u7b97\u6cd5\u3002\u5bf9\u4e8e\u4fe1\u606f\u589e\u76ca\uff08\u6761\u4ef6\u71b5\uff09\uff0c\u5728\u644a\u9500\u65f6\u95f4\u590d\u6742\u5ea6O(\u03b1^{-1}(1+m log d) log log n)\u5185\u5b9e\u73b0(1+\u03b1)\u8fd1\u4f3c\uff1b\u5bf9\u4e8e\u57fa\u5c3c\u6307\u6570\uff0c\u5728\u644a\u9500\u65f6\u95f4\u590d\u6742\u5ea6O(\u03b1^{-1}+m log d)\u5185\u5b9e\u73b0(1+\u03b1)\u8fd1\u4f3c\uff0c\u5176\u4e2dm\u662f\u6570\u636e\u70b9\u4e2d1\u7684\u6570\u91cf\uff0cn\u662f\u6570\u636e\u70b9\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u627e\u5230\u51e0\u4e4e\u6700\u4f18\u7684\u5206\u88c2\u70b9\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u5feb\uff0c\u5e76\u4e14\u5b9e\u9645\u6027\u80fd\u4f18\u4e8e\u7406\u8bba\u8fd1\u4f3c\u4fdd\u8bc1\u3002\u7279\u522b\u5728\u7a00\u758f\u6570\u636e(m\u226ad)\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u9488\u5bf9\u7a00\u758f\u4e8c\u5143\u7279\u5f81\u548c\u4e8c\u5143\u5206\u7c7b\u7684\u51b3\u7b56\u6811\u5206\u88c2\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u8fd1\u4f3c\u7b97\u6cd5\u5728\u4fdd\u8bc1\u8fd1\u4f3c\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u6d41\u573a\u666f\u4e0b\u7684\u7a00\u758f\u6570\u636e\u5904\u7406\u3002"}}
{"id": "2601.12543", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12543", "abs": "https://arxiv.org/abs/2601.12543", "authors": ["Alireza Ghahtarani", "Martin Cousineau", "Amir-massoud Farahmand", "Jorge E. Mendoza"], "title": "Press Start to Charge: Videogaming the Online Centralized Charging Scheduling Problem", "comment": "41 pages", "summary": "We study the online centralized charging scheduling problem (OCCSP). In this problem, a central authority must decide, in real time, when to charge dynamically arriving electric vehicles (EVs), subject to capacity limits, with the objective of balancing load across a finite planning horizon. To solve the problem, we first gamify it; that is, we model it as a game where charging blocks are placed within temporal and capacity constraints on a grid. We design heuristic policies, train learning agents with expert demonstrations, and improve them using Dataset Aggregation (DAgger). From a theoretical standpoint, we show that gamification reduces model complexity and yields tighter generalization bounds than vector-based formulations. Experiments across multiple EV arrival patterns confirm that gamified learning enhances load balancing. In particular, the image-to-movement model trained with DAgger consistently outperforms heuristic baselines, vector-based approaches, and supervised learning agents, while also demonstrating robustness in sensitivity analyses. These operational gains translate into tangible economic value. In a real-world case study for the Greater Montr\u00e9al Area (Qu\u00e9bec, Canada) using utility cost data, the proposed methods lower system costs by tens of millions of dollars per year over the prevailing practice and show clear potential to delay costly grid upgrades.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u7535\u52a8\u6c7d\u8f66\u5728\u7ebf\u96c6\u4e2d\u5145\u7535\u8c03\u5ea6\u95ee\u9898\uff08OCCSP\uff09\u6e38\u620f\u5316\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u52a8\u4f5c\u6a21\u578b\u7ed3\u5408DAgger\u8bad\u7ec3\uff0c\u5728\u8d1f\u8f7d\u5747\u8861\u548c\u7ecf\u6d4e\u6548\u76ca\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u5b9e\u65f6\u5145\u7535\u8c03\u5ea6\u95ee\u9898\uff0c\u9700\u8981\u5728\u6ee1\u8db3\u5bb9\u91cf\u9650\u5236\u7684\u540c\u65f6\u5e73\u8861\u7535\u7f51\u8d1f\u8f7d\uff0c\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u6a21\u578b\u590d\u6742\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\u3002", "method": "\u5c06\u5145\u7535\u8c03\u5ea6\u95ee\u9898\u6e38\u620f\u5316\u5efa\u6a21\u4e3a\u7f51\u683c\u4e0a\u7684\u7ea6\u675f\u653e\u7f6e\u95ee\u9898\uff0c\u8bbe\u8ba1\u542f\u53d1\u5f0f\u7b56\u7565\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5e76\u4f7f\u7528\u6570\u636e\u96c6\u805a\u5408\uff08DAgger\uff09\u8fdb\u884c\u6539\u8fdb\u3002", "result": "\u6e38\u620f\u5316\u65b9\u6cd5\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u5e76\u83b7\u5f97\u4e86\u66f4\u7d27\u7684\u6cdb\u5316\u8fb9\u754c\u3002\u5b9e\u9a8c\u8868\u660e\uff0cDAgger\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8d1f\u8f7d\u5747\u8861\u4e0a\u6301\u7eed\u4f18\u4e8e\u542f\u53d1\u5f0f\u57fa\u7ebf\u3001\u5411\u91cf\u65b9\u6cd5\u548c\u76d1\u7763\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5728\u8499\u7279\u5229\u5c14\u5730\u533a\u7684\u5b9e\u9645\u6848\u4f8b\u4e2d\u6bcf\u5e74\u53ef\u8282\u7701\u6570\u5343\u4e07\u7f8e\u5143\u7cfb\u7edf\u6210\u672c\u3002", "conclusion": "\u6e38\u620f\u5316\u5b66\u4e60\u4e3a\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u8c03\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u8d1f\u8f7d\u5747\u8861\u6027\u80fd\uff0c\u8fd8\u80fd\u663e\u8457\u964d\u4f4e\u7cfb\u7edf\u6210\u672c\u5e76\u63a8\u8fdf\u6602\u8d35\u7684\u7535\u7f51\u5347\u7ea7\u9700\u6c42\u3002"}}
{"id": "2601.12557", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12557", "abs": "https://arxiv.org/abs/2601.12557", "authors": ["Mark Moussa", "Amber V. Young", "Brianna Isola", "Vasuda Trehan", "Michael D. Himes", "Nicholas Wogan", "Giada Arney"], "title": "Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory", "comment": "8 pages, 4 figures. Submitted and accepted in AAAI-26 (IAAI Emerging Applications track)", "summary": "Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff08BCNN\u548cSQuAT\uff09\u7528\u4e8e\u4ece\u7cfb\u5916\u884c\u661f\u53cd\u5c04\u5149\u8c31\u9884\u6d4b\u751f\u7269\u6807\u5fd7\u7269\u901a\u91cf\uff0c\u4ee5\u652f\u6301\u672a\u6765\u65d7\u8230\u4efb\u52a1\u5982HWO\u7684\u89c2\u6d4b\u4f18\u5148\u7ea7\u51b3\u7b56\u3002", "motivation": "\u672a\u6765\u76f4\u63a5\u6210\u50cf\u65d7\u8230\u4efb\u52a1\uff08\u5982NASA\u7684HWO\uff09\u9762\u4e34\u4e25\u683c\u7684\u65f6\u95f4\u548c\u8d44\u6e90\u9650\u5236\uff0c\u9700\u8981\u9ad8\u6548\u4f18\u5148\u89c2\u6d4b\u76ee\u6807\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4ece\u53cd\u5c04\u5149\u8c31\u4e2d\u51c6\u786e\u9884\u6d4b\u751f\u7269\u6807\u5fd7\u7269\u901a\u91cf\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff1a1\uff09\u8d1d\u53f6\u65af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08BCNN\uff09\uff0c\u91cf\u5316\u8ba4\u77e5\u548c\u968f\u673a\u4e0d\u786e\u5b9a\u6027\uff1b2\uff09\u5149\u8c31\u67e5\u8be2\u81ea\u9002\u5e94\u53d8\u6362\u5668\uff08SQuAT\uff09\uff0c\u4f7f\u7528\u67e5\u8be2\u9a71\u52a8\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5c06\u5149\u8c31\u7279\u5f81\u4e0e\u7279\u5b9a\u751f\u7269\u6807\u5fd7\u7269\u7269\u79cd\u5173\u8054\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5728\u6db5\u76d6\u5e7f\u6cdb\u7cfb\u5916\u884c\u661f\u6761\u4ef6\u7684\u589e\u5f3a\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0cBCNN\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0cSQuAT\u5728\u5149\u8c31\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u6709\u671b\u52a0\u901f\u76ee\u6807\u7b5b\u9009\u3001\u4f18\u5316\u89c2\u6d4b\u8ba1\u5212\uff0c\u6700\u5927\u5316\u672a\u6765\u65d7\u8230\u4efb\u52a1\uff08\u5982HWO\uff09\u7684\u79d1\u5b66\u56de\u62a5\uff0c\u4e3a\u7cfb\u5916\u884c\u661f\u751f\u7269\u6807\u5fd7\u7269\u63a2\u6d4b\u63d0\u4f9b\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2601.12598", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12598", "abs": "https://arxiv.org/abs/2601.12598", "authors": ["Younes Bouhadjar", "Maxime Fabre", "Felix Schmidt", "Emre Neftci"], "title": "Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization", "comment": "11 pages, 4 figures and 4 tables", "summary": "Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench", "AI": {"tldr": "\u63d0\u51faSelectivBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u7684\u9009\u62e9\u6027\u80fd\u529b\uff0c\u53d1\u73b0\u95e8\u63a7\u548c\u5feb\u901f\u9057\u5fd8\u673a\u5236\u6709\u52a9\u4e8e\u53ec\u56de\uff0c\u901a\u9053\u6df7\u5408\u5bf9\u6cdb\u5316\u5173\u952e\uff0c\u6ce8\u610f\u529b\u673a\u5236\u56e0\u5185\u5b58\u5bb9\u91cf\u968f\u5e8f\u5217\u957f\u5ea6\u6269\u5c55\u800c\u4fdd\u6301\u4f18\u52bf\u3002", "motivation": "\u7ebf\u6027\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3aTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u67b6\u6784\u673a\u5236\u65e5\u76ca\u590d\u6742\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u76f4\u63a5\u6bd4\u8f83\u3002\u73b0\u6709\u57fa\u51c6\u4efb\u52a1\u8981\u4e48\u8fc7\u4e8e\u7b80\u5355\u65e0\u6cd5\u63ed\u793a\u663e\u8457\u5dee\u5f02\uff0c\u8981\u4e48\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u96be\u4ee5\u5b9e\u9a8c\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u7684\u7ec6\u5316\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f15\u5165SelectivBench\u2014\u2014\u4e00\u5957\u8f7b\u91cf\u7ea7\u53ef\u5b9a\u5236\u7684\u5408\u6210\u57fa\u51c6\u4efb\u52a1\u96c6\u3002\u8be5\u57fa\u51c6\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u6cd5\u751f\u6210\u53ef\u8c03\u6574\u590d\u6742\u5ea6\u7684\u5e8f\u5217\uff0c\u5305\u542b\u6545\u610f\u8fdd\u53cd\u8f6c\u6362\u89c4\u5219\u7684\u4e0d\u89c4\u5219\u95f4\u9694\uff0c\u4e13\u95e8\u8bc4\u4f30\u5e8f\u5217\u6a21\u578b\u5728\u5c0f\u5230\u4e2d\u7b49\u89c4\u6a21\u7684\u9009\u62e9\u6027\u80fd\u529b\u3002", "result": "\u5728\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6027\u80fd\u6a21\u5f0f\u4e0e\u5927\u89c4\u6a21\u8bed\u8a00\u4efb\u52a1\u7ed3\u679c\u4e00\u81f4\u3002\u5206\u6790\u8868\u660e\uff1a\u95e8\u63a7\u548c\u5feb\u901f\u9057\u5fd8\u673a\u5236\u4fc3\u8fdb\u53ec\u56de\uff1b\u72b6\u6001\u5185\u901a\u9053\u6df7\u5408\u5bf9\u9009\u62e9\u6027\u4e0d\u5fc5\u8981\u4f46\u5bf9\u6cdb\u5316\u5173\u952e\uff1bsoftmax\u6ce8\u610f\u529b\u56e0\u5185\u5b58\u5bb9\u91cf\u968f\u5e8f\u5217\u957f\u5ea6\u6269\u5c55\u800c\u4fdd\u6301\u4e3b\u5bfc\u5730\u4f4d\u3002", "conclusion": "SelectivBench\u4e3a\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u9488\u5bf9\u6027\u7684\u9ad8\u6548\u63a2\u7d22\u5de5\u5177\uff0c\u5e76\u4e3a\u7814\u7a76\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u53d7\u63a7\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4e0d\u540c\u67b6\u6784\u7279\u5f81\u7684\u4f5c\u7528\u3002"}}
{"id": "2601.12604", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12604", "abs": "https://arxiv.org/abs/2601.12604", "authors": ["Safwan Labbi", "Daniil Tiapkin", "Paul Mangold", "Eric Moulines"], "title": "Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization", "comment": null, "summary": "Policy gradient methods are known to be highly sensitive to the choice of policy parameterization. In particular, the widely used softmax parameterization can induce ill-conditioned optimization landscapes and lead to exponentially slow convergence. Although this can be mitigated by preconditioning, this solution is often computationally expensive. Instead, we propose replacing the softmax with an alternative family of policy parameterizations based on the generalized f-softargmax. We further advocate coupling this parameterization with a regularizer induced by the same f-divergence, which improves the optimization landscape and ensures that the resulting regularized objective satisfies a Polyak-Lojasiewicz inequality. Leveraging this structure, we establish the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods for finite MDPs without any form of preconditioning. We also derive sample-complexity bounds for the unregularized problem and show that f-PG, with Tsallis divergences achieves polynomial sample complexity in contrast to the exponential complexity incurred by the standard softmax parameterization.", "AI": {"tldr": "\u63d0\u51fa\u7528\u5e7f\u4e49f-softargmax\u66ff\u4ee3softmax\u7b56\u7565\u53c2\u6570\u5316\uff0c\u7ed3\u5408f-\u6563\u5ea6\u6b63\u5219\u5316\uff0c\u65e0\u9700\u9884\u6761\u4ef6\u5373\u53ef\u4fdd\u8bc1\u6709\u9650MDP\u4e2d\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u975e\u6e10\u8fd1\u6536\u655b", "motivation": "\u4f20\u7edfsoftmax\u53c2\u6570\u5316\u4f1a\u5bfc\u81f4\u75c5\u6001\u4f18\u5316\u666f\u89c2\u548c\u6307\u6570\u7ea7\u6162\u6536\u655b\uff0c\u800c\u9884\u6761\u4ef6\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4f7f\u7528\u5e7f\u4e49f-softargmax\u4f5c\u4e3a\u7b56\u7565\u53c2\u6570\u5316\uff0c\u914d\u5408\u5bf9\u5e94f-\u6563\u5ea6\u7684\u6b63\u5219\u5316\u5668\uff0c\u6539\u5584\u4f18\u5316\u666f\u89c2\u5e76\u6ee1\u8db3Polyak-Lojasiewicz\u4e0d\u7b49\u5f0f", "result": "\u9996\u6b21\u4e3a\u6709\u9650MDP\u7684\u968f\u673a\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u5efa\u7acb\u4e86\u65e0\u9700\u9884\u6761\u4ef6\u7684\u663e\u5f0f\u975e\u6e10\u8fd1\u6700\u540e\u8fed\u4ee3\u6536\u655b\u4fdd\u8bc1\uff1bTsallis\u6563\u5ea6\u7684f-PG\u5b9e\u73b0\u591a\u9879\u5f0f\u6837\u672c\u590d\u6742\u5ea6\uff0c\u800c\u6807\u51c6softmax\u9700\u8981\u6307\u6570\u590d\u6742\u5ea6", "conclusion": "f-softargmax\u53c2\u6570\u5316\u52a0f-\u6563\u5ea6\u6b63\u5219\u5316\u662f\u89e3\u51b3\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u6536\u655b\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfsoftmax\u65b9\u6cd5"}}
{"id": "2601.12624", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.12624", "abs": "https://arxiv.org/abs/2601.12624", "authors": ["Shiqi Wang", "Mahdi Khosravy", "Neeraj Gupta", "Olaf Witkowski"], "title": "Towards Robust Universal Perturbation Attacks: A Float-Coded, Penalty-Driven Evolutionary Approach", "comment": null, "summary": "Universal adversarial perturbations (UAPs) have garnered significant attention due to their ability to undermine deep neural networks across multiple inputs using a single noise pattern. Evolutionary algorithms offer a promising approach to generating such perturbations due to their ability to navigate non-convex, gradient-free landscapes. In this work, we introduce a float-coded, penalty-driven single-objective evolutionary framework for UAP generation that achieves lower visibility perturbations while enhancing attack success rates. Our approach leverages continuous gene representations aligned with contemporary deep learning scales, incorporates dynamic evolutionary operators with adaptive scheduling, and utilizes a modular PyTorch implementation for seamless integration with modern architectures. Additionally, we ensure the universality of the generated perturbations by testing across diverse models and by periodically switching batches to prevent overfitting. Experimental results on the ImageNet dataset demonstrate that our framework consistently produces perturbations with smaller norms, higher misclassification effectiveness, and faster convergence compared to existing evolutionary-based methods. These findings highlight the robustness and scalability of our approach for universal adversarial attacks across various deep learning architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d6e\u70b9\u7f16\u7801\u3001\u60e9\u7f5a\u9a71\u52a8\u7684\u5355\u76ee\u6807\u8fdb\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff0c\u5728\u964d\u4f4e\u53ef\u89c1\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u901a\u7528\u5bf9\u6297\u6270\u52a8\u80fd\u591f\u7528\u5355\u4e00\u566a\u58f0\u6a21\u5f0f\u7834\u574f\u591a\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8f93\u5165\uff0c\u8fdb\u5316\u7b97\u6cd5\u56e0\u5176\u5728\u975e\u51f8\u3001\u65e0\u68af\u5ea6\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u800c\u6210\u4e3a\u6709\u524d\u666f\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6d6e\u70b9\u7f16\u7801\u7684\u5355\u76ee\u6807\u8fdb\u5316\u6846\u67b6\uff0c\u5305\u542b\u4e0e\u5f53\u4ee3\u6df1\u5ea6\u5b66\u4e60\u89c4\u6a21\u5bf9\u9f50\u7684\u8fde\u7eed\u57fa\u56e0\u8868\u793a\u3001\u5177\u6709\u81ea\u9002\u5e94\u8c03\u5ea6\u7684\u52a8\u6001\u8fdb\u5316\u7b97\u5b50\uff0c\u4ee5\u53ca\u6a21\u5757\u5316PyTorch\u5b9e\u73b0\u3002\u901a\u8fc7\u8de8\u6a21\u578b\u6d4b\u8bd5\u548c\u5468\u671f\u6027\u6279\u6b21\u5207\u6362\u786e\u4fdd\u6270\u52a8\u7684\u901a\u7528\u6027\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u8fdb\u5316\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u80fd\u4ea7\u751f\u66f4\u5c0f\u8303\u6570\u3001\u66f4\u9ad8\u8bef\u5206\u7c7b\u6548\u679c\u548c\u66f4\u5feb\u6536\u655b\u7684\u6270\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e0a\u8fdb\u884c\u901a\u7528\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2601.12637", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.12637", "abs": "https://arxiv.org/abs/2601.12637", "authors": ["Long D. Nguyen", "Kelin Xia", "Binh P. Nguyen"], "title": "Topology-Aware Multiscale Mixture of Experts for Efficient Molecular Property Prediction", "comment": null, "summary": "Many molecular properties depend on 3D geometry, where non-covalent interactions, stereochemical effects, and medium- to long-range forces are determined by spatial distances and angles that cannot be uniquely captured by a 2D bond graph. Yet most 3D molecular graph neural networks still rely on globally fixed neighborhood heuristics, typically defined by distance cutoffs and maximum neighbor limits, to define local message-passing neighborhoods, leading to rigid, data-agnostic interaction budgets. We propose Multiscale Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across geometric regimes. Our contributions are threefold: (1) we introduce a distance-cutoff expert ensemble that explicitly captures short-, mid-, and long-range interactions without committing to a single cutoff; (2) we design a topological gating encoder that routes inputs to experts using filtration-based descriptors, including persistent homology features, summarizing how connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in module that consistently improves multiple strong 3D molecular backbones across diverse molecular and polymer property prediction benchmark datasets, covering both regression and classification tasks. These results highlight topology-aware multiscale routing as an effective principle for 3D molecular graph learning.", "AI": {"tldr": "MI-MoE\uff1a\u4e00\u79cd\u7528\u4e8e3D\u5206\u5b50\u56fe\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u4ea4\u4e92\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u8def\u7531\u673a\u5236\u81ea\u9002\u5e94\u5efa\u6a21\u77ed\u3001\u4e2d\u3001\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528", "motivation": "\u73b0\u67093D\u5206\u5b50\u56fe\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u56fa\u5b9a\u7684\u90bb\u57df\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u8ddd\u79bb\u622a\u65ad\u548c\u6700\u5927\u90bb\u5c45\u9650\u5236\uff09\uff0c\u5bfc\u81f4\u521a\u6027\u7684\u3001\u6570\u636e\u65e0\u5173\u7684\u4ea4\u4e92\u9884\u7b97\uff0c\u65e0\u6cd5\u81ea\u9002\u5e94\u5efa\u6a21\u4e0d\u540c\u51e0\u4f55\u5c3a\u5ea6\u4e0b\u7684\u76f8\u4e92\u4f5c\u7528", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u4ea4\u4e92\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MI-MoE\uff09\uff1a1\uff09\u8ddd\u79bb\u622a\u65ad\u4e13\u5bb6\u96c6\u5408\uff0c\u663e\u5f0f\u6355\u83b7\u77ed\u3001\u4e2d\u3001\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\uff1b2\uff09\u62d3\u6251\u95e8\u63a7\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u57fa\u4e8e\u8fc7\u6ee4\u7684\u63cf\u8ff0\u7b26\uff08\u5305\u62ec\u6301\u4e45\u540c\u8c03\u7279\u5f81\uff09\u5c06\u8f93\u5165\u8def\u7531\u5230\u4e13\u5bb6\uff1b3\uff09\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u6539\u8fdb\u73b0\u67093D\u5206\u5b50\u9aa8\u5e72\u7f51\u7edc", "result": "MI-MoE\u5728\u591a\u4e2a\u5206\u5b50\u548c\u805a\u5408\u7269\u6027\u8d28\u9884\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff08\u6db5\u76d6\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\uff09\u4e0a\uff0c\u4e00\u81f4\u6539\u8fdb\u4e86\u591a\u4e2a\u5f3a\u5927\u76843D\u5206\u5b50\u9aa8\u5e72\u7f51\u7edc\u6027\u80fd", "conclusion": "\u62d3\u6251\u611f\u77e5\u7684\u591a\u5c3a\u5ea6\u8def\u7531\u662f3D\u5206\u5b50\u56fe\u5b66\u4e60\u7684\u6709\u6548\u539f\u5219\uff0cMI-MoE\u901a\u8fc7\u81ea\u9002\u5e94\u5efa\u6a21\u4e0d\u540c\u51e0\u4f55\u5c3a\u5ea6\u4e0b\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u5347\u4e86\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u6027\u80fd"}}
{"id": "2601.12654", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12654", "abs": "https://arxiv.org/abs/2601.12654", "authors": ["Hyunseung Hwang", "Seungeun Lee", "Lucas Rosenblatt", "Julia Stoyanovich", "Steven Euijong Whang"], "title": "Explanation Multiplicity in SHAP: Characterization and Assessment", "comment": null, "summary": "Post-hoc explanations are widely used to justify, contest, and audit automated decisions in high-stakes domains. SHAP, in particular, is often treated as a reliable account of which features drove an individual prediction. Yet SHAP explanations can vary substantially across repeated runs even when the input, task, and trained model are held fixed. We term this phenomenon explanation multiplicity: multiple internally valid but substantively different explanations for the same decision. We present a methodology to characterize multiplicity in feature-attribution explanations and to disentangle sources due to model training/selection from stochasticity intrinsic to the explanation pipeline. We further show that apparent stability depends on the metric: magnitude-based distances can remain near zero while rank-based measures reveal substantial churn in the identity and ordering of top features. To contextualize observed disagreement, we derive randomized baseline values under plausible null models. Across datasets, model classes, and confidence regimes, we find explanation multiplicity is pervasive and persists even for high-confidence predictions, highlighting the need for metrics and baselines that match the intended use of explanations.", "AI": {"tldr": "SHAP\u89e3\u91ca\u5b58\u5728\u591a\u91cd\u6027\uff1a\u76f8\u540c\u8f93\u5165\u3001\u4efb\u52a1\u548c\u6a21\u578b\u4e0b\uff0c\u591a\u6b21\u8fd0\u884c\u4f1a\u4ea7\u751f\u4e0d\u540c\u4f46\u90fd\u6709\u6548\u7684\u7279\u5f81\u5f52\u56e0\u89e3\u91ca\uff0c\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u5728\u591a\u79cd\u573a\u666f\u4e0b\u666e\u904d\u5b58\u5728\u3002", "motivation": "SHAP\u7b49\u4e8b\u540e\u89e3\u91ca\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u51b3\u7b56\u7684\u9a8c\u8bc1\u548c\u5ba1\u8ba1\uff0c\u4f46SHAP\u89e3\u91ca\u5728\u91cd\u590d\u8fd0\u884c\u4e2d\u53ef\u80fd\u4ea7\u751f\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u79cd\u89e3\u91ca\u591a\u91cd\u6027\u73b0\u8c61\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u548c\u91cf\u5316\u3002", "method": "\u63d0\u51fa\u91cf\u5316\u7279\u5f81\u5f52\u56e0\u89e3\u91ca\u591a\u91cd\u6027\u7684\u65b9\u6cd5\u5b66\uff0c\u533a\u5206\u6a21\u578b\u8bad\u7ec3/\u9009\u62e9\u4e0e\u89e3\u91ca\u6d41\u7a0b\u5185\u5728\u968f\u673a\u6027\u7684\u6765\u6e90\uff0c\u4f7f\u7528\u5e45\u5ea6\u8ddd\u79bb\u548c\u6392\u5e8f\u8ddd\u79bb\u8bc4\u4f30\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u57fa\u7ebf\u6a21\u578b\u63d0\u4f9b\u5bf9\u6bd4\u57fa\u51c6\u3002", "result": "\u89e3\u91ca\u591a\u91cd\u6027\u666e\u904d\u5b58\u5728\uff0c\u5373\u4f7f\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4e2d\u4e5f\u4f1a\u6301\u7eed\u51fa\u73b0\uff1b\u5e45\u5ea6\u8ddd\u79bb\u53ef\u80fd\u663e\u793a\u7a33\u5b9a\uff0c\u4f46\u6392\u5e8f\u8ddd\u79bb\u63ed\u793a\u7279\u5f81\u8eab\u4efd\u548c\u987a\u5e8f\u7684\u663e\u8457\u53d8\u5316\uff1b\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6a21\u578b\u7c7b\u522b\u548c\u7f6e\u4fe1\u5ea6\u533a\u95f4\u5747\u89c2\u5bdf\u5230\u8fd9\u79cd\u73b0\u8c61\u3002", "conclusion": "SHAP\u89e3\u91ca\u7684\u591a\u91cd\u6027\u6311\u6218\u4e86\u5176\u4f5c\u4e3a\u53ef\u9760\u51b3\u7b56\u89e3\u91ca\u7684\u5730\u4f4d\uff0c\u9700\u8981\u5f00\u53d1\u4e0e\u89e3\u91ca\u9884\u671f\u7528\u9014\u76f8\u5339\u914d\u7684\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u7ebf\uff0c\u4ee5\u786e\u4fdd\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2601.12680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12680", "abs": "https://arxiv.org/abs/2601.12680", "authors": ["Zheng Fang", "Wolfgang Mayer", "Zeyu Zhang", "Jian Wang", "Hong-Yu Zhang", "Wanli Li", "Zaiwen Feng"], "title": "MetaToolAgent: Towards Generalizable Tool Usage in LLMs through Meta-Learning", "comment": null, "summary": "Tool learning is increasingly important for large language models (LLMs) to effectively coordinate and utilize a diverse set of tools in order to solve complex real-world tasks. By selecting and integrating appropriate tools, LLMs extend their capabilities beyond pure language understanding to perform specialized functions. However, existing methods for tool selection often focus on limited tool sets and struggle to generalize to novel tools encountered in practical deployments. To address these challenges, we introduce a comprehensive dataset spanning 7 domains, containing 155 tools and 9,377 question-answer pairs, which simulates realistic integration scenarios. Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed to improve cross-tool generalization. Experimental results show that MTA significantly outperforms baseline methods on unseen tools, demonstrating its promise for building flexible and scalable systems that require dynamic tool coordination.", "AI": {"tldr": "\u63d0\u51faMetaToolAgent (MTA)\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5305\u542b155\u4e2a\u5de5\u5177\u548c9,377\u4e2a\u95ee\u7b54\u5bf9\u7684\u8de87\u4e2a\u9886\u57df\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u9009\u62e9\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u6709\u9650\u5de5\u5177\u96c6\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u5b9e\u9645\u90e8\u7f72\u4e2d\u9047\u5230\u7684\u65b0\u5de5\u5177\uff0c\u8fd9\u9650\u5236\u4e86LLM\u5728\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u5de5\u5177\u534f\u8c03\u80fd\u529b\u3002", "method": "\u63d0\u51faMetaToolAgent (MTA)\uff0c\u4e00\u79cd\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u6539\u8fdb\u8de8\u5de5\u5177\u6cdb\u5316\u80fd\u529b\u3002\u6784\u5efa\u4e86\u5305\u542b7\u4e2a\u9886\u57df\u3001155\u4e2a\u5de5\u5177\u548c9,377\u4e2a\u95ee\u7b54\u5bf9\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u6a21\u62df\u771f\u5b9e\u96c6\u6210\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMTA\u5728\u672a\u89c1\u5de5\u5177\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6784\u5efa\u9700\u8981\u52a8\u6001\u5de5\u5177\u534f\u8c03\u7684\u7075\u6d3b\u53ef\u6269\u5c55\u7cfb\u7edf\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "MTA\u901a\u8fc7\u5143\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u5177\u9009\u62e9\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u80fd\u591f\u52a8\u6001\u534f\u8c03\u591a\u6837\u5316\u5de5\u5177\u7684LLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12703", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12703", "abs": "https://arxiv.org/abs/2601.12703", "authors": ["Andrew Gordon", "Garrett Baker", "George Wang", "William Snell", "Stan van Wingerden", "Daniel Murfet"], "title": "Towards Spectroscopy: Susceptibility Clusters in Language Models", "comment": null, "summary": "Spectroscopy infers the internal structure of physical systems by measuring their response to perturbations. We apply this principle to neural networks: perturbing the data distribution by upweighting a token $y$ in context $x$, we measure the model's response via susceptibilities $\u03c7_{xy}$, which are covariances between component-level observables and the perturbation computed over a localized Gibbs posterior via stochastic gradient Langevin dynamics (SGLD). Theoretically, we show that susceptibilities decompose as a sum over modes of the data distribution, explaining why tokens that follow their contexts \"for similar reasons\" cluster together in susceptibility space. Empirically, we apply this methodology to Pythia-14M, developing a conductance-based clustering algorithm that identifies 510 interpretable clusters ranging from grammatical patterns to code structure to mathematical notation. Comparing to sparse autoencoders, 50% of our clusters match SAE features, validating that both methods recover similar structure.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8c31\u5206\u6790\u7684\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u7ed3\u6784\u63a2\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5206\u5e03\u6d4b\u91cf\u6a21\u578b\u54cd\u5e94\uff0c\u8bc6\u522b\u51fa510\u4e2a\u53ef\u89e3\u91ca\u7684\u805a\u7c7b\u6a21\u5f0f", "motivation": "\u501f\u9274\u7269\u7406\u5b66\u4e2d\u7684\u8c31\u5206\u6790\u539f\u7406\uff0c\u901a\u8fc7\u6270\u52a8\u6570\u636e\u5206\u5e03\u6765\u63a2\u6d4b\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u7ed3\u6784\uff0c\u7406\u89e3\u6a21\u578b\u5982\u4f55\u5904\u7406\u4e0d\u540c\u4e0a\u4e0b\u6587\u6a21\u5f0f", "method": "\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\uff08SGLD\uff09\u8ba1\u7b97\u5c40\u90e8\u5409\u5e03\u65af\u540e\u9a8c\uff0c\u901a\u8fc7\u6d4b\u91cf\u6a21\u578b\u5bf9token\u6743\u91cd\u6270\u52a8\u7684\u54cd\u5e94\uff08\u654f\u611f\u6027\u03c7_xy\uff09\uff0c\u5f00\u53d1\u57fa\u4e8e\u7535\u5bfc\u7684\u805a\u7c7b\u7b97\u6cd5", "result": "\u5728Pythia-14M\u6a21\u578b\u4e2d\u8bc6\u522b\u51fa510\u4e2a\u53ef\u89e3\u91ca\u805a\u7c7b\uff0c\u6db5\u76d6\u8bed\u6cd5\u6a21\u5f0f\u3001\u4ee3\u7801\u7ed3\u6784\u3001\u6570\u5b66\u7b26\u53f7\u7b49\uff0c50%\u805a\u7c7b\u4e0e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5339\u914d", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u8c31\u5206\u6790\u539f\u7406\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u63a2\u6d4b\u6a21\u578b\u5185\u90e8\u7ed3\u6784\u7684\u65b0\u65b9\u6cd5\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u679c\u4e00\u81f4\u4e14\u66f4\u5177\u89e3\u91ca\u6027"}}
{"id": "2601.12704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12704", "abs": "https://arxiv.org/abs/2601.12704", "authors": ["Yan Ma", "Yumeng Ren"], "title": "Adaptively trained Physics-informed Radial Basis Function Neural Networks for Solving Multi-asset Option Pricing Problems", "comment": "30 pages,16 figures", "summary": "The present study investigates the numerical solution of Black-Scholes partial differential equation (PDE) for option valuation with multiple underlying assets. We develop a physics-informed (PI) machine learning algorithm based on a radial basis function neural network (RBFNN) that concurrently optimizes the network architecture and predicts the target option price. The physics-informed radial basis function neural network (PIRBFNN) combines the strengths of the traditional radial basis function collocation method and the physics-informed neural network machine learning approach to effectively solve PDE problems in the financial context. By employing a PDE residual-based technique to adaptively refine the distribution of hidden neurons during the training process, the PIRBFNN facilitates accurate and efficient handling of multidimensional option pricing models featuring non-smooth payoff conditions. The validity of the proposed method is demonstrated through a set of experiments encompassing a single-asset European put option, a double-asset exchange option, and a four-asset basket call option.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f84\u5411\u57fa\u51fd\u6570\u795e\u7ecf\u7f51\u7edc\u7684\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff08PIRBFNN\uff09\uff0c\u7528\u4e8e\u6c42\u89e3\u591a\u8d44\u4ea7\u671f\u6743\u5b9a\u4ef7\u7684Black-Scholes\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u63d0\u9ad8\u8ba1\u7b97\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6c42\u89e3\u591a\u8d44\u4ea7\u671f\u6743\u5b9a\u4ef7\u7684Black-Scholes\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u548c\u975e\u5149\u6ed1\u652f\u4ed8\u6761\u4ef6\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u51c6\u786e\u7684\u6570\u503c\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u5f84\u5411\u57fa\u51fd\u6570\u914d\u7f6e\u6cd5\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u51faPIRBFNN\u7b97\u6cd5\uff0c\u91c7\u7528PDE\u6b8b\u5dee\u6280\u672f\u81ea\u9002\u5e94\u4f18\u5316\u9690\u85cf\u795e\u7ecf\u5143\u5206\u5e03\uff0c\u5904\u7406\u591a\u7ef4\u671f\u6743\u5b9a\u4ef7\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5355\u8d44\u4ea7\u6b27\u5f0f\u770b\u8dcc\u671f\u6743\u3001\u53cc\u8d44\u4ea7\u4ea4\u6362\u671f\u6743\u548c\u56db\u8d44\u4ea7\u7bee\u5b50\u770b\u6da8\u671f\u6743\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51c6\u786e\u9ad8\u6548\u5904\u7406\u591a\u7ef4\u975e\u5149\u6ed1\u652f\u4ed8\u6761\u4ef6\u95ee\u9898\u3002", "conclusion": "PIRBFNN\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u4f20\u7edf\u5f84\u5411\u57fa\u51fd\u6570\u914d\u7f6e\u6cd5\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u70b9\uff0c\u4e3a\u91d1\u878d\u9886\u57df\u591a\u7ef4PDE\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u503c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12706", "abs": "https://arxiv.org/abs/2601.12706", "authors": ["Sina Kazemdehbashi"], "title": "Trend-Adjusted Time Series Models with an Application to Gold Price Forecasting", "comment": null, "summary": "Time series data play a critical role in various fields, including finance, healthcare, marketing, and engineering. A wide range of techniques (from classical statistical models to neural network-based approaches such as Long Short-Term Memory (LSTM)) have been employed to address time series forecasting challenges. In this paper, we reframe time series forecasting as a two-part task: (1) predicting the trend (directional movement) of the time series at the next time step, and (2) forecasting the quantitative value at the next time step. The trend can be predicted using a binary classifier, while quantitative values can be forecasted using models such as LSTM and Bidirectional Long Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values based on the predicted trend provided by the binary classifier. We validate the proposed approach through both theoretical analysis and empirical evaluation. The TATS model is applied to a volatile financial time series (the daily gold price) with the objective of forecasting the next days price. Experimental results demonstrate that TATS consistently outperforms standard LSTM and Bi-LSTM models by achieving significantly lower forecasting error. In addition, our results indicate that commonly used metrics such as MSE and MAE are insufficient for fully assessing time series model performance. Therefore, we also incorporate trend detection accuracy, which measures how effectively a model captures trends in a time series.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTATS\u6a21\u578b\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u91cd\u6784\u4e3a\u8d8b\u52bf\u9884\u6d4b\u548c\u6570\u503c\u9884\u6d4b\u4e24\u90e8\u5206\uff0c\u901a\u8fc7\u4e8c\u5143\u5206\u7c7b\u5668\u9884\u6d4b\u8d8b\u52bf\u65b9\u5411\uff0c\u518d\u7528LSTM/Bi-LSTM\u9884\u6d4b\u6570\u503c\uff0c\u6700\u540e\u57fa\u4e8e\u9884\u6d4b\u8d8b\u52bf\u8c03\u6574\u6570\u503c\u9884\u6d4b\u7ed3\u679c\uff0c\u5728\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff08\u5305\u62ec\u7edf\u8ba1\u6a21\u578b\u548cLSTM\u7b49\u795e\u7ecf\u7f51\u7edc\uff09\u901a\u5e38\u76f4\u63a5\u9884\u6d4b\u6570\u503c\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5e94\u5206\u4e3a\u8d8b\u52bf\u9884\u6d4b\uff08\u65b9\u5411\u6027\u8fd0\u52a8\uff09\u548c\u6570\u503c\u9884\u6d4b\u4e24\u90e8\u5206\u3002\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5982MSE\u548cMAE\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u52a0\u5165\u8d8b\u52bf\u68c0\u6d4b\u51c6\u786e\u7387\u6765\u8861\u91cf\u6a21\u578b\u6355\u6349\u8d8b\u52bf\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u8d8b\u52bf\u8c03\u6574\u65f6\u95f4\u5e8f\u5217\uff08TATS\uff09\u6a21\u578b\uff1a1\uff09\u4f7f\u7528\u4e8c\u5143\u5206\u7c7b\u5668\u9884\u6d4b\u4e0b\u4e00\u65f6\u95f4\u6b65\u7684\u8d8b\u52bf\u65b9\u5411\uff08\u4e0a\u6da8/\u4e0b\u8dcc\uff09\uff1b2\uff09\u4f7f\u7528LSTM\u6216Bi-LSTM\u9884\u6d4b\u4e0b\u4e00\u65f6\u95f4\u6b65\u7684\u6570\u503c\uff1b3\uff09\u57fa\u4e8e\u5206\u7c7b\u5668\u9884\u6d4b\u7684\u8d8b\u52bf\u65b9\u5411\u8c03\u6574LSTM/Bi-LSTM\u7684\u9884\u6d4b\u503c\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u5728\u6ce2\u52a8\u6027\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\uff08\u6bcf\u65e5\u9ec4\u91d1\u4ef7\u683c\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTATS\u6a21\u578b\u76f8\u6bd4\u6807\u51c6LSTM\u548cBi-LSTM\u6a21\u578b\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u3002\u540c\u65f6\u53d1\u73b0\u4f20\u7edf\u8bc4\u4f30\u6307\u6807MSE\u548cMAE\u4e0d\u8db3\u4ee5\u5168\u9762\u8bc4\u4f30\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u6027\u80fd\uff0c\u9700\u8981\u52a0\u5165\u8d8b\u52bf\u68c0\u6d4b\u51c6\u786e\u7387\u6307\u6807\u3002", "conclusion": "\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u91cd\u6784\u4e3a\u8d8b\u52bf\u9884\u6d4b\u548c\u6570\u503c\u9884\u6d4b\u4e24\u90e8\u5206\u662f\u6709\u6548\u7684\uff0cTATS\u6a21\u578b\u901a\u8fc7\u8d8b\u52bf\u8c03\u6574\u673a\u5236\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002\u672a\u6765\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u8bc4\u4f30\u5e94\u540c\u65f6\u8003\u8651\u6570\u503c\u9884\u6d4b\u7cbe\u5ea6\u548c\u8d8b\u52bf\u6355\u6349\u80fd\u529b\u3002"}}
{"id": "2601.12730", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12730", "abs": "https://arxiv.org/abs/2601.12730", "authors": ["Zhaochun Li", "Chen Wang", "Jionghao Bai", "Shisheng Cui", "Ge Lan", "Zhou Zhao", "Yue Wang"], "title": "Distribution-Centric Policy Optimization Dominates Exploration-Exploitation Trade-off", "comment": null, "summary": "The exploration-exploitation (EE) trade-off is a central challenge in reinforcement learning (RL) for large language models (LLMs). With Group Relative Policy Optimization (GRPO), training tends to be exploitation driven: entropy decreases monotonically, samples convergence, and exploration fades. Most existing fixes are \\textbf{sample-centric}: they seek or bonus rare samples, assuming exploration comes from novel trajectories and tokens. These heuristics depend on the \"luck\" of informative samples, lack principled control of the policy, and often yield limited or inconsistent gains. In this work, we are the first to introduce a \\textbf{distribution-centric} perspective for RL, in which exploration is always guided by a \"better\" target distribution, and reveal that a policy's ability to resist entropy collapse is governed by the distribution itself rather than individual samples. Building on this insight, we propose Distribution-Centric Policy Optimization (DCPO), which reformulates entropy regulation as distribution-level regularization. DCPO achieves controllable entropy fully on-policy without sampling from external distributions, enabling efficient exploration while maintaining training stability. Across multiple models and seven benchmarks, DCPO improves over GRPO by about 20\\% on average. Overall, DCPO replaces sample-level heuristics with distribution-level principles, offering a theoretically grounded and flexible framework for controllable exploration and a stronger EE trade-off. The code is available in https://github.com/597358816/DCPO.", "AI": {"tldr": "\u63d0\u51faDCPO\u65b9\u6cd5\uff0c\u4ece\u5206\u5e03\u4e2d\u5fc3\u89c6\u89d2\u89e3\u51b3LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u63a7\u5236\u71b5\uff0c\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u534720%\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5982GRPO\u503e\u5411\u4e8e\u8fc7\u5ea6\u5229\u7528\uff0c\u5bfc\u81f4\u71b5\u5355\u8c03\u4e0b\u964d\u3001\u6837\u672c\u6536\u655b\u3001\u63a2\u7d22\u4e0d\u8db3\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u591a\u4e3a\u6837\u672c\u4e2d\u5fc3\u89c6\u89d2\uff0c\u4f9d\u8d56\u7a00\u6709\u6837\u672c\u7684\"\u8fd0\u6c14\"\uff0c\u7f3a\u4e4f\u5bf9\u7b56\u7565\u7684\u539f\u5219\u6027\u63a7\u5236\uff0c\u6548\u679c\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u4e2d\u5fc3\u7b56\u7565\u4f18\u5316(DCPO)\uff0c\u5c06\u71b5\u8c03\u8282\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5206\u5e03\u7ea7\u6b63\u5219\u5316\u95ee\u9898\u3002\u901a\u8fc7\u5f15\u5bfc\u7b56\u7565\u671d\u5411\"\u66f4\u597d\"\u7684\u76ee\u6807\u5206\u5e03\uff0c\u5b9e\u73b0\u5b8c\u5168\u5728\u7ebf\u3001\u65e0\u9700\u5916\u90e8\u5206\u5e03\u91c7\u6837\u7684\u53ef\u63a7\u71b5\u8c03\u8282\uff0c\u4ece\u800c\u4fc3\u8fdb\u9ad8\u6548\u63a2\u7d22\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDCPO\u76f8\u6bd4GRPO\u5e73\u5747\u63d0\u5347\u7ea620%\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684\u71b5\u8c03\u8282\uff0c\u4fdd\u6301\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fc3\u8fdb\u4e86\u9ad8\u6548\u63a2\u7d22\u3002", "conclusion": "DCPO\u7528\u5206\u5e03\u7ea7\u539f\u5219\u66ff\u4ee3\u6837\u672c\u7ea7\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u53ef\u63a7\u63a2\u7d22\u548c\u66f4\u5f3a\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86RL\u4e2d\u71b5\u5d29\u6e83\u7684\u6839\u672c\u95ee\u9898\u3002"}}
{"id": "2601.12745", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12745", "abs": "https://arxiv.org/abs/2601.12745", "authors": ["Miao Ye", "Jing Cui", "Yuan huang", "Qian He", "Yong Wang", "Jiwen Zhang"], "title": "A Graph Prompt Fine-Tuning Method for WSN Spatio-Temporal Correlation Anomaly Detection", "comment": null, "summary": "Anomaly detection of multi-temporal modal data in Wireless Sensor Network (WSN) can provide an important guarantee for reliable network operation. Existing anomaly detection methods in multi-temporal modal data scenarios have the problems of insufficient extraction of spatio-temporal correlation features, high cost of anomaly sample category annotation, and imbalance of anomaly samples. In this paper, a graph neural network anomaly detection backbone network incorporating spatio-temporal correlation features and a multi-task self-supervised training strategy of \"pre-training - graph prompting - fine-tuning\" are designed for the characteristics of WSN graph structure data. First, the anomaly detection backbone network is designed by improving the Mamba model based on a multi-scale strategy and inter-modal fusion method, and combining it with a variational graph convolution module, which is capable of fully extracting spatio-temporal correlation features in the multi-node, multi-temporal modal scenarios of WSNs. Secondly, we design a three-subtask learning \"pre-training\" method with no-negative comparative learning, prediction, and reconstruction to learn generic features of WSN data samples from unlabeled data, and design a \"graph prompting-fine-tuning\" mechanism to guide the pre-trained self-supervised learning. The model is fine-tuned through the \"graph prompting-fine-tuning\" mechanism to guide the pre-trained self-supervised learning model to complete the parameter fine-tuning, thereby reducing the training cost and enhancing the detection generalization performance. The F1 metrics obtained from experiments on the public dataset and the actual collected dataset are up to 91.30% and 92.31%, respectively, which provides better detection performance and generalization ability than existing methods designed by the method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u65f6\u7a7a\u76f8\u5173\u7279\u5f81\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u8bad\u7ec3\u7684WSN\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdbMamba\u6a21\u578b\u548c\u56fe\u63d0\u793a\u5fae\u8c03\u673a\u5236\uff0c\u5728\u516c\u5f00\u548c\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\u591a\u65f6\u5e8f\u6a21\u6001\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5b58\u5728\u7684\u65f6\u7a7a\u76f8\u5173\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u3001\u5f02\u5e38\u6837\u672c\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u6837\u672c\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1) \u8bbe\u8ba1\u57fa\u4e8e\u591a\u5c3a\u5ea6\u7b56\u7565\u548c\u6a21\u6001\u95f4\u878d\u5408\u65b9\u6cd5\u6539\u8fdbMamba\u6a21\u578b\u7684\u5f02\u5e38\u68c0\u6d4b\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u53d8\u5206\u56fe\u5377\u79ef\u6a21\u5757\u5145\u5206\u63d0\u53d6\u65f6\u7a7a\u76f8\u5173\u7279\u5f81\uff1b2) \u8bbe\u8ba1\u5305\u542b\u65e0\u8d1f\u5bf9\u6bd4\u5b66\u4e60\u3001\u9884\u6d4b\u548c\u91cd\u6784\u4e09\u4e2a\u5b50\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff1b3) \u63d0\u51fa\"\u56fe\u63d0\u793a-\u5fae\u8c03\"\u673a\u5236\u6307\u5bfc\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u5fae\u8c03\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u5b9e\u9645\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\u7684F1\u6307\u6807\u5206\u522b\u8fbe\u523091.30%\u548c92.31%\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3WSN\u591a\u65f6\u5e8f\u6a21\u6001\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u548c\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u68c0\u6d4b\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2601.12751", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12751", "abs": "https://arxiv.org/abs/2601.12751", "authors": ["Manjish Pal"], "title": "A Boolean Function-Theoretic Framework for Expressivity in GNNs with Applications to Fair Graph Mining", "comment": null, "summary": "We propose a novel expressivity framework for Graph Neural Networks (GNNs) grounded in Boolean function theory, enabling a fine-grained analysis of their ability to capture complex subpopulation structures. We introduce the notion of \\textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly subsumes existing expressivity measures such as Weisfeiler-Lehman (WL), biconnectivity-based, and homomorphism-based frameworks. Our theoretical results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence as key barriers to expressivity in fairness-aware GNNs. We design a circuit-traversal-based fairness algorithm capable of handling subpopulations defined by high-complexity Boolean functions, such as parity, which break existing baselines. Experiments on real-world graphs show that our method achieves low fairness gaps across intersectional groups where state-of-the-art methods fail, providing the first principled treatment of GNN expressivity tailored to fairness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5e03\u5c14\u51fd\u6570\u7406\u8bba\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u65b0\u6846\u67b6\uff0c\u5f15\u5165\u5b50\u7fa4\u4f53\u5e03\u5c14\u540c\u6784\u6982\u5ff5\uff0c\u8d85\u8d8a\u73b0\u6709\u8868\u8fbe\u80fd\u529b\u5ea6\u91cf\uff0c\u8bc6\u522b\u5085\u91cc\u53f6\u5ea6\u3001\u7535\u8def\u7c7b\u548c\u5f71\u54cd\u529b\u4e3a\u516c\u5e73\u611f\u77e5GNN\u7684\u5173\u952e\u8868\u8fbe\u969c\u788d\uff0c\u8bbe\u8ba1\u80fd\u5904\u7406\u590d\u6742\u5e03\u5c14\u51fd\u6570\u5b9a\u4e49\u5b50\u7fa4\u4f53\u7684\u516c\u5e73\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u5206\u6790\u6846\u67b6\uff08\u5982Weisfeiler-Lehman\u3001\u53cc\u8fde\u901a\u6027\u3001\u540c\u6001\u7b49\uff09\u65e0\u6cd5\u7cbe\u7ec6\u5206\u6790GNN\u6355\u6349\u590d\u6742\u5b50\u7fa4\u4f53\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u516c\u5e73\u6027\u573a\u666f\u4e2d\u5904\u7406\u7531\u590d\u6742\u5e03\u5c14\u51fd\u6570\u5b9a\u4e49\u7684\u4ea4\u53c9\u5b50\u7fa4\u4f53\u65f6\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e03\u5c14\u51fd\u6570\u7406\u8bba\u7684\u8868\u8fbe\u80fd\u529b\u6846\u67b6\uff0c\u5f15\u5165\u5b50\u7fa4\u4f53\u5e03\u5c14\u540c\u6784\u6982\u5ff5\uff1b\u8bc6\u522b\u5085\u91cc\u53f6\u5ea6\u3001\u7535\u8def\u7c7b\uff08AC\u2070\u3001NC\u00b9\uff09\u548c\u5f71\u54cd\u529b\u4f5c\u4e3a\u8868\u8fbe\u80fd\u529b\u5173\u952e\u969c\u788d\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u7535\u8def\u904d\u5386\u7684\u516c\u5e73\u7b97\u6cd5\uff0c\u80fd\u5904\u7406\u5982\u5947\u5076\u6027\u7b49\u9ad8\u590d\u6742\u5ea6\u5e03\u5c14\u51fd\u6570\u5b9a\u4e49\u7684\u5b50\u7fa4\u4f53\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5b50\u7fa4\u4f53\u5e03\u5c14\u540c\u6784\u4e25\u683c\u5305\u542b\u73b0\u6709\u8868\u8fbe\u80fd\u529b\u5ea6\u91cf\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u73b0\u6709\u65b9\u6cd5\u5931\u8d25\u7684\u4ea4\u53c9\u5b50\u7fa4\u4f53\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u4e3aGNN\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u9996\u4e2a\u539f\u5219\u6027\u8868\u8fbe\u80fd\u529b\u5904\u7406\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5e03\u5c14\u51fd\u6570\u7406\u8bba\u7684\u539f\u5219\u6027\u8868\u8fbe\u80fd\u529b\u5206\u6790\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u8868\u8fbe\u80fd\u529b\u5ea6\u91cf\uff0c\u8bbe\u8ba1\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u5e03\u5c14\u51fd\u6570\u5b9a\u4e49\u7684\u5b50\u7fa4\u4f53\uff0c\u5728\u4ea4\u53c9\u5b50\u7fa4\u4f53\u516c\u5e73\u6027\u95ee\u9898\u4e0a\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2601.12775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12775", "abs": "https://arxiv.org/abs/2601.12775", "authors": ["Yuta Hirabayashi", "Daisuke Matusoka", "Konobu Kimura"], "title": "Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural Networks", "comment": null, "summary": "Research on data-driven ocean models has progressed rapidly in recent years; however, the application of these models to global eddy-resolving ocean forecasting remains limited. The accurate representation of ocean dynamics across a wide range of spatial scales remains a major challenge in such applications. This study proposes a multi-scale graph neural network-based ocean model for 10-day global forecasting that improves short-term prediction skill and enhances the representation of multi-scale ocean variability. The model employs an encoder-processor-decoder architecture and uses two spherical meshes with different resolutions to better capture the multi-scale nature of ocean dynamics. In addition, the model incorporates surface atmospheric variables along with ocean state variables as node inputs to improve short-term prediction accuracy by representing atmospheric forcing. Evaluation using surface kinetic energy spectra and case studies shows that the model accurately represents a broad range of spatial scales, while root mean square error comparisons demonstrate improved skill in short-term predictions. These results indicate that the proposed model delivers more accurate short-term forecasts and improved representation of multi-scale ocean dynamics, thereby highlighting its potential to advance data-driven, eddy-resolving global ocean forecasting.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u5c3a\u5ea6\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5168\u7403\u6d77\u6d0b\u9884\u62a5\u6a21\u578b\uff0c\u7528\u4e8e10\u5929\u9884\u62a5\uff0c\u901a\u8fc7\u53cc\u5206\u8fa8\u7387\u7403\u9762\u7f51\u683c\u548c\u5927\u6c14\u53d8\u91cf\u8f93\u5165\uff0c\u63d0\u5347\u77ed\u671f\u9884\u62a5\u7cbe\u5ea6\u548c\u591a\u5c3a\u5ea6\u6d77\u6d0b\u53d8\u7387\u8868\u5f81\u80fd\u529b\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u6d77\u6d0b\u6a21\u578b\u7814\u7a76\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5e94\u7528\u4e8e\u5168\u7403\u6da1\u65cb\u5206\u8fa8\u7387\u6d77\u6d0b\u9884\u62a5\u4ecd\u6709\u9650\u5236\u3002\u51c6\u786e\u8868\u5f81\u8de8\u5e7f\u6cdb\u7a7a\u95f4\u5c3a\u5ea6\u7684\u6d77\u6d0b\u52a8\u529b\u5b66\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u5904\u7406\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u7403\u9762\u7f51\u683c\u6355\u83b7\u591a\u5c3a\u5ea6\u6d77\u6d0b\u52a8\u529b\u5b66\u3002\u5c06\u8868\u9762\u5927\u6c14\u53d8\u91cf\u4e0e\u6d77\u6d0b\u72b6\u6001\u53d8\u91cf\u4f5c\u4e3a\u8282\u70b9\u8f93\u5165\uff0c\u4ee5\u8868\u5f81\u5927\u6c14\u5f3a\u8feb\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u8868\u9762\u52a8\u80fd\u8c31\u548c\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u8868\u5f81\u5e7f\u6cdb\u7a7a\u95f4\u5c3a\u5ea6\u3002\u5747\u65b9\u6839\u8bef\u5dee\u6bd4\u8f83\u663e\u793a\u77ed\u671f\u9884\u62a5\u6280\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6a21\u578b\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u77ed\u671f\u9884\u62a5\u548c\u6539\u5584\u7684\u591a\u5c3a\u5ea6\u6d77\u6d0b\u52a8\u529b\u5b66\u8868\u5f81\uff0c\u7a81\u663e\u5176\u5728\u63a8\u8fdb\u6570\u636e\u9a71\u52a8\u3001\u6da1\u65cb\u5206\u8fa8\u7387\u5168\u7403\u6d77\u6d0b\u9884\u62a5\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.12785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12785", "abs": "https://arxiv.org/abs/2601.12785", "authors": ["Yuqi Li", "Kuiye Ding", "Chuanguang Yang", "Szu-Yu Chen", "Yingli Tian"], "title": "Distilling Time Series Foundation Models for Efficient Forecasting", "comment": "Accepted by ICASSP-2026", "summary": "Time Series foundation models (TSFMs) deliver strong forecasting performance through large-scale pretraining, but their large parameter sizes make deployment costly. While knowledge distillation offers a natural and effective approach for model compression, techniques developed for general machine learning tasks are not directly applicable to time series forecasting due to the unique characteristics. To address this, we present DistilTS, the first distillation framework specifically designed for TSFMs. DistilTS addresses two key challenges: (1) task difficulty discrepancy, specific to forecasting, where uniform weighting makes optimization dominated by easier short-term horizons, while long-term horizons receive weaker supervision; and (2) architecture discrepancy, a general challenge in distillation, for which we design an alignment mechanism in the time series forecasting. To overcome these issues, DistilTS introduces horizon-weighted objectives to balance learning across horizons, and a temporal alignment strategy that reduces architectural mismatch, enabling compact models. Experiments on multiple benchmarks demonstrate that DistilTS achieves forecasting performance comparable to full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating inference by up to 6000x. Code is available at: https://github.com/itsnotacie/DistilTS-ICASSP2026.", "AI": {"tldr": "DistilTS\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u7684\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6c34\u5e73\u52a0\u6743\u76ee\u6807\u548c\u65f6\u95f4\u5bf9\u9f50\u7b56\u7565\u89e3\u51b3\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\u548c\u67b6\u6784\u5dee\u5f02\u95ee\u9898\uff0c\u5b9e\u73b0\u53c2\u6570\u51cf\u5c11150\u500d\u3001\u63a8\u7406\u52a0\u901f6000\u500d\u7684\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u867d\u7136\u9884\u6d4b\u6027\u80fd\u5f3a\uff0c\u4f46\u53c2\u6570\u91cf\u5927\u5bfc\u81f4\u90e8\u7f72\u6210\u672c\u9ad8\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u4e0d\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u7279\u6b8a\u6027\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u9488\u5bf9TSFMs\u7684\u84b8\u998f\u6846\u67b6\u3002", "method": "\u63d0\u51faDistilTS\u6846\u67b6\uff1a1\uff09\u9488\u5bf9\u9884\u6d4b\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\uff0c\u5f15\u5165\u6c34\u5e73\u52a0\u6743\u76ee\u6807\u5e73\u8861\u77ed\u671f\u548c\u957f\u671f\u9884\u6d4b\u7684\u5b66\u4e60\uff1b2\uff09\u9488\u5bf9\u67b6\u6784\u5dee\u5f02\uff0c\u8bbe\u8ba1\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5bf9\u9f50\u673a\u5236\uff0c\u51cf\u5c11\u67b6\u6784\u4e0d\u5339\u914d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDistilTS\u5b9e\u73b0\u4e86\u4e0e\u5b8c\u6574\u5927\u5c0fTSFMs\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u51cf\u5c11\u9ad8\u8fbe1/150\uff0c\u63a8\u7406\u52a0\u901f\u9ad8\u8fbe6000\u500d\u3002", "conclusion": "DistilTS\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u8bbe\u8ba1\u7684\u6709\u6548\u84b8\u998f\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4efb\u52a1\u96be\u5ea6\u5dee\u5f02\u548c\u67b6\u6784\u5dee\u5f02\u4e24\u5927\u6311\u6218\uff0c\u4e3aTSFMs\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12807", "abs": "https://arxiv.org/abs/2601.12807", "authors": ["Zixing Song", "Irwin King"], "title": "Semi-supervised Instruction Tuning for Large Language Models on Text-Attributed Graphs", "comment": null, "summary": "The emergent reasoning capabilities of Large Language Models (LLMs) offer a transformative paradigm for analyzing text-attributed graphs. While instruction tuning is the prevailing method for adapting pre-trained LLMs to graph learning tasks like node classification, it requires a substantial volume of annotated (INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is particularly prohibitive in the social domain, where obtaining expert labels for sensitive or evolving content is costly and slow. Furthermore, standard graph instruction tuning fails to exploit the vast amount of unlabeled nodes, which contain latent correlations due to edge connections that are beneficial for downstream predictions. To bridge this gap, we propose a novel Semi-supervised Instruction Tuning pipeline for Graph Learning, named SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly integrated into any graph instruction tuning method that utilizes LLMs as the predictor. SIT-Graph operates via an iterative self-training process. Initially, the model is fine-tuned using instruction pairs constructed solely from the labeled nodes. Then it generates confidence-filtered pseudo-responses for unlabeled nodes to strategically augment the dataset for the next round of fine-tuning. Finally, this iterative refinement progressively aligns the LLM with the underlying node correlations. Extensive experiments demonstrate that when incorporated into state-of-the-art graph instruction tuning methods, SIT-Graph significantly enhances their performance on text-attributed graph benchmarks, achieving over 20% improvement under the low label ratio settings.", "AI": {"tldr": "SIT-Graph\uff1a\u4e00\u79cd\u7528\u4e8e\u56fe\u5b66\u4e60\u7684\u534a\u76d1\u7763\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8bad\u7ec3\u5229\u7528\u672a\u6807\u8bb0\u8282\u70b9\u63d0\u5347LLM\u5728\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u56fe\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u8282\u70b9\u6570\u636e\uff0c\u8fd9\u5728\u793e\u4ea4\u9886\u57df\u7b49\u654f\u611f\u6216\u5feb\u901f\u6f14\u5316\u7684\u573a\u666f\u4e2d\u6210\u672c\u9ad8\u6602\u4e14\u7f13\u6162\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u672a\u6807\u8bb0\u8282\u70b9\u4e2d\u8574\u542b\u7684\u8fb9\u8fde\u63a5\u6f5c\u5728\u76f8\u5173\u6027", "method": "\u63d0\u51fa\u6a21\u578b\u65e0\u5173\u7684SIT-Graph\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u8bad\u7ec3\u8fc7\u7a0b\uff1a1\uff09\u5148\u7528\u6807\u8bb0\u8282\u70b9\u6307\u4ee4\u5bf9\u5fae\u8c03\u6a21\u578b\uff1b2\uff09\u4e3a\u672a\u6807\u8bb0\u8282\u70b9\u751f\u6210\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u7684\u4f2a\u54cd\u5e94\uff1b3\uff09\u7b56\u7565\u6027\u5730\u6269\u5145\u6570\u636e\u96c6\u8fdb\u884c\u4e0b\u4e00\u8f6e\u5fae\u8c03\uff1b4\uff09\u8fed\u4ee3\u4f18\u5316\u4f7fLLM\u4e0e\u5e95\u5c42\u8282\u70b9\u76f8\u5173\u6027\u5bf9\u9f50", "result": "\u5c06SIT-Graph\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u56fe\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4e2d\uff0c\u5728\u6587\u672c\u5c5e\u6027\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728\u4f4e\u6807\u7b7e\u7387\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u8d85\u8fc720%\u7684\u6539\u8fdb", "conclusion": "SIT-Graph\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6307\u4ee4\u8c03\u4f18\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u672a\u6807\u8bb0\u8282\u70b9\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u56fe\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u534a\u76d1\u7763\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2601.12816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12816", "abs": "https://arxiv.org/abs/2601.12816", "authors": ["Ishir Garg", "Neel Kolhe", "Andy Peng", "Rohan Gopalam"], "title": "Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning", "comment": null, "summary": "Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. The resulting update direction is invariant under reparameterization, guarantees descent in the Fisher metric, and helps preserve prior task outputs. We provide theoretical analysis establishing the properties of the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.", "AI": {"tldr": "\u63d0\u51faFOPNG\u4f18\u5316\u5668\uff0c\u901a\u8fc7Fisher\u6b63\u4ea4\u6295\u5f71\u7ea6\u675f\u53c2\u6570\u66f4\u65b0\uff0c\u5728\u8fde\u7eed\u5b66\u4e60\u4e2d\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\uff0c\u7edf\u4e00\u4e86\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u548c\u6b63\u4ea4\u68af\u5ea6\u65b9\u6cd5\u3002", "motivation": "\u8fde\u7eed\u5b66\u4e60\u9700\u8981\u795e\u7ecf\u7f51\u7edc\u5728\u987a\u5e8f\u4efb\u52a1\u4e2d\u83b7\u53d6\u65b0\u77e5\u8bc6\uff0c\u4f46\u5173\u952e\u6311\u6218\u662f\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u4e0d\u9057\u5fd8\u65e7\u4efb\u52a1\uff08\u707e\u96be\u6027\u9057\u5fd8\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6b27\u51e0\u91cc\u5f97\u53c2\u6570\u7a7a\u95f4\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u7684\u7edf\u4e00\u3002", "method": "\u63d0\u51faFisher\u6b63\u4ea4\u6295\u5f71\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\uff08FOPNG\uff09\u4f18\u5316\u5668\uff0c\u5c06\u68af\u5ea6\u6295\u5f71\u5230\u5148\u524d\u4efb\u52a1\u68af\u5ea6\u7684Fisher\u6b63\u4ea4\u8865\u7a7a\u95f4\u3002\u8be5\u65b9\u6cd5\u5728\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u4e0b\u7edf\u4e00\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u548c\u6b63\u4ea4\u68af\u5ea6\u65b9\u6cd5\uff0c\u66f4\u65b0\u65b9\u5411\u5bf9\u91cd\u53c2\u6570\u5316\u4e0d\u53d8\uff0c\u4fdd\u8bc1\u5728Fisher\u5ea6\u91cf\u4e0b\u4e0b\u964d\u3002", "result": "\u5728\u6807\u51c6\u8fde\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\uff08Permuted-MNIST\u3001Split-MNIST\u3001Rotated-MNIST\u3001Split-CIFAR10\u3001Split-CIFAR100\uff09\u4e0a\u5c55\u793a\u4e86\u5f3a\u52b2\u7ed3\u679c\uff0c\u8bc1\u660e\u80fd\u6709\u6548\u4fdd\u7559\u65e7\u4efb\u52a1\u6027\u80fd\u540c\u65f6\u5b66\u4e60\u65b0\u4efb\u52a1\u3002", "conclusion": "FOPNG\u901a\u8fc7Fisher\u6b63\u4ea4\u7ea6\u675f\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u4e14\u5b9e\u7528\u7684\u8fde\u7eed\u5b66\u4e60\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4fe1\u606f\u51e0\u4f55\u6846\u67b6\u4e0b\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002"}}
{"id": "2601.12839", "categories": ["cs.LG", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2601.12839", "abs": "https://arxiv.org/abs/2601.12839", "authors": ["Gyuyeon Na", "Minjung Park", "Soyoun Kim", "Jungbin Shin", "Sangmi Chai"], "title": "Knowledge-Integrated Representation Learning for Crypto Anomaly Detection under Extreme Label Scarcity; Relational Domain-Logic Integration with Retrieval-Grounded Context and Path-Level Explanations", "comment": "Gyuyeon Na, Minjung Park, Soyoun Kim contributed equally to this work", "summary": "Detecting anomalous trajectories in decentralized crypto networks is fundamentally challenged by extreme label scarcity and the adaptive evasion strategies of illicit actors. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they struggle to internalize multi hop, logic driven motifs such as fund dispersal and layering that characterize sophisticated money laundering, limiting their forensic accountability under regulations like the FATF Travel Rule. To address this limitation, we propose Relational Domain Logic Integration (RDLI), a framework that embeds expert derived heuristics as differentiable, logic aware latent signals within representation learning. Unlike static rule based approaches, RDLI enables the detection of complex transactional flows that evade standard message passing. To further account for market volatility, we incorporate a Retrieval Grounded Context (RGC) module that conditions anomaly scoring on regulatory and macroeconomic context, mitigating false positives caused by benign regime shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art GNN baselines by 28.9% in F1 score. A micro expert user study further confirms that RDLI path level explanations significantly improve trustworthiness, perceived usefulness, and clarity compared to existing methods, highlighting the importance of integrating domain logic with contextual grounding for both accuracy and explainability.", "AI": {"tldr": "RDLI\u6846\u67b6\u901a\u8fc7\u5c06\u4e13\u5bb6\u542f\u53d1\u5f0f\u903b\u8f91\u5d4c\u5165\u8868\u793a\u5b66\u4e60\uff0c\u7ed3\u5408\u68c0\u7d22\u5f0f\u4e0a\u4e0b\u6587\u6a21\u5757\uff0c\u5728\u6807\u7b7e\u6781\u5ea6\u7a00\u7f3a\u7684\u52a0\u5bc6\u8d27\u5e01\u7f51\u7edc\u4e2d\u663e\u8457\u63d0\u5347\u5f02\u5e38\u4ea4\u6613\u68c0\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u52a0\u5bc6\u8d27\u5e01\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u8f68\u8ff9\u68c0\u6d4b\u9762\u4e34\u6807\u7b7e\u6781\u5ea6\u7a00\u7f3a\uff080.01%\uff09\u548c\u6076\u610f\u884c\u4e3a\u8005\u81ea\u9002\u5e94\u89c4\u907f\u7b56\u7565\u7684\u6311\u6218\u3002\u4f20\u7edfGNN\u867d\u7136\u80fd\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u6a21\u5f0f\uff0c\u4f46\u96be\u4ee5\u5185\u5316\u8d44\u91d1\u5206\u6563\u3001\u5206\u5c42\u7b49\u591a\u8df3\u903b\u8f91\u9a71\u52a8\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5728FATF\u65c5\u884c\u89c4\u5219\u7b49\u76d1\u7ba1\u8981\u6c42\u4e0b\u7684\u6cd5\u8bc1\u53ef\u8ffd\u6eaf\u6027\u3002", "method": "\u63d0\u51fa\u5173\u7cfb\u57df\u903b\u8f91\u96c6\u6210\uff08RDLI\uff09\u6846\u67b6\uff0c\u5c06\u4e13\u5bb6\u63a8\u5bfc\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u4f5c\u4e3a\u53ef\u5fae\u5206\u7684\u903b\u8f91\u611f\u77e5\u6f5c\u5728\u4fe1\u53f7\u5d4c\u5165\u8868\u793a\u5b66\u4e60\u3002\u4e0d\u540c\u4e8e\u9759\u6001\u89c4\u5219\u65b9\u6cd5\uff0cRDLI\u80fd\u68c0\u6d4b\u89c4\u907f\u6807\u51c6\u6d88\u606f\u4f20\u9012\u7684\u590d\u6742\u4ea4\u6613\u6d41\u3002\u6b64\u5916\uff0c\u5f15\u5165\u68c0\u7d22\u5f0f\u4e0a\u4e0b\u6587\uff08RGC\uff09\u6a21\u5757\uff0c\u5c06\u5f02\u5e38\u8bc4\u5206\u57fa\u4e8e\u76d1\u7ba1\u548c\u5b8f\u89c2\u7ecf\u6d4e\u80cc\u666f\uff0c\u51cf\u8f7b\u826f\u6027\u5236\u5ea6\u53d8\u5316\u5bfc\u81f4\u7684\u8bef\u62a5\u3002", "result": "\u5728\u6781\u7aef\u6807\u7b7e\u7a00\u7f3a\uff080.01%\uff09\u6761\u4ef6\u4e0b\uff0cRDLI\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684GNN\u57fa\u7ebf28.9%\u3002\u5fae\u89c2\u4e13\u5bb6\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0cRDLI\u7684\u8def\u5f84\u7ea7\u89e3\u91ca\u5728\u53ef\u4fe1\u5ea6\u3001\u611f\u77e5\u6709\u7528\u6027\u548c\u6e05\u6670\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RDLI\u6846\u67b6\u901a\u8fc7\u5c06\u9886\u57df\u903b\u8f91\u4e0e\u4e0a\u4e0b\u6587\u57fa\u7840\u76f8\u7ed3\u5408\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u52a0\u5bc6\u8d27\u5e01\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.12859", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2601.12859", "abs": "https://arxiv.org/abs/2601.12859", "authors": ["Luca Schaufelberger", "Aline Hartgers", "Kjell Jorner"], "title": "Generating Cyclic Conformers with Flow Matching in Cremer-Pople Coordinates", "comment": null, "summary": "Cyclic molecules are ubiquitous across applications in chemistry and biology. Their restricted conformational flexibility provides structural pre-organization that is key to their function in drug discovery and catalysis. However, reliably sampling the conformer ensembles of ring systems remains challenging. Here, we introduce PuckerFlow, a generative machine learning model that performs flow matching on the Cremer-Pople space, a low-dimensional internal coordinate system capturing the relevant degrees of freedom of rings. Our approach enables generation of valid closed rings by design and demonstrates strong performance in generating conformers that are both diverse and precise. We show that PuckerFlow outperforms other conformer generation methods on nearly all quantitative metrics and illustrate the potential of PuckerFlow for ring systems relevant to chemical applications, particularly in catalysis and drug discovery. This work enables efficient and reliable conformer generation of cyclic structures, paving the way towards modeling structure-property relationships and the property-guided generation of rings across a wide range of applications in chemistry and biology.", "AI": {"tldr": "PuckerFlow\u662f\u4e00\u79cd\u7528\u4e8e\u73af\u72b6\u5206\u5b50\u6784\u8c61\u751f\u6210\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7Cremer-Pople\u7a7a\u95f4\u8fdb\u884c\u6d41\u5339\u914d\uff0c\u80fd\u591f\u9ad8\u6548\u53ef\u9760\u5730\u751f\u6210\u73af\u72b6\u7ed3\u6784\u7684\u6784\u8c61\u3002", "motivation": "\u73af\u72b6\u5206\u5b50\u5728\u5316\u5b66\u548c\u751f\u7269\u5b66\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5176\u53d7\u9650\u7684\u6784\u8c61\u7075\u6d3b\u6027\u5bf9\u836f\u7269\u53d1\u73b0\u548c\u50ac\u5316\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u53ef\u9760\u5730\u91c7\u6837\u73af\u7cfb\u7edf\u7684\u6784\u8c61\u96c6\u5408\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165PuckerFlow\u751f\u6210\u6a21\u578b\uff0c\u5728Cremer-Pople\u7a7a\u95f4\uff08\u6355\u83b7\u73af\u76f8\u5173\u81ea\u7531\u5ea6\u7684\u4f4e\u7ef4\u5185\u90e8\u5750\u6807\u7cfb\uff09\u4e0a\u6267\u884c\u6d41\u5339\u914d\uff0c\u80fd\u591f\u8bbe\u8ba1\u751f\u6210\u6709\u6548\u7684\u95ed\u5408\u73af\u3002", "result": "PuckerFlow\u5728\u51e0\u4e4e\u6240\u6709\u5b9a\u91cf\u6307\u6807\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u6784\u8c61\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u65e2\u591a\u6837\u53c8\u7cbe\u786e\u7684\u6784\u8c61\uff0c\u7279\u522b\u9002\u7528\u4e8e\u50ac\u5316\u548c\u836f\u7269\u53d1\u73b0\u76f8\u5173\u7684\u73af\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u73af\u72b6\u7ed3\u6784\u7684\u9ad8\u6548\u53ef\u9760\u6784\u8c61\u751f\u6210\uff0c\u4e3a\u5efa\u6a21\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u548c\u8de8\u5316\u5b66\u4e0e\u751f\u7269\u5b66\u5e94\u7528\u7684\u5c5e\u6027\u5f15\u5bfc\u73af\u751f\u6210\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2601.12879", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.12879", "abs": "https://arxiv.org/abs/2601.12879", "authors": ["Mohammed Mudassir Uddin", "Shahnawaz Alam", "Mohammed Kaif Pasha"], "title": "Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition", "comment": null, "summary": "Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks. On modular arithmetic tasks, the framework achieves up to 91% behavioral preservation ($\\pm$2.3\\% across runs) while maintaining interpretable subgraph sizes. Cross-architecture transfer experiments suggest that discovered circuits exhibit moderate structural similarity (averaging 67%) across model families, indicating potential shared computational patterns. These results provide preliminary foundations for interpretability at larger model scales while identifying significant limitations in current attribution methodologies that require future advances.", "AI": {"tldr": "HAGD\u6846\u67b6\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u62bd\u8c61\u5c42\u6b21\u548c\u53ef\u5fae\u5206\u7535\u8def\u641c\u7d22\uff0c\u5c06\u7535\u8def\u53d1\u73b0\u590d\u6742\u5ea6\u4eceO(2^n)\u964d\u81f3O(n\u00b2 log n)\uff0c\u5728GPT-2\u3001Llama\u548cPythia\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u4efb\u52a1\u548c\u81ea\u7136\u8bed\u8a00\u57fa\u51c6\u7684\u6027\u80fd\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u65e8\u5728\u5c06\u795e\u7ecf\u7f51\u7edc\u8ba1\u7b97\u53cd\u5de5\u7a0b\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7b97\u6cd5\uff0c\u4f46\u4ece\u6570\u5341\u4ebf\u53c2\u6570\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u7a00\u758f\u8ba1\u7b97\u7535\u8def\u9762\u4e34\u6307\u6570\u641c\u7d22\u590d\u6742\u6027\u548c\u666e\u904d\u591a\u4e49\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u5f52\u56e0\u56fe\u5206\u89e3(HAGD)\u6846\u67b6\uff0c\u6574\u5408\u8de8\u5c42\u8f6c\u7801\u5668\u8fdb\u884c\u5355\u4e49\u7279\u5f81\u63d0\u53d6\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u5143\u5b66\u4e60\u8fdb\u884c\u62d3\u6251\u9884\u6d4b\u3001\u56e0\u679c\u5e72\u9884\u534f\u8bae\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u6a21\u8fd0\u7b97\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe91%\u7684\u884c\u4e3a\u4fdd\u6301(\u00b12.3%)\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u5b50\u56fe\u5927\u5c0f\uff1b\u8de8\u67b6\u6784\u8fc1\u79fb\u5b9e\u9a8c\u663e\u793a\u53d1\u73b0\u7535\u8def\u5177\u670967%\u7684\u5e73\u5747\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "conclusion": "\u4e3a\u66f4\u5927\u6a21\u578b\u89c4\u6a21\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u521d\u6b65\u57fa\u7840\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u5f53\u524d\u5f52\u56e0\u65b9\u6cd5\u7684\u91cd\u8981\u5c40\u9650\u6027\uff0c\u9700\u8981\u672a\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2601.12893", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.12893", "abs": "https://arxiv.org/abs/2601.12893", "authors": ["Ting Dang", "Soumyajit Chatterjee", "Hong Jia", "Yu Wu", "Flora Salim", "Fahim Kawsar"], "title": "AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural ODEs", "comment": "Accepted by ICASSP 2026", "summary": "Test time adaptation (TTA) has emerged as a promising solution to adapt pre-trained models to new, unseen data distributions using unlabeled target domain data. However, most TTA methods are designed for independent data, often overlooking the time series data and rarely addressing forecasting tasks. This paper presents AdaNODEs, an innovative source-free TTA method tailored explicitly for time series forecasting. By leveraging Neural Ordinary Differential Equations (NODEs), we propose a novel adaptation framework that accommodates the unique characteristics of distribution shifts in time series data. Moreover, we innovatively propose a new loss function to tackle TTA for forecasting tasks. AdaNODEs only requires updating limited model parameters, showing effectiveness in capturing temporal dependencies while avoiding significant memory usage. Extensive experiments with one- and high-dimensional data demonstrate that AdaNODEs offer relative improvements of 5.88\\% and 28.4\\% over the SOTA baselines, especially demonstrating robustness across higher severity distribution shifts.", "AI": {"tldr": "AdaNODEs\u662f\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6e90\u81ea\u7531\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u5904\u7406\u5206\u5e03\u504f\u79fb\uff0c\u4ec5\u9700\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u72ec\u7acb\u6570\u636e\uff0c\u5ffd\u89c6\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7279\u6027\u548c\u9884\u6d4b\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u7684\u6e90\u81ea\u7531\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570\u6765\u5904\u7406\u9884\u6d4b\u4efb\u52a1\uff0c\u4ec5\u66f4\u65b0\u6709\u9650\u6a21\u578b\u53c2\u6570\u4ee5\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "result": "\u5728\u4e00\u7ef4\u548c\u9ad8\u7ef4\u6570\u636e\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5206\u522b\u53d6\u5f975.88%\u548c28.4%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0c\u5728\u66f4\u4e25\u91cd\u7684\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AdaNODEs\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5206\u5e03\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2601.12900", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12900", "abs": "https://arxiv.org/abs/2601.12900", "authors": ["Eliran Sherzer", "Yonit Barron"], "title": "Supervised Learning for the (s,S) Inventory Model with General Interarrival Demands and General Lead Times", "comment": null, "summary": "The continuous-review (s,S) inventory model is a cornerstone of stochastic inventory theory, yet its analysis becomes analytically intractable when dealing with non-Markovian systems. In such systems, evaluating long-run performance measures typically relies on costly simulation.\n  This paper proposes a supervised learning framework via a neural network model for approximating stationary performance measures of (s,S) inventory systems with general distributions for the interarrival time between demands and lead times under lost sales. Simulations are first used to generate training labels, after which the neural network is trained. After training, the neural network provides almost instantaneous predictions of various metrics of the system, such as the stationary distribution of inventory levels, the expected cycle time, and the probability of lost sales. We find that using a small number of low-order moments of the distributions as input is sufficient to train the neural networks and to accurately capture the steady-state distribution. Extensive numerical experiments demonstrate high accuracy over a wide range of system parameters. As such, it effectively replaces repeated and costly simulation runs. Our framework is easily extendable to other inventory models, offering an efficient and fast alternative for analyzing complex stochastic systems.", "AI": {"tldr": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u8fd1\u4f3c(s,S)\u5e93\u5b58\u7cfb\u7edf\u7684\u7a33\u6001\u6027\u80fd\u6307\u6807\uff0c\u66ff\u4ee3\u6602\u8d35\u7684\u4eff\u771f\u8ba1\u7b97", "motivation": "\u4f20\u7edf(s,S)\u5e93\u5b58\u6a21\u578b\u5728\u975e\u9a6c\u5c14\u53ef\u592b\u7cfb\u7edf\u4e2d\u5206\u6790\u56f0\u96be\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u4eff\u771f\u8ba1\u7b97\u6765\u8bc4\u4f30\u957f\u671f\u6027\u80fd\u6307\u6807", "method": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff1a\u5148\u7528\u4eff\u771f\u751f\u6210\u8bad\u7ec3\u6807\u7b7e\uff0c\u7136\u540e\u7528\u5c11\u91cf\u4f4e\u9636\u77e9\u4f5c\u4e3a\u8f93\u5165\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u9884\u6d4b\u5e93\u5b58\u6c34\u5e73\u7a33\u6001\u5206\u5e03\u3001\u671f\u671b\u5468\u671f\u65f6\u95f4\u3001\u7f3a\u8d27\u6982\u7387\u7b49\u6307\u6807", "result": "\u795e\u7ecf\u7f51\u7edc\u80fd\u5feb\u901f\u51c6\u786e\u9884\u6d4b\u5404\u79cd\u7cfb\u7edf\u6307\u6807\uff0c\u5728\u5e7f\u6cdb\u53c2\u6570\u8303\u56f4\u5185\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u6709\u6548\u66ff\u4ee3\u91cd\u590d\u6602\u8d35\u7684\u4eff\u771f\u8fd0\u884c", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u6790\u590d\u6742\u968f\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u5feb\u901f\u66ff\u4ee3\u65b9\u6848\uff0c\u6613\u4e8e\u6269\u5c55\u5230\u5176\u4ed6\u5e93\u5b58\u6a21\u578b"}}
{"id": "2601.12903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12903", "abs": "https://arxiv.org/abs/2601.12903", "authors": ["Meng Liu", "Ke Liang", "Siwei Wang", "Xingchen Hu", "Sihang Zhou", "Xinwang Liu"], "title": "Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets", "comment": null, "summary": "Temporal Graph Clustering (TGC) is a new task with little attention, focusing on node clustering in temporal graphs. Compared with existing static graph clustering, it can find the balance between time requirement and space requirement (Time-Space Balance) through the interaction sequence-based batch-processing pattern. However, there are two major challenges that hinder the development of TGC, i.e., inapplicable clustering techniques and inapplicable datasets. To address these challenges, we propose a comprehensive benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to illustrate the paradigm of temporal graph clustering and improve existing clustering techniques to fit temporal graphs. In addition, we also discuss problems with public temporal graph datasets and develop multiple datasets suitable for TGC task, called BenchTGC Datasets. According to extensive experiments, we not only verify the advantages of BenchTGC, but also demonstrate the necessity and importance of TGC task. We wish to point out that the dynamically changing and complex scenarios in real world are the foundation of temporal graph clustering. The code and data is available at: https://github.com/MGitHubL/BenchTGC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBenchTGC\u7684\u65f6\u5e8f\u56fe\u805a\u7c7b\uff08TGC\uff09\u7efc\u5408\u57fa\u51c6\uff0c\u5305\u62ec\u6846\u67b6\u548c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u805a\u7c7b\u6280\u672f\u4e0d\u9002\u7528\u548c\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u95ee\u9898\u3002", "motivation": "\u65f6\u5e8f\u56fe\u805a\u7c7b\u662f\u4e00\u4e2a\u65b0\u5174\u4f46\u5173\u6ce8\u5ea6\u4f4e\u7684\u4efb\u52a1\uff0c\u76f8\u6bd4\u9759\u6001\u56fe\u805a\u7c7b\uff0c\u5b83\u901a\u8fc7\u57fa\u4e8e\u4ea4\u4e92\u5e8f\u5217\u7684\u6279\u5904\u7406\u6a21\u5f0f\u80fd\u5728\u65f6\u95f4\u9700\u6c42\u548c\u7a7a\u95f4\u9700\u6c42\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002\u7136\u800c\uff0c\u73b0\u6709\u805a\u7c7b\u6280\u672f\u4e0d\u9002\u7528\u548c\u7f3a\u4e4f\u5408\u9002\u6570\u636e\u96c6\u4e24\u5927\u6311\u6218\u963b\u788d\u4e86TGC\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86BenchTGC\u57fa\u51c6\uff0c\u5305\u62ec\uff1a1\uff09\u8bbe\u8ba1BenchTGC\u6846\u67b6\u6765\u8bf4\u660e\u65f6\u5e8f\u56fe\u805a\u7c7b\u7684\u8303\u5f0f\uff1b2\uff09\u6539\u8fdb\u73b0\u6709\u805a\u7c7b\u6280\u672f\u4ee5\u9002\u5e94\u65f6\u5e8f\u56fe\uff1b3\uff09\u5f00\u53d1\u4e86\u591a\u4e2a\u9002\u5408TGC\u4efb\u52a1\u7684BenchTGC\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BenchTGC\u7684\u4f18\u52bf\uff0c\u5e76\u8bc1\u660e\u4e86\u65f6\u5e8f\u56fe\u805a\u7c7b\u4efb\u52a1\u7684\u5fc5\u8981\u6027\u548c\u91cd\u8981\u6027\u3002\u73b0\u5b9e\u4e16\u754c\u4e2d\u52a8\u6001\u53d8\u5316\u548c\u590d\u6742\u573a\u666f\u662f\u65f6\u5e8f\u56fe\u805a\u7c7b\u7684\u57fa\u7840\u3002", "conclusion": "BenchTGC\u4e3a\u89e3\u51b3\u65f6\u5e8f\u56fe\u805a\u7c7b\u9762\u4e34\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u6846\u67b6\u3001\u6539\u8fdb\u7b97\u6cd5\u548c\u4e13\u7528\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.12917", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.12917", "abs": "https://arxiv.org/abs/2601.12917", "authors": ["He Sun", "Jinrui Zhou", "Li Li", "Mingjun Xiao"], "title": "CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via ZOO-based Gradient Correction", "comment": "14 pages, 9 figures, under review", "summary": "Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning them on resource-constrained mobile devices is challenging due to high memory and computation costs, despite growing demands for privacy-preserving personalization. Federated Learning (FL) enables local-data training, yet existing methods either rely on memory-intensive backpropagation or use zeroth-order optimization (ZOO), which avoids backward passes but suffers from slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted edge-end cooperative federated fine-tuning framework that combines ZOO on mobile devices with cloud-guided gradient rectification. Mobile clients perform lightweight ZOO updates on private data, while the cloud fine-tunes on auxiliary public data using backpropagation and injects guided perturbations to rectify local updates, improving convergence and accuracy without violating privacy. To address system bottlenecks, CooperLLM introduces pipeline scheduling and adaptive compression to overlap computation and communication and reduce memory usage. Experiments on multiple Transformer models and datasets show that CooperLLM reduces on-device memory by up to $86.4\\%$, accelerates convergence by $8.8 \\times$, and improves accuracy by up to 10 percentage points over state-of-the-art ZOO-based baselines.", "AI": {"tldr": "CooperLLM\uff1a\u4e91\u8f85\u52a9\u7684\u8fb9\u7aef\u534f\u540c\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u96f6\u9636\u4f18\u5316\u548c\u4e91\u7aef\u68af\u5ea6\u6821\u6b63\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3001\u52a0\u901f\u6536\u655b\u5e76\u63d0\u5347\u7cbe\u5ea6\u3002", "motivation": "LLMs\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5fae\u8c03\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\uff0c\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5185\u5b58\u5bc6\u96c6\u7684\u53cd\u5411\u4f20\u64ad\uff0c\u8981\u4e48\u4f7f\u7528\u6536\u655b\u6162\u3001\u7cbe\u5ea6\u4f4e\u7684\u96f6\u9636\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCooperLLM\u6846\u67b6\uff1a\u79fb\u52a8\u5ba2\u6237\u7aef\u5728\u79c1\u6709\u6570\u636e\u4e0a\u6267\u884c\u8f7b\u91cf\u7ea7\u96f6\u9636\u4f18\u5316\u66f4\u65b0\uff0c\u4e91\u7aef\u5728\u8f85\u52a9\u516c\u5171\u6570\u636e\u4e0a\u4f7f\u7528\u53cd\u5411\u4f20\u64ad\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u6ce8\u5165\u5f15\u5bfc\u6270\u52a8\u6765\u6821\u6b63\u672c\u5730\u66f4\u65b0\u3002\u91c7\u7528\u6d41\u6c34\u7ebf\u8c03\u5ea6\u548c\u81ea\u9002\u5e94\u538b\u7f29\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2aTransformer\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCooperLLM\u5c06\u8bbe\u5907\u5185\u5b58\u964d\u4f4e\u8fbe86.4%\uff0c\u52a0\u901f\u6536\u655b8.8\u500d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684ZOO\u57fa\u7ebf\u65b9\u6cd5\u7cbe\u5ea6\u63d0\u5347\u8fbe10\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CooperLLM\u901a\u8fc7\u4e91\u8f85\u52a9\u7684\u8fb9\u7aef\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907\u4e0aLLM\u8054\u90a6\u5fae\u8c03\u7684\u5185\u5b58\u3001\u6536\u655b\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u9ad8\u6548\u4e2a\u6027\u5316\u3002"}}
{"id": "2601.12928", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.12928", "abs": "https://arxiv.org/abs/2601.12928", "authors": ["Yaima Paz Soto", "Silena Herold Garcia", "Ximo Gual-Arnau", "Antoni Jaume-i-Cap\u00f3", "Manuel Gonz\u00e1lez-Hidalgo"], "title": "An efficient heuristic for geometric analysis of cell deformations", "comment": null, "summary": "Sickle cell disease causes erythrocytes to become sickle-shaped, affecting their movement in the bloodstream and reducing oxygen delivery. It has a high global prevalence and places a significant burden on healthcare systems, especially in resource-limited regions. Automated classification of sickle cells in blood images is crucial, allowing the specialist to reduce the effort required and avoid errors when quantifying the deformed cells and assessing the severity of a crisis. Recent studies have proposed various erythrocyte representation and classification methods. Since classification depends solely on cell shape, a suitable approach models erythrocytes as closed planar curves in shape space. This approach employs elastic distances between shapes, which are invariant under rotations, translations, scaling, and reparameterizations, ensuring consistent distance measurements regardless of the curves' position, starting point, or traversal speed. While previous methods exploiting shape space distances had achieved high accuracy, we refined the model by considering the geometric characteristics of healthy and sickled erythrocytes. Our method proposes (1) to employ a fixed parameterization based on the major axis of each cell to compute distances and (2) to align each cell with two templates using this parameterization before computing distances. Aligning shapes to templates before distance computation, a concept successfully applied in areas such as molecular dynamics, and using a fixed parameterization, instead of minimizing distances across all possible parameterizations, simplifies calculations. This strategy achieves 96.03\\% accuracy rate in both supervised classification and unsupervised clustering. Our method ensures efficient erythrocyte classification, maintaining or improving accuracy over shape space models while significantly reducing computational costs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f62\u72b6\u7a7a\u95f4\u548c\u5f39\u6027\u8ddd\u79bb\u7684\u9570\u72b6\u7ec6\u80de\u81ea\u52a8\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u5316\u548c\u6a21\u677f\u5bf9\u9f50\u7b80\u5316\u8ba1\u7b97\uff0c\u5728\u76d1\u7763\u5206\u7c7b\u548c\u65e0\u76d1\u7763\u805a\u7c7b\u4e2d\u8fbe\u523096.03%\u51c6\u786e\u7387", "motivation": "\u9570\u72b6\u7ec6\u80de\u75c5\u5bfc\u81f4\u7ea2\u7ec6\u80de\u53d8\u5f62\uff0c\u5f71\u54cd\u8840\u6db2\u6d41\u52a8\u548c\u6c27\u6c14\u8f93\u9001\uff0c\u5168\u7403\u60a3\u75c5\u7387\u9ad8\u4e14\u533b\u7597\u8d1f\u62c5\u91cd\u3002\u81ea\u52a8\u5206\u7c7b\u53ef\u51cf\u5c11\u4e13\u5bb6\u5de5\u4f5c\u91cf\u3001\u907f\u514d\u91cf\u5316\u8bef\u5dee\uff0c\u5bf9\u8bc4\u4f30\u75c5\u60c5\u4e25\u91cd\u7a0b\u5ea6\u81f3\u5173\u91cd\u8981", "method": "\u5c06\u7ea2\u7ec6\u80de\u5efa\u6a21\u4e3a\u5f62\u72b6\u7a7a\u95f4\u4e2d\u7684\u95ed\u5408\u5e73\u9762\u66f2\u7ebf\uff0c\u4f7f\u7528\u5f39\u6027\u8ddd\u79bb\uff08\u5bf9\u65cb\u8f6c\u3001\u5e73\u79fb\u3001\u7f29\u653e\u548c\u91cd\u53c2\u6570\u5316\u4e0d\u53d8\uff09\u3002\u6539\u8fdb\u5305\u62ec\uff1a(1)\u57fa\u4e8e\u7ec6\u80de\u4e3b\u8f74\u4f7f\u7528\u56fa\u5b9a\u53c2\u6570\u5316\u8ba1\u7b97\u8ddd\u79bb\uff0c(2)\u5728\u8ba1\u7b97\u8ddd\u79bb\u524d\u5c06\u6bcf\u4e2a\u7ec6\u80de\u4e0e\u4e24\u4e2a\u6a21\u677f\u5bf9\u9f50\u3002\u76f8\u6bd4\u5728\u6240\u6709\u53ef\u80fd\u53c2\u6570\u5316\u4e2d\u6700\u5c0f\u5316\u8ddd\u79bb\uff0c\u8fd9\u79cd\u65b9\u6cd5\u7b80\u5316\u4e86\u8ba1\u7b97", "result": "\u5728\u76d1\u7763\u5206\u7c7b\u548c\u65e0\u76d1\u7763\u805a\u7c7b\u4e2d\u5747\u8fbe\u523096.03%\u7684\u51c6\u786e\u7387\u3002\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u5f62\u72b6\u7a7a\u95f4\u6a21\u578b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7ea2\u7ec6\u80de\u5206\u7c7b\uff0c\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u5316\u548c\u6a21\u677f\u5bf9\u9f50\u7b56\u7565\uff0c\u5728\u4fdd\u8bc1\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u5730\u533a\u7684\u533b\u7597\u7cfb\u7edf"}}
{"id": "2601.12965", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12965", "abs": "https://arxiv.org/abs/2601.12965", "authors": ["Doheon Kim"], "title": "Deterministic Dynamics of Sampling Processes in Score-Based Diffusion Models with Multiplicative Noise Conditioning", "comment": null, "summary": "Score-based diffusion models generate new samples by learning the score function associated with a diffusion process. While the effectiveness of these models can be theoretically explained using differential equations related to the sampling process, previous work by Song and Ermon (2020) demonstrated that neural networks using multiplicative noise conditioning can still generate satisfactory samples. In this setup, the model is expressed as the product of two functions: one depending on the spatial variable and the other on the noise magnitude. This structure limits the model's ability to represent a more general relationship between the spatial variable and the noise, indicating that it cannot fully learn the correct score. Despite this limitation, the models perform well in practice. In this work, we provide a theoretical explanation for this phenomenon by studying the deterministic dynamics of the associated differential equations, offering insight into how the model operates.", "AI": {"tldr": "\u57fa\u4e8e\u5206\u6570\u7684\u6269\u6563\u6a21\u578b\u901a\u8fc7\u4e58\u79ef\u7ed3\u6784\u5b66\u4e60\u5206\u6570\u51fd\u6570\uff0c\u867d\u7136\u65e0\u6cd5\u5b8c\u5168\u5b66\u4e60\u6b63\u786e\u5206\u6570\uff0c\u4f46\u5b9e\u8df5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u672c\u6587\u4ece\u786e\u5b9a\u6027\u52a8\u529b\u5b66\u89d2\u5ea6\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca", "motivation": "Song\u548cErmon(2020)\u53d1\u73b0\u4f7f\u7528\u4e58\u6027\u566a\u58f0\u8c03\u8282\u7684\u795e\u7ecf\u7f51\u7edc\u4ecd\u80fd\u751f\u6210\u6ee1\u610f\u6837\u672c\uff0c\u4f46\u8fd9\u79cd\u4e58\u79ef\u7ed3\u6784\u9650\u5236\u4e86\u6a21\u578b\u8868\u793a\u7a7a\u95f4\u53d8\u91cf\u4e0e\u566a\u58f0\u4e4b\u95f4\u66f4\u4e00\u822c\u5173\u7cfb\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u5b8c\u5168\u5b66\u4e60\u6b63\u786e\u5206\u6570\u3002\u5c3d\u7ba1\u6709\u8fd9\u79cd\u9650\u5236\uff0c\u6a21\u578b\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u9700\u8981\u7406\u8bba\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u76f8\u5173\u5fae\u5206\u65b9\u7a0b\u7684\u786e\u5b9a\u6027\u52a8\u529b\u5b66\uff0c\u5206\u6790\u6a21\u578b\u5982\u4f55\u64cd\u4f5c\uff0c\u4e3a\u8fd9\u79cd\u73b0\u8c61\u63d0\u4f9b\u7406\u8bba\u89e3\u91ca\u3002\u5173\u6ce8\u4e58\u79ef\u7ed3\u6784\u6a21\u578b\u5728\u65e0\u6cd5\u5b66\u4e60\u6b63\u786e\u5206\u6570\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\u7684\u673a\u5236\u3002", "result": "\u4e3a\u4e58\u79ef\u7ed3\u6784\u6269\u6563\u6a21\u578b\u5728\u65e0\u6cd5\u5b8c\u5168\u5b66\u4e60\u6b63\u786e\u5206\u6570\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u751f\u6210\u826f\u597d\u6837\u672c\u7684\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u63ed\u793a\u4e86\u786e\u5b9a\u6027\u52a8\u529b\u5b66\u7684\u4f5c\u7528\u673a\u5236\u3002", "conclusion": "\u5373\u4f7f\u4e58\u79ef\u7ed3\u6784\u9650\u5236\u4e86\u6a21\u578b\u5b66\u4e60\u6b63\u786e\u5206\u6570\u51fd\u6570\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u5206\u6790\u76f8\u5173\u5fae\u5206\u65b9\u7a0b\u7684\u786e\u5b9a\u6027\u52a8\u529b\u5b66\uff0c\u53ef\u4ee5\u89e3\u91ca\u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u8df5\u4e2d\u4ecd\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2601.12971", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12971", "abs": "https://arxiv.org/abs/2601.12971", "authors": ["Pancheng Niu", "Jun Guo", "Qiaolin He", "Yongming Chen", "Yanchao Shi"], "title": "Architecture-Optimization Co-Design for Physics-Informed Neural Networks Via Attentive Representations and Conflict-Resolved Gradients", "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) provide a learning-based framework for solving partial differential equations (PDEs) by embedding governing physical laws into neural network training. In practice, however, their performance is often hindered by limited representational capacity and optimization difficulties caused by competing physical constraints and conflicting gradients. In this work, we study PINN training from a unified architecture-optimization perspective. We first propose a layer-wise dynamic attention mechanism to enhance representational flexibility, resulting in the Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training as a multi-task learning problem and introduce a conflict-resolved gradient update strategy to alleviate gradient interference, leading to the Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components, we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines attentive representations with conflict-aware optimization while preserving the standard PINN loss formulation. Extensive experiments on benchmark PDEs, including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow problems, demonstrate that ACR-PINN achieves faster convergence and significantly lower relative $L_2$ and $L_\\infty$ errors than standard PINNs. These results highlight the effectiveness of architecture-optimization co-design for improving the robustness and accuracy of PINN-based solvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86ACR-PINN\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u8868\u793a\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u51b2\u7a81\u89e3\u51b3\u68af\u5ea6\u66f4\u65b0\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86PINNs\u5728\u6c42\u89e3PDE\u95ee\u9898\u4e2d\u7684\u6536\u655b\u901f\u5ea6\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfPINNs\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\u65f6\u9762\u4e34\u8868\u793a\u80fd\u529b\u6709\u9650\u548c\u4f18\u5316\u56f0\u96be\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u7269\u7406\u7ea6\u675f\u4e4b\u95f4\u7684\u7ade\u4e89\u548c\u68af\u5ea6\u51b2\u7a81\u4f1a\u963b\u788d\u8bad\u7ec3\u6548\u679c\u3002", "method": "1. \u63d0\u51fa\u5c42\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236(LDA-PINN)\u589e\u5f3a\u8868\u793a\u7075\u6d3b\u6027\uff1b2. \u5c06PINN\u8bad\u7ec3\u91cd\u6784\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u5f15\u5165\u51b2\u7a81\u89e3\u51b3\u68af\u5ea6\u66f4\u65b0\u7b56\u7565(GC-PINN)\uff1b3. \u6574\u5408\u4e24\u8005\u5f62\u6210ACR-PINN\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u8868\u793a\u548c\u51b2\u7a81\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5728Burgers\u3001Helmholtz\u3001Klein-Gordon\u548c\u8154\u4f53\u9a71\u52a8\u6d41\u7b49\u57fa\u51c6PDE\u95ee\u9898\u4e0a\uff0cACR-PINN\u76f8\u6bd4\u6807\u51c6PINNs\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u663e\u8457\u66f4\u4f4e\u7684\u76f8\u5bf9L2\u548cL\u221e\u8bef\u5dee\u3002", "conclusion": "\u67b6\u6784-\u4f18\u5316\u534f\u540c\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347PINN\u6c42\u89e3\u5668\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u6539\u8fdb\u57fa\u4e8ePINN\u7684PDE\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.12988", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.12988", "abs": "https://arxiv.org/abs/2601.12988", "authors": ["Zijian Wang", "Tiancheng Huang", "Hanqi Li", "Da Ma", "Lu Chen", "Kai Yu"], "title": "PaperGuide: Making Small Language-Model Paper-Reading Agents More Efficient", "comment": "35 pages, 9 figures, 7 tables", "summary": "The accelerating growth of the scientific literature makes it increasingly difficult for researchers to track new advances through manual reading alone. Recent progress in large language models (LLMs) has therefore spurred interest in autonomous agents that can read scientific papers and extract task-relevant information. However, most existing approaches rely either on heavily engineered prompting or on a conventional SFT-RL training pipeline, both of which often lead to excessive and low-yield exploration. Drawing inspiration from cognitive science, we propose PaperCompass, a framework that mitigates these issues by separating high-level planning from fine-grained execution. PaperCompass first drafts an explicit plan that outlines the intended sequence of actions, and then performs detailed reasoning to instantiate each step by selecting the parameters for the corresponding function calls. To train such behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored RL method that jointly optimizes both the draft plan and the final solution. DFPO can be viewed as a lightweight form of hierarchical reinforcement learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a theoretical analysis that establishes DFPO's favorable optimization properties, supporting a stable and reliable training process. Experiments on paper-based question answering (Paper-QA) benchmarks show that PaperCompass improves efficiency over strong baselines without sacrificing performance, achieving results comparable to much larger models.", "AI": {"tldr": "PaperCompass\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u9ad8\u5c42\u89c4\u5212\u4e0e\u7ec6\u7c92\u5ea6\u6267\u884c\uff0c\u4f7f\u7528Draft-and-Follow Policy Optimization\u8bad\u7ec3LLM\uff0c\u5728\u79d1\u5b66\u8bba\u6587\u95ee\u7b54\u4efb\u52a1\u4e2d\u63d0\u9ad8\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u901a\u8fc7\u624b\u52a8\u9605\u8bfb\u8ddf\u8e2a\u65b0\u8fdb\u5c55\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u4ee3\u7406\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u91cf\u5de5\u7a0b\u5316\u63d0\u793a\uff0c\u8981\u4e48\u4f7f\u7528\u4f20\u7edfSFT-RL\u8bad\u7ec3\u6d41\u7a0b\uff0c\u90fd\u5bb9\u6613\u5bfc\u81f4\u8fc7\u5ea6\u63a2\u7d22\u548c\u4f4e\u6548\u4ea7\u51fa\u3002", "method": "\u63d0\u51faPaperCompass\u6846\u67b6\uff0c\u5c06\u9ad8\u5c42\u89c4\u5212\u4e0e\u7ec6\u7c92\u5ea6\u6267\u884c\u5206\u79bb\uff1a\u5148\u8d77\u8349\u660e\u786e\u8ba1\u5212\uff0c\u7136\u540e\u901a\u8fc7\u8be6\u7ec6\u63a8\u7406\u5b9e\u4f8b\u5316\u6bcf\u4e2a\u6b65\u9aa4\u3002\u5f15\u5165Draft-and-Follow Policy Optimization\uff08DFPO\uff09\u8054\u5408\u4f18\u5316\u8ba1\u5212\u8349\u7a3f\u548c\u6700\u7ec8\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u8bba\u6587\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPaperCompass\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u53d6\u5f97\u4e86\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "conclusion": "PaperCompass\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u4e0e\u6267\u884c\u7684\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u65b9\u6cd5\uff0c\u7ed3\u5408DFPO\u4f18\u5316\uff0c\u6709\u6548\u7f29\u5c0f\u4e86LLM\u7684\"\u77e5\u884c\u5dee\u8ddd\"\uff0c\u4e3a\u79d1\u5b66\u6587\u732e\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13013", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13013", "abs": "https://arxiv.org/abs/2601.13013", "authors": ["Xiaohui Zhao", "Xinjian Zhao", "Jiahui Zhang", "Guoyu Liu", "Houzhi Wang", "Shu Wu"], "title": "HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value Prediction in Baidu Ads", "comment": null, "summary": "Lifetime value (LTV) prediction is crucial for news feed advertising, enabling platforms to optimize bidding and budget allocation for long-term revenue growth. However, it faces two major challenges: (1) demographic-based targeting creates segment-specific LTV distributions with large value variations across user groups; and (2) dynamic marketing strategies generate irregular behavioral sequences where engagement patterns evolve rapidly. We propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models demographic heterogeneity and temporal dynamics through three key components: (i) a hypergraph-supervised module capturing inter-segment relationships; (ii) a transformer-based temporal encoder with adaptive weighting; and (iii) a task-adaptive mixture-of-experts with dynamic prediction towers for multi-horizon LTV forecasting. Experiments on \\textit{Baidu Ads} with 15 million users demonstrate that HT-GNN consistently outperforms state-of-the-art methods across all metrics and prediction horizons.", "AI": {"tldr": "\u63d0\u51faHT-GNN\u6a21\u578b\u89e3\u51b3\u65b0\u95fb\u6d41\u5e7f\u544a\u4e2d\u7684LTV\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u8d85\u56fe\u76d1\u7763\u6a21\u5757\u3001Transformer\u65f6\u5e8f\u7f16\u7801\u5668\u548c\u4efb\u52a1\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u6709\u6548\u5904\u7406\u7528\u6237\u7fa4\u4f53\u5f02\u8d28\u6027\u548c\u52a8\u6001\u884c\u4e3a\u5e8f\u5217\u3002", "motivation": "\u65b0\u95fb\u6d41\u5e7f\u544a\u4e2d\u7684LTV\u9884\u6d4b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u7684\u76ee\u6807\u5b9a\u4f4d\u5bfc\u81f4\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684LTV\u5206\u5e03\u5dee\u5f02\u5de8\u5927\uff1b2) \u52a8\u6001\u8425\u9500\u7b56\u7565\u4ea7\u751f\u4e0d\u89c4\u5219\u7684\u3001\u5feb\u901f\u6f14\u53d8\u7684\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u3002", "method": "\u63d0\u51fa\u8d85\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc(HT-GNN)\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u8d85\u56fe\u76d1\u7763\u6a21\u5757\u6355\u6349\u7528\u6237\u7fa4\u4f53\u95f4\u5173\u7cfb\uff1b2) \u57fa\u4e8eTransformer\u7684\u65f6\u5e8f\u7f16\u7801\u5668\u5e26\u81ea\u9002\u5e94\u6743\u91cd\uff1b3) \u4efb\u52a1\u81ea\u9002\u5e94\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u4f7f\u7528\u52a8\u6001\u9884\u6d4b\u5854\u8fdb\u884c\u591a\u65f6\u95f4\u8303\u56f4LTV\u9884\u6d4b\u3002", "result": "\u5728\u767e\u5ea6\u5e7f\u544a\u5e73\u53f0\u76841500\u4e07\u7528\u6237\u6570\u636e\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cHT-GNN\u5728\u6240\u6709\u6307\u6807\u548c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HT-GNN\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u4eba\u53e3\u7edf\u8ba1\u5f02\u8d28\u6027\u548c\u65f6\u5e8f\u52a8\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65b0\u95fb\u6d41\u5e7f\u544a\u4e2d\u7684LTV\u9884\u6d4b\u6311\u6218\uff0c\u4e3a\u957f\u671f\u6536\u5165\u589e\u957f\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2601.13020", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13020", "abs": "https://arxiv.org/abs/2601.13020", "authors": ["Zhiyan Hou", "Haiyun Guo", "Haokai Ma", "Yandu Sun", "Yonghui Yang", "Jinqiao Wang"], "title": "PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via Pathway Activation Subspaces for Continual Learning", "comment": null, "summary": "Continual instruction tuning (CIT) requires multimodal large language models (MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A common strategy is to isolate updates by routing inputs to different LoRA experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often jointly update the router and experts in an indiscriminate way, causing the router's preferences to co-drift with experts' adaptation pathways and gradually deviate from early-stage input-expert specialization. We term this phenomenon Misaligned Co-drift, which blurs expert responsibilities and exacerbates forgetting.To address this, we introduce the pathway activation subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway directions an input activates in each expert, providing a capability-aligned coordinate system for routing and preservation. Based on PASs, we propose a fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided Reweighting, which calibrates routing using each expert's pathway activation signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank directions important to previous tasks. Experiments on a CIT benchmark show that our approach consistently outperforms a range of conventional continual learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting without adding parameters. Our code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51faPASs\uff08\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4\uff09\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u4e13\u5bb6\u5171\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u8def\u5f84\u6fc0\u6d3b\u4fe1\u53f7\u6821\u51c6\u8def\u7531\u5e76\u7a33\u5b9a\u91cd\u8981\u79e9\u65b9\u5411\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u51c6\u786e\u6027\u548c\u6297\u9057\u5fd8\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLoRA\u7684\u6df7\u5408\u4e13\u5bb6\u65b9\u6cd5\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4e2d\uff0c\u8def\u7531\u5668\u548c\u4e13\u5bb6\u4f1a\u5171\u540c\u6f02\u79fb\uff0c\u5bfc\u81f4\u65e9\u671f\u8f93\u5165-\u4e13\u5bb6\u4e13\u4e1a\u5316\u9010\u6e10\u504f\u79bb\uff0c\u4e13\u5bb6\u804c\u8d23\u6a21\u7cca\u5e76\u52a0\u5267\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8def\u5f84\u6fc0\u6d3b\u5b50\u7a7a\u95f4\uff08PASs\uff09\u4f5c\u4e3a\u80fd\u529b\u5bf9\u9f50\u7684\u5750\u6807\u7cfb\u7edf\uff0c\u5305\u542bPAS\u5f15\u5bfc\u7684\u91cd\u52a0\u6743\uff08\u7528\u8def\u5f84\u6fc0\u6d3b\u4fe1\u53f7\u6821\u51c6\u8def\u7531\uff09\u548cPAS\u611f\u77e5\u7684\u79e9\u7a33\u5b9a\u5316\uff08\u9009\u62e9\u6027\u7a33\u5b9a\u5bf9\u5148\u524d\u4efb\u52a1\u91cd\u8981\u7684\u79e9\u65b9\u5411\uff09\u3002", "result": "\u5728\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6297\u9057\u5fd8\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u57fa\u7ebf\u548cMoE-LoRA\u53d8\u4f53\uff0c\u4e14\u4e0d\u589e\u52a0\u989d\u5916\u53c2\u6570\u3002", "conclusion": "PASs\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u8def\u7531\u548c\u4e13\u5bb6\u66f4\u65b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u5171\u6f02\u79fb\u95ee\u9898\uff0c\u4e3a\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13021", "abs": "https://arxiv.org/abs/2601.13021", "authors": ["Nata\u0161a Petrovi\u0107", "Gabriel Moy\u00e0-Alcover", "Antoni Jaume-i-Cap\u00f3", "Jose Maria Buades Rubio"], "title": "Enhancing Generalization in Sickle Cell Disease Diagnosis through Ensemble Methods and Feature Importance Analysis", "comment": null, "summary": "This work presents a novel approach for selecting the optimal ensemble-based classification method and features with a primarly focus on achieving generalization, based on the state-of-the-art, to provide diagnostic support for Sickle Cell Disease using peripheral blood smear images of red blood cells. We pre-processed and segmented the microscopic images to ensure the extraction of high-quality features. To ensure the reliability of our proposed system, we conducted an in-depth analysis of interpretability. Leveraging techniques established in the literature, we extracted features from blood cells and employed ensemble machine learning methods to classify their morphology. Furthermore, we have devised a methodology to identify the most critical features for classification, aimed at reducing complexity and training time and enhancing interpretability in opaque models. Lastly, we validated our results using a new dataset, where our model overperformed state-of-the-art models in terms of generalization. The results of classifier ensembled of Random Forest and Extra Trees classifier achieved an harmonic mean of precision and recall (F1-score) of 90.71\\% and a Sickle Cell Disease diagnosis support score (SDS-score) of 93.33\\%. These results demonstrate notable enhancement from previous ones with Gradient Boosting classifier (F1-score 87.32\\% and SDS-score 89.51\\%). To foster scientific progress, we have made available the parameters for each model, the implemented code library, and the confusion matrices with the raw data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u9570\u72b6\u7ec6\u80de\u75c5\u8bca\u65ad\u652f\u6301\u7cfb\u7edf\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u4e3a\u9570\u72b6\u7ec6\u80de\u75c5\u63d0\u4f9b\u8bca\u65ad\u652f\u6301\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u8f85\u52a9\u533b\u7597\u8bca\u65ad", "method": "\u5bf9\u8840\u6db2\u6d82\u7247\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\u548c\u5206\u5272\uff0c\u63d0\u53d6\u9ad8\u8d28\u91cf\u7279\u5f81\uff1b\u4f7f\u7528\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u968f\u673a\u68ee\u6797\u548c\u6781\u7aef\u968f\u673a\u6811\uff09\u8fdb\u884c\u5206\u7c7b\uff1b\u8bbe\u8ba1\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u65b9\u6cd5\u6765\u8bc6\u522b\u5173\u952e\u7279\u5f81\uff1b\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1", "result": "\u968f\u673a\u68ee\u6797\u548c\u6781\u7aef\u968f\u673a\u6811\u96c6\u6210\u6a21\u578b\u53d6\u5f97\u4e86F1\u5206\u657090.71%\u548cSDS\u5206\u657093.33%\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u76f8\u6bd4\u68af\u5ea6\u63d0\u5347\u5206\u7c7b\u5668\uff08F1 87.32%\uff0cSDS 89.51%\uff09\u6709\u660e\u663e\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u9570\u72b6\u7ec6\u80de\u75c5\u8bca\u65ad\u652f\u6301\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8f85\u52a9\u5de5\u5177"}}
{"id": "2601.13048", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13048", "abs": "https://arxiv.org/abs/2601.13048", "authors": ["Srividya Ravikumar", "Abhinav Anand", "Shweta Verma", "Mira Mezini"], "title": "Analysis of Long Range Dependency Understanding in State Space Models", "comment": null, "summary": "Although state-space models (SSMs) have demonstrated strong performance on long-sequence benchmarks, most research has emphasized predictive accuracy rather than interpretability. In this work, we present the first systematic kernel interpretability study of the diagonalized state-space model (S4D) trained on a real-world task (vulnerability detection in source code). Through time and frequency domain analysis of the S4D kernel, we show that the long-range modeling capability of S4D varies significantly under different model architectures, affecting model performance. For instance, we show that the depending on the architecture, S4D kernel can behave as low-pass, band-pass or high-pass filter. The insights from our analysis can guide future work in designing better S4D-based models.", "AI": {"tldr": "\u9996\u6b21\u5bf9S4D\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u6838\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u5206\u6790\u5176\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff08\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u4e0bS4D\u6838\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6ee4\u6ce2\u7279\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9884\u6d4b\u51c6\u786e\u6027\u800c\u975e\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9996\u6b21\u5bf9S4D\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u6838\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "method": "\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff08\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\uff09\u4e0a\u8bad\u7ec3S4D\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u57df\u548c\u9891\u57df\u5206\u6790S4D\u6838\u7684\u7279\u6027\uff0c\u7814\u7a76\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e0b\u6838\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u53d1\u73b0S4D\u7684\u957f\u7a0b\u5efa\u6a21\u80fd\u529b\u5728\u4e0d\u540c\u67b6\u6784\u4e0b\u5dee\u5f02\u663e\u8457\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002S4D\u6838\u53ef\u8868\u73b0\u4e3a\u4f4e\u901a\u3001\u5e26\u901a\u6216\u9ad8\u901a\u6ee4\u6ce2\u5668\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u67b6\u6784\u8bbe\u8ba1\u3002", "conclusion": "S4D\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u597d\u7684S4D\u57fa\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u67b6\u6784\u9009\u62e9\u5bf9\u6a21\u578b\u6ee4\u6ce2\u7279\u6027\u548c\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2601.13054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13054", "abs": "https://arxiv.org/abs/2601.13054", "authors": ["Kamogelo Taueatsoala", "Caitlyn Daniels", "Angelina J. Ramsunar", "Petrus Bronkhorst", "Absalom E. Ezugwu"], "title": "TinyML-Enabled IoT for Sustainable Precision Irrigation", "comment": null, "summary": "Small-scale farming communities are disproportionately affected by water scarcity, erratic climate patterns, and a lack of access to advanced, affordable agricultural technologies. To address these challenges, this paper presents a novel, edge-first IoT framework that integrates Tiny Machine Learning (TinyML) for intelligent, offline-capable precision irrigation. The proposed four-layer architecture leverages low-cost hardware, an ESP32 microcontroller as an edge inference node, and a Raspberry Pi as a local edge server to enable autonomous decision-making without cloud dependency. The system utilizes capacitive soil moisture, temperature, humidity, pH, and ambient light sensors for environmental monitoring. A rigorous comparative analysis of ensemble models identified gradient boosting as superior, achieving an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%, outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This optimized model was converted and deployed as a lightweight TinyML inference engine on the ESP32 and predicts irrigation needs with exceptional accuracy (MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol, ensuring reliable operation in areas with limited or no internet connectivity. Experimental validation in a controlled environment demonstrated a significant reduction in water usage compared to traditional methods, while the system's low-power design and offline functionality confirm its viability for sustainable, scalable deployment in resource-constrained rural settings. This work provides a practical, cost-effective blueprint for bridging the technological divide in agriculture and enhancing water-use efficiency through on-device artificial intelligence.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTinyML\u7684\u8fb9\u7f18\u7269\u8054\u7f51\u6846\u67b6\uff0c\u7528\u4e8e\u5c0f\u519c\u573a\u7684\u667a\u80fd\u7cbe\u51c6\u704c\u6e89\uff0c\u65e0\u9700\u4e91\u7aef\u4f9d\u8d56\uff0c\u663e\u8457\u51cf\u5c11\u7528\u6c34\u91cf", "motivation": "\u89e3\u51b3\u5c0f\u89c4\u6a21\u519c\u573a\u9762\u4e34\u7684\u6c34\u8d44\u6e90\u77ed\u7f3a\u3001\u6c14\u5019\u4e0d\u7a33\u5b9a\u4ee5\u53ca\u7f3a\u4e4f\u5148\u8fdb\u3001\u53ef\u8d1f\u62c5\u519c\u4e1a\u6280\u672f\u7684\u95ee\u9898", "method": "\u56db\u5c42\u8fb9\u7f18\u7269\u8054\u7f51\u67b6\u6784\uff0c\u4f7f\u7528ESP32\u5fae\u63a7\u5236\u5668\u4f5c\u4e3a\u8fb9\u7f18\u63a8\u7406\u8282\u70b9\uff0cRaspberry Pi\u4f5c\u4e3a\u672c\u5730\u8fb9\u7f18\u670d\u52a1\u5668\uff0c\u96c6\u6210\u591a\u79cd\u73af\u5883\u4f20\u611f\u5668\uff0c\u91c7\u7528\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u5e76\u8f6c\u6362\u4e3aTinyML\u90e8\u7f72\uff0c\u57fa\u4e8eMQTT\u7684\u5c40\u57df\u7f51\u901a\u4fe1\u534f\u8bae", "result": "\u68af\u5ea6\u63d0\u5347\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08R\u00b2=0.9973\uff0cMAPE=0.99%\uff09\uff0c\u90e8\u7f72\u540e\u704c\u6e89\u9700\u6c42\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\uff08MAPE<1%\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u7528\u6c34\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u519c\u6751\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u7cbe\u51c6\u704c\u6e89\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bbe\u5907\u7aef\u4eba\u5de5\u667a\u80fd\u63d0\u5347\u6c34\u8d44\u6e90\u5229\u7528\u6548\u7387"}}
{"id": "2601.13075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13075", "abs": "https://arxiv.org/abs/2601.13075", "authors": ["Abhinav Rajeev Kumar", "Dhruv Trehan", "Paras Chopra"], "title": "METIS: Mentoring Engine for Thoughtful Inquiry & Solutions", "comment": "12 pages, 5 figures, 4 tables", "summary": "Many students lack access to expert research mentorship. We ask whether an AI mentor can move undergraduates from an idea to a paper. We build METIS, a tool-augmented, stage-aware assistant with literature search, curated guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise preferences, student-persona rubrics, short multi-turn tutoring, and evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores (clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly higher final quality than GPT-5. Gains concentrate in document-grounded stages (D-F), consistent with stage-aware routing and groundings failure modes include premature tool routing, shallow grounding, and occasional stage misclassification.", "AI": {"tldr": "METIS\u662f\u4e00\u4e2a\u9762\u5411\u672c\u79d1\u751f\u7684AI\u7814\u7a76\u5bfc\u5e08\u7cfb\u7edf\uff0c\u901a\u8fc7\u9636\u6bb5\u611f\u77e5\u7684\u8f85\u52a9\u5de5\u5177\uff08\u6587\u732e\u641c\u7d22\u3001\u6307\u5357\u3001\u65b9\u6cd5\u68c0\u67e5\u3001\u8bb0\u5fc6\uff09\u5e2e\u52a9\u5b66\u751f\u4ece\u60f3\u6cd5\u5230\u8bba\u6587\u5199\u4f5c\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-5\u548cClaude Sonnet 4.5\u3002", "motivation": "\u8bb8\u591a\u5b66\u751f\u7f3a\u4e4f\u4e13\u5bb6\u7814\u7a76\u6307\u5bfc\uff0c\u9700\u8981AI\u5bfc\u5e08\u5e2e\u52a9\u4ed6\u4eec\u4ece\u7814\u7a76\u60f3\u6cd5\u53d1\u5c55\u5230\u5b8c\u6574\u8bba\u6587\u3002", "method": "\u6784\u5efaMETIS\u7cfb\u7edf\uff0c\u5305\u542b\u5de5\u5177\u589e\u5f3a\u3001\u9636\u6bb5\u611f\u77e5\u7684\u52a9\u624b\uff0c\u5177\u6709\u6587\u732e\u641c\u7d22\u3001\u7cbe\u9009\u6307\u5357\u3001\u65b9\u6cd5\u5b66\u68c0\u67e5\u548c\u8bb0\u5fc6\u529f\u80fd\u3002\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8fdb\u884c\u6210\u5bf9\u504f\u597d\u8bc4\u4f30\u3001\u5b66\u751f\u89d2\u8272\u8bc4\u5206\u6807\u51c6\u3001\u77ed\u591a\u8f6e\u8f85\u5bfc\u4ee5\u53ca\u8bc1\u636e/\u5408\u89c4\u6027\u68c0\u67e5\u3002", "result": "\u572890\u4e2a\u5355\u8f6e\u63d0\u793a\u4e2d\uff0cLLM\u8bc4\u5224\u8005\u66f4\u504f\u597dMETIS\uff08vs Claude Sonnet 4.5 71%\uff0cvs GPT-5 54%\uff09\u3002\u5b66\u751f\u8bc4\u5206\uff08\u6e05\u6670\u5ea6/\u53ef\u64cd\u4f5c\u6027/\u7ea6\u675f\u5339\u914d\uff09\u5728\u6240\u6709\u9636\u6bb5\u90fd\u66f4\u9ad8\u3002\u5728\u591a\u8f6e\u4f1a\u8bdd\u4e2d\uff0cMETIS\u7684\u6700\u7ec8\u8d28\u91cf\u7565\u9ad8\u4e8eGPT-5\u3002\u4f18\u52bf\u96c6\u4e2d\u5728\u6587\u6863\u57fa\u7840\u9636\u6bb5\uff08D-F\uff09\u3002", "conclusion": "METIS\u4f5c\u4e3aAI\u7814\u7a76\u5bfc\u5e08\u5728\u5e2e\u52a9\u5b66\u751f\u5b8c\u6210\u7814\u7a76\u8bba\u6587\u65b9\u9762\u8868\u73b0\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u6587\u6863\u57fa\u7840\u9636\u6bb5\u3002\u5931\u8d25\u6a21\u5f0f\u5305\u62ec\u8fc7\u65e9\u5de5\u5177\u8def\u7531\u3001\u6d45\u5c42\u57fa\u7840\u548c\u5076\u5c14\u7684\u9636\u6bb5\u5206\u7c7b\u9519\u8bef\u3002"}}
{"id": "2601.13100", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13100", "abs": "https://arxiv.org/abs/2601.13100", "authors": ["Aaron R. Flouro", "Shawn P. Chadwick"], "title": "Recursive Meta-Distillation: An Axiomatic Framework for Iterative Knowledge Refinement", "comment": null, "summary": "Recent work in probability-domain knowledge distillation has established axiomatic frameworks for temperature scaling, multi-teacher aggregation, and bias-variance trade-offs in single-stage settings. However, the mathematical behavior of recursive or multi-generation distillation remains poorly understood, with prior approaches relying primarily on empirical heuristics. In this work, we introduce an axiomatic and operator-theoretic framework for recursive meta-distillation, formalizing iterative knowledge distillation as a sequence of probability-distribution operators with explicit anchoring to base teachers.\n  We define structural axioms for valid meta-teacher construction and prove the existence of non-trivial operator families satisfying these axioms without specifying particular algorithms or loss functions. Under mild realizability and convexity assumptions, we show that anchored recursive distillation induces contraction in KL divergence, yielding geometric convergence to base teacher distributions and a unique, globally attractive fixed point.\n  The contribution is foundational rather than algorithmic: the framework characterizes when recursive distillation is mathematically well-posed and convergent rather than error-accumulating, independent of model architecture, optimization details, or specific operator instantiations. These results provide a theoretical basis for understanding stability, bias-variance behavior, and failure modes in iterative and multi-teacher distillation under capacity constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9012\u5f52\u5143\u84b8\u998f\u7684\u7b97\u5b50\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8fed\u4ee3\u77e5\u8bc6\u84b8\u998f\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u5206\u5e03\u7b97\u5b50\u5e8f\u5217\uff0c\u8bc1\u660e\u4e86\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u951a\u5b9a\u9012\u5f52\u84b8\u998f\u80fd\u8bf1\u5bfcKL\u6563\u5ea6\u6536\u7f29\uff0c\u6536\u655b\u5230\u57fa\u7840\u6559\u5e08\u5206\u5e03\u7684\u552f\u4e00\u5168\u5c40\u5438\u5f15\u4e0d\u52a8\u70b9\u3002", "motivation": "\u73b0\u6709\u6982\u7387\u57df\u77e5\u8bc6\u84b8\u998f\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u9636\u6bb5\u8bbe\u7f6e\uff0c\u4f46\u9012\u5f52\u6216\u591a\u4ee3\u84b8\u998f\u7684\u6570\u5b66\u884c\u4e3a\u7406\u89e3\u4e0d\u8db3\uff0c\u5148\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u542f\u53d1\u5f0f\u3002\u9700\u8981\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u9012\u5f52\u84b8\u998f\u4f55\u65f6\u662f\u6570\u5b66\u826f\u5b9a\u4e49\u4e14\u6536\u655b\u7684\uff0c\u800c\u4e0d\u662f\u8bef\u5dee\u7d2f\u79ef\u7684\u3002", "method": "\u5f15\u5165\u9012\u5f52\u5143\u84b8\u998f\u7684\u516c\u7406\u5316\u548c\u7b97\u5b50\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8fed\u4ee3\u77e5\u8bc6\u84b8\u998f\u5f62\u5f0f\u5316\u4e3a\u5177\u6709\u663e\u5f0f\u951a\u5b9a\u5230\u57fa\u7840\u6559\u5e08\u7684\u6982\u7387\u5206\u5e03\u7b97\u5b50\u5e8f\u5217\u3002\u5b9a\u4e49\u6709\u6548\u5143\u6559\u5e08\u6784\u5efa\u7684\u7ed3\u6784\u516c\u7406\uff0c\u8bc1\u660e\u6ee1\u8db3\u8fd9\u4e9b\u516c\u7406\u7684\u975e\u5e73\u51e1\u7b97\u5b50\u65cf\u5b58\u5728\u6027\uff0c\u4e0d\u4f9d\u8d56\u7279\u5b9a\u7b97\u6cd5\u6216\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u6e29\u548c\u53ef\u5b9e\u73b0\u6027\u548c\u51f8\u6027\u5047\u8bbe\u4e0b\uff0c\u951a\u5b9a\u9012\u5f52\u84b8\u998f\u8bf1\u5bfcKL\u6563\u5ea6\u6536\u7f29\uff0c\u4ea7\u751f\u51e0\u4f55\u6536\u655b\u5230\u57fa\u7840\u6559\u5e08\u5206\u5e03\uff0c\u5e76\u5b58\u5728\u552f\u4e00\u5168\u5c40\u5438\u5f15\u4e0d\u52a8\u70b9\u3002\u6846\u67b6\u72ec\u7acb\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u4f18\u5316\u7ec6\u8282\u6216\u7279\u5b9a\u7b97\u5b50\u5b9e\u4f8b\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u57fa\u7840\u6027\u800c\u975e\u7b97\u6cd5\u6027\u7684\u8d21\u732e\uff0c\u523b\u753b\u4e86\u9012\u5f52\u84b8\u998f\u4f55\u65f6\u662f\u6570\u5b66\u826f\u5b9a\u4e49\u4e14\u6536\u655b\u7684\uff0c\u4e3a\u7406\u89e3\u5bb9\u91cf\u7ea6\u675f\u4e0b\u8fed\u4ee3\u548c\u591a\u6559\u5e08\u84b8\u998f\u7684\u7a33\u5b9a\u6027\u3001\u504f\u5dee-\u65b9\u5dee\u884c\u4e3a\u548c\u5931\u6548\u6a21\u5f0f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.13143", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13143", "abs": "https://arxiv.org/abs/2601.13143", "authors": ["Chaeyoung Jung", "Youngjoon Jang", "Seungwoo Lee", "Joon Son Chung"], "title": "FastAV: Efficient Token Pruning for Audio-Visual Large Language Model Inference", "comment": null, "summary": "In this work, we present FastAV, the first token pruning framework tailored for audio-visual large language models (AV-LLMs). While token pruning has been actively explored in standard large language models (LLMs) and vision-language models (LVLMs), its application to AV-LLMs has received little attention, even though multimodal integration substantially increases their token demands. To address this gap, we introduce a pruning strategy that utilizes attention weights to identify tokens emphasized at different stages and estimates their importance. Building on this analysis, FastAV applies a two-stage pruning strategy: (1) global pruning in intermediate layers to remove broadly less influential tokens, and (2) fine pruning in later layers considering the impact on next token generation. Notably, our method does not rely on full attention maps, which makes it fully compatible with efficient attention mechanisms such as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs by more than 40% on two representative AV-LLMs, while preserving or even improving model performance.", "AI": {"tldr": "FastAV\u662f\u9996\u4e2a\u9488\u5bf9\u97f3\u9891-\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08AV-LLMs\uff09\u7684token\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u526a\u679d\u7b56\u7565\u51cf\u5c1140%\u4ee5\u4e0a\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u867d\u7136token\u526a\u679d\u5728\u6807\u51c6LLMs\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5df2\u6709\u63a2\u7d22\uff0c\u4f46\u5728AV-LLMs\u4e2d\u5374\u5f88\u5c11\u88ab\u7814\u7a76\u3002\u591a\u6a21\u6001\u6574\u5408\u663e\u8457\u589e\u52a0\u4e86token\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9AV-LLMs\u7684\u9ad8\u6548\u526a\u679d\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u8bc6\u522b\u4e0d\u540c\u9636\u6bb5\u7684\u91cd\u8981token\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u526a\u679d\u7b56\u7565\uff1a1\uff09\u4e2d\u95f4\u5c42\u8fdb\u884c\u5168\u5c40\u526a\u679d\u53bb\u9664\u5f71\u54cd\u529b\u8f83\u5c0f\u7684token\uff1b2\uff09\u540e\u7eed\u5c42\u8fdb\u884c\u7cbe\u7ec6\u526a\u679d\uff0c\u8003\u8651\u5bf9\u4e0b\u4e00\u4e2atoken\u751f\u6210\u7684\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u5b8c\u6574\u6ce8\u610f\u529b\u56fe\uff0c\u4e0eFlashAttention\u7b49\u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u5b8c\u5168\u517c\u5bb9\u3002", "result": "\u5728\u4e24\u4e2a\u4ee3\u8868\u6027AV-LLMs\u4e0a\uff0cFastAV\u51cf\u5c11\u4e86\u8d85\u8fc740%\u7684FLOPs\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "FastAV\u4e3aAV-LLMs\u63d0\u4f9b\u4e86\u9996\u4e2a\u6709\u6548\u7684token\u526a\u679d\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13160", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13160", "abs": "https://arxiv.org/abs/2601.13160", "authors": ["Zhipeng Zhang", "Zhenjie Yao", "Kai Li", "Lei Yang"], "title": "Training instability in deep learning follows low-dimensional dynamical principles", "comment": null, "summary": "Deep learning systems achieve remarkable empirical performance, yet the stability of the training process itself remains poorly understood. Training unfolds as a high-dimensional dynamical system in which small perturbations to optimization, data, parameters, or learning signals can induce abrupt and irreversible collapse, undermining reproducibility and scalability.\n  We propose a unified dynamical perspective that characterizes training stability as an intrinsic property of learning systems, organized along four interacting dimensions: optimization, environmental/data, parametric, and learning-signal stability. We operationalize this perspective through controlled perturbation auditing of training trajectories, probing how learning dynamics respond to structured disturbances without modifying learning algorithms.\n  Across reinforcement learning and large language model training, we identify three recurring regularities: high final performance is frequently decoupled from training stability; controlled stochasticity consistently buffers learning dynamics across paradigms; and deviations in low-dimensional latent meta-states systematically precede observable performance collapse. Together, these findings establish training stability as a measurable and comparable dynamical property of learning systems, providing a descriptive foundation for studying learning dynamics beyond final performance outcomes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u52a8\u529b\u5b66\u89c6\u89d2\uff0c\u5c06\u8bad\u7ec3\u7a33\u5b9a\u6027\u4f5c\u4e3a\u5b66\u4e60\u7cfb\u7edf\u7684\u5185\u5728\u5c5e\u6027\uff0c\u901a\u8fc7\u53d7\u63a7\u6270\u52a8\u5ba1\u8ba1\u8bc6\u522b\u8bad\u7ec3\u7a33\u5b9a\u6027\u89c4\u5f8b\uff0c\u53d1\u73b0\u6700\u7ec8\u6027\u80fd\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u7ecf\u5e38\u89e3\u8026\uff0c\u53d7\u63a7\u968f\u673a\u6027\u53ef\u7f13\u51b2\u5b66\u4e60\u52a8\u6001\uff0c\u4f4e\u7ef4\u6f5c\u5728\u5143\u72b6\u6001\u504f\u5dee\u53ef\u9884\u6d4b\u6027\u80fd\u5d29\u6e83\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u867d\u7136\u53d6\u5f97\u4e86\u663e\u8457\u7684\u5b9e\u8bc1\u6027\u80fd\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u672c\u8eab\u7684\u7a33\u5b9a\u6027\u4ecd\u7136\u7406\u89e3\u4e0d\u8db3\u3002\u8bad\u7ec3\u4f5c\u4e3a\u9ad8\u7ef4\u52a8\u529b\u7cfb\u7edf\uff0c\u5bf9\u4f18\u5316\u3001\u6570\u636e\u3001\u53c2\u6570\u6216\u5b66\u4e60\u4fe1\u53f7\u7684\u5fae\u5c0f\u6270\u52a8\u53ef\u80fd\u5bfc\u81f4\u7a81\u7136\u4e14\u4e0d\u53ef\u9006\u7684\u5d29\u6e83\uff0c\u635f\u5bb3\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u52a8\u529b\u5b66\u89c6\u89d2\uff0c\u5c06\u8bad\u7ec3\u7a33\u5b9a\u6027\u7ec4\u7ec7\u4e3a\u56db\u4e2a\u76f8\u4e92\u4f5c\u7528\u7684\u7ef4\u5ea6\uff1a\u4f18\u5316\u7a33\u5b9a\u6027\u3001\u73af\u5883/\u6570\u636e\u7a33\u5b9a\u6027\u3001\u53c2\u6570\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u4fe1\u53f7\u7a33\u5b9a\u6027\u3002\u901a\u8fc7\u53d7\u63a7\u6270\u52a8\u5ba1\u8ba1\u8bad\u7ec3\u8f68\u8ff9\u6765\u64cd\u4f5c\u5316\u8fd9\u4e00\u89c6\u89d2\uff0c\u5728\u4e0d\u4fee\u6539\u5b66\u4e60\u7b97\u6cd5\u7684\u60c5\u51b5\u4e0b\u63a2\u6d4b\u5b66\u4e60\u52a8\u6001\u5bf9\u7ed3\u6784\u5316\u6270\u52a8\u7684\u54cd\u5e94\u3002", "result": "\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u53d1\u73b0\u4e86\u4e09\u4e2a\u91cd\u590d\u51fa\u73b0\u7684\u89c4\u5f8b\uff1a1) \u6700\u7ec8\u9ad8\u6027\u80fd\u7ecf\u5e38\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u89e3\u8026\uff1b2) \u53d7\u63a7\u968f\u673a\u6027\u5728\u4e0d\u540c\u8303\u5f0f\u4e2d\u4e00\u81f4\u5730\u7f13\u51b2\u5b66\u4e60\u52a8\u6001\uff1b3) \u4f4e\u7ef4\u6f5c\u5728\u5143\u72b6\u6001\u7684\u504f\u5dee\u7cfb\u7edf\u5730\u5148\u4e8e\u53ef\u89c2\u5bdf\u7684\u6027\u80fd\u5d29\u6e83\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u786e\u7acb\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u4f5c\u4e3a\u5b66\u4e60\u7cfb\u7edf\u53ef\u6d4b\u91cf\u548c\u53ef\u6bd4\u8f83\u7684\u52a8\u529b\u5b66\u5c5e\u6027\uff0c\u4e3a\u8d85\u8d8a\u6700\u7ec8\u6027\u80fd\u7ed3\u679c\u7814\u7a76\u5b66\u4e60\u52a8\u6001\u63d0\u4f9b\u4e86\u63cf\u8ff0\u6027\u57fa\u7840\u3002"}}
{"id": "2601.13162", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2601.13162", "abs": "https://arxiv.org/abs/2601.13162", "authors": ["Ali Shafiee Sarvestani", "Jason Schmidt", "Arman Roohi"], "title": "NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness", "comment": null, "summary": "Adversarial vulnerability and lack of interpretability are critical limitations of deep neural networks, especially in safety-sensitive settings such as autonomous driving. We introduce \\DesignII, a neuro-symbolic framework that integrates symbolic rule supervision into neural networks to enhance both adversarial robustness and explainability. Domain knowledge is encoded as logical constraints over appearance attributes such as shape and color, and enforced through semantic and symbolic logic losses applied during training. Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at a standard $\\ell_\\infty$ perturbation budget of $\\varepsilon = 8/255$. Relative to clean training, standard adversarial training provides modest improvements in robustness ($\\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic and PGD-Neuro-Symbolic models achieve substantially larger gains, improving adversarial accuracy by 18.1\\% and 17.35\\% over their corresponding adversarial-training baselines, representing roughly a three-fold larger robustness gain than standard adversarial training provides when both are measured relative to the same clean-training baseline, without reducing clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx, which require heavy architectures and extensive data augmentation, our PGD-Neuro-Symbolic variant attains comparable or superior robustness using a ResNet18 backbone trained for 10 epochs. These results show that symbolic reasoning offers an effective path to robust and interpretable AI.", "AI": {"tldr": "Neuro-symbolic\u6846\u67b6DesignII\u901a\u8fc7\u7b26\u53f7\u89c4\u5219\u76d1\u7763\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728GTSRB\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u6807\u51c6\u5bf9\u6297\u8bad\u7ec3\u83b7\u5f973\u500d\u9c81\u68d2\u6027\u63d0\u5347\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u5bf9\u6297\u8106\u5f31\u6027\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u5173\u952e\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u654f\u611f\u573a\u666f\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u63d0\u5347\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faDesignII\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u9886\u57df\u77e5\u8bc6\u7f16\u7801\u4e3a\u5f62\u72b6\u3001\u989c\u8272\u7b49\u5916\u89c2\u5c5e\u6027\u7684\u903b\u8f91\u7ea6\u675f\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7b26\u53f7\u903b\u8f91\u635f\u5931\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\uff0c\u4f7f\u7528FGSM\u548cPGD\u5bf9\u6297\u8bad\u7ec3\u53d8\u4f53\u3002", "result": "\u5728\u03b5=8/255\u7684\u6270\u52a8\u9884\u7b97\u4e0b\uff0cFGSM-Neuro-Symbolic\u548cPGD-Neuro-Symbolic\u6a21\u578b\u76f8\u6bd4\u5bf9\u5e94\u5bf9\u6297\u8bad\u7ec3\u57fa\u7ebf\u5206\u522b\u63d0\u534718.1%\u548c17.35%\u7684\u5bf9\u6297\u51c6\u786e\u7387\uff0c\u6bd4\u6807\u51c6\u5bf9\u6297\u8bad\u7ec3\u63d0\u4f9b\u7ea63\u500d\u7684\u9c81\u68d2\u6027\u589e\u76ca\uff0c\u4e14\u4e0d\u964d\u4f4e\u5e72\u51c0\u6837\u672c\u51c6\u786e\u7387\u3002", "conclusion": "\u7b26\u53f7\u63a8\u7406\u4e3a\u6784\u5efa\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684AI\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u4fdd\u6301\u8f7b\u91cf\u67b6\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e0e\u590d\u6742Transformer\u9632\u5fa1\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.13190", "categories": ["cs.LG", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2601.13190", "abs": "https://arxiv.org/abs/2601.13190", "authors": ["Vittoria De Pellegrini", "Tariq Alkhalifah"], "title": "LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow Simulations", "comment": null, "summary": "Modeling and forecasting subsurface multiphase fluid flow fields underpin applications ranging from geological CO2 sequestration (GCS) operations to geothermal production. This is essential for ensuring both operational performance and long-term safety. While high fidelity multiphase simulators are widely used for this purpose, they become prohibitively expensive once many forward runs are required for inversion purposes and quantify uncertainty. To tackle this challenge we propose LAViG-FLOW, a latent autoregressive video generation diffusion framework that explicitly learns the coupled evolution of saturation and pressure fields. Each state variable is compressed by a dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their coupled distribution across time. We first train the model on a given time horizon to learn their coupled relationship and then fine-tune it autoregressively so it can extrapolate beyond the observed time window. Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates saturation and pressure fields that stay consistent across time while running orders of magnitude faster than traditional numerical solvers.", "AI": {"tldr": "LAViG-FLOW\uff1a\u4e00\u79cd\u7528\u4e8e\u5730\u4e0b\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u5efa\u6a21\u7684\u6f5c\u5728\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u751f\u6210\u9971\u548c\u5ea6\u4e0e\u538b\u529b\u573a\u7684\u8026\u5408\u6f14\u5316\uff0c\u6bd4\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u5730\u4e0b\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u5efa\u6a21\u5bf9CO2\u5730\u8d28\u5c01\u5b58\u548c\u5730\u70ed\u751f\u4ea7\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u5728\u53cd\u6f14\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLAViG-FLOW\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u4e13\u75282D\u81ea\u7f16\u7801\u5668\u538b\u7f29\u6bcf\u4e2a\u72b6\u6001\u53d8\u91cf\uff1b2\uff09\u7528\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff08VDiT\uff09\u5efa\u6a21\u65f6\u95f4\u4e0a\u7684\u8026\u5408\u5206\u5e03\uff1b3\uff09\u5148\u5728\u7ed9\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u8bad\u7ec3\u5b66\u4e60\u8026\u5408\u5173\u7cfb\uff0c\u7136\u540e\u81ea\u56de\u5f52\u5fae\u8c03\u4ee5\u8fdb\u884c\u65f6\u95f4\u5916\u63a8\u3002", "result": "\u5728\u5f00\u6e90CO2\u5c01\u5b58\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLAViG-FLOW\u751f\u6210\u7684\u9971\u548c\u5ea6\u4e0e\u538b\u529b\u573a\u5728\u65f6\u95f4\u4e0a\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u8fd0\u884c\u901f\u5ea6\u6bd4\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "LAViG-FLOW\u4e3a\u5730\u4e0b\u591a\u76f8\u6d41\u4f53\u6d41\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u52a0\u901f\u4e86\u53cd\u6f14\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8fc7\u7a0b\uff0c\u5728CO2\u5c01\u5b58\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.13243", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13243", "abs": "https://arxiv.org/abs/2601.13243", "authors": ["Yapeng Li", "Jiakuo Yu", "Zhixin Liu", "Xinnan Liu", "Jing Yu", "Songze Li", "Tonghua Su"], "title": "A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u63a8\u7406\u8303\u5f0f\uff08\u76f4\u63a5\u751f\u6210\u3001CoT\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\uff0c\u53d1\u73b0\u7ed3\u6784\u590d\u6742\u6027\u4e0d\u4e00\u5b9a\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5f00\u653e\u57fa\u51c6MIMeBench\u6765\u8bc4\u4f30\u8bed\u4e49\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u63a8\u7406\u7cfb\u7edf\u90e8\u7f72\u65f6\uff0c\u4e0d\u540c\u63a8\u7406\u8303\u5f0f\uff08\u5982CoT\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u7684\u76f8\u5bf9\u6709\u6548\u6027\u3001\u6210\u672c-\u51c6\u786e\u7387\u6743\u8861\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3\uff0c\u9700\u8981\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u6765\u6307\u5bfc\u8303\u5f0f\u9009\u62e9\u3002", "method": "1\uff09\u5728\u591a\u6837\u5316\u95ed\u5f0f\u57fa\u51c6\u4e0a\u7edf\u4e00\u8bc4\u4f30\u76f4\u63a5\u751f\u6210\u3001CoT\u589e\u5f3a\u5355\u6a21\u578b\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5de5\u4f5c\u6d41\uff1b2\uff09\u901a\u8fc7\u89d2\u8272\u9694\u79bb\u5206\u6790\u63a2\u7a76MAS\u4e2d\u89d2\u8272\u7279\u5b9a\u80fd\u529b\u9700\u6c42\uff1b3\uff09\u5206\u6790\u6210\u672c-\u51c6\u786e\u7387\u6743\u8861\uff1b4\uff09\u63d0\u51fa\u65b0\u5f00\u653e\u57fa\u51c6MIMeBench\u8bc4\u4f30\u8bed\u4e49\u62bd\u8c61\u548c\u5bf9\u6bd4\u8fa8\u522b\u80fd\u529b\u3002", "result": "1\uff09\u7ed3\u6784\u590d\u6742\u6027\u589e\u52a0\u4e0d\u4e00\u5b9a\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5176\u76ca\u5904\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u63a8\u7406\u8303\u5f0f\u672c\u8eab\u7279\u6027\u4e0e\u9002\u7528\u6027\uff1b2\uff09\u8bc6\u522b\u51fa\u54ea\u4e9bMAS\u5de5\u4f5c\u6d41\u5728\u6210\u672c\u4e0e\u51c6\u786e\u7387\u95f4\u8fbe\u5230\u6709\u5229\u5e73\u8861\uff0c\u54ea\u4e9b\u56e0\u8fb9\u9645\u6536\u76ca\u800c\u6210\u672c\u8fc7\u9ad8\uff1b3\uff09MIMeBench\u63d0\u4f9b\u4e86\u73b0\u6709\u57fa\u51c6\u96be\u4ee5\u6355\u6349\u7684\u8bed\u4e49\u80fd\u529b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "conclusion": "\u9700\u8981\u6839\u636e\u5177\u4f53\u4efb\u52a1\u7279\u6027\u9009\u62e9\u63a8\u7406\u8303\u5f0f\uff0c\u800c\u975e\u76f2\u76ee\u8ffd\u6c42\u590d\u6742\u7ed3\u6784\uff1bMIMeBench\u4e3aLLM\u8bed\u4e49\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7ef4\u5ea6\uff1b\u7814\u7a76\u4e3aLLM\u63a8\u7406\u7cfb\u7edf\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2601.13244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13244", "abs": "https://arxiv.org/abs/2601.13244", "authors": ["Prateek Munjal", "Clement Christophe", "Ronnie Rajan", "Praveenkumar Kanithi"], "title": "Do Instruction-Tuned Models Always Perform Better Than Base Models? Evidence from Math and Domain-Shifted Benchmarks", "comment": null, "summary": "Instruction finetuning is standard practice for improving LLM performance, yet it remains unclear whether it enhances reasoning or merely induces surface-level pattern matching. We investigate this by evaluating base and instruction-tuned models on standard math benchmarks, structurally perturbed variants, and domain-shifted tasks. Our analysis highlights two key (often overlooked) limitations of instruction tuning. First, the performance advantage is unstable and depends heavily on evaluation settings. In zero-shot CoT settings on GSM8K, base models consistently outperform instruction-tuned variants, with drops as high as 32.67\\% (Llama3-70B). Instruction-tuned models only match or exceed this performance when provided with few-shot exemplars, suggesting a reliance on specific prompting patterns rather than intrinsic reasoning. Second, tuning gains are brittle under distribution shift. Our results show that base models surpass instruction-tuned variants on the domain-specific MedCalc benchmark. Additionally, instruction-tuned models show sharp declines on perturbed datasets, indicating sensitivity to prompt structure over robust reasoning.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u5e76\u4e0d\u771f\u6b63\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u800c\u662f\u8ba9\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u7684\u63d0\u793a\u6a21\u5f0f\uff1b\u5728\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u548c\u5206\u5e03\u504f\u79fb\u4e0b\uff0c\u57fa\u7840\u6a21\u578b\u8868\u73b0\u66f4\u597d", "motivation": "\u7814\u7a76\u6307\u4ee4\u5fae\u8c03\u662f\u5426\u771f\u6b63\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u8bf1\u5bfc\u8868\u9762\u6a21\u5f0f\u5339\u914d\uff0c\u63ed\u793a\u6307\u4ee4\u5fae\u8c03\u7684\u5b9e\u9645\u6548\u679c\u548c\u5c40\u9650\u6027", "method": "\u5728\u6807\u51c6\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08GSM8K\uff09\u3001\u7ed3\u6784\u6270\u52a8\u53d8\u4f53\u548c\u9886\u57df\u8f6c\u79fb\u4efb\u52a1\u4e0a\u8bc4\u4f30\u57fa\u7840\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u5dee\u5f02", "result": "1. \u5728GSM8K\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u8bbe\u7f6e\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\u53d8\u4f53\uff08Llama3-70B\u4e0b\u964d\u9ad8\u8fbe32.67%\uff09\uff1b2. \u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4ec5\u5728\u63d0\u4f9b\u5c11\u6837\u672c\u793a\u4f8b\u65f6\u624d\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u7840\u6a21\u578b\uff1b3. \u5728MedCalc\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\u53d8\u4f53\uff1b4. \u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u5728\u6270\u52a8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6025\u5267\u4e0b\u964d", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u7684\u4f18\u52bf\u4e0d\u7a33\u5b9a\u4e14\u4f9d\u8d56\u8bc4\u4f30\u8bbe\u7f6e\uff0c\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u63d0\u793a\u6a21\u5f0f\u800c\u975e\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6027\u80fd\u8106\u5f31\uff0c\u8868\u660e\u6307\u4ee4\u5fae\u8c03\u5e76\u672a\u771f\u6b63\u589e\u5f3a\u63a8\u7406\u80fd\u529b"}}
{"id": "2601.13284", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13284", "abs": "https://arxiv.org/abs/2601.13284", "authors": ["Duygu Nur Yaldiz", "Evangelia Spiliopoulou", "Zheng Qi", "Siddharth Varia", "Srikanth Doss", "Nikolaos Pappas"], "title": "Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in decision-making tasks, where not only accuracy but also reliable confidence estimates are essential. Well-calibrated confidence enables downstream systems to decide when to trust a model and when to defer to fallback mechanisms. In this work, we conduct a systematic study of calibration in two widely used fine-tuning paradigms: supervised fine-tuning (SFT) and reinforcement learning with verifiable rewards (RLVR). We show that while RLVR improves task performance, it produces extremely overconfident models, whereas SFT yields substantially better calibration, even under distribution shift, though with smaller performance gains. Through targeted experiments, we diagnose RLVR's failure, showing that decision tokens act as extraction steps of the decision in reasoning traces and do not carry confidence information, which prevents reinforcement learning from surfacing calibrated alternatives. Based on this insight, we propose a calibration-aware reinforcement learning formulation that directly adjusts decision-token probabilities. Our method preserves RLVR's accuracy level while mitigating overconfidence, reducing ECE scores up to 9 points.", "AI": {"tldr": "RLVR\u5fae\u8c03\u5bfc\u81f4LLM\u8fc7\u5ea6\u81ea\u4fe1\uff0cSFT\u6821\u51c6\u66f4\u597d\u4f46\u6027\u80fd\u63d0\u5347\u8f83\u5c0f\uff1b\u63d0\u51fa\u6821\u51c6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u4fdd\u6301\u6027\u80fd\u540c\u65f6\u6539\u5584\u6821\u51c6", "motivation": "LLM\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u9700\u8981\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u4f46\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1aRLVR\u63d0\u9ad8\u6027\u80fd\u4f46\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff0cSFT\u6821\u51c6\u66f4\u597d\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650", "method": "\u7cfb\u7edf\u7814\u7a76SFT\u548cRLVR\u7684\u6821\u51c6\u7279\u6027\uff1b\u8bca\u65adRLVR\u5931\u8d25\u539f\u56e0\uff08\u51b3\u7b56token\u4f5c\u4e3a\u63d0\u53d6\u6b65\u9aa4\u4e0d\u643a\u5e26\u7f6e\u4fe1\u4fe1\u606f\uff09\uff1b\u63d0\u51fa\u6821\u51c6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u76f4\u63a5\u8c03\u6574\u51b3\u7b56token\u6982\u7387", "result": "RLVR\u4ea7\u751f\u6781\u5ea6\u8fc7\u5ea6\u81ea\u4fe1\u6a21\u578b\uff0cSFT\u5728\u6821\u51c6\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff08\u5373\u4f7f\u5728\u5206\u5e03\u504f\u79fb\u4e0b\uff09\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u4fdd\u6301RLVR\u51c6\u786e\u6027\u540c\u65f6\u663e\u8457\u6539\u5584\u6821\u51c6\uff0cECE\u5206\u6570\u964d\u4f4e\u6700\u591a9\u70b9", "conclusion": "RLVR\u867d\u7136\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u4f46\u635f\u5bb3\u6821\u51c6\uff0cSFT\u63d0\u4f9b\u66f4\u597d\u6821\u51c6\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\uff1b\u901a\u8fc7\u6821\u51c6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u89e3\u51b3\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898"}}
{"id": "2601.13295", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2601.13295", "abs": "https://arxiv.org/abs/2601.13295", "authors": ["Arpandeep Khatua", "Hao Zhu", "Peter Tran", "Arya Prabhudesai", "Frederic Sadrieh", "Johann K. Lieberwirth", "Xinkai Yu", "Yicheng Fu", "Michael J. Ryan", "Jiaxin Pei", "Diyi Yang"], "title": "CooperBench: Why Coding Agents Cannot be Your Teammates Yet", "comment": "https://cooperbench.com", "summary": "Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CooperBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30AI\u4ee3\u7406\u5728\u534f\u4f5c\u7f16\u7a0b\u4e2d\u7684\u534f\u8c03\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u4ee3\u7406\u5b58\u5728\"\u534f\u8c03\u8bc5\u5492\"\u2014\u2014\u534f\u4f5c\u65f6\u6210\u529f\u7387\u6bd4\u5355\u72ec\u6267\u884c\u4f4e30%\uff0c\u63ed\u793a\u4e86\u6c9f\u901a\u3001\u627f\u8bfa\u548c\u671f\u671b\u7406\u89e3\u4e09\u5927\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u590d\u6742\u5de5\u4f5c\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u534f\u4f5c\uff0c\u5b83\u4eec\u9700\u8981\u53d1\u5c55\u534f\u8c03\u80fd\u529b\u6210\u4e3a\u6709\u6548\u7684\u56e2\u961f\u6210\u5458\u3002\u4f46\u4f5c\u8005\u5047\u8bbe\u5f53\u524d\u4ee3\u7406\u7f3a\u4e4f\u8fd9\u4e9b\u80fd\u529b\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165CooperBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b600\u591a\u4e2a\u534f\u4f5c\u7f16\u7a0b\u4efb\u52a1\uff0c\u6d89\u53ca12\u4e2a\u5e93\u548c4\u79cd\u7f16\u7a0b\u8bed\u8a00\u3002\u6bcf\u4e2a\u4efb\u52a1\u5206\u914d\u4e24\u4e2a\u4ee3\u7406\u4e0d\u540c\u7684\u529f\u80fd\uff0c\u8fd9\u4e9b\u529f\u80fd\u53ef\u4ee5\u72ec\u7acb\u5b9e\u73b0\u4f46\u9700\u8981\u534f\u8c03\u907f\u514d\u51b2\u7a81\u3002\u4efb\u52a1\u57fa\u4e8e\u771f\u5b9e\u5f00\u6e90\u4ed3\u5e93\u548c\u4e13\u5bb6\u7f16\u5199\u7684\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\"\u534f\u8c03\u8bc5\u5492\"\u73b0\u8c61\uff1a\u4ee3\u7406\u534f\u4f5c\u65f6\u7684\u5e73\u5747\u6210\u529f\u7387\u6bd4\u5355\u72ec\u6267\u884c\u4e24\u4e2a\u4efb\u52a1\u4f4e30%\u3002\u8fd9\u4e0e\u4eba\u7c7b\u56e2\u961f\u901a\u5e38\u56e0\u589e\u52a0\u961f\u53cb\u800c\u63d0\u9ad8\u751f\u4ea7\u529b\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u6c9f\u901a\u6e20\u9053\u5835\u585e\u3001\u4ee3\u7406\u504f\u79bb\u627f\u8bfa\u3001\u4ee3\u7406\u5bf9\u4ed6\u4eba\u8ba1\u5212\u548c\u6c9f\u901a\u6709\u9519\u8bef\u671f\u671b\u3002\u540c\u65f6\u89c2\u5bdf\u5230\u7f55\u89c1\u7684\u6d8c\u73b0\u534f\u8c03\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u534f\u4f5c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u547c\u5401\u4ece\u8ffd\u6c42\u4e2a\u4f53\u4ee3\u7406\u80fd\u529b\u8f6c\u5411\u53d1\u5c55\u793e\u4f1a\u667a\u80fd\uff0c\u5f3a\u8c03\u534f\u8c03\u80fd\u529b\u5bf9AI\u56e2\u961f\u534f\u4f5c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.13303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13303", "abs": "https://arxiv.org/abs/2601.13303", "authors": ["Minh Le", "Phuong Cao"], "title": "Verifying Local Robustness of Pruned Safety-Critical Networks", "comment": null, "summary": "Formal verification of Deep Neural Networks (DNNs) is essential for safety-critical applications, ranging from surgical robotics to NASA JPL autonomous systems. However, the computational cost of verifying large-scale models remains a significant barrier to adoption. This paper investigates the impact of pruning on formal local robustness certificates with different ratios. Using the state-of-the-art $\u03b1,\u03b2$-CROWN verifier, we evaluate ResNet4 models across varying pruning ratios on MNIST and, more importantly, on the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a non-linear relationship: light pruning (40%) in MNIST and heavy pruning (70%-90%) in JPL improve verifiability, allowing models to outperform unpruned baselines in proven $L_\\infty$ robustness properties. This suggests that reduced connectivity simplifies the search space for formal solvers and that the optimal pruning ratio varies significantly between datasets. This research highlights the complex nature of model compression, offering critical insights into selecting the optimal pruning ratio for deploying efficient, yet formally verified, DNNs in high-stakes environments where reliability is non-negotiable.", "AI": {"tldr": "\u7814\u7a76\u526a\u679d\u5bf9\u795e\u7ecf\u7f51\u7edc\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8f7b\u526a\u679d\uff08MNIST 40%\uff09\u548c\u91cd\u526a\u679d\uff08JPL 70-90%\uff09\u80fd\u63d0\u5347\u53ef\u9a8c\u8bc1\u6027\uff0c\u6700\u4f18\u526a\u679d\u6bd4\u4f8b\u56e0\u6570\u636e\u96c6\u800c\u5f02\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u63a2\u7d22\u6a21\u578b\u538b\u7f29\uff08\u5982\u526a\u679d\uff09\u5982\u4f55\u5f71\u54cd\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u03b1,\u03b2-CROWN\u9a8c\u8bc1\u5668\uff0c\u5728\u4e0d\u540c\u526a\u679d\u6bd4\u4f8b\u4e0b\u8bc4\u4f30ResNet4\u6a21\u578b\uff0c\u5728MNIST\u548cNASA JPL\u706b\u661f\u971c\u51bb\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u975e\u7ebf\u6027\u5173\u7cfb\uff1aMNIST\u4e0a\u8f7b\u526a\u679d\uff0840%\uff09\u548cJPL\u6570\u636e\u96c6\u4e0a\u91cd\u526a\u679d\uff0870-90%\uff09\u80fd\u6539\u5584\u53ef\u9a8c\u8bc1\u6027\uff0c\u4f7f\u6a21\u578b\u5728L\u221e\u9c81\u68d2\u6027\u8bc1\u660e\u4e0a\u4f18\u4e8e\u672a\u526a\u679d\u57fa\u7ebf\u3002", "conclusion": "\u51cf\u5c11\u8fde\u63a5\u6027\u7b80\u5316\u4e86\u5f62\u5f0f\u5316\u6c42\u89e3\u5668\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u6700\u4f18\u526a\u679d\u6bd4\u4f8b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u5dee\u5f02\u663e\u8457\u3002\u8fd9\u4e3a\u5728\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6548\u4e14\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684DNN\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2601.13350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13350", "abs": "https://arxiv.org/abs/2601.13350", "authors": ["Abdel Djalil Sad Saoud", "Fred Maurice Ngol\u00e8 Mboula", "Hanane Slimani"], "title": "Beyond Mapping : Domain-Invariant Representations via Spectral Embedding of Optimal Transport Plans", "comment": "5 pages, 2 figures", "summary": "Distributional shifts between training and inference time data remain a central challenge in machine learning, often leading to poor performance. It motivated the study of principled approaches for domain alignment, such as optimal transport based unsupervised domain adaptation, that relies on approximating Monge map using transport plans, which is sensitive to the transport problem regularization strategy and hyperparameters, and might yield biased domains alignment. In this work, we propose to interpret smoothed transport plans as adjacency matrices of bipartite graphs connecting source to target domain and derive domain-invariant samples' representations through spectral embedding. We evaluate our approach on acoustic adaptation benchmarks for music genre recognition, music-speech discrimination, as well as electrical cable defect detection and classification tasks using time domain reflection in different diagnosis settings, achieving overall strong performances.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8c31\u5d4c\u5165\u7684\u9886\u57df\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5e73\u6ed1\u4f20\u8f93\u8ba1\u5212\u89e3\u91ca\u4e3a\u4e8c\u5206\u56fe\u90bb\u63a5\u77e9\u9635\uff0c\u5728\u591a\u4e2a\u97f3\u9891\u548c\u7535\u7f06\u7f3a\u9677\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd", "motivation": "\u8bad\u7ec3\u548c\u63a8\u7406\u6570\u636e\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u65e0\u76d1\u7763\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56Monge\u6620\u5c04\u8fd1\u4f3c\uff0c\u5bf9\u6b63\u5219\u5316\u7b56\u7565\u548c\u8d85\u53c2\u6570\u654f\u611f\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u504f\u7684\u9886\u57df\u5bf9\u9f50", "method": "\u5c06\u5e73\u6ed1\u4f20\u8f93\u8ba1\u5212\u89e3\u91ca\u4e3a\u8fde\u63a5\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u4e8c\u5206\u56fe\u90bb\u63a5\u77e9\u9635\uff0c\u901a\u8fc7\u8c31\u5d4c\u5165\u63a8\u5bfc\u9886\u57df\u4e0d\u53d8\u7684\u6837\u672c\u8868\u793a", "result": "\u5728\u97f3\u4e50\u6d41\u6d3e\u8bc6\u522b\u3001\u97f3\u4e50-\u8bed\u97f3\u533a\u5206\u4ee5\u53ca\u7535\u7f06\u7f3a\u9677\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0c\u5728\u4e0d\u540c\u8bca\u65ad\u8bbe\u7f6e\u4e0b\u4f7f\u7528\u65f6\u57df\u53cd\u5c04\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u6574\u4f53\u5f3a\u52b2\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u8c31\u5d4c\u5165\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u514b\u670d\u4f20\u7edf\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u5bf9\u6b63\u5219\u5316\u548c\u8d85\u53c2\u6570\u7684\u654f\u611f\u6027\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2601.13365", "categories": ["cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2601.13365", "abs": "https://arxiv.org/abs/2601.13365", "authors": ["Kevin Slote", "Jeremie Fish", "Erik Bollt"], "title": "CausationEntropy: Pythonic Optimal Causation Entropy", "comment": null, "summary": "Optimal Causation Entropy (oCSE) is a robust causal network modeling technique that reveals causal networks from dynamical systems and coupled oscillators, distinguishing direct from indirect paths. CausationEntropy is a Python package that implements oCSE and several of its significant optimizations and methodological extensions. In this paper, we introduce the version 1.1 release of CausationEntropy, which includes new synthetic data generators, plotting tools, and several advanced information-theoretical causal network discovery algorithms with criteria for estimating Gaussian, k-nearest neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density (KDE) and Poisson entropic estimators. The package is easy to install from the PyPi software repository, is thoroughly documented, supplemented with extensive code examples, and is modularly structured to support future additions. The entire codebase is released under the MIT license and is available on GitHub and through PyPi Repository. We expect this package to serve as a benchmark tool for causal discovery in complex dynamical systems.", "AI": {"tldr": "CausationEntropy v1.1\u662f\u4e00\u4e2aPython\u5305\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u56e0\u679c\u71b5\uff08oCSE\uff09\u65b9\u6cd5\u53ca\u5176\u4f18\u5316\u6269\u5c55\uff0c\u7528\u4e8e\u4ece\u52a8\u6001\u7cfb\u7edf\u548c\u8026\u5408\u632f\u8361\u5668\u4e2d\u63ed\u793a\u56e0\u679c\u7f51\u7edc\uff0c\u533a\u5206\u76f4\u63a5\u4e0e\u95f4\u63a5\u8def\u5f84\u3002", "motivation": "\u9700\u8981\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e00\u4e2a\u57fa\u51c6\u5de5\u5177\uff0c\u5b9e\u73b0oCSE\u65b9\u6cd5\u53ca\u5176\u591a\u79cd\u4f18\u5316\u548c\u6269\u5c55\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u65b9\u4fbf\u5730\u8fdb\u884c\u56e0\u679c\u7f51\u7edc\u5efa\u6a21\u3002", "method": "\u5b9e\u73b0\u4e86oCSE\u7b97\u6cd5\u53ca\u5176\u4f18\u5316\uff0c\u5305\u542b\u591a\u79cd\u4fe1\u606f\u8bba\u56e0\u679c\u7f51\u7edc\u53d1\u73b0\u7b97\u6cd5\uff0c\u652f\u6301\u9ad8\u65af\u3001k\u8fd1\u90bb\u3001\u51e0\u4f55k\u8fd1\u90bb\u3001\u6838\u5bc6\u5ea6\u4f30\u8ba1\u548c\u6cca\u677e\u71b5\u4f30\u8ba1\u5668\uff0c\u5e76\u63d0\u4f9b\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u548c\u7ed8\u56fe\u5de5\u5177\u3002", "result": "\u53d1\u5e03\u4e86CausationEntropy v1.1\u5305\uff0c\u53ef\u901a\u8fc7PyPi\u5b89\u88c5\uff0c\u5177\u6709\u5b8c\u6574\u6587\u6863\u548c\u4ee3\u7801\u793a\u4f8b\uff0c\u91c7\u7528\u6a21\u5757\u5316\u7ed3\u6784\u652f\u6301\u672a\u6765\u6269\u5c55\uff0c\u4ee3\u7801\u5728MIT\u8bb8\u53ef\u4e0b\u5f00\u6e90\u4e8eGitHub\u548cPyPi\u3002", "conclusion": "\u8be5\u5305\u5c06\u6210\u4e3a\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u4e2d\u56e0\u679c\u53d1\u73b0\u7684\u57fa\u51c6\u5de5\u5177\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u5f3a\u5927\u4e14\u6613\u7528\u7684\u56e0\u679c\u7f51\u7edc\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13398", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.13398", "abs": "https://arxiv.org/abs/2601.13398", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "title": "Can LLMs Compress (and Decompress)? Evaluating Code Understanding and Execution via Invertibility", "comment": "32 pages (preprint)", "summary": "LLMs demonstrate strong performance on code benchmarks, yet round-trip code execution reveals limitations in their ability to maintain consistent reasoning across forward and backward execution. We present RoundTripCodeEval (RTCE), a comprehensive benchmark consisting of four distinct code execution reasoning tasks designed to rigorously test round-trip consistency. RTCE provides an execution-free, exact-match evaluation of bijection fidelity, assessing whether models preserve a consistent one-to-one mapping between encoding and decoding operations across various algorithms and directions. We systematically evaluate state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on execution traces, and self-reflection mechanisms. Each yields modest improvements, but none closes the gap, indicating that current LLMs struggle with true round-trip consistency, which demonstrates that they lack the internal coherence required for trustworthy code reasoning. RTCE surfaces several new and previously unmeasured insights that are not captured by existing I/O-prediction, execution-reasoning, or round-trip natural-language benchmarks. We will release the code and the dataset upon acceptance.", "AI": {"tldr": "RTCE\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793aLLMs\u5728\u4ee3\u7801\u5f80\u8fd4\u6267\u884c\u4e2d\u7f3a\u4e4f\u4e00\u81f4\u6027\u63a8\u7406\u80fd\u529b\uff0c\u5373\u4f7f\u91c7\u7528\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u4ecd\u65e0\u6cd5\u89e3\u51b3\u6b64\u95ee\u9898", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5f80\u8fd4\u4ee3\u7801\u6267\u884c\u4e2d\u4fdd\u6301\u4e00\u81f4\u6027\u63a8\u7406\u7684\u80fd\u529b\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u63d0\u51faRoundTripCodeEval(RTCE)\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u4ee3\u7801\u6267\u884c\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u63d0\u793a\u3001\u76d1\u7763\u5fae\u8c03\u548c\u81ea\u53cd\u601d\u673a\u5236\u8bc4\u4f30\u6a21\u578b", "result": "\u6240\u6709\u65b9\u6cd5\u4ec5\u5e26\u6765\u6709\u9650\u6539\u8fdb\uff0c\u65e0\u6cd5\u89e3\u51b3\u5f80\u8fd4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u8868\u660e\u5f53\u524dLLMs\u7f3a\u4e4f\u53ef\u4fe1\u4ee3\u7801\u63a8\u7406\u6240\u9700\u7684\u5185\u5728\u4e00\u81f4\u6027", "conclusion": "RTCE\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u51c6\u672a\u6355\u6349\u5230\u7684\u65b0\u89c1\u89e3\uff0c\u8868\u660eLLMs\u5728\u4ee3\u7801\u5f80\u8fd4\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677"}}
{"id": "2601.13422", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13422", "abs": "https://arxiv.org/abs/2601.13422", "authors": ["Dahai Yu", "Rongchao Xu", "Dingyi Zhuang", "Yuheng Bu", "Shenhao Wang", "Guang Wang"], "title": "TrustEnergy: A Unified Framework for Accurate and Reliable User-level Energy Usage Prediction", "comment": null, "summary": "Energy usage prediction is important for various real-world applications, including grid management, infrastructure planning, and disaster response. Although a plethora of deep learning approaches have been proposed to perform this task, most of them either overlook the essential spatial correlations across households or fail to scale to individualized prediction, making them less effective for accurate fine-grained user-level prediction. In addition, due to the dynamic and uncertain nature of energy usage caused by various factors such as extreme weather events, quantifying uncertainty for reliable prediction is also significant, but it has not been fully explored in existing work. In this paper, we propose a unified framework called TrustEnergy for accurate and reliable user-level energy usage prediction. There are two key technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal Representation module to efficiently capture both macro and micro energy usage patterns with a novel memory-augmented spatiotemporal graph neural network, and (ii) an innovative Sequential Conformalized Quantile Regression module to dynamically adjust uncertainty bounds to ensure valid prediction intervals over time, without making strong assumptions about the underlying data distribution. We implement and evaluate our TrustEnergy framework by working with an electricity provider in Florida, and the results show our TrustEnergy can achieve a 5.4% increase in prediction accuracy and 5.7% improvement in uncertainty quantification compared to state-of-the-art baselines.", "AI": {"tldr": "TrustEnergy\uff1a\u4e00\u4e2a\u7528\u4e8e\u51c6\u786e\u53ef\u9760\u7528\u6237\u7ea7\u80fd\u6e90\u4f7f\u7528\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u65f6\u7a7a\u8868\u793a\u548c\u987a\u5e8f\u4fdd\u5f62\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u80fd\u6e90\u4f7f\u7528\u9884\u6d4b\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u5bb6\u5ead\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u4e2a\u4f53\u5316\u9884\u6d4b\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7ec6\u7c92\u5ea6\u7528\u6237\u7ea7\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51faTrustEnergy\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\u7ec4\u4ef6\uff1a(1)\u5206\u5c42\u65f6\u7a7a\u8868\u793a\u6a21\u5757\uff0c\u4f7f\u7528\u65b0\u578b\u8bb0\u5fc6\u589e\u5f3a\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5b8f\u89c2\u548c\u5fae\u89c2\u80fd\u6e90\u4f7f\u7528\u6a21\u5f0f\uff1b(2)\u987a\u5e8f\u4fdd\u5f62\u5206\u4f4d\u6570\u56de\u5f52\u6a21\u5757\uff0c\u52a8\u6001\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u786e\u4fdd\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u6709\u6548\u9884\u6d4b\u533a\u95f4\uff0c\u65e0\u9700\u5bf9\u5e95\u5c42\u6570\u636e\u5206\u5e03\u505a\u5f3a\u5047\u8bbe\u3002", "result": "\u4e0e\u4f5b\u7f57\u91cc\u8fbe\u5dde\u7535\u529b\u4f9b\u5e94\u5546\u5408\u4f5c\u5b9e\u65bd\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aTrustEnergy\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad85.4%\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6539\u8fdb5.7%\u3002", "conclusion": "TrustEnergy\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7528\u6237\u7ea7\u80fd\u6e90\u4f7f\u7528\u9884\u6d4b\u4e2d\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u6355\u6349\u3001\u4e2a\u4f53\u5316\u9884\u6d4b\u6269\u5c55\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u7535\u7f51\u7ba1\u7406\u3001\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u548c\u707e\u5bb3\u54cd\u5e94\u7b49\u5e94\u7528\u63d0\u4f9b\u51c6\u786e\u53ef\u9760\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2601.13435", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13435", "abs": "https://arxiv.org/abs/2601.13435", "authors": ["Shuozhe Li", "Du Cheng", "Leqi Liu"], "title": "A Learnable Wavelet Transformer for Long-Short Equity Trading and Risk-Adjusted Return Optimization", "comment": null, "summary": "Learning profitable intraday trading policies from financial time series is challenging due to heavy noise, non-stationarity, and strong cross-sectional dependence among related assets. We propose \\emph{WaveLSFormer}, a learnable wavelet-based long-short Transformer that jointly performs multi-scale decomposition and return-oriented decision learning. Specifically, a learnable wavelet front-end generates low-/high-frequency components via an end-to-end trained filter bank, guided by spectral regularizers that encourage stable and well-separated frequency bands. To fuse multi-scale information, we introduce a low-guided high-frequency injection (LGHI) module that refines low-frequency representations with high-frequency cues while controlling training stability. The model outputs a portfolio of long/short positions that is rescaled to satisfy a fixed risk budget, and is optimized directly with a trading objective and risk-aware regularization. Extensive experiments on five years of hourly data across six industry groups, evaluated over ten random seeds, demonstrate that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones, with and without fixed discrete wavelet front-ends. On average in all industries, WaveLSFormer achieves a cumulative overall strategy return of $0.607 \\pm 0.045$ and a Sharpe ratio of $2.157 \\pm 0.166$, substantially improving both profitability and risk-adjusted returns over the strongest baselines.", "AI": {"tldr": "WaveLSFormer\uff1a\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u5c0f\u6ce2\u57fa\u957f\u77ed\u671fTransformer\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u5c0f\u6ce2\u524d\u7aef\u8fdb\u884c\u591a\u5c3a\u5ea6\u5206\u89e3\uff0c\u7ed3\u5408\u4f4e\u9891\u5f15\u5bfc\u9ad8\u9891\u6ce8\u5165\u6a21\u5757\uff0c\u76f4\u63a5\u4f18\u5316\u4ea4\u6613\u76ee\u6807\u548c\u98ce\u9669\u611f\u77e5\u6b63\u5219\u5316\uff0c\u5728\u65e5\u5185\u4ea4\u6613\u4e2d\u663e\u8457\u63d0\u5347\u76c8\u5229\u80fd\u529b\u548c\u98ce\u9669\u8c03\u6574\u6536\u76ca\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u65e5\u5185\u4ea4\u6613\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a1\uff09\u566a\u58f0\u4e25\u91cd\uff1b2\uff09\u975e\u5e73\u7a33\u6027\uff1b3\uff09\u76f8\u5173\u8d44\u4ea7\u95f4\u7684\u5f3a\u6a2a\u622a\u9762\u4f9d\u8d56\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u7279\u6027\u3002", "method": "1\uff09\u53ef\u5b66\u4e60\u5c0f\u6ce2\u524d\u7aef\uff1a\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u6ee4\u6ce2\u5668\u7ec4\u751f\u6210\u4f4e\u9891/\u9ad8\u9891\u5206\u91cf\uff0c\u4f7f\u7528\u9891\u8c31\u6b63\u5219\u5316\u5668\u9f13\u52b1\u7a33\u5b9a\u4e14\u5206\u79bb\u826f\u597d\u7684\u9891\u5e26\uff1b2\uff09\u4f4e\u9891\u5f15\u5bfc\u9ad8\u9891\u6ce8\u5165\uff08LGHI\uff09\u6a21\u5757\uff1a\u7528\u9ad8\u9891\u7ebf\u7d22\u7cbe\u70bc\u4f4e\u9891\u8868\u793a\uff0c\u540c\u65f6\u63a7\u5236\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b3\uff09\u98ce\u9669\u9884\u7b97\u7ea6\u675f\uff1a\u8f93\u51fa\u957f/\u77ed\u5934\u5bf8\u7ec4\u5408\uff0c\u6309\u56fa\u5b9a\u98ce\u9669\u9884\u7b97\u91cd\u65b0\u7f29\u653e\uff1b4\uff09\u76f4\u63a5\u4f18\u5316\uff1a\u4f7f\u7528\u4ea4\u6613\u76ee\u6807\u548c\u98ce\u9669\u611f\u77e5\u6b63\u5219\u5316\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u57285\u5e74\u6bcf\u5c0f\u65f6\u6570\u636e\u30016\u4e2a\u884c\u4e1a\u7ec4\u300110\u4e2a\u968f\u673a\u79cd\u5b50\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cWaveLSFormer\u59cb\u7ec8\u4f18\u4e8eMLP\u3001LSTM\u548cTransformer\u57fa\u7ebf\uff08\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u56fa\u5b9a\u79bb\u6563\u5c0f\u6ce2\u524d\u7aef\uff09\u3002\u5728\u6240\u6709\u884c\u4e1a\u4e2d\u5e73\u5747\u5b9e\u73b0\u7d2f\u8ba1\u603b\u7b56\u7565\u56de\u62a50.607\u00b10.045\uff0c\u590f\u666e\u6bd4\u73872.157\u00b10.166\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76c8\u5229\u80fd\u529b\u548c\u98ce\u9669\u8c03\u6574\u6536\u76ca\u3002", "conclusion": "WaveLSFormer\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5c0f\u6ce2\u5206\u89e3\u548c\u4f4e\u9891\u5f15\u5bfc\u9ad8\u9891\u6ce8\u5165\u673a\u5236\uff0c\u6709\u6548\u5904\u7406\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u566a\u58f0\u3001\u975e\u5e73\u7a33\u6027\u548c\u6a2a\u622a\u9762\u4f9d\u8d56\u6027\uff0c\u5728\u65e5\u5185\u4ea4\u6613\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u5c3a\u5ea6\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2601.13445", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.13445", "abs": "https://arxiv.org/abs/2601.13445", "authors": ["Ashish S. Nair", "Sandipp Krishnan Ravi", "Itzel Salgado", "Changjie Sun", "Sayan Ghosh", "Liping Wang"], "title": "BladeSDF : Unconditional and Conditional Generative Modeling of Representative Blade Geometries Using Signed Distance Functions", "comment": null, "summary": "Generative AI has emerged as a transformative paradigm in engineering design, enabling automated synthesis and reconstruction of complex 3D geometries while preserving feasibility and performance relevance. This paper introduces a domain-specific implicit generative framework for turbine blade geometry using DeepSDF, addressing critical gaps in performance-aware modeling and manufacturable design generation. The proposed method leverages a continuous signed distance function (SDF) representation to reconstruct and generate smooth, watertight geometries with quantified accuracy. It establishes an interpretable, near-Gaussian latent space that aligns with blade-relevant parameters, such as taper and chord ratios, enabling controlled exploration and unconditional synthesis through interpolation and Gaussian sampling. In addition, a compact neural network maps engineering descriptors, such as maximum directional strains, to latent codes, facilitating the generation of performance-informed geometry. The framework achieves high reconstruction fidelity, with surface distance errors concentrated within $1\\%$ of the maximum blade dimension, and demonstrates robust generalization to unseen designs. By integrating constraints, objectives, and performance metrics, this approach advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a practical and interpretable solution for data-driven turbine blade modeling and concept generation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eDeepSDF\u7684\u6da1\u8f6e\u53f6\u7247\u9886\u57df\u7279\u5b9a\u9690\u5f0f\u751f\u6210\u6846\u67b6\uff0c\u5b9e\u73b0\u6027\u80fd\u611f\u77e5\u7684\u53ef\u5236\u9020\u8bbe\u8ba1\u751f\u6210\uff0c\u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u5de5\u7a0b\u63cf\u8ff0\u7b26\u6620\u5c04\u5b9e\u73b0\u6027\u80fd\u5f15\u5bfc\u7684\u51e0\u4f55\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0fAI\u5728\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u5b58\u5728\u6027\u80fd\u611f\u77e5\u5efa\u6a21\u548c\u53ef\u5236\u9020\u8bbe\u8ba1\u751f\u6210\u7684\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u53ef\u884c\u6027\u3001\u6027\u80fd\u76f8\u5173\u6027\u7684\u6da1\u8f6e\u53f6\u7247\u51e0\u4f55\u81ea\u52a8\u5408\u6210\u4e0e\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8fde\u7eed\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570(SDF)\u8868\u793a\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u8fd1\u4f3c\u9ad8\u65af\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u63d2\u503c\u548c\u9ad8\u65af\u91c7\u6837\u5b9e\u73b0\u53ef\u63a7\u63a2\u7d22\u548c\u65e0\u6761\u4ef6\u5408\u6210\uff1b\u4f7f\u7528\u7d27\u51d1\u795e\u7ecf\u7f51\u7edc\u5c06\u5de5\u7a0b\u63cf\u8ff0\u7b26\uff08\u5982\u6700\u5927\u65b9\u5411\u5e94\u53d8\uff09\u6620\u5c04\u5230\u6f5c\u5728\u4ee3\u7801\u3002", "result": "\u5b9e\u73b0\u9ad8\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u8868\u9762\u8ddd\u79bb\u8bef\u5dee\u96c6\u4e2d\u5728\u6700\u5927\u53f6\u7247\u5c3a\u5bf8\u76841%\u4ee5\u5185\uff1b\u5bf9\u672a\u89c1\u8bbe\u8ba1\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff1b\u8d85\u8d8a\u4f20\u7edf2D\u5f15\u5bfc\u6216\u65e0\u7ea6\u675f3D\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u7ea6\u675f\u3001\u76ee\u6807\u548c\u6027\u80fd\u6307\u6807\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u6da1\u8f6e\u53f6\u7247\u5efa\u6a21\u548c\u6982\u5ff5\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u8fdb\u4e86\u5de5\u7a0b\u8bbe\u8ba1\u7684\u751f\u6210\u5f0fAI\u5e94\u7528\u3002"}}
{"id": "2601.13456", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.13456", "abs": "https://arxiv.org/abs/2601.13456", "authors": ["Sahasra Kokkula", "Daniel David", "Aaditya Baruah"], "title": "Federated Learning Under Temporal Drift -- Mitigating Catastrophic Forgetting via Experience Replay", "comment": "8 pages, 5 figures. Course project for Neural Networks & Deep Learning COMSW4776 course at Columbia University", "summary": "Federated Learning struggles under temporal concept drift where client data distributions shift over time. We demonstrate that standard FedAvg suffers catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy dropping from 74% to 28%. We propose client-side experience replay, where each client maintains a small buffer of past samples mixed with current data during local training. This simple approach requires no changes to server aggregation. Experiments show that a 50-sample-per-class buffer restores performance to 78-82%, effectively preventing forgetting. Our ablation study reveals a clear memory-accuracy trade-off as buffer size increases.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u968f\u65f6\u95f4\u53d8\u5316\u65f6\uff0c\u6807\u51c6FedAvg\u4f1a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u901a\u8fc7\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\uff08\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u5c11\u91cf\u5386\u53f2\u6837\u672c\u7f13\u51b2\u533a\uff09\uff0c\u65e0\u9700\u4fee\u6539\u670d\u52a1\u5668\u805a\u5408\u5373\u53ef\u6709\u6548\u9632\u6b62\u9057\u5fd8\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9762\u4e34\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u968f\u65f6\u95f4\u53d8\u5316\u4f1a\u5bfc\u81f4\u6a21\u578b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u4e00\u4e2a\u5c0f\u578b\u7f13\u51b2\u533a\u5b58\u50a8\u8fc7\u53bb\u6837\u672c\uff0c\u5728\u672c\u5730\u8bad\u7ec3\u65f6\u5c06\u5386\u53f2\u6837\u672c\u4e0e\u5f53\u524d\u6570\u636e\u6df7\u5408\u4f7f\u7528\uff0c\u65e0\u9700\u4fee\u6539\u670d\u52a1\u5668\u805a\u5408\u673a\u5236\u3002", "result": "\u5728Fashion-MNIST\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6807\u51c6FedAvg\u5728\u5b63\u8282\u6027\u6f02\u79fb\u4e0b\u51c6\u786e\u7387\u4ece74%\u964d\u81f328%\u3002\u4f7f\u7528\u6bcf\u7c7b50\u4e2a\u6837\u672c\u7684\u7f13\u51b2\u533a\u540e\uff0c\u6027\u80fd\u6062\u590d\u523078-82%\uff0c\u6709\u6548\u9632\u6b62\u4e86\u9057\u5fd8\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u5185\u5b58\u4e0e\u51c6\u786e\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u5ba2\u6237\u7aef\u7ecf\u9a8c\u56de\u653e\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u670d\u52a1\u5668\u805a\u5408\u7684\u60c5\u51b5\u4e0b\u9632\u6b62\u65f6\u95f4\u6982\u5ff5\u6f02\u79fb\u5bfc\u81f4\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u901a\u8fc7\u9002\u5ea6\u7684\u5185\u5b58\u5f00\u9500\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.13463", "categories": ["cs.LG", "hep-ph", "nucl-th", "quant-ph"], "pdf": "https://arxiv.org/pdf/2601.13463", "abs": "https://arxiv.org/abs/2601.13463", "authors": ["Brandon B. Le", "D. Keller"], "title": "Quantum Qualifiers for Neural Network Model Selection in Hadronic Physics", "comment": "12 pages, 5 figures. Proceedings for the 26th International Symposium on Spin Physics (SPIN2025), September 21-26, 2025; Qingdao, Shandong, China", "summary": "As quantum machine-learning architectures mature, a central challenge is no longer their construction, but identifying the regimes in which they offer practical advantages over classical approaches. In this work, we introduce a framework for addressing this question in data-driven hadronic physics problems by developing diagnostic tools - centered on a quantitative quantum qualifier - that guide model selection between classical and quantum deep neural networks based on intrinsic properties of the data. Using controlled classification and regression studies, we show how relative model performance follows systematic trends in complexity, noise, and dimensionality, and how these trends can be distilled into a predictive criterion. We then demonstrate the utility of this approach through an application to Compton form factor extraction from deeply virtual Compton scattering, where the quantum qualifier identifies kinematic regimes favorable to quantum models. Together, these results establish a principled framework for deploying quantum machine-learning tools in precision hadronic physics.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u5f3a\u5b50\u7269\u7406\u4e2d\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5b50\u8d28\u91cf\u6307\u6807\u6307\u5bfc\u91cf\u5b50\u4e0e\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u7684\u9009\u62e9\uff0c\u5e76\u5728\u5eb7\u666e\u987f\u5f62\u72b6\u56e0\u5b50\u63d0\u53d6\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u67b6\u6784\u7684\u6210\u719f\uff0c\u6838\u5fc3\u6311\u6218\u4e0d\u518d\u662f\u6784\u5efa\u8fd9\u4e9b\u67b6\u6784\uff0c\u800c\u662f\u8bc6\u522b\u5b83\u4eec\u5728\u54ea\u4e9b\u9886\u57df\u80fd\u63d0\u4f9b\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u5b9e\u9645\u4f18\u52bf\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u5f3a\u5b50\u7269\u7406\u95ee\u9898\u5f00\u53d1\u8bca\u65ad\u5de5\u5177\uff0c\u6307\u5bfc\u91cf\u5b50\u4e0e\u7ecf\u5178\u6a21\u578b\u7684\u9009\u62e9\u3002", "method": "\u5f00\u53d1\u4ee5\u5b9a\u91cf\u91cf\u5b50\u8d28\u91cf\u6307\u6807\u4e3a\u4e2d\u5fc3\u7684\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u53d7\u63a7\u7684\u5206\u7c7b\u548c\u56de\u5f52\u7814\u7a76\uff0c\u5206\u6790\u6a21\u578b\u6027\u80fd\u4e0e\u6570\u636e\u590d\u6742\u6027\u3001\u566a\u58f0\u548c\u7ef4\u5ea6\u7b49\u5185\u5728\u7279\u6027\u7684\u5173\u7cfb\uff0c\u5e76\u5c06\u8fd9\u4e9b\u8d8b\u52bf\u63d0\u70bc\u4e3a\u9884\u6d4b\u6027\u51c6\u5219\u3002", "result": "\u7814\u7a76\u8868\u660e\u76f8\u5bf9\u6a21\u578b\u6027\u80fd\u9075\u5faa\u590d\u6742\u6027\u3001\u566a\u58f0\u548c\u7ef4\u5ea6\u7684\u7cfb\u7edf\u6027\u8d8b\u52bf\uff0c\u8fd9\u4e9b\u8d8b\u52bf\u53ef\u4ee5\u63d0\u70bc\u4e3a\u9884\u6d4b\u6027\u51c6\u5219\u3002\u5728\u6df1\u5ea6\u865a\u5eb7\u666e\u987f\u6563\u5c04\u7684\u5eb7\u666e\u987f\u5f62\u72b6\u56e0\u5b50\u63d0\u53d6\u5e94\u7528\u4e2d\uff0c\u91cf\u5b50\u8d28\u91cf\u6307\u6807\u6210\u529f\u8bc6\u522b\u4e86\u91cf\u5b50\u6a21\u578b\u6709\u5229\u7684\u8fd0\u52a8\u5b66\u533a\u57df\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u7cbe\u5bc6\u5f3a\u5b50\u7269\u7406\u4e2d\u90e8\u7f72\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5de5\u5177\u5efa\u7acb\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8bca\u65ad\u5de5\u5177\u548c\u91cf\u5b50\u8d28\u91cf\u6307\u6807\u6307\u5bfc\u91cf\u5b50\u4e0e\u7ecf\u5178\u6a21\u578b\u7684\u9009\u62e9\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u7269\u7406\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2601.13476", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13476", "abs": "https://arxiv.org/abs/2601.13476", "authors": ["Jinhao Li", "Hao Wang"], "title": "A Unified Variational Imputation Framework for Electric Vehicle Charging Data Using Retrieval-Augmented Language Model", "comment": "15 pages", "summary": "The reliability of data-driven applications in electric vehicle (EV) infrastructure, such as charging demand forecasting, hinges on the availability of complete, high-quality charging data. However, real-world EV datasets are often plagued by missing records, and existing imputation methods are ill-equipped for the complex, multimodal context of charging data, often relying on a restrictive one-model-per-station paradigm that ignores valuable inter-station correlations. To address these gaps, we develop a novel PRobabilistic variational imputation framework that leverages the power of large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a pre-trained language model to encode heterogeneous data, spanning time-series demand, calendar features, and geospatial context, into a unified, semantically rich representation. This is dynamically fortified by retrieval-augmented memory that retrieves relevant examples from the entire charging network, enabling a single, unified imputation model empowered by variational neural architecture to overcome data sparsity. Extensive experiments on four public datasets demonstrate that PRAIM significantly outperforms established baselines in both imputation accuracy and its ability to preserve the original data's statistical distribution, leading to substantial improvements in downstream forecasting performance.", "AI": {"tldr": "\u63d0\u51faPRAIM\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u8bb0\u5fc6\u673a\u5236\uff0c\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63d2\u8865\u7cbe\u5ea6\u548c\u4e0b\u6e38\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u4e2d\u6570\u636e\u9a71\u52a8\u5e94\u7528\u7684\u53ef\u9760\u6027\u4f9d\u8d56\u4e8e\u5b8c\u6574\u3001\u9ad8\u8d28\u91cf\u7684\u5145\u7535\u6570\u636e\uff0c\u4f46\u73b0\u5b9e\u6570\u636e\u96c6\u5e38\u5b58\u5728\u7f3a\u5931\u8bb0\u5f55\uff0c\u73b0\u6709\u63d2\u8865\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5145\u7535\u6570\u636e\u7684\u590d\u6742\u591a\u6a21\u6001\u7279\u6027\uff0c\u4e14\u901a\u5e38\u91c7\u7528\"\u4e00\u7ad9\u4e00\u6a21\u578b\"\u7684\u5c40\u9650\u8303\u5f0f\uff0c\u5ffd\u7565\u4e86\u7ad9\u95f4\u76f8\u5173\u6027\u3002", "method": "\u5f00\u53d1PRAIM\u6846\u67b6\uff1a\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u65f6\u95f4\u5e8f\u5217\u9700\u6c42\u3001\u65e5\u5386\u7279\u5f81\u548c\u5730\u7406\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7b49\u5f02\u6784\u6570\u636e\u4e3a\u7edf\u4e00\u8bed\u4e49\u8868\u793a\uff1b\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u8bb0\u5fc6\u673a\u5236\u4ece\u6574\u4e2a\u5145\u7535\u7f51\u7edc\u4e2d\u68c0\u7d22\u76f8\u5173\u793a\u4f8b\uff1b\u91c7\u7528\u53d8\u5206\u795e\u7ecf\u67b6\u6784\u6784\u5efa\u7edf\u4e00\u7684\u63d2\u8865\u6a21\u578b\u514b\u670d\u6570\u636e\u7a00\u758f\u6027\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPRAIM\u5728\u63d2\u8865\u7cbe\u5ea6\u548c\u4fdd\u6301\u539f\u59cb\u6570\u636e\u7edf\u8ba1\u5206\u5e03\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u4e86\u4e0b\u6e38\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "PRAIM\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u80fd\u529b\u548c\u68c0\u7d22\u589e\u5f3a\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u7535\u52a8\u6c7d\u8f66\u57fa\u7840\u8bbe\u65bd\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u8d28\u91cf\u4fdd\u969c\u3002"}}
{"id": "2601.13522", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.13522", "abs": "https://arxiv.org/abs/2601.13522", "authors": ["Shuang Li"], "title": "StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor Sensing", "comment": null, "summary": "Low-rank tensor sensing is a fundamental problem with broad applications in signal processing and machine learning. Among various tensor models, low-Tucker-rank tensors are particularly attractive for capturing multi-mode subspace structures in high-dimensional data. Existing recovery methods either operate on the full tensor variable with expensive tensor projections, or adopt factorized formulations that still rely on full-gradient computations, while most stochastic factorized approaches are restricted to tensor decomposition settings. In this work, we propose a stochastic alternating minimization algorithm that operates directly on the core tensor and factor matrices under a Tucker factorization. The proposed method avoids repeated tensor projections and enables efficient mini-batch updates on low-dimensional tensor factors. Numerical experiments on synthetic tensor sensing demonstrate that the proposed algorithm exhibits favorable convergence behavior in wall-clock time compared with representative stochastic tensor recovery baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTucker\u5206\u89e3\u7684\u968f\u673a\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728\u6838\u5fc3\u5f20\u91cf\u548c\u56e0\u5b50\u77e9\u9635\u4e0a\u64cd\u4f5c\uff0c\u907f\u514d\u91cd\u590d\u5f20\u91cf\u6295\u5f71\uff0c\u5b9e\u73b0\u4f4e\u7ef4\u5f20\u91cf\u56e0\u5b50\u7684\u9ad8\u6548\u5c0f\u6279\u91cf\u66f4\u65b0\u3002", "motivation": "\u4f4eTucker\u79e9\u5f20\u91cf\u80fd\u6709\u6548\u6355\u6349\u9ad8\u7ef4\u6570\u636e\u7684\u591a\u6a21\u6001\u5b50\u7a7a\u95f4\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u6062\u590d\u65b9\u6cd5\u8981\u4e48\u5728\u5168\u5f20\u91cf\u53d8\u91cf\u4e0a\u64cd\u4f5c\uff08\u5f20\u91cf\u6295\u5f71\u6602\u8d35\uff09\uff0c\u8981\u4e48\u91c7\u7528\u56e0\u5b50\u5316\u516c\u5f0f\u4f46\u4ecd\u4f9d\u8d56\u5168\u68af\u5ea6\u8ba1\u7b97\uff0c\u800c\u5927\u591a\u6570\u968f\u673a\u56e0\u5b50\u5316\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5f20\u91cf\u5206\u89e3\u8bbe\u7f6e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u968f\u673a\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728Tucker\u5206\u89e3\u4e0b\u7684\u6838\u5fc3\u5f20\u91cf\u548c\u56e0\u5b50\u77e9\u9635\u4e0a\u64cd\u4f5c\uff0c\u907f\u514d\u91cd\u590d\u5f20\u91cf\u6295\u5f71\uff0c\u652f\u6301\u4f4e\u7ef4\u5f20\u91cf\u56e0\u5b50\u7684\u9ad8\u6548\u5c0f\u6279\u91cf\u66f4\u65b0\u3002", "result": "\u5728\u5408\u6210\u5f20\u91cf\u611f\u77e5\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u4ee3\u8868\u6027\u968f\u673a\u5f20\u91cf\u6062\u590d\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u7b97\u6cd5\u5728\u6302\u949f\u65f6\u95f4\u4e0a\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6536\u655b\u884c\u4e3a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u4f4eTucker\u79e9\u5f20\u91cf\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u968f\u673a\u4f18\u5316\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u74f6\u9888\u3002"}}
{"id": "2601.13534", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13534", "abs": "https://arxiv.org/abs/2601.13534", "authors": ["Xu Zhang", "Junwei Deng", "Chang Xu", "Hao Li", "Jiang Bian"], "title": "MN-TSG:Continuous Time Series Generation with Irregular Observations", "comment": "34 pages", "summary": "Time series generation (TSG) plays a critical role in a wide range of domains, such as healthcare. However, most existing methods assume regularly sampled observations and fixed output resolutions, which are often misaligned with real-world scenarios where data are irregularly sampled and sparsely observed. This mismatch is particularly problematic in applications such as clinical monitoring, where irregular measurements must support downstream tasks requiring continuous and high-resolution time series.\n  Neural Controlled Differential Equations (NCDEs) have shown strong potential for modeling irregular time series, yet they still face challenges in capturing complex dynamic temporal patterns and supporting continuous TSG. To address these limitations, we propose MN-TSG, a novel framework that explores Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG models for irregular and continuous generation tasks.\n  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically parameterized expert functions and a decoupled design that facilitates more effective optimization of MoE dynamics. Furthermore, we leverage existing TSG models to learn the joint distribution over the mixture of experts and the generated time series. This enables the framework not only to generate new samples, but also to produce appropriate expert configurations tailored to each sample, thereby supporting refined continuous TSG.\n  Extensive experiments on ten public and synthetic datasets demonstrate the effectiveness of MN-TSG, consistently outperforming strong TSG baselines on both irregular-to-regular and irregular-to-continuous generation tasks.", "AI": {"tldr": "MN-TSG\uff1a\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6NCDE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u7eed\u751f\u6210\uff0c\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u5316\u4e13\u5bb6\u51fd\u6570\u548c\u89e3\u8026\u8bbe\u8ba1\u4f18\u5316\u4e13\u5bb6\u6df7\u5408\u52a8\u6001\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u901a\u5e38\u662f\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u7a00\u758f\u89c2\u6d4b\u7684\uff0c\u800c\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u65b9\u6cd5\u5927\u591a\u5047\u8bbe\u89c4\u5219\u91c7\u6837\u548c\u56fa\u5b9a\u8f93\u51fa\u5206\u8fa8\u7387\uff0c\u8fd9\u5728\u4e34\u5e8a\u76d1\u6d4b\u7b49\u5e94\u7528\u4e2d\u5c24\u4e3a\u6210\u95ee\u9898\u3002\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08NCDE\uff09\u5728\u5efa\u6a21\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u96be\u4ee5\u6355\u6349\u590d\u6742\u52a8\u6001\u65f6\u95f4\u6a21\u5f0f\u548c\u652f\u6301\u8fde\u7eed\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u3002", "method": "\u63d0\u51faMN-TSG\u6846\u67b6\uff0c\u7ed3\u5408\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684NCDE\u67b6\u6784\uff0c\u5177\u6709\u52a8\u6001\u53c2\u6570\u5316\u4e13\u5bb6\u51fd\u6570\u548c\u89e3\u8026\u8bbe\u8ba1\u4ee5\u4f18\u5316MoE\u52a8\u6001\u3002\u5229\u7528\u73b0\u6709TSG\u6a21\u578b\u5b66\u4e60\u4e13\u5bb6\u6df7\u5408\u4e0e\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u7684\u8054\u5408\u5206\u5e03\uff0c\u4f7f\u6846\u67b6\u4e0d\u4ec5\u80fd\u751f\u6210\u65b0\u6837\u672c\uff0c\u8fd8\u80fd\u4e3a\u6bcf\u4e2a\u6837\u672c\u751f\u6210\u9002\u5f53\u7684\u4e13\u5bb6\u914d\u7f6e\u3002", "result": "\u5728\u5341\u4e2a\u516c\u5171\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMN-TSG\u5728\u4ece\u4e0d\u89c4\u5219\u5230\u89c4\u5219\u548c\u4e0d\u89c4\u5219\u5230\u8fde\u7eed\u7684\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5f3aTSG\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MN-TSG\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6NCDE\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u7684\u8fde\u7eed\u751f\u6210\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u76d1\u6d4b\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13548", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13548", "abs": "https://arxiv.org/abs/2601.13548", "authors": ["George Wang", "Daniel Murfet"], "title": "Patterning: The Dual of Interpretability", "comment": null, "summary": "Mechanistic interpretability aims to understand how neural networks generalize beyond their training data by reverse-engineering their internal structures. We introduce patterning as the dual problem: given a desired form of generalization, determine what training data produces it. Our approach is based on susceptibilities, which measure how posterior expectation values of observables respond to infinitesimal shifts in the data distribution. Inverting this linear response relationship yields the data intervention that steers the model toward a target internal configuration. We demonstrate patterning in a small language model, showing that re-weighting training data along principal susceptibility directions can accelerate or delay the formation of structure, such as the induction circuit. In a synthetic parentheses balancing task where multiple algorithms achieve perfect training accuracy, we show that patterning can select which algorithm the model learns by targeting the local learning coefficient of each solution. These results establish that the same mathematical framework used to read internal structure can be inverted to write it.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"\u6a21\u5f0f\u5316\"\u4f5c\u4e3a\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5bf9\u5076\u95ee\u9898\uff1a\u7ed9\u5b9a\u671f\u671b\u7684\u6cdb\u5316\u5f62\u5f0f\uff0c\u786e\u5b9a\u4ea7\u751f\u5b83\u7684\u8bad\u7ec3\u6570\u636e\u3002\u901a\u8fc7\u53cd\u8f6c\u7ebf\u6027\u54cd\u5e94\u5173\u7cfb\uff0c\u53ef\u4ee5\u8bbe\u8ba1\u6570\u636e\u5e72\u9884\u6765\u5f15\u5bfc\u6a21\u578b\u8fbe\u5230\u76ee\u6807\u5185\u90e8\u914d\u7f6e\u3002", "motivation": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u6cdb\u5316\u5230\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\uff0c\u4f46\u53cd\u5411\u95ee\u9898\u2014\u2014\u7ed9\u5b9a\u671f\u671b\u7684\u6cdb\u5316\u5f62\u5f0f\uff0c\u5982\u4f55\u8bbe\u8ba1\u8bad\u7ec3\u6570\u636e\u6765\u5b9e\u73b0\u5b83\u2014\u2014\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u4ece\"\u8bfb\u53d6\"\u5185\u90e8\u7ed3\u6784\u5230\"\u5199\u5165\"\u5185\u90e8\u7ed3\u6784\u7684\u6570\u5b66\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u654f\u611f\u6027\u6982\u5ff5\uff0c\u6d4b\u91cf\u53ef\u89c2\u6d4b\u91cf\u540e\u9a8c\u671f\u671b\u503c\u5bf9\u6570\u636e\u5206\u5e03\u5fae\u5c0f\u53d8\u5316\u7684\u54cd\u5e94\u3002\u901a\u8fc7\u53cd\u8f6c\u8fd9\u79cd\u7ebf\u6027\u54cd\u5e94\u5173\u7cfb\uff0c\u8ba1\u7b97\u51fa\u80fd\u591f\u5f15\u5bfc\u6a21\u578b\u8fbe\u5230\u76ee\u6807\u5185\u90e8\u914d\u7f6e\u7684\u6570\u636e\u5e72\u9884\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u6cbf\u4e3b\u654f\u611f\u6027\u65b9\u5411\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u52a0\u901f\u6216\u5ef6\u8fdf\u7279\u5b9a\u7ed3\u6784\uff08\u5982\u5f52\u7eb3\u7535\u8def\uff09\u7684\u5f62\u6210\u3002\u5728\u62ec\u53f7\u5e73\u8861\u4efb\u52a1\u4e2d\uff0c\u5f53\u591a\u4e2a\u7b97\u6cd5\u90fd\u80fd\u8fbe\u5230\u5b8c\u7f8e\u8bad\u7ec3\u51c6\u786e\u7387\u65f6\uff0c\u6a21\u5f0f\u5316\u53ef\u4ee5\u901a\u8fc7\u9488\u5bf9\u6bcf\u4e2a\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u90e8\u5b66\u4e60\u7cfb\u6570\u6765\u9009\u62e9\u6a21\u578b\u5b66\u4e60\u7684\u7b97\u6cd5\u3002", "conclusion": "\u76f8\u540c\u7684\u6570\u5b66\u6846\u67b6\u65e2\u53ef\u4ee5\u7528\u4e8e\u8bfb\u53d6\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u7ed3\u6784\uff0c\u4e5f\u53ef\u4ee5\u53cd\u8f6c\u7528\u4e8e\u5199\u5165\u5185\u90e8\u7ed3\u6784\u3002\u8fd9\u5efa\u7acb\u4e86\u673a\u5236\u53ef\u89e3\u91ca\u6027\u4e0e\u5176\u5bf9\u5076\u95ee\u9898\"\u6a21\u5f0f\u5316\"\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u4e3a\u63a7\u5236\u6a21\u578b\u5b66\u4e60\u7279\u5b9a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.13563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13563", "abs": "https://arxiv.org/abs/2601.13563", "authors": ["Aryan Karmore"], "title": "ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits", "comment": null, "summary": "Linear memory scaling stores $N$ independent expert weight matrices requiring $\\mathcal{O}(N \\cdot d^2)$ memory, which exceeds edge devices memory budget. Current compression methods like quantization, pruning and low-rank factorization reduce constant factors but leave the scaling bottleneck unresolved. We introduce ButterflyMoE, a method that treats experts not as independent weight matrices but as geometric reorientations of a unified shared quantized substrate. Diversity among experts arises from viewing different angles of shared capacity, not from redundant storage. By applying learned rotations to a shared ternary prototype, each expert yields $\\mathcal{O}(d^2 + N \\cdot d \\log d)$ memory -- sub-linear in the number of experts. The key insight: training these rotations with quantization reduces activation outliers and stabilizes extreme low bit training, where static methods collapse. Across language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction at 256 experts with negligible accuracy loss. This allows 64 experts to fit on 4GB devices compared to standard MoE's 8 experts, showing geometric parametrization breaks linear scaling.", "AI": {"tldr": "ButterflyMoE\u901a\u8fc7\u5c06\u4e13\u5bb6\u89c6\u4e3a\u5171\u4eab\u91cf\u5316\u57fa\u8d28\u7684\u51e0\u4f55\u91cd\u5b9a\u5411\uff0c\u800c\u975e\u72ec\u7acb\u6743\u91cd\u77e9\u9635\uff0c\u5b9e\u73b0\u4e86\u4e13\u5bb6\u6570\u91cf\u7684\u4e9a\u7ebf\u6027\u5185\u5b58\u589e\u957f\uff0c\u5728256\u4e2a\u4e13\u5bb6\u65f6\u8fbe\u5230150\u500d\u5185\u5b58\u538b\u7f29\u3002", "motivation": "\u4f20\u7edfMoE\u65b9\u6cd5\u4e2d\uff0cN\u4e2a\u72ec\u7acb\u4e13\u5bb6\u6743\u91cd\u77e9\u9635\u9700\u8981O(N\u00b7d\u00b2)\u5185\u5b58\uff0c\u8d85\u51fa\u8fb9\u7f18\u8bbe\u5907\u5185\u5b58\u9884\u7b97\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\uff08\u91cf\u5316\u3001\u526a\u679d\u3001\u4f4e\u79e9\u5206\u89e3\uff09\u53ea\u80fd\u51cf\u5c11\u5e38\u6570\u56e0\u5b50\uff0c\u65e0\u6cd5\u89e3\u51b3\u7ebf\u6027\u6269\u5c55\u74f6\u9888\u3002", "method": "\u5c06\u4e13\u5bb6\u89c6\u4e3a\u5171\u4eab\u91cf\u5316\u57fa\u8d28\u7684\u51e0\u4f55\u91cd\u5b9a\u5411\uff0c\u800c\u975e\u72ec\u7acb\u5b58\u50a8\u3002\u901a\u8fc7\u5bf9\u5171\u4eab\u4e09\u5143\u539f\u578b\u5e94\u7528\u5b66\u4e60\u5230\u7684\u65cb\u8f6c\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u83b7\u5f97O(d\u00b2 + N\u00b7d log d)\u5185\u5b58\u3002\u8bad\u7ec3\u8fd9\u4e9b\u65cb\u8f6c\u4e0e\u91cf\u5316\u7ed3\u5408\uff0c\u51cf\u5c11\u6fc0\u6d3b\u5f02\u5e38\u503c\u5e76\u7a33\u5b9a\u6781\u7aef\u4f4e\u6bd4\u7279\u8bad\u7ec3\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cButterflyMoE\u5728256\u4e2a\u4e13\u5bb6\u65f6\u5b9e\u73b0150\u500d\u5185\u5b58\u538b\u7f29\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002\u4f7f64\u4e2a\u4e13\u5bb6\u80fd\u9002\u914d4GB\u8bbe\u5907\uff0c\u800c\u6807\u51c6MoE\u53ea\u80fd\u5bb9\u7eb38\u4e2a\u4e13\u5bb6\u3002", "conclusion": "\u51e0\u4f55\u53c2\u6570\u5316\u6253\u7834\u4e86MoE\u7684\u7ebf\u6027\u5185\u5b58\u6269\u5c55\u74f6\u9888\uff0c\u901a\u8fc7\u5171\u4eab\u5bb9\u91cf\u548c\u65cb\u8f6c\u89c6\u89d2\u5b9e\u73b0\u4e13\u5bb6\u591a\u6837\u6027\uff0c\u800c\u975e\u5197\u4f59\u5b58\u50a8\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u89c4\u6a21\u4e13\u5bb6\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.13564", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.13564", "abs": "https://arxiv.org/abs/2601.13564", "authors": ["Yanheng Li", "Zhichen Pu", "Lijiang Yang", "Zehao Zhou", "Yi Qin Gao"], "title": "Multi-objective fluorescent molecule design with a data-physics dual-driven generative framework", "comment": "Total 43 pages: 32 pages Main Text + 11 pages SI", "summary": "Designing fluorescent small molecules with tailored optical and physicochemical properties requires navigating vast, underexplored chemical space while satisfying multiple objectives and constraints. Conventional generate-score-screen approaches become impractical under such realistic design specifications, owing to their low search efficiency, unreliable generalizability of machine-learning prediction, and the prohibitive cost of quantum chemical calculation. Here we present LUMOS, a data-and-physics driven framework for inverse design of fluorescent molecules. LUMOS couples generator and predictor within a shared latent representation, enabling direct specification-to-molecule design and efficient exploration. Moreover, LUMOS combines neural networks with a fast time-dependent density functional theory (TD-DFT) calculation workflow to build a suite of complementary predictors spanning different trade-offs in speed, accuracy, and generalizability, enabling reliable property prediction across diverse scenarios. Finally, LUMOS employs a property-guided diffusion model integrated with multi-objective evolutionary algorithms, enabling de novo design and molecular optimization under multiple objectives and constraints. Across comprehensive benchmarks, LUMOS consistently outperforms baseline models in terms of accuracy, generalizability and physical plausibility for fluorescence property prediction, and demonstrates superior performance in multi-objective scaffold- and fragment-level molecular optimization. Further validation using TD-DFT and molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid fluorophores that meet various target specifications. Overall, these results establish LUMOS as a data-physics dual-driven framework for general fluorophore inverse design.", "AI": {"tldr": "LUMOS\u662f\u4e00\u4e2a\u6570\u636e\u4e0e\u7269\u7406\u53cc\u9a71\u52a8\u7684\u8367\u5149\u5206\u5b50\u9006\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5668\u4e0e\u9884\u6d4b\u5668\u3001\u795e\u7ecf\u7f51\u7edc\u4e0e\u5feb\u901fTD-DFT\u8ba1\u7b97\u3001\u4ee5\u53ca\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u8367\u5149\u5206\u5b50\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u8367\u5149\u5206\u5b50\u8bbe\u8ba1\u65b9\u6cd5\u9762\u4e34\u5316\u5b66\u7a7a\u95f4\u63a2\u7d22\u6548\u7387\u4f4e\u3001\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6cdb\u5316\u80fd\u529b\u4e0d\u53ef\u9760\u3001\u91cf\u5b50\u5316\u5b66\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u591a\u76ee\u6807\u7ea6\u675f\u4e0b\u7684\u5b9e\u9645\u8bbe\u8ba1\u9700\u6c42\u3002", "method": "LUMOS\u6846\u67b6\u5305\u542b\uff1a1\uff09\u5171\u4eab\u6f5c\u5728\u8868\u5f81\u7684\u751f\u6210\u5668-\u9884\u6d4b\u5668\u8026\u5408\u7cfb\u7edf\uff1b2\uff09\u795e\u7ecf\u7f51\u7edc\u4e0e\u5feb\u901fTD-DFT\u8ba1\u7b97\u7ed3\u5408\u7684\u4e92\u8865\u9884\u6d4b\u5668\u5957\u4ef6\uff1b3\uff09\u5c5e\u6027\u5f15\u5bfc\u6269\u6563\u6a21\u578b\u4e0e\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\u96c6\u6210\u7684\u5206\u5b50\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLUMOS\u5728\u8367\u5149\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u591a\u76ee\u6807\u5206\u5b50\u4f18\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cTD-DFT\u548cMD\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u751f\u6210\u7b26\u5408\u76ee\u6807\u89c4\u683c\u7684\u6709\u6548\u8367\u5149\u5206\u5b50\u3002", "conclusion": "LUMOS\u5efa\u7acb\u4e86\u4e00\u4e2a\u6570\u636e\u4e0e\u7269\u7406\u53cc\u9a71\u52a8\u7684\u901a\u7528\u8367\u5149\u5206\u5b50\u9006\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u53ef\u9760\u5730\u8bbe\u8ba1\u6ee1\u8db3\u591a\u76ee\u6807\u7ea6\u675f\u7684\u8367\u5149\u5206\u5b50\u3002"}}
{"id": "2601.13566", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13566", "abs": "https://arxiv.org/abs/2601.13566", "authors": ["Tianyi Qiu", "Ahmed Hani Ismail", "Zhonghao He", "Shi Feng"], "title": "Self-Improvement as Coherence Optimization: A Theoretical Account", "comment": "39 pages", "summary": "Can language models improve their accuracy without external supervision? Methods such as debate, bootstrap, and internal coherence maximization achieve this surprising feat, even matching golden finetuning performance. Yet why they work remains theoretically unclear. We show that they are all special cases of coherence optimization: finding a context-to-behavior mapping that's most compressible and jointly predictable. We prove that coherence optimization is equivalent to description-length regularization, and that among all such regularization schemes, it is optimal for semi-supervised learning when the regularizer is derived from a pretrained model. Our theory, supported by preliminary experiments, explains why feedback-free self-improvement works and predicts when it should succeed or fail.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8fa9\u8bba\u3001\u81ea\u4e3e\u548c\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\u7b49\u65b9\u6cd5\u5b9e\u73b0\u65e0\u5916\u90e8\u76d1\u7763\u7684\u81ea\u6211\u63d0\u5347\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u8fde\u8d2f\u6027\u4f18\u5316\u7684\u7279\u4f8b\uff0c\u7b49\u540c\u4e8e\u63cf\u8ff0\u957f\u5ea6\u6b63\u5219\u5316\uff0c\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\u5bf9\u534a\u76d1\u7763\u5b66\u4e60\u6700\u4f18\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8fa9\u8bba\u3001\u81ea\u4e3e\u548c\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\u7b49\u65b9\u6cd5\u5728\u6ca1\u6709\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u51c6\u786e\u6027\uff0c\u751a\u81f3\u5339\u914d\u6709\u76d1\u7763\u5fae\u8c03\u7684\u6027\u80fd\u3002\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u4f55\u6709\u6548\u5728\u7406\u8bba\u4e0a\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u6765\u89e3\u91ca\u8fd9\u4e9b\u81ea\u6211\u63d0\u5347\u673a\u5236\u7684\u5de5\u4f5c\u539f\u7406\u3002", "method": "\u63d0\u51fa\u8fde\u8d2f\u6027\u4f18\u5316\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u8fa9\u8bba\u3001\u81ea\u4e3e\u548c\u5185\u90e8\u4e00\u81f4\u6027\u6700\u5927\u5316\u7b49\u65b9\u6cd5\u7edf\u4e00\u4e3a\u5bfb\u627e\u6700\u53ef\u538b\u7f29\u548c\u8054\u5408\u53ef\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u5230\u884c\u4e3a\u6620\u5c04\u3002\u8bc1\u660e\u8fde\u8d2f\u6027\u4f18\u5316\u7b49\u4ef7\u4e8e\u63cf\u8ff0\u957f\u5ea6\u6b63\u5219\u5316\uff0c\u5e76\u8bc1\u660e\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\uff0c\u8fd9\u79cd\u6b63\u5219\u5316\u65b9\u6848\u5bf9\u534a\u76d1\u7763\u5b66\u4e60\u662f\u6700\u4f18\u7684\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u65e0\u53cd\u9988\u7684\u81ea\u6211\u63d0\u5347\u65b9\u6cd5\u4e4b\u6240\u4ee5\u6709\u6548\uff0c\u662f\u56e0\u4e3a\u5b83\u4eec\u672c\u8d28\u4e0a\u662f\u8fde\u8d2f\u6027\u4f18\u5316\u7684\u7279\u4f8b\u3002\u521d\u6b65\u5b9e\u9a8c\u652f\u6301\u8be5\u7406\u8bba\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u89e3\u91ca\u73b0\u6709\u65b9\u6cd5\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u8fd8\u80fd\u9884\u6d4b\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u4f1a\u6210\u529f\u6216\u5931\u8d25\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u63d0\u5347\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u8fde\u8d2f\u6027\u4f18\u5316\u7406\u8bba\u5f97\u5230\u7edf\u4e00\u89e3\u91ca\uff0c\u8be5\u7406\u8bba\u63ed\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u672c\u8d28\u4e0a\u662f\u63cf\u8ff0\u957f\u5ea6\u6b63\u5219\u5316\u7684\u7279\u6b8a\u5f62\u5f0f\uff0c\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e0b\u5bf9\u534a\u76d1\u7763\u5b66\u4e60\u5177\u6709\u6700\u4f18\u6027\uff0c\u4e3a\u7406\u89e3\u65e0\u76d1\u7763\u81ea\u6211\u63d0\u5347\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2601.13569", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.13569", "abs": "https://arxiv.org/abs/2601.13569", "authors": ["Jiasen Li", "Yanwei Liu", "Zhuoyi Shang", "Xiaoyan Gu", "Weiping Wang"], "title": "DRGW: Learning Disentangled Representations for Robust Graph Watermarking", "comment": "Published at The Web Conference 2026 (WWW '26)", "summary": "Graph-structured data is foundational to numerous web applications, and watermarking is crucial for protecting their intellectual property and ensuring data provenance. Existing watermarking methods primarily operate on graph structures or entangled graph representations, which compromise the transparency and robustness of watermarks due to the information coupling in representing graphs and uncontrollable discretization in transforming continuous numerical representations into graph structures. This motivates us to propose DRGW, the first graph watermarking framework that addresses these issues through disentangled representation learning. Specifically, we design an adversarially trained encoder that learns an invariant structural representation against diverse perturbations and derives a statistically independent watermark carrier, ensuring both robustness and transparency of watermarks. Meanwhile, we devise a graph-aware invertible neural network to provide a lossless channel for watermark embedding and extraction, guaranteeing high detectability and transparency of watermarks. Additionally, we develop a structure-aware editor that resolves the issue of latent modifications into discrete graph edits, ensuring robustness against structural perturbations. Experiments on diverse benchmark datasets demonstrate the superior effectiveness of DRGW.", "AI": {"tldr": "DRGW\u662f\u9996\u4e2a\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u56fe\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u7ed3\u6784\u8868\u793a\u4e0e\u6c34\u5370\u8f7d\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u6c34\u5370\u65b9\u6cd5\u4e3b\u8981\u5728\u7ed3\u6784\u6216\u7ea0\u7f20\u8868\u793a\u4e0a\u64cd\u4f5c\uff0c\u7531\u4e8e\u56fe\u8868\u793a\u7684\u4fe1\u606f\u8026\u5408\u548c\u8fde\u7eed\u6570\u503c\u5230\u79bb\u6563\u7ed3\u6784\u8f6c\u6362\u7684\u4e0d\u53ef\u63a7\u6027\uff0c\u5bfc\u81f4\u6c34\u5370\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "1) \u5bf9\u6297\u8bad\u7ec3\u7f16\u7801\u5668\u5b66\u4e60\u4e0d\u53d8\u7ed3\u6784\u8868\u793a\u5e76\u751f\u6210\u7edf\u8ba1\u72ec\u7acb\u7684\u6c34\u5370\u8f7d\u4f53\uff1b2) \u56fe\u611f\u77e5\u53ef\u9006\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u65e0\u635f\u6c34\u5370\u5d4c\u5165/\u63d0\u53d6\u901a\u9053\uff1b3) \u7ed3\u6784\u611f\u77e5\u7f16\u8f91\u5668\u5c06\u6f5c\u5728\u4fee\u6539\u8f6c\u5316\u4e3a\u79bb\u6563\u56fe\u7f16\u8f91\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660eDRGW\u5177\u6709\u4f18\u8d8a\u7684\u6709\u6548\u6027\u3002", "conclusion": "DRGW\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u89e3\u51b3\u4e86\u56fe\u6c34\u5370\u7684\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u662f\u9996\u4e2a\u5728\u8be5\u65b9\u5411\u4e0a\u7684\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2601.13570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13570", "abs": "https://arxiv.org/abs/2601.13570", "authors": ["Tingting Dan", "Jiaqi Ding", "Guorong Wu"], "title": "GeoDynamics: A Geometric State-Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds", "comment": "Accepted to NeurIPS 2025", "summary": "State-space models (SSMs) have become a cornerstone for unraveling brain dynamics, revealing how latent neural states evolve over time and give rise to observed signals. By combining the flexibility of deep learning with the principled dynamical structure of SSMs, recent studies have achieved powerful fits to functional neuroimaging data. However, most existing approaches still view the brain as a set of loosely connected regions or impose oversimplified network priors, falling short of a truly holistic and self-organized dynamical system perspective. Brain functional connectivity (FC) at each time point naturally forms a symmetric positive definite (SPD) matrix, which resides on a curved Riemannian manifold rather than in Euclidean space. Capturing the trajectories of these SPD matrices is key to understanding how coordinated networks support cognition and behavior. To this end, we introduce GeoDynamics, a geometric state-space neural network that tracks latent brain-state trajectories directly on the high-dimensional SPD manifold. GeoDynamics embeds each connectivity matrix into a manifold-aware recurrent framework, learning smooth and geometry-respecting transitions that reveal task-driven state changes and early markers of Alzheimer's disease, Parkinson's disease, and autism. Beyond neuroscience, we validate GeoDynamics on human action recognition benchmarks (UTKinect, Florence, HDM05), demonstrating its scalability and robustness in modeling complex spatiotemporal dynamics across diverse domains.", "AI": {"tldr": "GeoDynamics\u662f\u4e00\u4e2a\u51e0\u4f55\u72b6\u6001\u7a7a\u95f4\u795e\u7ecf\u7f51\u7edc\uff0c\u76f4\u63a5\u5728\u5bf9\u79f0\u6b63\u5b9a\u6d41\u5f62\u4e0a\u5efa\u6a21\u5927\u8111\u529f\u80fd\u8fde\u63a5\u6027\u8f68\u8ff9\uff0c\u7528\u4e8e\u8ffd\u8e2a\u4efb\u52a1\u9a71\u52a8\u72b6\u6001\u53d8\u5316\u548c\u75be\u75c5\u65e9\u671f\u6807\u5fd7\u7269\u3002", "motivation": "\u73b0\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u901a\u5e38\u5c06\u5927\u8111\u89c6\u4e3a\u677e\u6563\u8fde\u63a5\u533a\u57df\u6216\u65bd\u52a0\u8fc7\u5ea6\u7b80\u5316\u7684\u7f51\u7edc\u5148\u9a8c\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u6574\u4f53\u81ea\u7ec4\u7ec7\u52a8\u529b\u5b66\u7cfb\u7edf\u89c6\u89d2\u3002\u5927\u8111\u529f\u80fd\u8fde\u63a5\u6027\u77e9\u9635\u4f4d\u4e8e\u9ece\u66fc\u6d41\u5f62\u800c\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u9700\u8981\u51e0\u4f55\u611f\u77e5\u7684\u65b9\u6cd5\u6765\u6355\u6349\u5176\u8f68\u8ff9\u3002", "method": "\u63d0\u51faGeoDynamics\u6a21\u578b\uff0c\u5c06\u6bcf\u4e2a\u8fde\u63a5\u6027\u77e9\u9635\u5d4c\u5165\u5230\u6d41\u5f62\u611f\u77e5\u7684\u5faa\u73af\u6846\u67b6\u4e2d\uff0c\u5728\u9ad8\u7ef4\u5bf9\u79f0\u6b63\u5b9a\u6d41\u5f62\u4e0a\u76f4\u63a5\u5b66\u4e60\u5e73\u6ed1\u4e14\u5c0a\u91cd\u51e0\u4f55\u7684\u72b6\u6001\u8f6c\u79fb\u3002", "result": "\u6a21\u578b\u80fd\u591f\u63ed\u793a\u4efb\u52a1\u9a71\u52a8\u72b6\u6001\u53d8\u5316\uff0c\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u5e15\u91d1\u68ee\u75c5\u548c\u81ea\u95ed\u75c7\u7684\u65e9\u671f\u6807\u5fd7\u7269\uff0c\u5e76\u5728\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u57fa\u51c6\uff08UTKinect\u3001Florence\u3001HDM05\uff09\u4e0a\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GeoDynamics\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u6d41\u5f62\u4e0a\u76f4\u63a5\u5efa\u6a21\u5927\u8111\u72b6\u6001\u8f68\u8ff9\u7684\u51e0\u4f55\u72b6\u6001\u7a7a\u95f4\u6846\u67b6\uff0c\u4e0d\u4ec5\u9002\u7528\u4e8e\u795e\u7ecf\u79d1\u5b66\uff0c\u8fd8\u80fd\u8de8\u9886\u57df\u5efa\u6a21\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u529b\u5b66\u3002"}}
{"id": "2601.13572", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13572", "abs": "https://arxiv.org/abs/2601.13572", "authors": ["Xiangchi Yuan", "Dachuan Shi", "Chunhui Zhang", "Zheyuan Liu", "Shenglong Yao", "Soroush Vosoughi", "Wenke Lee"], "title": "Behavior Knowledge Merge in Reinforced Agentic Models", "comment": null, "summary": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.", "AI": {"tldr": "RAM\uff1a\u9488\u5bf9RL\u8bad\u7ec3\u667a\u80fd\u4f53\u6a21\u578b\u7684\u5206\u5e03\u611f\u77e5\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u66f4\u65b0\uff0c\u89e3\u51b3\u6807\u51c6\u878d\u5408\u65b9\u6cd5\u5728RL\u667a\u80fd\u4f53\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u76d1\u7763\u5fae\u8c03\u8bbe\u8ba1\uff0c\u4e0d\u9002\u7528\u4e8eRL\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u6a21\u578b\u3002RL\u4ea7\u751f\u7684\u4efb\u52a1\u5411\u91cf\u7a00\u758f\u4e14\u5f02\u6784\uff0c\u800cSFT\u878d\u5408\u5047\u8bbe\u5bc6\u96c6\u4e14\u5168\u5c40\u53ef\u6bd4\uff0c\u5bfc\u81f4\u6807\u51c6\u5168\u5c40\u5e73\u5747\u4f1a\u7a00\u91caRL\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\u7684\u5173\u952e\u53c2\u6570\u66f4\u65b0", "method": "RAM\u6846\u67b6\uff1a1\uff09\u89e3\u8026\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u72ec\u7279\u53c2\u6570\u66f4\u65b0\uff1b2\uff09\u5bf9\u5171\u4eab\u7ec4\u4ef6\u8fdb\u884c\u5e73\u5747\uff1b3\uff09\u9009\u62e9\u6027\u4fdd\u7559\u5e76\u91cd\u65b0\u7f29\u653e\u72ec\u7279\u7ec4\u4ef6\u4ee5\u62b5\u6d88\u53c2\u6570\u66f4\u65b0\u7a00\u91ca", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u9886\u57df\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRAM\u4e0d\u4ec5\u8d85\u8d8a\u73b0\u6709\u878d\u5408\u57fa\u7ebf\uff0c\u8fd8\u80fd\u89e3\u9501\u667a\u80fd\u4f53\u95f4\u7684\u534f\u540c\u6f5c\u529b\uff0c\u5b9e\u73b0\u4f18\u4e8e\u5404\u81ea\u9886\u57df\u4e13\u95e8\u667a\u80fd\u4f53\u7684\u6027\u80fd", "conclusion": "RAM\u662f\u9488\u5bf9RL\u8bad\u7ec3\u667a\u80fd\u4f53\u6a21\u578b\u7684\u6709\u6548\u878d\u5408\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u5411\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u5e76\u5b9e\u73b0\u667a\u80fd\u4f53\u95f4\u7684\u534f\u540c\u6548\u5e94"}}
{"id": "2601.13578", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13578", "abs": "https://arxiv.org/abs/2601.13578", "authors": ["Qian Feng", "JiaHang Tu", "Mintong Kang", "Hanbin Zhao", "Chao Zhang", "Hui Qian"], "title": "FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality for Incremental Unlearning", "comment": "This paper has been accepted by ICCV 2025. code: \\url{https://github.com/RAIAN08/FG-OrIU}", "summary": "Incremental unlearning (IU) is critical for pre-trained models to comply with sequential data deletion requests, yet existing methods primarily suppress parameters or confuse knowledge without explicit constraints on both feature and gradient level, resulting in \\textit{superficial forgetting} where residual information remains recoverable. This incomplete forgetting risks security breaches and disrupts retention balance, especially in IU scenarios. We propose FG-OrIU (\\textbf{F}eature-\\textbf{G}radient \\textbf{Or}thogonality for \\textbf{I}ncremental \\textbf{U}nlearning), the first framework unifying orthogonal constraints on both features and gradients level to achieve deep forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes feature spaces via Singular Value Decomposition (SVD), separating forgetting and remaining class features into distinct subspaces. It then enforces dual constraints: feature orthogonal projection on both forgetting and remaining classes, while gradient orthogonal projection prevents the reintroduction of forgotten knowledge and disruption to remaining classes during updates. Additionally, dynamic subspace adaptation merges newly forgetting subspaces and contracts remaining subspaces, ensuring a stable balance between removal and retention across sequential unlearning tasks. Extensive experiments demonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51faFG-OrIU\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5f81\u548c\u68af\u5ea6\u7684\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\u5b9e\u73b0\u6df1\u5ea6\u9057\u5fd8\uff0c\u89e3\u51b3\u589e\u91cf\u9057\u5fd8\u4e2d\u6b8b\u7559\u4fe1\u606f\u53ef\u6062\u590d\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u589e\u91cf\u9057\u5fd8\u65b9\u6cd5\u4e3b\u8981\u5728\u53c2\u6570\u5c42\u9762\u6291\u5236\u6216\u6df7\u6dc6\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7684\u660e\u786e\u7ea6\u675f\uff0c\u5bfc\u81f4\"\u8868\u9762\u9057\u5fd8\"\u2014\u2014\u6b8b\u7559\u4fe1\u606f\u4ecd\u53ef\u6062\u590d\u3002\u8fd9\u79cd\u4e0d\u5b8c\u6574\u7684\u9057\u5fd8\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u7834\u574f\u4fdd\u7559\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u589e\u91cf\u9057\u5fd8\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faFG-OrIU\u6846\u67b6\uff1a1) \u901a\u8fc7SVD\u5206\u89e3\u7279\u5f81\u7a7a\u95f4\uff0c\u5c06\u9057\u5fd8\u7c7b\u548c\u4fdd\u7559\u7c7b\u7279\u5f81\u5206\u79bb\u5230\u4e0d\u540c\u5b50\u7a7a\u95f4\uff1b2) \u5b9e\u65bd\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\uff1a\u7279\u5f81\u6b63\u4ea4\u6295\u5f71\u9632\u6b62\u7279\u5f81\u6df7\u5408\uff0c\u68af\u5ea6\u6b63\u4ea4\u6295\u5f71\u9632\u6b62\u9057\u5fd8\u77e5\u8bc6\u91cd\u65b0\u5f15\u5165\u548c\u4fdd\u7559\u7c7b\u88ab\u5e72\u6270\uff1b3) \u52a8\u6001\u5b50\u7a7a\u95f4\u9002\u5e94\uff1a\u5408\u5e76\u65b0\u9057\u5fd8\u5b50\u7a7a\u95f4\u5e76\u6536\u7f29\u4fdd\u7559\u5b50\u7a7a\u95f4\uff0c\u786e\u4fdd\u5e8f\u5217\u9057\u5fd8\u4efb\u52a1\u4e2d\u79fb\u9664\u4e0e\u4fdd\u7559\u7684\u7a33\u5b9a\u5e73\u8861\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u6df1\u5ea6\u9057\u5fd8\uff08\u9057\u5fd8\u6548\u679c\u4e0d\u53ef\u9006\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u6b8b\u7559\u4fe1\u606f\u53ef\u6062\u590d\u7684\u95ee\u9898\u3002", "conclusion": "FG-OrIU\u662f\u9996\u4e2a\u5728\u7279\u5f81\u548c\u68af\u5ea6\u5c42\u9762\u7edf\u4e00\u6b63\u4ea4\u7ea6\u675f\u7684\u589e\u91cf\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u6b63\u4ea4\u7ea6\u675f\u548c\u52a8\u6001\u5b50\u7a7a\u95f4\u9002\u5e94\u5b9e\u73b0\u4e86\u6df1\u5ea6\u9057\u5fd8\uff0c\u786e\u4fdd\u4e86\u9057\u5fd8\u7684\u4e0d\u53ef\u9006\u6027\u548c\u4fdd\u7559\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2601.13580", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13580", "abs": "https://arxiv.org/abs/2601.13580", "authors": ["Ahmad Al-Zuraiqi"], "title": "Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation for Transformer Models", "comment": "27 pages, 8 figures, 16 tables. Decoder-only transformers (124M-20B parameters). Complete experimental results and reproducibility details in appendices. Code and checkpoints: https://github.com/zuraiqi/neural-organ-transplant", "summary": "We introduce Neural Organ Transplantation (NOT), a modular adaptation framework that enables trained transformer layers to function as reusable transferable checkpoints for domain adaptation. Unlike conventional fine-tuning approaches that tightly couple trained parameters to specific model instances and training data, NOT extracts contiguous layer subsets (\"donor organs\") from pre-trained models, trains them independently on domain-specific data, and saves them as standalone checkpoint files that can be transplanted into compatible recipient models without access to the original training data. Through experiments on three decoder-only transformer architectures spanning 124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that donor transplantation substantially outperforms existing adaptation methods, achieving an order-of-magnitude improvement in perplexity over LoRA while training significantly faster. The method exhibits position dependence, with early insertion positions yielding optimal results. Cross-domain transfer at billion-parameter scale reveals unexpected regularization benefits. These findings demonstrate that transformer middle layers can support efficient modular transfer for decoder-only architectures, enabling privacy-preserving expertise sharing through checkpoint distribution. We note that this approach is currently limited to decoder-only models; preliminary experiments on encoder-based architectures show reduced effectiveness.", "AI": {"tldr": "Neural Organ Transplantation (NOT) \u662f\u4e00\u79cd\u6a21\u5757\u5316\u9002\u914d\u6846\u67b6\uff0c\u53ef\u5c06\u8bad\u7ec3\u597d\u7684Transformer\u5c42\u4f5c\u4e3a\u53ef\u91cd\u7528\u3001\u53ef\u79fb\u690d\u7684\u68c0\u67e5\u70b9\u8fdb\u884c\u9886\u57df\u9002\u914d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5c06\u8bad\u7ec3\u53c2\u6570\u4e0e\u7279\u5b9a\u6a21\u578b\u5b9e\u4f8b\u548c\u8bad\u7ec3\u6570\u636e\u7d27\u5bc6\u8026\u5408\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u590d\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u53d6\u548c\u5171\u4eab\u8bad\u7ec3\u77e5\u8bc6\u800c\u4e0d\u66b4\u9732\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u4ece\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u63d0\u53d6\u8fde\u7eed\u7684\u5c42\u5b50\u96c6\uff08\"\u4f9b\u4f53\u5668\u5b98\"\uff09\uff0c\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e0a\u72ec\u7acb\u8bad\u7ec3\uff0c\u4fdd\u5b58\u4e3a\u72ec\u7acb\u7684\u68c0\u67e5\u70b9\u6587\u4ef6\uff0c\u7136\u540e\u79fb\u690d\u5230\u517c\u5bb9\u7684\u63a5\u6536\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728124M\u523020B\u53c2\u6570\u7684\u4e09\u79cd\u89e3\u7801\u5668\u67b6\u6784\uff08GPT-2\u3001TinyLlama\u3001GPT-OSS\uff09\u4e0a\uff0cNOT\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u9002\u914d\u65b9\u6cd5\uff0c\u56f0\u60d1\u5ea6\u6bd4LoRA\u63d0\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3002\u79fb\u690d\u4f4d\u7f6e\u6709\u4f9d\u8d56\u6027\uff0c\u65e9\u671f\u63d2\u5165\u6548\u679c\u6700\u4f73\u3002", "conclusion": "Transformer\u4e2d\u95f4\u5c42\u652f\u6301\u89e3\u7801\u5668\u67b6\u6784\u7684\u9ad8\u6548\u6a21\u5757\u5316\u8fc1\u79fb\uff0c\u901a\u8fc7\u68c0\u67e5\u70b9\u5206\u53d1\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u4e13\u5bb6\u77e5\u8bc6\u5171\u4eab\u3002\u8be5\u65b9\u6cd5\u76ee\u524d\u4ec5\u9650\u4e8e\u89e3\u7801\u5668\u6a21\u578b\uff0c\u5728\u7f16\u7801\u5668\u67b6\u6784\u4e0a\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2601.13592", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13592", "abs": "https://arxiv.org/abs/2601.13592", "authors": ["Hao Jing", "Sa Xiao", "Haoyu Li", "Huadong Xiao", "Wei Xue"], "title": "Machine learning based radiative parameterization scheme and its performance in operational reforecast experiments", "comment": null, "summary": "Radiation is typically the most time-consuming physical process in numerical models. One solution is to use machine learning methods to simulate the radiation process to improve computational efficiency. From an operational standpoint, this study investigates critical limitations inherent to hybrid forecasting frameworks that embed deep neural networks into numerical prediction models, with a specific focus on two fundamental bottlenecks: coupling compatibility and long-term integration stability. A residual convolutional neural network is employed to approximate the Rapid Radiative Transfer Model for General Circulation Models (RRTMG) within the global operational system of China Meteorological Administration. We adopted an offline training and online coupling approach. First, a comprehensive dataset is generated through model simulations, encompassing all atmospheric columns both with and without cloud cover. To ensure the stability of the hybrid model, the dataset is enhanced via experience replay, and additional output constraints based on physical significance are imposed. Meanwhile, a LibTorch-based coupling method is utilized, which is more suitable for real-time operational computations. The hybrid model is capable of performing ten-day integrated forecasts as required. A two-month operational reforecast experiment demonstrates that the machine learning emulator achieves accuracy comparable to that of the traditional physical scheme, while accelerating the computation speed by approximately eightfold.", "AI": {"tldr": "\u4f7f\u7528\u6b8b\u5dee\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u8f90\u5c04\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u7ea68\u500d\uff0c\u5b9e\u73b0\u5341\u5929\u96c6\u6210\u9884\u62a5", "motivation": "\u8f90\u5c04\u8fc7\u7a0b\u662f\u6570\u503c\u6a21\u578b\u4e2d\u6700\u8017\u65f6\u7684\u7269\u7406\u8fc7\u7a0b\uff0c\u9700\u8981\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff1b\u540c\u65f6\u9700\u8981\u89e3\u51b3\u6df7\u5408\u9884\u62a5\u6846\u67b6\u4e2d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e0e\u6570\u503c\u9884\u62a5\u6a21\u578b\u8026\u5408\u7684\u4e24\u4e2a\u5173\u952e\u74f6\u9888\uff1a\u8026\u5408\u517c\u5bb9\u6027\u548c\u957f\u671f\u79ef\u5206\u7a33\u5b9a\u6027", "method": "\u91c7\u7528\u6b8b\u5dee\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3cRRTMG\u8f90\u5c04\u65b9\u6848\uff0c\u4f7f\u7528\u79bb\u7ebf\u8bad\u7ec3\u548c\u5728\u7ebf\u8026\u5408\u65b9\u6cd5\u3002\u901a\u8fc7\u6a21\u578b\u6a21\u62df\u751f\u6210\u5305\u542b\u6709\u4e91\u548c\u65e0\u4e91\u5927\u6c14\u67f1\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u91c7\u7528\u7ecf\u9a8c\u56de\u653e\u589e\u5f3a\u6570\u636e\u96c6\u7a33\u5b9a\u6027\uff0c\u5e76\u57fa\u4e8e\u7269\u7406\u610f\u4e49\u65bd\u52a0\u989d\u5916\u8f93\u51fa\u7ea6\u675f\u3002\u4f7f\u7528\u57fa\u4e8eLibTorch\u7684\u8026\u5408\u65b9\u6cd5\u8fdb\u884c\u5b9e\u65f6\u8ba1\u7b97", "result": "\u6df7\u5408\u6a21\u578b\u80fd\u591f\u6309\u8981\u6c42\u8fdb\u884c\u5341\u5929\u96c6\u6210\u9884\u62a5\u3002\u4e24\u4e2a\u6708\u7684\u4e1a\u52a1\u56de\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u62df\u5668\u8fbe\u5230\u4e0e\u4f20\u7edf\u7269\u7406\u65b9\u6848\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u7ea68\u500d", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u4e2d\u56fd\u6c14\u8c61\u5c40\u5168\u7403\u4e1a\u52a1\u7cfb\u7edf\u7684\u673a\u5668\u5b66\u4e60\u8f90\u5c04\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u8026\u5408\u517c\u5bb9\u6027\u548c\u957f\u671f\u79ef\u5206\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387"}}
{"id": "2601.13599", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13599", "abs": "https://arxiv.org/abs/2601.13599", "authors": ["Linrui Ma", "Yufei Cui", "Kai Han", "Yunhe Wang"], "title": "Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block Diffusion Models", "comment": "Work In Progress", "summary": "Block diffusion language models, operating as semi-autoregressive paradigms, combine the strengths of both autoregressive and diffusion paradigms. However, their strict unidirectional block dependencies introduce irreversibility and sacrifice the global planning capabilities for which diffusion models are renowned. In order to address these issues, we propose Diffusion in Diffusion, a draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilise snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.", "AI": {"tldr": "\u63d0\u51faDiffusion in Diffusion\u6846\u67b6\uff0c\u901a\u8fc7\"\u8349\u7a3f-\u7cbe\u70bc\"\u4e24\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u5757\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u53ef\u9006\u6027\u548c\u77ed\u89c6\u95ee\u9898\uff0c\u5728OpenWebText\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5757\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u4e25\u683c\u7684\u5355\u5411\u5757\u4f9d\u8d56\u5bfc\u81f4\u4e0d\u53ef\u9006\u6027\uff0c\u5e76\u727a\u7272\u4e86\u6269\u6563\u6a21\u578b\u8457\u540d\u7684\u5168\u5c40\u89c4\u5212\u80fd\u529b\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u91c7\u7528\"\u8349\u7a3f-\u7cbe\u70bc\"\u6846\u67b6\uff1a1) \u4f7f\u7528\u5c0f\u5757\u8fdb\u884c\u5757\u6269\u6563\u5feb\u901f\u751f\u6210\u8349\u7a3f\uff1b2) \u901a\u8fc7\u5177\u6709\u66f4\u5927\u53cc\u5411\u611f\u53d7\u91ce\u7684\u5168\u5c40\u53cc\u5411\u6269\u6563\u7cbe\u70bc\u8349\u7a3f\uff1b3) \u4f7f\u7528\u5feb\u7167\u7f6e\u4fe1\u5ea6\u91cd\u63a9\u7801\u8bc6\u522b\u9700\u8981\u4fee\u6539\u7684\u5173\u952etoken\uff1b4) \u5e94\u7528\u6df7\u5408\u5c3a\u5ea6\u8bad\u7ec3\u6269\u5c55\u5757\u6269\u6563\u6a21\u578b\u7684\u5168\u5c40\u80fd\u529b\u3002", "result": "\u5728OpenWebText\u6570\u636e\u96c6\u4e0a\u4e3a\u79bb\u6563\u6269\u6563\u6a21\u578b\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff1a\u4ec5\u4f7f\u7528\u57fa\u7ebf\u6a21\u578b26%\u7684\u5fae\u8c03\u9884\u7b97\uff0c\u5c06\u751f\u6210\u56f0\u60d1\u5ea6\u4ece25.7\u964d\u4f4e\u523021.9\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "Diffusion in Diffusion\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5757\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u9006\u6027\u548c\u77ed\u89c6\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5168\u5c40\u89c4\u5212\u80fd\u529b\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2601.13608", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13608", "abs": "https://arxiv.org/abs/2601.13608", "authors": ["Zhipeng Chang", "Ting He", "Wenrui Hao"], "title": "Fisher-Informed Parameterwise Aggregation for Federated Learning with Heterogeneous Data", "comment": null, "summary": "Federated learning aggregates model updates from distributed clients, but standard first order methods such as FedAvg apply the same scalar weight to all parameters from each client. Under non-IID data, these uniformly weighted updates can be strongly misaligned across clients, causing client drift and degrading the global model. Here we propose Fisher-Informed Parameterwise Aggregation (FIPA), a second-order aggregation method that replaces client-level scalar weights with parameter-specific Fisher Information Matrix (FIM) weights, enabling true parameter-level scaling that captures how each client's data uniquely influences different parameters. With low-rank approximation, FIPA remains communication- and computation-efficient. Across nonlinear function regression, PDE learning, and image classification, FIPA consistently improves over averaging-based aggregation, and can be effectively combined with state-of-the-art client-side optimization algorithms to further improve image classification accuracy. These results highlight the benefits of FIPA for federated learning under heterogeneous data distributions.", "AI": {"tldr": "\u63d0\u51faFIPA\u65b9\u6cd5\uff0c\u4f7f\u7528Fisher\u4fe1\u606f\u77e9\u9635\u8fdb\u884c\u53c2\u6570\u7ea7\u805a\u5408\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u4e0b\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898", "motivation": "\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5982FedAvg\u5bf9\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u6240\u6709\u53c2\u6570\u4f7f\u7528\u76f8\u540c\u7684\u6807\u91cf\u6743\u91cd\uff0c\u5728\u975eIID\u6570\u636e\u4e0b\uff0c\u8fd9\u4e9b\u5747\u5300\u52a0\u6743\u7684\u66f4\u65b0\u4f1a\u5728\u5ba2\u6237\u7aef\u95f4\u4ea7\u751f\u5f3a\u70c8\u9519\u4f4d\uff0c\u5bfc\u81f4\u5ba2\u6237\u7aef\u6f02\u79fb\u5e76\u964d\u4f4e\u5168\u5c40\u6a21\u578b\u6027\u80fd", "method": "\u63d0\u51faFisher\u4fe1\u606f\u53c2\u6570\u7ea7\u805a\u5408(FIPA)\uff0c\u7528\u53c2\u6570\u7279\u5b9a\u7684Fisher\u4fe1\u606f\u77e9\u9635\u6743\u91cd\u66ff\u4ee3\u5ba2\u6237\u7aef\u7ea7\u6807\u91cf\u6743\u91cd\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u53c2\u6570\u7ea7\u7f29\u653e\uff0c\u6355\u6349\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6570\u636e\u5bf9\u4e0d\u540c\u53c2\u6570\u7684\u72ec\u7279\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u4fdd\u6301\u901a\u4fe1\u548c\u8ba1\u7b97\u6548\u7387", "result": "\u5728\u975e\u7ebf\u6027\u51fd\u6570\u56de\u5f52\u3001PDE\u5b66\u4e60\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cFIPA\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u5e73\u5747\u7684\u805a\u5408\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u5ba2\u6237\u7aef\u4f18\u5316\u7b97\u6cd5\u6709\u6548\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387", "conclusion": "FIPA\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u901a\u8fc7\u53c2\u6570\u7ea7\u805a\u5408\u6539\u5584\u4e86\u6a21\u578b\u6027\u80fd"}}
{"id": "2601.13645", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.13645", "abs": "https://arxiv.org/abs/2601.13645", "authors": ["Euijin You", "Hyang-Won Lee"], "title": "Quadratic Upper Bound for Boosting Robustness", "comment": "Accepted at ICML 2025. Published in PMLR 267:72656-72676", "summary": "Fast adversarial training (FAT) aims to enhance the robustness of models against adversarial attacks with reduced training time, however, FAT often suffers from compromised robustness due to insufficient exploration of adversarial space. In this paper, we develop a loss function to mitigate the problem of degraded robustness under FAT. Specifically, we derive a quadratic upper bound (QUB) on the adversarial training (AT) loss function and propose to utilize the bound with existing FAT methods. Our experimental results show that applying QUB loss to the existing methods yields significant improvement of robustness. Furthermore, using various metrics, we demonstrate that this improvement is likely to result from the smoothened loss landscape of the resulting model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e8c\u6b21\u4e0a\u754c\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u6539\u5584\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u4e2d\u56e0\u5bf9\u6297\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u5bfc\u81f4\u7684\u9c81\u68d2\u6027\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u867d\u7136\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\uff0c\u4f46\u5f80\u5f80\u56e0\u5bf9\u6297\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u800c\u5bfc\u81f4\u6a21\u578b\u9c81\u68d2\u6027\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u63a8\u5bfc\u51fa\u5bf9\u6297\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u7684\u4e8c\u6b21\u4e0a\u754c\uff0c\u5e76\u5c06\u8be5\u4e0a\u754c\u4e0e\u73b0\u6709\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c06QUB\u635f\u5931\u5e94\u7528\u4e8e\u73b0\u6709\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e14\u8fd9\u79cd\u6539\u8fdb\u53ef\u80fd\u6e90\u4e8e\u6240\u5f97\u6a21\u578b\u635f\u5931\u666f\u89c2\u7684\u5e73\u6ed1\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e8c\u6b21\u4e0a\u754c\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u7f13\u89e3\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u9c81\u68d2\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u6ed1\u635f\u5931\u666f\u89c2\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.13653", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13653", "abs": "https://arxiv.org/abs/2601.13653", "authors": ["Xingjian Wu", "Junkai Lu", "Zhengyu Li", "Xiangfei Qiu", "Jilin Hu", "Chenjuan Guo", "Christian S. Jensen", "Bin Yang"], "title": "TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation", "comment": null, "summary": "Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.", "AI": {"tldr": "TimeART\uff1a\u878d\u5408\u5f3a\u5de5\u5177\u5206\u6790\u80fd\u529b\u4e0eLLM\u63a8\u7406\u80fd\u529b\u7684\u5168\u81ea\u52a8\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7100k\u4e13\u5bb6\u8f68\u8ff9\u6570\u636e\u96c6\u548c\u56db\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u591a\u9879TSQA\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u667a\u80fd\u7cfb\u7edf", "method": "1. \u63d0\u51faTimeART\u6846\u67b6\uff0c\u878d\u5408\u73b0\u6210\u5de5\u5177\u7684\u5206\u6790\u80fd\u529b\u548cLLM\u7684\u63a8\u7406\u80fd\u529b\uff1b2. \u6536\u96c6100k\u4e13\u5bb6\u8f68\u8ff9\u6570\u636e\u96c6TimeToolBench\uff1b3. \u8bbe\u8ba1\u56db\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u8ba9\u6a21\u578b\u4ece\u65e9\u671f\u7ecf\u9a8c\u548c\u81ea\u6211\u53cd\u601d\u4e2d\u5b66\u4e60", "result": "\u8bad\u7ec3\u51fa8B\u53c2\u6570\u7684TSRM\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e00\u81f4\u6027\u7684\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "TimeART\u5f00\u521b\u4e86\u4ee3\u7406\u5f0f\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5168\u81ea\u52a8\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5206\u6790\u79d1\u5b66\u5bb6"}}
{"id": "2601.13676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13676", "abs": "https://arxiv.org/abs/2601.13676", "authors": ["Fabian Greifeneder", "Wolfgang Fenz", "Benedikt Alkin", "Johannes Brandstetter", "Michael Giretzlehner", "Philipp Moser"], "title": "Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery", "comment": null, "summary": "Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8111\u7ec4\u7ec7\u53d8\u5f62\u5feb\u901f\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u53d8\u6362\u5668\u548c\u968f\u673a\u6559\u5e08\u5f3a\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u795e\u7ecf\u5916\u79d1\u624b\u672f\u6a21\u62df\u4e2d\u7684\u5b9e\u65f6\u4ea4\u4e92", "motivation": "\u795e\u7ecf\u5916\u79d1\u624b\u672f\u6a21\u62df\u5668\u9700\u8981\u51c6\u786e\u6a21\u62df\u8111\u7ec4\u7ec7\u53d8\u5f62\uff0c\u4f46\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u66ff\u4ee3\u6a21\u578b", "method": "\u57fa\u4e8e\u901a\u7528\u7269\u7406\u53d8\u6362\u5668\u6784\u5efa\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\uff0c\u76f4\u63a5\u5904\u7406\u5927\u89c4\u6a21\u7f51\u683c\u6570\u636e\uff0c\u91c7\u7528\u968f\u673a\u6559\u5e08\u5f3a\u5236\u7b56\u7565\u51cf\u5c11\u81ea\u56de\u5f52\u63a8\u7406\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef", "result": "\u6a21\u578b\u5728\u591a\u79cd\u77ac\u6001\u8111\u53d8\u5f62\u573a\u666f\u4e2d\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u652f\u630115\u4e07\u4e2a\u8282\u70b9\u7684\u7f51\u683c\uff0c\u6700\u5927\u9884\u6d4b\u8bef\u5dee\u4ece6.7mm\u964d\u81f33.5mm\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u6bcf\u6b65\u6a21\u62df\u65f6\u95f4\u4f4e\u4e8e10ms", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u3001\u5e73\u6ed1\u3001\u51c6\u786e\u7684\u52a8\u6001\u8111\u7ec4\u7ec7\u751f\u7269\u529b\u5b66\u6a21\u62df\uff0c\u4e3a\u73b0\u5b9e\u624b\u672f\u8bad\u7ec3\u73af\u5883\u5960\u5b9a\u57fa\u7840"}}
{"id": "2601.13710", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.13710", "abs": "https://arxiv.org/abs/2601.13710", "authors": ["Sayeed Shafayet Chowdhury", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Vijay R. Ramakrishnan"], "title": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction", "comment": null, "summary": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u76d1\u7763\u5f0f\u673a\u5668\u5b66\u4e60\u4e0e\u751f\u6210\u5f0fAI\u5728\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u624b\u672f\u6548\u679c\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0MLP\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5efa\u8bae\u91c7\u7528ML\u4e3a\u4e3b\u3001GenAI\u4e3a\u8f85\u7684\u5de5\u4f5c\u6d41\u7a0b", "motivation": "\u5c3d\u7ba1AI\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u8fdb\u884c\u524d\u77bb\u6027\u51b3\u7b56\u652f\u6301\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u672f\u524d\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u624b\u672f\u6548\u679c\u7684\u53ef\u80fd\u6027\uff0c\u8bc6\u522b\u90a3\u4e9b\u624b\u672f\u6548\u679c\u4e0d\u4f73\u7684\u60a3\u8005\uff0c\u5e2e\u52a9\u4ed6\u4eec\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u624b\u672f\u3002", "method": "\u5728\u524d\u77bb\u6027\u6536\u96c6\u7684\u961f\u5217\u4e2d\uff0c\u6bd4\u8f83\u76d1\u7763\u5f0f\u673a\u5668\u5b66\u4e60\uff08\u903b\u8f91\u56de\u5f52\u3001\u6811\u96c6\u6210\u3001MLP\uff09\u4e0e\u751f\u6210\u5f0fAI\uff08ChatGPT\u3001Claude\u3001Gemini\u3001Perplexity\uff09\u7684\u8868\u73b0\u3002\u6240\u6709\u6a21\u578b\u63a5\u6536\u76f8\u540c\u7684\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u8f93\u51fa\u4e8c\u5143\u63a8\u8350\u53ca\u7f6e\u4fe1\u5ea6\u3002\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u8868\u683c\u6570\u636e\u5230\u751f\u6210\u5f0fAI\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u6700\u4f73ML\u6a21\u578b\uff08MLP\uff09\u8fbe\u523085%\u51c6\u786e\u7387\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u6821\u51c6\u548c\u51b3\u7b56\u66f2\u7ebf\u51c0\u6548\u76ca\u3002\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5728\u533a\u5206\u5ea6\u548c\u6821\u51c6\u65b9\u9762\u8868\u73b0\u8f83\u5dee\u3002\u751f\u6210\u5f0fAI\u7684\u63a8\u7406\u4e0e\u4e34\u5e8a\u7ecf\u9a8c\u548cMLP\u7279\u5f81\u91cd\u8981\u6027\u4e00\u81f4\uff0c\u90fd\u5f3a\u8c03\u57fa\u7ebfSNOT-22\u3001CT/\u5185\u7aa5\u955c\u4e25\u91cd\u7a0b\u5ea6\u3001\u606f\u8089\u8868\u578b\u548c\u5fc3\u7406/\u75bc\u75db\u5408\u5e76\u75c7\u3002", "conclusion": "\u652f\u6301\u91c7\u7528ML\u4f18\u5148\u3001GenAI\u589e\u5f3a\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a\u90e8\u7f72\u6821\u51c6\u7684ML\u8fdb\u884c\u624b\u672f\u5019\u9009\u8005\u7684\u521d\u6b65\u7b5b\u67e5\uff0c\u4f7f\u7528GenAI\u4f5c\u4e3a\u89e3\u91ca\u5668\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u5171\u540c\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2601.13748", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.13748", "abs": "https://arxiv.org/abs/2601.13748", "authors": ["Tien-Dat Pham", "Xuan-The Tran"], "title": "EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory", "comment": null, "summary": "Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation", "AI": {"tldr": "EEG-Titans\uff1a\u4e00\u79cd\u53cc\u5206\u652f\u67b6\u6784\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u5faa\u73af\u8bb0\u5fc6\u673a\u5236\uff0c\u7528\u4e8e\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\uff0c\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.46%\u7684\u5e73\u5747\u6bb5\u7ea7\u7075\u654f\u5ea6", "motivation": "\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u53d1\u4f5c\u524d\u52a8\u6001\u53ef\u80fd\u8de8\u8d8a\u957f\u65f6\u95f4\u8303\u56f4\uff0c\u800c\u4e34\u5e8a\u76f8\u5173\u7279\u5f81\u53ef\u80fd\u5fae\u5999\u4e14\u77ed\u6682\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6355\u83b7\u5c40\u90e8\u65f6\u7a7a\u6a21\u5f0f\u548c\u4fdd\u6301\u957f\u7a0b\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u6743\u8861", "method": "\u63d0\u51faEEG-Titans\u53cc\u5206\u652f\u67b6\u6784\uff1a1\uff09\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u5206\u652f\u6355\u83b7\u77ed\u671f\u5f02\u5e38\uff1b2\uff09\u5faa\u73af\u8bb0\u5fc6\u8def\u5f84\u603b\u7ed3\u968f\u65f6\u95f4\u53d8\u5316\u7684\u7f13\u6162\u6e10\u8fdb\u8d8b\u52bf\u3002\u91c7\u7528\u73b0\u4ee3\u795e\u7ecf\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21", "result": "\u5728CHB-MIT\u5934\u76aeEEG\u6570\u636e\u96c6\u4e0a\uff0c\u6309\u65f6\u95f4\u987a\u5e8f\u4fdd\u7559\u534f\u8bae\u8bc4\u4f30\uff0cEEG-Titans\u572818\u540d\u53d7\u8bd5\u8005\u4e2d\u8fbe\u523099.46%\u7684\u5e73\u5747\u6bb5\u7ea7\u7075\u654f\u5ea6\u3002\u901a\u8fc7\u5206\u5c42\u4e0a\u4e0b\u6587\u7b56\u7565\u6269\u5c55\u9ad8\u566a\u58f0\u53d7\u8bd5\u8005\u7684\u611f\u53d7\u91ce\uff0c\u53ef\u663e\u8457\u51cf\u5c11\u8bef\u62a5\uff08\u6781\u7aef\u5f02\u5e38\u503c\u4e2d\u964d\u81f30.00 FPR/h\uff09\u800c\u4e0d\u727a\u7272\u7075\u654f\u5ea6", "conclusion": "\u8bb0\u5fc6\u589e\u5f3a\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u53ef\u4ee5\u5728\u4e34\u5e8a\u7ea6\u675f\u8bc4\u4f30\u4e0b\u63d0\u4f9b\u7a33\u5065\u7684\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6355\u83b7\u5c40\u90e8\u5f02\u5e38\u548c\u7ef4\u6301\u957f\u7a0b\u4e0a\u4e0b\u6587\u4fe1\u606f\u65b9\u9762\u5177\u6709\u4f18\u52bf"}}
{"id": "2601.13768", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13768", "abs": "https://arxiv.org/abs/2601.13768", "authors": ["Wenzhen Yue", "Ruohao Guo", "Ji Shi", "Zihan Hao", "Shiyu Hu", "Xianghua Ying"], "title": "vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting", "comment": null, "summary": "In this paper, we present \\textbf{vLinear}, an effective yet efficient \\textbf{linear}-based multivariate time series forecaster featuring two components: the \\textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \\textbf{velocity-oriented} flow matching objectives, we demonstrate that a \\textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.", "AI": {"tldr": "vLinear\u662f\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u8fd0\u7b97\u7684\u9ad8\u6548\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\uff0c\u5305\u542bvecTrans\u6a21\u5757\u548cWFMLoss\u76ee\u6807\u51fd\u6570\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u5668\u901a\u5e38\u4f9d\u8d56\u81ea\u6ce8\u610f\u529b\u6216\u5176\u53d8\u4f53\u6765\u6355\u6349\u591a\u5143\u76f8\u5173\u6027\uff0c\u8fd9\u901a\u5e38\u5bfc\u81f4\u4e0e\u53d8\u91cf\u6570N\u76f8\u5173\u7684O(N\u00b2)\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) vecTrans\u6a21\u5757\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u5411\u91cf\u5efa\u6a21\u591a\u5143\u76f8\u5173\u6027\uff0c\u5c06\u590d\u6742\u5ea6\u964d\u81f3O(N)\uff1b2) WFMLoss\u76ee\u6807\u51fd\u6570\uff0c\u91c7\u7528\u6700\u7ec8\u5e8f\u5217\u5bfc\u5411\u7684\u6d41\u5339\u914d\u635f\u5931\uff0c\u7ed3\u5408\u8def\u5f84\u548c\u65f6\u57df\u52a0\u6743\u7b56\u7565\u3002", "result": "\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c124\u4e2a\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1bvecTrans\u53ef\u65e0\u7f1d\u96c6\u6210\u5230Transformer\u9884\u6d4b\u5668\u4e2d\uff0c\u5b9e\u73b0\u9ad8\u8fbe5\u500d\u7684\u63a8\u7406\u52a0\u901f\u548c\u6301\u7eed\u6027\u80fd\u63d0\u5347\uff1bWFMLoss\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u76ee\u6807\u51fd\u6570\u80fd\u6301\u7eed\u6539\u8fdb\u73b0\u6709\u9884\u6d4b\u5668\u3002", "conclusion": "vLinear\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u6a21\u5757\u548c\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2601.13780", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13780", "abs": "https://arxiv.org/abs/2601.13780", "authors": ["Antoine Siraudin", "Christopher Morris"], "title": "Principled Latent Diffusion for Graphs via Laplacian Autoencoders", "comment": "Preprint, under review", "summary": "Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\\times$ speed-up.", "AI": {"tldr": "LG-Flow\uff1a\u4e00\u79cd\u6f5c\u5728\u56fe\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u538b\u7f29\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6269\u6563\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u6269\u6563\u6a21\u578b\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8fd1\u65e0\u635f\u91cd\u5efa\u548c1000\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u56fe\u6269\u6563\u6a21\u578b\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e14\u5927\u90e8\u5206\u8ba1\u7b97\u80fd\u529b\u6d6a\u8d39\u5728\u5efa\u6a21\u7a00\u758f\u56fe\u4e2d\u7684\u8fb9\u7f3a\u5931\u4e0a\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u5728\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6269\u6563\uff0c\u540c\u65f6\u4fdd\u8bc1\u56fe\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u7f6e\u6362\u7b49\u53d8\u81ea\u7f16\u7801\u5668\u5c06\u6bcf\u4e2a\u8282\u70b9\u6620\u5c04\u5230\u56fa\u5b9a\u7ef4\u5ea6\u7684\u5d4c\u5165\u4e2d\uff0c\u4ece\u8be5\u5d4c\u5165\u4e2d\u53ef\u4ee5\u8bc1\u660e\u5b8c\u5168\u6062\u590d\u90bb\u63a5\u77e9\u9635\u3002\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u6d41\u5339\u914d\u8bad\u7ec3\u6269\u6563\u53d8\u6362\u5668\u3002", "result": "\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u56fe\u6269\u6563\u6a21\u578b\u7ade\u4e89\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u83b7\u5f97\u4e86\u9ad8\u8fbe1000\u500d\u7684\u52a0\u901f\uff0c\u6f5c\u5728\u8868\u793a\u7ef4\u5ea6\u4e0e\u8282\u70b9\u6570\u7ebf\u6027\u76f8\u5173\uff0c\u6d88\u9664\u4e86\u4e8c\u6b21\u74f6\u9888\u3002", "conclusion": "LG-Flow\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u6f5c\u5728\u6269\u6563\u4e2d\u7684\u91cd\u5efa\u7cbe\u5ea6\u6311\u6218\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13793", "abs": "https://arxiv.org/abs/2601.13793", "authors": ["ByeoungDo Kim", "JunYeop Na", "Kyungwook Tak", "JunTae Kim", "DongHyeon Kim", "Duckky Kim"], "title": "PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles", "comment": "7 pages, 3 figures, ITSC 2025, to be published", "summary": "In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684ETA\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6355\u6349\u5386\u53f2\u9053\u8def\u901f\u5ea6\u6a21\u5f0f\u7684\u65f6\u7a7a\u7279\u5f81\u6765\u63d0\u5347\u5230\u8fbe\u65f6\u95f4\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u51c6\u786e\u53ef\u9760\u7684ETA\u4f30\u8ba1\u5728\u5bfc\u822a\u3001\u51fa\u884c\u89c4\u5212\u548c\u4ea4\u901a\u7ba1\u7406\u4e2d\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4ea4\u901a\u6d41\u7684\u52a8\u6001\u590d\u6742\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u8981\u4e48\u7b80\u5355\u7ed3\u5408\u5b9e\u65f6\u548c\u5386\u53f2\u6570\u636e\uff0c\u8981\u4e48\u4f9d\u8d56\u590d\u6742\u7684\u89c4\u5219\u8ba1\u7b97\uff0c\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u80fd\u6709\u6548\u6355\u6349\u5173\u952e\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684ETA\u6a21\u578b\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u8def\u7ebf\u4e2d\u6bcf\u4e2a\u65f6\u7a7a\u70b9\u79ef\u7d2f\u7684\u65f6\u95f4\u7279\u5f81\uff0c\u6709\u6548\u6355\u6349\u65f6\u7a7a\u56e0\u679c\u5173\u7cfb\u3002\u8be5\u67b6\u6784\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u6574\u5408\u9053\u8def\u7279\u5f81\u3001\u5b9e\u65f6\u4ea4\u901a\u72b6\u51b5\u548c\u5386\u53f2\u901f\u5ea6\u6a21\u5f0f\u3002", "result": "\u5728\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u4ee5\u4efb\u52a1\u611f\u77e5\u7684\u65b9\u5f0f\u6709\u6548\u6574\u5408\u591a\u79cd\u4ea4\u901a\u4fe1\u606f\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684ETA\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u4f30\u8ba1\u5230\u8fbe\u65f6\u95f4\uff0c\u901a\u8fc7\u6709\u6548\u6355\u6349\u65f6\u7a7a\u6a21\u5f0f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.13824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13824", "abs": "https://arxiv.org/abs/2601.13824", "authors": ["Xiaohong Yang", "Tong Xie", "Minghui Liwang", "Chikai Shang", "Yang Lu", "Zhenzhen Jiao", "Liqun Fu", "Seyyedali Hosseinalipour"], "title": "ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks", "comment": "11 pages, 16 figures", "summary": "Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.", "AI": {"tldr": "ELSA\u662f\u4e00\u4e2a\u7ed3\u5408\u5206\u5272\u5b66\u4e60\u548c\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u7684\u9ad8\u6548LLM\u8fb9\u7f18\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5ba2\u6237\u7aef\u805a\u7c7b\u3001\u6a21\u578b\u5206\u5272\u548c\u8f7b\u91cf\u901a\u4fe1\u65b9\u6848\u89e3\u51b3\u8d44\u6e90\u9650\u5236\u3001\u6570\u636e\u5f02\u6784\u548c\u9690\u79c1\u98ce\u9669\u95ee\u9898\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u8bbe\u5907\u8d44\u6e90\u9650\u5236\u3001\u4e25\u91cd\u7684\u6570\u636e\u5f02\u6784\u6027\u4ee5\u53ca\u9690\u79c1\u98ce\u9669\u52a0\u5267\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4efb\u52a1\u65e0\u5173\u7684\u884c\u4e3a\u611f\u77e5\u5ba2\u6237\u7aef\u805a\u7c7b\u673a\u5236\uff1a\u4f7f\u7528\u516c\u5171\u63a2\u6d4b\u8f93\u5165\u548c\u5bf9\u79f0KL\u6563\u5ea6\u6784\u5efa\u8bed\u4e49\u6307\u7eb9\uff0c\u7ed3\u5408\u9884\u6d4b\u4e00\u81f4\u6027\u4fe1\u4efb\u8bc4\u5206\u548c\u5ef6\u8fdf\u611f\u77e5\u8fb9\u7f18\u5206\u914d\uff1b2. \u5c06LLM\u5206\u5272\u4e3a\u4e09\u90e8\u5206\u5206\u5e03\u5728\u5ba2\u6237\u7aef\u548c\u8fb9\u7f18\u670d\u52a1\u5668\uff0c\u4e91\u7aef\u4ec5\u7528\u4e8e\u9002\u914d\u5668\u805a\u5408\uff1b3. \u57fa\u4e8e\u8ba1\u7b97\u8349\u56fe\u7684\u8f7b\u91cf\u901a\u4fe1\u65b9\u6848\u7ed3\u5408\u8bed\u4e49\u5b50\u7a7a\u95f4\u6b63\u4ea4\u6270\u52a8(SS-OP)\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "result": "\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cELSA\u5728\u9002\u5e94\u6027\u3001\u6536\u655b\u884c\u4e3a\u548c\u9c81\u68d2\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u8fb9\u7f18\u4fa7LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "ELSA\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u5206\u5272\u5b66\u4e60\u548c\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18LLM\u5fae\u8c03\u4e2d\u7684\u8d44\u6e90\u9650\u5236\u3001\u6570\u636e\u5f02\u6784\u548c\u9690\u79c1\u98ce\u9669\u95ee\u9898\uff0c\u4e3a\u8fb9\u7f18\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2601.13844", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13844", "abs": "https://arxiv.org/abs/2601.13844", "authors": ["Gilad Karpel", "Edward Moroshko", "Ran Levinstein", "Ron Meir", "Daniel Soudry", "Itay Evron"], "title": "Optimal L2 Regularization in High-dimensional Continual Linear Regression", "comment": "Accepted to ALT 2026", "summary": "We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.", "AI": {"tldr": "\u5728\u8fc7\u53c2\u6570\u5316\u6301\u7eed\u7ebf\u6027\u56de\u5f52\u4e2d\uff0c\u5404\u5411\u540c\u6027\u6b63\u5219\u5316\u80fd\u7f13\u89e3\u6807\u7b7e\u566a\u58f0\uff0c\u6700\u4f18\u6b63\u5219\u5316\u5f3a\u5ea6\u968f\u4efb\u52a1\u6570T\u8fd1\u4f3c\u7ebf\u6027\u589e\u957f\u4e3aT/lnT", "motivation": "\u7814\u7a76\u8fc7\u53c2\u6570\u5316\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7ebf\u6027\u56de\u5f52\u8bbe\u7f6e\u4e0b\uff0c\u63a2\u7d22\u6b63\u5219\u5316\u5982\u4f55\u5f71\u54cd\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u6027\u80fd", "method": "\u5728\u9ad8\u7ef4\u673a\u5236\u4e0b\u63a8\u5bfc\u95ed\u5f0f\u8868\u8fbe\u5f0f\u5206\u6790\u671f\u671b\u6cdb\u5316\u635f\u5931\uff0c\u8003\u8651\u4efb\u610f\u7ebf\u6027\u6559\u5e08\u6a21\u578b\uff0c\u7814\u7a76\u5404\u5411\u540c\u6027\u6b63\u5219\u5316\u5bf9\u6807\u7b7e\u566a\u58f0\u7684\u7f13\u89e3\u4f5c\u7528", "result": "\u5404\u5411\u540c\u6027\u6b63\u5219\u5316\u80fd\u6709\u6548\u7f13\u89e3\u5355\u6559\u5e08\u548c\u591a\u6559\u5e08\u8bbe\u7f6e\u4e0b\u7684\u6807\u7b7e\u566a\u58f0\uff0c\u6700\u4f18\u56fa\u5b9a\u6b63\u5219\u5316\u5f3a\u5ea6\u968f\u4efb\u52a1\u6570T\u8fd1\u4f3c\u7ebf\u6027\u589e\u957f\u4e3aT/lnT", "conclusion": "\u8fd9\u662f\u7406\u8bba\u6301\u7eed\u5b66\u4e60\u9886\u57df\u7684\u9996\u4e2a\u6b64\u7c7b\u7ed3\u679c\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u56de\u5f52\u548c\u795e\u7ecf\u7f51\u7edc\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0"}}
{"id": "2601.13892", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13892", "abs": "https://arxiv.org/abs/2601.13892", "authors": ["Andrej Schwanke", "Lyubomir Ivanov", "David Salinas", "Frank Hutter", "Arber Zela"], "title": "Multi-Objective Hierarchical Optimization with Large Language Models", "comment": "23 pages, 21 figures, 9 tables", "summary": "Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.", "AI": {"tldr": "LLM\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u548c\u5019\u9009\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u5206\u5c42\u641c\u7d22\u7b56\u7565\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5c40\u90e8\u7a7a\u95f4\u751f\u6210\u5019\u9009\u89e3\uff0c\u6536\u655b\u5230\u771f\u5b9e\u5e15\u7d2f\u6258\u96c6", "motivation": "\u5c3d\u7ba1LLM\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u8fd8\u4e0d\u662f\u73b0\u6210\u7684\u9009\u62e9\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u5904\u7406\u6570\u503c\u8f93\u5165\u3001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3001\u5904\u7406\u591a\u4e2a\u51b2\u7a81\u76ee\u6807\u800c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u672c\u6587\u65e8\u5728\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5c06LLM\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u548c\u5019\u9009\u91c7\u6837\u5668\uff0c\u5d4c\u5165\u7ed3\u6784\u5316\u7684\u5206\u5c42\u641c\u7d22\u7b56\u7565\u4e2d\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5c06\u8f93\u5165\u7a7a\u95f4\u5212\u5206\u4e3a\u4e0d\u76f8\u4ea4\u7684\u8d85\u77e9\u5f62\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u590d\u5408\u8bc4\u5206\u51fd\u6570\u5bf9\u533a\u57df\u8fdb\u884c\u6392\u5e8f\uff0c\u5c06LLM\u7684\u751f\u6210\u8fc7\u7a0b\u9650\u5236\u5728\u7279\u5b9a\u9ad8\u6f5c\u529b\u5b50\u7a7a\u95f4\u4e2d\uff0c\u4f7fLLM\u53ea\u9700\u8fdb\u884c\u5c40\u90e8\u63a8\u7406\u800c\u975e\u5168\u5c40\u63a8\u7406\u3002", "result": "\u5728\u6807\u51c6\u6b63\u5219\u6027\u5047\u8bbe\u4e0b\uff0c\u7b97\u6cd5\u751f\u6210\u7684\u5019\u9009\u89e3\u5728Hausdorff\u8ddd\u79bb\u4e0a\u6536\u655b\u5230\u771f\u5b9e\u5e15\u7d2f\u6258\u96c6\u3002\u5b9e\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u5168\u5c40\u591a\u76ee\u6807\u4f18\u5316\u5668\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6807\u51c6\u8fdb\u5316\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7b97\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u4f5c\u4e3a\u66ff\u4ee3\u6a21\u578b\u5d4c\u5165\u5206\u5c42\u641c\u7d22\u7b56\u7565\uff0c\u6210\u529f\u5730\u5c06LLM\u5e94\u7528\u4e8e\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5229\u7528\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2601.13897", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13897", "abs": "https://arxiv.org/abs/2601.13897", "authors": ["Ankita Joshi", "Ashutosh Sharma", "Anoushkrit Goel", "Ranjeet Ranjan Jha", "Chirag Ahuja", "Arnav Bhavsar", "Aditya Nigam"], "title": "TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography", "comment": "Accepted at 23rd IEEE International Symposium on Biomedical Imaging (ISBI), 2026", "summary": "Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.", "AI": {"tldr": "\u63d0\u51faTractRLFusion\uff1a\u57fa\u4e8eGPT\u7684\u7b56\u7565\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u878d\u5408\u7b56\u7565\u6574\u5408\u591a\u4e2a\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u5347\u767d\u8d28\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u7684\u51c6\u786e\u6027\u548c\u89e3\u5256\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u767d\u8d28\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u91cd\u5efa\u767d\u8d28\u675f\u540c\u65f6\u6700\u5c0f\u5316\u865a\u5047\u8fde\u63a5\u7684\u6311\u6218\u3002\u867d\u7136\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u9700\u63d0\u5347\u8ffd\u8e2a\u7cbe\u5ea6\u548c\u89e3\u5256\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faTractRLFusion\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u8fc7\u7a0b\u8fdb\u884c\u6709\u6548\u7b56\u7565\u878d\u5408\uff1b2\uff09\u57fa\u4e8eGPT\u7684\u878d\u5408\u7b56\u7565\u6574\u5408\u591a\u4e2aRL\u7b56\u7565\uff1b3\uff09\u591acritic\u5fae\u8c03\u9636\u6bb5\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728HCP\u3001ISMRM\u548cTractoInferno\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTractRLFusion\u5728\u51c6\u786e\u6027\u548c\u89e3\u5256\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u5355\u4e2aRL\u7b56\u7565\u4ee5\u53ca\u6700\u5148\u8fdb\u7684\u7ecf\u5178\u65b9\u6cd5\u548cDRL\u65b9\u6cd5\u3002", "conclusion": "TractRLFusion\u901a\u8fc7\u878d\u5408\u591a\u4e2aRL\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u767d\u8d28\u7ea4\u7ef4\u675f\u8ffd\u8e2a\u7684\u6027\u80fd\uff0c\u4e3a\u795e\u7ecf\u5916\u79d1\u89c4\u5212\u548c\u8111\u8fde\u63a5\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2601.13953", "categories": ["cs.LG", "cs.AR", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.13953", "abs": "https://arxiv.org/abs/2601.13953", "authors": ["Gorgi Pavlov"], "title": "Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition", "comment": "35 pages, 22 figures. Code available at https://github.com/gogipav14/spectral-llm", "summary": "Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to \"fuzzy\" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.\n  We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.", "AI": {"tldr": "\u63d0\u51faHierarchical Spectral Composition\u67b6\u6784\uff0c\u901a\u8fc7\u9009\u62e9\u5e03\u5c14\u5085\u91cc\u53f6\u57fa\u7684\u8c31\u7cfb\u6570\uff0c\u7ed3\u5408Sinkhorn\u7ea6\u675f\u8def\u7531\u548c\u5217\u7b26\u53f7\u8c03\u5236\uff0c\u5b9e\u73b0\u53ef\u5fae\u7684\u5e03\u5c14\u903b\u8f91\u5408\u6210\uff0c\u5728GPU\u4e0a\u8fbe\u523010,959 MOps/s\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u5e03\u5c14\u903b\u8f91\u65f6\uff0c\u901a\u5e38\u4f1a\u6536\u655b\u5230\"\u6a21\u7cca\"\u7684\u8fd1\u4f3c\u89e3\uff0c\u8fd9\u4e9b\u89e3\u5728\u91cf\u5316\u540e\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7cbe\u786e\u5408\u6210\u5e03\u5c14\u903b\u8f91\u7684\u53ef\u5fae\u67b6\u6784\u3002", "method": "\u91c7\u7528\u5206\u5c42\u8c31\u5408\u6210\u67b6\u6784\uff1a\u4ece\u51bb\u7ed3\u7684\u5e03\u5c14\u5085\u91cc\u53f6\u57fa\u4e2d\u9009\u62e9\u8c31\u7cfb\u6570\uff0c\u901a\u8fc7Sinkhorn\u7ea6\u675f\u8def\u7531\uff08\u6295\u5f71\u5230Birkhoff\u591a\u9762\u4f53\uff09\u8fdb\u884c\u7ec4\u5408\uff0c\u5e76\u6dfb\u52a0\u5217\u7b26\u53f7\u8c03\u5236\u6765\u5b9e\u73b0\u5e03\u5c14\u5426\u5b9a\u529f\u80fd\u3002\u968f\u7740\u7ef4\u5ea6\u589e\u52a0\uff0c\u7ed3\u5408\u7cbe\u786e\u7684Walsh-Hadamard\u7cfb\u6570\u3001\u4e09\u5143\u91cf\u5316\u548cMCMC\u5e76\u884c\u56de\u706b\u4f18\u5316\u3002", "result": "\u5728n=2\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d\u8fbe\u5230100%\u51c6\u786e\u7387\uff0c\u96f6\u8def\u7531\u6f02\u79fb\u548c\u96f6\u635f\u5931\u4e09\u5143\u91cf\u5316\uff1bn=3\u65f6\uff0c\u68af\u5ea6\u4e0b\u964d76%\u51c6\u786e\u7387\uff0c\u4f46\u679a\u4e3e\u8bc1\u660e\u6240\u6709\u64cd\u4f5c\u90fd\u5b58\u5728\u6700\u4f18\u4e09\u5143\u63a9\u7801\uff08100%\u51c6\u786e\u7387\uff0c39%\u7a00\u758f\u6027\uff09\uff1bn=4\u65f6\uff0c\u8c31\u5408\u6210\u65b9\u6cd5\u5728\u6240\u6709\u64cd\u4f5c\u4e0a\u8fbe\u5230100%\u51c6\u786e\u7387\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6240\u6709\u6d4b\u8bd5\u51fd\u6570\u90fd\u5b58\u5728\u4e09\u5143\u591a\u9879\u5f0f\u9608\u503c\u8868\u793a\uff0c\u4f46\u968f\u7740\u7ef4\u5ea6\u589e\u52a0\uff0c\u9700\u8981\u8d85\u8d8a\u7eaf\u68af\u5ea6\u4e0b\u964d\u7684\u65b9\u6cd5\u6765\u627e\u5230\u8fd9\u4e9b\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728GPU\u4e0a\u5b9e\u73b0\u5355\u5468\u671f\u7ec4\u5408\u903b\u8f91\u63a8\u7406\uff0c\u901f\u5ea6\u4e3a10,959 MOps/s\uff0c\u5c55\u793a\u4e86\u786c\u4ef6\u9ad8\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u903b\u8f91\u5408\u6210\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2601.13964", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.13964", "abs": "https://arxiv.org/abs/2601.13964", "authors": ["Cheol-Hui Lee", "Hwa-Yeon Lee", "Dong-Joo Kim"], "title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning", "comment": null, "summary": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.", "AI": {"tldr": "RL-BioAug\uff1a\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347EEG\u5bf9\u6bd4\u5b66\u4e60\u6027\u80fd", "motivation": "EEG\u4fe1\u53f7\u5177\u6709\u975e\u5e73\u7a33\u6027\uff0c\u4f20\u7edf\u9759\u6001\u6216\u968f\u673a\u6570\u636e\u589e\u5f3a\u7b56\u7565\u96be\u4ee5\u4fdd\u7559\u5185\u5728\u4fe1\u606f\uff0c\u5f71\u54cd\u5bf9\u6bd4\u5b66\u4e60\u6027\u80fd", "method": "\u63d0\u51faRL-BioAug\u6846\u67b6\uff0c\u4f7f\u7528\u6807\u7b7e\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u81ea\u52a8\u786e\u5b9a\u6700\u4f18\u589e\u5f3a\u7b56\u7565\uff0c\u4ec5\u970010%\u6807\u7b7e\u6570\u636e\u6307\u5bfc\u4ee3\u7406\u7b56\u7565\uff0c\u7f16\u7801\u5668\u4ee5\u4e25\u683c\u81ea\u76d1\u7763\u65b9\u5f0f\u5b66\u4e60\u9c81\u68d2\u8868\u793a", "result": "\u5728Sleep-EDFX\u548cCHB-MIT\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u968f\u673a\u9009\u62e9\u7b56\u7565\u5206\u522b\u63d0\u53479.69%\u548c8.80%\u7684Macro-F1\u5206\u6570\uff1b\u4ee3\u7406\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u9009\u62e9\u4e0d\u540c\u6700\u4f18\u7b56\u7565", "conclusion": "RL-BioAug\u6709\u6f5c\u529b\u66ff\u4ee3\u4f20\u7edf\u542f\u53d1\u5f0f\u589e\u5f3a\u65b9\u6cd5\uff0c\u5efa\u7acb\u6570\u636e\u589e\u5f3a\u7684\u81ea\u4e3b\u8303\u5f0f"}}
{"id": "2601.13989", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.13989", "abs": "https://arxiv.org/abs/2601.13989", "authors": ["Wenbo Cao", "Weiwei Zhang"], "title": "A universal linearized subspace refinement framework for neural networks", "comment": null, "summary": "Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.", "AI": {"tldr": "LSR\u6846\u67b6\u901a\u8fc7\u56fa\u5b9a\u8bad\u7ec3\u7f51\u7edc\u72b6\u6001\u7684\u96c5\u53ef\u6bd4\u8bf1\u5bfc\u7ebf\u6027\u6b8b\u5dee\u6a21\u578b\uff0c\u5728\u5b50\u7a7a\u95f4\u4e2d\u6c42\u89e3\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u65e0\u9700\u4fee\u6539\u7f51\u7edc\u67b6\u6784\u6216\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u901a\u8fc7\u68af\u5ea6\u65b9\u6cd5\u8bad\u7ec3\uff0c\u4f46\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u5176\u6700\u7ec8\u9884\u6d4b\u7cbe\u5ea6\u8fdc\u672a\u8fbe\u5230\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u53ef\u8fbe\u5230\u7684\u6c34\u5e73\u3002\u68af\u5ea6\u8bad\u7ec3\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u6f5c\u529b\uff0c\u5373\u4f7f\u5c40\u90e8\u7ebf\u6027\u5316\u4ea7\u751f\u51f8\u95ee\u9898\u65f6\u4e5f\u662f\u5982\u6b64\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u5316\u5b50\u7a7a\u95f4\u7ec6\u5316(LSR)\u6846\u67b6\uff1a1) \u5728\u56fa\u5b9a\u8bad\u7ec3\u7f51\u7edc\u72b6\u6001\u5229\u7528\u96c5\u53ef\u6bd4\u8bf1\u5bfc\u7ebf\u6027\u6b8b\u5dee\u6a21\u578b\uff1b2) \u5728\u8be5\u5b50\u7a7a\u95f4\u4e2d\u6c42\u89e3\u7b80\u5316\u76f4\u63a5\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff1b3) \u8ba1\u7b97\u7ebf\u6027\u5316\u6b8b\u5dee\u6a21\u578b\u7684\u5b50\u7a7a\u95f4\u6700\u4f18\u89e3\uff1b4) \u5bf9\u4e8e\u7b97\u5b50\u7ea6\u675f\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u8fed\u4ee3LSR\uff0c\u4ea4\u66ff\u8fdb\u884c\u5355\u6b21LSR\u548c\u76d1\u7763\u975e\u7ebf\u6027\u5bf9\u9f50\u3002", "result": "LSR\u7cfb\u7edf\u6027\u5730\u66b4\u9732\u4e86\u68af\u5ea6\u8bad\u7ec3\u672a\u5145\u5206\u5229\u7528\u7684\u7cbe\u5ea6\u6c34\u5e73\uff0c\u7ecf\u5e38\u5b9e\u73b0\u6570\u91cf\u7ea7\u7684\u8bef\u5dee\u51cf\u5c11\u3002\u5728\u76d1\u7763\u51fd\u6570\u903c\u8fd1\u3001\u6570\u636e\u9a71\u52a8\u7b97\u5b50\u5b66\u4e60\u548c\u7269\u7406\u4fe1\u606f\u7b97\u5b50\u5fae\u8c03\u7b49\u4efb\u52a1\u4e2d\uff0cLSR\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u635f\u5931\u8bf1\u5bfc\u7684\u6570\u503c\u75c5\u6001\u6027\uff08\u800c\u975e\u975e\u51f8\u6027\u6216\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff09\u662f\u5b9e\u9645\u74f6\u9888\u3002LSR\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u795e\u7ecf\u8868\u793a\u4e0e\u56fa\u5b9a\u7ebf\u6027\u5316\u70b9\u7684\u964d\u9636\u7ebf\u6027\u6c42\u89e3\u5668\u7ed3\u5408\uff0c\u4e3a\u76d1\u7763\u5b66\u4e60\u3001\u7b97\u5b50\u5b66\u4e60\u548c\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6570\u503c\u57fa\u7840\u5e7f\u6cdb\u7684\u7ec6\u5316\u6846\u67b6\u3002"}}
{"id": "2601.14022", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14022", "abs": "https://arxiv.org/abs/2601.14022", "authors": ["Rodrigo Pereira David", "Luciano Araujo Dourado Filho", "Daniel Marques da Silva", "Jo\u00e3o Alfredo Cal-Braz"], "title": "Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment", "comment": null, "summary": "Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5728\u76f8\u540c\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u516c\u5e73\u6bd4\u8f83\u71c3\u6cb9\u8f66\u548c\u7535\u52a8\u8f66\u7684CO2\u6392\u653e", "motivation": "\u9053\u8def\u8fd0\u8f93\u8131\u78b3\u9700\u8981\u4e00\u81f4\u900f\u660e\u7684\u65b9\u6cd5\u6765\u6bd4\u8f83\u4e0d\u540c\u8f66\u8f86\u6280\u672f\u7684CO2\u6392\u653e\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u76f8\u540c\u9a7e\u9a76\u6761\u4ef6\u4e0b\u516c\u5e73\u6bd4\u8f83\u71c3\u6cb9\u8f66\u548c\u7535\u52a8\u8f66", "method": "\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u8bad\u7ec3\u71c3\u6cb9\u8f66\u548c\u7535\u52a8\u8f66\u6a21\u578b\uff0c\u5b66\u4e60\u4ece\u9a7e\u9a76\u53d8\u91cf\uff08\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u6e29\u5ea6\uff09\u5230\u5185\u90e8\u6267\u884c\u53d8\u91cf\uff08\u626d\u77e9\u3001\u6cb9\u95e8\uff09\u548c\u77ac\u65f6CO2\u5f53\u91cf\u6392\u653e\u7387\u7684\u6620\u5c04\uff0c\u6784\u5efa\u53cd\u4e8b\u5b9e\u573a\u666f\u8fdb\u884c\u5bf9\u6bd4", "result": "\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u77ac\u65f6\u6392\u653e\u6307\u6807\u4e0b\u516c\u5e73\u8bc4\u4f30\u52a8\u529b\u603b\u6210\u6280\u672f\uff0c\u4e3a\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u7684\u8f66\u8f86\u78b3\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u53ef\u4fe1\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u6846\u67b6\u4e3a\u8f66\u8f86\u6280\u672f\u6bd4\u8f83\u63d0\u4f9b\u4e86\u516c\u5e73\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u652f\u6301\u9053\u8def\u8fd0\u8f93\u8131\u78b3\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56"}}
{"id": "2601.14026", "categories": ["cs.LG", "cs.NE", "math.FA"], "pdf": "https://arxiv.org/pdf/2601.14026", "abs": "https://arxiv.org/abs/2601.14026", "authors": ["Vugar Ismailov"], "title": "Universal Approximation Theorem for Input-Connected Multilayer Perceptrons", "comment": "18 pages, 2 figures, 31 references", "summary": "We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\\mathbb{R}^n$.", "AI": {"tldr": "\u63d0\u51fa\u8f93\u5165\u8fde\u63a5\u591a\u5c42\u611f\u77e5\u673a\uff08IC-MLP\uff09\uff0c\u8be5\u67b6\u6784\u5728\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u4e2d\u9664\u4e86\u63a5\u6536\u524d\u4e00\u5c42\u8f93\u51fa\u5916\uff0c\u8fd8\u76f4\u63a5\u63a5\u6536\u539f\u59cb\u8f93\u5165\u7684\u4eff\u5c04\u8fde\u63a5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u4efb\u610f\u6709\u9650\u9690\u85cf\u5c42\u4e0b\u7684\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u6539\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u6bcf\u4e2a\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u4e2d\u5f15\u5165\u539f\u59cb\u8f93\u5165\u7684\u76f4\u63a5\u8fde\u63a5\uff0c\u589e\u5f3a\u7f51\u7edc\u7684\u4fe1\u606f\u4f20\u9012\u80fd\u529b\u548c\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faIC-MLP\u67b6\u6784\uff0c\u9996\u5148\u5728\u5355\u53d8\u91cf\u8bbe\u7f6e\u4e0b\u7814\u7a76\uff0c\u7ed9\u51fa\u4efb\u610f\u6709\u9650\u9690\u85cf\u5c42\u7684\u663e\u5f0f\u7cfb\u7edf\u63cf\u8ff0\u548c\u8fed\u4ee3\u516c\u5f0f\uff0c\u7136\u540e\u6269\u5c55\u5230\u5411\u91cf\u503c\u8f93\u5165\uff0c\u5efa\u7acb\u76f8\u5e94\u7684\u6570\u5b66\u5206\u6790\u6846\u67b6\u3002", "result": "\u8bc1\u660e\u4e86\u6df1\u5ea6IC-MLP\u5728\u6fc0\u6d3b\u51fd\u6570\u975e\u7ebf\u6027\u7684\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u903c\u8fd1\u95ed\u533a\u95f4\u4e0a\u7684\u4efb\u4f55\u8fde\u7eed\u51fd\u6570\uff08\u5355\u53d8\u91cf\uff09\u548c\u7d27\u81f4\u5b50\u96c6\u4e0a\u7684\u8fde\u7eed\u51fd\u6570\uff08\u591a\u53d8\u91cf\uff09\uff0c\u5373\u6ee1\u8db3\u901a\u7528\u903c\u8fd1\u5b9a\u7406\u3002", "conclusion": "IC-MLP\u662f\u4e00\u79cd\u6709\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u8f93\u5165\u76f4\u63a5\u8fde\u63a5\u589e\u5f3a\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u6fc0\u6d3b\u51fd\u6570\u975e\u7ebf\u6027\u7684\u6761\u4ef6\u4e0b\u5177\u6709\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.14033", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.14033", "abs": "https://arxiv.org/abs/2601.14033", "authors": ["Xiaochen Zhu", "Mayuri Sridhar", "Srinivas Devadas"], "title": "PAC-Private Responses with Adversarial Composition", "comment": "16 pages, 3 figures", "summary": "Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.\n  We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.\n  Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePAC\u9690\u79c1\u7684API\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u8f93\u51fa\u800c\u975e\u6743\u91cd\u4e0a\u5b9e\u65bd\u9690\u79c1\u4fdd\u62a4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7528\u548c\u5f3a\u9690\u79c1\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u901a\u8fc7API\u90e8\u7f72\uff0c\u4f20\u7edf\u7684\u6743\u91cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff08\u5982DP-SGD\uff09\u4f1a\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u566a\u58f0\u548c\u6548\u7528\u635f\u5931\u3002\u6a21\u578b\u6743\u91cd\u968f\u8bad\u7ec3\u6570\u636e\u53d8\u5316\u5927\uff0c\u4f46\u6a21\u578b\u5bf9\u7279\u5b9a\u8f93\u5165\u7684\u54cd\u5e94\u7ef4\u5ea6\u66f4\u4f4e\u4e14\u66f4\u7a33\u5b9a\uff0c\u56e0\u6b64\u76f4\u63a5\u5728\u6a21\u578b\u8f93\u51fa\u4e0a\u5b9e\u65bd\u9690\u79c1\u4fdd\u62a4\u66f4\u6709\u4f18\u52bf\u3002", "method": "\u91c7\u7528PAC\u9690\u79c1\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u4e92\u4fe1\u606f\uff08MI\uff09\u4e3a\u4efb\u610f\u9ed1\u76d2\u51fd\u6570\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u9690\u79c1\u4fdd\u8bc1\u3002\u63d0\u51fa\u65b0\u7b97\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6821\u51c6\u5b9e\u73b0\u5bf9\u6297\u6027\u7ec4\u5408\uff0c\u8bc1\u660e\u4e92\u4fe1\u606f\u4fdd\u8bc1\u5728\u81ea\u9002\u5e94\u548c\u5bf9\u6297\u6027\u67e5\u8be2\u4e0b\u7ebf\u6027\u7d2f\u79ef\u3002", "result": "\u5728\u8868\u683c\u3001\u89c6\u89c9\u548cNLP\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6781\u5c0f\u7684\u6bcf\u67e5\u8be2\u9690\u79c1\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7528\u3002\u5728CIFAR-10\u4e0a\u8fbe\u523087.79%\u51c6\u786e\u7387\uff0c\u6bcf\u6b65MI\u9884\u7b97\u4e3a2^{-32}\u3002\u670d\u52a1100\u4e07\u67e5\u8be2\u65f6\uff0c\u53ef\u8bc1\u660e\u5c06\u6210\u5458\u63a8\u7406\u653b\u51fb\u6210\u529f\u7387\u9650\u5236\u572851.08%\uff0c\u76f8\u5f53\u4e8e(0.04, 10^{-5})-DP\u4fdd\u8bc1\u3002\u901a\u8fc7\u79c1\u6709\u54cd\u5e94\u6807\u6ce8\u516c\u5171\u6570\u636e\u84b8\u998f\u6a21\u578b\uff0c\u5728ImageNet\u5b50\u96c6\u4e0a\uff0c\u4ece210,000\u4e2a\u54cd\u5e94\u84b8\u998f\u7684\u6a21\u578b\u5728CIFAR-10\u4e0a\u8fbe\u523091.86%\u51c6\u786e\u7387\uff0cMIA\u6210\u529f\u7387\u4e0a\u9650\u4e3a50.49%\uff0c\u76f8\u5f53\u4e8e(0.02,10^{-5})-DP\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728API\u90e8\u7f72\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7528\u548c\u5f3a\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\uff0c\u901a\u8fc7\u8f93\u51fa\u7ea7\u9690\u79c1\u4fdd\u62a4\u907f\u514d\u4e86\u4f20\u7edf\u6743\u91cd\u9690\u79c1\u65b9\u6cd5\u7684\u566a\u58f0\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u6821\u51c6\u89e3\u51b3\u4e86\u5bf9\u6297\u6027\u67e5\u8be2\u7ec4\u5408\u7684\u6311\u6218\u3002"}}
{"id": "2601.14053", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14053", "abs": "https://arxiv.org/abs/2601.14053", "authors": ["Badri N. Patro", "Vijay S. Agneeswaran"], "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems", "comment": null, "summary": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.", "AI": {"tldr": "LLMOrbit\u63d0\u51fa\u4e00\u4e2a2019-2025\u5e74\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5faa\u73af\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e8650\u591a\u4e2a\u6a21\u578b\uff0c\u8bc6\u522b\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u6210\u672c\u589e\u957f\u3001\u80fd\u8017\u5371\u673a\u4e09\u5927\u6311\u6218\uff0c\u5e76\u63ed\u793a\u4e86\u7a81\u7834\u6269\u5c55\u5899\u7684\u516d\u5927\u8303\u5f0f\u548c\u540e\u8bad\u7ec3\u6536\u76ca\u3001\u6548\u7387\u9769\u547d\u3001\u6c11\u4e3b\u5316\u4e09\u5927\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740AI\u4ece\u57fa\u7840Transformer\u67b6\u6784\u53d1\u5c55\u5230\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u9700\u8981\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u8fdb\u884c\u5168\u9762\u68b3\u7406\u3002\u7814\u7a76\u8005\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u6cd5\u6765\u5bfc\u822aLLM\u9886\u57df\uff0c\u8bc6\u522b\u5173\u952e\u6311\u6218\u548c\u7a81\u7834\u6027\u521b\u65b0\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u6d1e\u5bdf\u3002", "method": "\u91c7\u7528LLMOrbit\u5faa\u73af\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u516b\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8f68\u9053\u7ef4\u5ea6\u5206\u67902019-2025\u5e74\u95f4\u8d85\u8fc750\u4e2a\u6a21\u578b\u548c15\u4e2a\u7ec4\u7ec7\u3002\u8be5\u65b9\u6cd5\u6db5\u76d6\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u65b9\u6cd5\u3001\u6548\u7387\u6a21\u5f0f\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u5173\u952e\u5371\u673a\u548c\u7a81\u7834\u8303\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u5927\u5371\u673a\uff1a\u6570\u636e\u7a00\u7f3a\uff082026-2028\u5e74\u8017\u5c3d9-27T tokens\uff09\u3001\u6210\u672c\u6307\u6570\u589e\u957f\uff085\u5e74\u5185\u4ece300\u4e07\u52303\u4ebf+\u7f8e\u5143\uff09\u3001\u80fd\u8017\u4e0d\u53ef\u6301\u7eed\uff08\u589e\u52a022\u500d\uff09\u3002\u53d1\u73b0\u516d\u5927\u7a81\u7834\u8303\u5f0f\uff1a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u91cf\u5316\u3001\u5206\u5e03\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u3001\u6a21\u578b\u878d\u5408\u3001\u9ad8\u6548\u8bad\u7ec3\u3001\u5c0f\u578b\u4e13\u7528\u6a21\u578b\u3002\u63ed\u793a\u4e09\u5927\u8303\u5f0f\u8f6c\u53d8\uff1a\u540e\u8bad\u7ec3\u6536\u76ca\u3001\u6548\u7387\u9769\u547d\u3001\u6c11\u4e3b\u5316\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u9762\u4e34\u6269\u5c55\u5899\u9650\u5236\uff0c\u4f46\u901a\u8fc7\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u6548\u7387\u4f18\u5316\u548c\u5f00\u6e90\u6c11\u4e3b\u5316\u7b49\u521b\u65b0\u8303\u5f0f\u6b63\u5728\u7a81\u7834\u9650\u5236\u3002\u672a\u6765LLM\u5c06\u4ece\u88ab\u52a8\u751f\u6210\u8f6c\u5411\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7cfb\u7edf\uff0c\u540e\u8bad\u7ec3\u6280\u672f\u548c\u6548\u7387\u9769\u547d\u5c06\u6210\u4e3a\u5173\u952e\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2601.14092", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2601.14092", "abs": "https://arxiv.org/abs/2601.14092", "authors": ["Babacar Toure", "Dimitrios Tsilimantos", "Omid Esrafilian", "Marios Kountouris"], "title": "Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning", "comment": null, "summary": "Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u6570\u636e\u6536\u96c6\u4e0e\u80fd\u8017\u6743\u8861\u7684\u8def\u5f84\u89c4\u5212\uff0c\u65e0\u9700\u65e0\u7ebf\u4fe1\u9053\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u5355\u6a21\u578b\u9002\u5e94\u4e0d\u540c\u504f\u597d\u548c\u52a8\u6001\u573a\u666f\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u65e0\u7ebf\u7f51\u7edc\u6570\u636e\u6536\u96c6\u4efb\u52a1\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3001\u5ffd\u7565\u591a\u76ee\u6807\u672c\u8d28\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u9ad8\u5ea6\u52a8\u6001\u7684\u590d\u6742\u73af\u5883\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u663e\u5f0f\u5904\u7406\u6570\u636e\u6536\u96c6\u4e0e\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u65e0\u9700\u65e0\u7ebf\u4fe1\u9053\u5148\u9a8c\u77e5\u8bc6\uff0c\u5f00\u53d1\u5355\u4e00\u6a21\u578b\u9002\u5e94\u4e0d\u540c\u504f\u597d\u548c\u52a8\u6001\u53c2\u6570\u3002", "result": "\u5927\u91cf\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u6a21\u578b\u7d27\u51d1\u6027\u3001\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u591a\u76ee\u6807\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9002\u5e94\u6027\uff0c\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\u7f51\u7edc\u670d\u52a1\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2601.14099", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14099", "abs": "https://arxiv.org/abs/2601.14099", "authors": ["Shi-Shun Chen", "Xiao-Yang Li", "Enrico Zio"], "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping", "comment": null, "summary": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u6ede\u4ea4\u53c9\u6620\u5c04\u7684\u56e0\u679c\u7279\u5f81\u9009\u62e9\u6846\u67b6\uff0c\u89e3\u51b3\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u53d8\u91cf\u95f4\u65f6\u6ede\u548c\u76f8\u4e92\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u8f6f\u6d4b\u91cf\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u56e0\u679c\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u5ffd\u7565\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u7279\u6027\uff1a\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u5b58\u5728\u65f6\u95f4\u5ef6\u8fdf\uff0c\u4e14\u53d8\u91cf\u76f8\u4e92\u4f9d\u8d56\u8fdd\u53cd\u4f20\u7edf\u56e0\u679c\u63a8\u65ad\u7684\u53bb\u76f8\u5173\u5047\u8bbe\uff0c\u5bfc\u81f4\u8f6f\u6d4b\u91cf\u6a21\u578b\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0d\u8db3", "method": "\u63d0\u51fa\u65f6\u6ede\u4ea4\u53c9\u6620\u5c04\u6846\u67b6\uff0c\u4f7f\u7528\u65f6\u6ede\u6536\u655b\u4ea4\u53c9\u6620\u5c04(TDCCM)\u8fdb\u884c\u603b\u4f53\u56e0\u679c\u63a8\u65ad\uff0c\u65f6\u6ede\u90e8\u5206\u4ea4\u53c9\u6620\u5c04(TDPCM)\u8fdb\u884c\u76f4\u63a5\u56e0\u679c\u63a8\u65ad\uff0c\u5e76\u57fa\u4e8e\u9a8c\u8bc1\u96c6\u6027\u80fd\u81ea\u52a8\u786e\u5b9a\u56e0\u679c\u9608\u503c\u5b9e\u73b0\u7279\u5f81\u9009\u62e9", "result": "\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cTDCCM\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u6027\u80fd\uff0cTDPCM\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u63d0\u5347\u8f6f\u6d4b\u91cf\u7a33\u5b9a\u6027\u548c\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u65f6\u6ede\u4ea4\u53c9\u6620\u5c04\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u65f6\u6ede\u548c\u53d8\u91cf\u4f9d\u8d56\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u8f6f\u6d4b\u91cf\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027"}}
{"id": "2601.14115", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14115", "abs": "https://arxiv.org/abs/2601.14115", "authors": ["Liangsi Lu", "Jingchao Wang", "Zhaorong Dai", "Hanqian Liu", "Yang Shi"], "title": "Riemannian Liquid Spatio-Temporal Graph Network", "comment": "This paper has been accepted to The Web Conference 2026", "summary": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io", "AI": {"tldr": "RLSTG\u5c06\u8fde\u7eed\u65f6\u95f4\u6db2\u6001\u52a8\u529b\u5b66\u4e0e\u9ece\u66fc\u6d41\u5f62\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u76f8\u7ed3\u5408\uff0c\u5728\u975e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u5efa\u6a21\u65f6\u7a7a\u56fe\u6f14\u5316\uff0c\u514b\u670d\u4e86\u4f20\u7edfLTC\u7f51\u7edc\u5728\u6b27\u6c0f\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLiquid Time-Constant\u7f51\u7edc\uff08LTCs\uff09\u867d\u7136\u64c5\u957f\u5efa\u6a21\u4e0d\u89c4\u5219\u91c7\u6837\u52a8\u6001\uff0c\u4f46\u5c40\u9650\u4e8e\u6b27\u6c0f\u7a7a\u95f4\uff0c\u5728\u5904\u7406\u5177\u6709\u56fa\u6709\u975e\u6b27\u7ed3\u6784\uff08\u5982\u5c42\u6b21\u7ed3\u6784\u548c\u5faa\u73af\uff09\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u65f6\u4f1a\u4ea7\u751f\u51e0\u4f55\u5931\u771f\uff0c\u964d\u4f4e\u8868\u793a\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u9ece\u66fc\u6db2\u6001\u65f6\u7a7a\u56fe\u7f51\u7edc\uff08RLSTG\uff09\uff0c\u5728\u5f2f\u66f2\u6d41\u5f62\u4e0a\u76f4\u63a5\u516c\u5f0f\u5316\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODE\uff09\uff0c\u7edf\u4e00\u8fde\u7eed\u65f6\u95f4\u6db2\u6001\u52a8\u529b\u5b66\u4e0e\u9ece\u66fc\u6d41\u5f62\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\uff0c\u5efa\u6a21\u56fe\u5728\u975e\u6b27\u7a7a\u95f4\u4e2d\u7684\u6f14\u5316\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRLSTG\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u7684\u65f6\u95f4\u52a8\u529b\u5b66\u4e0e\u9ece\u66fc\u7a7a\u95f4\u8868\u793a\uff0c\u5728\u5177\u6709\u590d\u6742\u7ed3\u6784\u7684\u56fe\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5c06LTC\u7a33\u5b9a\u6027\u5b9a\u7406\u6269\u5c55\u5230\u9ece\u66fc\u57df\uff0c\u5e76\u901a\u8fc7\u72b6\u6001\u8f68\u8ff9\u5206\u6790\u91cf\u5316\u8868\u8fbe\u80fd\u529b\u3002", "conclusion": "RLSTG\u901a\u8fc7\u5c06\u8fde\u7eed\u65f6\u95f4\u6db2\u6001\u52a8\u529b\u5b66\u4e0e\u9ece\u66fc\u51e0\u4f55\u76f8\u7ed3\u5408\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfLTC\u7f51\u7edc\u5728\u975e\u6b27\u56fe\u7ed3\u6784\u4e2d\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u65f6\u7a7a\u56fe\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u51e0\u4f55\u611f\u77e5\u6846\u67b6\u3002"}}
{"id": "2601.14175", "categories": ["cs.LG", "cs.AI", "cs.CL", "hep-th"], "pdf": "https://arxiv.org/pdf/2601.14175", "abs": "https://arxiv.org/abs/2601.14175", "authors": ["Suvrat Raju", "Praneeth Netrapalli"], "title": "A model of errors in transformers", "comment": "8+17pages", "summary": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76LLM\u5728\u786e\u5b9a\u6027\u4efb\u52a1\uff08\u5982\u7b97\u672f\uff09\u4e0a\u7684\u9519\u8bef\u7387\uff0c\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u8bef\u5dee\u7d2f\u79ef\u7684\u5b9a\u91cf\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u4e2a\u53c2\u6570\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76LLM\u5728\u9700\u8981\u786e\u5b9a\u6027\u8f93\u51fa\u7684\u4efb\u52a1\uff08\u5982\u7b97\u672f\uff09\u4e0a\u4ea7\u751f\u9519\u8bef\u7684\u539f\u56e0\uff0c\u6311\u6218\u73b0\u6709\u5173\u4e8eLLM\u5728\u957f\u91cd\u590d\u4efb\u52a1\u4e0a\u51fa\u73b0\"\u63a8\u7406\u5d29\u6e83\"\u6216\u65e0\u6cd5\u8868\u8fbe\"\u7ec4\u5408\u51fd\u6570\"\u7684\u89c2\u70b9\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u8bef\u5dee\u7d2f\u79ef\u7684\u7406\u8bba\u6a21\u578b\uff0c\u5c06LLM\u7684\u4f17\u591a\u53c2\u6570\u91cd\u7ec4\u4e3a\u4e24\u4e2a\u5173\u952e\u53c2\u6570\uff1a\u57fa\u672c\u566a\u58f0\u7387\u548c\u53ef\u80fd\u9519\u8bef\u9884\u6d4b\u7684token\u6570\u91cf\u3002\u4f7f\u7528Gemini 2.5 Flash\u3001Gemini 2.5 Pro\u548cDeepSeek R1\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u3002", "result": "\u7406\u8bba\u9884\u6d4b\u4e0e\u89c2\u6d4b\u51c6\u786e\u7387\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u4e5f\u53d1\u73b0\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5b58\u5728\u504f\u5dee\u3002\u6a21\u578b\u80fd\u6709\u6548\u89e3\u91caLLM\u5728\u91cd\u590d\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u964d\u4f4e\u9519\u8bef\u7387\u3002", "conclusion": "LLM\u5728\u786e\u5b9a\u6027\u4efb\u52a1\u4e0a\u7684\u9519\u8bef\u53ef\u4ee5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7684\u5c0f\u8bef\u5dee\u7d2f\u79ef\u6765\u89e3\u91ca\uff0c\u800c\u975e\"\u63a8\u7406\u5d29\u6e83\"\u3002\u63d0\u51fa\u7684\u4e24\u53c2\u6570\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u9519\u8bef\u7387\uff0c\u4e3a\u7406\u89e3LLM\u9519\u8bef\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6307\u5bfc\u63d0\u793a\u8bbe\u8ba1\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2601.14196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14196", "abs": "https://arxiv.org/abs/2601.14196", "authors": ["Albina Galiullina", "Wouter van Heeswijk", "Tom van Woensel"], "title": "Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery", "comment": null, "summary": "Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.", "AI": {"tldr": "\u63d0\u51fa\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4f4d\u987e\u5ba2\u63a8\u8350\u5355\u4e00\u53d6\u8d27\u70b9\u800c\u975e\u81ea\u7531\u9009\u62e9\uff0c\u51cf\u5c11\u9001\u8d27\u5361\u8f66\u548c\u987e\u5ba2\u51fa\u884c\u7684\u603b\u78b3\u6392\u653e\uff0c\u5728\u52a8\u6001\u968f\u673a\u73af\u5883\u4e2d\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u63a8\u8350\u7b56\u7565\u3002", "motivation": "\u53d6\u8d27\u70b9\u4f5c\u4e3a\u5bb6\u5ead\u9001\u8d27\u7684\u53ef\u6301\u7eed\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u8ba2\u5355\u6574\u5408\u53ef\u4ee5\u7f29\u77ed\u9001\u8d27\u8def\u7ebf\u5e76\u63d0\u9ad8\u9996\u6b21\u6295\u9012\u6210\u529f\u7387\u3002\u7136\u800c\uff0c\u5f53\u987e\u5ba2\u5f00\u8f66\u53d6\u8d27\u65f6\uff0c\u8fd9\u4e9b\u73af\u5883\u6548\u76ca\u53ef\u80fd\u4f1a\u88ab\u62b5\u6d88\u3002\u9700\u8981\u4e00\u79cd\u7b56\u7565\u6765\u540c\u65f6\u51cf\u5c11\u9001\u8d27\u5361\u8f66\u8def\u7ebf\u548c\u987e\u5ba2\u51fa\u884c\u7684\u78b3\u6392\u653e\u3002", "method": "\u63d0\u51fa\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\uff0c\u4e3a\u6bcf\u4f4d\u5230\u8fbe\u987e\u5ba2\u63a8\u8350\u5355\u4e00\u53d6\u8d27\u70b9\u800c\u975e\u81ea\u7531\u9009\u62e9\uff0c\u540c\u65f6\u4fdd\u7559\u5bb6\u5ead\u9001\u8d27\u9009\u9879\u3002\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u8003\u8651\u987e\u5ba2\u4e0e\u53d6\u8d27\u70b9\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u53ca\u5176\u5bf9\u672a\u6765\u8def\u7ebf\u6574\u5408\u7684\u5f71\u54cd\uff0c\u5728\u52a8\u6001\u968f\u673a\u73af\u5883\u4e2d\u4f18\u5316\u63a8\u8350\u7b56\u7565\u3002", "result": "\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u603b\u78b3\u6392\u653e\u3002\u76f8\u5bf9\u4e8e\u7eaf\u5bb6\u5ead\u9001\u8d27\uff0c\u603b\u6392\u653e\u6700\u591a\u51cf\u5c119%\uff1b\u4e0e\u66ff\u4ee3\u7b56\u7565\uff08\u5305\u62ec\u81ea\u7531\u9009\u62e9\u53d6\u8d27\u70b9\u548c\u6700\u8fd1\u53d6\u8d27\u70b9\u5206\u914d\uff09\u76f8\u6bd4\uff0c\u5e73\u5747\u51cf\u5c112%\u3002\u5728\u53d6\u8d27\u70b9\u591a\u3001\u8ddd\u79bb\u77ed\u7684\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u6548\u679c\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "\u5dee\u5f02\u5316\u53d6\u8d27\u70b9\u63d0\u4f9b\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u9001\u8d27\u548c\u53d6\u8d27\u7684\u603b\u78b3\u6392\u653e\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u57ce\u5e02\u73af\u5883\u4e2d\u3002\u5f53\u987e\u5ba2\u4e0d\u592a\u503e\u5411\u4e8e\u9009\u62e9\u53d6\u8d27\u70b9\u9001\u8d27\u65f6\uff0c\u660e\u786e\u8003\u8651\u987e\u5ba2\u5230\u8fbe\u548c\u9009\u62e9\u7684\u52a8\u6001\u7279\u6027\u5c24\u4e3a\u91cd\u8981\u3002\u8be5\u7b56\u7565\u4e3a\u53ef\u6301\u7eed\u7269\u6d41\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.14209", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14209", "abs": "https://arxiv.org/abs/2601.14209", "authors": ["Matthew Y. R. Yang", "Hao Bai", "Ian Wu", "Gene Yang", "Amrith Setlur", "Aviral Kumar"], "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning", "comment": null, "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.", "AI": {"tldr": "\u63d0\u51faIntervention Training (InT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u63a8\u7406\u8f68\u8ff9\u6765\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u7ed3\u679c\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u5728\u4fe1\u7528\u5206\u914d\u4e0a\u5b58\u5728\u95ee\u9898\uff1a\u5931\u8d25\u8f68\u8ff9\u4e2d\u6b63\u786e\u7684\u4e2d\u95f4\u6b65\u9aa4\u88ab\u60e9\u7f5a\uff0c\u6210\u529f\u8f68\u8ff9\u4e2d\u9519\u8bef\u7684\u6b65\u9aa4\u5374\u88ab\u5f3a\u5316\u3002\u867d\u7136\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u662f\u81ea\u7136\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u51c6\u786e\u4f18\u5316\u8fd9\u7c7b\u6a21\u578b\u8bc6\u522b\u7ea0\u6b63\u6027\u63a8\u7406\u6b65\u9aa4\u4ecd\u7136\u56f0\u96be", "method": "\u5f15\u5165\u5e72\u9884\u8bad\u7ec3(InT)\uff1a\u6a21\u578b\u901a\u8fc7\u63d0\u51fa\u77ed\u5c0f\u3001\u6709\u9488\u5bf9\u6027\u7684\u4fee\u6b63\u6765\u5bf9\u81ea\u5df1\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fe1\u7528\u5206\u914d\u3002\u5229\u7528\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u5e38\u89c1\u7684\u53c2\u8003\u89e3\uff0c\u6a21\u578b\u8bc6\u522b\u63a8\u7406\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\uff0c\u63d0\u51fa\u5355\u6b65\u5e72\u9884\u5c06\u8f68\u8ff9\u8f6c\u5411\u6b63\u786e\u89e3\uff0c\u7136\u540e\u5bf9\u9519\u8bef\u70b9\u4e4b\u524d\u7684\u7b56\u7565\u5c55\u5f00\u52a0\u4e0a\u5e72\u9884\u8fdb\u884c\u76d1\u7763\u5fae\u8c03", "result": "\u7ecf\u8fc7InT\u548c\u540e\u7eedRL\u5fae\u8c03\u540e\uff0c\u5728IMO-AnswerBench\u4e0a\u5c064B\u53c2\u6570\u57fa\u7840\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e86\u8fd114%\uff0c\u8d85\u8fc7\u4e86gpt-oss-20b\u7b49\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b", "conclusion": "Intervention Training\u901a\u8fc7\u8ba9\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u63a8\u7406\u8f68\u8ff9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u4e3a\u540e\u7eedRL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u521d\u59cb\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b"}}
{"id": "2601.14228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14228", "abs": "https://arxiv.org/abs/2601.14228", "authors": ["Punit Kumar", "Vaibhav Saran", "Divyesh Patel", "Nitin Kulkarni", "Alina Vereshchaka"], "title": "Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment", "comment": "8 pages, 6 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/", "summary": "Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u8d25\u8840\u75c7\u51b3\u7b56\u652f\u6301\u6846\u67b6\uff0c\u5305\u542b\u98ce\u9669\u5206\u5c42\u3001\u6570\u636e\u589e\u5f3a\u3001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u7406\u7531\u751f\u6210\u56db\u4e2a\u6a21\u5757\uff0c\u5728MIMIC-III\u548ceICU\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u8d25\u8840\u75c7\u662fICU\u4e2d\u6b7b\u4ea1\u7387\u9ad8\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u6cbb\u7597\u51b3\u7b56\u5bf9\u60a3\u8005\u9884\u540e\u6709\u91cd\u8981\u5f71\u54cd\u3002\u5f53\u524d\u9700\u8981\u65e2\u80fd\u63d0\u4f9b\u51c6\u786e\u6cbb\u7597\u5efa\u8bae\u53c8\u80fd\u89e3\u91ca\u51b3\u7b56\u7406\u7531\u7684\u7cfb\u7edf\u3002", "method": "1. \u57fa\u4e8e\u805a\u7c7b\u7684\u98ce\u9669\u5206\u5c42\u6a21\u5757\uff1a\u5c06\u60a3\u8005\u5206\u4e3a\u4f4e\u3001\u4e2d\u3001\u9ad8\u98ce\u9669\u7ec4\uff1b2. \u5408\u6210\u6570\u636e\u589e\u5f3a\uff1a\u4f7f\u7528VAE\u548c\u6269\u6563\u6a21\u578b\u4e30\u5bcc\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6cbb\u7597\u8f68\u8ff9\uff1b3. \u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff1a\u4f7f\u7528AWR\u7b97\u6cd5\u8bad\u7ec3\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u7f16\u7801\u5668\u548c\u96c6\u6210\u6a21\u578b\uff1b4. \u7406\u7531\u751f\u6210\u6a21\u5757\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u5728MIMIC-III\u548ceICU\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6cbb\u7597\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4e3a\u4e34\u5e8a\u533b\u751f\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u7a33\u5065\u7684\u653f\u7b56\u5efa\u8bae\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u98ce\u9669\u5206\u5c42\u3001\u6570\u636e\u589e\u5f3a\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u8d25\u8840\u75c7\u6cbb\u7597\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2601.14232", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14232", "abs": "https://arxiv.org/abs/2601.14232", "authors": ["Egor Cherepanov", "Daniil Zelezetsky", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning", "comment": "38 pages, 44 figures, 3 tables", "summary": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.", "AI": {"tldr": "KAGE-Env\u662f\u4e00\u4e2aJAX\u539f\u751f\u76842D\u5e73\u53f0\u6e38\u620f\u73af\u5883\uff0c\u5c06\u89c2\u5bdf\u8fc7\u7a0b\u5206\u89e3\u4e3a\u72ec\u7acb\u53ef\u63a7\u7684\u89c6\u89c9\u8f74\uff0c\u540c\u65f6\u4fdd\u6301\u5e95\u5c42\u63a7\u5236\u95ee\u9898\u4e0d\u53d8\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u89c6\u89c9\u5206\u5e03\u504f\u79fb\u5bf9\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5c06\u591a\u79cd\u504f\u79fb\u6e90\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u963b\u788d\u4e86\u5bf9\u89c6\u89c9\u5206\u5e03\u504f\u79fb\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002\u50cf\u7d20\u57fa\u7840\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u7eaf\u89c6\u89c9\u5206\u5e03\u504f\u79fb\u4e0b\u7ecf\u5e38\u5931\u8d25\uff0c\u5373\u4f7f\u6f5c\u5728\u52a8\u6001\u548c\u5956\u52b1\u4fdd\u6301\u4e0d\u53d8\u3002", "method": "\u5f00\u53d1KAGE-Env\u73af\u5883\uff0c\u5c06\u89c2\u5bdf\u8fc7\u7a0b\u5206\u89e3\u4e3a\u72ec\u7acb\u53ef\u63a7\u7684\u89c6\u89c9\u8f74\uff1b\u57fa\u4e8e\u6b64\u6784\u5efaKAGE-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b6\u4e2a\u5df2\u77e5\u8f74\u5957\u4ef6\u548c34\u4e2a\u8bad\u7ec3-\u8bc4\u4f30\u914d\u7f6e\u5bf9\uff0c\u7528\u4e8e\u9694\u79bb\u5355\u4e2a\u89c6\u89c9\u504f\u79fb\uff1b\u4f7f\u7528\u6807\u51c6PPO-CNN\u57fa\u7ebf\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u89c2\u5bdf\u5230\u5f3a\u70c8\u7684\u8f74\u4f9d\u8d56\u6027\u5931\u8d25\uff1a\u80cc\u666f\u548c\u5149\u5ea6\u504f\u79fb\u901a\u5e38\u5bfc\u81f4\u4efb\u52a1\u5b8c\u5168\u5931\u8d25\uff0c\u800c\u4ee3\u7406\u5916\u89c2\u504f\u79fb\u76f8\u5bf9\u6e29\u548c\uff1b\u67d0\u4e9b\u504f\u79fb\u4fdd\u6301\u524d\u8fdb\u8fd0\u52a8\u4f46\u7834\u574f\u4efb\u52a1\u5b8c\u6210\uff0c\u663e\u793a\u4ec5\u9760\u56de\u62a5\u53ef\u80fd\u63a9\u76d6\u6cdb\u5316\u5931\u8d25\uff1bJAX\u5b9e\u73b0\u8fbe\u5230\u5355GPU\u6bcf\u79d23300\u4e07\u73af\u5883\u6b65\u7684\u901f\u5ea6\u3002", "conclusion": "KAGE-Env\u548cKAGE-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e72\u51c0\u7684\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u89c6\u89c9\u504f\u79fb\u5bf9\u6027\u80fd\u7684\u5dee\u5f02\u5316\u5f71\u54cd\uff0c\u5e76\u652f\u6301\u5feb\u901f\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\u3002"}}
{"id": "2601.14238", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14238", "abs": "https://arxiv.org/abs/2601.14238", "authors": ["Shaurya Mathur", "Shreyas Bellary Manjunath", "Nitin Kulkarni", "Alina Vereshchaka"], "title": "Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression", "comment": "6 pages, 5 figures (two of them in tables), Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/", "summary": "Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \\textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.", "AI": {"tldr": "FireCastRL\u662f\u4e00\u4e2a\u7ed3\u5408\u91ce\u706b\u9884\u6d4b\u4e0e\u667a\u80fd\u6251\u6551\u7684AI\u6846\u67b6\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u8d77\u706b\u70b9\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u7269\u7406\u6a21\u62df\u4e2d\u6267\u884c\u76f4\u5347\u673a\u706d\u706b\u6218\u672f\u3002", "motivation": "\u91ce\u706b\u9891\u7387\u548c\u5f3a\u5ea6\u4e0d\u65ad\u589e\u52a0\uff0c\u9020\u6210\u5de8\u5927\u751f\u6001\u548c\u7ecf\u6d4e\u635f\u5931\u3002\u4f20\u7edf\u91ce\u706b\u7ba1\u7406\u4e3b\u8981\u662f\u88ab\u52a8\u5e94\u5bf9\uff0c\u53ea\u5728\u706b\u707e\u53d1\u751f\u540e\u624d\u91c7\u53d6\u884c\u52a8\uff0c\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u9884\u9632\u548c\u54cd\u5e94\u65b9\u6cd5\u3002", "method": "1. \u4f7f\u7528\u6df1\u5ea6\u65f6\u7a7a\u6a21\u578b\u9884\u6d4b\u91ce\u706b\u8d77\u706b\u70b9\uff1b2. \u5bf9\u9ad8\u98ce\u9669\u9884\u6d4b\u90e8\u7f72\u9884\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5728\u7269\u7406\u4fe1\u606f3D\u6a21\u62df\u4e2d\u6267\u884c\u76f4\u5347\u673a\u706d\u706b\u6218\u672f\uff1b3. \u751f\u6210\u5a01\u80c1\u8bc4\u4f30\u62a5\u544a\u5e2e\u52a9\u5e94\u6025\u54cd\u5e94\u8005\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "result": "\u53d1\u5e03\u4e86\u5305\u542b950\u4e07\u4e2a\u73af\u5883\u53d8\u91cf\u6837\u672c\u7684\u5927\u89c4\u6a21\u65f6\u7a7a\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u652f\u6301\u91ce\u706b\u9884\u6d4b\u548c\u6218\u672f\u54cd\u5e94\u7684\u53ef\u884c\u6027\u3002", "conclusion": "FireCastRL\u6846\u67b6\u5c55\u793a\u4e86AI\u5728\u91ce\u706b\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u5c06\u9884\u6d4b\u4e0e\u4e3b\u52a8\u54cd\u5e94\u76f8\u7ed3\u5408\uff0c\u4e3a\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\uff0c\u6709\u671b\u6539\u5584\u4f20\u7edf\u88ab\u52a8\u5f0f\u7ba1\u7406\u65b9\u6cd5\u3002"}}
{"id": "2601.14243", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14243", "abs": "https://arxiv.org/abs/2601.14243", "authors": ["Haocheng Xi", "Charlie Ruan", "Peiyuan Liao", "Yujun Lin", "Han Cai", "Yilong Zhao", "Shuo Yang", "Kurt Keutzer", "Song Han", "Ligeng Zhu"], "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow", "comment": "11 pages, 6 figures, 4 tables", "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.", "AI": {"tldr": "Jet-RL\u662f\u4e00\u4e2aFP8\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u7a0b\u89e3\u51b3\u73b0\u6709BF16\u8bad\u7ec3+FP8 rollout\u7b56\u7565\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u4e14\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\u3002", "motivation": "\u73b0\u6709RL\u8bad\u7ec3\u7ba1\u9053\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0crollout\u9636\u6bb5\u5360\u8bad\u7ec3\u65f6\u95f470%\u4ee5\u4e0a\u3002\u867d\u7136FP8\u91cf\u5316\u8bad\u7ec3\u6709\u6f5c\u529b\u7f13\u89e3\u8fd9\u4e00\u74f6\u9888\uff0c\u4f46\u5e38\u7528\u7684BF16\u8bad\u7ec3+FP8 rollout\u7b56\u7565\u5728\u957f\u5e8f\u5217\u548c\u590d\u6742\u4efb\u52a1\u4e0b\u5b58\u5728\u4e25\u91cd\u4e0d\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51faJet-RL\u6846\u67b6\uff0c\u91c7\u7528\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u7a0b\u540c\u65f6\u7528\u4e8e\u8bad\u7ec3\u548crollout\uff0c\u6700\u5c0f\u5316\u6570\u503c\u5dee\u5f02\uff0c\u6d88\u9664\u4f4e\u6548\u7684\u8de8\u6b65\u9aa4\u6821\u51c6\u9700\u6c42\uff0c\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684RL\u4f18\u5316\u3002", "result": "Jet-RL\u5728rollout\u9636\u6bb5\u5b9e\u73b0\u6700\u9ad833%\u52a0\u901f\uff0c\u8bad\u7ec3\u9636\u6bb5\u6700\u9ad841%\u52a0\u901f\uff0c\u7aef\u5230\u7aef\u76f8\u6bd4BF16\u8bad\u7ec3\u63d0\u534716%\u901f\u5ea6\uff0c\u5728\u6240\u6709\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6536\u655b\uff0c\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u7edf\u4e00\u7684FP8\u7cbe\u5ea6\u6d41\u7a0b\u662f\u89e3\u51b3RL\u8bad\u7ec3\u4e2d\u6570\u503c\u4e0d\u5339\u914d\u95ee\u9898\u7684\u6709\u6548\u65b9\u6848\uff0cJet-RL\u6846\u67b6\u5728\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u7684\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u6548RL\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
