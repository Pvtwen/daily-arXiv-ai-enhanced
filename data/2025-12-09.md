<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 21]
- [cs.LG](#cs.LG) [Total: 145]
- [stat.ML](#stat.ML) [Total: 16]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Why Nonlinear Models Matter: Unified Analysis of Cognitive Load, Stress, and Exercise Using Wearable Physiological Signals](https://arxiv.org/abs/2512.06099)
*Khondakar Ashik Shahriar*

Main category: eess.SP

TL;DR: 非线性机器学习模型在可穿戴生理信号分析中显著优于线性模型，为认知负荷、压力和运动识别提供了统一基准


<details>
  <summary>Details</summary>
Motivation: 可穿戴生理信号表现出强烈的非线性和个体依赖性，传统线性模型难以有效处理这些复杂模式，需要探索更先进的建模方法

Method: 使用三个公开的Empatica E4数据集，统一评估认知负荷、压力和运动识别；对比非线性机器学习模型与线性基线模型；采用留一被试交叉验证；进行消融分析和SHAP可解释性分析

Result: 非线性模型在所有条件下均优于线性模型，准确率0.89-0.98，ROC-AUC 0.96-0.99；线性模型AUC低于0.70-0.73；多模态融合特别是EDA、温度和加速度计数据至关重要；SHAP分析揭示了生理学上有意义的特征贡献

Conclusion: 生理状态识别本质上是非线性的，研究为开发更鲁棒的可穿戴健康监测系统建立了统一基准，非线性模型在跨个体泛化方面表现中等但稳定

Abstract: Wearable physiological signals exhibit strong nonlinear and subject-dependent behavior, challenging traditional linear models. This study provides a unified evaluation of cognitive load, stress, and physical exercise recognition using three public Empatica~E4 datasets. Across all conditions, nonlinear machine learning models consistently outperformed linear baselines, achieving 0.89--0.98 accuracy and 0.96--0.99 ROC--AUC, while linear models remained below 0.70--0.73 AUC. Although Leave-One-Subject-Out validation revealed substantial inter-individual variability, nonlinear models maintained moderate cross-person generalization. Ablation and statistical analyses confirmed the necessity of multimodal fusion, particularly EDA, temperature, and ACC, while SHAP interpretability validated these findings by uncovering physiologically meaningful feature contributions across tasks. Overall, the results demonstrate that physiological state recognition is fundamentally nonlinear and establish a unified benchmark to guide the development of more robust wearable health-monitoring systems.

</details>


### [2] [Beamspace Dimensionality Reduction for Massive MU-MIMO: Geometric Insights and Information-Theoretic Limits](https://arxiv.org/abs/2512.06234)
*Canan Cebeci,Oveys Delafrooz Noroozi,Upamanyu Madhow*

Main category: eess.SP

TL;DR: 该论文研究了大规模MU-MIMO系统中波束空间降维的有效性，证明了在用户空间频率适当分离的条件下，干扰功率会集中在少量主导特征模上，从而可以用小噪声增强实现线性干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 大规模MU-MIMO系统中自适应接收的计算复杂度和训练开销很大。波束空间降维作为阵列处理的经典工具，在最近工作中被证明能显著降低这些开销。本文旨在从几何角度理解这种降维方法的有效性。

Method: 1. 对于简化信道模型，分析在用户空间频率适当分离条件下，干扰功率在期望用户波束空间窗口中的分布特性；2. 证明干扰功率集中在少于窗口尺寸W的主导特征模上，且期望用户在这些模上的投影相对较小；3. 将分析扩展到基于实测28 GHz数据的宽带多径信道MIMO-OFDM系统；4. 提出并评估基于信息论基准的每子载波降维波束空间LMMSE方法。

Result: 1. 当用户空间频率适当分离时，落入期望用户波束空间窗口的干扰功率集中在少于窗口尺寸W的主导特征模上；2. 期望用户在这些主导干扰模上的投影相对较小；3. 线性抑制主导干扰模只需较小的噪声增强；4. 这些观察结果同样适用于基于实测28 GHz数据的宽带MIMO-OFDM信道；5. 提出的每子载波降维波束空间LMMSE方法在信息论基准评估中表现良好。

Conclusion: 波束空间降维在大规模MU-MIMO系统中具有"不合理的高效性"，其根本原因在于：在用户空间频率适当分离的条件下，干扰功率集中在少量主导特征模上，而期望用户在这些模上的投影较小，使得线性干扰抑制能以较小的噪声增强代价实现。这一几何理解为波束空间降维方法的有效性提供了理论基础。

Abstract: Beamspace dimensionality reduction, a classical tool in array processing, has been shown in recent work to significantly reduce computational complexity and training overhead for adaptive reception in massive multiuser (MU) MIMO. For sparse multipath propagation and uniformly spaced antenna arrays, beamspace transformation, or application of a spatial FFT, concentrates the energy of each user into a small number of spatial frequency bins. Empirical evaluations demonstrate the efficacy of linear Minimum Mean Squared Error (LMMSE) detection performed in parallel using a beamspace window of small, fixed size for each user, even as the number of antennas and users scale up, while being robust to moderate variations in the relative powers of the users. In this paper, we develop a fundamental geometric understanding of this ``unreasonable effectiveness'' in a regime in which zero-forcing solutions do not exist. For simplified channel models, we show that, when we enforce a suitable separation in spatial frequency between users, the interference power falling into a desired user's beamspace window of size $W$ concentrates into a number of dominant eigenmodes smaller than $W$, with the desired user having relatively small projection onto these modes. Thus, linear suppression of dominant interference modes can be accomplished with small noise enhancement. We show that similar observations apply for MIMO-OFDM over wideband multipath channels synthesized from measured 28 GHz data. We propose, and evaluate via information-theoretic benchmarks, per-subcarrier reduced dimension beamspace LMMSE in this setting.

</details>


### [3] [Non-Equiprobable Signaling for Wireless Channels Subject to Mobility and Delay Spread](https://arxiv.org/abs/2512.06494)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 该论文提出了一种结合非等概率信号与LDPC编码的OFDM系统性能提升方法，通过将标准QAM星座划分为环形子星座并采用整形码实现非等概率传输，在Veh-A信道下1.5比特/符号速率时获得4dB增益。


<details>
  <summary>Details</summary>
Motivation: 传统OFDM系统使用等概率信号传输，未能充分利用信号能量分布特性。论文旨在通过非等概率信号与LDPC编码的结合，优化信号能量分布，提高系统性能。

Method: 1. 将标准QAM星座划分为等大小的环形子星座；2. 使用整形码实现非等概率信号传输，使高平均能量的子星座出现频率更低；3. 将整形集成到LDPC解码的LLR计算中；4. 在等概率信号中，LDPC码从内部子星座选择信号点；在非等概率信号中，内部信号点在每个子星座都有代表，整形码选择传输代表。

Result: 在Veh-A信道1.5比特/符号速率下，非等概率传输在BER=10^-3时获得4dB增益。随着传输速率增加，增益减小，但在16-QAM下仍保持显著增益。

Conclusion: 通过结合非等概率信号与LDPC编码，能够有效优化OFDM系统的信号能量分布，显著提升系统性能，特别是在低速率场景下效果更为明显。该方法可使用标准QAM星座实现任意分数比特率。

Abstract: This letter describes how to improve performance of OFDM systems by combining non-equiprobable signaling with low density parity check (LDPC) coding. We partition a standard QAM constellation into annular subconstellations of equal size, and we implement non-equiprobable signaling through a shaping code which selects subconstellations with large average energy less frequently than subconstellations with small average energy. In equiprobable signaling, the LDPC code selects a signal point from the inner subconstellation. In non-equiprobable signaling this inner signal point has a representative in each subconstellation and the shaping code selects the representative for transmission. It is possible to use standard QAM constellations to achieve any desired fractional bit rate with this method of shaping the energy distribution of the transmitted signal. We describe how to combine coding and shaping by integrating shaping into the calculation of log-likelihood ratios (LLRs) necessary for decoding LDPC codes. We present simulation results for non-equiprobable transmission at $1.5$ bits/symbol on a representative Veh-A channel showing gains of $4$ dB at a bit error rate (BER) of $10^{-3}$. As the transmission rate increases, the gains from non-equiprobable signaling diminish, but we show through simulation that they are still significant for $16$-QAM.

</details>


### [4] [Scaling Wideband Hybrid Beamforming for sub-THz Communication](https://arxiv.org/abs/2512.06532)
*Amirmohammad Haddad,Oveys Delafrooz Noroozi,Canan Cebeci,Mark J. W. Rodwell,Upamanyu Madhow*

Main category: eess.SP

TL;DR: 该论文研究大规模MIMO上行链路在同时扩展阵列尺寸和带宽时的容量问题，针对全数字阵列硬件复杂度和功耗过高的情况，提出了一种分块混合波束成形架构，并比较了多种波束展宽和用户共享策略的频谱效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决大规模MIMO上行链路在同时扩展阵列尺寸和带宽时面临的硬件复杂度和功耗挑战。全数字阵列在子太赫兹固定接入链路中会导致过高的硬件复杂性和功耗，因此需要探索更高效的架构。

Method: 采用分块混合波束成形架构，每个分块（子阵列）是一个相控阵，执行模拟（或射频）波束成形，然后对分块输出进行数字信号处理。研究了多种波束展宽方法来解决宽带射频波束成形中的"波束倾斜"问题，以及用户间共享分块的策略。

Result: 论文计算了理想化MIMO-OFDM系统的信息论基准，将线性每子载波多用户检测与无约束复杂度接收机进行比较，评估了不同策略在子太赫兹固定接入链路参数下的频谱效率。

Conclusion: 结论是在宽带大规模MIMO系统中，分块混合波束成形架构能够平衡硬件复杂度和系统性能，通过合理的分块大小和数量选择，结合适当的波束展宽和用户共享策略，可以在满足硬件约束的同时实现较高的频谱效率。

Abstract: We investigate the capacity attainable for a multiuser MIMO uplink as we scale both array size and bandwidth for regimes in which all-digital arrays incur excessive hardware complexity and power consumption. We consider a tiled hybrid beamforming architecture in which each tile, or subarray, is a phased array performing analog (or RF) beamforming, followed by DSP on the tile outputs. For parameters compatible with sub-THz fixed access links, we discuss hardware and power consumption considerations for choosing tile size and the number of tiles. Noting that the problem of optimal multiuser MIMO in our wideband regime is open even for the simplest possible channel models, we compare the spectral efficiencies attainable by a number of reasonable strategies for tile-level RF beamforming, assuming flexibility in the digital signal processing (DSP) of the tile outputs. We consider a number of beam broadening approaches for addressing the ``beam squint'' incurred by RF beamforming in our wideband regime, along with strategies for sharing tiles among users. Information-theoretic benchmarks are computed for an idealized MIMO-OFDM system, with linear per-subcarrier multiuser detection compared against an unconstrained complexity receiver.

</details>


### [5] [Scaling Wideband Massive MIMO Radar via Tiled Beamspace Processing](https://arxiv.org/abs/2512.06536)
*Oveys Delafrooz Noroozi,Jiyoon Han,Wei Tang,Zhengya Zhang,Upamanyu Madhow*

Main category: eess.SP

TL;DR: 提出一种用于大规模MIMO雷达系统的可扩展宽带数字波束成形瓦片架构，通过分布式空间FFT处理和紧凑全局波束空间域联合波束成形，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着孔径尺寸增大，传统全阵列MVDR处理因协方差估计和求逆的立方复杂度而变得不可行，需要更高效的可扩展架构。

Method: 基于波束空间能量集中原理，提出瓦片化窗口波束空间MVDR框架：每个瓦片执行2D空间DFT和角度相关波束空间窗口，生成降维表示；窗口输出拼接后由集中式MVDR波束成形器处理。

Result: 数值结果表明，该架构在检测和干扰抑制性能上与全维处理相当，同时显著降低计算成本、内存使用和训练需求。

Conclusion: 该框架为大规模MIMO雷达系统提供了可扩展的波束成形解决方案，并揭示了瓦片尺寸、窗口尺寸和波束空间分辨率之间的权衡关系，这些因素共同决定了系统整体可扩展性。

Abstract: We present a coordinated tiled architecture for scalable wideband digital beamforming in massive MIMO radar systems. As aperture size increases, conventional full-array MVDR processing becomes prohibitive due to the cubic complexity of covariance estimation and inversion. Building on the principle of energy concentration in beamspace, we introduce a tiled windowed-beamspace MVDR framework that distributes spatial FFT processing across subarrays while performing joint beamforming in a compact global beamspace domain. Each tile applies a 2D spatial DFT followed by an angle-of-arrival dependent beamspace window, producing a reduced-dimensional representation that preserves the dominant spatial structure of the received signal. The windowed outputs from the tiles are concatenated and processed by a centralized MVDR beamformer, enabling coherent full-aperture processing with drastically reduced dimensionality. Our numerical results demonstrate that the proposed architecture achieves detection and interference suppression performance comparable to full-dimensional processing, while substantially lowering computational cost, memory usage, and training requirements. The framework also reveals tradeoffs among tile size, window size, and beamspace resolution that govern overall system scalability.

</details>


### [6] [Teaching large language models to see in radar: aspect-distributed prototypes for few-shot HRRP ATR](https://arxiv.org/abs/2512.06617)
*De Bi,Chengbai Xu,Lingfeng Chen,Panhe Hu*

Main category: eess.SP

TL;DR: 本文提出了一种面向少样本条件下的高分辨率距离像自动目标识别方法，通过引入方位分布原型策略增强大语言模型的方位鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率距离像包含丰富的目标散射中心信息，但在少样本条件下传统学习方法容易过拟合且泛化能力差。现有基于大语言模型的方法在少样本场景中受限，主要原因是仅利用散射中心分布进行识别，忽略了方位敏感性导致的样本方差问题。

Method: 提出方位分布原型策略，用于增强大语言模型在少样本条件下的方位鲁棒性。该方法通过考虑方位敏感性导致的样本方差，构建更全面的原型表示。

Result: 在仿真和实测飞机电磁数据集上的实验表明，所提方法显著优于当前基准方法。

Conclusion: 方位分布原型策略是一种简单而有效的方法，能够提升大语言模型在少样本高分辨率距离像自动目标识别中的方位鲁棒性和识别性能。

Abstract: High-resolution range profiles (HRRPs) play a critical role in automatic target recognition (ATR) due to their richinformationregarding target scattering centers (SCs), which encapsulate the geometric and electromagnetic characteristics of thetarget.Under few-shot circumstances, traditional learning-based methods often suffer from overfitting and struggle togeneralizeeffectively. The recently proposed HRRPLLM, which leverages the in-context learning (ICL) capabilities of largelanguagemodels (LLMs) for one-shot HRRP ATR, is limited in few-shot scenarios. This limitation arises because it primarilyutilizesthe distribution of SCs for recognition while neglecting the variance of the samples caused by aspect sensitivity. Thispaperproposes a straightforward yet effective Aspect-Distributed Prototype (ADP) strategy for LLM-based ATRunder few-shotconditions to enhance aspect robustness. Experiments conducted on both simulated and measured aircraft electromagneticdatasets demonstrate that the proposed method significantly outperforms current benchmarks.

</details>


### [7] [Random Access for LEO Satellite Communication Systems via Deep Learning](https://arxiv.org/abs/2512.07053)
*Hyunwoo Lee,Ian P. Roberts,Jinkyo Jeong,Daesik Hong*

Main category: eess.SP

TL;DR: 提出基于深度学习的LEO卫星通信随机接入框架，包含早期前导码碰撞分类器和机会传输方案，提高接入成功率并降低延迟


<details>
  <summary>Details</summary>
Motivation: LEO卫星通信系统面临长传播延迟、大多普勒频移和大量同时接入尝试等挑战，这些因素降低了传统随机接入方案的效率和响应性，特别是在卫星物联网和直连设备服务场景中

Method: 提出深度学习随机接入框架：1) 早期前导码碰撞分类器，使用多天线相关特征和轻量级1D卷积神经网络估计碰撞用户数；2) 基于估计的机会传输方案，平衡接入概率和资源效率

Result: 在3GPP兼容的LEO设置下，该框架相比现有方案实现了更高的接入成功率、更低的延迟、更好的物理上行共享信道利用率和降低的计算复杂度

Conclusion: 提出的深度学习随机接入框架能有效应对LEO卫星通信的特殊挑战，显著提升系统性能

Abstract: Integrating contention-based random access procedures into low Earth orbit (LEO) satellite communication (SatCom) systems poses new challenges, including long propagation delays, large Doppler shifts, and a large number of simultaneous access attempts. These factors degrade the efficiency and responsiveness of conventional random access schemes, particularly in scenarios such as satellite-based internet of things and direct-to-device services. In this paper, we propose a deep learning-based random access framework designed for LEO SatCom systems. The framework incorporates an early preamble collision classifier that uses multi-antenna correlation features and a lightweight 1D convolutional neural network to estimate the number of collided users at the earliest stage. Based on this estimate, we introduce an opportunistic transmission scheme that balances access probability and resource efficiency to improve success rates and reduce delay. Simulation results under 3GPP-compliant LEO settings confirm that the proposed framework achieves higher access success probability, lower delay, better physical uplink shared channel utilization, and reduced computational complexity compared to existing schemes.

</details>


### [8] [Multi-Functional Programmable Metasurfaces for 6G and Beyond](https://arxiv.org/abs/2512.06693)
*Xu Gan,Xidong Mu,Yuanwei Liu,Marco Di Renzo,Josep Miquel Jornet,Nuria González Prelcic,Arman Shojaeifard,Tie Jun Cui*

Main category: eess.SP

TL;DR: 该论文对可编程超表面在B6G网络中的多功能应用进行全面综述，重点涵盖全空间通信覆盖、泛在感知以及智能信号处理与计算三大方向，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: B6G网络需要支持高速通信、高精度感知和高性能计算等先进应用，而可编程超表面作为一种能效高、成本效益好的技术，能够动态调控电磁波，为实现这些多功能需求提供有前景的解决方案。

Method: 采用系统性综述方法，分别探讨：1) STARS支持的全空间通信，包括其操作协议和在高效通信、物理层安全、无人机网络、无线能量传输中的应用；2) PM支持的泛在感知，包括信号模型构建、近场与协同感知优势分析、多频段操作性能；3) 堆叠智能超表面在波域模拟处理和空中数学计算中的实现。

Result: 论文系统梳理了多功能可编程超表面的代表性范式，展示了STARS在全空间通信中的关键作用，PM在感知架构中的性能优势，以及SIM在信号处理和计算中的创新应用，为B6G网络发展提供了技术路线图。

Conclusion: 多功能可编程超表面是支撑B6G网络发展的关键技术，在通信、感知和计算方面展现出巨大潜力。未来需要解决关键研究挑战，推动该技术向更智能、更集成的方向发展。

Abstract: The sixth-generation and beyond (B6G) networks are envisioned to support advanced applications that demand high-speed communication, high-precision sensing, and high-performance computing. To underpin this multi-functional evolution, energy- and cost-efficient programmable metasurfaces (PMs) have emerged as a promising technology for dynamically manipulating electromagnetic waves. This paper provides a comprehensive survey of representative multi-functional PM paradigms, with a specific focus on achieving \emph{full-space communication coverage}, \emph{ubiquitous sensing}, as well as \emph{intelligent signal processing and computing}. i) For simultaneously transmitting and reflecting surfaces (STARS)-enabled full-space communications, we elaborate on their operational protocols and pivotal applications in supporting efficient communications, physical layer security, unmanned aerial vehicle networks, and wireless power transfer. ii) For PM-underpinned ubiquitous sensing, we formulate the signal models for the PM-assisted architecture and systematically characterize its advantages in near-field and cooperative sensing, while transitioning to the PM-enabled transceiver architecture and demonstrating its superior performance in multi-band operations. iii) For advanced signal processing and computing, we explore the novel paradigm of stacked intelligent metasurfaces (SIMs), investigating their implementation in wave-domain analog processing and over-the-air mathematical computing. Finally, we identify key research challenges and envision future directions for multi-functional PMs towards B6G.

</details>


### [9] [Message Passing Based Demodulation of the Time-Encoded Digital Modulation Signal](https://arxiv.org/abs/2512.06706)
*Yuan Xu,Wenhui Xiong*

Main category: eess.SP

TL;DR: 提出基于消息传递的算法，用于解调时间编码数字调制信号，实现低延迟的"实时"解调


<details>
  <summary>Details</summary>
Motivation: 传统基于伪逆的方法计算复杂度随解调符号数量呈立方增长，需要更高效的低延迟解调算法

Method: 基于消息传递的算法，直接处理时间编码机产生的脉冲，实现逐脉冲的实时解调

Result: 提出的方法计算复杂度随解调符号数量线性增长，相比伪逆方法的立方复杂度显著降低

Conclusion: 消息传递算法能够实现低延迟的实时解调，计算效率显著优于传统伪逆方法

Abstract: This letter proposes a Message Passing (MP) based algorithm for demodulating the time-encoded digital modulation signal. The proposed algorithm processes the spikes generated by the Time-Encoding Machine (TEM) directly on a per-spike basis, which enables "on-the-fly" demodulatio with low latency. The computational complexity of our proposed method scales linearly with the number of demodulated symbols, while the computational complexity of the pseudo-inverse-based method scales cubically with the same number of demodulated symbols.

</details>


### [10] [Effective Electromagnetic Degrees of Freedom in Backscatter MIMO Systems](https://arxiv.org/abs/2512.06799)
*Philipp del Hougne*

Main category: eess.SP

TL;DR: 该论文针对反向散射MIMO系统提出了有效的电磁自由度定义，解决了现有理论缺失问题，并展示了其分布特性、与相干照明的关系以及优化方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态线性MIMO系统的有效电磁自由度已有明确定义，但反向散射MIMO系统的对应定义一直缺失。由于互耦效应，负载配置到观测场的映射具有非线性特性，这增加了分析BS-EEMDOFs的复杂性。

Method: 基于负载配置对观测场的雅可比矩阵引入BS-EEMDOFs定义，从多端口网络理论推导出闭式表达式，并通过数值模拟和实验验证在不同无线电环境中的可重构智能表面应用。

Result: BS-EEMDOFs数量本质上是分布变量，其分布取决于反向散射元件间的互耦和相干照明。相关模式位于从反向散射阵列端口到接收端口端到端信道矩阵的列空间中，但与相干馈电的基准EEMDOFs数量不同。相干照明可作为控制BS-EEMDOFs数量的优化参数。

Conclusion: 该研究为反向散射MIMO系统建立了有效的电磁自由度理论框架，揭示了其分布特性和对相干照明的依赖性，为优化可重构智能表面性能提供了理论基础和控制手段。

Abstract: While the definition of the effective electromagnetic degrees of freedom (EEMDOFs) of a static linear multiple-input multiple-output (MIMO) system is well established, the counterpart for a backscatter MIMO (BS-MIMO) system is so far missing. A BS-MIMO system encodes the input information into the loads of backscatter elements. Due to mutual coupling, the mapping from load configuration to observed fields is fundamentally non-linear, which complicates the analysis of BS-EEMDOFs. We introduce a definition of BS-EEMDOFs based on the Jacobian of the observed fields with respect to the load configuration. We derive a closed-form expression from multiport network theory which demonstrates that the number of BS-EEMDOFs is fundamentally a distributed variable, whose distribution depends on the mutual coupling between the backscatter elements and the coherent illumination. The modes associated with BS-EEMDOFs lie in the column space of the end-to-end channel matrix from backscatter array ports to receiver ports, but the number of BS-EEMDOFs is generally different from the number of benchmark EEMDOFs associated with the same array being coherently fed rather than tunably terminated. The dependence on the coherent illumination yields optimized coherent illumination as a control knob for the number of BS-EEMDOFs. We present numerical and experimental results for the evaluation and optimization of the number of BS-EEMDOFs in different radio environments with reconfigurable intelligent surfaces.

</details>


### [11] [Bruxism Recognition via Wireless Signal](https://arxiv.org/abs/2512.06909)
*Qiankai Shen,Yuanhao Cui,Jie Yang,Xiaojun Jing,Zhiyong Feng,Shi Jin*

Main category: eess.SP

TL;DR: 基于毫米波雷达的无接触磨牙症识别系统，通过分析磨牙运动对雷达回波信号的影响，提取11个特征，使用随机森林分类器达到96.1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 磨牙症严重影响睡眠质量和牙齿健康，但现有诊断方法存在不适感或侵犯隐私的问题，需要开发非侵入性且保护隐私的识别技术。

Method: 建立基于毫米波雷达的无接触磨牙症识别系统：1) 分析磨牙运动模式对雷达回波信号的潜在影响；2) 提取11个相关特征；3) 使用随机森林模型对毫米波雷达构建的数据集进行分类。

Result: 在测试集上达到96.1%的准确率，精确率、召回率和F1分数均保持在较高水平，验证了毫米波雷达用于磨牙症识别的有效性。

Conclusion: 毫米波雷达为磨牙症识别提供了非侵入性和隐私友好的替代方案，未来研究将专注于提高方法在不同人群和环境中的鲁棒性，并减少其他面部微运动对磨牙识别的干扰。

Abstract: Bruxism is an oromandibular movement disorder involving teeth grinding and clenching, which severely impairs sleep quality and dental health. However, its diagnosis remains challenging, as existing methods often cause discomfort or compromise user privacy. To address these limitations, we establish a contactless bruxism recognition system based on millimeter-wave radar. First, we analyzed the potential impact of the movement patterns of teeth grinding on radar echo signals. Based on this analysis, 11 features were extracted. Subsequently, using these features, we performed classification with a Random Forest model on the dataset constructed via millimeter-wave radar. Experimental results demonstrate that the proposed method achieves an accuracy of 96.1% on the test set, with precision, recall, and F1-score all remaining at a relatively high level. This study validates the effectiveness of millimeter-wave radar for SB recognition, providing a non-invasive and privacy-friendly alternative to existing recognition techniques. Future research will focus on enhancing the robustness of the method across diverse populations and environments, as well as striving to mitigate the interference of other facial micro-movements on teeth grinding recognition.

</details>


### [12] [Integrated Sensing, Communication, Computing, and Control Meets UAV Swarms in 6G](https://arxiv.org/abs/2512.07054)
*Yiyan Ma,Bo Ai,Jingli Li,Weijie Yuan,Boxiang He,Weiyang Feng,Zhengyu Zhang,Qingqing Cheng,Zhangdui Zhong*

Main category: eess.SP

TL;DR: 本文探讨了在6G低空无线网络中，如何通过集成感知、通信、计算和控制（ISCCC）策略来支持无人机集群的可靠飞行和有效功能，提出了三层ISCCC架构，并通过案例研究展示了ISCCC在提高网络可靠性和效率方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的发展和无人机数量的增加，如何支持无人机集群的可靠飞行和有效功能成为挑战。传统的分离式设计方法难以满足无人机集群在低空无线网络中的复杂需求，需要一种集成的解决方案。

Method: 提出了三层ISCCC架构，根据无人机集群的需求（飞行、自组织、功能）进行分类。研究了不同场景下的基本问题、有前景的技术和挑战。通过一个基于ISCCC原理最小化无人机集群飞行轨迹误差的案例研究，验证了方法的有效性。

Result: ISCCC策略能够作为智能低空无线网络系统中的有效物理反射弧链路。案例研究表明，ISCCC通过联合设计四个组件，有望同时提高低空无线网络的可靠性和效率。

Conclusion: ISCCC为6G时代无人机集群在低空无线网络中的应用提供了有前景的解决方案。通过集成感知、通信、计算和控制，能够有效支持无人机集群的复杂需求，但仍需进一步研究相关技术和挑战。

Abstract: To develop the low-altitude economy, the establishment of the low-altitude wireless network (LAWN) is the first priority. As the number of unmanned aerial vehicles (UAVs) increases, how to support the reliable flying and effective functioning of UAV swarms is challenging. Recently, the integrated sensing, communication, computing, and control (ISCCC) strategy was designed, which could act as effective physical {\it reflex arc} links in the intelligent LAWN system. Thus, in this article, we outline the challenges and opportunities when ISCCC meets UAV swarm in LAWN in 6G. First, we propose a three-layer ISCCC structure for the UAV swarm, which is categorized according to the UAV swarm's requirements, i.e., flying, self-organizing, and functioning. Second, for different scenarios, we study the basic problem, promising technologies, and challenges to design ISCCC for UAV swarms. Third, through a case study that minimizes the flying trajectory error of the UAV swarm based on the ISCCC principle, we show that ISCCC is promising to simultaneously improve the reliability and efficiency of LAWN via jointly designing four components. Finally, we discuss the promising directions for the ISCCC-based UAV swarm in LAWN.

</details>


### [13] [TagLabel: RFID Based Orientation and Material Sensing for Automated Package Inspection](https://arxiv.org/abs/2512.07097)
*David Wang,Jiale Zhang,Pei Zhang*

Main category: eess.SP

TL;DR: TagLabel是一个基于RFID的系统，利用低成本无源UHF标签通过分析RSSI和相位变化来检测包裹方向和内容，无需开箱即可识别材料，准确率超过80%


<details>
  <summary>Details</summary>
Motivation: 现代物流系统面临假冒产品、欺诈退货和危险品检测的困难，当前包裹筛查方法速度慢、成本高且不实用，需要一种快速、低成本、实用的解决方案

Method: 使用低成本无源UHF RFID标签，通过分析材料对RSSI和相位的影响来识别包裹内容；利用相位差异、标签遮挡和天线增益模式推断方向，选择遮挡最大的标签进行材料传感；评估2标签和3标签配置，使用机器学习分类器处理实际RF环境

Result: 系统在所有包裹方向上实现超过80%的准确率；仅需标准RFID硬件，扫描速度快，为物流包裹检测提供实用解决方案

Conclusion: TagLabel系统提供了一种实用方法来增强包裹检测并改善物流自动化，通过标准RFID硬件和快速扫描时间，能够有效识别包裹方向和内容

Abstract: Modern logistics systems face increasing difficulty in identifying counterfeit products, fraudulent returns, and hazardous items concealed within packages, yet current package screening methods remain too slow, expensive, and impractical for widespread use. This paper presents TagLabel, an RFID based system that determines both the orientation and contents of packages using low cost passive UHF tags. By analyzing how materials change RSSI and phase, the system identifies the contents of a package without opening it. Using orientation inferred from phase differences, tag occlusion, and antenna gain patterns, the system selects the tag with the greatest occlusion for accurate material sensing. We evaluate two and three tag configurations, and show that both can deliver high orientation and material sensing performance through the use of machine learning classifiers, even in realistic RF environments. When combined into a unified pipeline, TagLabel achieves more than 80 percent accuracy across all package orientations. Because it requires only standard RFID hardware and offers fast scanning times, this approach provides a practical way to enhance package inspection and improve automation in logistics operations.

</details>


### [14] [Near Field Electric (NFE): Energy-efficient, High-speed Communication at Decimeter-range](https://arxiv.org/abs/2512.07167)
*Samyadip Sarkar,Arunashish Datta,Sihun Kim,Amir Mokhtarpour,Shweta Katakdhond,Shovan Maity,Shreyas Sen*

Main category: eess.SP

TL;DR: 该论文提出了一种近场电场通信技术，通过电容耦合实现了低功耗、高数据速率和分米级距离的突破性组合，解决了现有近场通信技术中功率与性能之间的根本性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有近场通信技术面临根本性权衡：磁基方法（NFC/NFMI）功耗低但速率有限，毫米波技术速率高但功耗大且距离短。需要一种能同时实现超低功耗、高速率和可配置距离的通信技术。

Method: 采用近场电场通信技术，通过电容耦合和受限电场实现。使用对称电极在多种方向和配置下进行系统测量，并通过导电介质扩展通信范围。

Result: NFE实现了超低功耗（Tx 0.4 mW，Rx 0.6 mW）、高数据速率（>3 Mbps）和分米级距离（5-30 cm）。功耗比NFC低24倍，比NFMI低3倍，同时速率显著更高。在8个方向测试中表现一致稳定，通过导电介质在3.5米距离实现稳定2 Mbps吞吐量。

Conclusion: NFE技术打破了近场通信的功率-性能权衡，为下一代无线应用奠定了基础，特别适用于需要安全性、低功耗和高吞吐量并重的场景，如密集物联网部署、安全支付系统和高速设备间通信。

Abstract: Near-field technologies enable contactless payments, building access, automotive keyless entry, and supply chain tracking. Existing approaches face fundamental trade-offs: magnetic-based methods (NFC/NFMI) achieve low power but are limited to sub-megabit rates, while millimeter-wave techniques provide gigabits/sec connectivity at higher power consumption and only centimeter-scale ranges. We demonstrate that near-field electric (NFE) communication breaks this trade-off via capacitive coupling enabled by confined electric fields. NFE simultaneously achieves ultra-low power ($<$1 mW per transceiver), high-speed data throughput ($>$3 Mbps), and configurable decimeter-range (5-30 cm) capabilities previously considered mutually exclusive. Systematic measurements across multiple orientations and configurations show NFE can support decimeter communication coverage. The power consumption of 0.4 mW at the transmitter (Tx) and 0.6 mW at the receiver (Rx), when combined is up to $\sim$24$\times$ lower than NFC and $\sim$3$\times$ lower than NFMI while achieving significantly higher data rates, and a couple of orders of magnitude lower power than mm-wave based technique. Testing with symmetrical electrodes across eight orientations validated consistent performance and robustness for practical deployments. Extended-range experiments achieved stable 2 Mbps throughput at 3.5 meters using conductive media, demonstrating NFE's unique ability to leverage environmental conductors. Optimized device design can facilitate achieving an extended range for Body-assisted NFE up to 1 m. Results establish NFE as foundational for next-generation wireless applications where security, low power, and throughput converge, enabling dense IoT deployments, secure payment systems, and high-speed device-to-device communication previously limited by the power-performance trade-off.

</details>


### [15] [Non-negative DAG Learning from Time-Series Data](https://arxiv.org/abs/2512.07267)
*Samuel Rey,Gonzalo Mateos*

Main category: eess.SP

TL;DR: 提出一种基于凸优化的非负DAG学习方法，用于从多元时间序列中恢复瞬时依赖关系，相比现有方法具有全局最优性保证。


<details>
  <summary>Details</summary>
Motivation: 现有连续松弛方法通过邻接矩阵幂的平滑约束函数施加无环性，导致非凸优化问题难以求解。本文利用DAG边权非负的额外结构，通过凸约束实现无环性，从而获得全局最优解。

Method: 假设底层DAG只有非负边权，利用这一结构通过凸约束施加无环性，将多元时间序列的非负DAG恢复问题转化为凸优化问题，采用乘子法求解。

Result: 在合成时间序列数据上的实验表明，该方法优于现有替代方法，凸化公式保证了解决方案的全局最优性。

Conclusion: 通过利用非负边权结构，成功将DAG学习问题转化为凸优化问题，实现了全局最优解和更好的性能，为从时间序列数据中恢复因果结构提供了有效方法。

Abstract: This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.

</details>


### [16] [Verifiable Deep Quantitative Group Testing](https://arxiv.org/abs/2512.07279)
*Shreyas Jayant Grampurohit,Satish Mulleti,Ajit Rajwade*

Main category: eess.SP

TL;DR: 提出基于神经网络的定量组测试框架，实现高精度解码和结构可验证性


<details>
  <summary>Details</summary>
Motivation: 解决定量组测试问题，在M远小于N的情况下识别少量缺陷物品，需要高精度且可验证的恢复方法

Method: 训练多层感知器将噪声测量向量映射到二进制缺陷指示器，通过网络的Jacobian恢复底层池化结构

Result: 模型在稀疏有界扰动下实现准确鲁棒的恢复，并能从Jacobian中恢复池化结构，表明模型内化了QGT的组合关系

Conclusion: 标准前馈架构可以在结构化组合恢复问题中学习可验证的逆映射

Abstract: We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.

</details>


### [17] [Cost-Effective XL-MIMO Communication with Cylinder Directly-Connected Antenna Array](https://arxiv.org/abs/2512.07330)
*Xuancheng Zhu,Zhiwen Zhou,Zhenjun Dong,Yong Zeng*

Main category: eess.SP

TL;DR: 提出圆柱形直连天线阵列（DCAA）架构，通过多层三维结构中的简单均匀圆形阵列解决XL-MIMO信号遮挡问题，相比传统混合波束成形ULA实现更均匀的空间分辨率、更高通信速率和更低硬件成本。


<details>
  <summary>Details</summary>
Motivation: 现有射线天线阵列（RAA）虽然成本效益高，但其在同一平面配置的ULA存在信号遮挡问题，无法实现全空间覆盖，需要新的天线阵列架构来解决这一限制。

Method: 提出圆柱形直连天线阵列（DCAA），采用多层三维结构，每个简单均匀圆形阵列（sUCA）将UCA分成两个子阵列，每个子阵列的所有天线元件直接连接以实现对应物理方向的期望波束方向，从而实现全空间覆盖。

Result: 相比传统混合波束成形ULA，圆柱形DCAA能够实现均匀的空间分辨率、增强的通信速率和更低的硬件成本，仿真结果验证了其在毫米波和太赫兹等高频系统中的潜力。

Conclusion: 圆柱形DCAA架构有效解决了RAA的信号遮挡问题，为6G无线网络中XL-MIMO的实现提供了一种成本效益高且性能优越的解决方案，特别适用于高频通信系统。

Abstract: Extremely large-scale multi-input multi-output (XL-MIMO) is a promising technology for the sixth generation (6G) wireless networks, thanks to its superior spatial resolution and beamforming gains. In order to realize XL-MIMO costeffectively, an innovative ray antenna array (RAA) architecture with directly-connected uniform linear array (ULA) was recently proposed, which achieves flexible beamforming without relying on traditional analog phase shifters or digital beamforming. However, RAA suffers from the signal blockage issue since its ray-configured ULAs are placed in the same plane. To address this issue, this paper proposes a novel antenna array architecture termed cylinder directly-connected antenna array (DCAA), which is achieved via multiple simple uniform circular array (sUCA) with carefully designed orientations in a layered three-dimensional structure. The so-called sUCA partitions the uniform circular array (UCA) into two sub-arrays where each sub-array has all antenna elements directly connected to achieve a desired beam direction corresponding to the sub-array's physical orientation, thus achieving full spatial coverage. Compared with the conventional ULA architecture with hybrid analog/digital beamforming (HBF), the proposed cylinder DCAA can achieve uniform spatial resolution, enhanced communication rate and lower hardware costs. Simulation results are provided to validate the promised gains of cylinder DCAA, demonstrating its great potential for high-frequency systems such as millimeter wave (mmWave) and Terahertz (THz) systems.

</details>


### [18] [An Asynchronous Mixed-Signal Resonate-and-Fire Neuron](https://arxiv.org/abs/2512.07361)
*Giuseppe Leo,Paolo Gibertini,Irem Ilter,Erika Covi,Ole Richter,Elisabetta Chicca*

Main category: eess.SP

TL;DR: 该论文提出了一种基于CMOS混合信号的谐振-发放神经元电路，模拟生物神经元的频率选择性，用于边缘时间信号处理，具有低功耗和实时处理潜力。


<details>
  <summary>Details</summary>
Motivation: 边缘模拟计算能降低数据存储、传输需求和能耗，但实际实现仍处于早期阶段。将生物神经元特性转化为硬件是实现低功耗实时边缘处理的有效途径，特别是谐振神经元的频率选择性对时间信号处理有重要意义。

Method: 设计并制造了CMOS混合信号谐振-发放神经元电路，模拟中枢神经系统中控制振荡的神经细胞行为。集成了异步握手功能，进行了全面的变异性分析，并表征了其频率检测功能。

Result: 展示了该神经元电路在频率检测方面的功能，证明了在神经形态系统中大规模集成的可行性，为利用仿生电路进行高效边缘时间信号处理提供了进展。

Conclusion: 该CMOS谐振-发放神经元电路为生物启发的边缘时间信号处理提供了可行的硬件实现方案，推动了低功耗实时边缘计算的发展。

Abstract: Analog computing at the edge is an emerging strategy to limit data storage and transmission requirements, as well as energy consumption, and its practical implementation is in its initial stages of development. Translating properties of biological neurons into hardware offers a pathway towards low-power, real-time edge processing. Specifically, resonator neurons offer selectivity to specific frequencies as a potential solution for temporal signal processing. Here, we show a fabricated Complementary Metal-Oxide-Semiconductor (CMOS) mixed-signal Resonate-and-Fire (R&F) neuron circuit implementation that emulates the behavior of these neural cells responsible for controlling oscillations within the central nervous system. We integrate the design with asynchronous handshake capabilities, perform comprehensive variability analyses, and characterize its frequency detection functionality. Our results demonstrate the feasibility of large-scale integration within neuromorphic systems, thereby advancing the exploitation of bio-inspired circuits for efficient edge temporal signal processing.

</details>


### [19] [Impact of RIS Orientation on Throughput in UAV-Assisted Wireless Systems](https://arxiv.org/abs/2512.07411)
*Zawar Hussain,Faran Awais Butt,Ali Hussein Muqaibel,Saleh Ahmed Alawsh,Ijaz Haider Naqvi*

Main category: eess.SP

TL;DR: 研究RIS物理旋转角度对UAV辅助通信系统吞吐量的影响，通过调整方位角和仰角优化信道性能


<details>
  <summary>Details</summary>
Motivation: 在基站到用户的直接路径被阻挡的场景下，无人机搭载的RIS可以通过物理旋转调整方向来服务地面用户，但RIS方向对准对系统性能的具体影响需要量化研究

Method: 使用SimRIS信道模拟器，分析不同旋转角度（方位角和仰角）下的系统性能，生成性能热力图以识别最优RIS方向

Result: 研究表明RIS方向对准对可实现的速率有显著影响，通过优化RIS方向可以大幅提升系统吞吐量

Conclusion: RIS物理方向优化在实际部署中至关重要，需要开发方向感知的优化策略来最大化UAV-RIS通信系统的性能

Abstract: This paper investigates the impact of Reconfigurable Intelligent Surface (RIS) orientation on the throughput performance of Unmanned Aerial Vehicle (UAV)-assisted wireless communication systems. Specifically, we study how physical rotation of the RIS, through controlled azimuth and elevation adjustments, influences the effective channel and data rate. A UAV-mounted RIS enables directional alignment to serve ground users in scenarios where the direct Base Station (BS)-to-user path is blocked. Using the SimRIS channel simulator, we analyze the system under various rotation angles and present performance heatmaps that highlight optimal RIS orientations. The study shows that RIS alignment has a substantial effect on achievable rates, thereby motivating orientation-aware optimization in practical deployments.

</details>


### [20] [Dictionary-Based Contrastive Learning for GNSS Jamming Detection](https://arxiv.org/abs/2512.07512)
*Zawar Hussain,Arslan Majal,Aamir Hussain Chughtai,Talha Nadeem*

Main category: eess.SP

TL;DR: 提出基于字典的对比学习框架用于GNSS干扰检测，结合迁移学习、对比表示学习和模型压缩技术，在资源受限的嵌入式平台上实现实时低功耗检测。


<details>
  <summary>Details</summary>
Motivation: GNSS信号接收功率极低，易受射频干扰和有意干扰。现有数据驱动方法虽然强大，但在资源受限的嵌入式接收器上实现实时可靠的干扰检测面临计算和内存需求高的挑战。

Method: 提出字典基对比学习框架，整合迁移学习、对比表示学习和模型压缩技术。使用调优的对比损失和字典基损失函数增强低数据条件下的特征可分性，应用结构化剪枝和知识蒸馏降低模型复杂度。

Result: 在各种数据机制下的广泛评估表明，该算法持续优于现代CNN、MobileViT和ResNet-18架构。框架显著减少了内存占用和推理延迟，适合嵌入式平台实时低功耗GNSS干扰检测。

Conclusion: 该字典基对比学习框架为资源受限的嵌入式GNSS接收器提供了一种高效、准确的实时干扰检测解决方案，平衡了检测性能与计算资源需求。

Abstract: Global Navigation Satellite System (GNSS) signals are fundamental in applications across navigation, transportation, and industrial networks. However, their extremely low received power makes them highly vulnerable to radio-frequency interference (RFI) and intentional jamming. Modern data-driven methods offer powerful representational power for such applications, however real-time and reliable jamming detection on resource-limited embedded receivers remains a key challenge due to the high computational and memory demands of the conventional learning paradigm. To address these challenges, this work presents a dictionary-based contrastive learning (DBCL) framework for GNSS jamming detection that integrates transfer learning, contrastive representation learning, and model compression techniques. The framework combines tuned contrastive and dictionary-based loss functions to enhance feature separability under low-data conditions and applies structured pruning and knowledge distillation to reduce model complexity while maintaining high accuracy. Extensive evaluation across varying data regimes demonstrate that the proposed algorithm consistently outperforms modern CNN, MobileViT, and ResNet-18 architectures. The framework achieves a substantial reduction in memory footprint and inference latency, confirming its suitability for real-time, low-power GNSS interference detection on embedded platforms.

</details>


### [21] [Segmented Waveguide-Enabled Pinching-Antenna Systems (SWANs) for ISAC](https://arxiv.org/abs/2512.07649)
*Hao Jiang,Chongjun Ouyang,Zhaolin Wang,Yuanwei Liu,Arumugam Nallanathan,Zhiguo Ding,Robert Schober*

Main category: eess.SP

TL;DR: 提出分段波导夹持天线系统(SWAN)辅助的集成感知与通信框架，通过将波导分段提高感知性能并降低复杂度


<details>
  <summary>Details</summary>
Motivation: 传统夹持天线系统使用单一长波导存在接收模型复杂、波导内传播损耗大的问题，需要改进以提升集成感知与通信性能

Method: 将波导分为多个短段，每段有独立馈电点；提出三种段控制协议：段选择、段聚合、段复用；分析理论性能极限并扩展到多用户场景

Result: SWAN显著简化接收模型、降低传播损耗；理论分析揭示了感知性能随段数增加的增益规律；数值结果验证了推导正确性和算法有效性

Conclusion: SWAN辅助的ISAC框架在感知和通信性能间取得良好平衡，相比传统系统具有明显优势，为集成感知与通信提供了有效解决方案

Abstract: A segmented waveguide-enabled pinching-antenna system (SWAN)-assisted integrated sensing and communications (ISAC) framework is proposed. Unlike conventional pinching antenna systems (PASS), which use a single long waveguide, SWAN divides the waveguide into multiple short segments, each with a dedicated feed point. Thanks to the segmented structure, SWAN enhances sensing performance by significantly simplifying the reception model and reducing the in-waveguide propagation loss. To balance performance and complexity, three segment controlling protocols are proposed for the transceivers, namely i) \emph{segment selection} to select a single segment for signal transceiving, ii) \emph{segment aggregation} to aggregate signals from all segments using a single RF chain, and iii) \emph{segment multiplexing} to jointly process the signals from all segments using individual RF chains. The theoretical sensing performance limit is first analyzed for different protocols, unveiling how the sensing performance gain of SWAN scales with the number of segments. Based on this performance limit, the Pareto fronts of sensing and communication performance are characterized for the simple one-user one-target case, which is then extended to the general multi-user single-target case based on time-division multiple access (TDMA). Numerical results are presented to verify the correctness of the derivations and the effectiveness of the proposed algorithms, which jointly confirm the advantages of SWAN-assisted ISAC.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [A self-driving lab for solution-processed electrochromic thin films](https://arxiv.org/abs/2512.05989)
*Selma Dahms,Luca Torresi,Shahbaz Tareq Bandesha,Jan Hansmann,Holger Röhm,Alexander Colsmann,Marco Schott,Pascal Friederich*

Main category: cs.LG

TL;DR: 利用自驱动实验室加速电致变色涂层开发，结合自动化与机器学习优化旋涂工艺参数


<details>
  <summary>Details</summary>
Motivation: 溶液加工电致变色材料在智能窗户和显示器中具有高潜力，但其性能受材料和工艺条件影响。旋涂电致变色薄膜需要光滑无缺陷涂层以获得最佳对比度，而优化工艺参数复杂且耗时，阻碍了快速开发。

Method: 采用自驱动实验室方法，结合自动化数据采集、图像处理、光谱分析和贝叶斯优化，高效探索旋涂工艺参数。系统通过机器学习指导实验，实现有针对性的参数搜索。

Result: 该方法不仅提高了实验通量，还能有效寻找最优工艺参数。系统成功应用于电致变色涂层开发，展示了自驱动实验室在加速材料发现和工艺优化方面的潜力。

Conclusion: 自驱动实验室方法可显著加速电致变色涂层等溶液加工材料的开发，为材料发现和工艺优化提供了高效平台，具有广泛的应用前景。

Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.

</details>


### [23] [Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure](https://arxiv.org/abs/2512.05990)
*Xin Li*

Main category: cs.LG

TL;DR: 提出记忆摊销推理(MAI)框架，用代数拓扑统一学习和记忆为单一几何基质的相变，通过同调奇偶原则区分内容与上下文，将高复杂度搜索转化为低复杂度查找。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习将参数静态结构与推理动态流程分离，缺乏生物认知的样本效率和热力学经济性，需要统一学习和记忆的理论框架。

Method: 基于代数拓扑的MAI框架，引入同调奇偶原则：偶维同调实例化稳定内容，奇维同调实例化动态上下文。通过拓扑三位一体变换（搜索→闭包→结构）实现认知过程。

Result: 将高复杂度递归搜索（NPSPACE中的Savitch定理）转化为低复杂度查找（P中的动态规划），通过拓扑循环闭包机制实现，由Wake-Sleep算法的拓扑推广控制。

Conclusion: MAI为从慢思考到快思考的涌现提供严格解释，为后图灵架构提供蓝图，通过拓扑共振进行计算，实现更高效的认知系统。

Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.

</details>


### [24] [Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra](https://arxiv.org/abs/2512.06059)
*Andrea Della Valle,Annalisa D'Arco,Tiziana Mancini,Rosanna Mosetti,Maria Chiara Paolozzi,Stefano Lupi,Sebastiano Pilati,Andrea Perali*

Main category: cs.LG

TL;DR: 使用条件生成神经网络增强红外光谱数据集，训练可识别9种VOCs及其浓度的判别神经网络


<details>
  <summary>Details</summary>
Motivation: 挥发性有机化合物(VOCs)对健康有害，需要准确检测。红外光谱虽能超灵敏检测低浓度VOCs，但光谱复杂性限制了实时识别和定量。深度神经网络需要大量训练数据，而实验数据集有限。

Method: 创建9种不同类别化合物的实验VOC数据集，使用其红外吸收光谱。通过条件生成神经网络生成合成光谱来增强数据集，增加光谱数量和浓度多样性。训练判别神经网络进行VOC识别和浓度预测。

Result: 训练出稳健的判别神经网络，能够可靠识别9种VOCs，并精确预测其浓度。该网络适合集成到VOC识别和分析的传感设备中。

Conclusion: 通过数据增强技术解决了红外光谱VOC检测中训练数据不足的问题，开发了可用于实时VOC识别和浓度预测的神经网络模型。

Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.

</details>


### [25] [When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models](https://arxiv.org/abs/2512.06062)
*S. M. Mustaqim,Anantaa Kotal,Paul H. Yi*

Main category: cs.LG

TL;DR: 提出一种基于聚类中位数和邻域分析的黑盒成员推理攻击方法，利用生成模型合成数据与原始训练数据在流形结构上的重叠来推断成员信息，即使在差分隐私等保护机制下仍存在可测量的隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 生成模型被广泛用于生成保护隐私的合成数据，但现有研究主要关注样本级别的记忆化问题，忽略了数据流形结构重叠可能带来的隐私泄露风险。本文旨在揭示这种被忽视的攻击面。

Method: 提出黑盒成员推理攻击方法：1) 重复查询生成模型获取大量合成样本；2) 通过无监督聚类识别合成分布中的密集区域；3) 分析聚类中位数和邻域，这些区域对应原始训练数据的高密度区域；4) 利用这些邻域作为训练样本的代理来推断成员信息或重建近似记录。

Result: 在医疗、金融等敏感领域的实验表明，即使生成模型采用差分隐私或其他噪声机制训练，真实数据与合成数据之间的聚类重叠仍会导致可测量的成员信息泄露。

Conclusion: 合成数据生成管道存在被忽视的攻击面，需要更强的隐私保证机制，不仅要考虑样本级别的记忆化，还要考虑分布邻域推理。这强调了在隐私保护数据发布中需要更全面的隐私评估。

Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.

</details>


### [26] [JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning](https://arxiv.org/abs/2512.06102)
*Ufuk Çakır,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: JaxWildfire：基于JAX的高性能GPU加速野火模拟器，用于强化学习训练


<details>
  <summary>Details</summary>
Motivation: 现有野火模拟器速度慢，限制了强化学习在野火管理中的应用，需要高性能模拟器来支持RL代理的训练

Method: 基于细胞自动机的概率性火势传播模型，使用JAX实现，通过vmap实现向量化模拟，支持GPU加速

Result: 相比现有软件实现6-35倍加速，支持基于梯度的模拟器参数优化，能够训练RL代理学习野火抑制策略

Conclusion: JaxWildfire是推进强化学习在自然灾害管理中应用的重要一步，为RL技术发展提供了必要的模拟基础设施

Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.

</details>


### [27] [LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding](https://arxiv.org/abs/2512.06982)
*Yu Yu,Qian Xie,Nairen Cao,Li Jin*

Main category: cs.LG

TL;DR: 提出LLM驱动的神经架构搜索方法，用于多源强化学习中的状态编码器设计，相比传统NAS和GENIUS框架，能以更少评估次数发现更高性能架构


<details>
  <summary>Details</summary>
Motivation: 多源强化学习（传感器测量、时序信号、图像观察、文本指令等）的状态编码器设计缺乏探索且常需手动设计，现有NAS方法忽略了模块中间输出的有用信息，限制了样本效率

Method: 将多源状态编码器设计形式化为复合神经架构搜索问题，提出LLM驱动的NAS流程，利用语言模型先验和中间输出信号指导样本高效的搜索

Result: 在混合自主交通控制任务中，该方法比传统NAS基线和LLM-based GENIUS框架以更少的候选评估次数发现更高性能的架构

Conclusion: LLM驱动的NAS方法能有效利用先验知识和中间信号，在多源RL中实现样本高效的复合状态编码器搜索

Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.

</details>


### [28] [ARC-AGI Without Pretraining](https://arxiv.org/abs/2512.06104)
*Isaac Liao,Albert Gu*

Main category: cs.LG

TL;DR: 76K参数模型CompressARC无需预训练，通过最小描述长度在推理时解决20%的ARC-AGI视觉谜题，展现了极端泛化能力


<details>
  <summary>Details</summary>
Motivation: 挑战传统观点：反驳解决ARC-AGI视觉谜题需要大规模预训练的假设，探索在极有限数据条件下实现智能的替代方法

Method: 提出CompressARC模型，仅76K参数且无预训练，在推理时通过最小描述长度（MDL）原则处理目标谜题，仅使用单个样本（目标谜题本身，不含最终解信息）进行训练

Result: 在极端数据限制下（不使用ARC-AGI提供的训练集），成功解决20%的评估谜题，展现了传统深度学习难以实现的极端泛化能力

Conclusion: 最小描述长度（MDL）是除传统预训练外实现智能的可行替代途径，在极有限数据条件下仍能解决多样化的创造性谜题

Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.

</details>


### [29] [A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts](https://arxiv.org/abs/2512.06111)
*Arthur Mukwaya,Nancy Kasamala,Nana Kankam Gyimah,Judith Mwakalonge,Gurcan Comert,Saidi Siuhi,Denis Ruganuza,Mark Ngotonie*

Main category: cs.LG

TL;DR: 本研究提出机器学习框架，通过选择最优代表日进行短期交通计数，提高年度平均日交通量预测精度，相比传统方法显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 美国各州交通部门面临获取准确年度平均日交通量的挑战，特别是未监测道路。连续计数站成本高难以广泛部署，现有短期计数方法效果有限，需要更优的数据收集策略。

Method: 提出机器学习框架，采用"最优日"方法迭代选择对AADT估计最有效的天数，与"非最优日"基准对比。利用德克萨斯州2022-2023年交通数据，结合留一法技术生成无偏代表日交通特征。

Result: 最优日方法显著优于基准，最佳日（第186天）的RMSE、MAE、MAPE和R²分别为7,871.15、3,645.09、11.95%和0.9756，而基准为11,185.00、5,118.57、14.42%和0.9499。

Conclusion: 该方法为交通部门提供了替代传统短期计数实践的方案，可提高AADT估计精度，支持公路性能监测系统合规，并降低全州交通数据收集的运营成本。

Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.

</details>


### [30] [Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting](https://arxiv.org/abs/2512.06134)
*Georgi Hrusanov,Duy-Thanh Vu,Duy-Cat Can,Sophie Tascedda,Margaret Ryan,Julien Bodelet,Katarzyna Koscielska,Carsten Magnus,Oliver Y. Chén*

Main category: cs.LG

TL;DR: NKM是一种新的机器学习架构，通过融合多模态数据和注意力机制，实现阿尔茨海默病认知衰退的个性化、可解释预测。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期认知衰退预测对疾病评估和管理至关重要，但现有方法难以在保持可解释性的同时整合多模态数据进行纵向个性化预测。

Method: 提出Neural Koopman Machine (NKM)，结合动力系统和注意力机制，使用遗传、神经影像、蛋白质组和人口统计学等多模态数据同时预测多个认知评分。通过融合组感知分层注意力机制和Koopman算子框架，将复杂非线性轨迹转化为可解释的线性表示。

Result: 在ADNI数据集上，NKM在预测认知衰退轨迹方面始终优于传统机器学习和深度学习模型，能够同时预测多个认知评分变化、量化不同生物标志物对特定认知评分的贡献，并识别与认知恶化最相关的大脑区域。

Conclusion: NKM通过可解释的显式系统，利用过去的多模态数据推进了阿尔茨海默病未来认知衰退的个性化预测，并揭示了疾病进展的潜在多模态生物学基础。

Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($α$) and biological ($β$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.

</details>


### [31] [gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points](https://arxiv.org/abs/2512.06143)
*Marcus M. Noack,Mark D. Risser,Hengrui Luo,Vardaan Tekriwal,Ronald J. Pandolfi*

Main category: cs.LG

TL;DR: 提出gp2Scale方法，在不依赖诱导点、核插值或邻域近似的情况下，将精确高斯过程扩展到超过1000万数据点，通过利用灵活、紧支撑、非平稳核函数识别协方差矩阵中的稀疏结构。


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程扩展方法存在计算速度、预测精度和不确定性量化之间的权衡，大多数方法依赖各种近似降低了准确性并限制了核函数和噪声模型设计的灵活性，而当前许多领域对表达性非平稳核函数的需求正在增长。

Method: 提出gp2Scale方法，利用灵活、紧支撑、非平稳核函数识别协方差矩阵中自然出现的稀疏结构，然后利用这种稀疏性进行线性系统求解和对数行列式计算，实现精确高斯过程的扩展。

Result: 方法能够扩展到超过1000万数据点，在多个真实数据集上展示功能，与最先进的近似算法相比，在许多情况下表现出优越的近似性能。

Conclusion: 该方法的核心优势在于对任意高斯过程定制（核心核设计、噪声和均值函数）以及输入空间类型的不可知性，使其特别适合现代高斯过程应用。

Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.

</details>


### [32] [Learning Invariant Graph Representations Through Redundant Information](https://arxiv.org/abs/2512.06154)
*Barproda Halder,Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 该论文提出了一种基于部分信息分解(PID)的冗余引导不变图学习(RIG)框架，用于解决图表示学习中的分布外泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于经典信息论度量的不变表示学习方法存在局限性，学习到的表示往往保留虚假成分，无法有效实现分布外泛化。需要更精确地关注虚假子图和不变子图之间关于目标Y的冗余信息。

Method: 提出RIG框架：1)使用部分信息分解(PID)识别虚假子图Gs和不变子图Gc之间的冗余信息；2)采用多级优化框架，交替估计冗余信息下界并最大化该下界，同时隔离虚假和因果子图。

Result: 在合成和真实世界图数据集上的实验表明，RIG框架在各种分布偏移下具有优越的泛化能力。

Conclusion: 部分信息分解为不变图表示学习提供了新的理论工具，RIG框架通过精确处理冗余信息有效提升了分布外泛化性能。

Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.

</details>


### [33] [PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations](https://arxiv.org/abs/2512.06183)
*Lindong Liu,Zhixiong Jin,Seongjin Choi*

Main category: cs.LG

TL;DR: PMA-Diffusion：一种物理引导的掩码感知扩散框架，用于从稀疏观测中重建高速公路速度场，在仅有5%可见度的情况下仍优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 高速公路交通状态信息对智能交通系统至关重要，但现有的环路检测器和探测车数据通常过于稀疏和嘈杂，难以捕捉交通流的详细动态。需要一种能够从稀疏、不完整观测中准确重建交通状态的方法。

Method: 提出PMA-Diffusion框架，包含两个核心部分：1）直接在稀疏观测的速度场上训练扩散先验，采用单掩码和双掩码两种掩码感知训练策略；2）推理阶段使用物理引导的后验采样器，交替进行反向扩散更新、观测投影和基于自适应各向异性平滑的物理引导投影。

Result: 在I-24 MOTION数据集上测试，即使在仅有5%可见度的严重稀疏情况下，PMA-Diffusion在三个重建误差指标上均优于其他基线方法。使用稀疏观测训练的模型性能几乎接近使用完整观测训练的基线模型。

Conclusion: 将掩码感知扩散先验与物理引导后验采样器相结合，为实际传感稀疏条件下的交通状态估计提供了可靠且灵活的解决方案。

Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.

</details>


### [34] [How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?](https://arxiv.org/abs/2512.06200)
*Tomohiro Yamashita,Daichi Amagata,Yusuke Matsui*

Main category: cs.LG

TL;DR: 本文提出一个评估ANNS索引数据删除效率的实验框架和综合指标，将图基ANNS的数据删除方法分为三类并形式化，应用于HNSW分析删除效果，并提出动态选择删除方法的Deletion Control方法。


<details>
  <summary>Details</summary>
Motivation: 随着近似最近邻搜索在检索增强生成等应用中的重要性增加，动态数据上的ANNS问题受到关注，但目前缺乏对数据删除的全面评估方法。

Method: 提出实验框架和评估指标，将图基ANNS的数据删除方法分为三类并数学形式化，应用于HNSW分析删除效果，提出Deletion Control方法动态选择删除策略。

Result: 建立了数据删除的评估框架，分析了不同删除方法对HNSW性能的影响，提出的Deletion Control方法能在保证搜索精度要求下动态选择最优删除方法。

Conclusion: 该研究为ANNS索引的数据删除提供了首个全面的评估框架，提出的Deletion Control方法能有效管理动态数据环境下的删除操作，平衡精度和效率。

Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.

</details>


### [35] [K2-V2: A 360-Open, Reasoning-Enhanced LLM](https://arxiv.org/abs/2512.06201)
*K2 Team,Zhengzhong Liu,Liping Tang,Linghao Jin,Haonan Li,Nikhil Ranjan,Desai Fan,Shaurya Rohatgi,Richard Fan,Omkar Pangarkar,Huijuan Wang,Zhoujun Cheng,Suqi Sun,Seungwook Han,Bowen Tan,Gurpreet Gosal,Xudong Han,Varad Pimpalkhute,Shibo Hao,Ming Shan Hee,Joel Hestness,Haolong Jia,Liqun Ma,Aaryamonvikram Singh,Daria Soboleva,Natalia Vassilieva,Renxi Wang,Yingquan Wu,Yuekai Sun,Taylor Killian,Alexander Moreno,John Maggs,Hector Ren,Guowei He,Hongyi Wang,Xuezhe Ma,Yuqi Wang,Mikhail Yurochkin,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-V2是一个从头开始构建的360度开放大语言模型，专注于推理能力，在72B参数级别表现优异，超越Qwen2.5-72B并接近Qwen3-235B的性能。


<details>
  <summary>Details</summary>
Motivation: 构建一个专门针对推理任务优化的开放基础模型，为复杂推理任务提供强大的基础，同时支持对话、知识检索等通用功能，并推动开源社区的发展。

Method: 从头开始训练，在训练过程中主动注入领域知识、推理能力、长上下文和工具使用能力，通过简单的监督微调建立强基线，并发布完整的训练历史和数据组成。

Result: K2-V2成为最强的完全开放模型，在其规模类别中与开放权重领导者竞争，超越Qwen2.5-72B并接近Qwen3-235B的性能，为高级对齐留下了显著提升空间。

Conclusion: K2-V2提供了一个强大的、以推理为中心的基础模型，通过发布完整的训练历史和数据组成，最大化持续训练的效果，为社区赋能并推动开源生产场景的发展。

Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.

</details>


### [36] [Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks](https://arxiv.org/abs/2512.06297)
*Luca Di Carlo,Chase Goddard,David J. Schwab*

Main category: cs.LG

TL;DR: 神经网络损失函数景观中的吸引盆通过低损失路径相连，但优化过程通常局限于单个凸盆，很少探索中间点。研究发现这是由于曲率变化与优化噪声相互作用产生的熵垒所致。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络表现出一个矛盾现象：损失景观中的吸引盆通常通过低损失路径相连，但优化动态却往往局限于单个凸盆，很少探索中间点。研究者试图解释这种连通性与限制性之间的悖论。

Method: 通过识别曲率变化与优化动态中噪声相互作用产生的熵垒。实证研究发现，曲率随着远离最小值而系统性上升，产生有效力将噪声动态偏转回端点，即使损失保持近乎平坦。

Result: 研究发现熵垒比能量垒更持久，塑造了参数空间中解的后期定位。曲率诱导的熵力在深度学习的连通性和限制性中起主导作用。

Conclusion: 研究结果强调了曲率诱导的熵力在深度学习景观中同时控制连通性和限制性的关键作用，解释了为什么优化过程通常局限于单个凸盆，即使存在低损失连接路径。

Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.

</details>


### [37] [Quantifying Memory Use in Reinforcement Learning with Temporal Range](https://arxiv.org/abs/2512.06204)
*Rodney Lafuente-Mercado,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 提出Temporal Range指標，衡量RL策略對過去觀察的依賴程度，通過計算輸出對輸入序列的時間敏感性來量化記憶需求


<details>
  <summary>Details</summary>
Motivation: 量化訓練好的RL策略實際使用過去觀察的程度，提供一個模型無關的指標來比較不同代理和環境的記憶依賴性

Method: 提出Temporal Range指標，通過反向模式自動微分計算Jacobian塊∂y_s/∂x_t，並將其總結為幅度加權平均滯後時間，在線性設定下用一組自然公理進行表徵

Result: 在診斷和控制任務中，Temporal Range：(i)在完全觀察控制中保持較小，(ii)在Copy-k任務中與任務真實滯後時間成比例，(iii)與實現接近最優回報所需的最小歷史窗口一致

Conclusion: Temporal Range提供了一個實用的序列級讀數，用於比較代理和環境的記憶依賴性，並選擇最短足夠的上下文長度

Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.

</details>


### [38] [Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry](https://arxiv.org/abs/2512.06347)
*Naoki Yoshida,Isao Ishikawa,Masaaki Imaizumi*

Main category: cs.LG

TL;DR: 论文证明在师生框架下，一旦训练样本数超过某个阈值，插值器的泛化误差会变为0，这主要源于参数空间中插值器集合的几何结构特性。


<details>
  <summary>Details</summary>
Motivation: 理解大规模模型（如深度神经网络）的高泛化能力是机器学习理论的核心开放问题。虽然现有理论将这种现象归因于SGD的隐式偏置，但实证证据表明这主要源于模型本身的特性，特别是随机采样的插值器也能有效泛化。

Method: 在师生框架下，利用代数几何工具数学刻画参数空间中插值器集合的几何结构，证明泛化误差变为零的条件。

Result: 证明随机采样插值器的泛化误差在训练样本数超过由插值器集合几何结构决定的阈值时，会精确变为零。

Conclusion: 模型的泛化能力主要源于参数空间中插值器集合的几何特性，而非SGD的隐式偏置，这为理解大规模模型的泛化提供了新的理论视角。

Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.

</details>


### [39] [Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration](https://arxiv.org/abs/2512.06218)
*Huizhen Yu,Yi Wan,Richard S. Sutton*

Main category: cs.LG

TL;DR: 将异步随机逼近框架应用于平均奖励半马尔可夫决策过程的强化学习，建立了RVI Q-learning算法的收敛性，并引入了新的单调性条件来估计最优奖励率。


<details>
  <summary>Details</summary>
Motivation: 将作者最近在Borkar-Meyn框架下的异步随机逼近结果应用于平均奖励半马尔可夫决策过程的强化学习，扩展了现有算法框架并改进收敛性分析。

Method: 应用异步随机逼近框架到Schweitzer经典相对值迭代算法的异步SA版本（RVI Q-learning），针对有限空间、弱通信SMDPs，引入新的单调性条件来估计最优奖励率。

Result: 证明了算法几乎必然收敛到平均奖励最优方程解的紧致连通子集，在额外步长和异步条件下收敛到唯一的样本路径依赖解，新单调性条件显著扩展了先前考虑的算法框架。

Conclusion: 成功将异步随机逼近理论应用于平均奖励SMDPs的强化学习，建立了RVI Q-learning的收敛性，并通过新的单调性条件和稳定性分析扩展了算法框架。

Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.

</details>


### [40] [Optimizing Optimizers for Fast Gradient-Based Learning](https://arxiv.org/abs/2512.06370)
*Jaerin Lee,Kyoung Mu Lee*

Main category: cs.LG

TL;DR: 提出自动化优化器设计的理论框架，基于贪心原则将优化器设计问题转化为最大化瞬时损失下降的凸优化问题


<details>
  <summary>Details</summary>
Motivation: 为梯度学习中的优化器设计提供理论基础，实现优化器的自动化设计和超参数调整

Method: 将优化器视为将梯度信号转换为参数运动的函数，将设计问题转化为凸优化问题，在不同约束下求解得到优化器闭式解

Result: 不仅恢复了多种流行优化器作为闭式解，还能自动确定最优超参数，支持动态优化调整

Conclusion: 建立了优化器设计的系统化理论框架，实现了优化过程的动态自动化调整

Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.

</details>


### [41] [Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph](https://arxiv.org/abs/2512.06236)
*Haiyang Yu,Meng-Chieh Lee,Xiang song,Qi Zhu,Christos Faloutsos*

Main category: cs.LG

TL;DR: 提出GraphDeT框架，通过在图神经网络训练中加入目标图的边去噪辅助任务，提升图域适应中的节点分类性能


<details>
  <summary>Details</summary>
Motivation: 图域适应中，图结构域偏移（如不同时间或区域收集的图数据）会导致图神经网络在目标图上性能下降。研究发现，简单地在目标图上加入边去噪辅助任务能显著提升性能

Method: 提出GraphDeT框架，在图神经网络训练中集成边去噪辅助任务。理论分析将该辅助任务与图泛化边界和-divergence联系起来，证明该任务能约束并收紧泛化边界

Result: 实验结果表明，在处理时间和区域域图偏移时，GraphDeT相比现有基线方法表现出优越性能

Conclusion: 在目标图上加入边去噪辅助任务是提升图神经网络在域适应中泛化能力的有效方法，GraphDeT框架为此提供了理论和实践支持

Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.

</details>


### [42] [Prediction with Expert Advice under Local Differential Privacy](https://arxiv.org/abs/2512.06971)
*Ben Jacobsen,Kassem Fawaz*

Main category: cs.LG

TL;DR: 本文研究在本地差分隐私约束下的专家建议预测问题，提出了两种改进算法RW-AdaBatch和RW-Meta，在真实COVID-19医院数据上表现优于传统基线和中心差分隐私算法。


<details>
  <summary>Details</summary>
Motivation: 在本地差分隐私约束下进行预测与专家建议的经典问题研究，现有方法在隐私保护与预测性能之间存在权衡，需要开发既能保护隐私又具有良好性能的算法。

Method: 首先证明经典算法自然满足LDP，然后设计两种新算法：1) RW-AdaBatch利用LDP诱导的有限切换行为实现隐私放大，类似于离线学习中的洗牌模型；2) RW-Meta开发了一种在非平凡学习算法专家之间进行隐私选择的一般方法，在LDP背景下无需额外隐私成本。

Result: RW-AdaBatch在更容易的数据上隐私保护更强且基本无性能损失；RW-Meta在COVID-19医院数据预测任务中，比传统基线和最先进的中心差分隐私算法性能提升1.5-3倍，能够预测每周报告COVID患者密度最高的医院。

Conclusion: 本文提出的RW-AdaBatch和RW-Meta算法在本地差分隐私约束下显著改进了专家建议预测的性能，特别是在处理真实世界医疗数据时表现出色，为隐私保护机器学习提供了有效解决方案。

Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.

</details>


### [43] [Quantization Blindspots: How Model Compression Breaks Backdoor Defenses](https://arxiv.org/abs/2512.06243)
*Rohan Pandey,Eric Ye*

Main category: cs.LG

TL;DR: 量化（INT8/INT4）使现有后门防御失效，但后门攻击仍保持高成功率，暴露了防御评估与实际部署之间的不匹配


<details>
  <summary>Details</summary>
Motivation: 现实世界部署中，模型通常会被量化（如INT8/INT4）以减少内存和延迟，但现有后门防御主要在FP32模型上评估，缺乏对量化模型防御效果的研究

Method: 对5种代表性后门防御在3种精度设置（FP32、INT8动态、INT4模拟）和2个标准视觉基准（GTSRB、CIFAR-10）上，使用经典的BadNet攻击进行系统实证研究

Result: INT8量化使所有防御的检测率降至0%，而攻击成功率仍高于99%；INT4量化效果因数据集而异（Neural Cleanse在GTSRB有效但在CIFAR-10失效），但后门攻击成功率仍高于90%

Conclusion: 量化鲁棒性应成为未来后门防御评估和设计的必要维度，需要解决防御评估（FP32）与实际部署（量化模型）之间的不匹配问题

Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.

</details>


### [44] [A Bootstrap Perspective on Stochastic Gradient Descent](https://arxiv.org/abs/2512.07676)
*Hongjian Lan,Yucong Liu,Florian Schäfer*

Main category: cs.LG

TL;DR: SGD通过梯度协方差矩阵的迹隐式正则化，利用批次采样的梯度变异性作为数据收集过程随机性的代理，从而提高泛化性能


<details>
  <summary>Details</summary>
Motivation: 研究SGD比确定性GD具有更好泛化能力的原因，从统计bootstrap角度解释SGD如何利用梯度变异性来近似数据收集过程的随机性

Method: 通过经验风险最小化的理想化实验，展示SGD被吸引到在重采样下稳健的参数选择；理论证明SGD隐式正则化梯度协方差矩阵的迹来控制算法变异性；在神经网络训练中显式加入算法变异性估计作为正则器

Result: SGD能够避免虚假解，即使这些解位于训练损失更宽更深的极小值中；显式加入算法变异性正则器能够提高测试性能；SGD通过控制算法变异性使解对采样噪声更不敏感

Conclusion: SGD的泛化优势源于其通过梯度协方差矩阵的迹隐式正则化，利用批次采样的梯度变异性作为数据收集过程随机性的代理，这种bootstrap估计机制是SGD泛化优势的基础

Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.

</details>


### [45] [Auto-exploration for online reinforcement learning](https://arxiv.org/abs/2512.06244)
*Caleb Ju,Guanghui Lan*

Main category: cs.LG

TL;DR: 提出具有自动探索能力的新RL方法，无需先验参数知识，在表格和线性函数逼近设置下实现O(ε⁻²)样本复杂度


<details>
  <summary>Details</summary>
Motivation: 现有RL算法需要假设充分探索状态和动作空间，这导致算法不可实现且性能次优。需要参数无关的自动探索方法来解决探索-利用困境。

Method: 提出两类具有自动探索能力的方法：表格设置和线性函数逼近。采用动态混合时间、折扣状态分布采样、鲁棒梯度估计器和优势差距函数等创新技术。

Result: 在存在探索最优策略的假设下，两种方法都能达到O(ε⁻²)的样本复杂度来求解ε误差问题。复杂度不包含可能任意大的算法依赖参数。

Conclusion: 新方法通过参数无关的自动探索解决了RL中的探索-利用困境，实现了可实现的算法和最优性能，具有简单易实现的特点。

Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(ε^{-2})$ sample complexity to solve to $ε$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.

</details>


### [46] [Provable Long-Range Benefits of Next-Token Prediction](https://arxiv.org/abs/2512.07818)
*Xinyuan Cao,Santosh S. Vempala*

Main category: cs.LG

TL;DR: 论文证明：通过优化RNN的下一词预测，可以学习到训练分布的长程结构，使得模型生成的k个连续token与真实文档的k个token无法被有界描述长度的算法区分。


<details>
  <summary>Details</summary>
Motivation: 解释为什么现代语言模型（训练用于下一词预测）能够生成连贯文档并捕捉长程结构，为这一经验现象提供理论解释。

Method: 使用循环神经网络（RNN）优化下一词预测任务，从复杂性理论角度分析模型能力，证明模型可以近似训练分布。

Result: 证明了对于从训练分布采样的文档，任何有界描述长度的算法都无法区分模型生成的k个连续token与真实文档的k个token，且所需模型规模与k呈多项式关系。

Conclusion: 下一词预测任务在理论上足够强大，能够学习长程结构，这为实践中观察到的语言模型长程连贯性提供了复杂性理论解释。

Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.

</details>


### [47] [Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning](https://arxiv.org/abs/2512.06250)
*Chris Tava*

Main category: cs.LG

TL;DR: 提出一种强化学习方法，让自主代理学习在两种正交导航策略（系统探索与目标导向路径规划）之间切换的阈值，在迷宫导航任务中动态调整策略切换点以提升性能。


<details>
  <summary>Details</summary>
Motivation: 自主代理需要多种策略解决复杂任务，但确定何时切换策略仍然具有挑战性。固定阈值方法不够灵活，需要大量领域知识或手工启发式规则。

Method: 使用Q-learning学习策略切换阈值，基于覆盖率和目标距离动态调整。将状态空间离散化为覆盖率和距离桶，无需迷宫墙壁位置、最优阈值或手工启发式知识，只需迷宫尺寸和目标位置。

Result: 在240个测试配置（4种迷宫尺寸×10个独特迷宫×6种代理变体）中，自适应阈值学习优于单策略代理和固定40%阈值基线：完成时间提升23-55%，运行时间方差降低83%，最坏情况改进71%。性能增益随问题复杂度增加而增加。

Conclusion: 自适应策略切换学习能够有效提升自主代理在复杂导航任务中的性能，随着问题复杂度增加，自适应策略选择相比固定启发式的价值成比例增加，且学习到的切换行为能在同尺寸类别的未见迷宫配置中泛化。

Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.

</details>


### [48] [Learning Without Time-Based Embodiment Resets in Soft-Actor Critic](https://arxiv.org/abs/2512.06252)
*Homayoon Farrahi,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 研究探索在没有回合终止和机器人重置的情况下使用SAC算法进行强化学习的挑战，提出持续SAC版本并通过增加策略熵来补偿重置缺失带来的性能损失。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习任务通常依赖回合终止和环境重置等辅助组件来加速学习，但这些设置不自然且可能阻碍在现实世界中的长期性能。研究旨在探索在没有这些辅助组件的情况下学习所面临的挑战。

Method: 提出持续版本的SAC算法，通过简单修改现有任务的奖励函数，使其能在没有回合终止的情况下学习。在Gym Reacher任务上分析没有机器人重置时的失败原因，并通过在性能下降或停滞时增加策略熵来补偿重置缺失。

Result: 持续SAC在性能上可与或优于回合式SAC，同时降低了对折扣率γ值的敏感性。研究发现重置有助于状态空间探索，移除重置会导致探索不足和学习失败/缓慢。增加策略熵能有效恢复因不使用重置而损失的性能。

Conclusion: 在没有回合终止和机器人重置的情况下学习是可行的，但需要解决探索不足的问题。通过适当修改奖励函数和动态调整策略熵，可以在这些更自然的设置下实现有效的强化学习。

Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $γ$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.

</details>


### [49] [Networked Restless Multi-Arm Bandits with Reinforcement Learning](https://arxiv.org/abs/2512.06274)
*Hanmo Zhang,Zenghui Sun,Kai Wang*

Main category: cs.LG

TL;DR: 本文提出了Networked RMAB框架，将传统RMAB与独立级联模型结合以捕捉网络环境中个体间的交互作用，并设计了具有理论保证的近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统RMAB假设各臂独立，无法处理现实世界中个体间的交互作用，这在公共卫生等资源分配场景中尤为重要。

Method: 提出Networked RMAB框架，结合独立级联模型；证明Bellman方程的子模性，应用爬山算法获得1-1/e近似保证；设计收敛的近似Bellman更新；开发针对网络设置的Q-learning算法。

Result: 理论证明：Bellman方程具有子模性，爬山算法可获得1-1/e近似保证，近似Bellman更新保证收敛。实验验证：在真实图数据上，提出的Q-learning方法优于k步前瞻和网络盲方法。

Conclusion: Networked RMAB框架能有效捕捉网络效应，提出的算法具有理论保证且在实际应用中表现优异，证明了考虑网络交互的重要性。

Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.

</details>


### [50] [Theoretical Compression Bounds for Wide Multilayer Perceptrons](https://arxiv.org/abs/2512.06288)
*Houssam El Cheairi,David Gamarnik,Rahul Mazumder*

Main category: cs.LG

TL;DR: 该论文提出了一种随机贪心压缩算法，用于后训练剪枝和量化，并理论证明了多层感知机(MLP)中存在性能有竞争力的剪枝/量化子网络，结果无需数据假设，展示了可压缩性与网络宽度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 剪枝和量化技术在实践中能有效减少大型神经网络参数，但其经验成功的理论依据不足。本文旨在为这些压缩技术的经验成功提供理论支持，弥合理论与应用之间的差距。

Method: 提出了一种随机贪心压缩算法，用于后训练剪枝和量化。该方法类似于Optimal Brain Damage (OBD)的随机后训练版本，并扩展到MLP的结构化剪枝和卷积神经网络(CNN)的剪枝分析。

Result: 理论证明了MLP中存在性能有竞争力的剪枝/量化子网络，结果无需数据假设，展示了可压缩性与网络宽度之间的权衡关系。为宽多层感知机中压缩的经验成功提供了理论依据。

Conclusion: 该研究为剪枝和量化技术的经验成功提供了理论支持，提出的随机贪心压缩算法和理论分析弥合了理论与应用之间的差距，为神经网络压缩提供了统一的理论框架。

Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.

</details>


### [51] [BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination](https://arxiv.org/abs/2512.06457)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: BitStopper：一种无需稀疏预测器的细粒度算法-架构协同设计，通过位级稀疏推测和异步处理，显著提升Transformer加速器的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的大语言模型存在二次计算成本问题，动态稀疏注意力虽然能缓解，但预测阶段和内存访问开销限制了硬件效率，需要更高效的解决方案。

Method: 提出BitStopper框架：1) 位串行使能阶段融合(BESF)，重用内存访问并合并预测与执行阶段；2) 轻量自适应令牌选择(LATS)策略；3) 位级异步处理(BAP)策略；4) 精心设计的硬件架构。

Result: 相比SOTA Transformer加速器，BitStopper在Sanger和SOFA上分别实现2.03倍和1.89倍加速，能效提升2.4倍和2.1倍。

Conclusion: BitStopper通过算法-架构协同设计有效解决了动态稀疏注意力的硬件效率问题，为高效Transformer加速提供了新思路。

Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.

</details>


### [52] [Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media](https://arxiv.org/abs/2512.06293)
*Fatima Ashraf,Muhammad Ayub Sabir,Jiaxin Deng,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于社交媒体的城市交通服务风险监测方法，通过联合建模语言交互和用户影响力，构建影响力加权的关键词共现图，并使用泊松反卷积分解提取可解释的主题结构。


<details>
  <summary>Details</summary>
Motivation: 城市交通机构越来越依赖社交媒体监测服务风险（如拥挤、延误、安全事件），但相关信号稀疏、简短且容易被日常聊天淹没。现有方法难以有效提取这些稀疏但有影响力的风险信号。

Method: 1) 从清洗后的帖子构建影响力加权的关键词共现图；2) 提出泊松反卷积分解(PDF)方法，将图分解为低秩主题结构和主题局部残差交互；3) 使用去相关正则化促进主题区分；4) 通过一致性驱动的扫描选择主题数量。

Result: 在大规模社交数据流上，该模型实现了最先进的主题一致性，与领先基线相比具有强多样性。代码和数据集已公开。

Conclusion: 该方法能有效从稀疏的社交媒体信号中提取城市交通服务风险主题，提供可解释的主题-关键词基础和主题重要性评分，为交通机构提供实用的风险监测工具。

Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git

</details>


### [53] [Decoding Motor Behavior Using Deep Learning and Reservoir Computing](https://arxiv.org/abs/2512.06725)
*Tian Lan*

Main category: cs.LG

TL;DR: 提出ESNNet，结合CNN与回声状态网络，用于EEG运动行为分类，在滑板技巧数据集上表现优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构（如EEGNet和DeepConvNet）能有效捕捉局部空间模式，但难以建模长程时间依赖和非线性动态。需要解决EEG解码中时间动态建模的不足。

Method: 将回声状态网络（ESN）集成到解码流程中，ESN构建高维稀疏连接的循环储备池，擅长跟踪时间动态，与CNN的空间表征能力互补。

Result: 在经PREP管道预处理、MNE-Python实现的滑板技巧EEG数据集上，ESNNet达到83.2%的受试者内准确率和51.3%的留一受试者准确率，超越广泛使用的CNN基线。

Conclusion: ESNNet通过结合CNN的空间表征能力和ESN的时间动态建模能力，有效提升了EEG运动行为分类性能，为无创脑机接口提供了改进的解码方法。

Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals

</details>


### [54] [Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics](https://arxiv.org/abs/2512.06301)
*Jihun Ahn,Gabriella Pasya Irianti,Vikram Thapar,Su-Mi Hur*

Main category: cs.LG

TL;DR: CI-LLM框架结合HAPPY表示法和数值描述符，实现聚合物性质预测和逆向设计，比SMILES模型更快更准


<details>
  <summary>Details</summary>
Motivation: 机器学习已成功应用于无机化合物和小分子材料发现，但聚合物领域由于数据稀缺而进展有限。本文旨在通过创新的分子表示方法克服这一瓶颈。

Method: 提出CI-LLM框架：1) HAPPY表示法将化学亚结构编码为token；2) De$^3$BERTa编码器结合数值描述符进行性质预测；3) GPT-based生成器实现逆向设计

Result: 性质预测：De$^3$BERTa比SMILES模型推理速度快3.5倍，R²分数提升0.9-4.1%；逆向设计：GPT生成器实现100%骨架保留，成功优化负相关多目标性质

Conclusion: 通过战略性的分子表示方法，CI-LLM框架展示了聚合物领域机器学习应用的可行性，实现了高效的性质预测和逆向设计能力

Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.

</details>


### [55] [Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization](https://arxiv.org/abs/2512.06303)
*Preksha Girish,Rachana Mysore,Kiran K. N.,Hiranmayee R.,Shipra Prashanth,Shrey Kumar*

Main category: cs.LG

TL;DR: 提出多模态图神经网络框架，整合结构MRI、扩散张量成像和功能MRI，建模大脑网络的时空重组，生成可解释的生物标志物用于预测认知衰退风险。


<details>
  <summary>Details</summary>
Motivation: 理解大脑网络的动态重组对于预测认知衰退、神经进展和临床结果的个体差异至关重要。现有方法需要新的数据收集，而本研究旨在从现有成像数据中提取临床意义的生物标志物。

Method: 提出多模态图神经网络框架：将大脑区域表示为节点，结构和功能连接表示为边，形成纵向大脑图。使用分数随机微分算子嵌入图循环网络，捕捉长期依赖和随机波动。注意力机制融合多模态信息，生成网络能量熵、图曲率、分数记忆指数等可解释生物标志物，组合成预后指数。

Result: 在纵向神经影像数据集上的实验证明了预测准确性和可解释性。结果突出了数学严谨的多模态图方法从现有成像数据中提取临床意义生物标志物的潜力。

Conclusion: 该框架展示了无需新数据收集，从现有成像数据中提取临床意义生物标志物的可行性，为预测网络不稳定性和认知衰退提供了数学严谨的多模态图方法。

Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.

</details>


### [56] [Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness](https://arxiv.org/abs/2512.06341)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出Interpretive Efficiency，一种量化解释性表示中任务相关信息传输效率的标准化度量，基于五个公理，具有理论保证和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性度量很少能有效量化数据对解释性表示的支持程度，需要一种理论扎实且实用的度量来评估解释性表示的设计质量。

Method: 提出Interpretive Efficiency这一标准化、任务感知的函数，基于五个公理（有界性、Blackwell式单调性、数据处理稳定性、容许不变性、渐近一致性），与互信息相关，推导局部Fisher几何展开，使用标准经验过程工具建立渐近和有限样本估计保证。

Result: 在受控图像和信号任务上的实验表明，该度量能恢复理论排序，揭示被准确率掩盖的表示冗余，并与鲁棒性相关，成为表示设计的实用理论支持诊断工具。

Conclusion: Interpretive Efficiency是一种理论扎实、实用的度量，能够有效评估解释性表示中任务相关信息的传输效率，为表示设计提供有价值的诊断工具。

Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.

</details>


### [57] [When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models](https://arxiv.org/abs/2512.06343)
*Tong Xie,Andrew Bai,Yuanhao Ban,Yunqi Hong,Haoyu Li,Cho-jui Hsieh*

Main category: cs.LG

TL;DR: 本文分析了Bradley-Terry损失函数在奖励建模中的局限性，发现梯度范数受表示距离影响，导致小距离对的更新过弱、大距离对的更新过强。作者提出了NormBT方法，通过自适应归一化平衡表示驱动效应，专注于预测误差。


<details>
  <summary>Details</summary>
Motivation: 标准Bradley-Terry损失函数在奖励建模中存在缺陷：梯度范数不仅受预测误差影响，还受表示距离影响。这导致小距离对的梯度更新过弱（即使排序错误），而大距离对的更新过强，使得精细区分变得困难，影响奖励模型的性能。

Method: 提出了NormBT方法，这是一种自适应成对归一化方案。它通过平衡表示驱动效应，将学习信号聚焦于预测误差。NormBT是BT损失的轻量级即插即用改进，计算开销可忽略。

Result: 在各种LLM骨干网络和数据集上，NormBT一致提升了奖励模型性能。在RewardBench的推理类别中，性能提升超过5%，该类别包含许多小距离对。

Conclusion: 这项工作揭示了广泛使用的BT目标函数的关键局限性，并提供了一种简单有效的修正方法。NormBT通过平衡表示距离效应，使奖励建模更加稳健，特别是在需要精细区分的场景中。

Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.

</details>


### [58] [LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing](https://arxiv.org/abs/2512.06351)
*Zhiying Yang,Fang Liu,Wei Zhang,Xin Lou,Malcolm Yoke Hean Low,Boon Ping Gan*

Main category: cs.LG

TL;DR: LUCA是一个结合大语言模型和图神经网络的强化学习框架，用于碳感知柔性作业车间调度，在降低完工时间和碳排放方面优于现有算法。


<details>
  <summary>Details</summary>
Motivation: 智能制造系统中动态和可持续调度的挑战，需要同时优化完工时间和碳排放，传统方法难以有效处理结构特性和上下文语义的融合。

Method: 集成图神经网络和大语言模型，通过精心设计的提示策略生成融合嵌入，捕捉调度状态的结构特征和上下文语义，然后由深度强化学习策略网络生成实时调度决策，采用双目标奖励函数鼓励能源效率和调度及时性。

Result: 在合成数据集上，相比最佳对比算法平均降低4.1%、最高降低12.2%的完工时间，同时保持相同排放水平；在公共数据集上，完工时间和排放都有额外改善。

Conclusion: LUCA对于智能制造中的碳感知调度是有效且实用的，能够同时优化完工时间和碳排放目标。

Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.

</details>


### [59] [DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction](https://arxiv.org/abs/2512.06356)
*Yifan Song,Fenglin Yu,Yihong Luo,Xingjian Tao,Siya Qiu,Kai Han,Jing Tang*

Main category: cs.LG

TL;DR: DDFI：一种结合特征传播和图掩码自编码器的多样化、分布感知缺失特征填补方法，通过共标签链接增强图连接性，并采用两阶段推理减少分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现实图中节点特征常不完整（如用户隐私属性），导致GNN性能下降。现有特征传播方法存在三个问题：1）对非全连接图效果差，2）填补特征存在过平滑问题，3）仅适用于直推式任务，忽略归纳式任务中的特征分布偏移。

Method: 提出DDFI方法：1）设计共标签链接算法，随机连接训练集中相同标签的节点以增强图连接性；2）采用两阶段推理：先用特征传播填补缺失特征，再通过整个掩码自编码器重构特征以减少分布偏移并增强特征多样性。

Result: 在六个公共数据集和新收集的Sailing数据集（包含自然缺失特征）上实验表明，DDFI在直推式和归纳式设置下均优于现有最优方法。

Conclusion: DDFI通过结合特征传播和图掩码自编码器，有效解决了图缺失特征填补的三个关键问题，并在真实和模拟缺失场景中表现出色。

Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.

</details>


### [60] [Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction](https://arxiv.org/abs/2512.06357)
*Tony Sallooma,Okyay Kaynak,Xinbo Yub,Wei He*

Main category: cs.LG

TL;DR: 提出一种基于PID控制的神经网络预测增强方法，用于提高周期性时间序列多步预测精度，同时保持系统复杂度


<details>
  <summary>Details</summary>
Motivation: 神经网络在多步时间序列预测中应用广泛，但其结构复杂性会影响预测精度。需要一种方法在保持系统复杂度不变的情况下提升预测性能

Method: 受PID控制方法启发，在每个时间步对预测值进行PID校正，使其更接近真实值。该方法作为预测增强器应用于现有神经网络模型

Result: 在用水需求预测和能源消耗预测两个案例中，应用PID增强器后显著提高了预测精度，同时系统复杂度影响可忽略

Conclusion: 提出的PID增强方法能有效提升周期性时间序列多步预测的准确性，且不增加系统复杂度，具有广泛适用性

Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.

</details>


### [61] [RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs](https://arxiv.org/abs/2512.06392)
*Runlong Zhou,Lefan Zhang,Shang-Chen Wu,Kelvin Zou,Hanzhi Zhou,Ke Ye,Yihao Feng,Dong Yin,Alex Guillen Garcia,Dmytro Babych,Rohit Chatterjee,Matthew Hopkins,Xiang Kong,Chang Lan,Lezhi Li,Yiping Ma,Daniele Molinari,Senyu Tong,Yanchao Sun,Thomas Voice,Jianyu Wang,Chong Wang,Simon Wang,Floris Weers,Yechen Xu,Guolin Yin,Muyang Yu,Yi Zhang,Zheng Zhou,Danyang Zhuo,Ruoming Pang,Cheng Leong*

Main category: cs.LG

TL;DR: RLAX是一个在TPU上运行的可扩展强化学习框架，采用参数服务器架构，通过系统优化和数据集管理技术，在12小时内将QwQ-32B模型的pass@8准确率提升了12.8%。


<details>
  <summary>Details</summary>
Motivation: 强化学习已成为提升大语言模型推理能力的主要方法，但需要可扩展且能处理训练中断的框架来支持大规模训练。

Method: 采用参数服务器架构，主训练器定期推送更新的模型权重到参数服务器，推理工作节点拉取最新权重生成新的训练数据；开发了系统技术来支持多种先进RL算法的可扩展和可中断训练；设计了新的数据集管理和对齐技术。

Result: 在1024个v5p TPU上，仅用12小时48分钟就将QwQ-32B模型的pass@8准确率提升了12.8%，同时在训练中断时保持鲁棒性。

Conclusion: RLAX是一个高效、可扩展的强化学习框架，能够显著加速大语言模型的训练收敛并提升模型质量，同时具备处理训练中断的鲁棒性。

Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.

</details>


### [62] [Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator](https://arxiv.org/abs/2512.06417)
*Yifan Sun,Lei Cheng,Jianlong Li,Peter Gerstoft*

Main category: cs.LG

TL;DR: Hankel-FNO：基于傅里叶神经算子的高效水下声学制图方法，结合声传播知识和地形数据，在保持高计算速度的同时实现高精度预测


<details>
  <summary>Details</summary>
Motivation: 传统水下声学制图方法依赖计算昂贵的数值求解器，不适合大规模或实时应用。现有深度学习替代模型存在固定分辨率限制或依赖显式偏微分方程公式等问题，限制了其在不同环境中的适用性和泛化能力

Method: 提出Hankel-FNO模型，基于傅里叶神经算子框架，结合声传播知识和地形数据，实现高效准确的水下声学制图

Result: Hankel-FNO在速度上优于传统求解器，在精度上超越数据驱动替代方法，尤其在长距离预测方面表现突出。实验表明模型能够适应不同环境和声源设置，只需少量微调

Conclusion: Hankel-FNO为水下声学制图提供了一种高效准确的解决方案，结合了物理知识和深度学习优势，具有良好的泛化能力和实际应用前景

Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.

</details>


### [63] [A new initialisation to Control Gradients in Sinusoidal Neural network](https://arxiv.org/abs/2512.06427)
*Andrea Combette,Antoine Venaille,Nelly Pustelnik*

Main category: cs.LG

TL;DR: 提出一种针对SIREN等正弦激活函数网络的新初始化方法，通过控制梯度和预激活分布来改善训练稳定性和泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有初始化策略对梯度爆炸/消失问题的理论理解不足，特别是对于正弦激活函数网络如SIREN，需要更精确的初始化方法来控制梯度并改善泛化

Method: 通过预激活分布收敛和Jacobian序列方差的固定点分析，推导出参数初始化的闭式表达式，控制梯度和预激活分布，防止训练中出现不适当的频率

Result: 新初始化方法在函数拟合和图像重建任务中一致优于原始SIREN和其他基线方法，在物理信息神经网络等重建任务中表现优异

Conclusion: 提出的初始化策略通过精确控制梯度和预激活分布，显著改善了SIREN网络的训练稳定性和泛化性能，并通过NTK框架揭示了其对训练动态的影响

Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.

</details>


### [64] [Neural expressiveness for beyond importance model compression](https://arxiv.org/abs/2512.06440)
*Angelos-Christos Maroudis,Sotirios Xydis*

Main category: cs.LG

TL;DR: 提出基于"表达力"的新剪枝准则，强调神经元有效重分配信息的能力而非权重重要性，实现数据无关的剪枝策略，在参数压缩比上比权重方法提升10倍，性能仅下降1%。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法主要依赖神经元和滤波器权重的"重要性"，但这种方法与学习状态相关。作者希望建立一种与学习状态无关的新剪枝准则，解决"何时剪枝"的问题，并探索数据无关的剪枝策略。

Method: 提出"表达力"剪枝准则，基于激活重叠评估神经元或神经元组有效重分配信息的能力。该准则与网络初始化状态强相关，可通过任意数据或有限代表性样本近似计算。还可与重要性剪枝策略混合使用。

Result: 1) 表达力剪枝比基于权重的方法在参数压缩比上提升10倍，平均性能仅下降1%；2) 独立使用表达力剪枝在压缩效率上优于现有顶级方法；3) 在YOLOv8上实现46.1% MACs减少，55.4%参数移除，COCO数据集上mAP50-95提升3%。

Conclusion: 表达力作为与学习状态无关的新剪枝准则，为数据无关剪枝策略提供了基础，与重要性剪枝互补，显著提升了压缩效率和性能。

Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.

</details>


### [65] [Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control](https://arxiv.org/abs/2512.06471)
*Nathan P. Lawrence,Ali Mesbah*

Main category: cs.LG

TL;DR: 论文分析了基于最优控制的目标条件强化学习，推导了传统二次型目标与目标条件奖励之间的最优性差距，并将状态估计与概率奖励联系起来，验证了目标条件策略在非线性不确定环境中的优势。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习旨在训练智能体最大化达到目标状态的概率，但传统密集奖励（如二次型）在此类任务中可能失效。本文旨在从最优控制角度分析目标条件设置，阐明其成功原因及传统方法的局限性。

Method: 基于最优控制理论，推导传统二次型目标与目标条件奖励之间的最优性差距；在部分可观测马尔可夫决策过程中，将状态估计与概率奖励联系起来；使用强化学习和预测控制技术在非线性和不确定环境中验证目标条件策略的优势。

Result: 阐明了目标条件强化学习成功的原因，揭示了传统密集奖励可能失效的机制；建立了部分可观测环境下状态估计与概率奖励的理论联系；在非线性和不确定环境中验证了目标条件策略相对于传统方法的优势。

Conclusion: 目标条件强化学习为处理目标导向任务提供了有效的理论框架，特别适用于部分可观测和不确定环境，为双重控制问题提供了合适的解决方案。

Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.

</details>


### [66] [Optimizing LLMs Using Quantization for Mobile Execution](https://arxiv.org/abs/2512.06490)
*Agatsya Yadav,Renta Chintala Bhargavi*

Main category: cs.LG

TL;DR: 本文研究了使用4位后训练量化(PTQ)压缩LLM以在移动设备上部署，成功将Llama 3.2 3B模型大小减少68.66%，并在Android设备上实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)虽然功能强大，但其庞大的规模和计算需求阻碍了在资源受限的移动设备上的部署。需要找到压缩LLM的方法以实现移动端执行。

Method: 使用BitsAndBytes库和Hugging Face Transformers框架对Meta的Llama 3.2 3B模型进行4位后训练量化(PTQ)，然后将量化模型转换为GGUF格式，使用llama.cpp工具进行移动端优化推理。

Result: 4位量化实现了68.66%的模型大小减少，量化后的模型能够在Android设备上成功执行推理任务，通过Termux环境和Ollama框架在移动设备上运行。

Conclusion: 4位精度的后训练量化结合GGUF等移动优化格式，为在移动设备上部署能力强大的LLMs提供了实用途径，在模型大小和性能之间取得了良好平衡。

Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.

</details>


### [67] [Diagnosis-based mortality prediction for intensive care unit patients via transfer learning](https://arxiv.org/abs/2512.06511)
*Mengqi Xu,Subha Maity,Joel Dubin*

Main category: cs.LG

TL;DR: 该研究评估了迁移学习在ICU特定诊断死亡率预测中的应用，发现迁移学习优于仅使用诊断特定数据或APACHE IVa评分的模型，且Youden截断值比传统0.5阈值更合适。


<details>
  <summary>Details</summary>
Motivation: ICU危重病的根本原因在不同诊断间差异很大，但考虑诊断异质性的预测模型尚未得到系统研究。现有模型未能充分处理诊断特异性差异。

Method: 使用eICU协作研究数据库，评估基于GLM和XGBoost的迁移学习方法用于诊断特异性死亡率预测。比较迁移学习与仅使用诊断特定数据、APACHE IVa评分以及合并数据训练的模型。

Result: 迁移学习始终优于仅使用诊断特定数据的模型和单独使用APACHE IVa评分的模型，同时比合并数据训练的模型具有更好的校准性。Youden截断值比传统0.5阈值更适合二元结果预测，迁移学习在不同截断标准下均保持高预测性能。

Conclusion: 迁移学习是ICU诊断特异性死亡率预测的有效方法，能处理诊断异质性并提高预测性能。Youden截断值应作为二元结果预测的更合适决策阈值。

Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.

</details>


### [68] [Hierarchical geometric deep learning enables scalable analysis of molecular dynamics](https://arxiv.org/abs/2512.06520)
*Zihan Pengmei,Spencer C. Guo,Chatipat Lorpaiboon,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 提出一种基于图神经网络的分子动力学轨迹分析方法，通过局部信息聚合降低内存和计算需求，实现大规模生物分子系统的快速分析。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟产生原子级轨迹，但缺乏定量描述符时分析困难。传统GNN处理大规模生物分子系统（超过数百残基）时面临长程相互作用捕获困难、内存和运行时需求大的挑战。

Method: 开发基于图神经网络的局部信息聚合方法，通过减少内存和运行时需求而不牺牲原子细节，使大规模蛋白质-核酸复合物（数千残基）能在单GPU上快速分析。

Result: 该方法能在单GPU上几分钟内分析数千残基的蛋白质-核酸复合物模拟。对于数百残基系统，该方法提高了性能和分析可解释性。

Conclusion: 局部信息聚合方法解决了GNN分析大规模生物分子动力学时的计算瓶颈，为复杂系统的快速、可解释分析提供了有效工具。

Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.

</details>


### [69] [Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning](https://arxiv.org/abs/2512.06533)
*Ming Chen,Sheng Tang,Rong-Xi Tan,Ziniu Li,Jiacheng Chen,Ke Xue,Chao Qian*

Main category: cs.LG

TL;DR: 本文提出使用强化学习解决解码式回归中离散token目标与连续数值不对齐的问题，通过序列级奖励提升预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解码式回归将回归任务转化为序列生成，但面临离散token级目标（如交叉熵）与连续数值不对齐的问题。现有基于token级约束的方法难以捕捉目标值的全局幅度，限制了精度和泛化能力。

Method: 提出使用强化学习（RL）解锁解码式回归的潜力。将生成过程建模为马尔可夫决策过程，利用序列级奖励来强制全局数值一致性。具体采用ReMax和GRPO方法。

Result: 在表格回归和代码度量回归上的大量实验表明，该方法（特别是ReMax和GRPO）持续优于最先进的token级基线和传统回归头，显示了引入序列级信号的优势。分析进一步揭示RL显著提高了采样效率和预测精度。

Conclusion: 强化学习使解码式回归成为通用数值预测的鲁棒且准确的范式，通过序列级奖励解决了离散-连续不对齐问题，提升了全局数值一致性。

Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.

</details>


### [70] [A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation](https://arxiv.org/abs/2512.06547)
*Xiaocan Li,Shiliang Wu,Zheng Shen*

Main category: cs.LG

TL;DR: A-3PO通过近似计算近端策略，消除了异步强化学习中解耦损失算法的额外计算开销，减少18%训练时间，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 在异步强化学习中，解耦损失算法通过引入近端策略来提高学习稳定性，但近端策略需要在每个训练步骤中进行额外的前向传播，对于大语言模型造成计算瓶颈。

Method: 提出A-3PO方法，通过简单插值近似近端策略，避免显式计算。近端策略仅作为行为策略和目标策略之间的信任区域锚点，可以通过插值有效近似。

Result: A-3PO消除了计算开销，减少18%的训练时间，同时保持与原始解耦损失算法相当的性能。

Conclusion: A-3PO通过近似近端策略，有效解决了异步强化学习中解耦损失算法的计算瓶颈问题，在保持性能的同时显著提升训练效率。

Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md

</details>


### [71] [Deep Manifold Part 2: Neural Network Mathematics](https://arxiv.org/abs/2512.06563)
*Max Y. Ma,Gen-Hua Shi*

Main category: cs.LG

TL;DR: 该论文提出从流形几何、不动点理论和边界条件迭代的角度重新理解神经网络，认为神经网络本质上是可学习的数值计算，其能力源于不动点区域的稳定化，而非初始就具备固定结构。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络分析通常基于固定坐标和算子，但现实世界数据具有高度复杂性、近乎无限的规模和范围，以及小批量碎片化。训练动态产生学习复杂性，包括节点覆盖变化、曲率积累和可塑性兴衰。这些因素限制了可学习性，需要新的理论框架来解释神经网络的能力涌现。

Method: 通过堆叠分段流形、不动点理论和边界条件迭代建立神经网络的全局方程。移除固定坐标和算子，将神经网络视为由流形复杂性、高阶非线性和边界条件塑造的可学习数值计算。分析残差驱动迭代如何构建不动点区域。

Result: 神经网络的能力只有在不动点区域稳定时才会涌现，而非初始就具备固定点。这种视角揭示了单体模型在几何和数据诱导可塑性下的局限性，并解释了为什么需要分布式架构。

Conclusion: 该框架为理解神经网络提供了新的几何、代数和不动点理论基础，并激励设计能够将流形复杂性分布到多个弹性模型中的架构和联邦系统，形成基于几何、代数、不动点和真实数据复杂性的连贯世界建模框架。

Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed-point theory, and boundary-conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high-order nonlinearity, and boundary conditions. Real-world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed-point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual-driven iteration. This perspective clarifies the limits of monolithic models under geometric and data-induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world-modeling framework grounded in geometry, algebra, fixed points, and real-data complexity.

</details>


### [72] [QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling](https://arxiv.org/abs/2512.06582)
*Isaac Kofi Nti*

Main category: cs.LG

TL;DR: QL-LSTM通过参数共享统一门控机制减少48%参数，并通过层次门控递归与加法跳跃连接改善长距离信息流，在IMDB情感分类任务中达到竞争性准确率但尚未实现实际速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决传统LSTM和GRU的两个核心限制：冗余的门控参数和长距离信息保留能力不足。

Method: 提出QL-LSTM，包含两个独立组件：1) 参数共享统一门控机制，用单一共享权重矩阵替代所有门控特定变换；2) 层次门控递归与加法跳跃连接，添加无乘法路径改善长距离信息流。

Result: 在IMDB数据集的情感分类任务中，QL-LSTM达到竞争性准确率，同时参数减少约48%，但受限于循环模型的固有顺序特性，尚未实现实际速度提升。

Conclusion: QL-LSTM有效解决了参数冗余和长距离依赖问题，但需要进一步的核级优化才能在实际应用中实现速度提升。

Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.

</details>


### [73] [On fine-tuning Boltz-2 for protein-protein affinity prediction](https://arxiv.org/abs/2512.06592)
*James King,Lewis Cornwall,Andrei Cristian Nica,James Day,Aaron Sim,Neil Dalchau,Lilly Wollman,Joshua Meyers*

Main category: cs.LG

TL;DR: 将Boltz-2蛋白-配体亲和力预测器适配到蛋白-蛋白亲和力预测，发现结构模型表现不如序列模型，但两者结合能互补提升


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白-蛋白结合亲和力对于理解分子相互作用和设计治疗方法至关重要。现有结构基方法在蛋白-配体预测中表现优异，但尚未充分探索其在蛋白-蛋白亲和力预测中的应用

Method: 将最先进的结构基蛋白-配体亲和力预测器Boltz-2适配为蛋白-蛋白亲和力回归模型(Boltz-2-PPI)，在TCR3d和PPB-affinity两个数据集上进行评估，并与序列基方法比较。还尝试将Boltz-2-PPI的嵌入与序列基嵌入结合

Result: 尽管结构准确性高，但Boltz-2-PPI在小规模和较大规模数据情况下都表现不如序列基替代方法。将Boltz-2-PPI嵌入与序列基嵌入结合能产生互补性改进，特别是对于较弱的序列模型，表明序列基和结构基模型学习了不同的信号

Conclusion: 结果反映了与结构数据训练相关的已知偏差，表明当前的结构基表示方法尚未准备好进行高性能的亲和力预测。序列和结构方法的结合显示出互补优势，为未来改进提供了方向

Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.

</details>


### [74] [A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs](https://arxiv.org/abs/2512.06607)
*Humzah Merchant,Bradford Levy*

Main category: cs.LG

TL;DR: 提出一种通过调整大型基础模型logits来消除前瞻性偏差的方法，使用一对小型专业模型分别微调于需要遗忘和保留的信息，适用于金融预测任务。


<details>
  <summary>Details</summary>
Motivation: 将LLMs应用于金融预测任务时面临前瞻性偏差的挑战，因为LLMs训练于长时间序列数据。重新训练前沿模型以设定特定知识截止点成本过高，阻碍了金融领域常用的回测方法。

Method: 在推理时通过调整大型基础模型的logits来引导生成，使用一对小型专业模型：一个微调于需要遗忘的信息，另一个微调于需要保留的信息。这种方法快速、有效且成本低。

Result: 该方法能有效消除字面和语义知识，纠正偏差，并且在性能上优于先前的方法。

Conclusion: 提出了一种解决LLMs在金融预测中前瞻性偏差问题的实用方法，通过推理时调整logits而非重新训练，为金融应用提供了可行的解决方案。

Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.

</details>


### [75] [Vector Quantization using Gaussian Variational Autoencoder](https://arxiv.org/abs/2512.06609)
*Tongda Xu,Wendi Zheng,Jiajun He,Jose Miguel Hernandez-Lobato,Yan Wang,Ya-Qin Zhang,Jie Tang*

Main category: cs.LG

TL;DR: 提出Gaussian Quant方法，将带约束的高斯VAE转换为VQ-VAE，无需额外训练，通过理论保证和TDC启发式训练提升性能


<details>
  <summary>Details</summary>
Motivation: VQ-VAE由于离散化难以训练，需要一种更简单有效的方法来实现离散编码

Method: 提出Gaussian Quant技术：1）生成随机高斯噪声作为码本；2）找到与后验均值最接近的噪声；3）提出目标散度约束（TDC）启发式训练方法

Result: GQ在UNet和ViT架构上优于VQGAN、FSQ、LFQ、BSQ等现有VQ-VAE方法，TDC也改进了TokenBridge等高斯VAE离散化方法

Conclusion: Gaussian Quant提供了一种简单有效的VQ-VAE实现方法，通过理论保证和实用启发式训练，在多个基准上取得了优越性能

Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.

</details>


### [76] [Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study](https://arxiv.org/abs/2512.06630)
*Chi-Sheng Chen,Xinyu Zhang,Rong Fu,Qiuzhe Xie,Fan Zhang*

Main category: cs.LG

TL;DR: 量子时间卷积神经网络（QTCNN）结合经典时间编码器和参数高效的量子卷积电路，用于横截面股票收益预测，在JPX东京证券交易所数据集上实现了0.538的夏普比率，比最佳经典基准高出约72%。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型在处理噪声输入、制度转换和有限泛化能力方面存在困难，特别是在复杂、嘈杂且高度动态的金融环境中。量子机器学习为增强股票市场预测提供了有前景的途径。

Method: 提出量子时间卷积神经网络（QTCNN），结合经典时间编码器（提取顺序技术指标的多尺度模式）和参数高效的量子卷积电路（利用量子叠加和纠缠增强特征表示并抑制过拟合）。

Result: 在JPX东京证券交易所数据集上进行基准测试，通过构建多空组合并使用样本外夏普比率作为主要性能指标进行评估。QTCNN实现了0.538的夏普比率，比最佳经典基准高出约72%。

Conclusion: 量子增强的预测模型QTCNN在量化金融中具有稳健决策的实际潜力，能够有效处理金融环境中的噪声和动态变化。

Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.

</details>


### [77] [The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News](https://arxiv.org/abs/2512.06638)
*Isha Karn,David Jensen*

Main category: cs.LG

TL;DR: 现有假新闻检测基准数据集（GossipCop和PolitiFact）的图结构过于简单，无法有效评估GNN模型的结构建模能力，导致GNN与简单MLP性能相近。


<details>
  <summary>Details</summary>
Motivation: 研究发现当前广泛使用的假新闻检测基准数据集存在严重缺陷，其图拓扑结构过于简单（浅层、自我中心式），无法有效评估图神经网络（GNN）在建模传播结构方面的实际效用。

Method: 1) 系统评估5种GNN架构与使用相同节点特征的结构无关MLP的性能对比；2) 通过特征洗牌和边结构随机化的控制实验分离结构和特征的贡献；3) 对数据集进行结构分析；4) 在合成数据集上验证GNN的优势。

Result: 1) MLP性能与GNN相当或接近（差距1-2%，置信区间重叠）；2) 特征洗牌导致性能崩溃，但边随机化后性能保持稳定，表明结构贡献可忽略；3) 超过75%的节点距离根节点仅一跳，结构多样性极低；4) 在特征噪声大、结构信息丰富的合成数据集上，GNN显著优于MLP。

Conclusion: 当前广泛使用的假新闻检测基准数据集无法有效测试结构建模方法的实际价值，需要开发具有更丰富、更多样化图拓扑结构的数据集来推动该领域的发展。

Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.

</details>


### [78] [Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network](https://arxiv.org/abs/2512.06648)
*Xiao Li*

Main category: cs.LG

TL;DR: 提出基于卷积神经网络的中国A股上市公司财务舞弊检测框架，将面板数据转换为类图像表示，实现提前预测并优于传统方法


<details>
  <summary>Details</summary>
Motivation: 上市公司财务舞弊难以检测，传统统计模型无法处理非线性特征交互，机器学习模型缺乏可解释性，现有方法仅基于当年数据判断当年舞弊，时效性不足

Method: 设计特征工程方案将公司年度面板数据转换为类图像表示，使用CNN捕捉横截面和时间模式，实现提前预测；采用局部解释技术从实体、特征和时间三个维度分析模型

Result: CNN在准确性、鲁棒性和预警性能上优于逻辑回归和LightGBM；偿债能力、比率结构、治理结构和内部控制是舞弊的通用预测因子；舞弊公司表现出集中在短期窗口的异质模式

Conclusion: CNN框架能有效检测财务舞弊并实现提前预警，通过可解释性分析发现舞弊公司的特征模式具有异质性和时间集中性，为监管提供实用工具

Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.

</details>


### [79] [Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning](https://arxiv.org/abs/2512.06649)
*Camellia Zakaria,Aryan Sadeghi,Weaam Jaafar,Junshi Xu,Alex Mariakakis,Marianne Hatzopoulou*

Main category: cs.LG

TL;DR: 利用交通监控视频和天气数据，通过机器学习模型估计街道级黑碳浓度，填补交通与环境监测之间的数据鸿沟。


<details>
  <summary>Details</summary>
Motivation: 城市黑碳排放主要来自交通，但监测成本高导致数据缺乏，而交通监控系统却广泛部署。这种不平衡使得我们了解交通状况却不了解其环境影响，需要填补这一数据鸿沟。

Method: 提出机器学习驱动系统，从交通视频中提取车辆行为和状况的视觉信息，结合天气数据，建立模型估计街道级黑碳浓度。

Result: 模型达到R平方值0.72，RMSE为129.42 ng/m³，能够有效估计街道级黑碳浓度。

Conclusion: 该工作利用现有城市基础设施资源，生成与交通排放相关的信息，为污染减排、城市规划、公共卫生和环境正义提供可操作的见解。

Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.

</details>


### [80] [Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts](https://arxiv.org/abs/2512.06652)
*Xiaolei Lu,Shamim Nemati*

Main category: cs.LG

TL;DR: 本文提出AdaTTT框架，通过自监督学习和原型学习增强测试时训练，改善ICU患者有创机械通气预测模型在跨机构部署时的泛化性能。


<details>
  <summary>Details</summary>
Motivation: ICU患者有创机械通气预测模型在跨机构部署时，由于患者群体、临床实践和电子健康记录系统的差异导致域偏移，降低模型泛化性能。需要无需目标域标注数据的自适应方法。

Method: 提出自适应测试时训练框架AdaTTT：1) 基于信息论推导测试时预测误差界限；2) 引入自监督学习框架，包含重构和掩码特征建模任务，采用动态掩码策略强调主任务关键特征；3) 结合原型学习和部分最优传输进行灵活的部分特征对齐，保持临床有意义的患者表示。

Result: 在多中心ICU队列实验中，在不同测试时适应基准上展示了有竞争力的分类性能。

Conclusion: AdaTTT框架通过增强主任务与辅助任务的对齐，以及灵活的域适应机制，有效改善了ICU有创机械通气预测模型在跨机构部署时的泛化能力。

Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.

</details>


### [81] [GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering](https://arxiv.org/abs/2512.06655)
*Jehyeok Yeon,Federico Cinus,Yifan Wu,Luca Luceri*

Main category: cs.LG

TL;DR: GSAE（图正则化稀疏自编码器）通过Laplacian平滑惩罚在神经元共激活图上，学习分布式安全表示，实现运行时安全控制，显著提升有害内容拒绝率同时保持良性查询的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法存在局限：黑盒护栏只能过滤输出，而基于内部表示的方法通常将安全概念简化为单一潜在特征或维度。但研究表明，抽象概念（如拒绝、时间性）实际上是分布在多个特征中的，而非孤立于单一特征。

Method: 提出图正则化稀疏自编码器（GSAE），在标准SAE基础上增加Laplacian平滑惩罚项，作用于神经元共激活图。GSAE学习平滑的分布式安全表示，将特征组装成加权安全相关方向集，采用两阶段门控机制：仅在检测到有害提示或生成内容时激活干预。

Result: GSAE引导实现平均82%的选择性拒绝率，显著优于标准SAE引导（42%），同时在TriviaQA（70%）、TruthfulQA（65%）、GSM8K（74%）上保持强任务准确性。在LLaMA-3、Mistral、Qwen、Phi等模型家族上具有良好泛化性，对GCG、AutoDAN等越狱攻击保持≥90%的有害内容拒绝率。

Conclusion: GSAE通过分布式安全表示学习，有效解决了现有安全引导方法的局限性，实现了自适应拒绝有害内容同时保持良性查询实用性的平衡，为LLM安全防御提供了更鲁棒的解决方案。

Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.

</details>


### [82] [Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods](https://arxiv.org/abs/2512.06665)
*Panagiota Kiourti,Anu Singh,Preeti Duraipandian,Weichao Zhou,Wenchao Li*

Main category: cs.LG

TL;DR: 该论文研究了深度神经网络特征归因方法的鲁棒性，提出了新的相似输入定义、鲁棒性度量指标和基于生成对抗网络的输入生成方法，挑战了当前忽略模型输出差异的归因鲁棒性概念。


<details>
  <summary>Details</summary>
Motivation: 当前的特征归因鲁棒性评估方法存在缺陷，它们主要关注输入扰动下的归因变化，却忽略了模型输出本身的差异。这导致评估不够客观，无法准确反映归因方法本身的弱点。

Method: 提出了三个核心方法：1) 新的相似输入定义；2) 新的鲁棒性度量指标；3) 基于生成对抗网络生成这些相似输入的方法。同时使用现有指标和最先进的归因方法进行了全面评估。

Result: 研究发现需要更客观的度量指标来揭示归因方法本身的弱点，而不是神经网络的弱点。提出的新方法能够更准确地评估归因方法的鲁棒性。

Conclusion: 论文挑战了当前归因鲁棒性的概念，提出了更客观的评估框架，强调需要关注归因方法本身的弱点，为特征归因方法的鲁棒性评估提供了新方向。

Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.

</details>


### [83] [The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification](https://arxiv.org/abs/2512.06666)
*Urav Maniar*

Main category: cs.LG

TL;DR: 研究时间序列分类中精度与效率的权衡，探索两种高效算法组合能否在保持计算可行性的同时获得集成学习效益，发现当前元学习策略难以有效利用算法互补性。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类面临精度与计算效率的根本权衡。虽然像HIVE-COTE 2.0这样的综合集成方法能达到最先进的精度，但其在UCR基准测试上340小时的训练时间使其在大规模数据集上不实用。研究旨在探索两种来自互补范式的高效算法组合能否在保持计算可行性的同时获得集成学习效益。

Method: 结合Hydra（竞争卷积核）和Quant（分层区间分位数）两种高效算法，在六种集成配置下评估性能。在10个大规模MONSTER数据集（7,898到1,168,774个训练实例）上进行评估，分析预测组合集成和特征连接方法的效果。

Result: 最强配置将平均精度从0.829提升到0.836，在10个数据集中的7个上取得成功。但预测组合集成仅捕获了11%的理论oracle潜力，显示出显著的元学习优化差距。特征连接方法通过学习新的决策边界超过了oracle界限，预测级互补性与集成增益呈中等相关性。

Conclusion: 核心发现：挑战已从确保算法不同转变为学习如何有效组合它们。当前元学习策略难以利用oracle分析确认存在的互补性。改进的组合策略可能使集成增益在多样化时间序列分类应用中翻倍或三倍增长。

Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.

</details>


### [84] [GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning](https://arxiv.org/abs/2512.06678)
*Shrihari Sridharan,Deepak Ravikumar,Anand Raghunathan,Kaushik Roy*

Main category: cs.LG

TL;DR: GradientSpace：一个通过在全维梯度空间聚类样本来缓解指令调优中梯度干扰的框架，使用在线SVD算法识别潜在技能，训练专门的LoRA专家和轻量级路由器，在推理时选择最佳专家。


<details>
  <summary>Details</summary>
Motivation: 真实世界数据集通常异构，包含多样信息，导致梯度干扰（冲突梯度将模型拉向相反方向），降低性能。现有方法基于语义或嵌入相似性分组，但未能捕捉数据如何影响模型参数学习；而直接聚类梯度的方法使用随机投影降维导致精度损失，且依赖专家集成需要多次推理传递和昂贵的实时梯度计算。

Method: 提出GradientSpace框架，在全维梯度空间直接聚类样本。引入基于在线SVD的算法，在LoRA梯度上操作，识别潜在技能，避免存储所有样本梯度的不可行成本。每个聚类用于训练专门的LoRA专家，同时训练轻量级路由器在推理时选择最佳专家。

Result: 在数学推理、代码生成、金融和创意写作任务上的实验表明，GradientSpace导致一致的专家专业化，并在准确率上持续优于最先进的聚类方法和微调技术。路由到单个适当专家优于先前工作中的专家集成，同时显著降低推理延迟。

Conclusion: GradientSpace通过在全维梯度空间聚类有效缓解了指令调优中的梯度干扰问题，实现了更好的专家专业化和性能提升，同时减少了推理成本。

Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.

</details>


### [85] [State Diversity Matters in Offline Behavior Distillation](https://arxiv.org/abs/2512.06692)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文发现离线行为蒸馏(OBD)中存在原始数据集与蒸馏数据集的对齐问题，提出状态多样性在训练损失较大时比状态质量更重要，并设计了状态密度加权(SDW)算法来提升蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 离线行为蒸馏(OBD)能将大量离线RL数据压缩成紧凑的合成行为数据集，但研究发现高质量原始数据集不一定能产生优质合成数据集，存在对齐问题。需要理解这种不一致的原因并改进OBD方法。

Method: 通过实证分析不同训练损失下策略性能，发现状态多样性在损失大时更重要；理论分析将状态质量和多样性分别与关键误差和周围误差关联；提出状态密度加权(SDW)算法，通过状态密度倒数加权蒸馏目标来增强状态多样性。

Result: 实验表明：1）训练损失大时，状态多样性高的数据集表现更好；2）SDW算法在原始数据集状态多样性有限时能显著提升OBD性能；3）在多个D4RL数据集上验证了SDW的有效性。

Conclusion: 状态多样性在离线行为蒸馏中至关重要，特别是在训练损失较大时。提出的SDW算法通过强调状态多样性，有效解决了原始与蒸馏数据集的对齐问题，提升了OBD性能。

Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.

</details>


### [86] [Mitigating Barren plateaus in quantum denoising diffusion probabilistic models](https://arxiv.org/abs/2512.06695)
*Haipeng Cao,Kaining Zhang,Dacheng Tao,Zhaofeng Su*

Main category: cs.LG

TL;DR: 量子去噪扩散概率模型(QuDDPM)存在贫瘠高原问题，本文提出改进方案通过使用远离Haar分布的输入分布来解决该问题，提高了模型可训练性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 量子生成模型利用量子叠加和纠缠来提升学习效率，QuDDPM作为量子生成学习的有前景框架，在多种量子数据学习任务中表现出色。然而，研究发现QuDDPM由于在去噪过程中使用2-design状态作为输入，导致了贫瘠高原问题，严重影响了模型性能。

Method: 通过理论分析和实验验证确认原始QuDDPM存在贫瘠高原问题。为解决此问题，提出改进的QuDDPM，使用与Haar分布保持一定距离的分布作为输入，确保更好的可训练性。

Result: 实验结果表明，改进方法有效缓解了贫瘠高原问题，生成了更高质量的样本，为可扩展和高效的量子生成学习铺平了道路。

Conclusion: 本文揭示了QuDDPM中存在的贫瘠高原问题，并提出了一种有效的解决方案，通过调整输入分布来改善模型的可训练性，推动了量子生成学习的发展。

Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.

</details>


### [87] [Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models](https://arxiv.org/abs/2512.06702)
*Xiangjun Meng,Zhongjian Wang*

Main category: cs.LG

TL;DR: 论文提出了分析工具来估计流式生成模型在Wasserstein度量下的误差，并建立了最优采样迭代复杂度界限为O(√d)。误差由两部分控制：反向流推前映射的Lipschitz性（与维度无关）和局部离散化误差（与维度呈O(√d)关系）。


<details>
  <summary>Details</summary>
Motivation: 为流式生成模型提供可实现的误差估计工具，特别是在高维情况下，理解采样迭代复杂度如何随维度增长，这对于实际应用中的计算效率至关重要。

Method: 使用Wasserstein度量分析流式生成模型的误差，将误差分解为两部分：反向流推前映射的Lipschitz性（与维度无关）和局部离散化误差（与维度呈O(√d)关系）。这些分析适用于与Föllmer过程和1-整流流相关的流式生成模型。

Result: 证明了采样迭代复杂度以O(√d)的速率增长，与协方差算子迹的平方根成线性关系。在Föllmer过程和1-整流流模型下，这些假设在满足高斯尾假设时成立。

Conclusion: 流式生成模型的采样误差可以通过两个维度依赖特性明确控制，其中局部离散化误差主导了维度缩放行为。这为理解高维流式生成模型的收敛性提供了理论保证。

Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the Föllmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.

</details>


### [88] [A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations](https://arxiv.org/abs/2512.06708)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出了一种用于滚动轴承剩余使用寿命预测的多模态RUL框架，结合图像表示和时间频率表示，通过注意力机制和可解释性技术提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有RUL估计方法存在泛化能力差、鲁棒性不足、数据需求高和可解释性有限等问题，特别是滚动轴承作为机械故障的常见原因，需要更可靠的预测方法。

Method: 提出多模态RUL框架，包含三个分支：1) 图像表示分支，使用Bresenham线算法将振动信号转换为图像；2) 时间频率表示分支，使用连续小波变换；3) 融合分支，通过残差连接的多层扩张卷积提取空间特征，LSTM建模时间模式，多头注意力机制突出关键特征，最后线性层回归RUL。还引入了多模态层相关传播技术增强可解释性。

Result: 在XJTU-SY和PRONOSTIA基准数据集上验证，性能达到或超越现有最优方法，在未见工况下表现良好，训练数据需求减少28%-48%，具有强噪声鲁棒性，可解释性可视化确认预测可信度。

Conclusion: 该多模态框架通过结合图像和时间频率表示，配合可解释性技术，实现了高性能、低数据需求、强鲁棒性和良好可解释性的RUL预测，适合工业实际部署。

Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.

</details>


### [89] [A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting](https://arxiv.org/abs/2512.06714)
*Tony Salloom,Okyay Kaynak,Wei He*

Main category: cs.LG

TL;DR: 提出一种结合虚拟数据扩展和GRU-K-means混合模型的短期用水需求预测方法，显著降低极端点误差和模型复杂度


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在短期用水需求预测中存在两个主要问题：1）模型参数过多导致复杂度高；2）在极端点（极值点）预测误差较大。需要一种既能降低复杂度又能提高极端点预测精度的方法。

Method: 提出一种混合深度学习模型：1）使用GRU处理历史用水数据的时序关系；2）引入K-means无监督分类方法创建新特征以减少参数数量；3）提出虚拟数据扩展方法，在实际数据中插入虚拟数据以缓解极端点周围的非线性问题。

Result: 1）模型复杂度降低至文献中方法的六分之一，同时保持相同精度；2）数据集扩展使预测误差降低约30%；3）使用中国两个不同水厂的实际数据进行验证，证明方法有效性；4）虚拟数据扩展增加了训练时间。

Conclusion: 提出的GRU-K-means混合模型结合虚拟数据扩展方法，在保持预测精度的同时显著降低了模型复杂度，并有效解决了极端点预测误差问题，为短期用水需求预测提供了更高效的解决方案。

Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.

</details>


### [90] [KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models](https://arxiv.org/abs/2512.06727)
*Sourjya Roy,Shrihari Sridharan,Surya Selvam,Anand Raghunathan*

Main category: cs.LG

TL;DR: KV CAR：一个统一的KV缓存压缩框架，通过轻量级自编码器和相似性驱动的重用机制，在保持模型性能的同时显著减少KV缓存内存占用，实现高达47.85%的内存减少。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模和上下文长度的增加，KV缓存在自回归解码过程中的内存需求已成为主要瓶颈。KV缓存随序列长度和嵌入维度增长，常常超过模型本身的内存占用，限制了可实现的批处理大小和上下文窗口。

Method: KV CAR结合两种互补技术：1）轻量级自编码器沿嵌入维度学习键值张量的紧凑表示，在存储到KV缓存前压缩并在检索时恢复；2）相似性驱动的重用机制识别跨相邻层重用特定注意力头KV张量的机会。这两种方法无需改变Transformer架构即可减少KV张量的维度和结构冗余。

Result: 在GPT-2和TinyLLaMA模型上，使用Wikitext、C4、PIQA和Winogrande数据集评估，KV CAR实现了高达47.85%的KV缓存内存减少，同时对困惑度和零样本准确率影响最小。在NVIDIA A40 GPU上的系统级测量显示，减少的KV占用直接转化为推理过程中更长的序列长度和更大的批处理大小。

Conclusion: KV CAR框架有效实现了内存高效的大语言模型推理，通过压缩KV缓存显著减少了内存需求，同时保持了模型性能，为大规模语言模型部署提供了实用的解决方案。

Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.

</details>


### [91] [Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data](https://arxiv.org/abs/2512.06730)
*Lin Yang,Xiang Li,Xin Ma,Xinxin Zhao*

Main category: cs.LG

TL;DR: 提出基于增强现实的SSVEP脑机接口系统，结合多注意力机制的CNN-BiLSTM模型，用于提高运动功能障碍患者的康复训练参与度和运动意图识别


<details>
  <summary>Details</summary>
Motivation: 解决运动功能障碍患者康复训练参与度低的问题，传统SSVEP-BCI系统依赖外部视觉刺激设备，在实际应用中受限，且治疗师工作负担重

Method: 1) 设计基于HoloLens 2的四种EEG类别并采集数据；2) 在传统CNN-BiLSTM架构基础上集成多头注意力机制(MACNN-BiLSTM)；3) 提取10个时频EEG特征，通过CNN学习高级表示，BiLSTM建模序列依赖，多头注意力突出运动意图相关模式；4) 使用SHAP方法可视化EEG特征对决策的贡献

Result: 该方法提高了实时运动意图识别能力，支持运动障碍患者的康复恢复，增强了模型的可解释性

Conclusion: AR-SSVEP系统结合MACNN-BiLSTM模型能有效解决患者主动性不足和治疗师工作负担重的问题，为运动功能障碍康复提供了更实用的解决方案

Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.

</details>


### [92] [Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics](https://arxiv.org/abs/2512.06737)
*Nikhil Verma,Joonas Linnosmaa,Espinosa-Leal Leonardo,Napat Vajragupta*

Main category: cs.LG

TL;DR: ArcGD优化器在非凸基准函数和真实ML数据集上均表现优异，超越Adam等主流优化器，具有更好的泛化能力和抗过拟合特性。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的优化器（ArcGD），旨在解决现有优化器（如Adam）在非凸优化问题和深度学习任务中可能存在的收敛问题和过拟合风险。

Method: 1. 在非凸Rosenbrock函数上进行评估，对比ArcGD与Adam，消除学习率偏差；2. 在CIFAR-10数据集上评估ArcGD与Adam、AdamW、Lion、SGD等优化器，使用8种不同MLP架构。

Result: 1. 在Rosenbrock函数上，ArcGD在使用自身有效学习率时始终优于Adam；2. 在CIFAR-10上，ArcGD在20,000次迭代后获得最高平均测试准确率（50.7%），在8种架构中6种获胜或持平，且持续改进而不像Adam/AdamW出现过拟合。

Conclusion: ArcGD优化器在非凸优化和深度学习任务中表现出色，具有更好的泛化能力和抗过拟合特性，且与Lion优化器存在理论联系，值得进一步探索其广泛应用潜力。

Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.

</details>


### [93] [Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets](https://arxiv.org/abs/2512.06752)
*Chang Liu,Vivian Li,Linus Leong,Vladimir Radenkovic,Pietro Liò,Chaitanya K. Joshi*

Main category: cs.LG

TL;DR: 提出几何图U-Nets，通过递归粗化和细化蛋白质图来学习多尺度表示，在蛋白质折叠分类任务上优于现有几何GNN和Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 现有几何GNN和Transformer依赖消息传递，无法捕捉蛋白质功能中的层次相互作用（如全局结构域和长程变构调节），需要网络架构本身反映这种生物层次结构。

Method: 引入几何图U-Nets，通过递归粗化和细化蛋白质图来学习多尺度表示，采用U-Net架构设计，理论上比标准几何GNN更具表达力。

Result: 在蛋白质折叠分类任务上，几何U-Nets显著优于不变和等变基线模型，证明其能够学习定义蛋白质折叠的全局结构模式。

Conclusion: 为设计能够学习生物分子多尺度结构的几何深度学习架构提供了理论基础，展示了层次化设计在几何深度学习中的重要性。

Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.

</details>


### [94] [Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship](https://arxiv.org/abs/2512.06758)
*Zilong Wang,Shuai Li*

Main category: cs.LG

TL;DR: 本文提出了一种多级连续选择算法，在序列独裁匹配市场中实现了与下界匹配的遗憾上界O(Nlog(T)/Δ² + Klog(T)/Δ)，填补了现有上下界之间的差距。


<details>
  <summary>Details</summary>
Motivation: 在线匹配市场中，参与者通过多轮交互学习未知偏好。现有研究存在遗憾上下界之间的差距（从N到K），不清楚是下界需要改进还是上界需要改进。本文旨在解决这一理论差距。

Method: 提出多级连续选择算法，在满足序列独裁假设的市场中工作。该算法通过多级选择机制有效探索和利用，匹配了已知的下界。

Result: 算法获得了O(Nlog(T)/Δ² + Klog(T)/Δ)的遗憾上界，与Sankararaman等人提出的下界完全匹配。这是首次在匹配市场与赌博机问题中提出匹配下界的算法。

Conclusion: 本文填补了在线匹配市场中遗憾上下界的理论差距，证明了现有下界的紧致性，并为序列独裁市场提供了最优算法。

Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $Ω\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $Δ$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{Δ^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{Δ^2} + \frac{K\log(T)}Δ \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.

</details>


### [95] [Measuring Over-smoothing beyond Dirichlet energy](https://arxiv.org/abs/2512.06782)
*Weiqi Guan,Zihao Shi*

Main category: cs.LG

TL;DR: 提出基于高阶特征导数的节点相似性度量家族，分析过平滑衰减率与图拉普拉斯谱隙的内在联系，发现注意力GNN在这些度量下存在过平滑问题


<details>
  <summary>Details</summary>
Motivation: Dirichlet能量作为量化过平滑的主流度量，仅能捕捉一阶特征导数，存在局限性。需要更全面的度量来评估图神经网络中的过平滑现象

Method: 提出基于高阶特征导数能量的广义节点相似性度量家族，通过理论分析建立这些度量之间的关系，并推导连续热扩散和离散聚合算子下Dirichlet能量的衰减率

Result: 理论分析揭示了过平滑衰减率与图拉普拉斯谱隙的内在联系，实证结果表明注意力GNN在提出的高阶度量下确实存在过平滑问题

Conclusion: 提出的高阶特征导数能量度量能更全面地评估过平滑现象，揭示了过平滑与图结构谱特性之间的深层联系，为理解和缓解GNN过平滑提供了新视角

Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.

</details>


### [96] [Angular Regularization for Positive-Unlabeled Learning on the Hypersphere](https://arxiv.org/abs/2512.06785)
*Vasileios Sevetlidis,George Pavlidis,Antonios Gasteratos*

Main category: cs.LG

TL;DR: AngularPU：基于角度相似度和超球面表示的新型PU学习方法，通过可学习原型向量和角度正则化解决传统PU方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法要么依赖强分布假设，要么在高维设置中容易崩溃，需要一种更稳健且不需要显式负类建模的方法

Method: 在单位超球面上使用余弦相似度和角度间隔，正类由可学习原型向量表示，分类简化为阈值化嵌入与原型之间的余弦相似度，引入角度正则化器鼓励未标记集在超球面上分散

Result: 在基准数据集上达到或优于最先进的PU方法，特别是在正样本稀缺和高维嵌入设置中，同时提供几何可解释性和可扩展性

Conclusion: AngularPU通过角度决策规则和原型学习提供了一种理论保证的PU学习框架，避免了显式负类建模，在高维场景中表现优异

Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.

</details>


### [97] [Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games](https://arxiv.org/abs/2512.06791)
*Vedansh Sharma*

Main category: cs.LG

TL;DR: 提出Small-Gain Nash (SGN)方法，通过自定义块加权几何中的块小增益条件，为梯度学习在非单调博弈中提供收敛保证，即使伪梯度在欧几里得几何中不满足单调性条件。


<details>
  <summary>Details</summary>
Motivation: 传统博弈中梯度学习的收敛性分析需要伪梯度在欧几里得几何中满足(强)单调性条件，但这一条件在具有强跨玩家耦合的简单博弈中经常失效，限制了收敛性分析的应用范围。

Method: 引入SGN方法，在自定义块加权几何中建立块小增益条件，将局部曲率和跨玩家Lipschitz耦合边界转化为可处理的收缩证书。该方法构造加权块度量，使得伪梯度在满足这些边界的任何区域上变得强单调，即使它在欧几里得意义下是非单调的。

Result: 连续流在设计的几何中呈指数收缩，投影欧拉和RK4离散化在显式步长边界下收敛。分析揭示了"时间尺度带"证书，这是一个非渐近的、基于度量的证书，类似于TTUR的作用。在欧几里得单调性分析失败的二次博弈中，SGN成功认证了收敛性，并可扩展到马尔可夫博弈中的镜像/Fisher几何。

Conclusion: SGN提供了一个离线认证流程，可在紧凑区域上估计曲率、耦合和Lipschitz参数，优化块权重以扩大SGN边界，并返回一个结构化的、可计算的收敛证书，包括度量、收缩率和安全步长，用于非单调博弈。

Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.

</details>


### [98] [Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation](https://arxiv.org/abs/2512.06813)
*Agung Nugraha,Heungjun Im,Jihwan Lee*

Main category: cs.LG

TL;DR: 提出合作神经网络框架用于高性能混凝土的部分逆向设计，通过协同学习在单次前向传播中生成满足约束的配合比设计，相比基线方法显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 高性能混凝土配合比设计复杂，涉及多个相互依赖变量和实际约束。虽然数据驱动方法在正向预测方面有进展，但逆向设计（根据目标性能确定配合比）仍然有限，特别是在部分变量被约束固定的情况下。

Method: 提出合作神经网络框架，结合两个耦合的神经网络模型：一个插补模型推断未确定变量，一个代理模型预测抗压强度。通过协同学习，模型在单次前向传播中生成有效且性能一致的配合比设计，无需重新训练即可适应不同约束组合。

Result: 在基准数据集上评估，提出的模型获得稳定的R平方值0.87-0.92，相比自编码器基线平均减少50%均方误差，相比贝叶斯推理平均减少70%均方误差。

Conclusion: 合作神经网络为混凝土工程中约束感知、数据驱动的配合比设计提供了准确、鲁棒且计算高效的基础。

Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.

</details>


### [99] [Neural Factorization-based Bearing Fault Diagnosis](https://arxiv.org/abs/2512.06837)
*Zhenhao Li,Xu Cheng,Yi Zhou*

Main category: cs.LG

TL;DR: 提出基于神经分解的分类（NFC）框架用于高铁轴承故障诊断，通过多模态潜在特征向量嵌入和神经分解融合，在复杂条件下实现高精度诊断。


<details>
  <summary>Details</summary>
Motivation: 高铁轴承作为列车运行系统的核心部件，其健康状况直接关系到列车运行安全。传统诊断方法在复杂工况下面临诊断精度不足的挑战，需要更有效的故障诊断方法。

Method: 提出神经分解分类（NFC）框架：1）将振动时间序列嵌入到多个模态潜在特征向量中，捕捉不同的故障相关模式；2）利用神经分解原理将这些向量融合为统一的振动表示。基于CP和Tucker融合方案分别实例化为CP-NFC和Tucker-NFC模型。

Result: 实验结果表明，CP-NFC和Tucker-NFC模型相比传统机器学习方法都取得了优越的诊断性能。对比分析为高铁轴承监测中选择有效诊断策略提供了有价值的实证证据和实践指导。

Conclusion: 提出的NFC框架能够有效挖掘原始时间序列数据中的复杂潜在故障特征，为高铁轴承故障诊断提供了一种有效的解决方案，在复杂工况下具有优越的诊断性能。

Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.

</details>


### [100] [Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis](https://arxiv.org/abs/2512.06917)
*Clifford F,Devika Jay,Abhishek Sarkar,Satheesh K Perepu,Santhosh G S,Kaushik Dey,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出一个新颖的轨迹级可解释强化学习框架，通过结合Q值差异和"激进项"的新状态重要性指标来排名完整轨迹，并生成反事实推演来解释长期行为。


<details>
  <summary>Details</summary>
Motivation: 当前可解释强化学习主要关注局部单步决策，缺乏对智能体长期行为的解释。随着RL在现实应用中的部署，需要确保其行为透明可信，因此需要轨迹级的解释框架。

Method: 引入一个新颖框架，定义并聚合新的状态重要性指标来排名完整轨迹。该指标结合经典Q值差异和捕捉智能体到达目标倾向的"激进项"，提供更细致的状态关键性度量。通过从关键状态生成反事实推演来验证路径选择。

Result: 方法成功从异构智能体经验中识别出最优轨迹。实验在标准OpenAI Gym环境中验证，提出的重要性指标比经典方法更有效地识别最优行为，并能展示所选路径相对于替代方案的优越性。

Conclusion: 该框架为强化学习智能体的长期行为提供了强大的"为什么选这个而不是那个"的解释，向可信自主系统迈出了重要一步。

Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.

</details>


### [101] [Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models](https://arxiv.org/abs/2512.06920)
*Alexandr Plashchinsky*

Main category: cs.LG

TL;DR: PGSRM是一种轻量级强化学习奖励框架，使用父模型参考输出嵌入与子模型生成输出的余弦相似度作为语义奖励，无需人工标注或额外模型训练。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法需要二元正确性信号、人类偏好数据或训练奖励模型，这些方法成本高且复杂。PGSRM旨在提供一种简单、语义丰富的奖励信号，避免这些开销。

Method: PGSRM使用父模型（参考模型）的输出嵌入与子模型（被训练模型）生成输出的余弦相似度作为奖励信号。这种嵌入相似度提供了密集的语义奖励，无需人工标注或额外训练。

Result: 在五个语言任务上应用PGSRM，相比二元奖励基线，产生了更平滑的奖励改进和更稳定的PPO动态，表明基于嵌入的语义奖励是RLHF风格奖励建模的实用替代方案。

Conclusion: 嵌入语义奖励为小规模Transformer模型的父引导对齐提供了一种实用替代方案，避免了传统RLHF奖励建模的复杂性和成本。

Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.

</details>


### [102] [Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features](https://arxiv.org/abs/2512.06925)
*Aseer Al Faisal*

Main category: cs.LG

TL;DR: 本文提出了一种结合RoBERTa语义嵌入和手工词汇特征的QR-DQN方法，用于钓鱼网站检测，在105,000个URL数据集上取得了99.86%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击通过欺诈信息、误导广告和合法网站漏洞欺骗用户泄露个人信息，造成经济损失。传统DQN方法估计单一标量Q值，无法有效处理不确定性，需要更稳定和泛化能力强的检测方法。

Method: 提出Quantile Regression Deep Q-Network (QR-DQN)方法，结合RoBERTa语义嵌入和手工词汇特征。与传统DQN不同，QR-DQN使用分位数回归建模回报分布，提高稳定性和泛化能力。使用来自PhishTank、OpenPhish、Cloudflare等源的105,000个URL数据集，采用80/20训练测试分割。

Result: QR-DQN框架测试准确率达99.86%，精确率99.75%，召回率99.96%，F1分数99.85%。相比仅使用词汇特征的标准DQN，结合词汇和语义特征的混合QR-DQN将泛化差距从1.66%降至0.04%。五折交叉验证平均准确率99.90%，标准差0.04%。

Conclusion: 提出的混合方法能有效识别钓鱼威胁，适应不断演变的攻击策略，对未见数据具有良好泛化能力。QR-DQN通过建模回报分布提高了检测的稳定性和鲁棒性。

Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.

</details>


### [103] [Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise](https://arxiv.org/abs/2512.06926)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: 该研究系统分析了BiLSTM时间序列预测模型中输入序列长度和加性噪声两个数据因素对模型鲁棒性和泛化能力的影响，发现长序列会增加过拟合风险，噪声会降低预测精度，两者同时存在时模型稳定性下降最严重。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列预测中应用广泛，特别是BiLSTM在捕捉复杂时间依赖方面很有效。然而现有文献对模型鲁棒性和泛化能力对输入数据特征的敏感性研究不足，特别是输入序列长度和加性噪声这两个关键数据因素的影响尚未充分探索。

Method: 开发了一个模块化、可复现的预测流程，包含标准化预处理、序列生成、模型训练、验证和评估。在三个具有不同采样频率的真实世界数据集上进行控制实验，评估BiLSTM在不同输入条件下的性能表现。

Result: 三个关键发现：1) 较长的输入序列显著增加过拟合和数据泄露风险，特别是在数据受限环境中；2) 加性噪声在不同采样频率下都会持续降低预测精度；3) 两个因素同时存在时，模型稳定性下降最严重。较高观测频率的数据集表现出更强的鲁棒性，但在两个挑战同时存在时仍然脆弱。

Conclusion: 研究揭示了当前基于深度学习的预测流程的重要局限性，强调了数据感知设计策略的必要性。这项工作有助于更深入理解深度学习模型在动态时间序列环境中的行为，并为开发更可靠、更可泛化的预测系统提供了实用见解。

Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.

</details>


### [104] [Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding](https://arxiv.org/abs/2512.06929)
*MinCheol Jeon*

Main category: cs.LG

TL;DR: AdaMamba：一种集成自适应归一化、多尺度趋势提取和上下文序列建模的统一预测架构，用于解决时间序列预测中的非平稳性、多尺度模式和分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测面临非平稳性、多尺度时间模式和分布偏移等挑战，这些会降低模型稳定性和准确性。传统Transformer基线在这些问题上表现不足。

Method: 1. 自适应归一化块：通过多尺度卷积趋势提取和通道级重新校准去除非平稳成分；2. 上下文编码器：结合补丁嵌入、位置编码和Mamba增强的Transformer层（带专家混合前馈模块）；3. 轻量级预测头和反归一化机制。

Result: 实验评估表明，AdaMamba的自适应归一化和专家增强上下文建模相结合，在稳定性和准确性方面相比传统Transformer基线取得了一致的改进。

Conclusion: AdaMamba通过自适应归一化和上下文建模的有效集成，缓解了协变量偏移，增强了预测可靠性，具有强大的表示能力和模块化可扩展性，支持确定性预测和概率扩展。

Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.

</details>


### [105] [Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies](https://arxiv.org/abs/2512.06932)
*Salma Albelali,Moataz Ahmed*

Main category: cs.LG

TL;DR: 该研究评估了数据泄露对LSTM时间序列预测性能的影响，发现验证方法设计显著影响泄露敏感性，其中10折交叉验证最易受影响，而2-way和3-way分割更稳健。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中广泛使用LSTM等深度学习模型，但评估完整性常因数据泄露而受损。数据泄露指在数据集划分前构建输入输出序列，导致未来信息无意中影响训练。本研究旨在探究数据泄露对性能的影响，特别是验证设计如何调节泄露敏感性。

Method: 研究评估了三种常用验证技术（2-way分割、3-way分割和10折交叉验证）在泄露（分割前序列生成）和干净（分割后序列生成）条件下的表现。使用RMSE增益衡量泄露影响，计算泄露与干净设置之间的百分比差异。分析了输入窗口大小和滞后步长对泄露敏感性的影响。

Result: 10折交叉验证在较长滞后步长下表现出高达20.5%的RMSE增益，而2-way和3-way分割通常将RMSE增益保持在5%以下。较小的输入窗口和较长的滞后步长会增加泄露风险，而较大的窗口有助于减少泄露。

Conclusion: 验证方法选择对数据泄露敏感性有显著影响，需要配置感知、抗泄露的评估流程来确保可靠的性能估计。研究强调了在时间序列预测中采用适当验证设计的重要性。

Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.

</details>


### [106] [A Unifying Human-Centered AI Fairness Framework](https://arxiv.org/abs/2512.06944)
*Munshi Mahbubur Rahman,Shimei Pan,James R. Foulds*

Main category: cs.LG

TL;DR: 提出一个统一的人类中心公平性框架，覆盖八种公平性指标，帮助利益相关者根据价值观和情境选择公平性干预措施，并在四个真实数据集上验证了框架的有效性。


<details>
  <summary>Details</summary>
Motivation: AI在关键社会领域的使用增加引发了公平性担忧，但现有方法在平衡不同公平性概念和预测准确性方面存在挑战，阻碍了公平AI系统的实际部署。

Method: 引入统一的人类中心公平性框架，系统覆盖八种公平性指标（结合个体/群体公平性、边际内/交叉假设、结果导向/机会平等视角），使用一致易懂的公式，允许利益相关者为多个公平性目标分配权重。

Result: 在四个真实数据集（UCI Adult、COMPAS、German Credit、MEPS）上应用该框架，展示调整权重如何揭示不同公平性指标之间的微妙权衡，并通过司法决策和医疗保健案例研究证明框架的实际应用价值。

Conclusion: 该框架为公平AI系统的实际部署提供了实用且价值敏感的方法，使利益相关者能够根据具体情境和价值观做出明智的公平性决策，促进多方利益妥协。

Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.

</details>


### [107] [Comparing BFGS and OGR for Second-Order Optimization](https://arxiv.org/abs/2512.06969)
*Adrian Przybysz,Mikołaj Kołek,Franciszek Sobota,Jarek Duda*

Main category: cs.LG

TL;DR: 比较BFGS的Sherman-Morrison更新与新的在线梯度回归(OGR)方法在Hessian矩阵估计上的表现，OGR在非凸优化中表现更优


<details>
  <summary>Details</summary>
Motivation: 神经网络训练中Hessian矩阵估计面临高维度和计算成本挑战，传统BFGS方法需要正定Hessian近似且依赖凸性假设，需要更灵活的方法处理非凸优化问题

Method: 提出在线梯度回归(OGR)方法，通过梯度对位置的指数移动平均回归在线估计二阶导数，无需Hessian求逆，可以估计一般（非正定）Hessian矩阵

Result: 在标准测试函数上评估两种方法，OGR实现了更快的收敛速度和更低的损失，特别是在非凸设置中表现更优

Conclusion: OGR方法比传统BFGS更灵活，能够处理非凸结构，在神经网络训练等非凸优化问题中具有更好的性能

Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.

</details>


### [108] [OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction](https://arxiv.org/abs/2512.06987)
*Emily Jin,Andrei Cristian Nica,Mikhail Galkin,Jarrid Rector-Brooks,Kin Long Kelvin Lee,Santiago Miret,Frances H. Arnold,Michael Bronstein,Avishek Joey Bose,Alexander Tong,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: OXtal是一个100M参数的全原子扩散模型，直接从2D化学图预测3D分子晶体结构，通过数据增强和新型训练方案S⁴，在600K实验验证结构上训练，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测分子晶体结构（CSP）是计算化学中长期存在的开放挑战，对制药和有机半导体等领域至关重要，因为晶体堆积直接影响有机固体的物理化学性质。

Method: 提出OXtal大规模扩散模型，放弃显式等变架构，采用数据增强策略；提出S⁴训练方案，避免显式晶格参数化，高效捕获长程相互作用；在600K实验验证晶体结构数据集上训练。

Result: OXtal在恢复实验结构方面取得数量级改进（RMSD₁<0.5Å），达到80%以上的堆积相似率，比传统量子化学方法便宜几个数量级，能同时建模热力学和动力学规律。

Conclusion: OXtal通过大规模扩散模型和创新的训练策略，在晶体结构预测任务上取得了突破性进展，为从2D化学图预测3D晶体结构提供了高效准确的解决方案。

Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ Å and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.

</details>


### [109] [Flash Multi-Head Feed-Forward Network](https://arxiv.org/abs/2512.06989)
*Minshen Zhang,Xiang Hu,Jianguo Li,Wei Wu,Kewei Tu*

Main category: cs.LG

TL;DR: FlashMHF：用类似多头注意力的多头FFN替换Transformer中的FFN，通过融合内核和动态加权并行子网络设计，提升性能同时大幅减少内存使用


<details>
  <summary>Details</summary>
Motivation: 受单头注意力与FFN结构相似性的启发，探索多头FFN作为FFN的替代方案。多头机制能增强注意力层的表达能力，但直接应用于FFN面临内存消耗随头数增加而增加，以及中间维度与头维度比例失衡的问题

Method: 提出Flash Multi-Head FFN (FlashMHF)：1) 类似FlashAttention的I/O感知融合内核，在SRAM中在线计算输出；2) 使用动态加权并行子网络设计，保持中间维度与头维度的平衡比例

Result: 在128M到1.3B参数的模型上验证，FlashMHF相比SwiGLU FFN持续改善困惑度和下游任务准确率，同时减少峰值内存使用3-5倍，推理加速最高达1.08倍

Conclusion: 多头设计是FFN的优越架构原则，FlashMHF作为Transformer中FFN的强大、高效且可扩展的替代方案

Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.

</details>


### [110] [Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation](https://arxiv.org/abs/2512.06993)
*Ali Ebrahimpour-Boroojeny*

Main category: cs.LG

TL;DR: 论文提出了AMUN和TRW两种机器学习遗忘方法，分别针对样本遗忘和类别遗忘，通过对抗样本微调和类别分布调整来提升遗忘效果并抵御成员推理攻击。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法在保护隐私、移除特定数据或类别方面存在不足，无法有效模拟从头训练模型的行为，容易受到成员推理攻击。

Method: 1. AMUN：通过对抗样本微调降低模型对遗忘样本的置信度；2. FastClip：层谱范数裁剪控制Lipschitz常数以提升模型平滑性；3. TRW：基于类别相似性估计调整目标分布，使遗忘类别输入近似于从头训练模型的剩余类别分布。

Result: AMUN在图像分类任务中超越现有SOTA方法，TRW在多个基准测试中匹配或超越现有遗忘方法。提出的MIA-NN攻击揭示了现有方法的脆弱性。

Conclusion: 论文提出的遗忘方法通过模拟从头训练模型的行为，有效提升遗忘效果并抵御成员推理攻击，为机器学习隐私保护提供了新思路。

Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.

</details>


### [111] [Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation](https://arxiv.org/abs/2512.07010)
*Kevin Lee,Pablo Millan Arias*

Main category: cs.LG

TL;DR: DynamicLRP：首个模型无关的层间相关性传播框架，通过张量操作级分解和Promise系统实现真正的架构无关性，无需模型修改即可应用于任意计算图。


<details>
  <summary>Details</summary>
Motivation: 现有LRP实现工作在模块级别，需要架构特定的传播规则和修改，限制了目标模型的通用性和实现的可维护性，难以适应不断演进的神经网络架构。

Method: 提出DynamicLRP框架：1）在计算图内将相关性传播分解到单个张量操作级别；2）引入Promise系统实现延迟激活解析；3）独立于反向传播机制，可与梯度反向传播并行执行。

Result: 在31,465个计算图节点（15种不同架构）上实现99.92%的节点覆盖率，包括Mamba、Whisper、DePlot等复杂模型。性能匹配或超越专用实现（VGG上1.77 vs 1.69 ABPC，ViT相当性能，RoBERTa-large和Flan-T5-large在SQuADv2上分别达到93.70%和95.06%的top-1归因准确率）。

Conclusion: DynamicLRP通过操作级分解和Promise系统为LRP建立了可持续、可扩展的基础，实现了真正的架构无关性，为不断演进的神经网络架构提供了通用的可解释性解决方案。

Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.

</details>


### [112] [Block Sparse Flash Attention](https://arxiv.org/abs/2512.07011)
*Daniel Ohayon,Itay Lamprecht,Itay Hubara,Israel Cohen,Daniel Soudry,Noam Elata*

Main category: cs.LG

TL;DR: BSFA是一种无需训练的注意力加速方法，通过计算精确的查询-键相似度选择top-k重要值块，跳过约50%计算和内存传输，在保持99%以上基线准确率的同时实现1.10-1.24倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型需要长上下文进行推理和多文档任务，但注意力机制的二次复杂度造成了严重的计算瓶颈，需要高效的加速方法。

Method: BSFA通过计算精确的查询-键相似度，比较每个块的最大分数与校准阈值，为每个查询选择top-k最重要的值块。无需训练，仅需在小数据集上进行一次性阈值校准来学习每层每头的注意力分数分布。

Result: 在Llama-3.1-8B上，BSFA在真实世界推理基准测试中实现最高1.10倍加速，在"大海捞针"检索任务中实现最高1.24倍加速，同时保持99%以上基线准确率，某些配置甚至通过关注最相关内容提高了准确性。

Conclusion: BSFA是一种有效的训练无关注意力加速方法，可作为FlashAttention的即插即用替代方案，显著优于现有的稀疏注意力方法，为长上下文推理提供了实用的计算优化方案。

Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention

</details>


### [113] [Transferring Clinical Knowledge into ECGs Representation](https://arxiv.org/abs/2512.07021)
*Jose Geraldo Fernandes,Luiz Facury de Souza,Pedro Robles Dutenhefner,Gisele L. Pappa,Wagner Meira*

Main category: cs.LG

TL;DR: 提出三阶段训练范式，将多模态临床数据知识迁移到单模态ECG编码器，提高ECG分类准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 深度学习ECG分类模型虽然准确率高，但黑盒特性缺乏可解释性，阻碍临床采用。需要建立更可信的ECG分类模型

Method: 三阶段训练范式：1）自监督联合嵌入预训练，创建富含临床上下文信息的ECG表示；2）仅需ECG信号进行推理；3）训练模型从ECG嵌入预测相关实验室异常作为间接解释

Result: 在MIMIC-IV-ECG数据集上，模型在多标签诊断分类中优于标准信号基线，显著缩小了与需要所有数据推理的全多模态模型的性能差距

Conclusion: 通过将抽象预测转化为基于生理学的解释，为AI安全集成到临床工作流程提供了有前景的路径，创建了更准确、可信的ECG分类模型

Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.

</details>


### [114] [Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis](https://arxiv.org/abs/2512.07040)
*Sakib Mostafa,Lei Xing,Md. Tauhidul Islam*

Main category: cs.LG

TL;DR: Graph2Image将大型生物网络转换为二维图像，通过CNN分析，解决了现有方法在可扩展性、长距离依赖和可解释性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 生物网络规模庞大且复杂，现有分析方法（包括深度学习方法）面临可扩展性有限、长距离依赖过平滑、多模态整合困难、表达能力受限和可解释性差等挑战。

Method: 将大型生物网络转换为二维图像集，通过在2D网格上空间排列代表性网络节点，将节点解耦为图像，从而能够使用具有全局感受野和多尺度金字塔的卷积神经网络。

Result: 在多个大规模生物网络数据集上，Graph2Image比现有方法提高了67.2%的分类准确率，提供了可解释的可视化，揭示了生物一致性模式，并能在个人计算机上分析超过10亿节点的网络。

Conclusion: Graph2Image为生物网络分析提供了一个可扩展、可解释且支持多模态的方法，为疾病诊断和复杂生物系统研究提供了新机会。

Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.

</details>


### [115] [Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design](https://arxiv.org/abs/2512.07064)
*Jiannan Yang,Veronika Thost,Tengfei Ma*

Main category: cs.LG

TL;DR: 该研究通过概率框架系统评估分子图自监督学习中的掩码策略，发现对于常见节点级预测任务，复杂掩码分布相比均匀采样并无优势，而预测目标选择和编码器架构的协同更为关键。


<details>
  <summary>Details</summary>
Motivation: 分子表示学习中自监督学习（SSL）的核心作用日益凸显，但许多基于掩码的预训练方法创新缺乏原则性评估，难以确定哪些设计选择真正有效，需要系统框架来透明比较和理解不同掩码策略。

Method: 将整个预训练-微调流程统一到概率框架中，在严格控制条件下系统研究三个核心设计维度：掩码分布、预测目标和编码器架构，并使用信息论指标评估预训练信号的信息量，与下游性能进行关联分析。

Result: 研究发现：1）复杂掩码分布在常见节点级预测任务中相比均匀采样没有一致优势；2）预测目标选择和编码器架构的协同更为关键；3）转向语义更丰富的目标能显著提升下游性能，特别是与表达能力强的图Transformer编码器结合时。

Conclusion: 该研究为分子图自监督学习提供了实用指导：应更关注预测目标的选择及其与编码器架构的协同，而非过度优化掩码分布，这有助于开发更有效的分子表示学习方法。

Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.

</details>


### [116] [Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation](https://arxiv.org/abs/2512.07079)
*Anton Morgunov,Victor S. Batista*

Main category: cs.LG

TL;DR: RetroCast是一个统一的CASP评估套件，通过标准化模型输出、提供可重复基准测试和交互式路线检查平台，解决了该领域缺乏标准化评估基础设施的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机辅助合成规划（CASP）领域缺乏标准化评估基础设施，现有指标过于关注拓扑完成度而忽视化学有效性，导致进展评估不准确。

Method: 开发RetroCast评估套件，包括：1）将异构模型输出标准化为统一模式；2）包含分层采样和自举置信区间的可重复基准测试流程；3）用于定性路线检查的交互式平台SynthArena。

Result: 评估发现：1）"可解性"（库存终止率）与路线质量存在分歧，高可解性分数常掩盖化学无效性；2）搜索方法存在"复杂性悬崖"，在重建长程合成计划时性能急剧下降；3）序列方法在长程规划上表现更好。

Conclusion: RetroCast为CASP提供了透明可重复的评估框架，揭示了现有指标的局限性，并发布了完整框架、基准定义和标准化预测数据库以支持领域发展。

Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.

</details>


### [117] [TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization](https://arxiv.org/abs/2512.07082)
*Yuan-Ting Zhong,Ting Huang,Xiaolin Xiao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: TRACE是一个可迁移的概念漂移估计器，用于检测流数据中的分布变化，具有跨数据集泛化能力，并能集成到流优化器中实现自适应优化。


<details>
  <summary>Details</summary>
Motivation: 流数据驱动优化面临未知概念漂移的挑战，现有方法假设固定漂移间隔和完全环境可观测性，限制了在多样化动态环境中的适应性。

Method: TRACE采用原则性标记化策略从数据流中提取统计特征，使用基于注意力的序列学习建模漂移模式，实现跨数据集的准确检测和漂移模式的可迁移性。

Result: 在多样化基准测试中，TRACE展现出优异的泛化能力、鲁棒性和有效性，能够准确检测未见数据集中的漂移模式。

Conclusion: TRACE是一个可迁移的概念漂移估计器，具有即插即用特性，能够有效解决流数据驱动优化中的未知概念漂移问题，提升自适应优化能力。

Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.

</details>


### [118] [The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models](https://arxiv.org/abs/2512.07092)
*Zhixiang Wang*

Main category: cs.LG

TL;DR: 提出Soul Engine框架，基于线性表示假设，通过冻结基础模型权重提取解耦的人格向量，实现高精度人格分析、零样本人格注入和确定性行为控制，避免对齐税问题。


<details>
  <summary>Details</summary>
Motivation: 当前个性化大语言模型部署面临稳定性-可塑性困境，传统对齐方法（如监督微调）依赖随机权重更新，通常会导致"对齐税"——降低通用推理能力。需要一种不修改基础模型权重就能实现人格化的方法。

Method: 基于线性表示假设，提出Soul Engine框架，认为人格特质存在于正交线性子空间中。构建SoulBench数据集（通过动态上下文采样），在冻结的Qwen-2.5基础模型上使用双头架构，提取解耦的人格向量而不修改骨干网络权重。

Result: 实验展示三大突破：1) 高精度人格分析：对心理学基准的均方误差为0.011；2) 几何正交性：T-SNE可视化确认人格流形是离散且连续的，支持"零样本人格注入"并保持原始模型智能；3) 确定性控制：通过向量运算实现鲁棒行为控制，经大量消融研究验证。

Conclusion: 这项工作挑战了微调对于人格化的必要性。通过从概率提示转向确定性潜在干预，为安全、可控的AI人格化提供了数学严谨的基础，避免了传统对齐方法的缺陷。

Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.

</details>


### [119] [Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph](https://arxiv.org/abs/2512.07100)
*Hong Wang,Yinglong Zhang,Hanhan Guo,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: DRCL是一个完全无监督的框架，用于文本属性网络中的社区检测，通过双向精炼循环整合结构和语义信息，无需标签或类别定义。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型虽然具备强大的文本理解能力，但在真实世界的文本属性网络中部署困难，因为它们严重依赖标注数据。同时，传统的社区检测方法通常忽略文本语义，限制了其在内容组织、推荐和风险监控等下游应用中的实用性。

Method: DRCL通过热启动初始化和双向精炼循环整合结构和语义信息。它包括基于GCN的社区检测模块(GCN-CDM)和文本语义建模模块(TSMM)，这两个模块迭代交换伪标签，使语义线索增强结构聚类，结构模式指导文本表示学习，无需人工监督。

Result: 在多个文本属性图数据集上，DRCL持续提升了发现社区的结构和语义质量。此外，仅使用DRCL社区信号训练的Mamba分类器达到了与监督模型相当的准确率，展示了其在大规模系统中部署的潜力。

Conclusion: DRCL为无监督的文本属性网络分析提供了一个有效的框架，能够同时优化社区检测的语义和结构质量，在标注数据稀缺或昂贵的实际场景中具有重要应用价值。

Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.

</details>


### [120] [FOAM: Blocked State Folding for Memory-Efficient LLM Training](https://arxiv.org/abs/2512.07112)
*Ziqing Wen,Jiahuan Wang,Ping Luo,Dongsheng Li,Tao Sun*

Main category: cs.LG

TL;DR: FOAM是一种通过计算块级梯度均值压缩优化器状态的内存高效优化器，减少约50%训练内存，消除90%优化器状态开销，同时保持与Adam相当的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练时面临严重内存瓶颈，特别是使用Adam等内存密集型优化器时。现有内存高效方法存在计算开销大、需要额外内存或性能下降等问题。

Method: 提出FOAM方法：1）通过计算块级梯度均值压缩优化器状态；2）引入残差校正恢复丢失信息。理论证明在标准非凸优化设置下达到与Adam相当的收敛率。

Result: FOAM减少约50%总训练内存，消除高达90%的优化器状态内存开销，并加速收敛。与现有内存高效优化器兼容，性能匹配或超越全秩和现有基线。

Conclusion: FOAM提供了一种有效解决LLM训练内存瓶颈的方法，在保持性能的同时显著降低内存需求，为大规模模型训练提供了实用解决方案。

Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.

</details>


### [121] [PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes](https://arxiv.org/abs/2512.07113)
*Kepeng Lin,Qizhe Zhang,Rui Wang,Xuehai Hu,Wei Xu*

Main category: cs.LG

TL;DR: PlantBiMoE：轻量级植物基因组语言模型，结合双向Mamba和稀疏专家混合框架，在MPGB基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有植物基因组模型如AgroNT和PDLLMs存在参数过多或无法有效建模DNA双链双向性的问题，需要更高效且表达能力强的模型

Method: 提出PlantBiMoE模型，集成双向Mamba捕获DNA正反链的结构依赖，使用稀疏专家混合框架减少激活参数数量，提高计算效率

Result: 在MPGB基准测试（31个数据集，11个任务）中，PlantBiMoE在20个数据集上取得最佳性能，平均表现最优

Conclusion: PlantBiMoE能有效表示植物基因组序列，为基因组学、基因编辑和合成生物学提供强大的计算工具

Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE

</details>


### [122] [Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search](https://arxiv.org/abs/2512.07142)
*Tanay Arora,Christof Teuscher*

Main category: cs.LG

TL;DR: CTS算法通过组合优化和梯度平衡，在初始化阶段高效发现高性能稀疏子网络，性能超越传统彩票票证重绕方法，计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有彩票票证发现方法存在计算成本高（如LTR）或准确率-稀疏度权衡差（如PaI）的问题，PaI方法依赖一阶显著性指标忽略了权重间依赖关系，尤其在稀疏区域表现不佳。

Method: 提出Concrete Ticket Search (CTS)算法，将子网络发现建模为组合优化问题，使用Concrete松弛离散搜索空间，结合GRADBALANCE梯度平衡方案控制稀疏度，并采用基于反向KL散度的知识蒸馏式剪枝目标。

Result: 在图像分类任务中，CTS产生的子网络能通过基本合理性检验，准确率与LTR相当或更高，计算时间大幅减少（如ResNet-20在CIFAR10上达到99.3%稀疏度，74.0%准确率，仅需7.9分钟）。

Conclusion: CTS通过整体组合优化方法有效解决了彩票票证发现中的计算效率和性能权衡问题，在高度稀疏区域尤其优于传统方法，为高效神经网络剪枝提供了新思路。

Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.

</details>


### [123] [FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers](https://arxiv.org/abs/2512.07150)
*Jonghyun Park,Jong Chul Ye*

Main category: cs.LG

TL;DR: 提出FlowLPS框架，通过Langevin Proximal Sampling策略解决预训练流模型的逆问题，平衡重建保真度和感知质量


<details>
  <summary>Details</summary>
Motivation: 现有训练自由方法在应用于潜在流模型时，往往无法收敛到后验模式或在潜在空间中遭受流形偏差

Method: FlowLPS框架，结合Langevin动力学进行流形一致探索和近端优化进行精确模式搜索

Result: 在FFHQ和DIV2K的多个逆任务上，实现了重建保真度和感知质量的优越平衡，优于现有逆问题求解器

Conclusion: FlowLPS通过Langevin Proximal Sampling策略有效解决了流模型逆问题中的收敛和流形偏差问题

Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.

</details>


### [124] [Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration](https://arxiv.org/abs/2512.07173)
*Jucheng Shen,Gaurav Sarkar,Yeonju Ro,Sharath Nittur Sridhar,Zhangyang Wang,Aditya Akella,Souvik Kundu*

Main category: cs.LG

TL;DR: CadLLM是一种无需训练的方法，通过动态调整生成块大小、步长和阈值来加速基于扩散的大语言模型推理吞吐量，最高可达2.28倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的大语言模型推理速度较慢，需要加速其推理吞吐量。作者观察到不同块和步骤中token unmasking置信度的动态特性，这为优化提供了机会。

Method: 1. 研究token unmasking置信度在不同块和步骤中的动态特性；2. 基于平均置信度设计轻量级自适应方法，控制生成块大小、步长和阈值；3. 通过动态利用词汇表子集减少softmax开销，调节采样广度。

Result: 在四个流行任务上的实验表明，CadLLM相比最先进的基线方法，在保持竞争力的准确度下，实现了最高2.28倍的吞吐量提升。

Conclusion: CadLLM是一种即插即用、模型无关的方法，兼容基于KV缓存的扩散大语言模型，能显著加速推理吞吐量而不牺牲准确性。

Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.

</details>


### [125] [SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models](https://arxiv.org/abs/2512.07175)
*Yibo Wang,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: SPACE是一种新颖的自对弈微调方法，通过噪声对比估计来捕捉真实世界数据分布，解决了现有基于奖励差距方法的不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有自对弈微调方法主要关注真实数据和合成数据之间的奖励差距，忽略了它们的绝对值。通过理论分析发现，基于差距的方法由于目标可能退化而存在不稳定演化的问题。

Method: 提出SPACE方法，将合成样本视为辅助成分，以二元分类方式区分真实样本和合成样本。该方法独立优化每种类型数据的绝对奖励值，确保目标始终有意义。

Result: SPACE在多个任务上显著提升LLM性能，优于使用更多真实样本的监督微调。相比基于差距的自对弈微调方法，SPACE表现出显著优越性和稳定演化。

Conclusion: SPACE通过噪声对比估计有效捕捉真实世界数据分布，理论上保证稳定收敛到最优分布，实践上在各种任务中表现出色，解决了现有自对弈微调方法的不稳定性问题。

Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.

</details>


### [126] [UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting](https://arxiv.org/abs/2512.07184)
*Da Zhang,Bingyu Li,Zhuyuan Zhao,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: UniDiff：统一扩散框架用于多模态时间序列预测，通过并行融合模块整合文本和时间戳信息，实现跨模态信号的有效利用


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据日益增多，但现有扩散模型在时间序列预测中主要局限于单模态数值序列，忽略了复杂异构数据中丰富的跨模态信号

Method: 1) 将时间序列分块标记化并映射到嵌入空间；2) 核心是统一并行融合模块，使用单交叉注意力机制自适应加权整合时间戳结构信息和文本语义信息；3) 引入新颖的无分类器引导机制，支持多源条件控制

Result: 在八个领域的真实世界基准数据集上进行广泛实验，证明UniDiff模型实现了最先进的性能

Conclusion: UniDiff为多模态时间序列预测提供了一个有效的统一扩散框架，能够充分利用异构信息，显著提升预测性能

Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.

</details>


### [127] [Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction](https://arxiv.org/abs/2512.07200)
*Zhen Huang,Jiaxin Deng,Jiayu Xu,Junbiao Pang,Haitao Yu*

Main category: cs.LG

TL;DR: 提出基于强化学习的非均匀道路分割方法，用于公交车到达时间预测，相比传统均匀分割方法在效率和性能上都有显著提升


<details>
  <summary>Details</summary>
Motivation: 传统的均匀道路分割方法无法考虑道路物理约束的差异（如路况、交叉口、兴趣点等），限制了预测效率。需要一种能自适应学习非均匀道路分割的方法来提升到达时间预测性能。

Method: 采用两阶段方法：1）基于强化学习框架，根据影响分数提取非均匀道路段；2）在线性预测模型中对选定路段进行预测。该方法在保持计算效率的同时确保最优路段选择。

Result: 实验结果表明，该方法在效率和性能上都优于传统均匀分割方法，线性方法甚至能比更复杂的方法获得更好的性能，在大规模基准测试中表现出优越性。

Conclusion: 提出的强化学习非均匀道路分割方法为公交车到达时间预测提供了一种高效且性能优越的解决方案，证明了"少即是多"的理念，代码和数据集已公开。

Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.

</details>


### [128] [Geometric Prior-Guided Federated Prompt Calibration](https://arxiv.org/abs/2512.07208)
*Fei Luo,Ziwei Zhao,Mingxuan Wang,Duoyang Li,Zhe Qian,Jiayi Tuo,Chenyue Zhou,Yanbiao Ma*

Main category: cs.LG

TL;DR: GGTPC通过几何先验校准层解决联邦提示学习中数据异构导致的局部训练偏差问题，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 联邦提示学习在数据异构环境下性能严重受限，现有方法未能解决局部训练偏差的根本原因

Method: 提出几何引导文本提示校准框架，在服务器端重构全局几何先验，客户端使用几何先验校准层对齐局部特征分布

Result: 在标签倾斜的CIFAR-100数据集上优于SOTA 2.15%，极端倾斜下提升9.17%；在Office-Home数据集上提升FedAvg 4.60%

Conclusion: GGTPC通过修正局部训练偏差有效缓解数据异构问题，可作为通用模块增强各种联邦学习算法

Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($β$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($β$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.

</details>


### [129] [Pay Less Attention to Function Words for Free Robustness of Vision-Language Models](https://arxiv.org/abs/2512.07222)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Chao Shen*

Main category: cs.LG

TL;DR: FDA通过消除功能词的影响来提升VLM的鲁棒性，在保持性能的同时显著降低对抗攻击成功率


<details>
  <summary>Details</summary>
Motivation: 为了解决鲁棒VLM中鲁棒性与性能之间的权衡问题，作者观察到功能词是VLM在跨模态对抗攻击中的脆弱点，因此需要减少功能词的影响

Method: 提出Function-word De-Attention (FDA)，类似于差分放大器，计算原始交叉注意力和功能词交叉注意力，然后将后者从前者中差分减去，以获得更对齐和鲁棒的VLM

Result: 在3个模型上的检索任务中，FDA平均降低18/13/53%的攻击成功率，性能仅下降0.2/0.3/0.6%；在视觉定位任务中降低90%攻击成功率，性能还提升0.3%

Conclusion: FDA能有效提升VLM的鲁棒性，同时保持性能，具有良好的可扩展性、泛化能力和零样本性能

Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.

</details>


### [130] [PINE: Pipeline for Important Node Exploration in Attributed Networks](https://arxiv.org/abs/2512.07244)
*Elizaveta Kovtun,Maksim Makarenko,Natalia Semenova,Alexey Zaytsev,Semen Budennyy*

Main category: cs.LG

TL;DR: PINE是一种无监督的、属性感知的图节点重要性评估方法，通过注意力机制结合节点语义特征和网络结构来识别重要节点。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如节点度、PageRank）仅考虑网络结构而忽略节点属性，而现有神经网络方法需要监督学习。需要一种既能利用节点语义特征又无需监督的方法来识别图中的重要节点。

Method: 提出PINE（Pipeline for Important Node Exploration）框架，核心是基于注意力的图模型，将节点语义特征融入图结构属性的学习过程中，通过注意力分布计算节点重要性分数。

Result: PINE在各种同质和异质属性网络上表现出优越性能，并已作为工业系统应用于大规模企业图中关键实体的无监督识别。

Conclusion: PINE填补了无监督且属性感知的节点重要性评估方法的空白，通过结合语义特征和网络结构，有效识别图中的重要节点，具有实际工业应用价值。

Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.

</details>


### [131] [IFFair: Influence Function-driven Sample Reweighting for Fair Classification](https://arxiv.org/abs/2512.07249)
*Jingran Yang,Min Zhang,Lingfeng Zhang,Zhaohui Wang,Yonggang Zhang*

Main category: cs.LG

TL;DR: 提出基于影响函数的预处理方法IFFair，通过动态调整训练样本权重来减少机器学习算法中的偏见，无需修改网络结构或数据特征


<details>
  <summary>Details</summary>
Motivation: 机器学习在辅助或替代人类决策时，基于数据的模式会学习甚至加剧样本中的潜在偏见，导致对某些弱势群体的歧视性决策，损害社会福祉并阻碍相关应用发展

Method: 基于影响函数的预处理方法IFFair，利用不同群体训练样本的影响差异作为指导，在训练过程中动态调整样本权重，不修改网络结构、数据特征和决策边界

Result: 在多个真实数据集和指标上的实验表明，IFFair能够缓解分类设置中的多种偏见指标（人口统计均等、均衡机会、机会均等和错误率均等），且相比之前的预处理方法在效用和公平性指标间取得更好的权衡

Conclusion: IFFair作为一种有效的预处理公平性优化方法，能够在不修改模型结构的情况下通过动态权重调整减少算法偏见，在公平性和模型性能间实现良好平衡

Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.

</details>


### [132] [SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents](https://arxiv.org/abs/2512.07287)
*Sijia Li,Yuchen Huang,Zifan Liu,Zijian Li,Jingjing fu,Lei Song,Jiang Bian,Jun Zhang,Rui Wang*

Main category: cs.LG

TL;DR: SIT-Graph：一种增强多轮工具使用的状态集成工具图方法，通过利用部分重叠经验，结合情景记忆和程序记忆，在需要时检索状态摘要，在常规步骤中遵循工具依赖关系。


<details>
  <summary>Details</summary>
Motivation: 多轮工具使用场景仍然具有挑战性，因为意图是逐步澄清的，环境随着每个工具调用而演变。现有LLM代理要么将整个轨迹或预定义子任务视为不可分割单元，要么仅利用工具到工具的依赖关系，难以适应状态和信息在轮次间的演变。

Method: 提出状态集成工具图（SIT-Graph），从历史轨迹中捕获紧凑状态表示（情景式片段）和工具到工具的依赖关系（程序式例程）。首先从累积的工具使用序列构建工具图，然后为每条边增强一个可能影响下一个动作的对话和工具历史的紧凑状态摘要。在推理时，SIT-Graph实现情景回忆和程序执行之间的平衡。

Result: 在多个有状态的多轮工具使用基准测试中，SIT-Graph始终优于强大的基于记忆和图的方法基线，提供更稳健的工具选择和更有效的经验转移。

Conclusion: SIT-Graph通过利用部分重叠经验，结合情景记忆和程序记忆的灵感，有效解决了多轮工具使用的挑战，在需要上下文回忆时检索状态摘要，在常规步骤中遵循工具依赖关系，实现了更优的性能。

Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.

</details>


### [133] [Towards a Relationship-Aware Transformer for Tabular Data](https://arxiv.org/abs/2512.07310)
*Andrei V. Konstantinov,Valerii A. Zuev,Lev V. Utkin*

Main category: cs.LG

TL;DR: 提出基于改进注意力机制的模型，用于处理表格数据中的样本依赖关系，在回归和因果效应估计任务中表现优于梯度提升决策树。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型无法处理表格数据中样本间的外部依赖关系（如图结构），而图神经网络仅考虑相邻节点，难以应用于稀疏图。需要一种能有效利用样本间关系的模型。

Method: 提出基于改进注意力机制的解决方案，通过在注意力矩阵中添加额外项来考虑数据点间的可能关系。设计了多个模型变体，并与梯度提升决策树进行对比。

Result: 在合成和真实世界数据集的回归任务中，以及在IHDP数据集上的因果效应估计任务中，所提模型表现优于梯度提升决策树。

Conclusion: 基于改进注意力机制的模型能有效利用样本间依赖关系，在处理表格数据时优于传统方法，特别适用于需要考虑样本相关性的任务如因果效应估计。

Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.

</details>


### [134] [Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach](https://arxiv.org/abs/2512.07313)
*Bosun Kang,Hyejun Park,Chenglin Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯决策和机器学习预测的滑雪租赁问题新框架，通过维护精确的后验分布实现不确定性量化，在准确先验下达到接近最优性能，同时保持鲁棒的worst-case保证。


<details>
  <summary>Details</summary>
Motivation: 传统滑雪租赁问题算法专注于最小化最坏情况成本，而近期学习增强方法利用噪声预测但缺乏对不确定性的系统处理。本文旨在统一这两种视角，通过贝叶斯框架实现更原则性的不确定性量化和专家先验的整合。

Method: 提出了离散贝叶斯框架，维护时间范围上的精确后验分布，支持不确定性量化和专家先验的无缝整合。算法能够优雅地在最坏情况和完全知情设置之间插值，并自然扩展到多预测、非均匀先验和上下文信息。

Result: 算法实现了先验依赖的竞争性保证，在广泛实验评估中表现出优越的实证性能：在准确先验下达到接近最优结果，同时保持鲁棒的最坏情况保证。

Conclusion: 贝叶斯推理框架为具有不完美预测的在线决策问题提供了实际优势，能够统一传统算法和学习增强方法，实现原则性的不确定性量化和鲁棒性能保证。

Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.

</details>


### [135] [Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach](https://arxiv.org/abs/2512.07332)
*Zhengquan Luo,Guy Tadmor,Or Amar,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: RicciKGE：通过将知识图谱嵌入损失梯度与局部曲率耦合在扩展的Ricci流中，实现实体嵌入与底层流形几何的共同演化，自适应处理知识图谱的异质曲率结构。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法将所有实体放置在同质流形（欧几里得、球面、双曲或其乘积/多曲率变体）上，但预定义的同质流形无法适应真实图谱中局部区域表现出的急剧变化的曲率。这种几何先验与知识图谱局部曲率的不匹配会扭曲实体间距离并损害嵌入的表达能力。

Method: 提出RicciKGE方法，将KGE损失梯度与局部曲率耦合在扩展的Ricci流中，使实体嵌入与底层流形几何动态共同演化，实现相互适应。理论上证明了当耦合系数有界且适当选择时，所有边曲率呈指数衰减，流形被驱动向欧几里得平坦化，同时KGE距离严格收敛到全局最优。

Result: 在链接预测和节点分类基准测试中，RicciKGE表现出改进效果，证明了该方法在适应异质知识图谱结构方面的有效性。

Conclusion: RicciKGE通过将嵌入优化与几何演化相结合，能够自适应地处理知识图谱的异质曲率结构，相比固定几何的嵌入方法具有更好的表达能力和性能。

Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.

</details>


### [136] [Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning](https://arxiv.org/abs/2512.07374)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: R2F提出了一种基于LoRA适配器重建完整模型梯度方向的高效大语言模型遗忘方法，无需完整模型微调或原始训练数据


<details>
  <summary>Details</summary>
Motivation: 大基础模型（如LLMs）的遗忘对于动态知识更新、数据删除权利和模型行为修正至关重要，但现有方法需要完整模型微调或原始训练数据，限制了可扩展性和实用性

Method: Recover-to-Forget (R2F)框架：使用多个改写提示计算LoRA参数的梯度，训练梯度解码器近似对应的完整模型梯度；在代理模型上训练解码器并迁移到目标模型，实现跨模型泛化

Result: R2F实现了有效的遗忘同时保持一般模型性能，为预训练LLMs提供了可扩展、轻量级的遗忘替代方案，无需完整重新训练或访问内部参数

Conclusion: R2F通过从低秩LoRA适配器更新重建完整模型梯度方向，为大语言模型遗忘提供了高效实用的解决方案，特别适用于大型或黑盒模型

Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.

</details>


### [137] [LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples](https://arxiv.org/abs/2512.07375)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: LUNE：基于LoRA的轻量级遗忘框架，通过仅更新低秩适配器实现高效知识移除，计算成本降低约一个数量级


<details>
  <summary>Details</summary>
Motivation: 大语言模型难以移除特定信息，传统遗忘方法计算成本高，不适用于实际部署。需要轻量级解决方案来处理隐私、偏见缓解和知识修正问题。

Method: 基于LoRA的负向遗忘框架，仅更新低秩适配器而冻结主干模型，通过针对中间表示来抑制或替换目标知识，实现局部化编辑。

Result: 在多个事实遗忘任务上，LUNE达到与全微调和记忆编辑方法相当的效果，同时计算成本降低约一个数量级。

Conclusion: LUNE提供了一种高效、轻量级的模型遗忘解决方案，通过局部化编辑实现知识移除，为实际部署提供了可行性。

Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.

</details>


### [138] [Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood](https://arxiv.org/abs/2512.07390)
*Gilhyun Nam,Taewon Kim,Joonhyun Jeong,Eunho Yang*

Main category: cs.LG

TL;DR: SICL是一个基于风格不变性的测试时适应不确定性校准框架，无需反向传播，通过风格变换预测一致性来估计实例正确性概率。


<details>
  <summary>Details</summary>
Motivation: 测试时适应（TTA）方法在实际部署中常导致预测不确定性校准不佳，而现有校准方法假设固定模型或静态分布，无法适应动态测试条件。

Method: 提出SICL框架，利用风格不变性进行鲁棒不确定性估计。通过测量模型在风格变换变体上的预测一致性来估计实例正确性概率，仅需前向传播，无需反向传播。

Result: 在4个基线、5种TTA方法和2个现实场景、3种模型架构的综合评估中，SICL相比传统校准方法平均降低13个百分点的校准误差。

Conclusion: SICL提供了一个即插即用的不确定性校准模块，兼容任何TTA方法，显著提升了在动态测试条件下的预测不确定性校准性能。

Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.

</details>


### [139] [Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects](https://arxiv.org/abs/2512.07393)
*Yann Bourdin,Pierrick Legrand,Fanny Roche*

Main category: cs.LG

TL;DR: 论文研究了在数字音频效果建模中优化截断反向传播通过时间(TBPTT)的方法，重点关注动态范围压缩效果，通过调整TBPTT关键超参数提升模型性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 数字音频效果建模中，特别是动态范围压缩等复杂效果，需要有效的训练方法。TBPTT是训练循环神经网络的重要技术，但其超参数设置对模型性能和计算效率有重要影响，需要系统研究如何优化这些参数。

Method: 采用卷积-循环混合架构，系统评估TBPTT的三个关键超参数：序列数量、批大小和序列长度。在有无用户控制条件的数据集上进行大量实验，分析这些参数对模型性能的影响。

Result: 精心调整TBPTT超参数能显著提升模型精度和训练稳定性，同时降低计算需求。客观评估显示优化设置能改善性能，主观听音测试表明优化后的TBPTT配置保持了高感知质量。

Conclusion: TBPTT超参数优化对数字音频效果建模至关重要，特别是序列数量、批大小和序列长度的合理配置能平衡模型性能、训练稳定性和计算效率，为音频效果建模提供了实用的训练指导。

Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.

</details>


### [140] [Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse](https://arxiv.org/abs/2512.07400)
*Giulia Lanzillotta,Damiano Meier,Thomas Hofmann*

Main category: cs.LG

TL;DR: 论文揭示了持续学习中深度特征遗忘与浅层分类器遗忘的差异，发现小缓冲区足以防止特征漂移但需要大缓冲区来缓解分类器遗忘，提出通过修正统计伪影而非依赖大缓冲区来提升性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中的一个持久悖论是：神经网络即使输出预测失败，也经常保留过去任务的线性可分表示。本文旨在形式化这种深度特征空间与浅层分类器级别遗忘的区别，并解释为什么经验回放中缓冲区大小对这两种遗忘的影响不同。

Method: 将神经坍缩框架扩展到顺序设置，将深度遗忘形式化为几何漂移，证明任何非零回放分数都能保证线性可分性的保留。分析小缓冲区导致的"强坍缩"如何产生秩不足协方差和膨胀的类均值，使分类器对真实总体边界"失明"。

Result: 揭示了经验回放中的关键不对称性：最小缓冲区成功锚定特征几何并防止深度遗忘，但缓解浅层遗忘通常需要大得多的缓冲区容量。将持续学习与分布外检测统一起来，挑战了对大缓冲区的普遍依赖。

Conclusion: 通过显式修正统计伪影（秩不足协方差和膨胀类均值），可以在最小回放的情况下实现鲁棒性能，这为持续学习提供了新方向，减少了对大缓冲区的依赖。

Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.

</details>


### [141] [Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.07417)
*Giray Önür,Azita Dabiri,Bart De Schutter*

Main category: cs.LG

TL;DR: 提出一个多智能体强化学习框架，通过自适应调整状态反馈交通控制器的参数，结合状态反馈控制器的反应性和强化学习的适应性，提高交通控制效率。


<details>
  <summary>Details</summary>
Motivation: 传统交通管理策略（如路线引导、匝道调节、交通信号控制）依赖状态反馈控制器，虽然简单反应快，但缺乏应对复杂时变交通动态的适应性。

Method: 提出多智能体强化学习框架，每个智能体自适应调整状态反馈交通控制器的参数（而非直接高频控制），低频参数调整提高训练效率，多智能体结构增强系统鲁棒性。

Result: 在模拟多类别交通网络中评估，结果显示该框架优于无控制和固定参数状态反馈控制，与单智能体RL自适应状态反馈控制性能相当，但对部分故障有更好的恢复能力。

Conclusion: 多智能体强化学习框架成功结合了状态反馈控制的反应性和强化学习的适应性，在保持训练效率的同时提高了系统对交通条件变化的适应性和对部分故障的恢复能力。

Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.

</details>


### [142] [Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models](https://arxiv.org/abs/2512.07419)
*Haidong Kang,Jun Du,Lihong Lin*

Main category: cs.LG

TL;DR: 提出TAP框架，利用大语言模型自动发现混合精度量化的训练无关代理，无需人工专家参与，通过强化学习优化提示词提升LLM推理能力，实现最先进的量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法要么依赖昂贵的可微分优化（效率低、灵活性差），要么需要人工专家设计代理（劳动密集且需要专业知识）。能否设计一个无需人工专家参与和训练的代理？

Method: 提出TAP框架：1) 利用大语言模型自动发现适合混合精度量化的训练无关代理；2) 提出基于直接策略优化的强化学习来优化提示词，增强LLM推理能力，在LLM和MPQ任务间建立正反馈循环。

Result: 在主流基准测试上的大量实验表明，TAP实现了最先进的性能。

Conclusion: TAP通过LLM驱动的设计算法为MPQ社区提供了新视角，将显著推动该领域发展。

Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.

</details>


### [143] [MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis](https://arxiv.org/abs/2512.07430)
*Yangle Li,Danli Luo,Haifeng Hu*

Main category: cs.LG

TL;DR: 提出MIDG框架，通过混合不变专家模型提取领域不变特征，增强模态间协同关系，并设计跨模态适配器通过知识注入增强多模态表示语义丰富度，在领域泛化任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析的领域泛化方法在提取不变特征时忽略了模态间的协同作用，无法准确捕捉多模态数据的丰富语义信息。同时，现有的知识注入技术存在跨模态知识碎片化问题，忽略了超越单模态界限的特定表示。

Method: 1. 混合不变专家模型：提取领域不变特征，增强模型学习模态间协同关系的能力；2. 跨模态适配器：通过跨模态知识注入增强多模态表示的语义丰富度。

Result: 在三个数据集上进行的广泛领域实验表明，提出的MIDG框架取得了优越的性能表现。

Conclusion: 提出的MIDG框架通过增强模态间协同关系和跨模态知识注入，有效解决了多模态情感分析领域泛化中的关键问题，提升了模型性能。

Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.

</details>


### [144] [Mitigating Bias in Graph Hyperdimensional Computing](https://arxiv.org/abs/2512.07433)
*Yezi Liu,William Youngwoo Chung,Yang Ni,Hanning Chen,Mohsen Imani*

Main category: cs.LG

TL;DR: FairGHDC：一种用于图超维计算的公平感知训练框架，通过引入偏差校正项和公平因子来减少群体偏见，同时保持计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 图超维计算（HDC）在认知任务中表现出潜力，但其公平性影响尚未被充分研究。数据表示和决策规则中的偏见可能导致对不同群体的不平等对待，需要开发公平性解决方案。

Method: 提出FairGHDC框架，引入基于差距的人口统计奇偶正则化器的偏差校正项，将其转换为标量公平因子，用于缩放真实标签类超向量的更新。直接在超向量空间中进行去偏，无需修改图编码器或反向传播。

Result: 在六个基准数据集上的实验表明，FairGHDC显著减少了人口统计奇偶和机会均等差距，同时保持与标准GNN和公平感知GNN相当的准确性。在GPU上训练时间比GNN基线快约10倍。

Conclusion: FairGHDC成功解决了图HDC中的公平性问题，在保持计算效率优势的同时有效减少偏见，为公平感知的图超维计算提供了实用框架。

Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.

</details>


### [145] [KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models](https://arxiv.org/abs/2512.07437)
*Chenwei Shi,Xueyu Luan*

Main category: cs.LG

TL;DR: KAN-Dreamer将KAN和FastKAN架构集成到DreamerV3框架中，替换部分MLP和卷积组件，在DeepMind Control Suite上实现了与原始MLP架构相当的性能。


<details>
  <summary>Details</summary>
Motivation: 结合DreamerV3的卓越样本效率和KAN网络的参数效率与可解释性优势，同时通过FastKAN变体解决KAN的计算开销问题，探索在模型强化学习中应用新型网络架构的可能性。

Method: 提出KAN-Dreamer，将DreamerV3中的特定MLP和卷积组件替换为KAN和FastKAN层；在JAX-based World Model中实现完全向量化版本并简化网格管理；将研究分为视觉感知、潜在预测和行为学习三个子系统。

Result: 在DeepMind Control Suite (walker_walk)上的实验表明，使用适配的FastKAN作为奖励和继续预测器的替代方案，在样本效率、训练时间和渐近性能方面与原始MLP架构保持相当水平。

Conclusion: KAN架构可以成功集成到DreamerV3框架中，作为MLP的替代方案而不损失性能，这为未来基于KAN的世界模型开发提供了初步研究基础。

Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.

</details>


### [146] [Forget and Explain: Transparent Verification of GNN Unlearning](https://arxiv.org/abs/2512.07450)
*Imran Ahsan,Hyunwook Yu,Jinsung Kim,Mucheol Kim*

Main category: cs.LG

TL;DR: 提出基于可解释性的GNN遗忘验证器，通过对比删除前后的模型快照，使用属性偏移和局部结构变化作为透明证据来验证遗忘是否真正发生。


<details>
  <summary>Details</summary>
Motivation: 现有GNN遗忘方法主要关注效率和可扩展性，但缺乏透明度，难以验证遗忘是否真正发生，特别是在GDPR等隐私法规要求下。

Method: 提出可解释性驱动的验证器，对比删除前后的模型快照，使用五种可解释性指标：残差属性、热图偏移、可解释性分数偏差、图编辑距离和诊断图规则偏移。

Result: 评估两种骨干网络（GCN、GAT）和四种遗忘策略（Retrain、GraphEditor、GNNDelete、IDEA）在五个基准数据集上。结果显示Retrain和GNNDelete实现近乎完全遗忘，GraphEditor提供部分擦除，IDEA留下残差信号。

Conclusion: 解释性差异提供了主要的人类可读遗忘证据，同时成员推断ROC-AUC作为补充的图级隐私信号，为GNN遗忘提供了透明验证框架。

Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.

</details>


### [147] [Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification](https://arxiv.org/abs/2512.07463)
*Rongmei Liang,Zizheng Liu,Xiaofei Wu,Jingwen Tu*

Main category: cs.LG

TL;DR: 提出基于共识结构的统一优化框架，开发分布式并行ADMM算法处理分布式存储大数据中的组合正则化支持向量机，引入高斯回代法保证收敛，并应用于音乐信息检索。


<details>
  <summary>Details</summary>
Motivation: 在人工智能快速发展的时代，组合正则化支持向量机（CR-SVMs）能有效处理数据特征间的结构信息，但在分布式存储的大数据场景中缺乏高效算法。

Method: 提出基于共识结构的统一优化框架，适用于多种损失函数和组合正则项，并可扩展到非凸正则项。基于该框架开发分布式并行ADMM算法，引入高斯回代法保证收敛，并提出了稀疏群套索支持向量机（SGL-SVM）新模型。

Result: 理论分析表明算法计算复杂度不受不同正则项和损失函数影响，具有普适性。在合成和免费音乐档案数据集上的实验验证了算法的可靠性、稳定性和效率。

Conclusion: 提出的统一优化框架和分布式并行ADMM算法能有效解决分布式存储大数据中CR-SVMs的计算问题，算法具有强扩展性和普适性，在音乐信息检索等应用中表现良好。

Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.

</details>


### [148] [Materium: An Autoregressive Approach for Material Generation](https://arxiv.org/abs/2512.07486)
*Niklas Dobberstein,Jan Hamaekers*

Main category: cs.LG

TL;DR: Materium是一种基于自回归Transformer的晶体结构生成模型，通过将3D材料表示转换为token序列来快速生成精确的晶体结构。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法需要大量去噪步骤迭代优化原子位置，生成速度慢。需要开发一种能够快速、可扩展生成精确晶体结构的替代方法。

Method: 将3D材料表示（元素及氧化态、分数坐标、晶格参数）转换为token序列，使用自回归Transformer进行生成。与扩散方法不同，直接生成精确的分数坐标。

Result: 模型可在单GPU上几小时内完成训练，在GPU和CPU上生成速度远快于扩散方法。在单一和组合条件（密度、空间群、带隙、磁密度等）下均表现良好。

Conclusion: Materium提供了一种快速、可扩展的晶体结构生成方法，能够根据多种物理性质条件生成符合要求的候选结构，为材料发现提供了高效工具。

Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.

</details>


### [149] [Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent](https://arxiv.org/abs/2512.07490)
*Zhiyu Liu,Zhi Han,Yandong Tang,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出交替预条件梯度下降算法，解决低管秩张量估计中过参数化导致的收敛问题，实现与张量条件数无关的线性收敛。


<details>
  <summary>Details</summary>
Motivation: 传统张量奇异值分解计算昂贵，不适用于大规模张量。现有基于梯度下降的因子分解方法需要准确估计张量秩，当秩被高估时收敛显著减慢甚至发散。

Method: 提出交替预条件梯度下降算法，在原梯度基础上添加预条件项，交替更新两个因子张量，加速过参数化设置下的收敛。

Result: 在特定几何假设下，为一般低管秩张量估计问题建立线性收敛保证。对低管秩张量分解和恢复的具体案例分析显示，APGD在过参数化下仍能实现线性收敛，且收敛率与张量条件数无关。

Conclusion: APGD算法有效解决了低管秩张量估计中过参数化导致的收敛问题，在合成数据上的广泛模拟验证了理论断言。

Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.

</details>


### [150] [Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces](https://arxiv.org/abs/2512.07509)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 论文提出使用预定义向量系统（如An根系统）作为潜在空间配置目标，无需分类层即可训练分类器，特别适用于大规模类别数据集，并能加速训练收敛。


<details>
  <summary>Details</summary>
Motivation: 神经网络性能与潜在空间嵌入分布特性密切相关。传统方法在处理大规模类别数据集时面临挑战，需要更高效的潜在空间配置方法来加速训练并减少计算资源消耗。

Method: 使用预定义向量系统（包括An根系统）作为潜在空间配置目标，构建编码器和视觉变换器的潜在空间结构，通过最小化潜在空间维度来加速收敛。

Result: 在ImageNet-1K和50k-600k类别数据集上显著加速了训练收敛，同时最小化潜在空间维度的方法有助于减少向量数据库的存储需求。

Conclusion: 预定义向量系统为大规模类别分类任务提供了高效的潜在空间配置方案，既能加速训练收敛，又能减少存储需求，具有实际应用价值。

Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.

</details>


### [151] [Machine Learning: Progress and Prospects](https://arxiv.org/abs/2512.07519)
*Alexander Gammerman*

Main category: cs.LG

TL;DR: 这篇1996年的就职演讲追溯了机器学习的历史起源，从1949年香农的象棋学习算法到亚里士多德的实践学习理念，探讨了机器学习作为多学科交叉领域的本质。


<details>
  <summary>Details</summary>
Motivation: 演讲旨在探讨机器学习的起源和发展，通过历史视角展示这一领域如何从不同学科和哲学思想中演变而来，强调其跨学科特性和多元发展脉络。

Method: 采用历史回顾和哲学思辨的方法，从多个时间节点（14世纪奥卡姆剃刀、18世纪休谟归纳法、20世纪统计方法等）追溯机器学习思想的起源，并分析不同研究方向的平行发展。

Result: 揭示了机器学习并非单一学科的产物，而是融合了哲学、统计学、计算机科学等多领域思想，形成了包含归纳学习、神经网络、聚类等多种研究方向的异质性领域。

Conclusion: 机器学习的思想根源可以追溯到古代哲学，其现代发展体现了多学科交叉融合的特点，是一个包含多个平行研究方向的不完全同质化领域。

Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.

</details>


### [152] [Model-Based Reinforcement Learning Under Confounding](https://arxiv.org/abs/2512.07528)
*Nishanth Venkatesh,Andreas A. Malikopoulos*

Main category: cs.LG

TL;DR: 提出一种在上下文未观测的混淆环境中进行模型强化学习的方法，通过代理变量和近端策略评估解决混淆问题


<details>
  <summary>Details</summary>
Motivation: 在上下文未观测的C-MDPs中，传统模型学习方法存在根本性不一致问题，因为行为策略下的转移和奖励机制与评估状态策略所需的干预量不对应

Method: 采用近端策略评估方法，利用代理变量的可逆性条件识别混淆奖励期望；结合行为平均转移模型构建代理MDP，其Bellman算子对状态策略一致且可定义

Result: 该方法能够与最大因果熵模型学习框架无缝集成，为混淆环境中的模型学习和规划提供理论基础

Conclusion: 提出的方法能够在上下文未观测、不可用或难以收集的混淆环境中实现原则性的模型学习和规划

Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.

</details>


### [153] [FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting](https://arxiv.org/abs/2512.07539)
*Qingyuan Yang,Shizhuo,Dongyue Chen,Da Teng,Zehua Gan*

Main category: cs.LG

TL;DR: FRWKV：结合线性注意力与频域分析的时间序列预测框架，将计算复杂度从O(T²)降低到O(T)，在八个真实数据集上取得最佳平均排名。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长序列时间序列预测中存在两个主要瓶颈：1）二次复杂度O(T²)导致计算效率低下；2）未能有效利用频域信息。需要一种既能降低计算复杂度又能充分利用频域特征的方法。

Method: 提出FRWKV框架，结合线性注意力机制与频域分析。受RWKV的线性注意力启发，在注意力路径实现O(T)复杂度，同时通过频域编码器提取频谱信息来增强时间特征表示。

Result: 在八个真实世界数据集上，FRWKV取得了第一的平均排名。消融研究证实了线性注意力和频域编码器两个组件的关键作用。

Conclusion: 这项工作展示了线性注意力与频域分析之间的强大协同作用，为可扩展的时间序列建模建立了新范式。代码已开源。

Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.

</details>


### [154] [RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems](https://arxiv.org/abs/2512.07542)
*Jad Mounayer,Sebastian Rodriguez,Jerome Tomezyk,Chady Ghnatios,Francisco Chinesta*

Main category: cs.LG

TL;DR: RRAEDy：一种无需预设潜在维度、自动发现合适维度并强制正则化线性动态的潜在空间动态系统模型


<details>
  <summary>Details</summary>
Motivation: 现有潜在空间动态系统模型需要预先固定潜在维度，依赖复杂的损失平衡来近似线性动态，且缺乏对潜在变量的正则化

Method: 基于秩约减自编码器(RRAE)，自动通过奇异值对潜在变量进行排序和剪枝，同时学习控制时间演化的潜在动态模态分解(DMD)算子

Result: 在Van der Pol振荡器、Burgers方程、2D Navier-Stokes和旋转高斯等基准测试中实现了准确鲁棒的预测

Conclusion: RRAEDy通过无结构但线性约束的公式，无需辅助损失或手动调参即可学习稳定低维动态，并可扩展处理参数化ODE

Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.

</details>


### [155] [ReLaX: Reasoning with Latent Exploration for Large Reasoning Models](https://arxiv.org/abs/2512.07558)
*Shimin Zhang,Xianwei Chen,Yufan Shen,Ziyuan Ye,Jibin Wu*

Main category: cs.LG

TL;DR: 本文提出ReLaX方法，通过分析大推理模型的潜在动态来调节探索与利用平衡，解决RLVR中的熵崩溃问题，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 虽然RLVR能增强大推理模型的推理能力，但常导致熵崩溃，造成策略过早收敛和性能饱和。现有方法主要操纵token级熵来促进探索，但作者认为token生成背后的潜在动态编码了更丰富的计算结构，可用于指导策略优化。

Method: 利用Koopman算子理论获得模型隐藏状态动态的线性化表示，提出动态谱离散度(DSD)指标量化潜在动态的异质性，作为策略探索的直接指标。基于此提出ReLaX范式，在策略优化中显式地结合潜在动态来调节探索与利用。

Result: 在多模态和纯文本推理基准测试上的综合实验表明，ReLaX显著缓解了过早收敛问题，并持续实现了最先进的性能。

Conclusion: 通过分析大推理模型的潜在动态来调节探索与利用平衡的ReLaX方法，有效解决了RLVR中的熵崩溃问题，提升了推理模型的性能表现。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.

</details>


### [156] [Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting](https://arxiv.org/abs/2512.07569)
*Joel Ekstrand,Tor Mattsson,Zahra Taghiyarrenani,Slawomir Nowaczyk,Jens Lundström,Mikael Lindén*

Main category: cs.LG

TL;DR: WECA通过加权对比学习对齐正常和异常增强的表示，在ATM现金物流等场景中提升异常条件下的多变量时间序列预测可靠性，相比基线在异常数据上SMAPE提升6.1个百分点，正常数据性能几乎不受影响。


<details>
  <summary>Details</summary>
Motivation: 现代深度预测模型在正常数据上表现良好，但在分布偏移（如ATM现金物流中的突发需求变化）发生时经常失败，需要提高异常条件下的预测可靠性。

Method: 提出加权对比适应（WECA），使用加权对比目标对齐正常和异常增强的表示，保留异常相关信息的同时保持良性变化下的一致性。

Result: 在全国ATM交易数据集上评估，WECA在异常影响数据上将SMAPE提升了6.1个百分点，相比正常训练的基线，在正常数据上性能下降可忽略不计。

Conclusion: WECA能够在保持常规操作性能的同时，显著提升异常条件下的预测可靠性，为ATM现金物流等实际应用提供了有效的解决方案。

Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.

</details>


### [157] [Time Series Foundation Models for Process Model Forecasting](https://arxiv.org/abs/2512.07624)
*Yongbo Yu,Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 本文首次系统评估时间序列基础模型在过程模型预测中的应用，发现预训练的时间序列基础模型在零样本和微调设置下均优于传统方法，展示了跨领域时间结构迁移的有效性。


<details>
  <summary>Details</summary>
Motivation: 过程模型预测旨在预测业务流程控制流结构随时间的变化，但现有机器学习方法因直接跟随关系时间序列的稀疏性和异质性而表现有限。本文探索使用预训练的时间序列基础模型作为替代方案。

Method: 使用真实事件日志生成的直接跟随关系时间序列，比较时间序列基础模型的零样本使用（无额外训练）与在PMF特定数据上微调的变体，并与传统和专门模型进行对比。

Result: 时间序列基础模型通常比传统模型获得更低的预测误差（MAE和RMSE），表明从非过程领域有效迁移了时间结构。微调虽能进一步提高准确性，但增益有限，零样本使用仍是强默认选择。

Conclusion: 时间序列基础模型在过程相关时间序列预测中展现出强大的泛化能力和数据效率，为零样本和微调设置下的过程模型预测提供了有效解决方案。

Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.

</details>


### [158] [A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance](https://arxiv.org/abs/2512.07647)
*Georgios Tzachristas,Lei Deng,Ioannis Tzachristas,Gong Zhang,Renhai Chen*

Main category: cs.LG

TL;DR: 提出了一种认证Top-k注意力截断的统一数学框架，量化分布和输出层面的近似误差，推导出基于排序logits的确定性误差界，并在高斯评分模型下给出闭式解。


<details>
  <summary>Details</summary>
Motivation: 注意力机制中的Top-k截断是提高Transformer效率的关键技术，但现有方法缺乏严格的误差分析。需要建立数学框架来量化截断误差，并提供可认证的误差界，以平衡计算效率与模型精度。

Method: 开发统一的数学框架，证明总变差距离等于丢弃的softmax尾部质量，并满足TV(P,Ŝ)=1-e^{-KL(Ŝ∥P)}。推导基于排序logits的非渐近确定性边界，包括单边界间隙、多间隙和分块变体。使用精确的头尾分解证明输出误差可分解为τ∥μ_tail-μ_head∥₂，其中τ=TV(P,Ŝ)。在高斯评分模型下推导闭式尾部质量和最小k_ε的渐近规则。

Result: 在bert-base-uncased和合成logits上的实验证实了k_ε/n的预测缩放规律，显示认证Top-k截断平均可将评分键减少2-4倍，同时满足规定的总变差预算。推导出输出误差的头尾直径界∥Attn(q,K,V)-Attn_k(q,K,V)∥₂≤τ diam_{H,T}，以及与Var_P(V)相关的细化结果。

Conclusion: 该框架为Top-k注意力截断提供了严格的误差分析和认证保证，建立了总变差距离与KL散度的精确关系，推导出实用的误差界，并在高斯模型下给出闭式解，为高效Transformer设计提供了理论指导。

Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=τ\|μ_{\mathrm{tail}}-μ_{\mathrm{head}}\|_2$ with $τ=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\leτ\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(μ,σ^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approxΦ_c(σ+Φ^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.

</details>


### [159] [Depth-Wise Activation Steering for Honest Language Models](https://arxiv.org/abs/2512.07667)
*Gracjan Góral,Marysia Winkels,Steven Basart*

Main category: cs.LG

TL;DR: 提出一种无需训练的激活引导方法，通过高斯调度在深度上分配引导强度，改善大语言模型的诚实性而非准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型有时会断言虚假信息，尽管内部表示正确答案，这是诚实性而非准确性的失败，损害了可审计性和安全性。现有方法主要优化事实正确性或依赖于重新训练和脆弱的单层编辑，对真实报告的控制有限。

Method: 提出无需训练的激活引导方法，使用高斯调度在神经网络深度上加权引导强度。该方法简单、模型无关、无需微调，为从模型现有能力中引出真实报告提供低成本控制旋钮。

Result: 在MASK基准测试中（分离诚实性与知识），评估了LLaMA、Qwen和Mistral家族的七个模型，发现高斯调度在六个模型中比无引导和单层基线提高了诚实性。在LLaMA-3.1-8B-Instruct和Qwen-2.5-7B-Instruct上的等预算消融实验显示，高斯调度优于随机、均匀和盒式滤波器深度分配，表明干预在深度上的分布方式对结果有实质性影响。

Conclusion: 高斯调度激活引导方法有效改善大语言模型的诚实性，提供了一种简单、模型无关、无需训练的干预方式，为引出模型真实报告能力提供了实用工具。

Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.

</details>


### [160] [In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models](https://arxiv.org/abs/2512.07705)
*Saroj Gopali,Bipin Chhetri,Deepika Giri,Sima Siami-Namini,Akbar Siami Namin*

Main category: cs.LG

TL;DR: 比较时间序列预测方法，发现TimesFM表现最佳，LLMs在零样本学习中也表现良好


<details>
  <summary>Details</summary>
Motivation: 研究基础模型（如LLMs和TimesFM）是否能在时间序列预测中超越传统方法（ARIMA、Transformer、LSTM、TCN）

Method: 使用上下文学习、零样本学习和少样本学习训练LLMs，测试OpenAI o4-mini、Gemini 2.5 Flash Lite、TimesFM，并与TCN和LSTM进行对比

Result: TimesFM表现最佳（RMSE 0.3023，推理时间266秒），OpenAI o4-mini在零样本学习中表现良好

Conclusion: 预训练的时间序列基础模型是实时预测的有前景方向，能够以最小模型适应实现准确且可扩展的部署

Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.

</details>


### [161] [Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity](https://arxiv.org/abs/2512.07723)
*Yonggeon Lee,Jibin Hwang,Alfred Malengo Kondoro,Juhyun Song,Youngtae Noh*

Main category: cs.LG

TL;DR: 提出基于Transformer的时间到事件模型，用于准确预测电动汽车出发时间，以优化充电策略延长电池寿命


<details>
  <summary>Details</summary>
Motivation: 电动汽车锂电池在高电量状态下会加速退化，可通过延迟充电至出发前进行缓解，但需要准确预测用户出发时间

Method: 基于Transformer的实时到事件模型，将每天表示为离散化的网格标记序列，利用流式上下文信息而非仅依赖历史模式

Result: 在93名用户的真实世界研究中，该方法能有效捕捉个体日常中的不规则出发模式，性能优于基线模型

Conclusion: 该方法具有实际部署潜力，可为可持续交通系统做出贡献，通过优化充电策略延长电池寿命

Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.

</details>


### [162] [A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data](https://arxiv.org/abs/2512.07741)
*Agnes Norbury,George Fairs,Alexandra L. Georgescu,Matthew M. Nour,Emilia Molimpakis,Stefano Goria*

Main category: cs.LG

TL;DR: 基于大规模语音数据集（30,135名说话者），使用贝叶斯网络模型预测抑郁和焦虑症状，实现高精度（ROC-AUC>0.74）和良好校准（ECE<0.02），并评估了人口统计学公平性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 临床精神病学评估中，医生需要整合患者的言语内容和非言语信号（语调、语速、流畅性、反应性、肢体语言等），这是一项具有挑战性的任务。虽然智能工具可以支持这一过程，但在临床实践中尚未实现。本研究旨在通过贝叶斯网络模型解决采用障碍，构建透明、可解释的评估支持工具。

Method: 采用贝叶斯网络建模方法，从大规模语音和言语特征数据集（30,135名独特说话者）中预测抑郁和焦虑症状。模型在症状层面而非疾病层面表示常见心理状况，评估了不同输入模态类型的整合与冗余，并分析了人口统计学公平性。

Result: 模型在抑郁和焦虑预测上表现优异（ROC-AUC分别为0.842和0.831，校准误差ECE分别为0.018和0.015），核心个体症状的ROC-AUC均超过0.74。研究还评估了人口统计学公平性，探索了临床实用性指标和心理健康服务使用者的接受度。

Conclusion: 当提供足够丰富的大规模多模态数据流，并在症状层面而非疾病层面表示常见心理状况时，贝叶斯网络模型是构建稳健评估支持工具的原则性方法。该方法能以透明、可解释的格式提供临床相关输出，并直接适用于专家临床监督。

Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.

</details>


### [163] [Formalized Hopfield Networks and Boltzmann Machines](https://arxiv.org/abs/2512.07766)
*Matteo Cipollina,Michail Karatarakis,Freek Wiedijk*

Main category: cs.LG

TL;DR: 该论文在Lean 4中形式化神经网络，包括确定性的Hopfield网络和随机网络，证明了收敛性、Hebbian学习正确性以及Boltzmann机器的遍历性。


<details>
  <summary>Details</summary>
Motivation: 神经网络应用广泛但分析和验证困难，需要形式化方法来确保其正确性和可靠性。

Method: 使用Lean 4定理证明器形式化神经网络，包括确定性Hopfield网络（证明收敛性和Hebbian学习正确性）和随机网络（形式化Boltzmann机器并证明遍历性）。

Result: 成功形式化了Hopfield网络的收敛性和Hebbian学习正确性（限于两两正交模式），以及Boltzmann机器的遍历性和收敛到唯一平稳分布。

Conclusion: 在Lean 4中形式化神经网络是可行的，为神经网络的形式验证提供了基础，特别是通过新的Perron-Frobenius定理形式化证明了随机网络的收敛性。

Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.

</details>


### [164] [GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory](https://arxiv.org/abs/2512.07782)
*Jiaxu Liu,Yuhe Bai,Christos-Savvas Bouganis*

Main category: cs.LG

TL;DR: GatedFWA：一种记忆门控的滑动窗口注意力机制，在保持线性时间复杂度的同时，通过可学习的衰减偏置稳定内存更新并控制梯度流动，解决了传统滑动窗口注意力训练目标无界和Softmax注意力内存收缩的问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的Softmax全注意力具有二次复杂度，而滑动窗口注意力（SWA）虽然实现了线性时间编码/解码，但在关联记忆解释下，其差分式更新导致训练目标无界。同时，Softmax注意力通过归一化更新会导致内存收缩和梯度消失问题。

Method: 提出GatedFWA（记忆门控的滑动窗口注意力），通过为每个token/head累积门控值形成衰减偏置，添加到注意力logits中，作为记忆递归中的可学习收缩机制。实现了融合的单通道门控预处理和与FlashAttention兼容的内核，在滑动掩码下注入门控，确保I/O效率和数值稳定性。

Result: 在语言建模基准测试中，GatedFWA以可忽略的开销实现了有竞争力的吞吐量，更好地利用了全局上下文，并能与NSA等token压缩/选择方法无缝集成，泛化到各种自回归领域。

Conclusion: GatedFWA在保持滑动窗口注意力效率的同时，通过记忆门控机制稳定了内存更新并控制了梯度流动，解决了现有注意力机制的关键限制，为高效的自回归建模提供了新的解决方案。

Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.

</details>


### [165] [Group Representational Position Encoding](https://arxiv.org/abs/2512.07805)
*Yifan Zhang,Zixiang Chen,Yifeng Liu,Zhen Qin,Huizhuo Yuan,Kangping Xu,Yang Yuan,Quanquan Gu,Andrew Chi-Chih Yao*

Main category: cs.LG

TL;DR: GRAPE是一个基于群作用的统一位置编码框架，包含乘法旋转和加法logit偏置两种机制，统一了RoPE和ALiBi等现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法如RoPE和ALiBi虽然有效，但缺乏统一的理论框架。GRAPE旨在通过群作用理论提供一个统一的位置编码设计空间，将不同方法纳入同一理论体系。

Method: 提出了基于群作用的GRAPE框架，包含两种机制：1）乘法GRAPE：使用SO(d)群中的旋转操作，通过矩阵指数实现位置映射；2）加法GRAPE：使用GL群中的幂幺作用，产生加法logit偏置。框架允许学习可交换子空间和非可交换混合来扩展几何表达能力。

Result: GRAPE框架统一了RoPE和ALiBi等现有位置编码方法作为特例，同时提供了更灵活的扩展能力。乘法GRAPE可以精确恢复RoPE，加法GRAPE可以精确恢复ALiBi和Forgetting Transformer。框架支持相对位置关系、组合性、范数保持和流式缓存等特性。

Conclusion: GRAPE为长上下文模型中的位置几何提供了一个原则性的设计空间，将现有方法统一在群作用理论框架下，并为设计新的位置编码机制提供了理论基础和扩展能力。

Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,ω\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.

</details>


### [166] [The Adoption and Usage of AI Agents: Early Evidence from Perplexity](https://arxiv.org/abs/2512.07828)
*Jeremy Yang,Noah Yonack,Kate Zyskowski,Denis Yarats,Johnny Ho,Jerry Ma*

Main category: cs.LG

TL;DR: 首个大规模AI智能体在开放网络环境中的采用、使用强度和使用场景实地研究，基于Perplexity的Comet浏览器及其Comet Assistant智能体的数亿匿名用户交互数据。


<details>
  <summary>Details</summary>
Motivation: 理解通用AI智能体在真实世界中的采用模式、使用强度和使用场景，填补对这类新兴AI能力大规模实证研究的空白。

Method: 基于Perplexity的Comet浏览器及其Comet Assistant智能体的数亿匿名用户交互数据，采用分层智能体分类法（主题、子主题、任务三个层级）系统分析使用场景。

Result: 发现采用和使用存在显著异质性：早期采用者、高GDP/教育水平国家用户、数字/知识密集型行业从业者更可能采用；生产力与工作流（57%）、学习与研究是主要主题；个人使用占55%，专业和教学分别占30%和16%；短期使用有粘性，长期趋向认知导向主题。

Conclusion: AI智能体的扩散对研究者、企业、政策制定者和教育者具有重要影响，需要进一步研究这类快速发展的AI能力。

Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [167] [Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions](https://arxiv.org/abs/2512.06270)
*Nifei Lin,Heng Luo,L. Jeff Hong*

Main category: stat.ML

TL;DR: 本文研究上下文强凸模拟优化，采用"先优化后预测"方法进行实时决策。离线阶段在协变量集上进行模拟优化以近似最优解函数；在线阶段通过评估该近似值获得决策。核心理论挑战是理解模拟优化算法产生的不精确解如何影响最优性差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了模拟优化算法产生的不精确解对最优性差距的影响，需要开发统一的分析框架来同时考虑解偏差和方差，以理解"先优化后预测"方法的最优性差距。

Method: 采用"先优化后预测"方法，离线阶段使用模拟优化算法（以Polyak-Ruppert平均SGD为例）在协变量集上近似最优解函数，在线阶段通过评估该近似值进行决策。开发统一分析框架，考虑解偏差和方差，分析四种平滑技术（k近邻、核平滑、线性回归、核岭回归）下的最优性差距。

Result: 建立了收敛速率，推导了计算预算Γ在协变量数量和每个协变量模拟努力之间的最优分配规则，证明在适当的平滑技术和样本分配规则下，收敛速率可近似达到Γ^{-1}。数值研究验证了理论发现，证明了所提方法的有效性和实用价值。

Conclusion: 本文开发了统一分析框架，首次系统考虑了模拟优化算法的不精确性对最优性差距的影响，为上下文强凸模拟优化的"先优化后预测"方法提供了理论保证和实用指导，特别是在计算资源分配和平滑技术选择方面。

Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $Γ$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $Γ^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.

</details>


### [168] [Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders](https://arxiv.org/abs/2512.06348)
*Xiaoyu Ma,Likun Zhang,Christopher K. Wikle*

Main category: stat.ML

TL;DR: 提出一种基于条件变分自编码器(cXVAE)的新方法，整合气候指数来建模时空极端事件，通过卷积神经网络捕捉空间依赖关系，实现高效模拟、检测条件驱动变化，并支持反事实实验评估极端风险。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件在农业、生态和气象等领域被广泛研究，其时空共现模式在气候变化条件下可能增强或减弱。现有方法需要更高效、可扩展的框架来建模极端事件的时空依赖关系，并量化气候条件对极端风险的影响。

Method: 提出条件变分自编码器(cXVAE)方法，在解码器中嵌入卷积神经网络(CNN)，将气候指数与潜在空间中的空间依赖关系进行卷积，使解码器依赖于气候变量。该方法能够高效模拟空间场、恢复时空变化的极端依赖结构。

Result: 通过广泛模拟验证，cXVAE能够准确模拟空间场并以极低的计算成本恢复时空变化的极端依赖结构。方法提供了简单可扩展的途径来检测条件驱动变化，并评估依赖结构是否对条件变量不变。当依赖结构对条件敏感时，支持反事实实验来量化联合尾部风险、共现范围和重现期指标的变化。

Conclusion: 该方法在澳大利亚东部2014-2024年月最大火灾天气指数(FWI)与ENSO指数的实际应用中展示了实用性和性能，为分析气候条件对极端事件时空依赖的影响提供了有效工具，支持风险评估和气候适应策略制定。

Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Niño/Southern Oscillation (ENSO) index.

</details>


### [169] [Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals](https://arxiv.org/abs/2512.06435)
*Mara Sherlin Talento,Jordan Richards,Raphael Huser,Hernando Ombao*

Main category: stat.ML

TL;DR: 提出了一种基于尾部极值依赖性的脑信号分析方法，用于识别癫痫发作等极端事件，通过尾部典型相关分析可视化极端通道贡献，并用于新生儿癫痫的准确分类。


<details>
  <summary>Details</summary>
Motivation: 现有脑连接性分析方法在识别极端事件（如癫痫发作）时存在局限，传统尾部依赖模型无法识别驱动最大尾部依赖的具体通道，而这对癫痫患者分析至关重要，因为特定通道负责癫痫发作。

Method: 将传统典型相关分析扩展到尾部领域，开发尾部典型依赖度量；通过尾部成对依赖矩阵（TPDM）开发计算高效的估计器；使用该方法进行基于频率的软聚类分析。

Result: 尾部连接性提供了额外的判别能力，能够更准确地识别极端相关事件并改善癫痫风险管理；方法成功用于新生儿癫痫的准确分类，区分有癫痫和无癫痫的新生儿。

Conclusion: 尾部依赖分析揭示了极端事件的独特特征，尾部典型相关方法能够可视化极端通道贡献，为癫痫等极端事件的识别和管理提供了更有效的工具。

Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.

</details>


### [170] [Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions](https://arxiv.org/abs/2512.06615)
*Kaichen Shen,Wei Zhu*

Main category: stat.ML

TL;DR: 提出LNDSM方法，将非线性前向动力学与VAE潜在SGM框架结合，通过欧拉-丸山方案重构交叉熵项，实现更快合成和更好分布学习


<details>
  <summary>Details</summary>
Motivation: 现有基于分数的生成模型在潜在空间训练时通常使用线性前向动力学，这限制了模型学习结构化分布的能力。需要一种方法将非线性动力学整合到潜在SGM框架中，以提高样本质量和多样性。

Method: 提出潜在非线性去噪分数匹配(LNDSM)，通过欧拉-丸山方案诱导的近似高斯转移来重构交叉熵项，将非线性前向动力学整合到VAE基础的潜在SGM框架中。为确保数值稳定性，识别并移除了两个零均值但方差爆炸的小时间步项。

Result: 在MNIST数据集变体上的实验表明，该方法实现了更快的合成速度，并增强了对固有结构化分布的学习能力。与基准的结构不可知潜在SGM相比，LNDSM在样本质量和多样性方面始终表现更优。

Conclusion: LNDSM成功地将非线性前向动力学整合到潜在SGM框架中，通过数值稳定的训练目标实现了更高效的生成建模，为学习结构化分布提供了有效解决方案。

Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.

</details>


### [171] [ADAM Optimization with Adaptive Batch Selection](https://arxiv.org/abs/2512.06795)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: 提出AdamCB方法，将组合老虎机采样技术集成到Adam优化器中，通过自适应选择样本提高训练效率


<details>
  <summary>Details</summary>
Motivation: 传统Adam优化器对所有样本平等对待，但不同样本对模型更新的影响程度不同，这导致收敛效率低下。现有的老虎机采样方法虽然有所改进，但理论保证有限。

Method: 提出Adam with Combinatorial Bandit Sampling (AdamCB)，将组合老虎机技术集成到Adam中，能够同时利用多个样本的反馈信息

Result: 理论分析显示AdamCB比包括之前老虎机变体在内的Adam方法收敛更快。数值实验证明AdamCB在性能上持续优于现有方法

Conclusion: AdamCB通过组合老虎机采样技术解决了传统Adam优化器的效率问题，提供了更好的理论保证和实际性能

Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.

</details>


### [172] [Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets](https://arxiv.org/abs/2512.06945)
*Nabil Alami,Jad Zakharia,Souhaib Ben Taieb*

Main category: stat.ML

TL;DR: 提出SACP方法，通过对称聚合函数将多个预测模型的非一致性分数转化为e值进行聚合，以生成更精确的预测集


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，针对同一任务训练多个预测模型越来越普遍。如何聚合这些模型的预测不确定性以产生可靠且高效的量化是一个关键但尚未充分探索的挑战，特别是在共形预测框架下。虽然共形预测方法可以从每个模型生成单独的预测集，但将它们组合成单个更信息丰富的集合仍然是一个难题。

Method: 提出SACP（对称聚合共形预测）方法，将多个预测器的非一致性分数聚合。SACP将这些分数转化为e值，并使用任何对称聚合函数进行组合。这种灵活设计使得能够选择产生更尖锐预测集的聚合策略。

Result: 在不同数据集上的广泛实验表明，SACP持续提高效率，并且通常优于最先进的模型聚合基线方法。

Conclusion: SACP提供了一个稳健、数据驱动的框架，用于聚合多个预测模型的不确定性，生成更精确的预测集，同时提供了理论见解来证明该方法的有效性和性能。

Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.

</details>


### [173] [PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios](https://arxiv.org/abs/2512.06950)
*Enrico Camporeale*

Main category: stat.ML

TL;DR: PARIS提出基于表示定理的数据集剪枝框架，通过计算闭式表示器删除残差来优化训练集，解决不平衡回归问题，在空间天气预测中减少75%训练数据的同时保持或改进RMSE。


<details>
  <summary>Details</summary>
Motivation: 标准经验风险最小化（ERM）在不平衡回归中偏向高频数据区域，导致对罕见但高影响的"尾部"事件预测性能严重下降。现有方法如损失重加权或合成过采样会引入噪声、扭曲分布或增加算法复杂度。

Method: PARIS利用神经网络表示定理计算闭式表示器删除残差，量化单个训练点移除对验证损失的确切影响而无需重新训练。结合高效的Cholesky秩一降阶方案，实现快速迭代剪枝，消除无信息或性能降低的样本。

Result: 在真实空间天气预测任务中，PARIS将训练集减少高达75%，同时保持或改进整体RMSE，优于重加权、合成过采样和提升基线方法。

Conclusion: 表示器引导的数据集剪枝是解决罕见事件回归问题的强大、可解释且计算高效的方法，通过优化训练集本身而非修改损失函数或采样策略来缓解不平衡问题。

Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.

</details>


### [174] [Statistical analysis of Inverse Entropy-regularized Reinforcement Learning](https://arxiv.org/abs/2512.06956)
*Denis Belomestny,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 提出统计框架解决逆强化学习的奖励函数不唯一问题，通过熵正则化和最小二乘重构得到唯一的最小二乘奖励，建立专家策略估计的统计收敛界。


<details>
  <summary>Details</summary>
Motivation: 传统逆强化学习中存在奖励函数不唯一的问题（许多奖励函数都能诱导出相同的最优策略），这使得逆问题不适定。需要开发一个统计框架来解决这种模糊性。

Method: 结合熵正则化和基于软贝尔曼残差的最小二乘奖励重构，得到唯一的最小二乘奖励。将专家演示建模为马尔可夫链，通过惩罚最大似然估计在动作空间的条件分布类中估计策略。

Result: 建立了估计策略与专家策略之间超额KL散度的高概率界，考虑了策略类覆盖数的统计复杂性。得到了最小二乘奖励函数的非渐近极小极大最优收敛率，揭示了平滑（熵正则化）、模型复杂性和样本量之间的相互作用。

Conclusion: 该框架解决了逆强化学习的奖励模糊性问题，连接了行为克隆、逆强化学习和现代统计学习理论，为逆熵正则化强化学习提供了统计理论基础。

Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $π^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.

</details>


### [175] [Learning Conditional Independence Differential Graphs From Time-Dependent Data](https://arxiv.org/abs/2512.06960)
*Jitendra K Tugnait*

Main category: stat.ML

TL;DR: 提出了一种估计两个时间序列高斯图模型条件独立图差异的方法，考虑了数据的时间依赖性，使用频域惩罚D-trace损失函数和ADMM算法进行优化


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注独立同分布数据的精度矩阵差异估计，但缺乏对时间依赖性数据的处理方法。本文旨在估计两个时间序列高斯图模型的逆功率谱密度差异，以表征时间依赖性数据中条件依赖关系的变化

Method: 使用频域惩罚D-trace损失函数方法进行差异图学习，采用Wirtinger微积分处理复数域优化。考虑凸（群lasso）和非凸（log-sum和SCAD群惩罚）正则化函数，提出ADMM算法优化目标函数

Result: 在高维设置下建立了收敛性和图恢复的充分条件。合成数据实验显示，log-sum惩罚的时间序列差异图估计器显著优于lasso方法，而lasso方法又显著优于现有的i.i.d.建模方法（以F1分数为性能指标）

Conclusion: 提出了一种有效估计时间序列高斯图模型条件独立图差异的方法，能够处理数据的时间依赖性，在合成和真实数据上都表现出优越性能，特别适用于分析时间依赖性数据中的条件依赖关系变化

Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.

</details>


### [176] [Exact Synthetic Populations for Scalable Societal and Market Modeling](https://arxiv.org/abs/2512.07306)
*Thierry Petit,Arnault Pachot*

Main category: stat.ML

TL;DR: 提出一个基于约束规划的合成人口生成框架，能够精确重现目标统计数据并确保个体一致性，无需微观数据


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法需要从样本推断分布，而本方法旨在直接编码聚合统计数据和结构关系，实现对人口特征的精确控制，同时保护个人隐私数据

Method: 采用约束规划框架，直接编码目标统计数据和结构约束，生成合成人口，确保个体层面的完全一致性，无需依赖任何微观数据

Result: 在官方人口数据源上验证了方法的有效性，研究了分布偏差对下游分析的影响，证明能够高精度重现目标统计特征

Conclusion: 该方法为合成人口生成提供了新范式，结合大语言模型查询功能，可用于社会行为建模、市场政策场景探索，提供可重复的决策级洞察而无需个人数据

Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.

</details>


### [177] [Machine learning in an expectation-maximisation framework for nowcasting](https://arxiv.org/abs/2512.07335)
*Paul Wilsens,Katrien Antonio,Gerda Claeskens*

Main category: stat.ML

TL;DR: 提出基于EM框架的nowcasting方法，使用机器学习技术建模事件发生和报告过程，能够处理高维协变量和非线性效应，在COVID-19病例报告应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 决策制定常面临信息不完整问题，导致风险估计不足或过高。实践中，信息不完整通常由报告或观察延迟引起。现有基于广义线性模型的EM框架在处理非线性效应和高维协变量时存在局限。

Method: 提出基于期望最大化（EM）框架的nowcasting方法，使用神经网络和极端梯度提升机（XGBoost）等机器学习技术建模事件发生和报告过程。允许包含事件发生期和报告期的协变量信息以及实体特征。定制化最大化步骤和EM迭代间的信息流以利用机器学习模型的预测能力。

Result: 模拟实验表明，该方法能有效建模高维协变量下的事件发生和报告过程。在非线性效应存在时，优于使用广义线性模型的现有EM框架。在阿根廷COVID-19病例报告应用中，XGBoost方法表现最佳。

Conclusion: 提出的EM框架结合机器学习技术能够有效处理nowcasting问题，特别是在存在非线性效应和高维协变量的复杂场景中，为决策制定提供更准确的风险估计。

Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.

</details>


### [178] [High-Dimensional Change Point Detection using Graph Spanning Ratio](https://arxiv.org/abs/2512.07541)
*Youngwen Sun,Katerina Papagiannouli,Vladimir Spokoiny*

Main category: stat.ML

TL;DR: 提出基于图的跨图算法，用于检测低维到高维数据的离线/在线变化，控制错误概率，在变化幅度超过最小分离率下界时具有高检测能力


<details>
  <summary>Details</summary>
Motivation: 需要一种通用的变化检测方法，能够处理从低维到高维的数据、适用于欧几里得和图结构数据、且分布未知，同时控制错误概率，特别适合在线环境需要及时精确检测

Method: 基于图的方法论，开发新颖的图跨越算法，适用于欧几里得和图结构数据，分布未知，能够控制错误概率，算法在变化幅度超过最小分离率下界时有效

Result: 算法在变化幅度超过最小分离率下界（量级为√(nd)）时具有高检测能力，在高斯和非高斯数据上都优于其他技术，即使在小观察窗口下也能保持强检测能力

Conclusion: 提出的图跨越算法是一种通用有效的变化检测方法，特别适合在线环境，能够在控制错误概率的同时实现及时精确的变化检测

Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.

</details>


### [179] [On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series](https://arxiv.org/abs/2512.07557)
*Jitendra K. Tugnait*

Main category: stat.ML

TL;DR: 提出多属性高斯时间序列条件独立图估计的统一理论框架，使用频域惩罚对数似然方法，分析凸与非凸惩罚函数的高维一致性、局部凸性和图恢复性能


<details>
  <summary>Details</summary>
Motivation: 现有图估计方法主要针对单属性模型（每个节点对应标量时间序列），而多属性图模型中每个节点代表随机向量或向量时间序列，需要开发适用于多属性依赖时间序列的统一理论框架

Method: 使用频域惩罚对数似然方法，通过离散傅里叶变换将时域数据转换到频域，考虑凸惩罚（稀疏组lasso）和非凸惩罚（log-sum和SCAD组惩罚）函数

Result: 在高维设置下建立了充分条件，证明逆功率谱密度在Frobenius范数下的收敛一致性、使用非凸惩罚时的局部凸性以及图恢复性能，无需不相干或不可表示性条件

Conclusion: 为多属性依赖时间序列图学习提供了统一的理论分析框架，通过频域方法和凸/非凸惩罚函数实现了高维一致性图估计，并通过合成和真实数据验证了方法的有效性

Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.

</details>


### [180] [$φ$-test: Global Feature Selection and Inference for Shapley Additive Explanations](https://arxiv.org/abs/2512.07578)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: stat.ML

TL;DR: φ-test是一种结合Shapley归因和选择性推断的全局特征选择和显著性检验方法，用于黑盒预测器，能输出特征重要性分数、代理系数以及后选择p值和置信区间。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒模型的特征重要性解释方法缺乏统计推断支持，无法提供显著性检验和置信区间。需要一种方法既能保持模型预测能力，又能提供统计可靠的特征重要性评估。

Method: 结合SHAP归因进行特征筛选，然后通过具有可处理选择性推断形式的选择规则拟合线性代理模型。对保留的特征输出Shapley全局分数、代理系数以及后选择p值和置信区间。

Result: 在真实表格回归任务中，φ-test能保留原始模型大部分预测能力，仅使用少量特征，且特征集在不同重采样和骨干模型类别间保持相对稳定。

Conclusion: φ-test作为实用的全局解释层，将基于Shapley的重要性总结与经典统计推断联系起来，为黑盒模型提供统计可靠的特征重要性评估。

Abstract: We propose $φ$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $φ$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $φ$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $φ$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.

</details>


### [181] [Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion](https://arxiv.org/abs/2512.07755)
*Brenda Anague,Bamdad Hosseini,Issa Karambal,Jean Medard Ngnotchouye*

Main category: stat.ML

TL;DR: 提出基于神经正切核的加权自适应PINNs方法，用于解决具有未知速度和扩散系数的2D/3D对流扩散方程中的源反演和参数估计问题


<details>
  <summary>Details</summary>
Motivation: 在大气科学和环境监测中，从稀疏数据中同时估计排放源位置以及控制速度剖面和扩散参数的多个模型参数是一个困难任务。传统方法难以处理这种高度不适定的反问题。

Method: 扩展物理信息神经网络（PINNs），提出基于神经正切核的加权自适应方法，将解、源项和未知参数联合恢复，利用偏微分方程作为约束耦合多个未知函数参数

Result: 数值实验表明，该方法在不同类型的实际工程系统测量数据下都能成功实现源反演和参数估计，并且对测量噪声具有鲁棒性

Conclusion: 该方法成功地将PINNs扩展到高度不适定的源反演和参数估计问题，通过联合恢复策略更有效地利用有限测量信息，为解决工程和科学计算中的复杂反问题提供了有效工具

Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.

</details>


### [182] [Distribution-informed Online Conformal Prediction](https://arxiv.org/abs/2512.07770)
*Dongjian Hu,Junxi Wu,Shu-Tao Xia,Changliang Zou*

Main category: stat.ML

TL;DR: COP是一种在线共形预测算法，通过将数据模式纳入更新规则，在存在可预测模式时产生更紧的预测集，同时保持有效的覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 现有在线共形预测方法在完全对抗环境中处理数据分布漂移时，往往产生过于保守的预测集。需要一种能够利用数据潜在模式来减少保守性的方法。

Method: 提出Conformal Optimistic Prediction (COP)算法，通过非一致性分数的累积分布函数估计，将底层数据模式纳入更新规则。算法在存在可预测模式时产生更紧的预测集，即使估计不准确也能保持覆盖保证。

Result: 建立了覆盖和遗憾的联合界限，证明了COP在任意学习率下实现分布无关的有限样本覆盖，当分数独立同分布时能够收敛。实验显示COP能实现有效覆盖并构建比其他基线更短的预测区间。

Conclusion: COP是一种有效的在线共形预测方法，能够利用数据模式减少预测集的保守性，同时保持理论保证和实际性能优势。

Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.

</details>
