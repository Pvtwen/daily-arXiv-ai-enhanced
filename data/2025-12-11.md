<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 71]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [New Algorithm for Structured OFDM Channel Estimation using Subgroup Duality](https://arxiv.org/abs/2512.08947)
*Demerson N. Gonçalves,João T. Dias*

Main category: eess.SP

TL;DR: 提出基于群论的OFDM结构化信道估计框架，通过子群零化约束信道冲激响应到其对偶域湮灭子，实现低复杂度估计


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法复杂度高且缺乏结构解释性，需要一种既能保持性能又能降低复杂度、具有数学解释性的结构化估计方法

Method: 将OFDM子载波建模为循环群Z_N，通过零化子群H约束信道冲激响应到其对偶域湮灭子H^⊥，提出基于能量集中度的低复杂度检测算法

Result: 相比最小二乘和线性最小均方误差基准，在均方误差、误码率和吞吐量方面获得一致增益，以显著降低的复杂度实现竞争性性能

Conclusion: 群论框架为OFDM信道估计提供了结构化方法，在保持可解释性的同时实现了性能与复杂度的良好平衡

Abstract: This paper presents a group-theoretic framework for structured channel estimation in Orthogonal Frequency Division Multiplexing (OFDM). By modeling subcarriers as the cyclic group \(\mathbb{Z}_N\), we show that nulling a subgroup \(H \subseteq \mathbb{Z}_N\) constrains the channel impulse response to its annihilator \(H^\perp\) in the dual domain. A low-complexity estimator is proposed that detects such structure by evaluating energy concentration across candidate annihilators. Simulations demonstrate consistent gains in mean squared error, bit error rate, and throughput compared with least-squares and linear minimum mean square error baselines, achieving competitive performance with substantially lower complexity and preserved interpretability.

</details>


### [2] [A New Particle Filter for Target Tracking in MIMO OFDM Integrated Sensing and Communications](https://arxiv.org/abs/2512.09098)
*Shixiong Wang,Wei Dai,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 提出一种基于代价函数的粒子滤波框架，用于MIMO脉冲多普勒雷达目标跟踪，解决了传统方法的三大障碍，并在MIMO-OFDM系统中实现ISAC能力。


<details>
  <summary>Details</summary>
Motivation: 传统粒子滤波在MIMO脉冲多普勒雷达目标跟踪中面临三大长期障碍：1) 缺乏可靠的雷达原始数据似然模型；2) 将复杂路径增益等干扰参数增广到状态向量中带来的计算和统计问题；3) 从快照中提取距离、多普勒和角度噪声测量的计算负担过重。

Method: 基于贝叶斯规则的优化中心解释，提出新的粒子滤波框架，使用定制的代价函数评估每个假设状态，而不是依赖显式的似然关系。该方法在MIMO-OFDM系统中实现，用于集成感知与通信(ISAC)。

Result: 与现有方案相比，该框架在运行时间和跟踪误差方面都实现了显著降低。实验表明，采用脉冲多普勒处理的MIMO-OFDM系统在ISAC方面具有很大潜力，特别是在使用宽带宽、长目标时间和大型天线孔径时。

Conclusion: 通过基于代价函数的粒子滤波框架，有效解决了MIMO脉冲多普勒雷达目标跟踪的传统障碍，为现代通信基础设施提供了集成感知与通信能力，展示了MIMO-OFDM系统在ISAC应用中的良好前景。

Abstract: Particle filtering for target tracking using multi-input multi-output (MIMO) pulse-Doppler radars faces three long-standing obstacles: a) the absence of reliable likelihood models for raw radar data; b) the computational and statistical complications that arise when nuisance parameters (e.g., complex path gains) are augmented into state vectors; and c) the prohibitive computational burden of extracting noisy measurements of range, Doppler, and angles from snapshots. Motivated by an optimization-centric interpretation of Bayes' rule, this article addresses these challenges by proposing a new particle filtering framework that evaluates each hypothesized state using a tailored cost function, rather than relying on an explicit likelihood relation. The framework yields substantial reductions in both running time and tracking error compared to existing schemes. In addition, we examine the implementation of the proposed particle filter in MIMO orthogonal frequency-division multiplexing (OFDM) systems, aiming to equip modern communication infrastructure with integrated sensing and communications (ISAC) capabilities. Experiments suggest that MIMO-OFDM with pulse-Doppler processing holds considerable promise for ISAC, particularly when wide bandwidth, extended on-target time, and large antenna aperture are utilized.

</details>


### [3] [A Hybrid Residue Floating Numerical Architecture for High Precision Arithmetic on FPGAs](https://arxiv.org/abs/2512.09155)
*Mostafa Darvishi*

Main category: eess.SP

TL;DR: HRFNA是一种结合余数系统和浮点缩放因子的混合算术架构，在FPGA上相比IEEE 754单精度浮点实现了2.1倍吞吐提升和38-52% LUT资源减少，同时保持数值稳定性。


<details>
  <summary>Details</summary>
Motivation: FPGA平台上浮点运算成本高，需要宽数据通路和归一化逻辑，因此需要寻找能够保持动态范围但成本更低的替代表示方法。

Method: 提出混合余数浮点数值架构(HRFNA)，结合无进位余数通道和轻量级浮点缩放因子，开发完整的数学框架、有界误差归一化规则，以及针对FPGA优化的模块化乘法、指数管理和混合重建微架构。

Result: 在Xilinx ZCU104上实现，通过Vitis仿真、RTL综合和片上ILA跟踪验证了周期精确的正确性。相比IEEE 754单精度基准，实现了超过2.1倍的吞吐提升和38-52%的LUT资源减少，在长迭代序列中保持数值稳定性。

Conclusion: HRFNA为现代FPGA设备提供了一种高效且可扩展的浮点计算替代方案。

Abstract: Floating point arithmetic remains expensive on FPGA platforms due to wide datapaths and normalization logic, motivating alternative representations that preserve dynamic range at lower cost. This work introduces the Hybrid Residue Floating Numerical Architecture (HRFNA), a unified arithmetic system that combines carry free residue channels with a lightweight floating point scaling factor. We develop the full mathematical framework, derive bounded error normalization rules, and present FPGA optimized microarchitectures for modular multiplication, exponent management, and hybrid reconstruction. HRFNA is implemented on a Xilinx ZCU104, with Vitis simulation, RTL synthesis, and on chip ILA traces confirming cycle accurate correctness. The architecture achieves over 2.1 times throughput improvement and 38-52 percent LUT reduction compared to IEEE 754 single precision baselines while maintaining numerical stability across long iterative sequences. These results demonstrate that HRFNA offers an efficient and scalable alternative to floating point computation on modern FPGA devices.

</details>


### [4] [Secure Wireless Communication Using Distributed Coherent Transmission and Spatial Signal Decomposition](https://arxiv.org/abs/2512.09194)
*Anton Schlegel,Jason M/ Merlo,Samuel Wagner,John B. Lancaster,Jeffrey A. Nanzer*

Main category: eess.SP

TL;DR: 提出一种基于相干分布式天线阵列的无线通信安全方法，通过将信号分解为两个伪随机向量分量分别传输，在目标接收器处相干合成，实现空间受限的安全通信区域。


<details>
  <summary>Details</summary>
Motivation: 传统波束成形系统在所有位置都能接收信号，缺乏空间安全性。需要一种方法使信息仅在特定空间区域可恢复，在其他区域不可恢复，从而增强无线通信的安全性。

Method: 使用两单元相干分布式相控阵，将每个通信符号分解为两个伪随机信号向量之和，分别从两个发射器传输。通过分布式波束成形将传输定向到目标接收器，使两个向量分量主要在目标接收器处相干合成。

Result: 在3GHz、50波长阵列上实现。实验验证：在正对方向SER为0.0082（低错误率），其他所有位置SER高于0.25（高错误率）。相比传统波束成形系统在所有位置SER为0，新方法实现了空间受限的安全区域。

Conclusion: 提出的相干分布式传输方法能够创建空间受限的安全通信区域，信息仅在目标位置可恢复，在其他位置不可恢复，为无线通信安全提供了新的物理层安全解决方案。

Abstract: We present a new approach to secure wireless communications using coherent distributed transmission of signals that are spatially decomposed between a two-element distributed antenna array. High-accuracy distributed coordination of microwave wireless systems supports the ability to transmit different parts of a signal from separate transmitters such that they combine coherently at a designated destination. In this paper we explore this concept using a two-element coherent distributed phased array where each of the two transmitters sends a separate component of a communication signal where each symbol is decomposed into a sum of two pseudo-random signal vectors, the coherent summation of which yields the intended symbol. By directing the transmission to an intended receiver using distributed beamforming, the summation of the two vector components is largely confined to a spatial region at the destination receiver. We implement the technique in a 50 wavelength array operating at 3 GHz. We evaluate the symbol error ratio. (SER) in two-dimensional space through simulation and measurement, showing the approach yields a spatially confined secure region where the information is recoverable(i.e., the received signal has low SER), and outside of which the information is unrecoverable (high SER). The proposed system is also compared against a traditional beamforming system where each node sends the same data. We validate experimentally that our approach achieves a low SER of 0.0082 at broadside and a SER above 0.25 at all other locations compared to a traditional beamforming approach that achieves a SER of 0 at all locations measured.

</details>


### [5] [Joint Channel Estimation and Localization in Pinching-Antenna OFDM Systems: The Blessing of Multipath](https://arxiv.org/abs/2512.09432)
*Min Liu,Yue Xiao,Shuaixin Yang,Gang Wu,Xianfu Lei,Wei Xiang*

Main category: eess.SP

TL;DR: 提出了一种用于多用户OFDM上行链路Pinching-antenna系统的联合定位与信道估计混合推理框架，通过期望传播、OMP/BP-VI算法和迭代定位实现高精度估计，性能接近CRLB下界。


<details>
  <summary>Details</summary>
Motivation: Pinching-antenna系统能够灵活重构大规模无线信道，但其在存在多径色散情况下的联合定位和信道估计问题尚未得到充分研究。本文旨在解决这一挑战，利用CP-OFDM系统的特性，开发能够同时准确估计信道参数和用户位置的方法。

Method: 首先建立多用户OFDM上行链路PASS模型，利用循环前缀将时域多径色散转换为频域叠加正弦波。然后提出混合推理框架：1) 使用期望传播减轻多用户干扰；2) 采用OMP或BP-VI算法从噪声CSI中提取路径延迟；3) 通过迭代定位程序利用几何信息细化延迟估计；4) 将估计的信道矩阵递归反馈给EP。最后推导了CRLB作为性能基准。

Result: 仿真结果表明，所提出的框架性能接近CRLB下界，定位精度与协作多基站定位相当，但所需RF链数量显著减少，硬件复杂度大幅降低。

Conclusion: 该研究为PASS系统提供了一种高效的联合定位与信道估计解决方案，能够在减少硬件复杂度的同时实现接近理论极限的性能，为未来大规模MIMO系统的实际部署提供了重要参考。

Abstract: Pinching-antenna systems (PASS) have recently attracted considerable attention owing to their capability of flexibly reconfiguring large-scale wireless channels. Motivated by this potential, we investigate the issue of joint localization and channel estimation for the uplink PASS in the presence of multipath dispersion. To this end, a comprehensive multi-user orthogonal frequency division multiplexing (OFDM) uplink PASS model is first established, where the use of a cyclic prefix (CP) enables the multipath-induced time-domain dispersion to be transformed into a set of superimposed sinusoids in the frequency domain. Building upon this model, we propose a hybrid inference framework capable of accurately estimating both channel parameters and user locations. Specifically, expectation propagation is first employed to mitigate multi-user interference, while the path delays are then extracted from noisy channel state information using an orthogonal matching pursuit (OMP) based approach, or a hybrid belief propagation-variational inference (BP-VI) algorithm. Then the estimated delays are subsequently refined through the embedded geometric information via an iterative localization procedure, wherein the estimated channel matrices are recursively fed back to EP. Furthermore, the Cramer-Rao lower bound (CRLB) is derived to characterize the fundamental estimation limits. Finally, simulation results validate that our proposed framework closely approaches the CRLB, with performance comparable to cooperative multi-base station localization, with significantly fewer RF chains and reduced hardware complexity.

</details>


### [6] [Analytical and DNN-Aided Performance Evaluation of IRS-Assisted THz Communication Systems](https://arxiv.org/abs/2512.09515)
*Soumendu Das,Nagendra Kumar,Dharmendra Dixit*

Main category: eess.SP

TL;DR: 该论文研究了智能反射表面辅助的太赫兹通信系统性能，推导了中断概率、平均信道容量和平均符号错误率的闭式表达式，并开发了基于深度神经网络的性能预测框架。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信系统面临大气衰减、小尺度衰落和波束失准等挑战，特别是在没有直接传输路径的情况下。智能反射表面可以辅助建立连接，但需要准确分析其性能指标。

Method: 使用Laguerre级数展开近似建模IRS链路，将源-IRS和IRS-目的地信道建模为独立同分布的α-μ衰落信道。推导了OP、ACC和ASER的闭式表达式，并开发了基于深度神经网络的预测框架。

Result: 获得了中断概率、平均信道容量和平均符号错误率的闭式解析表达式。深度神经网络框架能够快速准确地预测性能指标。高信噪比下的渐近分析得到了编码增益和分集阶数的表达式。蒙特卡洛仿真验证了理论结果。

Conclusion: 该研究为IRS辅助的太赫兹通信系统提供了全面的性能分析框架，理论分析和深度学习预测相结合的方法能够有效评估系统在实际条件下的行为，为系统设计和优化提供了重要见解。

Abstract: This paper investigates the performance of an intelligent reflecting surface (IRS)-assisted terahertz (THz) communication system, where the IRS facilitates connectivity between the source and destination nodes in the absence of a direct transmission path. The source-IRS and IRS-destination links are subject to various challenges, including atmospheric attenuation, asymmetric $α$-$μ$ distributed small-scale fading, and beam misalignment-induced pointing errors. The IRS link is characterized using the Laguerre series expansion (LSE) approximation, while both the source-IRS and IRS-destination channels are modeled as independent and identically distributed (i.i.d.) $α$-$μ$ fading channels. Furthermore, closed-form analytical expressions are derived for the outage probability (OP), average channel capacity (ACC), and average symbol error rate (ASER) for rectangular QAM (RQAM) and hexagonal QAM (HQAM) schemes over the end-to-end (e2e) link. The impact of random co-phasing and phase quantization errors are also examined. In addition to the theoretical analysis, deep neural network-based frameworks are developed to predict key performance metrics, facilitating fast and accurate system evaluation without computationally intensive analytical computations. Moreover, the asymptotic analysis in the high-signal-to-noise ratio (SNR) regime yields closed-form expressions for coding gain and diversity order, providing further insights into performance trends. Finally, Monte Carlo simulations validate the theoretical formulations and present a comprehensive assessment of system behavior under practical conditions.

</details>


### [7] [CKM-Enabled Joint Spatial-Doppler Domain Clutter Suppression for Low-Altitude UAV ISAC](https://arxiv.org/abs/2512.09560)
*Zihan Xu,Zhiwen Zhou,Di Wu,Xiaoli Xu,Yong Zeng*

Main category: eess.SP

TL;DR: 提出基于杂波角度图（CLAM）的OFDM ISAC系统杂波抑制技术，用于低空无人机目标感知


<details>
  <summary>Details</summary>
Motivation: 低空经济快速发展对小型无人机目标感知提出更高要求，但复杂动态的低空环境（如城市和山区）中杂波严重影响感知性能。传统基于多普勒差或信号强度的杂波抑制方法在动态杂波和慢速目标（如低空无人机）场景中效果不佳。

Method: 基于信道知识图（CKM）概念，提出一种新型CKM——杂波角度图（CLAM），作为特定站点的数据库，包含ISAC基站覆盖区域内位置特定的主要杂波角度。利用CLAM在目标检测和参数估计前有效去除与环境杂波对应的感知信号分量。进一步提出两步CLAM启用的联合空间-多普勒域杂波抑制算法，处理目标和杂波方向接近的情况。

Result: 仿真结果表明，所提技术能有效抑制杂波并增强目标感知性能，实现对慢速低空无人机目标的准确参数估计。

Conclusion: 基于CLAM的杂波抑制技术为OFDM ISAC系统在复杂低空环境中感知慢速无人机目标提供了有效解决方案，通过利用先验环境信息显著提升了感知性能。

Abstract: The rapid development of low-altitude economy has placed higher demands on the sensing of small-sized unmanned aerial vehicle (UAV) targets. However, the complex and dynamic low-altitude environment, like the urban and mountainous areas, makes clutter a significant factor affecting the sensing performance. Traditional clutter suppression methods based on Doppler difference or signal strength are inadequate for scenarios with dynamic clutter and slow-moving targets like low-altitude UAVs. In this paper, motivated by the concept of channel knowledge map (CKM), we propose a novel clutter suppression technique for orthogonal frequency division multiplexing (OFDM) integrated sensing and communication (ISAC) system, by leveraging a new type of CKM named clutter angle map (CLAM). CLAM is a site-specific database, containing location-specific primary clutter angles for the coverage area of the ISAC base station (BS). With CLAM, the sensing signal components corresponding to the clutter environment can be effectively removed before target detection and parameter estimation, which greatly enhances the sensing performance. Besides, to take into account the scenarios when the targets and clutters are in close directions so that pure CLAM-based spatial domain clutter suppression is no longer effective, we further propose a two-step CLAM-enabled joint spatial-Doppler domain clutter suppression algorithm. Simulation results demonstrate that the proposed technique effectively suppresses clutter and enhances target sensing performance, achieving accurate parameter estimation for sensing slow-moving low-altitude UAV targets.

</details>


### [8] [Temporal Windows of Integration for Multisensory Wireless Systems as Enablers of Physical AI](https://arxiv.org/abs/2512.09589)
*Anup Mishra,João Henrique Inacio de Souza,Petar Popovski*

Main category: eess.SP

TL;DR: 该研究针对物理AI系统提出了一种基于时间窗口整合的时序感知网络设计方法，通过优化延迟-可靠性预算来确保决策时信息的有用性。


<details>
  <summary>Details</summary>
Motivation: 物理AI系统在实时交互中面临多模态传感器数据通过异构不可靠路径传输的问题，传统吞吐量或延迟保证无法确保正确决策，因为错位、错序或过时的输入仍会导致错误推断。

Method: 采用时间窗口整合(TWI)-因果性方法，将端到端路径延迟建模为感知/传播、计算和访问/传输延迟之和，将网络设计问题形式化为在交付可靠性约束下最小化有效性时间窗口的凸优化问题。

Result: 通过凸优化求解得到最小时间窗口和每路径可靠性分配，相比基于统一阈值分配的基准方法表现更优。

Conclusion: 该研究为下一代网络中时序感知的物理AI系统提供了网络设计框架，通过校准延迟-可靠性预算来确保决策时信息的有用性。

Abstract: Physical artificial intelligence (AI) refers to the AI that interacts with the physical world in real time. Similar to multisensory perception, Physical AI makes decisions based on multimodal updates from sensors and devices. Physical AI thus operates with a finite spatial footprint of its sensory tributaries. The multimodal updates traverse heterogeneous and unreliable paths, involving wireless links. Throughput or latency guarantees do not ensure correct decision-making, as misaligned, misordered, or stale inputs still yield wrong inferences. Preserving decision-time coherence hinges on three timing primitives at the network-application interface: (i) simultaneity, a short coincidence window that groups measurements as co-temporal, (ii) causality, path-wise delivery that never lets a consequence precede its precursor, and (iii) usefulness, a validity horizon that drops information too stale to influence the current action. In this work, we focus on usefulness and adopt temporal window of integration (TWI)-Causality: the TWI enforces decision-time usefulness by assuming path-wise causal consistency and cross-path simultaneity are handled upstream. We model end-to-end path delay as the sum of sensing/propagation, computation, and access/transmission latencies, and formulate network design as minimizing the validity horizon under a delivery reliability constraint. In effect, this calibrates delay-reliability budgets for a timing-aware system operating over sensors within a finite spatial footprint. The joint choice of horizon and per-path reliability is cast as a convex optimization problem, solved to global optimality to obtain the minimal horizon and per-path allocation of reliability. This is compared favourably to a benchmark based on uniform-after-threshold allocation. Overall, this study contributes to timing-aware Physical AI in next-generation networks.

</details>


### [9] [Flexible Reconfigurable Intelligent Surface-Aided Covert Communications in UAV Networks](https://arxiv.org/abs/2512.09714)
*Chong Huang,Gaojie Chen,Zhuoao Xu,Jing Zhu,Taisong Pan,Rahim Tafazolli,Wei Huang*

Main category: eess.SP

TL;DR: 本文提出了一种在无人机网络中引入柔性可重构智能表面(F-RIS)的隐蔽通信框架，通过联合优化无人机轨迹、F-RIS反射参数和NOMA功率分配，并采用深度强化学习算法解决复杂的非凸优化问题，显著提升了无人机隐蔽通信性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在无线通信网络中具有灵活性和动态适应性，但其通信的开放性带来了安全和隐私问题。传统RIS存在部署限制，无法适应曲面环境，因此需要更灵活的解决方案来增强无人机隐蔽通信性能。

Method: 1) 建立F-RIS的电磁模型和拟合模型，描述反射幅度、反射相位和入射角的关系；2) 提出联合优化框架，包括无人机轨迹、F-RIS反射向量、F-RIS入射角和NOMA功率分配；3) 采用深度强化学习算法解决复杂的非凸优化问题。

Result: 仿真结果表明，所提出的框架和优化方法显著优于传统基准方案，突出了F-RIS在增强无人机网络隐蔽通信性能方面的优势。

Conclusion: F-RIS通过其曲面适应性和动态电磁特性重构能力，为无人机隐蔽通信提供了有效的解决方案，联合优化策略和DRL算法能够有效提升隐蔽传输速率，满足隐蔽性和公共传输约束。

Abstract: In recent years, unmanned aerial vehicles (UAVs) have become a key role in wireless communication networks due to their flexibility and dynamic adaptability. However, the openness of UAV-based communications leads to security and privacy concerns in wireless transmissions. This paper investigates a framework of UAV covert communications which introduces flexible reconfigurable intelligent surfaces (F-RIS) in UAV networks. Unlike traditional RIS, F-RIS provides advanced deployment flexibility by conforming to curved surfaces and dynamically reconfiguring its electromagnetic properties to enhance the covert communication performance. We establish an electromagnetic model for F-RIS and further develop a fitted model that describes the relationship between F-RIS reflection amplitude, reflection phase, and incident angle. To maximize the covert transmission rate among UAVs while meeting the covert constraint and public transmission constraint, we introduce a strategy of jointly optimizing UAV trajectories, F-RIS reflection vectors, F-RIS incident angles, and non-orthogonal multiple access (NOMA) power allocation. Considering this is a complicated non-convex optimization problem, we propose a deep reinforcement learning (DRL) algorithm-based optimization solution. Simulation results demonstrate that our proposed framework and optimization method significantly outperform traditional benchmarks, and highlight the advantages of F-RIS in enhancing covert communication performance within UAV networks.

</details>


### [10] [On the Ambiguity Function of OFDM-based ISAC Signals Under Non-Ideal Power Amplifiers](https://arxiv.org/abs/2512.09803)
*Eya Gourar,Yahia Medjahdi,Laurent Clavier,Abdul Karim Gizzini,Patrick Sondi*

Main category: eess.SP

TL;DR: PA非线性失真对OFDM ISAC系统的感知性能有显著影响，会限制PSK和QAM调制的性能上限，改变模糊函数形状，降低检测概率，削弱单模信号的理论优势。


<details>
  <summary>Details</summary>
Motivation: 虽然OFDM在ISAC中因抗频率选择性衰落和良好测距性能而被广泛研究，但不同调制方案（PSK/QAM）在S&C性能上存在权衡。现有研究未考虑PA非线性失真对感知任务的影响，而OFDM的高PAPR特性使其易受PA非线性影响，这与雷达需要高发射功率的要求相冲突。

Method: 通过引入信号失真比(SDR)分析PA引起的失真对PSK和QAM星座感知任务的限制程度。结合仿真结果和模糊函数(AF)的理论表征，明确展示失真伪影在零多普勒旁瓣（测距旁瓣）和零延迟旁瓣中的表现。

Result: 仿真表明PA失真对两种星座都施加了明显的性能上限，重塑了模糊函数形状，降低了检测概率，削弱了单模信号的理论优势，并进一步损害了非均匀包络信号的OFDM感知性能。

Conclusion: PA非线性失真显著影响ISAC系统的感知性能，需要在系统设计中考虑这一因素。失真不仅限制了性能上限，还改变了信号特性，削弱了PSK相对于QAM的理论优势，这对实际ISAC系统设计具有重要意义。

Abstract: Integrated Sensing and Communications (ISAC) has garnered significant attention as a promising technology for next-generation wireless and vehicular communications. Among candidate waveforms, Orthogonal Frequency Division Multiplexing (OFDM) has been extensively investigated over the past decade for its robustness against frequency-selective fading and its favorable ranging performance. However, the waveform's sensing and communication (S&C) performance depends strongly on the modulation scheme; while variable-amplitude constellations such as quadrature amplitude (QAM) are more efficient for communication, constant-modulus modulations such as phase shift keying (PSK) are more suitable for sensing. Yet, it remains unclear whether these findings persist under power amplifier (PA) nonlinearity. Because OFDM signals exhibit a high peak-to-average power ratio (PAPR), they require highly linear PAs to avoid distortion, which conflicts with radar requirements, where high transmit power is always beneficial for sensing. In this work, we analyze the effect of PA-induced distortions on the sensing task for PSK and QAM constellations. By introducing the Signal-to-Distortion Ratio (SDR), we examine the extent of the distortion limitation on the ranging task. We complement simulation results with a theoretical characterization of the ambiguity function (AF), thereby explicitly demonstrating how distortion artifacts manifest in the zero-Doppler sidelobes (i.e, ranging sidelobes) and the zero-delay sidelobes. Simulations show that PA distortions impose a palpable performance ceiling for both constellations, reshape the AF, and reduce detection probability, diminishing the theoretical advantage of unimodular signaling and further compromising the OFDM sensing performance with non-uniform envelope signals.

</details>


### [11] [Energy-Efficient Federated Learning with Relay-Assisted Aggregation in IIoT Networks](https://arxiv.org/abs/2512.09827)
*Hamid Reza Hashempour,Mostafa Nozari,Gilberto Berardinelli,Yanjiao Li,Jie Zhang,Hien Quoc Ngo,Shashi Raj Pandey*

Main category: eess.SP

TL;DR: 提出了一种面向工业物联网的能效优化联邦学习传输框架，通过中继辅助和部分聚合减少通信开销，采用分组算法和SPCA方法最大化能量效率


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境中的联邦学习面临严格的延迟和能量约束，传统单跳传输效率低下，需要设计能效优化的传输框架来提升通信效率和系统可靠性

Method: 1) 中继辅助联邦学习架构，中继节点执行部分聚合；2) 将非凸优化问题分解为计算和通信能量子问题；3) 基于延迟最小化的设备分组算法和继电器选择机制；4) 采用顺序参数凸近似方法联合优化系统参数；5) 扩展到非完美信道状态信息场景

Result: 显著提升收敛速度，中断概率从10^-2降低到10^-6，SPCA方法相比无聚合协作节能至少2倍，相比单跳传输节能高达6倍

Conclusion: 提出的框架在工业物联网联邦学习中实现了能量效率、延迟和可靠性的显著提升，通过中继辅助和优化算法有效解决了严格约束下的通信挑战

Abstract: This paper presents an energy-efficient transmission framework for federated learning (FL) in industrial Internet of Things (IIoT) environments with strict latency and energy constraints. Machinery subnetworks (SNs) collaboratively train a global model by uploading local updates to an edge server (ES), either directly or via neighboring SNs acting as decode-and-forward relays. To enhance communication efficiency, relays perform partial aggregation before forwarding the models to the ES, significantly reducing overhead and training latency. We analyze the convergence behavior of this relay-assisted FL scheme. To address the inherent energy efficiency (EE) challenges, we decompose the original non-convex optimization problem into sub-problems addressing computation and communication energy separately. An SN grouping algorithm categorizes devices into single-hop and two-hop transmitters based on latency minimization, followed by a relay selection mechanism. To improve FL reliability, we further maximize the number of SNs that meet the roundwise delay constraint, promoting broader participation and improved convergence stability under practical IIoT data distributions. Transmit power levels are then optimized to maximize EE, and a sequential parametric convex approximation (SPCA) method is proposed for joint configuration of system parameters. We further extend the EE formulation to the imperfect channel state information (ICSI). Simulation results demonstrate that the proposed framework significantly enhances convergence speed, reduces outage probability from 10-2 in single-hop to 10-6 and achieves substantial energy savings, with the SPCA approach reducing energy consumption by at least 2x compared to unaggregated cooperation and up to 6x over single-hop transmission.

</details>


### [12] [A Speculative GLRT-Backed Approach for Adversarial Resilience on Deep Learning-Based Array Processing](https://arxiv.org/abs/2512.09893)
*Nian-Cin Wang,Rajeev Sahay*

Main category: eess.SP

TL;DR: 提出对抗性弹性推测阵列处理框架，结合深度学习快速推理与GLRT统计验证，解决传统方法延迟高和深度学习缺乏统计保证的问题


<details>
  <summary>Details</summary>
Motivation: 传统阵列处理方法（如GLRT）计算成本高，不适合低延迟场景；深度学习虽然推理快，但缺乏统计保证且易受对抗性扰动影响，在对抗性无线环境中可靠性存疑

Method: 提出对抗性弹性推测阵列处理框架：使用低延迟DL分类器进行快速推测推理，然后用理论基础的GLRT验证器确认。利用接收阵列二阶统计量对L-p有界对抗性扰动的空间不变性提供对抗鲁棒性

Result: 在多种L-p边界、扰动设计和扰动幅度下的实证评估验证了理论发现，所提框架在性能上优于多个最先进的基线方法

Conclusion: 通过结合DL的快速推理能力和GLRT的统计保证，提出的框架在保持低延迟的同时提供了对抗性鲁棒性和理论基础的验证，解决了阵列处理中的关键挑战

Abstract: Classical array processing methods such as the generalized likelihood ratio test (GLRT) provide statistically grounded solutions for signal detection and direction-of-arrival (DoA) estimation, but their high computational cost limits their use in low-latency settings. Deep learning (DL) has recently emerged as an efficient alternative, offering fast inference for array processing tasks. However, DL models lack statistical guarantees and, moreover, are highly susceptible to adversarial perturbations, raising fundamental concerns about their reliability in adversarial wireless environments. To address these challenges, we propose an adversarially resilient speculative array processing framework that consists of a low-latency DL classifier backed by a theoretically-grounded GLRT validator, where DL is used for fast speculative inference and later confirmed with the GLRT. We show that second order statistics of the received array, which the GLRT operates on, are spatially invariant to L-p bounded adversarial perturbations, providing adversarial robustness and theoretically-grounded validation of DL predictions. Empirical evaluations under multiple L-p bounds, perturbation designs, and perturbation magnitudes corroborate our theoretical findings, demonstrating the superior performance of our proposed framework in comparison to multiple state-of-the-art baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization](https://arxiv.org/abs/2512.08950)
*Aseel Rawashdeh*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯扩展的Act-Then-Measure（ATM）方法，用于移动健康干预中的强化学习，通过卡尔曼滤波器式的贝叶斯更新替代标准Q学习，在低数据环境下实现更稳定和样本高效的学习。


<details>
  <summary>Details</summary>
Motivation: 移动健康干预中的强化学习需要在干预效果和用户负担之间取得平衡，特别是当状态测量（如用户调查或反馈）成本高昂但又必不可少时。标准的ATM算法使用时间差分启发的Q学习方法，在稀疏和嘈杂环境中容易不稳定。

Method: 提出贝叶斯ATM扩展，用卡尔曼滤波器式的贝叶斯更新替代标准Q学习，维护Q值的不确定性感知估计，实现更稳定和样本高效的学习。在ACNO-MDP框架内解耦控制和测量动作。

Result: 在小型表格环境中，贝叶斯ATM实现了相当或改进的标量化回报，方差显著降低，策略行为更稳定。但在更大更复杂的移动健康设置中，标准和贝叶斯ATM变体都表现不佳，表明ATM的建模假设与真实世界移动健康领域结构挑战不匹配。

Conclusion: 不确定性感知方法在低数据设置中具有价值，但需要新的强化学习算法来显式建模因果结构、连续状态和观测成本约束下的延迟反馈，以应对真实世界移动健康领域的挑战。

Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.

</details>


### [14] [Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis](https://arxiv.org/abs/2512.08952)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.LG

TL;DR: 开发了一个虚拟人形机器人对话代理训练系统，将真实访谈数据转化为276个虚拟患者，通过强化学习训练对话策略，重点关注社交时机、节奏和信任等非语言因素，TD3算法在模拟中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统人形机器人测试存在速度慢、设备损耗大、迭代受限等问题，而心理疾病筛查需要机器人掌握复杂的对话时机、韵律、反馈信号等非语言社交技能。现有模拟器大多忽略非语言动态的策略学习，控制器过于关注任务准确性而忽视信任、节奏和融洽关系。

Method: 将人形机器人虚拟化为对话代理，构建以代理为中心的模拟优先流程：将访谈数据转化为276个Unreal Engine MetaHuman虚拟患者，包含同步的语音、视线/面部表情和头躯干姿态。采用感知-融合-策略循环决策说话内容和时机、反馈时机，避免打断，并配备安全防护。训练使用反事实回放（有限非语言扰动）和不确定性感知的回合管理器来减少诊断歧义。

Result: 在三种控制器比较中，自定义TD3（Twin Delayed DDPG）优于PPO和CEM，在相似奖励下实现接近完美的覆盖率且节奏更稳定。决策质量分析显示可忽略的回合重叠、对齐的打断时机、更少的澄清提示和更短的等待时间。性能在模态丢失和渲染器更换下保持稳定，在保留患者集上排名一致。

Conclusion: 提出了一个以代理为中心的模拟器，将访谈转化为交互式虚拟患者；建立了将时机和融洽关系作为首要控制变量的安全学习循环；通过比较研究证明TD3在完整性和社交时机方面的优势；消融和鲁棒性分析解释了性能提升，为临床监督下的人形机器人试点奠定了基础。

Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.

</details>


### [15] [An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings](https://arxiv.org/abs/2512.08954)
*Yuhao Xu,Jiaying Lu,Sirui Ding,Defu Cao,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 该研究评估了基础模型在ECG分析中的表现，发现通用时间序列/ECG基础模型能达到80%的顶级性能，证明了其在心电图分析中的有效性。


<details>
  <summary>Details</summary>
Motivation: 心电图分析需要领域专业知识，这限制了人工智能在医疗保健中的应用。虽然自监督学习和基础模型使AI系统能够获取领域知识，但目前缺乏对基础模型在ECG分析性能的全面评估。

Method: 通过评估语言/通用时间序列/ECG基础模型，并与时间序列深度学习模型进行比较，使用公开可用的基准数据集和代码进行实验。

Result: 实验结果显示，通用时间序列/ECG基础模型达到了80%的顶级性能表现，表明它们在ECG分析中是有效的。

Conclusion: 该研究强调了基础模型在推进生理波形分析方面的局限性和潜力，为未来研究提供了重要见解和基准。

Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.

</details>


### [16] [LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation](https://arxiv.org/abs/2512.08955)
*Renbin Li,Shuangshuang Li,Peihao Dong*

Main category: cs.LG

TL;DR: LLM4XCE：利用大语言模型的语义建模能力进行XL-MIMO信道估计的新框架，在混合场条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: XL-MIMO是6G网络的关键技术，但混合场（近场和远场共存）信道给准确估计带来挑战，传统方法泛化能力有限。大语言模型在语义通信任务中表现出色，因此探索将其用于信道估计。

Method: 提出LLM4XCE框架，通过精心设计的嵌入模块和并行特征-空间注意力机制，深度融合导频特征和空间结构，构建语义丰富的LLM输入表示。仅微调顶部两个Transformer层，高效捕获导频数据中的潜在依赖关系。

Result: 大量仿真表明，LLM4XCE在混合场条件下显著优于现有最先进方法，实现了优越的估计精度和泛化性能。

Conclusion: LLM4XCE成功将大语言模型的语义建模能力应用于XL-MIMO信道估计，为解决混合场信道估计挑战提供了有效解决方案，展示了语义通信思想在物理层任务中的潜力。

Abstract: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy.
  Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.

</details>


### [17] [DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability](https://arxiv.org/abs/2512.08956)
*Kumarjit Pathak,Karthik K,Sachin Madan,Jitin Kapila*

Main category: cs.LG

TL;DR: DW-KNN提出双重加权机制，结合指数距离和邻居有效性，在异构特征空间中提升KNN分类器的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统KNN及其变体假设所有k个邻居同等可靠，但在异构特征空间中这一假设限制了预测的可靠性。需要一种能够区分邻居可靠性并抑制噪声样本的方法。

Method: 提出DW-KNN（双重加权KNN），集成指数距离加权和邻居有效性加权。通过双重加权机制实现实例级可解释性，抑制噪声或错误标记样本，并降低超参数敏感性。

Result: 在9个数据集上的评估显示，DW-KNN平均准确率达到0.8988，在6种方法中排名第二，与最佳集成KNN相差仅0.2%。交叉验证方差最低（0.0156），表明预测稳定性可靠。统计显著性测试证实相对于紧密度加权KNN提升4.09%，相对于核加权KNN提升1.13%。

Conclusion: DW-KNN为复杂自适应方案提供了一个简单有效的替代方案，特别适用于需要可解释预测的高风险应用场景，在保持透明度的同时提升了分类性能。

Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.

</details>


### [18] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: LUMOS是一个基于Transformer的大规模用户模型，通过联合学习多个任务，仅使用原始用户活动数据，无需任务特定模型和手动特征工程，显著提升了用户行为预测性能。


<details>
  <summary>Details</summary>
Motivation: 在线B2C平台的用户行为预测面临规模化挑战。传统方法依赖任务特定模型和领域特征工程，耗时、计算成本高、需要专业知识且难以扩展。

Method: 提出LUMOS架构：基于Transformer，通过跨注意力机制将预测条件化于未来已知事件（如节假日、促销），采用多模态标记化整合用户交易、事件上下文和静态用户属性，通过专门嵌入路径处理。

Result: 在包含2750亿用户活动标记、2.5亿用户的生产数据集上，相比传统任务特定模型，在5个任务上平均提升：二分类任务ROC-AUC提高0.025，回归任务MAPE降低4.6%。在线A/B测试显示日活跃用户增加3.15%。

Conclusion: LUMOS通过消除任务特定模型和手动特征工程，实现了可扩展的用户行为预测，显著提升预测性能并带来实际业务价值。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [19] [EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications](https://arxiv.org/abs/2512.08959)
*Ard Kastrati,Josua Bürki,Jonas Lauer,Cheng Xuan,Raffaele Iaquinto,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出统一的EEG基础模型临床评估基准框架，涵盖11个诊断任务和14个公开数据集，结果显示基础模型在某些场景表现良好，但简单模型在临床分布偏移下仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏统一的EEG基础模型临床评估标准，需要建立标准化基准来公平比较传统方法和现代基础模型在真实临床场景中的表现。

Method: 构建包含11个诊断任务、14个公开EEG数据集的统一基准框架，采用最小预处理和标准化评估协议，支持传统方法与基础模型的并行比较。

Result: 基础模型在特定场景下表现强劲，但简单模型在临床分布偏移情况下仍保持竞争力，特别是在真实临床应用中。

Conclusion: 该基准框架为EEG基础模型的临床评估提供了标准化工具，揭示了简单模型在临床实践中的持续价值，并开源了所有数据和代码以促进可重复性和采用。

Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.

</details>


### [20] [Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces](https://arxiv.org/abs/2512.08960)
*Yueer Zhou,Yichen Wu,Ying Wei*

Main category: cs.LG

TL;DR: PS-LoRA通过双正则化目标和对齐优化子空间中的更新来解决LoRA在持续学习中的灾难性遗忘问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在持续学习中存在灾难性遗忘问题，主要原因是新任务梯度与历史权重轨迹之间的对抗性方向更新。这种破坏性干扰导致学习性能下降。

Method: 提出PS-LoRA框架：1）使用双正则化目标，惩罚冲突方向并约束幅度偏差；2）在优化子空间中对齐更新；3）采用基于幅度的合并策略，将顺序适配器合并为稳健表示而无需重新训练。

Result: 在NLP和视觉基准测试中，PS-LoRA优于最先进方法，能够保持学习表示的稳定性，同时高效适应新领域。

Conclusion: PS-LoRA通过解决更新冲突和确保参数稳定性，有效缓解了LoRA在持续学习中的灾难性遗忘问题，为高效持续学习提供了有效解决方案。

Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.

</details>


### [21] [SEA: Spectral Edge Attacks on Graph Neural Networks](https://arxiv.org/abs/2512.08964)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出SEA攻击方法，利用谱分析识别图中脆弱边进行删除或添加，无需梯度即可攻击GNN模型


<details>
  <summary>Details</summary>
Motivation: 现有图结构攻击大多依赖梯度启发式或局部连接模式，将边视为同等重要的候选对象，缺乏对图谱特性的考虑

Method: 通过谱嵌入捕获输入流形的最脆弱方向，为每条边或非边分配鲁棒性分数，提出两种攻击变体：谱引导删除攻击（删除最鲁棒的边）和谱引导添加攻击（在谱空间中最大不兼容节点间添加边）

Result: 在基准测试中验证了SEA攻击的有效性，能够有效降低GNN模型性能

Conclusion: SEA提供了一种基于谱分析的新型图对抗攻击框架，无需梯度即可实现模型感知的结构扰动，为GNN鲁棒性评估提供了新工具

Abstract: Graph Neural Networks (GNNs) achieve strong performance on graph-structured data, but are notoriously vulnerable to small, carefully crafted perturbations of the graph structure. Most existing structure-based attacks rely on gradient-based heuristics or local connectivity patterns, and treat edges as equally important candidates for manipulation. In this paper, we propose Spectral Edge Attacks (SEA), a new family of adversarial attacks that explicitly leverage spectral robustness evaluation to guide structural perturbations. Our key idea is to compute a spectral embedding that captures the most fragile directions of the input manifold and to use it to assign a robustness score to each edge or non-edge. Based on these scores, we introduce two complementary attack variants: (i) a Spade-guided deletion attack that removes the most spectrally robust edges, and (ii) a Spade-guided addition attack that inserts edges between nodes that are maximally incompatible in the fragile spectral space. Both attacks operate at the graph level, are model-aware but conceptually simple, and can be plugged into existing GNN architectures without requiring gradients. We describe the spectral formulation, the attack algorithms, and experiments on benchmarks.

</details>


### [22] [TinyDéjàVu: Smaller Memory Footprint & Faster Inference on Sensor Data Streams with Always-On Microcontrollers](https://arxiv.org/abs/2512.09786)
*Zhaolan Huang,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: TinyDéjàVu框架通过优化数据流和消除滑动窗口冗余计算，显著减少MCU上传感器时序数据推理的RAM占用


<details>
  <summary>Details</summary>
Motivation: 随着常开传感器需要运行各种微型神经网络进行持续推理，而MCU内存预算有限（如128kB RAM），优化神经网络层间数据流变得至关重要

Method: 提出TinyDéjàVu框架和新算法，通过优化数据流和消除滑动窗口输入中的冗余计算来减少RAM占用

Result: TinyDéjàVu可节省超过60%的RAM使用，并消除高达90%的滑动窗口重叠输入冗余计算

Conclusion: TinyDéjàVu为MCU上的传感器时序数据推理提供了有效的RAM优化解决方案，已开源实现并在硬件上进行可重复基准测试

Abstract: Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyDéjàVu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyDéjàVu as open source, and we perform reproducible benchmarks on hardware. We show that TinyDéjàVu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.

</details>


### [23] [Financial Instruction Following Evaluation (FIFE)](https://arxiv.org/abs/2512.08965)
*Glenn Matlin,Siddharth,Anirudh JM,Aditya Shukla,Yahya Hassan,Sudheer Chava*

Main category: cs.LG

TL;DR: FIFE是一个用于评估语言模型在金融分析任务中遵循复杂指令能力的高难度基准，包含88个人工编写的提示和可验证约束系统，测试显示开源权重模型表现优于专有系统，但所有模型都难以完全满足复杂要求。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理复杂、相互依赖的指令方面存在困难，特别是在金融等高精度要求领域，需要评估模型在金融分析任务中的指令遵循能力。

Method: 引入FIFE基准，包含88个人工编写的提示，采用具有可链接、可验证约束的验证系统，提供细粒度奖励信号，在零样本设置下评估53个模型（专有、开源权重、开源）。

Result: 性能层次清晰：顶级开源权重模型（76.1严格/79.5宽松）优于领先专有系统（65.9严格/70.5宽松），而最佳开源模型显著落后（45.5严格/48.9宽松），但即使是顶级模型也难以完全满足FIFE的复杂要求。

Conclusion: FIFE基准揭示了语言模型在金融领域复杂指令遵循方面的局限性，开源数据集和代码将促进金融领域强化学习研究，模型在复杂金融分析任务中仍有改进空间。

Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.

</details>


### [24] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 轻量级加法模型（Facebook Prophet）在北京PM2.5和PM10预测中表现优于复杂深度学习模型和传统统计方法，提供准确、可解释且易于部署的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和混合方法在空气污染预测研究中占主导地位，但其复杂性和有限的可解释性阻碍了实际应用。本研究旨在探索轻量级加法模型是否能在北京PM2.5和PM10预测中提供有竞争力的性能。

Method: 使用多年污染物和气象数据，应用系统特征选择（相关性、互信息、mRMR）、防泄漏缩放和时间序列数据分割。比较Facebook Prophet和NeuralProphet两种加法模型，同时实现LSTM、LightGBM和SARIMAX作为基线模型。NP模型额外利用滞后依赖关系。

Result: Facebook Prophet在7天保留测试集上表现最佳，对两种污染物的测试R²均超过0.94，优于NeuralProphet、SARIMAX以及学习型基线模型。

Conclusion: 可解释的加法模型在空气污染预测中与传统方法和复杂方法相比仍具有竞争力，提供了准确性、透明性和易部署性之间的实用平衡。

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [25] [CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing](https://arxiv.org/abs/2512.08967)
*Zixia Wang,Gaojie Jin,Jia Hu,Ronghui Mu*

Main category: cs.LG

TL;DR: CluCERT：通过聚类引导去噪平滑认证LLM鲁棒性的新框架，相比现有方法提供更紧的鲁棒性边界和更高计算效率


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能力强大，但仍易受对抗攻击（如同义词替换等语义保持的微小改动），现有认证方法存在两个关键局限：1）由于缺乏对扰动输出的语义验证导致鲁棒性边界松散；2）重复采样导致计算成本高

Method: 提出CluCERT框架，通过聚类引导去噪平滑认证LLM鲁棒性。包括：语义聚类过滤器减少噪声样本保留有意义的扰动；精炼模块提取核心语义；快速同义词替换策略加速去噪过程

Result: 在各种下游任务和越狱防御场景的实验中，该方法在鲁棒性边界和计算效率方面均优于现有认证方法

Conclusion: CluCERT通过语义聚类过滤和高效去噪机制，有效解决了现有LLM鲁棒性认证方法的局限性，提供了更紧的认证边界和更高的计算效率

Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.

</details>


### [26] [Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power](https://arxiv.org/abs/2512.09673)
*Yuzhu Chen,Tian Qin,Xinmei Tian,Fengxiang He,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文研究等变神经网络的表达能力，发现等变约束会限制2层ReLU网络的表达能力，但可通过扩大模型规模来补偿，且等变网络仍具有更低复杂度假设空间，泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 等变神经网络将对称性作为归纳偏置，在许多领域取得了良好性能，但其表达能力尚未得到充分理解。本文旨在研究等变约束对网络表达能力的影响。

Method: 聚焦于2层ReLU网络，通过分析边界超平面和通道向量，构建示例展示等变约束如何限制表达能力，并研究通过扩大模型规模来补偿这种限制的方法。

Result: 等变约束确实会严格限制表达能力，但这种限制可以通过扩大模型规模来补偿。尽管模型规模更大，等变网络对应的假设空间复杂度更低，表明其具有更好的泛化能力。

Conclusion: 等变神经网络在表达能力上存在限制，但可通过模型扩展来克服，同时保持更低的假设空间复杂度，这解释了等变网络在实际应用中表现优异的原因。

Abstract: Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.

</details>


### [27] [StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing](https://arxiv.org/abs/2512.08968)
*Mustapha Hamdi*

Main category: cs.LG

TL;DR: StructuredDNA是一种基于生物物理能量最小化的稀疏Transformer路由框架，通过语义能量引导的动态专家选择，显著降低计算能耗，在专业和开放领域基准测试中都表现出优异的能量效率和语义稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着大型计算模型的快速扩展，能源和计算成本急剧增加。受生物系统中结构和功能从低能量配置中涌现的启发，需要开发一种能量感知的模块化稀疏架构来替代传统的密集专家混合路由。

Method: 提出StructuredDNA框架，用基于语义能量最小化的生物物理能量引导路由层替代密集的专家混合路由。输入被动态分组为语义密码子，路由通过最小化结合了内聚性、不确定性和计算成本的全局能量函数来选择单个专家。

Result: 在BioASQ基准测试（K=50）中，实现了97.7%的能量利用密度降低和0.998的语义稳定性指数。在WikiText-103上展示了语义缩放定律，在扩展到2048个专家时仍保持超过99%的能量效率，证明该架构能泛化到开放领域。

Conclusion: StructuredDNA建立了生物物理原理与Transformer稀疏专家路由之间的明确联系，为未来的能量感知、模块化和可扩展计算系统提供了稳健的领域无关范式。虽然目前是概念验证研究，但为扩展到更大模型、数据集和硬件平台指明了方向。

Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .

</details>


### [28] [Provably Learning from Modern Language Models via Low Logit Rank](https://arxiv.org/abs/2512.09892)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 本文提出了一种基于低对数秩假设的高效算法，用于从查询中学习近似低对数秩模型，为现代语言模型提供了首个端到端学习保证。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型虽然复杂，但经验观察表明它们都具有近似低对数秩特性。本文旨在理解如何利用这种结构来获得可证明的学习保证，特别是考虑到低对数秩模型可以编码难以学习的分布（如噪声奇偶性）。

Method: 采用查询学习模型，使用对数查询来反映常见API的访问模式。提出了一种高效算法，可以从查询中学习任何近似低对数秩模型。

Result: 开发出了一种高效算法，能够从查询中学习近似低对数秩模型。该算法为生成模型提供了首个端到端学习保证，且该模型很可能捕捉了现代语言模型的行为。

Conclusion: 本文通过利用现代语言模型经验观察到的低对数秩特性，为生成模型提供了首个端到端学习保证，为理解语言模型的学习机制提供了理论框架。

Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.

</details>


### [29] [Learning Robust Representations for Malicious Content Detection via Contrastive Sampling and Uncertainty Estimation](https://arxiv.org/abs/2512.08969)
*Elias Hossain,Umesh Biswas,Charan Gudla,Sai Phani Parsa*

Main category: cs.LG

TL;DR: UCF是一个正例-未标记表示学习框架，通过不确定性感知对比损失、自适应温度缩放和自注意力LSTM编码器，在噪声和不平衡条件下改善分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声和不平衡条件下（如恶意内容分类）的传统分类方法性能下降问题，需要一种能够处理正例-未标记学习场景的鲁棒表示学习框架。

Method: 提出不确定性对比框架（UCF），包含：1）不确定性感知对比损失，基于样本置信度动态调整对比权重；2）自适应温度缩放，适应批次级变异性；3）自注意力引导的LSTM编码器；4）使用正锚点稳定训练。

Result: 在恶意内容分类任务中，UCF生成的嵌入使多个传统分类器达到93.38%以上的准确率、0.93以上的精确率、接近完美的召回率，假阴性极少且ROC-AUC分数具有竞争力。可视化分析显示正例和未标记实例之间有清晰分离。

Conclusion: UCF是一个鲁棒且可扩展的PU学习解决方案，适用于网络安全和生物医学文本挖掘等高风险领域，能够产生校准的、有区分度的嵌入表示。

Abstract: We propose the Uncertainty Contrastive Framework (UCF), a Positive-Unlabeled (PU) representation learning framework that integrates uncertainty-aware contrastive loss, adaptive temperature scaling, and a self-attention-guided LSTM encoder to improve classification under noisy and imbalanced conditions. UCF dynamically adjusts contrastive weighting based on sample confidence, stabilizes training using positive anchors, and adapts temperature parameters to batch-level variability. Applied to malicious content classification, UCF-generated embeddings enable multiple traditional classifiers to achieve more than 93.38% accuracy, precision above 0.93, and near-perfect recall, with minimal false negatives and competitive ROC-AUC scores. Visual analyses confirm clear separation between positive and unlabeled instances, highlighting the framework's ability to produce calibrated, discriminative embeddings. These results position UCF as a robust and scalable solution for PU learning in high-stakes domains such as cybersecurity and biomedical text mining.

</details>


### [30] [Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs](https://arxiv.org/abs/2512.08976)
*Isha Chaturvedi,Anjana Nair,Yushen Li,Adhitya Rajendra Kumar,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma*

Main category: cs.LG

TL;DR: CRM是一种无需训练的诊断方法，通过对比性区域掩码揭示多模态大语言模型在思维链推理中对特定视觉区域的依赖关系


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于最终答案或注意力图，无法提供因果、步骤级别的归因，需要更精细的方法来评估多模态模型推理的忠实性和鲁棒性

Method: 通过系统性地掩码标注的视觉区域，对比掩码与未掩码条件下的推理轨迹，分析模型在思维链推理每一步中对视觉区域的依赖

Result: 在VisArgs等数据集上揭示了两种失败模式：一些模型保持推理结构但在证据缺失时产生幻觉，另一些模型紧密依赖视觉线索但在扰动下崩溃

Conclusion: CRM将视觉基准重构为诊断工具，强调需要不仅评估性能，还要评估推理的鲁棒性和忠实性的多模态评估框架

Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.

</details>


### [31] [Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment](https://arxiv.org/abs/2512.09013)
*Paul Garnier,Pablo Jeken-Rico,Vincent Lannelongue,Chiara Faitini,Aurèle Goetz,Lea Chanvillard,Ramy Nemer,Jonathan Viquerat,Ugo Pelissier,Philippe Meliga,Jacques Sédat,Thomas Liebig,Yves Chau,Elie Hachem*

Main category: cs.LG

TL;DR: 提出基于图神经网络的颅内动脉瘤血流动力学替代模型，能在1分钟内从血管几何结构生成全场血流动力学预测，无需计算专家参与。


<details>
  <summary>Details</summary>
Motivation: 传统计算流体动力学模拟准确但速度慢、需要专业知识；4D Flow MRI等临床成像技术分辨率不足且昂贵不实用。需要一种快速、准确、临床可用的血流动力学预测方法。

Method: 使用图神经网络替代模型，结合图变换器和自回归预测，在患者特异性动脉瘤高保真模拟数据集上训练，直接从血管几何结构模拟血流、壁面剪切应力和振荡剪切指数。

Result: 模型能在每个心动周期不到1分钟内生成全场血流动力学预测，泛化能力强，适用于未见过的患者几何结构和流入条件，无需网格特定校准。

Conclusion: 该工作将高保真模拟从专家专用研究工具转变为可部署的数据驱动决策支持系统，实现床边实时动脉瘤分析，为临床可解释的血流动力学预测奠定基础。

Abstract: Intracranial aneurysms remain a major cause of neurological morbidity and mortality worldwide, where rupture risk is tightly coupled to local hemodynamics particularly wall shear stress and oscillatory shear index. Conventional computational fluid dynamics simulations provide accurate insights but are prohibitively slow and require specialized expertise. Clinical imaging alternatives such as 4D Flow MRI offer direct in-vivo measurements, yet their spatial resolution remains insufficient to capture the fine-scale shear patterns that drive endothelial remodeling and rupture risk while being extremely impractical and expensive.
  We present a graph neural network surrogate model that bridges this gap by reproducing full-field hemodynamics directly from vascular geometries in less than one minute per cardiac cycle. Trained on a comprehensive dataset of high-fidelity simulations of patient-specific aneurysms, our architecture combines graph transformers with autoregressive predictions to accurately simulate blood flow, wall shear stress, and oscillatory shear index. The model generalizes across unseen patient geometries and inflow conditions without mesh-specific calibration. Beyond accelerating simulation, our framework establishes the foundation for clinically interpretable hemodynamic prediction. By enabling near real-time inference integrated with existing imaging pipelines, it allows direct comparison with hospital phase-diagram assessments and extends them with physically grounded, high-resolution flow fields.
  This work transforms high-fidelity simulations from an expert-only research tool into a deployable, data-driven decision support system. Our full pipeline delivers high-resolution hemodynamic predictions within minutes of patient imaging, without requiring computational specialists, marking a step-change toward real-time, bedside aneurysm analysis.

</details>


### [32] [Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques](https://arxiv.org/abs/2512.09054)
*Alon Arad,Saharon Rosset*

Main category: cs.LG

TL;DR: 提出两种新的多类校准方法：NA-FIR（归一化感知保序回归）和SCIR（累积双变量保序回归），通过考虑概率归一化约束来改进多类概率预测的校准效果。


<details>
  <summary>Details</summary>
Motivation: 在多类监督学习任务中，准确可靠的概率预测至关重要，良好校准的模型能够支持理性决策。虽然保序回归在二元校准中很有效，但通过一对多扩展到多类问题时效果不如参数方法，限制了实际应用。

Method: 提出了两种新的保序归一化感知技术：1) NA-FIR：将归一化直接纳入优化过程；2) SCIR：将问题建模为累积双变量保序回归。两种方法都基于实践者期望的自然直观假设。

Result: 在多种文本和图像分类数据集及不同模型架构上的实证评估表明，该方法在负对数似然（NLL）和期望校准误差（ECE）指标上持续改进。

Conclusion: 提出的归一化感知保序回归方法有效地解决了多类校准问题，通过考虑概率归一化约束，相比传统方法获得了更好的校准性能。

Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.

</details>


### [33] [A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS](https://arxiv.org/abs/2512.09059)
*Marina Vicens-Miquel,Amy McGovern,Aaron J. Hill,Efi Foufoula-Georgiou,Clement Guilloteau,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 该研究提出了一种基于扩散的深度学习框架，系统比较了三种残差预测策略，用于高分辨率降水预报，在1-12小时预报范围内均优于传统数值天气预报系统。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预报对水文气象风险管理至关重要，特别是预测可能导致山洪暴发和基础设施破坏的极端降雨。传统数值天气预报系统在预报技能上仍有局限，需要深度学习方法来提升预报准确性和可靠性。

Method: 开发了基于扩散的深度学习框架，比较三种残差预测策略：1）纯数据驱动模型（仅使用MRMS雷达观测数据）；2）校正模型（仅使用HRRR数值预报数据）；3）混合模型（结合MRMS和HRRR变量）。在1公里空间分辨率下进行1-12小时自回归滚动预报，并采用针对残差学习设置的校准不确定性量化方法。

Result: 在所有预报时效上，深度学习框架在像素级和空间统计指标上均优于HRRR基准。混合模型在最短预报时效表现最佳，而HRRR校正模型在较长预报时效（达12小时）表现最优，保持高预报技能。不确定性量化增强了预报可靠性。

Conclusion: 该研究通过系统比较不同数据源的贡献，推进了基于深度学习的降水预报技术，显著提升了预报技能、可靠性和区域适用性，特别是较长预报时效的改进对应急准备和决策制定具有重要意义。

Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.

</details>


### [34] [Contrast transfer functions help quantify neural network out-of-distribution generalization in HRTEM](https://arxiv.org/abs/2512.09067)
*Luis Rangel DaCosta,Mary C. Scott*

Main category: cs.LG

TL;DR: 该研究通过模拟数据系统探究了神经网络在HRTEM纳米颗粒分割任务中的分布外泛化能力，发现模型性能随成像条件偏移而平稳可预测地下降。


<details>
  <summary>Details</summary>
Motivation: 神经网络在分布外（OOD）泛化方面表现不佳，这在实验条件变化或缺乏真实知识时尤其成问题。需要系统理解神经网络在HRTEM纳米颗粒分割中的OOD泛化行为，以支持其在实验工作流中的可靠部署。

Method: 使用随机结构采样和多层切片模拟生成合成数据，训练和测试超过12,000个神经网络分割模型。开发基于HRTEM对比传递函数的框架来比较数据集信息内容并量化OOD域偏移。

Result: 神经网络分割模型在成像条件偏移时表现出显著的性能稳定性，但性能会随着训练分布偏移而平稳且可预测地下降。建立了量化OOD域偏移与模型性能下降之间的关系。

Conclusion: 模拟数据为系统研究OOD泛化提供了有力工具，但该方法在解释原子结构等其他OOD偏移方面存在局限，需要补充技术来全面理解神经网络在复杂科学任务中的泛化行为。

Abstract: Neural networks, while effective for tackling many challenging scientific tasks, are not known to perform well out-of-distribution (OOD), i.e., within domains which differ from their training data. Understanding neural network OOD generalization is paramount to their successful deployment in experimental workflows, especially when ground-truth knowledge about the experiment is hard to establish or experimental conditions significantly vary. With inherent access to ground-truth information and fine-grained control of underlying distributions, simulation-based data curation facilitates precise investigation of OOD generalization behavior. Here, we probe generalization with respect to imaging conditions of neural network segmentation models for high-resolution transmission electron microscopy (HRTEM) imaging of nanoparticles, training and measuring the OOD generalization of over 12,000 neural networks using synthetic data generated via random structure sampling and multislice simulation. Using the HRTEM contrast transfer function, we further develop a framework to compare information content of HRTEM datasets and quantify OOD domain shifts. We demonstrate that neural network segmentation models enjoy significant performance stability, but will smoothly and predictably worsen as imaging conditions shift from the training distribution. Lastly, we consider limitations of our approach in explaining other OOD shifts, such as of the atomic structures, and discuss complementary techniques for understanding generalization in such settings.

</details>


### [35] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: DeepTherm：无需热相关死亡历史数据的模块化致命热浪早期预警系统，使用深度学习双预测管道分离基线死亡率


<details>
  <summary>Details</summary>
Motivation: 城市严重热浪对公共健康构成重大威胁，需要建立早期预警策略。现有方法难以预测即将到来的致命热浪，因为定义和估计热相关死亡率很困难，且早期预警系统需要满足数据可用性、时空鲁棒性和决策成本等额外要求。

Method: 提出DeepTherm模块化早期预警系统，采用深度学习双预测管道，将无热浪和其他异常事件时的基线死亡率与全因死亡率分离，无需热相关死亡历史数据。

Result: 在西班牙真实数据上评估显示，DeepTherm在不同地区、时间段和人群组中表现一致、鲁棒且准确，同时允许在漏报和误报之间进行权衡。

Conclusion: DeepTherm为解决致命热浪预测挑战提供了有效方案，展示了深度学习在早期预警系统中的灵活性，能够在不依赖历史热相关死亡数据的情况下实现可靠预测。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [36] [GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions](https://arxiv.org/abs/2512.09084)
*Oscar Eliasson*

Main category: cs.LG

TL;DR: GS-KAN提出了一种轻量化的KAN架构，通过每层共享一个父函数并应用可学习的线性变换来构建边缘函数，解决了标准KAN参数爆炸的问题，在保持高逼近能力的同时显著提升参数效率。


<details>
  <summary>Details</summary>
Motivation: 标准Kolmogorov-Arnold网络（KANs）虽然具有高逼近能力，但由于每条边都需要独立的参数化，导致参数效率低下，在高维场景下参数爆炸问题严重，限制了其实际应用。

Method: GS-KAN基于David Sprecher对叠加定理的改进，每层只学习一个共享的父函数，然后通过可学习的线性变换为每条边生成独特的函数，从而大幅减少参数数量。

Result: 在连续函数逼近任务中，GS-KAN优于MLP和标准KAN基线；在表格数据回归中与现有KAN架构竞争；在高维分类任务中超越MLP；最重要的是能在严格参数约束下部署于高维场景。

Conclusion: GS-KAN通过参数共享机制解决了标准KAN的参数爆炸问题，实现了KAN架构在高维场景下的可行部署，为Kolmogorov-Arnold表示定理的实际应用提供了轻量化解决方案。

Abstract: The Kolmogorov-Arnold representation theorem offers a theoretical alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate functions on edges rather than nodes. While recent implementations such as Kolmogorov-Arnold Networks (KANs) demonstrate high approximation capabilities, they suffer from significant parameter inefficiency due to the requirement of maintaining unique parameterizations for every network edge. In this work, we propose GS-KAN (Generalized Sprecher-KAN), a lightweight architecture inspired by David Sprecher's refinement of the superposition theorem. GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer. We evaluate GS-KAN against existing KAN architectures and MLPs across synthetic function approximation, tabular data regression and image classification tasks. Our results demonstrate that GS-KAN outperforms both MLPs and standard KAN baselines on continuous function approximation tasks while maintaining superior parameter efficiency. Additionally, GS-KAN achieves competitive performance with existing KAN architectures on tabular regression and outperforms MLPs on high-dimensional classification tasks. Crucially, the proposed architecture enables the deployment of KAN-based architectures in high-dimensional regimes under strict parameter constraints, a setting where standard implementations are typically infeasible due to parameter explosion. The source code is available at https://github.com/rambamn48/gs-impl.

</details>


### [37] [Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks](https://arxiv.org/abs/2512.09103)
*Shihao Li,Jiachen Li,Dongmei Chen*

Main category: cs.LG

TL;DR: 提出首个针对深度神经网络数据归因的认证鲁棒性框架，通过自然Wasserstein度量消除谱放大效应，实现非空认证边界


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法对分布扰动敏感，缺乏可靠性保证，需要建立认证鲁棒的归因框架

Method: 提出统一认证鲁棒归因框架：针对凸模型推导Wasserstein鲁棒影响函数；针对深度网络提出自然Wasserstein度量，消除谱放大效应，建立W-TRAK方法

Result: 在CIFAR-10上，自然W-TRAK认证68.7%的排序对，而欧几里得基线为0%；自影响项在标签噪声检测中达到0.970 AUROC，仅检查前20%训练数据即可识别94.1%的污染标签

Conclusion: 自然Wasserstein度量有效消除深度网络中的谱放大问题，实现首个神经网络归因的非空认证边界，为鲁棒数据归因提供理论框架

Abstract: Data attribution methods identify which training examples are responsible for a model's predictions, but their sensitivity to distributional perturbations undermines practical reliability. We present a unified framework for certified robust attribution that extends from convex models to deep networks. For convex settings, we derive Wasserstein-Robust Influence Functions (W-RIF) with provable coverage guarantees. For deep networks, we demonstrate that Euclidean certification is rendered vacuous by spectral amplification -- a mechanism where the inherent ill-conditioning of deep representations inflates Lipschitz bounds by over $10{,}000\times$. This explains why standard TRAK scores, while accurate point estimates, are geometrically fragile: naive Euclidean robustness analysis yields 0\% certification. Our key contribution is the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's own feature covariance. This eliminates spectral amplification, reducing worst-case sensitivity by $76\times$ and stabilizing attribution estimates. On CIFAR-10 with ResNet-18, Natural W-TRAK certifies 68.7\% of ranking pairs compared to 0\% for Euclidean baselines -- to our knowledge, the first non-vacuous certified bounds for neural network attribution. Furthermore, we prove that the Self-Influence term arising from our analysis equals the Lipschitz constant governing attribution stability, providing theoretical grounding for leverage-based anomaly detection. Empirically, Self-Influence achieves 0.970 AUROC for label noise detection, identifying 94.1\% of corrupted labels by examining just the top 20\% of training data.

</details>


### [38] [Learning Unmasking Policies for Diffusion Language Models](https://arxiv.org/abs/2512.09106)
*Metod Jazbec,Theo X. Olausson,Louis Béthune,Pierre Ablin,Michael Kirchhof,Joao Monterio,Victor Turrisi,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 本文提出用强化学习训练扩散语言模型的采样策略，替代需要手动调优的启发式方法，在保持生成质量的同时提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型使用启发式采样策略（如置信度阈值）存在需要手动调优、在大缓冲区下性能下降的问题，需要更自动化的采样方法。

Method: 将掩码扩散采样形式化为马尔可夫决策过程，使用单层Transformer架构的轻量级策略网络，根据dLLM的token置信度做出解掩码决策，通过强化学习训练。

Result: 训练的策略在半自回归生成中达到最先进启发式方法的性能，在全扩散设置中表现更优，且能泛化到新的dLLM和更长序列，但在域外数据和精度-效率权衡的精细调优方面存在挑战。

Conclusion: 强化学习训练的采样策略能有效替代启发式方法，实现自动化的高效采样，但在泛化性和调优灵活性方面仍需改进。

Abstract: Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.

</details>


### [39] [Spectral Embedding via Chebyshev Bases for Robust DeepONet Approximation](https://arxiv.org/abs/2512.09165)
*Muhammad Abid,Omer San*

Main category: cs.LG

TL;DR: 提出SEDONet，一种基于Chebyshev谱嵌入的DeepONet变体，用于解决标准DeepONet在边界域上处理非周期特征时的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet的trunk设计基于全连接层处理原始坐标，难以有效表示边界域PDE中常见的尖锐梯度、边界层和非周期结构。需要一种更适合有界域Dirichlet或Neumann边界条件的归纳偏置。

Method: 提出SEDONet，用固定的Chebyshev谱字典驱动trunk，替代原始坐标输入。这种非周期谱嵌入为有界域提供了定制的归纳偏置，使学习算子能捕捉傅里叶或MLP trunk难以表示的精细非周期特征。

Result: 在2D Poisson、1D Burgers、1D对流扩散、Allen-Cahn动力学和Lorenz-96混沌系统等PDE基准测试中，SEDONet始终获得最低的相对L2误差，比基线DeepONet平均改进约30-40%，在非周期几何上优于傅里叶嵌入变体。谱分析显示SEDONet更准确地保留高频和边界局部特征。

Conclusion: SEDONet为DeepONet提供了一个简单、参数中性的修改，为有界域PDE的代理建模提供了鲁棒高效的谱框架。Chebyshev嵌入在非周期算子学习中具有重要价值。

Abstract: Deep Operator Networks (DeepONets) have become a central tool in data-driven operator learning, providing flexible surrogates for nonlinear mappings arising in partial differential equations (PDEs). However, the standard trunk design based on fully connected layers acting on raw spatial or spatiotemporal coordinates struggles to represent sharp gradients, boundary layers, and non-periodic structures commonly found in PDEs posed on bounded domains with Dirichlet or Neumann boundary conditions. To address these limitations, we introduce the Spectral-Embedded DeepONet (SEDONet), a new DeepONet variant in which the trunk is driven by a fixed Chebyshev spectral dictionary rather than coordinate inputs. This non-periodic spectral embedding provides a principled inductive bias tailored to bounded domains, enabling the learned operator to capture fine-scale non-periodic features that are difficult for Fourier or MLP trunks to represent. SEDONet is evaluated on a suite of PDE benchmarks including 2D Poisson, 1D Burgers, 1D advection-diffusion, Allen-Cahn dynamics, and the Lorenz-96 chaotic system, covering elliptic, parabolic, advective, and multiscale temporal phenomena, all of which can be viewed as canonical problems in computational mechanics. Across all datasets, SEDONet consistently achieves the lowest relative L2 errors among DeepONet, FEDONet, and SEDONet, with average improvements of about 30-40% over the baseline DeepONet and meaningful gains over Fourier-embedded variants on non-periodic geometries. Spectral analyses further show that SEDONet more accurately preserves high-frequency and boundary-localized features, demonstrating the value of Chebyshev embeddings in non-periodic operator learning. The proposed architecture offers a simple, parameter-neutral modification to DeepONets, delivering a robust and efficient spectral framework for surrogate modeling of PDEs on bounded domains.

</details>


### [40] [Understanding the Failure Modes of Transformers through the Lens of Graph Neural Networks](https://arxiv.org/abs/2512.09182)
*Hunjae Lee*

Main category: cs.LG

TL;DR: 本文从图神经网络理论视角分析Transformer的失败模式，将深度学习视为可学习的信息混合与传播过程，揭示解码器Transformer因果性导致的信息传播几何特性问题，并统一现有解决方案的理论基础。


<details>
  <summary>Details</summary>
Motivation: Transformer虽然表现优异，但存在令人惊讶的失败模式和可预测的不对称性能退化。目前缺乏对这些失败模式的理论理解，现有解决方案多为启发式而非理论驱动。本文旨在通过GNN理论视角填补这一理论空白。

Method: 1. 将深度学习（包括Transformer）框架化为可学习的信息混合与传播过程；2. 运用GNN理论中丰富的信息传播瓶颈文献分析Transformer失败模式；3. 分析解码器Transformer因果性导致的信息传播几何特性；4. 将现有启发式解决方案统一到理论框架下。

Result: 1. 识别了GNN与Transformer共享的许多问题；2. 揭示了因果解码器Transformer信息传播的几何特性导致可预测且严重的失败模式；3. 为现有启发式解决方案提供了理论解释和统一框架；4. 提出了针对特定Transformer失败模式的改进方向。

Conclusion: 本文成功搭建了Transformer经验失败模式与理论理解之间的桥梁，通过GNN理论视角为Transformer失败模式提供了系统性分析框架，并为未来更理论驱动的解决方案设计奠定了基础。

Abstract: Transformers and more specifically decoder-only transformers dominate modern LLM architectures. While they have shown to work exceptionally well, they are not without issues, resulting in surprising failure modes and predictably asymmetric performance degradation. This article is a study of many of these observed failure modes of transformers through the lens of graph neural network (GNN) theory. We first make the case that much of deep learning, including transformers, is about learnable information mixing and propagation. This makes the study of model failure modes a study of bottlenecks in information propagation. This naturally leads to GNN theory, where there is already a rich literature on information propagation bottlenecks and theoretical failure modes of models. We then make the case that many issues faced by GNNs are also experienced by transformers. In addition, we analyze how the causal nature of decoder-only transformers create interesting geometric properties in information propagation, resulting in predictable and potentially devastating failure modes. Finally, we observe that existing solutions in transformer research tend to be ad-hoc and driven by intuition rather than grounded theoretical motivation. As such, we unify many such solutions under a more theoretical perspective, providing insight into why they work, what problem they are actually solving, and how they can be further improved to target specific failure modes of transformers. Overall, this article is an attempt to bridge the gap between observed failure modes in transformers and a general lack of theoretical understanding of them in this space.

</details>


### [41] [Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach](https://arxiv.org/abs/2512.09198)
*Phevos Paschalidis,Vasiliki Stoumpou,Lisa Everest,Yu Ma,Talhat Azemi,Jawad Haider,Steven Zweibel,Eleftherios M. Protopapas,Jeff Mather,Maciej Tysarowski,George E. Sarris,Robert C. Hagberg,Howard L. Haronian,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 开发数据驱动的临床决策工具，通过整合多国多源数据，为TAVR手术选择最优瓣膜类型，以降低永久起搏器植入风险。


<details>
  <summary>Details</summary>
Motivation: TAVR已成为治疗严重主动脉瓣狭窄的微创方法，但不同瓣膜类型的选择缺乏明确指南，永久起搏器植入是主要术后并发症，需要个性化决策工具来优化瓣膜选择。

Method: 整合美国和希腊患者数据，结合人口统计学、CT扫描和超声心动图三种数据源，采用叶级分析利用人群异质性，避免与不确定的反事实风险估计进行基准比较。

Result: 最终处方模型在美国内部人群和希腊验证队列中，分别将PPI率降低了26%和16%，相比当前标准治疗有显著改善。

Conclusion: 这是首个统一的、个性化的TAVR瓣膜选择处方策略，通过数据驱动方法显著降低了永久起搏器植入风险，为临床决策提供了有力支持。

Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.

</details>


### [42] [LLMs for Analog Circuit Design Continuum (ACDC)](https://arxiv.org/abs/2512.09199)
*Yasaman Esfandiari,Jocelyn Rego,Austin Meyer,Jonathan Gallagher,Mia Levy*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型在模拟电路设计中的可靠性和鲁棒性，发现模型对数据格式敏感、生成设计不稳定、对未见电路配置泛化能力有限等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和Transformer架构在各种自然语言任务中表现出色，但它们在真实工程领域（特别是需要领域特定推理、物理约束和结构化表示的模拟电路设计）的可靠性和鲁棒性尚未充分探索，限制了其在人类中心工作流程中的实际应用。

Method: 研究LLMs在模拟电路设计中的适用性和一致性，重点关注AI辅助设计中人类保持在循环中的场景。研究不同数据表示如何影响模型行为，比较较小模型（如T5、GPT-2）与较大基础模型（如Mistral-7B、GPT-oss-20B）在不同训练条件下的表现。

Result: 研究结果突显了关键可靠性挑战：对数据格式的敏感性、生成设计的不稳定性、以及对未见电路配置的有限泛化能力。

Conclusion: 这些发现为LLMs作为增强人类在复杂工程任务中能力的工具的局限性和潜力提供了早期证据，为设计可靠、可部署的基础模型用于结构化真实世界应用提供了见解。

Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.

</details>


### [43] [Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers](https://arxiv.org/abs/2512.09202)
*Jinming Lu,Jiayi Tian,Yequan Zhao,Hai Li,Zheng Zhang*

Main category: cs.LG

TL;DR: 提出一个用于边缘设备的高效PINN训练框架，通过量化训练、Stein估计器和张量分解实现显著的速度提升和能耗节省。


<details>
  <summary>Details</summary>
Motivation: PINNs在求解偏微分方程方面很有前景，但在资源受限的边缘设备上部署面临计算和内存开销大的挑战，主要源于高阶自动微分、密集张量运算和全精度算术。

Method: 提出一个集成全量化训练、Stein估计器残差损失计算和张量分解权重压缩的框架，包含三个创新：混合精度训练方法（SMX格式）、基于差分的Stein估计器量化方案、TT层的部分重建方案，并设计了PINTA硬件加速器。

Result: 在2-D泊松、20-D HJB和100-D热方程上的实验表明，该框架在保持与全精度基线相当或更好精度的同时，实现了5.5倍到83.5倍的速度提升和159.6倍到2324.1倍的能耗节省。

Conclusion: 这项工作实现了边缘设备上的实时PDE求解，为大规模能源高效的科学计算铺平了道路。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.

</details>


### [44] [Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation](https://arxiv.org/abs/2512.09267)
*Ce Wang,Weihang Dai,Hanru Bai,Xiaomeng Li*

Main category: cs.LG

TL;DR: 提出一种半监督对比回归方法，通过谱排序算法从标记和未标记样本中恢复序数关系，减少对昂贵标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法高度依赖标签信息来恢复特征的序数关系，限制了在半监督回归中的应用。需要扩展对比回归方法以利用未标记数据，减少对昂贵标注的依赖。

Method: 1) 使用标记和未标记样本构建特征相似度矩阵反映样本间关系；2) 通过谱排序算法恢复未标记样本的准确序数排名；3) 标记样本提供正则化指导；4) 使用动态规划算法选择鲁棒特征；5) 将恢复的序数关系用于未标记样本的对比学习；6) 用序数排名监督未标记样本的预测。

Result: 在多个数据集上的实验验证表明，该方法超越了现有的最先进半监督深度回归方法，提供了理论保证和实证验证。

Conclusion: 该方法成功扩展了对比回归到半监督设置，通过利用未标记数据提高了特征表示学习能力，实现了更鲁棒的结果，代码已开源。

Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.

</details>


### [45] [Goal inference with Rao-Blackwellized Particle Filters](https://arxiv.org/abs/2512.09269)
*Yixuan Wang,Dan P. Guralnik,Warren E. Dixon*

Main category: cs.LG

TL;DR: 使用Rao-Blackwellized粒子滤波器进行移动代理意图推断，通过分析边缘化线性高斯子结构提高样本效率，引入两种估计器并量化信息泄露。


<details>
  <summary>Details</summary>
Motivation: 从噪声轨迹观测中推断移动代理的最终目标是一个基本估计问题，特别是在代理具有可证明实用稳定性特性的闭环行为假设下。

Method: 使用Rao-Blackwellized粒子滤波器变体，利用代理动态的闭式形式分析边缘化线性高斯子结构，仅更新粒子权重。引入两种估计器：基于RBPF权重的高斯混合模型和限制在有效样本上的简化版本。

Result: 通过信息论泄露指标量化对手恢复代理意图的能力，提供KL散度的可计算下界。实验显示对合规代理的快速准确意图恢复，简化估计器性能接近完整版本。

Conclusion: 该方法能有效推断代理意图，为设计意图混淆控制器提供基础，简化估计器在保持性能的同时降低计算复杂度。

Abstract: Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.

</details>


### [46] [Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices](https://arxiv.org/abs/2512.09313)
*Yuki Oda,Yuta Ono,Hiroshi Nakamura,Hideki Takase*

Main category: cs.LG

TL;DR: 提出Hetero-SplitEE方法，通过异构早期退出机制让不同计算能力的IoT设备选择不同的分割点，支持异构设备协同训练共享神经网络。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习方法假设客户端同质性和统一分割点，不适用于计算资源异构的真实IoT系统，限制了实际部署。

Method: 提出Hetero-SplitEE方法，集成异构早期退出到分层训练中，让每个客户端根据计算能力选择不同分割点；提出两种协同训练策略：顺序策略（顺序训练共享服务器模型）和平均策略（并行训练定期跨层聚合）。

Result: 在CIFAR-10、CIFAR-100和STL-10数据集上使用ResNet-18的实验表明，该方法在保持竞争力的准确率的同时，有效支持不同的计算约束。

Conclusion: Hetero-SplitEE能够在异构IoT生态系统中实现实用的协同深度学习部署，支持设备计算能力多样性。

Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.

</details>


### [47] [Self-Supervised Learning with Gaussian Processes](https://arxiv.org/abs/2512.09322)
*Yunshan Duan,Sinead Williamson*

Main category: cs.LG

TL;DR: 提出GPSSL方法，使用高斯过程进行自监督学习，无需显式定义正样本对，并能提供不确定性量化


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法需要生成相似样本对，这在某些数据类型中具有挑战性，且缺乏不确定性量化能力，在样本外预测中表现不佳

Method: 提出高斯过程自监督学习(GPSSL)，将高斯过程先验施加在表示上，通过最小化损失函数获得广义贝叶斯后验，利用GP的协方差函数自然地将相似单元的表示拉近

Result: GPSSL在多个数据集上的分类和回归任务中，在准确性、不确定性量化和误差控制方面优于传统方法

Conclusion: GPSSL提供了一种无需显式正样本对的自监督学习框架，能够进行不确定性量化并传播到下游任务，与核PCA和VICReg相关但具有后验不确定性优势

Abstract: Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.

</details>


### [48] [Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design](https://arxiv.org/abs/2512.09329)
*Amin Tavakoli,Raswanth Murugan,Ozan Gokdemir,Arvind Ramanathan,Frances Arnold,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出一种快速监督微调蛋白质语言模型的通用方法，利用模型自身生成高质量训练数据，无需昂贵实验数据集，能生成更稳定、功能性更强且新颖的蛋白质序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列建模中，高质量标注数据获取困难，现有监督微调方法依赖昂贵实验数据集，需要一种更高效、通用的蛋白质语言模型微调方法。

Method: 提出轻量级筛选管道与领域特定过滤器结合的方法，利用蛋白质语言模型自身构建高质量训练数据，进行快速监督微调，不依赖预编译实验数据集。

Result: 在色氨酸合酶家族上应用基因组规模蛋白质语言模型（GenSLM），微调后模型生成的序列不仅更新颖，在目标设计约束和蛋白质特性指标上都显示出改进特征。

Conclusion: 该方法提供了一种简单通用的蛋白质语言模型快速监督微调方案，能生成更稳定、功能性更强的酶，同时扩展蛋白质序列空间的探索范围。

Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

</details>


### [49] [Improved Physics-Driven Neural Network to Solve Inverse Scattering Problems](https://arxiv.org/abs/2512.09333)
*Yutong Du,Zicheng Liu,Bo Wu,Jingwei Kou,Hang Li,Changyou Li,Yali Zong,Bo Qi*

Main category: cs.LG

TL;DR: 提出改进的物理驱动神经网络框架IPDNN，用于电磁逆散射问题求解，引入GLOW激活函数、动态散射子区域识别策略和迁移学习，实现高精度、鲁棒且高效的重建。


<details>
  <summary>Details</summary>
Motivation: 电磁逆散射问题求解面临收敛不稳定、计算成本高、实际应用适应性差等挑战，需要结合物理可解释性和实时推理能力的高效方法。

Method: 1. 引入高斯局部振荡抑制窗口激活函数稳定收敛；2. 开发动态散射子区域识别策略自适应细化计算域；3. 结合迁移学习扩展实际应用；4. 融合迭代算法的物理可解释性和神经网络的实时推理能力。

Result: 数值模拟和实验结果表明，相比现有最先进方法，该方法在重建精度、鲁棒性和效率方面均表现出优越性能。

Conclusion: IPDNN框架成功解决了电磁逆散射问题，实现了高精度、鲁棒且高效的重建，为实际应用提供了有效的解决方案。

Abstract: This paper presents an improved physics-driven neural network (IPDNN) framework for solving electromagnetic inverse scattering problems (ISPs). A new Gaussian-localized oscillation-suppressing window (GLOW) activation function is introduced to stabilize convergence and enable a lightweight yet accurate network architecture. A dynamic scatter subregion identification strategy is further developed to adaptively refine the computational domain, preventing missed detections and reducing computational cost. Moreover, transfer learning is incorporated to extend the solver's applicability to practical scenarios, integrating the physical interpretability of iterative algorithms with the real-time inference capability of neural networks. Numerical simulations and experimental results demonstrate that the proposed solver achieves superior reconstruction accuracy, robustness, and efficiency compared with existing state-of-the-art methods.

</details>


### [50] [Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality](https://arxiv.org/abs/2512.09355)
*Junru Zhou,Yicheng Wang,Pan Li*

Main category: cs.LG

TL;DR: 节点锚定子图GNN理论上能近似强分支评分，但实际应用中因计算复杂度高导致内存瓶颈和求解时间慢于MPNN和启发式方法。


<details>
  <summary>Details</summary>
Motivation: 研究图神经网络在混合整数线性规划分支选择中的应用，探索在表达能力和计算效率之间的平衡。标准MPNN表达能力不足，高阶GNN计算成本过高，需要寻找理论中间点。

Method: 采用节点锚定子图GNN作为理论中间方案，证明其表达能力虽低于3-WL但仍能近似强分支评分，并在四个基准数据集上进行实证评估。

Result: 理论证明节点锚定子图GNN能近似强分支评分，但实证显示其O(n)复杂度导致内存瓶颈和求解时间慢于MPNN和启发式方法，计算成本超过决策质量收益。

Conclusion: 对于MILP分支选择，表达性GNN的计算成本目前超过其决策质量收益，未来研究需关注保持效率的表达性提升。

Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.

</details>


### [51] [A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches](https://arxiv.org/abs/2512.09360)
*Boge Lyu,Qianye Yin,Iris Denise Tommelein,Hanyang Liu,Karnamohit Ranka,Karthik Yeluripati,Junzhe Shi*

Main category: cs.LG

TL;DR: 该研究开发了一个基于CSI MasterFormat的建筑材料价格预测框架，整合原材料价格、商品指数等解释变量，LSTM模型在预测精度上表现最佳，比传统ARIMA模型提升高达59%。


<details>
  <summary>Details</summary>
Motivation: 建筑材料价格的持续波动对成本估算、预算编制和项目交付构成重大风险，迫切需要更精细和可扩展的预测方法。

Method: 使用CSI MasterFormat作为目标数据结构，在六位数章节级别进行预测，整合原材料价格、商品指数和宏观经济指标等解释变量，评估LSTM、ARIMA、VECM和Chronos-Bolt四种时间序列模型。

Result: 解释变量的加入显著提升了所有模型的预测性能，LSTM模型表现最佳，RMSE低至1.390，MAPE为0.957，比传统ARIMA模型提升高达59%。验证了框架在多个CSI分区的可扩展性。

Conclusion: 该研究提供了一个稳健的方法论，使业主和承包商能够改进预算实践，在详细级别实现更可靠的成本估算。

Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.

</details>


### [52] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出一个利用最优传输生成伪标签的分子-蛋白质相互作用预测框架，通过整合多种生物数据模态解决数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现有MPI预测模型面临两个主要挑战：1) 标记分子-蛋白质对稀缺，限制了模型性能；2) 大多数方法仅依赖分子和蛋白质特征，忽略了基因、代谢通路等更广泛的生物上下文信息

Method: 首先整合多种生物数据集（分子、蛋白质、基因和通路级相互作用），然后开发基于最优传输的方法为未标记分子-蛋白质对生成高质量伪标签，利用已知相互作用的底层分布指导标签分配

Result: 在多个MPI数据集（包括虚拟筛选和蛋白质检索任务）上评估，在预测准确性和零样本能力方面显著优于现有最先进方法

Conclusion: 该框架不仅改进了MPI预测，还为利用多样化生物数据源解决传统单模态或双模态学习受限问题提供了新范式，为计算生物学和药物发现领域的未来发展铺平道路

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [53] [CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning](https://arxiv.org/abs/2512.09368)
*Mingyuan Li,Chunyu Liu,Zhuojun Li,Xiao Liu,Guangsheng Yu,Bo Du,Jun Shen,Qiang Wu*

Main category: cs.LG

TL;DR: 提出CFLight框架，使用反事实学习增强交通信号控制中的安全性，平衡安全与效率


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在交通信号控制中过于注重效率而忽视安全性，且缺乏可解释性，需要平衡安全与效率并提高可解释性

Method: 提出基于反事实学习的框架，通过结构因果模型预测不同行动的结果，结合反事实模块与额外"X"模块，开发CFLight算法实现近零碰撞控制策略

Result: CFLight在真实世界和合成数据集上显著减少碰撞，提升整体交通性能，优于传统强化学习方法和近期安全强化学习模型

Conclusion: CFLight为强化学习方法提供了一个通用且安全的框架，可应用于其他领域，有效解决交通信号控制中的安全问题

Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.

</details>


### [54] [Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.09369)
*Yezi Liu,William Youngwoo Chung,Hanning Chen,Calvin Yeung,Mohsen Imani*

Main category: cs.LG

TL;DR: PathHD：基于超维度计算的轻量级知识图谱推理框架，用HDC替代神经路径评分，每个查询只需一次LLM调用，实现高效、可解释的KG-LLM推理


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的LLM推理方法依赖重型神经编码器进行路径嵌入评分，或需要多次LLM调用来排序候选，导致高延迟、高GPU成本和不透明的决策，阻碍了可信、可扩展的部署

Method: 提出PathHD框架：1）使用超维度计算（HDC）替代神经路径评分，将关系路径编码为块对角GHRR超向量；2）通过块状余弦相似度和Top-K剪枝对候选进行排序；3）执行一次性LLM裁决，生成最终答案并引用支持路径

Result: 在WebQSP、CWQ和GrailQA数据集上：1）每个查询只需一次LLM调用，达到与强神经基线相当或更好的Hits@1；2）端到端延迟降低40-60%，GPU内存减少3-5倍；3）提供可信的路径基础推理，改善错误诊断和可控性

Conclusion: 精心设计的HDC表示为高效KG-LLM推理提供了实用基础，在准确性、效率和可解释性之间提供了有利的权衡，表明轻量级超维度计算是替代重型神经编码器的可行方案

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

</details>


### [55] [Rates and architectures for learning geometrically non-trivial operators](https://arxiv.org/abs/2512.09376)
*T. Mitchell Roddenberry,Leo Tzou,Ivan Dokmanić,Maarten V. de Hoop,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 该论文将深度学习算子学习理论扩展到包含双纤维化变换（如广义Radon变换和测地线射线变换）的几何积分算子，证明了这类算子没有维度灾难问题，误差随训练样本数超代数衰减。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习算子学习理论主要针对椭圆算子等简单几何情况，但科学机器学习常涉及波传播、对流、流体动力学等具有奇异性传播的问题，需要扩展理论到更广泛的算子类别。

Method: 将学习理论扩展到双纤维化变换这类几何积分算子，提出基于水平集方法的交叉注意力架构，该架构显式编码变换的几何结构。

Result: 证明了双纤维化变换类算子没有维度灾难，误差随训练样本数超代数衰减（快于任意固定幂次）。所提架构具有通用性、稳定性，并能从极少训练样本中学习这类变换。

Conclusion: 该研究扩展了科学机器学习中算子学习的理论框架，为处理涉及奇异性传播的问题提供了理论基础和有效架构，推动了科学机器学习理论的发展。

Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.

</details>


### [56] [Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM](https://arxiv.org/abs/2512.09378)
*Xun Li,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出基于轻量去噪扩散概率模型的联邦蒸馏辅助车辆边缘缓存方案，解决传统联邦学习的通信开销大和车辆移动导致的训练失败问题


<details>
  <summary>Details</summary>
Motivation: 车辆边缘缓存能显著降低车辆用户访问内容的延迟，但需要准确预测用户兴趣内容同时保护隐私。传统联邦学习虽然能保护隐私，但存在频繁模型传输导致的通信开销大，以及车辆可能离开RSU覆盖区域导致训练失败的问题。

Method: 提出基于轻量去噪扩散概率模型（LDPM）的联邦蒸馏辅助车辆边缘缓存方案。该方法结合联邦蒸馏技术，减少模型传输的通信开销，同时使用轻量化的扩散概率模型来适应当前车辆边缘缓存场景。

Result: 仿真结果表明，所提出的车辆边缘缓存方案对车辆速度变化具有良好的鲁棒性，能显著降低通信开销并提高缓存命中率。

Conclusion: 该方案有效解决了传统联邦学习在车辆边缘缓存场景中的通信开销和训练失败问题，为车辆边缘缓存提供了一种高效且隐私保护的解决方案。

Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

</details>


### [57] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出ConFormer框架，整合交通事故和管制数据，通过图传播和引导归一化层动态调整时空关系，在东京和加州数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型通常忽略交通事故、管制等外部因素的影响，导致预测准确性受限。需要整合多源数据并设计能动态适应时空关系的模型。

Method: 1) 构建包含交通事故和管制数据的东京和加州数据集；2) 提出ConFormer框架，结合图传播和引导归一化层，动态调整节点间的时空关系；3) 基于历史模式进行条件化建模。

Result: ConFormer在预测性能和效率上均超越当前最优的STAEFormer，计算成本更低、参数需求更少。在多个指标上持续优于主流时空基线方法。

Conclusion: ConFormer通过整合外部因素和动态调整时空关系，显著提升了交通预测性能，为领域研究提供了新的数据集和有效框架。

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [58] [Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs](https://arxiv.org/abs/2512.09403)
*Sohely Jahan,Ruimin Sun*

Main category: cs.LG

TL;DR: 本文提出一种针对安全对齐医疗大语言模型的黑盒蒸馏攻击，仅通过输出访问就能复制模型推理能力，同时剥离安全机制，揭示了功能-伦理差距的威胁。


<details>
  <summary>Details</summary>
Motivation: 随着医疗大语言模型在临床工作流中的集成，对其对齐鲁棒性和安全性的担忧日益增加。先前工作主要关注分类模型或记忆泄露，而对安全对齐的生成式医疗大语言模型的脆弱性研究不足。

Method: 提出黑盒蒸馏攻击：向Meditron-7B发出48,000个指令查询，收集25,000个良性指令-响应对，在零对齐监督设置下通过参数高效的LoRA微调LLaMA3 8B代理模型。开发动态对抗评估框架，结合基于生成查询的有害提示生成、验证器过滤、类别化失败分析和自适应随机搜索越狱攻击。

Result: 仅花费12美元，代理模型在良性输入上保持高保真度，同时对86%的对抗提示产生不安全完成，远超Meditron-7B（66%）和未调优基础模型（46%）。揭示了明显的功能-伦理差距：任务效用转移而安全对齐崩溃。

Conclusion: 仅使用良性数据的黑盒蒸馏暴露了实际且未被充分认识的威胁：攻击者可以廉价复制医疗大语言模型能力同时剥离安全机制，强调了需要提取感知的安全监控。提出了分层防御系统作为黑盒部署中实时对齐漂移的原型检测器。

Abstract: As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.
  We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments.
  Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.

</details>


### [59] [Cauchy-Schwarz Fairness Regularizer](https://arxiv.org/abs/2512.09467)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: 论文提出基于柯西-施瓦茨散度的公平性正则化器，通过惩罚敏感群体间预测分布的差异来提升群体公平性，相比现有方法具有更紧的泛化界、尺度鲁棒性和对任意预测分布的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有公平性正则化器基于不同的距离度量和设计选择，导致行为难以推理且性能在不同任务间不一致。需要研究什么特性构成好的公平性正则化器，并设计满足这些特性的方法。

Method: 将现有方法分为三类：(1)跨敏感群体匹配预测统计量，(2)对齐潜在表示，(3)直接最小化预测与敏感属性间的依赖性。基于理想距离度量的特性，提出柯西-施瓦茨公平性正则化器，惩罚敏感群体条件预测分布间的经验CS散度。

Result: 在高斯比较下，CS散度比KL散度、最大均值差异和人口统计均等中使用的均值差异具有更紧的界。在四个表格基准和一个图像数据集上的实验表明，CS正则化器在保持竞争性准确率的同时，一致改善人口统计均等和机会均等指标，并在超参数设置下实现更稳定的效用-公平性权衡。

Conclusion: 柯西-施瓦茨散度是构建公平性正则化器的有效距离度量，具有理论优势和实际性能提升。该方法能自然扩展到多个敏感属性，为机器学习中的群体公平性提供了更可靠和一致的解决方案。

Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.

</details>


### [60] [Representation Invariance and Allocation: When Subgroup Balance Matters](https://arxiv.org/abs/2512.09496)
*Anissa Alloula,Charles Jones,Zuzanna Wakefield-Skorniewska,Francesco Quinzan,Bartłomiej Papież*

Main category: cs.LG

TL;DR: 本文提出"潜在分离假说"，认为微调模型对子群表示的依赖程度取决于预训练模型中子群在潜在空间的分离程度，挑战了传统数据平衡假设。


<details>
  <summary>Details</summary>
Motivation: 传统实践假设平衡子群表示能优化模型性能，但近期实证结果与此矛盾：在某些情况下，不平衡数据分布反而改善子群性能，而有时即使训练数据完全缺失某个子群，其性能也不受影响。这促使研究者系统研究子群分配问题。

Method: 在四个视觉和语言模型上进行系统研究，通过改变训练数据组成来表征子群性能对数据平衡的敏感性。提出"潜在分离假说"，将其形式化并提供理论分析，最后通过实证验证。在基础模型微调中展示应用，证明潜在子群分离的定量分析可以指导数据收集和平衡决策。

Result: 研究发现子群性能对数据平衡的敏感性取决于预训练模型中子群在潜在空间的分离程度。当子群在潜在空间中高度分离时，平衡数据分布对性能影响显著；当子群在潜在空间中混合良好时，数据平衡的影响较小甚至可能适得其反。

Conclusion: 传统的数据平衡假设过于简化，实际效果取决于预训练模型中的潜在子群分离程度。提出的"潜在分离假说"为理解子群性能与数据分布关系提供了新框架，并能为实际微调任务中的数据收集和平衡策略提供定量指导。

Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.

</details>


### [61] [Contextual Dynamic Pricing with Heterogeneous Buyers](https://arxiv.org/abs/2512.09513)
*Thodoris Lykouris,Sloan Nietert,Princewill Okoroafor,Chara Podimata,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文研究了具有异质买家群体的上下文动态定价问题，提出了基于乐观后验采样的算法，实现了$\widetilde{O}(K_{\star}\sqrt{dT})$的遗憾界，并在非上下文情况下设计了方差感知的缩放算法。


<details>
  <summary>Details</summary>
Motivation: 现有动态定价研究大多假设买家同质，但实际中买家估值类型存在异质性。本文旨在解决具有未知有限支持度$K_{\star}$的异质买家群体的上下文动态定价问题。

Method: 提出了基于乐观后验采样的上下文定价算法，利用上下文信息和购买反馈来学习买家估值分布。在非上下文情况下，设计了方差感知的缩放算法来优化对$K_{\star}$的依赖。

Result: 上下文算法实现了$\widetilde{O}(K_{\star}\sqrt{dT})$的遗憾界，证明在$d$和$T$维度上达到对数项内的紧致性。非上下文算法获得了对$K_{\star}$的最优依赖。

Conclusion: 本文首次系统研究了异质买家群体的上下文动态定价问题，提出的算法在理论和实践上都具有重要意义，为处理买家异质性提供了有效解决方案。

Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.

</details>


### [62] [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](https://arxiv.org/abs/2512.09517)
*Nabil Anan Orka,Ehtashamul Haque,Maftahul Jannat,Md Abdul Awal,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: QuanvNeXt是一个用于EEG抑郁诊断的端到端全量子卷积模型，通过Cross Residual块减少特征同质性并增强跨特征关系，在两个数据集上达到93.1%准确率和97.2% AUC-ROC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效可靠的EEG抑郁诊断模型，解决现有方法在特征同质性和跨特征关系方面的问题，同时保持参数效率。

Method: 提出QuanvNeXt模型，包含新颖的Cross Residual块，减少特征同质性并增强跨特征关系。使用两个开源EEG数据集进行评估，进行不确定性分析和可解释AI分析。

Result: 在两个数据集上平均准确率93.1%，AUC-ROC 97.2%，优于InceptionTime等基线方法。不确定性分析显示即使在最高扰动(ε=0.1)下，ECE分数保持较低(0.0436-0.1159)。可解释AI分析确认模型能有效识别区分健康对照和抑郁障碍的频谱时间模式。

Conclusion: QuanvNeXt为EEG抑郁诊断建立了一个高效可靠的方法，通过Cross Residual块改善了特征表示，在准确性和鲁棒性方面表现出色。

Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.

</details>


### [63] [Latent-Autoregressive GP-VAE Language Model](https://arxiv.org/abs/2512.09535)
*Yves Ruffenach*

Main category: cs.LG

TL;DR: 提出一种基于高斯过程的完全潜在自回归方案，集成到变分自编码器中，将序列动态从观测空间转移到连续潜在空间，同时通过非自回归解码器保持并行语言生成。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型中部分时间结构是否可以通过潜在空间的概率几何来支持，而不是通过显式的神经操作。旨在将序列动态转移到连续潜在空间，同时保持语言生成的并行性。

Method: 提出完全潜在自回归方案：1）使用因果高斯过程作为先验；2）结构化摊销后验；3）基于正则化ELBO的训练协议；4）非自回归解码器实现并行生成。

Result: 在概念验证框架中，模型能够稳定训练，序列和并行采样变体表现一致。结果表明语言模型的部分时间结构可以通过潜在空间的概率几何来支持。

Conclusion: 语言模型的时间结构可以部分由潜在空间的概率几何支持，而非完全依赖显式神经操作。该方法为序列建模提供了新的视角，将序列动态与并行生成分离。

Abstract: We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.

</details>


### [64] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: 斯坦福睡眠基准：大规模PSG数据集与自监督学习方法评估，用于睡眠分析基础模型开发


<details>
  <summary>Details</summary>
Motivation: 睡眠分析领域缺乏共享数据集和系统性评估方法，阻碍了自监督学习在睡眠基础模型中的应用进展

Method: 构建斯坦福睡眠基准数据集（17,467条记录，163,000+小时），系统评估多种自监督预训练方法在睡眠分期、呼吸暂停诊断、年龄估计和疾病预测等任务上的表现

Result: 多种预训练方法在睡眠分期、呼吸暂停诊断和年龄估计任务上表现相当，但对比学习在疾病和死亡率预测任务上显著优于其他方法且收敛更快

Conclusion: 斯坦福睡眠基准填补了睡眠分析领域的数据集空白，对比学习在临床预测任务中表现优异，将开源数据集、预训练模型和代码以促进睡眠研究

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [65] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 提出基于参数分布噪声的多智能体近端策略优化任务卸载方法（MAPPO-PDN）与线性规划结合的TCSC框架，优化车联网语义通信中的任务卸载


<details>
  <summary>Details</summary>
Motivation: 车联网中结合语义通信与车辆边缘计算能提高边缘任务处理效率，但高速公路场景下的任务卸载优化仍面临挑战

Method: 提出三方协作语义通信框架，将MINLP问题分解为两个子问题：使用MAPPO-PDN优化语义符号数量，使用线性规划优化卸载比例

Result: 仿真结果表明该方案性能优于其他对比算法

Conclusion: TCSC框架能有效优化车联网语义通信中的任务卸载，提高系统性能

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [66] [Membership and Dataset Inference Attacks on Large Audio Generative Models](https://arxiv.org/abs/2512.09654)
*Jakub Proboszcz,Paweł Kochanski,Karol Korszun,Donato Crisostomi,Giorgio Strano,Emanuele Rodolà,Kamil Deja,Jan Dubinski*

Main category: cs.LG

TL;DR: 该论文研究了生成音频模型的版权保护问题，发现单独使用成员推理攻击效果有限，但数据集推理（聚合多个样本证据）在音频领域有效，可作为版权保护机制。


<details>
  <summary>Details</summary>
Motivation: 生成音频模型（基于扩散和自回归架构）的快速发展引发了版权担忧，需要可靠的方法来验证艺术家的作品是否被用于模型训练，以便版权持有人保护其内容。

Method: 研究通过成员推理攻击（MIA）验证音频样本是否属于训练集，发现单一样本效果有限后，转向数据集推理（DI）方法，聚合多个样本的成员证据来评估艺术家作品是否被用于训练。

Result: 成员推理攻击在大规模多样化数据集上效果有限，但数据集推理在音频领域成功有效，为评估艺术家作品是否贡献于模型训练提供了更实用的机制。

Conclusion: 数据集推理是大型音频生成模型时代版权保护和数据集问责的有前景方向，能够更有效地保护艺术家和媒体所有者的版权利益。

Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.

</details>


### [67] [A data-driven approach to linking design features with manufacturing process data for sustainable product development](https://arxiv.org/abs/2512.09690)
*Jiahang Li,Lucas Cazzonelli,Jacqueline Höllig,Markus Doellken,Sven Matthiesen*

Main category: cs.LG

TL;DR: 提出一种数据驱动方法，将设计特征与制造过程数据关联分析，通过机器学习模型实现自动化设计改进建议，支持可持续产品开发。


<details>
  <summary>Details</summary>
Motivation: 工业物联网技术实现了制造过程数据的实时采集，但当前数据驱动方法通常局限于特定领域（如设计或制造），缺乏设计特征与制造过程数据的整合。由于设计决策显著影响制造结果（如错误率、能耗、加工时间），这种整合的缺失限制了数据驱动产品设计的改进潜力。

Method: 开发了全面的系统架构确保连续数据采集与集成，建立了设计特征与制造过程数据之间的关联，以此为基础开发机器学习模型，实现自动化设计改进建议，并将制造过程数据与可持续性指标整合。

Result: 提出了一种能够映射和分析设计特征与制造过程数据关系的数据驱动方法，建立了支持自动化设计改进的机器学习模型，为可持续产品开发开辟了新可能性。

Conclusion: 通过整合设计特征与制造过程数据，该方法能够实现数据驱动的产品设计改进，特别在可持续产品开发方面具有重要应用价值，为工业物联网环境下的智能制造提供了新的解决方案。

Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.

</details>


### [68] [Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning](https://arxiv.org/abs/2512.09706)
*Kaichen He,Zihao Wang,Muyao Li,Anji Liu,Yitao Liang*

Main category: cs.LG

TL;DR: CrossAgent是一个统一智能体模型，能够掌握异构动作空间并在任务轨迹中自主选择最有效的交互接口，通过自适应动作切换在Minecraft环境中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常局限于静态预定义的动作空间（如API、GUI事件或机器人命令），这种刚性限制了它们在动态环境中的适应性，因为最优交互粒度会随上下文变化。

Method: 提出综合训练流程，结合冷启动监督微调和多轮组相对策略优化（GRPO）算法，使智能体能够学习自适应动作切换，无需人工指定规则。

Result: 在开放世界Minecraft环境中的800多个任务上进行广泛实验，CrossAgent实现了最先进的性能，显著优于固定动作基线，在长时程推理中表现出优越的泛化能力和效率。

Conclusion: 通过动态利用多样化动作空间的优势，CrossAgent展示了异构动作空间统一智能体的有效性，为动态环境中自适应交互提供了新范式。

Abstract: The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA

</details>


### [69] [Mixture of Lookup Key-Value Experts](https://arxiv.org/abs/2512.09723)
*Zongcheng Wang*

Main category: cs.LG

TL;DR: MoLKV模型通过上下文感知的键值对专家机制改进MoLE，在小型评估中显著降低验证损失


<details>
  <summary>Details</summary>
Motivation: MoLE模型虽然适合资源受限设备，但其基于输入ID的上下文无关专家选择机制可能限制模型性能。需要一种能考虑上下文信息的改进方法。

Method: 提出MoLKV模型，将每个专家构建为键值对。对于给定输入，从输入派生的查询与当前序列缓存的键值专家交互，生成上下文感知的专家输出。

Result: 实验结果表明，MoLKV在小型评估中实现了显著更低的验证损失。

Conclusion: MoLKV通过上下文感知机制有效缓解了MoLE的局限性，为资源受限设备上的LLM推理提供了改进方案。

Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

</details>


### [70] [Circuits, Features, and Heuristics in Molecular Transformers](https://arxiv.org/abs/2512.09757)
*Kristof Varadi,Mark Marosi,Peter Antal*

Main category: cs.LG

TL;DR: 该论文对用于生成化学分子的Transformer模型进行机制分析，揭示其在不同抽象层次捕捉分子表示规则的计算结构，发现与低层语法解析和化学有效性约束一致的计算模式，并通过稀疏自编码器提取化学相关特征。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型能够生成有效且多样的化学结构，但人们对这些模型如何捕捉分子表示规则的机制知之甚少。本研究旨在揭示自回归Transformer在药物样小分子生成任务中的计算结构和工作原理。

Method: 使用稀疏自编码器（SAEs）提取与化学相关激活模式关联的特征字典，对训练好的自回归Transformer进行多层次机制分析，识别从低层语法解析到高层化学有效性约束的计算模式。

Result: 识别出与低层语法解析和抽象化学有效性约束一致的计算模式，成功提取了与化学相关激活模式关联的特征字典，并在下游任务中验证了这些发现，表明机制洞察能够转化为实际应用中的预测性能提升。

Conclusion: 该研究为理解Transformer模型如何学习化学表示提供了机制性洞察，展示了从模型分析中获得的见解可以转化为实际应用性能，为化学信息学和药物发现领域的模型可解释性研究提供了新方法。

Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.

</details>


### [71] [Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems](https://arxiv.org/abs/2512.09780)
*Aoxiang Ma,Salah Ghamizi,Jun Cao,Pedro Rodriguez*

Main category: cs.LG

TL;DR: 本文提出了一种基于异构图神经网络的三相不平衡配电网电池储能系统优化调度方法，通过嵌入详细的三相电网信息和物理约束损失函数，实现了高精度预测和约束合规性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏明确的三相表示，难以准确建模相间动态和满足运行约束，导致调度方案不可行。需要一种能够准确建模三相不平衡配电网并确保约束合规的方法。

Method: 将三相电网信息（相电压、不平衡负载、BESS状态）嵌入异构图节点，采用多种GNN架构（GCN、GAT、GraphSAGE、GPS）联合预测网络状态变量，并通过物理信息损失函数以软惩罚方式纳入电池约束（SoC和C-rate限制）。

Result: 在CIGRE 18节点配电网系统上的实验验证显示，该方法实现了低预测误差：母线电压MSE分别为GCN 6.92e-07、GAT 1.21e-06、GPS 3.29e-05、SAGE 9.04e-07。更重要的是，物理信息方法确保了几乎为零的SoC和C-rate约束违反。

Conclusion: 通过嵌入三相电网信息和物理约束损失函数，该方法能够实现高精度网络状态预测，同时确保电池运行约束得到满足，为可靠、约束合规的BESS调度提供了有效解决方案。

Abstract: Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.

</details>


### [72] [Predicting Polymer Solubility in Solvents Using SMILES Strings](https://arxiv.org/abs/2512.09784)
*Andrew Reinhard*

Main category: cs.LG

TL;DR: 开发基于SMILES的深度学习框架，直接从聚合物和溶剂的SMILES表示预测溶解度，在模拟和实验数据上均表现良好


<details>
  <summary>Details</summary>
Motivation: 聚合物在不同溶剂中的溶解度预测对回收、制药等应用至关重要，需要可扩展的预测方法

Method: 构建包含8049个聚合物-溶剂对的模拟数据集，使用2394维特征表示，训练六层全连接神经网络，采用Adam优化器和均方误差损失

Result: 模型在模拟数据上预测准确，在Materials Genome Project的25个未见聚合物-溶剂组合上保持高精度

Conclusion: 基于SMILES的机器学习模型可用于可扩展的溶解度预测和高通量溶剂筛选，支持绿色化学、聚合物加工和材料设计应用

Abstract: Understanding and predicting polymer solubility in various solvents is critical for applications ranging from recycling to pharmaceutical formulation. This work presents a deep learning framework that predicts polymer solubility, expressed as weight percent (wt%), directly from SMILES representations of both polymers and solvents. A dataset of 8,049 polymer solvent pairs at 25 deg C was constructed from calibrated molecular dynamics simulations (Zhou et al., 2023), and molecular descriptors and fingerprints were combined into a 2,394 feature representation per sample. A fully connected neural network with six hidden layers was trained using the Adam optimizer and evaluated using mean squared error loss, achieving strong agreement between predicted and actual solubility values. Generalizability was demonstrated using experimentally measured data from the Materials Genome Project, where the model maintained high accuracy on 25 unseen polymer solvent combinations. These findings highlight the viability of SMILES based machine learning models for scalable solubility prediction and high-throughput solvent screening, supporting applications in green chemistry, polymer processing, and materials design.

</details>


### [73] [Knowledge Diversion for Efficient Morphology Control and Policy Transfer](https://arxiv.org/abs/2512.09796)
*Fu Feng,Ruixiao Shi,Yucheng Xie,Jianlu Shen,Jing Wang,Xin Geng*

Main category: cs.LG

TL;DR: DivMorph提出了一种模块化训练范式，通过知识分流学习可分解控制器，实现跨形态和跨任务的通用策略学习，显著提升样本效率和减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的通用形态控制方法存在两个主要问题：1) 计算成本高，部署开销大；2) 跨任务泛化能力有限，需要为每个新任务从头训练。需要一种既能高效部署又能有效跨任务迁移的解决方案。

Method: DivMorph采用模块化训练范式，首先通过SVD将随机初始化的Transformer权重分解为因子单元，然后使用动态软门控根据任务和形态嵌入调制这些单元，将其分离为共享的"learngenes"和特定于形态/任务的"tailors"，实现知识解耦。通过选择性激活相关组件，支持可扩展和高效的政策部署。

Result: 实验表明DivMorph达到最先进性能：跨任务迁移的样本效率比直接微调提升3倍，单智能体部署的模型规模减小17倍。

Conclusion: DivMorph通过知识分流和模块化设计，有效解决了通用形态控制中的计算成本和跨任务泛化问题，为可扩展和高效的策略学习提供了新范式。

Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.

</details>


### [74] [Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers](https://arxiv.org/abs/2512.09800)
*Zhaolan Huang,Kaspar Schleiser,Gyungmin Myung,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: Ariel-ML：首个为多核MCU上的TinyML推理提供自动并行化的Rust嵌入式软件平台，在推理延迟上优于现有方案，内存占用与C/C++工具包相当。


<details>
  <summary>Details</summary>
Motivation: 随着低功耗MCU从单核向多核架构演进，以及Rust在嵌入式领域逐渐取代C/C++，同时TinyML模型在边缘AI应用中的部署日益增多，但目前缺乏能够自动利用多核MCU进行TinyML推理并行化的Rust嵌入式平台。

Method: 设计并实现了Ariel-ML工具包，结合通用TinyML流水线和嵌入式Rust软件平台，能够充分利用多种32位微控制器家族（Arm Cortex-M、RISC-V、ESP-32）的多核能力。

Result: Ariel-ML在推理延迟方面优于现有技术，同时与使用嵌入式C/C++的现有工具包相比，实现了相当的内存占用。开源代码已发布，并通过多种TinyML模型进行了基准测试验证。

Conclusion: Ariel-ML填补了多核MCU上TinyML推理自动并行化的空白，为TinyML实践者和资源受限的嵌入式Rust开发者提供了有用的基础平台。

Abstract: Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.

</details>


### [75] [Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering](https://arxiv.org/abs/2512.09810)
*Adithya K Moorthy,V Vijaya Saradhi,Bhanu Prasad*

Main category: cs.LG

TL;DR: 该论文提出了在kNN和ε邻域图构建中主动实施人口统计公平性的新方法，通过在邻域选择阶段融入公平约束，确保敏感特征的成比例表示，从而为公平谱聚类提供预处理解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统图聚类方法在构建图时存在偏见，可能使某些群体代表性不足，导致不公平的聚类结果。当前研究旨在解决公平谱聚类中的预处理空白，证明图构建中的拓扑公平性对于实现公平聚类结果至关重要。

Method: 提出了公平kNN图和公平ε邻域图的构建方法，在邻域选择的最早阶段主动实施人口统计公平约束，将敏感特征的成比例表示融入局部图结构，同时保持几何一致性。

Result: 在三个合成数据集、七个真实世界表格数据集和三个真实世界图像数据集上的实验证明，提出的公平图构建方法在图聚类任务中超越了当前基线方法。

Conclusion: 图构建中的拓扑公平性本质上促进了更公平的谱聚类结果，无需改变聚类算法本身。这项研究填补了公平无监督学习中的重要空白，展示了在预处理阶段解决公平性问题的重要性。

Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.

</details>


### [76] [Predicting the Containment Time of California Wildfires Using Machine Learning](https://arxiv.org/abs/2512.09835)
*Shashank Bhardwaj*

Main category: cs.LG

TL;DR: 该研究使用机器学习模型预测加州野火完全控制所需天数，将持续时间预测作为回归任务，比较了随机森林、XGBoost和LSTM模型的性能。


<details>
  <summary>Details</summary>
Motivation: 加州野火季节日益严重，应急响应团队不堪重负，需要准确实用的预测来协助资源分配。现有研究多关注火灾风险或蔓延，少数研究持续时间预测也仅使用宽泛分类而非连续测量。

Method: 整合加州林业和消防部门三个公开数据集，将野火持续时间预测作为回归任务，比较基线集成回归器、随机森林、XGBoost和LSTM神经网络模型的性能。

Result: XGBoost模型略优于随机森林，因其能更好处理数据集中的静态特征；LSTM模型表现最差，因为数据集缺乏时间特征。模型选择取决于特征可用性。

Conclusion: 野火管理者可根据特征可用性选择最合适的模型来准确预测野火控制持续时间，从而有效分配资源。回归方法比分类预测提供更详细精确的预测。

Abstract: California's wildfire season keeps getting worse over the years, overwhelming the emergency response teams. These fires cause massive destruction to both property and human life. Because of these reasons, there's a growing need for accurate and practical predictions that can help assist with resources allocation for the Wildfire managers or the response teams. In this research, we built machine learning models to predict the number of days it will require to fully contain a wildfire in California. Here, we addressed an important gap in the current literature. Most prior research has concentrated on wildfire risk or how fires spread, and the few that examine the duration typically predict it in broader categories rather than a continuous measure. This research treats the wildfire duration prediction as a regression task, which allows for more detailed and precise forecasts rather than just the broader categorical predictions used in prior work. We built the models by combining three publicly available datasets from California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP). This study compared the performance of baseline ensemble regressor, Random Forest and XGBoost, with a Long Short-Term Memory (LSTM) neural network. The results show that the XGBoost model slightly outperforms the Random Forest model, likely due to its superior handling of static features in the dataset. The LSTM model, on the other hand, performed worse than the ensemble models because the dataset lacked temporal features. Overall, this study shows that, depending on the feature availability, Wildfire managers or Fire management authorities can select the most appropriate model to accurately predict wildfire containment duration and allocate resources effectively.

</details>


### [77] [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](https://arxiv.org/abs/2512.09850)
*Simone Cuonzo,Nina Deliu*

Main category: cs.LG

TL;DR: 将Conformal Prediction整合到bandit问题中，为顺序决策提供有限时间统计保证，在小差距场景下优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统bandit策略（如Thompson Sampling和UCB）通常依赖分布假设或渐近保证，主要关注遗憾最小化而忽视统计特性。本文旨在填补这一空白，通过Conformal Prediction将决策策略的遗憾最小化潜力与有限时间预测覆盖的统计保证相结合。

Method: 提出Conformal Bandits框架，将Conformal Prediction整合到bandit问题中。在投资组合分配应用中，进一步整合隐马尔可夫模型来捕捉金融市场的机制转换行为，增强探索-利用权衡。

Result: 在小差距场景下（如投资组合分配），Conformal Bandits在遗憾方面具有实际优势，并在传统UCB策略失败的情况下实现了名义覆盖保证。整合隐马尔可夫模型进一步提高了风险调整后的遗憾效率回报，同时保持覆盖保证。

Conclusion: Conformal Bandits框架成功地将Conformal Prediction的统计保证与bandit决策的遗憾最小化潜力相结合，在小差距场景下表现出优越性能，特别是在金融应用中可以捕捉市场机制转换行为，实现更好的探索-利用权衡。

Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.

</details>


### [78] [HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression](https://arxiv.org/abs/2512.09886)
*Gustavo Coelho Haase,Paulo Henrique Dourado da Silva*

Main category: cs.LG

TL;DR: HPM-KD是一个知识蒸馏框架，通过集成六个协同组件解决传统KD的四大限制：自适应配置管理、渐进蒸馏链、注意力加权多教师集成、元学习温度调度、并行处理管道和共享优化内存，实现10-15倍压缩、85%精度保持和30-40%训练时间减少。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏技术面临四个关键限制：1）对超参数敏感需要大量手动调优；2）从大教师模型到小学生模型时存在容量差距；3）多教师场景下协调不理想；4）计算资源使用效率低下。需要一种更自动化、高效的知识蒸馏框架。

Method: 提出HPM-KD框架，集成六个协同组件：1）基于元学习的自适应配置管理器，消除手动超参数调优；2）渐进蒸馏链，自动确定中间模型；3）注意力加权多教师集成，学习动态的每样本权重；4）元学习温度调度器，在整个训练过程中自适应调整温度；5）并行处理管道，具有智能负载均衡；6）共享优化内存，支持跨实验重用。

Result: 在CIFAR-10、CIFAR-100和表格数据集上的实验表明：HPM-KD实现10-15倍压缩同时保持85%的准确率保留；消除了手动调优需求；通过并行化减少30-40%的训练时间。消融研究确认每个组件都有独立贡献（0.10-0.98个百分点）。

Conclusion: HPM-KD是一个全面解决知识蒸馏关键限制的框架，通过自动化配置、渐进蒸馏、多教师协调和计算优化，实现了高效、可扩展的模型压缩。该框架已作为开源DeepBridge库的一部分提供。

Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

</details>


### [79] [Analysis of Dirichlet Energies as Over-smoothing Measures](https://arxiv.org/abs/2512.09890)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 本文分析了两种图拉普拉斯算子诱导的狄利克雷能量作为过平滑度量的区别，指出归一化图拉普拉斯不满足节点相似性度量的公理定义，并形式化了两者的谱特性以指导GNN架构中的度量选择。


<details>
  <summary>Details</summary>
Motivation: 在GNN中，过平滑是一个重要问题，但现有文献对使用哪种狄利克雷能量作为过平滑度量存在混淆。本文旨在澄清未归一化图拉普拉斯和归一化图拉普拉斯诱导的两种能量之间的区别，为选择合适的度量提供理论依据。

Method: 1) 分析两种狄利克雷能量的数学差异；2) 验证归一化图拉普拉斯能量是否满足Rusch等人提出的节点相似性度量公理；3) 形式化两种定义的谱特性；4) 建立度量选择与GNN架构谱兼容性的理论框架。

Result: 证明了归一化图拉普拉斯诱导的狄利克雷能量不满足节点相似性度量的公理定义。通过形式化谱特性，明确了两种度量在监测GNN动态时的关键区别，为根据GNN架构选择谱兼容的过平滑度量提供了理论指导。

Conclusion: 选择合适的狄利克雷能量作为过平滑度量需要考虑其与GNN架构的谱兼容性。归一化图拉普拉斯能量不满足节点相似性度量公理，这一发现有助于消除文献中的混淆，为准确监测GNN训练动态提供理论依据。

Abstract: We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.

</details>


### [80] [Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension](https://arxiv.org/abs/2512.09894)
*Mengren,Liu,Yixiang Zhang,Yiming,Zhang*

Main category: cs.LG

TL;DR: 系统评估不同蛋白质语言模型架构在抗体特异性预测任务中的表现，发现抗体专用模型能自然学习关注CDR区域，而通用蛋白模型需要显式CDR训练策略。


<details>
  <summary>Details</summary>
Motivation: 尽管蛋白质语言模型在理解蛋白质序列方面表现出色，但不同模型架构如何捕捉抗体特异性生物学特性仍不清楚。需要系统研究模型架构选择如何影响理解抗体序列特征和功能的能力。

Method: 评估三种最先进的蛋白质语言模型（AntiBERTa、BioBERT、ESM2）与通用语言模型（GPT-2）基线在抗体靶标特异性预测任务上的表现。通过注意力归因分析，研究模型如何捕捉V基因使用、体细胞超突变模式和同种型信息等生物学特征。

Result: 所有蛋白质语言模型都实现了高分类准确率，但在捕捉生物学特征方面表现出不同的偏差。抗体专用模型如AntiBERTa能自然学习关注互补决定区（CDRs），而通用蛋白模型通过显式CDR聚焦训练策略能显著受益。

Conclusion: 研究揭示了模型架构与生物学特征提取之间的关系，为计算抗体设计中未来蛋白质语言模型的发展提供了有价值的指导。

Abstract: Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.

</details>


### [81] [STACHE: Local Black-Box Explanations for Reinforcement Learning Policies](https://arxiv.org/abs/2512.09909)
*Andrew Elashkin,Orna Grumberg*

Main category: cs.LG

TL;DR: STACHE框架为离散马尔可夫游戏中的智能体行为生成局部黑盒解释，包含鲁棒性区域和最小反事实两个互补组件，通过精确搜索算法避免代理模型的保真度差距。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在稀疏奖励或安全关键环境中经常表现出意外行为，需要可靠的调试和验证工具来理解其决策过程。

Method: 提出STACHE框架，生成包含鲁棒性区域（智能体决策不变的连通邻域状态）和最小反事实（改变决策所需的最小状态扰动）的复合解释。利用因子化状态空间结构，引入精确的基于搜索的算法，避免代理模型的保真度差距。

Result: 在Gymnasium环境中的实证验证表明，该框架不仅能解释策略行为，还能有效捕捉训练过程中策略逻辑的演变——从不稳定行为到优化稳健策略，提供了对智能体敏感性和决策边界的可操作见解。

Conclusion: STACHE为强化学习智能体提供了全面、精确的解释框架，有助于理解智能体决策过程，支持调试和验证工作，特别适用于稀疏奖励和安全关键环境。

Abstract: Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.

</details>


### [82] [FALCON: Few-step Accurate Likelihoods for Continuous Flows](https://arxiv.org/abs/2512.09914)
*Danyal Rehman,Tara Akhound-Sadegh,Artem Gazizov,Yoshua Bengio,Alexander Tong*

Main category: cs.LG

TL;DR: 提出FALCON方法，通过混合训练目标提高可逆性，实现少步采样和准确似然计算，用于分子Boltzmann采样，比现有方法快两个数量级


<details>
  <summary>Details</summary>
Motivation: 热力学平衡中分子状态的可扩展采样是统计物理学的长期挑战。现有Boltzmann生成器使用连续归一化流，但似然计算成本极高（每个样本需要数千次函数评估），严重限制了其应用

Method: 提出FALCON方法，通过引入混合训练目标来鼓励可逆性，实现少步采样和足够准确的似然计算，适用于重要性采样应用

Result: FALCON在分子Boltzmann采样方面优于最先进的归一化流模型，比同等性能的CNF模型快两个数量级

Conclusion: FALCON解决了连续归一化流计算成本高的问题，为分子热力学平衡采样提供了高效实用的解决方案

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.

</details>


### [83] [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/abs/2512.09929)
*Arjun Parthasarathy,Nimit Kalra,Rohun Agrawal,Yann LeCun,Oumayma Bounou,Pavel Izmailov,Micah Goldblum*

Main category: cs.LG

TL;DR: 提出改进世界模型训练方法，通过训练时数据合成技术缩小训练-测试差距，使基于梯度的规划在10%时间预算内达到或超越传统无梯度方法性能


<details>
  <summary>Details</summary>
Motivation: 基于梯度的规划虽然计算效率高，但性能一直落后于其他方法。世界模型在训练时使用下一状态预测目标，但在测试时用于估计动作序列，存在训练-测试差距

Method: 提出训练时数据合成技术，改进世界模型训练以支持高效的基于梯度规划。通过合成训练数据来缩小训练目标（状态预测）与测试使用（动作序列估计）之间的差距

Result: 在多种物体操作和导航任务中，该方法在10%的时间预算内达到或超越了传统无梯度交叉熵方法（CEM）的性能

Conclusion: 通过缩小世界模型的训练-测试差距，基于梯度的规划可以实现高效且高性能的模型预测控制，为离线训练的世界模型在实际规划任务中的应用提供了有效解决方案

Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [84] [Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming](https://arxiv.org/abs/2512.08948)
*Yihang Gao,Michael K. Ng,Michael W. Mahoney,Sen Na*

Main category: stat.ML

TL;DR: 提出了一种用于约束随机优化问题的在线统计推断方法——随机序列二次规划(SSQP)，该方法结合动量式梯度移动平均来消除步长偏差，实现了全局收敛和局部渐近正态性，并提供了协方差矩阵估计器。


<details>
  <summary>Details</summary>
Motivation: 约束随机优化问题在统计学和机器学习中广泛存在（如约束M估计、物理信息模型、安全强化学习、算法公平性等），但现有方法难以同时实现在线求解和统计推断，特别是对于非线性约束问题。

Method: 开发随机序列二次规划(SSQP)方法：通过顺序执行目标函数的二次近似和约束的线性近似来计算步长方向；使用动量式梯度移动平均技术消除步长偏差；提供插件式协方差矩阵估计器。

Result: 方法实现了全局几乎必然收敛，具有局部渐近正态性和最优的原始-对偶极限协方差矩阵（Hájek-Le Cam意义下）；在基准非线性问题、约束广义线性模型和投资组合分配问题上表现出优越性能。

Conclusion: SSQP是第一个完全在线的方法，在不依赖约束集投影算子（非线性问题通常难以计算）的情况下实现了原始-对偶渐近极小极大最优性，为约束随机问题提供了有效且实用的在线推断工具。

Abstract: We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of Hájek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications.

</details>


### [85] [WTNN: Weibull-Tailored Neural Networks for survival analysis](https://arxiv.org/abs/2512.09163)
*Gabrielle Rives,Olivier Lopez,Nicolas Bousquet*

Main category: stat.ML

TL;DR: 提出WTNN框架，用深度神经网络建模Weibull生存分析，处理代理指标和右删失数据，结合先验知识改进传统回归方法。


<details>
  <summary>Details</summary>
Motivation: 军事车辆在多变严苛环境下的生存分析需求，现有方法在处理代理指标、删失数据和复杂协变量关系时存在局限，需要更灵活强大的建模框架。

Method: 设计WTNN神经网络框架，专门针对Weibull分布特性，能够整合关于关键协变量的定性先验知识，处理代理指标和右删失数据。

Result: 数值实验表明WTNN能可靠地在代理和右删失数据上训练，产生稳健且可解释的生存预测，优于现有方法。

Conclusion: WTNN为Weibull生存研究提供了新的神经网络建模框架，能有效处理复杂协变量关系，结合先验知识，在军事车辆等实际应用中具有优势。

Abstract: The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches.

</details>


### [86] [Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination](https://arxiv.org/abs/2512.09266)
*Ryosuke Nagumo,Hironori Fujisawa*

Main category: stat.ML

TL;DR: 本文研究了污染环境下稳健密度比估计的非渐近性质，证明了加权密度比估计在非渐近框架下即使存在严重污染也能实现稀疏一致性，为密度比估计和稳健估计提供了首个严重污染下的非渐近强稳健性分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法中加权密度比估计从渐近角度展现出双重强稳健性，但缺乏非渐近框架下的理论分析。本文旨在填补这一空白，研究在严重污染情况下密度比估计的非渐近性质，解决密度比估计和稳健估计中的两个重要挑战。

Method: 采用加权密度比估计方法，在非渐近框架下分析其性质。该方法假设加权密度比函数有界，即使真实密度比无界也能估计。对于稳健性分析，假设至少满足以下条件之一：(i) 污染比例较小，(ii) 异常值的加权值较小。

Result: 证明了加权密度比估计在非渐近框架下具有稀疏一致性，即使在严重污染情况下也能保持强稳健性。为无界密度比估计提供了非渐近性质分析，并首次在严重污染下建立了双重强稳健性的非渐近理论框架。

Conclusion: 加权密度比估计在非渐近框架下展现出优异的稳健性能，即使在严重污染环境中也能保持稀疏一致性。这项工作为密度比估计和稳健估计提供了首个严重污染下的非渐近强稳健性理论分析，具有重要的理论和实践意义。

Abstract: We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination.

</details>


### [87] [Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression](https://arxiv.org/abs/2512.09275)
*Weiyi He,Yue Xing*

Main category: stat.ML

TL;DR: 首次对包含可训练位置编码的单层Transformer进行泛化分析，发现PE会系统性增大泛化差距，并在对抗攻击下放大模型脆弱性


<details>
  <summary>Details</summary>
Motivation: 位置编码是Transformer的核心架构组件，但其对模型泛化能力和鲁棒性的影响尚不清楚，需要建立理论分析框架

Method: 对单层Transformer在上下文回归任务中进行泛化分析，考虑完全可训练的位置编码模块，推导干净和对抗场景下的Rademacher泛化界

Result: PE会系统性增大泛化差距，在对抗攻击下，有PE和无PE模型之间的差距被放大，PE放大了模型的脆弱性

Conclusion: 建立了理解带PE的ICL模型在干净和对抗场景下泛化的新理论框架，揭示了PE对模型泛化和鲁棒性的负面影响

Abstract: Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.

</details>


### [88] [Estimation of Stochastic Optimal Transport Maps](https://arxiv.org/abs/2512.09499)
*Sloan Nietert,Ziv Goldfeld*

Main category: stat.ML

TL;DR: 本文提出了一种新的随机最优传输映射评估度量，开发了具有近最优有限样本风险界的计算高效映射估计器，并提供了对抗性样本污染的鲁棒估计保证。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输映射估计理论存在局限性：依赖于Brenier定理（二次成本、绝对连续源分布）保证确定性映射的存在唯一性，需要额外正则性假设获得误差界。但在许多实际问题中这些条件不成立或无法验证，此时只能通过可分裂质量的随机映射进行最优传输。需要将映射估计理论扩展到这类随机传输场景。

Method: 引入新的随机映射传输质量评估度量；开发计算高效的映射估计器，在易于验证的最小假设下获得近最优的有限样本风险界；分析进一步考虑了常见的对抗性样本污染形式，提供具有鲁棒估计保证的估计器。

Result: 建立了首个通用映射估计理论，兼容广泛的实际应用场景；通过实证实验验证了理论，并展示了在现有理论失效场景下所提框架的实用性。

Conclusion: 本文提出的框架将最优传输映射估计理论扩展到随机传输场景，为现实世界中许多最优传输本质上是随机的应用提供了首个通用理论，具有重要的理论和实践意义。

Abstract: The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.

</details>


### [89] [Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport](https://arxiv.org/abs/2512.09530)
*Antonio Candelieri,Alessandro Quadrio*

Main category: stat.ML

TL;DR: 该研究从最优传输视角分析自注意力训练，提出基于OT的表格分类替代方法，通过生成类特定高斯分布计算OT对齐，实现与Transformer相当精度但计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力训练过程，发现其训练轨迹效率低下，希望通过最优传输理论提供更高效的替代方案，特别是针对表格分类任务。

Method: 1) 跟踪自注意力层中间投影，用Wasserstein距离等OT指标评估演化；2) 提出OT算法：生成类特定高斯分布，计算OT对齐，训练MLP泛化该映射；3) 在二类、三类分类任务和生物医学数据集上实验。

Result: 自注意力最终映射常近似OT最优耦合，但训练轨迹低效；MLP预训练部分改善收敛但对初始化敏感；OT方法精度与Transformer相当，计算成本更低，标准化输入下扩展性更好，但性能依赖虚拟几何设计。

Conclusion: 最优传输为自注意力训练提供了理论框架和高效替代方案，OT方法在表格分类中具有计算优势，但需要精心设计虚拟分布几何结构，所有实验在R中实现。

Abstract: This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.
  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.

</details>


### [90] [Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search](https://arxiv.org/abs/2512.09538)
*Ekaterina Fadeeva,Maiya Goloburda,Aleksandr Rubashevskii,Roman Vashurin,Artem Shelmanov,Preslav Nakov,Mrinmaya Sachan,Maxim Panov*

Main category: stat.ML

TL;DR: 提出使用束搜索替代多项式采样进行一致性不确定性量化，在短问答任务中减少重复和方差，达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 在短问答任务中，多项式采样容易产生重复结果，且不确定性估计在不同运行间方差较大，需要更稳定的方法

Method: 使用束搜索生成候选答案进行一致性不确定性量化，提供束集概率质量的理论下界保证

Result: 在六个QA数据集上评估，相比多项式采样有持续改进，达到最先进的不确定性量化性能

Conclusion: 束搜索方法在一致性不确定性量化中优于多项式采样，提供更稳定和准确的不确定性估计

Abstract: Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.

</details>


### [91] [Supervised learning pays attention](https://arxiv.org/abs/2512.09912)
*Erin Craig,Robert Tibshirani*

Main category: stat.ML

TL;DR: 论文提出了一种基于注意力加权的监督学习方法，用于表格数据的个性化建模，在保持模型简单性和可解释性的同时，提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法在处理异质性数据时缺乏灵活性，无法为每个预测点拟合个性化模型，同时保持模型的简单性和可解释性。需要一种方法能够自适应地处理异质数据，而无需预先指定聚类或相似性度量。

Method: 提出注意力加权方法，为每个测试观测值拟合局部模型，通过基于监督相似性度量的注意力权重对训练数据进行加权，强调对结果预测重要的特征和交互作用。该方法可应用于lasso回归、梯度提升等监督学习，并可扩展到时间序列、空间数据以及处理分布偏移。

Result: 在真实和模拟数据集上，注意力加权方法在保持可解释性的同时提高了预测性能。理论分析表明，在已知子组结构的混合模型数据生成过程中，注意力加权线性模型比标准线性模型具有更低的均方误差。

Conclusion: 注意力加权方法为表格数据提供了一种灵活、可解释的个性化建模框架，能够自适应处理异质数据，在保持模型简单性的同时提高预测准确性，并可扩展到多种数据类型和分布偏移场景。

Abstract: In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.
  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.

</details>
