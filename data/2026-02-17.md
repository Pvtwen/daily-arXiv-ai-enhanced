<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 28]
- [cs.LG](#cs.LG) [Total: 156]
- [stat.ML](#stat.ML) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [SIM-assisted Secure Mobile Communications via Enhanced Proximal Policy Optimization Algorithm](https://arxiv.org/abs/2602.13265)
*Wenxuan Ma,Bin Lin,Hongyang Pan,Geng Sun,Enyu Shi,Jiancheng An,Chau Yuen*

Main category: eess.SP

TL;DR: 本文提出了一种基于堆叠智能超表面(SIM)的6G移动用户安全通信系统，采用增强型近端策略优化算法(PPO-BOP)来最大化所有用户的可实现保密率，有效解决了移动环境中的信道不确定性、多用户干扰和硬件损伤等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线通信网络的发展，移动用户面临日益突出的安全挑战。物理层安全技术利用无线信道固有特性提供安全保障，而堆叠智能超表面通过其多层结构直接操控电磁波，为高效能提升物理层安全性能提供了重要潜力。

Method: 提出SIM辅助的安全通信系统，针对窃听者威胁下的移动用户。为解决信道不确定性、多用户干扰和硬件损伤等实际问题，构建了联合功率和相移优化问题。为解决这一非凸动态优化问题，设计了增强型近端策略优化算法(PPO-BOP)，包含双向LSTM机制、离策略数据利用机制和策略反馈机制。

Result: 广泛的仿真结果表明，PPO-BOP在可实现保密率方面显著优于基准策略和其他深度强化学习算法，能够有效捕捉短期信道衰落和长期用户移动性，提高样本利用效率并增强探索能力。

Conclusion: 本文提出的SIM辅助安全通信系统和PPO-BOP算法为6G移动用户提供了有效的物理层安全保障方案，通过智能优化功率和相移配置，在动态移动环境中实现了优越的保密性能。

Abstract: With the development of sixth-generation (6G) wireless communication networks, the security challenges are becoming increasingly prominent, especially for mobile users (MUs). As a promising solution, physical layer security (PLS) technology leverages the inherent characteristics of wireless channels to provide security assurance. Particularly, stacked intelligent metasurface (SIM) directly manipulates electromagnetic waves through their multilayer structures, offering significant potential for enhancing PLS performance in an energy efficient manner. Thus, in this work, we investigate an SIM-assisted secure communication system for MUs under the threat of an eavesdropper, addressing practical challenges such as channel uncertainty in mobile environments, multiple MU interference, and residual hardware impairments. Consequently, we formulate a joint power and phase shift optimization problem (JPPSOP), aiming at maximizing the achievable secrecy rate (ASR) of all MUs. Given the non-convexity and dynamic nature of this optimization problem, we propose an enhanced proximal policy optimization algorithm with a bidirectional long short-term memory mechanism, an off-policy data utilization mechanism, and a policy feedback mechanism (PPO-BOP). Through these mechanisms, the proposed algorithm can effectively capture short-term channel fading and long-term MU mobility, improve sample utilization efficiency, and enhance exploration capabilities. Extensive simulation results demonstrate that PPO-BOP significantly outperforms benchmark strategies and other deep reinforcement learning algorithms in terms of ASR.10.1109/TWC.2026.3658332

</details>


### [2] [Sample-level EEG-based Selective Auditory Attention Decoding with Markov Switching Models](https://arxiv.org/abs/2602.13447)
*Yuanyuan Yao,Simon Geirnaert,Tinne Tuytelaars,Alexander Bertrand*

Main category: eess.SP

TL;DR: 提出基于马尔可夫切换模型(MSM)的集成框架，用于从脑电信号中解码选择性听觉注意，实现样本级解码并提高注意力切换检测速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在窗口级操作，面临时间分辨率与解码精度的权衡。虽然HMM后处理可以平滑窗口级输出，但需要分离的步骤。本文旨在将解码和平滑集成到统一的概率框架中。

Method: 使用马尔可夫切换模型(MSM)将解码和平滑组件集成到单一概率框架中，直接建模每个注意力状态下脑电信号与语音包络的关系，同时纳入注意力的时间动态。通过期望最大化算法联合估计模型参数和注意力状态。

Result: 集成MSM框架在解码精度上与HMM后处理相当，同时提供更快的注意力切换检测，实现了样本级注意力解码。

Conclusion: MSM框架为选择性听觉注意解码提供了统一的概率方法，避免了分离的平滑步骤，在保持精度的同时提高了时间分辨率，代码已开源。

Abstract: Selective auditory attention decoding aims to identify the speaker of interest from listeners' neural signals, such as electroencephalography (EEG), in the presence of multiple concurrent speakers. Most existing methods operate at the window level, facing a trade-off between temporal resolution and decoding accuracy. Recent work has shown that hidden Markov model (HMM)-based post-processing can smooth window-level decoder outputs to improve this trade-off. Instead of using a separate smoothing step, we propose to integrate the decoding and smoothing components into a single probabilistic framework using a Markov switching model (MSM). It directly models the relationship between the EEG and speech envelopes under each attention state while incorporating the temporal dynamics of attention. This formulation enables sample-level attention decoding, with model parameters and attention states jointly estimated via the expectation-maximization algorithm. Experimental results demonstrate that this integrated MSM formulation achieves comparable decoding accuracy to HMM post-processing while providing faster attention switch detection. The code for the proposed method is available at https://github.com/YYao-42/MSM.

</details>


### [3] [Towards Causality-Aware Modeling for Multimodal Brain-Muscle Interactions](https://arxiv.org/abs/2602.13459)
*Farwa Abbas,Wei Dai,Zoran Cvetkovic,Verity McClelland*

Main category: eess.SP

TL;DR: 提出一种结合动态贝叶斯网络和收敛交叉映射的因果推断框架，用于分析多模态生物医学信号中的动态因果交互，并在肌张力障碍儿童的脑电-肌电数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如动态贝叶斯网络通常假设线性或简单统计依赖，而流形方法如收敛交叉映射能捕捉非线性滞后交互但缺乏概率量化和干预建模能力。需要一种能结合两者优势的框架来更稳健地表征多变量生物医学信号中的动态因果交互。

Method: 提出DBN informed CCM框架，将几何流形重建与概率时序建模相结合。该方法整合了动态贝叶斯网络的概率建模能力和收敛交叉映射的非线性流形分析优势，支持不确定性量化和干预模拟。

Result: 应用于肌张力障碍和正常儿童的脑电-肌电多模态记录，该方法量化了不确定性，支持干预模拟，并揭示了肌张力障碍中皮质肌肉通路的频率特异性重组。实验结果显示，与基线方法相比，在预测一致性和因果稳定性方面有显著改进。

Conclusion: 该因果感知的多模态建模框架具有开发定量生物标志物和指导靶向神经调节干预的潜力，为生物医学成像中的计算和算法方法提供了新的工具。

Abstract: Robust characterization of dynamic causal interactions in multivariate biomedical signals is essential for advancing computational and algorithmic methods in biomedical imaging. Conventional approaches, such as Dynamic Bayesian Networks (DBNs), often assume linear or simple statistical dependencies, while manifold based techniques like Convergent Cross Mapping (CCM) capture nonlinear, lagged interactions but lack probabilistic quantification and interventional modeling. We introduce a DBN informed CCM framework that integrates geometric manifold reconstruction with probabilistic temporal modeling. Applied to multimodal EEG-EMG recordings from dystonic and neurotypical children, the method quantifies uncertainty, supports interventional simulation, and reveals distinct frequency specific reorganization of corticomuscular pathways in dystonia. Experimental results show marked improvements in predictive consistency and causal stability as compared to baseline approaches, demonstrating the potential of causality aware multimodal modeling for developing quantitative biomarkers and guiding targeted neuromodulatory interventions.

</details>


### [4] [Blind Deconvolution Demixing using Modulated Inputs](https://arxiv.org/abs/2602.13481)
*Humera Hameed,Ali Ahmed*

Main category: eess.SP

TL;DR: 该论文提出了一种解决涉及调制输入的盲解卷积混合问题的方法，通过确定性子空间假设和梯度下降算法，在满足调制速率和采样复杂度条件下，可以从观测信号中恢复所有输入信号和信道。


<details>
  <summary>Details</summary>
Motivation: 解决盲解卷积混合问题，特别是当多个带限输入信号被已知随机序列调制后，经过不同信道卷积并叠加为单一观测信号时，如何从混合信号中恢复原始输入信号和信道响应。

Method: 采用确定性子空间假设处理输入信号，保持信道脉冲响应任意。当调制序列变化速率满足Q≥N²(B+M)且采样复杂度条件满足时，使用梯度下降算法从观测混合信号中估计所有信号和信道。

Result: 通过大量仿真验证了算法的鲁棒性，并使用相位转换数值研究了算法的理论保证。结果表明在满足条件的情况下，可以成功恢复所有输入信号和信道响应。

Conclusion: 该论文为盲解卷积混合问题提供了一种有效的解决方案，通过调制速率和采样复杂度的理论保证，结合梯度下降算法，能够从单一观测信号中恢复多个调制输入信号和对应的信道响应。

Abstract: This paper focuses on solving a challenging problem of blind deconvolution demixing involving modulated inputs. Specifically, multiple input signals $s_n(t)$, each bandlimited to $B$ Hz, are modulated with known random sequences $r_n(t)$ that alter at rate $Q$. Each modulated signal is convolved with a different M tap channel of impulse response $h_n(t)$, and the outputs of each channel are added at a common receiver to give the observed signal $y(t)=\sum_{n=1}^N (r_n(t)\odot s_n(t))\circledast h_n(t)$, where $\odot$ is the point wise multiplication, and $\circledast$ is circular convolution. Given this observed signal $y(t)$, we are concerned with recovering $s_n(t)$ and $h_n(t)$. We employ deterministic subspace assumption for the input signal $s_n(t)$ and keep the channel impulse response $h_n(t)$ arbitrary. We show that if modulating sequence is altered at a rate $Q \geq N^2 (B+M)$ and sample complexity bound is obeyed then all the signals and the channels, $\{s_n(t),h_n(t)\}_{n=1}^N$, can be estimated from the observed mixture $y(t)$ using gradient descent algorithm. We have performed extensive simulations that show the robustness of our algorithm and used phase transitions to numerically investigate the theoretical guarantees provided by our algorithm.

</details>


### [5] [Feasibility of simultaneous EEG-fMRI at 0.55 T: Recording, Denoising, and Functional Mapping](https://arxiv.org/abs/2602.13489)
*Parsa Razmara,Takfarinas Medani,Majid Abbasi Sisara,Anand A. Joshi,Rui Chen,Woojae Jeong,Ye Tian,Krishna S. Nayak,Richard M. Leahy*

Main category: eess.SP

TL;DR: 在0.55T低场强下实现同步EEG-fMRI的可行性验证，显示降低的BCG伪影和保留的alpha节律，支持神经血管耦合测量


<details>
  <summary>Details</summary>
Motivation: 高场强（≥3T）EEG-fMRI系统存在多种技术限制，包括EEG信号伪影、金属植入物兼容性差、高噪声以及高磁化率区域伪影。本研究探索低场强（0.55T）下同步EEG-fMRI的可行性，以克服这些限制。

Method: 在0.55T磁场下进行视觉任务实验，同时记录EEG和fMRI数据。分析梯度伪影和心搏伪影（BCG），并测试多模态整合流程，将EEG功率包络与血流动力学BOLD响应相关联。

Result: 观察到BCG幅度降低，与静态磁场强度相关的脉冲伪影预期缩放一致。保留了alpha节律和信号完整性，EEG功率包络与BOLD响应对应，支持神经血管耦合测量。

Conclusion: 0.55T下的同步EEG-fMRI是可行的，为多模态神经成像提供了一个有前景的环境，能够减少伪影并促进有效的去噪。

Abstract: Simultaneous recording of electroencephalography (EEG) and functional MRI (fMRI) can provide a more complete view of brain function by merging high temporal and spatial resolutions. High-field ($\geq$3T) systems are standard, and require technical trade-offs, including artifacts in the EEG signal, reduced compatibility with metallic implants, high acoustic noise, and artifacts around high-susceptibility areas such as the optic nerve and nasal sinus. This proof-of-concept study demonstrates the feasibility of simultaneous EEG-fMRI at 0.55T in a visual task. We characterize the gradient and ballistocardiogram (BCG) artifacts inherent to this environment and observe reduced BCG magnitude consistent with the expected scaling of pulse-related artifacts with static magnetic field strength. This reduction shows promise for facilitating effective denoising while preserving the alpha rhythm and signal integrity. Furthermore, we tested a multimodal integration pipeline and demonstrated that the EEG power envelope corresponds with the hemodynamic BOLD response, supporting the potential to measure neurovascular coupling in this environment. We demonstrate that combined EEG-fMRI at 0.55T is feasible and represents a promising environment for multimodal neuroimaging.

</details>


### [6] [Sub Specie Aeternitatis: Fourier Transforms from the Theory of Heat to Musical Signals](https://arxiv.org/abs/2602.13520)
*Victor Lazzarini*

Main category: eess.SP

TL;DR: 本文追溯傅里叶定理从热传导物理到现代音乐信号理论的发展历程，通过原始文献分析傅里叶的两个核心思想及其在数学物理、电气工程、计算机科学和音乐等领域的根本性影响。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过原始文献追溯傅里叶定理的发展历程，展示傅里叶在1822年《热的解析理论》中提出的两个核心思想如何从热传导物理演变为现代音乐信号理论的基础，并探讨时间与频率之间的内在对偶性。

Method: 本文采用文献研究方法，完全依赖原始文献（包括傅里叶、欧姆、亥姆霍兹、德摩根和狄拉克等人的著作），系统分析傅里叶定理从热传导理论到音乐信号理论的演变过程。

Result: 通过原始文献分析，本文展示了傅里叶的两个核心思想（三角级数系数求解方法和傅里叶双重积分）如何被欧姆和亥姆霍兹发展为音乐音调理论，被德摩根用于处理不连续函数中的无穷问题，最终通过狄拉克的见解形成现代信号处理中时间与频率对偶的完整理论框架。

Conclusion: 傅里叶定理从热传导物理出发，经过多个学科的发展演变，最终成为现代音乐信号理论的基础。该定理揭示了时间与频率之间的内在对偶性，这一对偶性在信号处理中具有根本重要性，体现了数学思想在不同学科间的迁移和统一。

Abstract: J. B. Fourier in his \emph{Théorie Analytique de la Chaleur} of 1822 introduced, amongst other things, two ideas that have made a fundamental impact in fields as diverse as Mathematical Physics, Electrical Engineering, Computer Science, and Music. The first one of these, a method to find the coefficients for a trigonometric series describing an arbitrary function, was very early on picked up by G. Ohm and H. Helmholtz as the foundation for a theory of \emph{musical tones}. The second one, which is described by Fourier's double integral, became the basis for treating certain kinds of infinity in discontinuous functions, as shown by A. De Morgan in his 1842 \emph{The Differential and Integral Calculus}. Both make up the fundamental basis for what is now commonly known as the \emph{Fourier theorem}. With the help of P. A. M. Dirac's insights into the nature of these infinities, we can have a compact description of the frequency spectrum of a function of time, or conversely of a waveform corresponding to a given function of frequency. This paper, using solely primary sources, takes us from the physics of heat propagation to the modern theory of musical signals. It concludes with some considerations on the inherent duality of time and frequency emerging from Fourier's theorem.

</details>


### [7] [DopplerGLRTNet for Radar Off-Grid Detection](https://arxiv.org/abs/2602.13546)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: eess.SP

TL;DR: 提出DopplerGLRTNet，一种摊销的离网格GLRT检测器，通过轻量级回归器预测连续多普勒频率，解决传统NMF检测器在离网格目标时检测概率饱和的问题。


<details>
  <summary>Details</summary>
Motivation: 传统归一化匹配滤波（NMF）检测器在处理离网格目标（多普勒或角度不在离散处理网格上）时，即使在高信噪比下，检测概率也会在操作相关的低虚警率下饱和。连续参数GLRT虽然能解决此问题，但密集扫描会增加测试时间成本，且对协方差失配敏感。

Method: 提出DopplerGLRTNet，一种摊销的离网格GLRT检测器。使用轻量级回归器从白化观测中预测分辨率单元内的连续多普勒频率，然后检测器输出类似GLRT/NMF的单一分数，该分数由预测多普勒频率处的归一化匹配滤波能量给出。

Result: 蒙特卡洛仿真在高斯和复合高斯杂波中显示，DopplerGLRTNet能够缓解离网格饱和问题，以远低于密集扫描的成本接近其性能，并在相同经验校准的虚警率下提高对协方差估计的鲁棒性。

Conclusion: DopplerGLRTNet提供了一种高效且鲁棒的离网格目标检测解决方案，通过摊销计算成本，在保持检测性能的同时显著降低计算复杂度。

Abstract: Off-grid targets whose Doppler (or angle) does not lie on the discrete processing grid can severely degrade classical normalized matched-filter (NMF) detectors: even at high SNR, the detection probability may saturate at operationally relevant low false-alarm rates. A principled remedy is the continuous-parameter GLRT, which maximizes a normalized correlation over the parameter domain; however, dense scanning increases test-time cost and remains sensitive to covariance mismatch through whitening. We propose DopplerGLRTNet, an amortized off-grid GLRT: a lightweight regressor predicts the continuous Doppler within a resolution cell from the whitened observation, and the detector outputs a single GLRT/NMF-like score given by the normalized matched-filter energy at the predicted Doppler. Monte Carlo simulations in Gaussian and compound-Gaussian clutter show that DopplerGLRTNet mitigates off-grid saturation, approaches dense-scan performance at a fraction of its cost, and improves robustness to covariance estimation at the same empirically calibrated Pfa.

</details>


### [8] [Twenty-five years of J-DSP Online Labs for Signal Processing Classes and Workforce Development Programs](https://arxiv.org/abs/2602.13863)
*Andreas Spanias*

Main category: eess.SP

TL;DR: J-DSP是一个用于数字信号处理教学的在线仿真程序，从2000年开始在ASU使用，经历了从Java到HTML5的技术转型，支持滤波器设计、FFT分析、机器学习信号分类和量子傅里叶变换等实验，并扩展到移动平台和多种教育项目。


<details>
  <summary>Details</summary>
Motivation: 开发J-DSP的主要动机是支持数字信号处理课程的在线实验室教学，提供交互式的仿真工具来帮助学生更好地理解DSP概念。最初是为了满足ASU的DSP课程需求，后来扩展到更广泛的教育应用。

Method: 采用基于Web的软件开发方法，最初使用Java技术，后来过渡到更安全的HTML5环境。开发团队通过多个NSF资助项目（包括CCLI和IUSE）支持程序扩展，开发了移动版本（iOS和Android），并持续进行功能更新和评估。

Result: J-DSP成功部署并持续使用至今，支持了多种实验练习：数字滤波器设计、FFT频谱分析、机器学习信号分类、量子傅里叶变换仿真等。程序在多个大学得到应用，并扩展到NSF REU、IRES、RET等人才培养项目以及高中推广项目。

Conclusion: J-DSP作为一个长期发展的在线仿真平台，成功支持了DSP教育的多个方面，通过持续的技术更新和功能扩展，为数字信号处理教学提供了有效的工具，并在不同层次的教育项目中发挥了重要作用。

Abstract: This paper presents the history of the online simulation program Java-DSP (J-DSP) and the most recent function development and deployment. J-DSP was created to support online laboratories in DSP classes and was first deployed in our ASU DSP class in 2000. The development of the program and its extensions was supported by several NSF grants including CCLI and IUSE. The web-based software was developed by our team in Java and later transitioned to the more secure HTML5 environment. J-DSP supports laboratory exercises on: digital filters and their design, the FFT and its utility in spectral analysis, machine learning for signal classification, and more recently online simulations with the Quantum Fourier Transform. Throughout the J-DSP development and deployment of this tool and its associated laboratory exercises, we documented evaluations. Mobile versions of the program for iOS and Android were also developed. J-DSP is used to this day in several universities, and specific functions of the program have been used in NSF REU, IRES and RET workforce development and high school outreach.

</details>


### [9] [Efficient Off-Grid Near-Field Cascade Channel Estimation for XL-IRS Systems via Tucker Decomposition](https://arxiv.org/abs/2602.13988)
*Wenzhou Cao,Yashuai Cao,Tiejun Lv,Mugen Peng*

Main category: eess.SP

TL;DR: 提出基于稀疏Tucker分解的离网格级联信道估计框架，用于解决XL-IRS近场效应下的信道估计问题，相比传统方法在精度和效率上有显著提升。


<details>
  <summary>Details</summary>
Motivation: XL-IRS的大孔径导致近场球面波传播效应，使级联信道估计复杂化。传统基于字典的方法存在累积量化误差和高计算复杂度问题，特别是在UPA系统中。

Method: 首先提出UPA结构BS和IRS响应向量的张量积建模方法，然后构建基于稀疏Tucker分解的离网格级联信道估计框架，利用HOSVD预处理、MM算法和定制张量过松弛快速迭代收缩阈值技术加速求解。

Result: 仿真显示所提方案在归一化均方误差上比基准方法提升13.6dB，同时显著减少运行时间。推导了Cramér-Rao下界并进行了收敛性分析。

Conclusion: 提出的张量建模和稀疏Tucker分解框架能有效解决XL-IRS近场级联信道估计问题，避免量化误差，提高估计精度和计算效率。

Abstract: Accurate cascaded channel state information is pivotal for extremely large-scale intelligent reflecting surfaces (XL-IRS) in next-generation wireless networks. However, the large XL-IRS aperture induces spherical wavefront propagation due to near-field (NF) effects, complicating cascaded channel estimation. Conventional dictionary-based methods suffer from cumulative quantization errors and high complexity, especially in uniform planar array (UPA) systems. To address these issues, we first propose a tensor modelization method for NF cascaded channels by exploiting the tensor product among the horizontal and vertical response vectors of the UPA-structured base station (BS) and the incident-reflective array response vector of the IRS. This structure leverages spatial characteristics, enabling independent estimation of factor matrices to improve efficiency. Meanwhile, to avoid quantization errors, we propose an off-grid cascaded channel estimation framework based on sparse Tucker decomposition. Specifically, we model the received signal as a Tucker tensor, where the sparse core tensor captures path gain-delay terms and three factor matrices are spanned by BS and NF IRS array responses. We then formulate a sparse core tensor minimization problem with tri-modal log-sum sparsity constraints to tackle the NP-hard challenge. Finally, the method is accelerated via higher-order singular value decomposition preprocessing, combined with majorization-minimization and a tailored tensor over-relaxation fast iterative shrinkage-thresholding technique. We derive the Cramér-Rao lower bound and conduct convergence analysis. Simulations show the proposed scheme achieves a 13.6 dB improvement in normalized mean square error over benchmarks with significantly reduced runtime.

</details>


### [10] [Lightweight Range-Angle Imaging Based Algorithm for Quasi-Static Human Detection on Low-Cost FMCW Radar](https://arxiv.org/abs/2602.14001)
*Huy Trinh,George Shaker*

Main category: eess.SP

TL;DR: 提出一种基于低成本60GHz FMCW雷达的轻量级非视觉图像处理方法，用于检测准静态人体活动，在Raspberry Pi 4B上实现120FPS实时处理。


<details>
  <summary>Details</summary>
Motivation: 准静态人体活动（如躺、站、坐）产生的多普勒频移极低且雷达特征高度扩散，传统CFAR检测器难以有效检测。同时，隐私问题和低光照条件限制了摄像头在长期护理设施中的应用。

Method: 采用基于图像的轻量级处理方法，使用低成本60GHz FMCW雷达，通过共享的距离-角度预处理管道，在嵌入式设备上实现实时处理。

Result: 在五种半静态活动数据集上，检测准确率显著提升：从CA-CFAR的68.3%和OS-CFAR的78.8%提升至93.24%（受试者1）；从51.3%/68.3%提升至92.3%（受试者2）；从57.72%/69.94%提升至94.82%（受试者3）。在Raspberry Pi 4B上平均每帧处理时间8.2ms，速度比OS-CFAR快74倍。

Conclusion: 简单的基于图像的处理方法能够在杂乱的室内环境中提供鲁棒且可部署的准静态人体感知，为隐私敏感应用提供有效解决方案。

Abstract: Quasi-static human activities such as lying, standing or sitting produce very low Doppler shifts and highly spread radar signatures, making them difficult to detect with conventional constant-false-alarm rate (CFAR) detectors tuned for point targets. Moreover, privacy concerns and low lighting conditions limit the use of cameras in long-term care (LTC) facilities. This paper proposes a lightweight, non-visual image-based method for robust quasi-static human presence detection using a low-cost 60 GHz FMCW radar. On a dataset covering five semi-static activities, the proposed method improves average detection accuracy from 68.3% for Cell-Averaging CFAR (CA-CFAR) and 78.8% for Order-Statistics CFAR (OS-CFAR) to 93.24% for Subject 1, from 51.3%, 68.3% to 92.3% for Subject 2, and 57.72%, 69.94% to 94.82% for Subject 3, respectively. Finally, we benchmarked all three detectors across all activities on a Raspberry Pi 4B using a shared Range-Angle (RA) preprocessing pipeline. The proposed algorithm obtains an average 8.2 ms per frame, resulting in over 120 frames per second (FPS) and a 74 times speed-up over OS-CFAR. These results demonstrate that simple image-based processing can provide robust and deployable quasi-static human sensing in cluttered indoor environments.

</details>


### [11] [Rethinking RSSI for WiFi Sensing](https://arxiv.org/abs/2602.14004)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Y. Jay Guo*

Main category: eess.SP

TL;DR: WiRSSI：仅使用RSSI测量的双基站WiFi被动人体追踪框架，在实用环境中实现约0.8米的中位定位误差，为基于CSI的WiFi感知提供低成本替代方案。


<details>
  <summary>Details</summary>
Motivation: RSSI在商用WiFi设备上广泛可用，但通常被认为过于粗糙，不适合细粒度感知。本文重新审视其感知潜力，探索仅使用RSSI测量实现被动人体追踪的可能性。

Method: 采用1Tx-3Rx配置的双基站WiFi感知框架，可扩展至MIMO部署。首先揭示CSI功率如何隐式编码相位相关信息，以及这种关系如何延续到RSSI。通过2D快速傅里叶变换提取多普勒-AoA特征，从仅振幅信息推断延迟，然后将估计的AoA和延迟映射到笛卡尔坐标并进行去噪以恢复运动轨迹。

Result: 在实用环境实验中，WiRSSI在椭圆、线性和矩形轨迹上分别实现0.905米、0.784米和0.785米的中位XY定位误差。相比之下，代表性的基于CSI的方法获得0.574米、0.599米和0.514米的误差，平均精度差距为0.26米。

Conclusion: 尽管分辨率较低，RSSI仍能支持实用的被动感知，为基于CSI的WiFi感知提供低成本替代方案，展示了RSSI在WiFi感知中的实际应用潜力。

Abstract: The Received Signal Strength Indicator (RSSI) is widely available on commodity WiFi devices but is commonly regarded as too coarse for fine-grained sensing. This paper revisits its sensing potential and presents WiRSSI, a bistatic WiFi sensing framework for passive human tracking using only RSSI measurements. WiRSSI adopts a 1Tx-3Rx configuration and is readily extensible to Multiple-Input Multiple-Output (MIMO) deployments. We first reveal how CSI power implicitly encodes phase-related information and how this relationship carries over to RSSI, showing that RSSI preserves exploitable Doppler, Angle-of-Arrival (AoA), and delay cues associated with human motion. WiRSSI then extracts Doppler-AoA features via a 2D Fast Fourier Transform and infers delay from amplitude-only information in the absence of subcarrier-level phase. The estimated AoA and delay are then mapped to Cartesian coordinates and denoised to recover motion trajectories. Experiments in practical environments show that WiRSSI achieves median XY localization errors of 0.905 m, 0.784 m, and 0.785 m for elliptical, linear, and rectangular trajectories, respectively. In comparison, a representative CSI-based method attains median errors of 0.574 m, 0.599 m, and 0.514 m, corresponding to an average accuracy gap of 0.26 m. These results demonstrate that, despite its lower resolution, RSSI can support practical passive sensing and offers a low-cost alternative to CSI-based WiFi sensing.

</details>


### [12] [Extended Universal Joint Source-Channel Coding for Digital Semantic Communications: Improving Channel-Adaptability](https://arxiv.org/abs/2602.14018)
*Eunsoo Kim,Yoon Huh,Wan Choi*

Main category: eess.SP

TL;DR: 提出euJSCC框架，通过超网络归一化和动态码本生成实现SNR和调制自适应传输，在块衰落和AWGN信道下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VQ-based JSCC方法（包括uJSCC）使用固定的调制特定编码器、解码器和码本，无法适应细粒度的SNR变化，限制了在动态无线环境中的适应性。

Method: 提出euJSCC框架：1) 超网络归一化层进行细粒度特征向量归一化；2) 动态码本生成网络根据块级SNR优化调制特定基码本；3) 内外编码器-解码器架构处理块衰落信道；4) 两阶段训练策略（AWGN预训练+块衰落微调）。

Result: 图像传输实验表明，euJSCC在块衰落和AWGN信道下均优于最先进的信道自适应数字JSCC方案。

Conclusion: euJSCC框架实现了SNR和调制自适应传输，在动态无线环境中表现出优越性能，为语义通信提供了更灵活的解决方案。

Abstract: Recent advances in deep learning (DL)-based joint source-channel coding (JSCC) have enabled efficient semantic communication in dynamic wireless environments. Among these approaches, vector quantization (VQ)-based JSCC effectively maps high-dimensional semantic feature vectors into compact codeword indices for digital modulation. However, existing methods, including universal JSCC (uJSCC), rely on fixed, modulation-specific encoders, decoders, and codebooks, limiting adaptability to fine-grained SNR variations. We propose an extended universal JSCC (euJSCC) framework that achieves SNR- and modulation-adaptive transmission within a single model. euJSCC employs a hypernetwork-based normalization layer for fine-grained feature vector normalization and a dynamic codebook generation (DCG) network that refines modulation-specific base codebooks according to block-wise SNR. To handle block fading channels, which consist of multiple coherence blocks, an inner-outer encoder-decoder architecture is adopted, where the outer encoder and decoder capture long-term channel statistics, and the inner encoder and decoder refine feature vectors to align with block-wise codebooks. A two-phase training strategy, i.e., pretraining on AWGN channels followed by finetuning on block fading channels, ensures stable convergence. Experiments on image transmission demonstrate that euJSCC consistently outperforms state-of-the-art channel-adaptive digital JSCC schemes under both block fading and AWGN channels.

</details>


### [13] [Convexity Meets Curvature: Lifted Near-Field Super-Resolution](https://arxiv.org/abs/2602.14063)
*Sajad Daei,Gábor Fodor,Mikael Skoglund*

Main category: eess.SP

TL;DR: 提出一种基于凸优化的无网格超分辨方法，用于近场测量中的联合角度-距离估计，特别适用于混合前端系统。


<details>
  <summary>Details</summary>
Motivation: 超大孔径、高频载波和通感一体化推动阵列处理进入菲涅尔区域，球面波前导致孔径上的距离相关相位变化，破坏了经典子空间方法的傅里叶/范德蒙结构。混合前端仅提供少量测量，现有近场方法依赖昂贵的2D网格化，需要连续角度分辨率和联合角度-距离推断。

Method: 通过贝塞尔-范德蒙分解菲涅尔相位流形，揭示角度上的隐藏范德蒙结构，将距离依赖隔离到紧凑系数图中。引入提升映射，将每个距离仓和连续角度映射到结构化秩一原子，将非线性近场模型转化为行稀疏矩阵的线性逆问题。通过原子范数最小化进行恢复，利用有界三角多项式的显式对偶表征实现基于证书的定位。

Result: 仿真验证了在强欠采样混合观测下，能够可靠地进行联合角度-距离恢复，适用于下一代无线和通感一体化系统。

Conclusion: 通过凸优化方法结合贝塞尔-范德蒙分解，实现了近场测量中的无网格超分辨联合角度-距离估计，克服了传统方法的局限性，为下一代无线系统提供了有效的解决方案。

Abstract: Extra-large apertures, high carrier frequencies, and integrated sensing and communications (ISAC) are pushing array processing into the Fresnel region, where spherical wavefronts induce a range-dependent phase across the aperture. This curvature breaks the Fourier/Vandermonde structure behind classical subspace methods, and it is especially limiting with hybrid front-ends that provide only a small number of pilot measurements. Consequently, practical systems need continuous angle resolution and joint angle-range inference where many near-field approaches still rely on costly 2D gridding. We show that convexity can meet curvature via a lifted, gridless superresolution framework for near-field measurements. The key is a Bessel-Vandermonde factorization of the Fresnel-phase manifold that exposes a hidden Vandermonde structure in angle while isolating the range dependence into a compact coefficient map. Building on this, we introduce a lifting that maps each range bin and continuous angle to a structured rank-one atom, converting the nonlinear near-field model into a linear inverse problem over a row-sparse matrix. Recovery is posed as atomic-norm minimization and an explicit dual characterization via bounded trigonometric polynomials yields certificate-based localization that super-resolves off-grid angles and identifies active range bins. Simulations with strongly undersampled hybrid observations validate reliable joint angle-range recovery for next-generation wireless and ISAC systems.

</details>


### [14] [Wireless Physical Neural Networks (WPNNs): Opportunities and Challenges](https://arxiv.org/abs/2602.14094)
*Meng Hua,Itsik Bergel,Tolga Girici,Marco Di Renzo,Deniz Gunduz*

Main category: eess.SP

TL;DR: 无线物理神经网络（WPNNs）将无线网络组件视为可学习架构中的计算层，将无线传播环境和网络元素作为可微分算子，实现通信与计算的联合优化。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统与神经网络在结构和功能上具有相似性：信号通过级联元素传播、与环境交互并经历变换。这种相似性启发我们将无线网络重新构想为可学习的计算架构。

Method: 提出无线物理神经网络（WPNNs）统一范式，将收发器、中继器、反向散射和智能表面等无线网络组件解释为学习架构中的计算层。将无线传播环境和网络元素视为可微分算子，通过基于学习的方法直接对物理网络进行优化。

Result: 通过数值示例展示了在信号处理、适应性、效率和端到端优化方面的潜在性能提升。该方法可以独立运行，也可以与传统数字神经网络层结合，实现混合通信学习管道。

Conclusion: 将无线系统重新配置为学习网络具有巨大潜力，为下一代通信框架中的联合通信-计算设计开辟了新途径，展示了将无线媒介作为计算资源的前景。

Abstract: Wireless communication systems exhibit structural and functional similarities to neural networks: signals propagate through cascaded elements, interact with the environment, and undergo transformations. Building upon this perspective, we introduce a unified paradigm, termed \textit{wireless physical neural networks (WPNNs)}, in which components of a wireless network, such as transceivers, relays, backscatter, and intelligent surfaces, are interpreted as computational layers within a learning architecture. By treating the wireless propagation environment and network elements as differentiable operators, new opportunities arise for joint communication-computation designs, where system optimization can be achieved through learning-based methods applied directly to the physical network. This approach may operate independently of, or in conjunction with, conventional digital neural layers, enabling hybrid communication learning pipelines. In the article, we outline representative architectures that embody this viewpoint and discuss the algorithmic and training considerations required to leverage the wireless medium as a computational resource. Through numerical examples, we highlight the potential performance gains in processing, adaptability, efficiency, and end-to-end optimization, demonstrating the promise of reconfiguring wireless systems as learning networks in next-generation communication frameworks.

</details>


### [15] [Electromagnetic Bounds on Realizing Targeted MIMO Transfer Functions in Real-World Systems with Wave-Domain Programmability](https://arxiv.org/abs/2602.14152)
*Philipp del Hougne*

Main category: eess.SP

TL;DR: 该研究为可重构波系统推导了电磁一致性的算子实现精度上限，基于多端口网络模型考虑互耦和硬件约束，通过半定松弛计算严格上界，并应用于三种实验设置验证。


<details>
  <summary>Details</summary>
Motivation: 可重构线性波系统（如RIS、DMA等）在实际应用中需要知道能多准确地实现期望的线性算子，但此前缺乏电磁一致性的性能界限分析。

Method: 建立电磁一致的多端口网络模型（考虑可调元件间的互耦），考虑实际硬件约束（有损、1比特可编程元件），将算子合成任务公式化为二次约束的分数二次问题，通过半定松弛计算严格的上界。

Result: 应用于三种实验设置：2.45GHz自由空间和富散射4×4 MIMO信道（100个1比特可编程RIS元件），以及19GHz 4×4 MIMO信道（DMA到用户）。研究发现可调元件间的耦合强度对性能界限有显著影响，对于RIS设置，界限表明波域灵活性不足以实现考虑的算子合成。

Conclusion: 该研究首次为可重构波系统提供了电磁一致性的算子实现精度界限，揭示了互耦对性能的重要影响，为实际系统设计提供了理论指导。

Abstract: A key question for most applications involving reconfigurable linear wave systems is how accurately a desired linear operator can be realized by configuring the system's tunable elements. The relevance of this question spans from hybrid-MIMO analog combiners via computational meta-imagers to programmable wave-domain signal processing. Yet, no electromagnetically consistent bounds have been derived for the fidelity with which a desired operator can be realized in a real-world reconfigurable wave system. Here, we derive such bounds based on an electromagnetically consistent multiport-network model (capturing mutual coupling between tunable elements) and accounting for real-world hardware constraints (lossy, 1-bit-programmable elements). Specifically, we formulate the operator-synthesis task as a quadratically constrained fractional-quadratic problem and compute rigorous fidelity upper bounds based on semidefinite relaxation. We apply our technique to three distinct experimental setups. The first two setups are, respectively, a free-space and a rich-scattering $4\times 4$ MIMO channel at 2.45 GHz parameterized by a reconfigurable intelligent surface (RIS) comprising 100 1-bit-programmable elements. The third setup is a $4\times 4$ MIMO channel at 19 GHz from four feeds of a dynamic metasurface antenna (DMA) to four users. We systematically study how the achievable fidelity scales with the number of tunable elements, and we probe the tightness of our bounds by trying to find optimized configurations approaching the bounds with standard discrete-optimization techniques. We observe a strong influence of the coupling strength between tunable elements on our fidelity bound. For the two RIS-based setups, our bound attests to insufficient wave-domain flexibility for the considered operator synthesis.

</details>


### [16] [Explainable Interictal Epileptiform Discharge Detection Method Based on Scalp EEG and Retrieval-Augmented Generation](https://arxiv.org/abs/2602.14170)
*Yu Zhu,Jiayang Guo,Jun Jiang,Peipei Gu,Xin Shu,Duo Chen*

Main category: eess.SP

TL;DR: 提出IED-RAG框架，通过检索显式证据实现癫痫样放电检测与报告生成的可解释多模态方法


<details>
  <summary>Details</summary>
Motivation: 癫痫样放电检测对癫痫诊断至关重要，但现有自动化方法缺乏可解释性，需要一种既能准确检测又能生成解释性报告的框架

Method: 使用双编码器提取电生理和语义特征，通过对比学习在共享EEG-文本嵌入空间中对齐，检索临床相关EEG-文本对作为显式证据，条件化大语言模型生成基于证据的报告

Result: 在武汉儿童医院私有数据集和公开TUEV数据集上分别达到89.17%和71.38%的平衡准确率，BLEU分数分别为89.61%和64.14%

Conclusion: 检索显式证据相比传统黑盒方法，既能提升诊断性能又能增强临床可解释性

Abstract: The detection of interictal epileptiform discharge (IED) is crucial for the diagnosis of epilepsy, but automated methods often lack interpretability. This study proposes IED-RAG, an explainable multimodal framework for joint IED detection and report generation. Our approach employs a dual-encoder to extract electrophysiological and semantic features, aligned via contrastive learning in a shared EEG-text embedding space. During inference, clinically relevant EEG-text pairs are retrieved from a vector database as explicit evidence to condition a large language model (LLM) for the generation of evidence-based reports. Evaluated on a private dataset from Wuhan Children's Hospital and the public TUH EEG Events Corpus (TUEV), the framework achieved balanced accuracies of 89.17\% and 71.38\%, with BLEU scores of 89.61\% and 64.14\%, respectively. The results demonstrate that retrieval of explicit evidence enhances both diagnostic performance and clinical interpretability compared to standard black-box methods.

</details>


### [17] [Localization Exploiting Spatial Variations in the Magnetic Field: Principles and Challenges](https://arxiv.org/abs/2602.14181)
*Isaac Skog,Manon Kok,Christophe Prieur,Gustaf Hendeby*

Main category: eess.SP

TL;DR: 本文综述了基于地球磁场空间变化的定位技术中的信号处理原理和研究挑战，从统计信号处理角度分析现有关键技术。


<details>
  <summary>Details</summary>
Motivation: 信号处理在现代定位技术发展中起基础作用，磁场定位技术需要统计状态推断、磁场建模和传感器校准等信号处理方法。目前基于磁场空间变化的定位技术已能达到分米级室内精度和战略级惯性导航系统的室外精度，但需要从信号处理角度系统梳理现有技术。

Method: 采用统一的参数化信号模型框架，将现有关键技术整合到与成熟统计推断方法兼容的框架中，从统计信号处理视角分析技术的相似性和差异性。

Result: 提供了基于地球磁场空间变化的定位技术中信号处理原理的广泛高层次概述，建立了统一的分析框架，识别了当前的研究挑战。

Conclusion: 信号处理在地球磁场空间变化定位技术中至关重要，通过统一的信号模型框架可以更好地理解和比较现有技术，为未来研究提供方向。

Abstract: Signal processing has played, and continues to play, a fundamental role in the evolution of modern localization technologies. Localization using spatial variations in the Earth's magnetic field is no exception. It relies on signal-processing methods for statistical state inference, magnetic-field modeling, and sensor calibration. Contemporary localization techniques based on spatial variations in the magnetic field can provide decimeter-level indoor localization accuracy and outdoor localization accuracy on par with strategic-grade inertial navigation systems. This article provides a broad, high-level overview of current signal-processing principles and open research challenges in localization using spatial variations in the Earth's magnetic field. The aim is to provide the reader with an understanding of the similarities and differences among existing key technologies from a statistical signal-processing perspective. To that end, existing key technologies will be presented within a common parametric signal-model framework compatible with well-established statistical inference methods.

</details>


### [18] [Robust SAC-Enabled UAV-RIS Assisted Secure MISO Systems With Untrusted EH Receivers](https://arxiv.org/abs/2602.14191)
*Hamid Reza Hashempour,Le-Nam Tran,Duy H. N. Nguyen,Hien Quoc Ngo*

Main category: eess.SP

TL;DR: 该论文研究无人机辅助RIS多用户MISO网络中的安全下行传输，提出SAC深度强化学习框架解决最坏情况保密能效最大化问题。


<details>
  <summary>Details</summary>
Motivation: 在无人机辅助RIS多用户网络中，存在不可信能量收集接收器可能窃听信息，且其信道状态信息不完美，需要解决最坏情况保密能效最大化问题。

Method: 提出基于软演员-评论家(SAC)的深度强化学习框架，联合优化无人机水平位置、RIS相位偏移和发射功率分配；同时为完美CSI情况开发了连续凸逼近(SCA)基准方法。

Result: SAC方法相比SCA和深度确定性策略梯度基线分别实现了28%和16%的保密能效提升，对CSI不确定性表现出优越鲁棒性，在不同发射功率水平和RIS尺寸下性能稳定。

Conclusion: SAC深度强化学习框架能有效解决无人机RIS网络中复杂的安全能效优化问题，在非完美CSI环境下具有优越性能和鲁棒性。

Abstract: This paper investigates secure downlink transmission in a UAV-assisted reconfigurable intelligent surface (RIS)-enabled multiuser multiple-input single-output network, where legitimate information-harvesting receivers coexist with untrusted energy-harvesting receivers (UEHRs) capable of eavesdropping. A UAV-mounted RIS provides blockage mitigation and passive beamforming, while the base station employs zero-forcing precoding for multiuser interference suppression. Due to limited feedback from UEHRs, their channel state information (CSI) is imperfect, leading to a worst-case secrecy energy efficiency (WCSEE) maximization problem. We jointly optimize the UAV horizontal position, RIS phase shifts, and transmit power allocation under both perfect and imperfect CSI, considering discrete RIS phases, UAV mobility, and energy-harvesting constraints. The resulting problem is highly nonconvex due to coupled channel geometry, robustness requirements, and discrete variables. To address this challenge, we propose a soft actor-critic (SAC)-based deep reinforcement learning framework that learns WCSEE-maximizing policies through interaction with the wireless environment. As a structured benchmark, a successive convex approximation (SCA) approach is developed for the perfect CSI case with continuous RIS phases. Simulation results show that the proposed SAC method achieves up to 28% and 16% secrecy energy efficiency gains over SCA and deep deterministic policy gradient baselines, respectively, while demonstrating superior robustness to CSI uncertainty and stable performance across varying transmit power levels and RIS sizes.

</details>


### [19] [Low-Cost Physical-Layer Security Design for IRS-Assisted mMIMO Systems with One-Bit DACs](https://arxiv.org/abs/2602.14292)
*Weijie Xiong,Jian Yang,Jingran Lin,Hongli Liu,Zhiling Xiao,Qiang Li*

Main category: eess.SP

TL;DR: 本文提出了一种使用1位DAC的IRS辅助mMIMO系统物理层安全设计，通过两种算法联合优化发射端1位量化预编码和IRS恒定模相移，以最大化保密速率。


<details>
  <summary>Details</summary>
Motivation: mMIMO系统与IRS结合可增强无线通信物理层安全，但大规模mMIMO阵列部署高分辨率量化器和众多IRS元件会导致硬件复杂度极高。本文旨在通过使用1位DAC降低硬件成本，同时保持物理层安全性能。

Method: 提出两种算法：1) WMMSE-PDD算法，将保密速率最大化问题转化为带辅助变量的非分式规划序列，使用加权最小均方误差方法和惩罚对偶分解求解；2) EPPRGD算法，将问题转化为乘积黎曼流形上的无约束优化，消除辅助变量，实现更快收敛。

Result: 仿真结果验证了所提方法的有效性：WMMSE-PDD算法提供优越的保密性能，EPPRGD算法收敛速度更快但保密性能略有折衷。两种算法都能收敛到KKT点，并提供每步迭代的解析解。

Conclusion: 本文提出的1位DAC IRS辅助mMIMO系统物理层安全设计在降低硬件复杂度的同时保持了良好的安全性能，两种算法各有优势，为实际系统部署提供了实用解决方案。

Abstract: Integrating massive multiple-input multiple-output (mMIMO) systems with intelligent reflecting surfaces (IRS) presents a promising paradigm for enhancing physical-layer security (PLS) in wireless communications. However, deploying high-resolution quantizers in large-scale mMIMO arrays, along with numerous IRS elements, leads to substantial hardware complexity. To address these challenges, this paper proposes a cost-effective PLS design for IRS-assisted mMIMO systems by employing one-bit digital-to-analog converters (DACs). The focus is on jointly optimizing one-bit quantized precoding at the transmitter and constant-modulus phase shifts at the IRS to maximize the secrecy rate. This leads to a highly non-convex fractional secrecy rate maximization (SRM) problem. To efficiently solve this problem, two algorithms are proposed: (1) the WMMSE-PDD algorithm, which reformulates the SRM problem into a sequence of non-fractional programs with auxiliary variables using the weighted minimum mean-square error (WMMSE) method and solves them via the penalty dual decomposition (PDD) approach, achieving superior secrecy performance; and (2) the exact penalty product Riemannian gradient descent (EPPRGD) algorithm, which transforms the SRM problem into an unconstrained optimization over a product Riemannian manifold, eliminating auxiliary variables and enabling faster convergence with a slight trade-off in secrecy performance. Both algorithms provide analytical solutions at each iteration and are proven to converge to Karush-Kuhn-Tucker (KKT) points. Simulation results confirm the effectiveness of the proposed methods and highlight their respective advantages.

</details>


### [20] [Online Architecture Search for Compressed Sensing based on Hypergradient Descent](https://arxiv.org/abs/2602.14411)
*Ayano Nakai-Kasai,Yusuke Nakane,Tadashi Wadayama*

Main category: eess.SP

TL;DR: 提出HGD-AS-ISTA和HGD-AS-FISTA算法，使用超梯度下降在线优化结构参数，避免传统深度展开方法需要训练数据和重新训练的问题


<details>
  <summary>Details</summary>
Motivation: AS-ISTA和AS-FISTA通过引入结构参数实现架构搜索，但使用深度展开方法需要训练数据和大量训练时间，且环境变化时需要重新训练

Method: 提出HGD-AS-ISTA和HGD-AS-FISTA，采用超梯度下降（一种在线超参数优化方法）来确定结构参数，实现无需训练数据的在线优化

Result: 实验结果表明，所提方法在提升传统ISTA/FISTA性能的同时，避免了环境变化时需要重新训练的问题

Conclusion: 超梯度下降方法能够有效优化压缩感知算法的结构参数，实现更好的性能且具有环境适应性

Abstract: AS-ISTA (Architecture Searched-Iterative Shrinkage Thresholding Algorithm) and AS-FISTA (AS-Fast ISTA) are compressed sensing algorithms introducing structural parameters to ISTA and FISTA to enable architecture search within the iterative process. The structural parameters are determined using deep unfolding, but this approach requires training data and the large overhead of training time. In this paper, we propose HGD-AS-ISTA (Hypergradient Descent-AS-ISTA) and HGD-AS-FISTA that use hypergradient descent, which is an online hyperparameter optimization method, to determine the structural parameters. Experimental results show that the proposed method improves performance of the conventional ISTA/FISTA while avoiding the need for re-training when the environment changes.

</details>


### [21] [Reconfigurable Intelligent Surfaces-assisted Positioning in Integrated Sensing and Communication Systems](https://arxiv.org/abs/2602.14415)
*Huyen-Trang Ta,Ngoc-Son Duong,Trung-Hieu Nguyen,Van-Linh Nguyen,Thai-Mai Dinh*

Main category: eess.SP

TL;DR: 本文提出了一种用于RIS辅助ISAC系统的高精度目标定位方法，通过粗估计初始化、可分离最小二乘解耦和修改的Levenberg算法实现高效精确定位。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信系统中，通过直接路径和RIS辅助反射路径进行目标定位时，需要解决高精度定位与计算复杂度之间的矛盾。传统方法计算成本高，需要更高效的算法。

Method: 1. 使用序列匹配滤波器获取粗角度参数；2. 基于子载波相位差进行距离恢复；3. 将定位问题建模为非线性能量最小化优化；4. 利用可分离最小二乘结构解耦线性路径增益和非线性几何依赖；5. 提出改进的Levenberg算法，采用近似策略降低计算成本。

Result: 仿真结果表明，所提出的细化方法在达到与传统方法相当精度的同时，显著降低了算法复杂度。

Conclusion: 本文提出的快速迭代细化算法能够有效解决RIS辅助ISAC系统中的高精度目标定位问题，在保持定位精度的同时大幅降低计算复杂度，具有实际应用价值。

Abstract: This paper investigates the problem of high-precision target localization in integrated sensing and communication (ISAC) systems, where the target is sensed via both a direct path and a reconfigurable intelligent surface (RIS)-assisted reflection path. We first develop a sequential matched-filter estimator to acquire coarse angular parameters, followed by a range recovery process based on subcarrier phase differences. Subsequently, we formulate the target localization problem as a non-linear least squares optimization, using the coarse estimates to initialize the target's position coordinates. To solve this efficiently, we introduce a fast iterative refinement algorithm tailored for RIS-aided ISAC environments. Recognizing that the signal model involves both linear path gains and non-linear geometric dependencies, we exploit the separable least-squares structure to decouple these parameters. Furthermore, we propose a modified Levenberg algorithm with an approximation strategy, which enables low-cost parameter updates without necessitating repeated evaluations of the full non-linear model. Simulation results show that the proposed refinement method achieves accuracy comparable to conventional approaches, while significantly reducing algorithmic complexity.

</details>


### [22] [Cramer--Rao Bounds for Magneto-Inductive Integrated Sensing and Communications](https://arxiv.org/abs/2602.14453)
*Haofan Dong,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 该论文推导了磁感应通信中联合估计距离和介质电导率的克拉美-罗界，发现在近场区域联合估计仅带来3dB的精度损失。


<details>
  <summary>Details</summary>
Motivation: 磁感应通信在射频受限环境（地下、水下、体内）中具有优势，但介质电导率会对信道产生确定性影响。需要研究在集成传感与通信框架下，如何同时估计距离和介质电导率，并评估这种联合估计对测距精度的影响。

Method: 推导了磁感应通信中基于MI导频观测的联合估计（距离和介质电导率）的闭式克拉美-罗界，分析了费舍尔信息矩阵，并通过蒙特卡洛最大似然仿真验证理论结果。

Result: 费舍尔信息矩阵分析显示，在近场区域，联合估计的精度损失收敛于3dB，意味着电导率传感最多使测距精度降低2倍。蒙特卡洛仿真证实该CRB在实际操作条件下是可实现的。

Conclusion: 磁感应通信中的联合距离和介质电导率估计在近场区域仅带来有限的精度损失（3dB），表明集成传感与通信框架在该场景下是可行的，且理论性能边界在实际中可达到。

Abstract: Magnetic induction (MI) enables communication in RF-denied environments (underground, underwater, in-body), where the medium conductivity imprints a deterministic signature on the channel. This letter derives a closed-form Cramér--Rao bound (CRB) for the joint estimation of range and medium conductivity from MI pilot observations in an integrated sensing and communication (ISAC) framework. The Fisher information matrix reveals that the joint estimation penalty converges to 3\,dB in the near-field regime, meaning conductivity sensing adds at most a factor-of-two loss in ranging precision. Monte Carlo maximum-likelihood simulations confirm that the CRB is achievable under practical operating conditions.

</details>


### [23] [All-pole centroids in the Wasserstein metric with applications to clustering of spectral densities](https://arxiv.org/abs/2602.14583)
*Rumeshika Pallewela,Filip Elvander*

Main category: eess.SP

TL;DR: 提出一种在谱Wasserstein-2度量下计算功率谱密度集合质心的方法，限制质心属于特定模型阶数的全极点谱集合，可视为寻找二阶平稳高斯过程的自回归代表。


<details>
  <summary>Details</summary>
Motivation: 现有Wasserstein质心方法在谱估计和聚类中虽然成功，但得到的质心是非参数的，其表示和存储复杂度依赖于离散化网格的选择。需要一种紧凑、低维且可解释的谱质心用于下游任务。

Method: 提出计算全极点质心的非凸优化问题，并设计梯度下降方案求解。虽然不能保证收敛到全局最优点，但可以量化所得质心的次优性。

Result: 该方法能够生成紧凑、低维且可解释的谱质心，在音素分类问题上进行了验证。

Conclusion: 该方法为谱Wasserstein-2度量下的谱质心计算提供了一种参数化解决方案，相比非参数方法具有更好的可解释性和存储效率。

Abstract: In this work, we propose a method for computing centroids, or barycenters, in the spectral Wasserstein-2 metric for sets of power spectral densities, where the barycenters are restricted to belong to the set of all-pole spectra with a certain model order. This may be interpreted as finding an autoregressive representative for sets of second-order stationary Gaussian processes. While Wasserstein, or optimal transport, barycenters have been successfully used earlier in problems of spectral estimation and clustering, the resulting barycenters are non-parametric and the complexity of representing and storing them depends on, e.g., the choice of discretization grid. In contrast, the herein proposed method yields compact, low-dimensional, and interpretable spectral centroids that can be used in downstream tasks. Computing the all-pole centroids corresponds to solving a non-convex optimization problem in the model parameters, and we present a gradient descent scheme for addressing this. Although convergence to a globally optimal point cannot be guaranteed, the sub-optimality of the obtained centroids can be quantified. The proposed method is illustrated on a problem of phoneme classification.

</details>


### [24] [Learning Dirac Spectral Transforms for Topological Signals](https://arxiv.org/abs/2602.14590)
*Leonardo Di Nino,Tiziana Cattai,Sergio Barbarossa,Ginestra Bianconi,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: 论文比较了Dirac算子和Hodge-Laplacian在信号处理中的性能，提出了一种参数化非冗余变换，通过数据学习质量参数获得最佳失真-稀疏性权衡。


<details>
  <summary>Details</summary>
Motivation: Dirac算子为不同阶拓扑域（如节点和边信号）的信号处理提供了统一框架，其本征模式能捕获跨域交互，而传统的Hodge-Laplacian本征模式仅在同一拓扑维度内操作。需要比较这两种方法的性能，并探索更好的变换方法。

Method: 1. 比较Dirac算子和Hodge-Laplacian在失真/稀疏性权衡方面的性能；2. 构建连接两个字典的超完备基；3. 提出参数化非冗余变换，其本征模式包含模式特定的质量参数，捕获节点和边模式之间的相互作用；4. 通过数据学习质量参数。

Result: 超完备基比单一方法性能更好；提出的参数化非冗余变换通过数据学习质量参数，能够在失真-稀疏性权衡方面优于完整基和超完备基。

Conclusion: Dirac算子为拓扑信号处理提供了有前景的框架，提出的参数化非冗余变换通过数据驱动学习，能够获得最佳的失真-稀疏性权衡性能，优于传统方法和超完备基方法。

Abstract: The Dirac operator provides a unified framework for processing signals defined over different order topological domains, such as node and edge signals. Its eigenmodes define a spectral representation that inherently captures cross-domain interactions, in contrast to conventional Hodge-Laplacian eigenmodes that operate within a single topological dimension. In this paper, we compare the two alternatives in terms of the distortion/sparsity trade-off and we show how an overcomplete basis built concatenating the two dictionaries can provide better performance with respect to each approach. Then, we propose a parameterized nonredundant transform whose eigenmodes incorporate a mode-specific mass parameter that captures the interplay between node and edge modes. Interestingly, we show that learning the mass parameters from data makes the proposed transform able to achieve the best distortion-sparsity tradeoff with respect to both complete and overcomplete bases.

</details>


### [25] [Synthetic Aperture Communication: Principles and Application to Massive IoT Satellite Uplink](https://arxiv.org/abs/2602.14629)
*Lucas Giroto,Marcus Henninger,Silvio Mandelli*

Main category: eess.SP

TL;DR: 论文提出合成孔径通信(SAC)概念，用于卫星直接对设备上行链路，通过精确到达角估计实现空间信号分离、定位和缓解链路预算限制，验证了在强干扰下的低功耗通信能力。


<details>
  <summary>Details</summary>
Motivation: 合成孔径雷达已广泛用于小阵列长距离高分辨率成像，但合成孔径通信概念尚未探索。本文旨在探索SAC在卫星直接对设备上行链路中的应用，解决物联网设备在非地面网络中的互干扰和严格功率限制问题。

Method: 提出合成孔径通信(SAC)原理，应用于低地球轨道卫星直接对设备上行链路。采用正交频分复用传输和极化编码，在3.5GHz频段进行仿真，卫星轨道高度600km，两个用户设备同时传输。

Result: 仿真结果显示，即使在强干扰情况下（用户设备被解析但位于彼此最强角度旁瓣上），传输功率低至-10dBm时，块错误率仍低于0.1。验证了方案处理互干扰和严格功率限制的能力。

Conclusion: 提出的合成孔径通信方案能够有效解决互干扰和功率限制问题，为非地面网络中的大规模物联网连接铺平了道路。

Abstract: While synthetic aperture radar is widely adopted to provide high-resolution imaging at long distances using small arrays, the concept of coherent synthetic aperture communication (SAC) has not yet been explored. This article introduces the principles of SAC for direct satellite-to-device uplink, showcasing precise direction-of-arrival estimation for user equipment (UE) devices, facilitating spatial signal separation, localization, and easing link budget constraints. Simulations for a low Earth orbit satellite at 600 km orbit and two UE devices performing orthogonal frequency-division multiplexing-based transmission with polar coding at 3.5 GHz demonstrate block error rates below 0.1 with transmission powers as low as -10 dBm, even under strong interference when UE devices are resolved but fall on each other's strongest angular sidelobe. These results validate the ability of the proposed scheme to address mutual interference and stringent power limitations, paving the way for massive Internet of Things connectivity in non-terrestrial networks.

</details>


### [26] [RF-GPT: Teaching AI to See the Wireless World](https://arxiv.org/abs/2602.14833)
*Hang Zou,Yu Tian,Bohao Wang,Lina Bariah,Samson Lasaulce,Chongwen Huang,Mérouane Debbah*

Main category: eess.SP

TL;DR: RF-GPT：首个射频语言模型，利用多模态LLM的视觉编码器处理RF频谱图，通过合成数据训练，在多种无线任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 当前LLM和多模态模型不支持射频信号处理，现有电信领域LLM方法主要处理文本和结构化数据，而传统RF深度学习模型仅针对特定任务，缺乏RF感知与高级推理的桥梁

Method: 将复杂IQ波形映射到时频频谱图，使用预训练视觉编码器处理，将RF表示作为token注入仅解码器LLM；通过合成RF语料库进行监督指令微调，使用标准兼容波形生成器创建六种无线技术的宽带场景，通过文本LLM自动生成指令-答案对

Result: 在宽带调制分类、重叠分析、无线技术识别、WLAN用户计数和5G NR信息提取等基准测试中，RF-GPT展现出强大的多任务性能，而通用视觉语言模型因缺乏RF基础而基本失败

Conclusion: RF-GPT成功填补了RF感知与高级推理之间的空白，通过视觉编码器处理RF频谱图并利用合成数据训练，为无线系统提供了首个原生支持RF信号的语言模型

Abstract: Large language models (LLMs) and multimodal models have become powerful general-purpose reasoning systems. However, radio-frequency (RF) signals, which underpin wireless systems, are still not natively supported by these models. Existing LLM-based approaches for telecom focus mainly on text and structured data, while conventional RF deep-learning models are built separately for specific signal-processing tasks, highlighting a clear gap between RF perception and high-level reasoning. To bridge this gap, we introduce RF-GPT, a radio-frequency language model (RFLM) that utilizes the visual encoders of multimodal LLMs to process and understand RF spectrograms. In this framework, complex in-phase/quadrature (IQ) waveforms are mapped to time-frequency spectrograms and then passed to pretrained visual encoders. The resulting representations are injected as RF tokens into a decoder-only LLM, which generates RF-grounded answers, explanations, and structured outputs. To train RF-GPT, we perform supervised instruction fine-tuning of a pretrained multimodal LLM using a fully synthetic RF corpus. Standards-compliant waveform generators produce wideband scenes for six wireless technologies, from which we derive time-frequency spectrograms, exact configuration metadata, and dense captions. A text-only LLM then converts these captions into RF-grounded instruction-answer pairs, yielding roughly 12,000 RF scenes and 0.625 million instruction examples without any manual labeling. Across benchmarks for wideband modulation classification, overlap analysis, wireless-technology recognition, WLAN user counting, and 5G NR information extraction, RF-GPT achieves strong multi-task performance, whereas general-purpose VLMs with no RF grounding largely fail.

</details>


### [27] [Lattice XBAR Filters in Thin-Film Lithium Niobate](https://arxiv.org/abs/2602.14937)
*Taran Anusorn,Byeongjin Kim,Ian Anderson,Ziqian Yao,Ruochen Lu*

Main category: eess.SP

TL;DR: 基于横向激励体声波谐振器(XBAR)的晶格滤波器演示，在P3F TFLN上实现27.42%和39.11%的3dB分数带宽，插入损耗低于1dB，面积小于1.3mm²


<details>
  <summary>Details</summary>
Motivation: 开发用于下一代无线通信和传感系统的紧凑、高性能射频前端，需要低损耗、宽带声学滤波器。XBAR谐振器具有强机电耦合特性，结合晶格拓扑的固有宽带特性，有望实现这一目标。

Method: 采用周期性极化压电薄膜(P3F)薄膜铌酸锂(TFLN)平台，设计并制造了两种晶格滤波器拓扑：直接晶格和布局平衡晶格。利用XBAR在P3F TFLN中的强机电耦合特性，结合晶格拓扑的宽带特性。

Result: 在约20GHz频率下，直接晶格滤波器实现27.42%的3dB分数带宽和0.88dB插入损耗；布局平衡晶格滤波器实现39.11%的3dB分数带宽和0.96dB插入损耗。所有原型芯片面积均小于1.3mm²。

Conclusion: XBAR基晶格架构在实现低损耗、宽带声学滤波器方面具有巨大潜力，适用于紧凑、高性能射频前端。同时指出了进一步优化的关键挑战和方向。

Abstract: This work presents the demonstration of lattice filters based on laterally excited bulk acoustic resonators (XBARs). Two filter implementations, namely direct lattice and layout-balanced lattice topologies, are designed and fabricated in periodically poled piezoelectric film (P3F) thin-film lithium niobate (TFLN). By leveraging the strong electromechanical coupling of XBARs in P3F TFLN together with the inherently wideband nature of the lattice topology, 3-dB fractional bandwidths (FBWs) of 27.42\% and 39.11\% and low insertion losses (ILs) of 0.88 dB and 0.96 dB are achieved at approximately 20 GHz for the direct and layout-balanced lattice filters, respectively, under conjugate matching. Notably, all prototypes feature compact footprints smaller than 1.3 mm\textsuperscript{2}. These results highlight the potential of XBAR-based lattice architectures to enable low-loss, wideband acoustic filters for compact, high-performance RF front ends in next-generation wireless communication and sensing systems, while also identifying key challenges and directions for further optimization.

</details>


### [28] [Real-time Range-Angle Estimation and Tag Localization for Multi-static Backscatter Systems](https://arxiv.org/abs/2602.14985)
*Tara Esmaeilbeig,Kartik Patel,Traian E. Abrudan,John Kimionis,Eleftherios Kampianakis,Michael S. Eggleston*

Main category: eess.SP

TL;DR: 提出两种低复杂度算法JRAC和SRAE用于多静态反向散射网络的实时定位，相比传统方法大幅降低计算复杂度而不损失精度，在真实测试中实现3米中值定位误差


<details>
  <summary>Details</summary>
Motivation: 6G环境物联网中，大规模多静态反向散射网络需要高效的实时定位算法，传统方法计算复杂度高，难以扩展到数千设备

Method: 提出JRAC和SRAE两种低复杂度范围-角度估计算法，以及ML梯度搜索和IRLS两种实时定位融合算法

Result: JRAC和SRAE相比FFT和子空间基线减少40倍运行时间，IRLS相比ML暴力搜索减少500倍复杂度，在真实测试中实现3米中值定位误差

Conclusion: 所提算法使多静态反向散射网络的实时、可扩展定位在下一代环境物联网中变得实用可行

Abstract: Multi-static backscatter networks (BNs) are strong candidates for joint communication and localization in the ambient IoT paradigm for 6G. Enabling real-time localization in large-scale multi-static deployments with thousands of devices require highly efficient algorithms for estimating key parameters such as range and angle of arrival (AoA), and for fusing these parameters into location estimates. We propose two low-complexity algorithms, Joint Range-Angle Clustering (JRAC) and Stage-wise Range-Angle Estimation (SRAE). Both deliver range and angle estimation accuracy comparable to FFT- and subspace-based baselines while significantly reducing the computation. We then introduce two real-time localization algorithms that fuse the estimated ranges and AoAs: a maximum-likelihood (ML) method solved via gradient search and an iterative re-weighted least squares (IRLS) method. Both achieve localization accuracy comparable to ML-based brute force search albeit with far lower complexity. Experiments on a real-world large-scale multi-static testbed with 4 illuminators, 1 multi-antenna receiver, and 100 tags show that JRAC and SRAE reduce runtime by up to 40X and IRLS achieves up to 500X reduction over ML-based brute force search without degrading localization accuracy. The proposed methods achieve 3 m median localization error across all 100 tags in a sub-6GHz band with 40 MHz bandwidth. These results demonstrate that multi-static range-angle estimation and localization algorithms can make real-time, scalable backscatter localization practical for next-generation ambient IoT networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Directional Concentration Uncertainty: A representational approach to uncertainty quantification for generative models](https://arxiv.org/abs/2602.13264)
*Souradeep Chattopadhyay,Brendan Kennedy,Sai Munikoti,Soumik Sarkar,Karl Pazdernik*

Main category: cs.LG

TL;DR: 提出基于von Mises-Fisher分布的Directional Concentration Uncertainty (DCU)框架，用于量化生成模型的不确定性，无需任务特定启发式方法，在单模态和多模态任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法依赖固定启发式规则，难以跨任务和跨模态泛化，需要更灵活通用的框架来提高生成模型的可靠性和鲁棒性。

Method: 提出Directional Concentration Uncertainty (DCU)，基于von Mises-Fisher分布统计程序，通过测量语言模型多个生成输出的嵌入向量几何分散度来量化不确定性，无需任务特定启发式。

Result: DCU在实验中达到或超过语义熵等先前方法的校准水平，在更复杂的多模态任务中表现出良好的泛化能力。

Conclusion: DCU提供了一个灵活的不确定性量化框架，具有集成到多模态和智能体框架中的潜力，为生成模型的可靠性和鲁棒性提供了新方向。

Abstract: In the critical task of making generative models trustworthy and robust, methods for Uncertainty Quantification (UQ) have begun to show encouraging potential. However, many of these methods rely on rigid heuristics that fail to generalize across tasks and modalities. Here, we propose a novel framework for UQ that is highly flexible and approaches or surpasses the performance of prior heuristic methods. We introduce Directional Concentration Uncertainty (DCU), a novel statistical procedure for quantifying the concentration of embeddings based on the von Mises-Fisher (vMF) distribution. Our method captures uncertainty by measuring the geometric dispersion of multiple generated outputs from a language model using continuous embeddings of the generated outputs without any task specific heuristics. In our experiments, we show that DCU matches or exceeds calibration levels of prior works like semantic entropy (Kuhn et al., 2023) and also generalizes well to more complex tasks in multi-modal domains. We present a framework for the wider potential of DCU and its implications for integration into UQ for multi-modal and agentic frameworks.

</details>


### [30] [BLUEPRINT Rebuilding a Legacy: Multimodal Retrieval for Complex Engineering Drawings and Documents](https://arxiv.org/abs/2602.13345)
*Ethan Seefried,Ran Eldegaway,Sanjay Das,Nathaniel Blanchard,Tirthankar Ghosal*

Main category: cs.LG

TL;DR: Blueprint是一个面向大规模工程图纸档案的多模态检索系统，通过区域检测、OCR识别和融合检索技术，显著提升了工程图纸的检索效果。


<details>
  <summary>Details</summary>
Motivation: 数十年的工程图纸和技术记录被锁在遗留档案中，元数据不一致或缺失，导致检索困难且通常需要人工操作。需要一种能够有效处理大规模工程档案的检索系统。

Method: 系统检测标准图纸区域，应用基于VLM的区域限制OCR，规范化标识符（如DWG、零件、设施），并通过轻量级区域级重排器融合词汇检索和密集检索。

Result: 在包含350个专家策划查询的5k文件基准测试中，Blueprint在Success@3上获得10.1%的绝对增益，在nDCG@3上获得18.9%的相对改进，优于所有视觉语言基线。

Conclusion: Blueprint系统能够自动生成结构化元数据，支持跨设施搜索，在工程图纸检索方面表现出色，并发布了所有查询、运行、标注和代码以促进可重复评估。

Abstract: Decades of engineering drawings and technical records remain locked in legacy archives with inconsistent or missing metadata, making retrieval difficult and often manual. We present Blueprint, a layout-aware multimodal retrieval system designed for large-scale engineering repositories. Blueprint detects canonical drawing regions, applies region-restricted VLM-based OCR, normalizes identifiers (e.g., DWG, part, facility), and fuses lexical and dense retrieval with a lightweight region-level reranker. Deployed on ~770k unlabeled files, it automatically produces structured metadata suitable for cross-facility search.
  We evaluate Blueprint on a 5k-file benchmark with 350 expert-curated queries using pooled, graded (0/1/2) relevance judgments. Blueprint delivers a 10.1% absolute gain in Success@3 and an 18.9% relative improvement in nDCG@3 over the strongest vision-language baseline}, consistently outperforming across vision, text, and multimodal intents. Oracle ablations reveal substantial headroom under perfect region detection and OCR. We release all queries, runs, annotations, and code to facilitate reproducible evaluation on legacy engineering archives.

</details>


### [31] [Exploring the Performance of ML/DL Architectures on the MNIST-1D Dataset](https://arxiv.org/abs/2602.13348)
*Michael Beebe,GodsGift Uzor,Manasa Chepuri,Divya Sree Vemula,Angel Ayala*

Main category: cs.LG

TL;DR: 本文评估了ResNet、TCN和DCNN在MNIST-1D数据集上的性能，发现这些先进架构优于简单模型，验证了MNIST-1D作为计算受限环境下机器学习架构评估基准的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统小数据集（如MNIST）的简单性限制了区分先进神经网络架构的能力。MNIST-1D作为一维适应版本，保持了小数据集的优势同时增加了复杂性，适合研究先进架构在序列数据上的表现。

Method: 在MNIST-1D数据集上实现并评估了三种先进架构：ResNet（残差网络）、TCN（时序卷积网络）和DCNN（扩张卷积神经网络），并与之前测试的逻辑回归、MLP、CNN和GRU等架构进行基准比较。

Result: TCN和DCNN等先进架构在MNIST-1D上表现优异，达到接近人类水平的性能。ResNet也显示出显著改进。这些模型在捕获序列模式和层次特征方面优于简单模型。

Conclusion: MNIST-1D是评估计算受限环境下机器学习架构的稳健基准。架构创新对提升模型性能至关重要，研究为资源有限环境下的深度学习模型优化提供了见解。

Abstract: Small datasets like MNIST have historically been instrumental in advancing machine learning research by providing a controlled environment for rapid experimentation and model evaluation. However, their simplicity often limits their utility for distinguishing between advanced neural network architectures. To address these challenges, Greydanus et al. introduced the MNIST-1D dataset, a one-dimensional adaptation of MNIST designed to explore inductive biases in sequential data. This dataset maintains the advantages of small-scale datasets while introducing variability and complexity that make it ideal for studying advanced architectures.
  In this paper, we extend the exploration of MNIST-1D by evaluating the performance of Residual Networks (ResNet), Temporal Convolutional Networks (TCN), and Dilated Convolutional Neural Networks (DCNN). These models, known for their ability to capture sequential patterns and hierarchical features, were implemented and benchmarked alongside previously tested architectures such as logistic regression, MLPs, CNNs, and GRUs. Our experimental results demonstrate that advanced architectures like TCN and DCNN consistently outperform simpler models, achieving near-human performance on MNIST-1D. ResNet also shows significant improvements, highlighting the importance of leveraging inductive biases and hierarchical feature extraction in small structured datasets.
  Through this study, we validate the utility of MNIST-1D as a robust benchmark for evaluating machine learning architectures under computational constraints. Our findings emphasize the role of architectural innovations in improving model performance and offer insights into optimizing deep learning models for resource-limited environments.

</details>


### [32] [The Speed-up Factor: A Quantitative Multi-Iteration Active Learning Performance Metric](https://arxiv.org/abs/2602.13359)
*Hannes Kath,Thiago S. Gouvêa,Daniel Sonntag*

Main category: cs.LG

TL;DR: 论文提出了一种新的主动学习评估指标——加速因子，用于量化查询方法在迭代过程中的性能提升，相比随机采样所需样本比例的减少程度。


<details>
  <summary>Details</summary>
Motivation: 主动学习研究主要关注查询方法的开发，但缺乏合适的性能指标来评估迭代过程。现有评估方法不足以准确衡量查询方法在多轮迭代中的性能表现。

Method: 提出了加速因子这一量化指标，通过回顾八年主动学习评估文献，并在四个不同领域的数据集上使用七种不同类型的查询方法进行实证评估，与现有最先进的主动学习性能指标进行比较。

Result: 结果验证了加速因子的假设前提，证明其能准确捕捉查询方法相比随机采样所需的样本比例减少程度，并显示出在迭代过程中具有更好的稳定性。

Conclusion: 加速因子是一个有效的主动学习评估指标，能够准确量化查询方法在多轮迭代中的性能提升，为主动学习研究提供了更可靠的评估工具。

Abstract: Machine learning models excel with abundant annotated data, but annotation is often costly and time-intensive. Active learning (AL) aims to improve the performance-to-annotation ratio by using query methods (QMs) to iteratively select the most informative samples. While AL research focuses mainly on QM development, the evaluation of this iterative process lacks appropriate performance metrics. This work reviews eight years of AL evaluation literature and formally introduces the speed-up factor, a quantitative multi-iteration QM performance metric that indicates the fraction of samples needed to match random sampling performance. Using four datasets from diverse domains and seven QMs of various types, we empirically evaluate the speed-up factor and compare it with state-of-the-art AL performance metrics. The results confirm the assumptions underlying the speed-up factor, demonstrate its accuracy in capturing the described fraction, and reveal its superior stability across iterations.

</details>


### [33] [Accelerated Discovery of Cryoprotectant Cocktails via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2602.13398)
*Daniel Emerson,Nora Gaby-Biegel,Purva Joshi,Yoed Rabin,Rebecca D. Sandlin,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出一个结合高通量筛选与多目标贝叶斯优化的主动学习框架，用于高效设计冷冻保护剂配方，在减少实验次数的同时获得高浓度和高细胞存活率的平衡方案。


<details>
  <summary>Details</summary>
Motivation: 设计冷冻保护剂配方面临挑战：配方需要足够浓度抑制冰晶形成，又要足够低毒性保持细胞活性。传统方法依赖专家直觉或大量实验，效率低下。

Method: 结合高通量筛选与多目标贝叶斯优化的主动学习框架。首先测量初始配方集，训练概率代理模型预测浓度和存活率并量化不确定性，然后迭代选择能改善帕累托前沿的实验配方，基于不确定性最大化期望帕累托改进。

Result: 湿实验验证表明，该方法高效发现同时实现高CPA浓度和高暴露后存活率的配方。相比朴素策略和强基线，分别提高支配超体积9.5%和4.5%，减少达到高质量解所需的实验次数。合成研究中仅需先前最先进多目标方法30%的评估次数。

Conclusion: 该框架仅需合适的检测方法和定义的配方空间，可适应不同CPA库、目标定义和细胞系，加速冷冻保存技术的发展。

Abstract: Designing cryoprotectant agent (CPA) cocktails for vitrification is challenging because formulations must be concentrated enough to suppress ice formation yet non-toxic enough to preserve cell viability. This tradeoff creates a large, multi-objective design space in which traditional discovery is slow, often relying on expert intuition or exhaustive experimentation. We present a data-efficient framework that accelerates CPA cocktail design by combining high-throughput screening with an active-learning loop based on multi-objective Bayesian optimization. From an initial set of measured cocktails, we train probabilistic surrogate models to predict concentration and viability and quantify uncertainty across candidate formulations. We then iteratively select the next experiments by prioritizing cocktails expected to improve the Pareto front, maximizing expected Pareto improvement under uncertainty, and update the models as new assay results are collected. Wet-lab validation shows that our approach efficiently discovers cocktails that simultaneously achieve high CPA concentrations and high post-exposure viability. Relative to a naive strategy and a strong baseline, our method improves dominated hypervolume by 9.5\% and 4.5\%, respectively, while reducing the number of experiments needed to reach high-quality solutions. In complementary synthetic studies, it recovers a comparably strong set of Pareto-optimal solutions using only 30\% of the evaluations required by the prior state-of-the-art multi-objective approach, which amounts to saving approximately 10 weeks of experimental time. Because the framework assumes only a suitable assay and defined formulation space, it can be adapted to different CPA libraries, objective definitions, and cell lines to accelerate cryopreservation development.

</details>


### [34] [Why is Normalization Preferred? A Worst-Case Complexity Theory for Stochastically Preconditioned SGD under Heavy-Tailed Noise](https://arxiv.org/abs/2602.13413)
*Yuchen Fang,James Demmel,Javad Lavaei*

Main category: cs.LG

TL;DR: 本文分析了随机预条件SGD在重尾噪声下的最坏情况复杂度，证明归一化方法能保证收敛而裁剪方法可能失败，为大规模模型训练中归一化优于裁剪提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 研究随机预条件SGD（如Adam、RMSProp、Shampoo等自适应方法）在重尾噪声下的最坏情况复杂度，解释为什么在实践中归一化比裁剪更受青睐。

Method: 开发了随机预条件SGD及其加速变体的最坏情况复杂度理论，假设随机梯度噪声具有有限p阶矩(p∈(1,2])，使用归一化和裁剪两种稳定化工具，并提出了新的向量值Burkholder型不等式进行分析。

Result: 归一化方法在已知问题参数时收敛率为O(T^{-(p-1)/(3p-2)})，未知参数时为O(T^{-(p-1)/(2p)})，匹配归一化SGD的最优速率；而裁剪方法在最坏情况下可能不收敛，因为随机预条件器与梯度估计之间存在统计依赖性。

Conclusion: 归一化能保证随机预条件SGD在重尾噪声下的收敛性，而裁剪可能失败，这为大规模模型训练中归一化优于裁剪的实证偏好提供了理论解释。

Abstract: We develop a worst-case complexity theory for stochastically preconditioned stochastic gradient descent (SPSGD) and its accelerated variants under heavy-tailed noise, a setting that encompasses widely used adaptive methods such as Adam, RMSProp, and Shampoo. We assume the stochastic gradient noise has a finite $p$-th moment for some $p \in (1,2]$, and measure convergence after $T$ iterations. While clipping and normalization are parallel tools for stabilizing training of SGD under heavy-tailed noise, there is a fundamental separation in their worst-case properties in stochastically preconditioned settings. We demonstrate that normalization guarantees convergence to a first-order stationary point at rate $\mathcal{O}(T^{-\frac{p-1}{3p-2}})$ when problem parameters are known, and $\mathcal{O}(T^{-\frac{p-1}{2p}})$ when problem parameters are unknown, matching the optimal rates for normalized SGD, respectively. In contrast, we prove that clipping may fail to converge in the worst case due to the statistical dependence between the stochastic preconditioner and the gradient estimates. To enable the analysis, we develop a novel vector-valued Burkholder-type inequality that may be of independent interest. These results provide a theoretical explanation for the empirical preference for normalization over clipping in large-scale model training.

</details>


### [35] [High-Resolution Climate Projections Using Diffusion-Based Downscaling of a Lightweight Climate Emulator](https://arxiv.org/abs/2602.13416)
*Haiwen Guan,Moein Darman,Dibyajyoti Chakraborty,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: 提出基于扩散模型的深度学习降尺度框架，将LUCIE气候模拟器的300km分辨率输出降尺度至25km分辨率，用于区域气候影响评估。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型在天气气候科学中表现出色，但存在长期不稳定性、气候漂移和计算成本高等问题。LUCIE气候模拟器虽然能准确再现长期统计特征，但其原生分辨率(~300km)不足以进行详细的区域影响评估。

Method: 采用基于概率扩散的生成模型，结合条件和后验采样框架，将粗分辨率LUCIE输出降尺度到25km分辨率。模型在2000-2009年约14,000个ERA5时间步长上训练，在2010-2020年LUCIE预测上进行评估。

Result: 模型能够保持LUCIE的粗粒度动力学特征，同时在~28km分辨率下生成精细尺度的气候统计特征。通过纬度平均RMSE、功率谱、概率密度函数和纬向风的第一经验正交函数等多种指标评估性能。

Conclusion: 提出的深度学习降尺度框架成功解决了LUCIE分辨率不足的问题，为区域气候影响评估提供了高分辨率的气候模拟能力，同时保持了原始模型的物理一致性。

Abstract: The proliferation of data-driven models in weather and climate sciences has marked a significant paradigm shift, with advanced models demonstrating exceptional skill in medium-range forecasting. However, these models are often limited by long-term instabilities, climatological drift, and substantial computational costs during training and inference, restricting their broader application for climate studies. Addressing these limitations, Guan et al. (2024) introduced LUCIE, a lightweight, physically consistent climate emulator utilizing a Spherical Fourier Neural Operator (SFNO) architecture. This model is able to reproduce accurate long-term statistics including climatological mean and seasonal variability. However, LUCIE's native resolution (~300 km) is inadequate for detailed regional impact assessments. To overcome this limitation, we introduce a deep learning-based downscaling framework, leveraging probabilistic diffusion-based generative models with conditional and posterior sampling frameworks. These models downscale coarse LUCIE outputs to 25 km resolution. They are trained on approximately 14,000 ERA5 timesteps spanning 2000-2009 and evaluated on LUCIE predictions from 2010 to 2020. Model performance is assessed through diverse metrics, including latitude-averaged RMSE, power spectrum, probability density functions and First Empirical Orthogonal Function of the zonal wind. We observe that the proposed approach is able to preserve the coarse-grained dynamics from LUCIE while generating fine-scaled climatological statistics at ~28km resolution.

</details>


### [36] [Text Has Curvature](https://arxiv.org/abs/2602.13418)
*Karish Grover,Hanqing Zeng,Yinglong Xia,Christos Faloutsos,Geoffrey J. Gordon*

Main category: cs.LG

TL;DR: 论文提出Texture——一种文本原生的离散曲率信号，证明语言具有内在曲率，定义曲率测量方法，并展示其在长文本推理和检索增强生成中的实用性。


<details>
  <summary>Details</summary>
Motivation: 语言建模中常使用弯曲几何（如双曲空间表示层次结构），但一个基本科学问题仍未解决：曲率对文本本身意味着什么？本文旨在探索文本是否具有内在曲率，并建立文本原生的曲率范式。

Method: 提出Texture方法：通过Schrödinger桥调和左右上下文对掩码词的信念，定义离散曲率场。曲率为正表示上下文聚焦意义，为负表示上下文发散为竞争性延续。

Result: 1) 提供理论和经验证据证明自然语料库中的语义推理是非平坦的；2) 定义可操作的Texture曲率信号；3) 在长文本推理（通过曲率引导压缩）和检索增强生成（通过曲率引导路由）两个任务中验证实用性。

Conclusion: 文本确实具有内在曲率，Texture建立了文本原生的曲率范式，使曲率可测量且实用，无需几何训练即可实现几何感知。

Abstract: Does text have an intrinsic curvature? Language is increasingly modeled in curved geometries - hyperbolic spaces for hierarchy, mixed-curvature manifolds for compositional structure - yet a basic scientific question remains unresolved: what does curvature mean for text itself, in a way that is native to language rather than an artifact of the embedding space we choose? We argue that text does indeed have curvature, and show how to detect it, define it, and use it. To this end, we propose Texture, a text-native, word-level discrete curvature signal, and make three contributions. (a) Existence: We provide empirical and theoretical certificates that semantic inference in natural corpora is non-flat, i.e. language has inherent curvature. (b) Definition: We define Texture by reconciling left- and right-context beliefs around a masked word through a Schrodinger bridge, yielding a curvature field that is positive where context focuses meaning and negative where it fans out into competing continuations. (c) Utility: Texture is actionable: it serves as a general-purpose measurement and control primitive enabling geometry without geometric training; we instantiate it on two representative tasks, improving long-context inference through curvature-guided compression and retrieval-augmented generation through curvature-guided routing. Together, our results establish a text-native curvature paradigm, making curvature measurable and practically useful.

</details>


### [37] [Comparing Classifiers: A Case Study Using PyCM](https://arxiv.org/abs/2602.13482)
*Sadra Sabouri,Alireza Zolanvari,Sepand Haghighi*

Main category: cs.LG

TL;DR: PyCM库教程：展示如何通过多维度评估框架深入分析多分类器性能，强调不同评估指标会改变对模型效能的解释


<details>
  <summary>Details</summary>
Motivation: 选择最佳分类模型需要全面理解模型性能，但标准评估指标可能忽略细微但重要的性能差异，需要更深入的分析工具

Method: 使用PyCM库进行深度评估，通过两个案例场景展示不同评估指标如何影响模型效能解释

Result: 发现评估指标的选择会根本性地改变对模型效能的解释，多维度评估框架能揭示模型性能中微小但重要的差异

Conclusion: 多维度评估框架对于全面理解分类模型性能至关重要，标准指标可能遗漏细微的性能权衡，PyCM库为此提供了有效工具

Abstract: Selecting an optimal classification model requires a robust and comprehensive understanding of the performance of the model. This paper provides a tutorial on the PyCM library, demonstrating its utility in conducting deep-dive evaluations of multi-class classifiers. By examining two different case scenarios, we illustrate how the choice of evaluation metrics can fundamentally shift the interpretation of a model's efficacy. Our findings emphasize that a multi-dimensional evaluation framework is essential for uncovering small but important differences in model performance. However, standard metrics may miss these subtle performance trade-offs.

</details>


### [38] [Finding Highly Interpretable Prompt-Specific Circuits in Language Models](https://arxiv.org/abs/2602.13483)
*Gabriel Franco,Lucas M. Tassis,Azalea Rohr,Mark Crovella*

Main category: cs.LG

TL;DR: 该论文挑战了"每个任务只有一个稳定机制"的假设，提出电路是提示特定的，并开发了ACC++方法来提取更清晰的因果信号，发现不同提示模板会诱导不同的机制，但提示可以聚类为具有相似电路的提示家族。


<details>
  <summary>Details</summary>
Motivation: 传统方法在任务层面识别电路时，假设每个任务只有一个稳定机制，通过平均多个提示来提取电路。但作者认为这种假设可能掩盖了电路结构的一个重要来源：即使在固定任务中，电路也是提示特定的。需要开发能够捕捉这种提示特定性的方法。

Method: 在注意力因果通信（ACC）基础上，提出了ACC++改进方法，通过单次前向传播提取注意力头内更干净、更低维的因果信号，减少归因噪声。将ACC++应用于GPT-2、Pythia和Gemma 2模型的间接宾语识别（IOI）任务，分析不同提示模板下的电路变化。

Result: 发现在任何模型中，IOI任务都不存在单一电路：不同提示模板会系统地诱导不同的机制。尽管存在这种变化，但提示可以聚类为具有相似电路的提示家族。为每个家族提出了代表性电路作为分析单元，并开发了自动化可解释性流水线来生成人类可理解的机制解释。

Conclusion: 研究结果通过将分析单元从任务转移到提示，重新定义了电路作为有意义的研究对象。这使在存在提示特定机制的情况下，能够实现可扩展的电路描述，为机制可解释性提供了新的分析框架。

Abstract: Understanding the internal circuits that language models use to solve tasks remains a central challenge in mechanistic interpretability. Most prior work identifies circuits at the task level by averaging across many prompts, implicitly assuming a single stable mechanism per task. We show that this assumption can obscure a crucial source of structure: circuits are prompt-specific, even within a fixed task. Building on attention causal communication (ACC) (Franco & Crovella, 2025), we introduce ACC++, refinements that extract cleaner, lower-dimensional causal signals inside attention heads from a single forward pass. Like ACC, our approach does not require replacement models (e.g., SAEs) or activation patching; ACC++ further improves circuit precision by reducing attribution noise. Applying ACC++ to indirect object identification (IOI) in GPT-2, Pythia, and Gemma 2, we find there is no single circuit for IOI in any model: different prompt templates induce systematically different mechanisms. Despite this variation, prompts cluster into prompt families with similar circuits, and we propose a representative circuit for each family as a practical unit of analysis. Finally, we develop an automated interpretability pipeline that uses ACC++ signals to surface human-interpretable features and assemble mechanistic explanations for prompt families behavior. Together, our results recast circuits as a meaningful object of study by shifting the unit of analysis from tasks to prompts, enabling scalable circuit descriptions in the presence of prompt-specific mechanisms.

</details>


### [39] [Federated Learning of Nonlinear Temporal Dynamics with Graph Attention-based Cross-Client Interpretability](https://arxiv.org/abs/2602.13485)
*Ayse Tursucular,Ayush Mohanty,Nazal Mohamed,Nagi Gebraeel*

Main category: cs.LG

TL;DR: 提出一个联邦学习框架，用于在去中心化非线性系统中学习跨客户端的时序依赖关系，同时保证隐私和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代工业系统网络由分布式传感器监控，各子系统生成高维时间序列数据且相互依赖。在去中心化环境中，原始测量数据无法共享，客户端观测具有异质性，且现有方法受限于固定专有模型无法修改或重新训练。非线性动态使得跨客户端时序依赖关系难以解释。

Method: 每个客户端使用非线性状态空间模型将高维本地观测映射到低维潜在状态。中央服务器使用图注意力网络在通信的潜在状态上学习图结构化的神经状态转移模型。为增强可解释性，将学习到的服务器端转移模型的雅可比矩阵与注意力系数关联。

Result: 建立了理论收敛保证，证明能够收敛到集中式oracle。通过合成实验验证了收敛性、可解释性、可扩展性和隐私性。真实世界实验显示性能与去中心化基线相当。

Conclusion: 该框架首次为去中心化非线性系统中的跨客户端时序依赖关系提供了可解释性表征，解决了实际部署中的隐私、异质性和模型固定等约束问题。

Abstract: Networks of modern industrial systems are increasingly monitored by distributed sensors, where each system comprises multiple subsystems generating high dimensional time series data. These subsystems are often interdependent, making it important to understand how temporal patterns at one subsystem relate to others. This is challenging in decentralized settings where raw measurements cannot be shared and client observations are heterogeneous. In practical deployments each subsystem (client) operates a fixed proprietary model that cannot be modified or retrained, limiting existing approaches. Nonlinear dynamics further make cross client temporal interdependencies difficult to interpret because they are embedded in nonlinear state transition functions. We present a federated framework for learning temporal interdependencies across clients under these constraints. Each client maps high dimensional local observations to low dimensional latent states using a nonlinear state space model. A central server learns a graph structured neural state transition model over the communicated latent states using a Graph Attention Network. For interpretability we relate the Jacobian of the learned server side transition model to attention coefficients, providing the first interpretable characterization of cross client temporal interdependencies in decentralized nonlinear systems. We establish theoretical convergence guarantees to a centralized oracle and validate the framework through synthetic experiments demonstrating convergence, interpretability, scalability and privacy. Additional real world experiments show performance comparable to decentralized baselines.

</details>


### [40] [Preventing Rank Collapse in Federated Low-Rank Adaptation with Client Heterogeneity](https://arxiv.org/abs/2602.13486)
*Fei Wu,Jia Hu,Geyong Min,Shiqiang Wang*

Main category: cs.LG

TL;DR: FedLoRA中存在的rank collapse问题：当客户端使用不同LoRA秩时，全局更新能量会集中在最小共享秩上，导致性能下降。论文提出raFLoRA方法，通过秩分区聚合解决此问题。


<details>
  <summary>Details</summary>
Motivation: 实际联邦学习场景中，客户端的系统资源和数据分布存在异质性，需要支持异构的LoRA秩。但现有FedLoRA方法在异构秩设置下会出现rank collapse现象，导致性能不佳且对秩配置敏感。

Method: 提出raFLoRA方法：将本地更新分解为秩分区，然后根据每个分区的有效客户端贡献进行加权聚合。通过秩分区聚合避免rank collapse，同时保持通信效率。

Result: 在分类和推理任务上的实验表明，raFLoRA能有效防止rank collapse，相比现有FedLoRA基线方法提升了模型性能，同时保持了通信效率。

Conclusion: 通过理论分析揭示了FedLoRA中rank collapse的根本原因，并提出raFLoRA解决方案。该方法在异构联邦学习场景下能有效提升性能，为实际应用提供了更好的基础模型微调方案。

Abstract: Federated low-rank adaptation (FedLoRA) has facilitated communication-efficient and privacy-preserving fine-tuning of foundation models for downstream tasks. In practical federated learning scenarios, client heterogeneity in system resources and data distributions motivates heterogeneous LoRA ranks across clients. We identify a previously overlooked phenomenon in heterogeneous FedLoRA, termed rank collapse, where the energy of the global update concentrates on the minimum shared rank, resulting in suboptimal performance and high sensitivity to rank configurations. Through theoretical analysis, we reveal the root cause of rank collapse: a mismatch between rank-agnostic aggregation weights and rank-dependent client contributions, which systematically suppresses higher-rank updates at a geometric rate over rounds. Motivated by this insight, we propose raFLoRA, a rank-partitioned aggregation method that decomposes local updates into rank partitions and then aggregates each partition weighted by its effective client contributions. Extensive experiments across classification and reasoning tasks show that raFLoRA prevents rank collapse, improves model performance, and preserves communication efficiency compared to state-of-the-art FedLoRA baselines.

</details>


### [41] [Testing For Distribution Shifts with Conditional Conformal Test Martingales](https://arxiv.org/abs/2602.13848)
*Shalev Shaer,Yarin Bar,Drew Prinster,Yaniv Romano*

Main category: cs.LG

TL;DR: 提出一种基于固定参考集的序列测试方法，用于检测任意分布漂移，避免了传统方法中的测试时污染问题，实现更快的检测速度和更强的统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有conformal test martingales (CTMs)方法通过不断扩展参考集来评估新样本的异常性，但这种方法存在测试时污染问题：漂移后的样本进入参考集会稀释分布漂移的证据，增加检测延迟并降低检测能力。

Method: 提出一种固定参考集的方法，将每个新样本与固定的零假设参考数据集进行比较。主要技术贡献是构建一个鲁棒的马丁格尔，通过显式考虑有限参考集引起的参考分布估计误差，确保在给定参考数据条件下的有效性。

Result: 该方法实现了任意时间有效的第一类错误控制，同时保证了渐近检测能力为1和有界的期望检测延迟。实证结果显示，该方法比标准CTMs检测漂移更快。

Conclusion: 该方法提供了一种强大可靠的分布漂移检测器，通过避免测试时污染问题，在保持统计保证的同时显著提高了检测性能。

Abstract: We propose a sequential test for detecting arbitrary distribution shifts that allows conformal test martingales (CTMs) to work under a fixed, reference-conditional setting. Existing CTM detectors construct test martingales by continually growing a reference set with each incoming sample, using it to assess how atypical the new sample is relative to past observations. While this design yields anytime-valid type-I error control, it suffers from test-time contamination: after a change, post-shift observations enter the reference set and dilute the evidence for distribution shift, increasing detection delay and reducing power.
  In contrast, our method avoids contamination by design by comparing each new sample to a fixed null reference dataset. Our main technical contribution is a robust martingale construction that remains valid conditional on the null reference data, achieved by explicitly accounting for the estimation error in the reference distribution induced by the finite reference set. This yields anytime-valid type-I error control together with guarantees of asymptotic power one and bounded expected detection delay. Empirically, our method detects shifts faster than standard CTMs, providing a powerful and reliable distribution-shift detector.

</details>


### [42] [TrasMuon: Trust-Region Adaptive Scaling for Orthogonalized Momentum Optimizers](https://arxiv.org/abs/2602.13498)
*Peng Cheng,Jiucheng Zang,Qingnan Li,Liheng Ma,Yufei Cui,Yingxue Zhang,Boxing Chen,Ming Jian,Wen Tong*

Main category: cs.LG

TL;DR: TrasMuon优化器在Muon的基础上引入全局RMS校准和基于能量的信任区域裁剪，既保持了近等距几何特性，又稳定了更新幅度，提高了收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器使用Newton-Schulz迭代正交化更新，虽然几何特性优于Adam系列方法，但正交化过程丢弃了幅度信息，导致训练对步长超参数敏感且容易产生高能量爆发。需要解决这些稳定性问题。

Method: TrasMuon在保持Muon近等距几何特性的同时，通过两种机制稳定幅度：(1) 全局RMS校准，(2) 基于相对能量比的信任区域裁剪，将更新限制在稳定区域内。

Result: 在视觉和语言模型上的实验表明，TrasMuon比基线方法收敛更快。在没有预热阶段的实验中，TrasMuon显示出更好的稳定性和鲁棒性。

Conclusion: TrasMuon通过结合全局RMS校准和能量信任区域裁剪，成功解决了Muon优化器的稳定性问题，在保持良好几何特性的同时实现了更稳定高效的优化。

Abstract: Muon-style optimizers leverage Newton-Schulz (NS) iterations to orthogonalize updates, yielding update geometries that often outperform Adam-series methods. However, this orthogonalization discards magnitude information, rendering training sensitive to step-size hyperparameters and vulnerable to high-energy bursts. To mitigate this, we introduce TrasMuon (\textbf{T}rust \textbf{R}egion \textbf{A}daptive \textbf{S}caling \textbf{Muon}). TrasMuon preserves the near-isometric geometry of Muon while stabilizing magnitudes through (i) global RMS calibration and (ii) energy-based trust-region clipping. We demonstrate that while reintroducing adaptive scaling improves optimization efficiency, it typically exacerbates instability due to high-energy outliers. TrasMuon addresses this by defining a trust region based on relative energy ratios, confining updates to a stable zone. Empirical experiments on vision and language models demonstrate that TrasMuon converges faster than baselines. Furthermore, experiments without warmup stages confirm TrasMuon's superior stability and robustness.

</details>


### [43] [Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws](https://arxiv.org/abs/2602.14208)
*Jinbo Wang,Binghui Li,Zhanpeng Zhou,Mingze Wang,Yuxuan Sun,Jiaqi Zhang,Xunliang Cai,Lei Wu*

Main category: cs.LG

TL;DR: 论文提出基于功能缩放定律框架分析批量大小调度，发现最优调度策略取决于任务难度：简单任务应持续增大批量，困难任务应保持小批量至后期再切换到大批量，并揭示了"快速追赶效应"的动力学机制。


<details>
  <summary>Details</summary>
Motivation: 批量大小调度在大规模深度学习训练中至关重要，但理论基础薄弱。需要建立理论框架来理解批量大小调度对优化动态和计算效率的影响，特别是解释不同任务难度的最优调度策略差异。

Method: 采用功能缩放定律框架分析批量大小调度，理论推导最优调度结构，揭示"快速追赶效应"的动力学机制，并通过大规模语言模型预训练实验验证理论预测，涵盖密集和MoE架构，参数达11亿，token达1万亿。

Result: 理论分析表明：简单任务的最优调度持续增大批量，困难任务的最优调度保持小批量至后期切换。实验验证中，后期切换调度在1.1B参数、1T token的LLM预训练中，持续优于恒定批量或早期切换基线。

Conclusion: 功能缩放定律为批量大小调度提供了理论框架，揭示了任务难度决定最优调度结构的规律。后期切换策略能显著减少数据消耗而不牺牲性能，为大规模深度学习训练提供了高效实用的调度方案。

Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.

</details>


### [44] [$γ$-weakly $θ$-up-concavity: Linearizable Non-Convex Optimization with Applications to DR-Submodular and OSS Functions](https://arxiv.org/abs/2602.13506)
*Mohammad Pedramfar,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 论文提出了γ-弱θ-上凹函数的新概念，统一了DR-子模函数和单边光滑函数，证明了这类函数具有上线性化性质，从而为离线和在线优化提供了统一的近似保证。


<details>
  <summary>Details</summary>
Motivation: 优化单调非凸函数是机器学习和组合优化中的基本挑战。现有研究主要关注DR-子模函数和单边光滑函数等特定类别，缺乏统一的框架。本文旨在提出一个更一般的函数类，能够统一这些现有类别，并提供通用的优化理论保证。

Method: 引入γ-弱θ-上凹性这一新的函数性质，证明这类函数具有上线性化性质：对于任何可行点，可以构造一个线性替代函数，其增益能够近似原始非线性目标函数。近似系数仅依赖于γ、θ和可行集的几何结构。

Result: 1. 为离线优化提供了统一的近似保证；2. 通过标准归约到线性优化，为在线设置提供了静态和动态遗憾界；3. 恢复了DR-子模最大化的最优近似系数；4. 显著改进了单边光滑优化在拟阵约束下的现有近似系数。

Conclusion: γ-弱θ-上凹函数提供了一个强大的统一框架，严格推广了DR-子模函数和单边光滑函数。上线性化性质使得能够为广泛的优化问题提供统一的近似保证，在理论和应用上都有重要意义。

Abstract: Optimizing monotone non-convex functions is a fundamental challenge across machine learning and combinatorial optimization. We introduce and study $γ$-weakly $θ$-up-concavity, a novel first-order condition that characterizes a broad class of such functions. This condition provides a powerful unifying framework, strictly generalizing both DR-submodular functions and One-Sided Smooth (OSS) functions. Our central theoretical contribution demonstrates that $γ$-weakly $θ$-up-concave functions are upper-linearizable: for any feasible point, we can construct a linear surrogate whose gains provably approximate the original non-linear objective. This approximation holds up to a constant factor, namely the approximation coefficient, dependent solely on $γ$, $θ$, and the geometry of the feasible set. This linearizability yields immediate and unified approximation guarantees for a wide range of problems. Specifically, we obtain unified approximation guarantees for offline optimization as well as static and dynamic regret bounds in online settings via standard reductions to linear optimization. Moreover, our framework recovers the optimal approximation coefficient for DR-submodular maximization and significantly improves existing approximation coefficients for OSS optimization, particularly over matroid constraints.

</details>


### [45] [The geometry of invariant learning: an information-theoretic analysis of data augmentation and generalization](https://arxiv.org/abs/2602.14423)
*Abdelali Bouyahia,Frédéric LeBlanc,Mario Marchand*

Main category: cs.LG

TL;DR: 本文提出一个信息论框架，系统分析数据增强对泛化和不变性学习的影响，通过分解泛化差距为三个可解释项，并引入群直径概念揭示增强强度与泛化性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 数据增强是现代机器学习中广泛使用的提升泛化能力的技术，但其理论作用尚未完全理解。需要建立一个系统框架来量化数据增强如何影响泛化性能和不变性学习。

Method: 基于互信息边界构建信息论框架，将增强分布建模为原始数据分布与变换分布的复合，推导出包含三个可解释项的泛化边界：(1)原始与增强数据分布差异，(2)算法对训练数据的稳定性，(3)增强变异性敏感性。引入群直径概念量化增强强度。

Result: 理论分析揭示了增强强度（群直径）与泛化性能的内在权衡：小直径保持数据保真度但正则化有限，大直径增强稳定性但增加偏差和敏感性。数值实验验证了理论边界能可靠追踪真实泛化差距。

Conclusion: 提出的信息论框架为理解数据增强的理论作用提供了系统工具，揭示了增强强度与泛化性能的权衡关系，为实践中选择合适的数据增强策略提供了理论指导。

Abstract: Data augmentation is one of the most widely used techniques to improve generalization in modern machine learning, often justified by its ability to promote invariance to label-irrelevant transformations. However, its theoretical role remains only partially understood. In this work, we propose an information-theoretic framework that systematically accounts for the effect of augmentation on generalization and invariance learning. Our approach builds upon mutual information-based bounds, which relate the generalization gap to the amount of information a learning algorithm retains about its training data. We extend this framework by modeling the augmented distribution as a composition of the original data distribution with a distribution over transformations, which naturally induces an orbit-averaged loss function. Under mild sub-Gaussian assumptions on the loss function and the augmentation process, we derive a new generalization bound that decompose the expected generalization gap into three interpretable terms: (1) a distributional divergence between the original and augmented data, (2) a stability term measuring the algorithm dependence on training data, and (3) a sensitivity term capturing the effect of augmentation variability. To connect our bounds to the geometry of the augmentation group, we introduce the notion of group diameter, defined as the maximal perturbation that augmentations can induce in the input space. The group diameter provides a unified control parameter that bounds all three terms and highlights an intrinsic trade-off: small diameters preserve data fidelity but offer limited regularization, while large diameters enhance stability at the cost of increased bias and sensitivity. We validate our theoretical bounds with numerical experiments, demonstrating that it reliably tracks and predicts the behavior of the true generalization gap.

</details>


### [46] [Singular Vectors of Attention Heads Align with Features](https://arxiv.org/abs/2602.13524)
*Gabriel Franco,Carson Loughridge,Mark Crovella*

Main category: cs.LG

TL;DR: 该论文研究了语言模型中特征表示与注意力矩阵奇异向量的对齐问题，证明了这种对齐的理论合理性，并提出了可操作的验证方法。


<details>
  <summary>Details</summary>
Motivation: 近期研究隐含假设特征表示可以从注意力矩阵的奇异向量推断，但缺乏理论依据。本文旨在探究奇异向量何时以及为何能与特征对齐，为特征识别提供理论基础。

Method: 1. 在可直接观察特征表示的模型中验证奇异向量与特征的对齐；2. 理论分析对齐发生的条件；3. 提出稀疏注意力分解作为可操作的对齐识别方法，并在真实模型中验证。

Result: 1. 奇异向量在可控模型中确实与特征对齐；2. 理论分析表明在一定条件下对齐是预期的；3. 稀疏注意力分解在真实模型中出现，与理论预测一致。

Conclusion: 奇异向量与特征的对齐可以作为语言模型中特征识别的合理且理论上有依据的基础，为机制可解释性研究提供了重要支持。

Abstract: Identifying feature representations in language models is a central task in mechanistic interpretability. Several recent studies have made an implicit assumption that feature representations can be inferred in some cases from singular vectors of attention matrices. However, sound justification for this assumption is lacking. In this paper we address that question, asking: why and when do singular vectors align with features? First, we demonstrate that singular vectors robustly align with features in a model where features can be directly observed. We then show theoretically that such alignment is expected under a range of conditions. We close by asking how, operationally, alignment may be recognized in real models where feature representations are not directly observable. We identify sparse attention decomposition as a testable prediction of alignment, and show evidence that it emerges in a manner consistent with predictions in real models. Together these results suggest that alignment of singular vectors with features can be a sound and theoretically justified basis for feature identification in language models.

</details>


### [47] [S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations](https://arxiv.org/abs/2602.14432)
*Arnav Chavan,Nahush Lele,Udbhav Bamba,Sankalp Dayal,Aditi Raghunathan,Deepak Gupta*

Main category: cs.LG

TL;DR: S²D方法通过选择性谱衰减正则化权重最大奇异值，有效减少激活异常值，提升大模型量化精度


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型中的激活异常值严重阻碍模型量化，导致量化后精度大幅下降。随着预训练规模增大（如从CLIP到SigLIP/SigLIP2），异常值问题更加严重，需要解决这一基础挑战以实现高效部署。

Method: 提出选择性谱衰减（S²D）方法，基于几何原理的条件化技术，在微调过程中仅对权重矩阵中对应最大奇异值的分量进行手术式正则化，从而减少激活异常值。

Result: S²D显著减少激活异常值，产生对量化友好的条件化表示。在W4A4量化下，ImageNet上PTQ精度提升达7%，结合QAT时提升4%。改进效果在下游任务和视觉语言模型中具有泛化性。

Conclusion: 通过理论分析和实证研究建立了激活异常值与权重主导奇异值的直接联系，提出的S²D方法能够在不牺牲部署效率的前提下，支持越来越大规模和严格训练的模型的扩展。

Abstract: Activation outliers in large-scale transformer models pose a fundamental challenge to model quantization, creating excessively large ranges that cause severe accuracy drops during quantization. We empirically observe that outlier severity intensifies with pre-training scale (e.g., progressing from CLIP to the more extensively trained SigLIP and SigLIP2). Through theoretical analysis as well as empirical correlation studies, we establish the direct link between these activation outliers and dominant singular values of the weights. Building on this insight, we propose Selective Spectral Decay ($S^2D$), a geometrically-principled conditioning method that surgically regularizes only the weight components corresponding to the largest singular values during fine-tuning. Through extensive experiments, we demonstrate that $S^2D$ significantly reduces activation outliers and produces well-conditioned representations that are inherently quantization-friendly. Models trained with $S^2D$ achieve up to 7% improved PTQ accuracy on ImageNet under W4A4 quantization and 4% gains when combined with QAT. These improvements also generalize across downstream tasks and vision-language models, enabling the scaling of increasingly large and rigorously trained models without sacrificing deployment efficiency.

</details>


### [48] [QuaRK: A Quantum Reservoir Kernel for Time Series Learning](https://arxiv.org/abs/2602.13531)
*Abdallah Aaraba,Soumaya Cherkaoui,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: QuaRK：一个端到端的量子储层计算框架，结合硬件现实的量子储层特征提取器和基于核的读出方案，用于时间序列学习，并提供学习理论保证。


<details>
  <summary>Details</summary>
Motivation: 量子储层计算为时间序列学习提供了有前景的途径，但现有研究中缺乏高效、可实现的量子储层架构以及模型学习保证。需要填补这一空白。

Method: 提出QuaRK框架：1）使用硬件现实的量子储层特征提取器，通过经典阴影层析术高效测量k-局部可观测量；2）结合基于核的经典读出方案，具有显式正则化和快速优化；3）提供明确的计算参数控制（电路宽度、深度和测量预算）。

Result: QuaRK框架提供了学习理论泛化保证，将设计和资源选择与有限样本性能联系起来。实证实验验证了QuaRK在合成beta混合时间序列任务上的插值和泛化行为。

Conclusion: QuaRK为构建可靠的时间序列学习器提供了原则性指导，结合了量子储层计算的丰富动力学和核方法的灵活性，同时保持可扩展性和硬件可实现性。

Abstract: Quantum reservoir computing offers a promising route for time series learning by modelling sequential data via rich quantum dynamics while the only training required happens at the level of a lightweight classical readout. However, studies featuring efficient and implementable quantum reservoir architectures along with model learning guarantees remain scarce in the literature. To close this gap, we introduce QuaRK, an end-to-end framework that couples a hardware-realistic quantum reservoir featurizer with a kernel-based readout scheme. Given a sequence of sample points, the reservoir injects the points one after the other to yield a compact feature vector from efficiently measured k-local observables using classical shadow tomography, after which a classical kernel-based readout learns the target mapping with explicit regularization and fast optimization. The resulting pipeline exposes clear computational knobs -- circuit width and depth as well as the measurement budget -- while preserving the flexibility of kernel methods to model nonlinear temporal functionals and being scalable to high-dimensional data. We further provide learning-theoretic generalization guarantees for dependent temporal data, linking design and resource choices to finite-sample performance, thereby offering principled guidance for building reliable temporal learners. Empirical experiments validate QuaRK and illustrate the predicted interpolation and generalization behaviours on synthetic beta-mixing time series tasks.

</details>


### [49] [Truly Adapting to Adversarial Constraints in Constrained MABs](https://arxiv.org/abs/2602.14543)
*Francesco Emanuele Stradi,Kalana Kalupahana,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 论文研究带未知约束的多臂老虎机问题，在非平稳环境下同时最小化损失和控制约束违反，提出了在不同反馈机制下的最优算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么假设约束是随机的，要么在完全对抗性约束下放松基准。本文旨在解决当约束是随机而损失任意变化时，如何同时实现次线性遗憾和次线性约束违反的问题。

Method: 针对不同反馈机制设计算法：1) 完全反馈下提出算法获得√T+C的遗憾和正约束违反；2) 扩展到只有损失有老虎机反馈的情况；3) 当约束也有老虎机反馈时，设计新算法实现√T+C的正约束违反和√T+C√T的遗憾。

Result: 在约束随机而损失任意变化的情况下，首次实现了最优的遗憾和正约束违反率。算法性能随约束对抗性程度平滑下降，其中C量化了约束的非平稳性。

Conclusion: 本文为带未知约束的多臂老虎机问题提供了首个在非平稳环境下同时保证次线性遗憾和次线性约束违反的算法，填补了随机约束与对抗性约束之间的空白。

Abstract: We study the constrained variant of the \emph{multi-armed bandit} (MAB) problem, in which the learner aims not only at minimizing the total loss incurred during the learning dynamic, but also at controlling the violation of multiple \emph{unknown} constraints, under both \emph{full} and \emph{bandit feedback}. We consider a non-stationary environment that subsumes both stochastic and adversarial models and where, at each round, both losses and constraints are drawn from distributions that may change arbitrarily over time. In such a setting, it is provably not possible to guarantee both sublinear regret and sublinear violation. Accordingly, prior work has mainly focused either on settings with stochastic constraints or on relaxing the benchmark with fully adversarial constraints (\emph{e.g.}, via competitive ratios with respect to the optimum). We provide the first algorithms that achieve optimal rates of regret and \emph{positive} constraint violation when the constraints are stochastic while the losses may vary arbitrarily, and that simultaneously yield guarantees that degrade smoothly with the degree of adversariality of the constraints. Specifically, under \emph{full feedback} we propose an algorithm attaining $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ regret and $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation, where $C$ quantifies the amount of non-stationarity in the constraints. We then show how to extend these guarantees when only bandit feedback is available for the losses. Finally, when \emph{bandit feedback} is available for the constraints, we design an algorithm achieving $\widetilde{\mathcal{O}}(\sqrt{T}+C)$ {positive} violation and $\widetilde{\mathcal{O}}(\sqrt{T}+C\sqrt{T})$ regret.

</details>


### [50] [Fast Swap-Based Element Selection for Multiplication-Free Dimension Reduction](https://arxiv.org/abs/2602.13532)
*Nobutaka Ono*

Main category: cs.LG

TL;DR: 提出一种基于元素选择的快速降维算法，通过选择输入元素的子集实现无乘法降维，相比PCA更适用于资源受限系统。


<details>
  <summary>Details</summary>
Motivation: 传统降维方法如PCA依赖矩阵乘法，在资源受限系统中乘法计算本身可能成为瓶颈。需要一种无需乘法的降维方法，通过简单选择元素子集来实现降维。

Method: 使用基于最小均方误差的线性回归评估候选子集，通过矩阵求逆引理推导元素交换对目标函数影响的公式，采用基于交换的局部搜索算法不断应用目标函数减少的交换直到无法改进。

Result: 在MNIST手写数字图像数据集上的实验证明了该方法的有效性，能够实现无乘法的快速降维。

Conclusion: 提出的元素选择算法提供了一种高效的乘法免费降维方案，特别适用于计算资源受限的环境，通过局部搜索优化元素选择实现有效降维。

Abstract: In this paper, we propose a fast algorithm for element selection, a multiplication-free form of dimension reduction that produces a dimension-reduced vector by simply selecting a subset of elements from the input. Dimension reduction is a fundamental technique for reducing unnecessary model parameters, mitigating overfitting, and accelerating training and inference. A standard approach is principal component analysis (PCA), but PCA relies on matrix multiplications; on resource-constrained systems, the multiplication count itself can become a bottleneck. Element selection eliminates this cost because the reduction consists only of selecting elements, and thus the key challenge is to determine which elements should be retained. We evaluate a candidate subset through the minimum mean-squared error of linear regression that predicts a target vector from the selected elements, where the target may be, for example, a one-hot label vector in classification. When an explicit target is unavailable, the input itself can be used as the target, yielding a reconstruction-based criterion. The resulting optimization is combinatorial, and exhaustive search is impractical. To address this, we derive an efficient formula for the objective change caused by swapping a selected and an unselected element, using the matrix inversion lemma, and we perform a swap-based local search that repeatedly applies objective-decreasing swaps until no further improvement is possible. Experiments on MNIST handwritten-digit images demonstrate the effectiveness of the proposed method.

</details>


### [51] [Replicable Constrained Bandits](https://arxiv.org/abs/2602.14580)
*Matteo Bollini,Gianmarco Genalti,Francesco Emanuele Stradi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.LG

TL;DR: 该论文首次研究了约束多臂老虎机问题中的算法可复制性，设计了可复制算法，其遗憾和约束违反与非可复制算法相当。


<details>
  <summary>Details</summary>
Motivation: 机器学习实验需要可重复性，算法可复制性要求算法在不同执行中做出相同决策。约束MAB问题中，学习者不仅需要最大化奖励，还要满足多个约束条件，研究如何在这种复杂环境下实现可复制性具有重要意义。

Method: 首先为无约束MAB问题开发了第一个可复制的UCB类算法，证明基于不确定性乐观原则的算法可以实现可复制性。然后将这种方法扩展到约束MAB问题，设计可复制算法来处理奖励最大化和约束满足的双重目标。

Result: 成功实现了约束MAB中的算法可复制性，设计的可复制算法在遗憾和约束违反方面与非可复制算法具有相同的T阶性能。这是约束MAB问题中可复制性的首次实现。

Conclusion: 算法可复制性可以在约束MAB问题中实现，且不影响算法性能。不确定性乐观原则的算法可以被设计为可复制的，这一发现对算法可复制性研究具有独立价值。

Abstract: Algorithmic \emph{replicability} has recently been introduced to address the need for reproducible experiments in machine learning. A \emph{replicable online learning} algorithm is one that takes the same sequence of decisions across different executions in the same environment, with high probability. We initiate the study of algorithmic replicability in \emph{constrained} MAB problems, where a learner interacts with an unknown stochastic environment for $T$ rounds, seeking not only to maximize reward but also to satisfy multiple constraints. Our main result is that replicability can be achieved in constrained MABs. Specifically, we design replicable algorithms whose regret and constraint violation match those of non-replicable ones in terms of $T$. As a key step toward these guarantees, we develop the first replicable UCB-like algorithm for \emph{unconstrained} MABs, showing that algorithms that employ the optimism in-the-face-of-uncertainty principle can be replicable, a result that we believe is of independent interest.

</details>


### [52] [Out-of-Support Generalisation via Weight Space Sequence Modelling](https://arxiv.org/abs/2602.13550)
*Roussel Desmond Nzoyem*

Main category: cs.LG

TL;DR: WeightCaster将分布外泛化问题重构为权重空间的序列建模任务，通过将训练集划分为同心壳层来生成合理、可解释且具有不确定性感知的预测，无需显式归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在训练集范围外的数据点上经常出现灾难性失败，产生不现实但过度自信的预测，这限制了AI在安全关键应用中的广泛采用。

Method: 将分布外泛化问题重构为权重空间的序列建模任务，将训练集划分为对应离散序列步骤的同心壳层，开发WeightCaster框架。

Result: 在合成余弦数据集和真实空气质量传感器读数上的实证验证显示，性能与最先进方法相当或更优，能够生成合理、可解释且具有不确定性感知的预测。

Conclusion: 通过增强分布外场景的可靠性，这些结果对人工智能在安全关键应用中的更广泛采用具有重要意义。

Abstract: As breakthroughs in deep learning transform key industries, models are increasingly required to extrapolate on datapoints found outside the range of the training set, a challenge we coin as out-of-support (OoS) generalisation. However, neural networks frequently exhibit catastrophic failure on OoS samples, yielding unrealistic but overconfident predictions. We address this challenge by reformulating the OoS generalisation problem as a sequence modelling task in the weight space, wherein the training set is partitioned into concentric shells corresponding to discrete sequential steps. Our WeightCaster framework yields plausible, interpretable, and uncertainty-aware predictions without necessitating explicit inductive biases, all the while maintaining high computational efficiency. Emprical validation on a synthetic cosine dataset and real-world air quality sensor readings demonstrates performance competitive or superior to the state-of-the-art. By enhancing reliability beyond in-distribution scenarios, these results hold significant implications for the wider adoption of artificial intelligence in safety-critical applications.

</details>


### [53] [Unbiased Approximate Vector-Jacobian Products for Efficient Backpropagation](https://arxiv.org/abs/2602.14701)
*Killian Bakong,Laurent Massoulié,Edouard Oyallon,Kevin Scaman*

Main category: cs.LG

TL;DR: 提出一种通过随机化、无偏近似向量-雅可比乘积来降低深度神经网络训练计算和内存成本的方法


<details>
  <summary>Details</summary>
Motivation: 降低深度神经网络训练的计算和内存成本，这是深度学习应用中的主要瓶颈

Method: 在反向传播过程中，用随机化、无偏的近似方法替代精确的向量-雅可比乘积计算。识别具有最小方差的最优无偏估计，并在稀疏性约束下建立最优性

Result: 理论分析了精度目标与每轮成本减少之间的权衡关系。实验验证了该方法在多层感知机、BagNets和视觉Transformer架构上的有效性

Conclusion: 提出的无偏随机化反向传播方法具有降低深度学习成本的潜力，理论和实验都验证了其有效性

Abstract: In this work we introduce methods to reduce the computational and memory costs of training deep neural networks. Our approach consists in replacing exact vector-jacobian products by randomized, unbiased approximations thereof during backpropagation. We provide a theoretical analysis of the trade-off between the number of epochs needed to achieve a target precision and the cost reduction for each epoch. We then identify specific unbiased estimates of vector-jacobian products for which we establish desirable optimality properties of minimal variance under sparsity constraints. Finally we provide in-depth experiments on multi-layer perceptrons, BagNets and Visual Transfomers architectures. These validate our theoretical results, and confirm the potential of our proposed unbiased randomized backpropagation approach for reducing the cost of deep learning.

</details>


### [54] [Scenario-Adaptive MU-MIMO OFDM Semantic Communication With Asymmetric Neural Network](https://arxiv.org/abs/2602.13557)
*Chongyang Li,Tianqian Zhang,Shouyin Liu*

Main category: cs.LG

TL;DR: 提出一种面向6G网络的下行MU-MIMO语义通信框架，通过场景感知语义编码和神经预编码网络解决多用户干扰和频率选择性衰落问题，在低信噪比下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度联合信源信道编码方案主要针对点对点链路设计，在多用户场景下存在性能饱和问题。将语义通信应用于实际的下行MU-MIMO OFDM系统面临严重的多用户干扰和频率选择性衰落挑战。

Method: 提出非对称架构的下行MU-MIMO语义通信框架：发射端采用场景感知语义编码器（根据CSI和SNR动态调整特征提取）和神经预编码网络（在语义域抑制MUI）；接收端采用轻量级解码器，配备新型导频引导注意力机制，利用参考导频符号隐式执行信道均衡和特征校准。

Result: 在3GPP信道模型上的广泛仿真表明，该框架在PSNR和分类准确率方面显著优于DJSCC和传统分离信源信道编码方案，特别是在低信噪比区域，同时在边缘设备上保持低延迟和低计算成本。

Conclusion: 该研究提出的场景自适应MU-MIMO语义通信框架有效解决了多用户干扰和频率选择性衰落问题，为6G网络中实际部署语义通信提供了可行方案，在性能、延迟和计算效率方面均表现出色。

Abstract: Semantic Communication (SemCom) has emerged as a promising paradigm for 6G networks, aiming to extract and transmit task-relevant information rather than minimizing bit errors. However, applying SemCom to realistic downlink Multi-User Multi-Input Multi-Output (MU-MIMO) Orthogonal Frequency Division Multiplexing (OFDM) systems remains challenging due to severe Multi-User Interference (MUI) and frequency-selective fading. Existing Deep Joint Source-Channel Coding (DJSCC) schemes, primarily designed for point-to-point links, suffer from performance saturation in multi-user scenarios. To address these issues, we propose a scenario-adaptive MU-MIMO SemCom framework featuring an asymmetric architecture tailored for downlink transmission. At the transmitter, we introduce a scenario-aware semantic encoder that dynamically adjusts feature extraction based on Channel State Information (CSI) and Signal-to-Noise Ratio (SNR), followed by a neural precoding network designed to mitigate MUI in the semantic domain. At the receiver, a lightweight decoder equipped with a novel pilot-guided attention mechanism is employed to implicitly perform channel equalization and feature calibration using reference pilot symbols. Extensive simulation results over 3GPP channel models demonstrate that the proposed framework significantly outperforms DJSCC and traditional Separate Source-Channel Coding (SSCC) schemes in terms of Peak Signal-to-Noise Ratio (PSNR) and classification accuracy, particularly in low-SNR regimes, while maintaining low latency and computational cost on edge devices.

</details>


### [55] [On the Stability of Nonlinear Dynamics in GD and SGD: Beyond Quadratic Potentials](https://arxiv.org/abs/2602.14789)
*Rotem Mulayoff,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 论文研究梯度下降和随机梯度下降的非线性动力学稳定性，发现线性分析可能误导，非线性项能导致稳定振荡或发散。


<details>
  <summary>Details</summary>
Motivation: 训练过程中迭代的动态稳定性对优化算法找到的最小值有重要影响。先前工作多依赖线性化分析稳定性，但线性化动力学是否能准确捕捉完整非线性行为尚不清楚。最近研究表明梯度下降可能在线性不稳定最小值附近稳定振荡，说明线性分析可能具有误导性。

Method: 1. 推导多元设置中梯度下降在最小值附近稳定振荡的精确准则，该条件依赖于高阶导数，推广了现有结果。2. 将分析扩展到随机梯度下降，研究非线性动力学在期望意义上的发散行为。3. 证明如果所有批次都线性稳定，则SGD的非线性动力学在期望意义上是稳定的。

Result: 1. 建立了梯度下降非线性稳定振荡的精确条件，该条件依赖于高阶导数。2. 发现SGD的非线性动力学可能因单个不稳定批次的振荡而发散，而非线性分析所建议的平均效应。3. 证明了当所有批次都线性稳定时，SGD的非线性动力学在期望意义上是稳定的。

Conclusion: 非线性项在优化算法的稳定性分析中起关键作用，线性分析可能误导对动态行为的理解。对于SGD，稳定性可能由单个不稳定批次决定，而非平均效应。当所有批次都线性稳定时，能保证非线性动力学的稳定性。

Abstract: The dynamical stability of the iterates during training plays a key role in determining the minima obtained by optimization algorithms. For example, stable solutions of gradient descent (GD) correspond to flat minima, which have been associated with favorable features. While prior work often relies on linearization to determine stability, it remains unclear whether linearized dynamics faithfully capture the full nonlinear behavior. Recent work has shown that GD may stably oscillate near a linearly unstable minimum and still converge once the step size decays, indicating that linear analysis can be misleading. In this work, we explicitly study the effect of nonlinear terms. Specifically, we derive an exact criterion for stable oscillations of GD near minima in the multivariate setting. Our condition depends on high-order derivatives, generalizing existing results. Extending the analysis to stochastic gradient descent (SGD), we show that nonlinear dynamics can diverge in expectation even if a single batch is unstable. This implies that stability can be dictated by a single batch that oscillates unstably, rather than an average effect, as linear analysis suggests. Finally, we prove that if all batches are linearly stable, the nonlinear dynamics of SGD are stable in expectation.

</details>


### [56] [Interpretable clustering via optimal multiway-split decision trees](https://arxiv.org/abs/2602.13586)
*Hayato Suzuki,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 提出基于最优多路分裂决策树的解释性聚类方法，通过0-1整数线性规划提高计算效率，结合一维K-means离散化连续变量，在准确性和可解释性上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法通常构建二叉决策树，需要求解混合整数非线性优化问题，计算成本高且易陷入次优解。同时，二叉决策树往往过深，降低了可解释性。

Method: 提出基于最优多路分裂决策树的解释性聚类方法，将问题重新表述为0-1整数线性优化问题，提高求解效率。关键创新是集成一维K-means算法对连续变量进行离散化，实现灵活的数据驱动分支。

Result: 在公开真实数据集上的大量数值实验表明，该方法在聚类准确性和可解释性方面优于基线方法。能够生成具有简洁决策规则的多路分裂决策树，同时在各种评估指标上保持竞争力。

Conclusion: 该方法通过多路分裂决策树和0-1整数线性规划，有效解决了传统聚类方法计算成本高、可解释性差的问题，在保持性能的同时提高了聚类结果的可解释性。

Abstract: Clustering serves as a vital tool for uncovering latent data structures, and achieving both high accuracy and interpretability is essential. To this end, existing methods typically construct binary decision trees by solving mixed-integer nonlinear optimization problems, often leading to significant computational costs and suboptimal solutions. Furthermore, binary decision trees frequently result in excessively deep structures, which makes them difficult to interpret. To mitigate these issues, we propose an interpretable clustering method based on optimal multiway-split decision trees, formulated as a 0-1 integer linear optimization problem. This reformulation renders the optimization problem more tractable compared to existing models. A key feature of our method is the integration of a one-dimensional K-means algorithm for the discretization of continuous variables, allowing for flexible and data-driven branching. Extensive numerical experiments on publicly available real-world datasets demonstrate that our method outperforms baseline methods in terms of clustering accuracy and interpretability. Our method yields multiway-split decision trees with concise decision rules while maintaining competitive performance across various evaluation metrics.

</details>


### [57] [Extending Multi-Source Bayesian Optimization With Causality Principles](https://arxiv.org/abs/2602.14791)
*Luuk Jacobs,Mohammad Ali Javidian*

Main category: cs.LG

TL;DR: 提出多源因果贝叶斯优化（MSCBO）算法，将因果贝叶斯优化与多源贝叶斯优化相结合，利用因果信息提升高维优化问题的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统多源贝叶斯优化（MSBO）假设输入变量独立同分布，无法有效利用临床实验或政策制定等场景中的因果信息和干预能力。因果贝叶斯优化（CBO）在单源领域已证明能提升优化效果，但尚未扩展到多源场景。

Method: 提出多源因果贝叶斯优化（MSCBO）算法，将MSBO和CBO的方法论进行原则性整合。利用因果原理建模变量依赖关系，结合多源信息（仿真、代理模型、实验）提升优化效率，降低高维问题的计算复杂度。

Result: 在合成和真实数据集上测试MSCBO，与基础算法对比，在不同噪声水平下均表现出更好的鲁棒性和适用性。MSCBO通过维度降低和操作成本降低，改善了收敛速度、性能和可扩展性。

Conclusion: 将MSBO与CBO的因果原理相结合，能够有效降低维度、减少操作成本，最终提升优化收敛速度、性能和可扩展性，为具有因果信息的复杂优化问题提供有效解决方案。

Abstract: Multi-Source Bayesian Optimization (MSBO) serves as a variant of the traditional Bayesian Optimization (BO) framework applicable to situations involving optimization of an objective black-box function over multiple information sources such as simulations, surrogate models, or real-world experiments. However, traditional MSBO assumes the input variables of the objective function to be independent and identically distributed, limiting its effectiveness in scenarios where causal information is available and interventions can be performed, such as clinical trials or policy-making. In the single-source domain, Causal Bayesian Optimization (CBO) extends standard BO with the principles of causality, enabling better modeling of variable dependencies. This leads to more accurate optimization, improved decision-making, and more efficient use of low-cost information sources. In this article, we propose a principled integration of the MSBO and CBO methodologies in the multi-source domain, leveraging the strengths of both to enhance optimization efficiency and reduce computational complexity in higher-dimensional problems. We present the theoretical foundations of both Causal and Multi-Source Bayesian Optimization, and demonstrate how their synergy informs our Multi-Source Causal Bayesian Optimization (MSCBO) algorithm. We compare the performance of MSCBO against its foundational counterparts for both synthetic and real-world datasets with varying levels of noise, highlighting the robustness and applicability of MSCBO. Based on our findings, we conclude that integrating MSBO with the causality principles of CBO facilitates dimensionality reduction and lowers operational costs, ultimately improving convergence speed, performance, and scalability.

</details>


### [58] [Benchmark Leakage Trap: Can We Trust LLM-based Recommendation?](https://arxiv.org/abs/2602.13626)
*Mingqiao Zhang,Qiyao Peng,Yumeng Wang,Chunyuan Liu,Hongtao Liu*

Main category: cs.LG

TL;DR: 论文发现LLM推荐系统中存在基准数据泄露问题，即LLM在预训练或微调时接触并记忆了基准数据集，导致性能指标虚高，不能反映真实模型能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在推荐系统中的广泛应用，评估可靠性面临严峻挑战。论文识别并研究了一个先前被忽视的问题：基于LLM的推荐系统中的基准数据泄露现象，这种现象会导致性能指标虚高，误导对模型真实能力的评估。

Method: 通过模拟不同的数据泄露场景，对基础模型进行持续预训练，使用策略性混合的语料库（包括来自域内和域外的用户-物品交互数据），来验证数据泄露现象。

Result: 实验揭示了数据泄露的双重效应：当泄露数据与领域相关时，会导致显著但虚假的性能提升，误导性地夸大模型能力；而当泄露数据与领域无关时，通常会降低推荐准确性，突显了这种污染的复杂性和条件性。

Conclusion: 数据泄露是基于LLM推荐系统中一个关键且先前未被考虑的因素，可能影响模型的真实性能。研究强调了在LLM推荐系统评估中需要更严格的数据隔离和验证方法。

Abstract: The expanding integration of Large Language Models (LLMs) into recommender systems poses critical challenges to evaluation reliability. This paper identifies and investigates a previously overlooked issue: benchmark data leakage in LLM-based recommendation. This phenomenon occurs when LLMs are exposed to and potentially memorize benchmark datasets during pre-training or fine-tuning, leading to artificially inflated performance metrics that fail to reflect true model performance. To validate this phenomenon, we simulate diverse data leakage scenarios by conducting continued pre-training of foundation models on strategically blended corpora, which include user-item interactions from both in-domain and out-of-domain sources. Our experiments reveal a dual-effect of data leakage: when the leaked data is domain-relevant, it induces substantial but spurious performance gains, misleadingly exaggerating the model's capability. In contrast, domain-irrelevant leakage typically degrades recommendation accuracy, highlighting the complex and contingent nature of this contamination. Our findings reveal that data leakage acts as a critical, previously unaccounted-for factor in LLM-based recommendation, which could impact the true model performance. We release our code at https://github.com/yusba1/LLMRec-Data-Leakage.

</details>


### [59] [On the Learning Dynamics of RLVR at the Edge of Competence](https://arxiv.org/abs/2602.14872)
*Yu Huang,Zixin Wen,Yuejie Chi,Yuting Wei,Aarti Singh,Yingbin Liang,Yuxin Chen*

Main category: cs.LG

TL;DR: RLVR（可验证奖励的强化学习）通过难度谱平滑度影响训练动态：平滑谱产生接力效应，不连续谱导致顿悟式相变。


<details>
  <summary>Details</summary>
Motivation: 理解为什么仅基于最终结果的奖励能够帮助克服长视野推理障碍，探究RLVR在组合推理任务中的训练动态机制。

Method: 开发基于Transformer的RL训练动态理论，使用有限群上的傅里叶分析工具，通过合成实验验证预测机制。

Result: RLVR效果受难度谱平滑度调控：平滑谱产生接力效应实现持续改进；不连续谱导致顿悟式相变和平台期。

Conclusion: RLVR通过提升模型在能力边缘的表现发挥作用，适当设计的数据混合可以产生可扩展的增益。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has been a main driver of recent breakthroughs in large reasoning models. Yet it remains a mystery how rewards based solely on final outcomes can help overcome the long-horizon barrier to extended reasoning. To understand this, we develop a theory of the training dynamics of RL for transformers on compositional reasoning tasks. Our theory characterizes how the effectiveness of RLVR is governed by the smoothness of the difficulty spectrum. When data contains abrupt discontinuities in difficulty, learning undergoes grokking-type phase transitions, producing prolonged plateaus before progress recurs. In contrast, a smooth difficulty spectrum leads to a relay effect: persistent gradient signals on easier problems elevate the model's capabilities to the point where harder ones become tractable, resulting in steady and continuous improvement. Our theory explains how RLVR can improve performance at the edge of competence, and suggests that appropriately designed data mixtures can yield scalable gains. As a technical contribution, our analysis develops and adapts tools from Fourier analysis on finite groups to our setting. We validate the predicted mechanisms empirically via synthetic experiments.

</details>


### [60] [Optimization-Free Graph Embedding via Distributional Kernel for Community Detection](https://arxiv.org/abs/2602.13634)
*Shuaibin Song,Kai Ming Ting,Kaifeng Zhang,Tianrun Liang*

Main category: cs.LG

TL;DR: 本文提出一种新的加权分布感知核方法，通过考虑节点分布和节点度分布特征来缓解图嵌入中邻域聚合策略的过平滑问题，无需优化即可提升社区检测性能。


<details>
  <summary>Details</summary>
Motivation: 邻域聚合策略（NAS）是图嵌入中的常用方法，但容易出现过平滑问题——随着迭代次数增加，节点可区分性降低。现有方法忽略了网络中节点分布和节点度分布这两个关键特征，这些被忽视的特征对NAS方法的过平滑有显著贡献。

Method: 提出一种新颖的加权分布感知核，在嵌入节点时考虑其分布特征。该方法有三个特点：1）首次明确结合两种分布特征；2）无需优化；3）有效缓解过平滑的负面影响，使WL方法即使在多次嵌入迭代后仍能保持节点可区分性和表达能力。

Result: 实验表明，通过谱聚类进行社区检测时，该方法优于现有的图嵌入方法（包括深度学习方法），在标准基准测试中取得了优越的性能。

Conclusion: 通过考虑节点分布和节点度分布特征，提出的加权分布感知核方法能够有效缓解邻域聚合策略的过平滑问题，无需优化即可提升图嵌入的表达能力，在社区检测任务中表现优异。

Abstract: Neighborhood Aggregation Strategy (NAS) is a widely used approach in graph embedding, underpinning both Graph Neural Networks (GNNs) and Weisfeiler-Lehman (WL) methods. However, NAS-based methods are identified to be prone to over-smoothing-the loss of node distinguishability with increased iterations-thereby limiting their effectiveness. This paper identifies two characteristics in a network, i.e., the distributions of nodes and node degrees that are critical for expressive representation but have been overlooked in existing methods. We show that these overlooked characteristics contribute significantly to over-smoothing of NAS-methods. To address this, we propose a novel weighted distribution-aware kernel that embeds nodes while taking their distributional characteristics into consideration. Our method has three distinguishing features: (1) it is the first method to explicitly incorporate both distributional characteristics; (2) it requires no optimization; and (3) it effectively mitigates the adverse effects of over-smoothing, allowing WL to preserve node distinguishability and expressiveness even after many iterations of embedding. Experiments demonstrate that our method achieves superior community detection performance via spectral clustering, outperforming existing graph embedding methods, including deep learning methods, on standard benchmarks.

</details>


### [61] [Locally Adaptive Multi-Objective Learning](https://arxiv.org/abs/2602.14952)
*Jivat Neet Kaur,Isaac Gibbs,Michael I. Jordan*

Main category: cs.LG

TL;DR: 提出一种自适应多目标在线学习方法，通过将传统多目标学习中的部分组件替换为自适应在线算法，实现在数据分布随时间任意变化时的局部自适应能力


<details>
  <summary>Details</summary>
Motivation: 现有多目标学习方法（如校准、遗憾、多精度等）在数据分布随时间任意变化的在线设置中，通常在整个时间范围内以最坏情况最小化目标，缺乏对分布变化的适应能力。虽然已有研究尝试通过引入针对连续子区间的局部保证来缓解此问题，但实证评估较少

Method: 将传统多目标学习方法中的部分组件替换为自适应在线算法，从而实现对数据分布变化的局部自适应。该方法在能源预测和算法公平数据集上进行评估

Result: 在能源预测和算法公平数据集上的实验表明，所提方法优于现有方法，能够在子组上实现无偏预测，同时在分布变化下保持鲁棒性

Conclusion: 通过将自适应在线算法集成到多目标学习框架中，可以有效地实现局部自适应，提高在动态变化环境中的预测性能，特别是在需要同时满足多个目标（如公平性和准确性）的应用场景中

Abstract: We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.

</details>


### [62] [Joint Time Series Chain: Detecting Unusual Evolving Trend across Time Series](https://arxiv.org/abs/2602.13649)
*Li Zhang,Nital Patel,Xiuqi Li,Jessica Lin*

Main category: cs.LG

TL;DR: 提出联合时间序列链(JointTSC)新定义，用于在中断时间序列或两个相关时间序列中发现意外演化趋势，解决现有方法只能处理单时间序列的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列链(TSC)定义仅考虑在单个时间序列中寻找链，容易错过中断时间序列或跨两个相关时间序列中的意外演化模式。需要新方法来处理时间序列中的间隔或中断问题。

Method: 引入联合时间序列链新定义，专门用于在中断时间序列或两个相关时间序列中发现意外演化趋势。提出有效的排序标准来识别最佳链，并缓解时间序列中缺口或中断引起的鲁棒性问题。

Result: 通过大量实证评估证明，所提方法在定位异常演化模式方面优于现有TSC工作。在英特尔的实际制造应用中展示了方法的实用性。

Conclusion: 联合时间序列链定义有效解决了现有TSC方法在处理中断时间序列和跨序列演化模式时的局限性，具有实际应用价值，代码已开源。

Abstract: Time series chain (TSC) is a recently introduced concept that captures the evolving patterns in large scale time series. Informally, a time series chain is a temporally ordered set of subsequences, in which consecutive subsequences in the chain are similar to one another, but the last and the first subsequences maybe be dissimilar. Time series chain has the great potential to reveal latent unusual evolving trend in the time series, or identify precursor of important events in a complex system. Unfortunately, existing definitions of time series chains only consider finding chains in a single time series. As a result, they are likely to miss unexpected evolving patterns in interrupted time series, or across two related time series. To address this limitation, in this work, we introduce a new definition called \textit{Joint Time Series Chain}, which is specially designed for the task of finding unexpected evolving trend across interrupted time series or two related time series. Our definition focuses on mitigating the robustness issues caused by the gap or interruption in the time series. We further propose an effective ranking criterion to identify the best chain. We demonstrate that our proposed approach outperforms existing TSC work in locating unusual evolving patterns through extensive empirical evaluations. We further demonstrate the utility of our work with a real-life manufacturing application from Intel. Our source code is publicly available at the supporting page https://github.com/lizhang-ts/JointTSC .

</details>


### [63] [Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees](https://arxiv.org/abs/2602.15008)
*Daniil Dmitriev,Zhihan Huang,Yuting Wei*

Main category: cs.LG

TL;DR: 该论文为离散扩散模型建立了理论收敛保证，针对均匀和掩码噪声过程，提出了τ-leaping采样器，在KL散度上达到ε精度，消除了对词汇大小S的线性依赖，并证明了维度依赖是不可避免的。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在经验上取得了显著成功，但其理论基础尚不完整。本文旨在研究基于连续时间马尔可夫链（CTMC）的离散扩散模型的采样效率，重点关注τ-leaping采样器，为离散扩散建立严格的理论保证。

Method: 采用连续时间马尔可夫链（CTMC）框架下的τ-leaping采样器。对于均匀离散扩散，分析标准τ-leaping算法；对于掩码离散扩散，引入改进的τ-leaping采样器。分析基于分数熵损失控制，无需有界性或平滑性假设。

Result: 对于均匀离散扩散，τ-leaping算法实现了Õ(d/ε)的迭代复杂度，消除了对词汇大小S的线性依赖，比现有界限改进了d倍，并证明了维度线性依赖是不可避免的。对于掩码离散扩散，收敛速率由有效总相关（bounded by d log S）控制，对于结构化数据（如隐马尔可夫模型、图像数据、随机图）可实现次线性甚至常数收敛速率。

Conclusion: 本文为离散扩散模型建立了严格的收敛理论，证明了τ-leaping采样器的高效性，特别是对于掩码扩散，算法能够自适应地利用数据的低维结构，无需先验知识或算法修改，为实际应用提供了理论保证。

Abstract: Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.

</details>


### [64] [Cumulative Utility Parity for Fair Federated Learning under Intermittent Client Participation](https://arxiv.org/abs/2602.13651)
*Stefan Behfar,Richard Mortier*

Main category: cs.LG

TL;DR: 提出累积效用公平性概念，关注联邦学习中客户端长期参与机会的公平性，而非单轮性能公平性


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习系统中客户端参与是间歇性、异质性的，且与数据特征或资源约束相关。现有公平性方法主要关注参与条件下的损失或准确性平等，但参与本身不均衡会导致间歇可用客户端系统性代表不足

Method: 提出累积效用公平性原则，评估客户端是否获得可比较的长期利益（按参与机会而非训练轮次）。引入可用性归一化累积效用，将不可避免的物理约束与可避免的算法偏倚分离

Result: 在时间偏斜、非独立同分布的联邦基准测试中，该方法显著改善了长期代表性公平性，同时保持接近完美的性能

Conclusion: 需要重新思考联邦学习中的公平性定义，考虑参与动态性，累积效用公平性能够解决间歇参与客户端系统性代表不足问题

Abstract: In real-world federated learning (FL) systems, client participation is intermittent, heterogeneous, and often correlated with data characteristics or resource constraints. Existing fairness approaches in FL primarily focus on equalizing loss or accuracy conditional on participation, implicitly assuming that clients have comparable opportunities to contribute over time. However, when participation itself is uneven, these objectives can lead to systematic under-representation of intermittently available clients, even if per-round performance appears fair. We propose cumulative utility parity, a fairness principle that evaluates whether clients receive comparable long-term benefit per participation opportunity, rather than per training round. To operationalize this notion, we introduce availability-normalized cumulative utility, which disentangles unavoidable physical constraints from avoidable algorithmic bias arising from scheduling and aggregation. Experiments on temporally skewed, non-IID federated benchmarks demonstrate that our approach substantially improves long-term representation parity, while maintaining near-perfect performance.

</details>


### [65] [Zero-Order Optimization for LLM Fine-Tuning via Learnable Direction Sampling](https://arxiv.org/abs/2602.13659)
*Valery Parfenov,Grigoriy Evseev,Andrey Veprikov,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 提出一种基于策略学习的零阶优化框架，通过可学习的扰动方向采样分布降低方差，改善大语言模型微调中的内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型微调需要大量内存（反向传播和大优化器状态），限制了在资源受限环境中的部署。传统零阶方法虽然节省内存，但存在高方差和对参数维度d的依赖问题。

Method: 提出策略驱动的零阶框架，将扰动方向的采样分布视为可学习策略，通过更新该策略来降低方向估计的方差。开发了实现该思想的实用算法，并提供了理论分析。

Result: 理论分析表明学习到的采样分布改善了梯度信息质量，并放松了收敛边界中对d的显式依赖。在具有挑战性的LLM微调基准测试中，相比标准零阶基线表现出显著改进的性能。

Conclusion: 自适应方向采样是使零阶微调在大规模应用中可行的有前景的途径，为资源受限环境下的LLM部署提供了新的解决方案。

Abstract: Fine-tuning large pretrained language models (LLMs) is a cornerstone of modern NLP, yet its growing memory demands (driven by backpropagation and large optimizer States) limit deployment in resource-constrained settings. Zero-order (ZO) methods bypass backpropagation by estimating directional derivatives from forward evaluations, offering substantial memory savings. However, classical ZO estimators suffer from high variance and an adverse dependence on the parameter dimensionality $d$, which has constrained their use to low-dimensional problems. In this work, we propose a policy-driven ZO framework that treats the sampling distribution over perturbation directions as a learnable policy and updates it to reduce the variance of directional estimates. We develop a practical algorithm implementing this idea and provide a theoretical analysis, showing that learned sampling distributions improve the quality of gradient information and relax the explicit dependence on $d$ in convergence bounds. Empirically, we validate the approach on challenging LLM fine-tuning benchmarks, demonstrating substantially improved performance compared to standard ZO baselines. Our results suggest that adaptive direction sampling is a promising route to make ZO fine-tuning viable at scale. The source code is available at https://github.com/brain-lab-research/zo_ldsd

</details>


### [66] [Optimized Certainty Equivalent Risk-Controlling Prediction Sets](https://arxiv.org/abs/2602.13660)
*Jiayi Huang,Amirmohammad Farzaneh,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出OCE-RCPS框架，为医学图像分割等安全关键应用提供基于优化确定性等价风险度量的高概率保证，超越传统风险控制预测集


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割等安全关键应用中，传统风险控制预测集(RCPS)只能提供期望风险的概率保证，无法捕捉尾部行为和最坏情况，而这些在高风险场景中至关重要

Method: 提出优化确定性等价RCPS(OCE-RCPS)框架，利用置信上界识别满足用户指定风险容忍水平的预测集参数，支持条件风险价值(CVaR)和熵风险等OCE风险度量

Result: 理论证明OCE-RCPS对误覆盖率和假阴性率等损失函数满足所需概率约束；图像分割实验显示OCE-RCPS在各种风险度量和可靠性配置下始终达到目标满足率，而OCE-CRC无法提供概率保证

Conclusion: OCE-RCPS为安全关键应用提供了更强大的风险控制框架，能够处理尾部风险和最坏情况，填补了传统RCPS在高风险场景中的不足

Abstract: In safety-critical applications such as medical image segmentation, prediction systems must provide reliability guarantees that extend beyond conventional expected loss control. While risk-controlling prediction sets (RCPS) offer probabilistic guarantees on the expected risk, they fail to capture tail behavior and worst-case scenarios that are crucial in high-stakes settings. This paper introduces optimized certainty equivalent RCPS (OCE-RCPS), a novel framework that provides high-probability guarantees on general optimized certainty equivalent (OCE) risk measures, including conditional value-at-risk (CVaR) and entropic risk. OCE-RCPS leverages upper confidence bounds to identify prediction set parameters that satisfy user-specified risk tolerance levels with provable reliability. We establish theoretical guarantees showing that OCE-RCPS satisfies the desired probabilistic constraint for loss functions such as miscoverage and false negative rate. Experiments on image segmentation demonstrate that OCE-RCPS consistently meets target satisfaction rates across various risk measures and reliability configurations, while OCE-CRC fails to provide probabilistic guarantees.

</details>


### [67] [ALMo: Interactive Aim-Limit-Defined, Multi-Objective System for Personalized High-Dose-Rate Brachytherapy Treatment Planning and Visualization for Cervical Cancer](https://arxiv.org/abs/2602.13666)
*Edward Chen,Natalie Dullerud,Pang Wei Koh,Thomas Niedermayr,Elizabeth Kidd,Sanmi Koyejo,Carlos Guestrin*

Main category: cs.LG

TL;DR: ALMo系统通过直观的"目标-限制"阈值交互界面，帮助临床医生在宫颈癌高剂量率近距离治疗中优化多目标决策，显著提高计划质量和效率。


<details>
  <summary>Details</summary>
Motivation: 临床决策中需要同时追踪多个竞争性指标（理想目标和严格限制），处理这些高维权衡关系对医生认知负担大且易产生变异性。特别是在宫颈癌HDR近距离治疗中，需要严格管理辐射热点，同时平衡肿瘤覆盖和器官保护。

Method: 提出ALMo（目标-限制定义的多目标系统），这是一个交互式决策支持系统。采用新颖的优化框架，通过自动参数设置减少手动输入，允许灵活控制毒性风险。关键创新是让医生通过直接操作直观的目标和限制值来导航帕累托前沿。

Result: 在25个临床病例的回顾性评估中，ALMo生成的计划质量始终达到或超过手动计划，65%的病例显示出剂量学改进。系统显著提高效率，平均计划时间减少到约17分钟，而传统方法需要30-60分钟。

Conclusion: ALMo在近距离治疗中得到验证，展示了一个通用的框架，可用于简化多标准临床决策中的交互过程，具有推广到其他临床决策场景的潜力。

Abstract: In complex clinical decision-making, clinicians must often track a variety of competing metrics defined by aim (ideal) and limit (strict) thresholds. Sifting through these high-dimensional tradeoffs to infer the optimal patient-specific strategy is cognitively demanding and historically prone to variability. In this paper, we address this challenge within the context of High-Dose-Rate (HDR) brachytherapy for cervical cancer, where planning requires strictly managing radiation hot spots while balancing tumor coverage against organ sparing. We present ALMo (Aim-Limit-defined Multi-Objective system), an interactive decision support system designed to infer and operationalize clinician intent. ALMo employs a novel optimization framework that minimizes manual input through automated parameter setup and enables flexible control over toxicity risks. Crucially, the system allows clinicians to navigate the Pareto surface of dosimetric tradeoffs by directly manipulating intuitive aim and limit values. In a retrospective evaluation of 25 clinical cases, ALMo generated treatment plans that consistently met or exceeded manual planning quality, with 65% of cases demonstrating dosimetric improvements. Furthermore, the system significantly enhanced efficiency, reducing average planning time to approximately 17 minutes, compared to the conventional 30-60 minutes. While validated in brachytherapy, ALMo demonstrates a generalized framework for streamlining interaction in multi-criteria clinical decision-making.

</details>


### [68] [Advancing Analytic Class-Incremental Learning through Vision-Language Calibration](https://arxiv.org/abs/2602.13670)
*Binyu Zhao,Wei Zhang,Xingrui Yu,Zhaonian Zou,Ivor Tsang*

Main category: cs.LG

TL;DR: VILA提出双分支视觉语言校准框架，通过特征级几何校准和决策级跨模态先验，解决预训练模型在类增量学习中的表示刚性问题，在保持解析学习效率的同时提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在类增量学习中面临快速适应与长期稳定性的关键权衡。解析学习虽然能实现快速递归闭式更新，但常因累积误差和特征不兼容而失效。研究发现表示刚性是主要瓶颈，需要克服解析学习的固有脆弱性。

Method: 提出VILA双分支框架：1）特征级几何校准：将可塑性任务适应特征与冻结的通用语义锚点融合；2）决策级跨模态先验：利用跨模态先验纠正预测偏差。该框架保持解析学习的极端效率同时克服其脆弱性。

Result: 在8个基准测试上的广泛实验表明，VILA始终表现出优越性能，特别是在细粒度和长序列场景中。该框架实现了高保真预测与解析学习简单性的和谐统一。

Conclusion: VILA通过双分支视觉语言校准策略，成功解决了预训练模型在类增量学习中的表示刚性问题，在保持解析学习效率的同时显著提升了学习稳定性和性能表现。

Abstract: Class-incremental learning (CIL) with pre-trained models (PTMs) faces a critical trade-off between efficient adaptation and long-term stability. While analytic learning enables rapid, recursive closed-form updates, its efficacy is often compromised by accumulated errors and feature incompatibility. In this paper, we first conduct a systematic study to dissect the failure modes of PTM-based analytic CIL, identifying representation rigidity as the primary bottleneck. Motivated by these insights, we propose \textbf{VILA}, a novel dual-branch framework that advances analytic CIL via a two-level vision-language calibration strategy. Specifically, we coherently fuse plastic, task-adapted features with a frozen, universal semantic anchor at the feature level through geometric calibration, and leverage cross-modal priors at the decision level to rectify prediction bias. This confluence maintains analytic-learning's extreme efficiency while overcoming its inherent brittleness. Extensive experiments across eight benchmarks demonstrate that VILA consistently yields superior performance, particularly in fine-grained and long-sequence scenarios. Our framework harmonizes high-fidelity prediction with the simplicity of analytic learning. Our code is available at https://github.com/byzhaoAI/VILA

</details>


### [69] [On the Sparsifiability of Correlation Clustering: Approximation Guarantees under Edge Sampling](https://arxiv.org/abs/2602.13684)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 本文研究了相关性聚类的稀疏化-近似权衡，发现在伪度量结构下可实现高效近似，而在一般加权实例中需要大量边信息才能保持近似保证。


<details>
  <summary>Details</summary>
Motivation: 相关性聚类（CC）的传统LP方法需要Θ(n³)三角不等式约束，在大规模场景下计算成本过高。本文旨在探索在保持LP近似保证的前提下，需要多少边信息（稀疏化程度）才能实现高效计算。

Method: 1) 证明聚类分歧类的VC维度为n-1，得到最优大小的ε-coresets；2) 证明最多有C(n,2)个三角不等式在LP顶点处活跃，实现精确切割平面求解器；3) 提出LP-PIVOT的稀疏化变体，通过三角不等式补全缺失的LP边际值；4) 使用Yao极小极大原理证明一般加权实例的负面结果。

Result: 1) 获得大小为Õ(n/ε²)的加性ε-coresets；2) 在伪度量实例中，观察到Õ(n^{3/2})条边时，稀疏化LP-PIVOT可获得10/3近似比（加上由可计算统计量Γ_w控制的加性项）；3) 证明该阈值是尖锐的；4) 在一般加权实例中，观察到o(n)条随机边时，任何算法都会有无界近似比。

Conclusion: 相关性聚类的稀疏化-近似权衡存在结构性二分：伪度量实例可实现高效稀疏化近似，而一般加权实例需要大量边信息。伪度量条件不仅影响计算可处理性，还决定了CC对不完全信息的鲁棒性。

Abstract: Correlation Clustering (CC) is a fundamental unsupervised learning primitive whose strongest LP-based approximation guarantees require $Θ(n^3)$ triangle inequality constraints and are prohibitive at scale. We initiate the study of \emph{sparsification--approximation trade-offs} for CC, asking how much edge information is needed to retain LP-based guarantees. We establish a structural dichotomy between pseudometric and general weighted instances. On the positive side, we prove that the VC dimension of the clustering disagreement class is exactly $n{-}1$, yielding additive $\varepsilon$-coresets of optimal size $\tilde{O}(n/\varepsilon^2)$; that at most $\binom{n}{2}$ triangle inequalities are active at any LP vertex, enabling an exact cutting-plane solver; and that a sparsified variant of LP-PIVOT, which imputes missing LP marginals via triangle inequalities, achieves a robust $\frac{10}{3}$-approximation (up to an additive term controlled by an empirically computable imputation-quality statistic $\overlineΓ_w$) once $\tildeΘ(n^{3/2})$ edges are observed, a threshold we prove is sharp. On the negative side, we show via Yao's minimax principle that without pseudometric structure, any algorithm observing $o(n)$ uniformly random edges incurs an unbounded approximation ratio, demonstrating that the pseudometric condition governs not only tractability but also the robustness of CC to incomplete information.

</details>


### [70] [Physics Aware Neural Networks: Denoising for Magnetic Navigation](https://arxiv.org/abs/2602.13690)
*Aritra Das,Yashas Shende,Muskaan Chugh,Reva Laxmi Chauhan,Arghya Pathak,Debayan Gupta*

Main category: cs.LG

TL;DR: 提出基于物理约束的框架，利用无散度场和E(3)-等变性处理飞机磁噪声，通过Contiformer架构和合成数据生成提升磁异常导航性能


<details>
  <summary>Details</summary>
Motivation: 磁异常导航在GPS不可用时是重要替代方案，但飞机自身会引入磁噪声。传统的Tolles-Lawson模型无法有效处理导航所需的随机噪声数据

Method: 提出基于两个物理约束的框架：1) 无散度矢量场约束，通过神经网络输出矢量势A，磁场定义为旋度；2) E(3)-等变性约束，使用球谐函数表示的几何张量。采用Contiformer架构处理连续时间动态和长期记忆，并使用条件GAN生成合成数据

Result: 物理约束显著提高了预测精度和物理合理性，Contiformer架构在磁时间序列建模中优于现有方法，合成数据生成缓解了数据稀缺问题

Conclusion: 嵌入物理约束的深度学习框架能有效处理飞机磁噪声，提升磁异常导航性能，为GPS受限环境提供了可靠解决方案

Abstract: Magnetic-anomaly navigation, leveraging small-scale variations in the Earth's magnetic field, is a promising alternative when GPS is unavailable or compromised. Airborne systems face a key challenge in extracting geomagnetic field data: the aircraft itself induces magnetic noise. Although the classical Tolles-Lawson model addresses this, it inadequately handles stochastically corrupted magnetic data required for navigation. To address stochastic noise, we propose a framework based on two physics-based constraints: divergence-free vector field and E(3)-equivariance. These ensure the learned magnetic field obeys Maxwell's equations and that outputs transform correctly with sensor position/orientation. The divergence-free constraint is implemented by training a neural network to output a vector potential $A$, with the magnetic field defined as its curl. For E(3)-equivariance, we use tensor products of geometric tensors representable via spherical harmonics with known rotational transformations. Enforcing physical consistency and restricting the admissible function space acts as an implicit regularizer that improves spatio-temporal performance. We present ablation studies evaluating each constraint alone and jointly across CNNs, MLPs, Liquid Time Constant models, and Contiformers. Continuous-time dynamics and long-term memory are critical for modelling magnetic time series; the Contiformer architecture, which provides both, outperforms state-of-the-art methods. To mitigate data scarcity, we generate synthetic datasets using the World Magnetic Model (WMM) with time-series conditional GANs, producing realistic, temporally consistent magnetic sequences across varied trajectories and environments. Experiments show that embedding these constraints significantly improves predictive accuracy and physical plausibility, outperforming classical and unconstrained deep learning approaches.

</details>


### [71] [sleep2vec: Unified Cross-Modal Alignment for Heterogeneous Nocturnal Biosignals](https://arxiv.org/abs/2602.13857)
*Weixuan Yuan,Zengrui Jin,Yichen Wang,Donglin Xie,Ziyi Ye,Chao Zhang,Xuesong Chen*

Main category: cs.LG

TL;DR: sleep2vec是一个用于处理多样化、不完整夜间生物信号的基础模型，通过跨模态对齐学习共享表示，在睡眠分期和临床评估任务上表现优异，并对传感器丢失具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统睡眠监测和临床诊断依赖于多种设备采集的夜间生物信号（如EEG、EOG、ECG、SpO2），但设备异质性和传感器频繁丢失给统一建模带来了挑战。

Method: 提出sleep2vec基础模型，通过跨模态对齐学习共享表示。使用包含人口统计、年龄、站点和历史信息的InfoNCE目标进行对比预训练，在42,249个夜间记录上涵盖九种模态，动态加权负样本以减轻队列特异性捷径。

Result: 在睡眠分期和临床结果评估任务上，sleep2vec始终优于强基线模型，对任何可用模态子集和传感器丢失保持鲁棒性。首次描述了夜间生物信号在模态多样性和模型容量方面的缩放规律。

Conclusion: 统一的跨模态对齐结合原则性缩放，能够实现真实世界夜间生物信号的标签高效、通用建模。

Abstract: Tasks ranging from sleep staging to clinical diagnosis traditionally rely on standard polysomnography (PSG) devices, bedside monitors and wearable devices, which capture diverse nocturnal biosignals (e.g., EEG, EOG, ECG, SpO$_2$). However, heterogeneity across devices and frequent sensor dropout pose significant challenges for unified modelling of these multimodal signals. We present \texttt{sleep2vec}, a foundation model for diverse and incomplete nocturnal biosignals that learns a shared representation via cross-modal alignment. \texttt{sleep2vec} is contrastively pre-trained on 42,249 overnight recordings spanning nine modalities using a \textit{Demography, Age, Site \& History-aware InfoNCE} objective that incorporates physiological and acquisition metadata (\textit{e.g.}, age, gender, recording site) to dynamically weight negatives and mitigate cohort-specific shortcuts. On downstream sleep staging and clinical outcome assessment, \texttt{sleep2vec} consistently outperforms strong baselines and remains robust to any subset of available modalities and sensor dropout. We further characterize, to our knowledge for the first time, scaling laws for nocturnal biosignals with respect to modality diversity and model capacity. Together, these results show that unified cross-modal alignment, coupled with principled scaling, enables label-efficient, general-purpose modelling of real-world nocturnal biosignals.

</details>


### [72] [Attention Head Entropy of LLMs Predicts Answer Correctness](https://arxiv.org/abs/2602.13699)
*Sophie Ostmeier,Brian Axelrod,Maya Varma,Asad Aali,Yabin Zhang,Magdalini Paschali,Sanmi Koyejo,Curtis Langlotz,Akshay Chaudhari*

Main category: cs.LG

TL;DR: 提出Head Entropy方法，通过注意力熵模式预测LLM答案正确性，在分布内和分布外任务上均优于基线方法


<details>
  <summary>Details</summary>
Motivation: LLM经常生成看似合理但错误的答案，在医疗等安全关键领域存在风险。现有白盒方法主要检测上下文幻觉，但两个问题未解决：能否扩展到预测答案正确性？能否泛化到分布外领域？

Method: 提出Head Entropy方法，通过测量注意力质量的分散程度（使用每头2-Renyi熵），并基于稀疏逻辑回归预测答案正确性

Result: Head Entropy在分布内匹配或超过基线，在分布外泛化能力显著更好，平均比最接近的基线提高+8.5% AUROC。仅使用问题/上下文注意力模式（答案生成前）也能预测，平均比基线提高+17.7% AUROC

Conclusion: 注意力熵模式是预测LLM答案正确性的有效信号，Head Entropy方法在多个LLM和QA数据集上表现出色，具有很好的泛化能力

Abstract: Large language models (LLMs) often generate plausible yet incorrect answers, posing risks in safety-critical settings such as medicine. Human evaluation is expensive, and LLM-as-judge approaches risk introducing hidden errors. Recent white-box methods detect contextual hallucinations using model internals, focusing on the localization of the attention mass, but two questions remain open: do these approaches extend to predicting answer correctness, and do they generalize out-of-domains? We introduce Head Entropy, a method that predicts answer correctness from attention entropy patterns, specifically measuring the spread of the attention mass. Using sparse logistic regression on per-head 2-Renyi entropies, Head Entropy matches or exceeds baselines in-distribution and generalizes substantially better on out-of-domains, it outperforms the closest baseline on average by +8.5% AUROC. We further show that attention patterns over the question/context alone, before answer generation, already carry predictive signal using Head Entropy with on average +17.7% AUROC over the closest baseline. We evaluate across 5 instruction-tuned LLMs and 3 QA datasets spanning general knowledge, multi-hop reasoning, and medicine.

</details>


### [73] [Decentralized Federated Learning With Energy Harvesting Devices](https://arxiv.org/abs/2602.14051)
*Kai Zhang,Xuanyu Cao,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 论文提出了一种用于能量收集无线去中心化联邦学习系统的完全去中心化策略迭代算法，通过联合设备调度和功率控制来加速收敛，同时降低通信开销和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）中的本地训练和设备间模型交换能耗巨大，会快速耗尽边缘设备的有限电池，降低设备运行寿命和学习性能。能量收集技术可以解决这一问题，但需要设计有效的资源管理策略来加速收敛。

Method: 首先推导了能量收集无线DFL的收敛界，然后将联合设备调度和功率控制问题建模为多智能体马尔可夫决策过程（MDP）。针对传统MDP算法需要集中协调且复杂度高的问题，提出了一种完全去中心化的策略迭代算法，仅利用两跳邻居设备的本地状态信息。

Result: 理论分析表明所提出的去中心化算法具有渐近最优性。在真实数据集上的综合数值实验验证了理论结果，并证实了算法的有效性。

Conclusion: 该研究为能量收集无线去中心化联邦学习系统提供了一种高效、可扩展的资源管理解决方案，通过完全去中心化的算法设计，在保证性能的同时显著降低了通信和计算开销。

Abstract: Decentralized federated learning (DFL) enables edge devices to collaboratively train models through local training and fully decentralized device-to-device (D2D) model exchanges. However, these energy-intensive operations often rapidly deplete limited device batteries, reducing their operational lifetime and degrading the learning performance. To address this limitation, we apply energy harvesting technique to DFL systems, allowing edge devices to extract ambient energy and operate sustainably. We first derive the convergence bound for wireless DFL with energy harvesting, showing that the convergence is influenced by partial device participation and transmission packet drops, both of which further depend on the available energy supply. To accelerate convergence, we formulate a joint device scheduling and power control problem and model it as a multi-agent Markov decision process (MDP). Traditional MDP algorithms (e.g., value or policy iteration) require a centralized coordinator with access to all device states and exhibit exponential complexity in the number of devices, making them impractical for large-scale decentralized networks. To overcome these challenges, we propose a fully decentralized policy iteration algorithm that leverages only local state information from two-hop neighboring devices, thereby substantially reducing both communication overhead and computational complexity. We further provide a theoretical analysis showing that the proposed decentralized algorithm achieves asymptotic optimality. Finally, comprehensive numerical experiments on real-world datasets are conducted to validate the theoretical results and corroborate the effectiveness of the proposed algorithm.

</details>


### [74] [Optimal Regret for Policy Optimization in Contextual Bandits](https://arxiv.org/abs/2602.13700)
*Orin Levy,Yishay Mansour*

Main category: cs.LG

TL;DR: 提出了首个高概率最优遗憾界的策略优化算法，用于解决具有通用离线函数逼近的随机上下文多臂老虎机问题，实现了$\widetilde{O}(\sqrt{K|\mathcal{A}|\log|\mathcal{F}|})$的最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在上下文多臂老虎机问题中，广泛使用的策略优化方法缺乏严格理论保证。现有理论与实际应用之间存在差距，需要为策略优化方法提供最优遗憾界的理论证明。

Method: 设计了一种高效的策略优化算法，利用通用离线函数逼近方法，通过函数类$\mathcal{F}$来近似损失函数，实现了高概率的最优遗憾界。

Result: 算法达到了$\widetilde{O}(\sqrt{K|\mathcal{A}|\log|\mathcal{F}|})$的最优遗憾界，其中$K$是轮数，$\mathcal{A}$是臂的集合，$\mathcal{F}$是函数类。这是首个为策略优化方法提供的严格最优遗憾界。

Conclusion: 该研究填补了理论与实践的鸿沟，证明了广泛使用的上下文老虎机策略优化方法可以获得严格证明的最优遗憾界，并通过实验验证了理论结果。

Abstract: We present the first high-probability optimal regret bound for a policy optimization technique applied to the problem of stochastic contextual multi-armed bandit (CMAB) with general offline function approximation. Our algorithm is both efficient and achieves an optimal regret bound of $\widetilde{O}(\sqrt{ K|\mathcal{A}|\log|\mathcal{F}|})$, where $K$ is the number of rounds, $\mathcal{A}$ is the set of arms, and $\mathcal{F}$ is the function class used to approximate the losses. Our results bridge the gap between theory and practice, demonstrating that the widely used policy optimization methods for the contextual bandit problem can achieve a rigorously-proved optimal regret bound. We support our theoretical results with an empirical evaluation of our algorithm.

</details>


### [75] [Parameter-Minimal Neural DE Solvers via Horner Polynomials](https://arxiv.org/abs/2602.14737)
*T. Matulić,D. Seršić*

Main category: cs.LG

TL;DR: 提出一种参数极少的神经网络架构，通过将假设类限制为Horner分解多项式来求解微分方程，得到隐式可微的试解，仅需少量可学习系数。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络求解微分方程需要大量参数，本文旨在开发参数极少的架构，实现资源高效的科学建模，在保持精度的同时减少参数数量。

Method: 1. 将假设类限制为Horner分解多项式，构建隐式可微的试解；2. 通过固定低阶多项式自由度精确满足初始条件；3. 引入分段（类样条）扩展，在子区间上训练多个小型Horner模型，并在边界处强制连续性和一阶导数连续性。

Result: 在ODE基准测试和热方程示例中，仅需数十个参数的Horner网络能准确匹配解及其导数，在相同训练设置下优于小型MLP和正弦表示基线，展示了实用的精度-参数权衡。

Conclusion: Horner网络为资源受限的科学建模提供了一种实用的精度-参数权衡方案，通过极少的参数实现了准确的微分方程求解，特别适合资源受限的应用场景。

Abstract: We propose a parameter-minimal neural architecture for solving differential equations by restricting the hypothesis class to Horner-factorized polynomials, yielding an implicit, differentiable trial solution with only a small set of learnable coefficients. Initial conditions are enforced exactly by construction by fixing the low-order polynomial degrees of freedom, so training focuses solely on matching the differential-equation residual at collocation points. To reduce approximation error without abandoning the low-parameter regime, we introduce a piecewise ("spline-like") extension that trains multiple small Horner models on subintervals while enforcing continuity (and first-derivative continuity) at segment boundaries. On illustrative ODE benchmarks and a heat-equation example, Horner networks with tens (or fewer) parameters accurately match the solution and its derivatives and outperform small MLP and sinusoidal-representation baselines under the same training settings, demonstrating a practical accuracy-parameter trade-off for resource-efficient scientific modeling.

</details>


### [76] [Near-Optimal Regret for Policy Optimization in Contextual MDPs with General Offline Function Approximation](https://arxiv.org/abs/2602.13706)
*Orin Levy,Aviv Rosenberg,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: OPO-CMDP是首个用于随机上下文马尔可夫决策过程(CMDPs)的离线函数逼近策略优化算法，实现了最优的|S|和|A|依赖性的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 当前CMDPs的离线函数逼近算法在状态空间和动作空间的依赖性上尚未达到最优，需要开发具有更好理论保证的算法。

Method: 采用乐观策略优化(OPO)方法，结合有限函数类来逼近损失函数和动态转移模型，通过高概率遗憾界分析。

Result: 获得了$\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)})$的高概率遗憾界，在|S|和|A|依赖性上达到最优，超越了当前最优结果(Qian, Hu, and Simchi-Levi, 2024)。

Conclusion: 乐观策略优化为求解CMDPs提供了一条自然、计算效率高且理论接近最优的路径。

Abstract: We introduce \texttt{OPO-CMDP}, the first policy optimization algorithm for stochastic Contextual Markov Decision Process (CMDPs) under general offline function approximation. Our approach achieves a high probability regret bound of $\widetilde{O}(H^4\sqrt{T|S||A|\log(|\mathcal{F}||\mathcal{P}|)}),$ where $S$ and $A$ denote the state and action spaces, $H$ the horizon length, $T$ the number of episodes, and $\mathcal{F}, \mathcal{P}$ the finite function classes used to approximate the losses and dynamics, respectively. This is the first regret bound with optimal dependence on $|S|$ and $|A|$, directly improving the current state-of-the-art (Qian, Hu, and Simchi-Levi, 2024). These results demonstrate that optimistic policy optimization provides a natural, computationally superior and theoretically near-optimal path for solving CMDPs.

</details>


### [77] [HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2602.13710)
*Xin Yan,Zhenglin Wan,Feiyang Ye,Xingrui Yu,Hangyu Du,Yang You,Ivor Tsang*

Main category: cs.LG

TL;DR: HBVLA是一个针对视觉-语言-动作模型的二值化框架，通过策略感知Hessian矩阵识别关键权重、稀疏正交变换降低熵值、Haar域分组二值化，在保持高性能的同时大幅降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型虽然能实现指令跟随的具身控制，但其巨大的计算和内存需求阻碍了在资源受限的机器人和边缘平台上的部署。现有二值化方法无法缩小二值化与全精度权重之间的分布差距，导致量化误差在长时程闭环执行中累积，严重降低动作质量。

Method: 1) 使用策略感知增强Hessian矩阵识别对动作生成真正关键的权重；2) 对非显著权重使用稀疏正交变换诱导低熵中间状态；3) 在Haar域中对显著和非显著权重进行分组1-bit量化。

Result: 在LIBERO基准上，量化的OpenVLA-OFT保持了92.2%的全精度性能；在SimplerEnv上，量化的CogAct保持了93.6%的性能，显著优于最先进的二值化方法。真实世界评估显示HBVLA仅带来边际成功率下降，在严格硬件约束下具有鲁棒部署能力。

Conclusion: HBVLA为VLA模型的超低比特量化提供了实用基础，能够在硬件受限的机器人平台上实现更可靠的部署，解决了二值化VLA模型在长时程闭环执行中的性能退化问题。

Abstract: Vision-Language-Action (VLA) models enable instruction-following embodied control, but their large compute and memory footprints hinder deployment on resource-constrained robots and edge platforms. While reducing weights to 1-bit precision through binarization can greatly improve efficiency, existing methods fail to narrow the distribution gap between binarized and full-precision weights, causing quantization errors to accumulate under long-horizon closed-loop execution and severely degrade actions. To fill this gap, we propose HBVLA, a VLA-tailored binarization framework. First, we use a policy-aware enhanced Hessian to identify weights that are truly critical for action generation. Then, we employ a sparse orthogonal transform for non-salient weights to induce a low-entropy intermediate state. Finally, we quantize both salient and non-salient weights in the Harr domain with group-wise 1-bit quantization. We have evaluated our approach on different VLAs: on LIBERO, quantized OpenVLA-OFT retains 92.2% of full-precision performance; on SimplerEnv, quantized CogAct retains 93.6%, significantly outperforming state-of-the-art binarization methods. We further validate our method on real-world evaluation suite and the results show that HBVLA incurs only marginal success-rate degradation compared to the full-precision model, demonstrating robust deployability under tight hardware constraints. Our work provides a practical foundation for ultra-low-bit quantization of VLAs, enabling more reliable deployment on hardware-limited robotic platforms.

</details>


### [78] [Data-driven Bi-level Optimization of Thermal Power Systems with embedded Artificial Neural Networks](https://arxiv.org/abs/2602.13746)
*Talha Ansar,Muhammad Mujtaba Abbas,Ramit Debnath,Vivek Dua,Waqar Muhammad Ashraf*

Main category: cs.LG

TL;DR: 提出基于机器学习的双层优化框架ANN-KKT，用于工业热电系统的数据驱动优化，通过神经网络近似目标函数和KKT条件嵌入下层问题，实现计算高效的分层优化。


<details>
  <summary>Details</summary>
Motivation: 工业热电系统具有耦合的性能变量和重要性分层，同时优化计算复杂或不可行，限制了系统集成和可扩展的操作优化，需要解决大规模工程系统的优化问题。

Method: 提出完全机器学习驱动的双层优化框架，用人工神经网络近似上下层目标函数，通过Karush-Kuhn-Tucker最优性条件解析嵌入下层问题，形成ANN-KKT单层优化框架。

Result: 在基准问题和实际电厂（660MW煤电和395MW燃气轮机）上验证，获得与双层解相当的结果，计算时间仅0.22-0.88秒，优化输出功率分别为583MW和402MW，热耗率7337和7542 kJ/kWh。

Conclusion: ANN-KKT为工业热电系统的分层数据驱动优化提供了可扩展且计算高效的途径，实现大规模工程系统的节能运行，有助于工业5.0发展。

Abstract: Industrial thermal power systems have coupled performance variables with hierarchical order of importance, making their simultaneous optimization computationally challenging or infeasible. This barrier limits the integrated and computationally scaleable operation optimization of industrial thermal power systems. To address this issue for large-scale engineering systems, we present a fully machine learning-powered bi-level optimization framework for data-driven optimization of industrial thermal power systems. The objective functions of upper and lower levels are approximated by artificial neural network (ANN) models and the lower-level problem is analytically embedded through Karush-Kuhn-Tucker (KKT) optimality conditions. The reformulated single level optimization framework integrating ANN models and KKT constraints (ANN-KKT) is validated on benchmark problems and on real-world power generation operation of 660 MW coal power plant and 395 MW gas turbine system. The results reveal a comparable solutions obtained from the proposed ANN-KKT framework to the bi-level solutions of the benchmark problems. Marginal computational time requirement (0.22 to 0.88 s) to compute optimal solutions yields 583 MW (coal) and 402 MW (gas turbine) of power output at optimal turbine heat rate of 7337 kJ/kWh and 7542 kJ/kWh, respectively. In addition, the method expands to delineate a feasible and robust operating envelope that accounts for uncertainty in operating variables while maximizing thermal efficiency in various scenarios. These results demonstrate that ANN-KKT offers a scalable and computationally efficient route for hierarchical, data-driven optimization of industrial thermal power systems, achieving energy-efficient operations of large-scale engineering systems and contributing to industry 5.0.

</details>


### [79] [Discrete Double-Bracket Flows for Isotropic-Noise Invariant Eigendecomposition](https://arxiv.org/abs/2602.13759)
*ZhiMing Li,JiaHe Feng*

Main category: cs.LG

TL;DR: 提出一种离散双括号流方法，用于矩阵自由特征分解，通过生成器对各项同性平移不变性，消除噪声影响，实现仅依赖迹自由协方差的稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 传统随机逼近方法在矩阵向量乘积（MVP）预言机下进行特征分解时面临挑战：固定步长方法将稳定性与协方差算子范数耦合，而自适应步长方法则因更新消失而减慢。需要一种能有效处理噪声协方差结构的方法。

Method: 引入离散双括号流，其生成器对各项同性平移具有不变性，从而在离散时间层面实现路径对噪声项σ_k²I的不变性。该方法使用最大稳定步长η_max ∝ 1/||C_e||₂²，仅依赖于迹自由协方差C_e。

Result: 通过严格鞍点几何和对角化目标的状态稳定性分析建立了全局收敛性，样本复杂度为O(||C_e||₂²/(Δ²ε))。对退化块的显式特征化实现了O(log(1/ζ))的鞍点逃逸率和有限时间高概率收敛保证。

Conclusion: 该方法在矩阵自由特征分解中有效处理噪声协方差结构，通过不变性设计消除噪声影响，实现仅依赖迹自由协方差的稳定高效收敛，为随机特征分解问题提供了新的理论框架。

Abstract: We study matrix-free eigendecomposition under a matrix-vector product (MVP) oracle, where each step observes a covariance operator $C_k = C_{sig} + σ_k^2 I + E_k$. Standard stochastic approximation methods either use fixed steps that couple stability to $\|C_k\|_2$, or adapt steps in ways that slow down due to vanishing updates. We introduce a discrete double-bracket flow whose generator is invariant to isotropic shifts, yielding pathwise invariance to $σ_k^2 I$ at the discrete-time level. The resulting trajectory and a maximal stable step size $η_{max} \propto 1/\|C_e\|_2^2$ depend only on the trace-free covariance $C_e$. We establish global convergence via strict-saddle geometry for the diagonalization objective and an input-to-state stability analysis, with sample complexity scaling as $O(\|C_e\|_2^2 / (Δ^2 ε))$ under trace-free perturbations. An explicit characterization of degenerate blocks yields an accelerated $O(\log(1/ζ))$ saddle-escape rate and a high-probability finite-time convergence guarantee.

</details>


### [80] [On Representation Redundancy in Large-Scale Instruction Tuning Data Selection](https://arxiv.org/abs/2602.13773)
*Youwei Shu,Shaomian Zheng,Dingnan Jin,Wenjie Qu,Ziyao Guo,Qing Cui,Jun Zhou,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 提出CRDS框架，通过压缩语义表示减少冗余，提升指令调优数据选择质量，仅用3.5%数据即可超越全数据基线


<details>
  <summary>Details</summary>
Motivation: 工业级指令调优数据选择方法不足，现有LLM编码器产生的语义嵌入高度冗余，影响数据选择效果

Method: 提出CRDS框架：CRDS-R使用Rademacher随机投影+Transformer隐藏层表示拼接；CRDS-W使用白化降维提升表示质量

Result: 两种变体显著提升数据质量，优于现有表示选择方法；CRDS-W仅用3.5%数据即可超越全数据基线，平均提升0.71%

Conclusion: CRDS通过压缩语义表示有效解决LLM编码器冗余问题，为工业级指令调优数据选择提供高效解决方案

Abstract: Data quality is a crucial factor in large language models training. While prior work has shown that models trained on smaller, high-quality datasets can outperform those trained on much larger but noisy or low-quality corpora, systematic methods for industrial-scale data selection in instruction tuning remain underexplored. In this work, we study instruction-tuning data selection through the lens of semantic representation similarity and identify a key limitation of state-of-the-art LLM encoders: they produce highly redundant semantic embeddings. To mitigate this redundancy, we propose Compressed Representation Data Selection (CRDS), a novel framework with two variants. CRDS-R applies Rademacher random projection followed by concatenation of transformer hidden-layer representations, while CRDS-W employs whitening-based dimensionality reduction to improve representational quality. Experimental results demonstrate that both variants substantially enhance data quality and consistently outperform state-of-the-art representation-based selection methods. Notably, CRDS-W achieves strong performance using only 3.5% of the data, surpassing the full-data baseline by an average of 0.71% across four datasets. Our code is available at https://github.com/tdano1/CRDS.

</details>


### [81] [MEMTS: Internalizing Domain Knowledge via Parameterized Memory for Retrieval-Free Domain Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2602.13783)
*Xiaoyun Yu,Li fan,Xiangfei Qiu,Nanqing Dong,Yonggui Huang,Honggang Qi,Geguang Pu,Wanli Ouyang,Xi Chen,Jilin Hu*

Main category: cs.LG

TL;DR: MEMTS提出了一种轻量级、即插即用的检索自由时间序列领域自适应方法，通过知识持久化模块将领域特定时序动态内化为可学习的潜在原型，实现恒定时间推理和近零延迟的准确领域自适应。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在垂直领域部署时面临两个主要问题：1）时间分布偏移和领域特定周期结构导致性能显著下降；2）现有解决方案（领域自适应预训练和检索增强生成）存在灾难性遗忘或检索开销大的问题，无法满足实时流处理的高效要求。

Method: 提出MEMTS方法，核心是知识持久化模块（KPM），将领域特定的时序动态（如季节性模式和趋势）内化为紧凑的可学习潜在原型集合，将碎片化的历史观测转化为连续、参数化的知识表示。该方法无需修改冻结的时间序列基础模型架构。

Result: 在多个数据集上的广泛实验表明，MEMTS实现了最先进的性能，能够以恒定时间推理和近零延迟实现准确的领域自适应，同时有效缓解对通用时序模式的灾难性遗忘。

Conclusion: MEMTS通过将领域知识内化为潜在原型，打破了现有领域自适应方法的瓶颈，为时间序列基础模型在垂直领域的实时部署提供了高效、可扩展的解决方案。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated exceptional performance in generalized forecasting, their performance often degrades significantly when deployed in real-world vertical domains characterized by temporal distribution shifts and domain-specific periodic structures. Current solutions are primarily constrained by two paradigms: Domain-Adaptive Pretraining (DAPT), which improves short-term domain fitting but frequently disrupts previously learned global temporal patterns due to catastrophic forgetting; and Retrieval-Augmented Generation (RAG), which incorporates external knowledge but introduces substantial retrieval overhead. This creates a severe scalability bottleneck that fails to meet the high-efficiency requirements of real-time stream processing. To break this impasse, we propose Memory for Time Series (MEMTS), a lightweight and plug-and-play method for retrieval-free domain adaptation in time series forecasting. The key component of MEMTS is a Knowledge Persistence Module (KPM), which internalizes domain-specific temporal dynamics, such as recurring seasonal patterns and trends into a compact set of learnable latent prototypes. In doing so, it transforms fragmented historical observations into continuous, parameterized knowledge representations. This paradigm shift enables MEMTS to achieve accurate domain adaptation with constant-time inference and near-zero latency, while effectively mitigating catastrophic forgetting of general temporal patterns, all without requiring any architectural modifications to the frozen TSFM backbone. Extensive experiments on multiple datasets demonstrate the SOTA performance of MEMTS.

</details>


### [82] [MechPert: Mechanistic Consensus as an Inductive Bias for Unseen Perturbation Prediction](https://arxiv.org/abs/2602.13791)
*Marc Boubnovski Martell,Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Robert Kitchen,Jesper Ferkinghoff-Borg,Jialin Yu,Philip Torr,Kaspar Märten*

Main category: cs.LG

TL;DR: MechPert：一个轻量级框架，使用LLM代理生成定向调控假设而非依赖功能相似性，用于预测未见遗传扰动的转录反应，在低数据条件下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖静态、可能不完整的知识图谱，要么使用语言模型检索功能相似基因，这些关联基于科学文本中的对称共现而非定向调控逻辑，无法准确预测未见遗传扰动的转录反应。

Method: MechPert框架鼓励LLM代理生成定向调控假设而非依赖功能相似性。多个代理独立提出候选调控因子及置信度分数，通过共识机制聚合并过滤虚假关联，生成加权邻域用于下游预测。

Result: 在四个人类细胞系的Perturb-seq基准测试中，低数据条件下（N=50个观测扰动）MechPert比基于相似性的基线方法提升皮尔逊相关性达10.5%。在实验设计中，MechPert选择的锚基因在特征明确的细胞系中比标准网络中心性启发式方法表现提升达46%。

Conclusion: MechPert通过LLM代理生成定向调控假设，有效解决了现有方法依赖对称共现关联的问题，在低数据条件下显著提升了遗传扰动预测的准确性，并为实验设计提供了更优的基因选择策略。

Abstract: Predicting transcriptional responses to unseen genetic perturbations is essential for understanding gene regulation and prioritizing large-scale perturbation experiments. Existing approaches either rely on static, potentially incomplete knowledge graphs, or prompt language models for functionally similar genes, retrieving associations shaped by symmetric co-occurrence in scientific text rather than directed regulatory logic. We introduce MechPert, a lightweight framework that encourages LLM agents to generate directed regulatory hypotheses rather than relying solely on functional similarity. Multiple agents independently propose candidate regulators with associated confidence scores; these are aggregated through a consensus mechanism that filters spurious associations, producing weighted neighborhoods for downstream prediction. We evaluate MechPert on Perturb-seq benchmarks across four human cell lines. For perturbation prediction in low-data regimes ($N=50$ observed perturbations), MechPert improves Pearson correlation by up to 10.5\% over similarity-based baselines. For experimental design, MechPert-selected anchor genes outperform standard network centrality heuristics by up to 46\% in well-characterized cell lines.

</details>


### [83] [Cast-R1: Learning Tool-Augmented Sequential Decision Policies for Time Series Forecasting](https://arxiv.org/abs/2602.13802)
*Xiaoyu Tao,Mingyue Cheng,Chuang Jiang,Tian Gao,Huanjian Zhang,Yaguo Liu*

Main category: cs.LG

TL;DR: Cast-R1将时间序列预测重新定义为顺序决策问题，通过基于记忆的状态管理和工具增强的智能体工作流实现迭代式预测优化


<details>
  <summary>Details</summary>
Motivation: 传统模型中心方法将预测视为从历史观测到未来值的单次映射，在复杂演化环境中表现不佳，缺乏自主获取证据、推理未来变化或通过迭代决策修正预测的能力

Method: 提出Cast-R1框架：1) 基于记忆的状态管理机制跨交互步骤维护决策相关信息；2) 工具增强的智能体工作流，自主与模块化工具包交互提取统计特征、调用轻量预测模型、执行基于推理的预测并通过自我反思迭代优化；3) 两阶段训练策略结合监督微调与多轮强化学习，辅以课程学习逐步增加任务难度

Result: 在多个真实世界时间序列数据集上的广泛实验证明了Cast-R1的有效性

Conclusion: 这项工作为时间序列建模的智能体范式探索提供了实用步骤，展示了将预测重构为顺序决策问题的潜力

Abstract: Time series forecasting has long been dominated by model-centric approaches that formulate prediction as a single-pass mapping from historical observations to future values. Despite recent progress, such formulations often struggle in complex and evolving settings, largely because most forecasting models lack the ability to autonomously acquire informative evidence, reason about potential future changes, or revise predictions through iterative decision processes. In this work, we propose Cast-R1, a learned time series forecasting framework that reformulates forecasting as a sequential decision-making problem. Cast-R1 introduces a memory-based state management mechanism that maintains decision-relevant information across interaction steps, enabling the accumulation of contextual evidence to support long-horizon reasoning. Building on this formulation, forecasting is carried out through a tool-augmented agentic workflow, in which the agent autonomously interacts with a modular toolkit to extract statistical features, invoke lightweight forecasting models for decision support, perform reasoning-based prediction, and iteratively refine forecasts through self-reflection. To train Cast-R1, we adopt a two-stage learning strategy that combines supervised fine-tuning with multi-turn reinforcement learning, together with a curriculum learning scheme that progressively increases task difficulty to improve policy learning. Extensive experiments on multiple real-world time series datasets demonstrate the effectiveness of Cast-R1. We hope this work provides a practical step towards further exploration of agentic paradigms for time series modeling. Our code is available at https://github.com/Xiaoyu-Tao/Cast-R1-TS.

</details>


### [84] [Fast Physics-Driven Untrained Network for Highly Nonlinear Inverse Scattering Problems](https://arxiv.org/abs/2602.13805)
*Yutong Du,Zicheng Liu,Yi Huang,Bazargul Matkerim,Bo Qi,Yali Zong,Peixian Han*

Main category: cs.LG

TL;DR: 提出实时物理驱动傅里叶谱(PDF)求解器，通过谱域降维实现亚秒级电磁逆散射重建，相比现有未训练神经网络提速100倍。


<details>
  <summary>Details</summary>
Motivation: 未训练神经网络(UNNs)虽然能实现高保真电磁逆散射重建，但受限于高维空间域优化带来的计算负担，无法满足实时应用需求。

Method: 1) 使用截断傅里叶基展开感应电流，将优化限制在紧凑的低频参数空间；2) 集成收缩积分方程(CIE)缓解高对比度非线性；3) 采用对比度补偿算子(CCO)校正谱诱导衰减；4) 提出桥抑制损失函数增强相邻散射体边界锐度。

Result: 相比最先进的未训练神经网络，实现100倍加速，在噪声和天线不确定性下仍保持鲁棒性能，达到亚秒级重建速度，支持实时微波成像应用。

Conclusion: PDF求解器通过谱域降维和物理驱动优化策略，成功解决了UNNs的计算瓶颈，实现了实时高效的电磁逆散射重建，为微波成像的实际应用提供了可行方案。

Abstract: Untrained neural networks (UNNs) offer high-fidelity electromagnetic inverse scattering reconstruction but are computationally limited by high-dimensional spatial-domain optimization. We propose a Real-Time Physics-Driven Fourier-Spectral (PDF) solver that achieves sub-second reconstruction through spectral-domain dimensionality reduction. By expanding induced currents using a truncated Fourier basis, the optimization is confined to a compact low-frequency parameter space supported by scattering measurements. The solver integrates a contraction integral equation (CIE) to mitigate high-contrast nonlinearity and a contrast-compensated operator (CCO) to correct spectral-induced attenuation. Furthermore, a bridge-suppressing loss is formulated to enhance boundary sharpness between adjacent scatterers. Numerical and experimental results demonstrate a 100-fold speedup over state-of-the-art UNNs with robust performance under noise and antenna uncertainties, enabling real-time microwave imaging applications.

</details>


### [85] [AnomaMind: Agentic Time Series Anomaly Detection with Tool-Augmented Reasoning](https://arxiv.org/abs/2602.13807)
*Xiaoyu Tao,Yuchong Wu,Mingyue Cheng,Ze Guo,Tian Gao*

Main category: cs.LG

TL;DR: AnomaMind：将时间序列异常检测重构为顺序决策过程的智能体框架，通过工具交互、自适应特征准备和自反思机制提升检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将异常检测视为固定特征输入的纯判别预测任务，而非证据驱动的诊断过程，导致在异常具有强上下文依赖或多样模式时表现不佳。这些限制源于缺乏自适应特征准备、推理感知检测和迭代优化。

Method: 提出AnomaMind智能体框架，将异常检测重构为顺序决策过程。采用从粗到细的渐进定位、多轮工具交互的自适应特征准备、自反思决策优化。核心设计是工具增强异常检测的混合推理机制：通用模型负责自主工具交互和自反思优化，核心检测决策通过强化学习在工作流级可验证反馈下学习。

Result: 在多样化设置下的广泛实验表明，AnomaMind持续提升异常检测性能。

Conclusion: AnomaMind通过将异常检测重构为顺序决策过程，结合自适应特征准备、工具增强推理和自反思机制，解决了现有方法的局限性，为复杂场景下的可靠异常检测提供了有效框架。

Abstract: Time series anomaly detection is critical in many real-world applications, where effective solutions must localize anomalous regions and support reliable decision-making under complex settings. However, most existing methods frame anomaly detection as a purely discriminative prediction task with fixed feature inputs, rather than an evidence-driven diagnostic process. As a result, they often struggle when anomalies exhibit strong context dependence or diverse patterns. We argue that these limitations stem from the lack of adaptive feature preparation, reasoning-aware detection, and iterative refinement during inference. To address these challenges, we propose AnomaMind, an agentic time series anomaly detection framework that reformulates anomaly detection as a sequential decision-making process. AnomaMind operates through a structured workflow that progressively localizes anomalous intervals in a coarse-to-fine manner, augments detection through multi-turn tool interactions for adaptive feature preparation, and refines anomaly decisions via self-reflection. The workflow is supported by a set of reusable tool engines, enabling context-aware diagnostic analysis. A key design of AnomaMind is an explicitly designed hybrid inference mechanism for tool-augmented anomaly detection. In this mechanism, general-purpose models are responsible for autonomous tool interaction and self-reflective refinement, while core anomaly detection decisions are learned through reinforcement learning under verifiable workflow-level feedback, enabling task-specific optimization within a flexible reasoning framework. Extensive experiments across diverse settings demonstrate that AnomaMind consistently improves anomaly detection performance. The code is available at https://anonymous.4open.science/r/AnomaMind.

</details>


### [86] [Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation](https://arxiv.org/abs/2602.13810)
*Guojian Zhan,Letian Tao,Pengcheng Wang,Yixiao Wang,Yiheng Li,Yuxin Chen,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出MVP（平均速度策略），一种新的生成式策略函数，通过建模平均速度场实现最快的一步动作生成，在表达能力和计算效率之间取得更好平衡


<details>
  <summary>Details</summary>
Motivation: 现有基于流的策略在表达能力和计算负担之间存在权衡，通常通过流步骤数量来控制。需要一种既能保持高表达能力又能实现快速推理的策略函数

Method: 提出MVP策略，建模平均速度场实现一步动作生成。引入瞬时速度约束（IVC）确保高表达能力，该约束作为关键边界条件，提高学习精度和策略表达能力

Result: 在Robomimic和OGBench的多个挑战性机器人操作任务中达到最先进成功率。相比现有基于流的策略基线，在训练和推理速度上有显著提升

Conclusion: MVP通过建模平均速度场和引入IVC约束，在保持高表达能力的同时实现了快速的一步动作生成，在表达能力和计算效率之间取得了更好平衡

Abstract: Learning expressive and efficient policy functions is a promising direction in reinforcement learning (RL). While flow-based policies have recently proven effective in modeling complex action distributions with a fast deterministic sampling process, they still face a trade-off between expressiveness and computational burden, which is typically controlled by the number of flow steps. In this work, we propose mean velocity policy (MVP), a new generative policy function that models the mean velocity field to achieve the fastest one-step action generation. To ensure its high expressiveness, an instantaneous velocity constraint (IVC) is introduced on the mean velocity field during training. We theoretically prove that this design explicitly serves as a crucial boundary condition, thereby improving learning accuracy and enhancing policy expressiveness. Empirically, our MVP achieves state-of-the-art success rates across several challenging robotic manipulation tasks from Robomimic and OGBench. It also delivers substantial improvements in training and inference speed over existing flow-based policy baselines.

</details>


### [87] [Pawsterior: Variational Flow Matching for Structured Simulation-Based Inference](https://arxiv.org/abs/2602.13813)
*Jorge Carrasco-Pollo,Floor Eijkelboom,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: Pawsterior是一个变分流匹配框架，用于改进和扩展基于模拟的推理（SBI），特别处理结构化约束域和离散潜变量问题。


<details>
  <summary>Details</summary>
Motivation: 许多SBI问题涉及结构化约束域（如有界物理参数或混合离散-连续变量），但标准流匹配方法通常在无约束空间中操作，导致学习效率低且难以满足物理约束。

Method: 提出端点诱导的仿射几何约束原则，通过双端变分模型将域几何直接纳入推理过程；变分参数化支持离散潜变量结构，扩展了流匹配的应用范围。

Result: 在标准SBI基准测试中表现出更好的分类器双样本测试性能，提高了采样时的数值稳定性和后验保真度，并能处理传统流匹配无法解决的离散潜变量问题。

Conclusion: Pawsterior通过同时解决几何约束和离散潜变量结构问题，将流匹配扩展到更广泛的结构化SBI问题领域，填补了传统方法的空白。

Abstract: We introduce Pawsterior, a variational flow-matching framework for improved and extended simulation-based inference (SBI). Many SBI problems involve posteriors constrained by structured domains, such as bounded physical parameters or hybrid discrete-continuous variables, yet standard flow-matching methods typically operate in unconstrained spaces. This mismatch leads to inefficient learning and difficulty respecting physical constraints. Our contributions are twofold. First, generalizing the geometric inductive bias of CatFlow, we formalize endpoint-induced affine geometric confinement, a principle that incorporates domain geometry directly into the inference process via a two-sided variational model. This formulation improves numerical stability during sampling and leads to consistently better posterior fidelity, as demonstrated by improved classifier two-sample test performance across standard SBI benchmarks. Second, and more importantly, our variational parameterization enables SBI tasks involving discrete latent structure (e.g., switching systems) that are fundamentally incompatible with conventional flow-matching approaches. By addressing both geometric constraints and discrete latent structure, Pawsterior extends flow-matching to a broader class of structured SBI problems that were previously inaccessible.

</details>


### [88] [Sufficient Conditions for Stability of Minimum-Norm Interpolating Deep ReLU Networks](https://arxiv.org/abs/2602.13910)
*Ouns El Harzli,Yoonsoo Nam,Ilja Kuzborskij,Bernardo Cuenca Grau,Ard A. Louis*

Main category: cs.LG

TL;DR: 本文研究深度ReLU齐次神经网络在实现零训练误差时的算法稳定性，发现当网络包含稳定子网络且后续层为低秩权重矩阵时稳定，否则不一定稳定。


<details>
  <summary>Details</summary>
Motivation: 算法稳定性是分析学习算法泛化误差的经典框架，但在深度神经网络分析中应用有限。本文旨在研究实现零训练误差的最小范数插值深度ReLU齐次神经网络的稳定性条件。

Method: 研究深度ReLU齐次神经网络在最小范数插值条件下的算法稳定性，分析网络包含稳定子网络且后续层为低秩权重矩阵时的稳定性条件。

Result: 1) 当网络包含稳定子网络且后续层权重矩阵为低秩时，网络稳定；2) 即使包含稳定子网络，如果后续层不是低秩，网络也不一定稳定。

Conclusion: 低秩权重矩阵是实现深度神经网络算法稳定性的关键因素，这解释了为什么深度神经网络训练倾向于产生低秩权重矩阵，特别是在最小范数插值和权重衰减正则化情况下。

Abstract: Algorithmic stability is a classical framework for analyzing the generalization error of learning algorithms. It predicts that an algorithm has small generalization error if it is insensitive to small perturbations in the training set such as the removal or replacement of a training point. While stability has been demonstrated for numerous well-known algorithms, this framework has had limited success in analyses of deep neural networks. In this paper we study the algorithmic stability of deep ReLU homogeneous neural networks that achieve zero training error using parameters with the smallest $L_2$ norm, also known as the minimum-norm interpolation, a phenomenon that can be observed in overparameterized models trained by gradient-based algorithms. We investigate sufficient conditions for such networks to be stable. We find that 1) such networks are stable when they contain a (possibly small) stable sub-network, followed by a layer with a low-rank weight matrix, and 2) such networks are not guaranteed to be stable even when they contain a stable sub-network, if the following layer is not low-rank. The low-rank assumption is inspired by recent empirical and theoretical results which demonstrate that training deep neural networks is biased towards low-rank weight matrices, for minimum-norm interpolation and weight-decay regularization.

</details>


### [89] [GREPO: A Benchmark for Graph Neural Networks on Repository-Level Bug Localization](https://arxiv.org/abs/2602.13921)
*Juntong Wang,Libin Chen,Xiyuan Wang,Shijia Kang,Haotong Yang,Da Zheng,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出了首个用于仓库级bug定位的GNN基准测试GREPO，包含86个Python仓库和47294个bug修复任务，GNN方法显著优于传统检索方法。


<details>
  <summary>Details</summary>
Motivation: 仓库级bug定位是关键的软件工程挑战，标准LLM由于上下文窗口限制无法处理整个代码仓库，现有检索方法（关键词匹配、文本相似度、简单图启发式）有限，GNN虽能建模复杂依赖但缺乏专用基准。

Method: 构建GREPO基准，包含86个Python仓库和47294个bug修复任务，提供可直接用于GNN处理的图数据结构，评估多种GNN架构与传统信息检索基线。

Result: GNN架构表现出色，显著优于传统信息检索基线方法，证明了GNN在bug定位任务中的潜力。

Conclusion: GREPO为仓库级bug定位任务提供了首个GNN基准，展示了GNN在该领域的优势，为未来研究奠定了基础资源。

Abstract: Repository-level bug localization-the task of identifying where code must be modified to fix a bug-is a critical software engineering challenge. Standard Large Language Modles (LLMs) are often unsuitable for this task due to context window limitations that prevent them from processing entire code repositories. As a result, various retrieval methods are commonly used, including keyword matching, text similarity, and simple graph-based heuristics such as Breadth-First Search. Graph Neural Networks (GNNs) offer a promising alternative due to their ability to model complex, repository-wide dependencies; however, their application has been hindered by the lack of a dedicated benchmark. To address this gap, we introduce GREPO, the first GNN benchmark for repository-scale bug localization tasks. GREPO comprises 86 Python repositories and 47294 bug-fixing tasks, providing graph-based data structures ready for direct GNN processing. Our evaluation of various GNN architectures shows outstanding performance compared to established information retrieval baselines. This work highlights the potential of GNNs for bug localization and established GREPO as a foundation resource for future research, The code is available at https://github.com/qingpingmo/GREPO.

</details>


### [90] [Why Code, Why Now: Learnability, Computability, and the Real Limits of Machine Learning](https://arxiv.org/abs/2602.13934)
*Zhimin Zhao*

Main category: cs.LG

TL;DR: 论文提出基于信息结构的五级可学习性层次，认为ML进展天花板更多取决于任务是否可学习而非模型大小，解释了为什么代码生成比强化学习更可靠。


<details>
  <summary>Details</summary>
Motivation: 代码生成比强化学习进展更可靠，因为代码具有更好的信息结构。作者想探究这种差异的本质原因，并挑战"仅靠扩展就能解决所有ML挑战"的常见假设。

Method: 提出基于信息结构的五级可学习性层次，建立计算问题的三个属性（可表达性、可计算性、可学习性）之间的形式化区别和关系，提供统一模板来明确结构差异。

Result: 分析表明代码的监督学习可预测地扩展，而强化学习则不能，因为代码在每个token都提供密集、局部、可验证的反馈，而大多数强化学习问题缺乏这种信息结构。

Conclusion: ML进展的天花板更多取决于任务是否可学习，而非模型大小。仅靠扩展无法解决所有ML挑战，需要关注问题的信息结构和可学习性层次。

Abstract: Code generation has progressed more reliably than reinforcement learning, largely because code has an information structure that makes it learnable. Code provides dense, local, verifiable feedback at every token, whereas most reinforcement learning problems do not. This difference in feedback quality is not binary but graded. We propose a five-level hierarchy of learnability based on information structure and argue that the ceiling on ML progress depends less on model size than on whether a task is learnable at all. The hierarchy rests on a formal distinction among three properties of computational problems (expressibility, computability, and learnability). We establish their pairwise relationships, including where implications hold and where they fail, and present a unified template that makes the structural differences explicit. The analysis suggests why supervised learning on code scales predictably while reinforcement learning does not, and why the common assumption that scaling alone will solve remaining ML challenges warrants scrutiny.

</details>


### [91] [A Multi-Agent Framework for Code-Guided, Modular, and Verifiable Automated Machine Learning](https://arxiv.org/abs/2602.13937)
*Dat Le,Duc-Cuong Le,Anh-Son Nguyen,Tuan-Dung Bui,Thu-Trang Nguyen,Son Nguyen,Hieu Dinh Vo*

Main category: cs.LG

TL;DR: iML是一个基于多智能体的AutoML框架，通过代码引导的规划、模块化实现和可验证集成，解决了传统AutoML黑盒问题和LLM代理的幻觉逻辑问题，在真实Kaggle竞赛中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML框架通常作为"黑盒"运行，缺乏复杂工程任务所需的灵活性和透明度。而基于LLM的代理虽然转向代码驱动方法，但经常出现幻觉逻辑和逻辑纠缠问题，导致不可恢复的运行时故障。

Method: iML提出三个核心创新：1) 代码引导规划 - 基于自主经验分析合成战略蓝图以消除幻觉；2) 代码模块化实现 - 将预处理和建模解耦为受严格接口约束的专门组件；3) 代码可验证集成 - 通过动态合约验证和迭代自校正强制物理可行性。

Result: 在MLE-BENCH上，iML实现了85%的有效提交率和45%的竞争奖牌率，平均标准化性能得分(APS)为0.77。在iML-BENCH上，iML显著优于其他方法38%-163%。即使在简化任务描述下，iML仍保持70%的成功率。

Conclusion: iML通过将AutoML从黑盒提示转向代码引导、模块化和可验证的架构范式，有效弥合了随机生成与可靠工程之间的差距，标志着向真正AutoML迈出了有意义的一步。

Abstract: Automated Machine Learning (AutoML) has revolutionized the development of data-driven solutions; however, traditional frameworks often function as "black boxes", lacking the flexibility and transparency required for complex, real-world engineering tasks. Recent Large Language Model (LLM)-based agents have shifted toward code-driven approaches. However, they frequently suffer from hallucinated logic and logic entanglement, where monolithic code generation leads to unrecoverable runtime failures. In this paper, we present iML, a novel multi-agent framework designed to shift AutoML from black-box prompting to a code-guided, modular, and verifiable architectural paradigm. iML introduces three main ideas: (1) Code-Guided Planning, which synthesizes a strategic blueprint grounded in autonomous empirical profiling to eliminate hallucination; (2) Code-Modular Implementation, which decouples preprocessing and modeling into specialized components governed by strict interface contracts; and (3) Code-Verifiable Integration, which enforces physical feasibility through dynamic contract verification and iterative self-correction. We evaluate iML across MLE-BENCH and the newly introduced iML-BENCH, comprising a diverse range of real-world Kaggle competitions. The experimental results show iML's superiority over state-of-the-art agents, achieving a valid submission rate of 85% and a competitive medal rate of 45% on MLE-BENCH, with an average standardized performance score (APS) of 0.77. On iML-BENCH, iML significantly outperforms the other approaches by 38%-163% in APS. Furthermore, iML maintains a robust 70% success rate even under stripped task descriptions, effectively filling information gaps through empirical profiling. These results highlight iML's potential to bridge the gap between stochastic generation and reliable engineering, marking a meaningful step toward truly AutoML.

</details>


### [92] [An Adaptive Model Selection Framework for Demand Forecasting under Horizon-Induced Degradation to Support Business Strategy and Operations](https://arxiv.org/abs/2602.13939)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: 论文提出AHSIV框架，用于在间歇性、高变异性需求环境下进行多步预测的模型选择，通过结合多种误差指标、需求分类和层次优化来解决预测时域导致的排名不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 在结构性需求间歇性、高变异性和多步规划的业务环境中，需要稳健可复现的模型选择机制。实证表明没有单一模型在所有情况下都最优，模型排名会随误差指标、需求机制和预测时域变化，导致多SKU决策中的模糊性。

Method: 提出AHSIV框架，整合了：1) 通过MDFH程序调整的缩放和绝对误差指标；2) 结构性需求分类；3) 多目标帕累托支配；4) 层次偏置优化。该框架具有时域感知和机制条件化的特点。

Result: 在Walmart、M3、M4、M5数据集上评估，AHSIV在聚合性能上与最强单指标基线统计等价，同时提高了时域特定最佳模型选择的频率。

Conclusion: 异质需求环境中的模型选择不能视为静态排名问题，时域一致、结构自适应的机制为多SKU预测提供了原则性、操作连贯的解决方案。

Abstract: Business environments characterized by structural demand intermittency, high variability, and multi-step planning horizons require robust and reproducible model selection mechanisms. Empirical evidence shows that no forecasting model is universally dominant and that relative rankings vary across error metrics, demand regimes, and forecast horizons, generating ambiguity in multi-SKU decision contexts. This study proposes AHSIV (Adaptive Hybrid Selector for Intermittency and Variability), a horizon-aware and regime-conditioned model selection framework designed to address horizon-induced ranking instability. The proposed approach integrates scaled and absolute error metrics adjusted through a Metric Degradation by Forecast Horizon (MDFH) procedure, structural demand classification, multi-objective Pareto dominance, and hierarchical bias refinement within a unified decision architecture. The empirical evaluation is conducted on the Walmart, M3, M4, and M5 datasets under multiple train-test partition schemes and twelve-step forecasting horizons. Results indicate that AHSIV achieves statistical equivalence with the strongest monometric baseline in terms of aggregated performance while increasing the frequency of horizon-specific best-model selection. The findings demonstrate that model selection in heterogeneous demand environments cannot be treated as a static ranking problem, and that horizon-consistent, structurally adaptive mechanisms provide a principled, operationally coherent solution for multi-SKU forecasting.

</details>


### [93] [You Can Learn Tokenization End-to-End with Reinforcement Learning](https://arxiv.org/abs/2602.13940)
*Sam Dauncey,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出使用分数函数估计来学习LLM中的token边界，相比之前的直通估计方法有更好的理论保证和性能表现


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练流程中的tokenization步骤仍然是硬编码的压缩过程，与架构日益端到端的趋势不符。虽然已有工作尝试将token边界学习纳入模型架构，但现有方法存在理论缺陷

Method: 采用分数函数估计来学习离散的token边界，结合强化学习中的时间折扣技术来降低方差，使方法具有实际可行性

Result: 在1亿参数规模上，该方法在定性和定量评估中都优于之前提出的直通估计方法

Conclusion: 分数函数估计能够更有效地学习LLM中的token边界，为端到端的语言模型训练提供了更好的解决方案

Abstract: Tokenization is a hardcoded compression step which remains in the training pipeline of Large Language Models (LLMs), despite a general trend towards architectures becoming increasingly end-to-end. Prior work has shown promising results at scale in bringing this compression step inside the LLMs' architecture with heuristics to draw token boundaries, and also attempts to learn these token boundaries with straight-through estimates, which treat the problem of drawing discrete token boundaries as a continuous one. We show that these token boundaries can instead be learned using score function estimates, which have tighter theoretical guarantees due to directly optimizing the problem of drawing discrete token boundaries to minimize loss. We observe that techniques from reinforcement learning, such as time discounting, are necessary to reduce the variance of this score function sufficiently to make it practicable. We demonstrate that the resultant method outperforms prior proposed straight-through estimates, both qualitatively and quantitatively at the $100$ million parameter scale.

</details>


### [94] [Experiential Reinforcement Learning](https://arxiv.org/abs/2602.13949)
*Taiwei Shi,Sihao Chen,Bowen Jiang,Linxin Song,Longqi Yang,Jieyu Zhao*

Main category: cs.LG

TL;DR: ERL（体验式强化学习）通过引入"经验-反思-巩固"循环，将环境反馈转化为结构化行为修正，在稀疏奖励环境中显著提升学习效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习中环境反馈通常是稀疏和延迟的，语言模型需要隐式推断如何将观察到的失败转化为未来迭代的行为改变，这一过程具有挑战性。

Method: ERL在强化学习过程中嵌入显式的经验-反思-巩固循环：模型生成初始尝试→接收环境反馈→产生反思→指导改进的第二次尝试→成功被强化并内化到基础策略中。

Result: 在稀疏奖励控制环境和智能推理基准测试中，ERL相比强基线持续提升学习效率和最终性能：在复杂多步环境中提升高达81%，在工具使用推理任务中提升高达11%。

Conclusion: 将显式自我反思整合到策略训练中，为将反馈转化为持久的行为改进提供了实用机制，同时无需额外推理成本即可保持部署时的收益。

Abstract: Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.

</details>


### [95] [QuRL: Efficient Reinforcement Learning with Quantized Rollout](https://arxiv.org/abs/2602.13953)
*Yuhang Li,Reena Elangovan,Xin Dong,Priyadarshini Panda,Brucek Khailany*

Main category: cs.LG

TL;DR: 提出QuRL方法，使用量化actor加速RL训练中的rollout过程，通过自适应裁剪范围和不变缩放技术解决训练崩溃和权重更新问题，在INT8和FP8量化实验中实现20-80%的rollout加速。


<details>
  <summary>Details</summary>
Motivation: 在可验证奖励的强化学习（RLVR）训练推理大语言模型时，由于自回归解码特性，rollout过程成为训练效率瓶颈，占用高达70%的总训练时间。

Method: 提出量化强化学习（QuRL），使用量化actor加速rollout。解决两个关键挑战：1）自适应裁剪范围（ACR），基于全精度actor和量化actor之间的策略比率动态调整裁剪比例，防止长期训练崩溃；2）不变缩放技术，减少量化噪声并增加权重更新，解决权重更新过小的问题。

Result: 在DeepScaleR和DAPO上进行INT8和FP8量化实验，实现20%到80%的rollout加速，显著提升训练效率。

Conclusion: QuRL通过量化actor有效加速RL训练中的rollout过程，提出的ACR和不变缩放技术解决了量化RL中的关键挑战，为高效训练推理大语言模型提供了实用解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a trending paradigm for training reasoning large language models (LLMs). However, due to the autoregressive decoding nature of LLMs, the rollout process becomes the efficiency bottleneck of RL training, consisting of up to 70\% of the total training time. In this work, we propose Quantized Reinforcement Learning (QuRL) that uses a quantized actor for accelerating the rollout. We address two challenges in QuRL. First, we propose Adaptive Clipping Range (ACR) that dynamically adjusts the clipping ratio based on the policy ratio between the full-precision actor and the quantized actor, which is essential for mitigating long-term training collapse. Second, we identify the weight update problem, where weight changes between RL steps are extremely small, making it difficult for the quantization operation to capture them effectively. We mitigate this problem through the invariant scaling technique that reduces quantization noise and increases weight update. We evaluate our method with INT8 and FP8 quantization experiments on DeepScaleR and DAPO, and achieve 20% to 80% faster rollout during training.

</details>


### [96] [Chemical Language Models for Natural Products: A State-Space Model Approach](https://arxiv.org/abs/2602.13958)
*Ho-Hsuan Wang,Afnan Sultan,Andrea Volkamer,Dietrich Klakow*

Main category: cs.LG

TL;DR: 开发了针对天然产物(NPs)的化学语言模型(NPCLMs)，首次系统比较了选择性状态空间模型(Mamba/Mamba-2)与Transformer(GPT)在NP任务上的表现，使用8种分词策略，在约100万NPs数据集上预训练，结果显示Mamba在分子生成和性质预测方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在化学领域广泛应用，但天然产物(NPs)在药物发现中非常重要却未被充分探索。现有模型主要针对一般分子，缺乏针对NPs的专门模型，需要填补这一研究空白。

Method: 1) 使用约100万NPs数据集预训练NP专用化学语言模型；2) 比较状态空间模型(Mamba和Mamba-2)与Transformer基线(GPT)；3) 采用8种分词策略：字符级、Atom-in-SMILES、字节对编码和NP专用BPE；4) 评估分子生成(有效性、独特性、新颖性)和性质预测(膜渗透性、味道、抗癌活性)。

Result: Mamba在分子生成方面比Mamba-2和GPT多生成1-2%的有效和独特分子，且长程依赖错误更少；GPT生成的结构略新颖。性质预测方面，Mamba变体在随机分割下比GPT高出0.02-0.04 MCC，骨架分割下性能相当。领域特定预训练在100万NPs数据集上可媲美在100倍大数据集上训练的模型。

Conclusion: 针对天然产物的领域特定预训练能显著提升模型性能，选择性状态空间模型(Mamba)在NP相关任务上表现优于传统Transformer，证明了针对特定化学领域开发专用模型的价值。

Abstract: Language models are widely used in chemistry for molecular property prediction and small-molecule generation, yet Natural Products (NPs) remain underexplored despite their importance in drug discovery. To address this gap, we develop NP-specific chemical language models (NPCLMs) by pre-training state-space models (Mamba and Mamba-2) and comparing them with transformer baselines (GPT). Using a dataset of about 1M NPs, we present the first systematic comparison of selective state-space models and transformers for NP-focused tasks, together with eight tokenization strategies including character-level, Atom-in-SMILES (AIS), byte-pair encoding (BPE), and NP-specific BPE. We evaluate molecule generation (validity, uniqueness, novelty) and property prediction (membrane permeability, taste, anti-cancer activity) using MCC and AUC-ROC. Mamba generates 1-2 percent more valid and unique molecules than Mamba-2 and GPT, with fewer long-range dependency errors, while GPT yields slightly more novel structures. For property prediction, Mamba variants outperform GPT by 0.02-0.04 MCC under random splits, while scaffold splits show comparable performance. Results demonstrate that domain-specific pre-training on about 1M NPs can match models trained on datasets over 100 times larger.

</details>


### [97] [Steady-State Behavior of Constant-Stepsize Stochastic Approximation: Gaussian Approximation and Tail Bounds](https://arxiv.org/abs/2602.13960)
*Zedong Wang,Yuyang Wang,Ijay Narang,Felix Wang,Yuzhou Wang,Siva Theja Maguluri*

Main category: cs.LG

TL;DR: 本文为固定步长随机逼近算法的稳态分布提供了非渐近误差界，包括Wasserstein距离和Berry-Esseen型尾概率界，适用于多种随机逼近设置。


<details>
  <summary>Details</summary>
Motivation: 固定步长随机逼近算法在计算效率方面广泛应用，但其稳态分布通常难以解析求解。虽然已知当步长趋近于0时，中心化缩放后的稳态分布弱收敛于高斯分布，但对于固定步长，这种弱收敛无法提供可用的误差界。本文旨在为固定步长提供显式的非渐近误差界。

Method: 首先证明通用定理，在漂移项正则性和噪声矩条件下，给出中心化缩放稳态分布与适当高斯分布之间的Wasserstein距离界。该方法适用于i.i.d.和马尔可夫噪声模型。然后将这些定理实例化到三个代表性随机逼近设置：1) 光滑强凸目标的随机梯度下降；2) 线性随机逼近；3) 压缩非线性随机逼近。进一步基于Wasserstein近似误差推导非均匀Berry-Esseen型尾概率界。

Result: 获得了阶数为α^{1/2}log(1/α)的维度依赖和步长依赖的显式Wasserstein距离界。对于小步长α，得到了同时依赖于偏离水平和步长α的显式误差项。对于一般凸目标的SGD，识别出非高斯（吉布斯）极限律，并提供了相应的预极限Wasserstein误差界。

Conclusion: 本文为固定步长随机逼近算法的稳态分布提供了首个非渐近误差界，填补了弱收敛理论到实际应用之间的空白。这些显式误差界对于理解随机逼近算法的实际性能具有重要意义，特别是在机器学习应用中。

Abstract: Constant-stepsize stochastic approximation (SA) is widely used in learning for computational efficiency. For a fixed stepsize, the iterates typically admit a stationary distribution that is rarely tractable. Prior work shows that as the stepsize $α\downarrow 0$, the centered-and-scaled steady state converges weakly to a Gaussian random vector. However, for fixed $α$, this weak convergence offers no usable error bound for approximating the steady-state by its Gaussian limit. This paper provides explicit, non-asymptotic error bounds for fixed $α$. We first prove general-purpose theorems that bound the Wasserstein distance between the centered-scaled steady state and an appropriate Gaussian distribution, under regularity conditions for drift and moment conditions for noise. To ensure broad applicability, we cover both i.i.d. and Markovian noise models. We then instantiate these theorems for three representative SA settings: (1) stochastic gradient descent (SGD) for smooth strongly convex objectives, (2) linear SA, and (3) contractive nonlinear SA. We obtain dimension- and stepsize-dependent, explicit bounds in Wasserstein distance of order $α^{1/2}\log(1/α)$ for small $α$. Building on the Wasserstein approximation error, we further derive non-uniform Berry--Esseen-type tail bounds that compare the steady-state tail probability to Gaussian tails. We achieve an explicit error term that decays in both the deviation level and stepsize $α$. We adapt the same analysis for SGD beyond strongly convexity and study general convex objectives. We identify a non-Gaussian (Gibbs) limiting law under the correct scaling, which is validated numerically, and provide a corresponding pre-limit Wasserstein error bound.

</details>


### [98] [KoopGen: Koopman Generator Networks for Representing and Predicting Dynamical Systems with Continuous Spectra](https://arxiv.org/abs/2602.14011)
*Liangyu Su,Jun Shu,Rui Liu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: KoopGen是一个基于生成器的神经Koopman框架，通过结构化、状态相关的Koopman生成器表示来建模高维时空混沌系统，分离保守传输和不可逆耗散，提高预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型在宽带或连续谱主导的混沌系统中缺乏稳定性、可解释性和可扩展性，而传统Koopman方法依赖有限维假设或显式谱参数化，在高维设置中效果不佳。

Method: 引入KoopGen框架，利用内在的笛卡尔分解将Koopman生成器分解为斜伴随和自伴随分量，在学习过程中强制执行精确的算子理论约束，分离保守传输和不可逆耗散。

Result: 在从非线性振荡器到高维混沌和时空动力学的各种系统中，KoopGen提高了预测准确性和稳定性，同时阐明了连续谱动力学的哪些组件具有可解释和可学习的表示。

Conclusion: KoopGen通过结构化生成器表示提供了一种有效的方法来处理高维混沌系统的建模挑战，在保持可解释性的同时提高了预测性能。

Abstract: Representing and predicting high-dimensional and spatiotemporally chaotic dynamical systems remains a fundamental challenge in dynamical systems and machine learning. Although data-driven models can achieve accurate short-term forecasts, they often lack stability, interpretability, and scalability in regimes dominated by broadband or continuous spectra. Koopman-based approaches provide a principled linear perspective on nonlinear dynamics, but existing methods rely on restrictive finite-dimensional assumptions or explicit spectral parameterizations that degrade in high-dimensional settings. Against these issues, we introduce KoopGen, a generator-based neural Koopman framework that models dynamics through a structured, state-dependent representation of Koopman generators. By exploiting the intrinsic Cartesian decomposition into skew-adjoint and self-adjoint components, KoopGen separates conservative transport from irreversible dissipation while enforcing exact operator-theoretic constraints during learning. Across systems ranging from nonlinear oscillators to high-dimensional chaotic and spatiotemporal dynamics, KoopGen improves prediction accuracy and stability, while clarifying which components of continuous-spectrum dynamics admit interpretable and learnable representations.

</details>


### [99] [S2SServiceBench: A Multimodal Benchmark for Last-Mile S2S Climate Services](https://arxiv.org/abs/2602.14017)
*Chenyue Li,Wen Deng,Zhuotao Sun,Mengxi Jin,Hanzhe Cui,Han Li,Shentong Li,Man Kit Yu,Ming Long Lai,Yuhao Yang,Mengqian Lu,Binhang Yuan*

Main category: cs.LG

TL;DR: S2SServiceBench：首个用于评估多模态大语言模型在次季节到季节（S2S）气候服务中"最后一公里"决策能力的基准，涵盖6个应用领域、10种服务产品、约500个任务。


<details>
  <summary>Details</summary>
Motivation: S2S预报对气候韧性和可持续发展至关重要，但存在"最后一公里"瓶颈：如何将科学预报转化为可信、可操作的气候服务，这需要可靠的多模态理解和不确定性下的决策推理。虽然多模态大语言模型在支持各种工作流方面进展迅速，但尚不清楚它们能否从业务服务产品中可靠地生成决策交付成果。

Method: 创建S2SServiceBench多模态基准，从一个业务气候服务系统中精心挑选，涵盖6个应用领域（农业、灾害、能源、金融、健康、航运），10种服务产品，约150+专家选择案例，每个案例在三个服务级别实例化，产生约500个任务和1000+评估项。

Result: 使用S2SServiceBench对最先进的多模态大语言模型和智能体进行基准测试，揭示了在S2S服务图理解和推理方面的持续挑战：可操作信号理解、将不确定性操作化为可执行交接、以及对动态灾害的稳定、基于证据的分析规划。

Conclusion: 该研究提供了构建未来气候服务智能体的可操作指导，强调了多模态大语言模型在气候服务"最后一公里"决策支持中的潜力和挑战，为改进气候服务交付系统指明了方向。

Abstract: Subseasonal-to-seasonal (S2S) forecasts play an essential role in providing a decision-critical weeks-to-months planning window for climate resilience and sustainability, yet a growing bottleneck is the last-mile gap: translating scientific forecasts into trusted, actionable climate services, requiring reliable multimodal understanding and decision-facing reasoning under uncertainty. Meanwhile, multimodal large language models (MLLMs) and corresponding agentic paradigms have made rapid progress in supporting various workflows, but it remains unclear whether they can reliably generate decision-making deliverables from operational service products (e.g., actionable signal comprehension, decision-making handoff, and decision analysis & planning) under uncertainty. We introduce S2SServiceBench, a multimodal benchmark for last-mile S2S climate services curated from an operational climate-service system to evaluate this capability. S2SServiceBenchcovers 10 service products with about 150+ expert-selected cases in total, spanning six application domains - Agriculture, Disasters, Energy, Finance, Health, and Shipping. Each case is instantiated at three service levels, yielding around 500 tasks and 1,000+ evaluation items across climate resilience and sustainability applications. Using S2SServiceBench, we benchmark state-of-the-art MLLMs and agents, and analyze performance across products and service levels, revealing persistent challenges in S2S service plot understanding and reasoning - namely, actionable signal comprehension, operationalizing uncertainty into executable handoffs, and stable, evidence-grounded analysis and planning for dynamic hazards-while offering actionable guidance for building future climate-service agents.

</details>


### [100] [EIDOS: Latent-Space Predictive Learning for Time Series Foundation Models](https://arxiv.org/abs/2602.14024)
*Xinxing Zhou,Qingren Yao,Yiji Zhao,Chenghao Liu,Flora Salim,Xiaojie Yuan,Yanlong Wen,Ming Jin*

Main category: cs.LG

TL;DR: EIDOS提出了一种新的时间序列基础模型预训练方法，从直接预测未来观测值转向潜在空间预测学习，通过预测潜在表示演化来获得更结构化、时间一致的表示。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型通常通过直接预测未来观测值进行预训练，这导致潜在表示捕获表面噪声而非连贯可预测的时间动态，产生弱结构化的表示。

Method: EIDOS采用因果Transformer预测潜在表示的演化，设计轻量级聚合分支构建目标表示，通过联合目标函数优化，包括潜在空间对齐、观测基础化和直接预测监督。

Result: 在GIFT-Eval基准测试中，EIDOS缓解了表示空间的结构碎片化问题，并实现了最先进的性能。

Conclusion: 约束模型学习可预测的潜在动态是构建更鲁棒可靠时间序列基础模型的原则性步骤。

Abstract: Most time series foundation models are pretrained by directly predicting future observations, which often yields weakly structured latent representations that capture surface noise rather than coherent and predictable temporal dynamics. In this work, we introduce EIDOS, a foundation model family that shifts pretraining from future value prediction to latent-space predictive learning. We train a causal Transformer to predict the evolution of latent representations, encouraging the emergence of structured and temporally coherent latent states. To ensure stable targets for latent-space learning, we design a lightweight aggregation branch to construct target representations. EIDOS is optimized via a joint objective that integrates latent-space alignment, observational grounding to anchor representations to the input signal, and direct forecasting supervision. On the GIFT-Eval benchmark, EIDOS mitigates structural fragmentation in the representation space and achieves state-of-the-art performance. These results demonstrate that constraining models to learn predictable latent dynamics is a principled step toward more robust and reliable time series foundation models.

</details>


### [101] [UniST-Pred: A Robust Unified Framework for Spatio-Temporal Traffic Forecasting in Transportation Networks Under Disruptions](https://arxiv.org/abs/2602.14049)
*Yue Wang,Areg Karapetyan,Djellel Difallah,Samer Madanat*

Main category: cs.LG

TL;DR: UniST-Pred是一个解耦时空建模的轻量级交通预测框架，通过自适应表示级融合实现鲁棒预测，在真实和模拟数据上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型通常紧密耦合时空建模，导致复杂度高、模块化差，且很少考虑实际部署中的结构和观测不确定性。需要一种既能处理不确定性又能保持轻量化的鲁棒预测框架。

Method: 提出UniST-Pred统一时空预测框架：1) 将时间建模与空间表示学习解耦；2) 通过自适应表示级融合集成两者；3) 使用基于MATSim的微观交通模拟器构建数据集评估网络断开等严重场景下的鲁棒性。

Result: 在标准交通预测数据集上，UniST-Pred与现有成熟模型相比具有竞争力；在模拟数据集上，即使在严重网络断开场景下也能保持强预测性能，并产生可解释的时空表示。

Conclusion: UniST-Pred通过解耦时空建模和自适应融合，实现了轻量化、鲁棒且可解释的交通预测，为智能交通系统提供了实用的解决方案。

Abstract: Spatio-temporal traffic forecasting is a core component of intelligent transportation systems, supporting various downstream tasks such as signal control and network-level traffic management. In real-world deployments, forecasting models must operate under structural and observational uncertainties, conditions that are rarely considered in model design. Recent approaches achieve strong short-term predictive performance by tightly coupling spatial and temporal modeling, often at the cost of increased complexity and limited modularity. In contrast, efficient time-series models capture long-range temporal dependencies without relying on explicit network structure. We propose UniST-Pred, a unified spatio-temporal forecasting framework that first decouples temporal modeling from spatial representation learning, then integrates both through adaptive representation-level fusion. To assess robustness of the proposed approach, we construct a dataset based on an agent-based, microscopic traffic simulator (MATSim) and evaluate UniST-Pred under severe network disconnection scenarios. Additionally, we benchmark UniST-Pred on standard traffic prediction datasets, demonstrating its competitive performance against existing well-established models despite a lightweight design. The results illustrate that UniST-Pred maintains strong predictive performance across both real-world and simulated datasets, while also yielding interpretable spatio-temporal representations under infrastructure disruptions. The source code and the generated dataset are available at https://anonymous.4open.science/r/UniST-Pred-EF27

</details>


### [102] [Position Encoding with Random Float Sampling Enhances Length Generalization of Transformers](https://arxiv.org/abs/2602.14050)
*Atsushi Shimizu,Shohei Taniguchi,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出Random Float Sampling (RFS)位置编码策略，通过随机采样连续值而非离散索引，解决语言模型长度泛化问题，可轻松集成到现有位置编码方法中。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在训练时未见长度上的泛化问题。传统位置编码使用预定义离散索引，导致在未见长度上出现分布外问题。

Method: 提出RFS位置编码策略：使用随机采样的连续值作为位置索引，而非预定义离散集。通过训练时暴露给模型多样化的索引，避免在未见长度上的OOD问题。该方法可轻松集成到绝对正弦编码、RoPE和ALiBi等现有位置编码方法中。

Result: 实验表明RFS在长度泛化任务上表现优异，同时在零样本常识推理基准测试中也取得良好性能。

Conclusion: RFS是一种简单而强大的位置编码策略，能有效提升语言模型在未见长度上的泛化能力，且易于集成到现有位置编码框架中。

Abstract: Length generalization is the ability of language models to maintain performance on inputs longer than those seen during pretraining. In this work, we introduce a simple yet powerful position encoding (PE) strategy, Random Float Sampling (RFS), that generalizes well to lengths unseen during pretraining or fine-tuning. In particular, instead of selecting position indices from a predefined discrete set, RFS uses randomly sampled continuous values, thereby avoiding out-of-distribution (OOD) issues on unseen lengths by exposing the model to diverse indices during training. Since assigning indices to tokens is a common and fundamental procedure in widely used PEs, the advantage of RFS can easily be incorporated into, for instance, the absolute sinusoidal encoding, RoPE, and ALiBi. Experiments corroborate its effectiveness by showing that RFS results in superior performance in length generalization tasks as well as zero-shot commonsense reasoning benchmarks.

</details>


### [103] [Policy Gradient with Adaptive Entropy Annealing for Continual Fine-Tuning](https://arxiv.org/abs/2602.14078)
*Yaqian Zhang,Bernhard Pfahringer,Eibe Frank,Albert Bifet*

Main category: cs.LG

TL;DR: 论文提出aEPG方法，通过强化学习视角直接最小化误分类误差，结合自适应熵退火策略，在参数高效微调中超越传统交叉熵损失


<details>
  <summary>Details</summary>
Motivation: 大型预训练视觉模型在类别增量学习中容易遭受灾难性遗忘。尽管参数高效微调通过限制可训练参数缓解了这一问题，但大多数方法仍依赖交叉熵损失，而交叉熵只是0-1损失的代理。本文重新审视这一选择，通过强化学习视角恢复真实目标（0-1损失）

Method: 将分类问题建模为一步马尔可夫决策过程，推导出期望策略梯度方法直接最小化误分类误差。分析发现交叉熵可解释为EPG加上额外的样本加权机制：交叉熵强调低置信度样本鼓励探索，而EPG优先处理高置信度样本。基于此提出自适应熵退火策略，从探索性（类似交叉熵）过渡到利用性（类似EPG）学习

Result: aEPG方法在多种基准测试和不同PEFT模块中均优于基于交叉熵的方法。更广泛地评估了各种熵正则化方法，证明输出预测分布的较低熵能增强预训练视觉模型的适应能力

Conclusion: 通过强化学习视角直接优化0-1损失是有效的，自适应熵退火策略结合了探索和利用的优势，在参数高效微调中取得了更好的性能，较低熵的预测分布有利于预训练模型的适应

Abstract: Despite their success, large pretrained vision models remain vulnerable to catastrophic forgetting when adapted to new tasks in class-incremental settings. Parameter-efficient fine-tuning (PEFT) alleviates this by restricting trainable parameters, yet most approaches still rely on cross-entropy (CE) loss, a surrogate for the 0-1 loss, to learn from new data. We revisit this choice and revive the true objective (0-1 loss) through a reinforcement learning perspective. By formulating classification as a one-step Markov Decision Process, we derive an Expected Policy Gradient (EPG) method that directly minimizes misclassification error with a low-variance gradient estimation. Our analysis shows that CE can be interpreted as EPG with an additional sample-weighting mechanism: CE encourages exploration by emphasizing low-confidence samples, while EPG prioritizes high-confidence ones. Building on this insight, we propose adaptive entropy annealing (aEPG), a training strategy that transitions from exploratory (CE-like) to exploitative (EPG-like) learning. aEPG-based methods outperform CE-based methods across diverse benchmarks and with various PEFT modules. More broadly, we evaluate various entropy regularization methods and demonstrate that lower entropy of the output prediction distribution enhances adaptation in pretrained vision models.

</details>


### [104] [Neural Optimal Transport in Hilbert Spaces: Characterizing Spurious Solutions and Gaussian Smoothing](https://arxiv.org/abs/2602.14086)
*Jae-Hwan Choi,Jiwoo Yoon,Dohyun Kwon,Jaewoong Choi*

Main category: cs.LG

TL;DR: 该论文研究无限维希尔伯特空间中的神经最优传输，通过高斯平滑策略解决半对偶神经OT在非正则设置下产生的伪解问题，证明了在正则源测度下该公式是适定的并能恢复唯一的Monge映射。


<details>
  <summary>Details</summary>
Motivation: 在无限维希尔伯特空间中，半对偶神经最优传输在非正则设置下经常产生伪解，这些伪解无法准确捕捉目标分布，需要解决这一适定性问题。

Method: 扩展半对偶框架，采用基于布朗运动的高斯平滑策略，通过正则测度框架分析伪解问题，并证明在正则源测度下该公式的适定性。

Result: 理论证明在正则源测度下，该公式是适定的并能恢复唯一的Monge映射；建立了平滑测度正则性的尖锐特征化；在合成函数数据和时序数据集上的实验表明，该方法有效抑制伪解并优于现有基线。

Conclusion: 通过高斯平滑策略扩展的半对偶神经OT框架成功解决了无限维希尔伯特空间中伪解问题，为函数数据的最优传输提供了理论保证和实用方法。

Abstract: We study Neural Optimal Transport in infinite-dimensional Hilbert spaces. In non-regular settings, Semi-dual Neural OT often generates spurious solutions that fail to accurately capture target distributions. We analytically characterize this spurious solution problem using the framework of regular measures, which generalize Lebesgue absolute continuity in finite dimensions. To resolve ill-posedness, we extend the semi-dual framework via a Gaussian smoothing strategy based on Brownian motion. Our primary theoretical contribution proves that under a regular source measure, the formulation is well-posed and recovers a unique Monge map. Furthermore, we establish a sharp characterization for the regularity of smoothed measures, proving that the success of smoothing depends strictly on the kernel of the covariance operator. Empirical results on synthetic functional data and time-series datasets demonstrate that our approach effectively suppresses spurious solutions and outperforms existing baselines.

</details>


### [105] [Geometry-Aware Physics-Informed PointNets for Modeling Flows Across Porous Structures](https://arxiv.org/abs/2602.14108)
*Luigi Ciceri,Corrado Mio,Jianyi Lin,Gabriele Gianini*

Main category: cs.LG

TL;DR: 该研究提出了两种物理信息学习方法（PIPN和PI-GANO）来预测通过和围绕多孔体的流动，通过统一损失函数结合自由流动区的纳维-斯托克斯方程和多孔区的达西-福希海默扩展，在2D管道和3D防风场景中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 预测通过和围绕多孔体的流动具有挑战性，因为需要在流体和多孔区域之间耦合物理，并且需要泛化到不同的几何形状和边界条件。传统方法难以处理这种复杂问题。

Method: 提出了两种物理信息学习方法：物理信息点网络（PIPN）和物理信息几何感知神经算子（PI-GANO）。在统一损失函数中强制执行自由流动区的不可压缩纳维-斯托克斯方程和多孔区的达西-福希海默扩展，并将网络条件化于几何和材料参数。使用OpenFOAM生成2D管道（含多孔障碍物）和3D防风场景（含树冠和建筑物）的数据集。

Result: 通过制造解方法验证了管道，评估了对未见形状的泛化能力，对于PI-GANO还评估了对可变边界条件和参数设置的泛化。结果显示在已见和未见情况下速度和压力误差均较低，能准确再现尾流结构。性能下降主要发生在尖锐界面附近和大梯度区域。

Conclusion: 该研究首次系统评估了PIPN/PI-GANO用于同时处理通过和围绕多孔体的流动，展示了它们在不需为每个几何形状重新训练的情况下加速设计研究的潜力。

Abstract: Predicting flows that occur both through and around porous bodies is challenging due to coupled physics across fluid and porous regions and the need to generalize across diverse geometries and boundary conditions. We address this problem using two Physics Informed learning approaches: Physics Informed PointNets (PIPN) and Physics Informed Geometry Aware Neural Operator (P-IGANO). We enforce the incompressible Navier Stokes equations in the free-flow region and a Darcy Forchheimer extension in the porous region within a unified loss and condition the networks on geometry and material parameters. Datasets are generated with OpenFOAM on 2D ducts containing porous obstacles and on 3D windbreak scenarios with tree canopies and buildings. We first verify the pipeline via the method of manufactured solutions, then assess generalization to unseen shapes, and for PI-GANO, to variable boundary conditions and parameter settings. The results show consistently low velocity and pressure errors in both seen and unseen cases, with accurate reproduction of the wake structures. Performance degrades primarily near sharp interfaces and in regions with large gradients. Overall, the study provides a first systematic evaluation of PIPN/PI-GANO for simultaneous through-and-around porous flows and shows their potential to accelerate design studies without retraining per geometry.

</details>


### [106] [Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?](https://arxiv.org/abs/2602.14111)
*Anton Korznikov,Andrey Galichin,Alexey Dontsov,Oleg Rogov,Ivan Oseledets,Elena Tutubalina*

Main category: cs.LG

TL;DR: SAEs在解释神经网络方面表现不佳，即使重建效果好也只恢复9%真实特征，随机基线与其性能相当，表明当前SAEs无法可靠分解模型内部机制。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏自编码器（SAEs）被认为是解释神经网络的有前途工具，但越来越多的下游任务负面结果让人质疑SAEs是否真的能恢复有意义的特征。

Method: 采用两种互补评估方法：1）在已知真实特征的合成设置中测试SAEs；2）在真实激活上引入三个基线，约束SAE特征方向或激活模式为随机值，并与完全训练的SAEs进行比较。

Result: 在合成设置中，SAEs仅恢复9%真实特征，尽管达到71%的解释方差；在真实激活上，随机基线在可解释性（0.87 vs 0.90）、稀疏探测（0.69 vs 0.72）和因果编辑（0.73 vs 0.72）方面与完全训练的SAEs表现相当。

Conclusion: 当前状态的SAEs无法可靠地分解模型的内部机制，需要重新评估和改进SAEs的设计与训练方法。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only $9\%$ of true features despite achieving $71\%$ explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.

</details>


### [107] [ROAST: Rollout-based On-distribution Activation Steering Technique](https://arxiv.org/abs/2602.14143)
*Xuanbo Su,Hao Luo,Yingfang Zhang,Lijun Zhang*

Main category: cs.LG

TL;DR: ROAST是一种基于模型自身分布内rollout的激活导向技术，通过连续软缩放和分组均值归一化来避免离散掩码，在各种模型规模上提升任务性能


<details>
  <summary>Details</summary>
Motivation: 现有的激活导向方法依赖分布外监督和离散掩码，导致干预脆弱，需要更鲁棒的分布内激活导向技术

Method: ROAST通过ROC从模型自身分布内rollout估计导向方向，采用连续软缩放避免硬稀疏化，使用分组均值归一化平衡样本贡献

Result: 在0.6B到32B规模的各种模型上，ROAST在多样化任务上持续提升性能（如Qwen3-0.6B在GSM8K上提升9.7%，GLM4-32B在TruthfulQA上提升12.1%）

Conclusion: ROAST通过分布内rollout和适当的归一化技术，实现了更鲁棒的激活导向，避免了高幅度激活对全局导向方向的过度影响

Abstract: Activation steering provides parameter-efficient control over large language models (LLMs) at inference time, but many methods rely on off-distribution supervision and discrete masking, leading to brittle interventions. We propose ROAST (Rollout-based On-distribution Activation Steering Technique), which estimates steering directions from the model's own on-distribution rollouts via ROC and avoids hard sparsification via Continuous Soft Scaling (CSS) and Grouped Mean Normalization. Our empirical analysis reveals that while activation magnitude correlates moderately with directional consistency, the variance in magnitude is significant and often disproportionate to semantic quality. This suggests that high-magnitude activations risk dominating the global steering direction if not properly normalized. To address this, ROAST employs grouped normalization to balance contributions across samples, ensuring a more robust estimation of the consensus steering direction. Across models (0.6B to 32B), ROAST consistently improves performance on diverse tasks (e.g., +9.7% on GSM8K for Qwen3-0.6B and +12.1% on TruthfulQA for GLM4-32B), and analyses show that CSS better preserves activation energy.

</details>


### [108] [A Penalty Approach for Differentiation Through Black-Box Quadratic Programming Solvers](https://arxiv.org/abs/2602.14154)
*Yuxuan Linghu,Zhiyuan Liu,Qi Deng*

Main category: cs.LG

TL;DR: dXPP：一种基于惩罚的二次规划可微框架，通过解耦求解与微分过程，使用任意黑盒QP求解器，在反向传播中通过平滑近似惩罚问题进行隐式微分，显著提升大规模问题的计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于KKT系统的可微优化方法在大规模问题上存在计算成本高和数值鲁棒性下降的问题，需要一种更高效、更稳健的可微二次规划框架。

Method: 提出dXPP惩罚框架：前向传播使用任意黑盒QP求解器；反向传播将解映射到平滑近似惩罚问题，通过隐式微分仅需求解更小的原始变量线性系统，避免显式KKT微分。

Result: 在随机生成QP、大规模稀疏投影问题和多周期投资组合优化等任务上，dXPP与基于KKT的方法竞争，并在大规模问题上实现显著加速。

Conclusion: dXPP通过解耦求解与微分、使用惩罚方法避免KKT系统，为大规模可微二次规划提供了高效、稳健的解决方案。

Abstract: Differentiating through the solution of a quadratic program (QP) is a central problem in differentiable optimization. Most existing approaches differentiate through the Karush--Kuhn--Tucker (KKT) system, but their computational cost and numerical robustness can degrade at scale. To address these limitations, we propose dXPP, a penalty-based differentiation framework that decouples QP solving from differentiation. In the solving step (forward pass), dXPP is solver-agnostic and can leverage any black-box QP solver. In the differentiation step (backward pass), we map the solution to a smooth approximate penalty problem and implicitly differentiate through it, requiring only the solution of a much smaller linear system in the primal variables. This approach bypasses the difficulties inherent in explicit KKT differentiation and significantly improves computational efficiency and robustness. We evaluate dXPP on various tasks, including randomly generated QPs, large-scale sparse projection problems, and a real-world multi-period portfolio optimization task. Empirical results demonstrate that dXPP is competitive with KKT-based differentiation methods and achieves substantial speedups on large-scale problems.

</details>


### [109] [Synergistic Intra- and Cross-Layer Regularization Losses for MoE Expert Specialization](https://arxiv.org/abs/2602.14159)
*Rizhen Hu,Yuan Cao,Boao Kong,Mou Sun,Kun Yuan*

Main category: cs.LG

TL;DR: 提出两种即插即用的正则化损失函数，增强稀疏MoE模型的专家专业化和路由效率，无需修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE模型存在专家重叠问题（专家间冗余表示和路由模糊性），导致模型容量严重未充分利用。现有架构解决方案需要大量结构修改且仅依赖层内信号。

Method: 提出两种正则化损失：1）层内专业化损失，惩罚相同token上专家SwiGLU激活的余弦相似度，鼓励专家专业化互补知识；2）跨层耦合损失，最大化相邻层间联合Top-k路由概率，建立连贯的专家路径。

Result: 在预训练、微调和零样本基准测试中展示了一致的任务增益、更高的专家专业化程度和更低熵的路由，这些改进通过更稳定的专家路径实现了更快的推理速度。

Conclusion: 提出的两种即插即用正则化损失有效解决了MoE模型的专家重叠问题，提高了模型效率和性能，且与现有架构兼容。

Abstract: Sparse Mixture-of-Experts (MoE) models scale Transformers efficiently but suffer from expert overlap -- redundant representations across experts and routing ambiguity, resulting in severely underutilized model capacity. While architectural solutions like DeepSeekMoE promote specialization, they require substantial structural modifications and rely solely on intra-layer signals. In this paper, we propose two plug-and-play regularization losses that enhance MoE specialization and routing efficiency without modifying router or model architectures. First, an intra-layer specialization loss penalizes cosine similarity between experts' SwiGLU activations on identical tokens, encouraging experts to specialize in complementary knowledge. Second, a cross-layer coupling loss maximizes joint Top-$k$ routing probabilities across adjacent layers, establishing coherent expert pathways through network depth while reinforcing intra-layer expert specialization. Both losses are orthogonal to the standard load-balancing loss and compatible with both the shared-expert architecture in DeepSeekMoE and vanilla top-$k$ MoE architectures. We implement both losses as a drop-in Megatron-LM module. Extensive experiments across pre-training, fine-tuning, and zero-shot benchmarks demonstrate consistent task gains, higher expert specialization, and lower-entropy routing; together, these improvements translate into faster inference via more stable expert pathways.

</details>


### [110] [When Benchmarks Lie: Evaluating Malicious Prompt Classifiers Under True Distribution Shift](https://arxiv.org/abs/2602.14161)
*Max Fomin*

Main category: cs.LG

TL;DR: 论文提出Leave-One-Dataset-Out (LODO)评估方法，揭示传统评估高估性能8.4% AUC，发现28%特征为数据集依赖的捷径特征，现有防护系统对间接攻击检测率仅7-37%


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理处理越来越多来自电子邮件、文档、工具输出和外部API的不受信任数据，检测提示注入和越狱攻击变得至关重要。然而当前评估实践和生产系统存在根本性限制，需要更可靠的检测方法。

Method: 1) 使用包含18个数据集的多样化基准测试；2) 提出Leave-One-Dataset-Out (LODO)评估方法测量真实分布外泛化能力；3) 分析稀疏自编码器(SAE)特征系数；4) 系统比较生产级防护系统(PromptGuard 2, LlamaGuard)和LLM-as-judge方法

Result: 1) 传统训练-测试分割严重高估性能：聚合指标显示8.4个百分点的AUC膨胀，每数据集差距从1%到25%准确率；2) 28%的顶级特征是数据集依赖的捷径特征；3) 现有防护系统对针对代理的间接攻击检测率仅7-37%；4) PromptGuard 2和LlamaGuard因架构限制无法评估代理工具注入

Conclusion: LODO评估协议是提示攻击检测研究的适当方法，能够揭示真实泛化能力并过滤数据集伪影。LODO稳定的SAE特征提供更可靠的分类器决策解释。需要开发能够处理间接攻击和代理工具注入的更强健防护系统。

Abstract: Detecting prompt injection and jailbreak attacks is critical for deploying LLM-based agents safely. As agents increasingly process untrusted data from emails, documents, tool outputs, and external APIs, robust attack detection becomes essential. Yet current evaluation practices and production systems have fundamental limitations. We present a comprehensive analysis using a diverse benchmark of 18 datasets spanning harmful requests, jailbreaks, indirect prompt injections, and extraction attacks. We propose Leave-One-Dataset-Out (LODO) evaluation to measure true out-of-distribution generalization, revealing that the standard practice of train-test splits from the same dataset sources severely overestimates performance: aggregate metrics show an 8.4 percentage point AUC inflation, but per-dataset gaps range from 1% to 25% accuracy-exposing heterogeneous failure modes. To understand why classifiers fail to generalize, we analyze Sparse Auto-Encoder (SAE) feature coefficients across LODO folds, finding that 28% of top features are dataset-dependent shortcuts whose class signal depends on specific dataset compositions rather than semantic content. We systematically compare production guardrails (PromptGuard 2, LlamaGuard) and LLM-as-judge approaches on our benchmark, finding all three fail on indirect attacks targeting agents (7-37% detection) and that PromptGuard 2 and LlamaGuard cannot evaluate agentic tool injection due to architectural limitations. Finally, we show that LODO-stable SAE features provide more reliable explanations for classifier decisions by filtering dataset artifacts. We release our evaluation framework at https://github.com/maxf-zn/prompt-mining to establish LODO as the appropriate protocol for prompt attack detection research.

</details>


### [111] [Deep Dense Exploration for LLM Reinforcement Learning via Pivot-Driven Resampling](https://arxiv.org/abs/2602.14169)
*Yiran Guo,Zhongjian Qiao,Yingqi Xie,Jie Liu,Dan Ye,Ruiqing Zhang,Shuang Qiu,Lijie Xu*

Main category: cs.LG

TL;DR: 提出Deep Dense Exploration (DDE)方法，专注于在失败轨迹中的深度可恢复状态（pivots）进行探索，通过DEEP-GRPO实现，在数学推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大型语言模型探索中存在局限：GRPO仅从根节点采样，导致高概率轨迹饱和而深度易错状态探索不足；树方法盲目分散采样预算，无法发现罕见正确后缀且破坏局部基线。

Method: 提出Deep Dense Exploration (DDE)策略，专注于失败轨迹中的pivot状态（深度可恢复状态）。通过DEEP-GRPO实现三个创新：1) 轻量级数据驱动的效用函数平衡可恢复性和深度偏差识别pivot；2) 在每个pivot进行局部密集重采样以增加发现正确后续轨迹概率；3) 双流优化目标将全局策略学习与局部纠正更新解耦。

Result: 在数学推理基准测试中，该方法一致优于GRPO、树方法和其他强基线。

Conclusion: DDE通过专注于失败轨迹中的深度可恢复状态进行密集探索，解决了现有强化学习方法在大型语言模型探索中的局限性，显著提升了性能。

Abstract: Effective exploration is a key challenge in reinforcement learning for large language models: discovering high-quality trajectories within a limited sampling budget from the vast natural language sequence space. Existing methods face notable limitations: GRPO samples exclusively from the root, saturating high-probability trajectories while leaving deep, error-prone states under-explored. Tree-based methods blindly disperse budgets across trivial or unrecoverable states, causing sampling dilution that fails to uncover rare correct suffixes and destabilizes local baselines. To address this, we propose Deep Dense Exploration (DDE), a strategy that focuses exploration on $\textit{pivots}$-deep, recoverable states within unsuccessful trajectories. We instantiate DDE with DEEP-GRPO, which introduces three key innovations: (1) a lightweight data-driven utility function that automatically balances recoverability and depth bias to identify pivot states; (2) local dense resampling at each pivot to increase the probability of discovering correct subsequent trajectories; and (3) a dual-stream optimization objective that decouples global policy learning from local corrective updates. Experiments on mathematical reasoning benchmarks demonstrate that our method consistently outperforms GRPO, tree-based methods, and other strong baselines.

</details>


### [112] [TS-Haystack: A Multi-Scale Retrieval Benchmark for Time Series Language Models](https://arxiv.org/abs/2602.14200)
*Nicolas Zumarraga,Thomas Kaar,Ning Wang,Maxwell A. Xu,Max Rosenblattl,Markus Kreft,Kevin O'Sullivan,Paul Schmiedmayer,Patrick Langer,Robert Jakob*

Main category: cs.LG

TL;DR: TS-Haystack是一个长上下文时间序列检索基准，用于评估时间序列语言模型在长序列中的表现，发现现有模型在压缩时会丢失时间局部信息，导致检索性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列语言模型通常在短序列上训练和评估，而真实世界的时间序列传感器数据可能包含数百万个数据点。当前基准无法捕捉在严格计算约束下进行精确时间定位的需求。

Method: 引入TS-Haystack基准，包含10种任务类型，分为4个类别：直接检索、时间推理、多步推理和上下文异常检测。通过将短活动片段嵌入到更长的纵向加速度计记录中，实现从秒到2小时不同上下文长度的系统评估。

Result: 学习到的潜在压缩在压缩比高达176倍时能保持或提高分类准确率，但检索性能随上下文长度增加而下降，导致时间局部信息丢失。分类和检索行为存在一致的分歧。

Conclusion: 需要设计能够解耦序列长度与计算复杂度同时保持时间保真度的架构。时间序列编码器在增加上下文长度时忽视了时间粒度，产生任务依赖性影响：压缩有助于分类但损害局部事件检索。

Abstract: Time Series Language Models (TSLMs) are emerging as unified models for reasoning over continuous signals in natural language. However, long-context retrieval remains a major limitation: existing models are typically trained and evaluated on short sequences, while real-world time-series sensor streams can span millions of datapoints. This mismatch requires precise temporal localization under strict computational constraints, a regime that is not captured by current benchmarks. We introduce TS-Haystack, a long-context temporal retrieval benchmark comprising ten task types across four categories: direct retrieval, temporal reasoning, multi-step reasoning and contextual anomaly. The benchmark uses controlled needle insertion by embedding short activity bouts into longer longitudinal accelerometer recordings, enabling systematic evaluation across context lengths ranging from seconds to 2 hours per sample. We hypothesize that existing TSLM time series encoders overlook temporal granularity as context length increases, creating a task-dependent effect: compression aids classification but impairs retrieval of localized events. Across multiple model and encoding strategies, we observe a consistent divergence between classification and retrieval behavior. Learned latent compression preserves or improves classification accuracy at compression ratios up to 176$\times$, but retrieval performance degrades with context length, incurring in the loss of temporally localized information. These results highlight the importance of architectural designs that decouple sequence length from computational complexity while preserving temporal fidelity.

</details>


### [113] [MAGE: All-[MASK] Block Already Knows Where to Look in Diffusion LLM](https://arxiv.org/abs/2602.14209)
*Omin Kwon,Yeonjae Kim,Doyeon Kim,Minseo Kim,Yeonhong Park,Jae W. Lee*

Main category: cs.LG

TL;DR: MAGE提出了一种针对块扩散LLM的KV缓存优化方法，利用首个去噪步骤的注意力模式预测重要KV条目，实现训练自由的稀疏去噪，显著提升长上下文生成速度。


<details>
  <summary>Details</summary>
Motivation: 块扩散LLM在长上下文生成中面临KV缓存内存访问瓶颈，现有为自回归LLM设计的动态稀疏注意力方法在块扩散场景下表现不佳，需要专门针对块扩散特性的优化方案。

Method: MAGE利用块扩散中首个All-[MASK]去噪步骤的注意力模式可靠预测重要KV条目和预算需求，每个块只需执行一次精确注意力计算，然后重用于训练自由的稀疏去噪过程。

Result: 在LongBench和Needle-in-a-Haystack等长上下文基准测试中，MAGE以少量KV预算实现接近无损的准确率，端到端速度提升3-4倍，显著优于自回归导向的稀疏注意力基线。

Conclusion: MAGE通过利用块扩散特有的注意力模式预测能力，有效解决了KV缓存瓶颈问题，轻量级微调策略进一步强化了[MASK]引导模式，为块扩散LLM的长上下文生成提供了高效解决方案。

Abstract: Block diffusion LLMs are emerging as a promising next paradigm for language generation, but their use of KV caching makes memory access a dominant bottleneck in long-context settings. While dynamic sparse attention has been actively explored, existing methods designed for autoregressive LLMs rely on approximate importance estimation and perform poorly when adapted to block diffusion. This work identifies a key opportunity unique to block diffusion: attention at the first All-[MASK] denoising step reliably predicts important KV entries and budget requirements, enabling MAGE to perform a single exact attention pass per block and reuse it for training-free sparse denoising. Across long-context benchmarks including LongBench and Needle-in-a-Haystack, MAGE achieves near-lossless accuracy with a fraction of the KV budget while delivering up to 3-4x end-to-end speedup, consistently outperforming AR-oriented sparse attention baselines. A lightweight fine-tuning strategy further strengthens [MASK]-guided patterns with minimal cost, requiring only a few hours of training on a single NVIDIA H100 GPU for both 1.5B and 7B models.

</details>


### [114] [Robust multi-task boosting using clustering and local ensembling](https://arxiv.org/abs/2602.14231)
*Seyedsaman Emami,Daniel Hernández-Lobato,Gonzalo Martínez-Muñoz*

Main category: cs.LG

TL;DR: RMB-CLE：一种通过误差驱动的任务聚类和局部集成实现鲁棒多任务学习的框架，有效防止负迁移


<details>
  <summary>Details</summary>
Motivation: 传统多任务学习方法在任务不相关或存在噪声时容易发生负迁移，强制共享表示会损害性能。需要一种能够自适应识别相关任务并鲁棒共享信息的框架。

Method: 提出RMB-CLE框架：1）基于跨任务误差计算任务间相似度，将风险分解为函数不匹配和不可约噪声；2）使用凝聚聚类自适应分组任务；3）在每个聚类内使用局部集成实现鲁棒知识共享，同时保留任务特定模式。

Result: 在合成数据中能恢复真实聚类，在多样化的真实世界和合成基准测试中持续优于多任务、单任务和基于池化的集成方法。

Conclusion: RMB-CLE不仅是聚类和集成的组合，而是一个通用、可扩展的框架，为鲁棒多任务学习建立了新的基础，通过误差驱动的任务聚类有效防止负迁移。

Abstract: Multi-Task Learning (MTL) aims to boost predictive performance by sharing information across related tasks, yet conventional methods often suffer from negative transfer when unrelated or noisy tasks are forced to share representations. We propose Robust Multi-Task Boosting using Clustering and Local Ensembling (RMB-CLE), a principled MTL framework that integrates error-based task clustering with local ensembling. Unlike prior work that assumes fixed clusters or hand-crafted similarity metrics, RMB-CLE derives inter-task similarity directly from cross-task errors, which admit a risk decomposition into functional mismatch and irreducible noise, providing a theoretically grounded mechanism to prevent negative transfer. Tasks are grouped adaptively via agglomerative clustering, and within each cluster, a local ensemble enables robust knowledge sharing while preserving task-specific patterns. Experiments show that RMB-CLE recovers ground-truth clusters in synthetic data and consistently outperforms multi-task, single-task, and pooling-based ensemble methods across diverse real-world and synthetic benchmarks. These results demonstrate that RMB-CLE is not merely a combination of clustering and boosting but a general and scalable framework that establishes a new basis for robust multi-task learning.

</details>


### [115] [Evaluating LLMs in Finance Requires Explicit Bias Consideration](https://arxiv.org/abs/2602.14233)
*Yaxuan Kong,Hoyoung Lee,Yoontae Hwang,Alejandro Lopez-Lira,Bradford Levy,Dhagash Mehta,Qingsong Wen,Chanyeol Choi,Yongjae Lee,Stefan Zohren*

Main category: cs.LG

TL;DR: 该论文指出金融LLM应用中存在五种常见偏见（前瞻性偏见、幸存者偏见、叙事偏见、目标偏见和成本偏见），这些偏见会夸大性能表现并导致无效的部署声明。作者提出了结构有效性框架和评估清单来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到金融工作流程中，评估实践未能跟上发展。金融特定的偏见会夸大性能表现、污染回测结果，并使报告的结果对于任何部署声明都无效。目前缺乏对这些偏见的系统性关注。

Method: 识别了金融LLM应用中的五种常见偏见，回顾了2023-2025年的164篇论文，分析了偏见讨论情况，提出了结构有效性框架和评估清单，要求偏见诊断和系统设计的最低要求。

Result: 研究发现，在164篇论文中，没有任何单一偏见在超过28%的研究中被讨论。偏见经常相互叠加，造成有效性的错觉。提出了具体的缓解偏见的方法和工具。

Conclusion: 金融LLM系统中的偏见需要明确关注，在支持部署声明之前必须强制执行结构有效性。提出的框架和清单为偏见诊断和未来系统设计提供了最低要求。

Abstract: Large Language Models (LLMs) are increasingly integrated into financial workflows, but evaluation practice has not kept up. Finance-specific biases can inflate performance, contaminate backtests, and make reported results useless for any deployment claim. We identify five recurring biases in financial LLM applications. They include look-ahead bias, survivorship bias, narrative bias, objective bias, and cost bias. These biases break financial tasks in distinct ways and they often compound to create an illusion of validity. We reviewed 164 papers from 2023 to 2025 and found that no single bias is discussed in more than 28 percent of studies. This position paper argues that bias in financial LLM systems requires explicit attention and that structural validity should be enforced before any result is used to support a deployment claim. We propose a Structural Validity Framework and an evaluation checklist with minimal requirements for bias diagnosis and future system design. The material is available at https://github.com/Eleanorkong/Awesome-Financial-LLM-Bias-Mitigation.

</details>


### [116] [Multi-Agent Debate: A Unified Agentic Framework for Tabular Anomaly Detection](https://arxiv.org/abs/2602.14251)
*Pinqiao Wang,Sheng Li*

Main category: cs.LG

TL;DR: 提出MAD框架，通过多智能体辩论机制处理表格异常检测中不同模型的异质性分歧，将分歧作为重要信号，通过数学协调层整合各智能体的异常评分、置信度和证据，生成最终异常分数和可审计的辩论轨迹。


<details>
  <summary>Details</summary>
Motivation: 表格异常检测通常使用单一检测器或静态集成方法，但表格数据的强性能通常来自异构模型族（如树集成、深度表格网络、表格基础模型），这些模型在分布偏移、缺失值和罕见异常情况下经常产生分歧。现有方法未能充分利用这种分歧信号。

Method: 提出MAD多智能体辩论框架：每个智能体是基于ML的检测器，输出归一化异常分数、置信度和结构化证据，并由LLM批评器增强；协调器将这些信息转换为有界的每智能体损失，通过指数梯度规则更新智能体影响力，生成最终辩论异常分数和可审计的辩论轨迹。

Result: 建立了合成损失的遗憾保证，展示了如何通过保形校准包装辩论分数以在可交换性下控制误报；在多样化表格异常基准测试中显示出比基线更好的鲁棒性和更清晰的模型分歧轨迹。

Conclusion: MAD是一个统一的智能体框架，能够恢复现有方法（如专家混合门控和学习专家建议聚合），通过限制消息空间和合成操作符；该框架将模型分歧作为一等信号处理，通过数学协调层解决分歧，提高了表格异常检测的鲁棒性和可解释性。

Abstract: Tabular anomaly detection is often handled by single detectors or static ensembles, even though strong performance on tabular data typically comes from heterogeneous model families (e.g., tree ensembles, deep tabular networks, and tabular foundation models) that frequently disagree under distribution shift, missingness, and rare-anomaly regimes. We propose MAD, a Multi-Agent Debating framework that treats this disagreement as a first-class signal and resolves it through a mathematically grounded coordination layer. Each agent is a machine learning (ML)-based detector that produces a normalized anomaly score, confidence, and structured evidence, augmented by a large language model (LLM)-based critic. A coordinator converts these messages into bounded per-agent losses and updates agent influence via an exponentiated-gradient rule, yielding both a final debated anomaly score and an auditable debate trace. MAD is a unified agentic framework that can recover existing approaches, such as mixture-of-experts gating and learning-with-expert-advice aggregation, by restricting the message space and synthesis operator. We establish regret guarantees for the synthesized losses and show how conformal calibration can wrap the debated score to control false positives under exchangeability. Experiments on diverse tabular anomaly benchmarks show improved robustness over baselines and clearer traces of model disagreement

</details>


### [117] [Cross-household Transfer Learning Approach with LSTM-based Demand Forecasting](https://arxiv.org/abs/2602.14267)
*Manal Rahal,Bestoun S. Ahmed,Roger Renström,Robert Stener*

Main category: cs.LG

TL;DR: DELTAiF是一个基于迁移学习的框架，用于预测家庭热水消耗，通过从代表性家庭学习知识并微调到其他家庭，实现可扩展的热水需求预测，减少67%的训练时间同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 随着住宅热泵安装的快速增加，优化家庭热水生产至关重要，但面临技术和可扩展性挑战。传统方法为每个家庭单独训练机器学习模型计算成本高，特别是在云连接热泵部署中。

Method: DELTAiF采用迁移学习框架，从代表性家庭学习知识，然后微调到其他家庭，避免为每个热泵安装单独训练模型。通过预测大量热水使用事件（如淋浴）来实现自适应且可扩展的热水生产。

Result: 该方法减少约67%的训练时间，同时保持高预测精度（0.874-0.991），平均绝对百分比误差在0.001-0.017之间。当源家庭表现出规律消费模式时，迁移学习特别有效。

Conclusion: DELTAiF框架证明迁移学习能够实现可扩展的家庭热水需求预测，减少计算成本同时保持高精度，特别适用于具有规律消费模式的家庭。

Abstract: With the rapid increase in residential heat pump (HP) installations, optimizing hot water production in households is essential, yet it faces major technical and scalability challenges. Adapting production to actual household needs requires accurate forecasting of hot water demand to ensure comfort and, most importantly, to reduce energy waste. However, the conventional approach of training separate machine learning models for each household becomes computationally expensive at scale, particularly in cloud-connected HP deployments.
  This study introduces DELTAiF, a transfer learning (TL) based framework that provides scalable and accurate prediction of household hot water consumption. By predicting large hot water usage events, such as showers, DELTAiF enables adaptive yet scalable hot water production at the household level. DELTAiF leverages learned knowledge from a representative household and fine-tunes it across others, eliminating the need to train separate machine learning models for each HP installation. This approach reduces overall training time by approximately 67 percent while maintaining high predictive accuracy values between 0.874 and 0.991, and mean absolute percentage error values between 0.001 and 0.017. The results show that TL is particularly effective when the source household exhibits regular consumption patterns, enabling hot water demand forecasting at scale.

</details>


### [118] [Radial-VCReg: More Informative Representation Learning Through Radial Gaussianization](https://arxiv.org/abs/2602.14272)
*Yilun Kuang,Yash Dagade,Deep Chakraborty,Erik Learned-Miller,Randall Balestriero,Tim G. J. Rudner,Yann LeCun*

Main category: cs.LG

TL;DR: Radial-VCReg通过添加径向高斯化损失来增强VCReg，将特征范数与卡方分布对齐，从而减少高阶依赖性并学习更多样化和信息丰富的表示。


<details>
  <summary>Details</summary>
Motivation: 自监督学习旨在学习最大信息表示，但显式信息最大化受到维度诅咒的阻碍。现有方法如VCReg通过正则化一阶和二阶特征统计量无法完全实现最大熵。

Method: 提出Radial-VCReg，在VCReg基础上添加径向高斯化损失，将特征范数与卡方分布对齐——这是高维高斯分布的一个定义性特征。

Result: 证明Radial-VCReg比VCReg能将更广泛的分布类别转化为正态分布；在合成和真实数据集上，它通过减少高阶依赖性和促进更多样化和信息丰富的表示，持续提高性能。

Conclusion: 径向高斯化损失是增强自监督学习表示质量的有效方法，通过更好地逼近最大熵分布来克服维度诅咒的限制。

Abstract: Self-supervised learning aims to learn maximally informative representations, but explicit information maximization is hindered by the curse of dimensionality. Existing methods like VCReg address this by regularizing first and second-order feature statistics, which cannot fully achieve maximum entropy. We propose Radial-VCReg, which augments VCReg with a radial Gaussianization loss that aligns feature norms with the Chi distribution-a defining property of high-dimensional Gaussians. We prove that Radial-VCReg transforms a broader class of distributions towards normality compared to VCReg and show on synthetic and real-world datasets that it consistently improves performance by reducing higher-order dependencies and promoting more diverse and informative representations.

</details>


### [119] [Integrating Unstructured Text into Causal Inference: Empirical Evidence from Real Data](https://arxiv.org/abs/2602.14274)
*Boning Zhou,Ziyu Wang,Han Hong,Haoqi Hu*

Main category: cs.LG

TL;DR: 提出基于Transformer语言模型的框架，利用非结构化文本进行因果推断，验证其与结构化数据方法结果的一致性


<details>
  <summary>Details</summary>
Motivation: 传统因果推断严重依赖结构化数据，但在许多实际场景中，结构化数据可能不完整或不可用，限制了因果推断的应用范围

Method: 开发基于Transformer语言模型的框架，从非结构化文本中提取信息进行因果推断，并在群体、组别和个体三个层面与结构化数据方法进行比较

Result: 两种方法在群体、组别和个体层面都得到一致的因果估计结果，验证了非结构化文本在因果推断任务中的潜力

Conclusion: 该框架扩展了因果推断方法的应用范围，使其在只有文本数据可用的情况下也能进行数据驱动的商业决策，解决了结构化数据稀缺的问题

Abstract: Causal inference, a critical tool for informing business decisions, traditionally relies heavily on structured data. However, in many real-world scenarios, such data can be incomplete or unavailable. This paper presents a framework that leverages transformer-based language models to perform causal inference using unstructured text. We demonstrate the effectiveness of our framework by comparing causal estimates derived from unstructured text against those obtained from structured data across population, group, and individual levels. Our findings show consistent results between the two approaches, validating the potential of unstructured text in causal inference tasks. Our approach extends the applicability of causal inference methods to scenarios where only textual data is available, enabling data-driven business decision-making when structured tabular data is scarce.

</details>


### [120] [Reverse N-Wise Output-Oriented Testing for AI/ML and Quantum Computing Systems](https://arxiv.org/abs/2602.14275)
*Lamine Rihani*

Main category: cs.LG

TL;DR: 提出反向n-wise输出测试，通过直接在输出等价类上构建覆盖阵列，解决AI/ML和量子计算的高维连续输入空间、概率性输出等测试挑战


<details>
  <summary>Details</summary>
Motivation: AI/ML系统和量子计算软件面临前所未有的测试挑战：高维连续输入空间、概率性非确定性输出分布、仅通过可观测预测行为定义正确性，以及公平性、鲁棒性、量子错误模式等关键质量维度需要通过复杂的多向交互来体现

Method: 引入反向n-wise输出测试范式，直接在领域特定的输出等价类（ML置信度校准桶、决策边界区域、公平性分区、嵌入聚类、量子测量结果分布、错误综合征模式）上构建覆盖阵列，通过无梯度元启发式优化解决黑盒逆映射问题，合成能够引发目标行为特征的输入配置或量子电路参数

Result: 框架为两个领域带来协同效益：明确的客户中心预测/测量覆盖保证、ML校准/边界失败和量子错误综合征的故障检测率显著提升、测试套件效率增强，以及通过不确定性分析和覆盖漂移监控实现结构化MLOps/量子验证流水线

Conclusion: 反向n-wise输出测试为AI/ML和量子计算系统提供了一种数学原理上的测试范式转换，通过输出空间覆盖和黑盒逆映射，有效应对这些新兴技术特有的测试挑战

Abstract: Artificial intelligence/machine learning (AI/ML) systems and emerging quantum computing software present unprecedented testing challenges characterized by high-dimensional/continuous input spaces, probabilistic/non-deterministic output distributions, behavioral correctness defined exclusively over observable prediction behaviors and measurement outcomes, and critical quality dimensions, trustworthiness, fairness, calibration, robustness, error syndrome patterns, that manifest through complex multi-way interactions among semantically meaningful output properties rather than deterministic input-output mappings. This paper introduces reverse n-wise output testing, a mathematically principled paradigm inversion that constructs covering arrays directly over domain-specific output equivalence classes, ML confidence calibration buckets, decision boundary regions, fairness partitions, embedding clusters, ranking stability bands, quantum measurement outcome distributions (0-dominant, 1-dominant, superposition collapse), error syndrome patterns (bit-flip, phase-flip, correlated errors), then solves the computationally challenging black-box inverse mapping problem via gradient-free metaheuristic optimization to synthesize input feature configurations or quantum circuit parameters capable of eliciting targeted behavioral signatures from opaque models. The framework delivers synergistic benefits across both domains: explicit customer-centric prediction/measurement coverage guarantees, substantial improvements in fault detection rates for ML calibration/boundary failures and quantum error syndromes, enhanced test suite efficiency, and structured MLOps/quantum validation pipelines with automated partition discovery from uncertainty analysis and coverage drift monitoring.

</details>


### [121] [Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions](https://arxiv.org/abs/2602.14279)
*Ruomeng Ding,Tianwei Gao,Thomas P. Zollo,Eitan Bachmat,Richard Zemel,Zhun Deng*

Main category: cs.LG

TL;DR: 提出自适应群体启发框架，结合LLM信息增益和图神经网络传播，在有限预算下优化问题选择和受访者选择，提高群体层面响应预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有启发方法主要优化问题选择，但未考虑受访者选择和利用群体结构来处理不完整响应。在现实调查中，存在查询成本和参与预算限制，需要更有效的自适应群体启发方法。

Method: 提出自适应群体启发框架：1) 使用LLM基于预期信息增益评分候选问题；2) 使用异构图神经网络传播聚合观察到的响应和参与者属性，填补缺失响应并指导每轮受访者选择；3) 在查询和参与预算约束下进行多轮自适应选择。

Result: 在三个真实世界意见数据集上，该方法在有限预算下持续提高群体层面响应预测准确性，在CES数据集上，10%受访者预算下获得超过12%的相对增益。

Conclusion: 提出的自适应群体启发框架能有效结合问题优化和受访者选择，利用群体结构推断缺失响应，在有限预算下显著提高群体层面信息获取效率。

Abstract: Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.

</details>


### [122] [KernelBlaster: Continual Cross-Task CUDA Optimization via Memory-Augmented In-Context Reinforcement Learning](https://arxiv.org/abs/2602.14293)
*Kris Shengjun Dong,Sahil Modi,Dima Nikiforov,Sana Damani,Edward Lin,Siva Kumar Sastry Hari,Christos Kozyrakis*

Main category: cs.LG

TL;DR: KernelBlaster是一个基于记忆增强上下文强化学习的框架，通过构建可检索的CUDA知识库，帮助LLM代理系统性地优化CUDA代码，实现跨多代GPU架构的高性能。


<details>
  <summary>Details</summary>
Motivation: 传统编译器受限于固定启发式方法，而微调大型语言模型成本高昂。现有的CUDA代码优化代理工作流缺乏从先前探索中聚合知识的能力，导致采样偏差和次优解。

Method: 提出Memory-Augmented In-context Reinforcement Learning (MAIC-RL)框架，构建Persistent CUDA Knowledge Base积累知识，采用基于配置文件的文本梯度代理流程进行CUDA生成和优化。

Result: 相比PyTorch基线，在KernelBench Level 1、2、3上分别实现了1.43倍、2.50倍和1.50倍的几何平均加速。框架已开源发布。

Conclusion: KernelBlaster通过记忆增强和知识积累机制，使LLM代理能够系统性地探索高性能优化策略，有效解决了跨多代GPU架构的CUDA代码优化挑战。

Abstract: Optimizing CUDA code across multiple generations of GPU architectures is challenging, as achieving peak performance requires an extensive exploration of an increasingly complex, hardware-specific optimization space. Traditional compilers are constrained by fixed heuristics, whereas finetuning Large Language Models (LLMs) can be expensive. However, agentic workflows for CUDA code optimization have limited ability to aggregate knowledge from prior exploration, leading to biased sampling and suboptimal solutions. We propose KernelBlaster, a Memory-Augmented In-context Reinforcement Learning (MAIC-RL) framework designed to improve CUDA optimization search capabilities of LLM-based GPU coding agents. KernelBlaster enables agents to learn from experience and make systematically informed decisions on future tasks by accumulating knowledge into a retrievable Persistent CUDA Knowledge Base. We propose a novel profile-guided, textual-gradient-based agentic flow for CUDA generation and optimization to achieve high performance across generations of GPU architectures. KernelBlaster guides LLM agents to systematically explore high-potential optimization strategies beyond naive rewrites. Compared to the PyTorch baseline, our method achieves geometric mean speedups of 1.43x, 2.50x, and 1.50x on KernelBench Levels 1, 2, and 3, respectively. We release KernelBlaster as an open-source agentic framework, accompanied by a test harness, verification components, and a reproducible evaluation pipeline.

</details>


### [123] [Machine Learning as a Tool (MLAT): A Framework for Integrating Statistical ML Models as Callable Tools within LLM Agent Workflows](https://arxiv.org/abs/2602.14295)
*Edwin Chen,Zulekha Bibi*

Main category: cs.LG

TL;DR: MLAT是一种设计模式，将预训练机器学习模型作为可调用工具集成到LLM工作流中，使LLM能根据上下文决定何时调用定量预测模型。通过PitchCraft系统验证，将提案生成时间从数小时缩短至10分钟内。


<details>
  <summary>Details</summary>
Motivation: 传统ML推理通常作为静态预处理步骤，缺乏与LLM的交互性。需要一种方法让LLM能根据对话上下文动态决定何时使用定量预测模型，实现定量估计与上下文推理的结合。

Method: 提出MLAT设计模式，将ML模型作为一等工具暴露给LLM代理。开发PitchCraft系统，包含研究代理（并行工具调用收集情报）和草稿代理（调用XGBoost定价模型并生成结构化输出）。在极度数据稀缺下，结合真实和人工验证的合成数据训练模型。

Result: 定价模型在70个训练样本上达到R^2=0.807，平均绝对误差3688美元。系统将提案生成时间从数小时减少到10分钟以内。敏感性分析显示模型学习了有意义的业务关系。

Conclusion: MLAT是一种有效的设计模式，将ML模型作为工具集成到LLM工作流中，适用于需要定量估计与上下文推理结合的领域。在数据稀缺情况下也能有效工作，显著提高工作效率。

Abstract: We introduce Machine Learning as a Tool (MLAT), a design pattern in which pre-trained statistical machine learning models are exposed as callable tools within large language model (LLM) agent workflows. This allows an orchestrating agent to invoke quantitative predictions when needed and reason about their outputs in context. Unlike conventional pipelines that treat ML inference as a static preprocessing step, MLAT positions the model as a first-class tool alongside web search, database queries, and APIs, enabling the LLM to decide when and how to use it based on conversational context.
  To validate MLAT, we present PitchCraft, a pilot production system that converts discovery call recordings into professional proposals with ML-predicted pricing. The system uses two agents: a Research Agent that gathers prospect intelligence via parallel tool calls, and a Draft Agent that invokes an XGBoost pricing model as a tool call and generates a complete proposal through structured outputs. The pricing model, trained on 70 examples combining real and human-verified synthetic data, achieves R^2 = 0.807 on held-out data with a mean absolute error of 3688 USD. The system reduces proposal generation time from multiple hours to under 10 minutes.
  We describe the MLAT framework, structured output architecture, training methodology under extreme data scarcity, and sensitivity analysis demonstrating meaningful learned relationships. MLAT generalizes to domains requiring quantitative estimation combined with contextual reasoning.

</details>


### [124] [DeepFusion: Accelerating MoE Training via Federated Knowledge Distillation from Heterogeneous Edge Devices](https://arxiv.org/abs/2602.14301)
*Songyuan Li,Jia Hu,Ahmed M. Abdelmoniem,Geyong Min,Haojun Huang,Jiwei Huang*

Main category: cs.LG

TL;DR: DeepFusion：首个可扩展的联邦MoE训练框架，通过联邦知识蒸馏融合异构设备上的LLM知识，解决资源受限设备无法承载大型MoE模型的问题


<details>
  <summary>Details</summary>
Motivation: MoE-based LLMs需要大量多样化训练数据，联邦学习可利用私有数据但传统方法要求设备本地承载MoE模型，这对资源受限设备不现实

Method: 1) 设备独立配置训练适合自身需求的本地LLM；2) 提出View-Aligned Attention模块，整合全局MoE模型的多阶段特征表示，构建与本地LLM对齐的预测视角，解决跨架构知识蒸馏中的视图不匹配问题

Result: 在行业级MoE模型（Qwen-MoE和DeepSeek-MoE）和真实数据集（医疗和金融）上，DeepFusion性能接近集中式MoE训练，相比基线减少通信成本达71%，token困惑度提升达5.28%

Conclusion: DeepFusion通过创新的视图对齐注意力机制，有效解决了联邦MoE训练中的架构异质性问题，实现了高效、隐私保护的分布式MoE模型训练

Abstract: Recent Mixture-of-Experts (MoE)-based large language models (LLMs) such as Qwen-MoE and DeepSeek-MoE are transforming generative AI in natural language processing. However, these models require vast and diverse training data. Federated learning (FL) addresses this challenge by leveraging private data from heterogeneous edge devices for privacy-preserving MoE training. Nonetheless, traditional FL approaches require devices to host local MoE models, which is impractical for resource-constrained devices due to large model sizes. To address this, we propose DeepFusion, the first scalable federated MoE training framework that enables the fusion of heterogeneous on-device LLM knowledge via federated knowledge distillation, yielding a knowledge-abundant global MoE model. Specifically, DeepFusion features each device to independently configure and train an on-device LLM tailored to its own needs and hardware limitations. Furthermore, we propose a novel View-Aligned Attention (VAA) module that integrates multi-stage feature representations from the global MoE model to construct a predictive perspective aligned with on-device LLMs, thereby enabling effective cross-architecture knowledge distillation. By explicitly aligning predictive perspectives, VAA resolves the view-mismatch problem in traditional federated knowledge distillation, which arises from heterogeneity in model architectures and prediction behaviors between on-device LLMs and the global MoE model. Experiments with industry-level MoE models (Qwen-MoE and DeepSeek-MoE) and real-world datasets (medical and finance) demonstrate that DeepFusion achieves performance close to centralized MoE training. Compared with key federated MoE baselines, DeepFusion reduces communication costs by up to 71% and improves token perplexity by up to 5.28%.

</details>


### [125] [In Transformer We Trust? A Perspective on Transformer Architecture Failure Modes](https://arxiv.org/abs/2602.14318)
*Trishit Mondal,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 该论文对Transformer模型在关键应用中的可信度进行全面评估，涵盖可解释性、鲁棒性、公平性、隐私等方面，识别其结构脆弱性和领域特定风险。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer架构在自然语言处理、计算机视觉、医疗健康、自主系统、气候建模、材料发现、药物发现、核科学、机器人等高风险应用中的广泛部署，需要对其可信度进行更深入和严谨的理解。

Method: 通过综合文献回顾，系统评估Transformer模型的可信度，包括可解释性、可解释性、对抗攻击的鲁棒性、公平性和隐私保护等方面。研究涵盖自然语言处理、计算机视觉、科学与工程领域（机器人、医学、地球科学、材料科学、流体动力学、核科学、自动定理证明等）的安全关键应用。

Result: 识别了Transformer模型在部署中存在的重复性结构脆弱性、领域特定风险以及限制其可靠部署的开放研究挑战。通过跨领域综合分析，揭示了这些架构在关键应用中面临的风险。

Conclusion: Transformer模型在高风险应用中的可信度需要系统评估和持续研究，存在结构脆弱性和领域特定风险，需要进一步解决可解释性、鲁棒性、公平性和隐私保护等方面的挑战，以确保其可靠部署。

Abstract: Transformer architectures have revolutionized machine learning across a wide range of domains, from natural language processing to scientific computing. However, their growing deployment in high-stakes applications, such as computer vision, natural language processing, healthcare, autonomous systems, and critical areas of scientific computing including climate modeling, materials discovery, drug discovery, nuclear science, and robotics, necessitates a deeper and more rigorous understanding of their trustworthiness. In this work, we critically examine the foundational question: \textitHow trustworthy are transformer models?} We evaluate their reliability through a comprehensive review of interpretability, explainability, robustness against adversarial attacks, fairness, and privacy. We systematically examine the trustworthiness of transformer-based models in safety-critical applications spanning natural language processing, computer vision, and science and engineering domains, including robotics, medicine, earth sciences, materials science, fluid dynamics, nuclear science, and automated theorem proving; highlighting high-impact areas where these architectures are central and analyzing the risks associated with their deployment. By synthesizing insights across these diverse areas, we identify recurring structural vulnerabilities, domain-specific risks, and open research challenges that limit the reliable deployment of transformers.

</details>


### [126] [Conformal Signal Temporal Logic for Robust Reinforcement Learning Control: A Case Study](https://arxiv.org/abs/2602.14322)
*Hani Beirami,M M Manjurul Islam*

Main category: cs.LG

TL;DR: 使用形式化时序逻辑规范增强强化学习在航空航天控制中的安全性和鲁棒性，通过符合性STL防护层在线过滤RL动作，在F-16仿真中验证效果


<details>
  <summary>Details</summary>
Motivation: 研究如何通过形式化时序逻辑规范提高强化学习在航空航天控制应用中的安全性和鲁棒性，解决传统RL方法在安全关键系统中可能存在的风险

Method: 使用PPO算法训练F-16仿真控制代理，将控制目标编码为STL要求，引入符合性STL防护层通过在线符合性预测过滤RL代理的动作，与基线PPO和传统规则型STL防护层进行对比

Result: 实验表明符合性防护层在保持STL满足性的同时维持接近基线的性能，在模型失配、执行器速率限制、测量噪声和设定点跳变等压力场景下，比传统防护层提供更强的鲁棒性保证

Conclusion: 形式化规范监控与数据驱动RL控制的结合能显著提高自主飞行控制在挑战性环境中的可靠性，符合性STL防护层是实现这一目标的有效方法

Abstract: We investigate how formal temporal logic specifications can enhance the safety and robustness of reinforcement learning (RL) control in aerospace applications. Using the open source AeroBench F-16 simulation benchmark, we train a Proximal Policy Optimization (PPO) agent to regulate engine throttle and track commanded airspeed. The control objective is encoded as a Signal Temporal Logic (STL) requirement to maintain airspeed within a prescribed band during the final seconds of each maneuver. To enforce this specification at run time, we introduce a conformal STL shield that filters the RL agent's actions using online conformal prediction. We compare three settings: (i) PPO baseline, (ii) PPO with a classical rule-based STL shield, and (iii) PPO with the proposed conformal shield, under both nominal conditions and a severe stress scenario involving aerodynamic model mismatch, actuator rate limits, measurement noise, and mid-episode setpoint jumps. Experiments show that the conformal shield preserves STL satisfaction while maintaining near baseline performance and providing stronger robustness guarantees than the classical shield. These results demonstrate that combining formal specification monitoring with data driven RL control can substantially improve the reliability of autonomous flight control in challenging environments.

</details>


### [127] [Train Less, Learn More: Adaptive Efficient Rollout Optimization for Group-Based Reinforcement Learning](https://arxiv.org/abs/2602.14338)
*Zhi Zhang,Zhen Han,Costas Mavromatis,Qi Zhu,Yunyi Zhang,Sheng Guan,Dingmin Wang,Xiong Zhou,Shuai Wang,Soji Adeshina,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: AERO改进GRPO算法，通过自适应采样、选择性拒绝和贝叶斯后验避免零优势死区，在保持性能的同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: GRPO算法在LLM后训练中广泛使用，但当组内所有rollout结果相同时，归一化优势为零，导致梯度信号消失和计算资源浪费。

Method: 提出AERO算法：1) 自适应rollout策略；2) 选择性拒绝策略修剪rollout；3) 贝叶斯后验防止零优势死区。

Result: 在相同总rollout预算下，AERO减少约48%总训练计算量，缩短约45%每步训练时间，同时保持或提升Pass@8和Avg@8性能。

Conclusion: AERO为基于RL的LLM对齐提供了一种实用、可扩展且计算高效的策略，显著提升训练效率而不牺牲性能。

Abstract: Reinforcement learning (RL) plays a central role in large language model (LLM) post-training. Among existing approaches, Group Relative Policy Optimization (GRPO) is widely used, especially for RL with verifiable rewards (RLVR) fine-tuning. In GRPO, each query prompts the LLM to generate a group of rollouts with a fixed group size $N$. When all rollouts in a group share the same outcome, either all correct or all incorrect, the group-normalized advantages become zero, yielding no gradient signal and wasting fine-tuning compute. We introduce Adaptive Efficient Rollout Optimization (AERO), an enhancement of GRPO. AERO uses an adaptive rollout strategy, applies selective rejection to strategically prune rollouts, and maintains a Bayesian posterior to prevent zero-advantage dead zones. Across three model configurations (Qwen2.5-Math-1.5B, Qwen2.5-7B, and Qwen2.5-7B-Instruct), AERO improves compute efficiency without sacrificing performance. Under the same total rollout budget, AERO reduces total training compute by about 48% while shortening wall-clock time per step by about 45% on average. Despite the substantial reduction in compute, AERO matches or improves Pass@8 and Avg@8 over GRPO, demonstrating a practical, scalable, and compute-efficient strategy for RL-based LLM alignment.

</details>


### [128] [Zero-Shot Instruction Following in RL via Structured LTL Representations](https://arxiv.org/abs/2602.14344)
*Mathias Jackermeier,Mattia Giuri,Jacques Cloete,Alessandro Abate*

Main category: cs.LG

TL;DR: 提出一种基于线性时序逻辑(LTL)的多任务强化学习方法，通过层次化神经网络和注意力机制学习结构化任务表示，实现零样本执行未见任务


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉LTL规范中的丰富逻辑和时间结构方面存在困难，需要更好的任务表示来促进训练和泛化

Method: 基于任务有限自动机构建布尔公式序列，采用层次化神经网络编码逻辑结构，引入注意力机制让策略能够推理未来子目标

Result: 在多种复杂环境中实验表明，该方法具有强大的泛化能力和优越性能

Conclusion: 提出的结构化任务表示方法有效解决了LTL规范中的逻辑和时间结构捕捉问题，显著提升了多任务强化学习的泛化能力

Abstract: We study instruction following in multi-task reinforcement learning, where an agent must zero-shot execute novel tasks not seen during training. In this setting, linear temporal logic (LTL) has recently been adopted as a powerful framework for specifying structured, temporally extended tasks. While existing approaches successfully train generalist policies, they often struggle to effectively capture the rich logical and temporal structure inherent in LTL specifications. In this work, we address these concerns with a novel approach to learn structured task representations that facilitate training and generalisation. Our method conditions the policy on sequences of Boolean formulae constructed from a finite automaton of the task. We propose a hierarchical neural architecture to encode the logical structure of these formulae, and introduce an attention mechanism that enables the policy to reason about future subgoals. Experiments in a variety of complex environments demonstrate the strong generalisation capabilities and superior performance of our approach.

</details>


### [129] [WIMLE: Uncertainty-Aware World Models with IMLE for Sample-Efficient Continuous Control](https://arxiv.org/abs/2602.14351)
*Mehran Aghabozorgi,Alireza Moazeni,Yanshu Zhang,Ke Li*

Main category: cs.LG

TL;DR: WIMLE是一种基于模型的强化学习方法，通过隐式最大似然估计学习随机多模态世界模型，使用集成和潜在采样估计预测不确定性，并通过置信度加权合成转移来稳定学习。


<details>
  <summary>Details</summary>
Motivation: 基于模型的强化学习虽然样本效率高，但在实践中表现不佳，主要原因是：模型误差累积、单模态世界模型平均多模态动态、过度自信的预测导致学习偏差。

Method: 将隐式最大似然估计(IMLE)扩展到基于模型的RL框架中，学习随机多模态世界模型（无需迭代采样），通过集成和潜在采样估计预测不确定性，在训练中根据预测置信度对合成转移进行加权。

Result: 在40个连续控制任务（DeepMind Control、MyoSuite、HumanoidBench）上，WIMLE实现了优越的样本效率和竞争性或更好的渐近性能。在Humanoid-run任务上，样本效率比最强竞争者提高50%以上；在HumanoidBench上解决了14个任务中的8个（BRO解决4个，SimbaV2解决5个）。

Conclusion: 基于IMLE的多模态和不确定性感知加权对于稳定的基于模型的强化学习具有重要价值，能够有效处理模型误差和多模态动态问题。

Abstract: Model-based reinforcement learning promises strong sample efficiency but often underperforms in practice due to compounding model error, unimodal world models that average over multi-modal dynamics, and overconfident predictions that bias learning. We introduce WIMLE, a model-based method that extends Implicit Maximum Likelihood Estimation (IMLE) to the model-based RL framework to learn stochastic, multi-modal world models without iterative sampling and to estimate predictive uncertainty via ensembles and latent sampling. During training, WIMLE weights each synthetic transition by its predicted confidence, preserving useful model rollouts while attenuating bias from uncertain predictions and enabling stable learning. Across $40$ continuous-control tasks spanning DeepMind Control, MyoSuite, and HumanoidBench, WIMLE achieves superior sample efficiency and competitive or better asymptotic performance than strong model-free and model-based baselines. Notably, on the challenging Humanoid-run task, WIMLE improves sample efficiency by over $50$\% relative to the strongest competitor, and on HumanoidBench it solves $8$ of $14$ tasks (versus $4$ for BRO and $5$ for SimbaV2). These results highlight the value of IMLE-based multi-modality and uncertainty-aware weighting for stable model-based RL.

</details>


### [130] [A Study on Multi-Class Online Fuzzy Classifiers for Dynamic Environments](https://arxiv.org/abs/2602.14375)
*Kensuke Ajimoto,Yuma Yamamoto,Yoshifumi Kusunoki,Tomoharu Nakashima*

Main category: cs.LG

TL;DR: 提出一种用于动态环境的多类在线模糊分类器，扩展了传统仅处理二分类问题的在线模糊分类器


<details>
  <summary>Details</summary>
Motivation: 传统在线模糊分类器只能处理二分类问题，但在实际应用中需要处理多类分类问题。同时，在动态环境中，训练数据不是一次性全部可用，而是随时间逐步出现

Method: 使用模糊if-then规则构建分类器，其中前件模糊集由用户预先确定，后件实值通过从训练数据中学习得到。在在线框架下，每次只处理少量样本，随时间逐步更新模型

Result: 通过合成动态数据和多个基准数据集的数值实验评估了多类在线模糊分类器的性能

Conclusion: 成功将在线模糊分类器扩展到多类问题，为动态环境中的多类分类提供了有效解决方案

Abstract: This paper proposes a multi-class online fuzzy classifier for dynamic environments. A fuzzy classifier comprises a set of fuzzy if-then rules where human users determine the antecedent fuzzy sets beforehand. In contrast, the consequent real values are determined by learning from training data. In an online framework, not all training dataset patterns are available beforehand. Instead, only a few patterns are available at a time step, and the subsequent patterns become available at the following time steps. The conventional online fuzzy classifier considered only two-class problems. This paper investigates the extension to the conventional fuzzy classifiers for multi-class problems. We evaluate the performance of the multi-class online fuzzy classifiers through numerical experiments on synthetic dynamic data and also several benchmark datasets.

</details>


### [131] [A unified framework for evaluating the robustness of machine-learning interpretability for prospect risking](https://arxiv.org/abs/2602.14430)
*Prithwijit Chowdhury,Ahmad Mustafa,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.LG

TL;DR: 该论文提出一个统一框架，使用反事实和必要性/充分性量化来评估LIME和SHAP在油气勘探风险数据上的解释鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在油气勘探风险评估中，机器学习分类器缺乏透明度，而现有的XAI方法（如LIME和SHAP）对同一场景的解释存在不一致，特别是在复杂数据上。不同解释策略对"重要性"和"相关性"的定义不同，需要更可靠的方法来提升解释的可信度。

Method: 提出统一框架，生成反事实并量化必要性和充分性，用于评估LIME和SHAP在高维结构化勘探风险数据上的解释鲁棒性。通过鲁棒性测试深入分析模型处理错误数据的能力，以及哪种XAI模块与哪种模型在油气指示数据集上表现最佳。

Result: 未在摘要中明确说明具体结果，但框架能够提供对模型处理错误数据能力的深入洞察，并确定在油气指示数据集上哪种XAI模块与哪种模型配对效果最佳。

Conclusion: 基于因果理论中的必要性和充分性概念来验证特征排序，是提高XAI解释可信度和鲁棒性的更可靠方法。提出的框架能够评估不同XAI方法的解释一致性，为油气勘探风险评估提供更可靠的决策支持。

Abstract: In geophysics, hydrocarbon prospect risking involves assessing the risks associated with hydrocarbon exploration by integrating data from various sources. Machine learning-based classifiers trained on tabular data have been recently used to make faster decisions on these prospects. The lack of transparency in the decision-making processes of such models has led to the emergence of explainable AI (XAI). LIME and SHAP are two such examples of these XAI methods which try to generate explanations of a particular decision by ranking the input features in terms of importance. However, explanations of the same scenario generated by these two different explanation strategies have shown to disagree or be different, particularly for complex data. This is because the definitions of "importance" and "relevance" differ for different explanation strategies. Thus, grounding these ranked features using theoretically backed causal ideas of necessity and sufficiency can prove to be a more reliable and robust way to improve the trustworthiness of the concerned explanation strategies.We propose a unified framework to generate counterfactuals as well as quantify necessity and sufficiency and use these to perform a robustness evaluation of the explanations provided by LIME and SHAP on high dimensional structured prospect risking data. This robustness test gives us deeper insights into the models capabilities to handle erronous data and which XAI module works best in pair with which model for our dataset for hydorcarbon indication.

</details>


### [132] [Broken Chains: The Cost of Incomplete Reasoning in LLMs](https://arxiv.org/abs/2602.14444)
*Ian Su,Gaurav Purushothaman,Jey Narayan,Ruhika Goel,Kevin Zhu,Sunishchal Dev,Yash More,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 研究比较不同推理模态（代码、自然语言、混合、无推理）在token预算受限下的性能表现，发现代码推理更稳健，混合推理效果差，不完整的推理链会误导模型。


<details>
  <summary>Details</summary>
Motivation: 推理专用模型在推理token上消耗大量计算资源，成本高昂。需要了解不同推理模态在token约束下的表现，为资源受限环境部署推理系统提供指导。

Method: 提出框架限制模型仅通过代码、注释、两者混合或无推理进行推理，系统性地将token预算削减至最优的10%、30%、50%、70%。在四个前沿模型（GPT-5.1、Gemini 3 Flash、DeepSeek-V3.2、Grok 4.1）上评估数学基准（AIME、GSM8K、HMMT）。

Result: 1. 截断推理有害：DeepSeek-V3.2在50%预算下无推理达53%，截断CoT仅17%；2. 代码推理稳健：Gemini注释降至0%时代码保持43-47%；3. 混合推理效果差于单一模态；4. 稳健性模型依赖：Grok在30%预算保持80-90%，而OpenAI和DeepSeek降至7-27%。

Conclusion: 不完整的推理链会主动误导模型，这对在资源约束下部署推理专用系统有重要影响。代码推理比自然语言推理更稳健，混合推理效果不佳。

Abstract: Reasoning-specialized models like OpenAI's 5.1 and DeepSeek-V3.2 allocate substantial inference compute to extended chain-of-thought (CoT) traces, yet reasoning tokens incur significant costs. How do different reasoning modalities of code, natural language, hybrid, or none do perform under token constraints? We introduce a framework that constrains models to reason exclusively through code, comments, both, or neither, then systematically ablates token budgets to 10\%, 30\%, 50\%, and 70\% of optimal. We evaluate four frontier models (GPT-5.1, Gemini 3 Flash, DeepSeek-V3.2, Grok 4.1) across mathematical benchmarks (AIME, GSM8K, HMMT). Our findings reveal: (1) \textbf{truncated reasoning can hurt} as DeepSeek-V3.2 achieves 53\% with no reasoning but only 17\% with truncated CoT at 50\% budget; (2) \textbf{code degrades gracefully} as Gemini's comments collapse to 0\% while code maintains 43-47\%; (3) \textbf{hybrid reasoning underperforms} single modalities; (4) \textbf{robustness is model-dependent} as Grok maintains 80-90\% at 30\% budget where OpenAI and DeepSeek collapse to 7-27\%. These results suggest incomplete reasoning chains actively mislead models, with implications for deploying reasoning-specialized systems under resource constraints.

</details>


### [133] [Selective Synchronization Attention](https://arxiv.org/abs/2602.14445)
*Hasi Hays*

Main category: cs.LG

TL;DR: 提出SSA注意力机制，基于Kuramoto耦合振荡器模型，用同步强度代替点积注意力，实现自然稀疏性、统一位置语义编码和单次闭式计算。


<details>
  <summary>Details</summary>
Motivation: Transformer的自注意力机制存在二次计算复杂度问题，且缺乏生物学神经计算基础。需要一种更高效、更具生物合理性的注意力机制。

Method: 提出选择性同步注意力（SSA），将每个token表示为具有可学习自然频率和相位的振荡器，基于Kuramoto模型的稳态解计算同步强度作为注意力权重。构建振荡同步网络（OSN）作为Transformer块的替代方案。

Result: SSA具有自然稀疏性（相位锁定阈值）、统一位置语义编码（自然频率谱）、单次闭式计算等优势。初始化时同步矩阵显示出非均匀、头多样化的耦合模式，比随机初始化Transformer的近似均匀注意力具有更强的架构归纳偏置。

Conclusion: SSA为Transformer架构提供了一种基于振荡器同步的替代注意力机制，具有计算效率、生物合理性和更强的归纳偏置，有望成为现代深度学习的新基础。

Abstract: The Transformer architecture has become the foundation of modern deep learning, yet its core self-attention mechanism suffers from quadratic computational complexity and lacks grounding in biological neural computation. We propose Selective Synchronization Attention (SSA), a novel attention mechanism that replaces the standard dot-product self-attention with a closed-form operator derived from the steady-state solution of the Kuramoto model of coupled oscillators. In SSA, each token is represented as an oscillator characterized by a learnable natural frequency and phase; the synchronization strength between token pairs, determined by a frequency-dependent coupling and phase-locking condition, serves as the attention weight. This formulation provides three key advantages: (i) natural sparsity arising from the phase-locking threshold, whereby tokens with incompatible frequencies automatically receive zero attention weight without explicit masking; (ii) unified positional-semantic encoding through the natural frequency spectrum, eliminating the need for separate positional encodings; and (iii) a single-pass, closed-form computation that avoids iterative ODE integration, with all components (coupling, order parameter, synchronization) derived from the oscillatory framework. We instantiate SSA within the Oscillatory Synchronization Network (OSN), a drop-in replacement for the Transformer block. Analysis of the synchronization matrices reveals non-uniform, head-diverse coupling patterns even at initialization, demonstrating a stronger architectural inductive bias than the approximately uniform attention produced by randomly initialized Transformers.

</details>


### [134] [WiSparse: Boosting LLM Inference Efficiency with Weight-Aware Mixed Activation Sparsity](https://arxiv.org/abs/2602.14452)
*Lei Chen,Yuan Meng,Xiaoyu Zhan,Zhi Wang,Wenwu Zhu*

Main category: cs.LG

TL;DR: WiSparse是一种无需训练的激活稀疏化方法，通过结合激活和权重信息进行自适应稀疏分配，在保持模型性能的同时显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的激活稀疏化方法仅依赖激活信息且使用均匀稀疏率，忽略了权重的重要性和不同模型块对稀疏化的敏感度差异，导致性能不佳。研究发现：1）不显著的激活可能对应重要的权重；2）不同模型块的稀疏敏感度呈非单调变化。

Method: 提出权重感知混合粒度激活稀疏化方法：1）权重感知机制结合激活幅度和预计算的权重范数来准确识别重要通道；2）混合粒度分配方案：通过进化搜索全局分配稀疏预算以保护敏感区域，然后在块内细化以最小化重构误差；3）改进稀疏核实现。

Result: 在三个代表性模型上验证有效性。在50%稀疏率下，WiSparse保持Llama3.1密集模型97%的性能，比最强基线提升2.23个百分点，同时端到端推理速度加速21.4%。

Conclusion: WiSparse推进了无需训练方法在高效LLM推理中的极限，在不训练的情况下实现了可达到的加速边界突破。

Abstract: Large Language Models (LLMs) offer strong capabilities but incur high inference costs due to dense computation and memory access. Training-free activation sparsity is a promising approach for efficient LLM inference, yet existing methods often rely solely on activation information and uniform sparsity ratios. This overlooks the critical interplay with weights and inter-block sensitivity variation, leading to suboptimal performance. We identify two key phenomena in modern LLMs: 1) less significant activations may align with highly important weights, and 2) sparsity sensitivity varies non-monotonically across model blocks. We propose Weight-aware Mixed-Granularity Training-free Activation Sparsity (WiSparse), which leverages both activation and weight information for adaptive sparsity allocation. Specifically, we introduce a weight-aware mechanism integrating activation magnitudes with precomputed weight norms to accurately identify salient channels. This is combined with a mixed-granularity allocation scheme: a global budget is distributed across blocks via evolutionary search to protect sensitive regions, then refined within blocks to minimize reconstruction error. We improve sparse kernels and demonstrate effectiveness on three representative models. Notably, at 50% sparsity, WiSparse preserves 97% of Llama3.1's dense performance, surpassing the strongest baseline by 2.23 percentage points while achieving a 21.4% acceleration in end-to-end inference speed. Our research advances the limits of training-free approaches for efficient LLM inference, pushing the boundaries of achievable speedup without training.

</details>


### [135] [Traceable Latent Variable Discovery Based on Multi-Agent Collaboration](https://arxiv.org/abs/2602.14456)
*Huaming Du,Tao Hu,Yijie Huang,Yu Zhao,Guisong Liu,Tao Gu,Gang Kou,Carl Yang*

Main category: cs.LG

TL;DR: TLVD是一个新颖的因果建模框架，结合大语言模型的元数据推理能力和传统因果发现算法的数据驱动建模能力，用于推断潜在变量及其语义，在多个数据集上显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 现实世界中揭示因果机制对科技发展至关重要，但传统因果发现算法存在数据质量不足、假设无潜在混杂变量、忽略潜在变量精确语义等问题，限制了因果发现的广泛应用。

Method: TLVD框架：1) 数据驱动构建包含潜在变量的因果图；2) 多LLM协作进行潜在变量推断，建模为不完全信息博弈并寻求贝叶斯纳什均衡；3) 利用LLM进行证据探索，在多源网络数据中验证推断的潜在变量。

Result: 在三个医院提供的真实患者数据集和两个基准数据集上全面评估，TLVD在五个数据集上平均提升：Acc 32.67%、CAcc 62.21%、ECit 26.72%，证实了其有效性和可靠性。

Conclusion: TLVD成功整合了LLM的元数据推理能力和传统因果发现算法的数据驱动建模，解决了潜在变量推断和语义理解的关键问题，为因果发现的实际应用提供了有效解决方案。

Abstract: Revealing the underlying causal mechanisms in the real world is crucial for scientific and technological progress. Despite notable advances in recent decades, the lack of high-quality data and the reliance of traditional causal discovery algorithms (TCDA) on the assumption of no latent confounders, as well as their tendency to overlook the precise semantics of latent variables, have long been major obstacles to the broader application of causal discovery. To address this issue, we propose a novel causal modeling framework, TLVD, which integrates the metadata-based reasoning capabilities of large language models (LLMs) with the data-driven modeling capabilities of TCDA for inferring latent variables and their semantics. Specifically, we first employ a data-driven approach to construct a causal graph that incorporates latent variables. Then, we employ multi-LLM collaboration for latent variable inference, modeling this process as a game with incomplete information and seeking its Bayesian Nash Equilibrium (BNE) to infer the possible specific latent variables. Finally, to validate the inferred latent variables across multiple real-world web-based data sources, we leverage LLMs for evidence exploration to ensure traceability. We comprehensively evaluate TLVD on three de-identified real patient datasets provided by a hospital and two benchmark datasets. Extensive experimental results confirm the effectiveness and reliability of TLVD, with average improvements of 32.67% in Acc, 62.21% in CAcc, and 26.72% in ECit across the five datasets.

</details>


### [136] [Silent Inconsistency in Data-Parallel Full Fine-Tuning: Diagnosing Worker-Level Optimization Misalignment](https://arxiv.org/abs/2602.14462)
*Hong Li,Zhen Zhou,Honggang Zhang,Yuping Luo,Xinyue Wang,Han Gong,Zhiyuan Liu*

Main category: cs.LG

TL;DR: 论文提出"静默不一致性"概念，指数据并行训练中虽然参数同步但优化动态不匹配的问题，并开发了轻量级诊断框架来量化这种隐藏的不一致性。


<details>
  <summary>Details</summary>
Motivation: 数据并行训练中，虽然参数同步保证了模型权重的数值等价性，但无法保证各工作节点在梯度聚合前的优化动态对齐。这种隐藏的不匹配（称为"静默不一致性"）在传统聚合监控信号下不可见，可能导致训练不稳定。

Method: 提出轻量级、模型无关的诊断框架，使用三个互补指标：损失分散度、梯度范数分散度、以及通过工作节点间余弦相似度衡量的梯度方向一致性。这些指标无需修改模型架构、同步机制或优化算法。

Result: 在8-NPU数据并行设置下微调1B参数模型，通过控制跨节点随机性扰动验证框架。实验显示，逐步去同步的数据洗牌和随机种子会导致损失/梯度分散度显著增加和方向对齐度降低，尽管全局平均损失曲线平滑。

Conclusion: 提出的指标为大规模数据并行微调中的隐藏不稳定模式提供了可操作的可见性，能够实现更可靠的诊断和配置评估，有助于提高训练可靠性。

Abstract: Data-parallel (DP) training with synchronous all-reduce is a dominant paradigm for full-parameter fine-tuning of large language models (LLMs). While parameter synchronization guarantees numerical equivalence of model weights after each iteration, it does not necessarily imply alignment of worker-level optimization dynamics before gradient aggregation. This paper identifies and studies this latent mismatch, termed \emph{silent inconsistency}, where cross-worker divergence in losses and gradients can remain invisible under conventional aggregated monitoring signals. We propose a lightweight, model-agnostic diagnostic framework that quantifies worker-level consistency using training signals readily available in standard pipelines. Specifically, we introduce three complementary metrics: loss dispersion, gradient-norm dispersion, and gradient-direction consistency measured by inter-worker cosine similarity. The proposed metrics incur negligible overhead and require no modification to model architecture, synchronization mechanisms, or optimization algorithms. We validate the framework by fully fine-tuning the 1B-parameter \texttt{openPangu-Embedded-1B-V1.1} model on the \texttt{tatsu-lab/alpaca} dataset using an 8-NPU DP setup, under controlled perturbations of cross-rank stochasticity. Experimental results show that progressively desynchronized data shuffling and random seeds lead to substantial increases in loss/gradient dispersion and reduced directional alignment, despite smooth globally averaged loss curves. These findings demonstrate that the proposed indicators provide actionable visibility into hidden instability modes in large-scale DP fine-tuning, enabling more reliable diagnosis and configuration assessment.

</details>


### [137] [LACONIC: Length-Aware Constrained Reinforcement Learning for LLM](https://arxiv.org/abs/2602.14468)
*Chang Liu,Yiran Zhao,Lawrence Liu,Yaoqi Ye,Csaba Szepesvári,Lin F. Yang*

Main category: cs.LG

TL;DR: LACONIC是一种强化学习方法，通过在训练中强制实施目标token预算来控制LLM输出长度，在保持任务性能的同时显著减少响应长度。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练LLM时会产生过长的响应，增加推理延迟和计算开销。现有的长度控制方法依赖固定的启发式奖励调整，可能与任务目标不一致且需要脆弱的调参。

Method: 提出LACONIC方法，在训练中使用增强目标函数更新策略模型，结合任务奖励和基于长度的成本。通过自适应调整成本尺度来平衡简洁性和任务性能。

Result: 在数学推理模型和数据集上，LACONIC保持或提高pass@1性能，同时减少50%以上的输出长度。在通用知识和多语言基准测试中保持性能，减少44%的token使用。

Conclusion: LACONIC能够在不改变推理过程且部署开销最小的情况下，实现鲁棒的长度控制并保持任务奖励，为LLM的强化学习训练提供了有效的长度控制解决方案。

Abstract: Reinforcement learning (RL) has enhanced the capabilities of large language models (LLMs) through reward-driven training. Nevertheless, this process can introduce excessively long responses, inflating inference latency and computational overhead. Prior length-control approaches typically rely on fixed heuristic reward shaping, which can misalign with the task objective and require brittle tuning. In this work, we propose LACONIC, a reinforcement learning method that enforces a target token budget during training. Specifically, we update policy models using an augmented objective that combines the task reward with a length-based cost. To balance brevity and task performance, the cost scale is adaptively adjusted throughout training. This yields robust length control while preserving task reward. We provide a theoretical guarantee that support the method. Across mathematical reasoning models and datasets, LACONIC preserves or improves pass@1 while reducing output length by over 50%. It maintains out-of-domain performance on general knowledge and multilingual benchmarks with 44% fewer tokens. Moreover, LACONIC integrates into standard RL-tuning with no inference changes and minimal deployment overhead.

</details>


### [138] [One Good Source is All You Need: Near-Optimal Regret for Bandits under Heterogeneous Noise](https://arxiv.org/abs/2602.14474)
*Aadirupa Saha,Amith Bhat,Haipeng Luo*

Main category: cs.LG

TL;DR: SOAR算法解决具有多个异质数据源的MAB问题，通过快速剪枝高方差源并平衡LCB-UCB方法，在不知道最小方差源的情况下达到接近单源最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 传统MAB问题假设单一数据源，但实际应用中常面临多个具有不同噪声方差的数据源。现有方法可能因使用高方差源而导致遗憾界恶化，需要一种能自适应选择最优（最小方差）数据源的方法。

Method: 提出SOAR算法：1) 使用尖锐的方差集中界快速剪枝高方差数据源；2) 采用"平衡最小-最大LCB-UCB方法"，同时识别最佳臂和最优（最小方差）数据源。

Result: SOAR达到实例依赖遗憾界$\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$，其中${σ^*}^2$是最小源方差。该界接近单源MAB的最优遗憾，仅增加不可避免的源识别成本。

Conclusion: SOAR在不知道最小方差源的情况下，实现了接近单源最优的遗憾性能，显著优于Uniform UCB等基线方法，在合成和真实数据集上都表现出优越性能。

Abstract: We study $K$-armed Multiarmed Bandit (MAB) problem with $M$ heterogeneous data sources, each exhibiting unknown and distinct noise variances $\{σ_j^2\}_{j=1}^M$. The learner's objective is standard MAB regret minimization, with the additional complexity of adaptively selecting which data source to query from at each round. We propose Source-Optimistic Adaptive Regret minimization (SOAR), a novel algorithm that quickly prunes high-variance sources using sharp variance-concentration bounds, followed by a `balanced min-max LCB-UCB approach' that seamlessly integrates the parallel tasks of identifying the best arm and the optimal (minimum-variance) data source. Our analysis shows SOAR achieves an instance-dependent regret bound of $\tilde{O}\left({σ^*}^2\sum_{i=2}^K \frac{\log T}{Δ_i} + \sqrt{K \sum_{j=1}^M σ_j^2}\right)$, up to preprocessing costs depending only on problem parameters, where ${σ^*}^2 := \min_j σ_j^2$ is the minimum source variance and $Δ_i$ denotes the suboptimality gap of the $i$-th arm. This result is both surprising as despite lacking prior knowledge of the minimum-variance source among $M$ alternatives, SOAR attains the optimal instance-dependent regret of standard single-source MAB with variance ${σ^*}^2$, while incurring only an small (and unavoidable) additive cost of $\tilde O(\sqrt{K \sum_{j=1}^M σ_j^2})$ towards the optimal (minimum variance) source identification. Our theoretical bounds represent a significant improvement over some proposed baselines, e.g. Uniform UCB or Explore-then-Commit UCB, which could potentially suffer regret scaling with $σ_{\max}^2$ in place of ${σ^*}^2$-a gap that can be arbitrarily large when $σ_{\max} \gg σ^*$. Experiments on multiple synthetic problem instances and the real-world MovieLens\;25M dataset, demonstrating the superior performance of SOAR over the baselines.

</details>


### [139] [Revisiting the Platonic Representation Hypothesis: An Aristotelian View](https://arxiv.org/abs/2602.14486)
*Fabian Gröger,Shuo Wen,Maria Brbić*

Main category: cs.LG

TL;DR: 该论文提出了一种基于排列的零校准框架来修正神经网络表示相似性度量中的尺度偏差，并重新检验了柏拉图表示假说，提出了亚里士多德表示假说。


<details>
  <summary>Details</summary>
Motivation: 现有用于衡量表示相似性的指标受到网络规模的混淆：增加模型深度或宽度会系统性地提高表示相似性分数。柏拉图表示假说认为神经网络表示正在收敛到现实的共同统计模型，但现有度量存在偏差。

Method: 引入基于排列的零校准框架，将任何表示相似性度量转换为具有统计保证的校准分数。使用该框架重新检验柏拉图表示假说，分析全局谱度量和局部邻域相似性的校准结果。

Result: 校准后，全局谱度量显示的明显收敛基本消失，而局部邻域相似性（而非局部距离）在不同模态间保持显著一致性。这表明表示收敛主要体现在局部邻域关系上。

Conclusion: 提出亚里士多德表示假说：神经网络中的表示正在收敛到共享的局部邻域关系，而不是全局统计模型。校准框架为表示相似性分析提供了更可靠的统计基础。

Abstract: The Platonic Representation Hypothesis suggests that representations from neural networks are converging to a common statistical model of reality. We show that the existing metrics used to measure representational similarity are confounded by network scale: increasing model depth or width can systematically inflate representational similarity scores. To correct these effects, we introduce a permutation-based null-calibration framework that transforms any representational similarity metric into a calibrated score with statistical guarantees. We revisit the Platonic Representation Hypothesis with our calibration framework, which reveals a nuanced picture: the apparent convergence reported by global spectral measures largely disappears after calibration, while local neighborhood similarity, but not local distances, retains significant agreement across different modalities. Based on these findings, we propose the Aristotelian Representation Hypothesis: representations in neural networks are converging to shared local neighborhood relationships.

</details>


### [140] [Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts](https://arxiv.org/abs/2602.14490)
*Buze Zhang,Jinkai Tao,Zilang Zeng,Neil He,Ali Maatouk,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: MoSLoRA：一种混合几何空间的参数高效微调方法，通过动态选择或组合不同几何空间（如双曲、球面等）来学习更丰富的曲率感知表示，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法主要在欧几里得空间中操作，限制了捕捉语言数据复杂几何结构的能力。虽然双曲几何（用于层次数据）和球面流形（用于循环模式）等替代几何空间具有理论优势，但强制表示到单一流形类型会限制表达能力。

Method: 提出混合空间框架，同时利用多个几何空间学习更丰富的曲率感知表示。基于此开发MoSLoRA，扩展低秩适应方法，引入异构几何专家，使模型能根据输入上下文动态选择或组合适当的几何空间。为减少频繁流形切换的计算开销，设计了轻量级路由机制。

Result: 在多个基准测试中，MoSLoRA始终优于强基线方法，在MATH500上提升达5.6%，在MAWPS上提升达15.9%。还提供了曲率优化对训练稳定性和模型性能影响的实证分析。

Conclusion: 混合空间框架和MoSLoRA方法通过利用多个几何空间，显著提升了参数高效微调的表达能力，为语言模型适应下游任务提供了更强大的几何感知解决方案。

Abstract: Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.

</details>


### [141] [Divine Benevolence is an $x^2$: GLUs scale asymptotically faster than MLPs](https://arxiv.org/abs/2602.14495)
*Alejandro Francisco Queiruga*

Main category: cs.LG

TL;DR: 论文通过数值分析揭示了GLU架构比MLP具有更优的缩放特性（L(P)∝P^{-3} vs P^{-2}），并提出新的Gated Quadratic Unit架构


<details>
  <summary>Details</summary>
Motivation: 当前前沿LLM中广泛使用的GLU变体架构的成功主要基于经验发现，缺乏理论解释。论文旨在从数值分析角度理解这些架构的缩放规律

Method: 应用数值分析工具，分析GLU架构的二次函数形式，推导其缩放斜率，提供参数构造和1D函数逼近的实证验证，并提出新的Gated Quadratic Unit架构

Result: 证明GLU的缩放斜率L(P)∝P^{-3}，而MLP仅为L(P)=P^{-2}，新提出的Gated Quadratic Unit具有更陡峭的缩放斜率

Conclusion: 从数值分析第一性原理出发可以指导架构设计，解锁大模型的优越缩放特性，为从理论到架构设计开辟了新途径

Abstract: Scaling laws can be understood from ground-up numerical analysis, where traditional function approximation theory can explain shifts in model architecture choices. GLU variants now dominate frontier LLMs and similar outer-product architectures are prevalent in ranking models. The success of these architectures has mostly been left as an empirical discovery. In this paper, we apply the tools of numerical analysis to expose a key factor: these models have an $x^2$ which enables \emph{asymptotically} faster scaling than MLPs. GLUs have piecewise quadratic functional forms that are sufficient to exhibit quadratic order of approximation. Our key contribution is to demonstrate that the $L(P)$ scaling slope is $L(P)\propto P^{-3}$ for GLUs but only $L(P)=P^{-2}$ for MLPs on function reconstruction problems. We provide a parameter construction and empirical verification of these slopes for 1D function approximation. From the first principles we discover, we make one stride and propose the ``Gated Quadratic Unit'' which has an even steeper $L(P)$ slope than the GLU and MLP. This opens the possibility of architecture design from first principles numerical theory to unlock superior scaling in large models. Replication code is available at https://github.com/afqueiruga/divine_scaling.

</details>


### [142] [Covariance-Aware Transformers for Quadratic Programming and Decision Making](https://arxiv.org/abs/2602.14506)
*Kutay Tire,Yufan Zhang,Ege Onur Taga,Samet Oymak*

Main category: cs.LG

TL;DR: Transformer模型能够解决二次规划问题，并通过Time2Decide方法将这种能力应用于包含协方差矩阵的决策问题，在投资组合优化中超越了基础时间序列模型和传统的预测-优化方法。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在解决二次规划问题方面的能力，以及如何利用这种能力改进涉及协方差矩阵的决策问题，特别是投资组合优化这类实际应用。

Method: 1. 理论证明线性注意力机制可以通过逐行标记化矩阵变量来求解无约束二次规划；2. 通过加入MLPs，Transformer块可以求解ℓ₁惩罚和约束的二次规划；3. 提出Time2Decide方法，在时间序列基础模型中显式输入变量间的协方差矩阵。

Result: 1. Transformer理论上能够求解各类二次规划问题；2. Time2Decide在投资组合优化问题上一致优于基础时间序列模型；3. 在某些设置下，Time2Decide甚至优于传统的"预测-优化"流程。

Conclusion: Transformer能够从显式使用二阶统计量中受益，这使得它们能够在单次前向传播中有效解决复杂的决策问题，如投资组合构建，为决策支持系统提供了新的可能性。

Abstract: We explore the use of transformers for solving quadratic programs and how this capability benefits decision-making problems that involve covariance matrices. We first show that the linear attention mechanism can provably solve unconstrained QPs by tokenizing the matrix variables (e.g.~$A$ of the objective $\frac{1}{2}x^\top Ax+b^\top x$) row-by-row and emulating gradient descent iterations. Furthermore, by incorporating MLPs, a transformer block can solve (i) $\ell_1$-penalized QPs by emulating iterative soft-thresholding and (ii) $\ell_1$-constrained QPs when equipped with an additional feedback loop. Our theory motivates us to introduce Time2Decide: a generic method that enhances a time series foundation model (TSFM) by explicitly feeding the covariance matrix between the variates. We empirically find that Time2Decide uniformly outperforms the base TSFM model for the classical portfolio optimization problem that admits an $\ell_1$-constrained QP formulation. Remarkably, Time2Decide also outperforms the classical "Predict-then-Optimize (PtO)" procedure, where we first forecast the returns and then explicitly solve a constrained QP, in suitable settings. Our results demonstrate that transformers benefit from explicit use of second-order statistics, and this can enable them to effectively solve complex decision-making problems, like portfolio construction, in one forward pass.

</details>


### [143] [DeepMTL2R: A Library for Deep Multi-task Learning to Rank](https://arxiv.org/abs/2602.14519)
*Chaosheng Dong,Peiyao Xiao,Yijia Wang,Kaiyi Ji*

Main category: cs.LG

TL;DR: DeepMTL2R是一个开源深度学习框架，用于多任务学习排序，通过Transformer自注意力机制整合异构相关性信号，支持21种多任务学习算法和多目标优化，实现帕累托最优排序模型。


<details>
  <summary>Details</summary>
Motivation: 现代排序系统需要同时优化多个相关性标准，这些标准可能相互冲突。现有方法难以有效整合异构相关性信号并处理复杂依赖关系，需要一个统一、可扩展的框架来解决多任务学习排序问题。

Method: 基于Transformer架构的自注意力机制，构建上下文感知的统一模型。框架包含21种最先进的多任务学习算法，支持多目标优化以识别帕累托最优排序模型，能够捕捉项目和标签间的复杂依赖关系和长距离交互。

Result: 在公开数据集上展示了有效性，报告了具有竞争力的性能，并可视化了目标间的权衡关系。框架已开源，便于进行多任务学习策略的受控比较。

Conclusion: DeepMTL2R为现代排序系统提供了一个可扩展且表达力强的解决方案，通过整合异构相关性信号和捕捉复杂依赖关系，有效解决了多任务学习排序问题，并促进了多任务学习策略的比较研究。

Abstract: This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.

</details>


### [144] [Governing AI Forgetting: Auditing for Machine Unlearning Compliance](https://arxiv.org/abs/2602.14553)
*Qinqi Lin,Ningning Ding,Lingjie Duan,Jianwei Huang*

Main category: cs.LG

TL;DR: 提出了首个机器学习遗忘合规性的经济审计框架，将认证遗忘理论与监管执行结合，通过博弈论模型分析审计者与运营者的策略互动，发现审计强度可随删除请求增加而降低。


<details>
  <summary>Details</summary>
Motivation: 尽管有"被遗忘权"的法律要求，AI运营者经常未能遵守数据删除请求。虽然机器学习遗忘提供了技术解决方案，但确保合规性仍然困难，因为存在技术可行性与监管实施之间的根本差距。

Method: 1) 使用认证遗忘的假设检验解释来表征遗忘的固有验证不确定性；2) 提出博弈论模型捕捉审计者与运营者的策略互动；3) 将复杂的双变量非线性固定点问题转化为可处理的单变量辅助问题，解耦系统并建立均衡存在性、唯一性和结构特性。

Result: 反直觉地发现：随着删除请求增加，审计者可以最优地降低检查强度，因为运营者的弱化遗忘使不合规更容易检测。这与最近中国审计减少但删除请求增加的情况一致。此外，证明未披露审计虽然为审计者提供信息优势，但相对于披露审计反而降低监管成本效益。

Conclusion: 提出了首个机器学习遗忘合规性的经济审计框架，解决了技术可行性与监管实施之间的差距。通过理论分析揭示了反直觉的审计策略，为实际监管实践提供了理论指导。

Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.

</details>


### [145] [Fluid-Agent Reinforcement Learning](https://arxiv.org/abs/2602.14559)
*Shishir Sharma,Doina Precup,Theodore J. Perkins*

Main category: cs.LG

TL;DR: 本文提出流体智能体环境框架，允许智能体动态创建其他智能体，解决了传统多智能体强化学习中智能体数量固定的限制，并展示了该框架能产生根据环境需求动态调整规模的智能体团队。


<details>
  <summary>Details</summary>
Motivation: 现实世界中智能体数量通常不固定且未知，智能体可以创建其他智能体（如细胞分裂、公司分拆部门），而传统多智能体强化学习只研究固定数量智能体在环境中的交互，存在局限性。

Method: 提出流体智能体环境框架，允许智能体动态创建其他智能体；为流体智能体游戏提出博弈论解决方案概念；在流体变体的Predator-Prey和Level-Based Foraging基准环境中，以及新引入的环境中，实证评估多种MARL算法性能。

Result: 实验证明该框架能产生根据环境需求动态调整规模的智能体团队，流体性能够解锁固定群体设置中无法观察到的新颖解决方案策略。

Conclusion: 流体智能体框架扩展了多智能体强化学习的边界，使智能体能够动态调整团队规模以适应环境需求，为解决现实世界中动态变化的智能体群体问题提供了新途径。

Abstract: The primary focus of multi-agent reinforcement learning (MARL) has been to study interactions among a fixed number of agents embedded in an environment. However, in the real world, the number of agents is neither fixed nor known a priori. Moreover, an agent can decide to create other agents (for example, a cell may divide, or a company may spin off a division). In this paper, we propose a framework that allows agents to create other agents; we call this a fluid-agent environment. We present game-theoretic solution concepts for fluid-agent games and empirically evaluate the performance of several MARL algorithms within this framework. Our experiments include fluid variants of established benchmarks such as Predator-Prey and Level-Based Foraging, where agents can dynamically spawn, as well as a new environment we introduce that highlights how fluidity can unlock novel solution strategies beyond those observed in fixed-population settings. We demonstrate that this framework yields agent teams that adjust their size dynamically to match environmental demands.

</details>


### [146] [DCTracks: An Open Dataset for Machine Learning-Based Drift Chamber Track Reconstruction](https://arxiv.org/abs/2602.14571)
*Qian Liyan,Zhang Yao,Yuan Ye,Zhang Zhaoke,Fang Jin,Jiang Shimiao,Zhang Jin,Li Ke,Liu Beijiang,Xu Chenglin,Zhang Yifan,Jia Xiaoqian,Qin Xiaoshuai,Huang Xingtao*

Main category: cs.LG

TL;DR: 提出一个用于机器学习轨道重建的蒙特卡洛数据集，包含单轨和双轨事件，并定义了标准化评估指标


<details>
  <summary>Details</summary>
Motivation: 为机器学习轨道重建研究提供标准化的数据集和评估框架，促进可比较、可复现的研究

Method: 创建蒙特卡洛数据集，定义轨道重建专用评估指标，比较传统算法和图神经网络方法

Result: 建立了标准化的评估框架，为未来研究提供了可比较的基准结果

Conclusion: 该数据集和评估指标将促进机器学习轨道重建领域的严谨、可复现研究

Abstract: We introduce a Monte Carlo (MC) dataset of single- and two-track drift chamber events to advance Machine Learning (ML)-based track reconstruction. To enable standardized and comparable evaluation, we define track reconstruction specific metrics and report results for traditional track reconstruction algorithms and a Graph Neural Networks (GNNs) method, facilitating rigorous, reproducible validation for future research.

</details>


### [147] [RNM-TD3: N:M Semi-structured Sparse Reinforcement Learning From Scratch](https://arxiv.org/abs/2602.14578)
*Isam Vrce,Andreas Kassler,Gökçe Aydos*

Main category: cs.LG

TL;DR: 首次在强化学习中研究N:M结构化稀疏性，提出RNM-TD3框架，在保持硬件加速兼容性的同时实现高性能稀疏网络训练。


<details>
  <summary>Details</summary>
Motivation: 现有DRL稀疏化方法多为非结构化细粒度稀疏，限制了硬件加速机会；结构化粗粒度稀疏虽能加速但通常性能下降且剪枝复杂。需要平衡压缩、性能和硬件效率的稀疏化方法。

Method: 提出RNM-TD3框架，在离策略RL（TD3）的所有网络中强制实施行级N:M稀疏性训练，保持与支持N:M稀疏矩阵运算的加速器兼容。

Result: 在连续控制基准测试中，RNM-TD3在50%-75%稀疏度（2:4和1:4）下优于密集对应模型，在Ant环境中2:4稀疏度下性能提升达14%。即使在87.5%稀疏度（1:8）下仍保持竞争力，同时实现潜在训练加速。

Conclusion: N:M结构化稀疏在强化学习中有效平衡了压缩、性能和硬件效率，为高效DRL训练提供了有前景的方向。

Abstract: Sparsity is a well-studied technique for compressing deep neural networks (DNNs) without compromising performance. In deep reinforcement learning (DRL), neural networks with up to 5% of their original weights can still be trained with minimal performance loss compared to their dense counterparts. However, most existing methods rely on unstructured fine-grained sparsity, which limits hardware acceleration opportunities due to irregular computation patterns. Structured coarse-grained sparsity enables hardware acceleration, yet typically degrades performance and increases pruning complexity. In this work, we present, to the best of our knowledge, the first study on N:M structured sparsity in RL, which balances compression, performance, and hardware efficiency. Our framework enforces row-wise N:M sparsity throughout training for all networks in off-policy RL (TD3), maintaining compatibility with accelerators that support N:M sparse matrix operations. Experiments on continuous-control benchmarks show that RNM-TD3, our N:M sparse agent, outperforms its dense counterpart at 50%-75% sparsity (e.g., 2:4 and 1:4), achieving up to a 14% increase in performance at 2:4 sparsity on the Ant environment. RNM-TD3 remains competitive even at 87.5% sparsity (1:8), while enabling potential training speedups.

</details>


### [148] [Decoupled Continuous-Time Reinforcement Learning via Hamiltonian Flow](https://arxiv.org/abs/2602.14587)
*Minh Nguyen*

Main category: cs.LG

TL;DR: 提出一种解耦的连续时间演员-评论家算法，通过交替更新解决标准离散时间RL在连续时间控制问题中的局限性，在连续控制基准和真实交易任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 许多现实控制问题（金融、机器人等）在连续时间中演化，具有非均匀、事件驱动的决策。标准离散时间RL基于固定步长Bellman更新，在这种设置下表现不佳：随着时间间隔缩小，Q函数会坍缩到价值函数V，消除动作排序。现有连续时间方法通过优势率函数q重新引入动作信息，但使用复杂的鞅损失或正交约束来强制最优性，这些方法对测试过程的选择敏感，并将V和q纠缠成难以可靠训练的大型复杂优化问题。

Method: 提出一种新颖的解耦连续时间演员-评论家算法，采用交替更新：q从V的扩散生成器中学习，V通过基于哈密顿量的价值流更新，该价值流在无限小时间步下仍保持信息性（而标准的max/softmax备份会失效）。

Result: 理论上通过新的概率论证证明了严格收敛，绕过了生成器基哈密顿量在sup-norm下缺乏Bellman式收缩的挑战。实证上，该方法在具有挑战性的连续控制基准和真实世界交易任务中优于先前的连续时间和领先的离散时间基线，在单个季度内实现21%的利润——几乎是第二好方法的两倍。

Conclusion: 该方法成功解决了连续时间RL中的关键挑战，通过解耦优化和交替更新实现了更可靠和有效的训练，在理论和实证上都表现出优越性能。

Abstract: Many real-world control problems, ranging from finance to robotics, evolve in continuous time with non-uniform, event-driven decisions. Standard discrete-time reinforcement learning (RL), based on fixed-step Bellman updates, struggles in this setting: as time gaps shrink, the $Q$-function collapses to the value function $V$, eliminating action ranking. Existing continuous-time methods reintroduce action information via an advantage-rate function $q$. However, they enforce optimality through complicated martingale losses or orthogonality constraints, which are sensitive to the choice of test processes. These approaches entangle $V$ and $q$ into a large, complex optimization problem that is difficult to train reliably. To address these limitations, we propose a novel decoupled continuous-time actor-critic algorithm with alternating updates: $q$ is learned from diffusion generators on $V$, and $V$ is updated via a Hamiltonian-based value flow that remains informative under infinitesimal time steps, where standard max/softmax backups fail. Theoretically, we prove rigorous convergence via new probabilistic arguments, sidestepping the challenge that generator-based Hamiltonians lack Bellman-style contraction under the sup-norm. Empirically, our method outperforms prior continuous-time and leading discrete-time baselines across challenging continuous-control benchmarks and a real-world trading task, achieving 21% profit over a single quarter$-$nearly doubling the second-best method.

</details>


### [149] [OPBench: A Graph Benchmark to Combat the Opioid Crisis](https://arxiv.org/abs/2602.14602)
*Tianyi Ma,Yiyang Li,Yiyue Qian,Zheyuan Zhang,Zehong Wang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: OPBench是首个全面的阿片类药物危机基准测试，包含5个数据集和3个关键应用领域，为图学习方法提供标准化评估框架。


<details>
  <summary>Details</summary>
Motivation: 阿片类药物危机持续恶化，需要有效的计算方法应对。虽然图学习方法在建模复杂药物相关现象方面显示出潜力，但缺乏系统评估这些方法在真实世界阿片危机场景中的综合基准。

Method: 创建OPBench基准，包含五个数据集覆盖三个关键应用领域：医疗索赔中的阿片类药物过量检测、数字平台中的非法药物贩运检测、以及饮食模式中的药物滥用预测。基准包含异构图和超图等多样图结构，并与领域专家合作进行数据标注，遵循隐私和伦理准则。建立了统一的评估框架，包含标准化协议、预定义数据分割和可复现基线。

Result: 通过大量实验分析了现有图学习方法的优势和局限性，为未来阿片危机研究提供了可操作的见解。源代码和数据集已公开。

Conclusion: OPBench填补了阿片危机研究中的关键空白，为系统评估图学习方法提供了首个综合基准，有助于推动对抗阿片危机的计算研究。

Abstract: The opioid epidemic continues to ravage communities worldwide, straining healthcare systems, disrupting families, and demanding urgent computational solutions. To combat this lethal opioid crisis, graph learning methods have emerged as a promising paradigm for modeling complex drug-related phenomena. However, a significant gap remains: there is no comprehensive benchmark for systematically evaluating these methods across real-world opioid crisis scenarios. To bridge this gap, we introduce OPBench, the first comprehensive opioid benchmark comprising five datasets across three critical application domains: opioid overdose detection from healthcare claims, illicit drug trafficking detection from digital platforms, and drug misuse prediction from dietary patterns. Specifically, OPBench incorporates diverse graph structures, including heterogeneous graphs and hypergraphs, to preserve the rich and complex relational information among drug-related data. To address data scarcity, we collaborate with domain experts and authoritative institutions to curate and annotate datasets while adhering to privacy and ethical guidelines. Furthermore, we establish a unified evaluation framework with standardized protocols, predefined data splits, and reproducible baselines to facilitate fair and systematic comparison among graph learning methods. Through extensive experiments, we analyze the strengths and limitations of existing graph learning methods, thereby providing actionable insights for future research in combating the opioid crisis. Our source code and datasets are available at https://github.com/Tianyi-Billy-Ma/OPBench.

</details>


### [150] [Concepts' Information Bottleneck Models](https://arxiv.org/abs/2602.14626)
*Karim Galliamov,Syed M Ahsan Kazmi,Adil Khan,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 引入信息瓶颈正则化到概念瓶颈模型，通过惩罚I(X;C)同时保留I(C;Y)，实现最小充分概念表示，提升预测性能和概念干预可靠性


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型存在准确率下降和概念泄漏问题，影响模型的可解释性和忠实性，需要一种理论基础的改进方法

Method: 在概念层引入显式信息瓶颈正则化器，惩罚输入与概念间的互信息I(X;C)，同时保留概念与标签间的互信息I(C;Y)。提出两种变体：变分目标和基于熵的替代方法，无需架构更改或额外监督

Result: 在六个CBM家族和三个基准测试中，IB正则化模型始终优于原始版本。信息平面分析证实了预期行为，表明最小充分概念瓶颈能同时提升预测性能和概念干预可靠性

Conclusion: 信息瓶颈正则化为概念瓶颈模型提供了理论基础、架构无关的改进路径，解决了先前评估不一致问题，展示了跨模型家族和数据集的稳健增益

Abstract: Concept Bottleneck Models (CBMs) aim to deliver interpretable predictions by routing decisions through a human-understandable concept layer, yet they often suffer reduced accuracy and concept leakage that undermines faithfulness. We introduce an explicit Information Bottleneck regularizer on the concept layer that penalizes $I(X;C)$ while preserving task-relevant information in $I(C;Y)$, encouraging minimal-sufficient concept representations. We derive two practical variants (a variational objective and an entropy-based surrogate) and integrate them into standard CBM training without architectural changes or additional supervision. Evaluated across six CBM families and three benchmarks, the IB-regularized models consistently outperform their vanilla counterparts. Information-plane analyses further corroborate the intended behavior. These results indicate that enforcing a minimal-sufficient concept bottleneck improves both predictive performance and the reliability of concept-level interventions. The proposed regularizer offers a theoretic-grounded, architecture-agnostic path to more faithful and intervenable CBMs, resolving prior evaluation inconsistencies by aligning training protocols and demonstrating robust gains across model families and datasets.

</details>


### [151] [Alignment Adapter to Improve the Performance of Compressed Deep Learning Models](https://arxiv.org/abs/2602.14635)
*Rohit Raj Rai,Abhishek Dhaka,Amit Awekar*

Main category: cs.LG

TL;DR: 提出Alignment Adapter (AlAd)轻量级适配器，通过滑动窗口对齐压缩模型与原大模型的token级嵌入，提升压缩模型性能


<details>
  <summary>Details</summary>
Motivation: 压缩深度学习模型在资源受限环境中部署很重要，但其性能通常不如原始大模型，需要缩小这一性能差距

Method: 设计基于滑动窗口的轻量级适配器AlAd，对齐压缩模型与原始大模型的token级嵌入，保持局部上下文语义，支持不同维度或架构的灵活对齐，与压缩方法无关

Result: 在BERT系列模型和三个token级NLP任务上的实验表明，AlAd显著提升压缩模型性能，仅带来微小的尺寸和延迟开销

Conclusion: AlAd是一种有效的轻量级适配器，能够显著提升压缩模型性能，支持即插即用或联合微调两种部署方式，具有实际应用价值

Abstract: Compressed Deep Learning (DL) models are essential for deployment in resource-constrained environments. But their performance often lags behind their large-scale counterparts. To bridge this gap, we propose Alignment Adapter (AlAd): a lightweight, sliding-window-based adapter. It aligns the token-level embeddings of a compressed model with those of the original large model. AlAd preserves local contextual semantics, enables flexible alignment across differing dimensionalities or architectures, and is entirely agnostic to the underlying compression method. AlAd can be deployed in two ways: as a plug-and-play module over a frozen compressed model, or by jointly fine-tuning AlAd with the compressed model for further performance gains. Through experiments on BERT-family models across three token-level NLP tasks, we demonstrate that AlAd significantly boosts the performance of compressed models with only marginal overhead in size and latency.

</details>


### [152] [An Embarrassingly Simple Way to Optimize Orthogonal Matrices at Scale](https://arxiv.org/abs/2602.14656)
*Adrián Javaloy,Antonio Vergari*

Main category: cs.LG

TL;DR: 提出POGO算法，一种快速GPU友好的正交约束优化器，相比现有方法显著提升效率，能在几分钟内优化包含数千个正交矩阵的问题。


<details>
  <summary>Details</summary>
Motivation: 正交约束在鲁棒和概率机器学习中普遍存在，但现有优化器计算成本高，难以扩展到包含数百或数千个约束的问题。虽然Landing算法是一个例外，但它以暂时放松正交性为代价。

Method: 改进Landing算法的思想，结合现代自适应优化器，确保正交约束有效满足。POGO算法仅需5个矩阵乘积，GPU友好，在实践中始终保持正交性。

Result: 在多个具有挑战性的基准测试中，POGO大幅优于最近的优化器，能在几分钟内优化包含数千个正交矩阵的问题，而替代方法需要数小时。

Conclusion: POGO为在机器学习中大规模利用正交约束设定了里程碑，其PyTorch实现已公开可用。

Abstract: Orthogonality constraints are ubiquitous in robust and probabilistic machine learning. Unfortunately, current optimizers are computationally expensive and do not scale to problems with hundreds or thousands of constraints. One notable exception is the Landing algorithm (Ablin et al., 2024) which, however comes at the expense of temporarily relaxing orthogonality. In this work, we revisit and improve on the ideas behind Landing, enabling the inclusion of modern adaptive optimizers while ensuring that orthogonal constraints are effectively met. Remarkably, these improvements come at little to no cost, and reduce the number of required hyperparemeters. Our algorithm POGO is fast and GPU-friendly, consisting of only 5 matrix products, and in practice maintains orthogonality at all times. On several challenging benchmarks, POGO greatly outperforms recent optimizers and shows it can optimize problems with thousands of orthogonal matrices in minutes while alternatives would take hours. As such, POGO sets a milestone to finally exploit orthogonality constraints in ML at scale. A PyTorch implementation of POGO is publicly available at https://github.com/adrianjav/pogo.

</details>


### [153] [Pseudo-differential-enhanced physics-informed neural networks](https://arxiv.org/abs/2602.14663)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 提出伪微分增强物理信息神经网络(PINNs)，通过在傅里叶空间进行梯度增强，改善训练效果和学习精度，适用于分数阶导数，能提升神经正切核的谱特征值衰减。


<details>
  <summary>Details</summary>
Motivation: 传统梯度增强PINNs在物理空间进行高阶微分，本文提出在傅里叶空间进行类似操作，因为傅里叶空间中的微分等价于与波数相乘，计算更高效，且能更好地处理高频成分。

Method: 将PINNs的PDE残差通过傅里叶变换转换到频域，在傅里叶空间进行微分增强（乘以波数），然后添加为目标函数的增强项。支持蒙特卡洛方法实现网格灵活性，兼容傅里叶特征嵌入等先进技术。

Result: 方法快速高效，在较少训练迭代中实现优于数值解的PINN误差，在低配置点设置中能打破平台期，改善神经正切核的谱特征值衰减，促进早期训练中高频成分的学习。

Conclusion: 伪微分增强PINNs通过傅里叶空间梯度增强有效缓解频率偏差问题，提升训练效率和精度，适用于分数阶导数和多种网格设置，为PINNs训练提供了新的增强策略。

Abstract: We present pseudo-differential enhanced physics-informed neural networks (PINNs), an extension of gradient enhancement but in Fourier space. Gradient enhancement of PINNs dictates that the PDE residual is taken to a higher differential order than prescribed by the PDE, added to the objective as an augmented term in order to improve training and overall learning fidelity. We propose the same procedure after application via Fourier transforms, since differentiating in Fourier space is multiplication with the Fourier wavenumber under suitable decay. Our methods are fast and efficient. Our methods oftentimes achieve superior PINN versus numerical error in fewer training iterations, potentially pair well with few samples in collocation, and can on occasion break plateaus in low collocation settings. Moreover, our methods are suitable for fractional derivatives. We establish that our methods improve spectral eigenvalue decay of the neural tangent kernel (NTK), and so our methods contribute towards the learning of high frequencies in early training, mitigating the effects of frequency bias up to the polynomial order and possibly greater with smooth activations. Our methods accommodate advanced techniques in PINNs, such as Fourier feature embeddings. A pitfall of discrete Fourier transforms via the Fast Fourier Transform (FFT) is mesh subjugation, and so we demonstrate compatibility of our methods for greater mesh flexibility and invariance on alternative Euclidean and non-Euclidean domains via Monte Carlo methods and otherwise.

</details>


### [154] [Exposing Diversity Bias in Deep Generative Models: Statistical Origins and Correction of Diversity Error](https://arxiv.org/abs/2602.14682)
*Farzan Farnia,Mohammad Jalali,Azim Ospanov*

Main category: cs.LG

TL;DR: 研究发现现代生成模型存在系统性多样性低估偏差，测试数据比生成样本具有更高的多样性分数，这源于有限样本下多样性估计的固有偏差。


<details>
  <summary>Details</summary>
Motivation: 虽然深度生成模型在生成高质量样本方面取得了巨大成功，但模型是否忠实捕捉底层数据分布的多样性这一重要问题尚未得到系统研究。本文旨在探究生成模型在多样性方面的表现。

Method: 使用最近提出的无参考熵基多样性评分方法Vendi和RKE，直接比较最先进模型生成的样本与从目标数据分布中抽取的测试样本的多样性。分析有限样本下熵基多样性评分的行为，并探讨基于Vendi和RKE的多样性感知正则化和引导策略。

Result: 在多个基准数据集上，测试数据始终获得比生成样本显著更高的Vendi和RKE多样性分数，表明现代生成模型存在系统性多样性低估偏差。分析显示熵基多样性评分的期望值随样本量增加而增加，导致基于有限训练集的多样性估计固有地低估真实分布的多样性。

Conclusion: 优化生成器以最小化与经验数据分布的散度会导致多样性损失。基于Vendi和RKE的多样性感知正则化和引导策略是缓解这种偏差的有原则方向，实证证据表明这些策略有潜力改善结果。

Abstract: Deep generative models have achieved great success in producing high-quality samples, making them a central tool across machine learning applications. Beyond sample quality, an important yet less systematically studied question is whether trained generative models faithfully capture the diversity of the underlying data distribution. In this work, we address this question by directly comparing the diversity of samples generated by state-of-the-art models with that of test samples drawn from the target data distribution, using recently proposed reference-free entropy-based diversity scores, Vendi and RKE. Across multiple benchmark datasets, we find that test data consistently attains substantially higher Vendi and RKE diversity scores than the generated samples, suggesting a systematic downward diversity bias in modern generative models. To understand the origin of this bias, we analyze the finite-sample behavior of entropy-based diversity scores and show that their expected values increase with sample size, implying that diversity estimated from finite training sets could inherently underestimate the diversity of the true distribution. As a result, optimizing the generators to minimize divergence to empirical data distributions would induce a loss of diversity. Finally, we discuss potential diversity-aware regularization and guidance strategies based on Vendi and RKE as principled directions for mitigating this bias, and provide empirical evidence suggesting their potential to improve the results.

</details>


### [155] [SynthSAEBench: Evaluating Sparse Autoencoders on Scalable Realistic Synthetic Data](https://arxiv.org/abs/2602.14687)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: SynthSAEBench是一个用于评估稀疏自编码器架构的工具包，通过生成具有真实特征的大规模合成数据，提供标准化基准模型，帮助研究人员精确诊断SAE故障模式。


<details>
  <summary>Details</summary>
Motivation: 当前SAE基准测试在LLMs上噪声太大，无法区分架构改进；而合成数据实验规模太小且不现实，无法提供有意义的比较。需要更好的基准来验证SAE架构创新。

Method: 引入SynthSAEBench工具包，生成具有相关性、层次结构和叠加等真实特征的大规模合成数据，并创建标准化基准模型SynthSAEBench-16k，实现SAE架构的直接比较。

Result: 基准重现了多个先前观察到的LLM SAE现象，包括重建与潜在质量指标的脱节、SAE探测结果不佳、以及由L0介导的精确率-召回率权衡。还发现了一个新的故障模式：匹配追踪SAE利用叠加噪声改进重建而不学习真实特征。

Conclusion: SynthSAEBench通过提供真实特征和受控消融实验，补充了LLM基准测试，使研究人员能够精确诊断SAE故障模式，并在扩展到LLMs之前验证架构改进。

Abstract: Improving Sparse Autoencoders (SAEs) requires benchmarks that can precisely validate architectural innovations. However, current SAE benchmarks on LLMs are often too noisy to differentiate architectural improvements, and current synthetic data experiments are too small-scale and unrealistic to provide meaningful comparisons. We introduce SynthSAEBench, a toolkit for generating large-scale synthetic data with realistic feature characteristics including correlation, hierarchy, and superposition, and a standardized benchmark model, SynthSAEBench-16k, enabling direct comparison of SAE architectures. Our benchmark reproduces several previously observed LLM SAE phenomena, including the disconnect between reconstruction and latent quality metrics, poor SAE probing results, and a precision-recall trade-off mediated by L0. We further use our benchmark to identify a new failure mode: Matching Pursuit SAEs exploit superposition noise to improve reconstruction without learning ground-truth features, suggesting that more expressive encoders can easily overfit. SynthSAEBench complements LLM benchmarks by providing ground-truth features and controlled ablations, enabling researchers to precisely diagnose SAE failure modes and validate architectural improvements before scaling to LLMs.

</details>


### [156] [A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)](https://arxiv.org/abs/2602.14696)
*Nihal V. Nayak,Paula Rodriguez-Diaz,Neha Hulkund,Sara Beery,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 本文系统分析了指令选择的两个核心要素：数据表示和选择算法，发现基于梯度的数据表示能一致预测性能，而贪心轮询算法在低预算下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 指令微调中的目标指令选择研究目前存在碎片化和不透明的问题：方法差异大、常忽略零样本基线、关键组件贡献混淆，导致实践者缺乏可操作的指导。

Method: 提出一个框架来解耦和系统分析数据表示和选择算法两个核心要素，支持跨模型、任务和预算的受控比较，并将多种现有算法统一为近似距离最小化形式。

Result: 发现只有基于梯度的数据表示能一致预测性能；贪心轮询算法在低预算下平均表现最佳；将多种选择算法统一为近似距离最小化，并提供了新的泛化边界支持。

Conclusion: 研究为LLM微调中的指令选择提供了关键见解和更原则化的基础，基于梯度的表示和贪心轮询算法在低预算下是有效组合，但大预算下优势减弱。

Abstract: Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.

</details>


### [157] [D2-LoRA: A Synergistic Approach to Differential and Directional Low-Rank Adaptation](https://arxiv.org/abs/2602.14728)
*Nozomu Fujisawa,Masaaki Kondo*

Main category: cs.LG

TL;DR: D2-LoRA：一种参数高效微调方法，通过带符号的低秩残差更新和列向投影，在少量训练数据下实现高性能，同时保持推理时的代数可合并性。


<details>
  <summary>Details</summary>
Motivation: 在有限数据和计算资源下，系统探索参数高效微调的设计空间，寻求在保持推理效率的同时提升性能的方法。

Method: 结合带符号的低秩残差更新（加减分量）和训练时的列向投影，保持每列接近原始范数，训练后将适配器合并为单一权重矩阵。

Result: 在8个问答和阅读理解基准上平均准确率达76.4%（仅用5k样本/任务，2个epoch），比LoRA提升2.2个百分点；在生成任务上提升1.2 ROUGE-L和1.1%胜率；训练波动降低36%，推理吞吐量提升1.91倍。

Conclusion: D2-LoRA在有限数据下实现了高性能参数高效微调，通过创新的架构设计而非增加参数数量获得提升，同时保持推理时的零延迟和数值保真度。

Abstract: We systematically investigate the parameter-efficient fine-tuning design space under practical data and compute constraints, and propose D2-LoRA. D2-LoRA achieves 76.4 percent average accuracy across eight question answering and reading comprehension benchmarks using only 5k training samples per task and two epochs, while preserving algebraic mergeability at inference with near-exact numerical equivalence. The method combines signed low-rank residual updates with additive and subtractive components, together with a train-time column-wise projection that keeps each column close to its original norm. After training, the adapter is merged into a single weight matrix, adding zero inference latency. Compared with LoRA, D2-LoRA improves average accuracy by 2.2 percentage points; at matched parameter counts (LoRA rank 2r versus D2-LoRA rank r), the improvement is 1.6 points, indicating gains from architectural design rather than increased parameterization. Compared with DoRA, it matches or exceeds performance on most tasks. Beyond QA and reading comprehension, D2-LoRA improves generative tasks (plus 1.2 ROUGE-L and plus 1.1 percent win rate) and shows 36 percent lower training volatility. The merge preserves numerical fidelity (mean gap about 0.03 percentage points) and recovers about 1.91x evaluation throughput. Training overhead is 19 percent, comparable to DoRA, and decreases with longer input sequences. We provide a geometric analysis explaining how the projection stabilizes training, together with ablation studies isolating the contribution of each design component.

</details>


### [158] [Scale redundancy and soft gauge fixing in positively homogeneous neural networks](https://arxiv.org/abs/2602.14729)
*Rodrigo Carmo Terin*

Main category: cs.LG

TL;DR: 该论文提出将神经网络中的正齐次激活函数尺度变换对称性解释为规范冗余，引入规范适应坐标和软轨道选择惩罚来改善优化条件


<details>
  <summary>Details</summary>
Motivation: 神经网络中正齐次激活函数存在精确的连续重参数化对称性（神经元尺度变换），这种对称性导致参数空间存在冗余轨道，可能影响优化过程。作者希望利用规范场论的思想来解决这一问题

Method: 1) 将尺度对称性解释为规范冗余；2) 引入规范适应坐标分离不变方向和尺度不平衡方向；3) 借鉴场论中的规范固定概念，提出软轨道选择（范数平衡）泛函；4) 该泛函仅作用于冗余尺度坐标，诱导不平衡模式的耗散松弛

Result: 理论分析表明轨道选择惩罚能诱导不平衡模式的耗散松弛以保持实现函数；实验显示该惩罚能扩大稳定学习率范围、抑制尺度漂移，且不改变表达能力

Conclusion: 建立了规范轨道几何与优化条件之间的结构联系，为规范理论概念与机器学习提供了具体连接，展示了利用规范理论思想改善神经网络优化的潜力

Abstract: Neural networks with positively homogeneous activations exhibit an exact continuous reparametrization symmetry: neuron-wise rescalings generate parameter-space orbits along which the input--output function is invariant. We interpret this symmetry as a gauge redundancy and introduce gauge-adapted coordinates that separate invariant and scale-imbalance directions. Inspired by gauge fixing in field theory, we introduce a soft orbit-selection (norm-balancing) functional acting only on redundant scale coordinates. We show analytically that it induces dissipative relaxation of imbalance modes to preserve the realized function. In controlled experiments, this orbit-selection penalty expands the stable learning-rate regime and suppresses scale drift without changing expressivity. These results establish a structural link between gauge-orbit geometry and optimization conditioning, providing a concrete connection between gauge-theoretic concepts and machine learning.

</details>


### [159] [Inner Loop Inference for Pretrained Transformers: Unlocking Latent Capabilities Without Training](https://arxiv.org/abs/2602.14759)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.LG

TL;DR: 提出推理时内部循环方法，通过重复应用预训练语言模型中的选定块范围来延长语义精炼过程，在冻结模型上实现精度提升


<details>
  <summary>Details</summary>
Motivation: 基于Transformer架构中内部表示可视为潜在表示的迭代精炼这一观察，以及早期层可能共享内部空间、某些层主要起精炼作用的假设，探索通过延长精炼过程来提升模型性能

Method: 提出推理时内部循环方法：在冻结的预训练语言模型中，重复应用选定的Transformer块范围，延长语义精炼过程而不改变模型参数

Result: 在多个基准测试中，内部循环方法带来了适度但一致的精度提升；潜在轨迹分析显示更稳定的状态演化和持续的语义精炼

Conclusion: 通过简单的测试时循环可以在冻结的预训练模型中扩展计算，获得额外的精炼效果，这支持了内部表示作为迭代精炼过程的观点

Abstract: Deep Learning architectures, and in particular Transformers, are conventionally viewed as a composition of layers. These layers are actually often obtained as the sum of two contributions: a residual path that copies the input and the output of a Transformer block. As a consequence, the inner representations (i.e. the input of these blocks) can be interpreted as iterative refinement of a propagated latent representation. Under this lens, many works suggest that the inner space is shared across layers, meaning that tokens can be decoded at early stages. Mechanistic interpretability even goes further by conjecturing that some layers act as refinement layers. Following this path, we propose inference-time inner looping, which prolongs refinement in pretrained off-the-shelf language models by repeatedly re-applying a selected block range. Across multiple benchmarks, inner looping yields modest but consistent accuracy improvements. Analyses of the resulting latent trajectories suggest more stable state evolution and continued semantic refinement. Overall, our results suggest that additional refinement can be obtained through simple test-time looping, extending computation in frozen pretrained models.

</details>


### [160] [Universal Algorithm-Implicit Learning](https://arxiv.org/abs/2602.14761)
*Stefano Woerner,Seong Joon Oh,Christian F. Baumgartner*

Main category: cs.LG

TL;DR: 提出TAIL，一个基于Transformer的算法隐式元学习器，能够在不同领域、模态和标签配置的任务中工作，实现了最先进的少样本学习性能，并能泛化到未见过的模态和更大标签空间。


<details>
  <summary>Details</summary>
Motivation: 当前元学习方法局限于狭窄的任务分布和固定的特征/标签空间，且文献中"通用"等术语使用不一致、缺乏精确定义，限制了方法的可比性和实际应用。

Method: 提出理论框架明确定义实用通用性，区分算法显式和算法隐式学习。基于此框架开发TAIL，采用三个创新：跨模态特征编码的随机投影、可外推到更大标签空间的随机注入标签嵌入、高效的内联查询处理。

Result: TAIL在标准少样本基准上达到最先进性能，能泛化到未见过的领域和模态（如仅用图像训练却能解决文本分类任务），处理比训练时多20倍的类别数，计算效率比之前基于Transformer的方法提高数个数量级。

Conclusion: 提出的理论框架为通用元学习提供了原则性词汇，TAIL展示了算法隐式元学习的强大能力，在跨模态、跨领域和可扩展性方面超越了现有方法，为更通用的元学习系统奠定了基础。

Abstract: Current meta-learning methods are constrained to narrow task distributions with fixed feature and label spaces, limiting applicability. Moreover, the current meta-learning literature uses key terms like "universal" and "general-purpose" inconsistently and lacks precise definitions, hindering comparability. We introduce a theoretical framework for meta-learning which formally defines practical universality and introduces a distinction between algorithm-explicit and algorithm-implicit learning, providing a principled vocabulary for reasoning about universal meta-learning methods. Guided by this framework, we present TAIL, a transformer-based algorithm-implicit meta-learner that functions across tasks with varying domains, modalities, and label configurations. TAIL features three innovations over prior transformer-based meta-learners: random projections for cross-modal feature encoding, random injection label embeddings that extrapolate to larger label spaces, and efficient inline query processing. TAIL achieves state-of-the-art performance on standard few-shot benchmarks while generalizing to unseen domains. Unlike other meta-learning methods, it also generalizes to unseen modalities, solving text classification tasks despite training exclusively on images, handles tasks with up to 20$\times$ more classes than seen during training, and provides orders-of-magnitude computational savings over prior transformer-based approaches.

</details>


### [161] [Learning Structural Hardness for Combinatorial Auctions: Instance-Dependent Algorithm Selection via Graph Neural Networks](https://arxiv.org/abs/2602.14772)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 论文提出一种机器学习方法预测组合拍卖中贪婪算法的难度，通过分类器识别困难实例，然后使用GNN专家求解，实现算法选择而非替代传统求解器。


<details>
  <summary>Details</summary>
Motivation: 组合拍卖的胜者确定问题（WDP）是NP难问题，现有方法无法可靠预测哪些实例会击败快速贪婪启发式算法。虽然机器学习社区专注于学习替代求解器，但图神经网络（GNNs）在标准基准测试中很少能超越精心调优的经典方法。

Method: 设计20维结构特征向量，训练轻量级MLP难度分类器预测贪婪最优性差距；对于被识别为困难的实例（呈现"鲸鱼-小鱼"陷阱结构），部署异构GNN专家求解器；构建结合难度分类器、GNN和贪婪求解器的混合分配器。

Result: 难度分类器预测贪婪最优性差距的平均绝对误差为0.033，皮尔逊相关系数0.937，二元分类准确率94.7%；GNN专家在6种对抗配置上实现约0%最优性差距（贪婪算法为3.75-59.24%）；混合分配器在混合分布上实现0.51%总体差距。

Conclusion: 学习何时部署昂贵求解器比学习替代它们更可行；算法选择框架比直接替代传统求解器更有效，在CATS基准测试中GNNs未能超越Gurobi（0.45-0.71 vs. 0.20差距）。

Abstract: The Winner Determination Problem (WDP) in combinatorial auctions is NP-hard, and no existing method reliably predicts which instances will defeat fast greedy heuristics. The ML-for-combinatorial-optimization community has focused on learning to \emph{replace} solvers, yet recent evidence shows that graph neural networks (GNNs) rarely outperform well-tuned classical methods on standard benchmarks. We pursue a different objective: learning to predict \emph{when} a given instance is hard for greedy allocation, enabling instance-dependent algorithm selection. We design a 20-dimensional structural feature vector and train a lightweight MLP hardness classifier that predicts the greedy optimality gap with mean absolute error 0.033, Pearson correlation 0.937, and binary classification accuracy 94.7\% across three random seeds. For instances identified as hard -- those exhibiting ``whale-fish'' trap structure where greedy provably fails -- we deploy a heterogeneous GNN specialist that achieves ${\approx}0\%$ optimality gap on all six adversarial configurations tested (vs.\ 3.75--59.24\% for greedy). A hybrid allocator combining the hardness classifier with GNN and greedy solvers achieves 0.51\% overall gap on mixed distributions. Our honest evaluation on CATS benchmarks confirms that GNNs do not outperform Gurobi (0.45--0.71 vs.\ 0.20 gap), motivating the algorithm selection framing. Learning \emph{when} to deploy expensive solvers is more tractable than learning to replace them.

</details>


### [162] [Learning State-Tracking from Code Using Linear RNNs](https://arxiv.org/abs/2602.14814)
*Julien Siems,Riccardo Grazzi,Kirill Kalinin,Hitesh Ballani,Babak Rahmani*

Main category: cs.LG

TL;DR: 论文将置换组合任务转化为代码REPL跟踪形式，研究不同序列模型在状态跟踪任务上的表现，发现线性RNN在完全可观测时表现好，但在部分可观测时不如非线性RNN。


<details>
  <summary>Details</summary>
Motivation: 现有状态跟踪任务（如置换组合）通常是序列到序列任务，与语言模型常用的下一个token预测设置不兼容。需要将这类任务转化为适合语言模型训练的形式，以研究不同架构在状态跟踪上的能力差异。

Method: 将置换组合任务通过REPL跟踪转化为代码形式，交织状态显示（prints）和变量转换。研究线性RNN、非线性RNN和Transformer在此设置下的表现。进一步将状态跟踪问题形式化为具有确定性状态显示的随机有限状态自动机。

Result: 线性RNN在完全可观测的代码REPL设置中表现优异，而Transformer仍然失败。但在部分可观测情况下（动作不完全可观测），线性RNN的状态跟踪能力可能比非线性RNN差。

Conclusion: 状态跟踪任务的表示形式对模型性能有重要影响。线性RNN在完全可观测情况下擅长状态跟踪，但在部分可观测场景中非线性RNN可能更优。这为理解不同序列模型架构的能力边界提供了新视角。

Abstract: Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.

</details>


### [163] [Interactionless Inverse Reinforcement Learning: A Data-Centric Framework for Durable Alignment](https://arxiv.org/abs/2602.14844)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出Interactionless Inverse Reinforcement Learning和Alignment Flywheel，将AI对齐从一次性消耗品转变为可验证的工程资产


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在结构性缺陷，将安全目标与智能体策略纠缠在一起，产生不透明、单次使用的对齐产物（Alignment Waste）

Method: 1. Interactionless Inverse Reinforcement Learning：将对齐产物学习与策略优化解耦，产生可检查、可编辑、模型无关的奖励模型
2. Alignment Flywheel：人工参与的循环生命周期，通过自动化审计和精化迭代强化奖励模型

Result: 创建了可检查、可编辑、模型无关的奖励模型，将对齐产物从一次性消耗品转变为可验证的工程资产

Conclusion: 该架构将安全性从一次性消耗品转变为持久、可验证的工程资产，解决了当前对齐方法的结构性缺陷

Abstract: AI alignment is growing in importance, yet current approaches suffer from a critical structural flaw that entangles the safety objectives with the agent's policy. Methods such as Reinforcement Learning from Human Feedback and Direct Preference Optimization create opaque, single-use alignment artifacts, which we term Alignment Waste. We propose Interactionless Inverse Reinforcement Learning to decouple alignment artifact learning from policy optimization, producing an inspectable, editable, and model-agnostic reward model. Additionally, we introduce the Alignment Flywheel, a human-in-the-loop lifecycle that iteratively hardens the reward model through automated audits and refinement. This architecture transforms safety from a disposable expense into a durable, verifiable engineering asset.

</details>


### [164] [Atomix: Timely, Transactional Tool Use for Reliable Agentic Workflows](https://arxiv.org/abs/2602.14849)
*Bardia Mohammadi,Nearchos Potamitis,Lars Klein,Akhil Arora,Laurent Bindschaedler*

Main category: cs.LG

TL;DR: Atomix为LLM智能体工具调用提供进度感知的事务语义，通过epoch标记、资源前沿跟踪和进度谓词控制提交，支持延迟缓冲效果和失败补偿，提高任务成功率并增强隔离性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在外部系统上执行操作时，工具效果是立即生效的。在故障、推测执行或资源竞争情况下，失败的分支可能会泄露意外的副作用，且无法安全回滚，需要一种安全的事务机制。

Method: Atomix运行时为智能体工具调用提供进度感知的事务语义：1) 用epoch标记每个调用；2) 跟踪每个资源的前沿状态；3) 仅当进度谓词指示安全时才提交；4) 可缓冲的效果可以延迟执行；5) 外部化效果被跟踪并在中止时进行补偿。

Result: 在真实工作负载和故障注入测试中：1) 事务重试提高了任务成功率；2) 前沿门控提交在推测执行和资源竞争情况下增强了隔离性。

Conclusion: Atomix通过提供进度感知的事务语义，解决了LLM智能体在外部系统操作中的副作用泄露和回滚问题，提高了系统的可靠性和隔离性。

Abstract: LLM agents increasingly act on external systems, yet tool effects are immediate. Under failures, speculation, or contention, losing branches can leak unintended side effects with no safe rollback. We introduce Atomix, a runtime that provides progress-aware transactional semantics for agent tool calls. Atomix tags each call with an epoch, tracks per-resource frontiers, and commits only when progress predicates indicate safety; bufferable effects can be delayed, while externalized effects are tracked and compensated on abort. Across real workloads with fault injection, transactional retry improves task success, while frontier-gated commit strengthens isolation under speculation and contention.

</details>


### [165] [BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations](https://arxiv.org/abs/2602.14853)
*Jonathan Gorard,Ammar Hakim,James Juno*

Main category: cs.LG

TL;DR: 提出BEACONS框架，通过形式化验证的神经网络求解PDE，即使在训练数据凸包外也能保证正确性，解决了传统神经网络在计算物理中泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在训练数据凸包外泛化能力不可靠，这在计算物理中是个严重问题，因为经常需要在远超实验或解析验证的区域内求解PDE。

Method: 1. 使用特征线法预测PDE解的解析性质，构建浅层神经网络近似的严格外推误差界；2. 将PDE解分解为简单函数组合，基于组合深度学习思想将浅层网络组合成深层架构，抑制大L^∞误差；3. 开发BEACONS框架，包括神经网络求解器的自动代码生成器和定制化自动定理证明系统。

Result: 将框架应用于线性和非线性PDE（线性平流方程、无粘Burgers方程、完全可压缩Euler方程），在1D和2D中展示了BEACONS架构能够以可靠且有界的方式将解外推到远超训练数据的区域。

Conclusion: BEACONS框架通过形式化验证的神经网络求解器，提供了严格的收敛性、稳定性和守恒性保证，即使在极端外推区域也能确保正确性，相比传统PINN方法具有多种优势。

Abstract: The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.

</details>


### [166] [A Pragmatic Method for Comparing Clusterings with Overlaps and Outliers](https://arxiv.org/abs/2602.14855)
*Ryan DeWolfe,Paweł Prałat,François Théberge*

Main category: cs.LG

TL;DR: 提出了一种用于比较包含重叠簇和异常值的聚类结果的相似性度量方法


<details>
  <summary>Details</summary>
Motivation: 现有的聚类比较方法无法处理包含异常值（不属于任何簇的对象）和重叠簇（对象可能属于多个簇）的情况，而实际应用中这两种情况很常见

Method: 定义了一种实用的相似性度量方法，用于比较包含重叠和异常值的聚类结果，该方法具有多个理想性质

Result: 实验证实该方法不受其他聚类比较度量常见的几种偏差影响

Conclusion: 该方法填补了聚类比较方法在处理重叠簇和异常值方面的空白，为聚类算法的外部评估提供了有效工具

Abstract: Clustering algorithms are an essential part of the unsupervised data science ecosystem, and extrinsic evaluation of clustering algorithms requires a method for comparing the detected clustering to a ground truth clustering. In a general setting, the detected and ground truth clusterings may have outliers (objects belonging to no cluster), overlapping clusters (objects may belong to more than one cluster), or both, but methods for comparing these clusterings are currently undeveloped. In this note, we define a pragmatic similarity measure for comparing clusterings with overlaps and outliers, show that it has several desirable properties, and experimentally confirm that it is not subject to several common biases afflicting other clustering comparison measures.

</details>


### [167] [Goldilocks RL: Tuning Task Difficulty to Escape Sparse Rewards for Reasoning](https://arxiv.org/abs/2602.14868)
*Ilia Mahrooghi,Aryo Lotfi,Emmanuel Abbe*

Main category: cs.LG

TL;DR: Goldilocks是一种基于教师模型的数据采样策略，通过预测每个问题对学生的难度，选择"刚刚好"难度的问题进行训练，提高强化学习的样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能解锁大语言模型的推理能力，但依赖稀疏奖励导致样本效率低下。传统课程学习通过复杂度排序数据，但难以确定适合特定模型的最佳排序。

Method: 提出Goldilocks策略：教师模型预测每个问题对学生的难度，选择"既不简单也不困难"的问题（Goldilocks原则），同时用GRPO训练学生模型。教师根据学生在已见样本上的表现持续适应其能力变化。

Result: 在OpenMathReasoning数据集上，Goldilocks数据采样在相同计算预算下，比标准GRPO训练获得了更好的模型性能。

Conclusion: Goldilocks通过教师驱动的自适应数据采样策略，有效提高了强化学习训练大语言模型的样本效率，实现了更好的性能表现。

Abstract: Reinforcement learning has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, relying on sparse rewards makes this process highly sample-inefficient, as models must navigate vast search spaces with minimal feedback. While classic curriculum learning aims to mitigate this by ordering data based on complexity, the right ordering for a specific model is often unclear. To address this, we propose Goldilocks, a novel teacher-driven data sampling strategy that aims to predict each question's difficulty for the student model. The teacher model selects questions of appropriate difficulty for the student model, i.e., questions that are neither too easy nor too hard (Goldilocks principle), while training the student with GRPO. By leveraging the student's performance on seen samples, the teacher continuously adapts to the student's evolving abilities. On OpenMathReasoning dataset, Goldilocks data sampling improves the performance of models trained with standard GRPO under the same compute budget.

</details>


### [168] [Web-Scale Multimodal Summarization using CLIP-Based Semantic Alignment](https://arxiv.org/abs/2602.14889)
*Mounvik K,N Harshit*

Main category: cs.LG

TL;DR: Web-Scale Multimodal Summarization是一个轻量级框架，通过结合从网络检索的文本和图像数据生成摘要。系统基于用户定义的主题进行并行网络、新闻和图像搜索，使用微调的CLIP模型对图像进行语义对齐排序，支持BLIP图像描述功能，并通过Gradio API提供可配置参数。


<details>
  <summary>Details</summary>
Motivation: 当前需要一种能够整合网络文本和图像数据的摘要生成工具，以提供更丰富、多模态的摘要内容，同时保持系统轻量化和可部署性。

Method: 1. 基于用户主题进行并行网络、新闻和图像搜索；2. 使用微调的CLIP模型对检索到的图像进行语义对齐排序；3. 可选BLIP图像描述功能增强多模态连贯性；4. 支持可调节的获取限制、语义过滤、摘要样式等功能；5. 通过Gradio API提供可控参数和预设配置。

Result: 在500个图像-标题对和20:1对比负样本上的评估显示：ROC-AUC为0.9270，F1分数为0.6504，准确率为96.99%，表明系统在多模态对齐方面表现优异。

Conclusion: 该工作提供了一个可配置、可部署的网络规模摘要工具，将语言、检索和视觉模型集成到用户可扩展的流程中，为多模态摘要生成提供了实用解决方案。

Abstract: We introduce Web-Scale Multimodal Summarization, a lightweight framework for generating summaries by combining retrieved text and image data from web sources. Given a user-defined topic, the system performs parallel web, news, and image searches. Retrieved images are ranked using a fine-tuned CLIP model to measure semantic alignment with topic and text. Optional BLIP captioning enables image-only summaries for stronger multimodal coherence.The pipeline supports features such as adjustable fetch limits, semantic filtering, summary styling, and downloading structured outputs. We expose the system via a Gradio-based API with controllable parameters and preconfigured presets.Evaluation on 500 image-caption pairs with 20:1 contrastive negatives yields a ROC-AUC of 0.9270, an F1-score of 0.6504, and an accuracy of 96.99%, demonstrating strong multimodal alignment. This work provides a configurable, deployable tool for web-scale summarization that integrates language, retrieval, and vision models in a user-extensible pipeline.

</details>


### [169] [Algorithmic Simplification of Neural Networks with Mosaic-of-Motifs](https://arxiv.org/abs/2602.14896)
*Pedram Bakhtiarifard,Tong Chen,Jonathan Wenshøj,Erik B Dam,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 论文提出使用算法复杂度视角解释深度神经网络的可压缩性，认为训练后的模型参数比随机初始化具有更低的算法复杂度，并开发了基于可重用模块的MoMos方法来压缩模型。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络表现出惊人的可压缩性（如剪枝、量化、知识蒸馏等方法能大幅减少参数而性能下降很小），但缺乏理论解释。本文旨在从算法复杂度角度解释为什么深度神经网络适合压缩。

Method: 提出MoMos（Mosaic-of-Motifs）方法：将参数划分为大小为s的块，每个块从k个可重用模块中选择，形成重用模式（马赛克）。这种方法约束了参数化，使模型具有更低的算法复杂度。

Result: 实验表明，通过近似Kolmogorov复杂度测量，神经网络的算法复杂度在训练过程中可以降低。MoMos方法产生的模型与无约束模型性能相当，但算法复杂度更低。

Conclusion: 深度神经网络的可压缩性源于训练后参数具有更低的算法复杂度。MoMos方法通过约束参数化为可重用模块的模式，能够有效利用这种特性来压缩模型，同时保持性能。

Abstract: Large-scale deep learning models are well-suited for compression. Methods like pruning, quantization, and knowledge distillation have been used to achieve massive reductions in the number of model parameters, with marginal performance drops across a variety of architectures and tasks. This raises the central question: \emph{Why are deep neural networks suited for compression?} In this work, we take up the perspective of algorithmic complexity to explain this behavior. We hypothesize that the parameters of trained models have more structure and, hence, exhibit lower algorithmic complexity compared to the weights at (random) initialization. Furthermore, that model compression methods harness this reduced algorithmic complexity to compress models. Although an unconstrained parameterization of model weights, $\mathbf{w} \in \mathbb{R}^n$, can represent arbitrary weight assignments, the solutions found during training exhibit repeatability and structure, making them algorithmically simpler than a generic program. To this end, we formalize the Kolmogorov complexity of $\mathbf{w}$ by $\mathcal{K}(\mathbf{w})$. We introduce a constrained parameterization $\widehat{\mathbf{w}}$, that partitions parameters into blocks of size $s$, and restricts each block to be selected from a set of $k$ reusable motifs, specified by a reuse pattern (or mosaic). The resulting method, $\textit{Mosaic-of-Motifs}$ (MoMos), yields algorithmically simpler model parameterization compared to unconstrained models. Empirical evidence from multiple experiments shows that the algorithmic complexity of neural networks, measured using approximations to Kolmogorov complexity, can be reduced during training. This results in models that perform comparably with unconstrained models while being algorithmically simpler.

</details>


### [170] [Picking the Right Specialist: Attentive Neural Process-based Selection of Task-Specialized Models as Tools for Agentic Healthcare Systems](https://arxiv.org/abs/2602.14901)
*Pramit Saha,Joshua Strong,Mohammad Alsharid,Divyanshu Mishra,J. Alison Noble*

Main category: cs.LG

TL;DR: ToolSelect：一种通过最小化任务条件选择损失的代理学习方法，用于在异构专家模型池中自适应选择最佳模型，在胸部X光多任务环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗代理系统中，单一任务往往需要多个专家模型协同工作，因为不同模型在不同数据样本上表现各异。现有方法缺乏可靠的模型选择机制，无法从异构工具池中为特定查询选择最合适的专家模型。

Method: 提出ToolSelect方法，基于注意力神经过程（Attentive Neural Process）构建选择器，以查询和每个模型的行为摘要为条件，通过最小化任务条件选择损失的一致代理来学习模型选择策略。

Result: 首次构建了胸部X光代理环境ToolSelectBench，包含17个疾病检测、19个报告生成、6个视觉定位和13个VQA模型，共1448个查询。ToolSelect在四个不同任务家族上一致优于10种SOTA方法。

Conclusion: ToolSelect为医疗代理系统中的模型选择问题提供了有效的解决方案，能够自适应地从异构专家模型池中选择最适合特定查询的模型，显著提升了多任务医疗系统的性能。

Abstract: Task-specialized models form the backbone of agentic healthcare systems, enabling the agents to answer clinical queries across tasks such as disease diagnosis, localization, and report generation. Yet, for a given task, a single "best" model rarely exists. In practice, each task is better served by multiple competing specialist models where different models excel on different data samples. As a result, for any given query, agents must reliably select the right specialist model from a heterogeneous pool of tool candidates. To this end, we introduce ToolSelect, which adaptively learns model selection for tools by minimizing a population risk over sampled specialist tool candidates using a consistent surrogate of the task-conditional selection loss. Concretely, we propose an Attentive Neural Process-based selector conditioned on the query and per-model behavioral summaries to choose among the specialist models. Motivated by the absence of any established testbed, we, for the first time, introduce an agentic Chest X-ray environment equipped with a diverse suite of task-specialized models (17 disease detection, 19 report generation, 6 visual grounding, and 13 VQA) and develop ToolSelectBench, a benchmark of 1448 queries. Our results demonstrate that ToolSelect consistently outperforms 10 SOTA methods across four different task families.

</details>


### [171] [Coverage Guarantees for Pseudo-Calibrated Conformal Prediction under Distribution Shift](https://arxiv.org/abs/2602.14913)
*Farbod Siahkali,Ashwin Verma,Vijay Gupta*

Main category: cs.LG

TL;DR: 本文提出了一种伪校准方法，用于在分布偏移下保持保形预测的覆盖保证，通过引入松弛参数和源调谐算法来缓解覆盖退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统保形预测在数据分布偏移时可能失效，需要开发能够应对标签条件协变量偏移的方法来维持覆盖保证。

Method: 使用伪校准作为工具，基于有界标签条件协变量偏移模型，推导目标覆盖的下界，设计带松弛参数的伪校准集，并提出源调谐伪校准算法。

Result: 理论分析显示伪校准行为与边界定性一致，源调谐方案在分布偏移下有效缓解覆盖退化，同时保持非平凡的预测集大小。

Conclusion: 伪校准是应对分布偏移的有效工具，源调谐算法在保持覆盖保证和预测集效率之间取得了良好平衡。

Abstract: Conformal prediction (CP) offers distribution-free marginal coverage guarantees under an exchangeability assumption, but these guarantees can fail if the data distribution shifts. We analyze the use of pseudo-calibration as a tool to counter this performance loss under a bounded label-conditional covariate shift model. Using tools from domain adaptation, we derive a lower bound on target coverage in terms of the source-domain loss of the classifier and a Wasserstein measure of the shift. Using this result, we provide a method to design pseudo-calibrated sets that inflate the conformal threshold by a slack parameter to keep target coverage above a prescribed level. Finally, we propose a source-tuned pseudo-calibration algorithm that interpolates between hard pseudo-labels and randomized labels as a function of classifier uncertainty. Numerical experiments show that our bounds qualitatively track pseudo-calibration behavior and that the source-tuned scheme mitigates coverage degradation under distribution shift while maintaining nontrivial prediction set sizes.

</details>


### [172] [Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation](https://arxiv.org/abs/2602.14914)
*Olivier Jeunen,Shashank Gupta*

Main category: cs.LG

TL;DR: 本文证明在离策略评估中，使用最优加性基线（β*-IPS）的估计器在均方误差上渐近优于自归一化逆倾向评分（SNIPS），为排序和推荐系统提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 离策略评估（OPE）对于评估排序和推荐系统至关重要，但需要降低方差。SNIPS是标准工具，但最近研究表明加性控制变量（基线校正）可能提供更好性能，然而缺乏理论保证。

Method: 提出β*-IPS估计器，使用最优加性基线，通过理论分析证明其渐近优势。通过解析分解方差差距，展示SNIPS等价于使用特定但通常次优的加性基线。

Result: 证明β*-IPS在均方误差上渐近主导SNIPS，为从自归一化转向最优基线校正提供了理论依据。

Conclusion: 研究结果为排序和推荐系统中的离策略评估提供了理论支持，建议从自归一化方法转向最优基线校正方法。

Abstract: Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.

</details>


### [173] [BHyGNN+: Unsupervised Representation Learning for Heterophilic Hypergraphs](https://arxiv.org/abs/2602.14919)
*Tianyi Ma,Yiyue Qian,Zehong Wang,Zheyuan Zhang,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: BHyGNN+ 是一个自监督学习框架，通过超图对偶性在无需标签的情况下学习异配超图的表示，无需负样本且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有超图神经网络在处理异配超图时性能下降，且依赖标注数据，限制了在真实无标签场景中的应用。需要开发无需标签的自监督学习方法。

Method: 基于超图对偶性（节点和超边角色互换），通过对比超图及其对偶的增强视图，使用余弦相似度捕获结构模式，无需负样本。

Result: 在11个基准数据集上，BHyGNN+在异配和同配超图上均优于最先进的监督和自监督基线方法。

Conclusion: 利用超图对偶性进行自监督学习是有效的，为无标签超图表示学习建立了新范式。

Abstract: Hypergraph Neural Networks (HyGNNs) have demonstrated remarkable success in modeling higher-order relationships among entities. However, their performance often degrades on heterophilic hypergraphs, where nodes connected by the same hyperedge tend to have dissimilar semantic representations or belong to different classes. While several HyGNNs, including our prior work BHyGNN, have been proposed to address heterophily, their reliance on labeled data significantly limits their applicability in real-world scenarios where annotations are scarce or costly. To overcome this limitation, we introduce BHyGNN+, a self-supervised learning framework that extends BHyGNN for representation learning on heterophilic hypergraphs without requiring ground-truth labels. The core idea of BHyGNN+ is hypergraph duality, a structural transformation where the roles of nodes and hyperedges are interchanged. By contrasting augmented views of a hypergraph against its dual using cosine similarity, our framework captures essential structural patterns in a fully unsupervised manner. Notably, this duality-based formulation eliminates the need for negative samples, a common requirement in existing hypergraph contrastive learning methods that is often difficult to satisfy in practice. Extensive experiments on eleven benchmark datasets demonstrate that BHyGNN+ consistently outperforms state-of-the-art supervised and self-supervised baselines on both heterophilic and homophilic hypergraphs. Our results validate the effectiveness of leveraging hypergraph duality for self-supervised learning and establish a new paradigm for representation learning on challenging, unlabeled hypergraphs.

</details>


### [174] [Variance-Reduced $(\varepsilon,δ)-$Unlearning using Forget Set Gradients](https://arxiv.org/abs/2602.14938)
*Martin Van Waerebeke,Marco Lorenzi,Kevin Scaman,El Mahdi El Mhamdi,Giovanni Neglia*

Main category: cs.LG

TL;DR: VRU算法是首个在更新规则中直接包含遗忘集梯度的一阶方法，同时可证明满足(ε,δ)-遗忘保证，在低误差状态下优于忽略遗忘集的方法。


<details>
  <summary>Details</summary>
Motivation: 现有(ε,δ)-遗忘方法仅使用遗忘集来校准注入噪声，而不将其作为直接优化信号；而高效的启发式方法虽然利用遗忘样本但缺乏形式化保证。需要弥合这一差距。

Method: 提出方差减少遗忘(VRU)算法，这是一种一阶方法，在更新规则中直接包含遗忘集梯度，同时通过理论证明满足(ε,δ)-遗忘保证。

Result: VRU算法收敛性得到证明，相比现有方法有更优的误差依赖率。在低误差状态下，VRU渐近优于任何忽略遗忘集的一阶方法。实验验证了理论优势。

Conclusion: VRU算法成功弥合了形式化保证方法与高效启发式方法之间的差距，首次实现了一阶方法中直接利用遗忘集梯度同时保持(ε,δ)-遗忘保证。

Abstract: In machine unlearning, $(\varepsilon,δ)-$unlearning is a popular framework that provides formal guarantees on the effectiveness of the removal of a subset of training data, the forget set, from a trained model. For strongly convex objectives, existing first-order methods achieve $(\varepsilon,δ)-$unlearning, but they only use the forget set to calibrate injected noise, never as a direct optimization signal. In contrast, efficient empirical heuristics often exploit the forget samples (e.g., via gradient ascent) but come with no formal unlearning guarantees. We bridge this gap by presenting the Variance-Reduced Unlearning (VRU) algorithm. To the best of our knowledge, VRU is the first first-order algorithm that directly includes forget set gradients in its update rule, while provably satisfying ($(\varepsilon,δ)-$unlearning. We establish the convergence of VRU and show that incorporating the forget set yields strictly improved rates, i.e. a better dependence on the achieved error compared to existing first-order $(\varepsilon,δ)-$unlearning methods. Moreover, we prove that, in a low-error regime, VRU asymptotically outperforms any first-order method that ignores the forget set.Experiments corroborate our theory, showing consistent gains over both state-of-the-art certified unlearning methods and over empirical baselines that explicitly leverage the forget set.

</details>


### [175] [Use What You Know: Causal Foundation Models with Partial Graphs](https://arxiv.org/abs/2602.14972)
*Arik Reuter,Anish Dhir,Cristiana Diaconu,Jake Robertson,Ole Ossen,Frank Hutter,Adrian Weller,Mark van der Wilk,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 该论文提出了一种在因果基础模型中融入领域知识的方法，通过将因果图或祖先信息作为条件输入，使通用CFM能够达到针对特定因果结构训练的专用模型的性能水平。


<details>
  <summary>Details</summary>
Motivation: 当前因果基础模型无法融入领域知识，导致预测效果不理想。传统因果估计方法需要针对特定假设定制估计器，而CFM虽然承诺统一方法，但缺乏利用领域专业知识的能力。

Method: 引入方法将因果信息（如因果图或更易获得的祖先信息）作为条件输入CFM。当完整因果图信息不可得时，也能有效利用部分因果信息。通过系统评估，发现将可学习偏置注入注意力机制是利用完整和部分因果信息的最有效方法。

Result: 实验表明，这种条件化方法使通用CFM能够匹配针对特定因果结构训练的专用模型的性能。该方法有效解决了CFM无法利用领域知识的关键障碍。

Conclusion: 该方法为构建全能因果基础模型铺平了道路，使模型能够在数据驱动的同时有效利用任何程度的领域专业知识来回答因果查询。

Abstract: Estimating causal quantities traditionally relies on bespoke estimators tailored to specific assumptions. Recently proposed Causal Foundation Models (CFMs) promise a more unified approach by amortising causal discovery and inference in a single step. However, in their current state, they do not allow for the incorporation of any domain knowledge, which can lead to suboptimal predictions. We bridge this gap by introducing methods to condition CFMs on causal information, such as the causal graph or more readily available ancestral information. When access to complete causal graph information is too strict a requirement, our approach also effectively leverages partial causal information. We systematically evaluate conditioning strategies and find that injecting learnable biases into the attention mechanism is the most effective method to utilise full and partial causal information. Our experiments show that this conditioning allows a general-purpose CFM to match the performance of specialised models trained on specific causal structures. Overall, our approach addresses a central hurdle on the path towards all-in-one causal foundation models: the capability to answer causal queries in a data-driven manner while effectively leveraging any amount of domain expertise.

</details>


### [176] [MacroGuide: Topological Guidance for Macrocycle Generation](https://arxiv.org/abs/2602.14977)
*Alicja Maksymiuk,Alexandre Duplessis,Michael Bronstein,Alexander Tong,Fernanda Duarte,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: MacroGuide：一种利用持续同调引导扩散模型生成大环分子的拓扑指导方法，将大环生成率从1%提升到99%


<details>
  <summary>Details</summary>
Motivation: 大环分子因其对困难靶点具有增强的选择性和结合亲和力而成为有前景的药物替代品，但由于公共数据集中稀缺以及标准深度生成模型中拓扑约束难以强制执行，在生成建模中尚未得到充分探索

Method: MacroGuide：一种扩散指导机制，使用持续同调引导预训练分子生成模型的采样过程。在每个去噪步骤中，从原子位置构建Vietoris-Rips复形，并通过优化持续同调特征来促进环形成

Result: 将MacroGuide应用于预训练扩散模型后，大环生成率从1%提高到99%，同时在化学有效性、多样性和PoseBusters检查等关键质量指标上达到或超过最先进性能

Conclusion: MacroGuide通过拓扑指导成功解决了大环分子生成中的挑战，显著提高了生成效率和质量，为大环药物发现提供了有效的生成建模工具

Abstract: Macrocycles are ring-shaped molecules that offer a promising alternative to small-molecule drugs due to their enhanced selectivity and binding affinity against difficult targets. Despite their chemical value, they remain underexplored in generative modeling, likely owing to their scarcity in public datasets and the challenges of enforcing topological constraints in standard deep generative models. We introduce MacroGuide: Topological Guidance for Macrocycle Generation, a diffusion guidance mechanism that uses Persistent Homology to steer the sampling of pretrained molecular generative models toward the generation of macrocycles, in both unconditional and conditional (protein pocket) settings. At each denoising step, MacroGuide constructs a Vietoris-Rips complex from atomic positions and promotes ring formation by optimizing persistent homology features. Empirically, applying MacroGuide to pretrained diffusion models increases macrocycle generation rates from 1% to 99%, while matching or exceeding state-of-the-art performance on key quality metrics such as chemical validity, diversity, and PoseBusters checks.

</details>


### [177] [Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations](https://arxiv.org/abs/2602.14983)
*Carolin Cissee,Raneen Younis,Zahra Ahmadi*

Main category: cs.LG

TL;DR: COrAL是一个多模态学习框架，通过正交约束和不对称掩码技术，显式地同时保留冗余、独特和协同信息，实现更稳定、可靠和全面的多模态表示。


<details>
  <summary>Details</summary>
Motivation: 现有自监督多模态对比学习方法主要捕获冗余的跨模态信号，往往忽略模态特定（独特）信息和交互驱动（协同）信息。现有扩展方法要么未能显式建模协同交互，要么以纠缠方式学习不同信息组件，导致表示不完整和潜在信息泄漏。

Method: 采用双路径架构配合正交约束来解耦共享和模态特定特征；引入具有互补视图特定模式的不对称掩码技术，强制模型推断跨模态依赖关系而非仅依赖冗余线索。

Result: 在合成基准和多样化MultiBench数据集上的广泛实验表明，COrAL始终匹配或优于最先进方法，同时在多次运行中表现出低性能方差。

Conclusion: 显式建模多模态信息的完整谱系能够产生更稳定、可靠和全面的嵌入表示。

Abstract: Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.

</details>


### [178] [Spectral Convolution on Orbifolds for Geometric Deep Learning](https://arxiv.org/abs/2602.14997)
*Tim Mangliers,Bernhard Mössner,Benjamin Himpel*

Main category: cs.LG

TL;DR: 论文介绍了在orbifolds上进行谱卷积的概念，为几何深度学习在orbifold结构数据上的应用提供了基础构建模块。


<details>
  <summary>Details</summary>
Motivation: 几何深度学习需要处理超越欧几里得结构的数据域（如图形或流形结构），而应用相关数据的需求要求识别更多拓扑和几何结构，使这些用例能够被机器学习所访问。

Method: 引入orbifolds上的谱卷积概念，作为在非欧几里得数据上构建卷积神经网络架构的基本构建模块。

Result: 提出了orbifolds上谱卷积的理论框架，使orbifold结构数据能够通过几何深度学习进行学习，并使用音乐理论的例子进行了说明。

Conclusion: orbifolds上的谱卷积为几何深度学习提供了新的基础构建模块，扩展了机器学习处理复杂拓扑和几何结构数据的能力。

Abstract: Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.

</details>


### [179] [Boundary Point Jailbreaking of Black-Box LLMs](https://arxiv.org/abs/2602.15001)
*Xander Davies,Giorgi Giglemiani,Edmund Lau,Eric Winsor,Geoffrey Irving,Yarin Gal*

Main category: cs.LG

TL;DR: BPJ是一种新型自动化越狱攻击，仅使用单比特分类器标志信息，通过边界点检测和课程学习策略，成功突破GPT-5等最强行业部署的安全防护系统。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击依赖白盒/灰盒假设或已有攻击库，无法有效对抗经过数千小时人工红队测试的行业最强防护系统，需要开发完全黑盒的自动化攻击方法。

Method: BPJ将有害字符串转换为课程学习目标，主动选择最能检测攻击强度微小变化的边界点作为评估点，仅使用分类器是否标记这一单比特信息进行优化。

Result: BPJ首次成功实现对宪法分类器的自动化通用越狱攻击，也是首个不依赖人工攻击种子就能突破GPT-5输入分类器的自动化攻击算法。

Conclusion: BPJ在单次交互中难以防御，但优化过程中会产生大量标记，表明有效防御需要将单次交互方法与批量级监控相结合。

Abstract: Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as "jailbreaks". Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength ("boundary points"). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5's input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.

</details>


### [180] [PDE foundation models are skillful AI weather emulators for the Martian atmosphere](https://arxiv.org/abs/2602.15004)
*Johannes Schmude,Sujit Roy,Liping Wang,Theodore van Kessel,Levente Klein,Marcus Freitag,Eloisa Bentivegna,Robert Manson-Sawko,Bjorn Lutjens,Manil Maskey,Campbell Watson,Rahul Ramachandran,Juan Bernabe-Moreno*

Main category: cs.LG

TL;DR: 该研究展示了预训练在偏微分方程数值解上的AI基础模型可以适应并微调为火星大气预测模拟器，通过将2D模型扩展到3D并利用预训练信息，在有限数据和计算预算下获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索AI基础模型在处理真实世界复杂系统时的潜力，特别是针对火星大气这种缺乏充足训练数据或合适计算预算的复杂相互作用系统，验证PDE基础模型在实际问题中的应用价值。

Method: 基于Poseidon PDE基础模型（针对二维系统），开发了将模型从二维扩展到三维的方法，同时保留预训练信息。研究了模型在稀疏初始条件下的性能，使用了四个火星年（约34GB）的训练数据和中等计算预算（13 GPU小时）。

Result: 预训练和模型扩展的组合使模型在保留年份上的性能提升了34.4%，证明了PDE基础模型不仅能够近似其他PDE的解，还能作为缺乏足够训练数据或计算预算的复杂真实世界问题的锚定模型。

Conclusion: PDE基础模型可以成功应用于火星大气预测等复杂真实世界问题，预训练和维度扩展策略在有限数据和计算资源下显著提升模型性能，为处理数据稀缺的复杂系统提供了有效方法。

Abstract: We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.

</details>


### [181] [Scaling Beyond Masked Diffusion Language Models](https://arxiv.org/abs/2602.15014)
*Subham Sekhar Sahoo,Jean-Marie Lemercier,Zhihan Yang,Justin Deschenaux,Jingyu Liu,John Thickstun,Ante Jukic*

Main category: cs.LG

TL;DR: 该研究首次对均匀状态和插值离散扩散方法进行了缩放定律研究，发现困惑度在扩散家族内部具有参考价值，但在跨家族比较时可能产生误导，挑战了掩码扩散是扩散语言建模未来的观点。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案具有更快生成的潜力，但目前掩码扩散方法在语言建模基准上占据主导地位。研究者希望探索其他离散扩散方法的缩放特性，并重新评估困惑度作为跨算法比较指标的有效性。

Method: 对均匀状态和插值离散扩散方法进行首次缩放定律研究；使用简单的交叉熵目标训练掩码扩散模型以提高FLOPs效率；将所有方法扩展到17亿参数规模进行对比分析。

Result: 掩码扩散模型通过交叉熵目标训练可提高约12%的FLOPs效率；均匀状态扩散在基于似然的基准上保持竞争力，并在GSM8K上优于自回归和掩码扩散模型，尽管验证困惑度较差；困惑度在扩散家族内部具有信息性，但在跨家族比较时可能产生误导。

Conclusion: 掩码扩散并非扩散语言建模的绝对未来，困惑度单独不足以进行跨算法比较；速度-质量帕累托前沿更能反映实际性能；均匀状态扩散在特定任务上表现出优势，挑战了当前对扩散方法优劣的认知。

Abstract: Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms

</details>


### [182] [Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation](https://arxiv.org/abs/2602.15022)
*Cai Zhou,Zijie Chen,Zian Li,Jike Wang,Kaiyi Jiang,Pan Li,Rose Yu,Muhan Zhang,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 该论文提出了一种新的生成建模方法——规范扩散，通过将样本映射到规范表示（如特定姿态或顺序），在规范切片上训练非等变扩散模型，然后在生成时应用随机对称变换，从而处理具有群对称性的分布。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过架构约束（如等变去噪器和不变先验）来强制实现对称性不变性，但这种方法可能限制了模型的表达能力。本文挑战这一传统，从规范化的角度出发，旨在提供更高效、表达能力更强的生成模型。

Method: 采用规范扩散框架：1）将每个样本映射到轨道代表（规范姿态或顺序）；2）在规范切片上训练无约束（非等变）扩散或流模型；3）在生成时通过随机对称变换恢复不变分布。基于商空间理论，结合对齐先验和最优传输来进一步提高训练效率。

Result: 在分子图生成任务中（具有Sn×SE(3)对称性），规范扩散显著优于等变基线方法，在3D分子生成任务中表现更好，计算量相似甚至更少。提出的CanonFlow架构在GEOM-DRUG数据集上达到最先进性能，在少步生成中优势明显。

Conclusion: 规范扩散为处理对称不变分布提供了理论严谨且实践有效的替代方案，相比传统等变方法具有更好的表达能力和训练效率，在分子生成等科学计算任务中展现出显著优势。

Abstract: Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.

</details>


### [183] [Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization](https://arxiv.org/abs/2602.15028)
*Shangding Gu*

Main category: cs.LG

TL;DR: 论文提出了PAPerBench基准，系统研究上下文长度对LLM个性化和隐私保护的影响，发现随着上下文增长，两者性能均下降，并提供了注意力稀释的理论解释。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地部署在隐私关键和个性化场景中，但上下文长度如何影响隐私泄露和个性化效果尚未得到充分探索。需要系统研究上下文长度对隐私保护和个性化质量的影响。

Method: 引入大规模基准PAPerBench，包含约29,000个实例，上下文长度从1K到256K token，总计377K个评估问题。该基准联合评估个性化性能和隐私风险，并提供了上下文缩放下注意力稀释的理论分析。

Result: 对最先进LLM的广泛评估显示，随着上下文长度增加，个性化和隐私保护性能均出现一致下降。理论分析表明这是固定容量Transformer中软注意力的固有局限性。

Conclusion: 当前模型存在普遍的缩放差距：长上下文导致注意力分散。研究揭示了LLM在长上下文场景下的局限性，为未来可扩展隐私和个性化研究提供了基准和理论指导。

Abstract: Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench

</details>


### [184] [Symmetry in language statistics shapes the geometry of model representations](https://arxiv.org/abs/2602.15029)
*Dhruva Karkada,Daniel J. Korchinski,Andres Nava,Matthieu Wyart,Yasaman Bahri*

Main category: cs.LG

TL;DR: 论文证明语言统计中的平移对称性导致LLM表示中出现简单几何结构（如月份形成圆形、年份形成流形），这些结构在统计扰动和中等维度下保持稳健。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络的表示学习取得了成功，但其基本性质仍不清楚。一个显著现象是LLM表示中出现简单几何结构（如月份形成圆形、年份形成流形），需要理解这些结构为何出现以及为何如此稳健。

Method: 1. 证明语言统计具有平移对称性（如两个月份共现概率仅取决于时间间隔）；2. 证明这种对称性控制高维词嵌入模型中的几何结构；3. 研究统计扰动下的结构稳健性；4. 提出连续潜在变量理论框架解释稳健性；5. 在词嵌入模型、文本嵌入模型和LLM中实证验证。

Result: 1. 语言统计的平移对称性确实导致表示中出现简单几何结构；2. 这些结构在统计强烈扰动（如删除所有两个月份共现的句子）和中等嵌入维度下保持稳健；3. 连续潜在变量理论框架能解释这种稳健性；4. 在多种模型中得到实证验证。

Conclusion: 语言统计的平移对称性是LLM表示中出现简单几何结构的关键原因，这些结构具有内在稳健性，可由连续潜在变量理论框架解释，为理解神经网络表示的基本性质提供了新视角。

Abstract: Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [185] [Nonparametric Distribution Regression Re-calibration](https://arxiv.org/abs/2602.13362)
*Ádám Jung,Domokos M. Kelen,András A. Benczúr*

Main category: stat.ML

TL;DR: 提出基于条件核均值嵌入的非参数重新校准算法，用于修正概率回归中的校准误差，无需限制性建模假设，并在多个回归基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 概率回归中确保预测分布准确反映真实经验不确定性是关键挑战。最小化总体预测误差常导致模型优先考虑信息性而非校准性，产生狭窄但过度自信的预测。在安全关键场景中，可信的不确定性估计比狭窄区间更有价值。现有后处理校正方法要么依赖弱校准概念（如PIT均匀性），要么对误差性质施加限制性参数假设。

Method: 提出基于条件核均值嵌入的非参数重新校准算法，能够在不施加限制性建模假设的情况下修正校准误差。针对实值目标的高效推断，引入了一种新的特征核，可在O(n log n)时间内评估大小为n的经验分布。

Result: 该方法在多样化的回归基准测试和模型类别中始终优于先前的重新校准方法。

Conclusion: 提出的非参数重新校准算法能够有效解决概率回归中的校准问题，提供更准确的不确定性估计，特别适用于安全关键应用场景。

Abstract: A key challenge in probabilistic regression is ensuring that predictive distributions accurately reflect true empirical uncertainty. Minimizing overall prediction error often encourages models to prioritize informativeness over calibration, producing narrow but overconfident predictions. However, in safety-critical settings, trustworthy uncertainty estimates are often more valuable than narrow intervals. Realizing the problem, several recent works have focused on post-hoc corrections; however, existing methods either rely on weak notions of calibration (such as PIT uniformity) or impose restrictive parametric assumptions on the nature of the error. To address these limitations, we propose a novel nonparametric re-calibration algorithm based on conditional kernel mean embeddings, capable of correcting calibration error without restrictive modeling assumptions. For efficient inference with real-valued targets, we introduce a novel characteristic kernel over distributions that can be evaluated in $\mathcal{O}(n \log n)$ time for empirical distributions of size $n$. We demonstrate that our method consistently outperforms prior re-calibration approaches across a diverse set of regression benchmarks and model classes.

</details>


### [186] [Metabolic cost of information processing in Poisson variational autoencoders](https://arxiv.org/abs/2602.13421)
*Hadi Vafaii,Jacob L. Yates*

Main category: stat.ML

TL;DR: 论文提出泊松变分自编码器（P-VAE）作为能量约束计算的理论基础，通过KL散度项与神经元基础发放率成比例，实现了编码保真度与能量消耗的权衡。


<details>
  <summary>Details</summary>
Motivation: 生物计算本质上是能量受限的，但传统计算理论将能量视为自由可用。需要建立能量感知的计算理论，将信息处理与代谢成本联系起来。

Method: 提出泊松变分自编码器（P-VAE），这是一种大脑启发的生成模型，将输入编码为离散脉冲计数。通过泊松变分自由能最小化，KL散度项与模型神经元的基础发放率成比例，产生代谢成本项。

Result: 增加KL项权重系数β会单调增加P-VAE的稀疏性并减少平均脉冲活动，而具有ReLU校正的高斯VAE（Grelu-VAE）表示保持不变，证明该效应是泊松统计特有的。

Conclusion: 泊松变分推断为资源受限计算理论提供了有前景的基础，将抽象信息量（编码率）与具体生物物理变量（发放率）耦合，实现了编码保真度与能量消耗的自然权衡。

Abstract: Computation in biological systems is fundamentally energy-constrained, yet standard theories of computation treat energy as freely available. Here, we argue that variational free energy minimization under a Poisson assumption offers a principled path toward an energy-aware theory of computation. Our key observation is that the Kullback-Leibler (KL) divergence term in the Poisson free energy objective becomes proportional to the prior firing rates of model neurons, yielding an emergent metabolic cost term that penalizes high baseline activity. This structure couples an abstract information-theoretic quantity -- the *coding rate* -- to a concrete biophysical variable -- the *firing rate* -- which enables a trade-off between coding fidelity and energy expenditure. Such a coupling arises naturally in the Poisson variational autoencoder (P-VAE) -- a brain-inspired generative model that encodes inputs as discrete spike counts and recovers a spiking form of *sparse coding* as a special case -- but is absent from standard Gaussian VAEs. To demonstrate that this metabolic cost structure is unique to the Poisson formulation, we compare the P-VAE against Grelu-VAE, a Gaussian VAE with ReLU rectification applied to latent samples, which controls for the non-negativity constraint. Across a systematic sweep of the KL term weighting coefficient $β$ and latent dimensionality, we find that increasing $β$ monotonically increases sparsity and reduces average spiking activity in the P-VAE. In contrast, Grelu-VAE representations remain unchanged, confirming that the effect is specific to Poisson statistics rather than a byproduct of non-negative representations. These results establish Poisson variational inference as a promising foundation for a resource-constrained theory of computation.

</details>


### [187] [Locally Private Parametric Methods for Change-Point Detection](https://arxiv.org/abs/2602.13619)
*Anuj Kumar Yadav,Cemre Cadir,Yanina Shkel,Michael Gastpar*

Main category: stat.ML

TL;DR: 本文研究局部差分隐私下的参数化变点检测问题，推导了非隐私和隐私两种设置下的算法性能界限，并分析了隐私保护带来的统计代价。


<details>
  <summary>Details</summary>
Motivation: 在时间序列分析中，检测分布变化（变点检测）是重要任务。随着隐私保护需求增加，需要在局部差分隐私约束下进行变点检测，这带来了新的统计挑战。

Method: 1. 非隐私设置：基于广义对数似然比检验的变点检测算法，使用鞅方法分析
2. 隐私设置：提出两种基于随机响应和二进制机制的局部差分隐私算法
3. 理论分析：推导检测精度界限，建立强数据处理不等式（SDPI）的结构性结果

Result: 1. 改进了非隐私变点检测算法的有限样本精度保证
2. 分析了隐私算法的理论性能，推导了检测精度界限
3. 实证验证了理论结果
4. 证明了Rényi散度及其对称变体（Jeffreys-Rényi散度）的SDPI系数由二元输入分布实现

Conclusion: 本文量化了局部差分隐私在变点检测中的统计代价，展示了隐私保护如何相对于非隐私基准降低性能。SDPI系数的结构结果对统计估计、数据压缩和马尔可夫链混合等应用具有独立价值。

Abstract: We study parametric change-point detection, where the goal is to identify distributional changes in time series, under local differential privacy. In the non-private setting, we derive improved finite-sample accuracy guarantees for a change-point detection algorithm based on the generalized log-likelihood ratio test, via martingale methods. In the private setting, we propose two locally differentially private algorithms based on randomized response and binary mechanisms, and analyze their theoretical performance. We derive bounds on detection accuracy and validate our results through empirical evaluation. Our results characterize the statistical cost of local differential privacy in change-point detection and show how privacy degrades performance relative to a non-private benchmark. As part of this analysis, we establish a structural result for strong data processing inequalities (SDPI), proving that SDPI coefficients for Rényi divergences and their symmetric variants (Jeffreys-Rényi divergences) are achieved by binary input distributions. These results on SDPI coefficients are also of independent interest, with applications to statistical estimation, data compression, and Markov chain mixing.

</details>


### [188] [Quantifying Normality: Convergence Rate to Gaussian Limit for Stochastic Approximation and Unadjusted OU Algorithm](https://arxiv.org/abs/2602.13906)
*Shaan Ul Haque,Zedong Wang,Zixuan Zhang,Siva Theja Maguluri*

Main category: stat.ML

TL;DR: 本文为非渐进随机逼近(SA)迭代提供了显式的Wasserstein距离界限，量化了有限时间内高斯近似的精度，并获得了SA迭代误差的尾部界限。


<details>
  <summary>Details</summary>
Motivation: 随机逼近(SA)是寻找受噪声扰动的算子根的方法。虽然已有丰富文献在相当温和条件下建立了重标度SA迭代的渐近正态性，但这些渐近结果无法量化有限时间内高斯近似的精度。

Method: 首先研究由一般噪声驱动的离散Ornstein-Uhlenbeck(O-U)过程的收敛速率，其平稳分布与重标度SA迭代的极限高斯分布相同。通过调整Stein方法处理矩阵加权独立同分布随机变量和，分析重标度SA迭代与离散时间O-U过程之间的误差动态，并结合后者的收敛速率获得SA的有限时间界限。

Result: 建立了重标度SA迭代在时间k的分布与渐近高斯极限之间Wasserstein距离的显式非渐近界限，适用于包括常数和多项式衰减在内的各种步长选择。作为直接推论，获得了任意时间SA迭代误差的尾部界限。

Conclusion: 本文提供了随机逼近的有限时间精度量化方法，通过分析离散O-U过程的收敛速率获得SA的尖锐速率，这一方法对采样文献具有独立意义。

Abstract: Stochastic approximation (SA) is a method for finding the root of an operator perturbed by noise. There is a rich literature establishing the asymptotic normality of rescaled SA iterates under fairly mild conditions. However, these asymptotic results do not quantify the accuracy of the Gaussian approximation in finite time. In this paper, we establish explicit non-asymptotic bounds on the Wasserstein distance between the distribution of the rescaled iterate at time k and the asymptotic Gaussian limit for various choices of step-sizes including constant and polynomially decaying. As an immediate consequence, we obtain tail bounds on the error of SA iterates at any time.
  We obtain the sharp rates by first studying the convergence rate of the discrete Ornstein-Uhlenbeck (O-U) process driven by general noise, whose stationary distribution is identical to the limiting Gaussian distribution of the rescaled SA iterates. We believe that this is of independent interest, given its connection to sampling literature. The analysis involves adapting Stein's method for Gaussian approximation to handle the matrix weighted sum of i.i.d. random variables. The desired finite-time bounds for SA are obtained by characterizing the error dynamics between the rescaled SA iterate and the discrete time O-U process and combining it with the convergence rate of the latter process.

</details>


### [189] [A Theoretical Framework for LLM Fine-tuning Using Early Stopping for Non-random Initialization](https://arxiv.org/abs/2602.13942)
*Zexuan Sun,Garvesh Raskutti*

Main category: stat.ML

TL;DR: 该论文提出了一个结合早期停止理论和注意力机制神经正切核的统计框架，用于解释大语言模型微调中为什么只需少量epoch就能获得良好性能的理论基础。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型时代，微调预训练模型已成为普遍做法，但其理论基础仍不明确。核心问题是为什么通常只需几个epoch的微调就能在不同任务上获得强大性能。

Method: 开发了一个统计框架，将严格的早期停止理论与大语言模型的注意力机制神经正切核相结合。将经典NTK理论扩展到非随机（预训练）初始化，并提供注意力机制微调的收敛保证。

Result: 理论分析表明收敛速度与NTK诱导的经验核矩阵的特征值衰减率密切相关。该框架还能解释大语言模型中多任务的任务向量。实验使用现代语言模型在真实数据集上提供了支持理论见解的经验证据。

Conclusion: 该研究为大语言模型微调实践提供了新的理论见解，解释了为什么少量epoch微调就能有效，并建立了收敛速度与核矩阵特征值衰减的理论联系。

Abstract: In the era of large language models (LLMs), fine-tuning pretrained models has become ubiquitous. Yet the theoretical underpinning remains an open question. A central question is why only a few epochs of fine-tuning are typically sufficient to achieve strong performance on many different tasks. In this work, we approach this question by developing a statistical framework, combining rigorous early stopping theory with the attention-based Neural Tangent Kernel (NTK) for LLMs, offering new theoretical insights on fine-tuning practices. Specifically, we formally extend classical NTK theory [Jacot et al., 2018] to non-random (i.e., pretrained) initializations and provide a convergence guarantee for attention-based fine-tuning. One key insight provided by the theory is that the convergence rate with respect to sample size is closely linked to the eigenvalue decay rate of the empirical kernel matrix induced by the NTK. We also demonstrate how the framework can be used to explain task vectors for multiple tasks in LLMs. Finally, experiments with modern language models on real-world datasets provide empirical evidence supporting our theoretical insights.

</details>


### [190] [Computable Bernstein Certificates for Cross-Fitted Clipped Covariance Estimation](https://arxiv.org/abs/2602.14020)
*Even He,Zaizai Yan*

Main category: stat.ML

TL;DR: 提出一种具有完全可计算偏差证书的交叉拟合裁剪协方差估计器，通过MinUpper选择器自适应调整裁剪水平，在重尾数据和异常值存在时提供稳健估计。


<details>
  <summary>Details</summary>
Motivation: 研究重尾样本中可能包含少量任意异常值的算子范数协方差估计问题。常用的欧几里得范数裁剪方法依赖于未知的裁剪水平，需要数据驱动的自适应方法。

Method: 提出交叉拟合裁剪协方差估计器，配备完全可计算的Bernstein型偏差证书。通过MinUpper选择器平衡认证的随机误差和裁剪偏差的稳健留出代理，实现数据驱动的参数调整。

Result: 该方法在温和尾部正则性条件下适应有效秩等内在复杂性度量，仅需有限四阶矩即可获得有意义的理论保证。在受污染的尖峰协方差基准测试中表现出稳定的性能和竞争力的准确性。

Conclusion: 提出的方法为处理重尾数据和异常值的协方差估计提供了理论保证和实际可行的解决方案，通过数据驱动的方式解决了裁剪水平选择的关键问题。

Abstract: We study operator-norm covariance estimation from heavy-tailed samples that may include a small fraction of arbitrary outliers. A simple and widely used safeguard is \emph{Euclidean norm clipping}, but its accuracy depends critically on an unknown clipping level. We propose a cross-fitted clipped covariance estimator equipped with \emph{fully computable} Bernstein-type deviation certificates, enabling principled data-driven tuning via a selector (\emph{MinUpper}) that balances certified stochastic error and a robust hold-out proxy for clipping bias. The resulting procedure adapts to intrinsic complexity measures such as effective rank under mild tail regularity and retains meaningful guarantees under only finite fourth moments. Experiments on contaminated spiked-covariance benchmarks illustrate stable performance and competitive accuracy across regimes.

</details>


### [191] [Why Self-Training Helps and Hurts: Denoising vs. Signal Forgetting](https://arxiv.org/abs/2602.14029)
*Mingqi Wu,Archer Y. Yang,Qiang Sun*

Main category: stat.ML

TL;DR: 研究过参数化线性回归中的迭代自训练（自蒸馏），分析预测风险和有效噪声的确定性等效递归，揭示去噪与遗忘的权衡导致U形测试风险曲线，提出迭代广义交叉验证准则用于选择最优停止时间。


<details>
  <summary>Details</summary>
Motivation: 研究迭代自训练（自蒸馏）在过参数化线性回归中的理论特性，理解该过程中预测风险和有效噪声随迭代变化的规律，揭示去噪与信号遗忘之间的权衡机制。

Method: 在过参数化线性回归框架下，初始估计器在带噪声标签上训练，后续迭代使用新协变量和前一模型的无噪声伪标签训练。推导高维机制下预测风险和有效噪声的确定性等效递归，证明经验量围绕这些极限集中。分析尖峰协方差模型中迭代作为迭代依赖谱滤波器的特性。

Result: 递归揭示两个竞争力量：系统性成分随迭代增长（信号遗忘），随机成分衰减（通过重复数据依赖投影去噪）。相互作用产生U形测试风险曲线和最优早停时间。在尖峰协方差模型中，迭代作为谱滤波器保留强特征方向、抑制弱特征方向，实现与岭回归不同的软特征选择。提出迭代广义交叉验证准则并证明其一致性。

Conclusion: 迭代自训练在过参数化线性回归中表现出独特的去噪-遗忘权衡，产生U形风险曲线和最优早停时间。迭代过程实现隐式软特征选择，提出的迭代广义交叉验证准则能有效选择停止时间和正则化参数，为实际应用提供理论指导。

Abstract: Iterative self-training (self-distillation) repeatedly refits a model on pseudo-labels generated by its own predictions. We study this procedure in overparameterized linear regression: an initial estimator is trained on noisy labels, and each subsequent iterate is trained on fresh covariates with noiseless pseudo-labels from the previous model. In the high-dimensional regime, we derive deterministic-equivalent recursions for the prediction risk and effective noise across iterations, and prove that the empirical quantities concentrate sharply around these limits. The recursion separates two competing forces: a systematic component that grows with iteration due to progressive signal forgetting, and a stochastic component that decays due to denoising via repeated data-dependent projections. Their interaction yields a $U$-shaped test-risk curve and an optimal early-stopping time. In spiked covariance models, iteration further acts as an iteration-dependent spectral filter that preserves strong eigendirections while suppressing weaker ones, inducing an implicit form of soft feature selection distinct from ridge regression. Finally, we propose an iterated generalized cross-validation criterion and prove its uniform consistency for estimating the risk along the self-training trajectory, enabling fully data-driven selection of the stopping time and regularization. Experiments on synthetic covariances validate the theory and illustrate the predicted denoising-forgetting trade-off.

</details>


### [192] [Federated Ensemble Learning with Progressive Model Personalization](https://arxiv.org/abs/2602.14244)
*Ala Emrani,Amir Najafi,Abolfazl Motahari*

Main category: stat.ML

TL;DR: 提出基于Boosting的个性化联邦学习框架，通过渐进增加个性化组件深度并控制复杂度，平衡共享特征提取器与个性化头部之间的权衡，在异构数据下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 个性化联邦学习通常将神经网络分解为共享特征提取器和客户端特定头部，但存在根本性权衡：深度共享组件阻碍个性化，而大型本地头部在有限数据下容易过拟合。现有方法采用固定浅层头部，无法在原则性方式下平衡这一权衡。

Method: 提出Boosting启发式框架，为每个客户端构建T个模型的集成。在Boosting迭代中，渐进增加个性化组件深度，同时通过低秩分解或宽度缩减系统控制其有效复杂度。这种方法同时限制过拟合并通过允许更具表达力的个性化来显著减少每个客户端的偏差。

Result: 理论分析建立了泛化边界，对平均本地样本大小和客户端总数具有有利依赖。实验证明，在基准和真实数据集（如EMNIST、CIFAR-10/100、Sent140）上，该框架在异构数据分布下始终优于最先进的PFL方法。

Conclusion: 提出的Boosting框架通过渐进增加个性化组件深度并控制复杂度，有效平衡了共享特征提取器与个性化头部之间的权衡，在个性化联邦学习中实现了更好的性能，特别是在数据异构场景下。

Abstract: Federated Learning provides a privacy-preserving paradigm for distributed learning, but suffers from statistical heterogeneity across clients. Personalized Federated Learning (PFL) mitigates this issue by considering client-specific models. A widely adopted approach in PFL decomposes neural networks into a shared feature extractor and client-specific heads. While effective, this design induces a fundamental tradeoff: deep or expressive shared components hinder personalization, whereas large local heads exacerbate overfitting under limited per-client data. Most existing methods rely on rigid, shallow heads, and therefore fail to navigate this tradeoff in a principled manner. In this work, we propose a boosting-inspired framework that enables a smooth control of this tradeoff. Instead of training a single personalized model, we construct an ensemble of $T$ models for each client. Across boosting iterations, the depth of the personalized component are progressively increased, while its effective complexity is systematically controlled via low-rank factorization or width shrinkage. This design simultaneously limits overfitting and substantially reduces per-client bias by allowing increasingly expressive personalization. We provide theoretical analysis that establishes generalization bounds with favorable dependence on the average local sample size and the total number of clients. Specifically, we prove that the complexity of the shared layers is effectively suppressed, while the dependence on the boosting horizon $T$ is controlled through parameter reduction. Notably, we provide a novel nonlinear generalization guarantee for decoupled PFL models. Extensive experiments on benchmark and real-world datasets (e.g., EMNIST, CIFAR-10/100, and Sent140) demonstrate that the proposed framework consistently outperforms state-of-the-art PFL methods under heterogeneous data distributions.

</details>


### [193] [Constrained and Composite Sampling via Proximal Sampler](https://arxiv.org/abs/2602.14478)
*Thanh Dang,Jiaming Liang*

Main category: stat.ML

TL;DR: 提出两种对数凹采样方法：约束采样和复合采样。约束采样通过提升变换将约束问题转化为提升空间中的均匀采样，复合采样通过双重提升变换处理复合目标函数。两种方法都基于近端采样器，仅需最小化预言机访问，无需投影或了解约束集几何。


<details>
  <summary>Details</summary>
Motivation: 传统约束采样方法依赖投影、反射、障碍函数或镜像映射，需要了解约束集几何或计算复杂操作。本文旨在开发仅需最小预言机访问（分离预言机和次梯度预言机）的实用无偏采样器，避免现有方法的局限性。

Method: 1. 约束采样：通过提升变换将约束采样问题转化为提升空间中的均匀采样问题，使用基于切割平面法和拒绝采样的近端采样器实现。2. 复合采样：通过双重提升变换将复合采样问题转化为约束采样问题，然后应用第一部分开发的算法，利用不同的预言机组合（次梯度和近端）构建提升问题的分离预言机。

Result: 开发了两种仅需最小预言机访问的采样算法：约束采样仅需K的分离预言机和f的次梯度预言机；复合采样可灵活组合不同预言机。建立了以Rényi和χ²散度衡量的混合时间界限。

Conclusion: 通过提升变换和近端采样器，实现了仅需最小预言机访问的约束和复合采样方法。这些方法无需投影、反射或了解约束集几何，提供了实用且无偏的采样器，适用于贝叶斯推断等应用场景。

Abstract: We study two log-concave sampling problems: constrained sampling and composite sampling. First, we consider sampling from a target distribution with density proportional to $\exp(-f(x))$ supported on a convex set $K \subset \mathbb{R}^d$, where $f$ is convex. The main challenge is enforcing feasibility without degrading mixing. Using an epigraph transformation, we reduce this task to sampling from a nearly uniform distribution over a lifted convex set in $\mathbb{R}^{d+1}$. We then solve the lifted problem using a proximal sampler. Assuming only a separation oracle for $K$ and a subgradient oracle for $f$, we develop an implementation of the proximal sampler based on the cutting-plane method and rejection sampling. Unlike existing constrained samplers that rely on projection, reflection, barrier functions, or mirror maps, our approach enforces feasibility using only minimal oracle access, resulting in a practical and unbiased sampler without knowing the geometry of the constraint set.
  Second, we study composite sampling, where the target is proportional to $\exp(-f(x)-h(x))$ with closed and convex $f$ and $h$. This composite structure is standard in Bayesian inference with $f$ modeling data fidelity and $h$ encoding prior information. We reduce composite sampling via an epigraph lifting of $h$ to constrained sampling in $\mathbb{R}^{d+1}$, which allows direct application of the constrained sampling algorithm developed in the first part. This reduction results in a double epigraph lifting formulation in $\mathbb{R}^{d+2}$, on which we apply a proximal sampler. By keeping $f$ and $h$ separate, we further demonstrate how different combinations of oracle access (such as subgradient and proximal) can be leveraged to construct separation oracles for the lifted problem. For both sampling problems, we establish mixing time bounds measured in Rényi and $χ^2$ divergences.

</details>


### [194] [Accelerating Posterior Inference from Pulsar Light Curves via Learned Latent Representations and Local Simulator-Guided Optimization](https://arxiv.org/abs/2602.14520)
*Farhana Taiyebah,Abu Bucker Siddik,Indronil Bhattacharjee,Diane Oyen,Soumi De,Greg Olmschenk,Constantinos Kalapotharakos*

Main category: stat.ML

TL;DR: 提出结合学习潜在表示与局部模拟器引导优化的框架，加速脉冲星光变曲线后验推断，在保持准确性的同时将推理时间从24小时减少到12分钟（120倍加速）。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法进行脉冲星光变曲线后验推断虽然准确但计算成本高昂，需要更高效的推理方法。

Method: 1) 使用掩码U-Net预训练重建完整光变曲线并产生信息丰富的潜在嵌入；2) 在嵌入空间中识别相似模拟光变曲线，获得后验分布的初始经验近似；3) 使用前向模拟器引导的局部优化（爬山更新）逐步将经验后验移向更高似然参数区域。

Result: 在PSR J0030+0451观测光变曲线（NICER数据）上的实验表明，该方法与传统MCMC方法的后验估计结果高度一致，同时将推理时间从24小时减少到12分钟，实现120倍加速。

Conclusion: 学习表示与模拟器引导优化相结合的方法能够有效加速后验推断，为脉冲星观测分析提供了高效准确的替代方案。

Abstract: Posterior inference from pulsar observations in the form of light curves is commonly performed using Markov chain Monte Carlo methods, which are accurate but computationally expensive. We introduce a framework that accelerates posterior inference while maintaining accuracy by combining learned latent representations with local simulator-guided optimization. A masked U-Net is first pretrained to reconstruct complete light curves from partial observations and to produce informative latent embeddings. Given a query light curve, we identify similar simulated light curves from the simulation bank by measuring similarity in the learned embedding space produced by pretrained U-Net encoder, yielding an initial empirical approximation to the posterior over parameters. This initialization is then refined using a local optimization procedure using hill-climbing updates, guided by a forward simulator, progressively shifting the empirical posterior toward higher-likelihood parameter regions. Experiments on the observed light curve of PSR J0030+0451, captured by NASA's Neutron Star Interior Composition Explorer (NICER), show that our method closely matches posterior estimates obtained using traditional MCMC methods while achieving 120 times reduction in inference time (from 24 hours to 12 minutes), demonstrating the effectiveness of learned representations and simulator-guided optimization for accelerated posterior inference.

</details>


### [195] [GenPANIS: A Latent-Variable Generative Framework for Forward and Inverse PDE Problems in Multiphase Media](https://arxiv.org/abs/2602.14642)
*Matthaios Chatzopoulos,Phaedon-Stelios Koutsourelakis*

Main category: stat.ML

TL;DR: GenPANIS：统一生成式框架，通过连续潜在嵌入保持精确离散微观结构，支持基于梯度的双向推理（正向预测和逆向恢复），在稀疏噪声观测下实现外推泛化。


<details>
  <summary>Details</summary>
Motivation: 多相介质中的逆问题和逆向设计需要处理离散值材料场，导致问题不可微分且与基于梯度的方法不兼容。现有方法要么妥协物理保真度进行连续近似，要么使用分离的重型模型，缺乏统一框架。

Method: 提出GenPANIS统一生成框架：1）学习微观结构和PDE解的联合分布；2）通过连续潜在嵌入保持精确离散微观结构；3）物理感知解码器集成可微分粗粒度PDE求解器；4）可学习归一化流先验捕获复杂后验结构；5）支持无标签数据、物理残差和少量标记对训练。

Result: 在Darcy流和Helmholtz方程上验证：1）在未见边界条件、体积分数和微观结构形态等外推场景保持准确性；2）优于现有方法；3）参数减少10-100倍；4）提供原则性不确定性量化；5）支持稀疏噪声观测。

Conclusion: GenPANIS为多相介质逆问题提供了统一的生成式解决方案，在保持物理保真度的同时实现高效梯度推理，显著减少参数需求并支持外推泛化，为材料逆向设计提供了新范式。

Abstract: Inverse problems and inverse design in multiphase media, i.e., recovering or engineering microstructures to achieve target macroscopic responses, require operating on discrete-valued material fields, rendering the problem non-differentiable and incompatible with gradient-based methods. Existing approaches either relax to continuous approximations, compromising physical fidelity, or employ separate heavyweight models for forward and inverse tasks. We propose GenPANIS, a unified generative framework that preserves exact discrete microstructures while enabling gradient-based inference through continuous latent embeddings. The model learns a joint distribution over microstructures and PDE solutions, supporting bidirectional inference (forward prediction and inverse recovery) within a single architecture. The generative formulation enables training with unlabeled data, physics residuals, and minimal labeled pairs. A physics-aware decoder incorporating a differentiable coarse-grained PDE solver preserves governing equation structure, enabling extrapolation to varying boundary conditions and microstructural statistics. A learnable normalizing flow prior captures complex posterior structure for inverse problems. Demonstrated on Darcy flow and Helmholtz equations, GenPANIS maintains accuracy on challenging extrapolative scenarios - including unseen boundary conditions, volume fractions, and microstructural morphologies, with sparse, noisy observations. It outperforms state-of-the-art methods while using 10 - 100 times fewer parameters and providing principled uncertainty quantification.

</details>


### [196] [The Well-Tempered Classifier: Some Elementary Properties of Temperature Scaling](https://arxiv.org/abs/2602.14862)
*Pierre-Alexandre Mattei,Bruno Loureiro*

Main category: stat.ML

TL;DR: 温度缩放是控制概率模型不确定性的简单方法，本文从理论和几何角度分析了其性质，挑战了关于LLM温度增加会提高多样性的常见观点。


<details>
  <summary>Details</summary>
Motivation: 温度缩放是分类器校准和LLM随机性调节中最流行的方法，但缺乏严格的理论分析。本文旨在填补这一空白，深入探讨温度缩放的性质和理论基础。

Method: 通过理论分析研究温度缩放的性质，从两个新角度进行表征：1）几何角度：证明温度缩放模型是原始模型在给定熵约束下的信息投影；2）线性缩放角度：证明温度缩放是唯一不改变模型硬预测的线性缩放方法。

Result: 对于分类任务，增加温度确实会增加模型的不确定性（特别是熵）。但对于LLM，挑战了"增加温度会增加多样性"的常见观点。提供了温度缩放的两个新理论特征。

Conclusion: 温度缩放具有深刻的理论基础，是信息投影和唯一保持硬预测不变的线性缩放方法。对分类和LLM应用中的温度效应提供了更精确的理解，纠正了关于LLM温度与多样性关系的误解。

Abstract: Temperature scaling is a simple method that allows to control the uncertainty of probabilistic models. It is mostly used in two contexts: improving the calibration of classifiers and tuning the stochasticity of large language models (LLMs). In both cases, temperature scaling is the most popular method for the job. Despite its popularity, a rigorous theoretical analysis of the properties of temperature scaling has remained elusive. We investigate here some of these properties. For classification, we show that increasing the temperature increases the uncertainty in the model in a very general sense (and in particular increases its entropy). However, for LLMs, we challenge the common claim that increasing temperature increases diversity. Furthermore, we introduce two new characterisations of temperature scaling. The first one is geometric: the tempered model is shown to be the information projection of the original model onto the set of models with a given entropy. The second characterisation clarifies the role of temperature scaling as a submodel of more general linear scalers such as matrix scaling and Dirichlet calibration: we show that temperature scaling is the only linear scaler that does not change the hard predictions of the model.

</details>


### [197] [Activation-Space Uncertainty Quantification for Pretrained Networks](https://arxiv.org/abs/2602.14934)
*Richard Bergna,Stefan Depeweg,Sergio Calvo-Ordoñez,Jonathan Plenk,Alvaro Cartea,Jose Miguel Hernández-Lobato*

Main category: stat.ML

TL;DR: GAPA是一种后处理方法，将贝叶斯建模从权重转移到激活空间，通过高斯过程激活函数保持原始模型预测不变，同时提供闭式不确定性估计，无需重训练或采样。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性估计方法通常需要重新训练、蒙特卡洛采样或昂贵的二阶计算，并且可能改变预训练模型的预测。需要一种能够保持冻结主干网络预测不变的高效后处理不确定性估计方法。

Method: GAPA将标准非线性激活函数替换为高斯过程激活函数，其后验均值精确匹配原始激活函数。采用稀疏变分诱导点近似和局部k近邻子集条件化，利用缓存的训练激活进行确定性单次前向传播，无需采样、反向传播或二阶信息。

Result: 在回归、分类、图像分割和语言建模任务中，GAPA在校准性和分布外检测方面匹配或优于强后处理基线方法，同时保持测试时的高效性。

Conclusion: GAPA提供了一种高效的后处理不确定性估计方法，能够保持预训练模型的预测不变，同时提供可靠的不确定性量化，适用于现代深度学习架构。

Abstract: Reliable uncertainty estimates are crucial for deploying pretrained models; yet, many strong methods for quantifying uncertainty require retraining, Monte Carlo sampling, or expensive second-order computations and may alter a frozen backbone's predictions. To address this, we introduce Gaussian Process Activations (GAPA), a post-hoc method that shifts Bayesian modeling from weights to activations. GAPA replaces standard nonlinearities with Gaussian-process activations whose posterior mean exactly matches the original activation, preserving the backbone's point predictions by construction while providing closed-form epistemic variances in activation space. To scale to modern architectures, we use a sparse variational inducing-point approximation over cached training activations, combined with local k-nearest-neighbor subset conditioning, enabling deterministic single-pass uncertainty propagation without sampling, backpropagation, or second-order information. Across regression, classification, image segmentation, and language modeling, GAPA matches or outperforms strong post-hoc baselines in calibration and out-of-distribution detection while remaining efficient at test time.

</details>
