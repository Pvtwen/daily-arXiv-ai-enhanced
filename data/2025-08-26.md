<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 18]
- [cs.LG](#cs.LG) [Total: 157]
- [stat.ML](#stat.ML) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Notes on Deterministic and Stochastic Approaches in Electromagnetic Information Theory](https://arxiv.org/abs/2508.16601)
*Marco Donald Migliore*

Main category: eess.SP

TL;DR: 这篇论文研究了电磁信息理论中确定性咆随机源模型的自由度数相等性，证明了两者在特征值和基函数上的基本一致性。


<details>
  <summary>Details</summary>
Motivation: 探索确定性模型与随机源模型在电磁信息理论中的关系，解释为什么确定性方法在该领域有效。

Method: 通过理论分析比较确定性模型和空间非相关均匀随机源模型的自由度数、特征值和基函数。

Result: 发现两种模型产生相同的自由度数，以及完全一致的特征值咆基函数。

Conclusion: 这种等价性既解释了确定性方法的有效性，也验证了经典电磁方法在电磁信息理论中的适用性。

Abstract: This paper investigates the relationship between the Number of Degrees of
Freedom ($N_{\rm DoF}$) of the field in deterministic and stochastic source
models within Electromagnetic Information Theory (EIT). Our findings
demonstrate a fundamental connection between these two approaches.
Specifically, we show that a deterministic model and a stochastic model with a
spatially incoherent and homogeneous source yield not only the same $N_{\rm
DoF}$ but also identical eigenvalues and basis functions for field
representation. This key equivalence not only explains the effectiveness of
deterministic approaches in EIT but also corroborates the use of classical
electromagnetic methods within this new discipline.

</details>


### [2] [A Practical Approach to the Design of an S-Band Image-Rejecting Dual-Conversion Super-Heterodyne RF Chain of a Receiver Considering Spur Signals](https://arxiv.org/abs/2508.16735)
*Seyed Mohammad Amin Shirinbayan,Gholamreza Moradi*

Main category: eess.SP

TL;DR: 本文提出了一种基于超外差双转换架构的雷达接收机RF段设计，通过两种相互验证的MATLAB代码来抑制杂散信号，优化组件选择并降低实现成本。


<details>
  <summary>Details</summary>
Motivation: 在超外差双转换架构中，杂散信号会严重影响RF链的动态范围，需要有效的方法来抑制这些不良影响。

Method: 采用两种相互验证的MATLAB代码来缓解杂散效应，优化组件选择，并设计了第二和第三滤波器来降低实现成本，使用多种微波软件和全波分析进行详细设计。

Result: 该方法使链路的无杂散动态范围(SFDR)与动态范围的差异最小化，组件选择得到优化，滤波器设计降低了实现成本。

Conclusion: 提出的方法有效抑制了杂散信号，优化了RF链性能，降低了实现成本，适用于各种超外差配置和混频器。

Abstract: This paper presents a typical design of the RF section of a radar receiver,
the chain within a superheterodyne dual-conversion architecture. A significant
challenge in this framework is the occurrence of spur signals, which negatively
impact the dynamic range of the RF chain. When addressing this issue, the paper
introduces an innovative approach to mitigate (or even wipe out) these
undesired effects, utilizing two mutually verifying MATLAB codes. These codes
have been tested with two distinct commercial mixers and could be applied to
any superheterodyne configuration with various mixers. The presented method
makes the Spurious-Free Dynamic Range (SFDR) of the chain the least different
from the dynamic range of the chain. Also, the selection of other components
gets optimized to align with spurious signals consideration, with explanations
provided for these choices. Moreover, two filters of the RF chain, the second
and the third, have been designed to reduce implementation costs. Various
Microwave software and full-wave analyses were employed for detailed design and
analysis, with the results compared to evaluate their performance.

</details>


### [3] [Dual Orthogonal Projections-Based Multiuser Interference Cancellation for mmWave Beamforming in XL-MIMO Systems](https://arxiv.org/abs/2508.16888)
*Jiazhe Li,Nicolò Decarli,Francesco Guidi,Anna Guerra,Alessandro Bazzi,Zhuoming Li*

Main category: eess.SP

TL;DR: 提出了一种迭代双正交投影算法(DOP)用于XL-MIMO系统中的毫米波波束成形，通过交替执行两个正交投影来消除多用户干扰并优化组合器，实现频谱效率的单调提升和快速收敛。


<details>
  <summary>Details</summary>
Motivation: 解决极大规模MIMO系统中毫米波波束成形的多用户干扰问题，提高频谱效率并逼近理论最优性能。

Method: 迭代双正交投影算法，交替执行消除多用户干扰的正交投影和优化接收组合器的正交投影，确保每次迭代都能单调提升频谱效率。

Result: 理论分析和仿真表明，每次迭代用户信号功率单调增加，等效噪声功率单调降低，频谱效率快速收敛并接近脏纸编码的理论最优值，优于现有线性算法。

Conclusion: DOP算法能有效消除XL-MIMO系统中的多用户干扰，以线性复杂度实现接近理论最优的频谱效率性能，为毫米波通信系统提供了高效的波束成形解决方案。

Abstract: This paper investigates multiuser interference (MUI) cancellation for
millimeter-wave (mmWave) beamforming in extremely large-scale multiple-input
multiple-output (XL-MIMO) communication systems. We propose a linear algorithm,
termed iterative dual orthogonal projections (DOP), which alternates between
two orthogonal projections: one to eliminate MUI and the other to refine
combiners, ensuring a monotonic increase in spectral efficiency. Theoretical
analysis and simulation results show that, with each iteration, the signal
power for each user increases monotonically, the equivalent noise power after
receive combining decreases monotonically, and the spectral efficiency improves
accordingly and converges rapidly, closely approaching the theoretical optimum
determined by dirty paper coding (DPC), outperforming existing linear
algorithms in spectral efficiency.

</details>


### [4] [Spatially Correlated Blockage Aware Placement of RIS in IIoT Networks](https://arxiv.org/abs/2508.16946)
*Rashmi Kumari,Gourab Ghatak,Abhishek K. Gupta*

Main category: eess.SP

TL;DR: 研究可重构智能表面(RIS)在工业物联网(IIoT)网络中缓解覆盖盲区和提升传输可靠性的影响，分析阻塞相关性对反射链路建立概率的影响，并与网络控制中继进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 工业物联网在密集阻塞环境中存在覆盖盲区和传输可靠性问题，需要研究RIS技术如何有效缓解这些问题，为实际部署提供设计指导。

Method: 首先分析单阻塞场景下BS-用户和RIS-用户链路阻塞事件的相关性；然后考虑多阻塞情况，推导SNR分布与数据大小、阻塞密度、RIS数量和部署区域的关系；分析阻塞半径阈值；最后与网络控制中继进行中断性能比较。

Result: 识别了独立阻塞假设偏离相关阻塞现实的阈值；发现中继在特定阻塞阈值后提供更高可靠性，但增加RIS数量可缓解这种效应。

Conclusion: 研究结果为在密集阻塞环境中部署RIS辅助的IIoT网络提供了有价值的设计指南，表明RIS技术能有效改善网络覆盖和可靠性。

Abstract: We study the impact of deploying reconfigurable intelligent surfaces (RISs)
in mitigating coverage gaps and enhancing transmission reliability in an
industrial internet of things (IIoT) network. First, we consider a single
blockage scenario and characterize the correlation between blocking events of
the base station (BS)-user and the RIS-user links and study its impact on the
probability of establishing a viable reflected link. Then, by considering
multiple blockages, we derive the distribution of the signal to noise ratio
(SNR) as a function of data size, blockage density, the number of RISs, and the
deployment area. We analyze the impact of normalized blockage radius and
identify the threshold beyond which the assumption of independent blockages
deviates from the ground truth of correlated blocking. Finally, we compare the
outage performance of this RIS-assisted system with that operated with network-
controlled relays, and demonstrate that while the relays provide a higher
reliability beyond a certain blockage threshold, increasing the number of RISs
may help mitigate this effect. These insights offer valuable design guidelines
for deploying RIS-aided IIoT networks in dense blockage environments.

</details>


### [5] [Radio Frequency Identification: Decades at a Time](https://arxiv.org/abs/2508.17051)
*Christopher Saetia,Daniel M. Dobkin,Gregory Durgin*

Main category: eess.SP

TL;DR: 本文简要回顾了RFID技术发展历史和标准，对比了过去预测与现实应用，分析了UHF RFID和HF NFC的成功场景，并展望了包括手机UHF读取、多天线技术、AI整合、感知RFID等未来发展趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 回顾RFID技术发展历程，对比历史预测与现实成就，探讨未来可能的技术进化路径和应用场景。

Method: 通过历史回顾、标准分析、应用场景对比、技术趋势预测等方法，系统性地分析RFID技术的过去、现在和未来。

Result: 确认了UHF RFID在某些领域的巨大成功，HF NFC在其他场景的优势，以及光学识别和主动无线通信在特定应用中的主导地位。

Conclusion: 预计UHF读取功能将普及到手机，RFID技术将向着更复杂的无线界面、AI整合、数字双生、感知能力和循环经济应用方向发展，但仍面临多重挑战和障碍。

Abstract: In this article, we briefly review the history of the use of radio signals to
identify objects, and of the key Radio Frequency Identification (RFID)
standards for ultra-high-frequency (UHF) and near-field communications that
enabled broad use of these technologies in daily life. We will compare the
vision for the future presented by the Auto-ID Lab in the early 21st century
with the reality we see today, two decades and a little after. We will review
some of the applications in which UHF RFID technology has become hugely
successful, others where High Frequency Near-field Communications (HF NFC) is
preferred, and applications where optical identification or active wireless
communications are dominant.
  We will then examine some possible future paths for RFID technology. We
anticipate that UHF read capability will become widely available for
cellphones, making it as universal as NFC and Bluetooth are today. We will look
at more sophisticated radio interfaces, such as multiple-antenna phased arrays
for readers, and tunnel diode reflection for tags. We will discuss the
integration of information from Artificial Intelligence (AI)-based image
processing, barcodes, NFC and UHF tags, into a digital twin of the real
environment experienced by the human user. We will examine the role of RFID
with sensing in improving the management of perishable goods. The role that
RFID might play in a truly circular economy, with intelligent recycling and
reuse, will be discussed. Finally, we survey the many hazards and obstacles
that obstruct the path to an RF-informed future.

</details>


### [6] [Graphon Signal Processing for Spiking and Biological Neural Networks](https://arxiv.org/abs/2508.17246)
*Takuma Sumi,Georgi S. Medvedev*

Main category: eess.SP

TL;DR: 该论文提出基于图论(graphon)的信号处理方法，用于解决计算和生物神经网络中的刺激识别问题，相比传统方法具有更好的稳定性和分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统图信号处理(GSP)方法在处理网络数据时对随机变化不够稳定，计算效率有限。图论(graphon)理论能够表示图和图序列的极限，为处理大规模网络数据提供更稳定的框架。

Method: 使用图论信号处理(GnSP)方法，通过图论谱投影获得试验不变的低维嵌入，应用于尖峰神经网络模拟和钙成像记录分析。

Result: 图论谱投影方法在刺激分类任务上优于主成分分析和离散GSP基线方法，嵌入表示在不同网络大小和噪声水平下保持稳定。

Conclusion: 这是首次将图论信号处理应用于生物神经网络，为神经科学中的图论分析开辟了新途径，证明了该方法在神经数据处理中的有效性和鲁棒性。

Abstract: Graph Signal Processing (GSP) extends classical signal processing to signals
defined on graphs, enabling filtering, spectral analysis, and sampling of data
generated by networks of various kinds. Graphon Signal Processing (GnSP)
develops this framework further by employing the theory of graphons. Graphons
are measurable functions on the unit square that represent graphs and limits of
convergent graph sequences. The use of graphons provides stability of GSP
methods to stochastic variability in network data and improves computational
efficiency for very large networks. We use GnSP to address the stimulus
identification problem (SIP) in computational and biological neural networks.
The SIP is an inverse problem that aims to infer the unknown stimulus s from
the observed network output f. We first validate the approach in spiking neural
network simulations and then analyze calcium imaging recordings. Graphon-based
spectral projections yield trial-invariant, lowdimensional embeddings that
improve stimulus classification over Principal Component Analysis and discrete
GSP baselines. The embeddings remain stable under variations in network
stochasticity, providing robustness to different network sizes and noise
levels. To the best of our knowledge, this is the first application of GnSP to
biological neural networks, opening new avenues for graphon-based analysis in
neuroscience.

</details>


### [7] [Toward Multi-Functional LAWNs with ISAC: Opportunities, Challenges, and the Road Ahead](https://arxiv.org/abs/2508.17354)
*Jun Wu,Weijie Yuan,Xiaoqi Zhang,Yaohuan Yu,Yuanhao Cui,Fan Liu,Geng Sun,Jiacheng Wang,Dusit Niyato,Dong In Kim*

Main category: eess.SP

TL;DR: 本文探讨了集成感知与通信(ISAC)在低空线性网络(LAWNs)中的关键作用，提出了支持控制、计算、无线能量传输和LLM智能的多功能框架，并通过案例研究展示了其优势和研究方向。


<details>
  <summary>Details</summary>
Motivation: 未来低空线性网络(LAWNs)需要实时环境感知和数据交换能力，ISAC被视为基础技术，但现有应用需要更广泛的功能支持。

Method: 从节点级和网络级角度分析ISAC在LAWNs中的作用，强调通过层次集成和协作获得的性能收益，提出支持控制、计算、无线能量传输和LLM智能的多功能框架。

Result: 通过代表性案例研究展示了ISAC启用的LAWNs的优势，高亮了关键设计交易，并提供了有前景的研究方向。

Conclusion: ISAC在LAWNs中具有重要价值，通过多功能扩展可以更好地支持新兴应用，为未来低空线性网络的发展提供了重要技术基础。

Abstract: Integrated sensing and communication (ISAC) has been envisioned as a
foundational technology for future low-altitude wireless networks (LAWNs),
enabling real-time environmental perception and data exchange across
aerial-ground systems. In this article, we first explore the roles of ISAC in
LAWNs from both node-level and network-level perspectives. We highlight the
performance gains achieved through hierarchical integration and cooperation,
wherein key design trade-offs are demonstrated. Apart from physical-layer
enhancements, emerging LAWN applications demand broader functionalities. To
this end, we propose a multi-functional LAWN framework that extends ISAC with
capabilities in control, computation, wireless power transfer, and large
language model (LLM)-based intelligence. We further provide a representative
case study to present the benefits of ISAC-enabled LAWNs and the promising
research directions are finally outlined.

</details>


### [8] [Near-Field Integrated Imaging and Communication in Distributed MIMO Networks](https://arxiv.org/abs/2508.17526)
*Kangda Zhi,Tianyu Yang,Shuangyang Li,Yi Song,Amir Rezaei,Giuseppe Caire*

Main category: eess.SP

TL;DR: 这篇论文提出了一种用于分布式MIMO宽带通信系统的无线电成像框架，考虑了多视角非同向性目标和近场传播效应。设计了两种情景的算法：室内高分辨率小物体成像和室外大规模环境重建。


<details>
  <summary>Details</summary>
Motivation: 解决分布式MIMO宽带系统中的无线电成像问题，考虑多视角非同向性目标和近场传播效应的复杂性，以获得高分辨率的小物体成像和准确的大规模环境重建。

Method: 室内场景采用基于范围迁移算法(RMA)的方案，使用三种天线数组架构：全数组、边界数组和分布式边界数组。室外场景采用基于稀疏贝叶斯学习(SBL)的算法来解决多量测量向量(MMV)问题。

Result: 数值结果证明了所提算法在获取高分辨率小物体和准确重建大规模环境方面的有效性。

Conclusion: 该框架能够有效处理分布式MIMO宽带系统中的无线电成像问题，适用于不同的场景需求，为高分辨率小物体成像和大规模环境重建提供了可靠的解决方案。

Abstract: In this work, we propose a general framework for wireless imaging in
distributed MIMO wideband communication systems, considering multi-view
non-isotropic targets and near-field propagation effects. For indoor scenarios
where the objective is to image small-scale objects with high resolution, we
propose a range migration algorithm (RMA)-based scheme using three kinds of
array architectures: the full array, boundary array, and distributed boundary
array. With non-isotropic near-field channels, we establish the Fourier
transformation (FT)-based relationship between the imaging reflectivity and the
distributed spatial-domain signals and discuss the corresponding theoretical
properties. Next, for outdoor scenarios where the objective is to reconstruct
the large-scale three-dimensional (3D) environment with coarse resolution, we
propose a sparse Bayesian learning (SBL)-based algorithm to solve the multiple
measurement vector (MMV) problem, which further addresses the non-isotropic
reflectivity across different subcarriers. Numerical results demonstrate the
effectiveness of the proposed algorithms in acquiring high-resolution small
objects and accurately reconstructing large-scale environments.

</details>


### [9] [Steerable Invariant Beamformer Using a Differential Line Array of Omnidirectional and Directional Microphones with Null Constraints](https://arxiv.org/abs/2508.17607)
*Yankai Zhang,Jiafeng Ding,Jingjing Ning,Qiaoxi Zhu*

Main category: eess.SP

TL;DR: 提出基于零点约束的方法设计线阵列频率和转向不变差分波束形成器，相比Jacobi-Anger展开方法具有更广的有效范围、更好的主瓣和零点对齐，以及更大的设计灵活性


<details>
  <summary>Details</summary>
Motivation: Jacobi-Anger展开方法需要理想波束模式的解析表达式和合适的截断阶数选择，在实际应用中不够实用。需要一种更灵活、实用的方法来设计频率和转向不变的差分波束形成器

Method: 采用多约束优化框架，首先基于指定零点和期望方向确定参考滤波器和理想波束模式，然后从参考滤波器推导白噪声增益约束，从理想波束模式推导波束模式约束，最后通过考虑波束模式、零点和白噪声增益相关约束获得最优滤波器

Result: 该方法在白噪声增益和均方误差之间取得平衡，实现了鲁棒的频率和转向不变差分波束形成性能，扩展了有效范围，改善了主瓣和零点对齐，提供了更大的麦克风阵列配置和波束模式设计灵活性

Conclusion: 零点约束方法解决了波束模式灵活性和截断误差的限制，提供了更大的设计自由度和改进的实际适用性，仅需要转向方向和零点信息，而不需要解析波束模式表达式

Abstract: Line differential microphone arrays have attracted attention for their
ability to achieve frequency-invariant beampatterns and high directivity.
Recently, the Jacobi-Anger expansion-based approach has enabled the design of
fully steerable-invariant differential beamformers for line arrays combining
omnidirectional and directional microphones. However, this approach relies on
the analytical expression of the ideal beam pattern and the proper selection of
truncation order, which is not always practical. This paper introduces a
null-constraint-based method for designing frequency- and steerable-invariant
differential beamformers using a line array of omnidirectional and directional
microphones. The approach employs a multi-constraint optimisation framework,
where the reference filter and ideal beam pattern are first determined based on
specified nulls and desired direction. Subsequently, the white noise gain
constraint is derived from the reference filter, and the beampattern constraint
is from the ideal beam pattern. The optimal filter is then obtained by
considering constraints related to the beampattern, nulls, and white noise
gain. This method achieves a balance between white noise gain and mean square
error, allowing robust, frequency- and steerableinvariant differential
beamforming performance. It addresses limitations in beam pattern flexibility
and truncation errors, offering greater design freedom and improved practical
applicability. Simulations and experiments demonstrate that this method
outperforms the Jacobi-Anger expansion-based approach in three key aspects: an
extended effective range, improved main lobe and null alignment, and greater
flexibility in microphone array configuration and beam pattern design,
requiring only steering direction and nulls instead of an analytic beam pattern
expression.

</details>


### [10] [Multimodal Radio and Vision Fusion for Robust Localization in Urban V2I Communications](https://arxiv.org/abs/2508.17640)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出基于多模态对比学习回归的V2I定位框架，融合CSI和视觉信息，在复杂城市环境中显著提升定位精度


<details>
  <summary>Details</summary>
Motivation: 城市环境中GPS信号易受高楼遮挡导致定位误差，需要替代或补充技术来实现V2I通信系统的精确定位

Method: 采用多模态对比学习回归框架，结合信道状态信息(CSI)和视觉信息，利用无线和视觉数据的互补优势

Result: 仿真结果表明，提出的CSI和视觉融合模型显著优于传统方法和单模态模型，在复杂城市环境中实现了卓越的定位精度

Conclusion: 该多模态融合方法为V2I应用提供了强大的定位解决方案，克服了传统定位方法的局限性

Abstract: Accurate localization is critical for vehicle-to-infrastructure (V2I)
communication systems, especially in urban areas where GPS signals are often
obstructed by tall buildings, leading to significant positioning errors,
necessitating alternative or complementary techniques for reliable and precise
positioning in applications like autonomous driving and smart city
infrastructure. This paper proposes a multimodal contrastive learning
regression based localization framework for V2I scenarios that combines channel
state information (CSI) with visual information to achieve improved accuracy
and reliability. The approach leverages the complementary strengths of wireless
and visual data to overcome the limitations of traditional localization
methods, offering a robust solution for V2I applications. Simulation results
demonstrate that the proposed CSI and vision fusion model significantly
outperforms traditional methods and single modal models, achieving superior
localization accuracy and precision in complex urban environments.

</details>


### [11] [Symbol Detection Using an Integrate-and-Fire Time Encoding Receiver](https://arxiv.org/abs/2508.17704)
*Neil Irwin Bernardo*

Main category: eess.SP

TL;DR: 提出了一种基于IF-TEM的事件驱动采样接收机架构，直接从时间戳编码中估计传输符号，无需波形重建，并分析了符号错误概率性能。


<details>
  <summary>Details</summary>
Motivation: 事件驱动采样相比均匀采样在功耗和硬件成本方面具有优势，但现有方法需要从时间编码重建波形才能进行符号检测，这增加了系统复杂度。

Method: 设计了一种IF-TEM接收机架构，直接从IF-TEM采样器产生的时间戳编码中估计传输符号，避免了波形重建步骤。开发了符号错误概率的解析近似模型。

Result: 提出的方法能够直接从时间编码进行符号检测，解析近似与蒙特卡洛仿真结果高度匹配。发现发射脉冲成形滤波器的3dB带宽变窄会降低接收机性能。

Conclusion: 证明了波形重建对于IF-TEM系统的符号检测不是必需的，揭示了频谱效率与错误恢复能力之间的权衡关系，为低功耗通信系统设计提供了新思路。

Abstract: Event-driven sampling is a promising alternative to uniform sampling methods,
particularly for systems constrained by power and hardware cost. A notable
example of this sampling approach is the integrate-and-fire time encoding
machine (IF-TEM), which encodes an analog signal into a sequence of time stamps
by generating an event each time the integral of the input signal reaches a
fixed threshold. In this paper, we propose a receiver architecture that
estimates the sequence of transmitted symbols directly from the encoded time
stamps, called time encodings, produced by the IF-TEM sampler on the received
signal. We show that waveform reconstruction from time encodings is not
necessary for symbol detection. We develop an analytical approximation for the
symbol error probability (SEP) of the proposed IF-TEM-based receiver and show
that it closely matches the SEP results obtained through Monte Carlo
simulations. Additionally, we demonstrate that narrowing the 3 dB bandwidth of
the transmit pulse shaping filter degrades the proposed IF-TEM receiver's
performance, highlighting a trade-off between spectral efficiency and error
resilience.

</details>


### [12] [Blind Channel Estimation for RIS-Assisted Millimeter Wave Communication Systems](https://arxiv.org/abs/2508.17710)
*Dianhao Jia,Wenqian Shen,Jianping An,Byonghyo Shim*

Main category: eess.SP

TL;DR: 提出了一种基于压缩感知的RIS辅助多用户毫米波通信系统盲信道估计方法，通过块状传输方案和RIS重构来估计级联信道，无需发送导频序列。


<details>
  <summary>Details</summary>
Motivation: 在RIS辅助通信系统中，传统盲信道估计方法面临两跳信道带来的新挑战，需要减少导频开销并同时估计信道和传输信号。

Method: 采用块状传输方案，在不同数据块中重新配置RIS元件以更好地估计级联信道，每个块内将用户数据映射到码字实现信号恢复和等效信道估计。

Result: 仿真结果表明该方法能够实现较高的信道估计精度和传输信号恢复准确性。

Conclusion: 该方法首次在RIS辅助多用户毫米波系统中实现了基于压缩感知的盲信道估计，有效解决了导频开销问题。

Abstract: In the research of RIS-assisted communication systems, channel estimation is
a problem of vital importance for further performance optimization. In order to
reduce the pilot overhead to the greatest extent, blind channel estimation
methods are required, which can estimate the channel and the transmit signals
at the same time without transmitting pilot sequence. Different from existing
researches in traditional MIMO systems, the RIS-assisted two-hop channel brings
new challenges to the blind channel estimation design. Hence, a novel blind
channel estimation method based on compressed sensing for RIS-assisted
multiuser millimeter wave communication systems is proposed for the first time
in this paper. Specifically, for accurately estimating the RIS-assisted two-hop
channel without transmitting pilots, we propose a block-wise transmission
scheme. Among different blocks of data transmission, RIS elements are
reconfigured for better estimating the cascade channel. Inside each block, data
for each user are mapped to a codeword for realizing the transmit signal
recovery and equivalent channel estimation simultaneously. Simulation results
demonstrate that our method can achieve a considerable accuracy of channel
estimation and transmit signal recovery.

</details>


### [13] [EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models](https://arxiv.org/abs/2508.17742)
*Wei Xiong,Jiangtong Li,Jie Li,Kun Zhu*

Main category: eess.SP

TL;DR: EEG-FM-Bench是首个用于系统评估EEG基础模型的综合基准，提供标准化评估框架、基准测试结果和表征分析，旨在促进EEG基础模型的公平比较和可重复研究。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型的快速发展缺乏标准化评估基准，导致模型比较困难，阻碍了系统性科学进展。为解决这一关键差距，需要建立统一的评估平台。

Method: 构建了包含多样化下游任务和数据集的标准处理与评估协议的统一开源框架，对现有先进基础模型进行基准测试，并进行表征的定性分析。

Result: 研究发现细粒度时空特征交互、多任务统一训练和神经心理学先验知识有助于提升模型性能和泛化能力。

Conclusion: EEG-FM-Bench为公平比较和可重复研究提供了统一平台，将推动更鲁棒和可泛化的EEG基础模型的发展。

Abstract: Electroencephalography (EEG) foundation models are poised to significantly
advance brain signal analysis by learning robust representations from
large-scale, unlabeled datasets. However, their rapid proliferation has
outpaced the development of standardized evaluation benchmarks, which
complicates direct model comparisons and hinders systematic scientific
progress. This fragmentation fosters scientific inefficiency and obscures
genuine architectural advancements. To address this critical gap, we introduce
EEG-FM-Bench, the first comprehensive benchmark for the systematic and
standardized evaluation of EEG foundation models (EEG-FMs). Our contributions
are threefold: (1) we curate a diverse suite of downstream tasks and datasets
from canonical EEG paradigms, implementing standardized processing and
evaluation protocols within a unified open-source framework; (2) we benchmark
prominent state-of-the-art foundation models to establish comprehensive
baseline results for a clear comparison of the current landscape; (3) we
perform qualitative analyses of the learned representations to provide insights
into model behavior and inform future architectural design. Through extensive
experiments, we find that fine-grained spatio-temporal feature interaction,
multitask unified training and neuropsychological priors would contribute to
enhancing model performance and generalization capabilities. By offering a
unified platform for fair comparison and reproducible research, EEG-FM-Bench
seeks to catalyze progress and guide the community toward the development of
more robust and generalizable EEG-FMs. Code is released at
https://github.com/xw1216/EEG-FM-Bench.

</details>


### [14] [Cross-Domain Lifelong Reinforcement Learning for Wireless Sensor Networks](https://arxiv.org/abs/2508.17852)
*Hossein Mohammadi Firouzjaei,Rafaela Scaciota,Sumudu Samarakoon,Beatriz Lorenzo*

Main category: eess.SP

TL;DR: 提出跨域终身强化学习框架CD-L2RL，用于解决能量收集无线传感器网络在动态环境中的能效优化问题，相比传统方法显著提升适应速度和能量收集效率。


<details>
  <summary>Details</summary>
Motivation: 6G系统中能量收集无线传感器网络面临动态环境挑战，包括能量收集条件、网络规模和流量速率的时变特性，需要能够快速适应不同学习和领域变化的智能解决方案。

Method: 提出跨域终身强化学习(CD-L2RL)框架，通过重用过去任务和领域的知识来加速策略适应，处理决策变量固定但策略变化的学习任务，以及决策空间和策略都演化的学习领域。

Result: 在多样化条件下的仿真验证显示，该方法比标准强化学习适应速度快35%，比基于Lyapunov的优化快70%，同时提高了总收集能量。

Conclusion: CD-L2RL框架在动态6G无线传感器网络中具有强大应用潜力，能够确保连续运行并实现快速策略适应。

Abstract: Wireless sensor networks (WSNs) with energy harvesting (EH) are expected to
play a vital role in intelligent 6G systems, especially in industrial sensing
and control, where continuous operation and sustainable energy use are
critical. Given limited energy resources, WSNs must operate efficiently to
ensure long-term performance. Their deployment, however, is challenged by
dynamic environments where EH conditions, network scale, and traffic rates
change over time. In this work, we address system dynamics that yield different
learning tasks, where decision variables remain fixed but strategies vary, as
well as learning domains, where both decision space and strategies evolve. To
handle such scenarios, we propose a cross-domain lifelong reinforcement
learning (CD-L2RL) framework for energy-efficient WSN design. Our CD-L2RL
algorithm leverages prior experience to accelerate adaptation across tasks and
domains. Unlike conventional approaches based on Markov decision processes or
Lyapunov optimization, which assume relatively stable environments, our
solution achieves rapid policy adaptation by reusing knowledge from past tasks
and domains to ensure continuous operations. We validate the approach through
extensive simulations under diverse conditions. Results show that our method
improves adaptation speed by up to 35% over standard reinforcement learning and
up to 70% over Lyapunov-based optimization, while also increasing total
harvested energy. These findings highlight the strong potential of CD-L2RL for
deployment in dynamic 6G WSNs.

</details>


### [15] [Compressed Learning for Nanosurface Deficiency Recognition Using Angle-resolved Scatterometry Data](https://arxiv.org/abs/2508.17873)
*Mehdi Abdollahpour,Carsten Bockelmann,Tajim Md Hasibur Rahman,Armin Dekorsy,Andreas Fischer*

Main category: eess.SP

TL;DR: 提出了一种基于粒子群优化的压缩学习框架，通过智能采样散射测量数据，仅需1%的数据即可实现86%以上的缺陷检测准确率，显著提高了纳米表面检测效率。


<details>
  <summary>Details</summary>
Motivation: 纳米制造需要高精度表面检测，但现有的角度分辨散射测量技术数据采集时间过长，影响了生产效率。需要开发一种能够大幅减少数据采集量同时保持高检测准确率的方法。

Method: 使用粒子群优化算法结合针对散射模式定制的采样方案，识别散射测量数据中的最优采样点，最大化ZnO纳米表面五种不同缺陷级别的检测准确率。

Result: 仅采样1%的数据即可达到86%以上的缺陷检测准确率，采样率提高到6%时准确率进一步提升至94%，在噪声环境下也能保持高性能。

Conclusion: 该压缩学习框架在数据缩减和分类性能之间取得了良好平衡，有效识别了关键采样区域，为纳米制造中的快速表面缺陷检测提供了实用解决方案。

Abstract: Nanoscale manufacturing requires high-precision surface inspection to
guarantee the quality of the produced nanostructures. For production
environments, angle-resolved scatterometry offers a non- invasive and in-line
compatible alternative to traditional surface inspection methods, such as
scanning electron microscopy. However, angle-resolved scatterometry currently
suffers from long data acquisition time. Our study addresses the issue of slow
data acquisition by proposing a compressed learning framework for the accurate
recognition of nanosurface deficiencies using angle-resolved scatterometry
data. The framework uses the particle swarm optimization algorithm with a
sampling scheme customized for scattering patterns. This combination allows the
identification of optimal sampling points in scatterometry data that maximize
the detection accuracy of five different levels of deficiency in ZnO
nanosurfaces. The proposed method significantly reduces the amount of sampled
data while maintaining a high accuracy in deficiency detection, even in noisy
environments. Notably, by sampling only 1% of the data, the method achieves an
accuracy of over 86%, which further improves to 94% when the sampling rate is
increased to 6%. These results demonstrate a favorable balance between data
reduction and classification performance. The obtained results also show that
the compressed learning framework effectively identifies critical sampling
areas.

</details>


### [16] [Synchrosqueezed X-Ray Wavelet-Chirplet Transform for Accurate Chirp Rate Estimation and Retrieval of Modes from Multicomponent Signals with Crossover Instantaneous Frequencies](https://arxiv.org/abs/2508.17942)
*Qingtang Jiang,Shuixin Li,Jiecheng Chen,Lin Li*

Main category: eess.SP

TL;DR: 本文提出了基于X光变换的小波鹅凢变换(XWCT)，解决了传统鹅凢变换在鹅凢率方向衰减慢的问题，并发展了三阶同步压缩变体提高了频率和鹅凢率估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的鹅凢变换和小波-鹅凢变换(WCT)在鹅凢率估计方面准确性较低，主要因为沿鹅凢率方向的衰减速度较慢。虽然同步压缩技术有所改善，但仍需要新方法来提升变换在鹅凢率方向的衰减性能。

Method: 提出了X光变换基础的小波-鹅凢变换(XWCT)，该方法在鹅凢率方向呈现更好的衰减特性。同时发展了WCT和XWCT的三阶同步压缩变体，获得更为锐利的时间-频率-鹅凢率表示。

Result: 实验结果显示XWCT在鹅凢率轴方向呈现显著更快的衰减速度。三阶同步压缩XWCT能够在不需多次同步压缩操作的情况下，实现准确的瞬时频率和鹅凢率估计，以及模态恢复。

Conclusion: XWCT方法有效解决了鹅凢变换在鹅凢率方向衰减慢的问题，三阶同步压缩技术进一步提升了信号分析的准确性和效率，为多组分信号的时频分析提供了更有效的工具。

Abstract: Recent advances in the chirplet transform and wavelet-chirplet transform
(WCT) have enabled the estimation of instantaneous frequencies (IFs) and
chirprates, as well as mode retrieval from multicomponent signals with
crossover IF curves. However, chirprate estimation via these approaches remains
less accurate than IF estimation, primarily due to the slow decay of the
chirplet transform or WCT along the chirprate direction. To address this, the
synchrosqueezed chirplet transform (SCT) and multiple SCT methods were
proposed, achieving moderate improvements in IF and chirprate estimation
accuracy. Nevertheless, a novel approach is still needed to enhance the
transform's decay along the chirprate direction.
  This paper introduces an X-ray transform-based wavelet-chirprate transform,
termed the X-ray wavelet-chirplet transform (XWCT), which exhibits superior
decay along the chirprate direction compared to the WCT. Furthermore,
third-order synchrosqueezed variants of the WCT and XWCT are developed to yield
sharp time-frequency-chirprate representations of signals. Experimental results
demonstrate that the XWCT achieves significantly faster decay along the
chirprate axis, while the third-order synchrosqueezed XWCT enables accurate IF
and chirprate estimation, as well as mode retrieval, without requiring multiple
synchrosqueezing operations.

</details>


### [17] [A Unified Transformer Architecture for Low-Latency and Scalable Wireless Signal Processing](https://arxiv.org/abs/2508.17960)
*Yuto Kawai,Rajeev Koodli*

Main category: eess.SP

TL;DR: 一种统一的Transformer基础的无线电信号处理架构，通过单个简洁的注意力机制整合频道估计、插值和解映射任务，提供低延迟、任务自适应的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的模块化接收器设计存在延迟高、缺乏灵活性的问题，需要一种统一的、可实时部署的数据驱动方案来替代手工设计的PHY层处理模块。

Method: 设计了一种基于Transformer的统一架构，将频道估计、插值和解映射集成到单个简洁的注意力驱动结构中。通过修改最终投影层实现动态适应不同输出格式，支持三个核心应用场景：端到端接收器、频率提值和频道估计。

Result: 在不同用户数量、调制方案和导频配置下都显示出强大的普适性，同时满足实际系统的延迟要求。在准确性、稳健性和计算效率方面都超过了传统基线方法。

Conclusion: 该工作提供了一种可部署的、数据驱动的PHY层处理方案，为下一代无线通信系统的智能化、软件定义信号处理奠定了基础。

Abstract: We propose a unified Transformer-based architecture for wireless signal
processing tasks, offering a low-latency, task-adaptive alternative to
conventional receiver pipelines. Unlike traditional modular designs, our model
integrates channel estimation, interpolation, and demapping into a single,
compact attention-driven architecture designed for real-time deployment. The
model's structure allows dynamic adaptation to diverse output formats by simply
modifying the final projection layer, enabling consistent reuse across receiver
subsystems. Experimental results demonstrate strong generalization to varying
user counts, modulation schemes, and pilot configurations, while satisfying
latency constraints imposed by practical systems. The architecture is evaluated
across three core use cases: (1) an End-to-End Receiver, which replaces the
entire baseband processing pipeline from pilot symbols to bit-level decisions;
(2) Channel Frequency Interpolation, implemented and tested within a
3GPP-compliant OAI+Aerial system; and (3) Channel Estimation, where the model
infers full-band channel responses from sparse pilot observations. In all
cases, our approach outperforms classical baselines in terms of accuracy,
robustness, and computational efficiency. This work presents a deployable,
data-driven alternative to hand-engineered PHY-layer blocks, and lays the
foundation for intelligent, software-defined signal processing in
next-generation wireless communication systems.

</details>


### [18] [Positioning via Probabilistic Graphical Models in RIS-Aided Systems with Channel Estimation Errors](https://arxiv.org/abs/2508.18009)
*Leonardo Tercas,Markku Juntti*

Main category: eess.SP

TL;DR: 提出基于贝叶斯的6D定位框架，在RIS辅助室内系统中估计移动站的位置和旋转角度，使用概率图模型和NUTS采样器处理信道参数误差，显著提升定位精度


<details>
  <summary>Details</summary>
Motivation: 解决室内环境中移动站的精确6D定位问题，特别是在存在信道参数估计误差的情况下，利用RIS技术来提升定位性能

Method: 使用概率图模型表示随机变量的联合概率分布，采用No-U-Turn Sampler (NUTS)基于估计的信道参数来近似后验分布，同时推导了Cramer-Rao下界(CRLB)来评估系统性能

Result: 结果表明RIS能够显著提高定位精度，通过比较有RIS和无RIS的系统性能，验证了RIS在定位中的增强作用

Conclusion: 提出的贝叶斯6D定位框架有效解决了室内RIS辅助系统中的移动站定位问题，RIS技术对提升定位精度具有重要价值

Abstract: We propose a 6D Bayesian-based localization framework to estimate the
position and rotation angles of a mobile station (MS) within an indoor
reconfigurable intelligent surface (RIS)-aided system. This framework relies on
a probabilistic graphical model to represent the joint probability distribution
of random variables through their conditional dependencies and employs the
No-U-Turn Sampler (NUTS) to approximate the posterior distribution based on the
estimated channel parameters. Our framework estimates both the position and
rotation of the mobile station (MS), in the presence of channel parameter
estimation errors. We derive the Cramer-Rao lower bound (CRLB) for the proposed
scenario and use it to evaluate the system's position error bound (PEB) and
rotation error bound (REB). We compare the system performances with and without
RIS. The results demonstrate that the RIS can enhance positioning accuracy
significantly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Convolutional Neural Networks for Accurate Measurement of Train Speed](https://arxiv.org/abs/2508.17096)
*Haitao Tian,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.LG

TL;DR: 本研究探索使用卷积神经网络提高列车速度估计精度，比较三种CNN架构与传统自适应卡尔曼滤波器的性能，发现多分支CNN模型在复杂工况下表现最优


<details>
  <summary>Details</summary>
Motivation: 解决现代铁路系统中列车速度估计的复杂挑战，提高铁路安全性和运营效率

Method: 研究三种CNN架构（单分支2D、单分支1D、多分支模型），与自适应卡尔曼滤波器进行比较，使用带/不带轮滑保护激活的模拟列车运行数据集进行分析

Result: 基于CNN的方法，特别是多分支模型，在准确性和鲁棒性方面优于传统方法，在具有挑战性的运行条件下表现尤为突出

Conclusion: 深度学习技术能够更有效地捕捉复杂交通数据集中的精细模式，具有提升铁路安全和运营效率的潜力

Abstract: In this study, we explore the use of Convolutional Neural Networks for
improving train speed estimation accuracy, addressing the complex challenges of
modern railway systems. We investigate three CNN architectures - single-branch
2D, single-branch 1D, and multiple-branch models - and compare them with the
Adaptive Kalman Filter. We analyse their performance using simulated train
operation datasets with and without Wheel Slide Protection activation. Our
results reveal that CNN-based approaches, especially the multiple-branch model,
demonstrate superior accuracy and robustness compared to traditional methods,
particularly under challenging operational conditions. These findings highlight
the potential of deep learning techniques to enhance railway safety and
operational efficiency by more effectively capturing intricate patterns in
complex transportation datasets.

</details>


### [20] [Quantum-Inspired DRL Approach with LSTM and OU Noise for Cut Order Planning Optimization](https://arxiv.org/abs/2508.16611)
*Yulison Herry Chrisnanto,Julian Evan Chrisnanto*

Main category: cs.LG

TL;DR: 提出量子启发深度强化学习框架(QI-DRL)，结合LSTM和Ornstein-Uhlenbeck噪声，在纺织裁剪规划中实现13%的布料成本节约


<details>
  <summary>Details</summary>
Motivation: 传统基于静态启发式和目录估算的方法难以适应动态生产环境，导致解决方案次优和浪费增加

Method: 量子启发深度强化学习框架，集成LSTM网络和Ornstein-Uhlenbeck噪声，通过1000轮训练

Result: 平均奖励0.81(±0.03)，预测损失降至0.15(±0.02)，相比传统方法节省高达13%的布料成本

Conclusion: 该可扩展自适应框架具有提升制造效率的潜力，为裁剪规划优化开辟了新途径

Abstract: Cut order planning (COP) is a critical challenge in the textile industry,
directly impacting fabric utilization and production costs. Conventional
methods based on static heuristics and catalog-based estimations often struggle
to adapt to dynamic production environments, resulting in suboptimal solutions
and increased waste. In response, we propose a novel Quantum-Inspired Deep
Reinforcement Learning (QI-DRL) framework that integrates Long Short-Term
Memory (LSTM) networks with Ornstein-Uhlenbeck noise. This hybrid approach is
designed to explicitly address key research questions regarding the benefits of
quantum-inspired probabilistic representations, the role of LSTM-based memory
in capturing sequential dependencies, and the effectiveness of OU noise in
facilitating smooth exploration and faster convergence. Extensive training over
1000 episodes demonstrates robust performance, with an average reward of 0.81
(-+0.03) and a steady decrease in prediction loss to 0.15 (-+0.02). A
comparative analysis reveals that the proposed approach achieves fabric cost
savings of up to 13% compared to conventional methods. Furthermore, statistical
evaluations indicate low variability and stable convergence. Despite the fact
that the simulation model makes several simplifying assumptions, these
promising results underscore the potential of the scalable and adaptive
framework to enhance manufacturing efficiency and pave the way for future
innovations in COP optimization.

</details>


### [21] [SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling](https://arxiv.org/abs/2508.17756)
*Fanjiang Ye,Zepeng Zhao,Yi Mu,Jucheng Shen,Renjie Li,Kaijian Wang,Desen Sun,Saurabh Agarwal,Myungjin Lee,Triston Cao,Aditya Akella,Arvind Krishnamurthy,T. S. Eugene Ng,Zhengzhong Tu,Yuke Wang*

Main category: cs.LG

TL;DR: SuperGen是一个基于分块的训练免费框架，用于超高清视频生成，通过分块处理、自适应缓存策略和并行化技术，显著降低计算和内存成本，支持多种分辨率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现出色，但对超高清视频生成存在计算和内存成本过高的问题，需要高效解决方案。

Method: 采用分块处理、训练免费的算法创新，结合自适应区域感知缓存策略和缓存引导的并行化技术。

Result: 在多个基准测试中实现了高性能增益和高输出质量，显著减少了内存占用和计算复杂度。

Conclusion: SuperGen有效解决了超高清视频生成的挑战，提供了高效且高质量的生成方案。

Abstract: Diffusion models have recently achieved remarkable success in generative
tasks (e.g., image and video generation), and the demand for high-quality
content (e.g., 2K/4K videos) is rapidly increasing across various domains.
However, generating ultra-high-resolution videos on existing
standard-resolution (e.g., 720p) platforms remains challenging due to the
excessive re-training requirements and prohibitively high computational and
memory costs. To this end, we introduce SuperGen, an efficient tile-based
framework for ultra-high-resolution video generation. SuperGen features a novel
training-free algorithmic innovation with tiling to successfully support a wide
range of resolutions without additional training efforts while significantly
reducing both memory footprint and computational complexity. Moreover, SuperGen
incorporates a tile-tailored, adaptive, region-aware caching strategy that
accelerates video generation by exploiting redundancy across denoising steps
and spatial regions. SuperGen also integrates cache-guided,
communication-minimized tile parallelism for enhanced throughput and minimized
latency. Evaluations demonstrate that SuperGen harvests the maximum performance
gains while achieving high output quality across various benchmarks.

</details>


### [22] [CrystalDiT: A Diffusion Transformer for Crystal Generation](https://arxiv.org/abs/2508.16614)
*Xiaohan Yi,Guikun Xu,Xi Xiao,Zhong Zhang,Liu Liu,Yatao Bian,Peilin Zhao*

Main category: cs.LG

TL;DR: CrystalDiT是一个用于晶体结构生成的扩散变换器，通过统一的transformer架构处理晶格和原子属性作为单一系统，在MP-20数据集上实现了9.62%的SUN率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 挑战当前架构复杂化的趋势，证明在数据有限的科学领域中，精心设计的简单架构比容易过拟合的复杂替代方案更有效。

Method: 使用统一的transformer架构，将晶格和原子属性视为单一相互依赖系统，结合基于元素周期表的原子表示和平衡训练策略。

Result: 在MP-20数据集上达到9.62%的SUN率（稳定、独特、新颖），显著优于FlowMM（4.38%）和MatterGen（3.42%），生成63.28%独特新颖结构的同时保持可比的稳定性。

Conclusion: 架构简单性在材料发现中比复杂性更有效，特别是在数据有限的科学领域，精心设计的简单架构能够避免过拟合问题。

Abstract: We present CrystalDiT, a diffusion transformer for crystal structure
generation that achieves state-of-the-art performance by challenging the trend
of architectural complexity. Instead of intricate, multi-stream designs,
CrystalDiT employs a unified transformer that imposes a powerful inductive
bias: treating lattice and atomic properties as a single, interdependent
system. Combined with a periodic table-based atomic representation and a
balanced training strategy, our approach achieves 9.62% SUN (Stable, Unique,
Novel) rate on MP-20, substantially outperforming recent methods including
FlowMM (4.38%) and MatterGen (3.42%). Notably, CrystalDiT generates 63.28%
unique and novel structures while maintaining comparable stability rates,
demonstrating that architectural simplicity can be more effective than
complexity for materials discovery. Our results suggest that in data-limited
scientific domains, carefully designed simple architectures outperform
sophisticated alternatives that are prone to overfitting.

</details>


### [23] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: 提出AQ-PCDSys系统，通过量化神经网络和自适应多传感器融合，在资源受限的太空探测平台上实现实时行星陨石坑检测


<details>
  <summary>Details</summary>
Motivation: 行星探测任务需要实时准确的环境感知，但在计算资源受限的硬件上部署深度学习模型面临重大挑战

Method: 结合量化神经网络(QNN)和量化感知训练(QAT)优化模型大小和推理延迟，使用自适应多传感器融合(AMF)模块动态融合光学图像和数字高程模型数据

Result: 系统在保持高精度的同时显著优化了模型大小和推理延迟，适合太空探测任务的实时部署

Conclusion: AQ-PCDSys为行星陨石坑检测提供了计算高效、可靠准确的解决方案，支持下一代自主行星着陆、导航和科学探索

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


### [24] [Leveraging the Christoffel Function for Outlier Detection in Data Streams](https://arxiv.org/abs/2508.16617)
*Kévin Ducharlet,Louise Travé-Massuyès,Jean-Bernard Lasserre,Marie-Véronique Le Lann,Youssef Miloudi*

Main category: cs.LG

TL;DR: 本文提出了两种新的数据流异常检测方法DyCF和DyCG，基于Christoffel函数理论，无需复杂参数调优，在计算效率和内存使用方面表现优异


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测在数据挖掘中至关重要，但现有方法存在参数调优复杂的问题，且需要处理数据流的非平稳性和海量数据挑战

Method: DyCF利用近似理论和正交多项式中的Christoffel函数，DyCG基于Christoffel函数的增长特性，两种方法都建立在严格的代数框架上，专注于低维数据处理且无需内存成本保存历史数据

Result: 实验表明DyCF在执行时间和内存使用方面优于精细调参方法，DyCG虽然性能稍逊但完全无需参数调优，在合成和真实工业数据流上都进行了全面对比

Conclusion: DyCF方法在性能上表现最佳，DyCG则提供了完全无需调参的便利性，两种方法都为数据流异常检测提供了有效的解决方案

Abstract: Outlier detection holds significant importance in the realm of data mining,
particularly with the growing pervasiveness of data acquisition methods. The
ability to identify outliers in data streams is essential for maintaining data
quality and detecting faults. However, dealing with data streams presents
challenges due to the non-stationary nature of distributions and the
ever-increasing data volume. While numerous methods have been proposed to
tackle this challenge, a common drawback is the lack of straightforward
parameterization in many of them. This article introduces two novel methods:
DyCF and DyCG. DyCF leverages the Christoffel function from the theory of
approximation and orthogonal polynomials. Conversely, DyCG capitalizes on the
growth properties of the Christoffel function, eliminating the need for tuning
parameters. Both approaches are firmly rooted in a well-defined algebraic
framework, meeting crucial demands for data stream processing, with a specific
focus on addressing low-dimensional aspects and maintaining data history
without memory cost. A comprehensive comparison between DyCF, DyCG, and
state-of-the-art methods is presented, using both synthetic and real industrial
data streams. The results show that DyCF outperforms fine-tuning methods,
offering superior performance in terms of execution time and memory usage. DyCG
performs less well, but has the considerable advantage of requiring no tuning
at all.

</details>


### [25] [STRelay: A Universal Spatio-Temporal Relaying Framework for Location Prediction with Future Spatiotemporal Contexts](https://arxiv.org/abs/2508.16620)
*Bangchao Deng,Lianhua Ji,Chunhua Chen,Xin Jing,Ling Ding,Bingqing QU,Pengyang Wang,Dingqi Yang*

Main category: cs.LG

TL;DR: STRelay是一个时空中继框架，通过显式建模未来时空上下文来提升位置预测模型性能，在四个真实轨迹数据集上平均提升3.19%-11.56%


<details>
  <summary>Details</summary>
Motivation: 现有位置预测方法主要依赖历史轨迹数据，但忽略了未来时空上下文的重要性，如用户将要旅行的时间和距离信息对预测未来位置具有关键提示作用

Method: STRelay以中继方式建模未来时空上下文，与基础位置预测模型的历史表示集成，通过多任务学习同时预测下一个时间间隔、移动距离间隔和最终位置

Result: 在四个真实轨迹数据集上集成四个最先进的基础模型，STRelay在所有情况下一致提升预测性能3.19%-11.56%，特别对娱乐相关位置和长距离旅行用户群体效果显著

Conclusion: 未来时空上下文特别有助于非日常例行活动的位置预测，这与擅长建模常规日常模式的基础模型形成互补，为高不确定性的移动行为提供了更好的预测能力

Abstract: Next location prediction is a critical task in human mobility modeling,
enabling applications like travel planning and urban mobility management.
Existing methods mainly rely on historical spatiotemporal trajectory data to
train sequence models that directly forecast future locations. However, they
often overlook the importance of the future spatiotemporal contexts, which are
highly informative for the future locations. For example, knowing how much time
and distance a user will travel could serve as a critical clue for predicting
the user's next location. Against this background, we propose \textbf{STRelay},
a universal \textbf{\underline{S}}patio\textbf{\underline{T}}emporal
\textbf{\underline{Relay}}ing framework explicitly modeling the future
spatiotemporal context given a human trajectory, to boost the performance of
different location prediction models. Specifically, STRelay models future
spatiotemporal contexts in a relaying manner, which is subsequently integrated
with the encoded historical representation from a base location prediction
model, enabling multi-task learning by simultaneously predicting the next time
interval, next moving distance interval, and finally the next location. We
evaluate STRelay integrated with four state-of-the-art location prediction base
models on four real-world trajectory datasets. Results demonstrate that STRelay
consistently improves prediction performance across all cases by
3.19\%-11.56\%. Additionally, we find that the future spatiotemporal contexts
are particularly helpful for entertainment-related locations and also for user
groups who prefer traveling longer distances. The performance gain on such
non-daily-routine activities, which often suffer from higher uncertainty, is
indeed complementary to the base location prediction models that often excel at
modeling regular daily routine patterns.

</details>


### [26] [A Retrieval Augmented Spatio-Temporal Framework for Traffic Prediction](https://arxiv.org/abs/2508.16623)
*Weilin Ruan,Xilin Dang,Ziyu Zhou,Sisuo Lyu,Yuxuan Liang*

Main category: cs.LG

TL;DR: RAST是一个基于检索增强的通用交通预测框架，通过解耦编码器、时空检索存储和通用预测器，有效解决了复杂时空依赖建模和细粒度预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前时空图神经网络在交通预测中存在两个关键挑战：(i)建模复杂时空依赖时的上下文容量有限，(ii)由于异质模式导致细粒度时空点预测性低。受检索增强生成(RAG)启发，需要开发新框架来解决这些问题。

Method: 提出RAST框架，包含三个核心设计：1)解耦编码器和查询生成器，捕获分离的空间和时间特征并通过残差融合构建融合查询；2)时空检索存储和检索器，维护和检索向量化的细粒度模式；3)通用骨干预测器，灵活适配预训练STGNN或简单MLP预测器。

Result: 在六个真实世界交通网络（包括大规模数据集）上的广泛实验表明，RAST实现了优越的性能，同时保持了计算效率。

Conclusion: RAST框架成功将检索增强机制与时空建模相结合，为交通预测提供了有效的解决方案，在性能和效率方面都表现出色。

Abstract: Traffic prediction is a cornerstone of modern intelligent transportation
systems and a critical task in spatio-temporal forecasting. Although advanced
Spatio-temporal Graph Neural Networks (STGNNs) and pre-trained models have
achieved significant progress in traffic prediction, two key challenges remain:
(i) limited contextual capacity when modeling complex spatio-temporal
dependencies, and (ii) low predictability at fine-grained spatio-temporal
points due to heterogeneous patterns. Inspired by Retrieval-Augmented
Generation (RAG), we propose RAST, a universal framework that integrates
retrieval-augmented mechanisms with spatio-temporal modeling to address these
challenges. Our framework consists of three key designs: 1) Decoupled Encoder
and Query Generator to capture decoupled spatial and temporal features and
construct a fusion query via residual fusion; 2) Spatio-temporal Retrieval
Store and Retrievers to maintain and retrieve vectorized fine-grained patterns;
and 3) Universal Backbone Predictor that flexibly accommodates pre-trained
STGNNs or simple MLP predictors. Extensive experiments on six real-world
traffic networks, including large-scale datasets, demonstrate that RAST
achieves superior performance while maintaining computational efficiency.

</details>


### [27] [Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework](https://arxiv.org/abs/2508.16629)
*Zeyu Zhang,Quanyu Dai,Rui Li,Xiaohe Bo,Xu Chen,Zhenhua Dong*

Main category: cs.LG

TL;DR: 提出了一种自适应数据驱动的记忆框架，通过建模记忆周期来优化LLM智能体的记忆能力，包括MoE门控检索、可学习聚合过程和任务特定反思机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的记忆机制需要人工预定义，成本高且性能不佳，同时忽略了交互场景中的记忆周期效应。

Method: 设计MoE门控函数促进记忆检索，提出可学习聚合过程改善记忆利用，开发任务特定反思机制适应记忆存储，支持离线和在线策略优化。

Result: 在多个方面进行了全面实验验证方法的有效性，并开源了项目代码。

Conclusion: 该记忆框架使LLM智能体能够在特定环境中有效学习记忆信息，解决了现有方法的局限性。

Abstract: LLM-based agents have been extensively applied across various domains, where
memory stands out as one of their most essential capabilities. Previous memory
mechanisms of LLM-based agents are manually predefined by human experts,
leading to higher labor costs and suboptimal performance. In addition, these
methods overlook the memory cycle effect in interactive scenarios, which is
critical to optimizing LLM-based agents for specific environments. To address
these challenges, in this paper, we propose to optimize LLM-based agents with
an adaptive and data-driven memory framework by modeling memory cycles.
Specifically, we design an MoE gate function to facilitate memory retrieval,
propose a learnable aggregation process to improve memory utilization, and
develop task-specific reflection to adapt memory storage. Our memory framework
empowers LLM-based agents to learn how to memorize information effectively in
specific environments, with both off-policy and on-policy optimization. In
order to evaluate the effectiveness of our proposed methods, we conduct
comprehensive experiments across multiple aspects. To benefit the research
community in this area, we release our project at
https://github.com/nuster1128/learn_to_memorize.

</details>


### [28] [Recurrent Transformer U-Net Surrogate for Flow Modeling and Data Assimilation in Subsurface Formations with Faults](https://arxiv.org/abs/2508.16631)
*Yifu Han,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 基于变化器U-Net的代模型，用于效极地预测地层碎裂带中CO2泄漏的压力和饱和度，支持全局敏感性分析和数据同化应用


<details>
  <summary>Details</summary>
Motivation: 地层碎裂带对液体流动有重要影响，特别是在地质碳存储中可能导致CO2泄漏。需要快速准确的代模型来预测压力和CO2饱和度分布

Method: 开发了一种新的循环变化器U-Net代理模型，包含目标水层、围岩、盖山岩、两条大断层和两个上遍水层。通过对4000个随机采样实例进行训练，考虑了地质元参数和细细细胎属性的层次不确定性

Result: 该代模型比之前的循环殊差U-Net更准确，并在不同泄漏场景下保持准确性。通过全局敏感性分析和数据同化应用，证明在三个水层中同时观测压力和饱和度可显著降低不确定性

Conclusion: 新变化器U-Net代模型能够提供准确快速的CO2泄漏预测，为地质碳存储风险评估和监测策略制定提供了有效工具，多层监测数据收集能显著提升预测准确性

Abstract: Many subsurface formations, including some of those under consideration for
large-scale geological carbon storage, include extensive faults that can
strongly impact fluid flow. In this study, we develop a new recurrent
transformer U-Net surrogate model to provide very fast predictions for pressure
and CO2 saturation in realistic faulted subsurface aquifer systems. The
geomodel includes a target aquifer (into which supercritical CO2 is injected),
surrounding regions, caprock, two extensive faults, and two overlying aquifers.
The faults can act as leakage pathways between the three aquifers. The
heterogeneous property fields in the target aquifer are characterized by
hierarchical uncertainty, meaning both the geological metaparameters (e.g.,
mean and standard deviation of log-permeability) and the detailed cell
properties of each realization, are uncertain. Fault permeabilities are also
treated as uncertain. The model is trained with simulation results for (up to)
4000 randomly sampled realizations. Error assessments show that this model is
more accurate than a previous recurrent residual U-Net, and that it maintains
accuracy for qualitatively different leakage scenarios. The new surrogate is
then used for global sensitivity analysis and data assimilation. A hierarchical
Markov chain Monte Carlo data assimilation procedure is applied. Different
monitoring strategies, corresponding to different amounts and types of observed
data collected at monitoring wells, are considered for three synthetic true
models. Detailed results demonstrate the degree of uncertainty reduction
achieved with the various monitoring strategies. Posterior results for 3D
saturation plumes and leakage volumes indicate the benefits of measuring
pressure and saturation in all three aquifers.

</details>


### [29] [Adaptive Variance-Penalized Continual Learning with Fisher Regularization](https://arxiv.org/abs/2508.16632)
*Krisanu Sarkar*

Main category: cs.LG

TL;DR: 提出了一种新颖的持续学习框架，通过Fisher加权的非对称正则化参数方差，在变分学习范式中动态调节正则化强度，有效解决神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 神经网络中的灾难性遗忘问题是持续学习研究的主要挑战，需要开发能够保持先前任务知识同时学习新任务的方法。

Method: 集成Fisher加权的非对称参数方差正则化到变分学习范式中，根据参数不确定性动态调节正则化强度。

Result: 在SplitMNIST、PermutedMNIST和SplitFashionMNIST等标准基准测试中，相比VCL和EWC等方法有显著改进，有效维持序列任务间的知识并提高模型精度。

Conclusion: 该方法不仅提升即时任务性能，还显著减轻随时间推移的知识退化，有效解决了神经网络灾难性遗忘的根本挑战。

Abstract: The persistent challenge of catastrophic forgetting in neural networks has
motivated extensive research in continual learning . This work presents a novel
continual learning framework that integrates Fisher-weighted asymmetric
regularization of parameter variances within a variational learning paradigm.
Our method dynamically modulates regularization intensity according to
parameter uncertainty, achieving enhanced stability and performance.
Comprehensive evaluations on standard continual learning benchmarks including
SplitMNIST, PermutedMNIST, and SplitFashionMNIST demonstrate substantial
improvements over existing approaches such as Variational Continual Learning
and Elastic Weight Consolidation . The asymmetric variance penalty mechanism
proves particularly effective in maintaining knowledge across sequential tasks
while improving model accuracy. Experimental results show our approach not only
boosts immediate task performance but also significantly mitigates knowledge
degradation over time, effectively addressing the fundamental challenge of
catastrophic forgetting in neural networks

</details>


### [30] [A Novel Unified Extended Matrix for Graph Signal Processing: Theory and Application](https://arxiv.org/abs/2508.16633)
*Yunyan Zheng,Zhichao Zhang,Wei Yao*

Main category: cs.LG

TL;DR: 提出了统一扩展矩阵(UEM)框架，通过参数化设计整合扩展邻接矩阵和统一图表示矩阵，解决了传统图移位算子在建模非相邻节点依赖关系方面的局限性。基于UEM的图傅里叶变换(UEM-GFT)能够自适应调整频谱特性，在异常检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图移位算子(GSOs)在建模非相邻节点之间的依赖关系方面缺乏灵活性，限制了其表示复杂图结构的能力，需要一种更灵活的框架来揭示更多图信号信息。

Method: 提出统一扩展矩阵(UEM)框架，通过参数化设计整合扩展邻接矩阵和统一图表示矩阵。基于UEM开发了图傅里叶变换(UEM-GFT)，能够自适应调整频谱特性。

Result: 理论分析证明了UEM在特定条件下的半正定性和特征值单调性。在合成和真实数据集上的实验表明，UEM-GFT在异常检测任务中优于现有的基于GSO的方法，在不同网络拓扑结构下都表现出优越性能。

Conclusion: UEM框架提供了更灵活的图信号处理工具，能够更好地适应不同图结构并揭示更多图信号信息，UEM-GFT在异常检测等应用中展现出显著优势。

Abstract: Graph signal processing has become an essential tool for analyzing data
structured on irregular domains. While conventional graph shift operators
(GSOs) are effective for certain tasks, they inherently lack flexibility in
modeling dependencies between non-adjacent nodes, limiting their ability to
represent complex graph structures. To address this limitation, this paper
proposes the unified extended matrix (UEM) framework, which integrates the
extended-adjacency matrix and the unified graph representation matrix through
parametric design, so as to be able to flexibly adapt to different graph
structures and reveal more graph signal information. Theoretical analysis of
the UEM is conducted, demonstrating positive semi-definiteness and eigenvalue
monotonicity under specific conditions. Then, we propose graph Fourier
transform based on UEM (UEM-GFT), which can adaptively tune spectral properties
to enhance signal processing performance. Experimental results on synthetic and
real-world datasets demonstrate that the UEM-GFT outperforms existing GSO-based
methods in anomaly detection tasks, achieving superior performance across
varying network topologies.

</details>


### [31] [Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations](https://arxiv.org/abs/2508.16634)
*Zhendong Yang,Jie Wang,Liansong Zong,Xiaorong Liu,Quan Qian,Shiqian Chen*

Main category: cs.LG

TL;DR: 本文提出DGGN框架，通过双粒度表示（细粒度捕获类特定特征，粗粒度保留类无关知识）和边界感知样本优先策略，解决小样本类增量故障诊断中的灾难性遗忘和新数据过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 解决工业系统中小样本类增量故障诊断的挑战，特别是灾难性遗忘旧知识和稀缺新数据过拟合的问题。

Method: 提出双粒度指导网络(DGGN)，包含细粒度表示流（多阶交互聚合模块）和粗粒度表示流，通过多语义交叉注意力机制动态融合，结合边界感知样本优先策略和解耦平衡随机森林分类器。

Result: 在TEP基准和真实MFF数据集上的实验表明，DGGN相比最先进方法具有更优的诊断性能和稳定性。

Conclusion: DGGN框架有效解决了小样本类增量故障诊断中的关键问题，通过双粒度表示和动态融合机制实现了更好的性能表现。

Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to
continuously learn from new fault classes with only a few samples without
forgetting old ones, is critical for real-world industrial systems. However,
this challenging task severely amplifies the issues of catastrophic forgetting
of old knowledge and overfitting on scarce new data. To address these
challenges, this paper proposes a novel framework built upon Dual-Granularity
Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN
explicitly decouples feature learning into two parallel streams: 1) a
fine-grained representation stream, which utilizes a novel Multi-Order
Interaction Aggregation module to capture discriminative, class-specific
features from the limited new samples. 2) a coarse-grained representation
stream, designed to model and preserve general, class-agnostic knowledge shared
across all fault types. These two representations are dynamically fused by a
multi-semantic cross-attention mechanism, where the stable coarse-grained
knowledge guides the learning of fine-grained features, preventing overfitting
and alleviating feature conflicts. To further mitigate catastrophic forgetting,
we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a
decoupled Balanced Random Forest classifier is employed to counter the decision
boundary bias caused by data imbalance. Extensive experiments on the TEP
benchmark and a real-world MFF dataset demonstrate that our proposed DGGN
achieves superior diagnostic performance and stability compared to
state-of-the-art FSC-FD approaches. Our code is publicly available at
https://github.com/MentaY/DGGN

</details>


### [32] [Enhancing Transformer-Based Foundation Models for Time Series Forecasting via Bagging, Boosting and Statistical Ensembles](https://arxiv.org/abs/2508.16641)
*Dhruv D. Modi,Rong Pan*

Main category: cs.LG

TL;DR: 时间序列基础模型在实际部署中存在方差、预偏和不确定性问题，本文通过统计和集成技术显著提升了模型的稳健性和准确性


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型展现出强大的通用性和零样本能力，但在实际运营数据上仍面临预测方差、预偏和不确定性量化的挑战

Method: 探索了一系列统计和集成增强技术，包括基于自助重采样的bagging、回归基础的stacking、预测区间构建、统计残差建模和迭代错误反馈

Result: 在比利时电力负荷预测数据集上，混合方法在多个预测水平上均超过单独基础模型：回归集成获得最低均方错，bootstrap聚合显著减少长上下文错误，残差建模缩正系统偏差，预测区间达到近名义覆盖率

Conclusion: 统计推理与现代基础模型的集成能够在实际应用中实现准确性、可靠性和可解释性的可量提升

Abstract: Time series foundation models (TSFMs) such as Lag-Llama, TimeGPT, Chronos,
MOMENT, UniTS, and TimesFM have shown strong generalization and zero-shot
capabilities for time series forecasting, anomaly detection, classification,
and imputation. Despite these advantages, their predictions still suffer from
variance, domain-specific bias, and limited uncertainty quantification when
deployed on real operational data. This paper investigates a suite of
statistical and ensemble-based enhancement techniques, including
bootstrap-based bagging, regression-based stacking, prediction interval
construction, statistical residual modeling, and iterative error feedback, to
improve robustness and accuracy. Using the Belgium Electricity Short-Term Load
Forecasting dataset as a case study, we demonstrate that the proposed hybrids
consistently outperform standalone foundation models across multiple horizons.
Regression-based ensembles achieve the lowest mean squared error; bootstrap
aggregation markedly reduces long-context errors; residual modeling corrects
systematic bias; and the resulting prediction intervals achieve near nominal
coverage with widths shrinking as context length increases. The results
indicate that integrating statistical reasoning with modern foundation models
yields measurable gains in accuracy, reliability, and interpretability for
real-world time series applications.

</details>


### [33] [From Classical Probabilistic Latent Variable Models to Modern Generative AI: A Unified Perspective](https://arxiv.org/abs/2508.16643)
*Tianhua Chen*

Main category: cs.LG

TL;DR: 本文提出了一个统一视角，将经典和现代生成方法都置于概率潜在变量模型(PLVM)框架下，揭示了生成式AI的理论基础和方法论谱系。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI系统架构各异，但许多都基于概率潜在变量模型的共同基础，需要一个统一框架来理解这些方法的共享原理和推理策略。

Method: 通过将经典模型(如概率PCA、高斯混合模型)到现代深度架构(如VAE、扩散模型、GAN)都纳入PLVM范式，建立统一的理论分类体系。

Result: 构建了一个概念路线图，展示了生成式AI的概率理论基础，阐明了方法论的谱系关系，揭示了不同架构的表征权衡和推理策略差异。

Conclusion: 统一的PLVM视角有助于巩固生成式AI的理论基础，澄清方法发展脉络，并为未来创新提供指导，将新兴架构植根于其概率传统中。

Abstract: From large language models to multi-modal agents, Generative Artificial
Intelligence (AI) now underpins state-of-the-art systems. Despite their varied
architectures, many share a common foundation in probabilistic latent variable
models (PLVMs), where hidden variables explain observed data for density
estimation, latent reasoning, and structured inference. This paper presents a
unified perspective by framing both classical and modern generative methods
within the PLVM paradigm. We trace the progression from classical flat models
such as probabilistic PCA, Gaussian mixture models, latent class analysis, item
response theory, and latent Dirichlet allocation, through their sequential
extensions including Hidden Markov Models, Gaussian HMMs, and Linear Dynamical
Systems, to contemporary deep architectures: Variational Autoencoders as Deep
PLVMs, Normalizing Flows as Tractable PLVMs, Diffusion Models as Sequential
PLVMs, Autoregressive Models as Explicit Generative Models, and Generative
Adversarial Networks as Implicit PLVMs. Viewing these architectures under a
common probabilistic taxonomy reveals shared principles, distinct inference
strategies, and the representational trade-offs that shape their strengths. We
offer a conceptual roadmap that consolidates generative AI's theoretical
foundations, clarifies methodological lineages, and guides future innovation by
grounding emerging architectures in their probabilistic heritage.

</details>


### [34] [Multidimensional Distributional Neural Network Output Demonstrated in Super-Resolution of Surface Wind Speed](https://arxiv.org/abs/2508.16686)
*Harrison J. Goldwyn,Mitchell Krock,Johann Rudi,Daniel Getter,Julie Bessac*

Main category: cs.LG

TL;DR: 提出了一种基于多维高斯损失的神经网络训练框架，能够生成保持空间相关性的闭式预测分布，有效捕捉异方差和非同分布数据的不确定性。


<details>
  <summary>Details</summary>
Motivation: 科学应用中高维相关数据的不确定性量化是一个核心挑战，现有方法难以同时捕捉偶然不确定性和认知不确定性，且缺乏保持空间相关性的闭式多维分布计算方法。

Method: 使用多维高斯损失训练神经网络，通过傅里叶表示协方差矩阵稳定训练并保持空间相关性，引入信息共享正则化策略在图像特定和全局协方差估计间插值。

Result: 在超分辨率降尺度任务中验证了方法的有效性，能够实现高效采样、显式相关性建模，且不影响预测性能。

Conclusion: 该框架为科学模型中的不确定性感知预测提供了有效解决方案，可扩展到更复杂的分布族。

Abstract: Accurate quantification of uncertainty in neural network predictions remains
a central challenge for scientific applications involving high-dimensional,
correlated data. While existing methods capture either aleatoric or epistemic
uncertainty, few offer closed-form, multidimensional distributions that
preserve spatial correlation while remaining computationally tractable. In this
work, we present a framework for training neural networks with a
multidimensional Gaussian loss, generating closed-form predictive distributions
over outputs with non-identically distributed and heteroscedastic structure.
Our approach captures aleatoric uncertainty by iteratively estimating the means
and covariance matrices, and is demonstrated on a super-resolution example. We
leverage a Fourier representation of the covariance matrix to stabilize network
training and preserve spatial correlation. We introduce a novel regularization
strategy -- referred to as information sharing -- that interpolates between
image-specific and global covariance estimates, enabling convergence of the
super-resolution downscaling network trained on image-specific distributional
loss functions. This framework allows for efficient sampling, explicit
correlation modeling, and extensions to more complex distribution families all
without disrupting prediction performance. We demonstrate the method on a
surface wind speed downscaling task and discuss its broader applicability to
uncertainty-aware prediction in scientific models.

</details>


### [35] [AdapSNE: Adaptive Fireworks-Optimized and Entropy-Guided Dataset Sampling for Edge DNN Training](https://arxiv.org/abs/2508.16647)
*Boran Zhao,Hetian Liu,Zihang Yuan,Li Zhu,Fan Yang,Lina Xie Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: AdapSNE是一种改进的边缘设备数据集采样方法，通过烟花算法和熵引导优化解决NMS方法的异常值和采样不均问题，并设计了专用加速器降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 边缘设备直接训练深度神经网络面临数据集规模大的挑战，现有NMS方法存在异常值和参数选择随意的问题，导致采样偏差和精度下降。

Method: 提出AdapSNE方法：1）使用烟花算法(FWA)进行非单调搜索抑制异常值；2）采用熵引导优化实现均匀采样；3）设计专用加速器优化计算能效。

Result: 该方法能够生成更具代表性的训练样本，提高训练精度，同时通过硬件加速显著降低边缘设备的能耗和面积开销。

Conclusion: AdapSNE有效解决了边缘设备数据集采样的关键问题，在保证精度的同时大幅降低了计算成本，为边缘AI训练提供了实用解决方案。

Abstract: Training deep neural networks (DNNs) directly on edge devices has attracted
increasing attention, as it offers promising solutions to challenges such as
domain adaptation and privacy preservation. However, conventional DNN training
typically requires large-scale datasets, which imposes prohibitive overhead on
edge devices-particularly for emerging large language model (LLM) tasks. To
address this challenge, a DNN-free method (ie., dataset sampling without DNN),
named NMS (Near-Memory Sampling), has been introduced. By first conducting
dimensionality reduction of the dataset and then performing exemplar sampling
in the reduced space, NMS avoids the architectural bias inherent in DNN-based
methods and thus achieves better generalization. However, The state-of-the-art,
NMS, suffers from two limitations: (1) The mismatch between the search method
and the non-monotonic property of the perplexity error function leads to the
emergence of outliers in the reduced representation; (2) Key parameter (ie.,
target perplexity) is selected empirically, introducing arbitrariness and
leading to uneven sampling. These two issues lead to representative bias of
examplars, resulting in degraded accuracy. To address these issues, we propose
AdapSNE, which integrates an efficient non-monotonic search method-namely, the
Fireworks Algorithm (FWA)-to suppress outliers, and employs entropy-guided
optimization to enforce uniform sampling, thereby ensuring representative
training samples and consequently boosting training accuracy. To cut the
edge-side cost arising from the iterative computations of FWA search and
entropy-guided optimization, we design an accelerator with custom dataflow and
time-multiplexing markedly reducing on-device training energy and area.

</details>


### [36] [Sig-DEG for Distillation: Making Diffusion Models Faster and Lighter](https://arxiv.org/abs/2508.16939)
*Lei Jiang,Wen Ge,Niels Cariou-Kotlarek,Mingxuan Yi,Po-Yu Chen,Lingyi Yang,Francois Buet-Golfouse,Gaurav Mittal,Hao Ni*

Main category: cs.LG

TL;DR: Sig-DEG是一种基于签名的扩散模型蒸馏方法，通过粗粒度时间分辨率近似反向扩散过程，将推理步骤减少一个数量级的同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中达到最先进效果，但在推理时计算密集，通常需要数千个离散化步骤，需要高效的推理方法

Method: 利用部分签名有效总结子区间上的布朗运动，采用循环结构实现SDE解的全局精确近似，将蒸馏制定为监督学习任务，在粗时间网格上匹配细分辨率扩散模型的输出

Result: Sig-DEG在减少推理步骤一个数量级的同时实现了竞争性的生成质量

Conclusion: 基于签名的近似方法对于高效生成建模是有效的

Abstract: Diffusion models have achieved state-of-the-art results in generative
modelling but remain computationally intensive at inference time, often
requiring thousands of discretization steps. To this end, we propose Sig-DEG
(Signature-based Differential Equation Generator), a novel generator for
distilling pre-trained diffusion models, which can universally approximate the
backward diffusion process at a coarse temporal resolution. Inspired by
high-order approximations of stochastic differential equations (SDEs), Sig-DEG
leverages partial signatures to efficiently summarize Brownian motion over
sub-intervals and adopts a recurrent structure to enable accurate global
approximation of the SDE solution. Distillation is formulated as a supervised
learning task, where Sig-DEG is trained to match the outputs of a
fine-resolution diffusion model on a coarse time grid. During inference,
Sig-DEG enables fast generation, as the partial signature terms can be
simulated exactly without requiring fine-grained Brownian paths. Experiments
demonstrate that Sig-DEG achieves competitive generation quality while reducing
the number of inference steps by an order of magnitude. Our results highlight
the effectiveness of signature-based approximations for efficient generative
modeling.

</details>


### [37] [LatentFlow: Cross-Frequency Experimental Flow Reconstruction from Sparse Pressure via Latent Mapping](https://arxiv.org/abs/2508.16648)
*Junle Liu,Chang Liu,Yanyu Ke,Qiuxiang Huang,Jiachen Zhao,Wenliang Chen,K. T. Tse,Gang Hu*

Main category: cs.LG

TL;DR: LatentFlow是一个跨模态时间上采样框架，通过融合低频流场和压力数据，仅使用高频壁面压力信号就能重建高频湍流尾流场


<details>
  <summary>Details</summary>
Motivation: 解决PIV实验中获取高频高分辨率湍流尾流场的硬件限制和测量噪声问题，利用更易获得的高频壁面压力测量来重建流场

Method: 两阶段方法：1) 训练压力条件β-VAE学习流场紧凑潜在表示；2) 次级网络将低频压力映射到潜在空间，推理时仅需高频压力输入即可通过解码器生成高频流场

Result: 能够从稀疏壁面压力信号重建512Hz的高频湍流尾流场，实现了时空解耦的流场重建

Conclusion: LatentFlow为数据受限的实验环境提供了可扩展且鲁棒的高频湍流尾流重建解决方案

Abstract: Acquiring temporally high-frequency and spatially high-resolution turbulent
wake flow fields in particle image velocimetry (PIV) experiments remains a
significant challenge due to hardware limitations and measurement noise. In
contrast, temporal high-frequency measurements of spatially sparse wall
pressure are more readily accessible in wind tunnel experiments. In this study,
we propose a novel cross-modal temporal upscaling framework, LatentFlow, which
reconstructs high-frequency (512 Hz) turbulent wake flow fields by fusing
synchronized low-frequency (15 Hz) flow field and pressure data during
training, and high-frequency wall pressure signals during inference. The first
stage involves training a pressure-conditioned $\beta$-variation autoencoder
($p$C-$\beta$-VAE) to learn a compact latent representation that captures the
intrinsic dynamics of the wake flow. A secondary network maps synchronized
low-frequency wall pressure signals into the latent space, enabling
reconstruction of the wake flow field solely from sparse wall pressure. Once
trained, the model utilizes high-frequency, spatially sparse wall pressure
inputs to generate corresponding high-frequency flow fields via the
$p$C-$\beta$-VAE decoder. By decoupling the spatial encoding of flow dynamics
from temporal pressure measurements, LatentFlow provides a scalable and robust
solution for reconstructing high-frequency turbulent wake flows in
data-constrained experimental settings.

</details>


### [38] [Curvature Learning for Generalization of Hyperbolic Neural Networks](https://arxiv.org/abs/2508.17232)
*Xiaomeng Fan,Yuwei Wu,Zhi Gao,Mehrtash Harandi,Yunde Jia*

Main category: cs.LG

TL;DR: 该论文提出了基于PAC-Bayesian泛化界的双曲神经网络曲率学习方法，通过优化损失景观平滑度来提升网络泛化性能


<details>
  <summary>Details</summary>
Motivation: 曲率在双曲神经网络中起关键作用，不合适的曲率会导致网络收敛到次优参数，但目前缺乏曲率对HNNs影响的理论基础

Method: 提出锐度感知曲率学习方法：设计曲率范围锐度度量，通过双层优化最小化该度量，并引入隐式微分算法高效求解梯度近似

Result: 实验在分类、长尾数据学习、噪声数据学习和少样本学习四个场景中验证了方法能有效提升HNNs性能

Conclusion: 该方法通过理论分析和实验验证，证明了曲率优化对双曲神经网络泛化能力的重要作用，为HNNs的曲率选择提供了理论基础和实用方法

Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in
representing real-world data with hierarchical structures via exploiting the
geometric properties of hyperbolic spaces characterized by negative curvatures.
Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may
cause HNNs to converge to suboptimal parameters, degrading overall performance.
So far, the theoretical foundation of the effect of curvatures on HNNs has not
been developed. In this paper, we derive a PAC-Bayesian generalization bound of
HNNs, highlighting the role of curvatures in the generalization of HNNs via
their effect on the smoothness of the loss landscape. Driven by the derived
bound, we propose a sharpness-aware curvature learning method to smooth the
loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized
through a bi-level optimization process. Then, we introduce an implicit
differentiation algorithm that efficiently solves the bi-level optimization by
approximating gradients of curvatures. We present the approximation error and
convergence analyses of the proposed method, showing that the approximation
error is upper-bounded, and the proposed method can converge by bounding
gradients of HNNs. Experiments on four settings: classification, learning from
long-tailed data, learning from noisy data, and few-shot learning show that our
method can improve the performance of HNNs.

</details>


### [39] [HiCL: Hippocampal-Inspired Continual Learning](https://arxiv.org/abs/2508.16651)
*Kushal Kapoor,Wyatt Mackey,Yiannis Aloimonos,Xiaomin Lin*

Main category: cs.LG

TL;DR: HiCL是一个受海马体启发的双记忆持续学习架构，通过网格细胞层编码、齿状回稀疏模式分离、CA3自联想记忆和基于余弦相似度的专家路由机制，有效缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘问题，借鉴海马体神经回路的生物学机制来设计更有效的学习架构。

Method: 使用网格细胞层编码输入，齿状回模块进行稀疏模式分离，CA3自联想记忆存储情景记忆，基于余弦相似度的DG门控专家混合机制进行任务路由，结合弹性权重巩固和优先回放机制。

Result: 在标准持续学习基准测试中表现出色，减少了任务干扰，以较低计算成本达到了接近最先进水平的性能。

Conclusion: HiCL架构通过生物学启发的设计原则，提供了一个可微分、可扩展的持续学习解决方案，在保持高性能的同时降低了计算复杂度。

Abstract: We propose HiCL, a novel hippocampal-inspired dual-memory continual learning
architecture designed to mitigate catastrophic forgetting by using elements
inspired by the hippocampal circuitry. Our system encodes inputs through a
grid-cell-like layer, followed by sparse pattern separation using a dentate
gyrus-inspired module with top-k sparsity. Episodic memory traces are
maintained in a CA3-like autoassociative memory. Task-specific processing is
dynamically managed via a DG-gated mixture-of-experts mechanism, wherein inputs
are routed to experts based on cosine similarity between their normalized
sparse DG representations and learned task-specific DG prototypes computed
through online exponential moving averages. This biologically grounded yet
mathematically principled gating strategy enables differentiable, scalable
task-routing without relying on a separate gating network, and enhances the
model's adaptability and efficiency in learning multiple sequential tasks.
Cortical outputs are consolidated using Elastic Weight Consolidation weighted
by inter-task similarity. Crucially, we incorporate prioritized replay of
stored patterns to reinforce essential past experiences. Evaluations on
standard continual learning benchmarks demonstrate the effectiveness of our
architecture in reducing task interference, achieving near state-of-the-art
results in continual learning tasks at lower computational costs.

</details>


### [40] [Riemannian Change Point Detection on Manifolds with Robust Centroid Estimation](https://arxiv.org/abs/2508.18045)
*Xiuheng Wang,Ricardo Borsoi,Arnaud Breloy,Cédric Richard*

Main category: cs.LG

TL;DR: 提出了一种基于鲁棒质心的流式时间序列变点检测方法，通过比较Karcher均值与Huber函数定义的鲁棒质心来构建检测统计量，解决了传统方法对步长调参敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 传统流式时间序列变点检测方法在计算质心更新时需要仔细调整步长参数，这在实际应用中存在困难。为了解决这个问题，研究者希望利用流形上的鲁棒质心理论来改进检测性能。

Method: 利用M-估计理论中的鲁棒质心概念，比较对变化敏感的经典Karcher均值与对变化鲁棒的Huber函数定义的质心，构建新的检测统计量，并提出了随机黎曼优化算法来高效估计这两种质心。

Result: 在模拟数据和真实世界数据上的实验表明，该方法在两个代表性流形上都表现出优越的性能，对底层估计方法的敏感性更低。

Conclusion: 基于鲁棒质心的变点检测方法有效解决了传统方法对步长调参敏感的问题，在流式时间序列变点检测中表现出更好的性能和鲁棒性。

Abstract: Non-parametric change-point detection in streaming time series data is a
long-standing challenge in signal processing. Recent advancements in statistics
and machine learning have increasingly addressed this problem for data residing
on Riemannian manifolds. One prominent strategy involves monitoring abrupt
changes in the center of mass of the time series. Implemented in a streaming
fashion, this strategy, however, requires careful step size tuning when
computing the updates of the center of mass. In this paper, we propose to
leverage robust centroid on manifolds from M-estimation theory to address this
issue. Our proposal consists of comparing two centroid estimates: the classical
Karcher mean (sensitive to change) versus one defined from Huber's function
(robust to change). This comparison leads to the definition of a test statistic
whose performance is less sensitive to the underlying estimation method. We
propose a stochastic Riemannian optimization algorithm to estimate both robust
centroids efficiently. Experiments conducted on both simulated and real-world
data across two representative manifolds demonstrate the superior performance
of our proposed method.

</details>


### [41] [Provable Generalization in Overparameterized Neural Nets](https://arxiv.org/abs/2508.17256)
*Aviral Dhingra*

Main category: cs.LG

TL;DR: 该论文提出了一种基于注意力矩阵有效秩的新容量度量方法，用于解释过参数化Transformer模型的泛化能力，而非传统的VC维度等复杂度度量。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络参数数量远超训练样本但仍能良好泛化，传统复杂度度量在过参数化机制下失效，需要新的理论来解释Transformer等模型的经验成功。

Method: 基于注意力矩阵的有效秩来定义注意力模型的容量，分析注意力机制的功能维度而非原始参数数量。

Result: 该容量度量导出的泛化边界与大型语言模型观察到的经验缩放规律相匹配（至对数因子），表明注意力谱特性是理解泛化的正确视角。

Conclusion: 虽然分析不是完整的过参数化学习理论，但提供了证据表明注意力的谱特性（而非原始参数数量）可能是理解这些模型泛化能力的关键因素。

Abstract: Deep neural networks often contain far more parameters than training
examples, yet they still manage to generalize well in practice. Classical
complexity measures such as VC-dimension or PAC-Bayes bounds usually become
vacuous in this overparameterized regime, offering little explanation for the
empirical success of models like Transformers. In this work, I explore an
alternative notion of capacity for attention-based models, based on the
effective rank of their attention matrices. The intuition is that, although the
parameter count is enormous, the functional dimensionality of attention is
often much lower. I show that this quantity leads to a generalization bound
whose dependence on sample size matches empirical scaling laws observed in
large language models, up to logarithmic factors. While the analysis is not a
complete theory of overparameterized learning, it provides evidence that
spectral properties of attention, rather than raw parameter counts, may be the
right lens for understanding why these models generalize.

</details>


### [42] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: 提出结合Transformer和Laplace扩散技术的模型，通过物理活动数据来预测心率波动，相比现有方法平均绝对误差降低43%


<details>
  <summary>Details</summary>
Motivation: 现有远程心率监测系统缺乏对患者实际体力活动的关联分析，导致难以判断心率变化的临床意义，AI模型集成活动数据的研究仍较少

Method: 使用Transformer模型结合Laplace扩散技术，通过专门的嵌入和注意力机制将活动上下文整合到整个建模过程中，采用情境化嵌入和专用编码器捕获长期模式和活动特异性心率动态

Result: 在29名患者4个月的真实数据集上验证，平均绝对误差比基线模型降低43%，决定系数R2达到0.97，表明预测心率与实际值高度一致

Conclusion: 该模型是支持医疗保健提供者和远程患者监测系统的实用有效工具，能够显著提升心率监测的准确性和上下文理解能力

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [43] [Convergence and Generalization of Anti-Regularization for Parametric Models](https://arxiv.org/abs/2508.17412)
*Dongseok Kim,Wonjun Jeong,Gisung Oh*

Main category: cs.LG

TL;DR: Anti-regularization (AR) 是一种在小样本情况下通过添加符号反转的奖励项来增加模型表达能力的方法，随着样本量增加使用幂律衰减来减弱干预，包含稳定性保障机制以防止过拟合和数值不稳定。


<details>
  <summary>Details</summary>
Motivation: 解决小样本学习中的欠拟合问题，在数据受限环境下提高模型表达能力，同时保持泛化性能和校准能力。

Method: 在损失函数中添加符号反转的奖励项，使用幂律衰减策略，结合投影算子和梯度裁剪的稳定性保障机制，在NTK机制下进行分析并设计自由度目标调度。

Result: AR方法减少了欠拟合，保持了泛化性能，改善了回归和分类任务的校准能力，在数据和资源受限的环境中实现了稳健学习。

Conclusion: Anti-regularization是一种简单易实现、可复现的方法，能够无缝集成到标准经验风险最小化流程中，在需要时进行干预并在不必要时逐渐消退。

Abstract: We propose Anti-regularization (AR), which adds a sign-reversed reward term
to the loss to intentionally increase model expressivity in the small-sample
regime, and then attenuates this intervention with a power-law decay as the
sample size grows. We formalize spectral safety and trust-region conditions,
and design a lightweight stability safeguard that combines a projection
operator with gradient clipping, ensuring stable intervention under stated
assumptions. Our analysis spans linear smoothers and the Neural Tangent Kernel
(NTK) regime, providing practical guidance on selecting the decay exponent by
balancing empirical risk against variance. Empirically, AR reduces underfitting
while preserving generalization and improving calibration in both regression
and classification. Ablation studies confirm that the decay schedule and the
stability safeguard are critical to preventing overfitting and numerical
instability. We further examine a degrees-of-freedom targeting schedule that
keeps per-sample complexity approximately constant. AR is simple to implement
and reproducible, integrating cleanly into standard empirical risk minimization
pipelines. It enables robust learning in data- and resource-constrained
settings by intervening only when beneficial and fading away when unnecessary.

</details>


### [44] [OASIS: Open-world Adaptive Self-supervised and Imbalanced-aware System](https://arxiv.org/abs/2508.16656)
*Miru Kim,Mugon Joe,Minhae Kwon*

Main category: cs.LG

TL;DR: 提出了一种针对类别不平衡数据的开放世界问题解决方案，通过对比预训练和可靠伪标签生成，显著提升了模型在开放世界场景中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 机器学习在动态环境中的应用面临开放世界挑战，包括标签偏移、协变量偏移和未知类别出现。现有后训练方法在处理类别不平衡的预训练数据时，对少数类的泛化能力有限。

Method: 采用基于对比学习的预训练方法增强分类性能，特别是对少数类的表现；设计后训练机制生成可靠伪标签；引入选择性激活准则优化后训练过程，减少不必要计算。

Result: 大量实验表明，该方法在多种开放世界场景中，在准确性和效率方面均显著优于最先进的适应技术。

Conclusion: 该方法有效解决了类别不平衡预训练数据下的开放世界问题，通过对比预训练和智能后训练机制，提升了模型对少数类的泛化能力和整体鲁棒性。

Abstract: The expansion of machine learning into dynamic environments presents
challenges in handling open-world problems where label shift, covariate shift,
and unknown classes emerge. Post-training methods have been explored to address
these challenges, adapting models to newly emerging data. However, these
methods struggle when the initial pre-training is performed on class-imbalanced
datasets, limiting generalization to minority classes. To address this, we
propose a method that effectively handles open-world problems even when
pre-training is conducted on imbalanced data. Our contrastive-based
pre-training approach enhances classification performance, particularly for
underrepresented classes. Our post-training mechanism generates reliable
pseudo-labels, improving model robustness against open-world problems. We also
introduce selective activation criteria to optimize the post-training process,
reducing unnecessary computation. Extensive experiments demonstrate that our
method significantly outperforms state-of-the-art adaptation techniques in both
accuracy and efficiency across diverse open-world scenarios.

</details>


### [45] [In-Context Algorithm Emulation in Fixed-Weight Transformers](https://arxiv.org/abs/2508.17550)
*Jerry Yao-Chieh Hu,Hude Liu,Jennifer Yuntong Zhang,Han Liu*

Main category: cs.LG

TL;DR: 研究表明，仅通过冻结权重的两层softmax注意力Transformer架构，就能通过上下文提示模拟多种算法，包括梯度下降和线性回归等，无需参数更新或前馈层。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型是否能够仅通过提示词(prompt)来模拟各种算法，而不需要更新模型参数，从而理解大语言模型的上下文学习能力与算法模拟之间的直接联系。

Method: 构建特定的提示词，将算法参数编码到token表示中，创建尖锐的点积差距，迫使softmax注意力机制按照预定计算路径执行。使用两层softmax注意力模块，甚至单头注意力层（需要更长的提示词）。

Result: 证明对于任何可由固定权重注意力头实现的算法，都存在相应的提示词能够驱动Transformer以任意精度重现算法输出。该方法无需前馈层和参数更新，所有适应都通过提示词完成。

Conclusion: 该研究建立了上下文学习与算法模拟之间的直接联系，揭示了GPT风格基础模型如何仅通过提示词切换算法，在现代Transformer模型中实现了一种算法通用性。

Abstract: We prove that a minimal Transformer architecture with frozen weights is
capable of emulating a broad class of algorithms by in-context prompting. In
particular, for any algorithm implementable by a fixed-weight attention head
(e.g. one-step gradient descent or linear/ridge regression), there exists a
prompt that drives a two-layer softmax attention module to reproduce the
algorithm's output with arbitrary precision. This guarantee extends even to a
single-head attention layer (using longer prompts if necessary), achieving
architectural minimality. Our key idea is to construct prompts that encode an
algorithm's parameters into token representations, creating sharp dot-product
gaps that force the softmax attention to follow the intended computation. This
construction requires no feed-forward layers and no parameter updates. All
adaptation happens through the prompt alone. These findings forge a direct link
between in-context learning and algorithmic emulation, and offer a simple
mechanism for large Transformers to serve as prompt-programmable libraries of
algorithms. They illuminate how GPT-style foundation models may swap algorithms
via prompts alone, establishing a form of algorithmic universality in modern
Transformer models.

</details>


### [46] [WISCA: A Lightweight Model Transition Method to Improve LLM Training via Weight Scaling](https://arxiv.org/abs/2508.16676)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Pingwei Sun,Feiye Huo,Jiayu Qin,Yerui Sun,Yuchen Xie,Xunliang Cai,Xiangyu Zhang,Maoxin He,Guangming Tan,Weile Jia,Tong Zhao*

Main category: cs.LG

TL;DR: 提出了WISCA权重缩放方法，通过优化神经网络权重模式来提升训练效率和模型质量，无需改变网络结构，在GQA架构和LoRA微调任务中显著改善收敛质量。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer训练优化方法主要关注架构修改或优化器调整，缺乏对权重模式的系统性优化。权重模式（权重参数的分布和相对大小）对训练效果有重要影响。

Method: 提出WISCA权重缩放方法，通过重新缩放权重同时保持模型输出不变，间接优化模型的训练轨迹，改善神经网络权重模式。

Result: 实验显示WISCA显著提升收敛质量，在零样本验证任务上平均提升5.6%，训练困惑度平均降低2.12%，在GQA架构和LoRA微调任务中表现尤为突出。

Conclusion: WISCA方法通过系统性优化权重模式，有效提升了LLM的训练效率和模型质量，为Transformer训练优化提供了新的方向。

Abstract: Transformer architecture gradually dominates the LLM field. Recent advances
in training optimization for Transformer-based large language models (LLMs)
primarily focus on architectural modifications or optimizer adjustments.
However, these approaches lack systematic optimization of weight patterns
during training. Weight pattern refers to the distribution and relative
magnitudes of weight parameters in a neural network. To address this issue, we
propose a Weight Scaling method called WISCA to enhance training efficiency and
model quality by strategically improving neural network weight patterns without
changing network structures. By rescaling weights while preserving model
outputs, WISCA indirectly optimizes the model's training trajectory.
Experiments demonstrate that WISCA significantly improves convergence quality
(measured by generalization capability and loss reduction), particularly in
LLMs with Grouped Query Attention (GQA) architectures and LoRA fine-tuning
tasks. Empirical results show 5.6% average improvement on zero-shot validation
tasks and 2.12% average reduction in training perplexity across multiple
architectures.

</details>


### [47] [On the Edge of Memorization in Diffusion Models](https://arxiv.org/abs/2508.17689)
*Sam Buchanan,Druv Pai,Yi Ma,Valentin De Bortoli*

Main category: cs.LG

TL;DR: 本文研究了扩散模型何时会记忆训练数据，何时能生成超越训练数据的新样本。通过理论分析和实验验证，提出了一个临界点理论来预测模型从泛化转向记忆的相变点。


<details>
  <summary>Details</summary>
Motivation: 理解扩散模型的记忆与泛化机制对于实际部署至关重要，特别是在版权侵权和数据隐私等现实问题中。需要科学地研究影响记忆和泛化的不同因素。

Method: 建立了一个科学数学"实验室"，在合成和自然图像数据上训练扩散模型。理论分析了记忆模型与泛化模型的训练损失差异，并通过精心设计的实验验证理论预测。

Result: 理论预测了一个临界参数化程度，当模型参数低于该临界值时，记忆行为占主导。实验验证了该临界点确实对应着扩散模型从泛化到记忆的相变。

Conclusion: 研究提供了一个可分析处理且有实际意义的理论框架，能够解析预测模型开始以记忆为主的规模大小，为未来的理论和实证研究奠定了基础。

Abstract: When do diffusion models reproduce their training data, and when are they
able to generate samples beyond it? A practically relevant theoretical
understanding of this interplay between memorization and generalization may
significantly impact real-world deployments of diffusion models with respect to
issues such as copyright infringement and data privacy. In this work, to
disentangle the different factors that influence memorization and
generalization in practical diffusion models, we introduce a scientific and
mathematical "laboratory" for investigating these phenomena in diffusion models
trained on fully synthetic or natural image-like structured data. Within this
setting, we hypothesize that the memorization or generalization behavior of an
underparameterized trained model is determined by the difference in training
loss between an associated memorizing model and a generalizing model. To probe
this hypothesis, we theoretically characterize a crossover point wherein the
weighted training loss of a fully generalizing model becomes greater than that
of an underparameterized memorizing model at a critical value of model
(under)parameterization. We then demonstrate via carefully-designed experiments
that the location of this crossover predicts a phase transition in diffusion
models trained via gradient descent, validating our hypothesis. Ultimately, our
theory enables us to analytically predict the model size at which memorization
becomes predominant. Our work provides an analytically tractable and
practically meaningful setting for future theoretical and empirical
investigations. Code for our experiments is available at
https://github.com/DruvPai/diffusion_mem_gen.

</details>


### [48] [Recall-Extend Dynamics: Enhancing Small Language Models through Controlled Exploration and Refined Offline Integration](https://arxiv.org/abs/2508.16677)
*Zhong Guan,Likang Wu,Hongke Zhao,Jiahui Wang,Le Wu*

Main category: cs.LG

TL;DR: RED方法通过控制探索空间和优化离线数据整合，增强小语言模型的推理能力，解决了探索不足和蒸馏冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型的推理能力提升，而小语言模型的推理能力增强尚未充分探索。结合大模型蒸馏数据和强化学习验证奖励是自然方法，但面临探索空间不足和蒸馏过程冗余等挑战。

Method: 提出RED方法：1）通过监控模型对离线/在线数据的熵变比率来调节离线SFT权重；2）设计基于样本准确率的策略偏移机制，动态选择模仿离线数据或学习自身策略；3）平衡离线蒸馏与在线强化学习。

Result: 该方法解决了小模型探索空间不足的问题，优化了蒸馏过程中的冗余和复杂性，并处理了离线数据与当前策略之间的分布差异。

Conclusion: RED方法为小语言模型的推理能力提升提供了有效的解决方案，通过控制探索和优化离线整合实现了性能改进。

Abstract: Many existing studies have achieved significant improvements in the reasoning
capabilities of large language models (LLMs) through reinforcement learning
with verifiable rewards (RLVR), while the enhancement of reasoning abilities in
small language models (SLMs) has not yet been sufficiently explored. Combining
distilled data from larger models with RLVR on small models themselves is a
natural approach, but it still faces various challenges and issues. Therefore,
we propose \textit{\underline{R}}ecall-\textit{\underline{E}}xtend
\textit{\underline{D}}ynamics(RED): Enhancing Small Language Models through
Controlled Exploration and Refined Offline Integration. In this paper, we
explore the perspective of varying exploration spaces, balancing offline
distillation with online reinforcement learning. Simultaneously, we
specifically design and optimize for the insertion problem within offline data.
By monitoring the ratio of entropy changes in the model concerning offline and
online data, we regulate the weight of offline-SFT, thereby addressing the
issues of insufficient exploration space in small models and the redundancy and
complexity during the distillation process. Furthermore, to tackle the
distribution discrepancies between offline data and the current policy, we
design a sample-accuracy-based policy shift mechanism that dynamically chooses
between imitating offline distilled data and learning from its own policy.

</details>


### [49] [Evaluating the Quality of the Quantified Uncertainty for (Re)Calibration of Data-Driven Regression Models](https://arxiv.org/abs/2508.17761)
*Jelke Wibbeke,Nico Schönfisch,Sebastian Rohjans,Andreas Rauh*

Main category: cs.LG

TL;DR: 本研究系统分析回归校准指标，发现不同指标经常产生冲突结果，识别ENCE和CWC为最可靠指标，强调指标选择在校准研究中的关键作用


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，数据驱动模型不仅需要准确，还需要提供可靠的不确定性估计（校准）。但现有校准指标在定义、假设和尺度上差异很大，难以跨研究比较结果，且大多数重新校准方法仅使用少量指标评估，不清楚改进是否适用于不同的校准概念

Method: 从文献中系统提取和分类回归校准指标，独立于特定建模方法或重新校准方法对这些指标进行基准测试。通过真实世界数据、合成数据和人工失准数据的受控实验进行评估

Result: 校准指标经常产生冲突结果，许多指标对同一重新校准结果的评估存在分歧，有些甚至得出矛盾结论。ENCE（期望归一化校准误差）和CWC（覆盖宽度准则）在测试中表现最为可靠

Conclusion: 指标选择在校准研究中起着关键作用，当前指标不一致性可能导致通过挑选指标来制造误导性的成功印象，需要谨慎选择可靠的校准评估指标

Abstract: In safety-critical applications data-driven models must not only be accurate
but also provide reliable uncertainty estimates. This property, commonly
referred to as calibration, is essential for risk-aware decision-making. In
regression a wide variety of calibration metrics and recalibration methods have
emerged. However, these metrics differ significantly in their definitions,
assumptions and scales, making it difficult to interpret and compare results
across studies. Moreover, most recalibration methods have been evaluated using
only a small subset of metrics, leaving it unclear whether improvements
generalize across different notions of calibration. In this work, we
systematically extract and categorize regression calibration metrics from the
literature and benchmark these metrics independently of specific modelling
methods or recalibration approaches. Through controlled experiments with
real-world, synthetic and artificially miscalibrated data, we demonstrate that
calibration metrics frequently produce conflicting results. Our analysis
reveals substantial inconsistencies: many metrics disagree in their evaluation
of the same recalibration result, and some even indicate contradictory
conclusions. This inconsistency is particularly concerning as it potentially
allows cherry-picking of metrics to create misleading impressions of success.
We identify the Expected Normalized Calibration Error (ENCE) and the Coverage
Width-based Criterion (CWC) as the most dependable metrics in our tests. Our
findings highlight the critical role of metric selection in calibration
research.

</details>


### [50] [CALR: Corrective Adaptive Low-Rank Decomposition for Efficient Large Language Model Layer Compression](https://arxiv.org/abs/2508.16680)
*Muchammad Daniyal Kautsar,Afra Majida Hariono,Widyawan,Syukron Abu Ishaq Alfarozi,Kuntpong Wararatpanya*

Main category: cs.LG

TL;DR: CALR是一种新型的LLM压缩方法，通过SVD主路径和可学习的低秩校正模块相结合，在减少26.93%-51.77%参数的同时保持59.45%-90.42%的原始性能


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法专注于最小化矩阵重构误差，但忽略了功能性能损失，导致模型功能显著退化

Method: CALR采用双组件压缩方法：SVD压缩层的主路径 + 并行可学习的低秩校正模块，专门训练用于恢复功能残差误差

Result: 在SmolLM2-135M、Qwen3-0.6B和Llama-3.2-1B上测试，CALR在显著减少参数的同时保持了较高性能，优于LaCo、ShortGPT和LoSparse等方法

Conclusion: 将功能信息损失作为可学习信号是一种高效的压缩范式，能够创建更小更高效的LLM，推动其在现实应用中的可访问性和实际部署

Abstract: Large Language Models (LLMs) present significant deployment challenges due to
their immense size and computational requirements. Model compression techniques
are essential for making these models practical for resource-constrained
environments. A prominent compression strategy is low-rank factorization via
Singular Value Decomposition (SVD) to reduce model parameters by approximating
weight matrices. However, standard SVD focuses on minimizing matrix
reconstruction error, often leading to a substantial loss of the model's
functional performance. This performance degradation occurs because existing
methods do not adequately correct for the functional information lost during
compression. To address this gap, we introduce Corrective Adaptive Low-Rank
Decomposition (CALR), a two-component compression approach. CALR combines a
primary path of SVD-compressed layers with a parallel, learnable, low-rank
corrective module that is explicitly trained to recover the functional residual
error. Our experimental evaluation on SmolLM2-135M, Qwen3-0.6B, and
Llama-3.2-1B, demonstrates that CALR can reduce parameter counts by 26.93% to
51.77% while retaining 59.45% to 90.42% of the original model's performance,
consistently outperforming LaCo, ShortGPT, and LoSparse. CALR's success shows
that treating functional information loss as a learnable signal is a highly
effective compression paradigm. This approach enables the creation of
significantly smaller, more efficient LLMs, advancing their accessibility and
practical deployment in real-world applications.

</details>


### [51] [Limits of message passing for node classification: How class-bottlenecks restrict signal-to-noise ratio](https://arxiv.org/abs/2508.17822)
*Jonathan Rubin,Sahil Loomba,Nick S. Jones*

Main category: cs.LG

TL;DR: 本文提出了一个统一的统计框架，通过MPNN表示的信噪比(SNR)揭示了异质性(heterophily)和结构瓶颈之间的关系，并开发了BRIDGE图重连算法来优化图结构。


<details>
  <summary>Details</summary>
Motivation: 消息传递神经网络(MPNNs)在节点分类任务中表现强大，但在异质性(低同类连接性)和结构瓶颈情况下性能受限，需要理论框架来理解这些限制并提出改进方法。

Method: 通过信号噪声比(SNR)分析框架，将模型性能分解为特征相关参数和特征无关敏感度；证明类信号敏感度受高阶同质性约束；提出BRIDGE图重连算法，基于图集合理论优化图结构。

Result: BRIDGE算法在合成基准测试中实现了近乎完美的分类准确率，在所有同质性机制下都表现优异，特别是在MPNN通常表现不佳的"中同质性陷阱"区域显著超越现有重连技术。

Conclusion: 该框架不仅提供了评估MPNN性能的诊断工具，还通过原则性的图修改方法有效提升性能，代码已公开供使用。

Abstract: Message passing neural networks (MPNNs) are powerful models for node
classification but suffer from performance limitations under heterophily (low
same-class connectivity) and structural bottlenecks in the graph. We provide a
unifying statistical framework exposing the relationship between heterophily
and bottlenecks through the signal-to-noise ratio (SNR) of MPNN
representations. The SNR decomposes model performance into feature-dependent
parameters and feature-independent sensitivities. We prove that the sensitivity
to class-wise signals is bounded by higher-order homophily -- a generalisation
of classical homophily to multi-hop neighbourhoods -- and show that low
higher-order homophily manifests locally as the interaction between structural
bottlenecks and class labels (class-bottlenecks). Through analysis of graph
ensembles, we provide a further quantitative decomposition of bottlenecking
into underreaching (lack of depth implying signals cannot arrive) and
oversquashing (lack of breadth implying signals arriving on fewer paths) with
closed-form expressions. We prove that optimal graph structures for maximising
higher-order homophily are disjoint unions of single-class and
two-class-bipartite clusters. This yields BRIDGE, a graph ensemble-based
rewiring algorithm that achieves near-perfect classification accuracy across
all homophily regimes on synthetic benchmarks and significant improvements on
real-world benchmarks, by eliminating the ``mid-homophily pitfall'' where MPNNs
typically struggle, surpassing current standard rewiring techniques from the
literature. Our framework, whose code we make available for public use,
provides both diagnostic tools for assessing MPNN performance, and simple yet
effective methods for enhancing performance through principled graph
modification.

</details>


### [52] [STGAtt: A Spatial-Temporal Unified Graph Attention Network for Traffic Flow Forecasting](https://arxiv.org/abs/2508.16685)
*Zhuding Liang,Jianxun Cui,Qingshuang Zeng,Feng Liu,Nenad Filipovic,Tijana Geroski*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的深度学习模型STGAtt，通过统一图表征和注意力机制有效捕捉交通流的空间-时间依赖关系，在多个数据集上较现有方法显示了更优的预测性能。


<details>
  <summary>Details</summary>
Motivation: 准确及时的交通流预测对智能交通系统至关重要，需要有效捕捉复杂的空间-时间依赖关系。

Method: 提出STGAtt模型，使用统一图表征和注意力机制直接建模空间-时间统一图中的相关性，并通过分区和交换机制捕捉短程和长程相关性。

Result: 在PEMS-BAY和SHMetro数据集上的实验表明STGAtt在各种预测距离下都显示出更优的性能，注意力可视化证实了其能够适应动态交通模式和捕捉长程依赖关系。

Conclusion: STGAtt模型具有强大的交通流预测能力，在实际应用中具有广阔潜力。

Abstract: Accurate and timely traffic flow forecasting is crucial for intelligent
transportation systems. This paper presents a novel deep learning model, the
Spatial-Temporal Unified Graph Attention Network (STGAtt). By leveraging a
unified graph representation and an attention mechanism, STGAtt effectively
captures complex spatial-temporal dependencies. Unlike methods relying on
separate spatial and temporal dependency modeling modules, STGAtt directly
models correlations within a Spatial-Temporal Unified Graph, dynamically
weighing connections across both dimensions. To further enhance its
capabilities, STGAtt partitions traffic flow observation signal into
neighborhood subsets and employs a novel exchanging mechanism, enabling
effective capture of both short-range and long-range correlations. Extensive
experiments on the PEMS-BAY and SHMetro datasets demonstrate STGAtt's superior
performance compared to state-of-the-art baselines across various prediction
horizons. Visualization of attention weights confirms STGAtt's ability to adapt
to dynamic traffic patterns and capture long-range dependencies, highlighting
its potential for real-world traffic flow forecasting applications.

</details>


### [53] [A Novel Framework for Uncertainty Quantification via Proper Scores for Classification and Beyond](https://arxiv.org/abs/2508.18001)
*Sebastian G. Gruber*

Main category: cs.LG

TL;DR: 本博士论文提出了一种基于合适评分(proper scores)的机器学习不确定性量化框架，通过函数Bregman散度实现偏差-方差分解，并将其应用于生成模型和大语言模型的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化是可信任机器学习的关键，但现有方法多为问题特定的，无法轻松转移。合适评分作为被目标分布最小化的损失函数，具有很强的通用性。

Method: 使用函数Bregman散度实现严格合适评分的偏差-方差分解，并采用核评分(kernel score)评估生成模型。同时将检验-锐度分解扩展到分类之外，引入合适检验误差和新的估计器。

Result: 在图像、音频和自然语言生成任务中成功应用，包括大语言模型的不确定性估计超越了现有最优方法。还提供了核球面评分的分解方法，实现了更细粒度的生成图像模型评估。

Conclusion: 该框架为机器学习不确定性量化提供了一种通用、可转移的解决方案，通过理论分析和实验验证，在多个领域取得了显著成效，为可靠机器学习应用提供了重要支撑。

Abstract: In this PhD thesis, we propose a novel framework for uncertainty
quantification in machine learning, which is based on proper scores.
Uncertainty quantification is an important cornerstone for trustworthy and
reliable machine learning applications in practice. Usually, approaches to
uncertainty quantification are problem-specific, and solutions and insights
cannot be readily transferred from one task to another. Proper scores are loss
functions minimized by predicting the target distribution. Due to their very
general definition, proper scores apply to regression, classification, or even
generative modeling tasks. We contribute several theoretical results, that
connect epistemic uncertainty, aleatoric uncertainty, and model calibration
with proper scores, resulting in a general and widely applicable framework. We
achieve this by introducing a general bias-variance decomposition for strictly
proper scores via functional Bregman divergences. Specifically, we use the
kernel score, a kernel-based proper score, for evaluating sample-based
generative models in various domains, like image, audio, and natural language
generation. This includes a novel approach for uncertainty estimation of large
language models, which outperforms state-of-the-art baselines. Further, we
generalize the calibration-sharpness decomposition beyond classification, which
motivates the definition of proper calibration errors. We then introduce a
novel estimator for proper calibration errors in classification, and a novel
risk-based approach to compare different estimators for squared calibration
errors. Last, we offer a decomposition of the kernel spherical score, another
kernel-based proper score, allowing a more fine-grained and interpretable
evaluation of generative image models.

</details>


### [54] [Enhancing Differentially Private Linear Regression via Public Second-Moment](https://arxiv.org/abs/2508.18037)
*Zilong Cao,Hai Zhang*

Main category: cs.LG

TL;DR: 提出一种利用公共数据改进差分隐私线性回归的方法，通过变换私有数据使用公共二阶矩矩阵，提高估计器精度和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私方法仅基于私有数据添加噪声会显著降低效用，需要利用公共数据来增强差分隐私方法的实用性

Method: 基于充分统计扰动(SSP)框架，使用公共二阶矩矩阵变换私有数据来计算变换后的SSP-普通最小二乘估计器(OLSE)，改善条件数

Result: 理论误差分析显示该方法相比标准SSP-OLSE具有更好的鲁棒性和准确性，在合成和真实数据集实验中验证了方法的有效性和实用性

Conclusion: 通过利用公共数据变换私有数据，提出的方法显著提高了差分隐私线性回归的估计精度和鲁棒性，为差分隐私统计学习提供了新思路

Abstract: Leveraging information from public data has become increasingly crucial in
enhancing the utility of differentially private (DP) methods. Traditional DP
approaches often require adding noise based solely on private data, which can
significantly degrade utility. In this paper, we address this limitation in the
context of the ordinary least squares estimator (OLSE) of linear regression
based on sufficient statistics perturbation (SSP) under the unbounded data
assumption. We propose a novel method that involves transforming private data
using the public second-moment matrix to compute a transformed SSP-OLSE, whose
second-moment matrix yields a better condition number and improves the OLSE
accuracy and robustness. We derive theoretical error bounds about our method
and the standard SSP-OLSE to the non-DP OLSE, which reveal the improved
robustness and accuracy achieved by our approach. Experiments on synthetic and
real-world datasets demonstrate the utility and effectiveness of our method.

</details>


### [55] [Native Logical and Hierarchical Representations with Subspace Embeddings](https://arxiv.org/abs/2508.16687)
*Gabriel Moreira,Zita Marinho,Manuel Marques,João Paulo Costeira,Chenyan Xiong*

Main category: cs.LG

TL;DR: 提出将概念表示为线性子空间而非点向量的新嵌入范式，通过子空间维度建模泛化性，子空间包含建模层次关系，支持集合运算和逻辑操作


<details>
  <summary>Details</summary>
Motivation: 传统神经嵌入将概念表示为点向量，擅长相似性计算但在高层推理和非对称关系建模方面存在局限，需要新的表示框架来支持逻辑运算和形式语义

Method: 使用线性子空间表示概念，提出正交投影算子的平滑松弛方法，可微分地学习子空间方向和维度

Result: 在WordNet重建和链接预测任务上达到state-of-the-art，在自然语言推理基准上超越双编码器基线，提供可解释的蕴含关系几何表示

Conclusion: 子空间嵌入为概念表示提供了新的几何基础框架，既保持几何直观性又支持逻辑运算，在推理任务中表现出色且具有良好解释性

Abstract: Traditional neural embeddings represent concepts as points, excelling at
similarity but struggling with higher-level reasoning and asymmetric
relationships. We introduce a novel paradigm: embedding concepts as linear
subspaces. This framework inherently models generality via subspace
dimensionality and hierarchy through subspace inclusion. It naturally supports
set-theoretic operations like intersection (conjunction), linear sum
(disjunction) and orthogonal complements (negations), aligning with classical
formal semantics. To enable differentiable learning, we propose a smooth
relaxation of orthogonal projection operators, allowing for the learning of
both subspace orientation and dimension. Our method achieves state-of-the-art
results in reconstruction and link prediction on WordNet. Furthermore, on
natural language inference benchmarks, our subspace embeddings surpass
bi-encoder baselines, offering an interpretable formulation of entailment that
is both geometrically grounded and amenable to logical operations.

</details>


### [56] [A novel auxiliary equation neural networks method for exactly explicit solutions of nonlinear partial differential equations](https://arxiv.org/abs/2508.16702)
*Shanhao Yuan,Yanqin Liu,Runfa Zhang,Limei Yan,Shunjun Wu,Libo Feng*

Main category: cs.LG

TL;DR: 提出了一种结合神经网络和辅助方程方法的新方法AENNM，用于求解非线性偏微分方程的精确解，通过引入基于Riccati方程的新型激活函数，提高了计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在求解非线性偏微分方程时存在计算效率低和精度不足的问题，需要开发一种结合神经网络近似能力和符号计算高精度的新方法。

Method: 提出辅助方程神经网络方法(AENNM)，集成神经网络模型与辅助方程方法，引入基于Riccati方程解的新型激活函数，构建"2-2-2-1"和"3-2-2-1"神经网络模型。

Result: 成功求解了非线性演化方程、Korteweg-de Vries-Burgers方程和(2+1)维Boussinesq方程，获得了用双曲函数、三角函数和有理函数表示的精确解析解，并通过三维图、等高线图和密度图展示了解的动态特性。

Conclusion: AENNM为求解非线性偏微分方程提供了新颖的方法论框架，在科学和工程领域具有广泛的应用潜力，建立了微分方程理论与深度学习之间的新数学联系。

Abstract: In this study, we firstly propose an auxiliary equation neural networks
method (AENNM), an innovative analytical method that integrates neural networks
(NNs) models with the auxiliary equation method to obtain exact solutions of
nonlinear partial differential equations (NLPDEs). A key novelty of this method
is the introduction of a novel activation function derived from the solutions
of the Riccati equation, establishing a new mathematical link between
differential equations theory and deep learning. By combining the strong
approximation capability of NNs with the high precision of symbolic
computation, AENNM significantly enhances computational efficiency and
accuracy. To demonstrate the effectiveness of the AENNM in solving NLPDEs,
three numerical examples are investigated, including the nonlinear evolution
equation, the Korteweg-de Vries-Burgers equation, and the (2+1)-dimensional
Boussinesq equation. Furthermore, some new trial functions are constructed by
setting specific activation functions within the "2-2-2-1" and "3-2-2-1" NNs
models. By embedding the auxiliary equation method into the NNs framework, we
derive previously unreported solutions. The exact analytical solutions are
expressed in terms of hyperbolic functions, trigonometric functions, and
rational functions. Finally, three-dimensional plots, contour plots, and
density plots are presented to illustrate the dynamic characteristics of the
obtained solutions. This research provides a novel methodological framework for
addressing NLPDEs, with broad applicability across scientific and engineering
fields.

</details>


### [57] [Aligning Distributionally Robust Optimization with Practical Deep Learning Needs](https://arxiv.org/abs/2508.16734)
*Dmitrii Feoktistov,Igor Ignashin,Andrey Veprikov,Nikita Borovko,Alexander Bogdanov,Savelii Chezhegov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: ALSO是一种自适应损失缩放优化器，通过改进的分布鲁棒优化目标，能够为样本组分配权重，在深度学习任务中优于传统优化器和现有DRO方法。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习优化方法对所有训练样本同等对待，而分布鲁棒优化(DRO)虽然能自适应分配样本重要性权重，但与当前DL实践存在显著差距，缺乏对随机梯度的处理能力和组级权重分配功能。

Method: 提出ALSO算法，针对改进的DRO目标设计自适应算法，支持样本组的权重分配，并证明了在非凸目标（典型DL模型）下的收敛性。

Result: 在从表格DL到分割学习等多样化深度学习任务中的实证评估表明，ALSO在性能上优于传统优化器和现有DRO方法。

Conclusion: ALSO成功弥合了DRO与当前DL实践之间的差距，提供了具有自适应性和组级权重分配能力的实用优化方法。

Abstract: While traditional Deep Learning (DL) optimization methods treat all training
samples equally, Distributionally Robust Optimization (DRO) adaptively assigns
importance weights to different samples. However, a significant gap exists
between DRO and current DL practices. Modern DL optimizers require adaptivity
and the ability to handle stochastic gradients, as these methods demonstrate
superior performance. Additionally, for practical applications, a method should
allow weight assignment not only to individual samples, but also to groups of
objects (for example, all samples of the same class). This paper aims to bridge
this gap by introducing ALSO $\unicode{x2013}$ Adaptive Loss Scaling Optimizer
$\unicode{x2013}$ an adaptive algorithm for a modified DRO objective that can
handle weight assignment to sample groups. We prove the convergence of our
proposed algorithm for non-convex objectives, which is the typical case for DL
models. Empirical evaluation across diverse Deep Learning tasks, from Tabular
DL to Split Learning tasks, demonstrates that ALSO outperforms both traditional
optimizers and existing DRO methods.

</details>


### [58] [Deep Learning for Markov Chains: Lyapunov Functions, Poisson's Equation, and Stationary Distributions](https://arxiv.org/abs/2508.16737)
*Yanlin Qu,Jose Blanchet,Peter Glynn*

Main category: cs.LG

TL;DR: 使用深度学习自动构建Lyapunov函数来验证马尔可夫模型的稳定性，通过神经网络求解首跳分析导出的积分方程


<details>
  <summary>Details</summary>
Motivation: Lyapunov函数的传统构造方法需要大量创造性和分析工作，深度学习可以自动化这一过程

Method: 训练神经网络来满足从首跳分析导出的积分方程，适用于紧凑和非紧凑状态空间的马尔可夫链

Result: 方法在排队论等多个示例中证明有效，还能用于求解泊松方程和估计稳态分布

Conclusion: 深度学习可以成功自动化Lyapunov函数的构造过程，为马尔可夫模型的稳定性分析提供有效工具

Abstract: Lyapunov functions are fundamental to establishing the stability of Markovian
models, yet their construction typically demands substantial creativity and
analytical effort. In this paper, we show that deep learning can automate this
process by training neural networks to satisfy integral equations derived from
first-transition analysis. Beyond stability analysis, our approach can be
adapted to solve Poisson's equation and estimate stationary distributions.
While neural networks are inherently function approximators on compact domains,
it turns out that our approach remains effective when applied to Markov chains
on non-compact state spaces. We demonstrate the effectiveness of this
methodology through several examples from queueing theory and beyond.

</details>


### [59] [WST: Weak-to-Strong Knowledge Transfer via Reinforcement Learning](https://arxiv.org/abs/2508.16741)
*Haosen Ge,Shuo Li,Lianghuan Huang*

Main category: cs.LG

TL;DR: 弱到强迁移(WST)框架使用小型教师模型自动生成指令来增强大型学生模型的性能，无需强教师模型即可实现高效提示工程


<details>
  <summary>Details</summary>
Motivation: 传统提示工程具有挑战性，特别是在大型模型闭源或难以微调的场景下，需要一种高效且广泛适用的自动提示优化方法

Method: 使用强化学习，让小型教师模型根据学生模型的结果迭代改进指令，通过弱教师指导强学生的方式实现提示优化

Result: 在推理(MATH-500, GSM8K)和对齐(HH-RLHF)基准测试中取得显著提升：MATH-500提升98%，HH-RLHF提升134%，超越GPT-4o-mini和Llama-70B基线

Conclusion: 小模型能够可靠地支撑大模型，解锁潜在能力同时避免强教师可能引入的误导性提示，WST为高效安全的LLM提示优化提供了可扩展解决方案

Abstract: Effective prompt engineering remains a challenging task for many
applications. We introduce Weak-to-Strong Transfer (WST), an automatic prompt
engineering framework where a small "Teacher" model generates instructions that
enhance the performance of a much larger "Student" model. Unlike prior work,
WST requires only a weak teacher, making it efficient and broadly applicable in
settings where large models are closed-source or difficult to fine-tune. Using
reinforcement learning, the Teacher Model's instructions are iteratively
improved based on the Student Model's outcomes, yielding substantial gains
across reasoning (MATH-500, GSM8K) and alignment (HH-RLHF) benchmarks - 98% on
MATH-500 and 134% on HH-RLHF - and surpassing baselines such as GPT-4o-mini and
Llama-70B. These results demonstrate that small models can reliably scaffold
larger ones, unlocking latent capabilities while avoiding misleading prompts
that stronger teachers may introduce, establishing WST as a scalable solution
for efficient and safe LLM prompt refinement.

</details>


### [60] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 研究探索双曲网络是否能提供更好的嵌入空间用于生物多样性分类，通过多模态对比学习和新型堆叠蕴含目标，在BIOSCAN-1M数据集上取得了竞争性结果，但在细粒度分类和开放世界泛化方面仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 生物多样性研究中的分类学分类需要将生物标本组织成基于多模态证据（如图像和遗传信息）的结构化层次体系，研究旨在探索双曲网络是否能为此类层次模型提供更好的嵌入空间。

Method: 使用对比学习和新型堆叠蕴含目标，将多模态输入嵌入到共享的双曲空间中，在BIOSCAN-1M数据集上进行实验验证。

Result: 双曲嵌入在性能上与欧几里得基线相当，在使用DNA条形码进行未见物种分类方面优于所有其他模型，但细粒度分类和开放世界泛化仍然具有挑战性。

Conclusion: 该框架为生物多样性建模提供了结构感知的基础，在物种发现、生态监测和保护工作方面具有潜在应用价值。

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [61] [Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling](https://arxiv.org/abs/2508.16745)
*Ivan Rodkin,Daniil Orel,Konstantin Smirnov,Arman Bolatov,Bilal Elbouardi,Besher Hassan,Yuri Kuratov,Aydar Bulatov,Preslav Nakov,Timothy Baldwin,Artem Shelmanov,Mikhail Burtsev*

Main category: cs.LG

TL;DR: 该研究探索不同架构和训练方法对模型多步推理能力的影响，发现在细胞自动机框架中，大多数神经网络能够抽象底层规则，但在多步推理任务上表现显著下降，增加模型深度和扩展有效深度可大幅提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何学习和执行多步推理是一个开放性问题，研究旨在探索不同架构和训练方法对模型多步推理能力的影响。

Method: 在细胞自动机框架中，使用随机布尔函数生成状态序列进行训练，排除记忆化影响，测试不同神经架构的多步推理能力。

Result: 模型在下一状态预测上达到高准确率，但在多步推理任务上表现急剧下降；增加模型深度、使用循环结构、记忆机制和测试时计算扩展可显著提升推理能力。

Conclusion: 模型深度对序列计算至关重要，扩展有效深度的方法（如循环、记忆和计算扩展）能大幅增强模型的多步推理能力。

Abstract: Reasoning is a core capability of large language models, yet understanding
how they learn and perform multi-step reasoning remains an open problem. In
this study, we explore how different architectures and training methods affect
model multi-step reasoning capabilities within a cellular automata framework.
By training on state sequences generated with random Boolean functions for
random initial conditions to exclude memorization, we demonstrate that most
neural architectures learn to abstract the underlying rules. While models
achieve high accuracy in next-state prediction, their performance declines
sharply if multi-step reasoning is required. We confirm that increasing model
depth plays a crucial role for sequential computations. We demonstrate that an
extension of the effective model depth with recurrence, memory, and test-time
compute scaling substantially enhances reasoning capabilities.

</details>


### [62] [FAIRWELL: Fair Multimodal Self-Supervised Learning for Wellbeing Prediction](https://arxiv.org/abs/2508.16748)
*Jiaee Cheong,Abtin Mogharabin,Paul Liang,Hatice Gunes,Sinan Kalkan*

Main category: cs.LG

TL;DR: 本文提出FAIRWELL捐失函数，通过多模态自监督学习提高医疗预测任务的公平性，在保持分类性能的同时显著改善公平性表现。


<details>
  <summary>Details</summary>
Motivation: 虽然自监督学习在改善机器学习公平性方面展现出潜力，但还未在多模态语境下进行探索。多模态数据包含模态独特信息，可互相补充，为提高公平性提供了机会。

Method: 基于VICReg方法，提出FAIRWELL主体级捐失函数，包括：(1)方差项-减少对保护属性的依赖；(2)不变性项-确保相似个体的一致预测；(3)协方差项-最小化与保护属性的相关性。

Result: 在三个医疗数据集(D-Vlog、MIMIC、MODMA)上评估，该框架在最小化分类性能下降的前提下，显著提高了公平性表现，并在性能-公平性Pareto前沿上取得重大改善。

Conclusion: FAIRWELL方法能够通过多模态自监督学习有效地学习主体独立表征，实现更公平的多模态预测，为医疗AI的公平性提供了新的解决方案。

Abstract: Early efforts on leveraging self-supervised learning (SSL) to improve machine
learning (ML) fairness has proven promising. However, such an approach has yet
to be explored within a multimodal context. Prior work has shown that, within a
multimodal setting, different modalities contain modality-unique information
that can complement information of other modalities. Leveraging on this, we
propose a novel subject-level loss function to learn fairer representations via
the following three mechanisms, adapting the variance-invariance-covariance
regularization (VICReg) method: (i) the variance term, which reduces reliance
on the protected attribute as a trivial solution; (ii) the invariance term,
which ensures consistent predictions for similar individuals; and (iii) the
covariance term, which minimizes correlational dependence on the protected
attribute. Consequently, our loss function, coined as FAIRWELL, aims to obtain
subject-independent representations, enforcing fairness in multimodal
prediction tasks. We evaluate our method on three challenging real-world
heterogeneous healthcare datasets (i.e. D-Vlog, MIMIC and MODMA) which contain
different modalities of varying length and different prediction tasks. Our
findings indicate that our framework improves overall fairness performance with
minimal reduction in classification performance and significantly improves on
the performance-fairness Pareto frontier.

</details>


### [63] [DR-CircuitGNN: Training Acceleration of Heterogeneous Circuit Graph Neural Network on GPUs](https://arxiv.org/abs/2508.16769)
*Yuebo Luo,Shiyang Li,Junran Tao,Kiran Thorat,Xi Xie,Hongwu Peng,Nuo Xu,Caiwen Ding,Shaoyi Huang*

Main category: cs.LG

TL;DR: 提出了DR-CircuitGNN方法，通过动态ReLU和SpMM内核优化加速EDA电路图上的异构图神经网络训练，实现了显著的速度提升


<details>
  <summary>Details</summary>
Motivation: EDA电路设计复杂度增加，传统GNN无法充分捕获设计复杂性，HGNN虽然能更好表示但计算成本过高，存在性能瓶颈

Method: 利用行稀疏感知动态ReLU优化SpMM内核，在异构图消息传递过程中加速；采用并行优化策略，通过多线程CPU初始化和多cudaStreams实现CPU-GPU并发处理

Result: 在三种CircuitNet设计上，前向和反向传播分别实现3.51倍和4.09倍加速；在完整CircuitNet和Mini-CircuitNet上实现2.71倍加速，相关性得分和错误率影响可忽略

Conclusion: DR-CircuitGNN有效解决了HGNN在EDA电路图中的计算瓶颈问题，为大规模电路设计分析提供了高效解决方案

Abstract: The increasing scale and complexity of integrated circuit design have led to
increased challenges in Electronic Design Automation (EDA). Graph Neural
Networks (GNNs) have emerged as a promising approach to assist EDA design as
circuits can be naturally represented as graphs. While GNNs offer a foundation
for circuit analysis, they often fail to capture the full complexity of EDA
designs. Heterogeneous Graph Neural Networks (HGNNs) can better interpret EDA
circuit graphs as they capture both topological relationships and geometric
features. However, the improved representation capability comes at the cost of
even higher computational complexity and processing cost due to their serial
module-wise message-passing scheme, creating a significant performance
bottleneck. In this paper, we propose DR-CircuitGNN, a fast GPU kernel design
by leveraging row-wise sparsity-aware Dynamic-ReLU and optimizing SpMM kernels
during heterogeneous message-passing to accelerate HGNNs training on
EDA-related circuit graph datasets. To further enhance performance, we propose
a parallel optimization strategy that maximizes CPU-GPU concurrency by
concurrently processing independent subgraphs using multi-threaded CPU
initialization and GPU kernel execution via multiple cudaStreams. Our
experiments show that on three representative CircuitNet designs (small,
medium, large), the proposed method can achieve up to 3.51x and 4.09x speedup
compared to the SOTA for forward and backward propagation, respectively. On
full-size CircuitNet and sampled Mini-CircuitNet, our parallel design enables
up to 2.71x speed up over the official DGL implementation cuSPARSE with
negligible impact on correlation scores and error rates.

</details>


### [64] [Latent Graph Learning in Generative Models of Neural Signals](https://arxiv.org/abs/2508.16776)
*Nathan X. Kodama,Kenneth A. Loparo*

Main category: cs.LG

TL;DR: 该研究探索从神经信号生成模型中学习潜在图结构的方法，通过数值模拟验证发现提取的网络表示与真实有向图存在适度对齐，而共输入图表示则有强对齐性。


<details>
  <summary>Details</summary>
Motivation: 构建神经科学系统的生成模型需要推断时间交互图和高阶结构，但当前基础模型难以提取可解释的潜在图表示，这是一个尚未解决的挑战。

Method: 通过在已知真实连接性的神经回路数值模拟上进行测试，评估多种解释学习模型权重的假设，比较提取的网络表示与底层有向图的对齐程度。

Result: 研究发现提取的网络表示与底层有向图存在适度对齐，而共输入图表示则显示出强对齐性。

Conclusion: 这些发现为在构建大规模神经数据基础模型时融入基于图的几何约束提供了路径和动机。

Abstract: Inferring temporal interaction graphs and higher-order structure from neural
signals is a key problem in building generative models for systems
neuroscience. Foundation models for large-scale neural data represent shared
latent structures of neural signals. However, extracting interpretable latent
graph representations in foundation models remains challenging and unsolved.
Here we explore latent graph learning in generative models of neural signals.
By testing against numerical simulations of neural circuits with known
ground-truth connectivity, we evaluate several hypotheses for explaining
learned model weights. We discover modest alignment between extracted network
representations and the underlying directed graphs and strong alignment in the
co-input graph representations. These findings motivate paths towards
incorporating graph-based geometric constraints in the construction of
large-scale foundation models for neural data.

</details>


### [65] [Interpreting the Effects of Quantization on LLMs](https://arxiv.org/abs/2508.16785)
*Manpreet Singh,Hassan Sajjad*

Main category: cs.LG

TL;DR: 量化对LLM内部表示的影响研究：4位和8位量化对模型校准影响较小，死亡神经元数量保持稳定，神经元贡献模式因模型而异，量化仍是可靠的模型压缩技术


<details>
  <summary>Details</summary>
Motivation: 量化是部署LLM的实用解决方案，但其对内部表示的影响尚未充分研究，需要评估量化模型的可靠性

Method: 使用多种可解释性技术分析4位和8位量化对多个LLM模型和神经元行为的影响

Result: 量化对模型校准影响较小；死亡神经元数量在不同量化下保持一致；小模型有较少显著神经元，大模型有较多（Llama-2-7B除外）；量化对神经元冗余的影响因模型而异

Conclusion: 量化的影响因模型和任务而异，但未观察到剧烈变化，量化仍可作为可靠的模型压缩技术

Abstract: Quantization offers a practical solution to deploy LLMs in
resource-constraint environments. However, its impact on internal
representations remains understudied, raising questions about the reliability
of quantized models. In this study, we employ a range of interpretability
techniques to investigate how quantization affects model and neuron behavior.
We analyze multiple LLMs under 4-bit and 8-bit quantization. Our findings
reveal that the impact of quantization on model calibration is generally minor.
Analysis of neuron activations indicates that the number of dead neurons, i.e.,
those with activation values close to 0 across the dataset, remains consistent
regardless of quantization. In terms of neuron contribution to predictions, we
observe that smaller full precision models exhibit fewer salient neurons,
whereas larger models tend to have more, with the exception of Llama-2-7B. The
effect of quantization on neuron redundancy varies across models. Overall, our
findings suggest that effect of quantization may vary by model and tasks,
however, we did not observe any drastic change which may discourage the use of
quantization as a reliable model compression technique.

</details>


### [66] [Anchor-MoE: A Mean-Anchored Mixture of Experts For Probabilistic Regression](https://arxiv.org/abs/2508.16802)
*Baozhuo Su,Zhengxian Qu*

Main category: cs.LG

TL;DR: Anchor-MoE是一个处理概率回归和点回归的统一模型，使用锚点预测和混合密度网络专家，在理论和实证上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 回归问题在科学和工程中至关重要，需要同时处理概率回归和点回归，现有方法往往只能处理其中一种。

Method: 使用梯度提升模型提供锚点均值，将锚点预测投影到潜在空间，通过可学习的度量窗口核评分局部性，软路由器将样本分发到混合密度网络专家进行异方差校正和预测方差估计。

Result: 理论证明达到最小最大最优风险率，实证在UCI回归数据集上匹配或超越NGBoost基线，多个数据集达到新的最先进概率回归结果。

Conclusion: Anchor-MoE提供了一个统一框架处理概率和点回归，理论保证优秀，实证表现强劲，代码已开源。

Abstract: Regression under uncertainty is fundamental across science and engineering.
We present an Anchored Mixture of Experts (Anchor-MoE), a model that handles
both probabilistic and point regression. For simplicity, we use a tuned
gradient-boosting model to furnish the anchor mean; however, any off-the-shelf
point regressor can serve as the anchor. The anchor prediction is projected
into a latent space, where a learnable metric-window kernel scores locality and
a soft router dispatches each sample to a small set of mixture-density-network
experts; the experts produce a heteroscedastic correction and predictive
variance. We train by minimizing negative log-likelihood, and on a disjoint
calibration split fit a post-hoc linear map on predicted means to improve point
accuracy. On the theory side, assuming a H\"older smooth regression function of
order~$\alpha$ and fixed Lipschitz partition-of-unity weights with bounded
overlap, we show that Anchor-MoE attains the minimax-optimal $L^2$ risk rate
$O\!\big(N^{-2\alpha/(2\alpha+d)}\big)$. In addition, the CRPS test
generalization gap scales as
$\widetilde{O}\!\Big(\sqrt{(\log(Mh)+P+K)/N}\Big)$; it is logarithmic in $Mh$
and scales as the square root in $P$ and $K$. Under bounded-overlap routing,
$K$ can be replaced by $k$, and any dependence on a latent dimension is
absorbed into $P$. Under uniformly bounded means and variances, an analogous
$\widetilde{O}\!\big(\sqrt{(\log(Mh)+P+K)/N}\big)$ scaling holds for the test
NLL up to constants. Empirically, across standard UCI regressions, Anchor-MoE
consistently matches or surpasses the strong NGBoost baseline in RMSE and NLL;
on several datasets it achieves new state-of-the-art probabilistic regression
results on our benchmark suite. Code is available at
https://github.com/BaozhuoSU/Probabilistic_Regression.

</details>


### [67] [Uncertainty Propagation Networks for Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.16815)
*Hadi Jahanshahi,Zheng H. Zhu*

Main category: cs.LG

TL;DR: UPN是一种新型神经微分方程，通过耦合均值和协方差的微分方程，在连续时间建模中自然整合不确定性量化，可应用于连续归一化流、时间序列预测和动力系统轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有神经ODE只能预测状态轨迹，缺乏不确定性量化能力。UPN旨在同时建模状态演化和相关不确定性，为连续时间模型提供原则性的不确定性评估。

Method: 参数化耦合的均值和协方差微分方程，通过求解状态和协方差演化的耦合ODE来有效传播非线性动力学中的不确定性，支持状态相关、可学习的过程噪声。

Result: 实验证明UPN在多个领域有效：带不确定性量化的连续归一化流、具有良好校准置信区间的时间序列预测、稳定和混沌动力系统中的鲁棒轨迹预测。

Conclusion: UPN提供连续深度建模，自适应评估策略，自然处理不规则采样观测，为连续时间模型提供了原则性的不确定性量化框架。

Abstract: This paper introduces Uncertainty Propagation Network (UPN), a novel family
of neural differential equations that naturally incorporate uncertainty
quantification into continuous-time modeling. Unlike existing neural ODEs that
predict only state trajectories, UPN simultaneously model both state evolution
and its associated uncertainty by parameterizing coupled differential equations
for mean and covariance dynamics. The architecture efficiently propagates
uncertainty through nonlinear dynamics without discretization artifacts by
solving coupled ODEs for state and covariance evolution while enabling
state-dependent, learnable process noise. The continuous-depth formulation
adapts its evaluation strategy to each input's complexity, provides principled
uncertainty quantification, and handles irregularly-sampled observations
naturally. Experimental results demonstrate UPN's effectiveness across multiple
domains: continuous normalizing flows (CNFs) with uncertainty quantification,
time-series forecasting with well-calibrated confidence intervals, and robust
trajectory prediction in both stable and chaotic dynamical systems.

</details>


### [68] [Understanding and Tackling Over-Dilution in Graph Neural Networks](https://arxiv.org/abs/2508.16829)
*Junhyun Lee,Veronika Thost,Bumsoo Kim,Jaewoo Kang,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文提出了MPNN中的过度稀释问题，包括节点内属性级稀释和节点间表示级稀释，并提出了基于Transformer的解决方案来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: MPNN在图机器学习中占据重要地位，但存在过度平滑和过度挤压等问题。作者发现即使在单层中，节点特定信息也会显著稀释，这一现象之前被忽视。

Method: 提出了过度稀释概念，用两个稀释因子进行形式化：节点内稀释（属性级）和节点间稀释（节点级表示）。引入了基于Transformer的解决方案来缓解过度稀释问题。

Result: 提出的方法能够有效缓解MPNN中的过度稀释问题，补充现有节点嵌入方法，为构建更具信息性的图表示提供了新见解。

Conclusion: 过度稀释是MPNN的一个重要限制因素，基于Transformer的解决方案能够有效缓解这一问题，为图表示学习的发展做出了贡献。

Abstract: Message Passing Neural Networks (MPNNs) hold a key position in machine
learning on graphs, but they struggle with unintended behaviors, such as
over-smoothing and over-squashing, due to irregular data structures. The
observation and formulation of these limitations have become foundational in
constructing more informative graph representations. In this paper, we delve
into the limitations of MPNNs, focusing on aspects that have previously been
overlooked. Our observations reveal that even within a single layer, the
information specific to an individual node can become significantly diluted. To
delve into this phenomenon in depth, we present the concept of Over-dilution
and formulate it with two dilution factors: intra-node dilution for
attribute-level and inter-node dilution for node-level representations. We also
introduce a transformer-based solution that alleviates over-dilution and
complements existing node embedding methods like MPNNs. Our findings provide
new insights and contribute to the development of informative representations.
The implementation and supplementary materials are publicly available at
https://github.com/LeeJunHyun/NATR.

</details>


### [69] [Out of Distribution Detection for Efficient Continual Learning in Quality Prediction for Arc Welding](https://arxiv.org/abs/2508.16832)
*Yannik Hahn,Jan Voets,Antonin Koenigsfeld,Hasan Tercan,Tobias Meisen*

Main category: cs.LG

TL;DR: 基于VQ-VAE Transformer架构，利用自回归损失进行OOD检测，结合持续学习策略，在动态焊接环境中实现鲁棒的质量预测


<details>
  <summary>Details</summary>
Motivation: 解决当前机器学习模型在动态制造环境中面对分布偏移时的局限性，特别是在焊接质量预测领域

Method: 扩展VQ-VAE Transformer架构，利用其自回归损失作为OOD检测机制，结合持续学习策略，仅在必要时触发模型更新

Result: 相比传统重建方法、嵌入误差技术和其他基准方法表现出更优性能，在真实焊接场景中有效维持跨分布偏移的鲁棒质量预测能力

Conclusion: 为动态制造过程中的质量保证提供了可解释且自适应的解决方案，是工业环境中鲁棒实用AI系统的重要进展

Abstract: Modern manufacturing relies heavily on fusion welding processes, including
gas metal arc welding (GMAW). Despite significant advances in machine
learning-based quality prediction, current models exhibit critical limitations
when confronted with the inherent distribution shifts that occur in dynamic
manufacturing environments. In this work, we extend the VQ-VAE Transformer
architecture - previously demonstrating state-of-the-art performance in weld
quality prediction - by leveraging its autoregressive loss as a reliable
out-of-distribution (OOD) detection mechanism. Our approach exhibits superior
performance compared to conventional reconstruction methods, embedding
error-based techniques, and other established baselines. By integrating OOD
detection with continual learning strategies, we optimize model adaptation,
triggering updates only when necessary and thereby minimizing costly labeling
requirements. We introduce a novel quantitative metric that simultaneously
evaluates OOD detection capability while interpreting in-distribution
performance. Experimental validation in real-world welding scenarios
demonstrates that our framework effectively maintains robust quality prediction
capabilities across significant distribution shifts, addressing critical
challenges in dynamic manufacturing environments where process parameters
frequently change. This research makes a substantial contribution to applied
artificial intelligence by providing an explainable and at the same time
adaptive solution for quality assurance in dynamic manufacturing processes - a
crucial step towards robust, practical AI systems in the industrial
environment.

</details>


### [70] [Physics-Inspired Spatial Temporal Graph Neural Networks for Predicting Industrial Chain Resilience](https://arxiv.org/abs/2508.16836)
*Bicheng Wang,Junping Wang,Yibo Xue*

Main category: cs.LG

TL;DR: 提出了一种物理信息神经符号方法来预测产业链弹性，通过整合物理实体状态动态和多层时空协同演化网络，实现物理符号动态与时空拓扑的联合学习。


<details>
  <summary>Details</summary>
Motivation: 产业链在国民经济可持续发展中日益重要，但数据驱动的深度学习在描述和分析复杂网络弹性方面仍处于起步阶段，核心问题是缺乏描述系统动态的理论框架。

Method: 采用物理信息神经符号方法，学习物理实体活动状态的动态变化，并将其整合到多层时空协同演化网络中，通过物理信息方法实现物理符号动态与时空协同演化拓扑的联合学习。

Result: 实验结果表明，该模型能够获得更好的结果，更准确有效地预测产业链弹性，对行业发展具有一定的实际意义。

Conclusion: 提出的物理信息神经符号方法为复杂网络弹性预测提供了一个有效的理论框架和实践解决方案，在产业链弹性分析方面表现出良好的性能和应用价值。

Abstract: Industrial chain plays an increasingly important role in the sustainable
development of national economy. However, as a typical complex network,
data-driven deep learning is still in its infancy in describing and analyzing
the resilience of complex networks, and its core is the lack of a theoretical
framework to describe the system dynamics. In this paper, we propose a
physically informative neural symbolic approach to describe the evolutionary
dynamics of complex networks for resilient prediction. The core idea is to
learn the dynamics of the activity state of physical entities and integrate it
into the multi-layer spatiotemporal co-evolution network, and use the physical
information method to realize the joint learning of physical symbol dynamics
and spatiotemporal co-evolution topology, so as to predict the industrial chain
resilience. The experimental results show that the model can obtain better
results and predict the elasticity of the industry chain more accurately and
effectively, which has certain practical significance for the development of
the industry.

</details>


### [71] [Neural Contrast Expansion for Explainable Structure-Property Prediction and Random Microstructure Design](https://arxiv.org/abs/2508.16857)
*Guangyu Nie,Yang Jiao,Yi Ren*

Main category: cs.LG

TL;DR: 提出Neural Contrast Expansion (NCE)方法，通过结合强对比展开理论和神经网络，从结构-性能数据中学习替代PDE核，实现既高效又可解释的复合材料性能预测。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解方法计算成本高但可解释性好，数据驱动模型效率高但可解释性差。需要一种既能保持计算效率又能提供可解释敏感性分析的方法。

Method: 基于强对比展开(SCE)形式，提出NCE架构，从有限尺寸材料样本的结构-性能数据中学习替代PDE核，无需PDE解场测量，仅需宏观性能测量。

Result: 在静态传导和电磁波传播案例中，NCE模型显示出准确且具有洞察力的敏感性信息，对材料设计有用。

Conclusion: NCE方法成功实现了成本效益和可解释性的平衡，为材料开发提供了更实用的性能预测工具。

Abstract: Effective properties of composite materials are defined as the ensemble
average of property-specific PDE solutions over the underlying microstructure
distributions. Traditionally, predicting such properties can be done by solving
PDEs derived from microstructure samples or building data-driven models that
directly map microstructure samples to properties. The former has a higher
running cost, but provides explainable sensitivity information that may guide
material design; the latter could be more cost-effective if the data overhead
is amortized, but its learned sensitivities are often less explainable. With a
focus on properties governed by linear self-adjoint PDEs (e.g., Laplace,
Helmholtz, and Maxwell curl-curl) defined on bi-phase microstructures, we
propose a structure-property model that is both cost-effective and explainable.
Our method is built on top of the strong contrast expansion (SCE) formalism,
which analytically maps $N$-point correlations of an unbounded random field to
its effective properties. Since real-world material samples have finite sizes
and analytical PDE kernels are not always available, we propose Neural Contrast
Expansion (NCE), an SCE-inspired architecture to learn surrogate PDE kernels
from structure-property data. For static conduction and electromagnetic wave
propagation cases, we show that NCE models reveal accurate and insightful
sensitivity information useful for material design. Compared with other PDE
kernel learning methods, our method does not require measurements about the PDE
solution fields, but rather only requires macroscopic property measurements
that are more accessible in material development contexts.

</details>


### [72] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种无监督的图匹配框架，通过伪坐标、自适应相似度平衡和分块后处理来解决地图匹配中的挑战，在真实数据集上达到了最先进的准确率。


<details>
  <summary>Details</summary>
Motivation: 地图到地图匹配面临缺乏真实对应关系、节点特征稀疏和可扩展性需求等挑战，传统方法难以处理大规模地图数据且需要标注训练样本。

Method: 采用无监督学习框架，引入伪坐标捕捉节点相对空间布局，设计自适应特征与几何相似度平衡机制和几何一致性损失函数，并开发基于分块的后处理流水线实现并行处理。

Result: 在真实数据集上的实验表明，该方法在匹配任务中达到了最先进的准确率，特别是在高噪声和大规模场景下显著优于现有方法。

Conclusion: 该框架为地图对齐提供了可扩展且实用的解决方案，是传统方法的稳健高效替代方案。

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>


### [73] [Quantifying Out-of-Training Uncertainty of Neural-Network based Turbulence Closures](https://arxiv.org/abs/2508.16891)
*Cody Grogan,Som Dhulipala,Mauricio Tano,Izabela Gutowska,Som Dutta*

Main category: cs.LG

TL;DR: 本文比较了三种神经网络方法和高斯过程在湍流闭合模型中的不确定性量化性能，发现在训练区域内GP精度最高，但在训练区域外深度集成方法表现更稳健且计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 神经网络湍流闭合模型缺乏不确定性量化，特别是在训练数据之外的输入区域，这限制了其在CFD模拟中的广泛应用。

Method: 使用已发布的代数湍流闭合模型，比较深度集成(DE)、蒙特卡洛dropout(MCD)、随机变分推断(SVI)和高斯过程(GP)四种方法的不确定性量化性能。

Result: 训练区域内GP精度最高(RMSE=2.14e-5)，DE次之(4.59e-4)；训练区域外GP和DE性能相近，但DE在负对数似然方面表现最佳；计算成本GP显著高于神经网络方法。

Conclusion: 深度集成方法在不确定性量化方面表现稳健且计算效率高，为神经网络湍流闭合模型的实际应用提供了可行的不确定性量化解决方案。

Abstract: Neural-Network (NN) based turbulence closures have been developed for being
used as pre-trained surrogates for traditional turbulence closures, with the
aim to increase computational efficiency and prediction accuracy of CFD
simulations. The bottleneck to the widespread adaptation of these ML-based
closures is the relative lack of uncertainty quantification (UQ) for these
models. Especially, quantifying uncertainties associated with out-of-training
inputs, that is when the ML-based turbulence closures are queried on inputs
outside their training data regime. In the current paper, a published algebraic
turbulence closure1 has been utilized to compare the quality of epistemic UQ
between three NN-based methods and Gaussian Process (GP). The three NN-based
methods explored are Deep Ensembles (DE), Monte-Carlo Dropout (MCD), and
Stochastic Variational Inference (SVI). In the in-training results, we find the
exact GP performs the best in accuracy with a Root Mean Squared Error (RMSE) of
$2.14 \cdot 10^{-5}$ followed by the DE with an RMSE of $4.59 \cdot 10^{-4}$.
Next, the paper discusses the performance of the four methods for quantifying
out-of-training uncertainties. For performance, the Exact GP yet again is the
best in performance, but has similar performance to the DE in the
out-of-training regions. In UQ accuracy for the out-of-training case, SVI and
DE hold the best miscalibration error for one of the cases. However, the DE
performs the best in Negative Log-Likelihood for both out-of-training cases. We
observe that for the current problem, in terms of accuracy GP > DE > SV I >
MCD. The DE results are relatively robust and provide intuitive UQ estimates,
despite performing naive ensembling. In terms of computational cost, the GP is
significantly higher than the NN-based methods with a $O(n^3)$ computational
complexity for each training step

</details>


### [74] [Tri-Accel: Curvature-Aware Precision-Adaptive and Memory-Elastic Optimization for Efficient GPU Usage](https://arxiv.org/abs/2508.16905)
*Mohsen Sheibanian,Pouya Shaeri,Alimohammad Beigi,Ryan T. Woo,Aryan Keluskar*

Main category: cs.LG

TL;DR: Tri-Accel是一个统一的优化框架，通过协同适配三种加速策略（自适应精度更新、稀疏二阶信号、内存弹性批次缩放）来减少深度神经网络的训练时间和内存使用，同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在优化过程中面临GPU内存和计算时间的瓶颈，现有加速技术通常孤立使用，需要一种统一的协同优化方法。

Method: Tri-Accel框架包含：(1)基于曲率和梯度方差动态分配混合精度级别的精度自适应更新；(2)利用Hessian/Fisher稀疏模式指导精度和步长决策的稀疏二阶信号；(3)根据VRAM可用性实时调整批次大小的内存弹性批次缩放。

Result: 在CIFAR-10上使用ResNet-18和EfficientNet-B0，Tri-Accel实现了训练时间减少9.9%，内存使用降低13.3%，准确率比FP32基线提高1.1个百分点。与静态混合精度训练相比，在保持78.1%准确率的同时将内存占用从0.35GB降至0.31GB。

Conclusion: 该框架展示了算法自适应性和硬件感知相结合可以改善资源受限环境中的可扩展性，为边缘设备和成本敏感云部署提供更高效的神经网络训练方案。

Abstract: Deep neural networks are increasingly bottlenecked by the cost of
optimization, both in terms of GPU memory and compute time. Existing
acceleration techniques, such as mixed precision, second-order methods, and
batch size scaling, are typically used in isolation. We present Tri-Accel, a
unified optimization framework that co-adapts three acceleration strategies
along with adaptive parameters during training: (1) Precision-Adaptive Updates
that dynamically assign mixed-precision levels to layers based on curvature and
gradient variance; (2) Sparse Second-Order Signals that exploit Hessian/Fisher
sparsity patterns to guide precision and step size decisions; and (3)
Memory-Elastic Batch Scaling that adjusts batch size in real time according to
VRAM availability. On CIFAR-10 with ResNet-18 and EfficientNet-B0, Tri-Accel
achieves up to 9.9% reduction in training time and 13.3% lower memory usage,
while improving accuracy by +1.1 percentage points over FP32 baselines. Tested
on CIFAR-10/100, our approach demonstrates adaptive learning behavior, with
efficiency gradually improving over the course of training as the system learns
to allocate resources more effectively. Compared to static mixed-precision
training, Tri-Accel maintains 78.1% accuracy while reducing memory footprint
from 0.35GB to 0.31GB on standard hardware. The framework is implemented with
custom Triton kernels, whose hardware-aware adaptation enables automatic
optimization without manual hyperparameter tuning, making it practical for
deployment across diverse computational environments. This work demonstrates
how algorithmic adaptivity and hardware awareness can be combined to improve
scalability in resource-constrained settings, paving the way for more efficient
neural network training on edge devices and cost-sensitive cloud deployments.

</details>


### [75] [Reinforcement-Guided Hyper-Heuristic Hyperparameter Optimization for Fair and Explainable Spiking Neural Network-Based Financial Fraud Detection](https://arxiv.org/abs/2508.16915)
*Sadman Mohammad Nasif,Md Abrar Jahin,M. F. Mridha*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲神经网络和强化学习超启发式优化的新型欺诈检测框架，在保持高准确率的同时实现了公平性和可解释性


<details>
  <summary>Details</summary>
Motivation: 家庭银行系统的普及增加了网络欺诈风险，需要既准确又公平且可解释的欺诈检测机制。现有AI模型存在计算效率低、可解释性差和收敛不稳定等问题

Method: 整合了皮层脉冲网络与群体编码(CSNPC)和强化引导超启发式优化器(RHOSS)，采用生物启发的脉冲神经网络进行鲁棒分类，使用Q-learning动态选择超参数优化启发式方法

Result: 在BAF数据集上实现了90.8%的召回率和5%的误报率，优于最先进的脉冲和非脉冲模型，同时在关键人口属性上保持超过98%的预测公平性

Conclusion: 证明了群体编码脉冲神经网络与强化引导超启发式优化相结合在现实金融应用中实现公平、透明和高性能欺诈检测的潜力

Abstract: The growing adoption of home banking systems has heightened the risk of
cyberfraud, necessitating fraud detection mechanisms that are not only accurate
but also fair and explainable. While AI models have shown promise in this
domain, they face key limitations, including computational inefficiency, the
interpretability challenges of spiking neural networks (SNNs), and the
complexity and convergence instability of hyper-heuristic reinforcement
learning (RL)-based hyperparameter optimization. To address these issues, we
propose a novel framework that integrates a Cortical Spiking Network with
Population Coding (CSNPC) and a Reinforcement-Guided Hyper-Heuristic Optimizer
for Spiking Systems (RHOSS). The CSNPC, a biologically inspired SNN, employs
population coding for robust classification, while RHOSS uses Q-learning to
dynamically select low-level heuristics for hyperparameter optimization under
fairness and recall constraints. Embedded within the Modular Supervisory
Framework for Spiking Network Training and Interpretation (MoSSTI), the system
incorporates explainable AI (XAI) techniques, specifically, saliency-based
attribution and spike activity profiling, to increase transparency. Evaluated
on the Bank Account Fraud (BAF) dataset suite, our model achieves a $90.8\%$
recall at a strict $5\%$ false positive rate (FPR), outperforming
state-of-the-art spiking and non-spiking models while maintaining over $98\%$
predictive equality across key demographic attributes. The explainability
module further confirms that saliency attributions align with spiking dynamics,
validating interpretability. These results demonstrate the potential of
combining population-coded SNNs with reinforcement-guided hyper-heuristics for
fair, transparent, and high-performance fraud detection in real-world financial
applications.

</details>


### [76] [Attention Layers Add Into Low-Dimensional Residual Subspaces](https://arxiv.org/abs/2508.16929)
*Junxuan Wang,Xuyang Ge,Wentao Shu,Zhengfu He,Xipeng Qiu*

Main category: cs.LG

TL;DR: 研究发现transformer注意力输出存在低维子空间现象，60%方向占99%方差，这是稀疏字典学习中死特征问题的根本原因。提出子空间约束训练方法，将死特征从87%降至1%以下。


<details>
  <summary>Details</summary>
Motivation: 解决transformer模型中注意力输出高维隐藏空间的认知偏差，以及由此导致的稀疏字典学习中的死特征问题。

Method: 提出子空间约束训练方法，将特征方向初始化到激活的活跃子空间中，适用于注意力输出SAE和其他稀疏字典学习方法。

Result: 在100万特征的注意力输出SAE中，死特征从87%减少到低于1%，方法可扩展到其他稀疏字典学习方法。

Conclusion: 研究揭示了注意力几何结构的新见解，并为改进大语言模型中稀疏字典学习提供了实用工具。

Abstract: While transformer models are widely believed to operate in high-dimensional
hidden spaces, we show that attention outputs are confined to a surprisingly
low-dimensional subspace, where about 60\% of the directions account for 99\%
of the variance--a phenomenon that is induced by the attention output
projection matrix and consistently observed across diverse model families and
datasets. Critically, we find this low-rank structure as a fundamental cause of
the prevalent dead feature problem in sparse dictionary learning, where it
creates a mismatch between randomly initialized features and the intrinsic
geometry of the activation space. Building on this insight, we propose a
subspace-constrained training method for sparse autoencoders (SAEs),
initializing feature directions into the active subspace of activations. Our
approach reduces dead features from 87\% to below 1\% in Attention Output SAEs
with 1M features, and can further extend to other sparse dictionary learning
methods. Our findings provide both new insights into the geometry of attention
and practical tools for improving sparse dictionary learning in large language
models.

</details>


### [77] [Degree of Staleness-Aware Data Updating in Federated Learning](https://arxiv.org/abs/2508.16931)
*Tao Liu,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出了DUFL激励机制，通过三要素协调数据陈旧性和数据量，使用DoS指标量化数据陈旧性，基于Stackelberg博弈优化联邦学习性能


<details>
  <summary>Details</summary>
Motivation: 处理联邦学习中数据陈旧性问题，现有工作未同时考虑数据陈旧性和数据量，需要协调两者以获得最佳效用

Method: 提出DUFL激励机制，使用DoS指标量化数据陈旧性，建立两阶段Stackelberg博弈模型，推导客户端最优数据更新策略和服务器近似最优策略

Result: 在真实数据集上的实验结果表明该方法具有显著性能优势

Conclusion: DUFL机制能有效协调数据陈旧性和数据量，提升联邦学习在时间敏感任务中的性能表现

Abstract: Handling data staleness remains a significant challenge in federated learning
with highly time-sensitive tasks, where data is generated continuously and data
staleness largely affects model performance. Although recent works attempt to
optimize data staleness by determining local data update frequency or client
selection strategy, none of them explore taking both data staleness and data
volume into consideration. In this paper, we propose DUFL(Data Updating in
Federated Learning), an incentive mechanism featuring an innovative local data
update scheme manipulated by three knobs: the server's payment, outdated data
conservation rate, and clients' fresh data collection volume, to coordinate
staleness and volume of local data for best utilities. To this end, we
introduce a novel metric called DoS(the Degree of Staleness) to quantify data
staleness and conduct a theoretic analysis illustrating the quantitative
relationship between DoS and model performance. We model DUFL as a two-stage
Stackelberg game with dynamic constraint, deriving the optimal local data
update strategy for each client in closed-form and the approximately optimal
strategy for the server. Experimental results on real-world datasets
demonstrate the significant performance of our approach.

</details>


### [78] [Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning](https://arxiv.org/abs/2508.16949)
*Yang Zhou,Sunzhu Li,Shunyu Liu,Wenkai Fang,Jiale Zhao,Jingwen Yang,Jianwei Lv,Kongcheng Zhang,Yihe Zhou,Hengtong Lu,Wei Chen,Yan Xie,Mingli Song*

Main category: cs.LG

TL;DR: RuscaRL是一个新颖的强化学习框架，通过引入检查表式评分标准来打破LLM推理的探索瓶颈，在多个基准测试中显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习面临探索困境：需要高质量样本但探索能力受限于模型本身，形成无法探索就无法学习的恶性循环。

Method: 提出RuscaRL框架：1) 在生成阶段使用评分标准作为显式脚手架指导多样化高质量响应；2) 在训练阶段使用评分标准作为可验证奖励进行有效RL训练。

Result: 在HealthBench-500上，Qwen-2.5-7B-Instruct从23.6提升到50.3，超越GPT-4.1；Qwen3-30B-A3B-Instruct达到61.1，超越OpenAI-o3等领先模型。

Conclusion: RuscaRL成功解决了LLM推理中的探索瓶颈问题，通过评分标准脚手架显著扩展了推理边界，在多个基准测试中表现出优越性能。

Abstract: Recent advances in Large Language Models (LLMs) have underscored the
potential of Reinforcement Learning (RL) to facilitate the emergence of
reasoning capabilities. Despite the encouraging results, a fundamental dilemma
persists as RL improvement relies on learning from high-quality samples, yet
the exploration for such samples remains bounded by the inherent limitations of
LLMs. This, in effect, creates an undesirable cycle in which what cannot be
explored cannot be learned. In this work, we propose Rubric-Scaffolded
Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework
designed to break the exploration bottleneck for general LLM reasoning.
Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit
scaffolding for exploration during rollout generation, where different rubrics
are provided as external guidance within task instructions to steer diverse
high-quality responses. This guidance is gradually decayed over time,
encouraging the model to internalize the underlying reasoning patterns; (2)
verifiable rewards for exploitation during model training, where we can obtain
robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL
on general reasoning tasks. Extensive experiments demonstrate the superiority
of the proposed RuscaRL across various benchmarks, effectively expanding
reasoning boundaries under the best-of-N evaluation. Notably, RuscaRL
significantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,
surpassing GPT-4.1. Furthermore, our fine-tuned variant on
Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading
LLMs including OpenAI-o3.

</details>


### [79] [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
*Manan Gupta,Dhruv Kumar*

Main category: cs.LG

TL;DR: 一种新的多义性指数(PSI)用于量化神经网络中的多义神经元，通过几何聚类质量、标签对齐和语义差异性综度进行综合评估，发现深层网络中多义性更为显著。


<details>
  <summary>Details</summary>
Motivation: 神经网络中存在多义神经元，这些神经元对多个有时无关的特征产生响应，结构化解释性复杂，需要一种量化方法来识别和研究这些单元。

Method: 提出PSI指标，结合三个经过检验的组件：几何聚类质量(S)、与标签类别的对齐度(Q)、通过CLIP实现的开放词汇语义差异性(D)。在ResNet-50上使用Tiny-ImageNet图片进行评估，并通过稳健性检验、广度分析和因果干预实验进行验证。

Result: 在预训练的ResNet-50中，PSI成功识别出激活集合可分解为一致可命名原型的神经元，并发现深度趋势：后期层比前期层显示出明显更高的PSI值。对齐补丁替换干预导致目标神经元激活强度显著增加。

Conclusion: PSI提供了一个有原则且实用的工具，用于发现、量化和研究神经网络中的多义单元，为机制解释性研究提供了新的视角。

Abstract: Neural networks often contain polysemantic neurons that respond to multiple,
sometimes unrelated, features, complicating mechanistic interpretability. We
introduce the Polysemanticity Index (PSI), a null-calibrated metric that
quantifies when a neuron's top activations decompose into semantically distinct
clusters. PSI multiplies three independently calibrated components: geometric
cluster quality (S), alignment to labeled categories (Q), and open-vocabulary
semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with
Tiny-ImageNet images, PSI identifies neurons whose activation sets split into
coherent, nameable prototypes, and reveals strong depth trends: later layers
exhibit substantially higher PSI than earlier layers. We validate our approach
with robustness checks (varying hyperparameters, random seeds, and
cross-encoder text heads), breadth analyses (comparing class-only vs.
open-vocabulary concepts), and causal patch-swap interventions. In particular,
aligned patch replacements increase target-neuron activation significantly more
than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI
thus offers a principled and practical lever for discovering, quantifying, and
studying polysemantic units in neural networks.

</details>


### [80] [Unveiling the Latent Directions of Reflection in Large Language Models](https://arxiv.org/abs/2508.16989)
*Fu-Chieh Chang,Yu-Ting Lee,Pei-Yuan Wu*

Main category: cs.LG

TL;DR: 通过激活引导技术探索大语言模型反思机制，发现可系统性控制反思行为，压制反思比激活更容易


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于计算机制设计，对LLM反思内部机制理解不深入，需要从激活层面探索反思的工作原理

Method: 采用激活引导技术，构建不同反思水平（无反思、内在反思、触发反思）之间的引导向量，通过实验验证可控性

Result: 在GSM8k-adv数据集上验证了反思水平的明确层次化，可系统性识别新的反思指令，通过激活干预直接增强或压制反思行为，压制反思比激活更容易

Conclusion: 为LLM反思机制提供了机制理解方向，既有利用价值（如反思增强防御）也存在风险（如监狱攻击中的反思抑制）

Abstract: Reflection, the ability of large language models (LLMs) to evaluate and
revise their own reasoning, has been widely used to improve performance on
complex reasoning tasks. Yet, most prior work emphasizes designing reflective
prompting strategies or reinforcement learning objectives, leaving the inner
mechanisms of reflection underexplored. In this paper, we investigate
reflection through the lens of latent directions in model activations. We
propose a methodology based on activation steering to characterize how
instructions with different reflective intentions: no reflection, intrinsic
reflection, and triggered reflection. By constructing steering vectors between
these reflection levels, we demonstrate that (1) new reflection-inducing
instructions can be systematically identified, (2) reflective behavior can be
directly enhanced or suppressed through activation interventions, and (3)
suppressing reflection is considerably easier than stimulating it. Experiments
on GSM8k-adv with Qwen2.5-3B and Gemma3-4B reveal clear stratification across
reflection levels, and steering interventions confirm the controllability of
reflection. Our findings highlight both opportunities (e.g.,
reflection-enhancing defenses) and risks (e.g., adversarial inhibition of
reflection in jailbreak attacks). This work opens a path toward mechanistic
understanding of reflective reasoning in LLMs.

</details>


### [81] [Online Learning for Approximately-Convex Functions with Long-term Adversarial Constraints](https://arxiv.org/abs/2508.16992)
*Dhruv Sarkar,Samrat Mukhopadhyay,Abhishek Sinha*

Main category: cs.LG

TL;DR: 本文研究具有长期预算约束的对抗性在线学习问题，提出了一种高效的一阶在线算法，在完整信息和bandit反馈设置下都能实现O(√T)的α-regret，同时资源消耗最多为O(B_T log T) + ˜O(√T)。


<details>
  <summary>Details</summary>
Motivation: 传统在线学习问题通常假设凸性，但许多实际应用涉及非凸优化问题。本文旨在扩展在线学习框架，使其能够处理更广泛的α-近似凸函数类，包括DR-次模最大化、在线顶点覆盖和正则化相位检索等常见非凸问题。

Method: 提出了一种高效的一阶在线算法，该算法在对抗性设置下工作，能够处理α-近似凸的成本和资源消耗函数。算法在完整信息和bandit反馈两种设置下都能有效运行。

Result: 算法保证了对最优固定可行基准的O(√T) α-regret，同时资源消耗最多为O(B_T log T) + ˜O(√T)。在bandit反馈设置下，为"带背包的对抗性bandits"问题提供了改进的保证。同时证明了匹配的下界，表明结果是紧的。

Conclusion: 本文成功地将在线学习框架扩展到α-近似凸函数类，为处理各种非凸优化问题提供了有效的解决方案，并在理论和算法层面都取得了显著的进展。

Abstract: We study an online learning problem with long-term budget constraints in the
adversarial setting. In this problem, at each round $t$, the learner selects an
action from a convex decision set, after which the adversary reveals a cost
function $f_t$ and a resource consumption function $g_t$. The cost and
consumption functions are assumed to be $\alpha$-approximately convex - a broad
class that generalizes convexity and encompasses many common non-convex
optimization problems, including DR-submodular maximization, Online Vertex
Cover, and Regularized Phase Retrieval. The goal is to design an online
algorithm that minimizes cumulative cost over a horizon of length $T$ while
approximately satisfying a long-term budget constraint of $B_T$. We propose an
efficient first-order online algorithm that guarantees $O(\sqrt{T})$
$\alpha$-regret against the optimal fixed feasible benchmark while consuming at
most $O(B_T \log T)+ \tilde{O}(\sqrt{T})$ resources in both full-information
and bandit feedback settings. In the bandit feedback setting, our approach
yields an efficient solution for the $\texttt{Adversarial Bandits with
Knapsacks}$ problem with improved guarantees. We also prove matching lower
bounds, demonstrating the tightness of our results. Finally, we characterize
the class of $\alpha$-approximately convex functions and show that our results
apply to a broad family of problems.

</details>


### [82] [Learned Structure in CARTRIDGES: Keys as Shareable Routers in Self-Studied Representations](https://arxiv.org/abs/2508.17032)
*Maurizio Diaz*

Main category: cs.LG

TL;DR: CARTRIDGE方法通过离线训练大幅压缩KV缓存（最高减少40倍内存使用），本文首次机制性探索其结构，发现keys作为稳定检索路由器，values承担主要压缩功能，并提出SCI初始化方法加速收敛。


<details>
  <summary>Details</summary>
Motivation: 长上下文LLM推理的瓶颈在于KV缓存线性增长，需要研究CARTRIDGE压缩方法的内部工作机制以优化性能。

Method: 通过实证分析CARTRIDGE keys和values的功能机制，提出Sampled Chunk Initialization (SCI)初始化方法改进训练收敛速度。

Result: 证实CARTRIDGE keys作为稳定检索路由器，values承担主要压缩功能；SCI初始化能加速CARTRIDGE收敛。

Conclusion: 本研究为CARTRIDGE训练优化的更广泛实证研究奠定了基础，对进一步扩展长上下文处理能力至关重要。

Abstract: A bottleneck for long-context LLM inference is the linearly growing KV cache.
Recent work has proposed CARTRIDGES, an approach which leverages offline
compute to train a much smaller KV cache than is typically required for a full
document (up to 40x less memory usage at inference time). In this paper, we
present the first mechanistic exploration of the learned CARTRIDGE key-value
cache structure. In particular, we propose that (1) CARTRIDGE keys act as
stable, shareable retrieval routers for the compressed corpora and (2) most of
the learned compression occurs within the CARTRIDGE value vectors. We present
empirical evidence of our routing theory across tasks, model families, and
model sizes; for example, we can ablate the learned CARTRIDGE key vectors
between tasks with little performance loss. Finally, we propose a slight
improvement in initialization called Sampled Chunk Initialization (SCI). We
suggest that SCI can lead to faster CARTRIDGE convergence than previously
demonstrated in the literature. Our findings lay the groundwork for broader
empirical study of CARTRIDGE training optimization which may be crucial for
further scaling.

</details>


### [83] [TabResFlow: A Normalizing Spline Flow Model for Probabilistic Univariate Tabular Regression](https://arxiv.org/abs/2508.17056)
*Kiran Madhusudhanan,Vijaya Krishna Yalavarthi,Jonas Sonntag,Maximilian Stubbemann,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: TabResFlow是一种用于表格回归的归一化样条流模型，通过MLP编码器、ResNet主干网络和条件样条流来灵活建模复杂的目标分布，在似然分数和推理速度上显著优于现有概率回归模型。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法主要关注点估计，容易产生过度自信的预测，而现有概率回归方法通常假设固定形状分布（如高斯分布），无法处理现实世界中复杂的真实分布。

Method: 提出TabResFlow模型，包含三个核心组件：(1)每个数值特征的MLP编码器；(2)全连接ResNet主干网络进行特征提取；(3)基于条件样条的归一化流进行灵活可处理的密度估计。

Result: 在9个公共基准数据集上，TabResFlow在似然分数上持续超越现有概率回归模型，相比最强的TreeFlow模型提升9.64%，推理速度比最强的NodeFlow快5.6倍。在二手车价格预测任务中，使用新的AURC指标也表现出优越性能。

Conclusion: TabResFlow为表格回归提供了一种灵活且高效的概率建模方法，能够更好地处理复杂的目标分布，在工业自动化等需要可信决策的场景中具有重要应用价值。

Abstract: Tabular regression is a well-studied problem with numerous industrial
applications, yet most existing approaches focus on point estimation, often
leading to overconfident predictions. This issue is particularly critical in
industrial automation, where trustworthy decision-making is essential.
Probabilistic regression models address this challenge by modeling prediction
uncertainty. However, many conventional methods assume a fixed-shape
distribution (typically Gaussian), and resort to estimating distribution
parameters. This assumption is often restrictive, as real-world target
distributions can be highly complex. To overcome this limitation, we introduce
TabResFlow, a Normalizing Spline Flow model designed specifically for
univariate tabular regression, where commonly used simple flow networks like
RealNVP and Masked Autoregressive Flow (MAF) are unsuitable. TabResFlow
consists of three key components: (1) An MLP encoder for each numerical
feature. (2) A fully connected ResNet backbone for expressive feature
extraction. (3) A conditional spline-based normalizing flow for flexible and
tractable density estimation. We evaluate TabResFlow on nine public benchmark
datasets, demonstrating that it consistently surpasses existing probabilistic
regression models on likelihood scores. Our results demonstrate 9.64%
improvement compared to the strongest probabilistic regression model
(TreeFlow), and on average 5.6 times speed-up in inference time compared to the
strongest deep learning alternative (NodeFlow). Additionally, we validate the
practical applicability of TabResFlow in a real-world used car price prediction
task under selective regression. To measure performance in this setting, we
introduce a novel Area Under Risk Coverage (AURC) metric and show that
TabResFlow achieves superior results across this metric.

</details>


### [84] [Learning ON Large Datasets Using Bit-String Trees](https://arxiv.org/abs/2508.17083)
*Prashant Gupta*

Main category: cs.LG

TL;DR: 这篇论文发展了相似性保持哈希、分类和癌症生物信息学方法，包括ComBI哈希算法、GRAF分类器和CRCS深度学习框架，在大规模数据处理和医学应用中实现了高效和可扩展的性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准空间分割哈希方法中二叉搜索树的指数增长和稀疏性问题，提高大规模数据的处理效率，并为癌症生物信息学提供可解释的分析工具。

Method: 提出了三种核心方法：1) ComBI（压缩倒排哈希表二叉树）用于快速近似最近邻搜索；2) GRAF（导向随机森林）统合全局和局部分割的树基集成分类器；3) CRCS（编码子开关连续表示）深度学习框架用于基因变异数值化表征。

Result: 在10亿样本数据集上，ComBI达到0.90精度，速度提升4-296倍；GRAF在115个数据集上展现竞争力或更优的准确性；CRCS在0胀腐、肝癌和脑癌中验证了生存预测能力，能够识别驱动基因和评估肝癌变异。

Conclusion: 这些方法为大规模数据分析和生物医学应用提供了高效、可扩展和可解释的工具集合，在哈希搜索、分类预测和癌症生存分析方面取得了显著成效。

Abstract: This thesis develops computational methods in similarity-preserving hashing,
classification, and cancer genomics. Standard space partitioning-based hashing
relies on Binary Search Trees (BSTs), but their exponential growth and sparsity
hinder efficiency. To overcome this, we introduce Compressed BST of Inverted
hash tables (ComBI), which enables fast approximate nearest-neighbor search
with reduced memory. On datasets of up to one billion samples, ComBI achieves
0.90 precision with 4X-296X speed-ups over Multi-Index Hashing, and also
outperforms Cellfishing.jl on single-cell RNA-seq searches with 2X-13X gains.
Building on hashing structures, we propose Guided Random Forest (GRAF), a
tree-based ensemble classifier that integrates global and local partitioning,
bridging decision trees and boosting while reducing generalization error.
Across 115 datasets, GRAF delivers competitive or superior accuracy, and its
unsupervised variant (uGRAF) supports guided hashing and importance sampling.
We show that GRAF and ComBI can be used to estimate per-sample classifiability,
which enables scalable prediction of cancer patient survival. To address
challenges in interpreting mutations, we introduce Continuous Representation of
Codon Switches (CRCS), a deep learning framework that embeds genetic changes
into numerical vectors. CRCS allows identification of somatic mutations without
matched normals, discovery of driver genes, and scoring of tumor mutations,
with survival prediction validated in bladder, liver, and brain cancers.
Together, these methods provide efficient, scalable, and interpretable tools
for large-scale data analysis and biomedical applications.

</details>


### [85] [Two Birds with One Stone: Enhancing Uncertainty Quantification and Interpretability with Graph Functional Neural Process](https://arxiv.org/abs/2508.17097)
*Lingkai Kong,Haotian Sun,Yuchen Zhuang,Haorui Wang,Wenhao Mu,Chao Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合图函数神经过程和图生成模型的不确定性感知可解释图分类方法，通过潜在原理学习和随机相关矩阵来提升GNN的校准性和可解释性


<details>
  <summary>Details</summary>
Motivation: 图神经网络在关键应用中存在预测校准不足和缺乏可解释性的问题，限制了其实际应用

Method: 假设一组潜在原理并映射到概率嵌入空间，通过学习随机相关矩阵使分类器预测分布以原理嵌入为条件，图生成器从嵌入空间解码原理图结构以实现可解释性，采用交替优化训练

Result: 在五个图分类数据集上超越现有方法，在不确定性量化和GNN可解释性方面表现优异

Conclusion: 该方法具有通用性，可应用于任何现有GNN架构，解码的原理结构能提供有意义的解释

Abstract: Graph neural networks (GNNs) are powerful tools on graph data. However, their
predictions are mis-calibrated and lack interpretability, limiting their
adoption in critical applications. To address this issue, we propose a new
uncertainty-aware and interpretable graph classification model that combines
graph functional neural process and graph generative model. The core of our
method is to assume a set of latent rationales which can be mapped to a
probabilistic embedding space; the predictive distribution of the classifier is
conditioned on such rationale embeddings by learning a stochastic correlation
matrix. The graph generator serves to decode the graph structure of the
rationales from the embedding space for model interpretability. For efficient
model training, we adopt an alternating optimization procedure which mimics the
well known Expectation-Maximization (EM) algorithm. The proposed method is
general and can be applied to any existing GNN architecture. Extensive
experiments on five graph classification datasets demonstrate that our
framework outperforms state-of-the-art methods in both uncertainty
quantification and GNN interpretability. We also conduct case studies to show
that the decoded rationale structure can provide meaningful explanations.

</details>


### [86] [Reconciling Communication Compression and Byzantine-Robustness in Distributed Learning](https://arxiv.org/abs/2508.17129)
*Diksha Gupta,Nirupam Gupta,Chuan Xu,Giovanni Neglia*

Main category: cs.LG

TL;DR: 提出了RoSDHB算法，将Polyak动量与协调压缩机制结合，在分布式学习中同时处理拜占庭故障和高通信成本问题，比现有方法依赖更少的假设条件。


<details>
  <summary>Details</summary>
Motivation: 分布式学习面临拜占庭故障和高通信成本的双重挑战，现有方法在结合通信压缩和拜占庭鲁棒聚合时存在性能下降问题，需要新的解决方案。

Method: 集成经典Polyak动量与新型协调压缩机制，仅假设诚实工作者平均损失函数的Lipschitz平滑性，相比现有方法减少了假设条件。

Result: 在标准(G,B)梯度异质性模型下性能与Byz-DASHA-PAGE相当，在基准图像分类任务中实现了强鲁棒性和显著的通信节省。

Conclusion: RoSDHB算法在保持拜占庭鲁棒性的同时有效降低了通信成本，且依赖更宽松的假设条件，为分布式学习提供了更实用的解决方案。

Abstract: Distributed learning (DL) enables scalable model training over decentralized
data, but remains challenged by Byzantine faults and high communication costs.
While both issues have been studied extensively in isolation, their interaction
is less explored. Prior work shows that naively combining communication
compression with Byzantine-robust aggregation degrades resilience to faulty
nodes (or workers). The state-of-the-art algorithm, namely Byz-DASHA-PAGE [29],
makes use of the momentum variance reduction scheme to mitigate the detrimental
impact of compression noise on Byzantine-robustness. We propose a new
algorithm, named RoSDHB, that integrates the classic Polyak's momentum with a
new coordinated compression mechanism. We show that RoSDHB performs comparably
to Byz-DASHA-PAGE under the standard (G, B)-gradient dissimilarity
heterogeneity model, while it relies on fewer assumptions. In particular, we
only assume Lipschitz smoothness of the average loss function of the honest
workers, in contrast to [29]that additionally assumes a special smoothness of
bounded global Hessian variance. Empirical results on benchmark image
classification task show that RoSDHB achieves strong robustness with
significant communication savings.

</details>


### [87] [MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices](https://arxiv.org/abs/2508.17137)
*Nishant Gavhane,Arush Mehrotra,Rohit Chawla,Peter Proenca*

Main category: cs.LG

TL;DR: MoE-Beyond是一个基于学习的专家激活预测器，通过训练轻量级Transformer模型来预测MoE模型在自回归解码时的专家激活，显著提高了GPU缓存命中率


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型在边缘设备部署面临内存限制挑战，传统启发式专家缓存策略在模型参数扩展时难以维持高缓存命中率

Method: 将任务构建为多标签序列预测问题，使用从DeepSeek-V2-Chat-Lite MoE提取的6600万专家激活轨迹训练轻量级Transformer模型

Result: 在WebGLM-QA数据集上达到97.5%准确率和86.6% F1分数，GPU缓存命中率从17%提升至72%（仅10%专家可放入GPU缓存时）

Conclusion: MoE-Beyond通过学习专家激活模式，有效解决了MoE模型在资源受限环境中的内存管理问题，显著优于传统启发式方法

Abstract: The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices
presents significant challenges due to memory constraints. While MoE
architectures enable efficient utilization of computational resources by
activating only a subset of experts per inference, they require careful memory
management to operate efficiently in resource-constrained environments.
Traditional heuristic-based expert caching strategies such as MoE-Infinity
struggle to maintain high cache hit rates as models parameters scale. In this
work, we introduce MoE-Beyond, a learning-based expert activation predictor
trained to predict expert activations during autoregressive decoding. By
framing the task as a multi-label sequence prediction problem, we train a
lightweight transformer model on 66 million expert activation traces extracted
from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor
generalizes effectively across unseen prompts from WebGLM-QA dataset [6],
achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that
MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts
fit in GPU cache, outperforming heuristic baselines.

</details>


### [88] [Stochastic Gradient Descent with Strategic Querying](https://arxiv.org/abs/2508.17144)
*Nanfei Jiang,Hoi-To Wai,Mahnoosh Alizadeh*

Main category: cs.LG

TL;DR: 本文研究有限和优化问题，比较策略性梯度查询与均匀查询的性能差异，提出了OGQ理想算法和SGQ实用算法，在满足PL条件的平滑目标函数下证明了策略性查询的优越性。


<details>
  <summary>Details</summary>
Motivation: 研究在有限和优化问题中，通过策略性选择梯度查询（而非均匀随机查询）是否能带来更好的性能表现，特别是在瞬态性能和稳态方差方面。

Method: 提出了Oracle Gradient Querying (OGQ)理想算法（需要所有用户梯度信息）和实用的Strategic Gradient Querying (SGQ)算法（每迭代仅查询一次），在满足Polyak-Lojasiewicz条件的平滑目标函数下进行理论分析。

Result: 理论证明在期望改进异质性假设下，OGQ能提升瞬态性能并降低稳态方差，SGQ在瞬态性能上优于SGD。数值实验验证了理论结果。

Conclusion: 策略性梯度查询相比均匀查询在有限和优化中具有显著优势，SGQ算法在保持实用性的同时实现了性能提升。

Abstract: This paper considers a finite-sum optimization problem under first-order
queries and investigates the benefits of strategic querying on stochastic
gradient-based methods compared to uniform querying strategy. We first
introduce Oracle Gradient Querying (OGQ), an idealized algorithm that selects
one user's gradient yielding the largest possible expected improvement (EI) at
each step. However, OGQ assumes oracle access to the gradients of all users to
make such a selection, which is impractical in real-world scenarios. To address
this limitation, we propose Strategic Gradient Querying (SGQ), a practical
algorithm that has better transient-state performance than SGD while making
only one query per iteration. For smooth objective functions satisfying the
Polyak-Lojasiewicz condition, we show that under the assumption of EI
heterogeneity, OGQ enhances transient-state performance and reduces
steady-state variance, while SGQ improves transient-state performance over SGD.
Our numerical experiments validate our theoretical findings.

</details>


### [89] [SACA: Selective Attention-Based Clustering Algorithm](https://arxiv.org/abs/2508.17150)
*Meysam Shirdel Bilehsavar,Razieh Ghaedi,Samira Seyed Taheri,Xinqi Fan,Christian O'Reilly*

Main category: cs.LG

TL;DR: 提出了一种基于选择性注意概念的新型密度聚类方法，无需用户定义参数即可运行，或仅需调整一个简单整数参数，有效解决了传统密度聚类算法的参数优化难题。


<details>
  <summary>Details</summary>
Motivation: 传统密度聚类算法（如DBSCAN）严重依赖用户定义的参数，这需要领域专业知识且存在优化挑战，限制了算法的易用性和可访问性。

Method: 受选择性注意概念启发，该方法首先无需用户参数即可运行；如需调整，仅引入一个易于调优的整数参数。算法通过计算阈值过滤稀疏点和离群点，形成初步聚类结构，然后重新整合被排除点以完成最终聚类。

Result: 在多样化数据集上的实验评估表明，该方法具有高度的可访问性和鲁棒性能，为密度聚类任务提供了有效替代方案。

Conclusion: 该研究提出的新型密度聚类方法显著降低了参数调优的复杂性，提高了算法的易用性，同时保持了良好的聚类性能，为实际应用提供了更便捷的解决方案。

Abstract: Clustering algorithms are widely used in various applications, with
density-based methods such as Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) being particularly prominent. These algorithms identify
clusters in high-density regions while treating sparser areas as noise.
However, reliance on user-defined parameters often poses optimization
challenges that require domain expertise. This paper presents a novel
density-based clustering method inspired by the concept of selective attention,
which minimizes the need for user-defined parameters under standard conditions.
Initially, the algorithm operates without requiring user-defined parameters. If
parameter adjustment is needed, the method simplifies the process by
introducing a single integer parameter that is straightforward to tune. The
approach computes a threshold to filter out the most sparsely distributed
points and outliers, forms a preliminary cluster structure, and then
reintegrates the excluded points to finalize the results. Experimental
evaluations on diverse data sets highlight the accessibility and robust
performance of the method, providing an effective alternative for density-based
clustering tasks.

</details>


### [90] [Towards Safeguarding LLM Fine-tuning APIs against Cipher Attacks](https://arxiv.org/abs/2508.17158)
*Jack Youstra,Mohammed Mahfoud,Yang Yan,Henry Sleight,Ethan Perez,Mrinank Sharma*

Main category: cs.LG

TL;DR: 提出了CIFR基准测试来评估防御策略在保持模型安全性的同时实现微调功能的能力，并展示了探测器监控方法的高检测准确率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调API存在安全风险，攻击者可以通过在看似无害的微调数据中编码有害内容来绕过安全机制。

Method: 引入CIFR基准测试，包含多种密码编码和家族，评估防御策略；训练基于模型内部激活的探测器监控器。

Result: 探测器监控器达到超过99%的检测准确率，能够泛化到未见过的密码变体和家族，优于现有监控方法。

Conclusion: CIFR基准测试和探测器监控方法为微调API安全防御提供了有效工具和解决方案，代码和数据已开源以促进进一步研究。

Abstract: Large language model fine-tuning APIs enable widespread model customization,
yet pose significant safety risks. Recent work shows that adversaries can
exploit access to these APIs to bypass model safety mechanisms by encoding
harmful content in seemingly harmless fine-tuning data, evading both human
monitoring and standard content filters. We formalize the fine-tuning API
defense problem, and introduce the Cipher Fine-tuning Robustness benchmark
(CIFR), a benchmark for evaluating defense strategies' ability to retain model
safety in the face of cipher-enabled attackers while achieving the desired
level of fine-tuning functionality. We include diverse cipher encodings and
families, with some kept exclusively in the test set to evaluate for
generalization across unseen ciphers and cipher families. We then evaluate
different defenses on the benchmark and train probe monitors on model internal
activations from multiple fine-tunes. We show that probe monitors achieve over
99% detection accuracy, generalize to unseen cipher variants and families, and
compare favorably to state-of-the-art monitoring approaches. We open-source
CIFR and the code to reproduce our experiments to facilitate further research
in this critical area. Code and data are available online
https://github.com/JackYoustra/safe-finetuning-api

</details>


### [91] [ONG: Orthogonal Natural Gradient Descent](https://arxiv.org/abs/2508.17169)
*Yajat Yadav,Jathin Korrapati,Patrick Mendoza*

Main category: cs.LG

TL;DR: ONG（正交自然梯度下降）将正交梯度下降与自然梯度相结合，通过EKFAC近似Fisher信息矩阵的逆来预处理梯度，并在黎曼度量下沿最陡下降方向更新，同时将自然梯度投影到先前任务梯度的正交补空间以保持先前任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的正交梯度下降方法使用欧几里得投影，忽略了神经网络参数化分布空间的信息几何结构，这可能导致学习任务中的次优收敛。

Method: 结合自然梯度思想，使用EKFAC高效近似Fisher信息矩阵的逆来预处理每个新任务的梯度，得到黎曼度量下的最陡下降方向更新，然后将这些自然梯度投影到先前任务梯度的正交补空间。

Result: 在Permuted和Rotated MNIST数据集上进行了基准测试，证明了方法的有效性。

Conclusion: ONG方法通过结合自然梯度和正交投影，在保持先前任务性能的同时，提供了更符合信息几何结构的优化方向，改善了持续学习任务的收敛性能。

Abstract: Orthogonal gradient descent has emerged as a powerful method for continual
learning tasks. However, its Euclidean projections overlook the underlying
information-geometric structure of the space of distributions parametrized by
neural networks, which can lead to suboptimal convergence in learning tasks. To
counteract this, we combine it with the idea of the natural gradient and
present ONG (Orthogonal Natural Gradient Descent). ONG preconditions each new
task gradient with an efficient EKFAC approximation of the inverse Fisher
information matrix, yielding updates that follow the steepest descent direction
under a Riemannian metric. To preserve performance on previously learned tasks,
ONG projects these natural gradients onto the orthogonal complement of prior
task gradients. We provide a theoretical justification for this procedure,
introduce the ONG algorithm, and benchmark its performance on the Permuted and
Rotated MNIST datasets. All code for our experiments/reproducibility can be
found at https://github.com/yajatyadav/orthogonal-natural-gradient.

</details>


### [92] [Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection](https://arxiv.org/abs/2508.17174)
*Jeng-Lin Li,Ming-Ching Chang,Wei-Chao Chen*

Main category: cs.LG

TL;DR: 提出SaGD框架，通过平滑对抗训练的尖锐损失景观来提升OOD检测的鲁棒性，能够区分对抗性ID样本和真实OOD样本。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法将对抗性ID样本误判为OOD样本，且在对抗攻击下的ID和OOD数据检测研究较少，需要开发能够区分两者的鲁棒方法。

Method: 提出Sharpness-aware Geometric Defense (SaGD)框架，使用Jitter-based扰动在对抗训练中平滑损失景观，改善潜在嵌入质量以准确计算OOD分数。

Result: 在CIFAR-100与六个OOD数据集的各种攻击下，SaGD显著改善了FPR和AUC指标，优于最先进的防御方法。

Conclusion: SaGD通过平滑对抗损失景观有效提升了对抗性OOD检测性能，揭示了尖锐损失景观与对抗OOD检测之间的关系。

Abstract: Out-of-distribution (OOD) detection ensures safe and reliable model
deployment. Contemporary OOD algorithms using geometry projection can detect
OOD or adversarial samples from clean in-distribution (ID) samples. However,
this setting regards adversarial ID samples as OOD, leading to incorrect OOD
predictions. Existing efforts on OOD detection with ID and OOD data under
attacks are minimal. In this paper, we develop a robust OOD detection method
that distinguishes adversarial ID samples from OOD ones. The sharp loss
landscape created by adversarial training hinders model convergence, impacting
the latent embedding quality for OOD score calculation. Therefore, we introduce
a {\bf Sharpness-aware Geometric Defense (SaGD)} framework to smooth out the
rugged adversarial loss landscape in the projected latent geometry. Enhanced
geometric embedding convergence enables accurate ID data characterization,
benefiting OOD detection against adversarial attacks. We use Jitter-based
perturbation in adversarial training to extend the defense ability against
unseen attacks. Our SaGD framework significantly improves FPR and AUC over the
state-of-the-art defense approaches in differentiating CIFAR-100 from six other
OOD datasets under various attacks. We further examine the effects of
perturbations at various adversarial training levels, revealing the
relationship between the sharp loss landscape and adversarial OOD detection.

</details>


### [93] [Scaling Graph Transformers: A Comparative Study of Sparse and Dense Attention](https://arxiv.org/abs/2508.17175)
*Leon Dimitrov*

Main category: cs.LG

TL;DR: 比较图变换器中稠密和稀疏注意力机制的优缺点，分析适用场景，并指出当前设计注意力机制面临的挑战


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络难以捕捉节点间的长程依赖关系，图变换器通过注意力机制实现全局信息交换，但存在稠密和稀疏两种注意力机制，需要系统比较和指导选择

Method: 对图变换器中的稠密和稀疏注意力机制进行对比分析，评估各自的性能表现和计算效率，总结适用条件

Result: 明确了两种注意力机制在不同场景下的优劣势，稠密注意力表达能力更强但计算成本高，稀疏注意力效率更高但可能损失部分信息

Conclusion: 需要根据具体任务需求选择合适的注意力机制，未来需要开发更高效的注意力设计来解决当前面临的挑战

Abstract: Graphs have become a central representation in machine learning for capturing
relational and structured data across various domains. Traditional graph neural
networks often struggle to capture long-range dependencies between nodes due to
their local structure. Graph transformers overcome this by using attention
mechanisms that allow nodes to exchange information globally. However, there
are two types of attention in graph transformers: dense and sparse. In this
paper, we compare these two attention mechanisms, analyze their trade-offs, and
highlight when to use each. We also outline current challenges and problems in
designing attention for graph transformers.

</details>


### [94] [LLM Assertiveness can be Mechanistically Decomposed into Emotional and Logical Components](https://arxiv.org/abs/2508.17182)
*Hikaru Tsujimura,Arush Tagade*

Main category: cs.LG

TL;DR: 通过机制可解释性方法分析LLM过度自信的内部机制，发现自信表征可分解为情感和逻辑两个正交子成分，与心理学双路径模型相似，情感向量广泛影响预测准确性，逻辑向量具有更局部化的因果效应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在关键场景中经常表现出过度自信，以不应有的确定性呈现信息，需要研究这种行为的内部机制基础。

Method: 使用基于人类标注自信度数据集微调的Llama 3.2模型，提取所有层的残差激活，计算相似性度量来定位自信表征，识别对自信度对比最敏感的层。

Result: 发现高自信表征可分解为情感和逻辑两个正交子成分簇，与心理学精细加工可能性模型的双路径模型相似。从这些子成分导出的转向向量显示不同的因果效应：情感向量广泛影响预测准确性，逻辑向量产生更局部化的效应。

Conclusion: 这些发现为LLM自信度的多组件结构提供了机制证据，并指出了减轻过度自信行为的途径。

Abstract: Large Language Models (LLMs) often display overconfidence, presenting
information with unwarranted certainty in high-stakes contexts. We investigate
the internal basis of this behavior via mechanistic interpretability. Using
open-sourced Llama 3.2 models fine-tuned on human annotated assertiveness
datasets, we extract residual activations across all layers, and compute
similarity metrics to localize assertive representations. Our analysis
identifies layers most sensitive to assertiveness contrasts and reveals that
high-assertive representations decompose into two orthogonal sub-components of
emotional and logical clusters-paralleling the dual-route Elaboration
Likelihood Model in Psychology. Steering vectors derived from these
sub-components show distinct causal effects: emotional vectors broadly
influence prediction accuracy, while logical vectors exert more localized
effects. These findings provide mechanistic evidence for the multi-component
structure of LLM assertiveness and highlight avenues for mitigating
overconfident behavior.

</details>


### [95] [BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens](https://arxiv.org/abs/2508.17196)
*Hao Wen,Xinrui Wu,Yi Sun,Feifei Zhang,Liye Chen,Jie Wang,Yunxin Liu,Ya-Qin Zhang,Yuanchun Li*

Main category: cs.LG

TL;DR: BudgetThinker是一个预算感知推理框架，通过插入控制令牌和两阶段训练，让LLM在有限计算预算下保持推理性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法计算成本高、延迟大，限制了在实时或资源受限场景中的应用，需要开发预算可控的推理方案

Method: 在推理时定期插入特殊控制令牌告知剩余令牌预算，采用两阶段训练：监督微调熟悉预算约束，课程强化学习优化准确性和预算遵守

Result: 在数学推理基准测试中，BudgetThinker在各种预算条件下显著超越基线方法，保持性能的同时控制计算成本

Conclusion: 该方法为开发高效可控的LLM推理提供了可扩展解决方案，使先进模型更适合资源受限和实时环境的部署

Abstract: Recent advancements in Large Language Models (LLMs) have leveraged increased
test-time computation to enhance reasoning capabilities, a strategy that, while
effective, incurs significant latency and resource costs, limiting their
applicability in real-world time-constrained or cost-sensitive scenarios. This
paper introduces BudgetThinker, a novel framework designed to empower LLMs with
budget-aware reasoning, enabling precise control over the length of their
thought processes. We propose a methodology that periodically inserts special
control tokens during inference to continuously inform the model of its
remaining token budget. This approach is coupled with a comprehensive two-stage
training pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize
the model with budget constraints, followed by a curriculum-based Reinforcement
Learning (RL) phase that utilizes a length-aware reward function to optimize
for both accuracy and budget adherence. We demonstrate that BudgetThinker
significantly surpasses strong baselines in maintaining performance across a
variety of reasoning budgets on challenging mathematical benchmarks. Our method
provides a scalable and effective solution for developing efficient and
controllable LLM reasoning, making advanced models more practical for
deployment in resource-constrained and real-time environments.

</details>


### [96] [How to make Medical AI Systems safer? Simulating Vulnerabilities, and Threats in Multimodal Medical RAG System](https://arxiv.org/abs/2508.17215)
*Kaiwen Zuo,Zelin Liu,Raman Dutt,Ziyang Wang,Zhongtian Sun,Yeming Wang,Fan Mo,Pietro Liò*

Main category: cs.LG

TL;DR: 这篇论文提出了MedThreatRAG多模态毒化框架，通过注入对抗性医学图像-文本对来攻击医疗RAG系统，曝露了该类系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 医疗AI中使用基于检索增强生成的大视觉-语言模型存在安全风险，需要系统性地探索其漏洞和攻击面。

Method: 构建模拟半开放攻击环境，通过跨模态冲突注入(CMCI)在医学图像和报告中注入细微语义矛盾，同时保持表面合理性以避免被过滤。

Result: 在IU-Xray和MIMIC-CXR QA任务上，MedThreatRAG使回答F1分数最高下降27.66%，LLaVA-Med-1.5的F1率降至51.36%。

Conclusion: 医疗RAG系统存在根本性安全缺陷，需要具有威胁意识的设计和健壮的多模态一致性检查，论文最后提出了安全开发指南。

Abstract: Large Vision-Language Models (LVLMs) augmented with Retrieval-Augmented
Generation (RAG) are increasingly employed in medical AI to enhance factual
grounding through external clinical image-text retrieval. However, this
reliance creates a significant attack surface. We propose MedThreatRAG, a novel
multimodal poisoning framework that systematically probes vulnerabilities in
medical RAG systems by injecting adversarial image-text pairs. A key innovation
of our approach is the construction of a simulated semi-open attack
environment, mimicking real-world medical systems that permit periodic
knowledge base updates via user or pipeline contributions. Within this setting,
we introduce and emphasize Cross-Modal Conflict Injection (CMCI), which embeds
subtle semantic contradictions between medical images and their paired reports.
These mismatches degrade retrieval and generation by disrupting cross-modal
alignment while remaining sufficiently plausible to evade conventional filters.
While basic textual and visual attacks are included for completeness, CMCI
demonstrates the most severe degradation. Evaluations on IU-Xray and MIMIC-CXR
QA tasks show that MedThreatRAG reduces answer F1 scores by up to 27.66% and
lowers LLaVA-Med-1.5 F1 rates to as low as 51.36%. Our findings expose
fundamental security gaps in clinical RAG systems and highlight the urgent need
for threat-aware design and robust multimodal consistency checks. Finally, we
conclude with a concise set of guidelines to inform the safe development of
future multimodal medical RAG systems.

</details>


### [97] [GPG-HT: Generalized Policy Gradient with History-Aware Decision Transformer for Probabilistic Path Planning](https://arxiv.org/abs/2508.17218)
*Xing Wei,Yuqi Ouyang*

Main category: cs.LG

TL;DR: 本文解决随机交通网络中的可靠最短路径问题，通过结合决策Transformer和广义策略梯度框架，提高了路径规划的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 城市交通拖塞问题严重，现有导航模型忽视了交通流的相关性和随机性，需要更有效的路径规划策略。

Method: 提出了结合决策Transformer和广义策略梯度(GPG)框架的路径规划方案，利用Transformer建模长期依赖关系的能力。

Result: 在Sioux Falls网络上的实验结果显示，该方法在准时到达概率方面超过了之前的基线方法，提供了更准确的路径规划解决方案。

Conclusion: 该研究为随机交通网络中的可靠路径规划提供了有效的方法，通过模型长期依赖关系显著提升了规划性能。

Abstract: With the rapidly increased number of vehicles in urban areas, existing road
infrastructure struggles to accommodate modern traffic demands, resulting in
the issue of congestion. This highlights the importance of efficient path
planning strategies. However, most recent navigation models focus solely on
deterministic or time-dependent networks, while overlooking the correlations
and the stochastic nature of traffic flows. In this work, we address the
reliable shortest path problem within stochastic transportation networks under
certain dependencies. We propose a path planning solution that integrates the
decision Transformer with the Generalized Policy Gradient (GPG) framework.
Based on the decision Transformer's capability to model long-term dependencies,
our proposed solution improves the accuracy and stability of path decisions.
Experimental results on the Sioux Falls Network (SFN) demonstrate that our
approach outperforms previous baselines in terms of on-time arrival
probability, providing more accurate path planning solutions.

</details>


### [98] [Module-Aware Parameter-Efficient Machine Unlearning on Transformers](https://arxiv.org/abs/2508.17233)
*Wenjie Bao,Jian Lou,Yuke Hu,Xiaochen Li,Zhihao Liu,Jiaqi Liu,Zhan Qin,Kui Ren*

Main category: cs.LG

TL;DR: MAPE-Unlearn是一种模块感知的参数高效机器学习遗忘方法，通过可学习的掩码对来精确定位Transformer中头和过滤器的影响关键参数，以提升遗忘性能。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效遗忘方法大多采用模块无关的方式，往往无法准确识别Transformer中的关键参数，导致遗忘性能不佳。为了遵守隐私法规并高效移除特定数据影响，需要开发更精确的遗忘方法。

Method: 提出MAPE-Unlearn方法，使用可学习的掩码对来识别Transformer中头和过滤器的影响关键参数。通过贪婪搜索和热启动的高效算法优化这些掩码的学习目标，该目标基于遗忘需求推导而来。

Result: 在各种Transformer模型和数据集上的广泛实验证明了MAPE-Unlearn在遗忘任务中的有效性和鲁棒性。

Conclusion: MAPE-Unlearn通过模块感知的方式准确识别关键参数，显著提升了Transformer模型的遗忘性能，为满足隐私法规要求提供了有效的解决方案。

Abstract: Transformer has become fundamental to a vast series of pre-trained large
models that have achieved remarkable success across diverse applications.
Machine unlearning, which focuses on efficiently removing specific data
influences to comply with privacy regulations, shows promise in restricting
updates to influence-critical parameters. However, existing parameter-efficient
unlearning methods are largely devised in a module-oblivious manner, which
tends to inaccurately identify these parameters and leads to inferior
unlearning performance for Transformers. In this paper, we propose {\tt
MAPE-Unlearn}, a module-aware parameter-efficient machine unlearning approach
that uses a learnable pair of masks to pinpoint influence-critical parameters
in the heads and filters of Transformers. The learning objective of these masks
is derived by desiderata of unlearning and optimized through an efficient
algorithm featured by a greedy search with a warm start. Extensive experiments
on various Transformer models and datasets demonstrate the effectiveness and
robustness of {\tt MAPE-Unlearn} for unlearning.

</details>


### [99] [DeepCFD: Efficient near-ground airfoil lift coefficient approximation with deep convolutional neural networks](https://arxiv.org/abs/2508.17278)
*Mohammad Amin Esabat,Saeed Jaamei,Fatemeh Asadi*

Main category: cs.LG

TL;DR: 使用VGG神经网络方法预测近地面翼型的升阻比系数，通过CFD数据和翼型截面图像矩阵进行训练，相比其他CNN方法精度更高


<details>
  <summary>Details</summary>
Motivation: 传统CFD软件计算近地面翼型气动系数耗时较长，而CFD模拟数据的可用性和新神经网络方法的发展使得能够用VGG等CNN方法快速呈现模拟结果

Method: 采用VGG神经网络方法，通过提供包含升阻比信息的原始数据和翼型截面图像转换的矩阵数据进行训练学习

Result: VGG方法相比其他CNN方法能够获得更准确的结果

Conclusion: 神经网络方法特别是VGG可以有效地预测近地面翼型的升阻比系数，为气动分析提供了一种更高效的替代方案

Abstract: . Predicting and calculating the aerodynamic coefficients of airfoils near
the ground with CFD software requires much time. However, the availability of
data from CFD simulation results and the development of new neural network
methods have made it possible to present the simulation results using methods
like VGG, a CCN neural network method. In this article, lift-to-drag
coefficients of airfoils near the ground surface are predicted with the help of
a neural network. This prediction can only be realized by providing data for
training and learning the code that contains information on the lift-to-drag
ratio of the primary data and images related to the airfoil cross-section,
which are converted into a matrix. One advantage of the VGG method over other
methods is that its results are more accurate than those of other CNN methods.

</details>


### [100] [Explainable AI (XAI) for Arrhythmia detection from electrocardiograms](https://arxiv.org/abs/2508.17294)
*Joschka Beck,Arlene John*

Main category: cs.LG

TL;DR: 这项研究将可解释性AI技术应用于心电图异常心律检测，通过比较四种SHAP方法找到了更符合临床工作流程的显著性地图解释方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习在心电图异常心律检测中准确性高但可解释性不足，影响临床应用。需要专门为时间序列ECG分析适配的可解释性技术。

Method: 使用MIT-BIH数据集开发卷积神经网络模型，通过Pan-Tompkins算法进行R波分割。进行用户需求评估确定医疗专业人员偏好的解释方式，并比较四种SHAP基础方法：排列重要性、KernelSHAP、梯度基础方法和DeepLIFT。

Result: 模型在MIT-BIH数据集上达到98.3%的验证准确率，但在混合数据集上性能下降。排列重要性和KernelSHAP产生杂乱的视觉输出，而梯度基础方法和DeepLIFT能够突出与临床推理一致的波形区域。医疗专业更偏好显著性地图解释。

Conclusion: 心电图分析需要领域特定的可解释性适配，显著性地图是更符合临床直觉的解释方法。数据集变异性是重要挑战，需要更一致的解释方法。

Abstract: Advancements in deep learning have enabled highly accurate arrhythmia
detection from electrocardiogram (ECG) signals, but limited interpretability
remains a barrier to clinical adoption. This study investigates the application
of Explainable AI (XAI) techniques specifically adapted for time-series ECG
analysis. Using the MIT-BIH arrhythmia dataset, a convolutional neural
network-based model was developed for arrhythmia classification, with
R-peak-based segmentation via the Pan-Tompkins algorithm. To increase the
dataset size and to reduce class imbalance, an additional 12-lead ECG dataset
was incorporated. A user needs assessment was carried out to identify what kind
of explanation would be preferred by medical professionals. Medical
professionals indicated a preference for saliency map-based explanations over
counterfactual visualisations, citing clearer correspondence with ECG
interpretation workflows. Four SHapley Additive exPlanations (SHAP)-based
approaches: permutation importance, KernelSHAP, gradient-based methods, and
Deep Learning Important FeaTures (DeepLIFT), were implemented and compared. The
model achieved 98.3% validation accuracy on MIT-BIH but showed performance
degradation on the combined dataset, underscoring dataset variability
challenges. Permutation importance and KernelSHAP produced cluttered visual
outputs, while gradient-based and DeepLIFT methods highlighted waveform regions
consistent with clinical reasoning, but with variability across samples.
Findings emphasize the need for domain-specific XAI adaptations in ECG analysis
and highlight saliency mapping as a more clinically intuitive approach

</details>


### [101] [Physics-informed neural network for fatigue life prediction of irradiated austenitic and ferritic/martensitic steels](https://arxiv.org/abs/2508.17303)
*Dhiraj S Kori,Abhinav Chandraker,Syed Abdur Rahman,Punit Rathore,Ankur Chauhan*

Main category: cs.LG

TL;DR: 提出了基于物理信息的神经网络(PINN)框架来预测核反应堆用辐照奥氏体和铁素体/马氏体钢的低周疲劳寿命，相比传统机器学习方法具有更高精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 核反应堆材料在高温循环载荷和辐照条件下会发生复杂退化，传统经验模型无法准确预测其疲劳寿命，需要开发更精确的预测方法。

Method: 开发PINN模型，在损失函数中融入物理疲劳寿命约束，使用495个数据点（包括辐照和未辐照条件）进行训练，并与随机森林、梯度提升等传统机器学习方法对比。

Result: PINN模型在所有对比模型中表现最佳，SHAP分析显示应变幅度、辐照剂量和测试温度是主导特征，均与疲劳寿命呈负相关，符合物理理解。模型成功捕捉了F/M钢在高应变幅度下的疲劳寿命饱和行为。

Conclusion: PINN框架为预测辐照合金疲劳寿命提供了可靠且可解释的方法，有助于指导合金选择决策。

Abstract: This study proposes a Physics-Informed Neural Network (PINN) framework to
predict the low-cycle fatigue (LCF) life of irradiated austenitic and
ferritic/martensitic (F/M) steels used in nuclear reactors. These materials
experience cyclic loading and irradiation at elevated temperatures, causing
complex degradation that traditional empirical models fail to capture
accurately. The developed PINN model incorporates physical fatigue life
constraints into its loss function, improving prediction accuracy and
generalizability. Trained on 495 data points, including both irradiated and
unirradiated conditions, the model outperforms traditional machine learning
models like Random Forest, Gradient Boosting, eXtreme Gradient Boosting, and
the conventional Neural Network. SHapley Additive exPlanations analysis
identifies strain amplitude, irradiation dose, and testing temperature as
dominant features, each inversely correlated with fatigue life, consistent with
physical understanding. PINN captures saturation behaviour in fatigue life at
higher strain amplitudes in F/M steels. Overall, the PINN framework offers a
reliable and interpretable approach for predicting fatigue life in irradiated
alloys, enabling informed alloy selection.

</details>


### [102] [AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation for Interpretable LLM Representations](https://arxiv.org/abs/2508.17320)
*Yifei Yao,Mengnan Du*

Main category: cs.LG

TL;DR: 提出了Adaptive Top K稀疏自编码器(AdaptiveK)，通过动态调整稀疏度水平来适应输入语义复杂度，显著优于固定稀疏度方法


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器使用固定稀疏度约束，无法适应不同输入的复杂度变化，需要更灵活的特征分配机制

Method: 利用线性探针发现上下文复杂度在线性编码中的线性关系，基于此信号在训练过程中动态调整特征分配，实现复杂度驱动的自适应稀疏度

Result: 在三个语言模型(Pythia-70M、Pythia-160M和Gemma-2-2B)上的实验显示，在重建保真度、解释方差和余弦相似度指标上显著优于固定稀疏度方法

Conclusion: 复杂度驱动的自适应方法不仅提升了性能表现，还消除了大量超参数调优的计算负担，为LLM内部表示的可解释性研究提供了有效解决方案

Abstract: Understanding the internal representations of large language models (LLMs)
remains a central challenge for interpretability research. Sparse autoencoders
(SAEs) offer a promising solution by decomposing activations into interpretable
features, but existing approaches rely on fixed sparsity constraints that fail
to account for input complexity. We propose Adaptive Top K Sparse Autoencoders
(AdaptiveK), a novel framework that dynamically adjusts sparsity levels based
on the semantic complexity of each input. Leveraging linear probes, we
demonstrate that context complexity is linearly encoded in LLM representations,
and we use this signal to guide feature allocation during training. Experiments
across three language models (Pythia-70M, Pythia-160M, and Gemma-2-2B)
demonstrate that this complexity-driven adaptation significantly outperforms
fixed-sparsity approaches on reconstruction fidelity, explained variance, and
cosine similarity metrics while eliminating the computational burden of
extensive hyperparameter tuning.

</details>


### [103] [Is the Frequency Principle always valid?](https://arxiv.org/abs/2508.17323)
*Qijia Zhai*

Main category: cs.LG

TL;DR: 本文研究了浅层ReLU神经网络在单位球面S²上的学习动力学，发现频率优先原则(FP)在固定和可训练权重下通常成立，但在特定条件下可能被违反，表明FP应被视为趋势而非绝对规则。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在弯曲域（如球面）上的频率学习特性，特别是频率优先原则(FP)在不同权重设置下的表现和局限性。

Method: 使用球谐展开分析固定和可训练权重下的学习动力学，通过谐波演化方程和数值实验验证理论分析。

Result: 固定权重时谐波系数以O(ℓ^{5/2}/2^ℓ)衰减，可训练权重时衰减阶数为O(ℓ^{7/2}/2^ℓ)，但FP在特定初始条件或误差分布下可能被违反。

Conclusion: 频率优先原则在球面等弯曲域上应被视为一种趋势而非绝对规则，权重可训练性增加了学习复杂性并可能改变频率学习顺序。

Abstract: We investigate the learning dynamics of shallow ReLU neural networks on the
unit sphere \(S^2\subset\mathbb{R}^3\) in polar coordinates \((\tau,\phi)\),
considering both fixed and trainable neuron directions \(\{w_i\}\). For fixed
weights, spherical harmonic expansions reveal an intrinsic low-frequency
preference with coefficients decaying as \(O(\ell^{5/2}/2^\ell)\), typically
leading to the Frequency Principle (FP) of lower-frequency-first learning.
However, this principle can be violated under specific initial conditions or
error distributions. With trainable weights, an additional rotation term in the
harmonic evolution equations preserves exponential decay with decay order
\(O(\ell^{7/2}/2^\ell)\) factor, also leading to the FP of
lower-frequency-first learning. But like fixed weights case, the principle can
be violated under specific initial conditions or error distributions. Our
numerical results demonstrate that trainable directions increase learning
complexity and can either maintain a low-frequency advantage or enable faster
high-frequency emergence. This analysis suggests the FP should be viewed as a
tendency rather than a rule on curved domains like \(S^2\), providing insights
into how direction updates and harmonic expansions shape frequency-dependent
learning.

</details>


### [104] [MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems](https://arxiv.org/abs/2508.17341)
*Muhammet Anil Yagiz,Zeynep Sude Cengiz,Polat Goktas*

Main category: cs.LG

TL;DR: MetaFed是一个去中心化的联邦学习框架，通过多智能体强化学习、同态加密和碳感知调度，为元宇宙应用提供可持续的智能资源编排方案，在降低碳排放的同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 集中式架构在应对元宇宙应用的性能、隐私和环境可持续性需求方面存在不足，导致高能耗、延迟和隐私问题。

Method: 集成多智能体强化学习进行动态客户端选择，使用同态加密实现隐私保护联邦学习，采用碳感知调度与可再生能源可用性对齐。

Result: 在MNIST和CIFAR-10数据集上使用轻量级ResNet架构进行评估，相比传统方法减少25%碳排放，同时保持高精度和最小通信开销。

Conclusion: MetaFed是构建环境友好且隐私合规的元宇宙基础设施的可扩展解决方案。

Abstract: The rapid expansion of immersive Metaverse applications introduces complex
challenges at the intersection of performance, privacy, and environmental
sustainability. Centralized architectures fall short in addressing these
demands, often resulting in elevated energy consumption, latency, and privacy
concerns. This paper proposes MetaFed, a decentralized federated learning (FL)
framework that enables sustainable and intelligent resource orchestration for
Metaverse environments. MetaFed integrates (i) multi-agent reinforcement
learning for dynamic client selection, (ii) privacy-preserving FL using
homomorphic encryption, and (iii) carbon-aware scheduling aligned with
renewable energy availability. Evaluations on MNIST and CIFAR-10 using
lightweight ResNet architectures demonstrate that MetaFed achieves up to 25\%
reduction in carbon emissions compared to conventional approaches, while
maintaining high accuracy and minimal communication overhead. These results
highlight MetaFed as a scalable solution for building environmentally
responsible and privacy-compliant Metaverse infrastructures.

</details>


### [105] [ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable Generation](https://arxiv.org/abs/2508.17345)
*Yuxuan Song,Zhe Zhang,Yu Pei,Jingjing Gong,Qiying Yu,Zheng Zhang,Mingxuan Wang,Hao Zhou,Jingjing Liu,Wei-Ying Ma*

Main category: cs.LG

TL;DR: SLM是一种基于单纯形质心的新型扩散模型，通过渐进候选剪枝方法降低生成复杂度，在离散变量生成任务中表现出色


<details>
  <summary>Details</summary>
Motivation: 离散变量的生成建模在自然语言处理和生物序列设计中具有重要应用价值，但面临挑战

Method: 提出Shortlisting Model (SLM)，采用单纯形质心扩散方法，结合渐进候选剪枝策略和灵活的classifier-free guidance实现

Result: 在DNA启动子和增强子设计、蛋白质设计、字符级和大词汇量语言建模等任务中展现出竞争性性能和强大潜力

Conclusion: SLM为离散变量生成提供了有效的解决方案，具有很好的可扩展性和应用前景

Abstract: Generative modeling of discrete variables is challenging yet crucial for
applications in natural language processing and biological sequence design. We
introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model
inspired by progressive candidate pruning. SLM operates on simplex centroids,
reducing generation complexity and enhancing scalability. Additionally, SLM
incorporates a flexible implementation of classifier-free guidance, enhancing
unconditional generation performance. Extensive experiments on DNA promoter and
enhancer design, protein design, character-level and large-vocabulary language
modeling demonstrate the competitive performance and strong potential of SLM.
Our code can be found at https://github.com/GenSI-THUAIR/SLM

</details>


### [106] [Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias](https://arxiv.org/abs/2508.17361)
*Shir Bernstein,David Beste,Daniel Ayzenshteyn,Lea Schonherr,Yisroel Mirsky*

Main category: cs.LG

TL;DR: 本文发现并利用LLM代码分析中的抽象偏见漏洞，提出熟悉模式攻击(FPA)，通过最小代码编辑即可劫持LLM的控制流解释，且攻击具有跨模型和跨语言的通用性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛用于自动化代码审查和静态分析，需要识别其存在的安全漏洞。研究发现LLM存在抽象偏见，会过度泛化熟悉编程模式而忽略小但有意义的bug。

Method: 开发了全自动黑盒算法来发现和注入FPA攻击，在多种编程语言(Python、C、Rust、Go)和多个LLM模型(GPT-4o、Claude 3.5、Gemini 2.0)上进行评估。

Result: FPA攻击不仅有效，而且具有跨模型和跨语言的通用性。即使在系统提示明确警告攻击的情况下，攻击仍然有效。

Conclusion: FPA攻击揭示了代码导向LLM的可靠性问题，同时探讨了FPA的防御性用途及其对LLM代码分析安全性的更广泛影响。

Abstract: Large Language Models (LLMs) are increasingly trusted to perform automated
code review and static analysis at scale, supporting tasks such as
vulnerability detection, summarization, and refactoring. In this paper, we
identify and exploit a critical vulnerability in LLM-based code analysis: an
abstraction bias that causes models to overgeneralize familiar programming
patterns and overlook small, meaningful bugs. Adversaries can exploit this
blind spot to hijack the control flow of the LLM's interpretation with minimal
edits and without affecting actual runtime behavior. We refer to this attack as
a Familiar Pattern Attack (FPA).
  We develop a fully automated, black-box algorithm that discovers and injects
FPAs into target code. Our evaluation shows that FPAs are not only effective,
but also transferable across models (GPT-4o, Claude 3.5, Gemini 2.0) and
universal across programming languages (Python, C, Rust, Go). Moreover, FPAs
remain effective even when models are explicitly warned about the attack via
robust system prompts. Finally, we explore positive, defensive uses of FPAs and
discuss their broader implications for the reliability and safety of
code-oriented LLMs.

</details>


### [107] [ShaLa: Multimodal Shared Latent Space Modelling](https://arxiv.org/abs/2508.17376)
*Jiali Cui,Yan-Ying Chen,Yanxia Zhang,Matthew Klenk*

Main category: cs.LG

TL;DR: ShaLa是一个新颖的多模态生成框架，通过整合创新的架构推理模型和扩散先验，解决了多模态VAE在共享潜在表示学习和合成质量方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态方法过于关注模态特定的细节组合，可能掩盖跨模态共享的高层语义概念。多模态VAE虽然旨在捕获共享表示，但在表达性联合变分后验设计和合成质量方面存在困难。

Method: ShaLa整合了新颖的架构推理模型和第二阶段表达性扩散先验，不仅促进共享潜在表示的有效推断，还显著提升下游多模态合成质量。

Result: 在多个基准测试中验证，相比最先进的多模态VAE，ShaLa展现出更优越的一致性和合成质量，并能扩展到更多模态。

Conclusion: ShaLa成功解决了多模态VAE的关键挑战，在共享潜在表示学习和多模态合成方面实现了显著改进，具有良好的可扩展性。

Abstract: This paper presents a novel generative framework for learning shared latent
representations across multimodal data. Many advanced multimodal methods focus
on capturing all combinations of modality-specific details across inputs, which
can inadvertently obscure the high-level semantic concepts that are shared
across modalities. Notably, Multimodal VAEs with low-dimensional latent
variables are designed to capture shared representations, enabling various
tasks such as joint multimodal synthesis and cross-modal inference. However,
multimodal VAEs often struggle to design expressive joint variational
posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses
these challenges by integrating a novel architectural inference model and a
second-stage expressive diffusion prior, which not only facilitates effective
inference of shared latent representation but also significantly improves the
quality of downstream multimodal synthesis. We validate ShaLa extensively
across multiple benchmarks, demonstrating superior coherence and synthesis
quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales
to many more modalities while prior multimodal VAEs have fallen short in
capturing the increasing complexity of the shared latent space.

</details>


### [108] [FedERL: Federated Efficient and Robust Learning for Common Corruptions](https://arxiv.org/abs/2508.17381)
*Omar Bekdache,Naresh Shanbhag*

Main category: cs.LG

TL;DR: FedERL是首个在联邦学习中同时解决客户端计算资源限制和抗数据损坏鲁棒性问题的框架，通过服务器端的数据无关鲁棒训练方法实现零客户端开销


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端计算资源受限和缺乏对常见数据损坏（噪声、模糊、天气效应）鲁棒性的挑战，现有鲁棒训练方法计算成本高且不适合资源受限的客户端

Method: 提出FedERL框架，核心是服务器端的数据无关鲁棒训练（DART）方法，无需访问训练数据即可增强模型鲁棒性，确保客户端零鲁棒性开销

Result: 大量实验表明FedERL能够以传统鲁棒训练方法的一小部分时间和能耗成本处理常见数据损坏，在有限时间和能源预算下性能超越传统方法

Conclusion: FedERL为现实世界联邦学习应用提供了一个实用且可扩展的解决方案，在保持数据隐私的同时实现了高效的鲁棒性训练

Abstract: Federated learning (FL) accelerates the deployment of deep learning models on
edge devices while preserving data privacy. However, FL systems face challenges
due to client-side constraints on computational resources, and from a lack of
robustness to common corruptions such as noise, blur, and weather effects.
Existing robust training methods are computationally expensive and unsuitable
for resource-constrained clients. We propose FedERL, federated efficient and
robust learning, as the first work to explicitly address corruption robustness
under time and energy constraints on the client side. At its core, FedERL
employs a novel data-agnostic robust training (DART) method on the server to
enhance robustness without access to the training data. In doing so, FedERL
ensures zero robustness overhead for clients. Extensive experiments demonstrate
FedERL's ability to handle common corruptions at a fraction of the time and
energy cost of traditional robust training methods. In scenarios with limited
time and energy budgets, FedERL surpasses the performance of traditional robust
training, establishing it as a practical and scalable solution for real-world
FL applications.

</details>


### [109] [Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning](https://arxiv.org/abs/2508.17387)
*Yicong Wu,Guangyue Lu,Yuan Zuo,Huarong Zhang,Junjie Wu*

Main category: cs.LG

TL;DR: 提出Graph-R1框架，将图任务转化为文本推理问题，使用强化学习引导大型推理模型进行零样本图学习


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络固定标签空间的限制和大型语言模型缺乏结构归纳偏置的问题，探索通过显式推理链实现零样本图任务泛化

Method: 将节点分类、链接预测和图分类任务重新表述为文本推理问题，使用特定任务的重思考模板指导线性化图上的推理，采用强化学习框架

Result: 在零样本设置下优于最先进的基线方法，产生可解释且有效的预测

Conclusion: 显式推理在图学习中具有巨大潜力，为未来研究提供了新的资源和方向

Abstract: Generalizing to unseen graph tasks without task-pecific supervision remains
challenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,
while Large Language Models (LLMs) lack structural inductive biases. Recent
advances in Large Reasoning Models (LRMs) provide a zero-shot alternative via
explicit, long chain-of-thought reasoning. Inspired by this, we propose a
GNN-free approach that reformulates graph tasks--node classification, link
prediction, and graph classification--as textual reasoning problems solved by
LRMs. We introduce the first datasets with detailed reasoning traces for these
tasks and develop Graph-R1, a reinforcement learning framework that leverages
task-specific rethink templates to guide reasoning over linearized graphs.
Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines in
zero-shot settings, producing interpretable and effective predictions. Our work
highlights the promise of explicit reasoning for graph learning and provides
new resources for future research.

</details>


### [110] [Effective Clustering for Large Multi-Relational Graphs](https://arxiv.org/abs/2508.17388)
*Xiaoyang Lin,Runhao Jiang,Renchi Yang*

Main category: cs.LG

TL;DR: 提出了DEMM和DEMM+两种多关系图聚类方法，通过新颖的两阶段优化目标解决现有方法质量差或扩展性差的问题，DEMM+通过优化技术实现线性时间聚类，在11个真实数据集上优于20个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有多关系图聚类方法要么因异构图结构和属性融合效果差而导致结果质量严重受损，要么因采用复杂昂贵的深度学习模型而无法处理大规模图数据。

Method: 基于新颖的两阶段优化目标：第一阶段通过优化多关系Dirichlet能量获得高质量节点特征向量，第二阶段在节点亲和图上最小化聚类结果的Dirichlet能量。DEMM+通过高效近似求解器和理论驱动的问题转换实现线性时间聚类。

Result: 在11个真实多关系图上与20个基线方法比较，DEMM+在聚类质量方面始终优于其他方法，且通常显著更快。

Conclusion: DEMM+通过有效的优化技术和理论创新，成功解决了多关系图聚类的质量和可扩展性问题，为大规模多关系图分析提供了高效解决方案。

Abstract: Multi-relational graphs (MRGs) are an expressive data structure for modeling
diverse interactions/relations among real objects (i.e., nodes), which pervade
extensive applications and scenarios. Given an MRG G with N nodes, partitioning
the node set therein into K disjoint clusters (MRGC) is a fundamental task in
analyzing MRGs, which has garnered considerable attention. However, the
majority of existing solutions towards MRGC either yield severely compromised
result quality by ineffective fusion of heterogeneous graph structures and
attributes, or struggle to cope with sizable MRGs with millions of nodes and
billions of edges due to the adoption of sophisticated and costly deep learning
models.
  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to
address the limitations above. Specifically, our algorithms are built on novel
two-stage optimization objectives, where the former seeks to derive
high-caliber node feature vectors by optimizing the multi-relational Dirichlet
energy specialized for MRGs, while the latter minimizes the Dirichlet energy of
clustering results over the node affinity graph. In particular, DEMM+ achieves
significantly higher scalability and efficiency over our based method DEMM
through a suite of well-thought-out optimizations. Key technical contributions
include (i) a highly efficient approximation solver for constructing node
feature vectors, and (ii) a theoretically-grounded problem transformation with
carefully-crafted techniques that enable linear-time clustering without
explicitly materializing the NxN dense affinity matrix. Further, we extend
DEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive
experiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit
that DEMM+ is consistently superior in terms of clustering quality measured
against ground-truth labels, while often being remarkably faster.

</details>


### [111] [Retrieval Capabilities of Large Language Models Scale with Pretraining FLOPs](https://arxiv.org/abs/2508.17400)
*Jacob Portes,Connor Jennings,Erica Ji Yuen,Sasha Doubov,Michael Carbin*

Main category: cs.LG

TL;DR: 检索性能随预训练FLOPs呈可预测的缩放规律，模型大小、训练时长和FLOPs都与零样本BEIR任务的检索性能正相关，且上下文学习得分与检索得分强相关


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型(LLM)的检索性能如何随预训练计算量(FLOPs)的变化而缩放，为开发基于LLM的检索器提供指导

Method: 对不同规模的LLM（1.25亿到70亿参数）在不同规模数据集（10亿到2万亿tokens）上进行预训练，然后在零样本BEIR任务上评估检索性能

Result: 检索性能随模型大小、训练时长和估计FLOPs呈可预测的缩放趋势；上下文学习得分与检索任务得分存在强相关性

Conclusion: 这些发现对基于LLM的检索器开发具有重要意义，表明可以通过模型规模和训练计算量来预测和优化检索性能

Abstract: How does retrieval performance scale with pretraining FLOPs? We benchmark
retrieval performance across LLM model sizes from 125 million parameters to 7
billion parameters pretrained on datasets ranging from 1 billion tokens to more
than 2 trillion tokens. We find that retrieval performance on zero-shot BEIR
tasks predictably scales with LLM size, training duration, and estimated FLOPs.
We also show that In-Context Learning scores are strongly correlated with
retrieval scores across retrieval tasks. Finally, we highlight the implications
this has for the development of LLM-based retrievers.

</details>


### [112] [Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems](https://arxiv.org/abs/2508.17403)
*Yinsong Wang,Xiao Liu,Quan Zeng,Yu Ding*

Main category: cs.LG

TL;DR: 提出了互信息惊喜(MIS)框架，将惊喜重新定义为认知增长的信号而非异常检测，通过量化新观测对互信息的影响来评估学习进展，并开发了MISRP策略来动态调整系统行为。


<details>
  <summary>Details</summary>
Motivation: 现有自主实验系统缺乏检测和适应意外情况的机制，传统惊喜度量只能检测偏差但无法捕捉系统是否真正在学习适应。

Method: 引入互信息惊喜(MIS)框架，开发统计测试序列检测互信息估计的有意义变化，提出MISRP策略通过采样调整和进程分叉动态控制系统行为。

Result: 在合成领域和动态污染地图估计任务中，MISRP控制策略在稳定性、响应性和预测准确性方面显著优于传统基于惊喜的方法。

Conclusion: MIS将惊喜从反应性转变为反思性，为构建更具自我意识和适应性的自主系统提供了路径。

Abstract: Recent breakthroughs in autonomous experimentation have demonstrated
remarkable physical capabilities, yet their cognitive control remains
limited--often relying on static heuristics or classical optimization. A core
limitation is the absence of a principled mechanism to detect and adapt to the
unexpectedness. While traditional surprise measures--such as Shannon or
Bayesian Surprise--offer momentary detection of deviation, they fail to capture
whether a system is truly learning and adapting. In this work, we introduce
Mutual Information Surprise (MIS), a new framework that redefines surprise not
as anomaly detection, but as a signal of epistemic growth. MIS quantifies the
impact of new observations on mutual information, enabling autonomous systems
to reflect on their learning progression. We develop a statistical test
sequence to detect meaningful shifts in estimated mutual information and
propose a mutual information surprise reaction policy (MISRP) that dynamically
governs system behavior through sampling adjustment and process forking.
Empirical evaluations--on both synthetic domains and a dynamic pollution map
estimation task--show that MISRP-governed strategies significantly outperform
classical surprise-based approaches in stability, responsiveness, and
predictive accuracy. By shifting surprise from reactive to reflective, MIS
offers a path toward more self-aware and adaptive autonomous systems.

</details>


### [113] [FRAME : Comprehensive Risk Assessment Framework for Adversarial Machine Learning Threats](https://arxiv.org/abs/2508.17405)
*Avishag Shapira,Simon Shigol,Asaf Shabtai*

Main category: cs.LG

TL;DR: FRAME是首个全面自动化评估对抗性机器学习风险的框架，通过系统评估部署环境、攻击技术和实证研究三个维度来量化风险，无需AML专业知识即可为系统所有者提供可操作结果。


<details>
  <summary>Details</summary>
Motivation: 传统风险评估框架无法有效应对对抗性机器学习(AML)带来的独特挑战，现有AML评估方法主要关注技术攻击鲁棒性，忽略了部署环境、系统依赖性和攻击可行性等关键现实因素。

Method: FRAME框架包含新颖的风险评估方法，系统评估三个关键维度：目标系统部署环境、多样化AML技术特征、先前研究的实证见解。采用可行性评分机制和基于LLM的系统特定评估定制，并开发了全面的结构化AML攻击数据集。

Result: 在六个不同的真实世界应用中验证，展示了卓越的准确性和与AML专家分析的高度一致性。

Conclusion: FRAME使组织能够优先处理AML风险，支持在真实环境中安全部署AI系统，为系统所有者提供可直接使用的可操作结果。

Abstract: The widespread adoption of machine learning (ML) systems increased attention
to their security and emergence of adversarial machine learning (AML)
techniques that exploit fundamental vulnerabilities in ML systems, creating an
urgent need for comprehensive risk assessment for ML-based systems. While
traditional risk assessment frameworks evaluate conventional cybersecurity
risks, they lack ability to address unique challenges posed by AML threats.
Existing AML threat evaluation approaches focus primarily on technical attack
robustness, overlooking crucial real-world factors like deployment
environments, system dependencies, and attack feasibility. Attempts at
comprehensive AML risk assessment have been limited to domain-specific
solutions, preventing application across diverse systems. Addressing these
limitations, we present FRAME, the first comprehensive and automated framework
for assessing AML risks across diverse ML-based systems. FRAME includes a novel
risk assessment method that quantifies AML risks by systematically evaluating
three key dimensions: target system's deployment environment, characteristics
of diverse AML techniques, and empirical insights from prior research. FRAME
incorporates a feasibility scoring mechanism and LLM-based customization for
system-specific assessments. Additionally, we developed a comprehensive
structured dataset of AML attacks enabling context-aware risk assessment. From
an engineering application perspective, FRAME delivers actionable results
designed for direct use by system owners with only technical knowledge of their
systems, without expertise in AML. We validated it across six diverse
real-world applications. Our evaluation demonstrated exceptional accuracy and
strong alignment with analysis by AML experts. FRAME enables organizations to
prioritize AML risks, supporting secure AI deployment in real-world
environments.

</details>


### [114] [Modular MeanFlow: Towards Stable and Scalable One-Step Generative Modeling](https://arxiv.org/abs/2508.17426)
*Haochen You,Baojing Liu,Hongyang He*

Main category: cs.LG

TL;DR: MMF是一种一步生成建模方法，通过学习时间平均速度场实现高效高质量数据生成，统一了现有一致性方法和流匹配方法。


<details>
  <summary>Details</summary>
Motivation: 传统扩散或流模型需要多步评估，效率较低。本文旨在开发单步生成的高效方法，同时保持样本质量和训练稳定性。

Method: 提出Modular MeanFlow (MMF)框架，基于瞬时速度与平均速度的微分恒等式推导损失函数族，引入梯度调制机制稳定训练，采用课程式预热调度。

Result: 在图像合成和轨迹建模任务中，MMF实现了竞争性的样本质量、鲁棒收敛和强泛化能力，特别是在低数据或分布外设置下表现优异。

Conclusion: MMF提供了一个灵活且理论完备的一步生成建模框架，避免了昂贵的高阶导数计算，在效率和性能之间取得了良好平衡。

Abstract: One-step generative modeling seeks to generate high-quality data samples in a
single function evaluation, significantly improving efficiency over traditional
diffusion or flow-based models. In this work, we introduce Modular MeanFlow
(MMF), a flexible and theoretically grounded approach for learning
time-averaged velocity fields. Our method derives a family of loss functions
based on a differential identity linking instantaneous and average velocities,
and incorporates a gradient modulation mechanism that enables stable training
without sacrificing expressiveness. We further propose a curriculum-style
warmup schedule to smoothly transition from coarse supervision to fully
differentiable training. The MMF formulation unifies and generalizes existing
consistency-based and flow-matching methods, while avoiding expensive
higher-order derivatives. Empirical results across image synthesis and
trajectory modeling tasks demonstrate that MMF achieves competitive sample
quality, robust convergence, and strong generalization, particularly under
low-data or out-of-distribution settings.

</details>


### [115] [TreePO: Bridging the Gap of Policy Optimization and Efficacy and Inference Efficiency with Heuristic Tree-based Modeling](https://arxiv.org/abs/2508.17445)
*Yizhi Li,Qingshui Gu,Zhoufutu Wen,Ziniu Li,Tianshun Xing,Shuyue Guo,Tianyu Zheng,Xin Zhou,Xingwei Qu,Wangchunshu Zhou,Zheng Zhang,Wei Shen,Qian Liu,Chenghua Lin,Jian Yang,Ge Zhang,Wenhao Huang*

Main category: cs.LG

TL;DR: TreePO是一种基于树结构搜索的强化学习方法，通过动态树采样和分段解码来减少计算负担，同时保持探索多样性，在推理效率上实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型对齐方法虽然能解决复杂推理问题，但存在昂贵的在线策略展开和有限推理路径探索的问题。

Method: TreePO采用自引导展开算法，将序列生成视为树结构搜索过程，包含动态树采样策略和固定长度分段解码，利用局部不确定性保证额外分支，并通过分摊计算和早期剪枝来减少计算负担。

Result: 在推理基准测试中表现出性能提升，GPU时间节省22%-43%，轨迹级采样计算减少40%，令牌级减少35%。

Conclusion: TreePO为基于强化学习的后训练提供了一条实用路径，能够以更少的样本和计算实现推理效率的提升。

Abstract: Recent advancements in aligning large language models via reinforcement
learning have achieved remarkable gains in solving complex reasoning problems,
but at the cost of expensive on-policy rollouts and limited exploration of
diverse reasoning paths. In this work, we introduce TreePO, involving a
self-guided rollout algorithm that views sequence generation as a
tree-structured searching process. Composed of dynamic tree sampling policy and
fixed-length segment decoding, TreePO leverages local uncertainty to warrant
additional branches. By amortizing computation across common prefixes and
pruning low-value paths early, TreePO essentially reduces the per-update
compute burden while preserving or enhancing exploration diversity. Key
contributions include: (1) a segment-wise sampling algorithm that alleviates
the KV cache burden through contiguous segments and spawns new branches along
with an early-stop mechanism; (2) a tree-based segment-level advantage
estimation that considers both global and local proximal policy optimization.
and (3) analysis on the effectiveness of probability and quality-driven dynamic
divergence and fallback strategy. We empirically validate the performance gain
of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours
from 22\% up to 43\% of the sampling design for the trained models, meanwhile
showing up to 40\% reduction at trajectory-level and 35\% at token-level
sampling compute for the existing models. While offering a free lunch of
inference efficiency, TreePO reveals a practical path toward scaling RL-based
post-training with fewer samples and less compute. Home page locates at
https://m-a-p.ai/TreePO.

</details>


### [116] [Rectified Robust Policy Optimization for Model-Uncertain Constrained Reinforcement Learning without Strong Duality](https://arxiv.org/abs/2508.17448)
*Shaocong Ma,Ziyi Chen,Yi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了Rectified Robust Policy Optimization (RRPO)算法，解决了鲁棒约束强化学习中强对偶性不成立的问题，通过纯原始方法实现了在模型不确定性下的安全约束优化。


<details>
  <summary>Details</summary>
Motivation: 传统基于对偶的鲁棒约束强化学习方法可能失效，因为强对偶性在鲁棒约束RL中通常不成立，需要开发不依赖对偶形式的原始算法。

Method: 提出Rectified Robust Policy Optimization (RRPO)算法，这是一种纯原始方法，直接在原始问题上操作，不依赖于对偶形式化。

Result: 理论证明在温和正则性假设下具有收敛保证，迭代复杂度匹配已知最佳下界。网格世界环境中的实证结果表明RRPO能在模型不确定性下实现鲁棒安全性能。

Conclusion: RRPO算法有效解决了鲁棒约束RL中的对偶性问题，提供了理论保证和实证有效性，为非对偶方法在安全关键应用中的使用奠定了基础。

Abstract: The goal of robust constrained reinforcement learning (RL) is to optimize an
agent's performance under the worst-case model uncertainty while satisfying
safety or resource constraints. In this paper, we demonstrate that strong
duality does not generally hold in robust constrained RL, indicating that
traditional primal-dual methods may fail to find optimal feasible policies. To
overcome this limitation, we propose a novel primal-only algorithm called
Rectified Robust Policy Optimization (RRPO), which operates directly on the
primal problem without relying on dual formulations. We provide theoretical
convergence guarantees under mild regularity assumptions, showing convergence
to an approximately optimal feasible policy with iteration complexity matching
the best-known lower bound when the uncertainty set diameter is controlled in a
specific level. Empirical results in a grid-world environment validate the
effectiveness of our approach, demonstrating that RRPO achieves robust and safe
performance under model uncertainties while the non-robust method can violate
the worst-case safety constraints.

</details>


### [117] [ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories](https://arxiv.org/abs/2508.17452)
*Dou Jiabao,Nie Jiayi,Yihang Cheng,Jinwei Liu,Yingrui Ji,Canran Xiao,Feixiang Du,Jiaping Xiao*

Main category: cs.LG

TL;DR: ReviBranch是一个新颖的深度强化学习框架，通过构建分支决策与图状态之间的历史对应关系来改进混合整数线性规划中的分支定界算法变量选择问题


<details>
  <summary>Details</summary>
Motivation: 传统分支启发式方法难以泛化到异构问题实例，现有学习方法如模仿学习依赖专家演示质量，强化学习面临稀疏奖励和动态状态表示挑战

Method: 构建复活轨迹，建立分支决策与图状态的显式历史对应关系；引入重要性加权奖励重分配机制，将稀疏终端奖励转化为密集的逐步反馈

Result: 在不同MILP基准测试中优于最先进的RL方法，大规模实例上减少4.0%的B&B节点和2.2%的LP迭代次数

Conclusion: ReviBranch在异构MILP问题类别中展现出强大的鲁棒性和泛化能力

Abstract: The Branch-and-bound (B&B) algorithm is the main solver for Mixed Integer
Linear Programs (MILPs), where the selection of branching variable is essential
to computational efficiency. However, traditional heuristics for branching
often fail to generalize across heterogeneous problem instances, while existing
learning-based methods such as imitation learning (IL) suffers from dependence
on expert demonstration quality, and reinforcement learning (RL) struggles with
limitations in sparse rewards and dynamic state representation challenges. To
address these issues, we propose ReviBranch, a novel deep RL framework that
constructs revived trajectories by reviving explicit historical correspondences
between branching decisions and their corresponding graph states along
search-tree paths. During training, ReviBranch enables agents to learn from
complete structural evolution and temporal dependencies within the branching
process. Additionally, we introduce an importance-weighted reward
redistribution mechanism that transforms sparse terminal rewards into dense
stepwise feedback, addressing the sparse reward challenge. Extensive
experiments on different MILP benchmarks demonstrate that ReviBranch
outperforms state-of-the-art RL methods, reducing B&B nodes by 4.0% and LP
iterations by 2.2% on large-scale instances. The results highlight the
robustness and generalizability of ReviBranch across heterogeneous MILP problem
classes.

</details>


### [118] [A Systematic Literature Review on Multi-label Data Stream Classification](https://arxiv.org/abs/2508.17455)
*H. Freire-Oliveira,E. R. F. Paiva,J. Gama,L. Khan,R. Cerri*

Main category: cs.LG

TL;DR: 对多标签数据流分类方法的系统性文献综述，分析了该领域的最新进展、评估策略和计算复杂度，并指出了未来研究方向


<details>
  <summary>Details</summary>
Motivation: 多标签数据流分类在现实世界中具有高度适用性，但面临数据高速连续到达、概念漂移、新标签出现和真实标签延迟到达等挑战，需要系统性地梳理现有方法

Method: 采用系统性文献综述方法，对多标签数据流分类提案进行深入分析，构建层次结构，讨论各方法如何应对不同问题，分析评估策略和计算复杂度

Result: 提供了多标签数据流分类领域的全面概述，建立了完整的层次结构，分析了方法的渐进复杂性和资源消耗

Conclusion: 识别了该领域的主要研究空白，并为未来的研究方向提供了建议，强调需要进一步解决动态环境中的挑战

Abstract: Classification in the context of multi-label data streams represents a
challenge that has attracted significant attention due to its high real-world
applicability. However, this task faces problems inherent to dynamic
environments, such as the continuous arrival of data at high speed and volume,
changes in the data distribution (concept drift), the emergence of new labels
(concept evolution), and the latency in the arrival of ground truth labels.
This systematic literature review presents an in-depth analysis of multi-label
data stream classification proposals. We characterize the latest methods in the
literature, providing a comprehensive overview, building a thorough hierarchy,
and discussing how the proposals approach each problem. Furthermore, we discuss
the adopted evaluation strategies and analyze the methods' asymptotic
complexity and resource consumption. Finally, we identify the main gaps and
offer recommendations for future research directions in the field.

</details>


### [119] [Adversarial Examples Are Not Bugs, They Are Superposition](https://arxiv.org/abs/2508.17456)
*Liv Gorton,Owen Lewis*

Main category: cs.LG

TL;DR: 该论文提出叠加(superposition)机制可能是对抗样本现象的主要成因，通过理论分析、玩具模型实验和ResNet18实验提供了四方面证据支持这一假设。


<details>
  <summary>Details</summary>
Motivation: 对抗样本作为深度学习中最令人困惑的现象之一，近十年来缺乏根本机制的统一解释。本文探索了叠加这一机制解释性概念是否可能是对抗现象的主要成因。

Method: 通过四个方面的证据链：理论分析叠加解释对抗现象的能力、在玩具模型中干预叠加控制鲁棒性、在玩具模型中通过对抗训练干预鲁棒性控制叠加、在ResNet18中通过对抗训练干预鲁棒性控制叠加。

Result: 研究证实了叠加与对抗鲁棒性之间的双向因果关系，在理论和实验层面都支持叠加是对抗现象主要成因的假设。

Conclusion: 叠加机制很可能是对抗样本现象的根本原因，这一发现为理解对抗性脆弱性提供了新的理论框架，并扩展了Elhage等人先前的工作。

Abstract: Adversarial examples -- inputs with imperceptible perturbations that fool
neural networks -- remain one of deep learning's most perplexing phenomena
despite nearly a decade of research. While numerous defenses and explanations
have been proposed, there is no consensus on the fundamental mechanism. One
underexplored hypothesis is that superposition, a concept from mechanistic
interpretability, may be a major contributing factor, or even the primary
cause. We present four lines of evidence in support of this hypothesis, greatly
extending prior arguments by Elhage et al. (2022): (1) superposition can
theoretically explain a range of adversarial phenomena, (2) in toy models,
intervening on superposition controls robustness, (3) in toy models,
intervening on robustness (via adversarial training) controls superposition,
and (4) in ResNet18, intervening on robustness (via adversarial training)
controls superposition.

</details>


### [120] [MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models](https://arxiv.org/abs/2508.17467)
*Krishna Teja Chitty-Venkata,Sylvia Howland,Golara Azar,Daria Soboleva,Natalia Vassilieva,Siddhisanket Raskar,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: MoE-Inference-Bench是一个系统性评估MoE模型推理性能的基准测试，分析了批处理大小、序列长度、专家数量等超参数对吞吐量的影响，并评估了多种硬件加速技术。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然实现了大规模参数的同时保持计算效率，但在推理时存在负载不均衡和路由计算开销等问题，需要系统性的硬件加速技术评估来充分发挥其优势。

Method: 在Nvidia H100 GPU上评估多种优化技术，包括剪枝、融合MoE操作、推测解码、量化和并行化策略，测试了Mixtral、DeepSeek、OLMoE和Qwen等MoE模型家族。

Result: 研究揭示了不同配置下的性能差异，为MoE模型的高效部署提供了重要见解。

Conclusion: 该基准测试为MoE模型的推理性能优化提供了系统性评估框架，有助于指导实际部署中的参数配置和硬件加速技术选择。

Abstract: Mixture of Experts (MoE) models have enabled the scaling of Large Language
Models (LLMs) and Vision Language Models (VLMs) by achieving massive parameter
counts while maintaining computational efficiency. However, MoEs introduce
several inference-time challenges, including load imbalance across experts and
the additional routing computational overhead. To address these challenges and
fully harness the benefits of MoE, a systematic evaluation of hardware
acceleration techniques is essential. We present MoE-Inference-Bench, a
comprehensive study to evaluate MoE performance across diverse scenarios. We
analyze the impact of batch size, sequence length, and critical MoE
hyperparameters such as FFN dimensions and number of experts on throughput. We
evaluate several optimization techniques on Nvidia H100 GPUs, including
pruning, Fused MoE operations, speculative decoding, quantization, and various
parallelization strategies. Our evaluation includes MoEs from the Mixtral,
DeepSeek, OLMoE and Qwen families. The results reveal performance differences
across configurations and provide insights for the efficient deployment of
MoEs.

</details>


### [121] [A Human-In-The-Loop Approach for Improving Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.17477)
*Martin Käppel,Julian Neuberger,Felix Möhrlein,Sven Weinzierl,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的模型无关方法，通过人在循环方式识别和纠正预测性过程监控模型中的偏见决策，并在敏感属性同时被公平和不公平使用的情况下实现公平性与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 预测性过程监控中的机器学习模型容易受到数据中不公平、偏见或不道德模式的影响，导致基于敏感属性的偏见预测。但敏感属性在同一过程中可能同时被公平和不公平使用，传统的删除敏感属性方法无法处理这种复杂情况。

Method: 使用人在循环方式，通过从原始预测模型精炼出的决策树模型进行简单修改，来区分公平和不公平的决策。该方法是模型无关的，适用于各种预测模型。

Result: 提出的方法在存在偏见数据的情况下，实现了公平性与准确性之间有前景的平衡效果。所有源代码和数据都公开可用。

Conclusion: 该研究为预测性过程监控领域提供了一种有效的方法来处理敏感属性同时被公平和不公平使用的复杂情况，通过人工干预策判断况的区分，实现了更加公平的人工智能决策系统。

Abstract: Predictive process monitoring enables organizations to proactively react and
intervene in running instances of a business process. Given an incomplete
process instance, predictions about the outcome, next activity, or remaining
time are created. This is done by powerful machine learning models, which have
shown impressive predictive performance. However, the data-driven nature of
these models makes them susceptible to finding unfair, biased, or unethical
patterns in the data. Such patterns lead to biased predictions based on
so-called sensitive attributes, such as the gender or age of process
participants. Previous work has identified this problem and offered solutions
that mitigate biases by removing sensitive attributes entirely from the process
instance. However, sensitive attributes can be used both fairly and unfairly in
the same process instance. For example, during a medical process, treatment
decisions could be based on gender, while the decision to accept a patient
should not be based on gender. This paper proposes a novel, model-agnostic
approach for identifying and rectifying biased decisions in predictive business
process monitoring models, even when the same sensitive attribute is used both
fairly and unfairly. The proposed approach uses a human-in-the-loop approach to
differentiate between fair and unfair decisions through simple alterations on a
decision tree model distilled from the original prediction model. Our results
show that the proposed approach achieves a promising tradeoff between fairness
and accuracy in the presence of biased data. All source code and data are
publicly available at https://doi.org/10.5281/zenodo.15387576.

</details>


### [122] [Multimodal Representation Learning Conditioned on Semantic Relations](https://arxiv.org/abs/2508.17497)
*Yang Qiao,Yuntong Hu,Liang Zhao*

Main category: cs.LG

TL;DR: RCML是一个多模态表示学习框架，通过自然语言关系描述来指导特征提取和对齐，解决了传统对比模型在语义关系利用、上下文对齐和模态内一致性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对比模型（如CLIP）存在三个主要问题：1）主要关注图像-文本对，未能充分利用不同对之间的语义关系；2）直接匹配全局嵌入而缺乏上下文化，忽略了特定子空间或关系维度的语义对齐需求；3）强调跨模态对比，对模态内一致性的支持有限。

Method: 提出RCML框架，构建由语义关系连接的多样化训练对，引入关系引导的交叉注意力机制来调节每个关系上下文下的多模态表示。训练目标结合了跨模态和模态内对比损失，鼓励跨模态和语义相关样本的一致性。

Result: 在不同数据集上的实验表明，RCML在检索和分类任务上始终优于强基线模型。

Conclusion: 利用语义关系来指导多模态表示学习是有效的，RCML框架通过关系条件化学习显著提升了多模态表示的质量和性能。

Abstract: Multimodal representation learning has advanced rapidly with contrastive
models such as CLIP, which align image-text pairs in a shared embedding space.
However, these models face limitations: (1) they typically focus on image-text
pairs, underutilizing the semantic relations across different pairs. (2) they
directly match global embeddings without contextualization, overlooking the
need for semantic alignment along specific subspaces or relational dimensions;
and (3) they emphasize cross-modal contrast, with limited support for
intra-modal consistency. To address these issues, we propose
Relation-Conditioned Multimodal Learning RCML, a framework that learns
multimodal representations under natural-language relation descriptions to
guide both feature extraction and alignment. Our approach constructs
many-to-many training pairs linked by semantic relations and introduces a
relation-guided cross-attention mechanism that modulates multimodal
representations under each relation context. The training objective combines
inter-modal and intra-modal contrastive losses, encouraging consistency across
both modalities and semantically related samples. Experiments on different
datasets show that RCML consistently outperforms strong baselines on both
retrieval and classification tasks, highlighting the effectiveness of
leveraging semantic relations to guide multimodal representation learning.

</details>


### [123] [Learning Interpretable Differentiable Logic Networks for Time-Series Classification](https://arxiv.org/abs/2508.17512)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: 首次将可微分逻辑网络(DLNs)应用于单变量时间序列分类(TSC)，通过特征提取方法将时间序列转换为适合DLN分类的向量形式，并通过联合超参数搜索优化配置，在51个基准数据集上验证了DLN在准确性、推理效率和可解释性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 将DLNs从表格数据领域扩展到时间序列分类领域，探索DLNs在新领域中保持准确性、计算效率和可解释性的能力。

Method: 使用Catch22和TSFresh特征提取方法将时间序列转换为向量形式，采用联合超参数搜索而非孤立消融实验来优化DLN配置。

Result: 在51个公开单变量TSC基准测试中，DLNs展现出竞争性的准确性、低推理成本和透明的决策逻辑，与表格分类和回归任务中的表现一致。

Conclusion: DLNs成功应用于时间序列分类领域，保持了其在准确性、效率和可解释性方面的核心优势，为TSC任务提供了新的有效解决方案。

Abstract: Differentiable logic networks (DLNs) have shown promising results in tabular
domains by combining accuracy, interpretability, and computational efficiency.
In this work, we apply DLNs to the domain of TSC for the first time, focusing
on univariate datasets. To enable DLN application in this context, we adopt
feature-based representations relying on Catch22 and TSFresh, converting
sequential time series into vectorized forms suitable for DLN classification.
Unlike prior DLN studies that fix the training configuration and vary various
settings in isolation via ablation, we integrate all such configurations into
the hyperparameter search space, enabling the search process to select jointly
optimal settings. We then analyze the distribution of selected configurations
to better understand DLN training dynamics. We evaluate our approach on 51
publicly available univariate TSC benchmarks. The results confirm that
classification DLNs maintain their core strengths in this new domain: they
deliver competitive accuracy, retain low inference cost, and provide
transparent, interpretable decision logic, thus aligning well with previous DLN
findings in the realm of tabular classification and regression tasks.

</details>


### [124] [GateTS: Versatile and Efficient Forecasting via Attention-Inspired routed Mixture-of-Experts](https://arxiv.org/abs/2508.17515)
*Kyrylo Yemets,Mykola Lukashchuk,Ivan Izonin*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的时间序列预测模型，通过结合稀疏MoE计算和新的注意力间控机制，简化了训练过程并提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有MoE模型在时间序列预测中需要复杂训练过程、辅助负载均衡损失和精心调整路由的问题，以提高实际应用性。

Method: 结合稀疏MoE计算与新领的注意力受启发的间控机制，替代传统的单层softmax路由器，无需辅助负载均衡损失。

Result: 模型在多样化数据集上表现优异，仅需小部分参数即可超越PatchTST等独化模型，计算效率更高于LSTM，能够同时处理短期和长期预测。

Conclusion: 该方法为实际时间序列预测应用提供了既准确又高效的解决方案，具有重要的实践价值。

Abstract: Accurate univariate forecasting remains a pressing need in real-world
systems, such as energy markets, hydrology, retail demand, and IoT monitoring,
where signals are often intermittent and horizons span both short- and
long-term. While transformers and Mixture-of-Experts (MoE) architectures are
increasingly favored for time-series forecasting, a key gap persists: MoE
models typically require complicated training with both the main forecasting
loss and auxiliary load-balancing losses, along with careful
routing/temperature tuning, which hinders practical adoption. In this paper, we
propose a model architecture that simplifies the training process for
univariate time series forecasting and effectively addresses both long- and
short-term horizons, including intermittent patterns. Our approach combines
sparse MoE computation with a novel attention-inspired gating mechanism that
replaces the traditional one-layer softmax router. Through extensive empirical
evaluation, we demonstrate that our gating design naturally promotes balanced
expert utilization and achieves superior predictive accuracy without requiring
the auxiliary load-balancing losses typically used in classical MoE
implementations. The model achieves better performance while utilizing only a
fraction of the parameters required by state-of-the-art transformer models,
such as PatchTST. Furthermore, experiments across diverse datasets confirm that
our MoE architecture with the proposed gating mechanism is more computationally
efficient than LSTM for both long- and short-term forecasting, enabling
cost-effective inference. These results highlight the potential of our approach
for practical time-series forecasting applications where both accuracy and
computational efficiency are critical.

</details>


### [125] [TANDEM: Temporal Attention-guided Neural Differential Equations for Missingness in Time Series Classification](https://arxiv.org/abs/2508.17519)
*YongKyung Oh,Dong-Young Lim,Sungil Kim,Alex Bui*

Main category: cs.LG

TL;DR: TANDEM是一个基于注意力机制和神经微分方程的框架，用于处理时间序列分类中的缺失数据问题，在多个基准数据集和真实医疗数据上表现优于现有方法


<details>
  <summary>Details</summary>
Motivation: 处理时间序列分类中的缺失数据是一个重要挑战，传统插补方法可能引入偏差或无法捕捉时间动态

Method: 提出TANDEM框架，通过注意力机制整合原始观测、插补控制路径和连续潜在动态，使用神经微分方程处理缺失数据

Result: 在30个基准数据集和真实医疗数据集上评估，证明TANDEM优于现有最先进方法，提高了分类准确性

Conclusion: 该框架不仅提升了分类精度，还为处理缺失数据提供了新的见解，具有实际应用价值

Abstract: Handling missing data in time series classification remains a significant
challenge in various domains. Traditional methods often rely on imputation,
which may introduce bias or fail to capture the underlying temporal dynamics.
In this paper, we propose TANDEM (Temporal Attention-guided Neural Differential
Equations for Missingness), an attention-guided neural differential equation
framework that effectively classifies time series data with missing values. Our
approach integrates raw observation, interpolated control path, and continuous
latent dynamics through a novel attention mechanism, allowing the model to
focus on the most informative aspects of the data. We evaluate TANDEM on 30
benchmark datasets and a real-world medical dataset, demonstrating its
superiority over existing state-of-the-art methods. Our framework not only
improves classification accuracy but also provides insights into the handling
of missing data, making it a valuable tool in practice.

</details>


### [126] [Modeling Irregular Astronomical Time Series with Neural Stochastic Delay Differential Equations](https://arxiv.org/abs/2508.17521)
*YongKyung Oh,Seungsu Kam,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: 提出了基于神经随机延迟微分方程(Neural SDDEs)的新框架，用于处理不规则采样的天文时间序列数据，在分类和异常检测方面表现优异


<details>
  <summary>Details</summary>
Motivation: 大规模巡天项目(如LSST)产生的天文时间序列数据往往存在不规则采样和不完整的问题，这给分类和异常检测带来了挑战

Method: 结合随机建模和神经网络，采用延迟感知的神经架构、SDDE数值求解器，以及从噪声稀疏序列中稳健学习的机制

Result: 在不规则采样的天文数据实验中表现出强大的分类准确性和有效的新型天体物理事件检测能力，即使在部分标注的情况下也能良好工作

Conclusion: 神经随机延迟微分方程是在观测约束下进行时间序列分析的一种原理性且实用的工具

Abstract: Astronomical time series from large-scale surveys like LSST are often
irregularly sampled and incomplete, posing challenges for classification and
anomaly detection. We introduce a new framework based on Neural Stochastic
Delay Differential Equations (Neural SDDEs) that combines stochastic modeling
with neural networks to capture delayed temporal dynamics and handle irregular
observations. Our approach integrates a delay-aware neural architecture, a
numerical solver for SDDEs, and mechanisms to robustly learn from noisy, sparse
sequences. Experiments on irregularly sampled astronomical data demonstrate
strong classification accuracy and effective detection of novel astrophysical
events, even with partial labels. This work highlights Neural SDDEs as a
principled and practical tool for time series analysis under observational
constraints.

</details>


### [127] [Gumbel-MPNN: Graph Rewiring with Gumbel-Softmax](https://arxiv.org/abs/2508.17531)
*Marcel Hoffmann,Lukas Galke,Ansgar Scherp*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Gumbel-Softmax的图重连方法，通过减少邻域分布的偏差来提升消息传递神经网络在节点分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统认为图同配性是MPNN性能的关键，但最新研究发现邻域类别分布的一致性更为重要。作者发现MPNN性能取决于类别内邻域分布组件的数量，需要解决邻域分布偏差问题。

Method: 提出基于Gumbel-Softmax的重连方法，通过分解类别的邻域分布组件并减少分布偏差，增强邻域信息性并处理长程依赖。

Result: 新方法提高了邻域信息性，缓解了过度压缩问题，显著提升了MPNN的分类性能，并能有效处理长程依赖关系。

Conclusion: Gumbel-Softmax重连方法通过优化邻域分布一致性，为MPNN提供了更有效的图结构处理方案，在节点分类任务中表现出色。

Abstract: Graph homophily has been considered an essential property for message-passing
neural networks (MPNN) in node classification. Recent findings suggest that
performance is more closely tied to the consistency of neighborhood class
distributions. We demonstrate that the MPNN performance depends on the number
of components of the overall neighborhood distribution within a class. By
breaking down the classes into their neighborhood distribution components, we
increase measures of neighborhood distribution informativeness but do not
observe an improvement in MPNN performance. We propose a Gumbel-Softmax-based
rewiring method that reduces deviations in neighborhood distributions. Our
results show that our new method enhances neighborhood informativeness, handles
long-range dependencies, mitigates oversquashing, and increases the
classification performance of the MPNN. The code is available at
https://github.com/Bobowner/Gumbel-Softmax-MPNN.

</details>


### [128] [Activation Transport Operators](https://arxiv.org/abs/2508.17540)
*Andrzej Szablewski,Marek Masiak*

Main category: cs.LG

TL;DR: 本文提出了激活传输算子(ATO)方法，用于分析Transformer残差流中特征的线性传输机制，通过线性映射来追踪特征在层间的流动，并提出了传输效率的概念。


<details>
  <summary>Details</summary>
Motivation: 虽然现有方法能够定位残差流中的特征和发现模型内部电路，但特征如何在残差流中流动的机制仍未被充分研究。理解这一动态过程可以改进越狱防护、实现模型错误的早期检测和修正。

Method: 提出激活传输算子(ATO)，这是一种从上游残差到下游残差(k层后)的线性映射，使用下游SAE解码器投影在特征空间中进行评估。开发了传输效率概念并提供了上界估计。

Result: 实证证明这些算子能够确定特征是从先前层线性传输而来还是从非线性层计算合成。报告了线性传输效率以及残差流中参与线性传输的子空间大小。

Conclusion: 这种计算轻量(无需微调，<50 GPU小时)的方法为安全性、调试提供了实用工具，并更清晰地展示了LLM中计算表现线性的位置。

Abstract: The residual stream mediates communication between transformer decoder layers
via linear reads and writes of non-linear computations. While sparse-dictionary
learning-based methods locate features in the residual stream, and activation
patching methods discover circuits within the model, the mechanism by which
features flow through the residual stream remains understudied. Understanding
this dynamic can better inform jailbreaking protections, enable early detection
of model mistakes, and their correction. In this work, we propose Activation
Transport Operators (ATO), linear maps from upstream to downstream residuals
$k$ layers later, evaluated in feature space using downstream SAE decoder
projections. We empirically demonstrate that these operators can determine
whether a feature has been linearly transported from a previous layer or
synthesised from non-linear layer computation. We develop the notion of
transport efficiency, for which we provide an upper bound, and use it to
estimate the size of the residual stream subspace that corresponds to linear
transport. We empirically demonstrate the linear transport, report transport
efficiency and the size of the residual stream's subspace involved in linear
transport. This compute-light (no finetuning, <50 GPU-h) method offers
practical tools for safety, debugging, and a clearer picture of where
computation in LLMs behaves linearly.

</details>


### [129] [Bridging Graph and State-Space Modeling for Intensive Care Unit Length of Stay Prediction](https://arxiv.org/abs/2508.17554)
*Shuqi Zi,Haitz Sáez de Ocáriz Borde,Emma Rocheteau,Pietro Lio'*

Main category: cs.LG

TL;DR: S²G-Net是一种新颖的神经网络架构，结合状态空间序列建模和多视图图神经网络，用于ICU住院时间预测，在MIMIC-IV数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ICU住院时间预测对医院资源管理至关重要，但由于电子健康记录(EHRs)的异构性和不规则采样特性，这一任务仍然具有挑战性。

Method: 提出S²G-Net架构，包含时间路径（使用Mamba状态空间模型捕捉患者轨迹）和图路径（使用优化的GraphGPS骨干网络整合异构患者相似性图，包括诊断、管理和语义特征）。

Result: 在MIMIC-IV队列数据集上的实验表明，S²G-Net在所有主要指标上均优于序列模型、图模型和混合方法。消融研究和可解释性分析验证了各组件的重要性。

Conclusion: S²G-Net为多模态临床数据的ICU住院时间预测提供了有效且可扩展的解决方案，证明了原理性图构建的重要性。

Abstract: Predicting a patient's length of stay (LOS) in the intensive care unit (ICU)
is a critical task for hospital resource management, yet remains challenging
due to the heterogeneous and irregularly sampled nature of electronic health
records (EHRs). In this work, we propose S$^2$G-Net, a novel neural
architecture that unifies state-space sequence modeling with multi-view Graph
Neural Networks (GNNs) for ICU LOS prediction. The temporal path employs Mamba
state-space models (SSMs) to capture patient trajectories, while the graph path
leverages an optimized GraphGPS backbone, designed to integrate heterogeneous
patient similarity graphs derived from diagnostic, administrative, and semantic
features. Experiments on the large-scale MIMIC-IV cohort dataset show that
S$^2$G-Net consistently outperforms sequence models (BiLSTM, Mamba,
Transformer), graph models (classic GNNs, GraphGPS), and hybrid approaches
across all primary metrics. Extensive ablation studies and interpretability
analyses highlight the complementary contributions of each component of our
architecture and underscore the importance of principled graph construction.
These results demonstrate that S$^2$G-Net provides an effective and scalable
solution for ICU LOS prediction with multi-modal clinical data.

</details>


### [130] [Exploring Efficient Learning of Small BERT Networks with LoRA and DoRA](https://arxiv.org/abs/2508.17586)
*Daniel Frees,Aditri Bhagirath,Moritz Bolling*

Main category: cs.LG

TL;DR: 这篇论文在小型语言模型minBERT上对LoRA和DoRA算法进行了效率性能测试，发现通过自定义配置和AMP技术可显著提升训练效率且保持性能。证实了小型模型中梯度更新本质上也是低秩的特性，并最终训练出多任务集成模型。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型引发了AI革呼，但细调LLM需要巨大计算成本，使得资源有限的小企业和研究团队难以参与。本文通过在小型模型minBERT上扩展LoRA和DoRA的研究，以验证这些效率优化方法在小规模模型上的适用性。

Method: 在小型minBERT模型上对LoRA和DoRA算法进行广法测试，采用自定义配置组合和自动混合精度(AMP)技术。还研究了多种网络架构、自定义损失函数和超参数，最终训练多任务集成模型。

Result: 研究发现优化配置的LoRA和DoRA统AMP能显著提升训练效率而不影响性能。证实了小型模型中梯度更新也具有内在低秩特性，秩1分解就能达到可忽略的性能损失。最终成功训练出能同时处理情感分析、句子改写检测和相似度评分的多任务集成模型。

Conclusion: 这项研究扩展了LoRA和DoRA在小型语言模型上的应用，证明了这些效率优化方法在不同规模模型上都有效。研究结果为资源有限的研究团队提供了高效细调小型模型的实用方法，并为小规模模型的多任务学习提供了新的视角。

Abstract: While Large Language Models (LLMs) have revolutionized artificial
intelligence, fine-tuning LLMs is extraordinarily computationally expensive,
preventing smaller businesses and research teams with limited GPU resources
from engaging with new research. Hu et al and Liu et al introduce Low-Rank
Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) as highly
efficient and performant solutions to the computational challenges of LLM
fine-tuning, demonstrating huge speedups and memory usage savings for models
such as GPT-3 and RoBERTa. We seek to expand upon the original LoRA and DoRA
papers by benchmarking efficiency and performance of LoRA and DoRA when applied
to a much smaller scale of language model: our case study here is the compact
minBERT model. Our findings reveal that optimal custom configurations of LoRA
and DoRA, coupled with Automatic Mixed Precision (AMP), significantly enhance
training efficiency without compromising performance. Furthermore, while the
parameterization of minBERT is significantly smaller than GPT-3, our results
validate the observation that gradient updates to language models are
inherently low-rank even in small model space, observing that rank 1
decompositions yield negligible performance deficits. Furthermore, aided by our
highly efficient minBERT implementation, we investigate numerous architectures,
custom loss functions, and hyperparameters to ultimately train an optimal
ensembled multitask minBERT model to simultaneously perform sentiment analysis,
paraphrase detection, and similarity scoring.

</details>


### [131] [ChartMaster: Advancing Chart-to-Code Generation with Real-World Charts and Chart Similarity Reinforcement Learning](https://arxiv.org/abs/2508.17608)
*Wentao Tan,Qiong Cao,Chao Xue,Yibing Zhan,Changxing Ding,Xiaodong He*

Main category: cs.LG

TL;DR: 提出了ReChartPrompt数据集和ChartSimRL强化学习算法，解决了图表到代码生成任务中的数据多样性不足和视觉一致性维护问题，ChartMaster模型在多个基准测试中达到SOTA效果


<details>
  <summary>Details</summary>
Motivation: 图表到代码生成任务面临两个主要挑战：数据多样性有限，以及训练过程中难以保持生成图表与原始图表的视觉一致性。现有数据集主要依赖种子数据通过GPT生成代码，导致样本同质化

Method: 1. ReChartPrompt：使用arXiv论文中的真实人类设计图表作为提示，构建大规模多样化数据集ReChartPrompt-240K；2. ChartSimRL：基于GRPO的强化学习算法，采用新颖的图表相似性奖励，包括属性相似性和视觉相似性

Result: ChartMaster模型在7B参数模型中达到最先进效果，在多个图表到代码生成基准测试中甚至可与GPT-4o相媲美

Conclusion: 通过整合ReChartPrompt数据集和ChartSimRL算法，有效解决了图表到代码生成任务中的数据多样性和视觉一致性挑战，显著提升了模型性能

Abstract: The chart-to-code generation task requires MLLMs to convert chart images into
executable code. This task faces two major challenges: limited data diversity
and insufficient maintenance of visual consistency between generated and
original charts during training. Existing datasets mainly rely on seed data to
prompt GPT models for code generation, resulting in homogeneous samples. To
address this, we propose ReChartPrompt, which leverages real-world,
human-designed charts from arXiv papers as prompts instead of synthetic seeds.
Using the diverse styles and rich content of arXiv charts, we construct
ReChartPrompt-240K, a large-scale and highly diverse dataset. Another challenge
is that although SFT effectively improve code understanding, it often fails to
ensure that generated charts are visually consistent with the originals. To
address this, we propose ChartSimRL, a GRPO-based reinforcement learning
algorithm guided by a novel chart similarity reward. This reward consists of
attribute similarity, which measures the overlap of chart attributes such as
layout and color between the generated and original charts, and visual
similarity, which assesses similarity in texture and other overall visual
features using convolutional neural networks. Unlike traditional text-based
rewards such as accuracy or format rewards, our reward considers the multimodal
nature of the chart-to-code task and effectively enhances the model's ability
to accurately reproduce charts. By integrating ReChartPrompt and ChartSimRL, we
develop the ChartMaster model, which achieves state-of-the-art results among
7B-parameter models and even rivals GPT-4o on various chart-to-code generation
benchmarks. All resources are available at
https://github.com/WentaoTan/ChartMaster.

</details>


### [132] [A Proportional-Integral Controller-Incorporated SGD Algorithm for High Efficient Latent Factor Analysis](https://arxiv.org/abs/2508.17609)
*Jinli Li,Shiyu Long,Minglian Han*

Main category: cs.LG

TL;DR: 提出了一种基于PI控制的加速SGD算法PILF模型，通过整合相关实例和历史信息来改进高维稀疏矩阵的特征提取性能


<details>
  <summary>Details</summary>
Motivation: 现有SGD-LFA方法仅依赖当前样本的瞬时梯度信息，忽略了历史迭代经验和样本间内在相关性，导致收敛速度慢和泛化性能不佳

Method: 开发PI加速SGD算法，通过比例-积分控制机制整合相关实例并精炼学习误差，同时利用当前和历史信息

Result: 对比实验证明PILF模型在HDI矩阵上具有优越的表征能力

Conclusion: PILF模型通过PI控制机制有效提升了高维稀疏矩阵特征提取的收敛速度和泛化性能

Abstract: In industrial big data scenarios, high-dimensional sparse matrices (HDI) are
widely used to characterize high-order interaction relationships among massive
nodes. The stochastic gradient descent-based latent factor analysis (SGD-LFA)
method can effectively extract deep feature information embedded in HDI
matrices. However, existing SGD-LFA methods exhibit significant limitations:
their parameter update process relies solely on the instantaneous gradient
information of current samples, failing to incorporate accumulated experiential
knowledge from historical iterations or account for intrinsic correlations
between samples, resulting in slow convergence speed and suboptimal
generalization performance. Thus, this paper proposes a PILF model by
developing a PI-accelerated SGD algorithm by integrating correlated instances
and refining learning errors through proportional-integral (PI) control
mechanism that current and historical information; Comparative experiments
demonstrate the superior representation capability of the PILF model on HDI
matrices

</details>


### [133] [Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning](https://arxiv.org/abs/2508.17630)
*An Ning,Tai Yue Li,Nan Yow Chen*

Main category: cs.LG

TL;DR: 量子图注意力网络（QGAT）将变分量子电路集成到注意力机制中，通过量子并行性同时生成多个注意力系数，显著降低计算开销和模型复杂度


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的优势来增强图神经网络的表达能力，特别是通过量子并行性来改进注意力机制的计算效率和处理复杂结构依赖的能力

Method: 采用强纠缠量子电路和振幅编码的节点特征，通过单一量子电路同时生成多个注意力系数，实现参数共享。经典投影权重和量子电路参数进行端到端联合优化

Result: 实验证明QGAT能有效捕捉复杂结构依赖，在归纳场景中具有更好的泛化能力，对特征和结构噪声具有更强的鲁棒性

Conclusion: QGAT展示了量子增强学习在化学、生物和网络分析等领域的潜力，其模块化设计便于集成到现有架构中，为量子-经典混合模型提供了可行方案

Abstract: We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neural
network that integrates variational quantum circuits into the attention
mechanism. At its core, QGAT employs strongly entangling quantum circuits with
amplitude-encoded node features to enable expressive nonlinear interactions.
Distinct from classical multi-head attention that separately computes each
head, QGAT leverages a single quantum circuit to simultaneously generate
multiple attention coefficients. This quantum parallelism facilitates parameter
sharing across heads, substantially reducing computational overhead and model
complexity. Classical projection weights and quantum circuit parameters are
optimized jointly in an end-to-end manner, ensuring flexible adaptation to
learning tasks. Empirical results demonstrate QGAT's effectiveness in capturing
complex structural dependencies and improved generalization in inductive
scenarios, highlighting its potential for scalable quantum-enhanced learning
across domains such as chemistry, biology, and network analysis. Furthermore,
experiments confirm that quantum embedding enhances robustness against feature
and structural noise, suggesting advantages in handling real-world noisy data.
The modularity of QGAT also ensures straightforward integration into existing
architectures, allowing it to easily augment classical attention-based models.

</details>


### [134] [ControlEchoSynth: Boosting Ejection Fraction Estimation Models via Controlled Video Diffusion](https://arxiv.org/abs/2508.17631)
*Nima Kondori,Hanwen Liang,Hooman Vaseli,Bingyu Xie,Christina Luong,Purang Abolmaesumi,Teresa Tsang,Renjie Liao*

Main category: cs.LG

TL;DR: 该研究提出了一种基于条件生成模型的合成超声心动图视图方法，通过生成合成数据来增强机器学习模型在射血分数估计中的性能，特别是在数据获取困难的临床环境中。


<details>
  <summary>Details</summary>
Motivation: 超声心动图数据获取和标注存在挑战，特别是在床旁超声(POCUS)环境中，操作者经验水平不一导致可用视图有限，影响了机器学习模型的性能。

Method: 使用条件生成模型，基于现有的真实心脏视图合成超声心动图视图，重点关注从双平面心尖视图测量射血分数(EF)这一关键参数。

Result: 初步结果表明，合成超声心动图视图用于增强现有数据集时，不仅提高了射血分数估计的准确性，还显示出开发更稳健、准确和临床相关机器学习模型的潜力。

Conclusion: 该方法有望推动合成数据在医学影像诊断中的进一步研究，为创新解决方案铺平道路，特别是在数据获取受限的临床环境中提升诊断准确性。

Abstract: Synthetic data generation represents a significant advancement in boosting
the performance of machine learning (ML) models, particularly in fields where
data acquisition is challenging, such as echocardiography. The acquisition and
labeling of echocardiograms (echo) for heart assessment, crucial in
point-of-care ultrasound (POCUS) settings, often encounter limitations due to
the restricted number of echo views available, typically captured by operators
with varying levels of experience. This study proposes a novel approach for
enhancing clinical diagnosis accuracy by synthetically generating echo views.
These views are conditioned on existing, real views of the heart, focusing
specifically on the estimation of ejection fraction (EF), a critical parameter
traditionally measured from biplane apical views. By integrating a conditional
generative model, we demonstrate an improvement in EF estimation accuracy,
providing a comparative analysis with traditional methods. Preliminary results
indicate that our synthetic echoes, when used to augment existing datasets, not
only enhance EF estimation but also show potential in advancing the development
of more robust, accurate, and clinically relevant ML models. This approach is
anticipated to catalyze further research in synthetic data applications, paving
the way for innovative solutions in medical imaging diagnostics.

</details>


### [135] [Longitudinal Progression Prediction of Alzheimer's Disease with Tabular Foundation Model](https://arxiv.org/abs/2508.17649)
*Yilang Ding,Jiawen Ren,Jiaying Lu,Gloria Hyunjung Kwak,Armin Iraji,Alex Fedorov*

Main category: cs.LG

TL;DR: L2C-TabPFN方法通过纵向到横断面的转换结合表格基础模型，在阿尔茨海默病预测中实现了竞争性性能，特别在脑室体积预测方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病具有多因素病因和多模态临床数据的复杂性，准确预测临床相关生物标志物对于疾病进展监测至关重要。

Method: 采用纵向到横断面(L2C)转换与预训练表格基础模型(TabPFN)相结合的方法，将序列患者记录转换为固定长度特征向量，用于预测诊断、认知评分和脑室体积。

Result: L2C-TabPFN在诊断和认知结果预测方面表现竞争性，在脑室体积预测方面达到最先进水平，该成像生物标志物反映了阿尔茨海默病的神经退行性变和进展。

Conclusion: 表格基础模型在推进阿尔茨海默病临床相关成像标志物的纵向预测方面具有巨大潜力。

Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that remains
challenging to predict due to its multifactorial etiology and the complexity of
multimodal clinical data. Accurate forecasting of clinically relevant
biomarkers, including diagnostic and quantitative measures, is essential for
effective monitoring of disease progression. This work introduces L2C-TabPFN, a
method that integrates a longitudinal-to-cross-sectional (L2C) transformation
with a pre-trained Tabular Foundation Model (TabPFN) to predict Alzheimer's
disease outcomes using the TADPOLE dataset. L2C-TabPFN converts sequential
patient records into fixed-length feature vectors, enabling robust prediction
of diagnosis, cognitive scores, and ventricular volume. Experimental results
demonstrate that, while L2C-TabPFN achieves competitive performance on
diagnostic and cognitive outcomes, it provides state-of-the-art results in
ventricular volume prediction. This key imaging biomarker reflects
neurodegeneration and progression in Alzheimer's disease. These findings
highlight the potential of tabular foundational models for advancing
longitudinal prediction of clinically relevant imaging markers in Alzheimer's
disease.

</details>


### [136] [Heterogeneous co-occurrence embedding for visual information exploration](https://arxiv.org/abs/2508.17663)
*Takuro Ishida,Tetsuo Furukawa*

Main category: cs.LG

TL;DR: 提出一种用于共现数据可视化的嵌入方法，将异质域元素映射到二维潜在空间，通过最大化互信息来保持依赖结构，支持多域分析和交互式可视化


<details>
  <summary>Details</summary>
Motivation: 处理异质域间的共现概率数据，旨在可视化非对称关系，帮助用户探索域间依赖结构

Method: 将异质元素映射到二维潜在空间，最大化互信息以保持原始依赖结构，可扩展到多域情况（使用总相关），并提出基于条件概率的颜色分配可视化方法

Result: 在形容词-名词数据集、NeurIPS数据集和主谓宾数据集上验证了方法的有效性，展示了域内和域间分析能力

Conclusion: 该方法能有效可视化异质域间的非对称关系，支持交互式信息探索，在多领域应用中表现出良好实用性

Abstract: This paper proposes an embedding method for co-occurrence data aimed at
visual information exploration. We consider cases where co-occurrence
probabilities are measured between pairs of elements from heterogeneous
domains. The proposed method maps these heterogeneous elements into
corresponding two-dimensional latent spaces, enabling visualization of
asymmetric relationships between the domains. The key idea is to embed the
elements in a way that maximizes their mutual information, thereby preserving
the original dependency structure as much as possible. This approach can be
naturally extended to cases involving three or more domains, using a
generalization of mutual information known as total correlation. For
inter-domain analysis, we also propose a visualization method that assigns
colors to the latent spaces based on conditional probabilities, allowing users
to explore asymmetric relationships interactively. We demonstrate the utility
of the method through applications to an adjective-noun dataset, the NeurIPS
dataset, and a subject-verb-object dataset, showcasing both intra- and
inter-domain analysis.

</details>


### [137] [Towards Synthesizing Normative Data for Cognitive Assessments Using Generative Multimodal Large Language Models](https://arxiv.org/abs/2508.17675)
*Victoria Yan,Honor Chotkowski,Fengran Wang,Alex Fedorov*

Main category: cs.LG

TL;DR: 本研究探索使用多模态大语言模型（GPT-4o和GPT-4o-mini）生成认知评估的合成规范数据，通过优化提示策略成功区分诊断组和人口统计差异，为新型图像认知测试开发提供了可行方案。


<details>
  <summary>Details</summary>
Motivation: 传统认知评估规范数据收集成本高、耗时长且更新不及时，限制了新认知测试的开发。多模态大语言模型的发展为从现有认知测试图像生成合成规范数据提供了新途径。

Method: 使用GPT-4o和GPT-4o-mini模型，采用两种提示策略（基础指令的朴素提示和包含上下文指导的高级提示）生成"Cookie Theft"图片描述任务的合成文本响应，并通过嵌入分析、BLEU、ROUGE、BERTScore和LLM-as-a-judge等方法进行评估。

Result: 高级提示策略生成的合成响应能更有效地区分诊断组并捕捉人口统计多样性。优质模型生成的响应具有更高的真实性和多样性。BERTScore是上下文相似性评估最可靠的指标，而BLEU对创造性输出的评估效果较差。

Conclusion: 研究表明，通过优化的提示方法引导的多模态大语言模型可以可行地生成现有认知测试的稳健合成规范数据，为克服传统限制开发新型图像认知评估奠定了基础。

Abstract: Cognitive assessments require normative data as essential benchmarks for
evaluating individual performance. Hence, developing new cognitive tests based
on novel image stimuli is challenging due to the lack of readily available
normative data. Traditional data collection methods are costly, time-consuming,
and infrequently updated, limiting their practical utility. Recent advancements
in generative multimodal large language models (MLLMs) offer a new approach to
generate synthetic normative data from existing cognitive test images. We
investigated the feasibility of using MLLMs, specifically GPT-4o and
GPT-4o-mini, to synthesize normative textual responses for established
image-based cognitive assessments, such as the "Cookie Theft" picture
description task. Two distinct prompting strategies-naive prompts with basic
instructions and advanced prompts enriched with contextual guidance-were
evaluated. Responses were analyzed using embeddings to assess their capacity to
distinguish diagnostic groups and demographic variations. Performance metrics
included BLEU, ROUGE, BERTScore, and an LLM-as-a-judge evaluation. Advanced
prompting strategies produced synthetic responses that more effectively
distinguished between diagnostic groups and captured demographic diversity
compared to naive prompts. Superior models generated responses exhibiting
higher realism and diversity. BERTScore emerged as the most reliable metric for
contextual similarity assessment, while BLEU was less effective for evaluating
creative outputs. The LLM-as-a-judge approach provided promising preliminary
validation results. Our study demonstrates that generative multimodal LLMs,
guided by refined prompting methods, can feasibly generate robust synthetic
normative data for existing cognitive tests, thereby laying the groundwork for
developing novel image-based cognitive assessments without the traditional
limitations.

</details>


### [138] [TiKMiX: Take Data Influence into Dynamic Mixture for Language Model Pre-training](https://arxiv.org/abs/2508.17677)
*Yifan Wang,Binbin Liu,Fengze Liu,Yuanfan Guo,Jiyao Deng,Xuecheng Wu,Weidong Zhou,Xiaohuan Zhou,Taifeng Wang*

Main category: cs.LG

TL;DR: TiKMiX是一种动态调整语言模型预训练数据混合比例的方法，通过Group Influence指标高效评估数据域对模型的影响，显著提升性能并减少计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 静态数据混合策略在语言模型预训练中效果不佳，因为模型对不同数据域的学习偏好会随训练动态变化，但高效观测这些变化存在挑战

Method: 提出Group Influence指标评估数据域影响，将数据混合问题转化为寻找最优影响最大化分布。提供两种解决方案：TiKMiX-D直接优化和TiKMiX-M使用回归模型预测更优混合比例

Result: TiKMiX-D仅用20%计算资源就超越REGMIX等先进方法性能。TiKMiX-M在9个下游基准测试中平均性能提升2%。模型在1万亿token规模上训练验证

Conclusion: 模型的数据偏好随训练进度和规模动态演变，基于Group Influence动态调整数据混合能显著提升性能，有效缓解静态比例下的数据消化不足问题

Abstract: The data mixture used in the pre-training of a language model is a
cornerstone of its final performance. However, a static mixing strategy is
suboptimal, as the model's learning preferences for various data domains shift
dynamically throughout training. Crucially, observing these evolving
preferences in a computationally efficient manner remains a significant
challenge. To address this, we propose TiKMiX, a method that dynamically
adjusts the data mixture according to the model's evolving preferences. TiKMiX
introduces Group Influence, an efficient metric for evaluating the impact of
data domains on the model. This metric enables the formulation of the data
mixing problem as a search for an optimal, influence-maximizing distribution.
We solve this via two approaches: TiKMiX-D for direct optimization, and
TiKMiX-M, which uses a regression model to predict a superior mixture. We
trained models with different numbers of parameters, on up to 1 trillion
tokens. TiKMiX-D exceeds the performance of state-of-the-art methods like
REGMIX while using just 20% of the computational resources. TiKMiX-M leads to
an average performance gain of 2% across 9 downstream benchmarks. Our
experiments reveal that a model's data preferences evolve with training
progress and scale, and we demonstrate that dynamically adjusting the data
mixture based on Group Influence, a direct measure of these preferences,
significantly improves performance by mitigating the underdigestion of data
seen with static ratios.

</details>


### [139] [Characterizing the Behavior of Training Mamba-based State Space Models on GPUs](https://arxiv.org/abs/2508.17679)
*Trinayan Baruah,Kaustubh Shivdikar,Sara Prescott,David Kaeli*

Main category: cs.LG

TL;DR: 这篇论文分析了Mamba基于状态空间模型(SSM)在GPU训练时的性能特征，通过构建代表性模型套件来分析其微架构要求，为GPU设计提供优化建议。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer模型表达能力强大，但关注计算的二次复杂度严重限制了序列长度扩展。SSM模型作为替代方案，能够降低计算复杂度，需要在GPU上进行详细的微架构特征分析。

Method: 构建了一个涉及不同模型架构的代表性Mamba基SSM模型套件，并使用该套件在GPU上进行训练过程的微架构行为分析。

Result: 对Mamba基SSM模型在GPU上运行时的架构特征进行了详细分析，获得了关于计算复杂度、内存访问模式等关键性能指标的见解。

Conclusion: 该研究为Mamba基SSM模型的GPU微架构优化提供了新的见解，持续推动这类模型性能扩展的潜在优化方向。

Abstract: Mamba-based State Space Models (SSM) have emerged as a promising alternative
to the ubiquitous transformers. Despite the expressive power of transformers,
the quadratic complexity of computing attention is a major impediment to
scaling performance as we increase the sequence length. SSMs provide an
alternative path that addresses this problem, reducing the computational
complexity requirements of self-attention with novel model architectures for
different domains and fields such as video, text generation and graphs. Thus,
it is important to characterize the behavior of these emerging workloads on
GPUs and understand their requirements during GPU microarchitectural design. In
this work we evaluate Mamba-based SSMs and characterize their behavior during
training on GPUs. We construct a workload suite that offers representative
models that span different model architectures. We then use this suite to
analyze the architectural implications of running Mamba-based SSMs on GPUs. Our
work sheds new light on potential optimizations to continue scaling the
performance for such models.

</details>


### [140] [Robustness Feature Adapter for Efficient Adversarial Training](https://arxiv.org/abs/2508.17680)
*Quanwei Wu,Jun Guo,Wei Wang,Yi Wang*

Main category: cs.LG

TL;DR: 提出基于适配器的高效对抗训练方法，直接在特征空间进行训练，解决了计算开销大和鲁棒过拟合问题


<details>
  <summary>Details</summary>
Motivation: 传统对抗训练在大规模骨干模型上计算开销过大，且存在鲁棒过拟合问题，需要同时解决这两个问题来构建更可信的基础模型

Method: 采用基于适配器的方法，直接在特征空间进行高效对抗训练，消除鲁棒过拟合并提高内循环收敛质量

Result: 显著提高计算效率，改善模型精度，并将对抗鲁棒性泛化到未见过的攻击

Conclusion: 基于适配器的方法在不同骨干架构和大规模对抗训练中均表现出有效性，为构建可信基础模型提供了高效解决方案

Abstract: Adversarial training (AT) with projected gradient descent is the most popular
method to improve model robustness under adversarial attacks. However,
computational overheads become prohibitively large when AT is applied to large
backbone models. AT is also known to have the issue of robust overfitting. This
paper contributes to solving both problems simultaneously towards building more
trustworthy foundation models. In particular, we propose a new adapter-based
approach for efficient AT directly in the feature space. We show that the
proposed adapter-based approach can improve the inner-loop convergence quality
by eliminating robust overfitting. As a result, it significantly increases
computational efficiency and improves model accuracy by generalizing
adversarial robustness to unseen attacks. We demonstrate the effectiveness of
the new adapter-based approach in different backbone architectures and in AT at
scale.

</details>


### [141] [Unlearning as Ablation: Toward a Falsifiable Benchmark for Generative Scientific Discovery](https://arxiv.org/abs/2508.17681)
*Robert Yang*

Main category: cs.LG

TL;DR: 本文提出了一种名为"unlearning-as-ablation"的可证伪测试方法，用于评估大型语言模型是否真正具备生成新科学知识的能力，而不仅仅是记忆和重组已有信息。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI在科学中作用的夸大说法（如AGI将治愈所有疾病）引发了一个核心认识论问题：LLM是真正生成新知识还是仅仅重组记忆片段。需要建立可验证的测试框架来区分这两种能力。

Method: 提出unlearning-as-ablation方法：系统性地移除目标结果及其整个遗忘闭包（引理、改述和多跳蕴含），然后评估模型是否仅从允许的公理和工具中重新推导出该结果。

Result: 这是一个概念性和方法论的观点论文，没有提供新的实证结果，但提出了一个原则性框架来绘制AI科学发现的真实范围和限制。

Conclusion: unlearning-as-ablation测试可以作为下一代基准测试，类似于ImageNet在视觉领域的催化作用，能够区分仅能回忆的模型和能够建设性生成新科学知识的模型。该方法为评估AI科学发现能力提供了系统框架。

Abstract: Bold claims about AI's role in science-from "AGI will cure all diseases" to
promises of radically accelerated discovery-raise a central epistemic question:
do large language models (LLMs) truly generate new knowledge, or do they merely
remix memorized fragments? We propose unlearning-as-ablation as a falsifiable
test of constructive scientific discovery. The method systematically removes a
target result and its entire forget-closure (lemmas, paraphrases, and multi-hop
entailments) and then evaluates whether the model can re-derive the result from
only permitted axioms and tools. Success provides evidence for genuine
generative capability; failure exposes current limits. Unlike prevailing
motivations for unlearning-privacy, copyright, or safety-our framing
repositions it as an epistemic probe for AI-for-Science. We argue that such
tests could serve as the next generation of benchmarks, much as ImageNet
catalyzed progress in vision: distinguishing models that can merely recall from
those that can constructively generate new scientific knowledge. We outline a
minimal pilot in mathematics and algorithms, and discuss extensions to physics,
chemistry, and biology. Whether models succeed or fail, unlearning-as-ablation
provides a principled framework to map the true reach and limits of AI
scientific discovery. This is a position paper: we advance a conceptual and
methodological argument rather than new empirical results.

</details>


### [142] [Rethinking Federated Learning Over the Air: The Blessing of Scaling Up](https://arxiv.org/abs/2508.17697)
*Jiaqi Zhu,Bikramjit Das,Yong Xie,Nikolaos Pappas,Howard H. Yang*

Main category: cs.LG

TL;DR: 本文分析了空中计算联邦学习在大规模客户端场景下的性能，发现增加客户端数量可以增强隐私保护、减轻信道衰落影响并改善收敛性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时进行协作模型训练，但通信资源有限限制了其性能，特别是在支持大量客户端的系统中。空中计算通过模拟信号传输中间参数来缓解通信瓶颈，但会带来信道失真问题。

Method: 开发了一个理论框架来分析大规模客户端场景下空中计算联邦学习的性能，并通过广泛的实验评估验证理论见解。

Result: 分析揭示了扩大参与客户端数量的三个关键优势：(1)增强隐私保护；(2)减轻信道衰落影响；(3)改善收敛性能。

Conclusion: 空中计算模型训练是联邦学习在具有大量客户端网络中可行的有效方法，理论分析得到了实验验证。

Abstract: Federated learning facilitates collaborative model training across multiple
clients while preserving data privacy. However, its performance is often
constrained by limited communication resources, particularly in systems
supporting a large number of clients. To address this challenge, integrating
over-the-air computations into the training process has emerged as a promising
solution to alleviate communication bottlenecks. The system significantly
increases the number of clients it can support in each communication round by
transmitting intermediate parameters via analog signals rather than digital
ones. This improvement, however, comes at the cost of channel-induced
distortions, such as fading and noise, which affect the aggregated global
parameters. To elucidate these effects, this paper develops a theoretical
framework to analyze the performance of over-the-air federated learning in
large-scale client scenarios. Our analysis reveals three key advantages of
scaling up the number of participating clients: (1) Enhanced Privacy: The
mutual information between a client's local gradient and the server's
aggregated gradient diminishes, effectively reducing privacy leakage. (2)
Mitigation of Channel Fading: The channel hardening effect eliminates the
impact of small-scale fading in the noisy global gradient. (3) Improved
Convergence: Reduced thermal noise and gradient estimation errors benefit the
convergence rate. These findings solidify over-the-air model training as a
viable approach for federated learning in networks with a large number of
clients. The theoretical insights are further substantiated through extensive
experimental evaluations.

</details>


### [143] [Adaptive Ensemble Learning with Gaussian Copula for Load Forecasting](https://arxiv.org/abs/2508.17700)
*Junying Yang,Gang Lu,Xiaoqing Yan,Peng Xia,Di Wu*

Main category: cs.LG

TL;DR: 使用高斯合作函数和适应性集成学习的新模型处理负载预测中的数据稀疏问题


<details>
  <summary>Details</summary>
Motivation: 机器学习虽然在完整数据上能够准确预测负载，但实际数据收集过程中存在不确定性导致数据稀疏，需要有效处理稀疏数据的方法

Method: 提出适应性集成学习高斯合作函数模型，包含三个模块：数据补充、机器学习构建和适应性集成。先用高斯合作函数消除稀疏性，然后用5个ML模型分别预测，最后通过适应性集成获得加权结果

Result: 实验证明该模型具有良好的稳健性

Conclusion: 该方法能够有效处理负载预测中的数据稀疏问题，提高预测准确性和稳健性

Abstract: Machine learning (ML) is capable of accurate Load Forecasting from complete
data. However, there are many uncertainties that affect data collection,
leading to sparsity. This article proposed a model called Adaptive Ensemble
Learning with Gaussian Copula to deal with sparsity, which contains three
modules: data complementation, ML construction, and adaptive ensemble. First,
it applies Gaussian Copula to eliminate sparsity. Then, we utilise five ML
models to make predictions individually. Finally, it employs adaptive ensemble
to get final weighted-sum result. Experiments have demonstrated that our model
are robust.

</details>


### [144] [Copyright Protection for 3D Molecular Structures with Watermarking](https://arxiv.org/abs/2508.17702)
*Runwen Hu,Peilin Chen,Keyan Ding,Shiqi Wang*

Main category: cs.LG

TL;DR: 提出了首个针对分子的鲁棒水印方法，利用原子级特征保持分子完整性，通过不变特征确保对仿射变换的鲁棒性，在保持分子基本性质的同时实现高精度水印嵌入。


<details>
  <summary>Details</summary>
Motivation: 人工智能在分子生成领域的快速发展带来了知识产权保护的迫切需求，需要一种能够有效保护分子知识产权而不影响其科学效用的水印技术。

Method: 利用原子级特征保持分子完整性，采用不变特征确保对仿射变换的鲁棒性，在QM9和GEOM-DRUG数据集上使用GeoBFN和GeoLDM生成模型进行验证。

Result: 水印准确率超过95.00%，基本性质保持率高于90.00%，下游对接模拟显示原始分子与水印分子性能相当，结合亲和力达到-6.00 kcal/mol，均方根偏差低于1.602 Å。

Conclusion: 该水印技术能有效保护分子知识产权而不影响科学效用，为分子发现和研究应用中安全、负责任的人工智能集成提供了可行方案。

Abstract: Artificial intelligence (AI) revolutionizes molecule generation in
bioengineering and biological research, significantly accelerating discovery
processes. However, this advancement introduces critical concerns regarding
intellectual property protection. To address these challenges, we propose the
first robust watermarking method designed for molecules, which utilizes
atom-level features to preserve molecular integrity and invariant features to
ensure robustness against affine transformations. Comprehensive experiments
validate the effectiveness of our method using the datasets QM9 and GEOM-DRUG,
and generative models GeoBFN and GeoLDM. We demonstrate the feasibility of
embedding watermarks, maintaining basic properties higher than 90.00\% while
achieving watermark accuracy greater than 95.00\%. Furthermore, downstream
docking simulations reveal comparable performance between original and
watermarked molecules, with binding affinities reaching -6.00 kcal/mol and root
mean square deviations below 1.602 \AA. These results confirm that our
watermarking technique effectively safeguards molecular intellectual property
without compromising scientific utility, enabling secure and responsible AI
integration in molecular discovery and research applications.

</details>


### [145] [Speculative Safety-Aware Decoding](https://arxiv.org/abs/2508.17739)
*Xuekang Wang,Shengyu Zhu,Xueqi Cheng*

Main category: cs.LG

TL;DR: SSD是一种轻量级解码时方法，通过推测采样和小模型集成，为大语言模型提供安全属性防御越狱攻击，同时加速推理


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型已与人类价值观和安全规则对齐，但越狱攻击仍不断出现，需要增强模型的安全属性来防御这些攻击，而传统微调方法资源密集且难以保证一致性

Method: SSD假设存在具备所需安全属性的小模型，在解码时集成推测采样，通过小模型与复合模型的匹配率量化越狱风险，动态切换解码方案，从原始模型和小模型组合的新分布中采样输出token

Result: 实验结果显示SSD成功为大模型赋予所需安全属性，同时保持对良性查询的有用性，并且由于推测采样设计还加速了推理时间

Conclusion: SSD提供了一种有效的解码时安全增强方法，既能防御越狱攻击又保持模型实用性，同时实现推理加速

Abstract: Despite extensive efforts to align Large Language Models (LLMs) with human
values and safety rules, jailbreak attacks that exploit certain vulnerabilities
continuously emerge, highlighting the need to strengthen existing LLMs with
additional safety properties to defend against these attacks. However, tuning
large models has become increasingly resource-intensive and may have difficulty
ensuring consistent performance. We introduce Speculative Safety-Aware Decoding
(SSD), a lightweight decoding-time approach that equips LLMs with the desired
safety property while accelerating inference. We assume that there exists a
small language model that possesses this desired property. SSD integrates
speculative sampling during decoding and leverages the match ratio between the
small and composite models to quantify jailbreak risks. This enables SSD to
dynamically switch between decoding schemes to prioritize utility or safety, to
handle the challenge of different model capacities. The output token is then
sampled from a new distribution that combines the distributions of the original
and the small models. Experimental results show that SSD successfully equips
the large model with the desired safety property, and also allows the model to
remain helpful to benign queries. Furthermore, SSD accelerates the inference
time, thanks to the speculative sampling design.

</details>


### [146] [Randomly Removing 50% of Dimensions in Text Embeddings has Minimal Impact on Retrieval and Classification Tasks](https://arxiv.org/abs/2508.17744)
*Sotaro Takeshita,Yurina Takeshita,Daniel Ruffinelli,Simone Paolo Ponzetto*

Main category: cs.LG

TL;DR: 研究发现随机截断文本嵌入向量高达50%维度仅导致性能轻微下降（<10%），这与传统认知相悖，表明许多均匀分布的维度实际上在移除后能提升性能


<details>
  <summary>Details</summary>
Motivation: 探索截断文本嵌入维度对下游任务性能的影响，研究较小嵌入尺寸的优势以及文本编码的潜在洞察

Method: 在6个最先进文本编码器和26个下游任务上进行实验，随机移除最多50%的嵌入维度，分析分类、检索和生成任务的表现

Result: 移除大量嵌入维度仅导致性能轻微下降（平均低于10%），发现许多均匀分布的维度在移除后反而提升性能，这种现象在生成任务中同样存在

Conclusion: 文本嵌入中存在大量冗余维度，随机截断嵌入向量是有效的压缩方法，这一发现对嵌入表示的有效使用和模型压缩具有重要意义

Abstract: In this paper, we study the surprising impact that truncating text embeddings
has on downstream performance. We consistently observe across 6
state-of-the-art text encoders and 26 downstream tasks, that randomly removing
up to 50% of embedding dimensions results in only a minor drop in performance,
less than 10%, in retrieval and classification tasks. Given the benefits of
using smaller-sized embeddings, as well as the potential insights about text
encoding, we study this phenomenon and find that, contrary to what is suggested
in prior work, this is not the result of an ineffective use of representation
space. Instead, we find that a large number of uniformly distributed dimensions
actually cause an increase in performance when removed. This would explain why,
on average, removing a large number of embedding dimensions results in a
marginal drop in performance. We make similar observations when truncating the
embeddings used by large language models to make next-token predictions on
generative tasks, suggesting that this phenomenon is not isolated to
classification or retrieval tasks.

</details>


### [147] [Multi-layer Abstraction for Nested Generation of Options (MANGO) in Hierarchical Reinforcement Learning](https://arxiv.org/abs/2508.17751)
*Alessio Arcudi,Davide Sartor,Alberto Sinigaglia,Vincent François-Lavet,Gian Antonio Susto*

Main category: cs.LG

TL;DR: MANGO是一个新颖的分层强化学习框架，通过多层抽象和嵌套选项来解决长期稀疏奖励环境中的挑战，提高了样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务中长期稀疏奖励环境下的学习效率问题，传统RL方法在这些环境中样本效率低下且泛化能力有限。

Method: 将复杂任务分解为多层抽象空间，每层定义抽象状态空间并使用选项将轨迹模块化为宏动作，通过嵌套选项实现学习动作的复用，引入层内策略和任务动作整合任务特定组件。

Result: 在程序生成的网格环境中实验显示，相比标准RL方法，样本效率和泛化能力都有显著提升，同时增强了决策过程的可解释性。

Conclusion: MANGO框架有效解决了长期稀疏奖励环境的问题，未来将探索自动抽象发现、连续环境适应和更鲁棒的多层训练策略。

Abstract: This paper introduces MANGO (Multilayer Abstraction for Nested Generation of
Options), a novel hierarchical reinforcement learning framework designed to
address the challenges of long-term sparse reward environments. MANGO
decomposes complex tasks into multiple layers of abstraction, where each layer
defines an abstract state space and employs options to modularize trajectories
into macro-actions. These options are nested across layers, allowing for
efficient reuse of learned movements and improved sample efficiency. The
framework introduces intra-layer policies that guide the agent's transitions
within the abstract state space, and task actions that integrate task-specific
components such as reward functions. Experiments conducted in
procedurally-generated grid environments demonstrate substantial improvements
in both sample efficiency and generalization capabilities compared to standard
RL methods. MANGO also enhances interpretability by making the agent's
decision-making process transparent across layers, which is particularly
valuable in safety-critical and industrial applications. Future work will
explore automated discovery of abstractions and abstract actions, adaptation to
continuous or fuzzy environments, and more robust multi-layer training
strategies.

</details>


### [148] [Puzzle: Scheduling Multiple Deep Learning Models on Mobile Device with Heterogeneous Processors](https://arxiv.org/abs/2508.17764)
*Duseok Kang,Yunseong Lee,Junghoon Kim*

Main category: cs.LG

TL;DR: 提出基于遗传算法的多深度学习网络调度方法Puzzle，通过子图划分和硬件在环分析，在异构处理器上实现高效调度，相比基准方法提升2.2-3.7倍请求频率。


<details>
  <summary>Details</summary>
Motivation: 移动设备硬件异构性增加，现有调度方法局限于单模型场景，忽视硬件/软件配置差异，执行时间估计不准确，无法满足实际多模型调度需求。

Method: 使用遗传算法，设计三种染色体类型进行分区/映射/优先级探索，结合设备在环分析和评估实现准确执行时间估计，将网络划分为多个子图进行调度。

Result: 在9个先进网络的随机场景测试中，Puzzle系统相比NPU Only和Best Mapping基准方法，平均支持3.7倍和2.2倍更高的请求频率，同时满足同等实时性要求。

Conclusion: 所提出的遗传算法调度方法能有效解决移动设备异构处理器上的多深度学习网络调度问题，显著提升系统性能和处理能力。

Abstract: As deep learning models are increasingly deployed on mobile devices, modern
mobile devices incorporate deep learning-specific accelerators to handle the
growing computational demands, thus increasing their hardware heterogeneity.
However, existing works on scheduling deep learning workloads across these
processors have significant limitations: most studies focus on single-model
scenarios rather than realistic multi-model scenarios, overlook performance
variations from different hardware/software configurations, and struggle with
accurate execution time estimation. To address these challenges, we propose a
novel genetic algorithm-based methodology for scheduling multiple deep learning
networks on heterogeneous processors by partitioning the networks into multiple
subgraphs. Our approach incorporates three different types of chromosomes for
partition/mapping/priority exploration, and leverages device-in-the-loop
profiling and evaluation for accurate execution time estimation. Based on this
methodology, our system, Puzzle, demonstrates superior performance in extensive
evaluations with randomly generated scenarios involving nine state-of-the-art
networks. The results demonstrate Puzzle can support 3.7 and 2.2 times higher
request frequency on average compared to the two heuristic baselines, NPU Only
and Best Mapping, respectively, while satisfying the equivalent level of
real-time requirements.

</details>


### [149] [Proximal Supervised Fine-Tuning](https://arxiv.org/abs/2508.17784)
*Wenhong Zhu,Ruobing Xie,Rui Wang,Xingwu Sun,Di Wang,Pengfei Liu*

Main category: cs.LG

TL;DR: 提出Proximal SFT (PSFT)方法，通过引入信任区域约束来防止监督微调中的策略漂移，提高模型的泛化能力和训练稳定性


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)会导致基础模型泛化能力下降，先前能力在新任务或领域上会退化，需要一种能约束策略漂移的微调方法

Method: 受强化学习中TRPO和PPO的启发，将SFT视为具有恒定正优势的策略梯度方法特例，提出PSFT目标函数，引入信任区域约束

Result: 在数学和人类价值观领域的实验表明，PSFT在域内性能与SFT相当，在域外泛化上优于SFT，训练稳定且不会导致熵崩溃

Conclusion: PSFT提供了更强的后续优化基础，稳定了优化过程并带来更好的泛化性能，同时为后续训练阶段留出优化空间

Abstract: Supervised fine-tuning (SFT) of foundation models often leads to poor
generalization, where prior capabilities deteriorate after tuning on new tasks
or domains. Inspired by trust-region policy optimization (TRPO) and proximal
policy optimization (PPO) in reinforcement learning (RL), we propose Proximal
SFT (PSFT). This fine-tuning objective incorporates the benefits of
trust-region, effectively constraining policy drift during SFT while
maintaining competitive tuning. By viewing SFT as a special case of policy
gradient methods with constant positive advantages, we derive PSFT that
stabilizes optimization and leads to generalization, while leaving room for
further optimization in subsequent post-training stages. Experiments across
mathematical and human-value domains show that PSFT matches SFT in-domain,
outperforms it in out-of-domain generalization, remains stable under prolonged
training without causing entropy collapse, and provides a stronger foundation
for the subsequent optimization.

</details>


### [150] [Multi-domain Distribution Learning for De Novo Drug Design](https://arxiv.org/abs/2508.17815)
*Arne Schneuing,Ilia Igashov,Adrian W. Dobbelstein,Thomas Castiglione,Michael Bronstein,Bruno Correia*

Main category: cs.LG

TL;DR: DrugFlow是一个基于结构的药物设计生成模型，结合连续流匹配和离散马尔可夫桥，在三维蛋白质-配体数据学习方面达到最先进性能，并提供不确定性估计和偏好对齐采样方案。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够同时学习化学、几何和物理特性的生成模型，用于基于结构的药物设计，并解决分布外样本检测和采样优化问题。

Method: 整合连续流匹配和离散马尔可夫桥方法，提供不确定性估计机制，提出联合偏好对齐方案，并扩展模型以联合采样蛋白质侧链角度和分子。

Result: 在三维蛋白质-配体数据学习方面表现出最先进的性能，能够检测分布外样本，并通过偏好对齐增强采样过程。

Conclusion: DrugFlow是一个有效的结构基药物设计生成模型，通过整合多种技术实现了全面的性能提升，并具有扩展到蛋白质构象探索的能力。

Abstract: We introduce DrugFlow, a generative model for structure-based drug design
that integrates continuous flow matching with discrete Markov bridges,
demonstrating state-of-the-art performance in learning chemical, geometric, and
physical aspects of three-dimensional protein-ligand data. We endow DrugFlow
with an uncertainty estimate that is able to detect out-of-distribution
samples. To further enhance the sampling process towards distribution regions
with desirable metric values, we propose a joint preference alignment scheme
applicable to both flow matching and Markov bridge frameworks. Furthermore, we
extend our model to also explore the conformational landscape of the protein by
jointly sampling side chain angles and molecules.

</details>


### [151] [Limitations of Normalization in Attention Mechanism](https://arxiv.org/abs/2508.17821)
*Timur Mudarisov,Mikhail Burtsev,Tatiana Petrova,Radu State*

Main category: cs.LG

TL;DR: 本文研究了注意力机制中归一化的局限性，通过理论分析和实验验证发现随着选择token数量增加，模型区分信息性token的能力下降，且softmax归一化在低温度设置下存在梯度敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制中softmax归一化的理论局限性，特别是在token选择和区分能力方面的不足，为改进注意力架构提供理论基础。

Method: 建立理论分析框架评估模型选择能力和几何分离特性，通过预训练GPT-2模型进行实验验证，分析距离界限和分离标准。

Result: 随着选择token数量增加，模型区分信息性token的能力显著下降，趋向均匀选择模式；softmax归一化在低温度设置下存在梯度敏感性训练挑战。

Conclusion: 研究结果深化了对softmax注意力机制的理解，表明需要开发更鲁棒的归一化和选择策略来改进未来注意力架构。

Abstract: This paper investigates the limitations of the normalization in attention
mechanisms. We begin with a theoretical framework that enables the
identification of the model's selective ability and the geometric separation
involved in token selection. Our analysis includes explicit bounds on distances
and separation criteria for token vectors under softmax scaling. Through
experiments with pre-trained GPT-2 model, we empirically validate our
theoretical results and analyze key behaviors of the attention mechanism.
Notably, we demonstrate that as the number of selected tokens increases, the
model's ability to distinguish informative tokens declines, often converging
toward a uniform selection pattern. We also show that gradient sensitivity
under softmax normalization presents challenges during training, especially at
low temperature settings. These findings advance current understanding of
softmax-based attention mechanism and motivate the need for more robust
normalization and selection strategies in future attention architectures.

</details>


### [152] [Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning in LLMs](https://arxiv.org/abs/2508.17850)
*Han Zhang,Ruibin Zheng,Zexuan Yi,Hanyang Peng,Hui Wang,Yue Yu*

Main category: cs.LG

TL;DR: HeteroRL异步强化学习架构，通过解耦采样和学习来解决分布式环境中网络延迟导致的KL散度问题，GEPO方法实现指数级方差减少，在1800秒延迟下性能下降小于3%


<details>
  <summary>Details</summary>
Motivation: 随着单中心计算面临功耗限制，去中心化训练变得至关重要。强化学习后训练能增强大语言模型，但在异构分布式环境中面临紧密耦合的采样-学习交替带来的挑战

Method: 提出HeteroRL异步RL架构，将rollout采样与参数学习解耦；提出Group Expectation Policy Optimization (GEPO)，通过改进的采样机制降低重要性权重方差

Result: 理论证明GEPO实现指数级方差减少；实验显示在1800秒延迟下性能下降小于3%，相比GRPO等方法保持优异稳定性

Conclusion: 该方法在异构网络中展示了去中心化RL的强大潜力，能够有效应对网络延迟导致的KL散度和重要性采样失败问题

Abstract: As single-center computing approaches power constraints, decentralized
training is becoming essential. Reinforcement Learning (RL) post-training
enhances Large Language Models (LLMs) but faces challenges in heterogeneous
distributed environments due to its tightly-coupled sampling-learning
alternation. We propose HeteroRL, an asynchronous RL architecture that
decouples rollout sampling from parameter learning, enabling robust deployment
across geographically distributed nodes under network delays. We identify that
latency-induced KL divergence causes importance sampling failure due to high
variance. To address this, we propose Group Expectation Policy Optimization
(GEPO), which reduces importance weight variance through a refined sampling
mechanism. Theoretically, GEPO achieves exponential variance reduction.
Experiments show it maintains superior stability over methods like GRPO, with
less than 3% performance degradation under 1800-second delays, demonstrating
strong potential for decentralized RL in heterogeneous networks.

</details>


### [153] [Ada-TransGNN: An Air Quality Prediction Model Based On Adaptive Graph Convolutional Networks](https://arxiv.org/abs/2508.17867)
*Dan Wang,Feng Jiang,Zhanquan Wang*

Main category: cs.LG

TL;DR: 提出基于Transformer的Ada-TransGNN模型，集成全局空间语义和时间行为，通过自适应图结构学习和辅助任务学习模块，显著提升空气质量预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有空气质量预测模型存在预测精度低、实时更新慢、结果滞后等问题，需要开发更准确高效的时空数据预测方法。

Method: 结合多头注意力机制和图卷积网络构建时空块集，提出自适应图结构学习模块和辅助任务学习模块，动态提取时空依赖特征并学习最优图结构。

Result: 在基准数据集和新数据集(Mete-air)上验证，模型在短期和长期预测中均优于现有最先进预测模型。

Conclusion: Ada-TransGNN模型通过有效整合时空特征和自适应学习机制，显著提高了空气质量预测的准确性和实时性。

Abstract: Accurate air quality prediction is becoming increasingly important in the
environmental field. To address issues such as low prediction accuracy and slow
real-time updates in existing models, which lead to lagging prediction results,
we propose a Transformer-based spatiotemporal data prediction method
(Ada-TransGNN) that integrates global spatial semantics and temporal behavior.
The model constructs an efficient and collaborative spatiotemporal block set
comprising a multi-head attention mechanism and a graph convolutional network
to extract dynamically changing spatiotemporal dependency features from complex
air quality monitoring data. Considering the interaction relationships between
different monitoring points, we propose an adaptive graph structure learning
module, which combines spatiotemporal dependency features in a data-driven
manner to learn the optimal graph structure, thereby more accurately capturing
the spatial relationships between monitoring points. Additionally, we design an
auxiliary task learning module that enhances the decoding capability of
temporal relationships by integrating spatial context information into the
optimal graph structure representation, effectively improving the accuracy of
prediction results. We conducted comprehensive evaluations on a benchmark
dataset and a novel dataset (Mete-air). The results demonstrate that our model
outperforms existing state-of-the-art prediction models in short-term and
long-term predictions.

</details>


### [154] [Spectrum Prediction in the Fractional Fourier Domain with Adaptive Filtering](https://arxiv.org/abs/2508.17872)
*Yanghao Qin,Bo Zhou,Guangliang Pan,Qihui Wu,Meixia Tao*

Main category: cs.LG

TL;DR: 提出了SFFP框架，通过分数傅里叶变换和自适应滤波来分离频谱数据中的可预测趋势和噪声，使用复数神经网络进行预测，在真实频谱数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 频谱数据具有独特特性，现有基于时域或频域的方法难以有效分离可预测模式与噪声，影响频谱预测的准确性。

Method: SFFP框架包含三个模块：自适应分数傅里叶变换模块将数据转换到合适分数域；自适应滤波模块选择性抑制噪声；复数神经网络预测模块学习并预测滤波后的趋势成分。

Result: 在真实世界频谱数据上的实验表明，SFFP框架优于领先的频谱预测和通用预测方法。

Conclusion: SFFP框架通过分数域变换和自适应滤波有效解决了频谱数据中模式与噪声分离的难题，提高了频谱预测的准确性。

Abstract: Accurate spectrum prediction is crucial for dynamic spectrum access (DSA) and
resource allocation. However, due to the unique characteristics of spectrum
data, existing methods based on the time or frequency domain often struggle to
separate predictable patterns from noise. To address this, we propose the
Spectral Fractional Filtering and Prediction (SFFP) framework. SFFP first
employs an adaptive fractional Fourier transform (FrFT) module to transform
spectrum data into a suitable fractional Fourier domain, enhancing the
separability of predictable trends from noise. Subsequently, an adaptive Filter
module selectively suppresses noise while preserving critical predictive
features within this domain. Finally, a prediction module, leveraging a
complex-valued neural network, learns and forecasts these filtered trend
components. Experiments on real-world spectrum data show that the SFFP
outperforms leading spectrum and general forecasting methods.

</details>


### [155] [Riemannian Optimization for LoRA on the Stiefel Manifold](https://arxiv.org/abs/2508.17901)
*Juneyoung Park,Minjae Kang,Seongbae Lee,Haegang Lee,Seongwan Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: 提出基于Stiefel流形的优化器，通过正交约束解决LoRA中B矩阵的基冗余问题，显著提升参数效率和表示能力


<details>
  <summary>Details</summary>
Motivation: 大型语言模型微调面临参数效率挑战，现有PEFT方法如LoRA存在优化器效率问题，特别是AdamW在B矩阵中产生基冗余，限制了性能表现

Method: 在Stiefel流形上优化B矩阵，施加显式正交约束，实现近乎完美的正交性和完全有效秩，采用几何优化方法

Result: Stiefel优化器在LoRA和DoRA基准测试中一致优于AdamW，显著提升了参数效率和表示容量

Conclusion: 几何约束是释放LoRA全部潜力的关键，为大型语言模型的有效微调提供了新的优化方案

Abstract: While powerful, large language models (LLMs) present significant fine-tuning
challenges due to their size. Parameter-efficient fine-tuning (PEFT) methods
like LoRA provide solutions, yet suffer from critical optimizer inefficiencies;
notably basis redundancy in LoRA's $B$ matrix when using AdamW, which
fundamentally limits performance. We address this by optimizing the $B$ matrix
on the Stiefel manifold, imposing explicit orthogonality constraints that
achieve near-perfect orthogonality and full effective rank. This geometric
approach dramatically enhances parameter efficiency and representational
capacity. Our Stiefel optimizer consistently outperforms AdamW across
benchmarks with both LoRA and DoRA, demonstrating that geometric constraints
are the key to unlocking LoRA's full potential for effective LLM fine-tuning.

</details>


### [156] [Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets](https://arxiv.org/abs/2508.17930)
*Sarina Penquitt,Tobias Riedlinger,Timo Heller,Markus Reischl,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出了一种统一的标签错误检测方法，通过注入标签错误并构建实例分割问题来检测目标检测、语义分割和实例分割数据集中的标注错误。


<details>
  <summary>Details</summary>
Motivation: 当前标签错误检测方法通常只针对单一计算机视觉任务，且不是基于学习的。错误标注数据会导致模型性能下降、基准测试结果偏差和整体准确率降低。

Method: 通过向真实标注中注入不同类型的标签错误，将标签错误检测问题构建为基于复合输入的实例分割问题，实现跨任务的统一检测。

Result: 在多个任务、数据集和基础模型上进行了实验，与各任务领域的最先进方法进行比较，并在Cityscapes数据集中发现了459个真实标签错误。

Conclusion: 该方法能够有效检测多种计算机视觉任务中的标签错误，提供了统一的解决方案，并发布了真实标签错误的基准数据集。

Abstract: Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.

</details>


### [157] [Choice Outweighs Effort: Facilitating Complementary Knowledge Fusion in Federated Learning via Re-calibration and Merit-discrimination](https://arxiv.org/abs/2508.17954)
*Ming Yang,Dongrun Li,Xin Wang,Xiaoyang Yu,Xiaoming Wu,Shibo He*

Main category: cs.LG

TL;DR: FedMate通过双边优化解决联邦学习中的数据异构性问题，在服务器端构建动态全局原型，在客户端采用互补分类融合，在五个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的跨客户端数据异构性导致偏见，阻碍无偏共识凝聚和泛化-个性化知识的互补融合。现有方法依赖静态指标和刚性全局对齐，导致共识扭曲和模型适应性下降。

Method: 双边优化方法：服务器端构建动态全局原型，聚合权重通过样本大小、当前参数和未来预测综合校准；客户端采用互补分类融合实现基于优点的判别训练，并引入成本感知特征传输平衡性能与通信效率。

Result: 在五个不同复杂度数据集上的实验表明，FedMate在协调泛化和适应性方面优于最先进方法。自动驾驶数据集上的语义分割实验验证了方法的实际可扩展性。

Conclusion: FedMate通过动态原型构建和互补融合机制，有效解决了联邦学习中的数据异构性问题，实现了更好的泛化-个性化平衡，具有实际应用价值。

Abstract: Cross-client data heterogeneity in federated learning induces biases that
impede unbiased consensus condensation and the complementary fusion of
generalization- and personalization-oriented knowledge. While existing
approaches mitigate heterogeneity through model decoupling and representation
center loss, they often rely on static and restricted metrics to evaluate local
knowledge and adopt global alignment too rigidly, leading to consensus
distortion and diminished model adaptability. To address these limitations, we
propose FedMate, a method that implements bilateral optimization: On the server
side, we construct a dynamic global prototype, with aggregation weights
calibrated by holistic integration of sample size, current parameters, and
future prediction; a category-wise classifier is then fine-tuned using this
prototype to preserve global consistency. On the client side, we introduce
complementary classification fusion to enable merit-based discrimination
training and incorporate cost-aware feature transmission to balance model
performance and communication efficiency. Experiments on five datasets of
varying complexity demonstrate that FedMate outperforms state-of-the-art
methods in harmonizing generalization and adaptation. Additionally, semantic
segmentation experiments on autonomous driving datasets validate the method's
real-world scalability.

</details>


### [158] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出生成式特征填补框架，通过空间错误集中分组、扩散模型特征重建和语义感知功率分配，提升语义通信在数字系统中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 语义通信在6G网络中具有高效通信潜力，但在数字系统中部署时面临传输错误导致语义内容失真的挑战，需要确保对关键语义内容的鲁棒性

Method: 1. 空间错误集中分组策略：基于信道映射编码特征元素，集中特征失真；2. 生成式特征填补方法：使用扩散模型高效重建因丢包缺失的特征；3. 语义感知功率分配：根据语义重要性进行不等差错保护的功率分配

Result: 实验结果表明，在块衰落条件下，该框架优于传统的深度联合信源信道编码(DJSCC)和JPEG2000方法，实现了更高的语义准确性和更低的LPIPS分数

Conclusion: 所提出的生成式特征填补框架有效解决了语义通信在数字系统中的鲁棒性问题，为6G网络中语义通信的实际部署提供了可行的解决方案

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for
achieving unprecedented communication efficiency in sixth-generation (6G)
networks by leveraging artificial intelligence (AI) to extract and transmit the
underlying meanings of source data. However, deploying SemCom over digital
systems presents new challenges, particularly in ensuring robustness against
transmission errors that may distort semantically critical content. To address
this issue, this paper proposes a novel framework, termed generative feature
imputing, which comprises three key techniques. First, we introduce a spatial
error concentration packetization strategy that spatially concentrates feature
distortions by encoding feature elements based on their channel mappings, a
property crucial for both the effectiveness and reduced complexity of the
subsequent techniques. Second, building on this strategy, we propose a
generative feature imputing method that utilizes a diffusion model to
efficiently reconstruct missing features caused by packet losses. Finally, we
develop a semantic-aware power allocation scheme that enables unequal error
protection by allocating transmission power according to the semantic
importance of each packet. Experimental results demonstrate that the proposed
framework outperforms conventional approaches, such as Deep Joint
Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,
achieving higher semantic accuracy and lower Learned Perceptual Image Patch
Similarity (LPIPS) scores.

</details>


### [159] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: 提出基于神经网络的拓扑感知时间变化标量场插值方法，通过持久图和时间关键帧学习时空关系，实现非关键帧数据的准确重建


<details>
  <summary>Details</summary>
Motivation: 解决时间变化标量场数据中非关键帧缺失的问题，传统插值方法难以保持数据的拓扑结构完整性，需要一种能够同时保持几何和拓扑特征的插值方案

Method: 使用神经网络架构学习时间值与对应标量场的关系，通过拓扑损失函数利用输入的持久图信息，在查询时通过单次网络传播快速生成插值结果

Result: 在2D和3D时间变化数据集上的实验表明，该方法在数据和拓扑拟合方面均优于参考插值方案，能够瞬时生成高质量的插值输出

Conclusion: 该方法成功实现了拓扑感知的时间插值，通过结合神经网络和拓扑损失，在保持几何精度的同时有效维护了数据的拓扑结构，为时间变化数据的处理提供了有效解决方案

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [160] [Does simple trump complex? Comparing strategies for adversarial robustness in DNNs](https://arxiv.org/abs/2508.18019)
*William Brooks,Marelie H. Davel,Coenraad Mouton*

Main category: cs.LG

TL;DR: 本研究通过分析两种对抗训练方法（简单损失函数修改和复杂的Dynamics-Aware Robust Training）的组件，识别出对增强深度神经网络对抗鲁棒性最有效的元素，重点关注输入空间中的边界距离。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在多种应用中表现出色，但仍易受对抗攻击。研究旨在确定不同对抗训练技术中哪些组件对提高对抗鲁棒性贡献最大，特别是通过输入空间边界距离的视角。

Method: 使用VGG-16模型作为基础，系统性地分离和评估两种方法的各个组件：简单的损失函数修改方法和复杂的Dynamics-Aware Robust Training方法。在CIFAR-10数据集上使用AutoAttack和PGD等对抗攻击评估每个组件的影响。

Result: 分析揭示了哪些元素最有效地增强对抗鲁棒性，为设计更鲁棒的深度神经网络提供了见解。

Conclusion: 研究通过组件隔离分析，确定了对抗训练方法中最关键的鲁棒性增强元素，为开发更有效的对抗防御策略提供了指导。

Abstract: Deep Neural Networks (DNNs) have shown substantial success in various
applications but remain vulnerable to adversarial attacks. This study aims to
identify and isolate the components of two different adversarial training
techniques that contribute most to increased adversarial robustness,
particularly through the lens of margins in the input space -- the minimal
distance between data points and decision boundaries. Specifically, we compare
two methods that maximize margins: a simple approach which modifies the loss
function to increase an approximation of the margin, and a more complex
state-of-the-art method (Dynamics-Aware Robust Training) which builds upon this
approach. Using a VGG-16 model as our base, we systematically isolate and
evaluate individual components from these methods to determine their relative
impact on adversarial robustness. We assess the effect of each component on the
model's performance under various adversarial attacks, including AutoAttack and
Projected Gradient Descent (PGD). Our analysis on the CIFAR-10 dataset reveals
which elements most effectively enhance adversarial robustness, providing
insights for designing more robust DNNs.

</details>


### [161] [Training Transformers for Mesh-Based Simulations](https://arxiv.org/abs/2508.18051)
*Paul Garnier,Vincent Lannelongue,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: 通过使用邻接矩阵作为注意力掩码的图Transformer架构，提出了一种高效可扩展的图神经网络方法，在流体力学模拟中实现了更小模型尺寸和更快速度的同时超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 传统消息传递图神经网络在处理大规模复杂网格时遇到扩展性和效率挑战，并且现有的多级网格和K-hop聚合方法往往引入太多复杂性且研究不深入。

Method: 提出了一种新的图Transformer架构，利用邻接矩阵作为注意力掩码，统计了扩张滑动窗口和全局注意力等创新增强技术来扩大感矩野而不损失计算效率。

Result: 在具有达30万节点和300万边的网格上表现出显著的扩展性。最小模型在性能与MeshGraphNet相当的情况下，速度提升7倍、模型尺寸减少6倍。最大模型在平均性能上超过之前最佳方法38.8%，在全滚动RMSE上超过MeshGraphNet 52%，而训练速度相似。

Conclusion: 该图Transformer方法为物理模拟领域提供了一种高效、可扩展的解决方案，能够处理极大规模网格数据，同时在性能、速度和模型尺寸方面都实现了显著收益。

Abstract: Simulating physics using Graph Neural Networks (GNNs) is predominantly driven
by message-passing architectures, which face challenges in scaling and
efficiency, particularly in handling large, complex meshes. These architectures
have inspired numerous enhancements, including multigrid approaches and $K$-hop
aggregation (using neighbours of distance $K$), yet they often introduce
significant complexity and suffer from limited in-depth investigations. In
response to these challenges, we propose a novel Graph Transformer architecture
that leverages the adjacency matrix as an attention mask. The proposed approach
incorporates innovative augmentations, including Dilated Sliding Windows and
Global Attention, to extend receptive fields without sacrificing computational
efficiency. Through extensive experimentation, we evaluate model size,
adjacency matrix augmentations, positional encoding and $K$-hop configurations
using challenging 3D computational fluid dynamics (CFD) datasets. We also train
over 60 models to find a scaling law between training FLOPs and parameters. The
introduced models demonstrate remarkable scalability, performing on meshes with
up to 300k nodes and 3 million edges. Notably, the smallest model achieves
parity with MeshGraphNet while being $7\times$ faster and $6\times$ smaller.
The largest model surpasses the previous state-of-the-art by $38.8$\% on
average and outperforms MeshGraphNet by $52$\% on the all-rollout RMSE, while
having a similar training speed. Code and datasets are available at
https://github.com/DonsetPG/graph-physics.

</details>


### [162] [Weisfeiler-Lehman meets Events: An Expressivity Analysis for Continuous-Time Dynamic Graph Neural Networks](https://arxiv.org/abs/2508.18052)
*Silvia Beddar-Wiesing,Alice Moallemy-Oureh*

Main category: cs.LG

TL;DR: 本文扩展了图神经网络理论，从离散动态图到连续时间动态图，提出了连续时间动态1-WL测试和对应的CGNN架构，保持了区分能力和通用逼近保证。


<details>
  <summary>Details</summary>
Motivation: 现实世界的系统（如通信网络、金融交易网络）通常以异步方式演化并可能分裂成断开连接的组件，而现有的GNN理论仅限于离散动态图序列，无法处理连续时间动态图和任意连接性的情况。

Method: 引入连续时间动态1-WL测试，证明其与连续时间动态展开树的等价性，基于离散动态GNN架构设计连续时间动态GNN（CGNN），使用分段连续可微时间函数处理异步断开图。

Result: 建立了连续时间动态图的等效理论框架，识别出保留区分能力和通用逼近保证的CGNN类别，提供了实用的设计指南。

Conclusion: 成功将GNN理论扩展到连续时间动态图领域，为处理现实世界中异步演化和断开连接的图系统提供了理论基础和实践指导。

Abstract: Graph Neural Networks (GNNs) are known to match the distinguishing power of
the 1-Weisfeiler-Lehman (1-WL) test, and the resulting partitions coincide with
the unfolding tree equivalence classes of graphs. Preserving this equivalence,
GNNs can universally approximate any target function on graphs in probability
up to any precision. However, these results are limited to attributed
discrete-dynamic graphs represented as sequences of connected graph snapshots.
Real-world systems, such as communication networks, financial transaction
networks, and molecular interactions, evolve asynchronously and may split into
disconnected components. In this paper, we extend the theory of attributed
discrete-dynamic graphs to attributed continuous-time dynamic graphs with
arbitrary connectivity. To this end, we introduce a continuous-time dynamic
1-WL test, prove its equivalence to continuous-time dynamic unfolding trees,
and identify a class of continuous-time dynamic GNNs (CGNNs) based on
discrete-dynamic GNN architectures that retain both distinguishing power and
universal approximation guarantees. Our constructive proofs further yield
practical design guidelines, emphasizing a compact and expressive CGNN
architecture with piece-wise continuously differentiable temporal functions to
process asynchronous, disconnected graphs.

</details>


### [163] [FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning](https://arxiv.org/abs/2508.18060)
*Emmanouil Kritharakis,Antonios Makris,Dusan Jakovetic,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: FedGreed是一种针对联邦学习的鲁棒聚合策略，通过基于服务器可信数据集的损失评估来排序和选择客户端更新，无需假设对抗性参与者的比例，在非IID数据分布下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端可能表现出拜占庭攻击行为的问题，同时处理现实部署中普遍存在的异构数据分布挑战。

Method: 提出FedGreed方法：基于服务器可信数据集评估客户端本地模型更新的损失指标，按损失排序并贪婪选择损失最小的客户端子集进行聚合。

Result: 在MNIST、FMNIST和CIFAR-10数据集上的实验表明，FedGreed在大多数对抗场景中显著优于标准方法和鲁棒基线方法，包括均值、修剪均值、中位数、Krum和Multi-Krum等方法。

Conclusion: FedGreed具有收敛保证和有界最优性差距，在强对抗行为和异构数据分布下都能可靠运行，是联邦学习中有效的鲁棒聚合解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients while preserving data privacy by keeping local datasets on-device. In
this work, we address FL settings where clients may behave adversarially,
exhibiting Byzantine attacks, while the central server is trusted and equipped
with a reference dataset. We propose FedGreed, a resilient aggregation strategy
for federated learning that does not require any assumptions about the fraction
of adversarial participants. FedGreed orders clients' local model updates based
on their loss metrics evaluated against a trusted dataset on the server and
greedily selects a subset of clients whose models exhibit the minimal
evaluation loss. Unlike many existing approaches, our method is designed to
operate reliably under heterogeneous (non-IID) data distributions, which are
prevalent in real-world deployments. FedGreed exhibits convergence guarantees
and bounded optimality gaps under strong adversarial behavior. Experimental
evaluations on MNIST, FMNIST, and CIFAR-10 demonstrate that our method
significantly outperforms standard and robust federated learning baselines,
such as Mean, Trimmed Mean, Median, Krum, and Multi-Krum, in the majority of
adversarial scenarios considered, including label flipping and Gaussian noise
injection attacks. All experiments were conducted using the Flower federated
learning framework.

</details>


### [164] [Quantum-Classical Hybrid Framework for Zero-Day Time-Push GNSS Spoofing Detection](https://arxiv.org/abs/2508.18085)
*Abyad Enan,Mashrur Chowdhury,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 提出了一种基于混合量子-经典自编码器(HQC-AE)的零日GNSS欺骗检测方法，仅使用真实信号训练，无需欺骗数据，在静态接收器中有效检测时间推送欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: GNSS系统对欺骗攻击高度脆弱，现有监督学习方法无法检测新型和未见过的攻击，需要开发能够主动检测零日攻击的方法。

Method: 使用混合量子-经典自编码器(HQC-AE)，仅基于真实GNSS信号进行训练，利用跟踪阶段提取的特征，在PNT解算前实现主动检测。

Result: HQC-AE在检测未见时间推送欺骗攻击方面表现优异，平均检测准确率达97.71%，误报率0.62%；对复杂攻击准确率达98.23%，误报率1.85%。

Conclusion: 该方法在多种静态GNSS接收器平台上都能有效主动检测零日时间推送欺骗攻击，性能优于传统监督学习和现有无监督学习方法。

Abstract: Global Navigation Satellite Systems (GNSS) are critical for Positioning,
Navigation, and Timing (PNT) applications. However, GNSS are highly vulnerable
to spoofing attacks, where adversaries transmit counterfeit signals to mislead
receivers. Such attacks can lead to severe consequences, including misdirected
navigation, compromised data integrity, and operational disruptions. Most
existing spoofing detection methods depend on supervised learning techniques
and struggle to detect novel, evolved, and unseen attacks. To overcome this
limitation, we develop a zero-day spoofing detection method using a Hybrid
Quantum-Classical Autoencoder (HQC-AE), trained solely on authentic GNSS
signals without exposure to spoofed data. By leveraging features extracted
during the tracking stage, our method enables proactive detection before PNT
solutions are computed. We focus on spoofing detection in static GNSS
receivers, which are particularly susceptible to time-push spoofing attacks,
where attackers manipulate timing information to induce incorrect time
computations at the receiver. We evaluate our model against different unseen
time-push spoofing attack scenarios: simplistic, intermediate, and
sophisticated. Our analysis demonstrates that the HQC-AE consistently
outperforms its classical counterpart, traditional supervised learning-based
models, and existing unsupervised learning-based methods in detecting zero-day,
unseen GNSS time-push spoofing attacks, achieving an average detection accuracy
of 97.71% with an average false negative rate of 0.62% (when an attack occurs
but is not detected). For sophisticated spoofing attacks, the HQC-AE attains an
accuracy of 98.23% with a false negative rate of 1.85%. These findings
highlight the effectiveness of our method in proactively detecting zero-day
GNSS time-push spoofing attacks across various stationary GNSS receiver
platforms.

</details>


### [165] [Provable Mixed-Noise Learning with Flow-Matching](https://arxiv.org/abs/2508.18122)
*Paul Hagemann,Robert Gruhlke,Bernhard Stankewitz,Claudia Schillings,Gabriele Steidl*

Main category: cs.LG

TL;DR: 提出基于条件流匹配和EM算法的混合噪声贝叶斯反问题推断框架，能够联合估计后验采样器和噪声参数，在高维场景下具有良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现实应用（特别是物理和化学领域）中的噪声通常具有未知和异质性结构，而传统方法往往假设固定或已知的噪声特性，无法有效处理混合噪声问题。

Method: 将条件流匹配嵌入期望最大化(EM)算法中，在E步使用基于ODE的无模拟流匹配作为生成模型，联合估计后验采样器和噪声参数。

Result: 理论证明在无限观测的总体极限下，EM更新收敛到真实噪声参数；数值结果验证了该方法在混合噪声贝叶斯反问题中的有效性。

Conclusion: 流匹配与EM推断的结合为处理混合噪声贝叶斯反问题提供了有效的解决方案，特别是在高维场景下表现出良好的可扩展性。

Abstract: We study Bayesian inverse problems with mixed noise, modeled as a combination
of additive and multiplicative Gaussian components. While traditional inference
methods often assume fixed or known noise characteristics, real-world
applications, particularly in physics and chemistry, frequently involve noise
with unknown and heterogeneous structure. Motivated by recent advances in
flow-based generative modeling, we propose a novel inference framework based on
conditional flow matching embedded within an Expectation-Maximization (EM)
algorithm to jointly estimate posterior samplers and noise parameters. To
enable high-dimensional inference and improve scalability, we use
simulation-free ODE-based flow matching as the generative model in the E-step
of the EM algorithm. We prove that, under suitable assumptions, the EM updates
converge to the true noise parameters in the population limit of infinite
observations. Our numerical results illustrate the effectiveness of combining
EM inference with flow matching for mixed-noise Bayesian inverse problems.

</details>


### [166] [CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics](https://arxiv.org/abs/2508.18124)
*Weida Wang,Dongchen Huang,Jiatong Li,Tengchao Yang,Ziyang Zheng,Di Zhang,Dong Han,Benteng Chen,Binzhao Luo,Zhiyu Liu,Kunling Liu,Zhiyuan Gao,Shiqi Geng,Wei Ma,Jiaming Su,Xin Li,Shuchen Pu,Yuhan Shui,Qianjia Cheng,Zhihao Dou,Dongfei Cui,Changyong He,Jin Zeng,Zeke Xie,Mao Su,Dongzhan Zhou,Yuqiang Li,Wanli Ouyang,Lei Bai,Yunqi Cai,Xi Dai,Shufei Zhang,Jinguang Cheng,Zhong Fang,Hongming Weng*

Main category: cs.LG

TL;DR: CMPhysBench是一个新的基准测试，专门评估大语言模型在凝聚态物理领域的计算问题解决能力，包含520多个研究生水平的问题，并提出了SEED评分系统进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在传统物理领域表现良好，但在凝聚态物理这种前沿实践领域的表现尚不清楚，需要专门的评估基准来测试其在该领域的真实能力。

Method: 构建包含520多个研究生级别计算问题的基准测试，涵盖磁学、超导、强关联系统等子领域。引入基于树结构的可扩展表达式编辑距离(SEED)评分系统，提供非二进制的细粒度部分评分。

Result: 最好的模型Grok-4在CMPhysBench上仅获得36的平均SEED分数和28%的准确率，显示出大语言模型在凝聚态物理领域存在显著的能力差距。

Conclusion: 大语言模型在凝聚态物理这一前沿实践领域的表现远不如传统物理领域，需要进一步改进和专门训练才能达到实用水平。

Abstract: We introduce CMPhysBench, designed to assess the proficiency of Large
Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.
CMPhysBench is composed of more than 520 graduate-level meticulously curated
questions covering both representative subfields and foundational theoretical
frameworks of condensed matter physics, such as magnetism, superconductivity,
strongly correlated systems, etc. To ensure a deep understanding of the
problem-solving process,we focus exclusively on calculation problems, requiring
LLMs to independently generate comprehensive solutions. Meanwhile, leveraging
tree-based representations of expressions, we introduce the Scalable Expression
Edit Distance (SEED) score, which provides fine-grained (non-binary) partial
credit and yields a more accurate assessment of similarity between prediction
and ground-truth. Our results show that even the best models, Grok-4, reach
only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a
significant capability gap, especially for this practical and frontier domain
relative to traditional physics. The code anddataset are publicly available at
https://github.com/CMPhysBench/CMPhysBench.

</details>


### [167] [Frozen in Time: Parameter-Efficient Time Series Transformers via Reservoir-Induced Feature Expansion and Fixed Random Dynamics](https://arxiv.org/abs/2508.18130)
*Pradeep Singh,Mehak Sharma,Anupriya Dey,Balasubramanian Raman*

Main category: cs.LG

TL;DR: FreezeTST是一种轻量级混合模型，将冻结的随机特征（水库）块与标准可训练Transformer层交错结合，在保持推理复杂度不变的同时显著降低训练成本和参数数量，在长期时间序列预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在序列建模中存在二次自注意力计算复杂度和弱时间偏置的问题，导致长程预测既昂贵又脆弱，需要更高效的解决方案。

Method: 采用混合架构设计：冻结的随机特征块提供丰富的非线性记忆而无需优化成本，可训练的Transformer层通过自注意力学习查询这些记忆，大幅减少可训练参数和训练时间。

Result: 在7个标准长期预测基准测试中，FreezeTST始终匹配或超越Informer、Autoformer、PatchTST等专门变体，且计算成本显著降低。

Conclusion: 将水库原理嵌入Transformer中为高效长期时间序列预测提供了一条简单而原则性的途径，证明了混合架构的有效性。

Abstract: Transformers are the de-facto choice for sequence modelling, yet their
quadratic self-attention and weak temporal bias can make long-range forecasting
both expensive and brittle. We introduce FreezeTST, a lightweight hybrid that
interleaves frozen random-feature (reservoir) blocks with standard trainable
Transformer layers. The frozen blocks endow the network with rich nonlinear
memory at no optimisation cost; the trainable layers learn to query this memory
through self-attention. The design cuts trainable parameters and also lowers
wall-clock training time, while leaving inference complexity unchanged. On
seven standard long-term forecasting benchmarks, FreezeTST consistently matches
or surpasses specialised variants such as Informer, Autoformer, and PatchTST;
with substantially lower compute. Our results show that embedding reservoir
principles within Transformers offers a simple, principled route to efficient
long-term time-series prediction.

</details>


### [168] [Unveiling the Actual Performance of Neural-based Models for Equation Discovery on Graph Dynamical Systems](https://arxiv.org/abs/2508.18173)
*Riccardo Cappi,Paolo Frazzetto,Nicolò Navarin,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 本文比较了符号回归技术在网络动态系统方程发现中的表现，包括稀疏回归、MLP架构和新提出的图KANs方法，发现MLP和KANs都能成功识别符号方程，且KANs具有更好的简洁性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型的"黑箱"特性阻碍了其在需要可解释性的科学发现中的应用，特别是在网络动态系统方程发现中，拓扑结构进一步增加了复杂性。

Method: 评估了稀疏回归和MLP架构等现有方法，并引入了图Kolmogorov-Arnold Networks (KANs)的新颖适配，利用其固有的可解释性优势。

Result: 在合成和真实动态系统测试中，MLP和KANs架构都能成功识别底层符号方程，显著超越现有基线。KANs以更大的简洁性和透明度实现这一性能。

Conclusion: 研究为研究人员提供了实用指南，阐明了模型表达能力和可解释性之间的权衡，并确立了基于神经网络的架构在复杂系统稳健科学发现中的可行性。

Abstract: The ``black-box'' nature of deep learning models presents a significant
barrier to their adoption for scientific discovery, where interpretability is
paramount. This challenge is especially pronounced in discovering the governing
equations of dynamical processes on networks or graphs, since even their
topological structure further affects the processes' behavior. This paper
provides a rigorous, comparative assessment of state-of-the-art symbolic
regression techniques for this task. We evaluate established methods, including
sparse regression and MLP-based architectures, and introduce a novel adaptation
of Kolmogorov-Arnold Networks (KANs) for graphs, designed to exploit their
inherent interpretability. Across a suite of synthetic and real-world dynamical
systems, our results demonstrate that both MLP and KAN-based architectures can
successfully identify the underlying symbolic equations, significantly
surpassing existing baselines. Critically, we show that KANs achieve this
performance with greater parsimony and transparency, as their learnable
activation functions provide a clearer mapping to the true physical dynamics.
This study offers a practical guide for researchers, clarifying the trade-offs
between model expressivity and interpretability, and establishes the viability
of neural-based architectures for robust scientific discovery on complex
systems.

</details>


### [169] [Amortized Sampling with Transferable Normalizing Flows](https://arxiv.org/abs/2508.18175)
*Charlie B. Tan,Majdi Hassan,Leon Klein,Saifuddin Syed,Dominique Beaini,Michael M. Bronstein,Alexander Tong,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 提出了Prose，一个2.8亿参数的可迁移归一化流模型，用于零样本采样任意肽系统，实现跨序列长度的迁移性，并通过重要性采样微调在四肽上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学和MCMC方法缺乏摊销性，每个系统都需要重新计算采样成本。现有学习采样器在跨系统迁移方面能力有限。

Method: 开发Prose可迁移归一化流模型，在肽分子动力学轨迹语料库上训练，支持零采样生成不相关样本，保留归一化流的高效似然评估能力。

Result: Prose能够为任意肽系统提供零样本不相关提案样本，实现跨序列长度的迁移，在四肽上的重要性采样微调性能优于顺序蒙特卡洛等传统方法。

Conclusion: 深度学习能够设计可扩展和可迁移的采样器，Prose证明了归一化流在分子构象采样中的潜力，开源代码和模型以促进摊销采样方法研究。

Abstract: Efficient equilibrium sampling of molecular conformations remains a core
challenge in computational chemistry and statistical inference. Classical
approaches such as molecular dynamics or Markov chain Monte Carlo inherently
lack amortization; the computational cost of sampling must be paid in-full for
each system of interest. The widespread success of generative models has
inspired interest into overcoming this limitation through learning sampling
algorithms. Despite performing on par with conventional methods when trained on
a single system, learned samplers have so far demonstrated limited ability to
transfer across systems. We prove that deep learning enables the design of
scalable and transferable samplers by introducing Prose, a 280 million
parameter all-atom transferable normalizing flow trained on a corpus of peptide
molecular dynamics trajectories up to 8 residues in length. Prose draws
zero-shot uncorrelated proposal samples for arbitrary peptide systems,
achieving the previously intractable transferability across sequence length,
whilst retaining the efficient likelihood evaluation of normalizing flows.
Through extensive empirical evaluation we demonstrate the efficacy of Prose as
a proposal for a variety of sampling algorithms, finding a simple importance
sampling-based finetuning procedure to achieve superior performance to
established methods such as sequential Monte Carlo on unseen tetrapeptides. We
open-source the Prose codebase, model weights, and training dataset, to further
stimulate research into amortized sampling methods and finetuning objectives.

</details>


### [170] [AdLoCo: adaptive batching significantly improves communications efficiency and convergence for Large Language Models](https://arxiv.org/abs/2508.18182)
*Nikolay Kutuzov,Makar Baderko,Stepan Kulibaba,Artem Dzhalilov,Daniel Bobrov,Maxim Mashtaler,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 一种三阶段分布式训练方法，通过多实例并行训练、适配批次DiLoCo和切换模式机制，提高异构硬件利用率和训练效率


<details>
  <summary>Details</summary>
Motivation: 解决现有分布式训练方法在动态负载下无法充分利用计算集群资源的问题

Method: 组合了三种技术：多实例训练(MIT)允许节点并行运行多个轻量训练流，适配批次DiLoCo动态调整本地批次大小，切换模式机制在批次过大时引入梯度累积

Result: 提高了系统吞吐量和减少空闲时间，大幅降低同步延迟，改善了收敛速度和系统效率

Conclusion: 该三阶段方法能够有效利用异构硬件资源在动态负载下进行大语言模型训练，并提供了相关的理论收敛速度估计

Abstract: Scaling distributed training of Large Language Models (LLMs) requires not
only algorithmic advances but also efficient utilization of heterogeneous
hardware resources. While existing methods such as DiLoCo have demonstrated
promising results, they often fail to fully exploit computational clusters
under dynamic workloads. To address this limitation, we propose a three-stage
method that combines Multi-Instance Training (MIT), Adaptive Batched DiLoCo,
and switch mode mechanism. MIT allows individual nodes to run multiple
lightweight training streams with different model instances in parallel and
merge them to combine knowledge, increasing throughput and reducing idle time.
Adaptive Batched DiLoCo dynamically adjusts local batch sizes to balance
computation and communication, substantially lowering synchronization delays.
Switch mode further stabilizes training by seamlessly introducing gradient
accumulation once adaptive batch sizes grow beyond hardware-friendly limits.
Together, these innovations improve both convergence speed and system
efficiency. We also provide a theoretical estimate of the number of
communications required for the full convergence of a model trained using our
method.

</details>


### [171] [HypER: Hyperbolic Echo State Networks for Capturing Stretch-and-Fold Dynamics in Chaotic Flows](https://arxiv.org/abs/2508.18196)
*Pradeep Singh,Sutirtha Ghosh,Ashutosh Kumar,Hrishit B P,Balasubramanian Raman*

Main category: cs.LG

TL;DR: 提出了Hyperbolic Embedding Reservoir (HypER)，一种在双曲空间中构建的Echo State Network，通过双曲距离的指数衰减连接来更好地匹配混沌系统的拉伸-折叠结构，显著延长了混沌动力学的有效预测时间。


<details>
  <summary>Details</summary>
Motivation: 现有的Echo State Networks (ESNs) 使用欧几里得几何的储备池，与混沌系统的拉伸-折叠结构不匹配，导致预测混沌动力学的有效时间有限。

Method: 在Poincare球中采样神经元，连接权重随双曲距离指数衰减，将指数度量直接嵌入潜在空间，使储备池的局部扩展-收缩谱与系统的Lyapunov方向对齐，同时保持标准ESN特性。

Result: 在Lorenz-63、Roessler系统和Chen-Ueta超混沌吸引子上，HypER一致延长了平均有效预测时间，超过欧几里得和图结构ESN基线，在30次独立运行中具有统计显著性增益；在心率变异性和太阳黑子数等真实世界基准测试中也显示出优势。

Conclusion: HypER通过双曲几何结构更好地匹配混沌动力学特性，显著提高了混沌系统长期预测的能力，并建立了状态发散速率的下界，反映了Lyapunov增长特性。

Abstract: Forecasting chaotic dynamics beyond a few Lyapunov times is difficult because
infinitesimal errors grow exponentially. Existing Echo State Networks (ESNs)
mitigate this growth but employ reservoirs whose Euclidean geometry is
mismatched to the stretch-and-fold structure of chaos. We introduce the
Hyperbolic Embedding Reservoir (HypER), an ESN whose neurons are sampled in the
Poincare ball and whose connections decay exponentially with hyperbolic
distance. This negative-curvature construction embeds an exponential metric
directly into the latent space, aligning the reservoir's local
expansion-contraction spectrum with the system's Lyapunov directions while
preserving standard ESN features such as sparsity, leaky integration, and
spectral-radius control. Training is limited to a Tikhonov-regularized readout.
On the chaotic Lorenz-63 and Roessler systems, and the hyperchaotic Chen-Ueta
attractor, HypER consistently lengthens the mean valid-prediction horizon
beyond Euclidean and graph-structured ESN baselines, with statistically
significant gains confirmed over 30 independent runs; parallel results on
real-world benchmarks, including heart-rate variability from the Santa Fe and
MIT-BIH datasets and international sunspot numbers, corroborate its advantage.
We further establish a lower bound on the rate of state divergence for HypER,
mirroring Lyapunov growth.

</details>


### [172] [Deep Learning and Matrix Completion-aided IoT Network Localization in the Outlier Scenarios](https://arxiv.org/abs/2508.18225)
*Sunwoo Kim*

Main category: cs.LG

TL;DR: 提出一种基于深度学习和矩阵补全的方法，用于恢复物联网网络定位中受异常值污染的欧氏距离矩阵，通过联合恢复距离矩阵和传感器坐标，有效处理异常值。


<details>
  <summary>Details</summary>
Motivation: 传统定位技术在矩阵全集上搜索解决方案，而本方法将搜索限制在欧氏距离矩阵集合上，旨在更准确地恢复传感器位置信息，特别是在存在异常值的情况下。

Method: 将距离矩阵表示为传感器坐标矩阵的函数，利用深度神经网络联合恢复距离矩阵和坐标矩阵；将异常值建模为稀疏矩阵并加入正则化项，通过交替更新坐标矩阵、距离矩阵和异常值矩阵来解决问题。

Result: 数值实验表明，即使在存在异常值的情况下，该技术也能准确恢复传感器的位置信息。

Conclusion: 所提出的深度学习和矩阵补全辅助方法能够有效处理异常值污染，在物联网网络定位中实现准确的位置恢复。

Abstract: In this paper, we propose a deep learning and matrix completion aided
approach for recovering an outlier contaminated Euclidean distance matrix D in
IoT network localization. Unlike conventional localization techniques that
search the solution over a whole set of matrices, the proposed technique
restricts the search to the set of Euclidean distance matrices. Specifically,
we express D as a function of the sensor coordinate matrix X that inherently
satisfies the unique properties of D, and then jointly recover D and X using a
deep neural network. To handle outliers effectively, we model them as a sparse
matrix L and add a regularization term of L into the optimization problem. We
then solve the problem by alternately updating X, D, and L. Numerical
experiments demonstrate that the proposed technique can recover the location
information of sensors accurately even in the presence of outliers.

</details>


### [173] [Type-Compliant Adaptation Cascades: Adapting Programmatic LM Workflows to Data](https://arxiv.org/abs/2508.18244)
*Chu-Cheng Lin,Daiyi Peng,Yifeng Lu,Ming Zhang,Eugene Ie*

Main category: cs.LG

TL;DR: TACs框架通过将工作流重新表述为类型化概率程序，解决了LLM在复杂多步工作流中的可靠性问题，显著提升了结构化任务的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于离散提示优化的LLM工作流组合方法存在脆弱性，难以满足结构化任务的形式化合规要求，需要更可靠的理论基础和方法。

Method: 引入Type-Compliant Adaptation Cascades框架，将工作流视为未归一化的联合分布，使用参数高效适配的LLM和确定性逻辑，支持基于梯度的原则性训练。

Result: 在结构化任务上显著优于现有提示优化方法，MGSM-SymPy任务从57.1%提升到75.9%（27B模型），MGSM任务从1.6%提升到27.3%（7B模型）。

Conclusion: TACs为开发可靠、任务合规的LLM系统提供了一个鲁棒且理论完备的新范式。

Abstract: Reliably composing Large Language Models (LLMs) for complex, multi-step
workflows remains a significant challenge. The dominant paradigm-optimizing
discrete prompts in a pipeline-is notoriously brittle and struggles to enforce
the formal compliance required for structured tasks. We introduce
Type-Compliant Adaptation Cascades (TACs), a framework that recasts workflow
adaptation as learning typed probabilistic programs. TACs treats the entire
workflow, which is composed of parameter-efficiently adapted LLMs and
deterministic logic, as an unnormalized joint distribution. This enables
principled, gradient-based training even with latent intermediate structures.
We provide theoretical justification for our tractable optimization objective,
proving that the optimization bias vanishes as the model learns type
compliance. Empirically, TACs significantly outperforms state-of-the-art
prompt-optimization baselines. Gains are particularly pronounced on structured
tasks, improving MGSM-SymPy from $57.1\%$ to $75.9\%$ for a 27B model, MGSM
from $1.6\%$ to $27.3\%$ for a 7B model. TACs offers a robust and theoretically
grounded paradigm for developing reliable, task-compliant LLM systems.

</details>


### [174] [Aligning the Evaluation of Probabilistic Predictions with Downstream Value](https://arxiv.org/abs/2508.18251)
*Novin Shahroudi,Viacheslav Komisarenko,Meelis Kull*

Main category: cs.LG

TL;DR: 提出一种数据驱动方法，通过神经网络的加权评分规则来学习与下游任务对齐的代理评估函数，解决预测评估与下游效用之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于预测性能的评估指标往往与真实下游任务的实际影响存在偏差，现有方法要么需要多个任务特定指标，要么需要事先已知成本结构，使用不便。

Method: 基于适当评分规则理论，探索保持适当性的评分规则变换，使用神经网络参数化的加权评分规则，通过学习权重来与下游任务性能对齐。

Result: 通过合成和真实数据的回归任务实验证明，该框架能够有效弥合预测评估与下游效用之间的差距。

Conclusion: 该方法为模块化预测系统提供了快速可扩展的评估循环，能够在权重复杂或未知的情况下实现预测评估与下游任务的对齐。

Abstract: Every prediction is ultimately used in a downstream task. Consequently,
evaluating prediction quality is more meaningful when considered in the context
of its downstream use. Metrics based solely on predictive performance often
diverge from measures of real-world downstream impact. Existing approaches
incorporate the downstream view by relying on multiple task-specific metrics,
which can be burdensome to analyze, or by formulating cost-sensitive
evaluations that require an explicit cost structure, typically assumed to be
known a priori. We frame this mismatch as an evaluation alignment problem and
propose a data-driven method to learn a proxy evaluation function aligned with
the downstream evaluation. Building on the theory of proper scoring rules, we
explore transformations of scoring rules that ensure the preservation of
propriety. Our approach leverages weighted scoring rules parametrized by a
neural network, where weighting is learned to align with the performance in the
downstream task. This enables fast and scalable evaluation cycles across tasks
where the weighting is complex or unknown a priori. We showcase our framework
through synthetic and real-data experiments for regression tasks, demonstrating
its potential to bridge the gap between predictive evaluation and downstream
utility in modular prediction systems.

</details>


### [175] [ANO : Faster is Better in Noisy Landscape](https://arxiv.org/abs/2508.18258)
*Adrien Kegreisz*

Main category: cs.LG

TL;DR: Ano是一种新型优化器，通过解耦方向和平滑来提升在非平稳和噪声环境中的鲁棒性，使用动量进行方向平滑，瞬时梯度幅度确定步长。


<details>
  <summary>Details</summary>
Motivation: 现有优化器如Adam和Adan在非平稳或噪声环境中性能下降，主要因为依赖基于动量的幅度估计。

Method: 提出Ano优化器，将方向和平滑解耦：动量用于方向平滑，瞬时梯度幅度决定步长；进一步提出Anolog，通过对数调度扩展动量窗口来消除对动量系数的敏感性。

Result: 建立了非凸收敛保证，收敛速率与其他基于符号的方法相似；在强化学习等噪声和非平稳环境中表现显著提升，在计算机视觉基准测试中保持竞争力。

Conclusion: Ano优化器在保持一阶方法简洁高效的同时，提高了对梯度噪声的鲁棒性，特别适用于噪声和非平稳环境。

Abstract: Stochastic optimizers are central to deep learning, yet widely used methods
such as Adam and Adan can degrade in non-stationary or noisy environments,
partly due to their reliance on momentum-based magnitude estimates. We
introduce Ano, a novel optimizer that decouples direction and magnitude:
momentum is used for directional smoothing, while instantaneous gradient
magnitudes determine step size. This design improves robustness to gradient
noise while retaining the simplicity and efficiency of first-order methods. We
further propose Anolog, which removes sensitivity to the momentum coefficient
by expanding its window over time via a logarithmic schedule. We establish
non-convex convergence guarantees with a convergence rate similar to other
sign-based methods, and empirically show that Ano provides substantial gains in
noisy and non-stationary regimes such as reinforcement learning, while
remaining competitive on low-noise tasks such as standard computer vision
benchmarks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [176] [GraphPPD: Posterior Predictive Modelling for Graph-Level Inference](https://arxiv.org/abs/2508.16995)
*Soumyasundar Pal,Liheng Ma,Amine Natik,Yingxue Zhang,Mark Coates*

Main category: stat.ML

TL;DR: 提出了一种新的变分建模框架，用于图级学习任务中的后验预测分布建模，以获取不确定性感知的预测能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习中准确建模和量化预测不确定性至关重要，但现有的图神经网络不确定性方法主要针对节点或链接级任务，无法直接应用于图级学习问题。

Method: 基于现有GNNs的图级嵌入，开发了一个变分建模框架来学习后验预测分布，采用数据自适应的方式进行不确定性建模。

Result: 在多个基准数据集上的实验结果表明该方法的有效性。

Conclusion: 该框架成功解决了图级学习任务中的不确定性建模问题，为图神经网络在图级预测中提供了可靠的不确定性量化能力。

Abstract: Accurate modelling and quantification of predictive uncertainty is crucial in
deep learning since it allows a model to make safer decisions when the data is
ambiguous and facilitates the users' understanding of the model's confidence in
its predictions. Along with the tremendously increasing research focus on
\emph{graph neural networks} (GNNs) in recent years, there have been numerous
techniques which strive to capture the uncertainty in their predictions.
However, most of these approaches are specifically designed for node or
link-level tasks and cannot be directly applied to graph-level learning
problems. In this paper, we propose a novel variational modelling framework for
the \emph{posterior predictive distribution}~(PPD) to obtain uncertainty-aware
prediction in graph-level learning tasks. Based on a graph-level embedding
derived from one of the existing GNNs, our framework can learn the PPD in a
data-adaptive fashion. Experimental results on several benchmark datasets
exhibit the effectiveness of our approach.

</details>


### [177] [Limitations of refinement methods for weak to strong generalization](https://arxiv.org/abs/2508.17018)
*Seamus Somerstep,Ya'acov Ritov,Mikhail Yurochkin,Subha Maity,Yuekai Sun*

Main category: stat.ML

TL;DR: 本文分析了LLM对齐中的标签精炼和弱训练方法，发现它们都存在不可约误差，与理想oracle方法存在性能差距，需要开发新的弱到强泛化方法。


<details>
  <summary>Details</summary>
Motivation: 标准LLM对齐技术使用人类生成数据，限制了模型能力只能达到人类水平。需要解决超对齐问题，使模型能力超越人类水平。

Method: 采用概率假设分析标签精炼方法，并与计算不可行的oracle方法进行比较，评估弱训练和标签精炼的性能极限。

Result: 发现弱训练和标签精炼都存在不可约误差，与理想oracle方法存在性能差距，无法达到最优效果。

Conclusion: 需要开发新的弱到强泛化方法，结合标签精炼/弱训练的实用性和oracle程序的最优性。

Abstract: Standard techniques for aligning large language models (LLMs) utilize
human-produced data, which could limit the capability of any aligned LLM to
human level. Label refinement and weak training have emerged as promising
strategies to address this superalignment problem. In this work, we adopt
probabilistic assumptions commonly used to study label refinement and analyze
whether refinement can be outperformed by alternative approaches, including
computationally intractable oracle methods. We show that both weak training and
label refinement suffer from irreducible error, leaving a performance gap
between label refinement and the oracle. These results motivate future research
into developing alternative methods for weak to strong generalization that
synthesize the practicality of label refinement or weak training and the
optimality of the oracle procedure.

</details>


### [178] [CP4SBI: Local Conformal Calibration of Credible Sets in Simulation-Based Inference](https://arxiv.org/abs/2508.17077)
*Luben M. C. Cabezas,Vagner S. Santos,Thiago R. Ramos,Pedro L. C. Rodrigues,Rafael Izbicki*

Main category: stat.ML

TL;DR: 开发了CP4SBI框架，通过保形校准方法为模拟推断(SBI)提供有限样本局部覆盖保证，改善神经后验估计器的不确定性量化质量


<details>
  <summary>Details</summary>
Motivation: 当前实验科学家越来越依赖模拟推断(SBI)来反演复杂非线性模型，但SBI获得的后验近似往往存在校准错误，导致置信区域无法覆盖真实参数

Method: 提出了模型无关的保形校准框架CP4SBI，包含两种变体：通过回归树进行局部校准和基于CDF的校准，可为任何评分函数构建具有局部贝叶斯覆盖的置信集

Result: 在广泛使用的SBI基准测试中，该方法改善了使用归一化流和分数扩散建模的神经后验估计器的不确定性量化质量

Conclusion: CP4SBI框架为SBI提供了有效的校准方法，能够确保有限样本局部覆盖保证，提升后验估计的可靠性

Abstract: Current experimental scientists have been increasingly relying on
simulation-based inference (SBI) to invert complex non-linear models with
intractable likelihoods. However, posterior approximations obtained with SBI
are often miscalibrated, causing credible regions to undercover true
parameters. We develop $\texttt{CP4SBI}$, a model-agnostic conformal
calibration framework that constructs credible sets with local Bayesian
coverage. Our two proposed variants, namely local calibration via regression
trees and CDF-based calibration, enable finite-sample local coverage guarantees
for any scoring function, including HPD, symmetric, and quantile-based regions.
Experiments on widely used SBI benchmarks demonstrate that our approach
improves the quality of uncertainty quantification for neural posterior
estimators using both normalizing flows and score-diffusion modeling.

</details>


### [179] [Neural Stochastic Differential Equations on Compact State-Spaces](https://arxiv.org/abs/2508.17090)
*Yue-Jane Liu,Malinda Lu,Matthew K. Nock,Yaniv Yacoby*

Main category: stat.ML

TL;DR: 在缩穹多面体空间上提出了一类新的神经SDEs，解决了传统SDEs的不稳定性、导向偏偏和求解器限制问题


<details>
  <summary>Details</summary>
Motivation: 现代概率模型依赖SDEs，但存在不稳定、在无界域上导向偏偏差、以及依赖限制性动力学或训练技巧等问题。虽然有工作通过反射动力学将SDEs限制在缩穹空间，但缺乏连续动力学和高效的高阶求解器

Method: 提出了一类新的神经SDEs，在缩穹多面体空间上实现连续动力学，适合高阶求解器使用

Result: 该方法具有较好的导向偏偏，提高了模型的可解释性和应用能力

Conclusion: 这种在缩穹多面体空间上的连续神经SDEs提供了更好的性能和应用潜力，充分利用了高阶求解器的优势

Abstract: Many modern probabilistic models rely on SDEs, but their adoption is hampered
by instability, poor inductive bias outside bounded domains, and reliance on
restrictive dynamics or training tricks. While recent work constrains SDEs to
compact spaces using reflected dynamics, these approaches lack continuous
dynamics and efficient high-order solvers, limiting interpretability and
applicability. We propose a novel class of neural SDEs on compact polyhedral
spaces with continuous dynamics, amenable to higher-order solvers, and with
favorable inductive bias.

</details>


### [180] [Rao Differential Privacy](https://arxiv.org/abs/2508.17135)
*Carlos Soto*

Main category: stat.ML

TL;DR: 本文从信息几何角度重新定义差分隐私，使用Rao距离替代传统的散度度量，在保持隐私解释的同时改进了序列组合性能


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私定义都基于密度散度，作者希望从信息几何的新视角来重新定义隐私，寻求更好的理论性质

Method: 采用信息几何中的Rao距离作为隐私定义的基础，替代传统的散度度量方法

Result: 提出的新隐私定义保持了原有隐私解释，同时在序列组合方面有所改进

Conclusion: 信息几何视角为差分隐私提供了新的定义框架，Rao距离作为隐私度量工具具有理论优势

Abstract: Differential privacy (DP) has recently emerged as a definition of privacy to
release private estimates. DP calibrates noise to be on the order of an
individuals contribution. Due to the this calibration a private estimate
obscures any individual while preserving the utility of the estimate. Since the
original definition, many alternate definitions have been proposed. These
alternates have been proposed for various reasons including improvements on
composition results, relaxations, and formalizations. Nevertheless, thus far
nearly all definitions of privacy have used a divergence of densities as the
basis of the definition. In this paper we take an information geometry
perspective towards differential privacy. Specifically, rather than define
privacy via a divergence, we define privacy via the Rao distance. We show that
our proposed definition of privacy shares the interpretation of previous
definitions of privacy while improving on sequential composition.

</details>


### [181] [Factor Informed Double Deep Learning For Average Treatment Effect Estimation](https://arxiv.org/abs/2508.17136)
*Jianqing Fan,Soham Jana,Sanjeev Kulkarni,Qishuo Yin*

Main category: stat.ML

TL;DR: 提出FIDDLE方法，使用双深度学习策略结合FAST-NN估计器，在高维相关协变量下估计平均处理效应，具有模型错误设定鲁棒性和半参数效率性


<details>
  <summary>Details</summary>
Motivation: 解决高维、高度相关、具有稀疏非线性效应的协变量环境下平均处理效应的估计问题，传统方法在此类复杂数据场景下表现受限

Method: 基于AIPW框架，使用FAST-NN神经网络估计器非参数选择变量并学习低维函数结构，构建因子增强的双深度学习估计器FIDDLE

Result: FIDDLE在模型错误设定下仍能一致估计ATE，达到半参数效率，在合成和真实数据集上优于传统方法，特别在高维数据中表现突出

Conclusion: FIDDLE为高维复杂协变量环境下的因果推断提供了有效的解决方案，结合深度学习和因子方法的优势，具有理论保证和实际应用价值

Abstract: We investigate the problem of estimating the average treatment effect (ATE)
under a very general setup where the covariates can be high-dimensional, highly
correlated, and can have sparse nonlinear effects on the propensity and outcome
models. We present the use of a Double Deep Learning strategy for estimation,
which involves combining recently developed factor-augmented deep
learning-based estimators, FAST-NN, for both the response functions and
propensity scores to achieve our goal. By using FAST-NN, our method can select
variables that contribute to propensity and outcome models in a completely
nonparametric and algorithmic manner and adaptively learn low-dimensional
function structures through neural networks. Our proposed novel estimator,
FIDDLE (Factor Informed Double Deep Learning Estimator), estimates ATE based on
the framework of augmented inverse propensity weighting AIPW with the
FAST-NN-based response and propensity estimates. FIDDLE consistently estimates
ATE even under model misspecification and is flexible to also allow for
low-dimensional covariates. Our method achieves semiparametric efficiency under
a very flexible family of propensity and outcome models. We present extensive
numerical studies on synthetic and real datasets to support our theoretical
guarantees and establish the advantages of our methods over other traditional
choices, especially when the data dimension is large.

</details>


### [182] [On the sample complexity of semi-supervised multi-objective learning](https://arxiv.org/abs/2508.17152)
*Tobias Wegel,Geelon So,Junhyung Park,Fanny Yang*

Main category: stat.ML

TL;DR: 本文研究了多目标学习中的统计成本问题，发现在某些损失函数下模型复杂度成本不可避免，但对于Bregman损失，未标记数据可以显著减少对标记数据的需求。


<details>
  <summary>Details</summary>
Motivation: 多目标学习中多个预测任务需要联合解决，使用更大容量的模型会增加统计成本。本文旨在探索在何种条件下未标记数据可以减轻对标记数据的需求。

Method: 采用半监督学习设置，分析不同损失函数下的样本复杂性，提出基于伪标记的简单半监督算法。

Result: 证明对于某些损失函数，模型复杂度成本不可避免；但对于Bregman损失，模型复杂度的影响主要体现在未标记数据上，未标记数据可以显著减少标记数据需求。

Conclusion: Bregman损失函数的多目标学习中，未标记数据可以有效降低对标记样本的需求，这为半监督多目标学习提供了理论依据和实用算法。

Abstract: In multi-objective learning (MOL), several possibly competing prediction
tasks must be solved jointly by a single model. Achieving good trade-offs may
require a model class $\mathcal{G}$ with larger capacity than what is necessary
for solving the individual tasks. This, in turn, increases the statistical
cost, as reflected in known MOL bounds that depend on the complexity of
$\mathcal{G}$. We show that this cost is unavoidable for some losses, even in
an idealized semi-supervised setting, where the learner has access to the
Bayes-optimal solutions for the individual tasks as well as the marginal
distributions over the covariates. On the other hand, for objectives defined
with Bregman losses, we prove that the complexity of $\mathcal{G}$ may come
into play only in terms of unlabeled data. Concretely, we establish sample
complexity upper bounds, showing precisely when and how unlabeled data can
significantly alleviate the need for labeled data. These rates are achieved by
a simple, semi-supervised algorithm via pseudo-labeling.

</details>


### [183] [High-Order Langevin Monte Carlo Algorithms](https://arxiv.org/abs/2508.17545)
*Thanh Dang,Mert Gurbuzbalaban,Mohammad Rafiqul Islam,Nian Yao,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 提出了基于P阶Langevin动力学的离散化蒙特卡洛算法，通过分裂和精确积分方法设计，在采样对数凹平滑密度分布时具有更好的维度和精度依赖性的收敛保证。


<details>
  <summary>Details</summary>
Motivation: Langevin算法是数据科学中大规模采样问题的流行MCMC方法，但现有方法在维度和精度方面的收敛性能有待提升。

Method: 结合分裂方法和精确积分技术，设计P≥3阶的Langevin蒙特卡洛算法，对P阶Langevin动力学进行离散化。

Result: 获得了Wasserstein收敛保证，混合时间随维度d和精度ε的依赖关系为O(d^(1/R)/ε^(1/(2R)))，其中R随P增大而改善。数值实验验证了算法效率。

Conclusion: 高阶LMC算法在采样对数凹平滑密度分布时，随着阶数P的增加，在维度和精度方面表现出更好的收敛性能。

Abstract: Langevin algorithms are popular Markov chain Monte Carlo (MCMC) methods for
large-scale sampling problems that often arise in data science. We propose
Monte Carlo algorithms based on the discretizations of $P$-th order Langevin
dynamics for any $P\geq 3$. Our design of $P$-th order Langevin Monte Carlo
(LMC) algorithms is by combining splitting and accurate integration methods. We
obtain Wasserstein convergence guarantees for sampling from distributions with
log-concave and smooth densities. Specifically, the mixing time of the $P$-th
order LMC algorithm scales as
$O\left(d^{\frac{1}{R}}/\epsilon^{\frac{1}{2R}}\right)$ for $R=4\cdot 1_{\{
P=3\}}+ (2P-1)\cdot 1_{\{ P\geq 4\}}$, which has a better dependence on the
dimension $d$ and the accuracy level $\epsilon$ as $P$ grows. Numerical
experiments illustrate the efficiency of our proposed algorithms.

</details>


### [184] [The Statistical Fairness-Accuracy Frontier](https://arxiv.org/abs/2508.17622)
*Alireza Fallah,Michael I. Jordan,Annie Ulichney*

Main category: stat.ML

TL;DR: 本文研究了有限样本下的公平性-准确性边界，推导了极小极大最优估计器，分析了有限样本对不同群体风险的不对称影响，并提出了最优样本分配策略，将FA边界从理论概念转化为实际工具。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型需要在准确性和公平性之间取得平衡，但现有研究假设完全了解总体分布，这在现实中不切实际。需要研究有限样本情况下FA边界的行为。

Method: 推导极小极大最优估计器，分析有限样本对群体风险的不对称影响，识别最优样本分配策略，量化有限样本边界与总体边界之间的最坏情况差距。

Result: 建立了有限样本FA边界的理论框架，提供了依赖于协变量分布知识的估计器，揭示了有限样本效应对不同群体风险的不对称影响。

Conclusion: 该研究将FA边界从理论概念转化为实际可用的工具，为政策制定者和从业者在有限数据条件下设计算法提供了实用指导。

Abstract: Machine learning models must balance accuracy and fairness, but these goals
often conflict, particularly when data come from multiple demographic groups. A
useful tool for understanding this trade-off is the fairness-accuracy (FA)
frontier, which characterizes the set of models that cannot be simultaneously
improved in both fairness and accuracy. Prior analyses of the FA frontier
provide a full characterization under the assumption of complete knowledge of
population distributions -- an unrealistic ideal. We study the FA frontier in
the finite-sample regime, showing how it deviates from its population
counterpart and quantifying the worst-case gap between them. In particular, we
derive minimax-optimal estimators that depend on the designer's knowledge of
the covariate distribution. For each estimator, we characterize how
finite-sample effects asymmetrically impact each group's risk, and identify
optimal sample allocation strategies. Our results transform the FA frontier
from a theoretical construct into a practical tool for policymakers and
practitioners who must often design algorithms with limited data.

</details>


### [185] [Algebraic Approach to Ridge-Regularized Mean Squared Error Minimization in Minimal ReLU Neural Network](https://arxiv.org/abs/2508.17783)
*Ryoya Fukasaku,Yutaro Kabata,Akifumi Okuno*

Main category: stat.ML

TL;DR: 本文提出了一种基于计算代数的分析方法，用于系统研究ReLU感知机的岭正则化均方误差(RR-MSE)的所有局部最小值，包括孤立点和高维连通集。


<details>
  <summary>Details</summary>
Motivation: 传统数值优化方法只能找到零维的孤立局部最小值，无法发现高维的连通最小值集（如曲线、曲面等）。本文旨在通过代数方法全面分析ReLU感知机的损失函数结构。

Method: 采用Divide-Enumerate-Merge策略，利用RR-MSE对ReLU感知机是分段多项式这一特性，运用计算代数工具穷举枚举所有局部最小值。

Result: 方法能够识别零维和高维最小值，但由于计算复杂度高，目前仅适用于具有少量隐藏单元的最小感知机作为概念验证。

Conclusion: 该代数方法为理解ReLU神经网络损失函数的结构提供了新的视角，虽然计算成本限制了实际应用规模，但在理论分析方面具有重要意义。

Abstract: This paper investigates a perceptron, a simple neural network model, with
ReLU activation and a ridge-regularized mean squared error (RR-MSE). Our
approach leverages the fact that the RR-MSE for ReLU perceptron is piecewise
polynomial, enabling a systematic analysis using tools from computational
algebra. In particular, we develop a Divide-Enumerate-Merge strategy that
exhaustively enumerates all local minima of the RR-MSE. By virtue of the
algebraic formulation, our approach can identify not only the typical
zero-dimensional minima (i.e., isolated points) obtained by numerical
optimization, but also higher-dimensional minima (i.e., connected sets such as
curves, surfaces, or hypersurfaces). Although computational algebraic methods
are computationally very intensive for perceptrons of practical size, as a
proof of concept, we apply the proposed approach in practice to minimal
perceptrons with a few hidden units.

</details>


### [186] [Clinical characteristics, complications and outcomes of critically ill patients with Dengue in Brazil, 2012-2024: a nationwide, multicentre cohort study](https://arxiv.org/abs/2508.18207)
*Igor Tona Peres,Otavio T. Ranzani,Leonardo S. L. Bastos,Silvio Hamacher,Tom Edinburgh,Esteban Garcia-Gallo,Fernando Augusto Bozza*

Main category: stat.ML

TL;DR: 巴西ICU登革热患者研究：10.1%出现并发症，高龄、慢性肾病、肝硬化、低血小板和高白细胞是主要风险因素，开发了机器学习预测工具


<details>
  <summary>Details</summary>
Motivation: 巴西占2024年全球登革热病例的71%，研究旨在描述ICU重症登革热患者特征、评估时间趋势、识别并发症风险因素

Method: 前瞻性研究，纳入253个ICU的11,047例患者，使用描述性统计、逻辑回归和机器学习框架分析数据

Result: 1,117例(10.1%)出现并发症，包括呼吸支持、升压药使用等。高龄(>80岁)、慢性肾病、肝硬化、低血小板(<50,000)和高白细胞(>7,000)是显著风险因素

Conclusion: 确定了ICU登革热患者并发症的关键风险因素，开发的预测工具可用于早期识别和高风险患者干预，改善登革热流行地区预后

Abstract: Background. Dengue outbreaks are a major public health issue, with Brazil
reporting 71% of global cases in 2024. Purpose. This study aims to describe the
profile of severe dengue patients admitted to Brazilian Intensive Care units
(ICUs) (2012-2024), assess trends over time, describe new onset complications
while in ICU and determine the risk factors at admission to develop
complications during ICU stay. Methods. We performed a prospective study of
dengue patients from 253 ICUs across 56 hospitals. We used descriptive
statistics to describe the dengue ICU population, logistic regression to
identify risk factors for complications during the ICU stay, and a machine
learning framework to predict the risk of evolving to complications.
Visualisations were generated using ISARIC VERTEX. Results. Of 11,047
admissions, 1,117 admissions (10.1%) evolved to complications, including
non-invasive (437 admissions) and invasive ventilation (166), vasopressor
(364), blood transfusion (353) and renal replacement therapy (103). Age>80 (OR:
3.10, 95% CI: 2.02-4.92), chronic kidney disease (OR: 2.94, 2.22-3.89), liver
cirrhosis (OR: 3.65, 1.82-7.04), low platelets (<50,000 cells/mm3; OR: OR:
2.25, 1.89-2.68), and high leukocytes (>7,000 cells/mm3; OR: 2.47, 2.02-3.03)
were significant risk factors for complications. A machine learning tool for
predicting complications was proposed, showing accurate discrimination and
calibration. Conclusion. We described a large cohort of dengue patients
admitted to ICUs and identified key risk factors for severe dengue
complications, such as advanced age, presence of comorbidities, higher level of
leukocytes and lower level of platelets. The proposed prediction tool can be
used for early identification and targeted interventions to improve outcomes in
dengue-endemic regions.

</details>
