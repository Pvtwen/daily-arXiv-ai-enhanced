<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.LG](#cs.LG) [Total: 62]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Time and Frequency Synchronization for Multiuser OTFS in Uplink](https://arxiv.org/abs/2507.17966)
*Mohsen Bayat,Sanoopkumar P. S.,Arman Farhang*

Main category: eess.SP

TL;DR: 本文提出了高移动性场景下上行多用户OTFS系统的时间和频率同步技术，重点解决了定时偏移和载波频率偏移的估计与校正问题。


<details>
  <summary>Details</summary>
Motivation: 在高移动性场景中，准确估计和校正定时偏移（TOs）和载波频率偏移（CFOs）对多用户OTFS系统的性能至关重要。

Method: 提出了两种TO估计技术：基于SU-PCP的TO估计和基于MU-PCP的TO估计，并利用CPF-BEM模型简化CFO估计的多维搜索问题。

Result: 通过改进的导频结构和相关技术，实现了高精度的TO和CFO估计。

Conclusion: 所提出的技术显著提升了上行多用户OTFS系统在高移动性场景下的同步性能。

Abstract: In this paper, we propose time and frequency synchronization techniques for
uplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work
focuses on accurately estimating and correcting timing offsets (TOs) and
carrier frequency offsets (CFOs). Specifically, TO estimation is essential for
locating users' pilots on the delay-time plane, while CFO estimation enhances
channel estimation accuracy. First, we propose a TO estimation technique for an
existing multiuser pilot structure in MU-OTFS. We replace the impulse pilot
(IMP) in this pilot structure with a more practical pilot with a cyclic prefix
(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs
different Zadoff-Chu (ZC) sequences, which enables pilot separation via
correlation at the receiver side. Consequently, we introduce a
correlation-based TO estimation technique for uplink MU-OTFS using this pilot
structure. Next, a spectrally efficient and practical pilot pattern is
proposed, where each user transmits a PCP within a shared pilot region on the
delay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO
estimation technique utilizes a bank of filters to separate different users'
signals and accurately estimate their TOs. Then, we derive a mathematical
threshold range to enhance TO estimation accuracy by finding the first major
peak in the correlation function rather than relying solely on the highest
peak. After locating the received users' pilot signals using one of the
proposed TO estimation techniques, our proposed CFO estimation technique
reduces the multi-dimensional maximum likelihood (ML) search problem into
multiple one-dimensional search problems. In this technique, we apply the
Chebyshev polynomials of the first kind basis expansion model (CPF-BEM) to
effectively handle the time-variations of the channel in obtaining the CFO
estimates for all the users.

</details>


### [2] [Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model](https://arxiv.org/abs/2507.17982)
*Pablo Ramírez-Espinosa,Cleofás Segura-Gómez,Ángel Palomares-Caballero,F. Javier López-Martínez,David Morales-Jiménez*

Main category: eess.SP

TL;DR: 本文提出了一种基于超表面的流体天线系统（FAS）的完整分析模型，解决了电子可重构天线在理论建模上的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于电子可重构天线在理论建模上的困难，以及流体天线系统（FAS）在无线通信中的广泛应用需求，本文旨在提出一种实用的分析模型。

Method: 利用电路理论重新构建FAS的信号模型，结合动态超表面天线（DMA）实现FAS概念，并通过全波仿真验证模型。

Result: 模型与仿真结果吻合良好，并提供了关键性能指标的闭式表达式，证实DMA-based FAS可实现与理想化位置灵活天线相似的性能。

Conclusion: 提出的分析模型为FAS的系统设计提供了理论支持，展示了DMA-based FAS的实用潜力。

Abstract: Fluid antenna systems (FASs) have become a popular topic in the wireless
community as an effective yet simple means of exploiting spatial diversity. Due
to the limitations of physically moving radiating elements, electronically
reconfigurable antennas are emerging as practical implementations of FASs,
since changing the radiation pattern is functionally equivalent to physically
moving the device. However, electronically reconfigurable antennas pose a
challenge in terms of analytical modeling, often requiring full-wave
simulations or measurements for their characterization; this severely limits
the extraction of theoretical insights useful for system design. Motivated by
these difficulties and the growing interest in FASs, we propose in this paper a
complete analytical model for metasurface-based embodiments of FASs.
Specifically, we advocate for the implementation of the FAS concept through
dynamic metasurface antennas (DMAs), hitherto proposed as array replacements in
multiple-input multiple-output (MIMO) systems. We leverage circuit theory to
rewrite the conventional signal model of FASs in terms of admittance matrices
accounting for the electromagnetic effects inherent to metasurfaces. The model
is validated with full-wave simulations, showing good agreement. We further
illustrate how to apply the model for standard performance analysis, and
provide closed-form expressions for key metrics, including the resulting signal
covariance matrix. Results confirm that practical DMA-based FASs can achieve
similar performance to that of idealized implementations of position-flexible
antennas.

</details>


### [3] [Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming](https://arxiv.org/abs/2507.18035)
*Hyeonho Noh,Hyeonsu Lyu,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 本文提出了一种基于多主动STAR-RIS的集成感知与通信网络，通过联合优化基站波束成形和STAR-RIS反射/透射系数，最大化通信总速率，同时满足感知SINR、信息泄漏限制和功率约束。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用多主动STAR-RIS提升集成感知与通信网络的性能，解决传统被动RIS和单STAR-RIS的局限性。

Method: 采用交替优化（AO）框架，将问题分解为子问题，分别通过KKT条件和SCA方法优化基站波束成形和STAR-RIS系数。

Result: 仿真表明，所提算法在通信总速率上显著优于被动RIS和单STAR-RIS基线，同时满足感知和安全约束。

Conclusion: 多主动STAR-RIS和联合优化方法为集成感知与通信网络提供了高效解决方案。

Abstract: This paper explores an integrated sensing and communication (ISAC) network
empowered by multiple active simultaneously transmitting and reflecting
reconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes
downlink communication to multiple users while concurrently interrogating a
sensing target. We jointly optimize the BS transmit beamformer and the
reflection/transmission coefficients of every active STAR-RIS in order to
maximize the aggregate communication sum-rate, subject to (i) a stringent
sensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an
upper bound on the leakage of confidential information, and (iii) individual
hardware and total power constraints at both the BS and the STAR-RISs. The
resulting highly non-convex program is tackled with an efficient alternating
optimization (AO) framework. First, the original formulation is reformulated
into an equivalent yet more tractable representation and partitioned into
subproblems. The BS beamformer is updated in closed form via the
Karush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and
transmission vectors are refined through successive convex approximation (SCA),
yielding a semidefinite program that is then solved via semidefinite
relaxation. Comprehensive simulations demonstrate that the proposed algorithm
delivers substantial sum-rate gains over passive-RIS and single STAR-RIS
baselines, all the while rigorously meeting the prescribed sensing and security
constraints.

</details>


### [4] [Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation](https://arxiv.org/abs/2507.18096)
*Jihong Huang,Rong Yang,Wei Gao,Xingqun Zhan,Zheng Yao*

Main category: eess.SP

TL;DR: 本文通过几何分析量化了多径误差对直接位置估计（DPE）的影响，提出了卫星圆形多径偏差（SCMB）模型，并通过模拟和测试验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对DPE理论中多径误差的理论描述，本文旨在填补这一空白。

Method: 通过几何分析量化多径误差对CAF和PVT解的影响，提出SCMB模型，并通过蒙特卡洛模拟和城市峡谷测试验证。

Result: 最大PVT偏差取决于各卫星通道中的最大多径误差，且PVT偏差随卫星仰角增加而增加。

Conclusion: 选择高、低仰角卫星组合可优化DPE卫星几何配置，减少多径误差影响。

Abstract: Direct Position Estimation (DPE) is a method that directly estimate position,
velocity, and time (PVT) information from cross ambiguity function (CAF) of the
GNSS signals, significantly enhancing receiver robustness in urban
environments. However, there is still a lack of theoretical characterization on
multipath errors in the context of DPE theory. Geometric observations highlight
the unique characteristics of DPE errors stemming from multipath and thermal
noise as estimation bias and variance respectively. Expanding upon the
theoretical framework of DPE noise variance through geometric analysis, this
paper focuses on a geometric representation of multipath errors by quantifying
the deviations in CAF and PVT solutions caused by off-centering bias relative
to the azimuth and elevation angles. A satellite circular multipath bias (SCMB)
model is introduced, amalgamating CAF and PVT errors from multiple satellite
channels. The boundaries for maximum or minimum PVT bias are established
through discussions encompassing various multipath conditions. The correctness
of the multipath geometrical portrait is confirmed through both Monte Carlo
simulations and urban canyon tests. The findings indicate that the maximum PVT
bias depends on the largest multipath errors observed across various satellite
channels. Additionally, the PVT bias increases with satellite elevation angles,
influenced by the CAF multipath bias projection. This serves as a reference for
selecting DPE satellites from a geometric standpoint, underscoring the
importance of choosing a balanced combination of high and low elevation angles
to achieve an optimal satellite geometry configuration.

</details>


### [5] [Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems](https://arxiv.org/abs/2507.18149)
*Dongdong Zou,Wei Wang,Jiawen Yao,Zhongxing Tian,Zeyu Feng,Huan Huang,Fan Li,Gordon Ning Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: 提出了一种针对峰值功率受限IM-DD系统的间接概率整形方案，通过动态选择性映射和改进的M-BCJR算法，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决概率整形在IM-DD系统中因记忆效应和系统模型限制难以有效应用的问题。

Method: 采用动态选择性映射（DSLM）控制信号包络，结合改进的M-BCJR算法的turbo均衡器。

Result: 在56GBaud PAM8系统中，接收灵敏度提升了1dB，且兼容典型概率幅度整形架构。

Conclusion: 该方案为峰值功率受限IM-DD系统的概率整形应用提供了新思路。

Abstract: Probabilistic shaping (PS) has attracted significant attention in
intensity-modulation and direct-detection (IM-DD) systems. However, due to the
unique system model and inherent constraints, the effective application of the
PS technique is still an open question in IM-DD systems, particularly in
systems with memory effects. In this paper, a novel indirect PS scheme tailored
for peak power constrained (PPC) IM-DD systems is proposed. The key idea lies
in strategically controlling the signal envelope to mitigate memory-induced
impairments, such as nonlinearity, overshoot, peak-to-average power ratio
enhancement, etc. The proposed scheme incorporates a dynamic selective mapping
(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol
mapping in which the current symbol is not only determined by the current bits
pattern but also by previously generated symbols within a specified memory
length. At the receiver side, a turbo equalizer with a modified M-BCJR
algorithm is proposed to achieve the recovery of ambiguous bits induced by
DSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the
proposed scheme exhibits 1dB receiver sensitivity improvement over 2km
single-mode fiber transmission. In addition, the proposed scheme has also been
demonstrated to be compatible with the typical probabilistic amplitude shaping
architecture, enabling a simple and fine-granularity rate adaptation
capability. To the best of our knowledge, this work opens a new sight for the
application of the PS technique in PPC IM-DD systems with memory effects.

</details>


### [6] [GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing](https://arxiv.org/abs/2507.18166)
*Jonas Elmiger,Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: SCHIEBER是一种新型多天线GNSS接收器方法，无需先验知识即可抑制干扰和欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: GNSS信号易受干扰和欺骗攻击，现有方法需要先验知识或无法同时应对两种攻击。

Method: 采用自适应空间滤波技术抑制干扰，通过一致性测试（DoA和伪距估计）识别和拒绝欺骗信号。

Result: 在GPS L1 C/A系统模拟中，SCHIEBER有效应对了干扰和欺骗攻击。

Conclusion: SCHIEBER为GNSS接收器提供了一种无需先验知识的安全解决方案。

Abstract: Modern positioning relies on radio signals from global navigation satellite
systems (GNSS). Their low receive power renders these radio signals susceptible
to jamming attacks, in which malicious transmitters emit strong interference to
disrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,
in which malicious transmitters mimic legitimate satellites by transmitting
spurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna
GNSS receivers that mitigates jammers as well as spoofers without requiring any
prior knowledge of the receiver position or attack type: Jammers are mitigated
during signal acquisition using a recently developed adaptive spatial filtering
technique. Spoofers are identified and rejected after signal acquisition using
a novel approach that tests the consistency of acquired signals by comparing
their respective direction of arrival (DoA) and pseudorange estimates in a test
that is invariant with respect to the unknown receiver position. We demonstrate
the efficacy of our method using extensive simulations of a GPS L1 C/A system
under spoofing and jamming attacks.

</details>


### [7] [ICWLM: A Multi-Task Wireless Large Model via In-Context Learning](https://arxiv.org/abs/2507.18167)
*Yuxuan Wen,Xiaoming Chen,Maojun Zhang,Zhaoyang Zhang*

Main category: eess.SP

TL;DR: 提出了一种新型无线原生基础模型ICWLM，用于物理层的多任务学习，解决了传统深度学习方法的数据稀缺和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 无线通信技术的快速发展（如mMIMO和mmWave）带来了网络复杂性和计算需求，传统深度学习方法任务特定且难以泛化。

Method: ICWLM直接从大规模混合无线数据训练，利用上下文学习（ICL）适应不同系统配置，并采用动态权重平均（DWA）平衡多任务损失。

Result: ICWLM在多项物理层任务中表现优异，泛化能力强，适用于未见过的系统配置。

Conclusion: ICWLM为未来无线网络提供了统一且自适应的AI模型，有望降低部署复杂性并提升资源管理智能性。

Abstract: The rapid evolution of wireless communication technologies, particularly
massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),
introduces significant network complexity and computational demands.
Significant research efforts have been made to improve physical layer
performance by resorting to deep learning (DL) methods, which, however, are
usually task-specific and struggle with data scarcity and generalization. To
address these challenges, we propose a novel In-Context Wireless Large Model
(ICWLM), a wireless-native foundation model designed for simultaneous
multi-task learning at the physical layer. Unlike conventional methods that
adapt wireless data to pre-trained large language models (LLMs), ICWLM is
trained directly on large-scale, mixed wireless datasets from scratch. It
jointly solves multiple classical physical layer problems, including multi-user
precoding (sum-rate maximization and max-min SINR) and channel prediction. A
key innovation of ICWLM is its utilization of in-context learning (ICL),
enabling the model to adapt to varying system configurations and channel
conditions with minimal demonstration pairs, eliminating the need for extensive
retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm
to dynamically balance the individual task losses during multi-task training,
ensuring efficient and stable learning across diverse objectives. Extensive
simulation results demonstrate that ICWLM achieves competitive performance
compared to task-specific methods while exhibiting remarkable generalization
capabilities to unseen system configurations. This work offers a promising
paradigm for developing unified and adaptive AI models for future wireless
networks, potentially reducing deployment complexity and enhancing intelligent
resource management.

</details>


### [8] [Quantized Signal Recovery with Interference via Parametrized Look-Up Tables](https://arxiv.org/abs/2507.18370)
*Morriel Kasher,Michael Tinston,Predrag Spasojevic*

Main category: eess.SP

TL;DR: 通过使用查找表（LUTs）和参数化模型，优化低分辨率模数转换器的后校正性能，提出三种分析估计器，并在模拟中验证其高精度实时恢复信号的能力。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率、非线性或宽带量化器的信号恢复问题，尤其是面对高功率带外干扰时的谐波失真消除。

Method: 结合参数化LUTs，提出三种分析估计器，并针对相位键控输入信号和线性调频干扰信号提出近似方法以提高估计问题的可解性。

Result: 模拟结果显示，该方法能实时高精度恢复信号，显著优于传统线性滤波技术，并在3位量化音调输入下实现>10 dB的均方误差改善和>20 dBc的无杂散动态范围提升。

Conclusion: 该方法在低分辨率量化器中表现出色，对输入参数变化、非线性量化器和时变干扰源具有鲁棒性。

Abstract: Efficient all-digital post-correction of low-resolution analog-to-digital
converters can be achieved by using Look-Up Tables (LUTs). The performance of a
LUT can be optimized by incorporating a parametric model for the expected input
signal, noise level, and interference signals. We evaluate three analytical
estimators for integration with parametrized LUTs, especially with applications
to low-resolution, non-linear, or wideband quantizers. We also propose several
approximations to improve tractability of the estimation problem for
Phase-Shift Keyed input signals and Linear Frequency Modulated interference
signals. Simulated results validate the ability of our estimator to recover the
instantaneous value of the desired input signal in real-time with a high degree
of accuracy. This includes cancellation of harmonic distortion that aliases
into the desired signal bandwidth from front-end saturation due to high-power
out-of-band interference. Our estimators are shown to achieve a significant
gain over conventional linear-filtering techniques while also being robust to
changes in input parameters, non-linear quantizers, and time-variant
interference sources. For a tone input quantized to 3 bits and estimated with a
fixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error
and $>$20 dBc improvement in Spurious-Free Dynamic Range.

</details>


### [9] [A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff](https://arxiv.org/abs/2507.18587)
*Jérôme Emery,Ali Hasanzadeh Karkan,Jean-François Frigon,François Leduc-Primeau*

Main category: eess.SP

TL;DR: 提出了一种基于Transformer的基础模型，用于mMIMO预编码，旨在降低发射机能耗并动态适应用户速率需求，同时解决了数据稀缺和训练复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 解决mMIMO系统中因数据稀缺和训练复杂性而难以部署深度学习模型的问题。

Method: 使用Transformer基础模型，结合数据增强方法，通过预训练特征提取器找到与目标分布相似的训练样本。

Result: 在相同能耗下，零样本部署性能显著优于零强迫方法，接近加权最小均方误差性能，且复杂度降低8倍。

Conclusion: 该工作为实际部署基于深度学习的解决方案提供了可行性，同时为资源分配和调度算法提供了更高效的控制手段。

Abstract: Deep learning (DL) has emerged as a solution for precoding in massive
multiple-input multiple-output (mMIMO) systems due to its capacity to learn the
characteristics of the propagation environment. However, training such a model
requires high-quality, local datasets at the deployment site, which are often
difficult to collect. We propose a transformer-based foundation model for mMIMO
precoding that seeks to minimize the energy consumption of the transmitter
while dynamically adapting to per-user rate requirements. At equal energy
consumption, zero-shot deployment of the proposed foundation model
significantly outperforms zero forcing, and approaches weighted minimum mean
squared error performance with 8x less complexity. To address model adaptation
in data-scarce settings, we introduce a data augmentation method that finds
training samples similar to the target distribution by computing the cosine
similarity between the outputs of the pre-trained feature extractor. Our work
enables the implementation of DL-based solutions in practice by addressing
challenges of data availability and training complexity. Moreover, the ability
to dynamically configure per-user rate requirements can be leveraged by higher
level resource allocation and scheduling algorithms for greater control over
energy efficiency, spectral efficiency and fairness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Gait Recognition Based on Tiny ML and IMU Sensors](https://arxiv.org/abs/2507.18627)
*Jiahang Zhang,Mingtong Chen,Zhengbao Yang*

Main category: cs.LG

TL;DR: 开发了一个基于Tiny ML和IMU传感器的步态识别系统，通过XIAO-nRF52840 Sense微控制器和LSM6DS3 IMU传感器采集运动数据，利用Edge Impulse平台训练模型，实现实时活动分类。


<details>
  <summary>Details</summary>
Motivation: 通过微型化和低功耗的Tiny ML技术，结合IMU传感器，实现高效且节能的步态识别系统。

Method: 使用滑动窗口和数据归一化预处理数据，训练深度神经网络（DNN）分类器，并通过Edge Impulse平台部署到微控制器。

Result: 模型在测试数据集上达到80%以上的准确率，并能有效分类四种活动。

Conclusion: 系统展示了Tiny ML在低功耗设备中的潜力，适用于电池供电或能量收集设备。

Abstract: This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.

</details>


### [11] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: QuaRC是一个在边缘设备上结合核心集选择的量化感知训练框架，通过相对熵评分和级联层校正策略显著减少量化误差，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上低比特量化模型的需求增加，但传统量化感知训练依赖完整数据集且计算成本高，核心集选择方法在小规模数据集上难以消除量化误差。

Method: QuaRC分两阶段：核心集选择阶段使用相对熵评分选取代表性子集；训练阶段采用级联层校正策略对齐量化模型与全精度模型的中间层输出。

Result: 在ResNet-18的2-bit量化中，仅用1%数据子集，QuaRC在ImageNet-1K上Top-1准确率比现有技术提升5.72%。

Conclusion: QuaRC有效解决了小规模数据集上的量化误差问题，显著提升了边缘设备上量化模型的性能。

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [12] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种低复杂度、通用的AI模型，通过因果不变学习改进语义通信中信道解码器的数据重建。


<details>
  <summary>Details</summary>
Motivation: 解决用户数据多样化和知识分散问题，提升语义通信中的数据重建效果和跨域一致性。

Method: 采用生成对抗网络结合因果不变学习，提取数据的因果和非因果表示，并设计稀疏更新协议。

Result: 因果不变知识确保跨设备一致性，分类任务表现优异，数据重建效果超越现有方法。

Conclusion: 该方法在语义通信中展现出鲁棒性和高效性，适用于多样化数据场景。

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [13] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: 本文提出了一种基于隐藏层神经元输出特征的复杂网络建模方法，研究了不同隐藏层构建的特征网络的自相似性，并分析了调整自相似性程度如何提升深度神经网络的分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究发现深度神经网络在特征表示或参数分布上表现出强烈的层次自相似性，但缺乏对隐藏空间几何自相似性如何影响权重优化的定量分析，以及对内部神经元动态行为的清晰理解。

Method: 基于隐藏层神经元输出特征构建复杂网络模型，研究不同隐藏层的特征网络自相似性，并通过调整自相似性程度优化模型性能。

Result: 实验表明，特征网络的自相似性程度因模型架构而异，且在训练过程中嵌入自相似性约束可将自相似深度神经网络（MLP和注意力架构）的性能提升高达6个百分点。

Conclusion: 通过调整特征网络的自相似性程度，可以有效提升深度神经网络的分类性能，尤其在MLP和注意力架构中表现显著。

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [14] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的自适应优化算法，用于降维的气动形状优化，通过代理模型和MCMC方法减少计算量并解释优化结果。


<details>
  <summary>Details</summary>
Motivation: 目标是减少计算成本，并通过优化结果解释极值点对实现目标流场的作用。

Method: 采用基于代理模型的actor-critic策略评估MCMC方法，允许部分参数临时冻结，通过局部优化参数变化加速全局优化。

Result: 在简单流体动力学问题上展示了方法的有效性，并实现了特征重要性评分的解释。

Conclusion: 该方法在计算效率和结果解释性方面表现出潜力。

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [15] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 论文探讨了欧几里得几何是否适合所有基础模型，提出双曲空间作为替代方案，以解决现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在欧几里得几何下的表现存在局限性，如有限的表示能力、较低的适应性和可扩展性。

Method: 通过引入双曲空间（一种非欧几里得几何）来改进基础模型，利用其低维嵌入和高效表示能力。

Result: 双曲空间在提升语言模型推理能力、视觉语言模型零样本泛化等方面表现出色。

Conclusion: 双曲空间为改进基础模型提供了新方向，但仍需进一步研究以解决挑战。

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [16] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: 论文研究了LLM在排名任务中的位置偏差和重复一致性，提出动态早停方法以减少计算成本。


<details>
  <summary>Details</summary>
Motivation: LLM在排名或评估任务中受候选项目顺序影响（位置偏差），且重复调用结果不一致，需高效解决方案。

Method: 提出动态早停方法，自适应决定每实例所需重复次数，并引入基于置信度的改进。

Result: 动态策略平均减少81%的LLM调用，置信度改进版减少87%，精度损失小。

Conclusion: 动态早停方法高效减少计算成本，适用于LLM的排名任务。

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [17] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: 提出了一种名为CICME的三步法，用于从多领域异构数据中推断因果机制，结合因果迁移学习（CTL）原理，可靠检测领域不变的因果机制，并在制造过程场景中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 通过因果性视角深入理解复杂传感器系统，解决多领域异构数据中因果机制的推断问题。

Method: 采用三步法CICME，结合CTL原理，先识别领域不变的因果机制，再指导各领域剩余因果机制的估计。

Result: 在线性高斯模型及制造过程场景中，CICME表现优于基线方法，尤其在结合池化数据与个体领域数据时效果显著。

Conclusion: CICME能有效推断多领域数据中的因果机制，尤其在领域不变性假设下表现优越，为复杂系统分析提供新工具。

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [18] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix是一个开源的、基于Python的软件框架，旨在为表格数据提供可重复且可解释的机器学习工作流程。


<details>
  <summary>Details</summary>
Motivation: 解决对透明实验数据分析来源的需求，确保整个分析过程（包括数据转换和方法选择）被记录、可访问、可重复且易于理解。

Method: Helix包含标准化数据预处理、可视化、模型训练、评估、解释、结果检查和预测模块，并提供用户友好的界面。

Result: Helix支持社区驱动开发，遵循FAIR原则，并通过GitHub和PyPI提供访问。

Conclusion: Helix为研究人员提供了一个透明、可解释且易于使用的机器学习框架，特别适合非数据科学背景的用户。

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [19] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: 提出了一种结合生成式人工智能和Copula模型的新框架CoCAI，用于多变量时间序列的预测和异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列分析中的准确预测和稳健异常检测问题。

Method: 使用基于扩散的模型捕捉数据依赖关系，结合保形预测技术校准输出，并通过降维和Copula模型进行异常检测。

Result: 在真实水务系统数据上验证了CoCAI的预测准确性和异常检测能力。

Conclusion: CoCAI框架在多变量时间序列分析中表现出色，具有理论支持和实际应用价值。

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [20] [Efficient Uncertainty in LLMs through Evidential Knowledge Distillation](https://arxiv.org/abs/2507.18366)
*Lakshmana Sri Harsha Nemani,P. K. Srijith,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: 论文提出了一种通过蒸馏技术实现高效不确定性量化的方法，避免了传统贝叶斯和集成方法的多重计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准LLMs在不确定性量化方面存在挑战，传统方法计算成本高。

Method: 通过蒸馏技术将需要多重前向传播的教师模型压缩为学生模型，采用LoRA微调，并比较了两种蒸馏策略：传统softmax输出和Dirichlet分布输出。

Result: 实验表明，学生模型在单次前向传播下能达到与教师模型相当或更好的性能。

Conclusion: 首次证明通过证据蒸馏可以在LLMs中实现高效且鲁棒的不确定性量化。

Abstract: Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.

</details>


### [21] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种结合扩散模型和大型语言模型（LLM）的时空扩散模型（LSDM），用于提升个人用户服务级移动流量预测的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有流量预测方法在不同城市环境中的适应性不足，且因个人流量模式的高不确定性、缺乏详细环境背景及网络服务间复杂依赖关系导致预测不准确。

Method: LSDM结合扩散模型的生成能力和Transformer的自适应学习能力，利用LLM捕捉多模态环境信息，建模服务级模式和动态。

Result: 实验表明，LSDM在流量预测中表现优异，具有出色的泛化能力和适应性。加入LLM后，决定系数提升至少2.83%，均方根误差较同类模型（如CSDI）降低至少8.29%。

Conclusion: LSDM通过结合扩散模型和LLM，显著提升了服务级移动流量预测的性能和适应性。

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [22] [Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2507.18555)
*Jun'ichi Takeuchia,Yoshinari Takeishia,Noboru Muratab,Kazushi Mimurac,Ka Long Keith Hod,Hiroshi Nagaoka*

Main category: cs.LG

TL;DR: 本文探讨了2层ReLU网络的Fisher信息矩阵与神经正切核（NTK）的关系，并展示了NTK的谱分解及主要特征值对应的特征函数形式，同时提出了2层神经网络的函数近似公式。


<details>
  <summary>Details</summary>
Motivation: 研究Fisher信息矩阵与NTK之间的关系，以理解2层ReLU网络的优化和泛化行为。

Method: 通过线性变换分析Fisher信息矩阵与NTK的关系，并进行NTK的谱分解，推导主要特征值对应的特征函数形式。

Result: 展示了NTK的谱分解及特征函数形式，并提出了2层神经网络的函数近似公式。

Conclusion: 揭示了Fisher信息矩阵与NTK的关联，为理解2层ReLU网络的优化和泛化提供了理论支持。

Abstract: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.

</details>


### [23] [Beyond Internal Data: Constructing Complete Datasets for Fairness Testing](https://arxiv.org/abs/2507.18561)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 论文提出了一种利用重叠数据集构建合成数据的方法，以解决公平性测试中数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的广泛应用，测试其潜在危害和偏见变得至关重要，但获取包含人口统计数据的真实数据面临法律、隐私和代表性挑战。

Method: 通过利用重叠数据集构建包含人口统计信息的合成数据，并验证其与真实数据的一致性。

Result: 实验证明，基于合成数据的公平性指标与真实数据一致，为解决数据稀缺问题提供了可行方案。

Conclusion: 该方法为公平性测试提供了一种替代方案，尤其适用于真实数据受限的场景。

Abstract: As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.

</details>


### [24] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: GenSelect利用LLM的长推理能力从N个候选方案中选择最佳解决方案，优于现有的点对点和成对评分方法，尤其在数学推理任务中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有方法（点对点或成对比较）未能充分利用LLM的比较能力或在大规模采样预算下效率低下。

Method: 提出GenSelect方法，通过LLM的长推理从多个候选方案中选择最佳方案。

Result: 在数学推理任务中，GenSelect表现优于现有评分方法。

Conclusion: GenSelect有效结合了LLM的比较优势，且在大规模采样预算下具有高效性。

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [25] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 使用Wasserstein生成对抗网络（WGAN）进行降水降尺度预测，提高视觉真实感，并提供新的数据集评估视角。


<details>
  <summary>Details</summary>
Motivation: 高分辨率降水预测对减少局部强降雨灾害至关重要，但传统数值天气预测模型仍面临挑战。

Method: 采用WGAN进行降水降尺度，利用最优传输成本生成降水场。

Result: WGAN生成的降水场视觉真实感强，尽管传统评估指标稍低，但能识别数据潜在问题。

Conclusion: WGAN框架不仅提升降水降尺度的视觉真实感，还为数据集评估和质量控制提供新视角。

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [26] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: GraphEXT是一个基于合作博弈论和社会外部性的新解释框架，用于提升GNN的可解释性，通过节点联盟划分和Shapley值量化节点重要性。


<details>
  <summary>Details</summary>
Motivation: GNN的“黑盒”特性使其难以解释，现有方法未能有效捕捉节点间的复杂交互模式。

Method: GraphEXT将图节点划分为联盟，分解为独立子图，结合图结构作为外部性，利用Shapley值量化节点对预测的边际贡献。

Result: 在合成和真实数据集上，GraphEXT在多种GNN架构中均优于基线方法，显著提升了模型的可解释性。

Conclusion: GraphEXT通过强调节点交互和结构变化对预测的影响，有效提升了GNN的可解释性。

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [27] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: 论文提出分子任务算术方法，通过负样本训练模型学习属性方向，无需正样本即可生成理想分子，实验证明其多样性和性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决理想分子稀缺问题，避免依赖正样本数据。

Method: 利用负样本训练模型学习属性方向，反向移动生成理想分子。

Result: 在20个零样本实验中，生成更多样且成功的分子设计，并在双目标和少样本任务中表现优异。

Conclusion: 分子任务算术因其简单、高效和性能，有望成为分子设计的标准迁移学习策略。

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [28] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: 本文提出了一种基于算子的神经网络MFNO，用于学习随机系统的动态，通过镜像填充处理非周期性输入，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决标准Fourier神经算子（FNO）无法处理非周期性输入的问题，同时提升随机系统动态学习的准确性和效率。

Method: 扩展FNO，引入镜像填充技术，结合Wong--Zakai定理和近似技术进行理论分析。

Result: MFNO在分辨率泛化和样本路径生成速度上优于LSTM、TCN和DeepONet等基线模型。

Conclusion: MFNO在理论和实验上均表现出色，为随机系统动态学习提供了高效且准确的解决方案。

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [29] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: 论文研究了在差分隐私机器学习中，当公共数据和私有数据存在显著分布偏移时的性能界限。结果表明，偏移较小时需大量数据，偏移较大时公共数据无益。


<details>
  <summary>Details</summary>
Motivation: 探讨在公共数据和私有数据分布偏移情况下，差分隐私算法的性能界限，填补现有研究的空白。

Method: 扩展了已知的公共-私有学习下界，分析高斯均值估计和高斯线性回归中分布偏移的影响。

Result: 偏移较小时需大量数据估计私有参数；偏移较大时公共数据无益。

Conclusion: 公共数据在分布偏移下的价值取决于偏移大小，为实际应用提供了理论指导。

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [30] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）在云机器人操作中的应用，分析了其优势、挑战及未来机会。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习需要集中式数据训练，而FL通过分布式设备训练共享模型，保护隐私。云机器人操作受限于计算资源，FL可解决这一问题。

Method: 介绍了FL的基本概念及其与云机器人操作的关联，探讨了集中式和分散式FL模型的设计与验证。

Result: FL在云机器人操作中具有潜力，但需解决效率和可靠性问题。

Conclusion: FL为云机器人操作提供了新思路，但需进一步研究以应对挑战。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [31] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: 该研究利用深度学习生成框架探索多孔超材料的逆向设计，开发了一种增强型变分自编码器（pVAE），用于生成具有定制水力特性的结构。


<details>
  <summary>Details</summary>
Motivation: 旨在通过深度学习方法高效设计具有特定水力特性的多孔超材料，减少传统模拟的计算成本。

Method: 结合变分自编码器（VAE）和回归器构建pVAE，利用卷积神经网络（CNN）预测水力特性，并通过LBM生成数据验证。

Result: pVAE成功生成了具有目标特性的多孔结构，并通过潜在空间分析展示了其高效的结构-特性映射能力。

Conclusion: 该方法为多孔超材料的逆向设计提供了高效工具，并开源了数据集和代码以促进进一步研究。

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [32] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: 本文提出了一种半经验学习理论（SETOL），解释了最先进神经网络（NNs）的卓越性能，并提供了重尾自正则化（HTSR）中关键量（alpha和alpha-hat）的理论来源。


<details>
  <summary>Details</summary>
Motivation: 研究最先进神经网络的性能来源，并为其提供理论解释。

Method: 结合统计力学、随机矩阵理论和量子化学方法，推导出理想学习的新数学前提条件，并引入新指标ERG。

Result: 在3层多层感知机（MLP）上验证了理论假设，并在最先进NNs中通过权重矩阵的谱密度估计层质量。

Conclusion: SETOL和HTSR的层质量指标（alpha和ERG）在MLP和最先进NNs中表现一致，验证了理论的有效性。

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [33] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: Seed2Harvest是一种混合红队方法，结合人类和机器生成对抗性提示的优势，用于扩展文化多样性的对抗性提示数据集。


<details>
  <summary>Details</summary>
Motivation: 当前对抗性提示数据集要么规模小且不平衡，要么缺乏人类提示的创造性和现实性，需要一种结合两者优势的方法。

Method: 提出Seed2Harvest，通过混合红队方法扩展人类生成的对抗性提示种子，保留人类提示的特征和攻击模式。

Result: 扩展后的数据集在多样性和攻击成功率上显著提升（535个地理位置，Shannon熵7.48；攻击成功率0.31-0.36）。

Conclusion: 人类与机器协作能有效结合创造性和计算能力，实现全面、可扩展的红队测试，持续评估T2I模型的安全性。

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [34] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: UrbanPulse是一种可扩展的深度学习框架，用于超细粒度、城市范围的OD流预测，通过将每个POI作为独立节点建模，结合时空依赖关系，实现跨城市的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在静态空间假设、跨城市泛化能力差、计算成本高或分辨率低等问题，限制了城市范围分析的实用性。

Method: UrbanPulse采用时间图卷积编码器和基于Transformer的解码器建模多尺度时空依赖，并通过三阶段迁移学习策略（预训练、冷启动适应和强化学习微调）实现跨城市泛化。

Result: 在加州三个大都市区的1.03亿条GPS记录上测试，UrbanPulse实现了最先进的准确性和可扩展性。

Conclusion: UrbanPulse通过高效迁移学习，为高分辨率AI城市预测的跨城市实际部署迈出了关键一步。

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [35] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: 论文提出MFTRR框架，通过多模态细粒度主题-帖子关系推理改进帖子质量评估，解决了现有方法的三个主要限制。


<details>
  <summary>Details</summary>
Motivation: 现有研究在帖子质量评估中存在三个问题：单模态分类无法利用多模态线索和细粒度质量区分，多模态融合引入噪声，以及缺乏捕捉复杂语义关系的能力。

Method: MFTRR框架将任务重新定义为排序问题，包含两个模块：局部-全局语义关联推理模块和多层次证据关系推理模块。

Result: 在三个新构建的多模态数据集和公开数据集上，MFTRR显著优于现有方法，NDCG@3提升最高达9.52%。

Conclusion: MFTRR通过模仿人类认知过程，有效提升了帖子质量评估的准确性和鲁棒性。

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [36] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE是一种两阶段Transformer模型，融合多模态视频、音频和文本特征预测fMRI活动，性能优于早期版本。


<details>
  <summary>Details</summary>
Motivation: 通过融合多模态特征提升fMRI活动预测的准确性。

Method: 使用开源模型提取特征，通过模态融合Transformer和时间解码Transformer进行预测。

Result: 在CNeuroMod数据集上训练，性能显著优于早期版本，并在Algonauts 2025挑战赛中表现优异。

Conclusion: VIBE展示了多模态融合在fMRI预测中的潜力，性能显著提升。

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


### [37] [Improving the Computational Efficiency and Explainability of GeoAggregator](https://arxiv.org/abs/2507.17977)
*Rui Deng,Ziqi Li,Mingshu Wang*

Main category: cs.LG

TL;DR: 本文改进了GeoAggregator（GA）模型，通过优化数据加载流程和引入模型集成策略及解释功能，提升了计算效率和模型可解释性。实验验证了改进后的GA在预测精度和推理速度上的提升。


<details>
  <summary>Details</summary>
Motivation: 准确建模和解释地理空间表格数据（GTD）对理解地理空间现象及其潜在过程至关重要。

Method: 1) 优化数据加载流程和模型前向传播；2) 引入模型集成策略和基于GeoShapley框架的后验解释功能。

Result: 改进后的GA在合成数据集上表现出更高的预测精度和推理速度，并能有效捕捉空间效应。

Conclusion: 改进后的GA模型在计算效率和可解释性上均有显著提升，代码已开源。

Abstract: Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).

</details>


### [38] [SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning](https://arxiv.org/abs/2507.17979)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.LG

TL;DR: SIFOTL是一种隐私保护且抗噪声的方法，用于识别表格数据中的数据偏移因素，在医疗数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决医疗数据中因隐私限制和复杂噪声导致的数据分析困难。

Method: 提取隐私合规的统计摘要，使用双XGBoost模型分离信号与噪声，结合LLM辅助，并通过Pareto加权决策树合并结果。

Result: 在MEPS和Synthea ABM数据集上，SIFOTL的F1分数显著优于基线方法，抗噪声能力强。

Conclusion: SIFOTL提供了一种可解释、隐私安全且抗噪声的数据分析方法。

Abstract: Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.

</details>


### [39] [Machine Unlearning of Traffic State Estimation and Prediction](https://arxiv.org/abs/2507.17984)
*Xin Wang,R. Tyrrell Rockafellar,Xuegang,Ban*

Main category: cs.LG

TL;DR: 提出了一种新的学习范式TSEP-Machine Unlearning，用于选择性遗忘敏感、有毒或过时的数据，以提升交通状态估计与预测的信任度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的交通状态估计与预测（TSEP）依赖敏感数据，引发隐私、网络安全和数据新鲜度问题，可能削弱公众对智能交通系统的信任。

Method: 引入TSEP-Machine Unlearning范式，使训练后的TSEP模型能够选择性遗忘隐私敏感、有毒或过时数据。

Result: 通过让模型“遗忘”特定数据，提升了数据驱动TSEP的可信度和可靠性。

Conclusion: TSEP-Machine Unlearning为解决隐私和数据安全问题提供了有效途径，增强了智能交通系统的信任度。

Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.

</details>


### [40] [Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models](https://arxiv.org/abs/2507.18014)
*Datta Nimmaturi,Vaishnavi Bhargava,Rajat Ghosh,Johnu George,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出了一种预测框架，用于优化基于GRPO的大语言模型微调的资源使用，通过实验发现训练可分为三个阶段，并建议提前停止以减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLM）用于推理任务的计算成本高昂，需要优化资源使用。

Method: 提出预测框架，建模训练动态，并通过实验在Llama和Qwen模型上验证，推导出基于模型大小、初始性能和训练进度的经验缩放规律。

Result: 发现训练可分为三个阶段（缓慢启动、快速提升、平台期），并证明超过一定轮次后训练收益有限。

Conclusion: 该方法可推广到不同模型类型，为基于GRPO的高效微调提供了实用指南。

Abstract: Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.

</details>


### [41] [Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents](https://arxiv.org/abs/2507.18067)
*Abdessamad El-Kabid,Loubna Benabbou,Redouane Lguensat,Alex Hernández-García*

Main category: cs.LG

TL;DR: 提出了一种基于神经算子的深度学习框架，用于解决偏微分方程并提供任意分辨率解，应用于Copernicus海洋数据降尺度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率海洋数据对沿海管理、环境监测和海上安全至关重要，但现有卫星数据和全球海洋模型缺乏足够空间粒度。

Method: 采用监督深度学习框架，基于神经算子解决偏微分方程，实现任意分辨率预测，并应用于Copernicus海洋数据降尺度。

Result: 在真实Copernicus海洋数据和合成Navier-Stokes模拟数据集上验证了模型的有效性。

Conclusion: 该方法能够为海洋数据提供高分辨率解，解决了现有数据空间粒度不足的问题。

Abstract: Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.

</details>


### [42] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: GSPO是一种稳定、高效且性能优越的强化学习算法，用于训练大语言模型，通过序列级优化显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统算法在token级重要性比率上的局限性，提出基于序列似然的序列级优化方法。

Method: 定义基于序列似然的重要性比率，进行序列级裁剪、奖励和优化。

Result: 相比GRPO算法，GSPO在训练效率和性能上表现更优，稳定了MoE RL训练，并简化了RL基础设施设计。

Conclusion: GSPO的优越性为最新Qwen3模型的显著改进提供了支持。

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


### [43] [C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams](https://arxiv.org/abs/2507.18072)
*Ryusei Fujimoto,Yugo Nakamura,Yutaka Arakawa*

Main category: cs.LG

TL;DR: C-AAE结合匿名自编码器和差分编码技术，有效保护用户隐私并减少数据量，同时保持活动识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备的行为数据可能泄露用户身份，隐私保护在医疗应用中至关重要。

Method: C-AAE结合匿名自编码器（AAE）和自适应差分脉冲编码调制（ADPCM），先通过AAE提取活动特征并抑制身份信息，再用ADPCM进一步编码以减少数据量和残留身份信息。

Result: 实验显示，C-AAE将用户重识别F1分数降低10-15%，同时活动识别F1分数仅下降5%，数据量减少约75%。

Conclusion: C-AAE在医疗传感器数据中平衡了隐私保护和实用性。

Abstract: Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.

</details>


### [44] [Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method](https://arxiv.org/abs/2507.18073)
*Qingcheng Zhu,Yangyang Ren,Linlin Yang,Mingbao Lin,Yanjing Li,Sheng Xu,Zichao Feng,Haodong Zhu,Yuguang Yang,Juan Zhang,Runqi Wang,Baochang Zhang*

Main category: cs.LG

TL;DR: Squeeze10-LLM是一种分阶段混合精度后训练量化框架，通过将80%的权重量化为1位和20%量化为4位，平均每个权重仅1.6位，显著降低LLMs的存储和计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）部署因参数庞大和计算成本高而具有挑战性，极低比特量化虽能减少存储和加速推理，但极端压缩（如平均比特宽度≤2）常导致性能严重下降。

Method: 提出Squeeze10-LLM框架，结合Post-Binarization Activation Robustness（PBAR）和Full Information Activation Supervision（FIAS）两项创新，PBAR改进权重重要性度量，FIAS保留完整激活信息以减少误差传播。

Result: 在LLaMA和LLaMA2上的实验表明，Squeeze10-LLM在低于2位的仅权重量化中达到最佳性能，六项零样本分类任务的平均准确率从43%提升至56%。

Conclusion: Squeeze10-LLM通过混合精度量化和创新技术，显著提升了极低比特量化下的模型性能，为LLMs的高效部署提供了有效解决方案。

Abstract: Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.

</details>


### [45] [Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes](https://arxiv.org/abs/2507.18098)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，通过将硬标签和额外监督信息结合为概率分布来构建软标签，揭示了额外监督的关键作用，并通过实验验证了其提升分类准确性的潜力。


<details>
  <summary>Details</summary>
Motivation: 在训练数据有限的情况下，如何利用额外监督信息（如标签置信度）提升分类模型的准确性是一个关键问题。

Method: 提出理论框架，将硬标签和额外监督视为概率分布，并通过仿射组合构建软标签。

Result: 理论分析表明，额外监督的关键在于非硬标签类的分布信息，而非硬标签的置信度。实验验证了该方法能提升分类准确性。

Conclusion: 额外监督及其混合系数在软标签优化中起互补作用，理论框架为设计更有效的监督信息提供了指导。

Abstract: In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.

</details>


### [46] [Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN](https://arxiv.org/abs/2507.18111)
*Peyman Tehrani,Anas Alsoliman*

Main category: cs.LG

TL;DR: 论文提出了一种基于百分位数的延迟感知深度强化学习（PDA-DRL）方法，用于解决开放无线接入网（O-RAN）中的网络切片问题，显著降低了平均延迟，并提出了基于奖励的个性化模型权重共享方法。


<details>
  <summary>Details</summary>
Motivation: 解决开放无线接入网（O-RAN）中多个移动虚拟网络运营商（MVNOs）竞争物理资源块（PRBs）的问题，同时满足客户端的概率延迟上限约束并最小化PRB利用率。

Method: 基于大数定律（LLN）设计奖励函数，提出PDA-DRL方法，并引入基于奖励的个性化模型权重共享技术。

Result: PDA-DRL方法比优化平均延迟约束的DRL模型降低了38%的平均延迟，且个性化权重共享方法优于传统聚合方法。

Conclusion: PDA-DRL方法在降低延迟和优化资源分配方面表现出色，个性化权重共享技术为多MVNOs协作提供了新思路。

Abstract: In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.

</details>


### [47] [Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification](https://arxiv.org/abs/2507.18113)
*Junyong Jiang,Buwei Tian,Chenxing Xu,Songze Li,Lu Dong*

Main category: cs.LG

TL;DR: 提出了一种新的对抗攻击方法，利用环境中的现有代理引导目标策略输出次优动作，无需修改环境。结合LLM生成针对性奖励和关键状态识别算法，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习在多个领域取得成功，但对抗攻击仍具挑战性。现有方法通常需要修改环境或策略，实用性受限。

Method: 提出奖励迭代优化框架，利用LLM生成针对目标代理漏洞的对抗性奖励，并设计关键状态识别算法。

Result: 在多样环境中实验验证了方法的优越性。

Conclusion: 该方法有效诱导目标代理做出次优决策，且无需修改环境，具有更高的实用性。

Abstract: Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.

</details>


### [48] [Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning](https://arxiv.org/abs/2507.18122)
*Matthias Otth,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: 语言模型通过最大化自身预测置信度实现自我提升，无需外部验证。研究发现，在数学推理任务中，基于前缀置信度的选择能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在数学推理任务中如何通过自我置信度选择最优解，以提升性能。

Method: 使用前缀置信度选择最有希望的尝试，并在五个数学推理数据集上评估其效果。

Result: 前缀置信度缩放（32个标记）在准确性与计算效率上优于多数投票，且对长度偏差更不敏感。

Conclusion: 前缀置信度缩放是一种有效的自我提升方法，优于测试时训练和多数投票。

Abstract: Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.

</details>


### [49] [Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2507.18139)
*Alberto Marchisio,Muhammad Shafique*

Main category: cs.LG

TL;DR: 本文综述了神经形态计算在自主系统中的最新进展，重点关注算法、硬件和优化策略，以及事件动态视觉传感器的作用。


<details>
  <summary>Details</summary>
Motivation: 满足对智能、自适应和节能自主系统（如机器人、无人机和自动驾驶车辆）的需求，通过仿生神经形态方法提升感知、决策和响应能力。

Method: 调查神经形态算法、专用硬件和跨层优化策略，特别关注事件动态视觉传感器和脉冲神经网络的应用。

Result: 提出了提高能效、鲁棒性、适应性和可靠性的新方法，并整合了机器学习、机器人学、神经科学和神经形态工程的多学科视角。

Conclusion: 探讨了实时决策、持续学习以及安全、弹性自主系统开发中的新兴趋势和开放挑战。

Abstract: The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.

</details>


### [50] [When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label](https://arxiv.org/abs/2507.18153)
*Riting Xia,Rucong Wang,Yulin Liu,Anchen Li,Xueyan Liu,Yan Zhang*

Main category: cs.LG

TL;DR: 论文提出GraphALP框架，结合LLM和伪标签技术，解决带噪声标签的类别不平衡图节点分类问题。


<details>
  <summary>Details</summary>
Motivation: 现实图中的标签常含噪声，现有研究假设标签干净，与实际不符。

Method: 基于LLM的过采样生成少数类节点，动态加权伪标签减少噪声，二次LLM过采样避免分布偏移。

Result: GraphALP在带噪声标签的类别不平衡图上优于现有方法。

Conclusion: GraphALP有效解决了噪声标签和类别不平衡问题，性能优越。

Abstract: Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.

</details>


### [51] [ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory](https://arxiv.org/abs/2507.18183)
*Jianchao Wang,Qingfeng Li,Pengcheng Zheng,Xiaorong Pu,Yazhou Ren*

Main category: cs.LG

TL;DR: ChronoSelect是一个新颖的框架，通过四阶段内存架构和滑动更新机制，动态分析样本的时序轨迹，将样本划分为干净、边界和噪声子集，显著提升了噪声标签下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中噪声标签的存在会损害深度神经网络的泛化性能，现有方法未能充分利用学习演化的时序动态。

Method: 提出ChronoSelect框架，采用四阶段内存架构压缩预测历史为时序分布，通过滑动更新机制和双分支一致性实现样本划分。

Result: 理论证明框架在噪声条件下的收敛性和稳定性，实验验证其在合成和真实数据集上的先进性能。

Conclusion: ChronoSelect通过动态时序分析有效解决了噪声标签问题，性能优于现有方法。

Abstract: Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.

</details>


### [52] [Goal-based Trajectory Prediction for improved Cross-Dataset Generalization](https://arxiv.org/abs/2507.18196)
*Daniel Grimm,Ahmed Abouelazm,J. Marius Zöllner*

Main category: cs.LG

TL;DR: 提出了一种基于异构图神经网络的模型，通过多阶段目标分类提升自动驾驶中对未见过场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要准确预测交通参与者的未来状态，但现有模型在新场景中泛化能力不足。

Method: 使用包含交通参与者和矢量化路网的异构图神经网络，通过多阶段目标分类预测轨迹终点。

Result: 在跨数据集评估（Argoverse2训练，NuScenes测试）中验证了目标选择过程的有效性。

Conclusion: 该方法显著提升了模型在新场景中的泛化性能。

Abstract: To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.

</details>


### [53] [FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting](https://arxiv.org/abs/2507.18219)
*Zhongzheng Yuan,Lianshuai Guo,Xunkai Li,Yinlin Zhu,Wenyu Wang,Meixia Qu*

Main category: cs.LG

TL;DR: FedSA-GCL是一种半异步联邦图学习框架，通过ClusterCast机制解决现有同步通信和异步方法的不足，提升了效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法依赖同步通信效率低，而异步方法未考虑图数据的拓扑特性，导致语义漂移和表示不一致。

Method: 提出FedSA-GCL框架，结合客户端标签分布差异和图拓扑特性，采用ClusterCast机制进行高效训练。

Result: 在多个真实图数据集上测试，FedSA-GCL平均优于基线方法2.92%（Louvain）和3.4%（Metis）。

Conclusion: FedSA-GCL在效率和鲁棒性上表现优异，适用于大规模分布式图学习。

Abstract: Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.

</details>


### [54] [Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective](https://arxiv.org/abs/2507.18220)
*Ansei Yonezawa,Heisei Yonezawa,Shuichi Yahagi,Itsuro Kajiwara,Shinya Kijimoto,Hikaru Taniuchi,Kentaro Murakami*

Main category: cs.LG

TL;DR: SINDy-LOM结合稀疏回归技术和新型库学习策略，优化基础函数设计，提升模型的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统SINDy方法在库设计上存在挑战，难以适用于复杂动力系统。

Method: 采用双层优化架构：内层稀疏回归提取模型，外层基于递归长期预测精度优化基础函数。

Result: SINDy-LOM在柴油发动机空气路径系统中验证了其有效性，模型更简洁可靠。

Conclusion: SINDy-LOM显著减轻用户负担，提升模型长期预测能力。

Abstract: The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.

</details>


### [55] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: 本文首次对六种基于线性规划的完全校正提升方法进行了大规模实验研究，包括两种新方法NM-Boost和QRLP-Boost，结果表明这些方法在使用浅层树时能优于或匹敌XGBoost等先进方法，同时生成更稀疏的集成模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于线性规划的完全校正提升方法在理论上具有吸引力，但缺乏实证研究，本文旨在填补这一空白。

Method: 在20个多样化数据集上评估了六种LP提升方法，包括NM-Boost和QRLP-Boost，并比较了启发式和最优基学习器的效果，分析了准确性、集成稀疏性、边际分布等指标。

Result: 完全校正方法在使用浅层树时表现优于或匹敌XGBoost等先进方法，且能生成更稀疏的集成模型，还能在不牺牲性能的情况下精简预训练集成。

Conclusion: 完全校正提升方法在特定条件下具有竞争力，但使用最优决策树时存在局限性。

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [56] [Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring](https://arxiv.org/abs/2507.18293)
*Sjoerd van Straten,Alessandro Padella,Marwan Hassani*

Main category: cs.LG

TL;DR: SiamSA-PPM是一种结合Siamese学习和统计增强的自监督学习框架，用于预测过程监控，通过生成语义有效的轨迹变体提升数据多样性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习PPM方法因真实事件日志数据量小和多样性低而受限的问题。

Method: 结合Siamese学习和统计增强，利用三种基于控制流语义和频繁行为模式的转换方法生成新轨迹变体。

Result: 在真实事件日志上表现出色，优于现有方法，尤其在数据多样性和预测任务中。

Conclusion: SiamSA-PPM为过程预测中的数据增强提供了有前景的方向。

Abstract: Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.

</details>


### [57] [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://arxiv.org/abs/2507.18297)
*Sergei Shumilin,Alexander Ryabov,Nikolay Yavich,Evgeny Burnaev,Vladimir Vanovskiy*

Main category: cs.LG

TL;DR: 提出了一种基于可微分物理的无结构网格粗化算法，通过k均值聚类、自动微分和随机最小化算法，显著减少网格点数同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现代数值模拟计算负载高，需要减少离散问题规模同时保持合理精度。

Method: 使用k均值聚类、自动微分和随机最小化算法设计网格粗化算法。

Result: 在线性抛物方程和波动方程中，网格点数减少10倍，同时保持关注点的变量动态。

Conclusion: 该方法适用于任意由演化偏微分方程描述的系统模拟。

Abstract: Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.

</details>


### [58] [Regression-aware Continual Learning for Android Malware Detection](https://arxiv.org/abs/2507.18313)
*Daniele Ghiani,Daniele Angioni,Giorgio Piras,Angelo Sotgiu,Luca Minnei,Srishti Gupta,Maura Pintor,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 论文分析了机器学习恶意软件检测中的安全回归问题，提出了一种回归感知的惩罚方法，以减少模型更新时的有害预测变化。


<details>
  <summary>Details</summary>
Motivation: 恶意软件快速演变，传统完全重新训练不切实际，而持续学习可能引发安全回归问题，即模型更新后对某些样本的预测变差，这对安全关键应用构成严重风险。

Method: 通过形式化和量化安全回归，提出了一种回归感知的惩罚方法，并调整了Positive Congruent Training（PCT）以适应持续学习环境。

Result: 在ELSA、Tesseract和AZ-Class数据集上的实验表明，该方法有效减少了不同持续学习场景下的回归，同时保持了良好的检测性能。

Conclusion: 该研究为持续学习中的安全回归问题提供了解决方案，有助于提升恶意软件检测系统的可靠性和用户信任。

Abstract: Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.

</details>


### [59] [State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer](https://arxiv.org/abs/2507.18320)
*Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出了一种名为TIDSIT的新架构，用于处理电池健康监测中的不规则时间序列数据，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 电池健康监测对安全和效率至关重要，但现有模型难以处理现实世界中的不规则数据。

Method: 提出TIDSIT架构，结合连续时间嵌入和时序注意力机制，处理非均匀采样和变长序列。

Result: 在NASA电池退化数据集上，TIDSIT预测误差降低50%以上，误差低于0.58%。

Conclusion: TIDSIT在电池健康监测中表现优异，且适用于其他不规则时间序列任务。

Abstract: The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.

</details>


### [60] [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333)
*Kale-ab Abebe Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 论文指出当前多智能体强化学习（MARL）算法通过简单惯例而非有效信号恢复取得成功，但这类惯例在与非自适应智能体合作时易失效。建议设计新环境以测试真实技能。


<details>
  <summary>Details</summary>
Motivation: 探讨当前MARL算法在Dec-POMDP框架下的实际表现，揭示其依赖简单惯例而非环境观察与记忆的局限性。

Method: 通过案例研究分析智能体学习惯例的脆弱性，并展示任务设计对策略学习的影响。

Result: 发现当前MARL环境未能充分测试Dec-POMDP核心假设，且智能体策略易受任务设计影响。

Conclusion: 呼吁设计新环境，要求智能体基于观察和记忆推理，以测试真实合作能力。

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.

</details>


### [61] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets通过低秩适应（LoRA）降低HyperDeepONets的复杂度，减少参数数量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决HyperDeepONets因高表达性导致的高内存和计算成本问题。

Method: 在物理信息机器学习中引入LoRA，将超网络的输出层权重矩阵分解为两个低秩矩阵。

Result: 参数减少70%，预测精度和泛化能力优于常规HyperDeepONets。

Conclusion: PI-LoRA-HyperDeepONets在减少参数的同时提升了性能，适用于微分方程求解。

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


### [62] [A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges](https://arxiv.org/abs/2507.18376)
*Xing Hua,Haodong Chen,Qianqian Duan,Danfeng Hong,Ruijiao Li,Huiliang Shang,Linghua Jiang,Haima Yang,Dawei Zhang*

Main category: cs.LG

TL;DR: 综述了扩散模型在农业中的应用，包括病虫害检测、遥感图像增强等，展示了其在数据增强和图像生成中的优势。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长和耕地资源稀缺促使智能农业和精准农业成为发展方向，扩散模型因其稳定性和生成质量优于传统方法。

Method: 回顾扩散模型在农业中的最新应用，重点分析其在病虫害检测、遥感图像增强等任务中的潜力。

Result: 实验表明扩散模型显著提升了数据增强、图像生成和去噪的准确性和鲁棒性。

Conclusion: 尽管存在计算效率和泛化能力的挑战，扩散模型有望在智能农业中发挥更大作用，支持全球农业可持续发展。

Abstract: With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.

</details>


### [63] [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](https://arxiv.org/abs/2507.18423)
*Mizuki Funato,Yohei Sawada*

Main category: cs.LG

TL;DR: HYPER是一种结合多模型集成和储层计算的新方法，用于在数据稀缺条件下提高洪水预测和水管理的准确性。


<details>
  <summary>Details</summary>
Motivation: 许多地区缺乏足够的河流流量观测数据，限制了降雨-径流分析的准确性，现有模型在数据稀缺条件下难以同时实现高精度、可解释性和计算效率。

Method: HYPER方法首先对43个未校准的概念水文模型应用贝叶斯模型平均（BMA），然后通过储层计算（RC）模型进行误差校正，计算高效且无需迭代。对于无测站流域，通过关联有测站流域的属性推断权重。

Result: 在数据丰富和稀缺场景下，HYPER表现优于基准LSTM模型，计算时间仅为5%，且在数据稀缺时性能稳健（KGE 0.55）。

Conclusion: HYPER提供了一种高效、稳健且可推广的解决方案，特别适用于无测站流域的流量预测。

Abstract: Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.

</details>


### [64] [Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning](https://arxiv.org/abs/2507.18519)
*Leiji Zhang,Zeyu Wang,Xin Li,Yao-Hui Li*

Main category: cs.LG

TL;DR: 论文提出了一种改进的双模拟度量方法，解决了传统方法在表示特定场景和依赖预定义权重方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量存在无法表示某些独特场景以及依赖预定义权重的问题，限制了其在强化学习任务中的表现。

Method: 通过引入状态-动作对的度量，提出了一种改进的双模拟度量，包括更精确的奖励差距定义和自适应系数的更新算子。

Result: 理论分析证明了新度量的收敛性和表示独特性，实验在DeepMind Control和Meta-World基准上验证了其有效性。

Conclusion: 改进的双模拟度量在理论和实验上均表现出优越性，为强化学习任务提供了更有效的表示学习技术。

Abstract: Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.

</details>


### [65] [GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning](https://arxiv.org/abs/2507.18521)
*Zhongtian Sun,Anoushka Harit,Alexandra Cristea,Christl A. Donnelly,Pietro Liò*

Main category: cs.LG

TL;DR: GLANCE框架通过逻辑引导、动态图优化和自适应聚类，提升了异质图上的图神经网络性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质图上表现不佳，因邻居聚合和结构模式利用不足。

Method: GLANCE结合逻辑层、多头注意力边剪枝和聚类机制。

Result: 在Cornell等数据集上表现优异，提供轻量且可解释的解决方案。

Conclusion: GLANCE适用于异质图，性能强且可解释。

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.

</details>


### [66] [C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation](https://arxiv.org/abs/2507.18533)
*Magnus Bengtsson,Kenneth Östberg*

Main category: cs.LG

TL;DR: C2G-KD是一种无需真实数据、通过教师模型和PCA几何约束生成合成样本的知识蒸馏框架。


<details>
  <summary>Details</summary>
Motivation: 解决无需真实数据但仍能有效进行知识蒸馏的问题。

Method: 使用类条件生成器，结合教师模型的输出和PCA几何约束，通过语义和结构损失生成样本。

Result: 在MNIST上实验表明，即使每类仅用两个真实样本也能生成有效的合成训练数据。

Conclusion: C2G-KD展示了在极少量真实数据下仍能保持拓扑一致性和多样性的潜力。

Abstract: We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.

</details>


### [67] [The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection](https://arxiv.org/abs/2507.18549)
*Steven A. Frank*

Main category: cs.LG

TL;DR: 论文通过Price方程提出了一个通用的力-度量-偏置（FMB）定律，统一了多种学习算法和自然选择，揭示了其共同的数学结构。


<details>
  <summary>Details</summary>
Motivation: 揭示不同学习算法、优化方法和自然选择背后的共同数学结构，为理解和设计跨学科的学习算法提供理论基础。

Method: 使用Price方程对变化进行简单的符号划分，提出FMB定律：Δθ = Mf + b + ξ，其中力f驱动参数改进，度量M重新缩放运动，偏置b添加动量或改变参考系，噪声ξ支持探索。

Result: FMB定律统一了自然选择、贝叶斯更新、牛顿法、随机梯度下降、Adam优化等多种算法，并解释了Fisher信息、KL散度等在学习动力学中的自然出现。

Conclusion: FMB定律为跨学科的学习算法提供了统一的理论基础，有助于算法的理解和设计。

Abstract: Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.

</details>


### [68] [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)
*Jiale Chen,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文揭示了GPTQ量化方法与Babai最近平面算法的数学等价性，为量化算法提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究GPTQ量化方法的内在机制，揭示其几何意义和误差上限，为大规模语言模型的量化提供理论依据。

Method: 通过数学论证，证明GPTQ与Babai最近平面算法在特定条件下的等价性。

Result: GPTQ的误差传播具有几何解释，且在无裁剪条件下继承了Babai算法的误差上限。

Conclusion: GPTQ的理论基础得到巩固，为未来大规模模型量化算法的设计提供了新思路。

Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.

</details>


### [69] [Linear Memory SE(2) Invariant Attention](https://arxiv.org/abs/2507.18597)
*Ethan Pronovost,Neha Boloor,Peter Schleede,Noureldin Hendy,Andres Morales,Nicholas Roy*

Main category: cs.LG

TL;DR: 提出了一种线性内存消耗的SE(2)不变性注意力机制，用于自动驾驶中的空间数据处理。


<details>
  <summary>Details</summary>
Motivation: 现有SE(2)不变性网络架构需要显式计算所有对象对的相对位姿，导致二次内存消耗，限制了其在大规模场景中的应用。

Method: 设计了一种SE(2)不变性的缩放点积注意力机制，仅需线性内存。

Result: 实验证明该方法易于实现，且性能优于非不变性架构。

Conclusion: 提出的SE(2)不变性Transformer架构在性能和内存效率上均有优势，适用于大规模场景。

Abstract: Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.

</details>


### [70] [Demystify Protein Generation with Hierarchical Conditional Diffusion Models](https://arxiv.org/abs/2507.18603)
*Zinan Ling,Yi Shi,Da Yan,Yang Zhou,Bo Hui*

Main category: cs.LG

TL;DR: 提出了一种多级条件扩散模型，结合序列和结构信息进行端到端蛋白质设计，并引入新的评估指标Protein-MMD。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列的可靠生成是蛋白质设计中的关键问题，尤其是条件扩散模型的应用仍存在挑战。

Method: 提出多级条件扩散模型，整合序列和结构信息，同时生成多级表示，并引入Protein-MMD评估指标。

Result: 实验结果表明，该框架和评估指标在条件蛋白质生成任务中表现优异。

Conclusion: 多级条件扩散模型和Protein-MMD为蛋白质设计提供了高效且可靠的解决方案。

Abstract: Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.

</details>


### [71] [Moving Out: Physically-grounded Human-AI Collaboration](https://arxiv.org/abs/2507.18623)
*Xuhui Kang,Sung-Wook Lee,Haolin Liu,Yuyan Wang,Yen-Ling Kuo*

Main category: cs.LG

TL;DR: 论文提出了一种新的人机协作基准测试Moving Out，用于评估AI在物理约束下的适应能力，并提出了BASS方法提升协作效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决物理环境中人机协作的复杂性和动态约束问题，提升AI在连续状态-动作空间中的适应能力。

Method: 提出了BASS方法（行为增强、模拟和选择），通过增强行为多样性和动作结果理解来优化协作。

Result: 实验表明BASS在AI-AI和人机协作中优于现有模型。

Conclusion: Moving Out基准和BASS方法为物理环境中的人机协作提供了有效解决方案。

Abstract: The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [72] [Sliding Window Informative Canonical Correlation Analysis](https://arxiv.org/abs/2507.17921)
*Arvind Prasadan*

Main category: stat.ML

TL;DR: 提出了一种名为SWICCA的在线流数据CCA扩展方法，结合滑动窗口和流式PCA实时估计CCA组件。


<details>
  <summary>Details</summary>
Motivation: 解决传统CCA无法处理在线流数据的问题。

Method: 使用流式PCA算法和滑动窗口样本实时估计CCA组件。

Result: 通过数值模拟和理论性能保证验证了方法的有效性和高维扩展性。

Conclusion: SWICCA适用于高维数据，并在实际数据中展示了其能力。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlated
sets of features between two datasets. In this paper, we propose a novel
extension of CCA to the online, streaming data setting: Sliding Window
Informative Canonical Correlation Analysis (SWICCA). Our method uses a
streaming principal component analysis (PCA) algorithm as a backend and uses
these outputs combined with a small sliding window of samples to estimate the
CCA components in real time. We motivate and describe our algorithm, provide
numerical simulations to characterize its performance, and provide a
theoretical performance guarantee. The SWICCA method is applicable and scalable
to extremely high dimensions, and we provide a real-data example that
demonstrates this capability.

</details>


### [73] [A Two-armed Bandit Framework for A/B Testing](https://arxiv.org/abs/2507.18118)
*Jinjuan Wang,Qianglin Wen,Yu Zhang,Xiaodong Yan,Chengchun Shi*

Main category: stat.ML

TL;DR: 本文提出了一种基于两臂老虎机框架的A/B测试方法，通过双重稳健估计、构建测试统计量和置换法计算p值，显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: A/B测试在技术公司中广泛用于政策评估和产品部署，但现有方法在统计功效上存在不足，需要更高效的框架。

Method: 采用双重稳健估计生成伪结果，利用两臂老虎机框架构建测试统计量，并通过置换法计算p值。

Result: 通过理论分析、数值实验和真实数据验证，证明该方法在统计功效上优于现有方法。

Conclusion: 提出的两臂老虎机框架显著提升了A/B测试的统计功效，适用于实际应用。

Abstract: A/B testing is widely used in modern technology companies for policy
evaluation and product deployment, with the goal of comparing the outcomes
under a newly-developed policy against a standard control. Various causal
inference and reinforcement learning methods developed in the literature are
applicable to A/B testing. This paper introduces a two-armed bandit framework
designed to improve the power of existing approaches. The proposed procedure
consists of three main steps: (i) employing doubly robust estimation to
generate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to
construct the test statistic, and (iii) applying a permutation-based method to
compute the $p$-value. We demonstrate the efficacy of the proposed method
through asymptotic theories, numerical experiments and real-world data from a
ridesharing company, showing its superior performance in comparison to existing
methods.

</details>


### [74] [Learning graphons from data: Random walks, transfer operators, and spectral clustering](https://arxiv.org/abs/2507.18147)
*Stefan Klus,Jason J. Bramburger*

Main category: stat.ML

TL;DR: 论文提出了一种将信号随机过程与图上的随机游走联系起来的方法，利用图论中的图子（graphon）理论，扩展了传统谱聚类方法，并展示了如何从信号数据中估计转移概率密度和重建图子。


<details>
  <summary>Details</summary>
Motivation: 研究信号在时间上的随机演化过程，尤其是信号在离散时间点上的状态切换，需要一种新的理论框架来连接随机过程与图论。

Method: 引入与图子上随机游走相关的转移算子（如Koopman和Perron-Frobenius算子），并通过信号数据估计这些算子的特征值和特征函数，用于聚类分析。

Result: 方法成功应用于合成和真实信号（如气温和股票指数），能够检测聚类并重建转移概率密度和可逆随机游走的图子。

Conclusion: 研究扩展了谱聚类方法到图子领域，提供了一种数据驱动的方法来分析信号随机过程。

Abstract: Many signals evolve in time as a stochastic process, randomly switching
between states over discretely sampled time points. Here we make an explicit
link between the underlying stochastic process of a signal that can take on a
bounded continuum of values and a random walk process on a graphon. Graphons
are infinite-dimensional objects that represent the limit of convergent
sequences of graphs whose size tends to infinity. We introduce transfer
operators, such as the Koopman and Perron--Frobenius operators, associated with
random walk processes on graphons and then illustrate how these operators can
be estimated from signal data and how their eigenvalues and eigenfunctions can
be used for detecting clusters, thereby extending conventional spectral
clustering methods from graphs to graphons. Furthermore, we show that it is
also possible to reconstruct transition probability densities and, if the
random walk process is reversible, the graphon itself using only the signal.
The resulting data-driven methods are applied to a variety of synthetic and
real-world signals, including daily average temperatures and stock index
values.

</details>


### [75] [On Reconstructing Training Data From Bayesian Posteriors and Trained Models](https://arxiv.org/abs/2507.18372)
*George Wynne*

Main category: stat.ML

TL;DR: 论文提出了一种数学框架来应对训练数据重构攻击，通过最大均值差异等价性识别易受攻击的数据特征，并首次在文献中提出贝叶斯模型的分数匹配重构方法。


<details>
  <summary>Details</summary>
Motivation: 公开模型规范及训练参数可能导致训练数据被重构攻击，这是现代机器学习的主要漏洞。

Method: 建立数学框架，通过最大均值差异等价性分析易受攻击的数据特征，并提出贝叶斯和非贝叶斯模型的分数匹配重构方法。

Result: 首次在文献中实现了贝叶斯模型的训练数据重构。

Conclusion: 论文为防范训练数据重构攻击提供了理论和方法支持。

Abstract: Publicly releasing the specification of a model with its trained parameters
means an adversary can attempt to reconstruct information about the training
data via training data reconstruction attacks, a major vulnerability of modern
machine learning methods. This paper makes three primary contributions:
establishing a mathematical framework to express the problem, characterising
the features of the training data that are vulnerable via a maximum mean
discrepancy equivalance and outlining a score matching framework for
reconstructing data in both Bayesian and non-Bayesian models, the former is a
first in the literature.

</details>


### [76] [DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts](https://arxiv.org/abs/2507.18464)
*Miguel Aspis,Sebastián A. Cajas Ordónez,Andrés L. Suárez-Cetrulo,Ricardo Simón Carbajo*

Main category: stat.ML

TL;DR: DriftMoE是一种在线混合专家架构，通过协同训练框架解决现有自适应集成方法的局限性，实现高效的概念漂移适应。


<details>
  <summary>Details</summary>
Motivation: 现有自适应集成方法通常依赖粗粒度适应机制或简单投票方案，无法充分利用专家知识。

Method: DriftMoE采用紧凑的神经路由器和增量Hoeffding树专家池协同训练，通过反馈循环加速专家专业化。

Result: 在九种数据流学习基准测试中，DriftMoE表现优异，与最先进的自适应集成方法竞争。

Conclusion: DriftMoE提供了一种高效且原则性的概念漂移适应方法，代码和数据已开源。

Abstract: Learning from non-stationary data streams subject to concept drift requires
models that can adapt on-the-fly while remaining resource-efficient. Existing
adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or
simple voting schemes that fail to optimally leverage specialized knowledge.
This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture
that addresses these limitations through a novel co-training framework.
DriftMoE features a compact neural router that is co-trained alongside a pool
of incremental Hoeffding tree experts. The key innovation lies in a symbiotic
learning loop that enables expert specialization: the router selects the most
suitable expert for prediction, the relevant experts update incrementally with
the true label, and the router refines its parameters using a multi-hot
correctness mask that reinforces every accurate expert. This feedback loop
provides the router with a clear training signal while accelerating expert
specialization. We evaluate DriftMoE's performance across nine state-of-the-art
data stream learning benchmarks spanning abrupt, gradual, and real-world drifts
testing two distinct configurations: one where experts specialize on data
regimes (multi-class variant), and another where they focus on single-class
specialization (task-based variant). Our results demonstrate that DriftMoE
achieves competitive results with state-of-the-art stream learning adaptive
ensembles, offering a principled and efficient approach to concept drift
adaptation. All code, data pipelines, and reproducibility scripts are available
in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.

</details>


### [77] [Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise](https://arxiv.org/abs/2507.18520)
*Keyi Li,Yuval Kluger,Boris Landa*

Main category: stat.ML

TL;DR: 论文提出了一种无超参数的方法，用于估计异方差噪声下的噪声幅度并校正欧氏距离，适用于高维数据且无需先验知识。


<details>
  <summary>Details</summary>
Motivation: 许多机器学习算法依赖欧氏距离，但异方差噪声会扭曲距离计算，影响数据几何表示。

Method: 开发了一种联合估计噪声幅度和校正距离的数学方法，提供理论保证。

Result: 理论和实验表明，该方法能准确估计噪声和距离，显著提升距离计算的鲁棒性。

Conclusion: 该方法在合成和单细胞RNA测序数据中表现优异，为下游分析提供了可靠基础。

Abstract: Pairwise Euclidean distance calculation is a fundamental step in many machine
learning and data analysis algorithms. In real-world applications, however,
these distances are frequently distorted by heteroskedastic
noise$\unicode{x2014}$a prevalent form of inhomogeneous corruption
characterized by variable noise magnitudes across data observations. Such noise
inflates the computed distances in a nontrivial way, leading to
misrepresentations of the underlying data geometry. In this work, we address
the tasks of estimating the noise magnitudes per observation and correcting the
pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,
we show that in general high-dimensional settings and without assuming prior
knowledge on the clean data structure or noise distribution, both tasks can be
performed reliably, even when the noise levels vary considerably. Specifically,
we develop a principled, hyperparameter-free approach that jointly estimates
the noise magnitudes and corrects the distances. We provide theoretical
guarantees for our approach, establishing probabilistic bounds on the
estimation errors of both noise magnitudes and distances. These bounds,
measured in the normalized $\ell_1$ norm, converge to zero at polynomial rates
as both feature dimension and dataset size increase. Experiments on synthetic
datasets demonstrate that our method accurately estimates distances in
challenging regimes, significantly improving the robustness of subsequent
distance-based computations. Notably, when applied to single-cell RNA
sequencing data, our method yields noise magnitude estimates consistent with an
established prototypical model, enabling accurate nearest neighbor
identification that is fundamental to many downstream analyses.

</details>
