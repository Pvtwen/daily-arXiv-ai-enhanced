<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [cs.LG](#cs.LG) [Total: 69]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [GNSS Jammer Direction Finding in Dynamic Scenarios Using an Inertial-based Multi-Antenna System](https://arxiv.org/abs/2512.05128)
*Lucas Heublein,Thorsten Nowak,Tobias Feigl,Jaspar Pahl,Felix Ott*

Main category: eess.SP

TL;DR: 论文提出了一种结合软件定义无线电、IMU和合成孔径技术的GNSS干扰源定位方法，通过融合IQ数据、频谱图和22个AoA特征来提升动态场景下的干扰源方向估计精度。


<details>
  <summary>Details</summary>
Motivation: GNSS干扰设备会破坏卫星导航信号，威胁定位可靠性，因此需要检测和定位干扰源以实现态势感知、减轻影响并实施有效对抗措施。

Method: 使用2×2贴片天线系统（Ettus USRP X440）采集IQ样本，结合IMU预测天线相对运动，采用合成孔径系统通过平台运动合成更大虚拟孔径，融合IQ数据、FFT计算的频谱图和22个AoA特征来增强GNSS干扰源方向估计。

Result: 该方法能够预测干扰源的角度、仰角和距离，在动态场景和多径环境中提供更精确的定位，相比传统AoA方法在多径环境下有更好的性能。

Conclusion: 提出的融合IQ、频谱图、AoA特征和IMU运动预测的方法能够有效提升GNSS干扰源定位精度，特别是在多径传播的动态环境中，为干扰检测和对抗提供了更可靠的解决方案。

Abstract: Jamming devices disrupt signals from the global navigation satellite system (GNSS) and pose a significant threat by compromising the reliability of accurate positioning. Consequently, the detection and localization of these interference signals are essential to achieve situational awareness, mitigating their impact, and implementing effective countermeasures. In this paper, we utilize a two-times-two patch antenna system (i.e., the software defined radio device Ettus USRP X440) to predict the angle, elevation, and distance to the jamming source based on in-phase and quadrature (IQ) samples. We propose to use an inertial measurement unit (IMU) attached to the antenna system to predict the relative movement of the antenna in dynamic scenarios. We present a synthetic aperture system that enables coherent spatial imaging using platform motion to synthesize larger virtual apertures, offering superior angular resolution without mechanically rotating antennas. While classical angle-of-arrival (AoA) methods exhibit reduced accuracy in multipath environments due to signal reflections and scattering, leading to localization errors, we utilize a methodology that fuses IQ and Fast Fourier Transform (FFT)-computed spectrograms with 22 AoA features and the predicted relative movement to enhance GNSS jammer direction finding.

</details>


### [2] [Elevation- and Tilt-Aware Shadow Fading Correlation Modeling for UAV Communications](https://arxiv.org/abs/2512.05332)
*Mushfiqur Rahman,Ismail Guvenc,Mihail Sichitiu,Jason A. Abrahamson,Bryton J. Petersen,Amitabh Mishra,Arupjyoti Bhuyan*

Main category: eess.SP

TL;DR: 本文提出了一种考虑无人机俯仰角和仰角几何的空间相关性模型，用于改进无人机通信中的阴影衰落预测，相比传统模型提升了1.5dB的预测精度。


<details>
  <summary>Details</summary>
Motivation: 无人机在未来无线网络中扮演重要角色，但传统阴影衰落相关性模型仅考虑空间距离，忽略了无人机的三维方向和仰角几何。即使5-10度的俯仰角变化也会显著影响信号强度，因此需要更精确的模型来表征无人机通信中的阴影衰落行为。

Method: 提出了一种考虑仰角和倾斜角的空间相关性模型，并使用真实世界固定高度无人机测量数据集（农村环境，3.32 GHz，125 kHz带宽）进行验证。将该模型集成到普通克里金（OK）框架中进行信号强度预测。

Result: 结果显示：10度倾斜角分离和20度仰角分离分别可使阴影衰落相关性降低15%和40%。与传统忽略无人机方向和仰角的模型相比，提出的相关性模型在信号强度预测中将中位数RMSE提升了约1.5 dB。

Conclusion: 无人机俯仰角和仰角几何对阴影衰落相关性有显著影响，提出的考虑这些因素的空间相关性模型能有效改进无人机通信中的信号强度预测精度，为未来无线网络提供更准确的信道行为理解。

Abstract: Future wireless networks demand a more accurate understanding of channel behavior to enable efficient communication with reduced interference. Uncrewed Aerial Vehicles (UAVs) are poised to play an integral role in these networks, offering versatile applications and flexible deployment options. However, accurately characterizing the shadow fading (SF) behavior in UAV communications remains a challenge. Traditional SF correlation models rely on spatial distance and neglect the UAV's 3D orientation and elevation angle. Yet even slight variations in pitch angle (5 to 10 degrees) can significantly affect the signal strength observed by a UAV. In this study, we investigate the impact of UAV pitch and elevation geometry on SF and propose an elevation- and tilt-aware spatial correlation model. We use a real-world fixed-altitude UAV measurement dataset collected in a rural environment at 3.32 GHz with a 125 kHz bandwidth. Results show that a 10-degree tilt-angle separation and a 20-degree elevation-angle separation can reduce the SF correlation by up to 15% and 40%, respectively. In addition, integrating the proposed correlation model into the ordinary Kriging (OK) framework for signal strength prediction yields an approximate 1.5 dB improvement in median RMSE relative to the traditional correlation model that ignores UAV orientation and elevation.

</details>


### [3] [A Non-Invasive Path to Animal Welfare: Contactless Vital Signs and Activity Monitoring of In-Vivo Rodents Using a mm-Wave FMCW Radar](https://arxiv.org/abs/2512.05595)
*Tommaso Polonelli,Manuel Glahn,Stefano Kron,Stefan Selbert,Marco Garzola,Michele Magno*

Main category: eess.SP

TL;DR: 基于60GHz FMCW雷达的非侵入式啮齿动物生理监测系统，可在笼内连续追踪活动与生命体征，无需植入或人工干预


<details>
  <summary>Details</summary>
Motivation: 传统实验室啮齿动物监测方法通常依赖侵入式传感器或频繁操作，会引发动物应激并影响数据准确性，需要开发非侵入式监测方案

Method: 采用低功耗60GHz FMCW雷达传感器，设计完整的信号处理流程，包括距离单元选择、相位提取和频域估计，优化传感器布局策略

Result: 系统实现3cm运动检测精度和0.1m/s活动检测灵敏度，呼吸率提取精度达2bpm，能分辨与心肺活动相关的微运动（2μm距离分辨率）

Conclusion: 该系统为临床前研究提供了可扩展、自动化、符合伦理的监测方案，首次实现群养笼内自由活动啮齿动物的连续雷达生命体征监测

Abstract: Monitoring physiological and behavioral parameters of laboratory rodents is fundamental for biomedical research, yet conventional techniques often rely on invasive sensors or frequent handling that can induce stress and compromise data fidelity. To address these limitations, this paper presents a contactless and non-invasive in-vivo monitoring system based on a low-power 60 GHz frequency-modulated continuous wave (FMCW) radar. The proposed system enables simultaneous detection of rodent activity and vital signs directly within home-cage environments, eliminating the need for implants, electrodes, or human intervention. The hardware platform leverages a compact Infineon BGT60 series radar sensor, optimized for low power consumption and continuous operation. We investigate sensor placement strategies and design a complete signal processing pipeline, including range bin selection, phase extraction, and frequency-domain estimation tailored to rodent vital signs. The system achieves 3 cm and 0.1 m/s sensitivity for motion and activity detection, while allowing discrimination of micro-movements associated with cardiopulmonary activity with a 2 um distance resolution. Experimental validation with two rodents in realistic in-vivo cages demonstrates that the radar can track animal position and extract respiration rates with 2 bpm accuracy. By minimizing stress and disturbance, this work improves both animal welfare and the reliability of physiological measurements, offering a refined alternative to traditional monitoring methods. This work represents the first demonstration of continuous radar-based vital sign monitoring in freely moving rodents within group-housed cages. The proposed approach lays the foundation for scalable, automated, and ethical monitoring solutions in preclinical and translational research.

</details>


### [4] [Noise Suppression for Time Difference of Arrival: Performance Evaluation of a Generalized Cross-Correlation Method Using Mean Signal and Inverse Filter](https://arxiv.org/abs/2512.05355)
*Hirotaka Obo,Yuki Fujita,Masahisa Ishii,Hideki Moriyama,Ryota Tsuchiya,Yuta Ohashi,Kotaro Seki*

Main category: eess.SP

TL;DR: 提出GCC-MSIF方法，通过多通道输入估计"平均信号"并使用"逆滤波器"虚拟重建源信号，在低信噪比环境下显著提升TDOA估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统GCC方法在低信噪比条件下性能下降，特别是在信号带宽未知时。需要一种能在噪声环境中提高TDOA估计精度的方法。

Method: GCC-MSIF方法：1) 从多通道输入估计"平均信号"；2) 使用"逆滤波器"虚拟重建源信号；3) 自适应抑制带外噪声。

Result: 在低信噪比区域显著优于GCC-PHAT和GCC-SCOT等传统方法，性能与GCC-ML相当或更好。估计精度随阵列元素数量可扩展提升。

Conclusion: GCC-MSIF是实际盲环境中稳健被动定位的有前景解决方案，在噪声环境下具有优越的TDOA估计性能。

Abstract: This paper proposes a novel generalized cross-correlation (GCC) method, termed GCC-MSIF, to improve time difference of arrival (TDOA) estimation accuracy in noisy environments. Conventional GCC methods often suffer from performance degradation under low signal-to-noise ratio (SNR) conditions, particularly when the signal bandwidth is unknown. GCC-MSIF introduces a "mean signal" estimated from multi-channel inputs and an "inverse filter" to virtually reconstruct the source signal, enabling adaptive suppression of out-of-band noise. Numerical simulations simulating a small-scale array demonstrate that GCC-MSIF significantly outperforms conventional methods, such as GCC-PHAT and GCC-SCOT, in low SNR regions and achieves robustness comparable to or exceeding the maximum likelihood (GCC-ML) method. Furthermore, the estimation accuracy improves scalably with the number of array elements. These results suggest that GCC-MSIF is a promising solution for robust passive localization in practical blind environments.

</details>


### [5] [Hardware-Impaired Over-the-Air Computation with Fluid Antenna Array](https://arxiv.org/abs/2512.05368)
*Zilong Li,Jianxin Dai,Zhaohui Yang,Zhaoyang Zhang,Kai-Kit Wong,Zhiyang Li,Shunkuan Cheng*

Main category: eess.SP

TL;DR: 本文研究了一种在硬件损伤存在下，利用流体天线阵列增强的空中计算系统，通过联合优化发射功率控制、接收波束成形和天线位置向量来最小化聚合信号的均方误差。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线阵列在硬件损伤存在时性能受限，而流体天线阵列通过可重构天线位置提供了新的自由度，有望提升空中计算系统的性能并增强对硬件损伤的鲁棒性。

Method: 采用块坐标下降框架，将非凸且高度耦合的联合优化问题分解为三个子问题：发射功率控制、接收波束成形和天线位置向量优化。对每个子问题推导了闭式解或高效数值算法。

Result: 仿真结果表明，所提出的联合收发器和天线位置向量设计相比传统固定位置天线阵列显著降低了均方误差，并展现出对硬件损伤的增强鲁棒性。所提算法在各种系统配置下都表现出有效性和收敛性。

Conclusion: 流体天线阵列通过利用可重构天线位置的新自由度，能够有效提升空中计算系统的性能，特别是在硬件损伤存在的情况下。所提出的联合优化算法为解决这类复杂问题提供了高效解决方案。

Abstract: This paper investigates a fluid antenna (FA) array-enhanced over-the-air computation (AirComp) system in the presence of hardware impairments (HWIs), exploiting the new degrees of freedom offered by reconfigurable antenna positioning. To minimize the mean squared error (MSE) of the aggregated signal, we jointly optimize transmit power control, receive beamforming, and the antenna position vector (APV), subject to practical constraints such as HWI-induced distortion noise, FA movement energy consumption, and total power budgets. The resulting optimization problem is non-convex and highly coupled. To address it efficiently, we adopt a block coordinate descent (BCD) framework, decomposing it into three manageable subproblems. For each subproblem, closed-form solutions or efficient numerical algorithms are derived. Simulation results demonstrate that the proposed joint transceiver and APV design significantly reduces the MSE compared to conventional fixed-position antenna (FPA) arrays and exhibits enhanced robustness against hardware impairments. The effectiveness and convergence of the proposed algorithm are further validated under various system configurations.

</details>


### [6] [Individual Channel Estimation for Beyond Diagonal Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2512.05404)
*Kangchun Zhao,Yijie Mao*

Main category: eess.SP

TL;DR: 提出一种针对BD-RIS的个体信道估计框架，分别估计静态的BS-RIS信道和动态的RIS-用户信道，显著提高估计精度并降低导频开销


<details>
  <summary>Details</summary>
Motivation: BD-RIS通过RIS单元间的互连提供了更大的波束操控灵活性，但互连结构使得传统对角RIS的信道估计方法失效，并显著增加了导频开销，需要新的信道估计解决方案

Method: 提出个体信道估计框架：1) 利用BS-RIS信道的固有稀疏性，采用全双工方法估计静态的BS-RIS信道；2) 使用最小二乘法估计动态变化的RIS-用户信道

Result: 数值结果表明，所提框架在RIS单元数量较大时能显著提高信道估计精度，同时相比传统的级联信道估计方法大幅降低了导频开销

Conclusion: 该研究为BD-RIS系统提供了一种有效的信道估计解决方案，解决了互连结构带来的挑战，在保证估计精度的同时降低了系统开销

Abstract: Beyond Diagonal Reconfigurable Intelligent Surfaces (BD-RIS) has emerged as a promising evolution of RIS technology. By enabling interconnections between RIS elements, BD-RIS architectures offer greater flexibility in wave manipulation compared to traditional diagonal RIS designs. However, these interconnections introduce new research challenges for channel estimation, making existing approaches developed for conventional diagonal RISs ineffective and significantly increasing pilot overhead. To address these challenges, we propose a novel individual channel estimation framework that separately estimates the BS-RIS channel, which typically remains static over time, and the RIS-user channels, which vary rapidly due to user mobility. Specifically, we develop a full-duplex (FD) approach to estimate the BS-RIS channel by leveraging its inherent sparsity. Following this, the RIS-user channels are estimated using a least squares (LS) approach. Numerical results demonstrate that the proposed framework achieves significantly higher channel estimation accuracy, particularly when the number of RIS elements is large, while substantially reducing pilot overhead compared to conventional cascaded channel estimation methods.

</details>


### [7] [Over-the-Air Semantic Alignment with Stacked Intelligent Metasurfaces](https://arxiv.org/abs/2512.05657)
*Mario Edoardo Pandolfo,Kyriakos Stylianopoulos,George C. Alexandropoulos,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: 基于堆叠智能超表面的首个空中语义对齐框架，直接在波域实现潜在空间对齐，降低设备计算负担


<details>
  <summary>Details</summary>
Motivation: 语义通信系统在异构收发器模型产生不对齐的潜在表示时性能会下降，现有方法依赖额外的数字处理增加了设备复杂度

Method: 使用堆叠智能超表面作为可训练线性算子，模拟监督线性对齐器和零样本Parseval框架均衡器，开发基于梯度的优化程序定制超表面传递函数

Result: 实验显示SIM能准确复现监督和零样本语义均衡器，在高信噪比下达到90%任务准确率，在低信噪比下保持强鲁棒性

Conclusion: SIM框架首次实现了空中语义对齐，直接在波域对齐潜在表示，显著降低设备计算复杂度，为语义通信系统提供了高效解决方案

Abstract: Semantic communication systems aim to transmit task-relevant information between devices capable of artificial intelligence, but their performance can degrade when heterogeneous transmitter-receiver models produce misaligned latent representations. Existing semantic alignment methods typically rely on additional digital processing at the transmitter or receiver, increasing overall device complexity. In this work, we introduce the first over-the-air semantic alignment framework based on stacked intelligent metasurfaces (SIM), which enables latent-space alignment directly in the wave domain, reducing substantially the computational burden at the device level. We model SIMs as trainable linear operators capable of emulating both supervised linear aligners and zero-shot Parseval-frame-based equalizers. To realize these operators physically, we develop a gradient-based optimization procedure that tailors the metasurface transfer function to a desired semantic mapping. Experiments with heterogeneous vision transformer (ViT) encoders show that SIMs can accurately reproduce both supervised and zero-shot semantic equalizers, achieving up to 90% task accuracy in regimes with high signal-to-noise ratio (SNR), while maintaining strong robustness even at low SNR values.

</details>


### [8] [Exploiting Spatial Multiplexing Based on Pixel Antennas: An Antenna Coding Approach](https://arxiv.org/abs/2512.05706)
*Zixiang Han,Shanpu Shen,Ross Murch*

Main category: eess.SP

TL;DR: 提出一种利用像素天线空间复用能力的天线编码方法，通过波束空间域增加自由度来传输更多信息流，相比传统MIMO可提升频谱效率达12 bits/s/Hz或降低90%发射功率。


<details>
  <summary>Details</summary>
Motivation: 传统MIMO系统的频谱效率有限，需要新的技术来提升无线通信容量。像素天线作为可重构天线，通过控制射频开关状态可以灵活调整辐射模式，但如何有效利用这种可重构性进行波束空间复用尚未充分探索。

Method: 提出天线编码器和模式编码器概念，建立包含天线编码的波束空间域MIMO通信系统模型，推导频谱效率表达式。分析像素天线的辐射模式，提供天线编码设计的高效优化算法。

Result: 数值仿真表明，使用像素天线的天线编码技术可将4×4 MIMO的频谱效率提升高达12 bits/s/Hz，或等效降低所需发射功率达90%，相比传统MIMO有显著改进。

Conclusion: 天线编码技术能有效提升频谱效率，展示了像素天线在波束空间复用中的潜力，为未来6G无线通信提供了有前景的技术方向。

Abstract: An antenna coding approach for exploiting the spatial multiplexing capability of pixel antennas is proposed. This approach can leverage additional degrees of freedom in the beamspace domain to transmit more information streams. Pixel antennas are a general reconfigurable antenna design where a radiating structure with arbitrary shape and size can be discretized into sub-wavelength elements called pixels which are connected by radio frequency switches. By controlling the switch states, the pixel antenna topology can be flexibly adjusted so that the resulting radiation pattern can be reconfigured for beamspace spatial multiplexing. In this work, we introduce the antenna coder and pattern coder for pixel antennas, provide a multiple-input multiple-output (MIMO) communication system model with antenna coding in the beamspace domain, and derive the spectral efficiency. Utilizing the antenna coder, the radiation pattern of the pixel antenna is analyzed and efficient optimization algorithms are provided for antenna coding design. Numerical simulation results show that the proposed technique using pixel antennas can enhance spectral efficiency of 4-by-4 MIMO by up to 12 bits/s/Hz or equivalently reduce the required transmit power by up to 90% when compared to conventional MIMO, demonstrating the effectiveness of the antenna coding technique in spectral efficiency enhancement and its promise for future sixth generation (6G) wireless communication.

</details>


### [9] [Codebook-based Port Selection and Combining for CSI-Free Uplink Fluid Antenna Multiple Access](https://arxiv.org/abs/2512.05748)
*Chenguang Rao,Kai-Kit Wong,Sai Xu,Xusheng Zhu,Yangyang Zhang,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 提出基于码本的端口选择与组合(CPSC)流体天线多址接入框架，用于无基站CSI的上行通信，通过码本匹配和投影分离用户信号，实现高速率和低复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有FAMA研究主要关注下行传输和完美CSI接收，上行通信和无CSI场景研究不足。需要开发低复杂度、可扩展的上行FAMA方案，以支持大规模多用户连接。

Method: 设计预定义码本广播给用户，每个用户基于本地CSI独立选择码字、激活对应流体天线端口并确定组合权重。基站通过码本引导的投影操作分离叠加用户信号，无需全局CSI或多用户联合优化。引入三种轻量级调度策略处理码字冲突。

Result: 仿真结果表明CPSC-FAMA相比固定天线系统显著提高速率，同时保持低复杂度。通过将优化成本分摊给用户，有效减轻基站处理负担并增强可扩展性。

Conclusion: 提出的CPSC-FAMA框架为无CSI上行通信提供了高效解决方案，在速率、复杂度和可扩展性方面表现优异，是未来6G网络的强有力候选方案。

Abstract: Fluid antenna multiple access (FAMA) has recently emerged as a simple, promising scheme for large-scale multiuser connectivity, offering strong scalability with low implementation complexity. Nevertheless, most existing FAMA studies focus on downlink transmission under perfect channel state information (CSI) at the receiver side, while the uplink counterpart remains largely unexplored. This paper proposes a novel codebook-based port selection and combining (CPSC) FAMA framework for the uplink communications without CSI at the base station (BS). In the proposed scheme, a predefined codebook is designed and broadcast by the BS. Each user equipment (UE) employs a fluid antenna, acquires its local CSI and independently chooses the most suitable codeword, activates the corresponding fluid antenna ports, and determines the combining weights to achieve a two-way match between the selected codeword and the instantaneous effective channel. The BS then separates the superimposed user signals through codebook-guided projection operations without requiring global CSI or multiuser joint optimization. To handle potential codeword collisions, three lightweight scheduling strategies are introduced, offering flexible trade-offs between signaling overhead and collision avoidance. Simulation results demonstrate that the proposed CPSC-FAMA approach achieves substantially higher rates than fixed-antenna systems while maintaining low complexity. Moreover, the results confirm that amortizing the optimization cost over the UEs effectively reduces the BS processing burden and enhances scalability, making the proposed scheme a strong candidate for future sixth-generation (6G) networks.

</details>


### [10] [Radar Network Waveform Design for Target Tracking](https://arxiv.org/abs/2512.05757)
*Tao Fan,Augusto Aubry,Antonio De Maio,Luca Pallotta,Xianxiang Yu,Guolong Cui*

Main category: eess.SP

TL;DR: 提出一种基于PCRLB的慢时间编码波形合成方法，用于雷达网络在有色高斯干扰下的单目标跟踪，通过块MM算法优化波形设计以提升跟踪精度


<details>
  <summary>Details</summary>
Motivation: 在有色高斯干扰环境下，雷达网络需要进行有效的单目标跟踪，但现有波形设计方法在跟踪精度和实际约束条件（如功率预算、发射机限制）方面存在优化空间

Method: 基于后验克拉美罗下界(PCRLB)建立优化问题，通过泰勒级数展开近似目标函数，采用定制化的块Majorization-Minimization(块MM)算法求解波形设计问题

Result: 数值结果表明，所提方法在目标状态估计过程中实现了精度提升，并在不确定目标状态条件下展现出鲁棒的跟踪性能

Conclusion: 提出的慢时间编码波形合成方法能够有效提高雷达网络在有色高斯干扰下的单目标跟踪精度，同时满足实际约束条件，具有较好的收敛性和鲁棒性

Abstract: This paper addresses the synthesis of slow-time coded waveforms for single target tracking in a radar network operating under colored Gaussian interference. Based on the Posterior Cramér Rao Lower Bound (PCRLB), which characterizes the theoretically optimal accuracy of target state estimation, the problem at each tracking frame is formulated as the minimization of the trace of the PCRLB, together with power budget requirements and a similarity constraint to account for transmitter limitations and appropriate waveform features. To tackle this challenging optimization problem, an approximation solution technique is proposed, aimed at better tracking accuracy than the reference code. The resulting approximated problems, endowed with more tractable objective functions through Taylor-series expansion, are solved using a customized block Majorization-Minimization (block-MM) algorithm. The convergence properties of the developed procedure are thoroughly analyzed. Numerical results illustrate the accuracy improvements in the target state estimation process, and robust tracking performance under uncertain target state conditions achieved by the proposed technique.

</details>


### [11] [A Residual Variance Matching Recursive Least Squares Filter for Real-time UAV Terrain Following](https://arxiv.org/abs/2512.05918)
*Xiaobo Wu,Youmin Zhang*

Main category: eess.SP

TL;DR: 提出RVM-RLS滤波器，通过残差方差匹配准则自适应估计非线性时变无人机地形跟随系统的实时航点，在模拟环境中相比基准算法提高约88%的航点估计精度。


<details>
  <summary>Details</summary>
Motivation: 无人机在野火巡逻任务中进行在线地形跟随时，需要准确的实时航点估计以确保飞行安全和野火检测。现有实时滤波算法在非线性时变系统的测量噪声下难以保持准确航点，存在飞行不稳定和漏检野火的风险。

Method: 提出基于残差方差匹配估计(RVME)准则的残差方差匹配递归最小二乘(RVM-RLS)滤波器，自适应估计非线性时变无人机地形跟随系统的实时航点。

Result: 在模拟地形环境中验证，RVM-RLS滤波器相比基准算法在多个评估指标上提高约88%的航点估计精度。

Conclusion: RVM-RLS滤波器在实时滤波方法上取得进展，具有无人机在线野火巡逻的实际应用潜力。

Abstract: Accurate real-time waypoints estimation for the UAV-based online Terrain Following during wildfire patrol missions is critical to ensuring flight safety and enabling wildfire detection. However, existing real-time filtering algorithms struggle to maintain accurate waypoints under measurement noise in nonlinear and time-varying systems, posing risks of flight instability and missed wildfire detections during UAV-based terrain following. To address this issue, a Residual Variance Matching Recursive Least Squares (RVM-RLS) filter, guided by a Residual Variance Matching Estimation (RVME) criterion, is proposed to adaptively estimate the real-time waypoints of nonlinear, time-varying UAV-based terrain following systems. The proposed method is validated using a UAV-based online terrain following system within a simulated terrain environment. Experimental results show that the RVM-RLS filter improves waypoints estimation accuracy by approximately 88$\%$ compared with benchmark algorithms across multiple evaluation metrics. These findings demonstrate both the methodological advances in real-time filtering and the practical potential of the RVM-RLS filter for UAV-based online wildfire patrol.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques](https://arxiv.org/abs/2512.05169)
*Abdelmalik Moujahid,Fadi Dornaika*

Main category: cs.LG

TL;DR: 这篇论文是关于多视图聚类（MVC）的综述，系统分类了MVC方法，分析了优缺点，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 机器学习面临计算约束、单视图学习算法局限性和处理多领域大数据复杂性的挑战。多视图聚类作为无监督多视图学习，能克服这些挑战，提供更丰富的数据表示和有效解决方案。

Method: 对140多篇基础与近期文献进行系统综述，将MVC方法分为七类：协同训练、协同正则化、子空间、深度学习、基于核、基于锚点和基于图的方法，并比较早期融合、晚期融合和联合学习等集成策略。

Result: 提供了MVC方法的系统性分类框架，深入分析了各类方法的优缺点和实际挑战（如可扩展性和不完整数据），并探讨了在医疗健康、多媒体和社交网络分析等领域的实际应用案例。

Conclusion: 该综述填补了MVC研究领域的空白，为领域发展提供了可操作的见解，并展望了新兴趋势、跨学科应用和未来研究方向，推动多视图聚类技术的进步。

Abstract: Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.

</details>


### [13] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出CV-Masking方法，根据生物标志物的波动性调整掩码概率，改进电子健康记录的自编码器预训练


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法采用均匀随机掩码，假设所有特征同等可预测，但实际上实验室检测指标存在显著异质性波动性，稳定指标（如钠）与波动指标（如乳酸）的建模难度不同，波动性指标通常反映急性病理生理变化，需要更复杂的建模

Method: 提出波动性感知预训练策略CV-Masking，根据每个特征的内在变异性自适应调整掩码概率，结合与临床工作流程对齐的仅值掩码目标

Result: CV-Masking在大量实验室检测指标上相比随机和基于方差的策略，提高了重建质量，改善了下游预测性能，加速了收敛，产生了更稳健和临床有意义的EHR表示

Conclusion: CV-Masking通过考虑生物标志物的固有波动性，系统性地改进了EHR的表示学习，为临床任务提供了更有效的预训练方法

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [14] [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: 提出基于LMI框架的L-Lipschitz深度残差网络设计方法，通过LDL^T分解扩展至任意非线性架构，在121个UCI数据集上比SLL Layers提升3%-13%准确率


<details>
  <summary>Details</summary>
Motivation: ResNet在计算机视觉任务中表现出色，但需要控制网络的Lipschitz常数以增强对抗鲁棒性和网络可验证性。现有方法在构建Lipschitz约束网络方面存在局限性。

Method: 将ResNet架构重新表述为循环三对角LMI，推导网络参数的闭式约束以保证L-Lipschitz连续性；使用新的LDL^T分解方法验证LMI可行性，将L-Lipschitz网络构造扩展到任意非线性架构；采用Cholesky分解进行高效参数化。

Result: LDL^T公式是SDP-based网络的紧致松弛，保持完全表达能力；在121个UCI数据集上比SLL Layers获得3%-13%的准确率提升。

Conclusion: 提出了一种可证明的参数化方法，用于构建Lipschitz约束的残差网络和其他分层架构，为对抗鲁棒性、认证训练和控制系统提供了强大的网络设计框架。

Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.

</details>


### [15] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 系统评估临床时间序列建模中的分词策略，发现时间编码无显著效益，值特征重要性任务相关，冻结预训练编码器优于可训练版本，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录的分词策略影响模型处理效果，但目前缺乏公平比较。需要系统评估不同分词方法在临床时间序列建模中的有效性。

Method: 在MIMIC-IV数据集上，针对四个临床预测任务进行控制性消融实验，使用基于Transformer的架构，比较不同分词策略（包括时间编码、值特征等）的效果。

Result: 1) 显式时间编码对下游任务无一致的统计显著效益；2) 值特征重要性任务相关（影响死亡率预测但不影响再入院预测）；3) 冻结预训练编码器显著优于可训练版本且参数更少；4) 更大的临床编码器带来一致改进。

Conclusion: 更简单、参数高效的方法在许多情况下能取得良好性能，但最优分词策略仍取决于具体任务。该研究为公平比较分词策略提供了框架。

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [16] [Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance](https://arxiv.org/abs/2512.05222)
*Yanhua Xu*

Main category: cs.LG

TL;DR: 结合预训练蛋白质语言模型与半监督学习，可在标记数据稀缺时准确预测流感病毒抗原性，解决HI实验数据不足的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 流感病毒抗原性快速进化需要频繁更新疫苗，但传统的血凝抑制实验耗时且难以规模化，导致基因组数据远多于可用的表型标记数据，限制了传统监督模型的效果。

Method: 评估两种半监督学习策略（自训练和标签传播），使用四种PLM嵌入（ESM-2、ProtVec、ProtT5、ProtBert）处理血凝素序列，通过嵌套交叉验证模拟不同标记数据比例（25%、50%、75%、100%）在四种流感亚型中的应用。

Result: 半监督学习在标记数据稀缺时持续提升性能，自训练结合ProtVec获得最大相对增益，ESM-2在仅25%标记数据时仍保持F1分数高于0.82，H1N1和H9N2预测准确度高，但高变异H3N2亚型仍具挑战性。

Conclusion: 整合蛋白质语言模型与半监督学习可解决抗原性标记瓶颈，更有效地利用未标记监测序列，支持快速变异株优先级排序和及时疫苗株选择。

Abstract: Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.

</details>


### [17] [Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)](https://arxiv.org/abs/2512.05306)
*Y. Sungtaek Ju*

Main category: cs.LG

TL;DR: SVGP KANs：将稀疏变分高斯过程与Kolmogorov-Arnold网络结合，为科学机器学习提供可解释的不确定性量化框架


<details>
  <summary>Details</summary>
Motivation: Kolmogorov-Arnold网络虽然可解释，但缺乏系统的不确定性量化能力，而这对科学应用至关重要。需要一种既能保持可解释性又能提供贝叶斯不确定性估计的架构。

Method: 提出SVGP KANs框架：将稀疏变分高斯过程推断与Kolmogorov-Arnold拓扑结构集成，通过解析矩匹配在深层加性结构中传播不确定性，计算复杂度与样本量呈准线性关系。

Result: 通过三个案例研究展示了框架区分偶然不确定性和认知不确定性的能力：流体流动重建中的异方差测量噪声校准、平流扩散动力学多步预测中的置信度退化量化、卷积自编码器中的分布外检测。

Conclusion: SVGP KANs是科学机器学习中不确定性感知学习的有前景架构，结合了可解释性和可扩展的贝叶斯推断能力。

Abstract: Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.

</details>


### [18] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 提出VaRDASS方法，通过分层采样减少无监督域适应中域差异估计的方差，提高模型在目标域的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 无监督域适应（UDA）通过最小化域差异来解决域偏移问题，但在随机设置中，域差异估计存在高方差问题，这会阻碍方法理论优势的实现。

Method: 提出VaRDASS方法，针对相关性对齐和最大均值差异（MMD）两种域差异度量，推导分层采样目标，并设计k-means风格的优化算法。

Result: 在三个域偏移数据集上的实验表明，该方法提高了域差异估计的准确性和目标域性能。

Conclusion: VaRDASS是首个专门用于UDA的随机方差减少技术，在特定假设下对MMD的方差最小化是理论最优的，能有效提升域适应效果。

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [19] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 测试AI天气预报模型FourCastNetv2对输入噪声的鲁棒性，发现模型在低到中等噪声下能保持飓风特征，高噪声下轨迹准确性下降，但始终低估风暴强度


<details>
  <summary>Details</summary>
Motivation: 评估AI天气预报模型对输入不确定性的鲁棒性，特别是在飓风等极端天气事件中，这对输出可靠性评估至关重要

Method: 进行两个实验：1) 在飓风Florence的初始条件中注入不同水平的高斯噪声，观察轨迹和强度预测变化；2) 使用完全随机初始条件，观察模型对无意义输入的响应

Result: FCNv2在低到中等噪声下能准确保持飓风特征；高噪声下仍保持基本风暴轨迹和结构，但位置准确性下降；在所有噪声水平下都低估风暴强度和持续性；完全随机初始条件下，模型在几个时间步后能生成平滑连贯的预报

Conclusion: FCNv2对输入噪声表现出良好的鲁棒性，但存在系统性低估风暴强度的偏差；模型倾向于生成稳定平滑的输出；该方法简单且可移植到其他数据驱动的AI天气预报模型

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [20] [MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System](https://arxiv.org/abs/2512.05234)
*Felix Mulitze,Herbert Woisetschläger,Hans Arno Jacobsen*

Main category: cs.LG

TL;DR: MAR-FL是一种新型的P2P联邦学习系统，通过迭代分组聚合大幅降低通信开销，同时保持对网络变动的鲁棒性，通信复杂度从O(N²)降低到O(N log N)。


<details>
  <summary>Details</summary>
Motivation: 下一代无线系统与分布式机器学习的融合需要高效且鲁棒的联邦学习方法，而现有的P2P FL方法存在通信复杂度过高的问题，限制了实际可扩展性。

Method: 采用迭代分组聚合机制，通过分组方式减少通信开销，同时保持对不可靠客户端和网络变动的鲁棒性，并能集成隐私计算。

Result: MAR-FL将通信复杂度从传统方法的O(N²)降低到O(N log N)，显著提升了系统可扩展性，特别是在聚合轮次中节点数量增加时仍能保持有效性。

Conclusion: MAR-FL通过创新的分组聚合方法解决了P2P FL的通信瓶颈问题，为无线网络环境下的分布式机器学习提供了高效、鲁棒且可扩展的解决方案。

Abstract: The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.

</details>


### [21] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: CATR框架通过选择稀疏必要的文本标记子集来解决文本因果推断中的正性假设违反问题，提高因果效应估计的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 文本数据在因果推断中的应用日益增多，但高维文本特征会导致正性假设违反，产生极端倾向得分、权重不稳定和方差膨胀等问题，需要有效的方法来筛选关键文本特征。

Method: 提出Confounding-Aware Token Rationalization (CATR)框架，使用残差独立性诊断选择稀疏必要的标记子集，保留足够的混杂信息以保持无混杂性，同时丢弃无关文本。

Result: 在合成数据和MIMIC-III数据库的真实世界研究中，CATR相比现有基线方法能产生更准确、稳定和可解释的因果效应估计。

Conclusion: CATR通过选择关键文本标记有效缓解了观测层面的正性假设违反问题，稳定了下游因果效应估计器，为文本数据在因果推断中的应用提供了实用框架。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [22] [Edged Weisfeiler-Lehman Algorithm](https://arxiv.org/abs/2512.05238)
*Xiao Yue,Bo Liu,Feng Zhang,Guangzhi Qu*

Main category: cs.LG

TL;DR: 提出E-WL算法扩展1-WL以包含边特征，并基于此构建EGIN模型，在12个边特征图数据集上取得优越性能


<details>
  <summary>Details</summary>
Motivation: 传统GNN和1-WL算法都未充分利用边特征，而边特征在许多图学习任务中具有重要价值，需要开发能够有效利用边特征的算法和模型

Method: 提出E-WL算法扩展1-WL以包含边特征，并基于此构建EGIN模型，通过边特征增强的传播-聚合机制进行图学习

Result: 在12个边特征基准图数据集上，EGIN模型相比现有SOTA基线模型表现出优越性能

Conclusion: E-WL算法和EGIN模型有效解决了传统方法忽略边特征的问题，在边特征图分类任务中具有显著优势

Abstract: As a classical approach on graph learning, the propagation-aggregation methodology is widely exploited by many of Graph Neural Networks (GNNs), wherein the representation of a node is updated by aggregating representations from itself and neighbor nodes recursively. Similar to the propagation-aggregation methodology, the Weisfeiler-Lehman (1-WL) algorithm tests isomorphism through color refinement according to color representations of a node and its neighbor nodes. However, 1-WL does not leverage any edge features (labels), presenting a potential improvement on exploiting edge features in some fields. To address this limitation, we proposed a novel Edged-WL algorithm (E-WL) which extends the original 1-WL algorithm to incorporate edge features. Building upon the E-WL algorithm, we also introduce an Edged Graph Isomorphism Network (EGIN) model for further exploiting edge features, which addresses one key drawback in many GNNs that do not utilize any edge features of graph data. We evaluated the performance of proposed models using 12 edge-featured benchmark graph datasets and compared them with some state-of-the-art baseline models. Experimental results indicate that our proposed EGIN models, in general, demonstrate superior performance in graph learning on graph classification tasks.

</details>


### [23] [Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection](https://arxiv.org/abs/2512.05567)
*Antoine Blais,Nicolas Couëllan*

Main category: cs.LG

TL;DR: 提出基于最优传输的半监督学习方法，利用Wasserstein距离作为图像相似性度量，通过标签传播机制处理标注数据稀缺问题，在GNSS多径干扰检测应用中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 在图像数据标注稀缺的情况下，需要开发能够有效利用少量标注数据的半监督学习方法。传统方法在处理复杂图像特征时可能不够有效，因此需要结合最优传输理论来改进相似性度量和标签传播机制。

Method: 采用基于图的半监督学习方法，使用Wasserstein距离作为图像样本间的相似性度量，在标签传播过程中利用该距离度量。方法基于深度卷积网络，通过最优传输理论改进传统的半监督学习框架。

Result: 实验在GNSS多径干扰检测应用中进行，在不同信号条件下测试。结果显示，通过适当选择控制半监督程度和度量敏感度的超参数，分类准确率相比全监督训练方法有显著提升。

Conclusion: 基于最优传输的半监督学习方法能够有效利用少量标注数据，在GNSS多径干扰检测等实际应用中显著提升分类性能，为解决标注数据稀缺问题提供了有效解决方案。

Abstract: The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.

</details>


### [24] [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)
*Bruno Jacob,Amanda A. Howard,Panos Stinis*

Main category: cs.LG

TL;DR: 量子PDE求解器受限于硬件约束，作者提出多保真度学习框架，用量子粗解结合少量经典数据训练校正模型，实现高精度预测和时间外推。


<details>
  <summary>Details</summary>
Motivation: 量子PDE求解器面临严重硬件限制：量子比特数限制空间分辨率，电路深度限制长时间积分精度。这些瓶颈使量子求解器只能提供低保真度解，尽管理论上有计算加速潜力。

Method: 引入多保真度学习框架，用量子求解器生成大量低保真度解作为代理模型，然后通过多保真度神经网络架构学习校正映射，平衡线性和非线性变换。使用少量经典高保真度数据进行训练。

Result: 在粘性Burgers方程和不可压缩Navier-Stokes流等非线性PDE基准测试中，框架成功校正了粗量子预测，实现了远超经典训练窗口的时间外推，预测精度可与经典方法竞争。

Conclusion: 该框架通过桥接硬件受限的量子模拟与应用需求，为当前量子设备在科学计算中提取计算价值建立了途径，推动了近量子计算在计算物理中的算法开发和实际部署。

Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.

</details>


### [25] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 论文提出了"模块化喷射"概念，用于分析机器学习模型的内部分解是否可由数据唯一确定，区分了"幻象"和"可识别"两种机制。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习主要通过预测风险评估模型，但无法确定模型的内部分解是否由数据和评估设计唯一确定。需要一种方法来分析模型模块化分解的可识别性问题。

Method: 提出模块化喷射概念，通过估计经验喷射（局部线性响应映射）来描述模块对输入扰动的反应。开发了MoJet算法用于经验喷射估计和幻象诊断，并在两模块线性回归管道中证明了喷射可识别性定理。

Result: 在满足温和秩假设和模块级喷射可访问条件下，内部分解是唯一确定的；而仅基于风险评估则存在大量实现相同输入-输出映射的幻象分解。在线性和深度回归以及管道分类中验证了该框架。

Conclusion: 模块化喷射提供了一种超越预测风险的分析工具，能够区分可识别和幻象分解机制，为理解模型内部结构提供了新的理论框架和实用算法。

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [26] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 论文提出了一种高效的机器学习遗忘框架，通过识别对模型输出影响可忽略的训练数据子集，在遗忘前减少数据集规模，从而显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中数据隐私问题的日益突出，从训练模型中遗忘或移除特定数据点的能力变得越来越重要。现有遗忘方法通常平等对待遗忘集中的所有数据点，但作者质疑那些对模型学习影响可忽略的点是否真的需要被移除。

Method: 通过跨语言和视觉任务的比较分析，使用影响函数识别对模型输出影响可忽略的训练数据子集。基于这一洞察，提出一个高效的遗忘框架，在遗忘前减少数据集规模。

Result: 该方法在实际应用案例中实现了显著的计算节省（约50%），同时保持了有效的遗忘效果。

Conclusion: 并非所有数据点都需要平等对待的遗忘方法，通过识别和移除影响可忽略的数据点，可以在保持遗忘效果的同时显著提高计算效率。

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [27] [DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models](https://arxiv.org/abs/2512.05287)
*Ziqi Zhang*

Main category: cs.LG

TL;DR: 提出DMAGT模型，基于多层Transformer图神经网络预测药物-miRNA关联，在三个数据集上达到95.24% AUC，实验验证了14/20预测关联。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验在探索药物与miRNA关联时存在效率和成本限制，需要计算模型来加速miRNA靶向药物开发。

Method: 将药物-miRNA关联转化为图结构，使用Word2Vec嵌入药物分子结构和miRNA碱基序列特征，采用图Transformer模型学习嵌入特征和关系结构进行预测。

Result: 在ncDR、RNAInter和SM2miR三个数据集上达到最高95.24±0.05%的AUC，优于其他对比方法。对5-氟尿嘧啶和奥沙利铂的预测中，20个最可能关联中有14个得到验证。

Conclusion: DMAGT在预测药物-miRNA关联方面表现出色且稳定，为miRNA药物开发提供了新的捷径。

Abstract: MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.

</details>


### [28] [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)
*Neha Gupta,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 提出基于LSTM的循环神经网络框架，用于估计具有记忆和长程依赖性的分数泊松过程参数，相比传统矩估计方法将均方误差降低约55.3%，并在真实高频数据上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 分数泊松过程能够建模具有记忆和长程依赖性的事件到达，但传统参数估计方法（如矩估计）可能不够准确。需要开发更有效的参数估计方法来处理复杂的时间依赖性。

Method: 使用基于LSTM的循环神经网络框架，从到达时间间隔序列中估计分数泊松过程的两个关键参数μ和β。LSTM能够有效建模时间依赖性，通过序列数据学习参数估计。

Result: 在合成数据上，相比传统矩估计方法，LSTM方法将均方误差降低约55.3%。在真实世界高频数据集（蒙哥马利县紧急呼叫记录和AAPL股票交易数据）上，LSTM能够有效跟踪每日模式和参数变化。

Conclusion: 基于LSTM的神经网络框架能够有效估计分数泊松过程参数，显著优于传统矩估计方法，并在具有复杂时间依赖性的真实世界数据上表现出良好的性能。

Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $μ>0$ and $β\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.

</details>


### [29] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出RSA2C算法，通过RKHS-SHAP状态归因增强Actor-Critic方法，实现可解释性、高效性和稳定性


<details>
  <summary>Details</summary>
Motivation: 传统Actor-Critic方法缺乏可解释性，现有可解释RL方法未能充分利用状态归因来辅助训练，忽视了不同状态维度对奖励的异质性影响

Method: 提出基于RKHS-SHAP的归因感知、核化、双时间尺度AC算法，包含Actor、价值评论家和优势评论家。Actor在向量值RKHS中实现，使用马氏加权算子值核；价值评论家和优势评论家在标量RKHS中。通过RKHS-SHAP计算状态归因，转换为马氏门控权重来调节Actor梯度和优势评论家目标

Result: 理论上推导了全局非渐近收敛边界，显示通过扰动误差项实现稳定性，通过收敛误差项实现效率。在三个标准连续控制环境上的实验结果表明算法实现了高效性、稳定性和可解释性

Conclusion: RSA2C算法成功地将状态归因整合到Actor-Critic框架中，通过RKHS-SHAP归因和马氏门控机制，在保持传统AC方法效率的同时，显著提升了可解释性和稳定性

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [30] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: CFO框架通过流匹配直接学习PDE的右端项，无需通过ODE求解器反向传播，实现了时间分辨率不变性，在长期稳定性和数据效率上优于自回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子代理使用自回归预测方案，会在长时间推演中累积误差且需要均匀时间离散化。现有连续方法（如神经ODE）计算负担重。需要一种既能学习连续时间PDE动态又计算高效的方法。

Method: CFO框架将流匹配重新用于直接学习PDE右端项。首先对轨迹数据进行时间样条拟合，在节点处使用时间导数的有限差分估计来构建概率路径，其速度场近似真实PDE动态。然后通过流匹配训练神经算子来预测这些解析速度场。

Result: 在四个基准测试（Lorenz、1D Burgers、2D扩散反应、2D浅水方程）中，CFO展示了优越的长期稳定性和显著的数据效率。仅使用25%不规则子采样时间点训练的CFO就优于使用完整数据训练的自回归基线，相对误差减少高达87%。推理时仅需50%的函数评估即可超越自回归基线。

Conclusion: CFO提供了一种时间分辨率不变的方法，能够在任意非均匀时间网格上训练，并通过ODE积分在任意时间分辨率下查询解。该方法独特地支持反向时间推理和任意时间查询，同时保持计算效率。

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


### [31] [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931)
*Neil G. Marchant,Andrew C. Cullen,Feng Liu,Sarah M. Erfani*

Main category: cs.LG

TL;DR: 论文提出了一种新的分歧损失函数，解决了现有替代损失函数在最大化分歧差异时缺乏贝叶斯一致性的问题，并通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在现实世界部署中经常因分布偏移而失败，分歧差异是解决这一问题的关键方法。然而，现有最大化分歧差异的替代损失函数存在根本缺陷——缺乏贝叶斯一致性，导致无法有效最大化真实分歧差异。

Method: 首先证明现有分歧差异替代损失函数缺乏贝叶斯一致性；然后提出新的理论结果，为这类替代损失函数提供最优性间隙的上下界；最后设计了一种新的分歧损失函数，与交叉熵配对使用，形成可证明一致的替代损失函数。

Result: 理论分析表明新方法具有贝叶斯一致性；在多个基准测试上的实验表明，该方法比现有方法能提供更准确、更稳健的分歧差异估计，特别是在具有挑战性的对抗条件下表现更优。

Conclusion: 论文解决了现有分歧差异替代损失函数的根本缺陷，提出了具有理论保证的一致替代损失函数，为处理分布偏移问题提供了更可靠的工具，有助于构建更安全可靠的AI系统。

Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.

</details>


### [32] [The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?](https://arxiv.org/abs/2512.05311)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee*

Main category: cs.LG

TL;DR: 研究评估了先进机器学习模型区分人类与LLM生成科学想法的能力，发现连续改写会显著降低检测性能，而研究问题作为上下文信息能提升检测效果


<details>
  <summary>Details</summary>
Motivation: 随着LLM在研究领域应用的增加，区分LLM与人类生成的想法对于理解LLM的认知能力变得至关重要。虽然LLM生成文本的检测已有研究，但区分科学想法的来源仍未被探索

Method: 系统评估了最先进的机器学习模型区分人类与LLM生成想法的能力，特别关注连续改写阶段后的检测性能，并研究了研究问题作为上下文信息的影响

Result: 检测性能在连续五次改写后平均下降25.4%；加入研究问题作为上下文信息可将检测性能提升最多2.97%；当想法被改写成简化的非专家风格时，检测算法面临最大困难

Conclusion: 区分人类与LLM生成科学想法具有挑战性，连续改写会显著降低检测性能，而上下文信息能提升检测效果，特别是在想法被简化改写时检测最为困难

Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.

</details>


### [33] [Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay](https://arxiv.org/abs/2512.05320)
*Mehmet Efe Lorasdagi,Dogan Can Cicek,Furkan Burak Mutlu,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: 提出DPER方法，通过为Actor和Critic网络分别采样不同的经验回放批次，提升深度确定性策略梯度算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Actor-Critic架构通常使用相同的经验回放批次训练两个网络，但Actor和Critic的学习目标和更新动态不同，这种统一的使用方式可能不是最优的。

Method: 提出解耦优先经验回放（DPER）方法，允许为Actor和Critic独立采样不同的过渡批次。该方法可与任何连续控制领域的离策略深度强化学习算法集成，本文将其与最先进的Twin Delayed DDPG算法结合。

Result: DPER在多个MuJoCo任务中优于传统的经验回放策略（如普通经验回放和优先经验回放）。

Conclusion: 解耦Actor和Critic的经验回放可以改善训练动态和最终策略质量。DPER为广泛的Actor-Critic离策略强化学习算法提供了一个通用的性能增强机制。

Abstract: Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.
  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.
  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.
  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.
  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.

</details>


### [34] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 提出新的联邦优化模型，量化通信和本地计算成本，并基于SAGA开发RG-SAGA算法，在非凸优化中实现最佳通信和本地计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有联邦优化算法使用不同的客户端选择策略（随机采样、全客户端通信或混合方案），这些策略在实践中产生不同的通信成本，但现有比较指标未能区分这些成本差异。

Method: 1) 提出量化通信和本地计算复杂度的联邦优化模型；2) 基于SAGA开发RG-SAGA梯度估计器；3) 引入递归梯度技术改进梯度估计器误差界；4) 设计基于不精确复合梯度方法的算法，包含精心构造的梯度估计器和辅助子问题求解过程。

Result: 新算法在非凸联邦优化中实现了现有方法中最佳的通信和本地计算复杂度。RG-SAGA相比原始SAGA具有改进的误差界，递归梯度技术可应用于SAGA和SVRG等条件无偏梯度估计器。

Conclusion: 提出的新模型能更准确地比较不同客户端选择策略的联邦优化算法，RG-SAGA算法在通信和计算效率方面达到最优，递归梯度技术为改进梯度估计器提供了通用框架。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [35] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER使用蒙特卡洛树搜索生成训练路径轨迹，通过子答案召回和LLM作为裁判验证过滤错误轨迹，并重新表述子查询处理检索失败，从而提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问答任务中，基于训练的方法仍受LLM幻觉和错误推理路径影响，导致性能受限，需要改进训练数据质量和处理检索失败问题。

Method: 1) 使用蒙特卡洛树搜索生成训练路径轨迹；2) 通过子答案召回和LLM作为裁判验证过滤错误和冗长轨迹；3) 重新表述子查询处理检索失败情况。

Result: PATHFINDER在公开基准数据集上提升了多跳问答的性能表现。

Conclusion: 通过改进训练数据质量和处理检索失败，PATHFINDER能有效提升多跳问答系统的性能，解决现有方法中的幻觉和错误推理问题。

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [36] [Interaction Tensor Shap](https://arxiv.org/abs/2512.05338)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TL;DR: 提出IT SHAP方法，通过张量网络收缩高效计算高阶Shapley交互作用，将指数复杂度降至多项式时间


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型变得更深、维度更高，理解特征如何影响预测变得困难。现有Shapley值方法无法高效计算高阶交互作用：STII需要指数级枚举，MST只限于一阶效应

Method: 提出Interaction Tensor SHAP (IT SHAP)，将Shapley Taylor交互指数重新表述为值张量和权重张量的收缩，并假设权重张量具有多项式TT秩的有限状态张量列车表示

Result: 在TT结构化模型和分布张量下，IT SHAP将STII的指数复杂度Θ(4^n)降低到NC2并行时间，实现了多项式时间和多对数深度计算

Conclusion: IT SHAP为高维模型中的主效应和高阶交互作用提供了统一、公理化和计算可行的表述，为可扩展的交互感知可解释AI奠定了基础

Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.

</details>


### [37] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: Roblox Guard 1.0是基于Llama-3.1-8B-Instruct构建的指令微调LLM，用于增强LLM系统的安全性，通过输入输出审查和管道化LLM提升审查能力，并发布了RobloxGuard-Eval评估基准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在训练后阶段进行了安全对齐，但仍可能生成不适当的输出，对用户构成风险，因此需要跨模型输入和输出的强大安全保障机制。

Method: 基于Llama-3.1-8B-Instruct骨干，通过指令微调使其能泛化到未见过的安全分类法；使用合成和开源安全数据集混合，增强链式思维推理和输入反转以提升上下文理解和决策能力。

Result: 模型在领域外安全基准测试中表现出色，展示了强大的性能；同时发布了RobloxGuard-Eval基准，包含可扩展的安全分类法来评估LLM防护栏和审查框架的有效性。

Conclusion: Roblox Guard 1.0通过综合的输入输出审查增强了LLM系统的安全性，其指令微调方法能有效泛化到新安全威胁，为LLM安全防护提供了实用解决方案和评估工具。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [38] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出一种针对硬件代码生成的LLM遗忘框架，通过语法保持遗忘策略和细粒度选择性损失，有效移除问题知识而不损害代码生成能力。


<details>
  <summary>Details</summary>
Motivation: LLM在加速数字硬件设计方面潜力巨大，但现有模型存在记忆专有IP、基准污染和不安全编码模式等问题，需要确保其可靠性。

Method: 结合语法保持遗忘策略（保护硬件代码结构完整性）和细粒度floor-aware选择性损失（精确高效移除问题知识）的遗忘框架。

Result: 实验表明该框架支持3倍大的遗忘集，通常只需单个训练周期，同时保持RTL代码的语法正确性和功能完整性。

Conclusion: 该工作为实现可靠的LLM辅助硬件设计开辟了新途径。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [39] [Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation](https://arxiv.org/abs/2512.05367)
*Mariia Karabin,Isaac Armstrong,Leo Beck,Paulina Apanel,Markus Eisenbach,David B. Mitzi,Hanna Terletska,Hendrik Heinz*

Main category: cs.LG

TL;DR: 提出机器学习框架预测杂化金属卤化物结构维度，使用化学特征工程和类别不平衡处理技术提升预测准确率


<details>
  <summary>Details</summary>
Motivation: 杂化金属卤化物（包括有机-无机钙钛矿）的结构维度（0D、1D、2D、3D）预测对材料设计至关重要，但现有数据集类别高度不平衡，导致预测模型性能受限

Method: 结合化学特征工程（开发相互作用描述符）和类别不平衡处理技术（SMOTE数据增强），采用多阶段工作流程包括特征选择、模型堆叠和性能优化

Result: 显著提高了少数类别的F1分数，在所有维度上都实现了稳健的交叉验证性能，数据集从494个样本扩展到1336个

Conclusion: 该框架有效解决了杂化金属卤化物结构维度预测中的类别不平衡问题，为材料设计提供了可靠的预测工具

Abstract: We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.

</details>


### [40] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: 该研究扩展了CorrDiff扩散模型，用于中国区域的气象统计降尺度，将覆盖区域扩大20倍，并增加了高空变量，在3km分辨率预报中优于传统区域模型。


<details>
  <summary>Details</summary>
Motivation: 数值天气预报中高效生成高分辨率预报是一个基本挑战。统计降尺度通过建立低分辨率与高分辨率历史数据间的统计关系来解决此问题。深度学习特别是扩散模型在此任务中表现出强大能力，但现有方法在覆盖区域和变量类型上有限制。

Method: 基于CorrDiff扩散降尺度框架，将覆盖区域扩大近20倍，不仅考虑地表变量，还增加了6个气压层的高空变量作为目标降尺度变量。添加全局残差连接以提高精度。使用CMA-GFS（25km全球网格预报）和SFF（基于球形傅里叶神经算子的数据驱动模型）作为输入，生成中国区域3km预报。

Result: 实验结果表明，该方法降尺度的预报在目标变量的MAE方面普遍优于CMA-MESO区域模型的直接预报。雷达组合反射率预报显示，CorrDiff作为生成模型能够生成精细尺度细节，相比确定性回归模型产生更真实的预测。

Conclusion: 扩展后的CorrDiff扩散模型能够有效应用于更大区域和更多变量类型的气象降尺度任务，生成的高分辨率预报在精度和真实性方面优于传统区域模型，展示了生成模型在气象预报中的潜力。

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [41] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: 评估蛋白质-配体评分函数在新靶点上的泛化能力，发现常用基准测试无法反映真实挑战，探索大规模自监督预训练和简单测试数据利用方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在分子设计中越来越重要，需要确保可学习的蛋白质-配体评分函数在新蛋白质靶点上的可靠性。虽然许多评分函数在标准基准测试中表现良好，但它们超越训练数据的泛化能力仍然是一个重大挑战。

Method: 评估最先进评分函数在模拟有限已知结构和实验亲和力测量的新靶点数据集分割上的泛化能力。研究大规模自监督预训练是否能弥合泛化差距，并探索利用有限测试靶点数据改进评分函数性能的简单方法。

Result: 分析显示常用的基准测试不能反映泛化到新靶点的真实挑战。提供了大规模自监督预训练潜力的初步证据，并探讨了利用有限测试数据改进性能的方法。

Conclusion: 研究结果强调需要更严格的评估协议，并为设计具有扩展到新蛋白质靶点预测能力的评分函数提供实用指导。

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [42] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: 提出MineROI-Net，一个基于Transformer的模型，用于预测比特币矿机购买时机，将硬件采购决策转化为时间序列分类任务，预测一年内投资回报率是否盈利。


<details>
  <summary>Details</summary>
Motivation: 比特币挖矿硬件采购面临市场波动、技术快速淘汰和协议驱动收入周期等挑战，但缺乏何时购买新ASIC硬件的指导，也没有计算框架解决这一决策问题。

Method: 将硬件采购决策制定为时间序列分类任务，预测购买ASIC机器在一年内是否盈利。提出MineROI-Net，一种开源Transformer架构，旨在捕捉挖矿盈利能力的多尺度时间模式。

Result: 在2015-2024年间发布的20款ASIC矿机数据上评估，MineROI-Net优于LSTM和TSLANet基线，达到83.7%准确率和83.1%宏观F1分数。模型在经济相关性方面表现强劲，检测无盈利时期的精确度达93.6%，盈利时期达98.5%。

Conclusion: MineROI-Net为矿机采购时机提供了实用的数据驱动工具，可能降低资本密集型挖矿操作的财务风险。模型已开源。

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [43] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD：一种反射式进化编排器，通过多轮多专家共识、自适应反射探索和帕累托进化选择，将LLM推理与反馈对齐的架构搜索有效结合，实现高性能神经架构设计。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的神经架构设计系统存在挑战：令牌级设计循环是离散且不可微分的，无法通过反馈平滑指导架构改进。这些方法容易陷入冗余结构的模式崩溃或漂向不可行的设计。

Method: 1. 多轮多专家共识：将孤立的设计规则转化为有意义的架构线索；2. 自适应反射探索：根据奖励方差调整探索程度，在反馈不确定时探索，在稳定时细化；3. 帕累托进化选择：联合优化准确性、效率、延迟、置信度和结构多样性。

Result: 在CIFAR10、CIFAR100、ImageNet16-120、COCO-5K和Cityscape等多个数据集上实现了最先进的性能。消融和迁移研究进一步验证了RevoNAD在实际可靠和可部署神经架构设计中的有效性。

Conclusion: RevoNAD通过将LLM推理与反馈对齐的架构搜索有效结合，解决了现有LLM驱动架构设计方法的局限性，实现了高性能、可靠且可部署的神经架构设计。

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [44] [Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets](https://arxiv.org/abs/2512.05416)
*Bozhi Dan,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: Triplet-GCN：一种基于图卷积网络的脓毒症早期预警模型，通过患者-特征-值三元组构建二分图，在稀疏、异构的电子健康记录数据上优于传统表格模型。


<details>
  <summary>Details</summary>
Motivation: 在重症监护环境中，脓毒症是导致患者发病和死亡的主要原因，但其及时检测受到电子健康记录数据复杂、稀疏和异构性的阻碍。传统方法难以有效处理这种数据结构。

Method: 提出Triplet-GCN模型：1）将每次就诊表示为患者-特征-值三元组；2）构建二分EHR图；3）使用图卷积网络学习患者嵌入，后接轻量级多层感知机；4）采用类型特定的预处理策略（数值变量中位数插补和标准化，二元特征效应编码，稀有分类属性众数插补和低维嵌入）；5）用汇总统计初始化患者节点，同时在边上保留测量值以保持"谁测量了什么以及测量了多少"的信息。

Result: 在中国三家三级医院的回顾性多中心队列（N=648；70/30训练测试分割）中，Triplet-GCN在区分度和平衡误差指标上持续优于强大的表格基线模型（KNN、SVM、XGBoost、随机森林），产生更有利的敏感性-特异性权衡，并提高了早期预警的总体效用。

Conclusion: 将EHR编码为三元组并在患者-特征图上传播信息，比特征独立模型产生更具信息量的患者表示，为可部署的脓毒症风险分层提供了一个简单、端到端的蓝图。

Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient-feature-value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train-test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity-specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient-feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.

</details>


### [45] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: TS-Hint：基于时间序列基础模型和思维链推理的半导体制造MRR预测框架，在有限数据下通过少样本学习有效工作


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法从时间序列中提取静态特征来近似半导体制造过程的材料去除率，但会丢失时间动态信息，且需要大量数据进行有效训练

Method: 提出TS-Hint框架，集成时间序列基础模型和思维链推理，在训练过程中基于注意力机制数据和显著性数据提供注意力提示

Result: 实验结果表明该模型在有限数据设置下通过少样本学习有效，并能直接从多元时间序列特征中学习

Conclusion: TS-Hint框架能够克服现有方法的局限性，在数据有限的情况下有效预测半导体制造过程的材料去除率

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [46] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: IdealTSF框架利用非理想负样本增强时间序列预测，通过预训练、训练和优化三阶段，结合对抗扰动机制，在噪声数据场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据中常见的缺失值和异常值问题阻碍了深度学习预测性能的进一步提升。传统方法主要关注从序列数据提取特征或将非理想数据作为正样本进行知识迁移，而本文提出利用非理想负样本来增强事件预测效果。

Method: 提出IdealTSF框架，包含三个渐进步骤：1) 预训练阶段从负样本数据中提取知识；2) 训练阶段将序列数据转化为理想正样本；3) 优化阶段采用带有对抗扰动的负优化机制。

Result: 实验表明负样本数据在基础注意力架构中释放了显著的时间序列预测潜力，IdealTSF特别适合噪声样本或低质量数据的应用场景。

Conclusion: IdealTSF框架通过整合理想正负样本，有效利用非理想负样本提升时间序列预测性能，为噪声数据环境下的预测任务提供了有效解决方案。

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [47] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: 研究探讨集成模型如何在保持高精度的同时控制过拟合，通过四个表格分类任务分析不同集成方法的泛化能力，发现集成在非线性数据上能显著提升精度而保持小泛化差距。


<details>
  <summary>Details</summary>
Motivation: 集成模型通常比单一学习器获得更高精度，但其保持小泛化差距的能力尚未被充分理解。本研究旨在探究集成模型如何在精度和过拟合之间取得平衡，为实际表格应用提供模型选择指导。

Method: 使用重复分层交叉验证和统计显著性检验，在四个表格分类任务（乳腺癌、心脏病、皮马糖尿病、信用卡欺诈）上比较线性模型、单一决策树和九种集成方法。同时计算数据集复杂度指标（线性度得分、Fisher比率、噪声估计）来解释集成效果。

Result: 在接近线性的干净数据上，线性模型已能很好泛化，集成提供额外收益有限。在具有有意义非线性结构的数据集上，基于树的集成能将测试精度提高5-7个百分点，同时保持泛化差距低于3%。在噪声大或高度不平衡的数据集上，集成仍具竞争力但需要正则化以避免拟合噪声或多数类模式。

Conclusion: 集成模型通过平均或受控提升减少方差，能在保持高精度的同时控制过拟合。数据集复杂度指标能有效预测集成何时能有效控制方差。研究为实际表格应用中的模型选择提供了清晰指导。

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [48] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 比较不同几何量子机器学习模型在分子几何结构学习中的性能，发现置换对称嵌入是最具泛化性的量子模型


<details>
  <summary>Details</summary>
Motivation: 研究分子几何结构层次下，不同对称性等变量子机器学习模型的性能差异，为几何数据集选择合适模型提供标准

Method: 使用两种分子数据集（线性LiH分子和三角锥形NH3分子），比较无对称性、旋转置换等变性、图嵌入置换等变性量子模型，以经典等变模型为基准

Result: 图嵌入特征能有效提升几何数据集的训练能力，置换对称嵌入量子模型在几何学习中具有最佳泛化性能

Conclusion: 分子几何结构影响模型选择，置换对称嵌入量子机器学习模型是几何学习中最具泛化性的选择

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [49] [Turbulence Regression](https://arxiv.org/abs/2512.05483)
*Yingang Fan,Binjie Ding,Baiyi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于离散化数据的NeuTucker分解模型，用于处理三维风场数据的湍流预测问题，通过构建四维Tucker交互张量来捕捉时空交互，在缺失观测估计中表现优于传统回归模型。


<details>
  <summary>Details</summary>
Motivation: 低空湍流结果复杂多变，传统方法在使用风廓线雷达数据时难以准确预测湍流状态，需要新的建模方法来处理连续但稀疏的三维风场数据。

Method: 1) 将连续输入数据离散化以适应需要离散数据输入的模型；2) 构建四维Tucker交互张量来表示不同高度和三维风速之间所有可能的时空交互；3) 基于Tucker神经网络构建低秩Tucker分解模型来捕捉三维风场数据中的潜在交互。

Result: 在真实数据集的缺失观测估计中，离散化NeuTucF模型相比各种常见回归模型表现出更优越的性能。

Conclusion: 提出的NeuTucker分解模型能够有效处理三维风场数据的湍流预测问题，通过离散化处理和Tucker张量分解方法，显著提升了在稀疏观测条件下的预测准确性。

Abstract: Air turbulence refers to the disordered and irregular motion state generated by drastic changes in velocity, pressure, or direction during airflow. Various complex factors lead to intricate low-altitude turbulence outcomes. Under current observational conditions, especially when using only wind profile radar data, traditional methods struggle to accurately predict turbulence states. Therefore, this paper introduces a NeuTucker decomposition model utilizing discretized data. Designed for continuous yet sparse three-dimensional wind field data, it constructs a low-rank Tucker decomposition model based on a Tucker neural network to capture the latent interactions within the three-dimensional wind field data. Therefore, two core ideas are proposed here: 1) Discretizing continuous input data to adapt to models like NeuTucF that require discrete data inputs. 2) Constructing a four-dimensional Tucker interaction tensor to represent all possible spatio-temporal interactions among different elevations and three-dimensional wind speeds. In estimating missing observations in real datasets, this discretized NeuTucF model demonstrates superior performance compared to various common regression models.

</details>


### [50] [GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop](https://arxiv.org/abs/2512.05502)
*Omid Bazgir,Vineeth Manthapuri,Ilia Rattsev,Mohammad Jafarnejad*

Main category: cs.LG

TL;DR: GRASP是一个多智能体、图推理框架，通过将QSP模型编码为类型化生物知识图谱并编译为可执行代码，显著提升了药物开发中定量系统药理学建模的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 定量系统药理学（QSP）建模对药物开发至关重要，但传统方法需要大量时间投入，限制了领域专家的吞吐量。需要一种既能提高效率又能保持生物医学严谨性的解决方案。

Method: GRASP采用多智能体、图推理框架，将QSP模型编码为类型化生物知识图谱，编译为可执行的MATLAB/SimBiology代码。采用两阶段工作流：理解阶段（重构遗留代码为图）和行动阶段（约束检查、语言驱动的修改），通过状态机进行协调。使用广度优先参数对齐来发现依赖关系并提出生物学合理的默认值。

Result: 在LLM作为评判者的头对头评估中，GRASP在生物合理性、数学正确性、结构保真度和代码质量方面显著优于SME引导的CoT和ToT基线（约9-10/10 vs. 5-7/10）。BFS对齐在依赖发现、单位和范围方面达到F1=0.95。

Conclusion: 图结构化的智能体工作流可以使QSP模型开发既易于访问又保持严谨性，使领域专家能够用自然语言指定机制而不牺牲生物医学保真度。

Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.

</details>


### [51] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 提出CDEC和IDEC两种新方法，利用信度集和证据预测分布区间来处理分类任务中的不确定性量化，避免过拟合并系统评估认知和偶然不确定性，在不确定性过高时能够拒绝分类。


<details>
  <summary>Details</summary>
Motivation: 不确定性量化是AI领域的关键挑战，直接影响决策制定、风险评估和模型可靠性。现有方法存在局限性，需要新的方法来系统评估认知和偶然不确定性。

Method: 提出CDEC（信度深度证据分类）和IDEC（区间深度证据分类）两种方法：CDEC使用信度集（封闭凸概率集），IDEC使用证据预测分布区间。两者都采用标准反向传播和基于证据理论的损失函数进行训练。

Result: 在MNIST、CIFAR-10、CIFAR-100及其自然OoD偏移数据集上的实验表明：CDEC和IDEC实现了有竞争力的预测准确率、在认知和总不确定性下的最先进OoD检测能力，以及紧密且校准良好的预测区域。CDEC仅需小规模集成就能获得稳定的不确定性估计。

Conclusion: CDEC和IDEC是处理分类任务中不确定性量化的有效新方法，能够克服先前工作的不足，扩展了当前证据深度学习的文献，为模型可靠性提供了系统的不确定性评估框架。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [52] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: IDK-S是一种用于数据流异常检测的增量分布核方法，通过动态核均值嵌入实现高精度实时检测，比现有方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 数据流异常检测面临两大挑战：需要在分布不断演变的情况下保持高检测精度，同时确保实时效率。现有方法难以同时满足这两个要求。

Method: 提出IDK-S（增量分布核流式异常检测），继承离线检测器Isolation Distributional Kernel的优势，采用轻量级增量更新机制，避免完整模型重训练，在核均值嵌入框架中创建动态表示。

Result: 在13个基准测试中，IDK-S实现了优于现有最先进方法的检测精度，同时运行速度显著更快（在许多情况下快一个数量级），且与完整重训练模型统计等价。

Conclusion: IDK-S成功解决了数据流异常检测中精度与效率的平衡问题，通过创新的增量分布核方法，在保持高检测精度的同时大幅提升计算效率。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [53] [On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability](https://arxiv.org/abs/2512.05534)
*Yiming Tang,Harshvardhan Saini,Yizhen Liao,Dianbo Liu*

Main category: cs.LG

TL;DR: 该论文提出了首个统一的理论框架来分析稀疏字典学习（SDL）方法，为包括稀疏自编码器、转码器和交叉编码器在内的多种SDL方法提供理论基础，并解释了特征吸收、死亡神经元等经验现象。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力增强，理解其内部表示和处理机制变得至关重要。虽然稀疏字典学习方法在经验上取得了成功，但缺乏统一的理论框架，现有理论仅局限于特定类型的稀疏自编码器，无法解释更广泛的SDL方法。

Method: 提出了一个统一的优化问题框架，将各种SDL方法（稀疏自编码器、转码器、交叉编码器等）纳入其中。通过严格的优化景观分析，为不同方法提供了理论解释，并设计了控制实验验证理论结果。

Result: 建立了首个统一的SDL理论框架，成功解释了特征吸收、死亡神经元等经验现象，并为神经元重采样技术提供了理论依据。控制实验验证了理论预测的正确性。

Conclusion: 该工作填补了稀疏字典学习领域的理论空白，为理解神经网络表示学习提供了坚实的理论基础，有助于推动可解释AI的发展。

Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.

</details>


### [54] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出SCoNE方法解决多视图异常检测问题，通过球面一致邻域集成实现O(N)时间复杂度，无需学习过程，在稀疏和密集区域自适应调整邻域大小，显著提升检测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有多视图异常检测方法存在两个关键问题：1）在不同视图密度变化区域难以有效捕获一致邻域，导致检测精度低；2）学习过程计算复杂度为O(N²)，不适用于大规模数据集。

Method: 提出SCoNE方法，直接使用多视图实例表示一致邻域，无需中间表示；邻域具有数据依赖特性，在稀疏区域使用大邻域，在密集区域使用小邻域；无需学习过程，实现O(N)时间复杂度。

Result: 实验评估表明，SCoNE具有优越的检测精度，在大规模数据集上运行速度比现有方法快几个数量级。

Conclusion: SCoNE通过数据依赖的球面一致邻域表示，有效解决了多视图异常检测中的一致邻域捕获和计算效率问题，为大规模应用提供了可行方案。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [55] [RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs](https://arxiv.org/abs/2512.05542)
*Jonathan Geuter,Gregor Kornhardt*

Main category: cs.LG

TL;DR: RoBoN是一种多LLM推理方法，通过在线路由在不同模型间顺序生成响应，利用奖励模型和一致性信号选择最佳输出，无需额外训练即可超越传统单模型best-of-n方法。


<details>
  <summary>Details</summary>
Motivation: 尽管证据表明不同LLM在不同任务上具有互补优势，但传统的best-of-n方法仅依赖单一模型生成响应。作者希望利用多模型多样性来提升推理性能。

Method: RoBoN（Routed Online Best-of-n）是一种顺序多LLM推理方法：给定一组模型，基于奖励模型分数和预测响应的一致性信号，逐个路由生成过程。该方法无需额外训练，保持计算对等，可与任何插件奖励模型配合使用。

Result: 在多个推理基准测试（MATH500、OlympiadBench、MinervaMath、GSM8K、MMLU）上，RoBoN在较大n值时始终优于对每个单独模型应用的标准best-of-n方法，绝对准确率提升高达3.4%，也优于均匀多模型组合基线。

Conclusion: 模型间的多样性可以在推理时被利用来提升best-of-n性能，超越任何单一组成模型，为多LLM测试时扩展提供了一种简单、无需训练的路径。

Abstract: Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.

</details>


### [56] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 提出一种基于MARS和N-ball采样的新型解释方法，相比LIME能更好地捕捉非线性局部边界，提高解释的忠实度。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒机器学习模型在关键领域的应用增加，需要提供可靠的预测解释。现有LIME方法假设局部决策边界是线性的，无法捕捉非线性关系，导致解释不准确。

Method: 使用多元自适应回归样条(MARS)建模非线性局部边界，捕捉参考模型的底层行为；采用N-ball采样技术直接从期望分布采样，而非像LIME那样重新加权样本。

Result: 在三个UCI数据集上测试不同分类器和核宽度，相比基线方法平均减少37%的均方根误差，显著提高了局部忠实度。

Conclusion: 提出的方法能生成更高忠实度的解释，通过MARS建模非线性边界和N-ball采样技术，有效解决了LIME的局限性。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [57] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出熵比裁剪(ERC)机制，通过约束当前策略与先前策略的熵比来稳定强化学习训练，解决分布偏移问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型后训练依赖强化学习，但离策略训练范式引入分布偏移，导致训练不稳定（策略熵波动、梯度不稳定）。PPO-Clip通过重要性裁剪缓解此问题，但忽略了动作的全局分布偏移。

Method: 提出熵比作为衡量策略探索相对变化的全局指标，并基于此引入熵比裁剪(ERC)机制，对熵比施加双向约束。将ERC集成到DAPO和GPPO强化学习算法中。

Result: 在多个基准测试上的实验表明，ERC能持续提升性能，稳定策略更新。

Conclusion: ERC机制能有效量化策略探索的相对变化，在全局分布层面稳定策略更新，弥补PPO-clip无法调节未采样动作概率偏移的不足。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [58] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 该研究探讨了如何通过超参数迁移来扩展预条件优化器（如Shampoo、SOAP、Muon）的规模，发现遵循μP规则缩放学习率并结合块化和显式谱归一化可改善迁移效果，同时提出计算最优缩放中权重衰减应按1/宽度缩放。应用这些规则后，Muon和Shampoo在190M到1.4B参数的Llama架构语言模型训练中分别实现了1.4倍和1.3倍于AdamW的加速。


<details>
  <summary>Details</summary>
Motivation: 近期基于矩阵级预条件的深度学习优化器在小规模实验中显示出优于AdamW的加速效果，但验证和复现结果不一。为了理解这些优化器在大规模场景下的有效性，需要研究如何通过超参数迁移来扩展预条件优化器的规模。

Method: 研究学习率和权重衰减如何随模型宽度和深度缩放，涵盖Shampoo、SOAP、Muon等多种优化器，考虑块化和嫁接等常用技术的影响。应用μP理论框架，分析有限宽度偏差问题，并提出通过块化和显式谱归一化来缓解。

Result: 遵循μP规则缩放学习率可改善迁移效果，但仍存在有限宽度偏差导致最优学习率漂移，块化和显式谱归一化可缓解此问题。在计算最优缩放中，权重衰减按1/宽度缩放近乎最优。应用这些规则后，Muon和Shampoo在190M到1.4B参数的Llama模型训练中分别实现1.4倍和1.3倍于AdamW的加速，而错误缩放下加速效果随规模增大迅速消失。

Conclusion: 研究最优超参数迁移对于在现实调优预算下可靠比较大规模优化器至关重要。正确的缩放规则能确保预条件优化器在大规模训练中保持优势，而错误缩放会导致加速效果消失。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [59] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 提出一种灵活的GNN社区检测框架，允许用户指定社区数量范围或精确值，解决传统GNN方法无法可靠控制输出社区数量的问题。


<details>
  <summary>Details</summary>
Motivation: 传统社区检测方法需要预先指定社区数量，而GNN方法即使指定了期望数量也常常无法准确返回该数量，存在设计上的局限性。

Method: 提出一个灵活的原则性框架，允许用户指定社区数量的合理范围，并在训练过程中强制执行这些边界约束；同时也支持指定精确的社区数量。

Result: 该方法能够可靠地返回用户指定的社区数量，无论是范围约束还是精确值，解决了GNN在社区数量控制方面的不足。

Conclusion: 该框架为GNN社区检测提供了灵活且可靠的数量控制机制，增强了方法的实用性和适应性。

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [60] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM（选择性梯度掩码）是一种改进的梯度路由技术，通过在预训练时选择性屏蔽梯度，将目标知识隔离到特定参数中以便后续移除，相比数据过滤和传统梯度路由，在标签噪声下具有更好的保留/遗忘权衡和对抗微调鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具有双重用途风险，数据过滤作为预训练缓解方法面临标签成本高和标签噪声问题，即使少量错误标记也可能导致危险能力。需要更鲁棒的方法来处理错误标记的有害内容。

Method: 提出选择性梯度掩码（SGTM），改进梯度路由技术，通过选择性零掩码梯度，使目标领域示例仅更新其专用参数，从而将目标知识隔离到特定参数子集中。

Result: 在双语数据集上移除特定语言知识，以及在英文维基百科上移除生物学知识的实验中，SGTM在标签错误情况下相比数据过滤和传统梯度路由提供更好的保留/遗忘权衡。对抗微调鲁棒性比基于微调的遗忘方法（RMU）强7倍。

Conclusion: SGTM为现有安全缓解措施提供了有前景的预训练补充，特别是在标签噪声不可避免的场景中，能够鲁棒地处理错误标记的有害内容。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [61] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI辅助的终端用户编程（通过自然语言与AI助手交互生成代码）相比传统可视化低代码/无代码平台，可能为组织数字化转型提供更灵活、快速、可复用且减少供应商锁定的终端用户开发新范式。


<details>
  <summary>Details</summary>
Motivation: 探索AI辅助的终端用户编程是否可行，能否补充甚至替代现有的可视化低代码/无代码平台，为组织数字化转型提供新的终端用户开发范式。

Method: 通过案例研究，让非程序员通过与AI助手交互来开发基础Web应用，分析任务完成情况、时间效率及用户反馈。

Result: 大多数研究参与者在合理时间内成功完成任务，并支持AI辅助的终端用户编程作为可行的终端用户开发方法。

Conclusion: AI辅助的终端用户编程是可行的终端用户开发范式，可能补充或替代传统低代码/无代码平台，对实践、未来研究和学术教学具有重要启示。

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [62] [Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks](https://arxiv.org/abs/2512.05680)
*Alexander Mattick,George Yammine,Georgios Kontes,Setareh Maghsudi,Christopher Mutschler*

Main category: cs.LG

TL;DR: 该论文提出了一种基于POMDP的波束选择方法，将波束选择问题建模为在线搜索过程，优于现有的监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 在5G/6G网络中，大规模天线阵列的模拟波束成形面临挑战：大码本、反射和遮挡效应使得最优波束选择困难。现有方法使用监督学习基于历史波束预测最优波束，但无法处理新的轨迹和环境变化。

Method: 将波束选择问题建模为部分可观测马尔可夫决策过程（POMDP），将环境建模为码本本身。在每个时间步，基于不可观测最优波束的信念状态和先前探测的波束来选择候选波束，将波束选择问题框架化为在线搜索过程。

Result: 该方法能够处理新的或不可预见的轨迹和物理环境变化，性能比先前工作高出几个数量级。

Conclusion: 基于POMDP的在线搜索方法比监督学习方法更有效地解决了移动用户的最优波束选择问题，具有更好的适应性和性能。

Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.

</details>


### [63] [BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language](https://arxiv.org/abs/2512.05721)
*Nitin Priyadarshini Shankar,Vaibhav Singh,Sheetal Kalyani,Christian Maciocco*

Main category: cs.LG

TL;DR: BERTO是一个基于BERT的框架，用于蜂窝网络的流量预测和能耗优化，通过自然语言提示在节能和性能之间进行权衡调整。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络需要智能的流量预测和能耗优化方案，以平衡节能需求和网络性能保障。现有模型难以灵活调整节能与性能之间的权衡关系。

Method: 基于Transformer架构构建BERTO框架，采用平衡损失函数和基于提示的自定义机制，通过自然语言提示指导模型管理预测偏差，根据运营商意图调整节能与性能的权衡。

Result: 在真实数据集上的实验表明，BERTO相比现有模型将MSE降低了4.13%，能够在1.4kW功率范围和9倍服务质量变化范围内灵活操作，通过简单自然语言输入平衡节能与性能目标。

Conclusion: BERTO为智能RAN部署提供了一个有效的解决方案，通过自然语言提示实现了节能与性能目标的灵活平衡，具有实际应用价值。

Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.

</details>


### [64] [Teaching Language Models Mechanistic Explainability Through Arrow-Pushing](https://arxiv.org/abs/2512.05722)
*Théo A. Neukomm,Zlatko Jončev,Philippe Schwaller*

Main category: cs.LG

TL;DR: 提出MechSMILES格式和语言模型框架，用于预测化学反应机理，提升计算机辅助合成规划的机理基础


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划系统缺乏机理基础，需要建立能够预测化学反应机理的框架，使合成规划更具化学有效性和可解释性

Method: 开发MechSMILES紧凑文本格式编码分子结构和电子流动，训练语言模型在四个复杂度递增的机理预测任务上，使用mech-USPTO-31k和FlowER等机理反应数据集

Result: 模型在基本步骤预测上达到95%以上top-3准确率，在mech-USPTO-31k上超过73%，在FlowER数据集上达到93%的完整反应机理检索准确率

Conclusion: 该工作通过基于物理意义的电子移动预测，为计算合成规划提供了更可解释和化学有效的途径，同时建立了架构无关的机理预测基准框架

Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.

</details>


### [65] [Towards agent-based-model informed neural networks](https://arxiv.org/abs/2512.05764)
*Nino Antulov-Fantulin*

Main category: cs.LG

TL;DR: ABM-NNs框架：将基于主体模型的原则融入神经网络设计，通过受限图神经网络和层次分解学习可解释、结构保持的动态系统。


<details>
  <summary>Details</summary>
Motivation: 标准神经微分方程在建模复杂系统时存在局限，缺乏物理不变量（如能量），但需要强制执行其他约束（如质量守恒、网络局部性、有限理性）。需要开发能够保持基于主体模型原则的神经网络框架。

Method: 提出基于主体模型知情的神经网络（ABM-NNs），利用受限图神经网络和层次分解来学习可解释、结构保持的动态系统。该方法将ABM的结构约束融入神经网络架构中。

Result: 在三个复杂度递增的案例研究中验证：1)广义Lotka-Volterra系统中从短轨迹恢复真实参数；2)图基SIR传染模型中在样本外预测和噪声鲁棒性上优于GCN、GraphSAGE、Graph Transformer等基线；3)十大经济体宏观经济模型中从经验数据学习耦合GDP动态，并进行基于梯度的反事实政策分析。

Conclusion: ABM-NNs框架成功地将基于主体模型的原则融入神经网络设计，实现了可解释、结构保持的动态学习，在多个复杂系统建模任务中表现出优越性能。

Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.

</details>


### [66] [Learnability Window in Gated Recurrent Neural Networks](https://arxiv.org/abs/2512.05790)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，解释门控机制如何决定循环神经网络的"可学习窗口"——梯度信息在时间上保持统计可恢复的最大时间范围。研究发现，可学习性由门控诱导的有效学习率控制，而不是传统的雅可比乘积稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统分析强调雅可比乘积的数值稳定性，但作者认为这不足以解释循环神经网络学习长期依赖的能力。需要一个新的理论框架来理解门控机制如何影响梯度信息在时间上的传播和统计可恢复性。

Method: 通过一阶展开门控诱导的雅可比乘积，推导出每个时间滞后和每个神经元的有效学习率μ_{t,ℓ}。在重尾（α-稳定）梯度噪声假设下，分析检测滞后ℓ依赖所需的最小样本量N(ℓ) ∝ f(ℓ)^{-α}，其中f(ℓ)=‖μ_{t,ℓ}‖₁是有效学习率包络。

Result: 理论得出可学习窗口H_N的显式公式，以及对数、多项式和指数衰减的闭式标度律。更宽或更异构的门控谱产生更慢的f(ℓ)衰减，从而产生更大的可学习窗口；而更重的尾部噪声通过减慢统计集中来压缩H_N。

Conclusion: 有效学习率是控制门控循环网络何时以及多长时间能够学习长期时间依赖的基本量。该框架将门控诱导的时间尺度结构、梯度噪声和样本复杂度联系起来，为理解循环神经网络的学习能力提供了新的理论基础。

Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $μ_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($α$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-α}$, where $f(\ell)=\|μ_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.

</details>


### [67] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: 比较TopK SAE和Ordered SAE在抗体语言模型p-IgGen中的表现：TopK SAE能揭示生物意义特征但因果控制有限，Ordered SAE能可靠识别可操控特征但激活模式更复杂


<details>
  <summary>Details</summary>
Motivation: 研究稀疏自编码器在蛋白质语言模型中的机制可解释性，特别是如何利用SAE技术来理解和操控抗体语言模型p-IgGen的生成过程

Method: 使用TopK SAE和Ordered SAE两种稀疏自编码器技术分析自回归抗体语言模型p-IgGen，比较它们在特征识别和生成操控方面的表现

Result: TopK SAE能揭示具有生物学意义的潜在特征，但高特征概念相关性不能保证对生成的因果控制；Ordered SAE通过层次结构能可靠识别可操控特征，但激活模式更复杂且可解释性降低

Conclusion: TopK SAE适用于将潜在特征映射到概念，而Ordered SAE在需要精确生成操控时更优，这推进了领域特定蛋白质语言模型的机制可解释性

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [68] [Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws](https://arxiv.org/abs/2512.05817)
*Zhengquan Luo,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架来分析数据集蒸馏，揭示了性能随蒸馏样本数增加的缩放规律，以及所需样本数与配置多样性之间的线性关系，解释了现有方法的共同原理。


<details>
  <summary>Details</summary>
Motivation: 数据集蒸馏虽然取得了快速的经验进展，但缺乏统一的理论基础。现有方法基于不同的代理目标和优化假设，难以分析其共同原理或提供一般性保证。同时，不清楚在训练配置（如优化器、架构、数据增强）变化时，蒸馏数据如何保持有效性。

Method: 提出了一个统一的理论框架——配置-动态-误差分析，将主要的数据集蒸馏方法重新表述在共同的泛化误差视角下。该框架提供了两个主要结果：缩放规律（单配置上界）和覆盖规律（所需蒸馏样本数与配置多样性呈线性关系）。

Result: 理论分析表明：1）误差随蒸馏样本数增加而减少，解释了常见的性能饱和效应；2）所需蒸馏样本数与配置多样性呈线性比例关系，具有可证明匹配的上界和下界；3）各种匹配方法是可互换的代理，减少相同的泛化误差。

Conclusion: 该统一框架为数据集蒸馏提供了理论基础，揭示了现有方法的共同原理，解释了为什么它们都能实现数据集蒸馏，并为设计紧凑、配置鲁棒的数据集蒸馏提供了理论指导。实验验证了推导的规律，推动了数据集蒸馏的理论发展。

Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.

</details>


### [69] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文填补了多目标贝叶斯优化中HV改进近似算法文献空白，提供了完整的数学和算法描述


<details>
  <summary>Details</summary>
Motivation: 超体积(HV)贝叶斯优化是多目标决策的标准方法，但计算成本高，主要瓶颈是HV改进计算。虽然HV盒分解能处理频繁的精确改进计算，但在最坏情况下具有超多项式内存复杂度。现有近似算法缺乏严格的算法描述。

Method: 提供Couckuyt等人(2012)提出的超体积改进近似算法的全面数学和算法细节，填补文献中的描述空白

Result: 填补了多目标贝叶斯优化中HV改进近似算法在文献中的描述空白，提供了完整的算法实现细节

Conclusion: 本文通过提供超体积改进近似算法的全面数学和算法描述，解决了该领域长期存在的文献空白问题

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [70] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: NEAT是一种用于3D分子生成的邻域引导、高效、自回归的集合Transformer，通过将分子图视为原子集合并学习图边界上可接受标记的顺序无关分布，解决了现有自回归模型中顺序假设与分子图置换不变性不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在3D分子结构生成中存在一个关键限制：它们假设存在标记顺序，而分子图中的下一个标记预测应该对原子置换保持不变。先前的工作通过使用规范顺序或焦点原子来回避这种不匹配，但作者认为这是不必要的。

Method: NEAT将分子图视为原子集合，使用自回归流模型学习图边界上可接受标记的顺序无关分布。它是一个邻域引导、高效、自回归的集合Transformer，具有原子级置换不变性。

Result: NEAT在3D分子生成中达到了最先进的性能，具有高计算效率，并建立了可扩展分子设计的实用基础。

Conclusion: NEAT通过将分子图视为集合并学习顺序无关分布，解决了自回归模型中顺序假设与分子置换不变性之间的不匹配问题，为可扩展的分子设计提供了实用基础。

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [71] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出一种简单的后训练方法，使Transformer注意力稀疏化而不损失性能，可将注意力连接减少到约0.3%，同时保持预训练损失不变。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer注意力中存在的计算冗余，希望通过稀疏化来获得更结构化、可解释的模型，而不仅仅是为了计算效率。

Method: 采用简单的后训练方法，在约束损失目标下应用灵活的稀疏正则化，使注意力稀疏化，同时保留原始预训练损失。

Result: 在高达10亿参数的模型上，可将注意力连接减少到约0.3%，任务特定电路涉及的组件（注意力头和MLP）显著减少，连接边减少达100倍。

Conclusion: Transformer注意力可以变得稀疏数个数量级，表明其大部分计算是冗余的，稀疏性可作为构建更结构化、可解释模型的指导原则。

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [72] [Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks](https://arxiv.org/abs/2512.05868)
*Brian Ezinwoke,Oliver Rhodes*

Main category: cs.LG

TL;DR: 该研究将脉冲神经网络应用于高频价格尖峰预测，通过贝叶斯优化和惩罚性脉冲准确率目标函数提升性能，在模拟交易中取得显著超额收益。


<details>
  <summary>Details</summary>
Motivation: 传统金融模型难以捕捉高频交易中的细粒度时间结构，而脉冲神经网络具有处理离散事件和保持毫秒级时序的天然优势，适合用于高频价格尖峰预测。

Method: 将高频股票数据转换为脉冲序列，评估三种架构：无监督STDP训练的SNN、具有显式抑制竞争的新型SNN、监督反向传播网络。使用贝叶斯优化和新型目标函数PSA（惩罚性脉冲准确率）进行超参数调优。

Result: 使用PSA优化的模型在模拟交易中始终优于SA优化的模型和基线。扩展的SNN模型在简单回测中实现了最高的累计收益（76.8%），显著超过监督替代方案（42.54%收益）。

Conclusion: 研究验证了脉冲神经网络在通过任务特定目标函数进行稳健调优后，在高频交易价格尖峰预测中的有效潜力。

Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.

</details>


### [73] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: 提出基于机器学习的数据驱动方法预测蒸汽压，用于筛选和发现适用于太空环境的新型液体润滑剂。


<details>
  <summary>Details</summary>
Motivation: 太空环境中移动机械组件（MMAs）的功能和寿命依赖于润滑剂性能。高速或高循环的MMAs需要液体润滑剂，但只有少数液体润滑剂具有足够低的蒸汽压以适应太空真空条件，且现有润滑剂都有设计限制。

Method: 采用数据驱动的机器学习方法预测蒸汽压，模型使用高通量分子动力学模拟和实验数据库数据进行训练，并注重模型的可解释性以识别化学结构与蒸汽压之间的关系。

Result: 基于机器学习模型的见解，提出了几种可能在未来太空润滑剂应用中具有潜力的候选分子。

Conclusion: 该研究开发了一种可解释的机器学习方法，能够有效预测和筛选适用于太空环境的液体润滑剂，为新型太空润滑剂的发现提供了数据驱动的解决方案。

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [74] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: 提出基于神经一致性的模型选择方法，仅需少量无标签目标域数据即可有效选择预训练模型检查点


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖目标域标注数据或同分布验证集，但在数据稀缺、无标签、分布外场景下不可靠。需要一种仅需少量无标签目标域数据的高效模型选择方法

Method: 提出神经一致性概念，通过分析模型在源域和目标域的激活统计特征，设计数据高效的模型选择方法。在ImageNet预训练模型上，针对Food-101、PlantNet-300K、iNaturalist等目标域进行验证

Result: 相比基线方法，该方法在不同目标域上显著提升泛化性能，并在元学习设置中有效。神经一致性还可用于训练数据选择，展现其作为通用原则的潜力

Conclusion: 神经一致性是一种有效的模型选择原则，仅需少量无标签目标域数据即可可靠选择预训练检查点，在数据稀缺、分布外场景中具有重要应用价值

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [75] [DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints](https://arxiv.org/abs/2512.05881)
*Rahul Golder,Bimol Nath Roy,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: DAE-HardNet：一种通过可微分投影层严格满足微分代数方程约束的物理约束神经网络，相比传统PINNs能实现物理损失的数量级降低。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）通常以软约束方式最小化物理约束违反，难以严格满足包含微分算子的约束。数据驱动模型将原始函数视为黑箱，其导数只能在函数求值后获得，这使得严格嵌入微分代数方程（DAEs）的领域知识和第一性原理具有挑战性。

Method: 提出DAE-HardNet，一种物理约束神经网络，通过可微分投影层将模型预测投影到约束流形上，同时学习函数及其导数，并强制满足代数和微分约束。

Result: 相比多层感知机（MLPs）和PINNs，DAE-HardNet在保持预测精度的同时，实现了物理损失的数量级降低。具有学习导数的额外优势，改进了投影层之前骨干神经网络的约束学习。对于特定问题，可以绕过投影层实现更快推理。

Conclusion: DAE-HardNet通过可微分投影层严格满足微分代数方程约束，显著优于传统PINNs的软约束方法，能够同时学习函数和导数，并在多个测试问题中验证了有效性，包括参数估计能力。

Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.

</details>


### [76] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: KQ-SVD：通过直接对注意力矩阵进行最优低秩分解来压缩KV缓存的新方法，相比现有方法能更准确地保持注意力输出


<details>
  <summary>Details</summary>
Motivation: 随着序列长度和批处理大小的增长，Transformer大语言模型中的KV缓存成为主要内存瓶颈。现有压缩方法通常只对键进行低秩分解或联合嵌入查询和键，但这些方法忽略了注意力本质上依赖于它们的内积，因此不是最优的。

Method: 提出KQ-SVD方法，通过闭式解直接对注意力矩阵进行最优低秩分解。该方法针对冗余的真正来源，在压缩时能更高保真度地保持注意力输出。

Result: 在LLaMA和Mistral模型上的广泛评估表明，KQ-SVD方法在投影质量方面始终优于现有方法。

Conclusion: KQ-SVD是一种简单且计算高效的方法，通过直接分解注意力矩阵来解决KV缓存压缩问题，相比现有方法能更准确地保持注意力机制的核心计算。

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [77] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: 该论文提出使用机器学习方法为美国年度商业调查(ABS)创建合成公共使用微观数据样本(PUMS)，以解决商业数据去匿名化风险，并通过与真实数据的计量经济学复制验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力和大数据可用性的增加，传统匿名化方法面临重新识别风险，可能违反调查受访者的保密承诺。商业数据尤其具有挑战性，因为企业缺乏匿名性且某些行业在特定地理区域容易被识别。

Method: 使用机器学习模型构建合成PUMS数据，该方法生成保留原始数据关键统计特征但不包含任何真实个体或企业记录的数据。论文以2007年企业主调查为例开发了两个合成PUMS版本。

Result: 通过将合成数据与真实数据在《小企业经济学》上发表的高影响力分析进行计量经济学复制，证明了合成数据与真实数据的相似性。虽然ABS PUMS仍在完善中且结果保密，但验证了方法的可行性。

Conclusion: 合成数据方法为解决商业调查数据的保密性问题提供了可行方案，能够平衡数据可用性与隐私保护，为ABS等商业调查数据的公共使用开辟了新的可能性。

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>


### [78] [Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950)
*Zalish Mahmud,Anantaa Kotal,Aritran Piplai*

Main category: cs.LG

TL;DR: Impugan使用条件生成对抗网络(cGAN)处理缺失数据和整合异构数据集，相比传统方法能更好地捕捉非线性多模态关系。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常存在缺失值，传统插补方法基于线性和独立性假设，难以处理复杂异构数据，导致有偏或过度平滑的估计。

Method: 提出Impugan模型，基于条件生成对抗网络(cGAN)。生成器根据观测特征重建缺失值，判别器区分真实与插补数据，通过对抗训练学习缺失变量与观测变量间的依赖关系。

Result: 在基准数据集和多源整合任务中，Impugan相比领先基线方法，地球移动距离(EMD)降低达82%，互信息偏差(MI)降低达70%。

Conclusion: 对抗训练的生成模型为处理不完整、异构数据的插补和整合提供了可扩展且有原则的方法。

Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025

</details>


### [79] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: MaxShapley是一种用于生成式搜索引擎的高效公平归因算法，基于可分解的最大和效用函数，将Shapley值计算复杂度从指数级降低到线性级，显著减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的生成式搜索引擎取代传统搜索，信息提供者的补偿方式发生根本变化。需要公平的机制来归因和补偿内容提供者，以维持生态系统。

Method: 提出MaxShapley算法，这是Shapley值的一个特例，利用可分解的最大和效用函数，在检索增强生成(RAG)管道中计算归因，计算复杂度从指数级降低到线性级。

Result: 在三个多跳QA数据集(HotPotQA、MuSiQUE、MS MARCO)上评估，MaxShapley达到与精确Shapley计算相当的归因质量，同时显著减少资源消耗（例如，在相同归因准确度下，比先前最先进方法减少8倍资源消耗）。

Conclusion: MaxShapley为生成式搜索引擎提供了一种高效公平的归因机制，能够以线性计算成本实现高质量的归因，有助于建立可持续的内容提供者补偿生态系统。

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [80] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: 该论文提出一种基于α-散度的新方法来解决RL微调LLM时导致的多样性损失问题，通过在Lean定理证明基准上实现覆盖率和精度的帕累托最优。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在微调LLM时会导致显著的多样性损失，这是因为RL隐式优化了"模式寻求"的Reverse KL散度，使模型集中在目标分布的高概率区域而忽略其他区域。

Method: 从显式目标分布出发（通过过滤错误答案同时保留正确答案的相对概率），使用α-散度族来近似这个目标分布，通过插值模式寻求和质量覆盖散度来直接控制精度-多样性权衡。

Result: 在Lean定理证明基准上，该方法在覆盖率-精度帕累托前沿上实现了最先进的性能，在覆盖率轴上优于所有先前方法。

Conclusion: α-散度方法能够有效解决RL微调LLM时的多样性损失问题，通过显式控制精度和多样性之间的权衡，在定理证明任务中取得了更好的覆盖性能。

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [81] [How to Tame Your LLM: Semantic Collapse in Continuous Systems](https://arxiv.org/abs/2512.05162)
*C. M. Wyss*

Main category: stat.ML

TL;DR: 该论文提出将大语言模型形式化为连续状态机，证明了语义表征定理：转移算子的主导特征函数诱导出有限个谱盆地，每个盆地都可在实数上的o-极小结构中定义，从而解释离散符号语义如何从连续计算中涌现。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的语义动态学，解释离散符号语义如何从连续的神经网络计算中涌现，建立连续计算与离散逻辑语义之间的理论联系。

Method: 将大语言模型形式化为连续状态机（CSMs）——具有概率转移算子的光滑动力系统，分析其转移算子P的谱性质，在紧致性、遍历性、有界雅可比等正则性假设下，证明语义表征定理。

Result: 证明了语义表征定理：转移算子P的主导特征函数诱导出有限个谱盆地，每个盆地都可在实数上的o-极小结构中定义，谱可聚合性与逻辑可驯性一致。该结果可扩展到随机和绝热（时间非齐次）设置。

Conclusion: 连续激活流形坍缩为有限、逻辑可解释的本体论，这解释了离散符号语义如何从连续计算中涌现。谱盆地结构在缓慢漂移的核下保持不变，为理解大语言模型的语义动态提供了理论基础。

Abstract: We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,μ) \to L^2(M,μ)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.

</details>


### [82] [One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow](https://arxiv.org/abs/2512.05251)
*Pascal Jutras-Dube,Jiaru Zhang,Ziran Wang,Ruqi Zhang*

Main category: stat.ML

TL;DR: 提出一步扩散采样器，通过状态空间一致性损失学习步长条件ODE，实现一步生成高质量样本，同时引入确定性流重要性权重和体积一致性正则化，在少步采样下保持稳定的ELBO估计。


<details>
  <summary>Details</summary>
Motivation: 现有采样算法通常需要大量迭代步骤才能生成高质量样本，导致计算成本高昂。此外，在少步采样情况下，标准ELBO估计会因离散积分器导致的前向/后向转移核不匹配而性能下降。

Method: 1) 学习步长条件ODE，通过状态空间一致性损失使一个大步复制多个小步的轨迹；2) 推导确定性流重要性权重，无需后向核即可进行ELBO估计；3) 引入体积一致性正则化，校准不同步长分辨率下的累积体积变化。

Result: 在具有挑战性的合成和贝叶斯基准测试中，该方法以数量级更少的网络评估实现了具有竞争力的样本质量，同时保持了稳健的ELBO估计。

Conclusion: 提出的一步扩散采样器能够在仅一步或少数步骤内同时实现高质量采样和稳定的证据估计，显著降低了计算成本。

Abstract: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.

</details>


### [83] [Symmetric Linear Dynamical Systems are Learnable from Few Observations](https://arxiv.org/abs/2512.05337)
*Minh Vu,Andrey Y. Lokhov,Marc Vuffray*

Main category: stat.ML

TL;DR: 提出一种基于矩方法的估计器，仅需T=O(log N)个观测即可高精度恢复对称动态矩阵，无论矩阵稀疏与否


<details>
  <summary>Details</summary>
Motivation: 解决在高维随机线性动力学中，从单条轨迹（无论是完全观测还是部分观测）学习参数的问题，特别是在结构发现等应用中需要精确恢复矩阵元素

Method: 基于矩方法的新估计器，不依赖问题特定的正则化，能够处理对称动态矩阵

Result: 该估计器仅需T=O(log N)个观测就能实现对称动态矩阵恢复的微小最大元素误差，无论矩阵是稀疏还是稠密

Conclusion: 该方法为高维动力学参数估计提供了高效解决方案，特别适用于结构发现等应用，且不依赖正则化假设

Abstract: We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.

</details>


### [84] [Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data](https://arxiv.org/abs/2512.05456)
*Stephen Salerno,Kentaro Hoffman,Awan Afiaz,Anna Neufeld,Tyler H. McCormick,Jeffrey T. Leek*

Main category: stat.ML

TL;DR: 使用预测数据做推断（IPD）存在统计挑战，高预测精度不能保证有效推断，问题可归结为偏差和方差问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML工具普及和数据收集困难（成本上升、调查响应率下降），研究者越来越多地使用预训练算法的预测来替代缺失或未观测数据，但标准推断工具可能导致错误关联。

Method: 分析使用预测数据进行推断（IPD）的统计挑战，将问题归结为偏差（预测系统性改变估计量或扭曲变量关系）和方差（忽略预测模型不确定性和真实数据内在变异性），回顾现有方法并连接经典统计理论。

Result: 高预测准确性不能保证有效的下游推断，所有IPD问题都可归结为统计偏差和方差问题，需要透明且统计原则的方法来使用预测数据。

Conclusion: 使用预测数据需要谨慎，必须考虑偏差和方差问题，未来需要开发透明且统计原则的方法，确保科学研究的有效性。

Abstract: As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.

</details>


### [85] [Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches](https://arxiv.org/abs/2512.05611)
*Aurélien Pion,Emmanuel Vazquez*

Main category: stat.ML

TL;DR: 本文研究高斯过程预测分布在插值设置下的校准问题，提出了两种方法：cps-gp和bcr-gp，用于改善预测分布的设计边际校准性能。


<details>
  <summary>Details</summary>
Motivation: 高斯过程预测分布在插值设置下可能存在校准不足的问题，特别是在设计边际视角下。需要开发能够保证有限样本边际校准的方法，同时保持预测分布的平滑性以适应顺序设计。

Method: 提出了两种方法：1) cps-gp：将保形预测系统适配到GP插值，使用标准化留一残差，产生具有有限样本边际校准的分步预测分布；2) bcr-gp：保留GP后验均值，用拟合到交叉验证标准化残差的广义正态模型替换高斯残差，通过贝叶斯选择规则控制离散度和尾部行为。

Result: 在基准函数上的数值实验比较了cps-gp、bcr-gp、Jackknife+ for GPs和full conformal Gaussian process。使用校准指标（覆盖率、Kolmogorov-Smirnov、积分绝对误差）和准确性/锐度指标（缩放连续排名概率得分）进行评估。

Conclusion: cps-gp和bcr-gp方法能够改善高斯过程预测分布的设计边际校准性能，其中bcr-gp特别适合需要平滑预测分布的顺序设计应用，而cps-gp提供有限样本边际校准保证。

Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure μ, we formalize μ-coverage for central intervals and μ-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.

</details>


### [86] [BalLOT: Balanced $k$-means clustering with optimal transport](https://arxiv.org/abs/2512.05926)
*Wenyan Luo,Dustin G. Mixon*

Main category: stat.ML

TL;DR: BalLOT：一种基于最优传输的交替最小化方法，用于解决平衡k-means聚类问题，具有快速有效、理论保证和初始化方案等特点。


<details>
  <summary>Details</summary>
Motivation: 解决平衡k-means聚类这一基本问题，传统方法在处理平衡约束时存在效率或效果上的不足。

Method: 提出BalLOT方法，将最优传输与交替最小化相结合，用于平衡k-means聚类。该方法包含初始化方案，能够实现一步恢复植入的聚类。

Result: 数值实验显示BalLOT快速有效；理论证明显示：1）对一般数据产生整数耦合；2）在随机球模型下对植入聚类有精确和部分恢复的理论保证。

Conclusion: BalLOT为平衡k-means聚类提供了一个快速有效的解决方案，具有坚实的理论保证和实用的初始化策略。

Abstract: We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.

</details>


### [87] [Consequences of Kernel Regularity for Bandit Optimization](https://arxiv.org/abs/2512.05957)
*Madison Lee,Tara Javidi*

Main category: stat.ML

TL;DR: 该论文研究了核函数正则性与RKHS函数bandit优化算法性能之间的关系，通过核的谱特性将全局核回归方法与局部平滑方法统一分析，推导了多种核族的显式遗憾界，并分析了结合两种方法的LP-GP-UCB算法。


<details>
  <summary>Details</summary>
Motivation: 传统RKHS方法依赖全局核回归器，而平滑方法利用局部近似，这两种视角之间的关系尚不明确。论文旨在通过核的谱特性揭示这两种方法的内在联系，建立统一的分析框架。

Method: 通过分析Matérn、平方指数、有理二次、γ-指数、分段多项式和Dirichlet等各向同性核的傅里叶谱特性，将谱衰减率与算法性能联系起来。对于核化bandit算法，谱衰减决定了最大信息增益的上界；对于平滑方法，相同的衰减率建立了Hölder空间嵌入和Besov空间范数等价性。还分析了结合全局高斯过程代理和局部多项式估计的LP-GP-UCB算法。

Result: 建立了核谱衰减率与渐近遗憾之间的直接关系，为每个核族推导了显式遗憾界，在多个情况下获得了新结果并改进了现有分析。LP-GP-UCB算法在多个核族上实现了阶最优性，虽然不总是优于专门方法。

Conclusion: 核的正则性（通过谱衰减表征）为分析RKHS函数bandit优化提供了统一框架，连接了全局核方法和局部平滑方法。这种统一视角使得能够推导出各种核族的显式遗憾界，并为混合算法提供了理论基础。

Abstract: In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Matérn, square-exponential, rational-quadratic, $γ$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish Hölder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.

</details>
