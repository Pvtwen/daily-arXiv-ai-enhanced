{"id": "2507.07239", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07239", "abs": "https://arxiv.org/abs/2507.07239", "authors": ["Jorge R. Colon-Berrios", "Jason M. Merlo", "Jeffrey A. Nanzer"], "title": "Three-Dimensional Millimeter-Wave Imaging Using Active Incoherent Fourier Processing and Pulse Compression", "comment": null, "summary": "We present a novel three-dimensional (3D) imaging approach that combines\ntwo-dimensional spatial Fourier-domain imaging techniques with traditional\nradar pulse compression to recover both cross-range and down-range scene\ninformation. The imaging system employs four transmitters, three of which emit\nspatially and temporally incoherent noise signals, while the fourth transmits a\nknown linear frequency modulated (LFM) pulsed signal. The spatial incoherence\nof the noise signals enables sampling of the 2D spatial Fourier spectrum of the\nscene from which two-dimensional cross-range (azimuth and elevation) images can\nbe formed via interferometric processing. Simultaneously, the LFM signal\nenables high-resolution downrange imaging through matched filtering. The\nreceived signals consist of a superposition of the noise sources and the known\npulse allowing for joint recovery of all three dimensions. We describe the\nsystem architecture and waveform design, and demonstrate the imaging technique\nusing both simulations with a linear array and experimental data from a 38 GHz\nactive incoherent millimeter-wave imaging system with 23-element randomized\narray. Results show the reconstruction of targets in three dimensions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e8c\u7ef4\u7a7a\u95f4\u5085\u91cc\u53f6\u57df\u6210\u50cf\u6280\u672f\u4e0e\u4f20\u7edf\u96f7\u8fbe\u8109\u51b2\u538b\u7f29\u7684\u4e09\u7ef4\u6210\u50cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u4fe1\u53f7\u548c\u7ebf\u6027\u8c03\u9891\u8109\u51b2\u4fe1\u53f7\u8054\u5408\u6062\u590d\u4e09\u7ef4\u573a\u666f\u4fe1\u606f\u3002", "motivation": "\u4f20\u7edf\u6210\u50cf\u65b9\u6cd5\u5728\u540c\u65f6\u83b7\u53d6\u9ad8\u5206\u8fa8\u7387\u8de8\u8303\u56f4\u548c\u4e0b\u8303\u56f4\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6280\u672f\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u7cfb\u7edf\u4f7f\u7528\u56db\u4e2a\u53d1\u5c04\u5668\uff0c\u5176\u4e2d\u4e09\u4e2a\u53d1\u5c04\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0d\u76f8\u5e72\u7684\u566a\u58f0\u4fe1\u53f7\uff0c\u7b2c\u56db\u4e2a\u53d1\u5c04\u5df2\u77e5\u7684\u7ebf\u6027\u8c03\u9891\u8109\u51b2\u4fe1\u53f7\u3002\u901a\u8fc7\u566a\u58f0\u4fe1\u53f7\u7684\u7a7a\u95f4\u4e0d\u76f8\u5e72\u6027\u91c7\u6837\u4e8c\u7ef4\u7a7a\u95f4\u5085\u91cc\u53f6\u8c31\uff0c\u7ed3\u5408LFM\u4fe1\u53f7\u7684\u9ad8\u5206\u8fa8\u7387\u4e0b\u8303\u56f4\u6210\u50cf\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u6570\u636e\uff0838 GHz\u6beb\u7c73\u6ce2\u6210\u50cf\u7cfb\u7edf\uff09\u9a8c\u8bc1\u4e86\u4e09\u7ef4\u76ee\u6807\u7684\u91cd\u5efa\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4e09\u7ef4\u573a\u666f\u7684\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u4e3a\u590d\u6742\u573a\u666f\u7684\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07285", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07285", "abs": "https://arxiv.org/abs/2507.07285", "authors": ["Kavian Zirak", "Mohammadreza F. Imani"], "title": "A RIS-Enabled Computational Radar Coincidence Imaging", "comment": null, "summary": "This paper introduces an innovative imaging method using reconfigurable\nintelligent surfaces (RISs) by combining radar coincidence imaging (RCI) and\ncomputational imaging techniques. In the proposed framework, RISs\nsimultaneously redirect beams toward a desired region of interest (ROI). The\ninterference of these beams forms spatially diverse speckle patterns that carry\ninformation about the entire ROI. As a result, this method can take advantage\nof the benefits of both random patterns and spotlight imaging. Since the\nspeckle pattern is formed by directive beams (instead of random patterns\ntypically used in computational imaging), this approach results in a higher\nsignal-to-noise ratio (SNR) and reduced clutter. In contrast to raster\nscanning, which requires the number of measurements to be at least equal to the\nnumber of unknowns, our proposed approach follows a computational imaging\nframework and can obtain high-quality images even when only a few measurements\nare taken. Using numerical simulation, we demonstrate this method's\ncapabilities and contrast it against other conventional techniques. The\nproposed imaging approach can be applied to security screening, wireless user\ntracking, and activity recognition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RIS\u3001RCI\u548c\u8ba1\u7b97\u6210\u50cf\u7684\u521b\u65b0\u6210\u50cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u5411\u5149\u675f\u5f62\u6210\u9ad8\u4fe1\u566a\u6bd4\u7684\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u6210\u50cf\u65b9\u6cd5\u5982\u5149\u6805\u626b\u63cf\u9700\u8981\u5927\u91cf\u6d4b\u91cf\uff0c\u800c\u8ba1\u7b97\u6210\u50cf\u7684\u968f\u673a\u6a21\u5f0f\u4fe1\u566a\u6bd4\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u6d4b\u91cf\u6b21\u6570\u53c8\u80fd\u63d0\u9ad8\u4fe1\u566a\u6bd4\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528RIS\u5b9a\u5411\u5149\u675f\u5f62\u6210\u7a7a\u95f4\u591a\u6837\u7684\u6563\u6591\u56fe\u6848\uff0c\u7ed3\u5408RCI\u548c\u8ba1\u7b97\u6210\u50cf\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u6210\u50cf\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u91cf\u6d4b\u91cf\u4e0b\u4ecd\u80fd\u83b7\u5f97\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u4e14\u4fe1\u566a\u6bd4\u9ad8\u3001\u6742\u6ce2\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5b89\u5168\u7b5b\u67e5\u3001\u65e0\u7ebf\u7528\u6237\u8ddf\u8e2a\u548c\u6d3b\u52a8\u8bc6\u522b\u7b49\u9886\u57df\u3002"}}
{"id": "2507.07331", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07331", "abs": "https://arxiv.org/abs/2507.07331", "authors": ["Anurag Pallaprolu", "Winston Hurst", "Yasamin Mostofi"], "title": "mmFlux: Crowd Flow Analytics with Commodity mmWave MIMO Radar", "comment": null, "summary": "In this paper, we present a novel framework for extracting underlying crowd\nmotion patterns and inferring crowd semantics using mmWave radar. First, our\nproposed signal processing pipeline combines optical flow estimation concepts\nfrom vision with novel statistical and morphological noise filtering to\ngenerate high-fidelity mmWave flow fields - compact 2D vector representations\nof crowd motion. We then introduce a novel approach that transforms these\nfields into directed geometric graphs, where edges capture dominant flow\ncurrents, vertices mark crowd splitting or merging, and flow distribution is\nquantified across edges. Finally, we show that by analyzing the local Jacobian\nand computing the corresponding curl and divergence, we can extract key crowd\nsemantics for both structured and diffused crowds. We conduct 21 experiments on\ncrowds of up to (and including) 20 people across 3 areas, using commodity\nmmWave radar. Our framework achieves high-fidelity graph reconstruction of the\nunderlying flow structure, even for complex crowd patterns, demonstrating\nstrong spatial alignment and precise quantitative characterization of flow\nsplit ratios. Finally, our curl and divergence analysis accurately infers key\ncrowd semantics, e.g., abrupt turns, boundaries where flow directions shift,\ndispersions, and gatherings. Overall, these findings validate our framework,\nunderscoring its potential for various crowd analytics applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u63d0\u53d6\u4eba\u7fa4\u8fd0\u52a8\u6a21\u5f0f\u5e76\u63a8\u65ad\u8bed\u4e49\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u548c\u51e0\u4f55\u56fe\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u7684\u4eba\u7fa4\u8fd0\u52a8\u8868\u5f81\u548c\u8bed\u4e49\u63d0\u53d6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4eba\u7fa4\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u8868\u5f81\u548c\u8bed\u4e49\u63d0\u53d6\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u5149\u5b66\u6d41\u4f30\u8ba1\u4e0e\u566a\u58f0\u8fc7\u6ee4\u751f\u6210\u6beb\u7c73\u6ce2\u6d41\u573a\uff0c\u5c06\u5176\u8f6c\u5316\u4e3a\u6709\u5411\u51e0\u4f55\u56fe\uff0c\u5e76\u901a\u8fc7\u96c5\u53ef\u6bd4\u77e9\u9635\u5206\u6790\u63d0\u53d6\u8bed\u4e49\u3002", "result": "\u572821\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u6210\u529f\u91cd\u5efa\u590d\u6742\u4eba\u7fa4\u8fd0\u52a8\u7ed3\u6784\uff0c\u5e76\u51c6\u786e\u63a8\u65ad\u51fa\u5173\u952e\u8bed\u4e49\uff08\u5982\u8f6c\u5411\u3001\u8fb9\u754c\u3001\u5206\u6563\u7b49\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4eba\u7fa4\u5206\u6790\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u7cbe\u786e\u6027\u3002"}}
{"id": "2507.07474", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07474", "abs": "https://arxiv.org/abs/2507.07474", "authors": ["Ruhui Zhang", "Wei Lin", "Binbin Chen"], "title": "Featureless Wireless Communications using Enhanced Autoencoder", "comment": null, "summary": "Artificial intelligence (AI) techniques, particularly autoencoders (AEs),\nhave gained significant attention in wireless communication systems. This paper\ninvestigates using an AE to generate featureless signals with a low probability\nof detection and interception (LPD/LPI). Firstly, we introduce a novel loss\nfunction that adds a KL divergence term to the categorical cross entropy,\nenhancing the noise like characteristics of AE-generated signals while\npreserving block error rate (BLER). Secondly, to support long source message\nblocks for the AE's inputs, we replace one-hot inputs of source blocks with\nbinary inputs pre-encoded by conventional error correction coding schemes. The\nAE's outputs are then decoded back to the source blocks using the same scheme.\nThis design enables the AE to learn the coding structure, yielding superior\nBLER performance on coded blocks and the BLER of the source blocks is further\ndecreased by the error correction decoder. Moreover, we also validate the AE\nbased communication system in the over-the-air communication. Experimental\nresults demonstrate that our proposed methods improve the featureless\nproperties of AE signals and significantly reduce the BLER of message blocks,\nunderscoring the promise of our AE-based approach for secure and reliable\nwireless communication systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\uff08AE\uff09\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u8f93\u5165\u7f16\u7801\u65b9\u5f0f\uff0c\u751f\u6210\u4f4e\u68c0\u6d4b/\u62e6\u622a\u6982\u7387\uff08LPD/LPI\uff09\u7684\u65e0\u7279\u5f81\u4fe1\u53f7\uff0c\u540c\u65f6\u964d\u4f4e\u5757\u9519\u8bef\u7387\uff08BLER\uff09\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528AI\u6280\u672f\uff08\u7279\u522b\u662fAE\uff09\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u751f\u6210\u96be\u4ee5\u68c0\u6d4b\u548c\u62e6\u622a\u7684\u4fe1\u53f7\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u4fe1\u7684\u53ef\u9760\u6027\u3002", "method": "1. \u5f15\u5165\u5305\u542bKL\u6563\u5ea6\u9879\u7684\u635f\u5931\u51fd\u6570\uff0c\u589e\u5f3a\u4fe1\u53f7\u7684\u566a\u58f0\u7279\u6027\uff1b2. \u7528\u4e8c\u8fdb\u5236\u8f93\u5165\u66ff\u4ee3\u72ec\u70ed\u7f16\u7801\uff0c\u7ed3\u5408\u4f20\u7edf\u7ea0\u9519\u7f16\u7801\u65b9\u6848\uff0c\u652f\u6301\u957f\u6e90\u6d88\u606f\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u53f7\u7684\u65e0\u7279\u5f81\u6027\uff0c\u5e76\u964d\u4f4e\u4e86BLER\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8eAE\u7684\u65b9\u6cd5\u4e3a\u5b89\u5168\u53ef\u9760\u7684\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07150", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.07150", "abs": "https://arxiv.org/abs/2507.07150", "authors": ["Jean-Baptiste Fermanian", "Mohamed Hebiri", "Joseph Salmon"], "title": "Class conditional conformal prediction for multiple inputs by p-value aggregation", "comment": null, "summary": "Conformal prediction methods are statistical tools designed to quantify\nuncertainty and generate predictive sets with guaranteed coverage\nprobabilities. This work introduces an innovative refinement to these methods\nfor classification tasks, specifically tailored for scenarios where multiple\nobservations (multi-inputs) of a single instance are available at prediction\ntime. Our approach is particularly motivated by applications in citizen\nscience, where multiple images of the same plant or animal are captured by\nindividuals. Our method integrates the information from each observation into\nconformal prediction, enabling a reduction in the size of the predicted label\nset while preserving the required class-conditional coverage guarantee. The\napproach is based on the aggregation of conformal p-values computed from each\nobservation of a multi-input. By exploiting the exact distribution of these\np-values, we propose a general aggregation framework using an abstract scoring\nfunction, encompassing many classical statistical tools. Knowledge of this\ndistribution also enables refined versions of standard strategies, such as\nmajority voting. We evaluate our method on simulated and real data, with a\nparticular focus on Pl@ntNet, a prominent citizen science platform that\nfacilitates the collection and identification of plant species through\nuser-submitted images.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u8f93\u5165\u5206\u7c7b\u4efb\u52a1\uff0c\u7279\u522b\u9002\u7528\u4e8e\u516c\u6c11\u79d1\u5b66\u4e2d\u591a\u56fe\u50cf\u573a\u666f\u3002\u901a\u8fc7\u6574\u5408\u591a\u89c2\u6d4b\u4fe1\u606f\uff0c\u51cf\u5c11\u9884\u6d4b\u6807\u7b7e\u96c6\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u8986\u76d6\u6982\u7387\u3002", "motivation": "\u516c\u6c11\u79d1\u5b66\u4e2d\u591a\u56fe\u50cf\u573a\u666f\uff08\u5982\u690d\u7269\u8bc6\u522b\uff09\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u5229\u7528\u591a\u89c2\u6d4b\u4fe1\u606f\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u57fa\u4e8e\u591a\u8f93\u5165\u5171\u5f62p\u503c\u7684\u805a\u5408\u6846\u67b6\uff0c\u5229\u7528\u5176\u7cbe\u786e\u5206\u5e03\u8bbe\u8ba1\u8bc4\u5206\u51fd\u6570\uff0c\u6db5\u76d6\u7ecf\u5178\u7edf\u8ba1\u5de5\u5177\uff0c\u5e76\u6539\u8fdb\u591a\u6570\u6295\u7968\u7b49\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\uff08\u5982Pl@ntNet\u5e73\u53f0\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u51cf\u5c11\u9884\u6d4b\u6807\u7b7e\u96c6\u5927\u5c0f\u5e76\u4fdd\u6301\u8986\u76d6\u6982\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u8f93\u5165\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u5171\u5f62\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u516c\u6c11\u79d1\u5b66\u5e94\u7528\u3002"}}
{"id": "2507.07559", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07559", "abs": "https://arxiv.org/abs/2507.07559", "authors": ["Amirhossein Sadough", "Mahyar Shahsavari", "Mark Wijtvliet", "Marcel van Gerven"], "title": "Real-Time Decorrelation-Based Anomaly Detection for Multivariate Time Series", "comment": null, "summary": "Anomaly detection (AD) plays a vital role across a wide range of real-world\ndomains by identifying data instances that deviate from expected patterns,\npotentially signaling critical events such as system failures, fraudulent\nactivities, or rare medical conditions. The demand for real-time AD has surged\nwith the rise of the (Industrial) Internet of Things, where massive volumes of\nmultivariate sensor data must be processed instantaneously. Real-time AD\nrequires methods that not only handle high-dimensional streaming data but also\noperate in a single-pass manner, without the burden of storing historical\ninstances, thereby ensuring minimal memory usage and fast decision-making. We\npropose DAD, a novel real-time decorrelation-based anomaly detection method for\nmultivariate time series, based on an online decorrelation learning approach.\nUnlike traditional proximity-based or reconstruction-based detectors that\nprocess entire data or windowed instances, DAD dynamically learns and monitors\nthe correlation structure of data sample by sample in a single pass, enabling\nefficient and effective detection. To support more realistic benchmarking\npractices, we also introduce a practical hyperparameter tuning strategy\ntailored for real-time anomaly detection scenarios. Extensive experiments on\nwidely used benchmark datasets demonstrate that DAD achieves the most\nconsistent and superior performance across diverse anomaly types compared to\nstate-of-the-art methods. Crucially, its robustness to increasing\ndimensionality makes it particularly well-suited for real-time,\nhigh-dimensional data streams. Ultimately, DAD not only strikes an optimal\nbalance between detection efficacy and computational efficiency but also sets a\nnew standard for real-time, memory-constrained anomaly detection.", "AI": {"tldr": "DAD\u662f\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u53bb\u76f8\u5173\u5b66\u4e60\u7684\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5185\u5b58\u53cb\u597d\u6027\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u7269\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u9700\u6c42\u6fc0\u589e\uff0c\u9700\u8981\u5904\u7406\u9ad8\u7ef4\u6d41\u6570\u636e\u4e14\u65e0\u9700\u5b58\u50a8\u5386\u53f2\u5b9e\u4f8b\u7684\u65b9\u6cd5\u3002", "method": "DAD\u901a\u8fc7\u5355\u6b21\u904d\u5386\u52a8\u6001\u5b66\u4e60\u6570\u636e\u7684\u76f8\u5173\u7ed3\u6784\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u57fa\u4e8e\u90bb\u8fd1\u6216\u91cd\u6784\u7684\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAD\u5728\u591a\u79cd\u5f02\u5e38\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u5408\u9ad8\u7ef4\u5b9e\u65f6\u6570\u636e\u6d41\u3002", "conclusion": "DAD\u5728\u68c0\u6d4b\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u5b9e\u65f6\u5185\u5b58\u53d7\u9650\u7684\u5f02\u5e38\u68c0\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.07129", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07129", "abs": "https://arxiv.org/abs/2507.07129", "authors": ["A. Bochkov"], "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate", "comment": null, "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u53ef\u8bad\u7ec3\u3001\u786e\u5b9a\u6027\u8f93\u5165\u5d4c\u5165\u7684\u6784\u9020\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u652f\u6301\u6a21\u5757\u5316\u7ec4\u5408\u548c\u9010\u5c42\u589e\u957f\u3002", "motivation": "\u5f53\u524dLLMs\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u5f0f\u8d44\u6e90\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u56fa\u5b9a\u5d4c\u5165\u4f5c\u4e3a\u901a\u7528\u63a5\u53e3\uff0c\u5b9e\u73b0\u4e13\u5bb6\u6a21\u578b\u7684\u6a21\u5757\u5316\u7ec4\u5408\uff08\u901a\u8fc7\u5e73\u5747\u8f93\u51falogits\uff09\u548c\u9010\u5c42\u589e\u957f\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u6a21\u5757\u5316\u7ec4\u5408\u540e\u7684\u6a21\u578b\u5728MMLU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u4e2a\u4e13\u5bb6\u6a21\u578b\uff1b\u9010\u5c42\u589e\u957f\u65b9\u6cd5\u7a33\u5b9a\u6536\u655b\u4e14\u6df1\u5ea6\u4e0e\u590d\u6742\u63a8\u7406\u80fd\u529b\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAI\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u751f\u7269\u5316\u6216\u6784\u9020\u6027\u7684\u8303\u5f0f\uff0c\u652f\u6301\u8d44\u6e90\u9ad8\u6548\u6269\u5c55\u548c\u6301\u7eed\u5b66\u4e60\u3002"}}
{"id": "2507.07567", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07567", "abs": "https://arxiv.org/abs/2507.07567", "authors": ["Reza Ghasemi Alavicheh", "Thomas Feys", "MD Arifur Rahman", "Fran\u00e7ois Rottenberg"], "title": "Leveraging Power Amplifier Distortion for Physical Layer Security", "comment": null, "summary": "This paper introduces a new approach to physical layer security (PLS) by\nleveraging power amplifier (PA) nonlinear distortion through distortion-aware\nprecoding. While some conventional PLS techniques inject artificial noise\northogonal to legitimate channels, we demonstrate that inherent PA\nnonlinearities typically considered undesirable can be exploited to enhance\nsecurity. The zero 3rd order (Z3RO) precoder applies a negative polarity to\nseveral antennas to cancel the PA distortion at the user location, resulting in\ndistortion being transmitted in non-user locations. Redirecting the distortion\nto non-user locations creates interference for potential eavesdroppers,\nlowering their signal-to-noise-and-distortion ratio (SNDR). Numerical\nsimulations reveal that the Z3RO precoder achieves up to a $2.5\\times$\nimprovement in secrecy rate compared to conventional maximum ratio transmission\n(MRT) precoding under a $10\\%$ outage probability, SNR of $32$ dB and $-5$ dB\ninput back-off (IBO) where the PAs enter the saturation regime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u5931\u771f\u7684\u7269\u7406\u5c42\u5b89\u5168\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7Z3RO\u9884\u7f16\u7801\u5668\u5c06\u5931\u771f\u5bfc\u5411\u975e\u7528\u6237\u4f4d\u7f6e\u4ee5\u589e\u5f3a\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u5c42\u5b89\u5168\u6280\u672f\u901a\u5e38\u6ce8\u5165\u4e0e\u5408\u6cd5\u4fe1\u9053\u6b63\u4ea4\u7684\u4eba\u5de5\u566a\u58f0\uff0c\u800c\u672c\u6587\u5229\u7528\u901a\u5e38\u88ab\u89c6\u4e3a\u4e0d\u5229\u7684PA\u975e\u7ebf\u6027\u5931\u771f\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528Z3RO\u9884\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8d1f\u6781\u6027\u5929\u7ebf\u62b5\u6d88\u7528\u6237\u4f4d\u7f6e\u7684PA\u5931\u771f\uff0c\u5c06\u5931\u771f\u5bfc\u5411\u975e\u7528\u6237\u4f4d\u7f6e\u4ee5\u5e72\u6270\u6f5c\u5728\u7a83\u542c\u8005\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0cZ3RO\u9884\u7f16\u7801\u5668\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6bd4\u4f20\u7edfMRT\u9884\u7f16\u7801\u5668\u63d0\u53472.5\u500d\u7684\u4fdd\u5bc6\u7387\u3002", "conclusion": "PA\u975e\u7ebf\u6027\u5931\u771f\u53ef\u88ab\u6709\u6548\u5229\u7528\u4ee5\u589e\u5f3a\u7269\u7406\u5c42\u5b89\u5168\u6027\u80fd\u3002"}}
{"id": "2507.07156", "categories": ["stat.ML", "cs.CG", "cs.LG", "math.AT", "55N31"], "pdf": "https://arxiv.org/pdf/2507.07156", "abs": "https://arxiv.org/abs/2507.07156", "authors": ["Nicole Abreu", "Parker B. Edwards", "Francis Motta"], "title": "Topological Machine Learning with Unreduced Persistence Diagrams", "comment": "10 figures, 2 tables, 8 pages(without appendix and references)", "summary": "Supervised machine learning pipelines trained on features derived from\npersistent homology have been experimentally observed to ignore much of the\ninformation contained in a persistence diagram. Computing persistence diagrams\nis often the most computationally demanding step in such a pipeline, however.\nTo explore this, we introduce several methods to generate topological feature\nvectors from unreduced boundary matrices. We compared the performance of\npipelines trained on vectorizations of unreduced PDs to vectorizations of\nfully-reduced PDs across several data and task types. Our results indicate that\nmodels trained on PDs built from unreduced diagrams can perform on par and even\noutperform those trained on fully-reduced diagrams on some tasks. This\nobservation suggests that machine learning pipelines which incorporate\ntopology-based features may benefit in terms of computational cost and\nperformance by utilizing information contained in unreduced boundary matrices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6301\u4e45\u540c\u8c03\u7279\u5f81\u63d0\u53d6\u4e2d\uff0c\u672a\u5b8c\u5168\u7b80\u5316\u7684\u8fb9\u754c\u77e9\u9635\u53ef\u80fd\u6bd4\u5b8c\u5168\u7b80\u5316\u7684\u6301\u4e45\u56fe\u63d0\u4f9b\u66f4\u9ad8\u6548\u4e14\u6027\u80fd\u76f8\u5f53\u7684\u673a\u5668\u5b66\u4e60\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u6301\u4e45\u540c\u8c03\u7279\u5f81\u63d0\u53d6\u4e2d\uff0c\u6301\u4e45\u56fe\u7684\u4fe1\u606f\u5e38\u88ab\u5ffd\u7565\uff0c\u800c\u8ba1\u7b97\u6301\u4e45\u56fe\u662f\u8ba1\u7b97\u6210\u672c\u6700\u9ad8\u7684\u6b65\u9aa4\u3002", "method": "\u5f15\u5165\u591a\u79cd\u65b9\u6cd5\u4ece\u672a\u7b80\u5316\u7684\u8fb9\u754c\u77e9\u9635\u751f\u6210\u62d3\u6251\u7279\u5f81\u5411\u91cf\uff0c\u5e76\u4e0e\u5b8c\u5168\u7b80\u5316\u7684\u6301\u4e45\u56fe\u7279\u5f81\u5411\u91cf\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u672a\u7b80\u5316\u6301\u4e45\u56fe\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0e\u5b8c\u5168\u7b80\u5316\u6301\u4e45\u56fe\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u5229\u7528\u672a\u7b80\u5316\u8fb9\u754c\u77e9\u9635\u7684\u4fe1\u606f\u53ef\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.07769", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07769", "abs": "https://arxiv.org/abs/2507.07769", "authors": ["Ruohong Liu", "Jack Umenberger", "Yize Chen"], "title": "BEAVER: Building Environments with Assessable Variation for Evaluating Multi-Objective Reinforcement Learning", "comment": "Accepted at the Workshop on Computational Optimization of Buildings\n  (ICML CO-BUILD), 42nd International Conference on Machine Learning (ICML\n  2025), Vancouver, Canada", "summary": "Recent years have seen significant advancements in designing reinforcement\nlearning (RL)-based agents for building energy management. While individual\nsuccess is observed in simulated or controlled environments, the scalability of\nRL approaches in terms of efficiency and generalization across building\ndynamics and operational scenarios remains an open question. In this work, we\nformally characterize the generalization space for the cross-environment,\nmulti-objective building energy management task, and formulate the\nmulti-objective contextual RL problem. Such a formulation helps understand the\nchallenges of transferring learned policies across varied operational contexts\nsuch as climate and heat convection dynamics under multiple control objectives\nsuch as comfort level and energy consumption. We provide a principled way to\nparameterize such contextual information in realistic building RL environments,\nand construct a novel benchmark to facilitate the evaluation of generalizable\nRL algorithms in practical building control tasks. Our results show that\nexisting multi-objective RL methods are capable of achieving reasonable\ntrade-offs between conflicting objectives. However, their performance degrades\nunder certain environment variations, underscoring the importance of\nincorporating dynamics-dependent contextual information into the policy\nlearning process.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u591a\u76ee\u6807\u4e0a\u4e0b\u6587RL\u95ee\u9898\uff0c\u5e76\u6784\u5efa\u4e86\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u90e8\u5206\u73af\u5883\u53d8\u5316\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u89e3\u51b3RL\u5728\u5efa\u7b51\u80fd\u6e90\u7ba1\u7406\u4e2d\u8de8\u73af\u5883\u548c\u591a\u76ee\u6807\u4efb\u52a1\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u5f62\u5f0f\u5316\u6cdb\u5316\u7a7a\u95f4\uff0c\u63d0\u51fa\u591a\u76ee\u6807\u4e0a\u4e0b\u6587RL\u95ee\u9898\uff0c\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u73b0\u6709\u591a\u76ee\u6807RL\u65b9\u6cd5\u80fd\u5e73\u8861\u51b2\u7a81\u76ee\u6807\uff0c\u4f46\u5728\u67d0\u4e9b\u73af\u5883\u53d8\u5316\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u9700\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u7eb3\u5165\u52a8\u6001\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.07135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07135", "abs": "https://arxiv.org/abs/2507.07135", "authors": ["Fran\u00e7ois Gard\u00e8res", "Shizhe Chen", "Camille-Sovanneary Gauthier", "Jean Ponce"], "title": "FACap: A Large-scale Fashion Dataset for Fine-grained Composed Image Retrieval", "comment": null, "summary": "The composed image retrieval (CIR) task is to retrieve target images given a\nreference image and a modification text. Recent methods for CIR leverage large\npretrained vision-language models (VLMs) and achieve good performance on\ngeneral-domain concepts like color and texture. However, they still struggle\nwith application domains like fashion, because the rich and diverse vocabulary\nused in fashion requires specific fine-grained vision and language\nunderstanding. An additional difficulty is the lack of large-scale fashion\ndatasets with detailed and relevant annotations, due to the expensive cost of\nmanual annotation by specialists. To address these challenges, we introduce\nFACap, a large-scale, automatically constructed fashion-domain CIR dataset. It\nleverages web-sourced fashion images and a two-stage annotation pipeline\npowered by a VLM and a large language model (LLM) to generate accurate and\ndetailed modification texts. Then, we propose a new CIR model FashionBLIP-2,\nwhich fine-tunes the general-domain BLIP-2 model on FACap with lightweight\nadapters and multi-head query-candidate matching to better account for\nfine-grained fashion-specific information. FashionBLIP-2 is evaluated with and\nwithout additional fine-tuning on the Fashion IQ benchmark and the enhanced\nevaluation dataset enhFashionIQ, leveraging our pipeline to obtain\nhigher-quality annotations. Experimental results show that the combination of\nFashionBLIP-2 and pretraining with FACap significantly improves the model's\nperformance in fashion CIR especially for retrieval with fine-grained\nmodification texts, demonstrating the value of our dataset and approach in a\nhighly demanding environment such as e-commerce websites. Code is available at\nhttps://fgxaos.github.io/facap-paper-website/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFACap\u6570\u636e\u96c6\u548cFashionBLIP-2\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u65f6\u5c1a\u9886\u57df\u56fe\u50cf\u68c0\u7d22\u4e2d\u7ec6\u7c92\u5ea6\u8bed\u8a00\u7406\u89e3\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u5c1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5229\u7528\u7f51\u7edc\u8d44\u6e90\u6784\u5efaFACap\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faFashionBLIP-2\u6a21\u578b\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u548c\u591a\u5934\u90e8\u67e5\u8be2\u5339\u914d\u3002", "result": "FashionBLIP-2\u5728FashionIQ\u548cenhFashionIQ\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u6587\u672c\u68c0\u7d22\u4e2d\u3002", "conclusion": "FACap\u6570\u636e\u96c6\u548cFashionBLIP-2\u6a21\u578b\u4e3a\u65f6\u5c1a\u9886\u57df\u56fe\u50cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07643", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07643", "abs": "https://arxiv.org/abs/2507.07643", "authors": ["Seonghoon Yoo", "Jaemin Jung", "Seongah Jeong", "Jinkyu Kang", "Markku Juntti", "Joonhyuk Kang"], "title": "RIS-assisted ISAC Systems for Industrial Revolution 6.0: Exploring the Near-field and Far-field Coexistence", "comment": null, "summary": "The Industrial Internet of Things (IIoT) has emerged as a key technology for\nrealizing the vision of Industry 6.0, requiring the seamless integration of\ndiverse connected devices. In particular, integrated sensing and communication\n(ISAC) plays a critical role in supporting real-time control and automation\nwithin IIoT systems. In this paper, we explore reconfigurable intelligent\nsurface (RIS)-assisted ISAC systems for IIoT in the coexistence of near-field\nand far-field regions. The system consists of a full-duplex access point (AP),\na RIS and multiple IIoT devices, where the near-field devices simultaneously\nperform sensing and communication, while the far-field devices rely on a\nRIS-assisted communication. To enhance spectral efficiency for both sensing and\ncommunication functionalities, we consider the use of both traditional\nsensing-only (SO) and ISAC frequency bands. Moreover, uplink non-orthogonal\nmultiple access (NOMA) is employed to facilitate the sequential decoding of\nsuperimposed communication and sensing signals from IIoT devices. To maximize\nsensing accuracy in terms of Cram${\\Grave{\\textrm{e}}}$r-Rao bound (CRB), we\nformulate a joint optimization of RIS phase shift, bandwidth splitting ratio\nand receive beamforming vector subject to the minimum data rate requirements of\nIIoT devices and resource budget constraints. The algorithmic solution is\ndeveloped via the successive convex approximation (SCA)-based alternating\noptimization (AO) method with the semi-definite relaxation (SDR) technique.\nNumerical results demonstrate that the proposed method significantly\noutperforms conventional methods relying solely on either ISAC or SO band by\nachieving superior performance across RIS and device configurations, while\nensuring robust ISAC performance under the near-field and far-field coexistence\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u7684\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u7528\u4e8e\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\uff0c\u4f18\u5316\u4e86\u9891\u8c31\u6548\u7387\u548c\u4f20\u611f\u7cbe\u5ea6\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u9700\u8981\u5b9e\u65f6\u63a7\u5236\u548c\u81ea\u52a8\u5316\uff0c\u800c\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u662f\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u6280\u672f\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd1\u573a\u548c\u8fdc\u573a\u533a\u57df\u5171\u5b58\u4e0b\u7684ISAC\u6027\u80fd\u4f18\u5316\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u5168\u53cc\u5de5\u63a5\u5165\u70b9\uff08AP\uff09\u3001RIS\u548c\u591a\u4e2aIIoT\u8bbe\u5907\uff0c\u7ed3\u5408\u4f20\u611f\u4e13\u7528\u9891\u6bb5\uff08SO\uff09\u548cISAC\u9891\u6bb5\uff0c\u91c7\u7528\u975e\u6b63\u4ea4\u591a\u5740\u63a5\u5165\uff08NOMA\uff09\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u548c\u534a\u5b9a\u677e\u5f1b\uff08SDR\uff09\u65b9\u6cd5\u4f18\u5316RIS\u76f8\u4f4d\u3001\u5e26\u5bbd\u5206\u914d\u548c\u6ce2\u675f\u6210\u5f62\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728RIS\u548c\u8bbe\u5907\u914d\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u786e\u4fdd\u8fd1\u573a\u548c\u8fdc\u573a\u5171\u5b58\u573a\u666f\u4e0b\u7684\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "RIS\u8f85\u52a9\u7684ISAC\u7cfb\u7edf\u5728IIoT\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u5347\u9891\u8c31\u6548\u7387\u548c\u4f20\u611f\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.07338", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.07338", "abs": "https://arxiv.org/abs/2507.07338", "authors": ["Nick Polson", "Vadim Sokolov"], "title": "Bayesian Double Descent", "comment": null, "summary": "Double descent is a phenomenon of over-parameterized statistical models. Our\ngoal is to view double descent from a Bayesian perspective. Over-parameterized\nmodels such as deep neural networks have an interesting re-descending property\nin their risk characteristics. This is a recent phenomenon in machine learning\nand has been the subject of many studies. As the complexity of the model\nincreases, there is a U-shaped region corresponding to the traditional\nbias-variance trade-off, but then as the number of parameters equals the number\nof observations and the model becomes one of interpolation, the risk can become\ninfinite and then, in the over-parameterized region, it re-descends -- the\ndouble descent effect. We show that this has a natural Bayesian interpretation.\nMoreover, we show that it is not in conflict with the traditional Occam's razor\nthat Bayesian models possess, in that they tend to prefer simpler models when\npossible. We illustrate the approach with an example of Bayesian model\nselection in neural networks. Finally, we conclude with directions for future\nresearch.", "AI": {"tldr": "\u8bba\u6587\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u63a2\u8ba8\u4e86\u8fc7\u53c2\u6570\u5316\u7edf\u8ba1\u6a21\u578b\u4e2d\u7684\u53cc\u4e0b\u964d\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u5176\u4e0e\u4f20\u7edf\u5965\u5361\u59c6\u5243\u5200\u539f\u5219\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u6a21\u578b\u9009\u62e9\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "motivation": "\u7814\u7a76\u53cc\u4e0b\u964d\u73b0\u8c61\u5728\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u7684\u89e3\u91ca\uff0c\u63a2\u8ba8\u5176\u4e0e\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u504f\u7f6e-\u65b9\u5dee\u6743\u8861\u7684\u5173\u7cfb\u3002", "method": "\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u5206\u6790\u53cc\u4e0b\u964d\u73b0\u8c61\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u8d1d\u53f6\u65af\u6a21\u578b\u9009\u62e9\u8fdb\u884c\u5b9e\u4f8b\u9a8c\u8bc1\u3002", "result": "\u53cc\u4e0b\u964d\u73b0\u8c61\u5728\u8d1d\u53f6\u65af\u6846\u67b6\u4e0b\u5177\u6709\u81ea\u7136\u89e3\u91ca\uff0c\u4e14\u4e0e\u4f20\u7edf\u5965\u5361\u59c6\u5243\u5200\u539f\u5219\u4e0d\u51b2\u7a81\u3002", "conclusion": "\u8d1d\u53f6\u65af\u65b9\u6cd5\u4e3a\u53cc\u4e0b\u964d\u73b0\u8c61\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u3002"}}
{"id": "2507.07792", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07792", "abs": "https://arxiv.org/abs/2507.07792", "authors": ["Hermann Klein", "Max Heinz Herkersdorf", "Oliver Nelles"], "title": "Space-Filling Regularization for Robust and Interpretable Nonlinear State Space Models", "comment": null, "summary": "The state space dynamics representation is the most general approach for\nnonlinear systems and often chosen for system identification. During training,\nthe state trajectory can deform significantly leading to poor data coverage of\nthe state space. This can cause significant issues for space-oriented training\nalgorithms which e.g. rely on grid structures, tree partitioning, or similar.\nBesides hindering training, significant state trajectory deformations also\ndeteriorate interpretability and robustness properties. This paper proposes a\nnew type of space-filling regularization that ensures a favorable data\ndistribution in state space via introducing a data-distribution-based penalty.\nThis method is demonstrated in local model network architectures where good\ninterpretability is a major concern. The proposed approach integrates ideas\nfrom modeling and design of experiments for state space structures. This is why\nwe present two regularization techniques for the data point distributions of\nthe state trajectories for local affine state space models. Beyond that, we\ndemonstrate the results on a widely known system identification benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u586b\u5145\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u5584\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u6570\u636e\u5206\u5e03\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u52a8\u6001\u8868\u793a\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u8bc6\u522b\u4e2d\u5e38\u7528\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u72b6\u6001\u8f68\u8ff9\u7684\u663e\u8457\u53d8\u5f62\u4f1a\u5bfc\u81f4\u6570\u636e\u8986\u76d6\u4e0d\u8db3\uff0c\u5f71\u54cd\u8bad\u7ec3\u7b97\u6cd5\u6548\u679c\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u5206\u5e03\u7684\u7a7a\u95f4\u586b\u5145\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6570\u636e\u5206\u5e03\u60e9\u7f5a\u9879\u4f18\u5316\u72b6\u6001\u7a7a\u95f4\u6570\u636e\u5206\u5e03\uff0c\u5e94\u7528\u4e8e\u5c40\u90e8\u6a21\u578b\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728\u5c40\u90e8\u4eff\u5c04\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e2d\u9a8c\u8bc1\u4e86\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\uff0c\u5e76\u5728\u7cfb\u7edf\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u72b6\u6001\u7a7a\u95f4\u6570\u636e\u5206\u5e03\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u679c\u548c\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.07137", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07137", "abs": "https://arxiv.org/abs/2507.07137", "authors": ["Eric Yeats", "Darryl Hannan", "Henry Kvinge", "Timothy Doster", "Scott Mahan"], "title": "Automating Evaluation of Diffusion Model Unlearning with (Vision-) Language Model World Knowledge", "comment": null, "summary": "Machine unlearning (MU) is a promising cost-effective method to cleanse\nundesired information (generated concepts, biases, or patterns) from\nfoundational diffusion models. While MU is orders of magnitude less costly than\nretraining a diffusion model without the undesired information, it can be\nchallenging and labor-intensive to prove that the information has been fully\nremoved from the model. Moreover, MU can damage diffusion model performance on\nsurrounding concepts that one would like to retain, making it unclear if the\ndiffusion model is still fit for deployment. We introduce autoeval-dmun, an\nautomated tool which leverages (vision-) language models to thoroughly assess\nunlearning in diffusion models. Given a target concept, autoeval-dmun extracts\nstructured, relevant world knowledge from the language model to identify nearby\nconcepts which are likely damaged by unlearning and to circumvent unlearning\nwith adversarial prompts. We use our automated tool to evaluate popular\ndiffusion model unlearning methods, revealing that language models (1) impose\nsemantic orderings of nearby concepts which correlate well with unlearning\ndamage and (2) effectively circumvent unlearning with synthetic adversarial\nprompts.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177autoeval-dmun\uff0c\u7528\u4e8e\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4fe1\u606f\u9057\u5fd8\u6548\u679c\uff0c\u5e76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u6392\u5e8f\u548c\u5bf9\u6297\u63d0\u793a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u9057\u5fd8\uff08MU\uff09\u662f\u4e00\u79cd\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6e05\u9664\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e0d\u826f\u4fe1\u606f\uff0c\u4f46\u8bc1\u660e\u4fe1\u606f\u5b8c\u5168\u6e05\u9664\u548c\u907f\u514d\u5bf9\u5468\u56f4\u6982\u5ff5\u7684\u635f\u5bb3\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faautoeval-dmun\u5de5\u5177\uff0c\u5229\u7528\uff08\u89c6\u89c9\uff09\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u8bc6\u522b\u53ef\u80fd\u53d7\u635f\u7684\u90bb\u8fd1\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u63d0\u793a\u7ed5\u8fc7\u9057\u5fd8\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u80fd\u6709\u6548\u6392\u5e8f\u90bb\u8fd1\u6982\u5ff5\uff08\u4e0e\u9057\u5fd8\u635f\u5bb3\u76f8\u5173\uff09\uff0c\u5e76\u80fd\u901a\u8fc7\u5408\u6210\u5bf9\u6297\u63d0\u793a\u7ed5\u8fc7\u9057\u5fd8\u3002", "conclusion": "autoeval-dmun\u4e3a\u8bc4\u4f30\u6269\u6563\u6a21\u578b\u9057\u5fd8\u6548\u679c\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u5bf9\u6297\u6027\u6d4b\u8bd5\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.07647", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.07647", "abs": "https://arxiv.org/abs/2507.07647", "authors": ["Shenghua Hu", "Guangyang Zeng", "Wenchao Xue", "Haitao Fang", "Biqiang Mu"], "title": "Consistent and Asymptotically Efficient Localization from Bearing-only Measurements", "comment": null, "summary": "We study the problem of signal source localization using bearing-only\nmeasurements. Initially, we present easily verifiable geometric conditions for\nsensor deployment to ensure the asymptotic identifiability of the model and\ndemonstrate the consistency and asymptotic efficiency of the maximum likelihood\n(ML) estimator. However, obtaining the ML estimator is challenging due to its\nassociation with a non-convex optimization problem. To address this, we propose\na two-step estimator that shares the same asymptotic properties as the ML\nestimator while offering low computational complexity, linear in the number of\nmeasurements. The primary challenge lies in obtaining a preliminary consistent\nestimator in the first step. To achieve this, we construct a linear\nleast-squares problem through algebraic operations on the measurement nonlinear\nmodel to first obtain a biased closed-form solution. We then eliminate the bias\nusing the data to yield an asymptotically unbiased and consistent estimator.\nThe key to this process is obtaining a consistent estimator of the variance of\nthe sine of the noise by taking the reciprocal of the maximum eigenvalue of a\nspecially constructed matrix from the data. In the second step, we perform a\nsingle Gauss-Newton iteration using the preliminary consistent estimator as the\ninitial value, achieving the same asymptotic properties as the ML estimator.\nFinally, simulation results demonstrate the superior performance of the\nproposed two-step estimator for large sample sizes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8e\u65b9\u4f4d\u6d4b\u91cf\u7684\u4fe1\u53f7\u6e90\u5b9a\u4f4d\u95ee\u9898\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u4e0e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u76f8\u540c\u7684\u6e10\u8fd1\u6027\u8d28\u3002", "motivation": "\u89e3\u51b3\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5728\u975e\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u578b\u7684\u53ef\u8bc6\u522b\u6027\u548c\u4f30\u8ba1\u5668\u7684\u6e10\u8fd1\u6548\u7387\u3002", "method": "\u901a\u8fc7\u4ee3\u6570\u64cd\u4f5c\u6784\u5efa\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\uff0c\u83b7\u5f97\u521d\u6b65\u4e00\u81f4\u4f30\u8ba1\u5668\uff0c\u518d\u901a\u8fc7\u9ad8\u65af-\u725b\u987f\u8fed\u4ee3\u5b9e\u73b0\u6e10\u8fd1\u6027\u8d28\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u6b65\u4f30\u8ba1\u5668\u5728\u5927\u6837\u672c\u91cf\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u4e24\u6b65\u4f30\u8ba1\u5668\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6e10\u8fd1\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.07461", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.07461", "abs": "https://arxiv.org/abs/2507.07461", "authors": ["Joshua Murphy", "Conor Rosato", "Andrew Millard", "Lee Devlin", "Paul Horridge", "Simon Maskell"], "title": "Hess-MC2: Sequential Monte Carlo Squared using Hessian Information and Second Order Proposals", "comment": "Accepted to IEEE Machine Learning Signal Processing conference 2025", "summary": "When performing Bayesian inference using Sequential Monte Carlo (SMC)\nmethods, two considerations arise: the accuracy of the posterior approximation\nand computational efficiency. To address computational demands, Sequential\nMonte Carlo Squared (SMC$^2$) is well-suited for high-performance computing\n(HPC) environments. The design of the proposal distribution within SMC$^2$ can\nimprove accuracy and exploration of the posterior as poor proposals may lead to\nhigh variance in importance weights and particle degeneracy. The\nMetropolis-Adjusted Langevin Algorithm (MALA) uses gradient information so that\nparticles preferentially explore regions of higher probability. In this paper,\nwe extend this idea by incorporating second-order information, specifically the\nHessian of the log-target. While second-order proposals have been explored\npreviously in particle Markov Chain Monte Carlo (p-MCMC) methods, we are the\nfirst to introduce them within the SMC$^2$ framework. Second-order proposals\nnot only use the gradient (first-order derivative), but also the curvature\n(second-order derivative) of the target distribution. Experimental results on\nsynthetic models highlight the benefits of our approach in terms of step-size\nselection and posterior approximation accuracy when compared to other\nproposals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728SMC$^2$\u6846\u67b6\u4e2d\u5f15\u5165\u4e8c\u9636\u4fe1\u606f\uff08Hessian\u77e9\u9635\uff09\u4ee5\u6539\u8fdb\u63d0\u6848\u5206\u5e03\uff0c\u63d0\u5347\u540e\u9a8c\u8fd1\u4f3c\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3SMC\u65b9\u6cd5\u4e2d\u540e\u9a8c\u8fd1\u4f3c\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u3002", "method": "\u6269\u5c55MALA\u65b9\u6cd5\uff0c\u5f15\u5165\u76ee\u6807\u51fd\u6570\u7684\u4e8c\u9636\u5bfc\u6570\uff08Hessian\u77e9\u9635\uff09\u4ee5\u6539\u8fdb\u63d0\u6848\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e8c\u9636\u63d0\u6848\u5728\u6b65\u957f\u9009\u62e9\u548c\u540e\u9a8c\u8fd1\u4f3c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u5728SMC$^2$\u4e2d\u5f15\u5165\u4e8c\u9636\u4fe1\u606f\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u540e\u9a8c\u8fd1\u4f3c\u8d28\u91cf\u3002"}}
{"id": "2507.07138", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07138", "abs": "https://arxiv.org/abs/2507.07138", "authors": ["Francesco Ferrini", "Veronica Lachi", "Antonio Longa", "Bruno Lepri", "Andrea Passerini"], "title": "GNNs Meet Sequence Models Along the Shortest-Path: an Expressive Method for Link Prediction", "comment": null, "summary": "Graph Neural Networks (GNNs) often struggle to capture the link-specific\nstructural patterns crucial for accurate link prediction, as their node-centric\nmessage-passing schemes overlook the subgraph structures connecting a pair of\nnodes. Existing methods to inject such structural context either incur high\ncomputational cost or rely on simplistic heuristics (e.g., common neighbor\ncounts) that fail to model multi-hop dependencies. We introduce SP4LP (Shortest\nPath for Link Prediction), a novel framework that combines GNN-based node\nencodings with sequence modeling over shortest paths. Specifically, SP4LP first\napplies a GNN to compute representations for all nodes, then extracts the\nshortest path between each candidate node pair and processes the resulting\nsequence of node embeddings using a sequence model. This design enables SP4LP\nto capture expressive multi-hop relational patterns with computational\nefficiency. Empirically, SP4LP achieves state-of-the-art performance across\nlink prediction benchmarks. Theoretically, we prove that SP4LP is strictly more\nexpressive than standard message-passing GNNs and several state-of-the-art\nstructural features methods, establishing it as a general and principled\napproach for link prediction in graphs.", "AI": {"tldr": "SP4LP\u662f\u4e00\u79cd\u7ed3\u5408GNN\u8282\u70b9\u7f16\u7801\u548c\u6700\u77ed\u8def\u5f84\u5e8f\u5217\u5efa\u6a21\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6355\u6349\u591a\u8df3\u4f9d\u8d56\u5173\u7cfb\uff0c\u63d0\u5347\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u94fe\u63a5\u7279\u5b9a\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "SP4LP\u5148\u901a\u8fc7GNN\u8ba1\u7b97\u8282\u70b9\u8868\u793a\uff0c\u518d\u63d0\u53d6\u5019\u9009\u8282\u70b9\u5bf9\u7684\u6700\u77ed\u8def\u5f84\uff0c\u7528\u5e8f\u5217\u6a21\u578b\u5904\u7406\u8282\u70b9\u5d4c\u5165\u5e8f\u5217\u3002", "result": "SP4LP\u5728\u94fe\u63a5\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u7406\u8bba\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "SP4LP\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709GNN\u548c\u7ed3\u6784\u7279\u5f81\u65b9\u6cd5\u3002"}}
{"id": "2507.07692", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07692", "abs": "https://arxiv.org/abs/2507.07692", "authors": ["Mohammad Ali Vahedifar", "Qi Zhang"], "title": "Signal Prediction for Loss Mitigation in Tactile Internet: A Leader-Follower Game-Theoretic Approach", "comment": "This work has been accepted for publication in the IEEE Machine\n  Learning and Signal Processing Conference (MLSP 2025)", "summary": "Tactile Internet (TI) requires achieving ultra-low latency and highly\nreliable packet delivery for haptic signals. In the presence of packet loss and\ndelay, the signal prediction method provides a viable solution for recovering\nthe missing signals. To this end, we introduce the Leader-Follower (LeFo)\napproach based on a cooperative Stackelberg game, which enables both users and\nrobots to learn and predict actions. With accurate prediction, the\nteleoperation system can safely relax its strict delay requirements. Our method\nachieves high prediction accuracy, ranging from 80.62% to 95.03% for remote\nrobot signals at the Human ($H$) side and from 70.44% to 89.77% for human\noperation signals at the remote Robot ($R$) side. We also establish an upper\nbound for maximum signal loss using Taylor Expansion, ensuring robustness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eStackelberg\u535a\u5f08\u7684Leader-Follower\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u89e6\u89c9\u4e92\u8054\u7f51\u4e2d\u7684\u4fe1\u53f7\u4e22\u5931\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u8981\u6c42\u3002", "motivation": "\u89e6\u89c9\u4e92\u8054\u7f51\u9700\u8981\u8d85\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\u7684\u4fe1\u53f7\u4f20\u8f93\uff0c\u4f46\u5728\u4e22\u5305\u548c\u5ef6\u8fdf\u60c5\u51b5\u4e0b\uff0c\u4fe1\u53f7\u9884\u6d4b\u6210\u4e3a\u5173\u952e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5408\u4f5cStackelberg\u535a\u5f08\u7684Leader-Follower\u65b9\u6cd5\uff0c\u4f7f\u7528\u6237\u548c\u673a\u5668\u4eba\u80fd\u5b66\u4e60\u548c\u9884\u6d4b\u52a8\u4f5c\u3002", "result": "\u9884\u6d4b\u51c6\u786e\u7387\u5728\u4eba\u7c7b\u7aef\u4e3a80.62%-95.03%\uff0c\u673a\u5668\u4eba\u7aef\u4e3a70.44%-89.77%\uff0c\u5e76\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u5ef6\u8fdf\u8981\u6c42\uff0c\u63d0\u9ad8\u4e86\u4fe1\u53f7\u4f20\u8f93\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07469", "categories": ["stat.ML", "cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2507.07469", "abs": "https://arxiv.org/abs/2507.07469", "authors": ["Haojie Liu", "Zihan Lin"], "title": "Galerkin-ARIMA: A Two-Stage Polynomial Regression Framework for Fast Rolling One-Step-Ahead Forecasting", "comment": null, "summary": "Time-series models like ARIMA remain widely used for forecasting but limited\nto linear assumptions and high computational cost in large and complex\ndatasets. We propose Galerkin-ARIMA that generalizes the AR component of ARIMA\nand replace it with a flexible spline-based function estimated by Galerkin\nprojection. This enables the model to capture nonlinear dependencies in lagged\nvalues and retain the MA component and Gaussian noise assumption. We derive a\nclosed-form OLS estimator for the Galerkin coefficients and show the model is\nasymptotically unbiased and consistent under standard conditions. Our method\nbridges classical time-series modeling and nonparametric regression, which\noffering improved forecasting performance and computational efficiency.", "AI": {"tldr": "Galerkin-ARIMA\u6269\u5c55\u4e86ARIMA\u7684AR\u90e8\u5206\uff0c\u4f7f\u7528\u57fa\u4e8e\u6837\u6761\u7684\u7075\u6d3b\u51fd\u6570\u66ff\u4ee3\uff0c\u4ee5\u6355\u6349\u975e\u7ebf\u6027\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u7559MA\u90e8\u5206\u548c\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edfARIMA\u6a21\u578b\u53d7\u9650\u4e8e\u7ebf\u6027\u5047\u8bbe\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7Galerkin\u6295\u5f71\u4f30\u8ba1\u6837\u6761\u57fa\u51fd\u6570\uff0c\u63a8\u5bfc\u95ed\u5f0fOLS\u4f30\u8ba1\u5668\uff0c\u4fdd\u7559MA\u90e8\u5206\u3002", "result": "\u6a21\u578b\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\u5177\u6709\u6e10\u8fd1\u65e0\u504f\u6027\u548c\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Galerkin-ARIMA\u7ed3\u5408\u4e86\u7ecf\u5178\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u548c\u975e\u53c2\u6570\u56de\u5f52\uff0c\u5177\u6709\u66f4\u597d\u7684\u9884\u6d4b\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2507.07140", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07140", "abs": "https://arxiv.org/abs/2507.07140", "authors": ["Samin Yeasar Arnob", "Zhan Su", "Minseon Kim", "Oleksiy Ostapenko", "Riyasat Ohib", "Esra'a Saleh", "Doina Precup", "Lucas Caccia", "Alessandro Sordoni"], "title": "Exploring Sparse Adapters for Scalable Merging of Parameter Efficient Experts", "comment": null, "summary": "Merging parameter-efficient task experts has recently gained growing\nattention as a way to build modular architectures that can be rapidly adapted\non the fly for specific downstream tasks, without requiring additional\nfine-tuning. Typically, LoRA serves as the foundational building block of such\nparameter-efficient modular architectures, leveraging low-rank weight\nstructures to reduce the number of trainable parameters. In this paper, we\nstudy the properties of sparse adapters, which train only a subset of weights\nin the base neural network, as potential building blocks of modular\narchitectures. First, we propose a simple method for training highly effective\nsparse adapters, which is conceptually simpler than existing methods in the\nliterature and surprisingly outperforms both LoRA and full fine-tuning in our\nsetting. Next, we investigate the merging properties of these sparse adapters\nby merging adapters for up to 20 natural language processing tasks, thus\nscaling beyond what is usually studied in the literature. Our findings\ndemonstrate that sparse adapters yield superior in-distribution performance\npost-merging compared to LoRA or full model merging. Achieving strong held-out\nperformance remains a challenge for all methods considered.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7a00\u758f\u9002\u914d\u5668\u4f5c\u4e3a\u6a21\u5757\u5316\u67b6\u6784\u7684\u6784\u5efa\u5757\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u5e76\u591a\u4efb\u52a1\u65f6\u8868\u73b0\u4f18\u4e8eLoRA\u548c\u5168\u5fae\u8c03\u3002", "motivation": "\u63a2\u7d22\u7a00\u758f\u9002\u914d\u5668\u4f5c\u4e3a\u53c2\u6570\u9ad8\u6548\u6a21\u5757\u5316\u67b6\u6784\u7684\u6f5c\u5728\u6784\u5efa\u5757\uff0c\u4ee5\u5feb\u901f\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\u800c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u8bad\u7ec3\u7a00\u758f\u9002\u914d\u5668\u7684\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5728\u591a\u4efb\u52a1\u5408\u5e76\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u7a00\u758f\u9002\u914d\u5668\u5728\u5408\u5e76\u540e\u8868\u73b0\u51fa\u4f18\u4e8eLoRA\u548c\u5168\u6a21\u578b\u5408\u5e76\u7684\u5206\u5e03\u5185\u6027\u80fd\uff0c\u4f46\u4fdd\u6301\u5f3a\u6cdb\u5316\u6027\u80fd\u4ecd\u5177\u6311\u6218\u3002", "conclusion": "\u7a00\u758f\u9002\u914d\u5668\u662f\u6a21\u5757\u5316\u67b6\u6784\u7684\u6709\u529b\u5019\u9009\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.07832", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07832", "abs": "https://arxiv.org/abs/2507.07832", "authors": ["Xinyi Lin", "Peizheng Li", "Adnan Aijaz"], "title": "Flying Base Stations for Offshore Wind Farm Monitoring and Control: Holistic Performance Evaluation and Optimization", "comment": "Accepted by PIMRC 2025", "summary": "Ensuring reliable and low-latency communication in offshore wind farms is\ncritical for efficient monitoring and control, yet remains challenging due to\nthe harsh environment and lack of infrastructure. This paper investigates a\nflying base station (FBS) approach for wide-area monitoring and control in the\nUK Hornsea offshore wind farm project. By leveraging mobile, flexible FBS\nplatforms in the remote and harsh offshore environment, the proposed system\noffers real-time connectivity for turbines without the need for deploying\npermanent infrastructure at the sea. We develop a detailed and practical\nend-to-end latency model accounting for five key factors: flight duration,\nconnection establishment, turbine state information upload, computational\ndelay, and control transmission, to provide a holistic perspective often\nmissing in prior studies. Furthermore, we combine trajectory planning,\nbeamforming, and resource allocation into a multi-objective optimization\nframework for the overall latency minimization, specifically designed for\nlarge-scale offshore wind farm deployments. Simulation results verify the\neffectiveness of our proposed method in minimizing latency and enhancing\nefficiency in FBS-assisted offshore monitoring across various power levels,\nwhile consistently outperforming baseline designs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98de\u884c\u57fa\u7ad9\uff08FBS\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u82f1\u56fdHornsea\u6d77\u4e0a\u98ce\u7535\u573a\u7684\u5e7f\u57df\u76d1\u63a7\uff0c\u901a\u8fc7\u4f18\u5316\u8f68\u8ff9\u89c4\u5212\u3001\u6ce2\u675f\u6210\u5f62\u548c\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u6d77\u4e0a\u98ce\u7535\u573a\u7531\u4e8e\u73af\u5883\u6076\u52a3\u4e14\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd\uff0c\u53ef\u9760\u4e14\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u5bf9\u5176\u76d1\u63a7\u548c\u63a7\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u5ef6\u8fdf\u6a21\u578b\uff0c\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u3001\u6ce2\u675f\u6210\u5f62\u548c\u8d44\u6e90\u5206\u914d\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u6700\u5c0f\u5316\u5ef6\u8fdf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u529f\u7387\u6c34\u5e73\u4e0b\u5747\u80fd\u6709\u6548\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5e76\u4f18\u4e8e\u57fa\u7ebf\u8bbe\u8ba1\u3002", "conclusion": "FBS\u65b9\u6cd5\u4e3a\u6d77\u4e0a\u98ce\u7535\u573a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6c38\u4e45\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u65f6\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76d1\u63a7\u6548\u7387\u3002"}}
{"id": "2507.07771", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07771", "abs": "https://arxiv.org/abs/2507.07771", "authors": ["Shuying Huang", "Junpeng Li", "Changchun Hua", "Yana Yang"], "title": "A Unified Empirical Risk Minimization Framework for Flexible N-Tuples Weak Supervision", "comment": null, "summary": "To alleviate the annotation burden in supervised learning, N-tuples learning\nhas recently emerged as a powerful weakly-supervised method. While existing\nN-tuples learning approaches extend pairwise learning to higher-order\ncomparisons and accommodate various real-world scenarios, they often rely on\ntask-specific designs and lack a unified theoretical foundation. In this paper,\nwe propose a general N-tuples learning framework based on empirical risk\nminimization, which systematically integrates pointwise unlabeled data to\nenhance learning performance. This paper first unifies the data generation\nprocesses of N-tuples and pointwise unlabeled data under a shared probabilistic\nformulation. Based on this unified view, we derive an unbiased empirical risk\nestimator that generalizes a broad class of existing N-tuples models. We\nfurther establish a generalization error bound for theoretical support. To\ndemonstrate the flexibility of the framework, we instantiate it in four\nrepresentative weakly supervised scenarios, each recoverable as a special case\nof our general model. Additionally, to address overfitting issues arising from\nnegative risk terms, we adopt correction functions to adjust the empirical\nrisk. Extensive experiments on benchmark datasets validate the effectiveness of\nthe proposed framework and demonstrate that leveraging pointwise unlabeled data\nconsistently improves generalization across various N-tuples learning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7684\u901a\u7528N\u5143\u7ec4\u5b66\u4e60\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u70b9\u72b6\u672a\u6807\u8bb0\u6570\u636e\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u65e0\u504f\u7ecf\u9a8c\u98ce\u9669\u4f30\u8ba1\u5668\uff0c\u540c\u65f6\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684N\u5143\u7ec4\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u652f\u6301\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6982\u7387\u516c\u5f0f\u7edf\u4e00\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u63d0\u51fa\u65e0\u504f\u7ecf\u9a8c\u98ce\u9669\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u6821\u6b63\u51fd\u6570\u89e3\u51b3\u8fc7\u62df\u5408\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\uff0c\u4e14\u5229\u7528\u70b9\u72b6\u672a\u6807\u8bb0\u6570\u636e\u80fd\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aN\u5143\u7ec4\u5b66\u4e60\u63d0\u4f9b\u4e86\u7edf\u4e00\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5728\u591a\u79cd\u5f31\u76d1\u7763\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.07141", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07141", "abs": "https://arxiv.org/abs/2507.07141", "authors": ["Dongxiao He", "Yongqi Huang", "Jitao Zhao", "Xiaobao Wang", "Zhen Wang"], "title": "Str-GCL: Structural Commonsense Driven Graph Contrastive Learning", "comment": "Accepted by WWW 2025", "summary": "Graph Contrastive Learning (GCL) is a widely adopted approach in\nself-supervised graph representation learning, applying contrastive objectives\nto produce effective representations. However, current GCL methods primarily\nfocus on capturing implicit semantic relationships, often overlooking the\nstructural commonsense embedded within the graph's structure and attributes,\nwhich contains underlying knowledge crucial for effective representation\nlearning. Due to the lack of explicit information and clear guidance in general\ngraph, identifying and integrating such structural commonsense in GCL poses a\nsignificant challenge. To address this gap, we propose a novel framework called\nStructural Commonsense Unveiling in Graph Contrastive Learning (Str-GCL).\nStr-GCL leverages first-order logic rules to represent structural commonsense\nand explicitly integrates them into the GCL framework. It introduces\ntopological and attribute-based rules without altering the original graph and\nemploys a representation alignment mechanism to guide the encoder in\neffectively capturing this commonsense. To the best of our knowledge, this is\nthe first attempt to directly incorporate structural commonsense into GCL.\nExtensive experiments demonstrate that Str-GCL outperforms existing GCL\nmethods, providing a new perspective on leveraging structural commonsense in\ngraph representation learning.", "AI": {"tldr": "Str-GCL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u9636\u903b\u8f91\u89c4\u5219\u663e\u5f0f\u6574\u5408\u7ed3\u6784\u5e38\u8bc6\uff0c\u63d0\u5347\u8868\u793a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524dGCL\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9690\u5f0f\u8bed\u4e49\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u56fe\u7ed3\u6784\u548c\u5c5e\u6027\u4e2d\u7684\u7ed3\u6784\u5e38\u8bc6\uff0c\u8fd9\u4e9b\u5e38\u8bc6\u5bf9\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002", "method": "Str-GCL\u5229\u7528\u4e00\u9636\u903b\u8f91\u89c4\u5219\u8868\u793a\u7ed3\u6784\u5e38\u8bc6\uff0c\u5e76\u901a\u8fc7\u8868\u793a\u5bf9\u9f50\u673a\u5236\u5c06\u5176\u6574\u5408\u5230GCL\u6846\u67b6\u4e2d\uff0c\u4e0d\u6539\u53d8\u539f\u59cb\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStr-GCL\u4f18\u4e8e\u73b0\u6709GCL\u65b9\u6cd5\u3002", "conclusion": "Str-GCL\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u5229\u7528\u7ed3\u6784\u5e38\u8bc6\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.07261", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.07261", "abs": "https://arxiv.org/abs/2507.07261", "authors": ["Chunzhuo Wang", "Hans Hallez", "Bart Vanrumste"], "title": "Robust Multimodal Learning Framework For Intake Gesture Detection Using Contactless Radar and Wearable IMU Sensors", "comment": "This manuscript has been submitted to a peer-reviewed journal and is\n  currently under review", "summary": "Automated food intake gesture detection plays a vital role in dietary\nmonitoring, enabling objective and continuous tracking of eating behaviors to\nsupport better health outcomes. Wrist-worn inertial measurement units (IMUs)\nhave been widely used for this task with promising results. More recently,\ncontactless radar sensors have also shown potential. This study explores\nwhether combining wearable and contactless sensing modalities through\nmultimodal learning can further improve detection performance. We also address\na major challenge in multimodal learning: reduced robustness when one modality\nis missing. To this end, we propose a robust multimodal temporal convolutional\nnetwork with cross-modal attention (MM-TCN-CMA), designed to integrate IMU and\nradar data, enhance gesture detection, and maintain performance under missing\nmodality conditions. A new dataset comprising 52 meal sessions (3,050 eating\ngestures and 797 drinking gestures) from 52 participants is developed and made\npublicly available. Experimental results show that the proposed framework\nimproves the segmental F1-score by 4.3% and 5.2% over unimodal Radar and IMU\nmodels, respectively. Under missing modality scenarios, the framework still\nachieves gains of 1.3% and 2.4% for missing radar and missing IMU inputs. This\nis the first study to demonstrate a robust multimodal learning framework that\neffectively fuses IMU and radar data for food intake gesture detection.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408IMU\u548c\u96f7\u8fbe\u6570\u636e\u7684\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff08MM-TCN-CMA\uff09\uff0c\u7528\u4e8e\u98df\u7269\u6444\u5165\u624b\u52bf\u68c0\u6d4b\uff0c\u5e76\u5728\u6a21\u6001\u7f3a\u5931\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u5347\u98df\u7269\u6444\u5165\u624b\u52bf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u89e3\u51b3\u6a21\u6001\u7f3a\u5931\u65f6\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faMM-TCN-CMA\u6846\u67b6\uff0c\u6574\u5408IMU\u548c\u96f7\u8fbe\u6570\u636e\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u5728\u591a\u6a21\u6001\u548c\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u68c0\u6d4b\u6027\u80fd\u5747\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u53474.3%\u548c5.2%\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u6210\u529f\u878d\u5408IMU\u548c\u96f7\u8fbe\u6570\u636e\uff0c\u4e3a\u98df\u7269\u6444\u5165\u624b\u52bf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07276", "categories": ["cs.LG", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07276", "abs": "https://arxiv.org/abs/2507.07276", "authors": ["Aaron Foote", "Danny Krizanc"], "title": "TRIP: A Nonparametric Test to Diagnose Biased Feature Importance Scores", "comment": "Accepted at the Workshop on Explainable Artificial Intelligence (XAI)\n  at IJCAI 2025", "summary": "Along with accurate prediction, understanding the contribution of each\nfeature to the making of the prediction, i.e., the importance of the feature,\nis a desirable and arguably necessary component of a machine learning model.\nFor a complex model such as a random forest, such importances are not innate --\nas they are, e.g., with linear regression. Efficient methods have been created\nto provide such capabilities, with one of the most popular among them being\npermutation feature importance due to its efficiency, model-agnostic nature,\nand perceived intuitiveness. However, permutation feature importance has been\nshown to be misleading in the presence of dependent features as a result of the\ncreation of unrealistic observations when permuting the dependent features. In\nthis work, we develop TRIP (Test for Reliable Interpretation via Permutation),\na test requiring minimal assumptions that is able to detect unreliable\npermutation feature importance scores that are the result of model\nextrapolation. To build on this, we demonstrate how the test can be\ncomplemented in order to allow its use in high dimensional settings. Through\ntesting on simulated data and applications, our results show that the test can\nbe used to reliably detect when permutation feature importance scores are\nunreliable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTRIP\u6d4b\u8bd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u56e0\u7279\u5f81\u4f9d\u8d56\u5bfc\u81f4\u7684\u4e0d\u53ef\u9760\u7684\u7f6e\u6362\u7279\u5f81\u91cd\u8981\u6027\u5206\u6570\u3002", "motivation": "\u7406\u89e3\u7279\u5f81\u5bf9\u9884\u6d4b\u7684\u8d21\u732e\u662f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u91cd\u8981\u9700\u6c42\uff0c\u4f46\u7f6e\u6362\u7279\u5f81\u91cd\u8981\u6027\u5728\u7279\u5f81\u4f9d\u8d56\u65f6\u53ef\u80fd\u8bef\u5bfc\u3002", "method": "\u5f00\u53d1TRIP\u6d4b\u8bd5\uff0c\u68c0\u6d4b\u4e0d\u53ef\u9760\u7684\u7f6e\u6362\u7279\u5f81\u91cd\u8981\u6027\u5206\u6570\uff0c\u5e76\u6269\u5c55\u81f3\u9ad8\u7ef4\u573a\u666f\u3002", "result": "\u6a21\u62df\u6570\u636e\u548c\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u4e86TRIP\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u3002", "conclusion": "TRIP\u6d4b\u8bd5\u80fd\u6709\u6548\u68c0\u6d4b\u7f6e\u6362\u7279\u5f81\u91cd\u8981\u6027\u7684\u4e0d\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07143", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07143", "abs": "https://arxiv.org/abs/2507.07143", "authors": ["Karthik Pappu", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Understanding Malware Propagation Dynamics through Scientific Machine Learning", "comment": "17 pages, 6 figures, 4 tables", "summary": "Accurately modeling malware propagation is essential for designing effective\ncybersecurity defenses, particularly against adaptive threats that evolve in\nreal time. While traditional epidemiological models and recent neural\napproaches offer useful foundations, they often fail to fully capture the\nnonlinear feedback mechanisms present in real-world networks. In this work, we\napply scientific machine learning to malware modeling by evaluating three\napproaches: classical Ordinary Differential Equations (ODEs), Universal\nDifferential Equations (UDEs), and Neural ODEs. Using data from the Code Red\nworm outbreak, we show that the UDE approach substantially reduces prediction\nerror compared to both traditional and neural baselines by 44%, while\npreserving interpretability. We introduce a symbolic recovery method that\ntransforms the learned neural feedback into explicit mathematical expressions,\nrevealing suppression mechanisms such as network saturation, security response,\nand malware variant evolution. Our results demonstrate that hybrid\nphysics-informed models can outperform both purely analytical and purely neural\napproaches, offering improved predictive accuracy and deeper insight into the\ndynamics of malware spread. These findings support the development of early\nwarning systems, efficient outbreak response strategies, and targeted cyber\ndefense interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u7684\u6df7\u5408\u65b9\u6cd5\uff08UDEs\uff09\uff0c\u7528\u4e8e\u5efa\u6a21\u6076\u610f\u8f6f\u4ef6\u4f20\u64ad\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u795e\u7ecf\u65b9\u6cd5\uff0c\u9884\u6d4b\u8bef\u5dee\u51cf\u5c1144%\uff0c\u5e76\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u6d41\u884c\u75c5\u5b66\u6a21\u578b\u548c\u795e\u7ecf\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u6355\u6349\u771f\u5b9e\u7f51\u7edc\u4e2d\u7684\u975e\u7ebf\u6027\u53cd\u9988\u673a\u5236\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u51c6\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u6f14\u53d8\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u7ecf\u5178ODE\u3001UDE\u548c\u795e\u7ecfODE\uff0c\u5e76\u4f7f\u7528Code Red\u8815\u866b\u7206\u53d1\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7b26\u53f7\u6062\u590d\u65b9\u6cd5\uff0c\u5c06\u795e\u7ecf\u53cd\u9988\u8f6c\u5316\u4e3a\u663e\u5f0f\u6570\u5b66\u8868\u8fbe\u5f0f\u3002", "result": "UDE\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\uff0844%\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u7f51\u7edc\u9971\u548c\u3001\u5b89\u5168\u54cd\u5e94\u548c\u6076\u610f\u8f6f\u4ef6\u53d8\u79cd\u6f14\u5316\u7b49\u6291\u5236\u673a\u5236\u3002", "conclusion": "\u6df7\u5408\u7269\u7406\u4fe1\u606f\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u52a8\u6001\u6d1e\u5bdf\u529b\u4e0a\u4f18\u4e8e\u7eaf\u5206\u6790\u548c\u7eaf\u795e\u7ecf\u65b9\u6cd5\uff0c\u652f\u6301\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u548c\u9488\u5bf9\u6027\u7f51\u7edc\u9632\u5fa1\u7b56\u7565\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.07359", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07359", "abs": "https://arxiv.org/abs/2507.07359", "authors": ["Zheyu Zhang", "Jiayuan Dong", "Jie Liu", "Xun Huan"], "title": "Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning", "comment": "10 pages, 6 figures", "summary": "We present GO-CBED, a goal-oriented Bayesian framework for sequential causal\nexperimental design. Unlike conventional approaches that select interventions\naimed at inferring the full causal model, GO-CBED directly maximizes the\nexpected information gain (EIG) on user-specified causal quantities of\ninterest, enabling more targeted and efficient experimentation. The framework\nis both non-myopic, optimizing over entire intervention sequences, and\ngoal-oriented, targeting only model aspects relevant to the causal query. To\naddress the intractability of exact EIG computation, we introduce a variational\nlower bound estimator, optimized jointly through a transformer-based policy\nnetwork and normalizing flow-based variational posteriors. The resulting policy\nenables real-time decision-making via an amortized network. We demonstrate that\nGO-CBED consistently outperforms existing baselines across various causal\nreasoning and discovery tasks-including synthetic structural causal models and\nsemi-synthetic gene regulatory networks-particularly in settings with limited\nexperimental budgets and complex causal mechanisms. Our results highlight the\nbenefits of aligning experimental design objectives with specific research\ngoals and of forward-looking sequential planning.", "AI": {"tldr": "GO-CBED\u662f\u4e00\u4e2a\u76ee\u6807\u5bfc\u5411\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5e8f\u5217\u56e0\u679c\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u76f4\u63a5\u6700\u5927\u5316\u7528\u6237\u6307\u5b9a\u56e0\u679c\u91cf\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u65e8\u5728\u63a8\u65ad\u5b8c\u6574\u56e0\u679c\u6a21\u578b\uff0c\u800cGO-CBED\u4e13\u6ce8\u4e8e\u7528\u6237\u611f\u5174\u8da3\u7684\u56e0\u679c\u91cf\uff0c\u63d0\u9ad8\u5b9e\u9a8c\u7684\u9488\u5bf9\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u53d8\u5206\u4e0b\u754c\u4f30\u8ba1\u5668\u548c\u57fa\u4e8eTransformer\u7684\u7b56\u7565\u7f51\u7edc\uff0c\u5b9e\u73b0\u5b9e\u65f6\u51b3\u7b56\uff0c\u4f18\u5316\u5e72\u9884\u5e8f\u5217\u3002", "result": "\u5728\u5404\u79cd\u56e0\u679c\u63a8\u7406\u548c\u53d1\u73b0\u4efb\u52a1\u4e2d\uff0cGO-CBED\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u5b9e\u9a8c\u9884\u7b97\u6709\u9650\u548c\u56e0\u679c\u673a\u5236\u590d\u6742\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "GO-CBED\u5c55\u793a\u4e86\u5c06\u5b9e\u9a8c\u8bbe\u8ba1\u76ee\u6807\u4e0e\u7279\u5b9a\u7814\u7a76\u76ee\u6807\u5bf9\u9f50\u4ee5\u53ca\u524d\u77bb\u6027\u5e8f\u5217\u89c4\u5212\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.07145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07145", "abs": "https://arxiv.org/abs/2507.07145", "authors": ["Zhaojing Zhou", "Xunchao Li", "Minghao Li", "Handi Zhang", "Haoshuang Wang", "Wenbin Chang", "Yiqun Liu", "Qingqing Dang", "Dianhai Yu", "Yanjun Ma", "Haifeng Wang"], "title": "CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs", "comment": "11 pages, 3 figures", "summary": "The rapid scaling of Large Language Models (LLMs) elevates inference costs\nand compounds substantial deployment barriers. While quantization to 8 or 4\nbits mitigates this, sub-3-bit methods face severe accuracy, scalability, and\nefficiency degradation. We propose Convolutional Code Quantization (CCQ), an\ninference-optimized quantization approach compressing LLMs to 2.0-2.75 bits\nwith minimal accuracy loss. Departing from error-prone scalar quantization or\nslow vector quantization, CCQ integrates a hardware-aware bit-shift encoding\nand decoding solution with Convolutional Code, Hybrid Encoding, and Code\nCluster, jointly overcoming accuracy-speed bottlenecks. We construct a\nlookup-free encoding space, enabling a linear mapping between the codebook and\nweight vectors, thereby optimizing inference performance. Meanwhile, by drawing\non the concept of data mapping from vector quantization, we minimize the\nperformance degradation of the model under extremely low-bit conditions.\nExperiments demonstrate that CCQ achieves outstanding performance on LLMs\nacross various benchmarks. We compress DeepSeek-V3 (671B total parameters) to\n184GB and ERNIE-4.5-300B-A47B to 89GB, enabling single-GPU deployment of ERNIE\n4.5 and eliminating inter-card communication. The 2-bit ERNIE-4.5-300B-A47B\nmodel and inference engine have been open-sourced.", "AI": {"tldr": "CCQ\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u76842.0-2.75\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5377\u79ef\u7801\u548c\u786c\u4ef6\u611f\u77e5\u7f16\u7801\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u968f\u7740LLM\u89c4\u6a21\u7684\u6269\u5927\uff0c\u63a8\u7406\u6210\u672c\u9ad8\u6602\u4e14\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u57283\u4f4d\u4ee5\u4e0b\u65f6\u7cbe\u5ea6\u548c\u6548\u7387\u4e0b\u964d\u4e25\u91cd\u3002", "method": "CCQ\u7ed3\u5408\u5377\u79ef\u7801\u3001\u6df7\u5408\u7f16\u7801\u548c\u7801\u7c07\u6280\u672f\uff0c\u91c7\u7528\u786c\u4ef6\u611f\u77e5\u7684\u4f4d\u79fb\u7f16\u7801\u65b9\u6848\uff0c\u6784\u5efa\u65e0\u67e5\u627e\u8868\u7684\u7f16\u7801\u7a7a\u95f4\uff0c\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCCQ\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u5c06DeepSeek-V3\u538b\u7f29\u81f3184GB\uff0cERNIE-4.5-300B-A47B\u538b\u7f29\u81f389GB\uff0c\u652f\u6301\u5355GPU\u90e8\u7f72\u3002", "conclusion": "CCQ\u5728\u6781\u4f4e\u4f4d\u91cf\u5316\u4e0b\u663e\u8457\u51cf\u5c11\u6a21\u578b\u6027\u80fd\u635f\u5931\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.07826", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07826", "abs": "https://arxiv.org/abs/2507.07826", "authors": ["Erfan Mirzaei", "Andreas Maurer", "Vladimir R. Kostic", "Massimiliano Pontil"], "title": "An Empirical Bernstein Inequality for Dependent Data in Hilbert Spaces and Applications", "comment": "In The 28th International Conference on Artificial Intelligence and\n  Statistics (2025)", "summary": "Learning from non-independent and non-identically distributed data poses a\npersistent challenge in statistical learning. In this study, we introduce\ndata-dependent Bernstein inequalities tailored for vector-valued processes in\nHilbert space. Our inequalities apply to both stationary and non-stationary\nprocesses and exploit the potential rapid decay of correlations between\ntemporally separated variables to improve estimation. We demonstrate the\nutility of these bounds by applying them to covariance operator estimation in\nthe Hilbert-Schmidt norm and to operator learning in dynamical systems,\nachieving novel risk bounds. Finally, we perform numerical experiments to\nillustrate the practical implications of these bounds in both contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5411\u91cf\u503c\u8fc7\u7a0b\u7684\u6570\u636e\u4f9d\u8d56Bernstein\u4e0d\u7b49\u5f0f\uff0c\u9002\u7528\u4e8e\u5e73\u7a33\u548c\u975e\u5e73\u7a33\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5feb\u901f\u8870\u51cf\u76f8\u5173\u6027\u6539\u8fdb\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u7edf\u8ba1\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u6570\u636e\u4f9d\u8d56Bernstein\u4e0d\u7b49\u5f0f\uff0c\u5e94\u7528\u4e8e\u534f\u65b9\u5dee\u7b97\u5b50\u4f30\u8ba1\u548c\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u7b97\u5b50\u5b66\u4e60\u3002", "result": "\u5728Hilbert-Schmidt\u8303\u6570\u4e0b\u83b7\u5f97\u65b0\u7684\u98ce\u9669\u754c\u9650\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u534f\u65b9\u5dee\u7b97\u5b50\u4f30\u8ba1\u548c\u52a8\u6001\u7cfb\u7edf\u5b66\u4e60\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.07146", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07146", "abs": "https://arxiv.org/abs/2507.07146", "authors": ["Zixuan Huang", "Kecheng Huang", "Lihao Yin", "Bowei He", "Huiling Zhen", "Mingxuan Yuan", "Zili Shao"], "title": "An attention-aware GNN-based input defender against multi-turn jailbreak on LLMs", "comment": null, "summary": "Large Language Models (LLMs) have gained widespread popularity and are\nincreasingly integrated into various applications. However, their capabilities\ncan be exploited for both benign and harmful purposes. Despite rigorous\ntraining and fine-tuning for safety, LLMs remain vulnerable to jailbreak\nattacks. Recently, multi-turn attacks have emerged, exacerbating the issue.\nUnlike single-turn attacks, multi-turn attacks gradually escalate the dialogue,\nmaking them more difficult to detect and mitigate, even after they are\nidentified.\n  In this study, we propose G-Guard, an innovative attention-aware GNN-based\ninput classifier designed to defend against multi-turn jailbreak attacks on\nLLMs. G-Guard constructs an entity graph for multi-turn queries, explicitly\ncapturing relationships between harmful keywords and queries even when those\nkeywords appear only in previous queries. Additionally, we introduce an\nattention-aware augmentation mechanism that retrieves the most similar\nsingle-turn query based on the multi-turn conversation. This retrieved query is\ntreated as a labeled node in the graph, enhancing the ability of GNN to\nclassify whether the current query is harmful. Evaluation results demonstrate\nthat G-Guard outperforms all baselines across all datasets and evaluation\nmetrics.", "AI": {"tldr": "G-Guard\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u611f\u77e5\u7684GNN\u8f93\u5165\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u9632\u5fa1LLMs\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u901a\u8fc7\u6784\u5efa\u5b9e\u4f53\u56fe\u548c\u6ce8\u610f\u529b\u589e\u5f3a\u673a\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "LLMs\u6613\u53d7\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u4f20\u7edf\u5355\u8f6e\u653b\u51fb\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u63d0\u51faG-Guard\uff0c\u6784\u5efa\u591a\u8f6e\u67e5\u8be2\u7684\u5b9e\u4f53\u56fe\uff0c\u6355\u6349\u6709\u5bb3\u5173\u952e\u8bcd\u4e0e\u67e5\u8be2\u7684\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u6ce8\u610f\u529b\u611f\u77e5\u589e\u5f3a\u673a\u5236\uff0c\u63d0\u5347\u5206\u7c7b\u80fd\u529b\u3002", "result": "G-Guard\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "G-Guard\u80fd\u6709\u6548\u9632\u5fa1\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff0c\u4e3aLLMs\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07852", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07852", "abs": "https://arxiv.org/abs/2507.07852", "authors": ["Haichen Hu", "David Simchi-Levi"], "title": "Pre-Trained AI Model Assisted Online Decision-Making under Missing Covariates: A Theoretical Perspective", "comment": null, "summary": "We study a sequential contextual decision-making problem in which certain\ncovariates are missing but can be imputed using a pre-trained AI model. From a\ntheoretical perspective, we analyze how the presence of such a model influences\nthe regret of the decision-making process. We introduce a novel notion called\n\"model elasticity\", which quantifies the sensitivity of the reward function to\nthe discrepancy between the true covariate and its imputed counterpart. This\nconcept provides a unified way to characterize the regret incurred due to model\nimputation, regardless of the underlying missingness mechanism. More\nsurprisingly, we show that under the missing at random (MAR) setting, it is\npossible to sequentially calibrate the pre-trained model using tools from\northogonal statistical learning and doubly robust regression. This calibration\nsignificantly improves the quality of the imputed covariates, leading to much\nbetter regret guarantees. Our analysis highlights the practical value of having\nan accurate pre-trained model in sequential decision-making tasks and suggests\nthat model elasticity may serve as a fundamental metric for understanding and\nimproving the integration of pre-trained models in a wide range of data-driven\ndecision-making problems.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u5728\u7f3a\u5931\u534f\u53d8\u91cf\u60c5\u51b5\u4e0b\u4f7f\u7528\u9884\u8bad\u7ec3AI\u6a21\u578b\u8fdb\u884c\u586b\u8865\u7684\u5e8f\u5217\u4e0a\u4e0b\u6587\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u6a21\u578b\u5f39\u6027\u201d\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u5176\u5bf9\u9057\u61be\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u793a\u4e86\u5728MAR\u8bbe\u7f6e\u4e0b\u901a\u8fc7\u6821\u51c6\u63d0\u5347\u586b\u8865\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u9884\u8bad\u7ec3AI\u6a21\u578b\u5728\u7f3a\u5931\u534f\u53d8\u91cf\u60c5\u51b5\u4e0b\u5bf9\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6a21\u578b\u5f39\u6027\u91cf\u5316\u586b\u8865\u8bef\u5dee\u5bf9\u5956\u52b1\u51fd\u6570\u7684\u654f\u611f\u6027\u3002", "method": "\u5f15\u5165\u201c\u6a21\u578b\u5f39\u6027\u201d\u6982\u5ff5\uff0c\u5229\u7528\u6b63\u4ea4\u7edf\u8ba1\u5b66\u4e60\u548c\u53cc\u91cd\u7a33\u5065\u56de\u5f52\u5de5\u5177\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6821\u51c6\u3002", "result": "\u5728MAR\u8bbe\u7f6e\u4e0b\uff0c\u6821\u51c6\u663e\u8457\u63d0\u5347\u4e86\u586b\u8865\u534f\u53d8\u91cf\u7684\u8d28\u91cf\uff0c\u6539\u5584\u4e86\u9057\u61be\u4fdd\u8bc1\u3002", "conclusion": "\u6a21\u578b\u5f39\u6027\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u4e2d\u96c6\u6210\u6548\u679c\u7684\u57fa\u672c\u6307\u6807\uff0c\u5177\u6709\u5e7f\u6cdb\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.07147", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07147", "abs": "https://arxiv.org/abs/2507.07147", "authors": ["Sua Lee", "Kyubum Shin", "Jung Ho Park"], "title": "Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation", "comment": "Published as a conference paper at ICLR 2025", "summary": "Recent advances in pre-trained Vision Language Models (VLM) have shown\npromising potential for effectively adapting to downstream tasks through prompt\nlearning, without the need for additional annotated paired datasets. To\nsupplement the text information in VLM trained on correlations with vision\ndata, new approaches leveraging Large Language Models (LLM) in prompts have\nbeen proposed, enhancing robustness to unseen and diverse data. Existing\nmethods typically extract text-based responses (i.e., descriptions) from LLM to\nincorporate into prompts; however, this approach suffers from high variability\nand low reliability. In this work, we propose Description-free Multi-prompt\nLearning(DeMul), a novel method that eliminates the process of extracting\ndescriptions and instead directly distills knowledge from LLM into prompts. By\nadopting a description-free approach, prompts can encapsulate richer semantics\nwhile still being represented as continuous vectors for optimization, thereby\neliminating the need for discrete pre-defined templates. Additionally, in a\nmulti-prompt setting, we empirically demonstrate the potential of prompt\nweighting in reflecting the importance of different prompts during training.\nExperimental results show that our approach achieves superior performance\nacross 11 recognition datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeMul\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u84b8\u998fLLM\u77e5\u8bc6\u5230\u63d0\u793a\u4e2d\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u63d0\u53d6\u63cf\u8ff0\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u4eceLLM\u63d0\u53d6\u6587\u672c\u63cf\u8ff0\u6765\u589e\u5f3a\u63d0\u793a\uff0c\u4f46\u5b58\u5728\u9ad8\u53d8\u5f02\u6027\u4e0e\u4f4e\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faDescription-free Multi-prompt Learning (DeMul)\uff0c\u76f4\u63a5\u84b8\u998fLLM\u77e5\u8bc6\u5230\u63d0\u793a\u4e2d\uff0c\u907f\u514d\u63d0\u53d6\u63cf\u8ff0\uff0c\u5e76\u91c7\u7528\u591a\u63d0\u793a\u52a0\u6743\u673a\u5236\u3002", "result": "\u572811\u4e2a\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DeMul\u65b9\u6cd5\u901a\u8fc7\u6d88\u9664\u63cf\u8ff0\u63d0\u53d6\u6b65\u9aa4\uff0c\u63d0\u5347\u4e86\u63d0\u793a\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u4f18\u5316\u6548\u7387\u3002"}}
{"id": "2507.07965", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07965", "abs": "https://arxiv.org/abs/2507.07965", "authors": ["Yuxin Bai", "Cecelia Shuai", "Ashwin De Silva", "Siyu Yu", "Pratik Chaudhari", "Joshua T. Vogelstein"], "title": "Prospective Learning in Retrospect", "comment": "Accepted to AGI 2025", "summary": "In most real-world applications of artificial intelligence, the distributions\nof the data and the goals of the learners tend to change over time. The\nProbably Approximately Correct (PAC) learning framework, which underpins most\nmachine learning algorithms, fails to account for dynamic data distributions\nand evolving objectives, often resulting in suboptimal performance. Prospective\nlearning is a recently introduced mathematical framework that overcomes some of\nthese limitations. We build on this framework to present preliminary results\nthat improve the algorithm and numerical results, and extend prospective\nlearning to sequential decision-making scenarios, specifically foraging. Code\nis available at: https://github.com/neurodata/prolearn2.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u524d\u77bb\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u52a8\u6001\u6570\u636e\u5206\u5e03\u548c\u53d8\u5316\u76ee\u6807\uff0c\u5e76\u6269\u5c55\u5230\u987a\u5e8f\u51b3\u7b56\u573a\u666f\uff08\u5982\u89c5\u98df\uff09\u3002", "motivation": "\u4f20\u7edfPAC\u5b66\u4e60\u6846\u67b6\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u6570\u636e\u5206\u5e03\u548c\u76ee\u6807\u53d8\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u524d\u77bb\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u6539\u8fdb\u7b97\u6cd5\u548c\u6570\u503c\u7ed3\u679c\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u987a\u5e8f\u51b3\u7b56\u573a\u666f\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u6539\u8fdb\u540e\u7684\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u524d\u77bb\u6027\u5b66\u4e60\u6846\u67b6\u5728\u52a8\u6001\u548c\u987a\u5e8f\u51b3\u7b56\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.07192", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07192", "abs": "https://arxiv.org/abs/2507.07192", "authors": ["Huibo Xu", "Runlong Yu", "Likang Wu", "Xianquan Wang", "Qi Liu"], "title": "Bridging the Last Mile of Prediction: Enhancing Time Series Forecasting with Conditional Guided Flow Matching", "comment": null, "summary": "Diffusion models, a type of generative model, have shown promise in time\nseries forecasting. But they face limitations like rigid source distributions\nand limited sampling paths, which hinder their performance. Flow matching\noffers faster generation, higher-quality outputs, and greater flexibility,\nwhile also possessing the ability to utilize valuable information from the\nprediction errors of prior models, which were previously inaccessible yet\ncritically important. To address these challenges and fully unlock the untapped\npotential of flow matching, we propose Conditional Guided Flow Matching (CGFM).\nCGFM extends flow matching by incorporating the outputs of an auxiliary model,\nenabling a previously unattainable capability in the field: learning from the\nerrors of the auxiliary model. For time series forecasting tasks, it integrates\nhistorical data as conditions and guidance, constructs two-sided conditional\nprobability paths, and uses a general affine path to expand the space of\nprobability paths, ultimately leading to improved predictions. Extensive\nexperiments show that CGFM consistently enhances and outperforms\nstate-of-the-art models, highlighting its effectiveness in advancing\nforecasting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCGFM\u7684\u6761\u4ef6\u5f15\u5bfc\u6d41\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u8f85\u52a9\u6a21\u578b\u7684\u9519\u8bef\u4fe1\u606f\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u521a\u6027\u6e90\u5206\u5e03\u548c\u6709\u9650\u91c7\u6837\u8def\u5f84\uff0c\u800c\u6d41\u5339\u914d\u867d\u5177\u4f18\u52bf\u4f46\u672a\u5145\u5206\u5229\u7528\u8f85\u52a9\u6a21\u578b\u7684\u9519\u8bef\u4fe1\u606f\u3002", "method": "CGFM\u6269\u5c55\u6d41\u5339\u914d\uff0c\u6574\u5408\u8f85\u52a9\u6a21\u578b\u8f93\u51fa\uff0c\u6784\u5efa\u53cc\u9762\u6761\u4ef6\u6982\u7387\u8def\u5f84\uff0c\u5e76\u4f7f\u7528\u4e00\u822c\u4eff\u5c04\u8def\u5f84\u6269\u5c55\u6982\u7387\u8def\u5f84\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCGFM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "CGFM\u901a\u8fc7\u5229\u7528\u8f85\u52a9\u6a21\u578b\u9519\u8bef\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.07969", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07969", "abs": "https://arxiv.org/abs/2507.07969", "authors": ["Qiyang Li", "Zhiyuan Zhou", "Sergey Levine"], "title": "Reinforcement Learning with Action Chunking", "comment": "25 pages, 15 figures", "summary": "We present Q-chunking, a simple yet effective recipe for improving\nreinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks.\nOur recipe is designed for the offline-to-online RL setting, where the goal is\nto leverage an offline prior dataset to maximize the sample-efficiency of\nonline learning. Effective exploration and sample-efficient learning remain\ncentral challenges in this setting, as it is not obvious how the offline data\nshould be utilized to acquire a good exploratory policy. Our key insight is\nthat action chunking, a technique popularized in imitation learning where\nsequences of future actions are predicted rather than a single action at each\ntimestep, can be applied to temporal difference (TD)-based RL methods to\nmitigate the exploration challenge. Q-chunking adopts action chunking by\ndirectly running RL in a 'chunked' action space, enabling the agent to (1)\nleverage temporally consistent behaviors from offline data for more effective\nonline exploration and (2) use unbiased $n$-step backups for more stable and\nefficient TD learning. Our experimental results demonstrate that Q-chunking\nexhibits strong offline performance and online sample efficiency, outperforming\nprior best offline-to-online methods on a range of long-horizon, sparse-reward\nmanipulation tasks.", "AI": {"tldr": "Q-chunking\u662f\u4e00\u79cd\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u957f\u65f6\u7a0b\u3001\u7a00\u758f\u5956\u52b1\u4efb\u52a1\uff0c\u901a\u8fc7\u52a8\u4f5c\u5206\u5757\u6280\u672f\u63d0\u5347\u79bb\u7ebf\u5230\u5728\u7ebf\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u63a2\u7d22\u548c\u6837\u672c\u6548\u7387\u7684\u6311\u6218\uff0c\u5229\u7528\u79bb\u7ebf\u6570\u636e\u63d0\u5347\u5728\u7ebf\u5b66\u4e60\u7684\u63a2\u7d22\u7b56\u7565\u3002", "method": "\u91c7\u7528\u52a8\u4f5c\u5206\u5757\u6280\u672f\uff0c\u5c06\u672a\u6765\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\u5e94\u7528\u4e8e\u57fa\u4e8e\u65f6\u95f4\u5dee\u5206\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u5206\u5757\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fd0\u884cRL\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQ-chunking\u5728\u79bb\u7ebf\u6027\u80fd\u548c\u5728\u7ebf\u6837\u672c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Q-chunking\u901a\u8fc7\u52a8\u4f5c\u5206\u5757\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2507.07197", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07197", "abs": "https://arxiv.org/abs/2507.07197", "authors": ["Elia Piccoli", "Malio Li", "Giacomo Carf\u00ec", "Vincenzo Lomonaco", "Davide Bacciu"], "title": "Combining Pre-Trained Models for Enhanced Feature Representation in Reinforcement Learning", "comment": "Published at 4th Conference on Lifelong Learning Agents (CoLLAs),\n  2025", "summary": "The recent focus and release of pre-trained models have been a key components\nto several advancements in many fields (e.g. Natural Language Processing and\nComputer Vision), as a matter of fact, pre-trained models learn disparate\nlatent embeddings sharing insightful representations. On the other hand,\nReinforcement Learning (RL) focuses on maximizing the cumulative reward\nobtained via agent's interaction with the environment. RL agents do not have\nany prior knowledge about the world, and they either learn from scratch an\nend-to-end mapping between the observation and action spaces or, in more recent\nworks, are paired with monolithic and computationally expensive Foundational\nModels. How to effectively combine and leverage the hidden information of\ndifferent pre-trained models simultaneously in RL is still an open and\nunderstudied question. In this work, we propose Weight Sharing Attention (WSA),\na new architecture to combine embeddings of multiple pre-trained models to\nshape an enriched state representation, balancing the tradeoff between\nefficiency and performance. We run an extensive comparison between several\ncombination modes showing that WSA obtains comparable performance on multiple\nAtari games compared to end-to-end models. Furthermore, we study the\ngeneralization capabilities of this approach and analyze how scaling the number\nof models influences agents' performance during and after training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeight Sharing Attention (WSA)\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u4fe1\u606f\uff0c\u4ee5\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f62\u6210\u66f4\u4e30\u5bcc\u7684\u72b6\u6001\u8868\u793a\uff0c\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u901a\u5e38\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\uff0c\u800c\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u63d0\u4f9b\u4e30\u5bcc\u7684\u6f5c\u5728\u8868\u793a\u3002\u5982\u4f55\u6709\u6548\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u4fe1\u606f\u4ee5\u63d0\u5347RL\u6027\u80fd\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u7814\u7a76\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faWSA\u67b6\u6784\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u4fe1\u606f\uff0c\u5f62\u6210\u5f3a\u5316\u5b66\u4e60\u7684\u72b6\u6001\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2aAtari\u6e38\u620f\u4e2d\uff0cWSA\u7684\u6027\u80fd\u4e0e\u7aef\u5230\u7aef\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u7814\u7a76\u4e86\u6a21\u578b\u6570\u91cf\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "WSA\u4e3a\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4fe1\u606f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728RL\u4e2d\u7684\u6f5c\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.07207", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.07207", "abs": "https://arxiv.org/abs/2507.07207", "authors": ["Florian Redhardt", "Yassir Akram", "Simon Schug"], "title": "Scale leads to compositional generalization", "comment": "Code available at https://github.com/smonsays/scale-compositionality", "summary": "Can neural networks systematically capture discrete, compositional task\nstructure despite their continuous, distributed nature? The impressive\ncapabilities of large-scale neural networks suggest that the answer to this\nquestion is yes. However, even for the most capable models, there are still\nfrequent failure cases that raise doubts about their compositionality. Here, we\nseek to understand what it takes for a standard neural network to generalize\nover tasks that share compositional structure. We find that simply scaling data\nand model size leads to compositional generalization. We show that this holds\nacross different task encodings as long as the training distribution\nsufficiently covers the task space. In line with this finding, we prove that\nstandard multilayer perceptrons can approximate a general class of\ncompositional task families to arbitrary precision using only a linear number\nof neurons with respect to the number of task modules. Finally, we uncover that\nif networks successfully compositionally generalize, the constituents of a task\ncan be linearly decoded from their hidden activations. We show that this metric\ncorrelates with failures of text-to-image generation models to compose known\nconcepts.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6269\u5927\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\uff0c\u4e14\u5176\u9690\u85cf\u6fc0\u6d3b\u80fd\u7ebf\u6027\u89e3\u7801\u4efb\u52a1\u6210\u5206\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u80fd\u7cfb\u7edf\u6027\u6355\u83b7\u79bb\u6563\u3001\u7ec4\u5408\u4efb\u52a1\u7ed3\u6784\uff0c\u5c3d\u7ba1\u5176\u672c\u8d28\u662f\u8fde\u7eed\u548c\u5206\u5e03\u5f0f\u7684\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u7814\u7a76\u4e0d\u540c\u4efb\u52a1\u7f16\u7801\u4e0b\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8bc1\u660e\u591a\u5c42\u611f\u77e5\u673a\u7684\u8fd1\u4f3c\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u53ef\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\uff0c\u4e14\u4efb\u52a1\u6210\u5206\u53ef\u4ece\u9690\u85cf\u6fc0\u6d3b\u4e2d\u7ebf\u6027\u89e3\u7801\u3002", "conclusion": "\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u5728\u8db3\u591f\u8986\u76d6\u4efb\u52a1\u7a7a\u95f4\u7684\u8bad\u7ec3\u5206\u5e03\u4e0b\uff0c\u80fd\u5b9e\u73b0\u7ec4\u5408\u6cdb\u5316\uff0c\u4e14\u5176\u9690\u85cf\u6fc0\u6d3b\u53ef\u89e3\u7801\u4efb\u52a1\u6210\u5206\u3002"}}
{"id": "2507.07216", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07216", "abs": "https://arxiv.org/abs/2507.07216", "authors": ["Yunyi Li", "Maria De-Arteaga", "Maytal Saar-Tsechansky"], "title": "Bias-Aware Mislabeling Detection via Decoupled Confident Learning", "comment": null, "summary": "Reliable data is a cornerstone of modern organizational systems. A notable\ndata integrity challenge stems from label bias, which refers to systematic\nerrors in a label, a covariate that is central to a quantitative analysis, such\nthat its quality differs across social groups. This type of bias has been\nconceptually and empirically explored and is widely recognized as a pressing\nissue across critical domains. However, effective methodologies for addressing\nit remain scarce. In this work, we propose Decoupled Confident Learning\n(DeCoLe), a principled machine learning based framework specifically designed\nto detect mislabeled instances in datasets affected by label bias, enabling\nbias aware mislabelling detection and facilitating data quality improvement. We\ntheoretically justify the effectiveness of DeCoLe and evaluate its performance\nin the impactful context of hate speech detection, a domain where label bias is\na well documented challenge. Empirical results demonstrate that DeCoLe excels\nat bias aware mislabeling detection, consistently outperforming alternative\napproaches for label error detection. Our work identifies and addresses the\nchallenge of bias aware mislabeling detection and offers guidance on how DeCoLe\ncan be integrated into organizational data management practices as a powerful\ntool to enhance data reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeCoLe\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u6807\u7b7e\u504f\u89c1\u5bfc\u81f4\u7684\u6570\u636e\u9519\u8bef\u6807\u8bb0\uff0c\u5e76\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6807\u7b7e\u504f\u89c1\u662f\u6570\u636e\u5b8c\u6574\u6027\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faDecoupled Confident Learning (DeCoLe)\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u68c0\u6d4b\u53d7\u6807\u7b7e\u504f\u89c1\u5f71\u54cd\u7684\u9519\u8bef\u6807\u8bb0\u6570\u636e\u3002", "result": "DeCoLe\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u5176\u4ed6\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "DeCoLe\u4e3a\u504f\u89c1\u611f\u77e5\u7684\u9519\u8bef\u6807\u8bb0\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u53ef\u63d0\u5347\u6570\u636e\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07222", "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.07222", "abs": "https://arxiv.org/abs/2507.07222", "authors": ["Minchan Jeong", "J. Jon Ryu", "Se-Young Yun", "Gregory W. Wornell"], "title": "Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems", "comment": "28 pages, 4 figures. Under review for NeurIPS 2025. The first two\n  authors contributed equally", "summary": "The Koopman operator provides a principled framework for analyzing nonlinear\ndynamical systems through linear operator theory. Recent advances in dynamic\nmode decomposition (DMD) have shown that trajectory data can be used to\nidentify dominant modes of a system in a data-driven manner. Building on this\nidea, deep learning methods such as VAMPnet and DPNet have been proposed to\nlearn the leading singular subspaces of the Koopman operator. However, these\nmethods require backpropagation through potentially numerically unstable\noperations on empirical second moment matrices, such as singular value\ndecomposition and matrix inversion, during objective computation, which can\nintroduce biased gradient estimates and hinder scalability to large systems. In\nthis work, we propose a scalable and conceptually simple method for learning\nthe top-k singular functions of the Koopman operator for stochastic dynamical\nsystems based on the idea of low-rank approximation. Our approach eliminates\nthe need for unstable linear algebraic operations and integrates easily into\nmodern deep learning pipelines. Empirical results demonstrate that the learned\nsingular subspaces are both reliable and effective for downstream tasks such as\neigen-analysis and multi-step prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u8fd1\u4f3c\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60Koopman\u7b97\u5b50\u7684\u524dk\u4e2a\u5947\u5f02\u51fd\u6570\uff0c\u907f\u514d\u4e86\u4e0d\u7a33\u5b9a\u7684\u7ebf\u6027\u4ee3\u6570\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982VAMPnet\u548cDPNet\uff09\u5728\u8ba1\u7b97\u76ee\u6807\u65f6\u9700\u8981\u53cd\u5411\u4f20\u64ad\u901a\u8fc7\u6570\u503c\u4e0d\u7a33\u5b9a\u7684\u64cd\u4f5c\uff08\u5982\u5947\u5f02\u503c\u5206\u89e3\u548c\u77e9\u9635\u6c42\u9006\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u5927\u578b\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u4f4e\u79e9\u8fd1\u4f3c\u7684\u601d\u60f3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5b66\u4e60Koopman\u7b97\u5b50\u7684\u524dk\u4e2a\u5947\u5f02\u51fd\u6570\uff0c\u65e0\u9700\u4e0d\u7a33\u5b9a\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b66\u4e60\u7684\u5947\u5f02\u5b50\u7a7a\u95f4\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u7279\u5f81\u5206\u6790\u548c\u591a\u6b65\u9884\u6d4b\uff09\u4e2d\u65e2\u53ef\u9760\u53c8\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\uff0c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u4e2d\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u968f\u673a\u52a8\u529b\u7cfb\u7edf\u3002"}}
{"id": "2507.07236", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07236", "abs": "https://arxiv.org/abs/2507.07236", "authors": ["Maya Kruse", "Majid Afshar", "Saksham Khatwani", "Anoop Mayampurath", "Guanhua Chen", "Yanjun Gao"], "title": "An Information-Theoretic Perspective on Multi-LLM Uncertainty Estimation", "comment": "Under review", "summary": "Large language models (LLMs) often behave inconsistently across inputs,\nindicating uncertainty and motivating the need for its quantification in\nhigh-stakes settings. Prior work on calibration and uncertainty quantification\noften focuses on individual models, overlooking the potential of model\ndiversity. We hypothesize that LLMs make complementary predictions due to\ndifferences in training and the Zipfian nature of language, and that\naggregating their outputs leads to more reliable uncertainty estimates. To\nleverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a\nsimple information-theoretic method that uses Jensen-Shannon Divergence to\nidentify and aggregate well-calibrated subsets of LLMs. Experiments on binary\nprediction tasks demonstrate improved calibration and predictive performance\ncompared to single-model and naive ensemble baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMUSE\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2aLLM\u7684\u5b50\u96c6\u6765\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u7b80\u5355\u96c6\u6210\u65b9\u6cd5\u3002", "motivation": "LLM\u5728\u4e0d\u540c\u8f93\u5165\u4e0b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u9700\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e00\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u591a\u6837\u6027\u6f5c\u529b\u3002", "method": "\u63d0\u51faMUSE\u65b9\u6cd5\uff0c\u5229\u7528Jensen-Shannon\u6563\u5ea6\u8bc6\u522b\u5e76\u96c6\u6210\u6821\u51c6\u826f\u597d\u7684LLM\u5b50\u96c6\u3002", "result": "\u5728\u4e8c\u5143\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMUSE\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6821\u51c6\u6027\u548c\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u591a\u6837\u6027\u53ef\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0cMUSE\u65b9\u6cd5\u7b80\u5355\u6709\u6548\u3002"}}
{"id": "2507.07237", "categories": ["cs.LG", "physics.data-an", "74R10, 74B20, 74A40, 68T07", "J.2; I.6.3; I.6.5"], "pdf": "https://arxiv.org/pdf/2507.07237", "abs": "https://arxiv.org/abs/2507.07237", "authors": ["Erfan Hamdi", "Emma Lejeune"], "title": "Towards Robust Surrogate Models: Benchmarking Machine Learning Approaches to Expediting Phase Field Simulations of Brittle Fracture", "comment": "29 pages, 13 figures", "summary": "Data driven approaches have the potential to make modeling complex, nonlinear\nphysical phenomena significantly more computationally tractable. For example,\ncomputational modeling of fracture is a core challenge where machine learning\ntechniques have the potential to provide a much needed speedup that would\nenable progress in areas such as mutli-scale modeling and uncertainty\nquantification. Currently, phase field modeling (PFM) of fracture is one such\napproach that offers a convenient variational formulation to model crack\nnucleation, branching and propagation. To date, machine learning techniques\nhave shown promise in approximating PFM simulations. However, most studies rely\non overly simple benchmarks that do not reflect the true complexity of the\nfracture processes where PFM excels as a method. To address this gap, we\nintroduce a challenging dataset based on PFM simulations designed to benchmark\nand advance ML methods for fracture modeling. This dataset includes three\nenergy decomposition methods, two boundary conditions, and 1,000 random initial\ncrack configurations for a total of 6,000 simulations. Each sample contains 100\ntime steps capturing the temporal evolution of the crack field. Alongside this\ndataset, we also implement and evaluate Physics Informed Neural Networks\n(PINN), Fourier Neural Operators (FNO) and UNet models as baselines, and\nexplore the impact of ensembling strategies on prediction accuracy. With this\ncombination of our dataset and baseline models drawn from the literature we aim\nto provide a standardized and challenging benchmark for evaluating machine\nlearning approaches to solid mechanics. Our results highlight both the promise\nand limitations of popular current models, and demonstrate the utility of this\ndataset as a testbed for advancing machine learning in fracture mechanics\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u76f8\u573a\u65ad\u88c2\u6a21\u578b\uff08PFM\uff09\u7684\u590d\u6742\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u65ad\u88c2\u529b\u5b66\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728\u65ad\u88c2\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u591a\u57fa\u4e8e\u7b80\u5355\u57fa\u51c6\uff0c\u672a\u80fd\u53cd\u6620\u5b9e\u9645\u65ad\u88c2\u8fc7\u7a0b\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u80fd\u91cf\u5206\u89e3\u65b9\u6cd5\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u521d\u59cb\u88c2\u7eb9\u914d\u7f6e\u7684PFM\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u4e86PINN\u3001FNO\u548cUNet\u7b49\u57fa\u7ebf\u6a21\u578b\u53ca\u96c6\u6210\u7b56\u7565\u3002", "result": "\u7814\u7a76\u5c55\u793a\u4e86\u5f53\u524d\u6d41\u884c\u6a21\u578b\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u4f5c\u4e3a\u65ad\u88c2\u529b\u5b66\u673a\u5668\u5b66\u4e60\u7814\u7a76\u6d4b\u8bd5\u5e73\u53f0\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\u4e3a\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u5728\u56fa\u4f53\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.07247", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.07247", "abs": "https://arxiv.org/abs/2507.07247", "authors": ["Zhengyu Tian", "Anantha Padmanaban Krishna Kumar", "Hemant Krishnakumar", "Reza Rawassizadeh"], "title": "Attentions Under the Microscope: A Comparative Study of Resource Utilization for Variants of Self-Attention", "comment": "6 pages, 8 figures", "summary": "As large language models (LLMs) and visual language models (VLMs) grow in\nscale and application, attention mechanisms have become a central computational\nbottleneck due to their high memory and time complexity. While many efficient\nattention variants have been proposed, there remains a lack of rigorous\nevaluation on their actual energy usage and hardware resource demands during\ntraining. In this work, we benchmark eight attention mechanisms in training\nGPT-2 architecture, measuring key metrics including training time, GPU memory\nusage, FLOPS, CPU usage, and power consumption. Our results reveal that\nattention mechanisms with optimized kernel implementations, including Flash\nAttention, Locality-Sensitive Hashing (LSH) Attention, and Multi-Head Latent\nAttention (MLA), achieve the best energy efficiency. We further show that lower\nGPU power alone does not guarantee reduced energy use, as training time plays\nan equally important role. Our study highlights the importance of energy-aware\nbenchmarking in attention design and provides a practical insight for selecting\nresource-efficient mechanisms. All our codes are available at GitHub.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u516b\u79cd\u6ce8\u610f\u529b\u673a\u5236\u5728GPT-2\u8bad\u7ec3\u4e2d\u7684\u80fd\u6e90\u548c\u786c\u4ef6\u8d44\u6e90\u6d88\u8017\uff0c\u53d1\u73b0\u4f18\u5316\u5185\u6838\u5b9e\u73b0\u7684\u673a\u5236\uff08\u5982Flash Attention\u3001LSH Attention\u548cMLA\uff09\u80fd\u6548\u6700\u9ad8\u3002", "motivation": "\u968f\u7740LLMs\u548cVLMs\u89c4\u6a21\u6269\u5927\uff0c\u6ce8\u610f\u529b\u673a\u5236\u6210\u4e3a\u8ba1\u7b97\u74f6\u9888\uff0c\u7f3a\u4e4f\u5bf9\u5176\u80fd\u6e90\u548c\u786c\u4ef6\u8d44\u6e90\u9700\u6c42\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "method": "\u5728GPT-2\u67b6\u6784\u4e0a\u5bf9\u516b\u79cd\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6d4b\u91cf\u8bad\u7ec3\u65f6\u95f4\u3001GPU\u5185\u5b58\u4f7f\u7528\u3001FLOPS\u3001CPU\u4f7f\u7528\u548c\u529f\u8017\u3002", "result": "\u4f18\u5316\u5185\u6838\u5b9e\u73b0\u7684\u6ce8\u610f\u529b\u673a\u5236\uff08\u5982Flash Attention\u3001LSH Attention\u548cMLA\uff09\u80fd\u6548\u6700\u9ad8\uff1bGPU\u529f\u8017\u4f4e\u5e76\u4e0d\u4fdd\u8bc1\u80fd\u8017\u4f4e\uff0c\u8bad\u7ec3\u65f6\u95f4\u540c\u6837\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u80fd\u6e90\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u9009\u62e9\u8d44\u6e90\u9ad8\u6548\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2507.07259", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07259", "abs": "https://arxiv.org/abs/2507.07259", "authors": ["Giulio Rossolini", "Fabio Brau", "Alessandro Biondi", "Battista Biggio", "Giorgio Buttazzo"], "title": "Exploiting Edge Features for Transferable Adversarial Attacks in Distributed Machine Learning", "comment": "under review", "summary": "As machine learning models become increasingly deployed across the edge of\ninternet of things environments, a partitioned deep learning paradigm in which\nmodels are split across multiple computational nodes introduces a new dimension\nof security risk. Unlike traditional inference setups, these distributed\npipelines span the model computation across heterogeneous nodes and\ncommunication layers, thereby exposing a broader attack surface to potential\nadversaries. Building on these motivations, this work explores a previously\noverlooked vulnerability: even when both the edge and cloud components of the\nmodel are inaccessible (i.e., black-box), an adversary who intercepts the\nintermediate features transmitted between them can still pose a serious threat.\nWe demonstrate that, under these mild and realistic assumptions, an attacker\ncan craft highly transferable proxy models, making the entire deep learning\nsystem significantly more vulnerable to evasion attacks. In particular, the\nintercepted features can be effectively analyzed and leveraged to distill\nsurrogate models capable of crafting highly transferable adversarial examples\nagainst the target model. To this end, we propose an exploitation strategy\nspecifically designed for distributed settings, which involves reconstructing\nthe original tensor shape from vectorized transmitted features using simple\nstatistical analysis, and adapting surrogate architectures accordingly to\nenable effective feature distillation. A comprehensive and systematic\nexperimental evaluation has been conducted to demonstrate that surrogate models\ntrained with the proposed strategy, i.e., leveraging intermediate features,\ntremendously improve the transferability of adversarial attacks. These findings\nunderscore the urgent need to account for intermediate feature leakage in the\ndesign of secure distributed deep learning systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4e2d\u95f4\u7279\u5f81\u6cc4\u9732\u6f0f\u6d1e\uff0c\u5c55\u793a\u4e86\u653b\u51fb\u8005\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u6784\u5efa\u4ee3\u7406\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u6297\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7269\u8054\u7f51\u8fb9\u7f18\u90e8\u7f72\u7684\u589e\u52a0\uff0c\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\u3002\u5373\u4f7f\u8fb9\u7f18\u548c\u4e91\u7aef\u6a21\u578b\u4e0d\u53ef\u8bbf\u95ee\uff0c\u653b\u51fb\u8005\u4ecd\u53ef\u901a\u8fc7\u62e6\u622a\u4e2d\u95f4\u7279\u5f81\u6784\u6210\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5206\u5e03\u5f0f\u73af\u5883\u7684\u5229\u7528\u7b56\u7565\uff0c\u5305\u62ec\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u91cd\u6784\u4f20\u8f93\u7279\u5f81\u7684\u539f\u59cb\u5f20\u91cf\u5f62\u72b6\uff0c\u5e76\u8c03\u6574\u4ee3\u7406\u67b6\u6784\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u7279\u5f81\u84b8\u998f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u4e2d\u95f4\u7279\u5f81\u8bad\u7ec3\u7684\u4ee3\u7406\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u53ef\u8f6c\u79fb\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5b89\u5168\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u8003\u8651\u4e2d\u95f4\u7279\u5f81\u6cc4\u9732\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2507.07271", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07271", "abs": "https://arxiv.org/abs/2507.07271", "authors": ["Julianna Piskorz", "Krzysztof Kacprzyk", "Mihaela van der Schaar"], "title": "Beyond the ATE: Interpretable Modelling of Treatment Effects over Dose and Time", "comment": "Presented at the Actionable Interpretability Workshop at ICML 2025", "summary": "The Average Treatment Effect (ATE) is a foundational metric in causal\ninference, widely used to assess intervention efficacy in randomized controlled\ntrials (RCTs). However, in many applications -- particularly in healthcare --\nthis static summary fails to capture the nuanced dynamics of treatment effects\nthat vary with both dose and time. We propose a framework for modelling\ntreatment effect trajectories as smooth surfaces over dose and time, enabling\nthe extraction of clinically actionable insights such as onset time, peak\neffect, and duration of benefit. To ensure interpretability, robustness, and\nverifiability -- key requirements in high-stakes domains -- we adapt\nSemanticODE, a recent framework for interpretable trajectory modelling, to the\ncausal setting where treatment effects are never directly observed. Our\napproach decouples the estimation of trajectory shape from the specification of\nclinically relevant properties (e.g., maxima, inflection points), supporting\ndomain-informed priors, post-hoc editing, and transparent analysis. We show\nthat our method yields accurate, interpretable, and editable models of\ntreatment dynamics, facilitating both rigorous causal analysis and practical\ndecision-making.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5efa\u6a21\u6cbb\u7597\u6548\u5e94\u8f68\u8ff9\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5242\u91cf\u548c\u65f6\u95f4\u5e73\u6ed1\u8868\u9762\u63d0\u53d6\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5982\u8d77\u6548\u65f6\u95f4\u3001\u5cf0\u503c\u6548\u5e94\u548c\u6301\u7eed\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u5728\u52a8\u6001\u53d8\u5316\u7684\u6cbb\u7597\u6548\u5e94\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u533b\u7597\u9886\u57df\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u5206\u6790\u3002", "method": "\u91c7\u7528SemanticODE\u6846\u67b6\uff0c\u5c06\u5176\u9002\u914d\u5230\u56e0\u679c\u63a8\u65ad\u4e2d\uff0c\u4f30\u8ba1\u8f68\u8ff9\u5f62\u72b6\u5e76\u652f\u6301\u4e34\u5e8a\u76f8\u5173\u5c5e\u6027\u7684\u540e\u9a8c\u7f16\u8f91\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u7f16\u8f91\u5730\u5efa\u6a21\u6cbb\u7597\u52a8\u6001\uff0c\u652f\u6301\u4e25\u8c28\u7684\u56e0\u679c\u5206\u6790\u548c\u5b9e\u9645\u51b3\u7b56\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u3001\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u6cbb\u7597\u6548\u5e94\u8f68\u8ff9\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2507.07288", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.07288", "abs": "https://arxiv.org/abs/2507.07288", "authors": ["Pierre Osselin", "Masaki Adachi", "Xiaowen Dong", "Michael A. Osborne"], "title": "Natural Evolutionary Search meets Probabilistic Numerics", "comment": "8 pages, 5 figures (24 pages, 11 figures including references and\n  appendices)", "summary": "Zeroth-order local optimisation algorithms are essential for solving\nreal-valued black-box optimisation problems. Among these, Natural Evolution\nStrategies (NES) represent a prominent class, particularly well-suited for\nscenarios where prior distributions are available. By optimising the objective\nfunction in the space of search distributions, NES algorithms naturally\nintegrate prior knowledge during initialisation, making them effective in\nsettings such as semi-supervised learning and user-prior belief frameworks.\nHowever, due to their reliance on random sampling and Monte Carlo estimates,\nNES algorithms can suffer from limited sample efficiency. In this paper, we\nintroduce a novel class of algorithms, termed Probabilistic Natural\nEvolutionary Strategy Algorithms (ProbNES), which enhance the NES framework\nwith Bayesian quadrature. We show that ProbNES algorithms consistently\noutperforms their non-probabilistic counterparts as well as global sample\nefficient methods such as Bayesian Optimisation (BO) or $\\pi$BO across a wide\nrange of tasks, including benchmark test functions, data-driven optimisation\ntasks, user-informed hyperparameter tuning tasks and locomotion tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProbNES\u7684\u65b0\u578b\u7b97\u6cd5\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u79ef\u5206\u589e\u5f3a\u81ea\u7136\u8fdb\u5316\u7b56\u7565\uff08NES\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u4f18\u4e8e\u975e\u6982\u7387\u65b9\u6cd5\u548c\u5168\u5c40\u9ad8\u6548\u65b9\u6cd5\u3002", "motivation": "\u81ea\u7136\u8fdb\u5316\u7b56\u7565\uff08NES\uff09\u5728\u89e3\u51b3\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u65f6\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u548c\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\uff0c\u5bfc\u81f4\u6837\u672c\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u8d1d\u53f6\u65af\u79ef\u5206\u6539\u8fdbNES\u6846\u67b6\u3002", "method": "\u63d0\u51faProbNES\u7b97\u6cd5\uff0c\u5c06\u8d1d\u53f6\u65af\u79ef\u5206\u878d\u5165NES\u6846\u67b6\uff0c\u4f18\u5316\u641c\u7d22\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProbNES\u5728\u57fa\u51c6\u6d4b\u8bd5\u3001\u6570\u636e\u9a71\u52a8\u4f18\u5316\u3001\u7528\u6237\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u8fd0\u52a8\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u975e\u6982\u7387\u65b9\u6cd5\u548c\u5168\u5c40\u9ad8\u6548\u65b9\u6cd5\uff08\u5982\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u3002", "conclusion": "ProbNES\u901a\u8fc7\u8d1d\u53f6\u65af\u79ef\u5206\u663e\u8457\u63d0\u5347\u4e86NES\u7684\u6837\u672c\u6548\u7387\uff0c\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07291", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07291", "abs": "https://arxiv.org/abs/2507.07291", "authors": ["Paola Causin", "Alessio Marta"], "title": "Estimating Dataset Dimension via Singular Metrics under the Manifold Hypothesis: Application to Inverse Problems", "comment": null, "summary": "High-dimensional datasets often exhibit low-dimensional geometric structures,\nas suggested by the manifold hypothesis, which implies that data lie on a\nsmooth manifold embedded in a higher-dimensional ambient space. While this\ninsight underpins many advances in machine learning and inverse problems, fully\nleveraging it requires to deal with three key tasks: estimating the intrinsic\ndimension (ID) of the manifold, constructing appropriate local coordinates, and\nlearning mappings between ambient and manifold spaces. In this work, we propose\na framework that addresses all these challenges using a Mixture of Variational\nAutoencoders (VAEs) and tools from Riemannian geometry. We specifically focus\non estimating the ID of datasets by analyzing the numerical rank of the VAE\ndecoder pullback metric. The estimated ID guides the construction of an atlas\nof local charts using a mixture of invertible VAEs, enabling accurate manifold\nparameterization and efficient inference. We how this approach enhances\nsolutions to ill-posed inverse problems, particularly in biomedical imaging, by\nenforcing that reconstructions lie on the learned manifold. Lastly, we explore\nthe impact of network pruning on manifold geometry and reconstruction quality,\nshowing that the intrinsic dimension serves as an effective proxy for\nmonitoring model capacity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u548c\u9ece\u66fc\u51e0\u4f55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f30\u8ba1\u9ad8\u7ef4\u6570\u636e\u7684\u672c\u5f81\u7ef4\u5ea6\uff08ID\uff09\u3001\u6784\u5efa\u5c40\u90e8\u5750\u6807\u5e76\u5b66\u4e60\u6620\u5c04\uff0c\u63d0\u5347\u9006\u95ee\u9898\uff08\u5982\u751f\u7269\u533b\u5b66\u6210\u50cf\uff09\u7684\u89e3\u51b3\u6548\u679c\u3002", "motivation": "\u9ad8\u7ef4\u6570\u636e\u901a\u5e38\u5177\u6709\u4f4e\u7ef4\u51e0\u4f55\u7ed3\u6784\uff08\u6d41\u5f62\u5047\u8bbe\uff09\uff0c\u4f46\u5145\u5206\u5229\u7528\u8fd9\u4e00\u7ed3\u6784\u9700\u89e3\u51b3\u672c\u5f81\u7ef4\u5ea6\u4f30\u8ba1\u3001\u5c40\u90e8\u5750\u6807\u6784\u5efa\u548c\u6620\u5c04\u5b66\u4e60\u4e09\u5927\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u6df7\u5408VAEs\u548c\u9ece\u66fc\u51e0\u4f55\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790VAE\u89e3\u7801\u5668\u56de\u62c9\u5ea6\u91cf\u7684\u6570\u503c\u79e9\u4f30\u8ba1ID\uff0c\u6784\u5efa\u5c40\u90e8\u5750\u6807\u56fe\u96c6\uff0c\u5e76\u5b66\u4e60\u6d41\u5f62\u6620\u5c04\u3002", "result": "\u65b9\u6cd5\u80fd\u51c6\u786e\u4f30\u8ba1ID\u5e76\u6784\u5efa\u6d41\u5f62\u53c2\u6570\u5316\uff0c\u63d0\u5347\u9006\u95ee\u9898\uff08\u5982\u751f\u7269\u533b\u5b66\u6210\u50cf\uff09\u7684\u89e3\u51b3\u6548\u679c\uff0c\u540c\u65f6ID\u53ef\u4f5c\u4e3a\u6a21\u578b\u5bb9\u91cf\u7684\u6709\u6548\u6307\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u7ef4\u6570\u636e\u6d41\u5f62\u5b66\u4e60\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.07292", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07292", "abs": "https://arxiv.org/abs/2507.07292", "authors": ["Jacob Hauck", "Yanzhi Zhang"], "title": "Discretization-independent multifidelity operator learning for partial differential equations", "comment": "33 pages, 9 figures, submitted to the Journal of Machine Learning\n  Research", "summary": "We develop a new and general encode-approximate-reconstruct operator learning\nmodel that leverages learned neural representations of bases for input and\noutput function distributions. We introduce the concepts of \\textit{numerical\noperator learning} and \\textit{discretization independence}, which clarify the\nrelationship between theoretical formulations and practical realizations of\noperator learning models. Our model is discretization-independent, making it\nparticularly effective for multifidelity learning. We establish theoretical\napproximation guarantees, demonstrating uniform universal approximation under\nstrong assumptions on the input functions and statistical approximation under\nweaker conditions. To our knowledge, this is the first comprehensive study that\ninvestigates how discretization independence enables robust and efficient\nmultifidelity operator learning. We validate our method through extensive\nnumerical experiments involving both local and nonlocal PDEs, including\ntime-independent and time-dependent problems. The results show that\nmultifidelity training significantly improves accuracy and computational\nefficiency. Moreover, multifidelity training further enhances empirical\ndiscretization independence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u901a\u7528\u7f16\u7801-\u8fd1\u4f3c-\u91cd\u6784\u7b97\u5b50\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u795e\u7ecf\u8868\u793a\u5b66\u4e60\u8f93\u5165\u548c\u8f93\u51fa\u51fd\u6570\u7684\u57fa\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u79bb\u6563\u5316\u72ec\u7acb\u6027\u548c\u6570\u503c\u7b97\u5b50\u5b66\u4e60\u6982\u5ff5\uff0c\u9a8c\u8bc1\u4e86\u591a\u4fdd\u771f\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u79bb\u6563\u5316\u72ec\u7acb\u6027\u5b9e\u73b0\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u591a\u4fdd\u771f\u5ea6\u7b97\u5b50\u5b66\u4e60\uff0c\u586b\u8865\u7406\u8bba\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u79bb\u6563\u5316\u72ec\u7acb\u7684\u7f16\u7801-\u8fd1\u4f3c-\u91cd\u6784\u7b97\u5b50\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u8868\u793a\u5b66\u4e60\u57fa\u5206\u5e03\uff0c\u5e76\u5728\u591a\u4fdd\u771f\u5ea6\u8bad\u7ec3\u4e2d\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6a21\u578b\u7684\u8fd1\u4f3c\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u4fdd\u771f\u5ea6\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u79bb\u6563\u5316\u72ec\u7acb\u6027\u4e3a\u591a\u4fdd\u771f\u5ea6\u7b97\u5b50\u5b66\u4e60\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5c40\u90e8\u548c\u975e\u5c40\u90e8PDE\u95ee\u9898\u4e2d\u5747\u6709\u6548\u3002"}}
{"id": "2507.07316", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07316", "abs": "https://arxiv.org/abs/2507.07316", "authors": ["Md Abrar Jahin", "Taufikur Rahman Fuad", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "title": "AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid Classical-Quantum Models with Dynamic Layer Sparing", "comment": "Accepted in 1st International Workshop on ICCV'25 BISCUIT (Biomedical\n  Image and Signal Computing for Unbiasedness, Interpretability, and\n  Trustworthiness)", "summary": "Federated Learning (FL) faces inherent challenges in balancing model\nperformance, privacy preservation, and communication efficiency, especially in\nnon-IID decentralized environments. Recent approaches either sacrifice formal\nprivacy guarantees, incur high overheads, or overlook quantum-enhanced\nexpressivity. We introduce AdeptHEQ-FL, a unified hybrid classical-quantum FL\nframework that integrates (i) a hybrid CNN-PQC architecture for expressive\ndecentralized learning, (ii) an adaptive accuracy-weighted aggregation scheme\nleveraging differentially private validation accuracies, (iii) selective\nhomomorphic encryption (HE) for secure aggregation of sensitive model layers,\nand (iv) dynamic layer-wise adaptive freezing to minimize communication\noverhead while preserving quantum adaptability. We establish formal privacy\nguarantees, provide convergence analysis, and conduct extensive experiments on\nthe CIFAR-10, SVHN, and Fashion-MNIST datasets. AdeptHEQ-FL achieves a $\\approx\n25.43\\%$ and $\\approx 14.17\\%$ accuracy improvement over Standard-FedQNN and\nFHE-FedQNN, respectively, on the CIFAR-10 dataset. Additionally, it reduces\ncommunication overhead by freezing less important layers, demonstrating the\nefficiency and practicality of our privacy-preserving, resource-aware design\nfor FL.", "AI": {"tldr": "AdeptHEQ-FL\u662f\u4e00\u4e2a\u6df7\u5408\u7ecf\u5178-\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3001\u9690\u79c1\u548c\u901a\u4fe1\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408CNN-PQC\u67b6\u6784\u3001\u5dee\u5206\u9690\u79c1\u9a8c\u8bc1\u3001\u9009\u62e9\u6027\u540c\u6001\u52a0\u5bc6\u548c\u52a8\u6001\u5c42\u51bb\u7ed3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u73af\u5883\u4e2d\u6a21\u578b\u6027\u80fd\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22\u91cf\u5b50\u589e\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51faAdeptHEQ-FL\u6846\u67b6\uff0c\u5305\u62ec\uff1a(1) \u6df7\u5408CNN-PQC\u67b6\u6784\uff0c(2) \u5dee\u5206\u9690\u79c1\u9a8c\u8bc1\u7684\u52a0\u6743\u805a\u5408\uff0c(3) \u9009\u62e9\u6027\u540c\u6001\u52a0\u5bc6\uff0c(4) \u52a8\u6001\u5c42\u51bb\u7ed3\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cAdeptHEQ-FL\u6bd4Standard-FedQNN\u548cFHE-FedQNN\u5206\u522b\u63d0\u5347\u7ea625.43%\u548c14.17%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "AdeptHEQ-FL\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u8d44\u6e90\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u73af\u5883\u3002"}}
{"id": "2507.07320", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07320", "abs": "https://arxiv.org/abs/2507.07320", "authors": ["Dongyu Wei", "Xiaoren Xu", "Shiwen Mao", "Mingzhe Chen"], "title": "Optimizing Communication and Device Clustering for Clustered Federated Learning with Differential Privacy", "comment": null, "summary": "In this paper, a secure and communication-efficient clustered federated\nlearning (CFL) design is proposed. In our model, several base stations (BSs)\nwith heterogeneous task-handling capabilities and multiple users with\nnon-independent and identically distributed (non-IID) data jointly perform CFL\ntraining incorporating differential privacy (DP) techniques. Since each BS can\nprocess only a subset of the learning tasks and has limited wireless resource\nblocks (RBs) to allocate to users for federated learning (FL) model parameter\ntransmission, it is necessary to jointly optimize RB allocation and user\nscheduling for CFL performance optimization. Meanwhile, our considered CFL\nmethod requires devices to use their limited data and FL model information to\ndetermine their task identities, which may introduce additional communication\noverhead. We formulate an optimization problem whose goal is to minimize the\ntraining loss of all learning tasks while considering device clustering, RB\nallocation, DP noise, and FL model transmission delay. To solve the problem, we\npropose a novel dynamic penalty function assisted value decomposed multi-agent\nreinforcement learning (DPVD-MARL) algorithm that enables distributed BSs to\nindependently determine their connected users, RBs, and DP noise of the\nconnected users but jointly minimize the training loss of all learning tasks\nacross all BSs. Different from the existing MARL methods that assign a large\npenalty for invalid actions, we propose a novel penalty assignment scheme that\nassigns penalty depending on the number of devices that cannot meet\ncommunication constraints (e.g., delay), which can guide the MARL scheme to\nquickly find valid actions, thus improving the convergence speed. Simulation\nresults show that the DPVD-MARL can improve the convergence rate by up to 20%\nand the ultimate accumulated rewards by 15% compared to independent Q-learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u4e14\u901a\u4fe1\u9ad8\u6548\u7684\u96c6\u7fa4\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u4f18\u5316\u8d44\u6e90\u5757\u5206\u914d\u548c\u7528\u6237\u8c03\u5ea6\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u60e9\u7f5a\u51fd\u6570\u8f85\u52a9\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08DPVD-MARL\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u57fa\u7ad9\uff08BSs\uff09\u548c\u7528\u6237\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u95ee\u9898\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u91c7\u7528\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u63d0\u51faDPVD-MARL\u7b97\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u60e9\u7f5a\u51fd\u6570\u4ee5\u4f18\u5316\u8d44\u6e90\u5757\u5206\u914d\u548c\u7528\u6237\u8c03\u5ea6\u3002", "result": "\u4eff\u771f\u663e\u793aDPVD-MARL\u6bd4\u72ec\u7acbQ\u5b66\u4e60\u6536\u655b\u901f\u5ea6\u5feb20%\uff0c\u7d2f\u8ba1\u5956\u52b1\u63d0\u9ad815%\u3002", "conclusion": "DPVD-MARL\u7b97\u6cd5\u5728\u96c6\u7fa4\u8054\u90a6\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2507.07323", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07323", "abs": "https://arxiv.org/abs/2507.07323", "authors": ["Dongyu Wei", "Xiaoren Xu", "Yuchen Liu", "H. Vincent Poor", "Mingzhe Chen"], "title": "Optimizing Model Splitting and Device Task Assignment for Deceptive Signal Assisted Private Multi-hop Split Learning", "comment": null, "summary": "In this paper, deceptive signal-assisted private split learning is\ninvestigated. In our model, several edge devices jointly perform collaborative\ntraining, and some eavesdroppers aim to collect the model and data information\nfrom devices. To prevent the eavesdroppers from collecting model and data\ninformation, a subset of devices can transmit deceptive signals. Therefore, it\nis necessary to determine the subset of devices used for deceptive signal\ntransmission, the subset of model training devices, and the models assigned to\neach model training device. This problem is formulated as an optimization\nproblem whose goal is to minimize the information leaked to eavesdroppers while\nmeeting the model training energy consumption and delay constraints. To solve\nthis problem, we propose a soft actor-critic deep reinforcement learning\nframework with intrinsic curiosity module and cross-attention (ICM-CA) that\nenables a centralized agent to determine the model training devices, the\ndeceptive signal transmission devices, the transmit power, and sub-models\nassigned to each model training device without knowing the position and\nmonitoring probability of eavesdroppers. The proposed method uses an ICM module\nto encourage the server to explore novel actions and states and a CA module to\ndetermine the importance of each historical state-action pair thus improving\ntraining efficiency. Simulation results demonstrate that the proposed method\nimproves the convergence rate by up to 3x and reduces the information leaked to\neavesdroppers by up to 13% compared to the traditional SAC algorithm.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6b3a\u9a97\u4fe1\u53f7\u8f85\u52a9\u7684\u79c1\u6709\u5206\u5272\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\u4ee5\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u6ee1\u8db3\u80fd\u8017\u548c\u5ef6\u8fdf\u7ea6\u675f\u3002", "motivation": "\u9632\u6b62\u7a83\u542c\u8005\u4ece\u8fb9\u7f18\u8bbe\u5907\u6536\u96c6\u6a21\u578b\u548c\u6570\u636e\u4fe1\u606f\uff0c\u786e\u4fdd\u534f\u4f5c\u8bad\u7ec3\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08ICM-CA\uff09\uff0c\u7ed3\u5408\u5185\u5728\u597d\u5947\u5fc3\u6a21\u5757\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u52a8\u6001\u9009\u62e9\u8bbe\u5907\u5206\u914d\u548c\u4f20\u8f93\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edfSAC\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u63d0\u53473\u500d\uff0c\u4fe1\u606f\u6cc4\u9732\u51cf\u5c1113%\u3002", "conclusion": "ICM-CA\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2507.07328", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.07328", "abs": "https://arxiv.org/abs/2507.07328", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "title": "Bridging the Plausibility-Validity Gap by Fine-Tuning a Reasoning-Enhanced LLM for Chemical Synthesis and Discovery", "comment": "42 pages, 8 figures, 1 equation, 2 algorithms, 31 tables, to be\n  published in ISPACS Conference 2025, unabridged version", "summary": "Large Language Models (LLMs) often generate scientifically plausible but\nfactually invalid information, a challenge we term the \"plausibility-validity\ngap,\" particularly in specialized domains like chemistry. This paper presents a\nsystematic methodology to bridge this gap by developing a specialized\nscientific assistant. We utilized the Magistral Small model, noted for its\nintegrated reasoning capabilities, and fine-tuned it using Low-Rank Adaptation\n(LoRA). A key component of our approach was the creation of a \"dual-domain\ndataset,\" a comprehensive corpus curated from various sources encompassing both\nmolecular properties and chemical reactions, which was standardized to ensure\nquality. Our evaluation demonstrates that the fine-tuned model achieves\nsignificant improvements over the baseline model in format adherence, chemical\nvalidity of generated molecules, and the feasibility of proposed synthesis\nroutes. The results indicate a hierarchical learning pattern, where syntactic\ncorrectness is learned more readily than chemical possibility and synthesis\nfeasibility. While a comparative analysis with human experts revealed\ncompetitive performance in areas like chemical creativity and reasoning, it\nalso highlighted key limitations, including persistent errors in\nstereochemistry, a static knowledge cutoff, and occasional reference\nhallucination. This work establishes a viable framework for adapting generalist\nLLMs into reliable, specialized tools for chemical research, while also\ndelineating critical areas for future improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03LLM\uff08Magistral Small\u6a21\u578b\uff09\u548c\u6784\u5efa\u53cc\u9886\u57df\u6570\u636e\u96c6\uff0c\u89e3\u51b3LLM\u5728\u5316\u5b66\u9886\u57df\u751f\u6210\u4fe1\u606f\u65f6\u5b58\u5728\u7684\u201c\u53ef\u4fe1\u5ea6-\u6709\u6548\u6027\u5dee\u8ddd\u201d\u95ee\u9898\u3002", "motivation": "LLM\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5316\u5b66\uff09\u4e2d\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u65e0\u6548\u7684\u4fe1\u606f\uff0c\u8fd9\u4e00\u95ee\u9898\u88ab\u79f0\u4e3a\u201c\u53ef\u4fe1\u5ea6-\u6709\u6548\u6027\u5dee\u8ddd\u201d\u3002", "method": "\u4f7f\u7528Magistral Small\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u6784\u5efa\u53cc\u9886\u57df\u6570\u636e\u96c6\uff08\u6db5\u76d6\u5206\u5b50\u6027\u8d28\u548c\u5316\u5b66\u53cd\u5e94\uff09\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u683c\u5f0f\u4e00\u81f4\u6027\u3001\u5206\u5b50\u5316\u5b66\u6709\u6548\u6027\u53ca\u5408\u6210\u8def\u7ebf\u53ef\u884c\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4f46\u5728\u7acb\u4f53\u5316\u5b66\u7b49\u65b9\u9762\u4ecd\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c06\u901a\u7528LLM\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u5316\u5b66\u7814\u7a76\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2507.07335", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07335", "abs": "https://arxiv.org/abs/2507.07335", "authors": ["Ankit Jyothish", "Ali Jannesari"], "title": "Leveraging Manifold Embeddings for Enhanced Graph Transformer Representations and Learning", "comment": null, "summary": "Graph transformers typically embed every node in a single Euclidean space,\nblurring heterogeneous topologies. We prepend a lightweight Riemannian\nmixture-of-experts layer that routes each node to various kinds of manifold,\nmixture of spherical, flat, hyperbolic - best matching its local structure.\nThese projections provide intrinsic geometric explanations to the latent space.\nInserted into a state-of-the-art ensemble graph transformer, this projector\nlifts accuracy by up to 3% on four node-classification benchmarks. The ensemble\nmakes sure that both euclidean and non-euclidean features are captured.\nExplicit, geometry-aware projection thus sharpens predictive power while making\ngraph representations more interpretable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9ece\u66fc\u6df7\u5408\u4e13\u5bb6\u5c42\uff0c\u5c06\u8282\u70b9\u6295\u5f71\u5230\u591a\u79cd\u6d41\u5f62\u4e0a\uff0c\u63d0\u5347\u4e86\u56fe\u53d8\u6362\u5668\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u56fe\u53d8\u6362\u5668\u5c06\u6240\u6709\u8282\u70b9\u5d4c\u5165\u5355\u4e00\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u5ffd\u7565\u4e86\u5f02\u6784\u62d3\u6251\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5728\u73b0\u6709\u56fe\u53d8\u6362\u5668\u524d\u52a0\u5165\u9ece\u66fc\u6df7\u5408\u4e13\u5bb6\u5c42\uff0c\u5c06\u8282\u70b9\u8def\u7531\u5230\u6700\u9002\u5408\u5176\u5c40\u90e8\u7ed3\u6784\u7684\u6d41\u5f62\uff08\u5982\u7403\u9762\u3001\u5e73\u5766\u3001\u53cc\u66f2\u7a7a\u95f4\uff09\u3002", "result": "\u5728\u56db\u4e2a\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u4e0a\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe3%\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6f5c\u5728\u7a7a\u95f4\u7684\u51e0\u4f55\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u7684\u51e0\u4f55\u611f\u77e5\u6295\u5f71\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u80fd\u529b\u5e76\u4f7f\u56fe\u8868\u793a\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.07348", "categories": ["cs.LG", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.07348", "abs": "https://arxiv.org/abs/2507.07348", "authors": ["James Chapman", "Kedar Karhadkar", "Guido Montufar"], "title": "Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts", "comment": "10 pages, 8 figures, 3 tables, submitted to Neurips 2025", "summary": "Deep reinforcement learning (DRL) has achieved remarkable success across\nmultiple domains, including competitive games, natural language processing, and\nrobotics. Despite these advancements, policies trained via DRL often struggle\nto generalize to evaluation environments with different parameters. This\nchallenge is typically addressed by training with multiple contexts and/or by\nleveraging additional structure in the problem. However, obtaining sufficient\ntraining data across diverse contexts can be impractical in real-world\napplications. In this work, we consider contextual Markov decision processes\n(CMDPs) with transition and reward functions that exhibit regularity in context\nparameters. We introduce the context-enhanced Bellman equation (CEBE) to\nimprove generalization when training on a single context. We prove both\nanalytically and empirically that the CEBE yields a first-order approximation\nto the Q-function trained across multiple contexts. We then derive context\nsample enhancement (CSE) as an efficient data augmentation method for\napproximating the CEBE in deterministic control environments. We numerically\nvalidate the performance of CSE in simulation environments, showcasing its\npotential to improve generalization in DRL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u589e\u5f3a\u7684\u8d1d\u5c14\u66fc\u65b9\u7a0b\uff08CEBE\uff09\u548c\u4e0a\u4e0b\u6587\u6837\u672c\u589e\u5f3a\uff08CSE\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u5355\u4e00\u4e0a\u4e0b\u6587\u8bad\u7ec3\u65f6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1DRL\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u7b56\u7565\u5728\u53c2\u6570\u4e0d\u540c\u7684\u8bc4\u4f30\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u901a\u8fc7\u5f15\u5165CEBE\u548cCSE\u65b9\u6cd5\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u53c2\u6570\u7684\u89c4\u5f8b\u6027\uff0c\u63d0\u5347\u5355\u4e00\u4e0a\u4e0b\u6587\u8bad\u7ec3\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0cCEBE\u80fd\u8fd1\u4f3c\u591a\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684Q\u51fd\u6570\uff0cCSE\u5728\u786e\u5b9a\u6027\u63a7\u5236\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CSE\u65b9\u6cd5\u5728\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u63d0\u5347DRL\u6cdb\u5316\u80fd\u529b\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.07354", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07354", "abs": "https://arxiv.org/abs/2507.07354", "authors": ["Farnam Mansouri", "Shai Ben-David"], "title": "Learning from positive and unlabeled examples -Finite size sample bounds", "comment": null, "summary": "PU (Positive Unlabeled) learning is a variant of supervised classification\nlearning in which the only labels revealed to the learner are of positively\nlabeled instances. PU learning arises in many real-world applications. Most\nexisting work relies on the simplifying assumptions that the positively labeled\ntraining data is drawn from the restriction of the data generating distribution\nto positively labeled instances and/or that the proportion of positively\nlabeled points (a.k.a. the class prior) is known apriori to the learner. This\npaper provides a theoretical analysis of the statistical complexity of PU\nlearning under a wider range of setups. Unlike most prior work, our study does\nnot assume that the class prior is known to the learner. We prove upper and\nlower bounds on the required sample sizes (of both the positively labeled and\nthe unlabeled samples).", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86PU\uff08\u6b63\u672a\u6807\u8bb0\uff09\u5b66\u4e60\u7684\u7edf\u8ba1\u590d\u6742\u6027\uff0c\u653e\u5bbd\u4e86\u5bf9\u7c7b\u5148\u9a8c\u5df2\u77e5\u7684\u5047\u8bbe\uff0c\u5e76\u8bc1\u660e\u4e86\u6b63\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6837\u672c\u6240\u9700\u6837\u672c\u91cf\u7684\u4e0a\u4e0b\u754c\u3002", "motivation": "PU\u5b66\u4e60\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u4f9d\u8d56\u4e8e\u6b63\u6807\u8bb0\u6570\u636e\u6765\u81ea\u6b63\u6807\u8bb0\u5b9e\u4f8b\u5206\u5e03\u9650\u5236\u6216\u7c7b\u5148\u9a8c\u5df2\u77e5\u7684\u7b80\u5316\u5047\u8bbe\u3002\u672c\u6587\u65e8\u5728\u653e\u5bbd\u8fd9\u4e9b\u5047\u8bbe\uff0c\u7814\u7a76\u66f4\u5e7f\u6cdb\u60c5\u51b5\u4e0b\u7684\u7edf\u8ba1\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u4e86PU\u5b66\u4e60\u5728\u4e0d\u540c\u8bbe\u5b9a\u4e0b\u7684\u7edf\u8ba1\u590d\u6742\u6027\uff0c\u672a\u5047\u8bbe\u7c7b\u5148\u9a8c\u5df2\u77e5\u3002", "result": "\u8bc1\u660e\u4e86\u6b63\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6837\u672c\u6240\u9700\u6837\u672c\u91cf\u7684\u4e0a\u4e0b\u754c\u3002", "conclusion": "\u672c\u6587\u4e3aPU\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\uff0c\u653e\u5bbd\u4e86\u5bf9\u7c7b\u5148\u9a8c\u7684\u5047\u8bbe\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.07373", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07373", "abs": "https://arxiv.org/abs/2507.07373", "authors": ["Irsyad Adam", "Steven Swee", "Erika Yilin", "Ethan Ji", "William Speier", "Dean Wang", "Alex Bui", "Wei Wang", "Karol Watson", "Peipei Ping"], "title": "Atherosclerosis through Hierarchical Explainable Neural Network Analysis", "comment": null, "summary": "In this work, we study the problem pertaining to personalized classification\nof subclinical atherosclerosis by developing a hierarchical graph neural\nnetwork framework to leverage two characteristic modalities of a patient:\nclinical features within the context of the cohort, and molecular data unique\nto individual patients. Current graph-based methods for disease classification\ndetect patient-specific molecular fingerprints, but lack consistency and\ncomprehension regarding cohort-wide features, which are an essential\nrequirement for understanding pathogenic phenotypes across diverse\natherosclerotic trajectories. Furthermore, understanding patient subtypes often\nconsiders clinical feature similarity in isolation, without integration of\nshared pathogenic interdependencies among patients. To address these\nchallenges, we introduce ATHENA: Atherosclerosis Through Hierarchical\nExplainable Neural Network Analysis, which constructs a novel hierarchical\nnetwork representation through integrated modality learning; subsequently, it\noptimizes learned patient-specific molecular fingerprints that reflect\nindividual omics data, enforcing consistency with cohort-wide patterns. With a\nprimary clinical dataset of 391 patients, we demonstrate that this\nheterogeneous alignment of clinical features with molecular interaction\npatterns has significantly boosted subclinical atherosclerosis classification\nperformance across various baselines by up to 13% in area under the receiver\noperating curve (AUC) and 20% in F1 score. Taken together, ATHENA enables\nmechanistically-informed patient subtype discovery through explainable AI\n(XAI)-driven subnetwork clustering; this novel integration framework\nstrengthens personalized intervention strategies, thereby improving the\nprediction of atherosclerotic disease progression and management of their\nclinical actionable outcomes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aATHENA\u7684\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5206\u7c7b\u4e9a\u4e34\u5e8a\u52a8\u8109\u7ca5\u6837\u786c\u5316\uff0c\u7ed3\u5408\u60a3\u8005\u7684\u4e34\u5e8a\u7279\u5f81\u548c\u5206\u5b50\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u75be\u75c5\u5206\u7c7b\u4e2d\u7f3a\u4e4f\u5bf9\u961f\u5217\u8303\u56f4\u7279\u5f81\u7684\u4e00\u81f4\u6027\u548c\u7406\u89e3\uff0c\u4e14\u672a\u6574\u5408\u60a3\u8005\u95f4\u7684\u5171\u4eab\u81f4\u75c5\u4f9d\u8d56\u6027\u3002", "method": "ATHENA\u901a\u8fc7\u5206\u5c42\u7f51\u7edc\u8868\u793a\u548c\u6a21\u6001\u5b66\u4e60\u4f18\u5316\u60a3\u8005\u7279\u5f02\u6027\u5206\u5b50\u6307\u7eb9\uff0c\u786e\u4fdd\u4e0e\u961f\u5217\u6a21\u5f0f\u4e00\u81f4\u3002", "result": "\u5728391\u540d\u60a3\u8005\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08AUC\u63d0\u9ad813%\uff0cF1\u5206\u6570\u63d0\u9ad820%\uff09\u3002", "conclusion": "ATHENA\u901a\u8fc7\u53ef\u89e3\u91caAI\u9a71\u52a8\u7684\u5b50\u7f51\u7edc\u805a\u7c7b\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5e72\u9884\u7b56\u7565\uff0c\u6539\u5584\u52a8\u8109\u7ca5\u6837\u786c\u5316\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u3002"}}
{"id": "2507.07375", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07375", "abs": "https://arxiv.org/abs/2507.07375", "authors": ["Zhiwei Zhang", "Hui Liu", "Xiaomin Li", "Zhenwei Dai", "Jingying Zeng", "Fali Wang", "Minhua Lin", "Ramraj Chandradevan", "Zhen Li", "Chen Luo", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "Bradley-Terry and Multi-Objective Reward Modeling Are Complementary", "comment": null, "summary": "Reward models trained on human preference data have demonstrated strong\neffectiveness in aligning Large Language Models (LLMs) with human intent under\nthe framework of Reinforcement Learning from Human Feedback (RLHF). However,\nRLHF remains vulnerable to reward hacking, where the policy exploits\nimperfections in the reward function rather than genuinely learning the\nintended behavior. Although significant efforts have been made to mitigate\nreward hacking, they predominantly focus on and evaluate in-distribution\nscenarios, where the training and testing data for the reward model share the\nsame distribution. In this paper, we empirically show that state-of-the-art\nmethods struggle in more challenging out-of-distribution (OOD) settings. We\nfurther demonstrate that incorporating fine-grained multi-attribute scores\nhelps address this challenge. However, the limited availability of high-quality\ndata often leads to weak performance of multi-objective reward functions, which\ncan negatively impact overall performance and become the bottleneck. To address\nthis issue, we propose a unified reward modeling framework that jointly trains\nBradley--Terry (BT) single-objective and multi-objective regression-based\nreward functions using a shared embedding space. We theoretically establish a\nconnection between the BT loss and the regression objective and highlight their\ncomplementary benefits. Specifically, the regression task enhances the\nsingle-objective reward function's ability to mitigate reward hacking in\nchallenging OOD settings, while BT-based training improves the scoring\ncapability of the multi-objective reward function, enabling a 7B model to\noutperform a 70B baseline. Extensive experimental results demonstrate that our\nframework significantly improves both the robustness and the scoring\nperformance of reward models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u8bad\u7ec3\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u5956\u52b1\u6a21\u578b\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3RLHF\u4e2d\u5956\u52b1\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u8bc4\u5206\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLHF\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u591a\u76ee\u6807\u5956\u52b1\u6a21\u578b\u56e0\u6570\u636e\u8d28\u91cf\u9650\u5236\u6027\u80fd\u8f83\u5f31\u3002", "method": "\u63d0\u51fa\u8054\u5408\u8bad\u7ec3Bradley-Terry\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u56de\u5f52\u5956\u52b1\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u8bc4\u5206\u6027\u80fd\uff0c7B\u6a21\u578b\u4f18\u4e8e70B\u57fa\u7ebf\u3002", "conclusion": "\u8054\u5408\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u6a21\u578b\u7684OOD\u8106\u5f31\u6027\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2507.07388", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07388", "abs": "https://arxiv.org/abs/2507.07388", "authors": ["Zesheng Liu", "Maryam Rahnemoonfar"], "title": "GRIT: Graph Transformer For Internal Ice Layer Thickness Prediction", "comment": "Accepted for 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025)", "summary": "Gaining a deeper understanding of the thickness and variability of internal\nice layers in Radar imagery is essential in monitoring the snow accumulation,\nbetter evaluating ice dynamics processes, and minimizing uncertainties in\nclimate models. Radar sensors, capable of penetrating ice, capture detailed\nradargram images of internal ice layers. In this work, we introduce GRIT, graph\ntransformer for ice layer thickness. GRIT integrates an inductive geometric\ngraph learning framework with an attention mechanism, designed to map the\nrelationships between shallow and deeper ice layers. Compared to baseline graph\nneural networks, GRIT demonstrates consistently lower prediction errors. These\nresults highlight the attention mechanism's effectiveness in capturing temporal\nchanges across ice layers, while the graph transformer combines the strengths\nof transformers for learning long-range dependencies with graph neural networks\nfor capturing spatial patterns, enabling robust modeling of complex\nspatiotemporal dynamics.", "AI": {"tldr": "GRIT\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u53d8\u6362\u5668\u7684\u51b0\u5c42\u539a\u5ea6\u9884\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u51e0\u4f55\u56fe\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u7814\u7a76\u51b0\u5c42\u539a\u5ea6\u53ca\u5176\u53d8\u5316\u5bf9\u76d1\u6d4b\u79ef\u96ea\u3001\u8bc4\u4f30\u51b0\u52a8\u529b\u5b66\u8fc7\u7a0b\u53ca\u51cf\u5c11\u6c14\u5019\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "GRIT\u7ed3\u5408\u4e86\u5f52\u7eb3\u51e0\u4f55\u56fe\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u5efa\u6a21\u6d45\u5c42\u4e0e\u6df1\u5c42\u51b0\u5c42\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u56fe\u795e\u7ecf\u7f51\u7edc\uff0cGRIT\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\uff0c\u80fd\u6709\u6548\u6355\u6349\u51b0\u5c42\u7684\u65f6\u95f4\u53d8\u5316\u548c\u7a7a\u95f4\u6a21\u5f0f\u3002", "conclusion": "GRIT\u901a\u8fc7\u7ed3\u5408\u53d8\u6362\u5668\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u65f6\u7a7a\u52a8\u6001\u7684\u9c81\u68d2\u5efa\u6a21\u3002"}}
{"id": "2507.07389", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07389", "abs": "https://arxiv.org/abs/2507.07389", "authors": ["Zesheng Liu", "Maryam Rahnemoonfar"], "title": "ST-GRIT: Spatio-Temporal Graph Transformer For Internal Ice Layer Thickness Prediction", "comment": "Accepted for 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Understanding the thickness and variability of internal ice layers in radar\nimagery is crucial for monitoring snow accumulation, assessing ice dynamics,\nand reducing uncertainties in climate models. Radar sensors, capable of\npenetrating ice, provide detailed radargram images of these internal layers. In\nthis work, we present ST-GRIT, a spatio-temporal graph transformer for ice\nlayer thickness, designed to process these radargrams and capture the\nspatiotemporal relationships between shallow and deep ice layers. ST-GRIT\nleverages an inductive geometric graph learning framework to extract local\nspatial features as feature embeddings and employs a series of temporal and\nspatial attention blocks separately to model long-range dependencies\neffectively in both dimensions. Experimental evaluation on radargram data from\nthe Greenland ice sheet demonstrates that ST-GRIT consistently outperforms\ncurrent state-of-the-art methods and other baseline graph neural networks by\nachieving lower root mean-squared error. These results highlight the advantages\nof self-attention mechanisms on graphs over pure graph neural networks,\nincluding the ability to handle noise, avoid oversmoothing, and capture\nlong-range dependencies. Moreover, the use of separate spatial and temporal\nattention blocks allows for distinct and robust learning of spatial\nrelationships and temporal patterns, providing a more comprehensive and\neffective approach.", "AI": {"tldr": "ST-GRIT\u662f\u4e00\u79cd\u65f6\u7a7a\u56fe\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u5904\u7406\u96f7\u8fbe\u56fe\u50cf\u4e2d\u7684\u51b0\u5c42\u539a\u5ea6\uff0c\u901a\u8fc7\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u51b0\u5c42\u539a\u5ea6\u53ca\u5176\u53d8\u5316\u5bf9\u76d1\u6d4b\u79ef\u96ea\u3001\u8bc4\u4f30\u51b0\u52a8\u6001\u548c\u51cf\u5c11\u6c14\u5019\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "ST-GRIT\u7ed3\u5408\u4e86\u5f52\u7eb3\u51e0\u4f55\u56fe\u5b66\u4e60\u6846\u67b6\u548c\u65f6\u7a7a\u6ce8\u610f\u529b\u5757\uff0c\u5206\u522b\u5efa\u6a21\u7a7a\u95f4\u548c\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u683c\u9675\u5170\u51b0\u76d6\u6570\u636e\u4e0a\uff0cST-GRIT\u7684\u5747\u65b9\u6839\u8bef\u5dee\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ST-GRIT\u5c55\u793a\u4e86\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u5728\u566a\u58f0\u5904\u7406\u3001\u907f\u514d\u8fc7\u5e73\u6ed1\u548c\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u65b9\u9762\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.07390", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07390", "abs": "https://arxiv.org/abs/2507.07390", "authors": ["Seonghyun Park", "Kiyoung Seong", "Soojung Yang", "Rafael G\u00f3mez-Bombarelli", "Sungsoo Ahn"], "title": "Learning Collective Variables from Time-lagged Generation", "comment": null, "summary": "Rare events such as state transitions are difficult to observe directly with\nmolecular dynamics simulations due to long timescales. Enhanced sampling\ntechniques overcome this by introducing biases along carefully chosen\nlow-dimensional features, known as collective variables (CVs), which capture\nthe slow degrees of freedom. Machine learning approaches (MLCVs) have automated\nCV discovery, but existing methods typically focus on discriminating\nmeta-stable states without fully encoding the detailed dynamics essential for\naccurate sampling. We propose TLC, a framework that learns CVs directly from\ntime-lagged conditions of a generative model. Instead of modeling the static\nBoltzmann distribution, TLC models a time-lagged conditional distribution\nyielding CVs to capture the slow dynamic behavior. We validate TLC on the\nAlanine Dipeptide system using two CV-based enhanced sampling tasks: (i)\nsteered molecular dynamics (SMD) and (ii) on-the-fly probability enhanced\nsampling (OPES), demonstrating equal or superior performance compared to\nexisting MLCV methods in both transition path sampling and state\ndiscrimination.", "AI": {"tldr": "TLC\u6846\u67b6\u901a\u8fc7\u4ece\u751f\u6210\u6a21\u578b\u7684\u65f6\u95f4\u6ede\u540e\u6761\u4ef6\u4e2d\u5b66\u4e60\u96c6\u4f53\u53d8\u91cf\uff08CVs\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u52a8\u6001\u884c\u4e3a\u7f16\u7801\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728Alanine Dipeptide\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u96be\u4ee5\u76f4\u63a5\u89c2\u5bdf\u7f55\u89c1\u4e8b\u4ef6\uff08\u5982\u72b6\u6001\u8f6c\u6362\uff09\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728CV\u53d1\u73b0\u4e2d\u672a\u80fd\u5145\u5206\u7f16\u7801\u52a8\u6001\u7ec6\u8282\u3002", "method": "TLC\u901a\u8fc7\u5efa\u6a21\u65f6\u95f4\u6ede\u540e\u6761\u4ef6\u5206\u5e03\u800c\u975e\u9759\u6001Boltzmann\u5206\u5e03\uff0c\u5b66\u4e60\u80fd\u6355\u6349\u6162\u52a8\u6001\u884c\u4e3a\u7684CVs\u3002", "result": "\u5728Alanine Dipeptide\u7cfb\u7edf\u4e2d\uff0cTLC\u5728SMD\u548cOPES\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u73b0\u6709MLCV\u65b9\u6cd5\u3002", "conclusion": "TLC\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684CV\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u52a8\u6001\u884c\u4e3a\u7f16\u7801\u7684\u589e\u5f3a\u91c7\u6837\u4efb\u52a1\u3002"}}
{"id": "2507.07399", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07399", "abs": "https://arxiv.org/abs/2507.07399", "authors": ["Yuntian Liu", "Tao Zhu", "Xiaoyang Liu", "Yu Chen", "Zhaoxuan Liu", "Qingfeng Guo", "Jiashuo Zhang", "Kangjie Bao", "Tao Luo"], "title": "Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for Statement Autoformalization", "comment": "Accepted to AI4Math@ICML25", "summary": "Statement autoformalization, the automated translation of statement from\nnatural language into formal languages, has become a subject of extensive\nresearch, yet the development of robust automated evaluation metrics remains\nlimited. Existing evaluation methods often lack semantic understanding, face\nchallenges with high computational costs, and are constrained by the current\nprogress of automated theorem proving. To address these issues, we propose GTED\n(Generalized Tree Edit Distance), a novel evaluation framework that first\nstandardizes formal statements and converts them into operator trees, then\ndetermines the semantic similarity using the eponymous GTED metric. On the\nminiF2F and ProofNet benchmarks, GTED outperforms all baseline metrics by\nachieving the highest accuracy and Kappa scores, thus providing the community\nwith a more faithful metric for automated evaluation. The code and experimental\nresults are available at https://github.com/XiaoyangLiu-sjtu/GTED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTED\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u8bed\u53e5\u8f6c\u5316\u4e3a\u5f62\u5f0f\u5316\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u5316\u548c\u6811\u7f16\u8f91\u8ddd\u79bb\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u53d7\u9650\u4e8e\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u8fdb\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "GTED\u6846\u67b6\u9996\u5148\u6807\u51c6\u5316\u5f62\u5f0f\u5316\u8bed\u53e5\u5e76\u8f6c\u6362\u4e3a\u64cd\u4f5c\u6811\uff0c\u7136\u540e\u4f7f\u7528GTED\u5ea6\u91cf\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5728miniF2F\u548cProofNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGTED\u7684\u51c6\u786e\u6027\u548cKappa\u5f97\u5206\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GTED\u4e3a\u81ea\u52a8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6307\u6807\uff0c\u4ee3\u7801\u548c\u5b9e\u9a8c\u7ed3\u679c\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.07405", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07405", "abs": "https://arxiv.org/abs/2507.07405", "authors": ["Pengfei Jiao", "Jialong Ni", "Di Jin", "Xuan Guo", "Huan Liu", "Hongjiang Chen", "Yanxian Bi"], "title": "HGMP:Heterogeneous Graph Multi-Task Prompt Learning", "comment": "The 25th International Joint Conference on Artificial Intelligence\n  (IJCAI-25)", "summary": "The pre-training and fine-tuning methods have gained widespread attention in\nthe field of heterogeneous graph neural networks due to their ability to\nleverage large amounts of unlabeled data during the pre-training phase,\nallowing the model to learn rich structural features. However, these methods\nface the issue of a mismatch between the pre-trained model and downstream\ntasks, leading to suboptimal performance in certain application scenarios.\nPrompt learning methods have emerged as a new direction in heterogeneous graph\ntasks, as they allow flexible adaptation of task representations to address\ntarget inconsistency. Building on this idea, this paper proposes a novel\nmulti-task prompt framework for the heterogeneous graph domain, named HGMP.\nFirst, to bridge the gap between the pre-trained model and downstream tasks, we\nreformulate all downstream tasks into a unified graph-level task format. Next,\nwe address the limitations of existing graph prompt learning methods, which\nstruggle to integrate contrastive pre-training strategies in the heterogeneous\ngraph domain. We design a graph-level contrastive pre-training strategy to\nbetter leverage heterogeneous information and enhance performance in multi-task\nscenarios. Finally, we introduce heterogeneous feature prompts, which enhance\nmodel performance by refining the representation of input graph features.\nExperimental results on public datasets show that our proposed method adapts\nwell to various tasks and significantly outperforms baseline methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHGMP\u7684\u591a\u4efb\u52a1\u63d0\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u4efb\u52a1\u683c\u5f0f\u3001\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u7279\u5f81\u63d0\u793a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u65b9\u6cd5\u5728\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "method": "\u63d0\u51faHGMP\u6846\u67b6\uff1a1) \u7edf\u4e00\u4e0b\u6e38\u4efb\u52a1\u683c\u5f0f\uff1b2) \u8bbe\u8ba1\u56fe\u7ea7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\uff1b3) \u5f15\u5165\u5f02\u6784\u56fe\u7279\u5f81\u63d0\u793a\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGMP\u80fd\u9002\u5e94\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HGMP\u901a\u8fc7\u7edf\u4e00\u4efb\u52a1\u683c\u5f0f\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u56fe\u4efb\u52a1\u4e2d\u7684\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.07432", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.07432", "abs": "https://arxiv.org/abs/2507.07432", "authors": ["Paul M. Riechers", "Thomas J. Elliott", "Adam S. Shai"], "title": "Neural networks leverage nominally quantum and post-quantum representations", "comment": null, "summary": "We show that deep neural networks, including transformers and RNNs,\npretrained as usual on next-token prediction, intrinsically discover and\nrepresent beliefs over 'quantum' and 'post-quantum' low-dimensional generative\nmodels of their training data -- as if performing iterative Bayesian updates\nover the latent state of this world model during inference as they observe more\ncontext. Notably, neural nets easily find these representation whereas there is\nno finite classical circuit that would do the job. The corresponding geometric\nrelationships among neural activations induced by different input sequences are\nfound to be largely independent of neural-network architecture. Each point in\nthis geometry corresponds to a history-induced probability density over all\npossible futures, and the relative displacement of these points reflects the\ndifference in mechanism and magnitude for how these distinct pasts affect the\nfuture.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5982Transformer\u548cRNN\uff09\u901a\u8fc7\u5e38\u89c4\u7684\u4e0b\u4e00\u8bcd\u9884\u6d4b\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u5185\u5728\u5730\u53d1\u73b0\u5e76\u8868\u8fbe\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4f4e\u7ef4\u751f\u6210\u6a21\u578b\u7684\u4fe1\u5ff5\uff0c\u7c7b\u4f3c\u4e8e\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5bf9\u6f5c\u5728\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u8d1d\u53f6\u65af\u66f4\u65b0\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u901a\u8fc7\u9884\u8bad\u7ec3\u4efb\u52a1\uff08\u5982\u4e0b\u4e00\u8bcd\u9884\u6d4b\uff09\u81ea\u7136\u5730\u5b66\u4e60\u5230\u5bf9\u6570\u636e\u7684\u751f\u6210\u6a21\u578b\u7684\u4fe1\u5ff5\u8868\u793a\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u7ecf\u5178\u7535\u8def\u7684\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5982Transformer\u548cRNN\uff09\u5728\u9884\u8bad\u7ec3\u540e\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u7814\u7a76\u5176\u5982\u4f55\u8868\u793a\u4f4e\u7ef4\u751f\u6210\u6a21\u578b\u7684\u4fe1\u5ff5\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u8f7b\u677e\u53d1\u73b0\u8fd9\u4e9b\u8868\u793a\uff0c\u800c\u7ecf\u5178\u7535\u8def\u65e0\u6cd5\u5b9e\u73b0\uff1b\u4e0d\u540c\u67b6\u6784\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u8f93\u5165\u5e8f\u5217\u8bf1\u5bfc\u7684\u6fc0\u6d3b\u51e0\u4f55\u5173\u7cfb\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u9884\u8bad\u7ec3\u4efb\u52a1\u81ea\u7136\u5730\u5b66\u4e60\u5230\u5bf9\u6570\u636e\u7684\u751f\u6210\u6a21\u578b\u7684\u4fe1\u5ff5\u8868\u793a\uff0c\u4e14\u8fd9\u79cd\u8868\u793a\u5177\u6709\u8de8\u67b6\u6784\u7684\u666e\u9002\u6027\u3002"}}
{"id": "2507.07456", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.07456", "abs": "https://arxiv.org/abs/2507.07456", "authors": ["Nawaf Alampara", "Anagha Aneesh", "Marti\u00f1o R\u00edos-Garc\u00eda", "Adrian Mirza", "Mara Schilling-Wilhelmi", "Ali Asghar Aghajani", "Meiling Sun", "Gordan Prastalo", "Kevin Maik Jablonka"], "title": "General purpose models for the chemical sciences", "comment": null, "summary": "Data-driven techniques have a large potential to transform and accelerate the\nchemical sciences. However, chemical sciences also pose the unique challenge of\nvery diverse, small, fuzzy datasets that are difficult to leverage in\nconventional machine learning approaches completely. A new class of models,\ngeneral-purpose models (GPMs) such as large language models, have shown the\nability to solve tasks they have not been directly trained on, and to flexibly\noperate with low amounts of data in different formats. In this review, we\ndiscuss fundamental building principles of GPMs and review recent applications\nof those models in the chemical sciences across the entire scientific process.\nWhile many of these applications are still in the prototype phase, we expect\nthat the increasing interest in GPMs will make many of them mature in the\ncoming years.", "AI": {"tldr": "\u7efc\u8ff0\u8ba8\u8bba\u4e86\u901a\u7528\u6a21\u578b\uff08GPMs\uff09\u5728\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u591a\u6837\u5316\u3001\u5c0f\u89c4\u6a21\u3001\u6a21\u7cca\u6570\u636e\u96c6\u65f6\u7684\u4f18\u52bf\u3002", "motivation": "\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u6570\u636e\u901a\u5e38\u591a\u6837\u5316\u4e14\u89c4\u6a21\u5c0f\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\uff0c\u800c\u901a\u7528\u6a21\u578b\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5c55\u73b0\u4e86\u5904\u7406\u6b64\u7c7b\u6570\u636e\u7684\u6f5c\u529b\u3002", "method": "\u7efc\u8ff0\u4e86\u901a\u7528\u6a21\u578b\u7684\u57fa\u672c\u6784\u5efa\u539f\u5219\uff0c\u5e76\u56de\u987e\u4e86\u5176\u5728\u5316\u5b66\u79d1\u5b66\u4e2d\u5404\u4e2a\u79d1\u5b66\u8fc7\u7a0b\u7684\u5e94\u7528\u3002", "result": "\u5c3d\u7ba1\u8bb8\u591a\u5e94\u7528\u4ecd\u5904\u4e8e\u539f\u578b\u9636\u6bb5\uff0c\u4f46\u901a\u7528\u6a21\u578b\u7684\u5174\u8da3\u589e\u957f\u9884\u8ba1\u5c06\u63a8\u52a8\u5176\u6210\u719f\u3002", "conclusion": "\u901a\u7528\u6a21\u578b\u6709\u671b\u5728\u672a\u6765\u51e0\u5e74\u5185\u6210\u4e3a\u5316\u5b66\u79d1\u5b66\u4e2d\u6570\u636e\u9a71\u52a8\u7814\u7a76\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.07485", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07485", "abs": "https://arxiv.org/abs/2507.07485", "authors": ["Wooseong Jeong", "Kuk-Jin Yoon"], "title": "Resolving Token-Space Gradient Conflicts: Token Space Manipulation for Transformer-Based Multi-Task Learning", "comment": "Accepted at ICCV 2025", "summary": "Multi-Task Learning (MTL) enables multiple tasks to be learned within a\nshared network, but differences in objectives across tasks can cause negative\ntransfer, where the learning of one task degrades another task's performance.\nWhile pre-trained transformers significantly improve MTL performance, their\nfixed network capacity and rigid structure limit adaptability. Previous dynamic\nnetwork architectures attempt to address this but are inefficient as they\ndirectly convert shared parameters into task-specific ones. We propose Dynamic\nToken Modulation and Expansion (DTME-MTL), a framework applicable to any\ntransformer-based MTL architecture. DTME-MTL enhances adaptability and reduces\noverfitting by identifying gradient conflicts in token space and applying\nadaptive solutions based on conflict type. Unlike prior methods that mitigate\nnegative transfer by duplicating network parameters, DTME-MTL operates entirely\nin token space, enabling efficient adaptation without excessive parameter\ngrowth. Extensive experiments demonstrate that DTME-MTL consistently improves\nmulti-task performance with minimal computational overhead, offering a scalable\nand effective solution for enhancing transformer-based MTL models.", "AI": {"tldr": "DTME-MTL\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4ee4\u724c\u8c03\u5236\u4e0e\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u4ee4\u724c\u7a7a\u95f4\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u5e76\u81ea\u9002\u5e94\u89e3\u51b3\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u6027\u80fd\uff0c\u907f\u514d\u53c2\u6570\u5197\u4f59\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u4efb\u52a1\u76ee\u6807\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff0c\u800c\u73b0\u6709\u9884\u8bad\u7ec3Transformer\u7684\u56fa\u5b9a\u7ed3\u6784\u548c\u5bb9\u91cf\u9650\u5236\u4e86\u9002\u5e94\u6027\uff0c\u52a8\u6001\u7f51\u7edc\u67b6\u6784\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faDTME-MTL\u6846\u67b6\uff0c\u5728\u4ee4\u724c\u7a7a\u95f4\u4e2d\u8bc6\u522b\u68af\u5ea6\u51b2\u7a81\u5e76\u81ea\u9002\u5e94\u8c03\u6574\uff0c\u907f\u514d\u76f4\u63a5\u8f6c\u6362\u5171\u4eab\u53c2\u6570\u4e3a\u4efb\u52a1\u7279\u5b9a\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDTME-MTL\u80fd\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "DTME-MTL\u4e3aTransformer\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07511", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07511", "abs": "https://arxiv.org/abs/2507.07511", "authors": ["Joris Suurmeijer", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea"], "title": "Uncertainty Quantification for Motor Imagery BCI -- Machine Learning vs. Deep Learning", "comment": "6 pages, 3 figures", "summary": "Brain-computer interfaces (BCIs) turn brain signals into functionally useful\noutput, but they are not always accurate. A good Machine Learning classifier\nshould be able to indicate how confident it is about a given classification, by\ngiving a probability for its classification. Standard classifiers for Motor\nImagery BCIs do give such probabilities, but research on uncertainty\nquantification has been limited to Deep Learning. We compare the uncertainty\nquantification ability of established BCI classifiers using Common Spatial\nPatterns (CSP-LDA) and Riemannian Geometry (MDRM) to specialized methods in\nDeep Learning (Deep Ensembles and Direct Uncertainty Quantification) as well as\nstandard Convolutional Neural Networks (CNNs).\n  We found that the overconfidence typically seen in Deep Learning is not a\nproblem in CSP-LDA and MDRM. We found that MDRM is underconfident, which we\nsolved by adding Temperature Scaling (MDRM-T). CSP-LDA and MDRM-T give the best\nuncertainty estimates, but Deep Ensembles and standard CNNs give the best\nclassifications. We show that all models are able to separate between easy and\ndifficult estimates, so that we can increase the accuracy of a Motor Imagery\nBCI by rejecting samples that are ambiguous.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e0d\u540cBCI\u5206\u7c7b\u5668\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0CSP-LDA\u548cMDRM-T\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76BCI\u5206\u7c7b\u5668\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ee5\u63d0\u9ad8\u5206\u7c7b\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6bd4\u8f83\u4e86CSP-LDA\u3001MDRM\u3001\u6df1\u5ea6\u96c6\u6210\u3001\u76f4\u63a5\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u53ca\u6807\u51c6CNN\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0a\u7684\u8868\u73b0\u3002", "result": "CSP-LDA\u548cMDRM-T\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u6027\u4e0a\u66f4\u4f18\u3002\u6240\u6709\u6a21\u578b\u5747\u80fd\u533a\u5206\u96be\u6613\u6837\u672c\u3002", "conclusion": "\u901a\u8fc7\u62d2\u7edd\u6a21\u7cca\u6837\u672c\uff0c\u53ef\u4ee5\u63d0\u9ad8BCI\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0cCSP-LDA\u548cMDRM-T\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.07532", "categories": ["cs.LG", "cs.AI", "68T01, 68T07", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.07532", "abs": "https://arxiv.org/abs/2507.07532", "authors": ["Berkant Turan", "Suhrab Asadulla", "David Steinmann", "Wolfgang Stammer", "Sebastian Pokutta"], "title": "Neural Concept Verifier: Scaling Prover-Verifier Games via Concept Encodings", "comment": "16 pages, 4 figures, 8 tables", "summary": "While Prover-Verifier Games (PVGs) offer a promising path toward\nverifiability in nonlinear classification models, they have not yet been\napplied to complex inputs such as high-dimensional images. Conversely, Concept\nBottleneck Models (CBMs) effectively translate such data into interpretable\nconcepts but are limited by their reliance on low-capacity linear predictors.\nIn this work, we introduce the Neural Concept Verifier (NCV), a unified\nframework combining PVGs with concept encodings for interpretable, nonlinear\nclassification in high-dimensional settings. NCV achieves this by utilizing\nrecent minimally supervised concept discovery models to extract structured\nconcept encodings from raw inputs. A prover then selects a subset of these\nencodings, which a verifier -- implemented as a nonlinear predictor -- uses\nexclusively for decision-making. Our evaluations show that NCV outperforms CBM\nand pixel-based PVG classifier baselines on high-dimensional, logically complex\ndatasets and also helps mitigate shortcut behavior. Overall, we demonstrate NCV\nas a promising step toward performative, verifiable AI.", "AI": {"tldr": "NCV\u7ed3\u5408PVGs\u548cCBMs\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u975e\u7ebf\u6027\u5206\u7c7b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u89e3\u51b3PVGs\u672a\u5e94\u7528\u4e8e\u590d\u6742\u8f93\u5165\uff08\u5982\u9ad8\u7ef4\u56fe\u50cf\uff09\u548cCBMs\u4f9d\u8d56\u4f4e\u5bb9\u91cf\u7ebf\u6027\u9884\u6d4b\u5668\u7684\u5c40\u9650\u6027\u3002", "method": "\u5229\u7528\u6982\u5ff5\u53d1\u73b0\u6a21\u578b\u63d0\u53d6\u7ed3\u6784\u5316\u6982\u5ff5\u7f16\u7801\uff0c\u7531\u8bc1\u660e\u8005\u9009\u62e9\u5b50\u96c6\uff0c\u9a8c\u8bc1\u8005\uff08\u975e\u7ebf\u6027\u9884\u6d4b\u5668\uff09\u7528\u4e8e\u51b3\u7b56\u3002", "result": "NCV\u5728\u9ad8\u7ef4\u590d\u6742\u6570\u636e\u96c6\u4e0a\u4f18\u4e8eCBM\u548c\u57fa\u4e8e\u50cf\u7d20\u7684PVG\u57fa\u7ebf\uff0c\u5e76\u51cf\u5c11\u6377\u5f84\u884c\u4e3a\u3002", "conclusion": "NCV\u662f\u5b9e\u73b0\u9ad8\u6027\u80fd\u3001\u53ef\u9a8c\u8bc1AI\u7684\u6709\u524d\u666f\u7684\u4e00\u6b65\u3002"}}
{"id": "2507.07580", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.NA", "65F55, 68T50"], "pdf": "https://arxiv.org/pdf/2507.07580", "abs": "https://arxiv.org/abs/2507.07580", "authors": ["Uliana Parkina", "Maxim Rakhuba"], "title": "COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation", "comment": null, "summary": "Recent studies suggest that context-aware low-rank approximation is a useful\ntool for compression and fine-tuning of modern large-scale neural networks. In\nthis type of approximation, a norm is weighted by a matrix of input\nactivations, significantly improving metrics over the unweighted case.\nNevertheless, existing methods for neural networks suffer from numerical\ninstabilities due to their reliance on classical formulas involving explicit\nGram matrix computation and their subsequent inversion. We demonstrate that\nthis can degrade the approximation quality or cause numerically singular\nmatrices.\n  To address these limitations, we propose a novel inversion-free regularized\nframework that is based entirely on stable decompositions and overcomes the\nnumerical pitfalls of prior art. Our method can handle possible challenging\nscenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when\ninput activation matrices are nearly singular, and even (3) when insufficient\ndata prevents unique approximation. For the latter, we prove that our solution\nconverges to a desired approximation and derive explicit error bounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u5206\u89e3\u7684\u65e0\u9006\u6b63\u5219\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f4e\u79e9\u8fd1\u4f3c\u65b9\u6cd5\u4e2d\u7684\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u663e\u5f0fGram\u77e9\u9635\u8ba1\u7b97\u548c\u9006\u8fd0\u7b97\uff0c\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\uff0c\u5f71\u54cd\u8fd1\u4f3c\u8d28\u91cf\u6216\u4ea7\u751f\u5947\u5f02\u77e9\u9635\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9006\u6b63\u5219\u5316\u6846\u67b6\uff0c\u5b8c\u5168\u57fa\u4e8e\u7a33\u5b9a\u5206\u89e3\uff0c\u89e3\u51b3\u4e86GPU\u5185\u5b58\u4e0d\u8db3\u3001\u8f93\u5165\u6fc0\u6d3b\u77e9\u9635\u63a5\u8fd1\u5947\u5f02\u6216\u6570\u636e\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5b9a\uff0c\u8bc1\u660e\u4e86\u6536\u655b\u6027\u5e76\u63a8\u5bfc\u4e86\u663e\u5f0f\u8bef\u5dee\u754c\u3002", "conclusion": "\u65b0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u79e9\u8fd1\u4f3c\u7684\u6570\u503c\u7a33\u5b9a\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2507.07581", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.07581", "abs": "https://arxiv.org/abs/2507.07581", "authors": ["Michail Kalntis", "Fernando A. Kuipers", "George Iosifidis"], "title": "CHOMET: Conditional Handovers via Meta-Learning", "comment": null, "summary": "Handovers (HOs) are the cornerstone of modern cellular networks for enabling\nseamless connectivity to a vast and diverse number of mobile users. However, as\nmobile networks become more complex with more diverse users and smaller cells,\ntraditional HOs face significant challenges, such as prolonged delays and\nincreased failures. To mitigate these issues, 3GPP introduced conditional\nhandovers (CHOs), a new type of HO that enables the preparation (i.e., resource\nallocation) of multiple cells for a single user to increase the chance of HO\nsuccess and decrease the delays in the procedure. Despite its advantages, CHO\nintroduces new challenges that must be addressed, including efficient resource\nallocation and managing signaling/communication overhead from frequent cell\npreparations and releases. This paper presents a novel framework aligned with\nthe O-RAN paradigm that leverages meta-learning for CHO optimization, providing\nrobust dynamic regret guarantees and demonstrating at least 180% superior\nperformance than other 3GPP benchmarks in volatile signal conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u6761\u4ef6\u5207\u6362\uff08CHO\uff09\uff0c\u5728\u52a8\u6001\u4fe1\u53f7\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e3GPP\u57fa\u51c6180%\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u7f51\u7edc\u590d\u6742\u6027\u548c\u5c0f\u533a\u89c4\u6a21\u7f29\u5c0f\uff0c\u4f20\u7edf\u5207\u6362\u9762\u4e34\u5ef6\u8fdf\u548c\u5931\u8d25\u589e\u52a0\u7684\u95ee\u9898\uff0c\u6761\u4ef6\u5207\u6362\uff08CHO\uff09\u867d\u80fd\u7f13\u89e3\u4f46\u5f15\u5165\u65b0\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0eO-RAN\u8303\u5f0f\u5bf9\u9f50\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4f18\u5316CHO\u7684\u8d44\u6e90\u5206\u914d\u548c\u4fe1\u4ee4\u5f00\u9500\u3002", "result": "\u5728\u52a8\u6001\u4fe1\u53f7\u6761\u4ef6\u4e0b\uff0c\u8be5\u6846\u67b6\u8868\u73b0\u4f18\u4e8e3GPP\u57fa\u51c6\u81f3\u5c11180%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aCHO\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.07582", "categories": ["cs.LG", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.07582", "abs": "https://arxiv.org/abs/2507.07582", "authors": ["Iago Xabier V\u00e1zquez Garc\u00eda", "Damla Partanaz", "Emrullah Fatih Yetkin"], "title": "Improving Clustering on Occupational Text Data through Dimensionality Reduction", "comment": "Preprint, 10 figures", "summary": "In this study, we focused on proposing an optimal clustering mechanism for\nthe occupations defined in the well-known US-based occupational database,\nO*NET. Even though all occupations are defined according to well-conducted\nsurveys in the US, their definitions can vary for different firms and\ncountries. Hence, if one wants to expand the data that is already collected in\nO*NET for the occupations defined with different tasks, a map between the\ndefinitions will be a vital requirement. We proposed a pipeline using several\nBERT-based techniques with various clustering approaches to obtain such a map.\nWe also examined the effect of dimensionality reduction approaches on several\nmetrics used in measuring performance of clustering algorithms. Finally, we\nimproved our results by using a specialized silhouette approach. This new\nclustering-based mapping approach with dimensionality reduction may help\ndistinguish the occupations automatically, creating new paths for people\nwanting to change their careers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBERT\u548c\u805a\u7c7b\u7684\u4f18\u5316\u673a\u5236\uff0c\u7528\u4e8e\u6620\u5c04\u4e0d\u540c\u804c\u4e1a\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u964d\u7ef4\u548c\u4e13\u7528\u8f6e\u5ed3\u65b9\u6cd5\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u804c\u4e1a\u5b9a\u4e49\u5728\u4e0d\u540c\u4f01\u4e1a\u548c\u56fd\u5bb6\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u6620\u5c04\u8fd9\u4e9b\u5dee\u5f02\uff0c\u6269\u5c55O*NET\u6570\u636e\u5e93\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u591a\u79cdBERT\u6280\u672f\u548c\u805a\u7c7b\u65b9\u6cd5\u6784\u5efa\u7ba1\u9053\uff0c\u7ed3\u5408\u964d\u7ef4\u548c\u4e13\u7528\u8f6e\u5ed3\u65b9\u6cd5\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u964d\u7ef4\u548c\u4e13\u7528\u8f6e\u5ed3\u65b9\u6cd5\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u804c\u4e1a\u5b9a\u4e49\u7684\u81ea\u52a8\u533a\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u804c\u4e1a\u8f6c\u6362\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07589", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.07589", "abs": "https://arxiv.org/abs/2507.07589", "authors": ["Arpana Sinhal", "Anay Sinhal", "Amit Sinhal"], "title": "Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data", "comment": null, "summary": "Healthcare professionals, particularly nurses, face elevated occupational\nstress, a concern amplified during the COVID-19 pandemic. While wearable\nsensors offer promising avenues for real-time stress monitoring, existing\nstudies often lack comprehensive datasets and robust analytical frameworks.\nThis study addresses these gaps by introducing a multimodal dataset comprising\nphysiological signals, electrodermal activity, heart rate and skin temperature.\nA systematic literature review identified limitations in prior stress-detection\nmethodologies, particularly in handling class imbalance and optimizing model\ngeneralizability. To overcome these challenges, the dataset underwent\npreprocessing with the Synthetic Minority Over sampling Technique (SMOTE),\nensuring balanced representation of stress states. Advanced machine learning\nmodels including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were\nevaluated and combined into a Stacking Classifier to leverage their collective\npredictive strengths. By using a publicly accessible dataset and a reproducible\nanalytical pipeline, this work advances the development of deployable\nstress-monitoring systems, offering practical implications for safeguarding\nhealthcare workers' mental health. Future research directions include expanding\ndemographic diversity and exploring edge-computing implementations for low\nlatency stress alerts.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u6539\u8fdb\u4e86\u533b\u62a4\u4eba\u5458\u5b9e\u65f6\u538b\u529b\u76d1\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u548c\u6a21\u578b\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u533b\u62a4\u4eba\u5458\uff08\u5c24\u5176\u662f\u62a4\u58eb\uff09\u9762\u4e34\u9ad8\u804c\u4e1a\u538b\u529b\uff0cCOVID-19\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u6570\u636e\u548c\u5f3a\u5927\u5206\u6790\u6846\u67b6\u3002", "method": "\u4f7f\u7528SMOTE\u9884\u5904\u7406\u6570\u636e\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u3001XGBoost\u548cMLP\u6a21\u578b\uff0c\u6784\u5efa\u5806\u53e0\u5206\u7c7b\u5668\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u90e8\u7f72\u7684\u538b\u529b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4e3a\u533b\u62a4\u4eba\u5458\u5fc3\u7406\u5065\u5eb7\u63d0\u4f9b\u652f\u6301\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u6269\u5927\u4eba\u53e3\u591a\u6837\u6027\u548c\u63a2\u7d22\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u538b\u529b\u8b66\u62a5\u3002"}}
{"id": "2507.07604", "categories": ["cs.LG", "q-bio.QM", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2507.07604", "abs": "https://arxiv.org/abs/2507.07604", "authors": ["Sebastian Lotter", "Elisabeth Mohr", "Andrina Rutsch", "Lukas Brand", "Francesca Ronchi", "Laura D\u00edaz-Marug\u00e1n"], "title": "Synthetic MC via Biological Transmitters: Therapeutic Modulation of the Gut-Brain Axis", "comment": null, "summary": "Synthetic molecular communication (SMC) is a key enabler for future\nhealthcare systems in which Internet of Bio-Nano-Things (IoBNT) devices\nfacilitate the continuous monitoring of a patient's biochemical signals. To\nclose the loop between sensing and actuation, both the detection and the\ngeneration of in-body molecular communication (MC) signals is key. However,\ngenerating signals inside the human body, e.g., via synthetic nanodevices,\nposes a challenge in SMC, due to technological obstacles as well as legal,\nsafety, and ethical issues. Hence, this paper considers an SMC system in which\nsignals are generated indirectly via the modulation of a natural in-body MC\nsystem, namely the gut-brain axis (GBA). Therapeutic GBA modulation is already\nestablished as treatment for neurological diseases, e.g., drug refractory\nepilepsy (DRE), and performed via the administration of nutritional supplements\nor specific diets. However, the molecular signaling pathways that mediate the\neffect of such treatments are mostly unknown. Consequently, existing treatments\nare standardized or designed heuristically and able to help only some patients\nwhile failing to help others. In this paper, we propose to leverage personal\nhealth data, e.g., gathered by in-body IoBNT devices, to design more versatile\nand robust GBA modulation-based treatments as compared to the existing ones. To\nshow the feasibility of our approach, we define a catalog of theoretical\nrequirements for therapeutic GBA modulation. Then, we propose a machine\nlearning model to verify these requirements for practical scenarios when only\nlimited data on the GBA modulation exists. By evaluating the proposed model on\nseveral datasets, we confirm its excellent accuracy in identifying different\nmodulators of the GBA. Finally, we utilize the proposed model to identify\nspecific modulatory pathways that play an important role for therapeutic GBA\nmodulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80a0\u9053-\u8111\u8f74\uff08GBA\uff09\u95f4\u63a5\u751f\u6210\u5206\u5b50\u4fe1\u53f7\u7684\u5408\u6210\u5206\u5b50\u901a\u4fe1\uff08SMC\uff09\u7cfb\u7edf\uff0c\u5229\u7528\u4e2a\u4eba\u5065\u5eb7\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u6cbb\u7597\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6cbb\u7597\u65b9\u6cd5\u6807\u51c6\u5316\u4e14\u6548\u679c\u6709\u9650\uff0c\u7f3a\u4e4f\u5bf9\u5206\u5b50\u4fe1\u53f7\u901a\u8def\u7684\u7406\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e2a\u6027\u5316\u7684GBA\u8c03\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5229\u7528\u4e2a\u4eba\u5065\u5eb7\u6570\u636e\u8bbe\u8ba1GBA\u8c03\u5236\u6cbb\u7597\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9a8c\u8bc1\u7406\u8bba\u8981\u6c42\u3002", "result": "\u6a21\u578b\u5728\u8bc6\u522bGBA\u8c03\u8282\u5242\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u6210\u529f\u8bc6\u522b\u4e86\u7279\u5b9a\u8c03\u8282\u901a\u8def\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e2a\u6027\u5316GBA\u8c03\u5236\u6cbb\u7597\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\uff0c\u63d0\u5347\u4e86\u6cbb\u7597\u6548\u679c\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2507.07613", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07613", "abs": "https://arxiv.org/abs/2507.07613", "authors": ["Davide Domini", "Laura Erhan", "Gianluca Aguzzi", "Lucia Cavallaro", "Amirhossein Douzandeh Zenoozi", "Antonio Liotta", "Mirko Viroli"], "title": "Sparse Self-Federated Learning for Energy Efficient Cooperative Intelligence in Society 5.0", "comment": null, "summary": "Federated Learning offers privacy-preserving collaborative intelligence but\nstruggles to meet the sustainability demands of emerging IoT ecosystems\nnecessary for Society 5.0-a human-centered technological future balancing\nsocial advancement with environmental responsibility. The excessive\ncommunication bandwidth and computational resources required by traditional FL\napproaches make them environmentally unsustainable at scale, creating a\nfundamental conflict with green AI principles as billions of\nresource-constrained devices attempt to participate. To this end, we introduce\nSparse Proximity-based Self-Federated Learning (SParSeFuL), a resource-aware\napproach that bridges this gap by combining aggregate computing for\nself-organization with neural network sparsification to reduce energy and\nbandwidth consumption.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSParSeFuL\u7684\u7a00\u758f\u90bb\u8fd1\u81ea\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u8d44\u6e90\u6d88\u8017\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u6ee1\u8db3Society 5.0\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u9700\u6c42\u65f6\u9762\u4e34\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u7269\u8054\u7f51\u8bbe\u5907\u53c2\u4e0e\u65f6\u3002", "method": "SParSeFuL\u7ed3\u5408\u4e86\u805a\u5408\u8ba1\u7b97\u548c\u795e\u7ecf\u7f51\u7edc\u7a00\u758f\u5316\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u80fd\u6e90\u548c\u5e26\u5bbd\u6d88\u8017\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u7ec4\u7ec7\u548c\u7a00\u758f\u5316\u4f18\u5316\u4e86\u8d44\u6e90\u4f7f\u7528\uff0c\u63d0\u5347\u4e86\u53ef\u6301\u7eed\u6027\u3002", "conclusion": "SParSeFuL\u4e3a\u7eff\u8272AI\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u6280\u672f\u8fdb\u6b65\u4e0e\u73af\u5883\u4fdd\u62a4\u3002"}}
{"id": "2507.07621", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07621", "abs": "https://arxiv.org/abs/2507.07621", "authors": ["Junyu Luo", "Yuhao Tang", "Yiwei Fu", "Xiao Luo", "Zhizhuo Kou", "Zhiping Xiao", "Wei Ju", "Wentao Zhang", "Ming Zhang"], "title": "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation", "comment": "ICML 2025", "summary": "Unsupervised Graph Domain Adaptation (UGDA) leverages labeled source domain\ngraphs to achieve effective performance in unlabeled target domains despite\ndistribution shifts. However, existing methods often yield suboptimal results\ndue to the entanglement of causal-spurious features and the failure of global\nalignment strategies. We propose SLOGAN (Sparse Causal Discovery with\nGenerative Intervention), a novel approach that achieves stable graph\nrepresentation transfer through sparse causal modeling and dynamic intervention\nmechanisms. Specifically, SLOGAN first constructs a sparse causal graph\nstructure, leveraging mutual information bottleneck constraints to disentangle\nsparse, stable causal features while compressing domain-dependent spurious\ncorrelations through variational inference. To address residual spurious\ncorrelations, we innovatively design a generative intervention mechanism that\nbreaks local spurious couplings through cross-domain feature recombination\nwhile maintaining causal feature semantic consistency via covariance\nconstraints. Furthermore, to mitigate error accumulation in target domain\npseudo-labels, we introduce a category-adaptive dynamic calibration strategy,\nensuring stable discriminative learning. Extensive experiments on multiple\nreal-world datasets demonstrate that SLOGAN significantly outperforms existing\nbaselines.", "AI": {"tldr": "SLOGAN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u56e0\u679c\u5efa\u6a21\u548c\u52a8\u6001\u5e72\u9884\u673a\u5236\u7684\u56fe\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u56e0\u679c-\u865a\u5047\u7279\u5f81\u7ea0\u7f20\u548c\u5168\u5c40\u5bf9\u9f50\u7b56\u7565\u5931\u8d25\u5bfc\u81f4\u7684\u6027\u80fd\u4e0d\u4f73\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u56fe\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u56e0\u56e0\u679c\u4e0e\u865a\u5047\u7279\u5f81\u7ea0\u7f20\u53ca\u5168\u5c40\u5bf9\u9f50\u7b56\u7565\u5931\u6548\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "SLOGAN\u901a\u8fc7\u7a00\u758f\u56e0\u679c\u5efa\u6a21\u548c\u52a8\u6001\u5e72\u9884\u673a\u5236\uff0c\u5305\u62ec\u7a00\u758f\u56e0\u679c\u56fe\u6784\u5efa\u3001\u751f\u6210\u5e72\u9884\u673a\u5236\u548c\u7c7b\u522b\u81ea\u9002\u5e94\u52a8\u6001\u6821\u51c6\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cSLOGAN\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SLOGAN\u901a\u8fc7\u7a00\u758f\u56e0\u679c\u5efa\u6a21\u548c\u52a8\u6001\u5e72\u9884\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u56fe\u8868\u793a\u8fc1\u79fb\uff0c\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u56fe\u57df\u81ea\u9002\u5e94\u7684\u6027\u80fd\u3002"}}
{"id": "2507.07622", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07622", "abs": "https://arxiv.org/abs/2507.07622", "authors": ["Federico Del Pup", "Riccardo Brun", "Filippo Iotti", "Edoardo Paccagnella", "Mattia Pezzato", "Sabrina Bertozzo", "Andrea Zanola", "Louis Fabrice Tshimanga", "Henning M\u00fcller", "Manfredo Atzori"], "title": "TransformEEG: Towards Improving Model Generalizability in Deep Learning-based EEG Parkinson's Disease Detection", "comment": "Submitted for possible publication. GitHub repository: see\n  https://github.com/MedMaxLab/transformeeg", "summary": "Electroencephalography (EEG) is establishing itself as an important,\nlow-cost, noninvasive diagnostic tool for the early detection of Parkinson's\nDisease (PD). In this context, EEG-based Deep Learning (DL) models have shown\npromising results due to their ability to discover highly nonlinear patterns\nwithin the signal. However, current state-of-the-art DL models suffer from poor\ngeneralizability caused by high inter-subject variability. This high\nvariability underscores the need for enhancing model generalizability by\ndeveloping new architectures better tailored to EEG data. This paper introduces\nTransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's\ndisease detection using EEG data. Unlike transformer models based on the EEGNet\nstructure, TransformEEG incorporates a depthwise convolutional tokenizer. This\ntokenizer is specialized in generating tokens composed by channel-specific\nfeatures, which enables more effective feature mixing within the self-attention\nlayers of the transformer encoder. To evaluate the proposed model, four public\ndatasets comprising 290 subjects (140 PD patients, 150 healthy controls) were\nharmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out\n(N-LNSO) cross-validation was performed to provide an unbiased comparison\nagainst seven other consolidated EEG deep learning models. TransformEEG\nachieved the highest balanced accuracy's median (78.45%) as well as the lowest\ninterquartile range (6.37%) across all the N-LNSO partitions. When combined\nwith data augmentation and threshold correction, median accuracy increased to\n80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG\nproduces more consistent and less skewed results. It demonstrates a substantial\nreduction in variability and more reliable PD detection using EEG data compared\nto the other investigated models.", "AI": {"tldr": "TransformEEG\u662f\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u548cTransformer\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7EEG\u6570\u636e\u68c0\u6d4b\u5e15\u91d1\u68ee\u75c5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u66f4\u4f4e\u7684\u53d8\u5f02\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eEEG\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u53d7\u8bd5\u8005\u95f4\u9ad8\u53d8\u5f02\u6027\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5408EEG\u6570\u636e\u7684\u65b0\u67b6\u6784\u3002", "method": "\u63d0\u51faTransformEEG\u6a21\u578b\uff0c\u91c7\u7528\u6df1\u5ea6\u5377\u79ef\u5206\u8bcd\u5668\u751f\u6210\u901a\u9053\u7279\u5b9a\u7279\u5f81\uff0c\u7ed3\u5408Transformer\u7f16\u7801\u5668\u8fdb\u884c\u7279\u5f81\u6df7\u5408\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0cTransformEEG\u7684\u4e2d\u4f4d\u6570\u5e73\u8861\u51c6\u786e\u7387\u4e3a78.45%\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u9608\u503c\u6821\u6b63\u540e\u63d0\u5347\u81f380.10%\u3002", "conclusion": "TransformEEG\u5728EEG\u6570\u636e\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4e00\u81f4\u3001\u66f4\u53ef\u9760\u7684\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53d8\u5f02\u6027\u3002"}}
{"id": "2507.07637", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07637", "abs": "https://arxiv.org/abs/2507.07637", "authors": ["Carlos Beis Penedo", "Rebeca P. D\u00edaz Redondo", "Ana Fern\u00e1ndez Vilas", "Manuel Fern\u00e1ndez Veiga", "Francisco Troncoso Pastoriza"], "title": "HLF-FSL. A Decentralized Federated Split Learning Solution for IoT on Hyperledger Fabric", "comment": "19 pages, 7 figures and 6 tables", "summary": "Collaborative machine learning in sensitive domains demands scalable, privacy\npreserving solutions for enterprise deployment. Conventional Federated Learning\n(FL) relies on a central server, introducing single points of failure and\nprivacy risks, while Split Learning (SL) partitions models for privacy but\nscales poorly due to sequential training. We present a decentralized\narchitecture that combines Federated Split Learning (FSL) with the permissioned\nblockchain Hyperledger Fabric (HLF). Our chaincode orchestrates FSL's split\nmodel execution and peer-to-peer aggregation without any central coordinator,\nleveraging HLF's transient fields and Private Data Collections (PDCs) to keep\nraw data and model activations private. On CIFAR-10 and MNIST benchmarks,\nHLF-FSL matches centralized FSL accuracy while reducing per epoch training time\ncompared to Ethereum-based works. Performance and scalability tests show\nminimal blockchain overhead and preserved accuracy, demonstrating enterprise\ngrade viability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5206\u5272\u5b66\u4e60\uff08FSL\uff09\u4e0eHyperledger Fabric\uff08HLF\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u4e2d\u5fc3\u5316\u98ce\u9669\u548c\u5206\u5272\u5b66\u4e60\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u4e86\u9690\u79c1\u548c\u6027\u80fd\u3002", "motivation": "\u5728\u654f\u611f\u9886\u57df\uff0c\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u9700\u8981\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u9690\u79c1\u98ce\u9669\uff0c\u800c\u5206\u5272\u5b66\u4e60\u56e0\u987a\u5e8f\u8bad\u7ec3\u6269\u5c55\u6027\u5dee\u3002", "method": "\u91c7\u7528FSL\u4e0eHLF\u7ed3\u5408\u7684\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u5229\u7528HLF\u7684\u77ac\u6001\u5b57\u6bb5\u548c\u79c1\u6709\u6570\u636e\u96c6\u5408\uff08PDCs\uff09\u4fdd\u62a4\u6570\u636e\u548c\u6a21\u578b\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u65e0\u4e2d\u5fc3\u534f\u8c03\u7684\u5206\u5272\u6a21\u578b\u6267\u884c\u548c\u70b9\u5bf9\u70b9\u805a\u5408\u3002", "result": "\u5728CIFAR-10\u548cMNIST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHLF-FSL\u8fbe\u5230\u4e0e\u4e2d\u5fc3\u5316FSL\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6bcf\u8f6e\u8bad\u7ec3\u65f6\u95f4\uff0c\u6027\u80fd\u6d4b\u8bd5\u663e\u793a\u533a\u5757\u94fe\u5f00\u9500\u6781\u5c0f\u4e14\u51c6\u786e\u6027\u4fdd\u6301\u3002", "conclusion": "HLF-FSL\u5c55\u793a\u4e86\u4f01\u4e1a\u7ea7\u53ef\u884c\u6027\uff0c\u4e3a\u654f\u611f\u9886\u57df\u7684\u534f\u4f5c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07675", "abs": "https://arxiv.org/abs/2507.07675", "authors": ["Darshan Makwana"], "title": "Some Theoretical Results on Layerwise Effective Dimension Oscillations in Finite Width ReLU Networks", "comment": null, "summary": "We analyze the layerwise effective dimension (rank of the feature matrix) in\nfully-connected ReLU networks of finite width. Specifically, for a fixed batch\nof $m$ inputs and random Gaussian weights, we derive closed-form expressions\nfor the expected rank of the \\$m\\times n\\$ hidden activation matrices. Our main\nresult shows that $\\mathbb{E}[EDim(\\ell)]=m[1-(1-2/\\pi)^\\ell]+O(e^{-c m})$ so\nthat the rank deficit decays geometrically with ratio $1-2 / \\pi \\approx\n0.3634$. We also prove a sub-Gaussian concentration bound, and identify the\n\"revival\" depths at which the expected rank attains local maxima. In\nparticular, these peaks occur at depths\n$\\ell_k^*\\approx(k+1/2)\\pi/\\log(1/\\rho)$ with height $\\approx (1-e^{-\\pi/2}) m\n\\approx 0.79m$. We further show that this oscillatory rank behavior is a\nfinite-width phenomenon: under orthogonal weight initialization or strong\nnegative-slope leaky-ReLU, the rank remains (nearly) full. These results\nprovide a precise characterization of how random ReLU layers alternately\ncollapse and partially revive the subspace of input variations, adding nuance\nto prior work on expressivity of deep networks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6709\u9650\u5bbd\u5ea6\u5168\u8fde\u63a5ReLU\u7f51\u7edc\u4e2d\u6bcf\u5c42\u7684\u6709\u6548\u7ef4\u5ea6\uff08\u7279\u5f81\u77e9\u9635\u7684\u79e9\uff09\uff0c\u63a8\u5bfc\u4e86\u968f\u673a\u9ad8\u65af\u6743\u91cd\u4e0b\u9690\u85cf\u6fc0\u6d3b\u77e9\u9635\u7684\u671f\u671b\u79e9\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u53d1\u73b0\u79e9\u7684\u8870\u51cf\u5448\u51e0\u4f55\u7ea7\u6570\uff0c\u5e76\u8bc6\u522b\u4e86\u79e9\u5c40\u90e8\u6781\u5927\u503c\u7684\u201c\u590d\u5174\u201d\u6df1\u5ea6\u3002", "motivation": "\u7406\u89e3\u968f\u673aReLU\u5c42\u5982\u4f55\u4ea4\u66ff\u538b\u7f29\u548c\u90e8\u5206\u6062\u590d\u8f93\u5165\u53d8\u5316\u7684\u5b50\u7a7a\u95f4\uff0c\u4e3a\u6df1\u5ea6\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u7684\u7814\u7a76\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u89c6\u89d2\u3002", "method": "\u56fa\u5b9a\u4e00\u6279\u8f93\u5165\u548c\u968f\u673a\u9ad8\u65af\u6743\u91cd\uff0c\u63a8\u5bfc\u9690\u85cf\u6fc0\u6d3b\u77e9\u9635\u7684\u671f\u671b\u79e9\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u5206\u6790\u5176\u51e0\u4f55\u8870\u51cf\u548c\u5c40\u90e8\u6781\u5927\u503c\u73b0\u8c61\u3002", "result": "\u79e9\u7684\u671f\u671b\u503c\u5448\u51e0\u4f55\u8870\u51cf\uff0c\u5c40\u90e8\u6781\u5927\u503c\u51fa\u73b0\u5728\u7279\u5b9a\u6df1\u5ea6\uff0c\u4e14\u8fd9\u79cd\u73b0\u8c61\u5728\u6b63\u4ea4\u6743\u91cd\u521d\u59cb\u5316\u6216\u5f3a\u8d1f\u659c\u7387Leaky-ReLU\u4e0b\u6d88\u5931\u3002", "conclusion": "\u968f\u673aReLU\u5c42\u5728\u6709\u9650\u5bbd\u5ea6\u4e0b\u8868\u73b0\u51fa\u79e9\u7684\u632f\u8361\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u8f93\u5165\u5b50\u7a7a\u95f4\u7684\u52a8\u6001\u53d8\u5316\uff0c\u4e3a\u7f51\u7edc\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u3002"}}
{"id": "2507.07712", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07712", "abs": "https://arxiv.org/abs/2507.07712", "authors": ["Zhuang Qi", "Lei Meng", "Han Yu"], "title": "Balancing the Past and Present: A Coordinated Replay Framework for Federated Class-Incremental Learning", "comment": null, "summary": "Federated Class Incremental Learning (FCIL) aims to collaboratively process\ncontinuously increasing incoming tasks across multiple clients. Among various\napproaches, data replay has become a promising solution, which can alleviate\nforgetting by reintroducing representative samples from previous tasks.\nHowever, their performance is typically limited by class imbalance, both within\nthe replay buffer due to limited global awareness and between replayed and\nnewly arrived classes. To address this issue, we propose a class wise balancing\ndata replay method for FCIL (FedCBDR), which employs a global coordination\nmechanism for class-level memory construction and reweights the learning\nobjective to alleviate the aforementioned imbalances. Specifically, FedCBDR has\ntwo key components: 1) the global-perspective data replay module reconstructs\nglobal representations of prior task in a privacy-preserving manner, which then\nguides a class-aware and importance-sensitive sampling strategy to achieve\nbalanced replay; 2) Subsequently, to handle class imbalance across tasks, the\ntask aware temperature scaling module adaptively adjusts the temperature of\nlogits at both class and instance levels based on task dynamics, which reduces\nthe model's overconfidence in majority classes while enhancing its sensitivity\nto minority classes. Experimental results verified that FedCBDR achieves\nbalanced class-wise sampling under heterogeneous data distributions and\nimproves generalization under task imbalance between earlier and recent tasks,\nyielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods.", "AI": {"tldr": "FedCBDR\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u7c7b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u7c7b\u5e73\u8861\u6570\u636e\u56de\u653e\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u534f\u8c03\u673a\u5236\u548c\u4efb\u52a1\u611f\u77e5\u6e29\u5ea6\u7f29\u653e\u6a21\u5757\u89e3\u51b3\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u7c7b\u589e\u91cf\u5b66\u4e60\uff08FCIL\uff09\u4e2d\uff0c\u6570\u636e\u56de\u653e\u65b9\u6cd5\u56e0\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff08\u5305\u62ec\u56de\u653e\u7f13\u51b2\u533a\u5185\u548c\u4efb\u52a1\u95f4\uff09\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u5e73\u8861\u7b56\u7565\u3002", "method": "1) \u5168\u5c40\u89c6\u89d2\u6570\u636e\u56de\u653e\u6a21\u5757\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u65b9\u5f0f\u91cd\u6784\u5148\u524d\u4efb\u52a1\u7684\u5168\u5c40\u8868\u793a\uff0c\u6307\u5bfc\u7c7b\u611f\u77e5\u548c\u91cd\u8981\u6027\u654f\u611f\u7684\u91c7\u6837\u7b56\u7565\uff1b2) \u4efb\u52a1\u611f\u77e5\u6e29\u5ea6\u7f29\u653e\u6a21\u5757\u6839\u636e\u4efb\u52a1\u52a8\u6001\u8c03\u6574logits\u6e29\u5ea6\uff0c\u51cf\u5c11\u5bf9\u591a\u6570\u7c7b\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedCBDR\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u5b9e\u73b0\u4e86\u7c7b\u5e73\u8861\u91c7\u6837\uff0c\u5e76\u5728\u4efb\u52a1\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0cTop-1\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e862%-15%\u3002", "conclusion": "FedCBDR\u901a\u8fc7\u5168\u5c40\u534f\u8c03\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86FCIL\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.07735", "categories": ["cs.LG", "cs.CL", "cs.CR", "I.2.7; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.07735", "abs": "https://arxiv.org/abs/2507.07735", "authors": ["Peiyan Zhang", "Haibo Jin", "Liying Kang", "Haohan Wang"], "title": "GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing", "comment": "24 pages", "summary": "Jailbreak attacks reveal critical vulnerabilities in Large Language Models\n(LLMs) by causing them to generate harmful or unethical content. Evaluating\nthese threats is particularly challenging due to the evolving nature of LLMs\nand the sophistication required in effectively probing their vulnerabilities.\nCurrent benchmarks and evaluation methods struggle to fully address these\nchallenges, leaving gaps in the assessment of LLM vulnerabilities. In this\npaper, we review existing jailbreak evaluation practices and identify three\nassumed desiderata for an effective jailbreak evaluation protocol. To address\nthese challenges, we introduce GuardVal, a new evaluation protocol that\ndynamically generates and refines jailbreak prompts based on the defender LLM's\nstate, providing a more accurate assessment of defender LLMs' capacity to\nhandle safety-critical situations. Moreover, we propose a new optimization\nmethod that prevents stagnation during prompt refinement, ensuring the\ngeneration of increasingly effective jailbreak prompts that expose deeper\nweaknesses in the defender LLMs. We apply this protocol to a diverse set of\nmodels, from Mistral-7b to GPT-4, across 10 safety domains. Our findings\nhighlight distinct behavioral patterns among the models, offering a\ncomprehensive view of their robustness. Furthermore, our evaluation process\ndeepens the understanding of LLM behavior, leading to insights that can inform\nfuture research and drive the development of more secure models.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86GuardVal\uff0c\u4e00\u79cd\u52a8\u6001\u751f\u6210\u548c\u4f18\u5316\u8d8a\u72f1\u63d0\u793a\u7684\u65b0\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30LLMs\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8bc4\u4f30LLMs\u7684\u6f0f\u6d1e\uff0c\u5c24\u5176\u662f\u5728\u8d8a\u72f1\u653b\u51fb\u65b9\u9762\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faGuardVal\u534f\u8bae\uff0c\u52a8\u6001\u751f\u6210\u548c\u4f18\u5316\u8d8a\u72f1\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u9632\u505c\u6ede\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\uff08\u5982Mistral-7b\u548cGPT-4\uff09\u4e0a\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5f0f\u53ca\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "GuardVal\u4e3aLLMs\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u6a21\u578b\u3002"}}
{"id": "2507.07738", "categories": ["cs.LG", "econ.EM"], "pdf": "https://arxiv.org/pdf/2507.07738", "abs": "https://arxiv.org/abs/2507.07738", "authors": ["Tomu Hirata", "Undral Byambadalai", "Tatsushi Oka", "Shota Yasui", "Shingo Uto"], "title": "Efficient and Scalable Estimation of Distributional Treatment Effects with Multi-Task Neural Networks", "comment": null, "summary": "We propose a novel multi-task neural network approach for estimating\ndistributional treatment effects (DTE) in randomized experiments. While DTE\nprovides more granular insights into the experiment outcomes over conventional\nmethods focusing on the Average Treatment Effect (ATE), estimating it with\nregression adjustment methods presents significant challenges. Specifically,\nprecision in the distribution tails suffers due to data imbalance, and\ncomputational inefficiencies arise from the need to solve numerous regression\nproblems, particularly in large-scale datasets commonly encountered in\nindustry. To address these limitations, our method leverages multi-task neural\nnetworks to estimate conditional outcome distributions while incorporating\nmonotonic shape constraints and multi-threshold label learning to enhance\naccuracy. To demonstrate the practical effectiveness of our proposed method, we\napply our method to both simulated and real-world datasets, including a\nrandomized field experiment aimed at reducing water consumption in the US and a\nlarge-scale A/B test from a leading streaming platform in Japan. The\nexperimental results consistently demonstrate superior performance across\nvarious datasets, establishing our method as a robust and practical solution\nfor modern causal inference applications requiring a detailed understanding of\ntreatment effect heterogeneity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4efb\u52a1\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u968f\u673a\u5b9e\u9a8c\u4e2d\u7684\u5206\u5e03\u5904\u7406\u6548\u5e94\uff08DTE\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\uff0c\u800cDTE\u80fd\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b9e\u9a8c\u7ed3\u679c\u5206\u6790\uff0c\u4f46\u73b0\u6709\u56de\u5f52\u8c03\u6574\u65b9\u6cd5\u5728\u5c3e\u90e8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u5229\u7528\u591a\u4efb\u52a1\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u6761\u4ef6\u7ed3\u679c\u5206\u5e03\uff0c\u7ed3\u5408\u5355\u8c03\u5f62\u72b6\u7ea6\u675f\u548c\u591a\u9608\u503c\u6807\u7b7e\u5b66\u4e60\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982\u7f8e\u56fd\u8282\u6c34\u5b9e\u9a8c\u548c\u65e5\u672c\u6d41\u5a92\u4f53\u5e73\u53f0A/B\u6d4b\u8bd5\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u4ee3\u56e0\u679c\u63a8\u65ad\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u8be6\u7ec6\u7406\u89e3\u5904\u7406\u6548\u5e94\u5f02\u8d28\u6027\u7684\u573a\u666f\u3002"}}
{"id": "2507.07754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07754", "abs": "https://arxiv.org/abs/2507.07754", "authors": ["Jaeheun Jung", "Bosung Jung", "Suhyun Bae", "Donghun Lee"], "title": "OPC: One-Point-Contraction Unlearning Toward Deep Feature Forgetting", "comment": null, "summary": "Machine unlearning seeks to remove the influence of particular data or class\nfrom trained models to meet privacy, legal, or ethical requirements. Existing\nunlearning methods tend to forget shallowly: phenomenon of an unlearned model\npretend to forget by adjusting only the model response, while its internal\nrepresentations retain information sufficiently to restore the forgotten data\nor behavior. We empirically confirm the widespread shallowness by reverting the\nforgetting effect of various unlearning methods via training-free performance\nrecovery attack and gradient-inversion-based data reconstruction attack. To\naddress this vulnerability fundamentally, we define a theoretical criterion of\n``deep forgetting'' based on one-point-contraction of feature representations\nof data to forget. We also propose an efficient approximation algorithm, and\nuse it to construct a novel general-purpose unlearning algorithm:\nOne-Point-Contraction (OPC). Empirical evaluations on image classification\nunlearning benchmarks show that OPC achieves not only effective unlearning\nperformance but also superior resilience against both performance recovery\nattack and gradient-inversion attack. The distinctive unlearning performance of\nOPC arises from the deep feature forgetting enforced by its theoretical\nfoundation, and recaps the need for improved robustness of machine unlearning\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u673a\u5668\u9057\u5fd8\u65b9\u6cd5OPC\uff0c\u901a\u8fc7\u6df1\u5c42\u7279\u5f81\u9057\u5fd8\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u6d45\u5c42\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u5176\u5bf9\u6297\u6027\u80fd\u6062\u590d\u548c\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u4ec5\u8c03\u6574\u6a21\u578b\u54cd\u5e94\uff0c\u5185\u90e8\u8868\u5f81\u4ecd\u4fdd\u7559\u88ab\u9057\u5fd8\u6570\u636e\u4fe1\u606f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9690\u79c1\u3001\u6cd5\u5f8b\u6216\u4f26\u7406\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5355\u70b9\u6536\u7f29\u7279\u5f81\u8868\u793a\u7684\u6df1\u5c42\u9057\u5fd8\u7406\u8bba\u51c6\u5219\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u8fd1\u4f3c\u7b97\u6cd5OPC\u3002", "result": "OPC\u5728\u56fe\u50cf\u5206\u7c7b\u9057\u5fd8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u5bf9\u6297\u6027\u80fd\u6062\u590d\u548c\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u3002", "conclusion": "OPC\u901a\u8fc7\u6df1\u5c42\u7279\u5f81\u9057\u5fd8\u63d0\u5347\u673a\u5668\u9057\u5fd8\u7684\u9c81\u68d2\u6027\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.07768", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07768", "abs": "https://arxiv.org/abs/2507.07768", "authors": ["Tejaswini Medi", "Steffen Jung", "Margret Keuper"], "title": "TRIX- Trading Adversarial Fairness via Mixed Adversarial Training", "comment": null, "summary": "Adversarial Training (AT) is a widely adopted defense against adversarial\nexamples. However, existing approaches typically apply a uniform training\nobjective across all classes, overlooking disparities in class-wise\nvulnerability. This results in adversarial unfairness: classes with well\ndistinguishable features (strong classes) tend to become more robust, while\nclasses with overlapping or shared features(weak classes) remain\ndisproportionately susceptible to adversarial attacks. We observe that strong\nclasses do not require strong adversaries during training, as their non-robust\nfeatures are quickly suppressed. In contrast, weak classes benefit from\nstronger adversaries to effectively reduce their vulnerabilities. Motivated by\nthis, we introduce TRIX, a feature-aware adversarial training framework that\nadaptively assigns weaker targeted adversaries to strong classes, promoting\nfeature diversity via uniformly sampled targets, and stronger untargeted\nadversaries to weak classes, enhancing their focused robustness. TRIX further\nincorporates per-class loss weighting and perturbation strength adjustments,\nbuilding on prior work, to emphasize weak classes during the optimization.\nComprehensive experiments on standard image classification benchmarks,\nincluding evaluations under strong attacks such as PGD and AutoAttack,\ndemonstrate that TRIX significantly improves worst-case class accuracy on both\nclean and adversarial data, reducing inter-class robustness disparities, and\npreserves overall accuracy. Our results highlight TRIX as a practical step\ntoward fair and effective adversarial defense.", "AI": {"tldr": "TRIX\u662f\u4e00\u79cd\u7279\u5f81\u611f\u77e5\u7684\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u4e0d\u540c\u5f3a\u5ea6\u7684\u5bf9\u6297\u6837\u672c\uff0c\u51cf\u5c11\u7c7b\u522b\u95f4\u9c81\u68d2\u6027\u5dee\u5f02\uff0c\u63d0\u5347\u6700\u5dee\u7c7b\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u6240\u6709\u7c7b\u522b\u91c7\u7528\u7edf\u4e00\u76ee\u6807\uff0c\u5ffd\u89c6\u4e86\u7c7b\u522b\u95f4\u8106\u5f31\u6027\u5dee\u5f02\uff0c\u5bfc\u81f4\u5bf9\u6297\u4e0d\u516c\u5e73\u6027\u3002\u5f3a\u7c7b\u522b\u53d8\u5f97\u66f4\u9c81\u68d2\uff0c\u800c\u5f31\u7c7b\u522b\u4ecd\u6613\u53d7\u653b\u51fb\u3002", "method": "TRIX\u4e3a\u5f3a\u7c7b\u522b\u5206\u914d\u8f83\u5f31\u7684\u9488\u5bf9\u6027\u5bf9\u6297\u6837\u672c\u4ee5\u4fc3\u8fdb\u7279\u5f81\u591a\u6837\u6027\uff0c\u4e3a\u5f31\u7c7b\u522b\u5206\u914d\u66f4\u5f3a\u7684\u975e\u9488\u5bf9\u6027\u5bf9\u6297\u6837\u672c\u4ee5\u589e\u5f3a\u5176\u9c81\u68d2\u6027\uff0c\u5e76\u7ed3\u5408\u635f\u5931\u52a0\u6743\u548c\u6270\u52a8\u5f3a\u5ea6\u8c03\u6574\u3002", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTRIX\u663e\u8457\u63d0\u5347\u4e86\u6700\u5dee\u7c7b\u522b\u51c6\u786e\u7387\uff0c\u51cf\u5c11\u4e86\u7c7b\u522b\u95f4\u9c81\u68d2\u6027\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "TRIX\u662f\u5b9e\u73b0\u516c\u5e73\u4e14\u6709\u6548\u5bf9\u6297\u9632\u5fa1\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2507.07778", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07778", "abs": "https://arxiv.org/abs/2507.07778", "authors": ["Wooseong Jeong", "Jegyeong Cho", "Youngho Yoon", "Kuk-Jin Yoon"], "title": "Synchronizing Task Behavior: Aligning Multiple Tasks during Test-Time Training", "comment": "Accepted at ICCV 2025", "summary": "Generalizing neural networks to unseen target domains is a significant\nchallenge in real-world deployments. Test-time training (TTT) addresses this by\nusing an auxiliary self-supervised task to reduce the domain gap caused by\ndistribution shifts between the source and target. However, we find that when\nmodels are required to perform multiple tasks under domain shifts, conventional\nTTT methods suffer from unsynchronized task behavior, where the adaptation\nsteps needed for optimal performance in one task may not align with the\nrequirements of other tasks. To address this, we propose a novel TTT approach\ncalled Synchronizing Tasks for Test-time Training (S4T), which enables the\nconcurrent handling of multiple tasks. The core idea behind S4T is that\npredicting task relations across domain shifts is key to synchronizing tasks\nduring test time. To validate our approach, we apply S4T to conventional\nmulti-task benchmarks, integrating it with traditional TTT protocols. Our\nempirical results show that S4T outperforms state-of-the-art TTT methods across\nvarious benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS4T\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTT\uff09\u7684\u4efb\u52a1\u540c\u6b65\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u6d4b\u4efb\u52a1\u5173\u7cfb\u6765\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\uff0c\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u57df\uff0c\u800c\u4f20\u7edf\u7684TTT\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u573a\u666f\u4e0b\u5b58\u5728\u4efb\u52a1\u884c\u4e3a\u4e0d\u540c\u6b65\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faS4T\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u4efb\u52a1\u5173\u7cfb\u6765\u540c\u6b65\u591a\u4efb\u52a1\u5728\u6d4b\u8bd5\u65f6\u7684\u8bad\u7ec3\uff0c\u5e76\u4e0e\u4f20\u7edfTTT\u534f\u8bae\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS4T\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u7684TTT\u65b9\u6cd5\u3002", "conclusion": "S4T\u901a\u8fc7\u4efb\u52a1\u5173\u7cfb\u9884\u6d4b\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1TTT\u4e2d\u7684\u540c\u6b65\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.07804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07804", "abs": "https://arxiv.org/abs/2507.07804", "authors": ["Alba Garrido", "Alejandro Almod\u00f3var", "Patricia A. Apell\u00e1niz", "Juan Parras", "Santiago Zazo"], "title": "Deep Survival Analysis in Multimodal Medical Data: A Parametric and Probabilistic Approach with Competing Risks", "comment": "29 pages, 9 Figures", "summary": "Accurate survival prediction is critical in oncology for prognosis and\ntreatment planning. Traditional approaches often rely on a single data\nmodality, limiting their ability to capture the complexity of tumor biology. To\naddress this challenge, we introduce a multimodal deep learning framework for\nsurvival analysis capable of modeling both single and competing risks\nscenarios, evaluating the impact of integrating multiple medical data sources\non survival predictions. We propose SAMVAE (Survival Analysis Multimodal\nVariational Autoencoder), a novel deep learning architecture designed for\nsurvival prediction that integrates six data modalities: clinical variables,\nfour molecular profiles, and histopathological images. SAMVAE leverages\nmodality specific encoders to project inputs into a shared latent space,\nenabling robust survival prediction while preserving modality specific\ninformation. Its parametric formulation enables the derivation of clinically\nmeaningful statistics from the output distributions, providing patient-specific\ninsights through interactive multimedia that contribute to more informed\nclinical decision-making and establish a foundation for interpretable,\ndata-driven survival analysis in oncology. We evaluate SAMVAE on two cancer\ncohorts breast cancer and lower grade glioma applying tailored preprocessing,\ndimensionality reduction, and hyperparameter optimization. The results\ndemonstrate the successful integration of multimodal data for both standard\nsurvival analysis and competing risks scenarios across different datasets. Our\nmodel achieves competitive performance compared to state-of-the-art multimodal\nsurvival models. Notably, this is the first parametric multimodal deep learning\narchitecture to incorporate competing risks while modeling continuous time to a\nspecific event, using both tabular and image data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMVAE\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u80bf\u7624\u751f\u5b58\u5206\u6790\uff0c\u6574\u5408\u4e86\u516d\u79cd\u6570\u636e\u6a21\u6001\uff0c\u5e76\u5728\u4e73\u817a\u764c\u548c\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u751f\u5b58\u9884\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u6570\u636e\u6a21\u6001\uff0c\u96be\u4ee5\u6355\u6349\u80bf\u7624\u751f\u7269\u5b66\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faSAMVAE\u6846\u67b6\uff0c\u5229\u7528\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u5c06\u591a\u6a21\u6001\u6570\u636e\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u652f\u6301\u751f\u5b58\u5206\u6790\u548c\u7ade\u4e89\u98ce\u9669\u573a\u666f\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u4f4e\u7ea7\u522b\u80f6\u8d28\u7624\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9996\u6b21\u7ed3\u5408\u7ade\u4e89\u98ce\u9669\u548c\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u3002", "conclusion": "SAMVAE\u4e3a\u80bf\u7624\u5b66\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9a71\u52a8\u7684\u751f\u5b58\u5206\u6790\u57fa\u7840\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2507.07814", "categories": ["cs.LG", "cs.NA", "math.NA", "15A42, 15A60, 68T07"], "pdf": "https://arxiv.org/pdf/2507.07814", "abs": "https://arxiv.org/abs/2507.07814", "authors": ["Nikolay Yudin", "Alexander Gaponov", "Sergei Kudriashov", "Maxim Rakhuba"], "title": "Pay Attention to Attention Distribution: A New Local Lipschitz Bound for Transformers", "comment": null, "summary": "We present a novel local Lipschitz bound for self-attention blocks of\ntransformers. This bound is based on a refined closed-form expression for the\nspectral norm of the softmax function. The resulting bound is not only more\naccurate than in the prior art, but also unveils the dependence of the\nLipschitz constant on attention score maps. Based on the new findings, we\nsuggest an explanation of the way distributions inside the attention map affect\nthe robustness from the Lipschitz constant perspective. We also introduce a new\nlightweight regularization term called JaSMin (Jacobian Softmax norm\nMinimization), which boosts the transformer's robustness and decreases local\nLipschitz constants of the whole network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8Lipschitz\u754c\u9650\uff0c\u7528\u4e8eTransformer\u7684\u81ea\u6ce8\u610f\u529b\u5757\uff0c\u57fa\u4e8e\u6539\u8fdb\u7684softmax\u8c31\u8303\u6570\u8868\u8fbe\u5f0f\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u5206\u6570\u56fe\u5bf9Lipschitz\u5e38\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u9879JaSMin\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u7814\u7a76\u81ea\u6ce8\u610f\u529b\u5757\u7684\u5c40\u90e8Lipschitz\u6027\u8d28\uff0c\u4ee5\u7406\u89e3\u5176\u5bf9Transformer\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u7684softmax\u8c31\u8303\u6570\u8868\u8fbe\u5f0f\u63a8\u5bfc\u5c40\u90e8Lipschitz\u754c\u9650\uff0c\u5e76\u8bbe\u8ba1JaSMin\u6b63\u5219\u5316\u9879\u3002", "result": "\u65b0\u754c\u9650\u66f4\u51c6\u786e\uff0c\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u56fe\u5bf9Lipschitz\u5e38\u6570\u7684\u5f71\u54cd\uff0cJaSMin\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u5c40\u90e8Lipschitz\u754c\u9650\u548cJaSMin\u6b63\u5219\u5316\u6709\u52a9\u4e8e\u63d0\u5347Transformer\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.07829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07829", "abs": "https://arxiv.org/abs/2507.07829", "authors": ["Martin Mr\u00e1z", "Breenda Das", "Anshul Gupta", "Lennart Purucker", "Frank Hutter"], "title": "Towards Benchmarking Foundation Models for Tabular Data With Text", "comment": "Accepted at Foundation Models for Structured Data workshop at ICML\n  2025", "summary": "Foundation models for tabular data are rapidly evolving, with increasing\ninterest in extending them to support additional modalities such as free-text\nfeatures. However, existing benchmarks for tabular data rarely include textual\ncolumns, and identifying real-world tabular datasets with semantically rich\ntext features is non-trivial. We propose a series of simple yet effective\nablation-style strategies for incorporating text into conventional tabular\npipelines. Moreover, we benchmark how state-of-the-art tabular foundation\nmodels can handle textual data by manually curating a collection of real-world\ntabular datasets with meaningful textual features. Our study is an important\nstep towards improving benchmarking of foundation models for tabular data with\ntext.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6587\u672c\u7279\u5f81\u6574\u5408\u5230\u8868\u683c\u6570\u636e\u4e2d\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u8868\u683c\u57fa\u7840\u6a21\u578b\u5904\u7406\u6587\u672c\u6570\u636e\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u6570\u636e\u57fa\u51c6\u5f88\u5c11\u5305\u542b\u6587\u672c\u5217\uff0c\u4e14\u771f\u5b9e\u4e16\u754c\u4e2d\u5177\u6709\u4e30\u5bcc\u8bed\u4e49\u6587\u672c\u7279\u5f81\u7684\u8868\u683c\u6570\u636e\u96c6\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u7b80\u5355\u6709\u6548\u7684\u7b56\u7565\uff0c\u5c06\u6587\u672c\u6574\u5408\u5230\u4f20\u7edf\u8868\u683c\u6d41\u7a0b\u4e2d\uff0c\u5e76\u624b\u52a8\u6574\u7406\u4e86\u4e00\u7ec4\u771f\u5b9e\u4e16\u754c\u7684\u8868\u683c\u6570\u636e\u96c6\u3002", "result": "\u8bc4\u4f30\u4e86\u73b0\u6709\u8868\u683c\u57fa\u7840\u6a21\u578b\u5904\u7406\u6587\u672c\u6570\u636e\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6539\u8fdb\u8868\u683c\u6570\u636e\u57fa\u7840\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u91cd\u8981\u6b65\u9aa4\u3002"}}
{"id": "2507.07848", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07848", "abs": "https://arxiv.org/abs/2507.07848", "authors": ["Giovanni Dispoto", "Paolo Bonetti", "Marcello Restelli"], "title": "\"So, Tell Me About Your Policy...\": Distillation of interpretable policies from Deep Reinforcement Learning agents", "comment": null, "summary": "Recent advances in Reinforcement Learning (RL) largely benefit from the\ninclusion of Deep Neural Networks, boosting the number of novel approaches\nproposed in the field of Deep Reinforcement Learning (DRL). These techniques\ndemonstrate the ability to tackle complex games such as Atari, Go, and other\nreal-world applications, including financial trading. Nevertheless, a\nsignificant challenge emerges from the lack of interpretability, particularly\nwhen attempting to comprehend the underlying patterns learned, the relative\nimportance of the state features, and how they are integrated to generate the\npolicy's output. For this reason, in mission-critical and real-world settings,\nit is often preferred to deploy a simpler and more interpretable algorithm,\nalthough at the cost of performance. In this paper, we propose a novel\nalgorithm, supported by theoretical guarantees, that can extract an\ninterpretable policy (e.g., a linear policy) without disregarding the\npeculiarities of expert behavior. This result is obtained by considering the\nadvantage function, which includes information about why an action is superior\nto the others. In contrast to previous works, our approach enables the training\nof an interpretable policy using previously collected experience. The proposed\nalgorithm is empirically evaluated on classic control environments and on a\nfinancial trading scenario, demonstrating its ability to extract meaningful\ninformation from complex expert policies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u80fd\u5728\u4e0d\u5ffd\u7565\u4e13\u5bb6\u884c\u4e3a\u7279\u70b9\u7684\u60c5\u51b5\u4e0b\u63d0\u53d6\u53ef\u89e3\u91ca\u7b56\u7565\uff08\u5982\u7ebf\u6027\u7b56\u7565\uff09\uff0c\u5e76\u901a\u8fc7\u4f18\u52bf\u51fd\u6570\u5b9e\u73b0\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u4f18\u52bf\u51fd\u6570\u63d0\u53d6\u53ef\u89e3\u91ca\u7b56\u7565\uff0c\u652f\u6301\u4ece\u5df2\u6709\u7ecf\u9a8c\u4e2d\u8bad\u7ec3\u3002", "result": "\u5728\u7ecf\u5178\u63a7\u5236\u73af\u5883\u548c\u91d1\u878d\u4ea4\u6613\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u63d0\u53d6\u590d\u6742\u4e13\u5bb6\u7b56\u7565\u4e2d\u7684\u6709\u610f\u4e49\u4fe1\u606f\uff0c\u5e73\u8861\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.07853", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.07853", "abs": "https://arxiv.org/abs/2507.07853", "authors": ["Navish Kumar", "Thomas M\u00f6llenhoff", "Mohammad Emtiyaz Khan", "Aurelien Lucchi"], "title": "Optimization Guarantees for Square-Root Natural-Gradient Variational Inference", "comment": null, "summary": "Variational inference with natural-gradient descent often shows fast\nconvergence in practice, but its theoretical convergence guarantees have been\nchallenging to establish. This is true even for the simplest cases that involve\nconcave log-likelihoods and use a Gaussian approximation. We show that the\nchallenge can be circumvented for such cases using a square-root\nparameterization for the Gaussian covariance. This approach establishes novel\nconvergence guarantees for natural-gradient variational-Gaussian inference and\nits continuous-time gradient flow. Our experiments demonstrate the\neffectiveness of natural gradient methods and highlight their advantages over\nalgorithms that use Euclidean or Wasserstein geometries.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u5728\u53d8\u5206\u63a8\u65ad\u4e2d\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e73\u65b9\u6839\u53c2\u6570\u5316\u7684\u9ad8\u65af\u534f\u65b9\u5dee\u65b9\u6cd5\uff0c\u4e3a\u81ea\u7136\u68af\u5ea6\u53d8\u5206\u9ad8\u65af\u63a8\u65ad\u53ca\u5176\u8fde\u7eed\u65f6\u95f4\u68af\u5ea6\u6d41\u5efa\u7acb\u4e86\u65b0\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u81ea\u7136\u68af\u5ea6\u4e0b\u964d\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u5feb\u901f\u6536\u655b\uff0c\u4f46\u5176\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u4e00\u76f4\u96be\u4ee5\u5efa\u7acb\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u51f9\u5bf9\u6570\u4f3c\u7136\u548c\u9ad8\u65af\u8fd1\u4f3c\u7684\u7b80\u5355\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u5e73\u65b9\u6839\u53c2\u6570\u5316\u9ad8\u65af\u534f\u65b9\u5dee\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u81ea\u7136\u68af\u5ea6\u53d8\u5206\u9ad8\u65af\u63a8\u65ad\u7684\u6536\u655b\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u81ea\u7136\u68af\u5ea6\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u663e\u793a\u4e86\u5176\u5728\u6b27\u51e0\u91cc\u5f97\u6216Wasserstein\u51e0\u4f55\u7b97\u6cd5\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u901a\u8fc7\u5e73\u65b9\u6839\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u4e3a\u81ea\u7136\u68af\u5ea6\u53d8\u5206\u9ad8\u65af\u63a8\u65ad\u5efa\u7acb\u4e86\u65b0\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.07854", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07854", "abs": "https://arxiv.org/abs/2507.07854", "authors": ["Zizhou Zhang", "Qinyan Shen", "Zhuohuan Hu", "Qianying Liu", "Huijie Shen"], "title": "Credit Risk Analysis for SMEs Using Graph Neural Networks in Supply Chain", "comment": "The paper will be published on 2025 International Conference on Big\n  Data, Artificial Intelligence and Digital Economy", "summary": "Small and Medium-sized Enterprises (SMEs) are vital to the modern economy,\nyet their credit risk analysis often struggles with scarce data, especially for\nonline lenders lacking direct credit records. This paper introduces a Graph\nNeural Network (GNN)-based framework, leveraging SME interactions from\ntransaction and social data to map spatial dependencies and predict loan\ndefault risks. Tests on real-world datasets from Discover and Ant Credit (23.4M\nnodes for supply chain analysis, 8.6M for default prediction) show the GNN\nsurpasses traditional and other GNN baselines, with AUCs of 0.995 and 0.701 for\nsupply chain mining and default prediction, respectively. It also helps\nregulators model supply chain disruption impacts on banks, accurately\nforecasting loan defaults from material shortages, and offers Federal Reserve\nstress testers key data for CCAR risk buffers. This approach provides a\nscalable, effective tool for assessing SME credit risk.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e2d\u5c0f\u4f01\u4e1a\u7684\u4ea4\u6613\u548c\u793e\u4ea4\u6570\u636e\u9884\u6d4b\u8d37\u6b3e\u8fdd\u7ea6\u98ce\u9669\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e2d\u5c0f\u4f01\u4e1a\u7684\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u5e38\u56e0\u6570\u636e\u7a00\u7f3a\u800c\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5bf9\u7f3a\u4e4f\u76f4\u63a5\u4fe1\u7528\u8bb0\u5f55\u7684\u5728\u7ebf\u8d37\u6b3e\u673a\u6784\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5206\u6790\u4e2d\u5c0f\u4f01\u4e1a\u7684\u4ea4\u6613\u548c\u793e\u4ea4\u6570\u636e\uff0c\u6784\u5efa\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u4ee5\u9884\u6d4b\u8fdd\u7ea6\u98ce\u9669\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cGNN\u7684AUC\u503c\u8fbe\u52300.995\uff08\u4f9b\u5e94\u94fe\u5206\u6790\uff09\u548c0.701\uff08\u8fdd\u7ea6\u9884\u6d4b\uff09\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u4ed6GNN\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5de5\u5177\uff0c\u5e76\u4e3a\u76d1\u7ba1\u673a\u6784\u548c\u7f8e\u8054\u50a8\u538b\u529b\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u3002"}}
{"id": "2507.07855", "categories": ["cs.LG", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.07855", "abs": "https://arxiv.org/abs/2507.07855", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "title": "Principled Foundations for Preference Optimization", "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u504f\u597d\u5b66\u4e60\u7684\u4e24\u79cd\u4e3b\u8981\u7406\u8bba\uff08\u635f\u5931\u51fd\u6570\u548c\u968f\u673a\u9009\u62e9\uff09\u4e4b\u95f4\u7684\u7279\u5b9a\u8054\u7cfb\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "motivation": "\u7406\u89e3DPO\u7684\u666e\u9002\u6027\u539f\u7406\u5bf9\u4e8e\u5176\u591a\u6837\u5316\u7684\u5e94\u7528\u573a\u666f\u3001\u5f53\u524d\u7684\u7814\u7a76\u70ed\u6f6e\u4ee5\u53ca\u907f\u514d\u6f5c\u5728\u9677\u9631\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5efa\u7acbSavage\u635f\u5931\u51fd\u6570\u4e0e\u968f\u673a\u9009\u62e9\u7406\u8bba\uff08Doignon-Falmagne\u548cMachina\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u652f\u6301\u4e86\u5305\u62ec\u5f03\u6743\u3001\u975e\u51f8\u76ee\u6807\u7b49\u7279\u6027\u3002", "result": "\u7814\u7a76\u6db5\u76d6\u4e86DPO\u7684\u6240\u6709\u6269\u5c55\uff08\u5982\u8fb9\u754c\u548c\u957f\u5ea6\u6821\u6b63\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709DPO\u53d8\u4f53\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u4ece\u7406\u8bba\u89d2\u5ea6\u7406\u89e3DPO\u6709\u52a9\u4e8e\u4f18\u5316\u5176\u5e94\u7528\u5e76\u907f\u514d\u504f\u79bb\u6838\u5fc3\u539f\u7406\u7684\u9677\u9631\u3002"}}
{"id": "2507.07862", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.07862", "abs": "https://arxiv.org/abs/2507.07862", "authors": ["Tianang Leng", "Fangping Wan", "Marcelo Der Torossian Torres", "Cesar de la Fuente-Nunez"], "title": "Predicting and generating antibiotics against future pathogens with ApexOracle", "comment": "3 figures", "summary": "Antimicrobial resistance (AMR) is escalating and outpacing current antibiotic\ndevelopment. Thus, discovering antibiotics effective against emerging pathogens\nis becoming increasingly critical. However, existing approaches cannot rapidly\nidentify effective molecules against novel pathogens or emerging drug-resistant\nstrains. Here, we introduce ApexOracle, an artificial intelligence (AI) model\nthat both predicts the antibacterial potency of existing compounds and designs\nde novo molecules active against strains it has never encountered. Departing\nfrom models that rely solely on molecular features, ApexOracle incorporates\npathogen-specific context through the integration of molecular features\ncaptured via a foundational discrete diffusion language model and a\ndual-embedding framework that combines genomic- and literature-derived strain\nrepresentations. Across diverse bacterial species and chemical modalities,\nApexOracle consistently outperformed state-of-the-art approaches in activity\nprediction and demonstrated reliable transferability to novel pathogens with\nlittle or no antimicrobial data. Its unified representation-generation\narchitecture further enables the in silico creation of \"new-to-nature\"\nmolecules with high predicted efficacy against priority threats. By pairing\nrapid activity prediction with targeted molecular generation, ApexOracle offers\na scalable strategy for countering AMR and preparing for future\ninfectious-disease outbreaks.", "AI": {"tldr": "ApexOracle\u662f\u4e00\u79cdAI\u6a21\u578b\uff0c\u80fd\u9884\u6d4b\u73b0\u6709\u5316\u5408\u7269\u7684\u6297\u83cc\u6548\u529b\u5e76\u8bbe\u8ba1\u9488\u5bf9\u65b0\u75c5\u539f\u4f53\u7684\u5206\u5b50\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6297\u751f\u7d20\u8010\u836f\u6027\uff08AMR\uff09\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5feb\u901f\u5e94\u5bf9\u65b0\u75c5\u539f\u4f53\u6216\u8010\u836f\u83cc\u682a\u3002", "method": "\u7ed3\u5408\u5206\u5b50\u7279\u5f81\u548c\u75c5\u539f\u4f53\u7279\u5f02\u6027\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u79bb\u6563\u6269\u6563\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u5d4c\u5165\u6846\u67b6\u6574\u5408\u57fa\u56e0\u7ec4\u4e0e\u6587\u732e\u6570\u636e\u3002", "result": "\u5728\u591a\u79cd\u7ec6\u83cc\u548c\u5316\u5b66\u6a21\u6001\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u53ef\u9760\u8fc1\u79fb\u81f3\u65e0\u6297\u83cc\u6570\u636e\u7684\u65b0\u75c5\u539f\u4f53\uff0c\u5e76\u80fd\u8bbe\u8ba1\u9ad8\u6548\u65b0\u5206\u5b50\u3002", "conclusion": "ApexOracle\u4e3a\u5e94\u5bf9AMR\u548c\u672a\u6765\u4f20\u67d3\u75c5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7b56\u7565\u3002"}}
{"id": "2507.07882", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07882", "abs": "https://arxiv.org/abs/2507.07882", "authors": ["Wei-Tse Hsu", "Savva Grevtsev", "Thomas Douglas", "Aniket Magarkar", "Philip C. Biggin"], "title": "Can AI-predicted complexes teach machine learning to compute drug binding affinity?", "comment": null, "summary": "We evaluate the feasibility of using co-folding models for synthetic data\naugmentation in training machine learning-based scoring functions (MLSFs) for\nbinding affinity prediction. Our results show that performance gains depend\ncritically on the structural quality of augmented data. In light of this, we\nestablished simple heuristics for identifying high-quality co-folding\npredictions without reference structures, enabling them to substitute for\nexperimental structures in MLSF training. Our study informs future data\naugmentation strategies based on co-folding models.", "AI": {"tldr": "\u8bc4\u4f30\u5171\u6298\u53e0\u6a21\u578b\u7528\u4e8e\u5408\u6210\u6570\u636e\u589e\u5f3a\u5728\u673a\u5668\u5b66\u4e60\u8bc4\u5206\u51fd\u6570\u8bad\u7ec3\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u4e8e\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u65e0\u53c2\u8003\u7ed3\u6784\u7684\u9ad8\u8d28\u91cf\u9884\u6d4b\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5171\u6298\u53e0\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u5b66\u4e60\u8bc4\u5206\u51fd\u6570\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u5171\u6298\u53e0\u6a21\u578b\u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\uff0c\u63d0\u51fa\u542f\u53d1\u5f0f\u65b9\u6cd5\u7b5b\u9009\u9ad8\u8d28\u91cf\u9884\u6d4b\u3002", "result": "\u6027\u80fd\u63d0\u5347\u4f9d\u8d56\u4e8e\u6570\u636e\u8d28\u91cf\uff0c\u542f\u53d1\u5f0f\u65b9\u6cd5\u53ef\u66ff\u4ee3\u5b9e\u9a8c\u7ed3\u6784\u7528\u4e8e\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u57fa\u4e8e\u5171\u6298\u53e0\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.07883", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07883", "abs": "https://arxiv.org/abs/2507.07883", "authors": ["Hao Ban", "Gokul Ram Subramani", "Kaiyi Ji"], "title": "SAMO: A Lightweight Sharpness-Aware Approach for Multi-Task Optimization with Joint Global-Local Perturbation", "comment": null, "summary": "Multi-task learning (MTL) enables a joint model to capture commonalities\nacross multiple tasks, reducing computation costs and improving data\nefficiency. However, a major challenge in MTL optimization is task conflicts,\nwhere the task gradients differ in direction or magnitude, limiting model\nperformance compared to single-task counterparts. Sharpness-aware minimization\n(SAM) minimizes task loss while simultaneously reducing the sharpness of the\nloss landscape. Our empirical observations show that SAM effectively mitigates\ntask conflicts in MTL. Motivated by these findings, we explore integrating SAM\ninto MTL but face two key challenges. While both the average loss gradient and\nindividual task gradients-referred to as global and local\ninformation-contribute to SAM, how to combine them remains unclear. Moreover,\ndirectly computing each task gradient introduces significant computational and\nmemory overheads. To address these challenges, we propose SAMO, a lightweight\n\\textbf{S}harpness-\\textbf{A}ware \\textbf{M}ulti-task \\textbf{O}ptimization\napproach, that leverages a joint global-local perturbation. The local\nperturbations are approximated using only forward passes and are layerwise\nnormalized to improve efficiency. Extensive experiments on a suite of\nmulti-task benchmarks demonstrate both the effectiveness and efficiency of our\nmethod. Code is available at https://github.com/OptMN-Lab/SAMO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684SAMO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u6270\u52a8\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u4e2d\u4efb\u52a1\u51b2\u7a81\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u800cSAM\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5982\u4f55\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u4ee5\u53ca\u8ba1\u7b97\u6548\u7387\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51faSAMO\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5168\u5c40-\u5c40\u90e8\u6270\u52a8\u548c\u5c42\u5f52\u4e00\u5316\u8fd1\u4f3c\u5c40\u90e8\u6270\u52a8\uff0c\u4ec5\u9700\u524d\u5411\u4f20\u64ad\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86SAMO\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "SAMO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u591a\u4efb\u52a1\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u7f13\u89e3\u4efb\u52a1\u51b2\u7a81\u5e76\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.07885", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07885", "abs": "https://arxiv.org/abs/2507.07885", "authors": ["Ashe Neth", "Sawinder kaur", "Mohammad Nur Hossain Khan", "Subrata Biswas", "Asif Salekin", "Bashima Islam"], "title": "UnIT: Scalable Unstructured Inference-Time Pruning for MAC-efficient Neural Inference on MCUs", "comment": "Submitted to SenSys 2026 on July 1, 2025", "summary": "Existing pruning methods are typically applied during training or compile\ntime and often rely on structured sparsity. While compatible with low-power\nmicrocontrollers (MCUs), structured pruning underutilizes the opportunity for\nfine-grained efficiency on devices without SIMD support or parallel compute. To\naddress these limitations, we introduce UnIT (Unstructured Inference-Time\npruning), a lightweight method that dynamically identifies and skips\nunnecessary multiply-accumulate (MAC) operations during inference, guided by\ninput-specific activation patterns. Unlike structured pruning, UnIT embraces\nirregular sparsity and does not require retraining or hardware specialization.\nIt transforms pruning decisions into lightweight comparisons, replacing\nmultiplications with threshold checks and approximated divisions. UnIT further\noptimizes compute by reusing threshold computations across multiple connections\nand applying layer- and group-specific pruning sensitivity. We present three\nfast, hardware-friendly division approximations tailored to the capabilities of\ncommon embedded platforms. Demonstrated on the MSP430 microcontroller, UnIT\nachieves 11.02% to 82.03% MAC reduction, 27.30% to 84.19% faster inference, and\n27.33% to 84.38% lower energy consumption compared to training-time pruned\nmodels, while maintaining accuracy with 0.48-7%. Under domain shift, UnIT\nmatches or exceeds the accuracy of retrained models while requiring\nsignificantly fewer MACs. These results establish unstructured inference-time\npruning as a viable and practical solution for efficient, retraining-free\ndeployment of deep neural networks on MCUs.", "AI": {"tldr": "UnIT\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8df3\u8fc7\u4e0d\u5fc5\u8981\u7684MAC\u64cd\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017MCU\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u65e0SIMD\u652f\u6301\u8bbe\u5907\u7684\u7ec6\u7c92\u5ea6\u6548\u7387\u3002", "method": "UnIT\u901a\u8fc7\u8f93\u5165\u7279\u5b9a\u7684\u6fc0\u6d3b\u6a21\u5f0f\u52a8\u6001\u8df3\u8fc7MAC\u64cd\u4f5c\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u786c\u4ef6\u5b9a\u5236\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6bd4\u8f83\u548c\u8fd1\u4f3c\u9664\u6cd5\u3002", "result": "\u5728MSP430\u4e0a\uff0cUnIT\u51cf\u5c1111.02%-82.03%\u7684MAC\u64cd\u4f5c\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534727.30%-84.19%\uff0c\u80fd\u8017\u964d\u4f4e27.33%-84.38%\uff0c\u7cbe\u5ea6\u635f\u59310.48-7%\u3002", "conclusion": "UnIT\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u6548\u90e8\u7f72\u65b9\u6848\uff0c\u9002\u7528\u4e8eMCU\u4e0a\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3002"}}
{"id": "2507.07898", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.07898", "abs": "https://arxiv.org/abs/2507.07898", "authors": ["Mohammad Fesanghary", "Achintya Gopal"], "title": "Efficient Causal Discovery for Autoregressive Time Series", "comment": "10 pages, 8 figures", "summary": "In this study, we present a novel constraint-based algorithm for causal\nstructure learning specifically designed for nonlinear autoregressive time\nseries. Our algorithm significantly reduces computational complexity compared\nto existing methods, making it more efficient and scalable to larger problems.\nWe rigorously evaluate its performance on synthetic datasets, demonstrating\nthat our algorithm not only outperforms current techniques, but also excels in\nscenarios with limited data availability. These results highlight its potential\nfor practical applications in fields requiring efficient and accurate causal\ninference from nonlinear time series data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u7ebf\u6027\u81ea\u56de\u5f52\u65f6\u95f4\u5e8f\u5217\u7684\u65b0\u578b\u7ea6\u675f\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u7b97\u6cd5\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u6548\u7387\u66f4\u9ad8\u4e14\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u81ea\u56de\u5f52\u65f6\u95f4\u5e8f\u5217\u7684\u56e0\u679c\u7ed3\u6784\u5b66\u4e60\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u6548\u7387\u4e0d\u8db3\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u57fa\u7b97\u6cd5\uff0c\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\u4ee5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u63a8\u65ad\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.07906", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07906", "abs": "https://arxiv.org/abs/2507.07906", "authors": ["Anant Gupta", "Rajarshi Bhowmik", "Geoffrey Gunow"], "title": "Agentic Retrieval of Topics and Insights from Earnings Calls", "comment": "The 2nd Workshop on Financial Information Retrieval in the Era of\n  Generative AI, The 48th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval July 13-17, 2025 | Padua, Italy", "summary": "Tracking the strategic focus of companies through topics in their earnings\ncalls is a key task in financial analysis. However, as industries evolve,\ntraditional topic modeling techniques struggle to dynamically capture emerging\ntopics and their relationships. In this work, we propose an LLM-agent driven\napproach to discover and retrieve emerging topics from quarterly earnings\ncalls. We propose an LLM-agent to extract topics from documents, structure them\ninto a hierarchical ontology, and establish relationships between new and\nexisting topics through a topic ontology. We demonstrate the use of extracted\ntopics to infer company-level insights and emerging trends over time. We\nevaluate our approach by measuring ontology coherence, topic evolution\naccuracy, and its ability to surface emerging financial trends.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM-agent\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5b63\u5ea6\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u4e2d\u52a8\u6001\u53d1\u73b0\u548c\u68c0\u7d22\u65b0\u5174\u4e3b\u9898\uff0c\u5e76\u901a\u8fc7\u4e3b\u9898\u672c\u4f53\u5efa\u7acb\u65b0\u8001\u4e3b\u9898\u7684\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u5efa\u6a21\u6280\u672f\u96be\u4ee5\u52a8\u6001\u6355\u6349\u65b0\u5174\u4e3b\u9898\u53ca\u5176\u5173\u7cfb\uff0c\u800c\u91d1\u878d\u5206\u6790\u9700\u8981\u8ffd\u8e2a\u516c\u53f8\u6218\u7565\u7126\u70b9\u7684\u53d8\u5316\u3002", "method": "\u4f7f\u7528LLM-agent\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u4e3b\u9898\uff0c\u6784\u5efa\u5206\u5c42\u672c\u4f53\uff0c\u5e76\u901a\u8fc7\u4e3b\u9898\u672c\u4f53\u5efa\u7acb\u65b0\u8001\u4e3b\u9898\u7684\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u672c\u4f53\u4e00\u81f4\u6027\u3001\u4e3b\u9898\u6f14\u5316\u51c6\u786e\u6027\u53ca\u6355\u6349\u65b0\u5174\u91d1\u878d\u8d8b\u52bf\u7684\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u52a8\u6001\u6355\u6349\u65b0\u5174\u4e3b\u9898\uff0c\u4e3a\u91d1\u878d\u5206\u6790\u63d0\u4f9b\u516c\u53f8\u7ea7\u6d1e\u5bdf\u548c\u8d8b\u52bf\u9884\u6d4b\u3002"}}
{"id": "2507.07919", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.07919", "abs": "https://arxiv.org/abs/2507.07919", "authors": ["Jakub \u010cern\u00fd", "Ji\u0159\u00ed N\u011bme\u010dek", "Ivan Dovica", "Jakub Mare\u010dek"], "title": "Plausible Counterfactual Explanations of Recommendations", "comment": "8 pages, 3 figures, 6 tables", "summary": "Explanations play a variety of roles in various recommender systems, from a\nlegally mandated afterthought, through an integral element of user experience,\nto a key to persuasiveness. A natural and useful form of an explanation is the\nCounterfactual Explanation (CE). We present a method for generating highly\nplausible CEs in recommender systems and evaluate it both numerically and with\na user study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u751f\u6210\u9ad8\u53ef\u4fe1\u5ea6\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CE\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u91ca\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u5177\u6709\u591a\u91cd\u4f5c\u7528\uff0c\u4ece\u6cd5\u5f8b\u8981\u6c42\u7684\u8865\u5145\u5230\u7528\u6237\u4f53\u9a8c\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u751a\u81f3\u662f\u8bf4\u670d\u529b\u7684\u5173\u952e\u3002\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CE\uff09\u662f\u4e00\u79cd\u81ea\u7136\u4e14\u6709\u7528\u7684\u5f62\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u9ad8\u53ef\u4fe1\u5ea6CE\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u53ef\u4fe1\u5ea6\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u89e3\u91ca\u529f\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.07947", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07947", "abs": "https://arxiv.org/abs/2507.07947", "authors": ["Sol Yarkoni", "Roi Livni"], "title": "Low Resource Reconstruction Attacks Through Benign Prompts", "comment": null, "summary": "The recent advances in generative models such as diffusion models have raised\nseveral risks and concerns related to privacy, copyright infringements and data\nstewardship. To better understand and control the risks, various researchers\nhave created techniques, experiments and attacks that reconstruct images, or\npart of images, from the training set. While these techniques already establish\nthat data from the training set can be reconstructed, they often rely on\nhigh-resources, excess to the training set as well as well-engineered and\ndesigned prompts.\n  In this work, we devise a new attack that requires low resources, assumes\nlittle to no access to the actual training set, and identifies, seemingly,\nbenign prompts that lead to potentially-risky image reconstruction. This\nhighlights the risk that images might even be reconstructed by an uninformed\nuser and unintentionally. For example, we identified that, with regard to one\nexisting model, the prompt ``blue Unisex T-Shirt'' can generate the face of a\nreal-life human model. Our method builds on an intuition from previous works\nwhich leverages domain knowledge and identifies a fundamental vulnerability\nthat stems from the use of scraped data from e-commerce platforms, where\ntemplated layouts and images are tied to pattern-like prompts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u8d44\u6e90\u3001\u65e0\u9700\u8bad\u7ec3\u96c6\u8bbf\u95ee\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u770b\u4f3c\u65e0\u5bb3\u7684\u63d0\u793a\u8bcd\u91cd\u5efa\u8bad\u7ec3\u96c6\u4e2d\u7684\u56fe\u50cf\uff0c\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u9690\u79c1\u548c\u7248\u6743\u65b9\u9762\u7684\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u9690\u79c1\u3001\u7248\u6743\u548c\u6570\u636e\u7ba1\u7406\u65b9\u9762\u7684\u98ce\u9669\u3002\u73b0\u6709\u7814\u7a76\u867d\u80fd\u91cd\u5efa\u8bad\u7ec3\u96c6\u4e2d\u7684\u56fe\u50cf\uff0c\u4f46\u4f9d\u8d56\u9ad8\u8d44\u6e90\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u8bcd\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u66f4\u666e\u904d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u9886\u57df\u77e5\u8bc6\uff0c\u901a\u8fc7\u7279\u5b9a\u63d0\u793a\u8bcd\uff08\u5982\u201c\u84dd\u8272Unisex T-Shirt\u201d\uff09\u91cd\u5efa\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u9ad8\u8d44\u6e90\u6216\u8bad\u7ec3\u96c6\u8bbf\u95ee\uff0c\u63ed\u793a\u4e86\u7535\u5546\u5e73\u53f0\u6570\u636e\u5e03\u5c40\u7684\u6f0f\u6d1e\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u67d0\u4e9b\u770b\u4f3c\u65e0\u5bb3\u7684\u63d0\u793a\u8bcd\uff08\u5982\u201c\u84dd\u8272Unisex T-Shirt\u201d\uff09\u53ef\u4ee5\u91cd\u5efa\u771f\u5b9e\u4eba\u7c7b\u7684\u9762\u90e8\u56fe\u50cf\uff0c\u8868\u660e\u98ce\u9669\u53ef\u80fd\u88ab\u666e\u901a\u7528\u6237\u65e0\u610f\u89e6\u53d1\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u6570\u636e\u91cd\u5efa\u65b9\u9762\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u7535\u5546\u5e73\u53f0\u6570\u636e\u7684\u6a21\u578b\uff0c\u547c\u5401\u5bf9\u6570\u636e\u6765\u6e90\u548c\u6a21\u578b\u8bbe\u8ba1\u8fdb\u884c\u66f4\u4e25\u683c\u7684\u5ba1\u67e5\u3002"}}
{"id": "2507.07955", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07955", "abs": "https://arxiv.org/abs/2507.07955", "authors": ["Sukjun Hwang", "Brandon Wang", "Albert Gu"], "title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling", "comment": null, "summary": "Despite incredible progress in language models (LMs) in recent years, largely\nresulting from moving away from specialized models designed for specific tasks\nto general models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data, pre-processing steps such as tokenization\nremain a barrier to true end-to-end foundation models. We introduce a\ncollection of new techniques that enable a dynamic chunking mechanism which\nautomatically learns content -- and context -- dependent segmentation\nstrategies learned jointly with the rest of the model. Incorporating this into\nan explicit hierarchical network (H-Net) allows replacing the (implicitly\nhierarchical) tokenization-LM-detokenization pipeline with a single model\nlearned fully end-to-end. When compute- and data- matched, an H-Net with one\nstage of hierarchy operating at the byte level outperforms a strong Transformer\nlanguage model operating over BPE tokens. Iterating the hierarchy to multiple\nstages further increases its performance by modeling multiple levels of\nabstraction, demonstrating significantly better scaling with data and matching\na token-based Transformer of twice its size. H-Nets pretrained on English show\nsignificantly increased character-level robustness, and qualitatively learn\nmeaningful data-dependent chunking strategies without any heuristics or\nexplicit supervision. Finally, the H-Net's improvement over tokenized pipelines\nis further increased in languages and modalities with weaker tokenization\nheuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement\nin data efficiency over baselines), showing the potential of true end-to-end\nmodels that learn and scale better from unprocessed data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5206\u5757\u673a\u5236\uff08H-Net\uff09\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5206\u8bcd-LM-\u53bb\u5206\u8bcd\u6d41\u7a0b\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8eBPE\u5206\u8bcd\u7684Transformer\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u4ecd\u4f9d\u8d56\u9884\u5206\u8bcd\u6b65\u9aa4\uff0c\u9650\u5236\u4e86\u7aef\u5230\u7aef\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u5f15\u5165\u52a8\u6001\u5206\u5757\u673a\u5236\uff0c\u7ed3\u5408\u5206\u5c42\u7f51\u7edc\uff08H-Net\uff09\uff0c\u5b9e\u73b0\u5185\u5bb9\u4e0e\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u5206\u5272\u7b56\u7565\u3002", "result": "H-Net\u5728\u5b57\u8282\u7ea7\u522b\u8868\u73b0\u4f18\u4e8eBPE\u5206\u8bcd\u7684Transformer\uff0c\u591a\u7ea7\u5206\u5c42\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e2d\u6587\u3001\u4ee3\u7801\u548cDNA\u5e8f\u5217\u4e2d\u6548\u679c\u663e\u8457\u3002", "conclusion": "H-Net\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u6a21\u578b\u5728\u672a\u5904\u7406\u6570\u636e\u4e0a\u7684\u6f5c\u529b\uff0c\u6027\u80fd\u4e0e\u6269\u5c55\u6027\u66f4\u4f18\u3002"}}
{"id": "2507.07986", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07986", "abs": "https://arxiv.org/abs/2507.07986", "authors": ["Perry Dong", "Qiyang Li", "Dorsa Sadigh", "Chelsea Finn"], "title": "EXPO: Stable Reinforcement Learning with Expressive Policies", "comment": null, "summary": "We study the problem of training and fine-tuning expressive policies with\nonline reinforcement learning (RL) given an offline dataset. Training\nexpressive policy classes with online RL present a unique challenge of stable\nvalue maximization. Unlike simpler Gaussian policies commonly used in online\nRL, expressive policies like diffusion and flow-matching policies are\nparameterized by a long denoising chain, which hinders stable gradient\npropagation from actions to policy parameters when optimizing against some\nvalue function. Our key insight is that we can address stable value\nmaximization by avoiding direct optimization over value with the expressive\npolicy and instead construct an on-the-fly RL policy to maximize Q-value. We\npropose Expressive Policy Optimization (EXPO), a sample-efficient online RL\nalgorithm that utilizes an on-the-fly policy to maximize value with two\nparameterized policies -- a larger expressive base policy trained with a stable\nimitation learning objective and a light-weight Gaussian edit policy that edits\nthe actions sampled from the base policy toward a higher value distribution.\nThe on-the-fly policy optimizes the actions from the base policy with the\nlearned edit policy and chooses the value maximizing action from the base and\nedited actions for both sampling and temporal-difference (TD) backup. Our\napproach yields up to 2-3x improvement in sample efficiency on average over\nprior methods both in the setting of fine-tuning a pretrained policy given\noffline data and in leveraging offline data to train online.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEXPO\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7a33\u5b9a\u7684\u6a21\u4eff\u5b66\u4e60\u76ee\u6807\u548c\u8f7b\u91cf\u7ea7\u9ad8\u65af\u7f16\u8f91\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u79bb\u7ebf\u6570\u636e\u96c6\u7684\u57fa\u7840\u4e0a\u8bad\u7ec3\u548c\u5fae\u8c03\u8868\u8fbe\u6027\u7b56\u7565\uff0c\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a33\u5b9a\u4ef7\u503c\u6700\u5927\u5316\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faEXPO\u7b97\u6cd5\uff0c\u5229\u7528\u4e24\u79cd\u53c2\u6570\u5316\u7b56\u7565\uff1a\u4e00\u4e2a\u8868\u8fbe\u6027\u57fa\u7840\u7b56\u7565\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9ad8\u65af\u7f16\u8f91\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u7b56\u7565\u4f18\u5316\u52a8\u4f5c\u3002", "result": "\u5728\u6837\u672c\u6548\u7387\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u53472-3\u500d\u3002", "conclusion": "EXPO\u7b97\u6cd5\u5728\u8868\u8fbe\u6027\u7b56\u7565\u7684\u7a33\u5b9a\u4f18\u5316\u548c\u6837\u672c\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.07996", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07996", "abs": "https://arxiv.org/abs/2507.07996", "authors": ["Ziyue Li", "Yang Li", "Tianyi Zhou"], "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs", "comment": "9 pages, 7 figures", "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8df3\u8fc7\u6216\u91cd\u590d\u5c42\u6765\u6784\u5efa\u9488\u5bf9\u6bcf\u4e2a\u6d4b\u8bd5\u6837\u672c\u7684\u5b9a\u5236\u5316\u6a21\u578b\uff08CoLa\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u80fd\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u8f93\u5165\uff0c\u800c\u65e0\u9700\u5fae\u8c03\uff0c\u5e76\u9a8c\u8bc1\u662f\u5426\u9700\u8981\u6240\u6709\u5c42\u6765\u5904\u7406\u4e0d\u540c\u96be\u5ea6\u7684\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u4f18\u5316\u94fe\u5f0f\u5c42\uff08CoLa\uff09\u7ed3\u6784\uff0c\u652f\u6301\u8df3\u8fc7\u3001\u91cd\u590d\u6216\u91cd\u65b0\u7ec4\u5408\u5c42\u3002", "result": "\u5bf9\u4e8e75%\u7684\u6b63\u786e\u9884\u6d4b\u6837\u672c\uff0c\u53ef\u4ee5\u627e\u5230\u66f4\u77ed\u7684CoLa\u63d0\u5347\u6548\u7387\uff1b\u5bf9\u4e8e60%\u7684\u9519\u8bef\u9884\u6d4b\u6837\u672c\uff0c\u53ef\u4ee5\u627e\u5230\u6b63\u786e\u7684CoLa\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u56fa\u5b9a\u67b6\u6784\u7684\u9884\u8bad\u7ec3LLM\u5b58\u5728\u5c40\u9650\u6027\uff0c\u52a8\u6001\u6df1\u5ea6\u9002\u914d\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
