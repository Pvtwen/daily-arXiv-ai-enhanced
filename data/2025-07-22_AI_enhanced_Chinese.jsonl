{"id": "2507.14444", "categories": ["stat.ML", "cs.AI", "cs.LG", "math.OC", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.14444", "abs": "https://arxiv.org/abs/2507.14444", "authors": ["Yuejie Chi", "Yuxin Chen", "Yuting Wei"], "title": "Statistical and Algorithmic Foundations of Reinforcement Learning", "comment": "reading materials for INFORMS Tutorial in OR 2025", "summary": "As a paradigm for sequential decision making in unknown environments,\nreinforcement learning (RL) has received a flurry of attention in recent years.\nHowever, the explosion of model complexity in emerging applications and the\npresence of nonconvexity exacerbate the challenge of achieving efficient RL in\nsample-starved situations, where data collection is expensive, time-consuming,\nor even high-stakes (e.g., in clinical trials, autonomous systems, and online\nadvertising). How to understand and enhance the sample and computational\nefficacies of RL algorithms is thus of great interest. In this tutorial, we aim\nto introduce several important algorithmic and theoretical developments in RL,\nhighlighting the connections between new ideas and classical topics. Employing\nMarkov Decision Processes as the central mathematical model, we cover several\ndistinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,\nrobust RL, and RL with human feedback), and present several mainstream RL\napproaches (i.e., model-based approach, value-based approach, and policy\noptimization). Our discussions gravitate around the issues of sample\ncomplexity, computational efficiency, as well as algorithm-dependent and\ninformation-theoretic lower bounds from a non-asymptotic viewpoint.", "AI": {"tldr": "\u8be5\u6559\u7a0b\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u6837\u672c\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6311\u6218\uff0c\u5e76\u63a2\u8ba8\u4e86\u63d0\u9ad8\u6837\u672c\u548c\u8ba1\u7b97\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u4e86\u591a\u79cdRL\u573a\u666f\u548c\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u5728\u6570\u636e\u6536\u96c6\u6602\u8d35\u6216\u9ad8\u98ce\u9669\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u4f55\u63d0\u9ad8RL\u7684\u6837\u672c\u548c\u8ba1\u7b97\u6548\u7387\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4f5c\u4e3a\u6838\u5fc3\u6a21\u578b\uff0c\u6db5\u76d6\u591a\u79cdRL\u573a\u666f\uff08\u5982\u6a21\u62df\u5668RL\u3001\u5728\u7ebfRL\u3001\u79bb\u7ebfRL\u7b49\uff09\u548c\u4e3b\u6d41\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6a21\u578b\u3001\u57fa\u4e8e\u4ef7\u503c\u548c\u7b56\u7565\u4f18\u5316\uff09\u3002", "result": "\u8ba8\u8bba\u4e86\u6837\u672c\u590d\u6742\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u975e\u6e10\u8fd1\u89c6\u89d2\u4e0b\u7684\u7b97\u6cd5\u4f9d\u8d56\u548c\u4fe1\u606f\u8bba\u4e0b\u754c\u3002", "conclusion": "\u8be5\u6559\u7a0b\u4e3a\u7406\u89e3\u548c\u63d0\u5347RL\u7b97\u6cd5\u7684\u6548\u7387\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u548c\u7b97\u6cd5\u53d1\u5c55\u89c6\u89d2\u3002"}}
{"id": "2507.14507", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14507", "abs": "https://arxiv.org/abs/2507.14507", "authors": ["Chen Su", "Zhengzhou Cai", "Yuanhe Tian", "Zihong Zheng", "Yan Song"], "title": "Diffusion Models for Time Series Forecasting: A Survey", "comment": null, "summary": "Diffusion models, initially developed for image synthesis, demonstrate\nremarkable generative capabilities. Recently, their application has expanded to\ntime series forecasting (TSF), yielding promising results. In this survey, we\nfirstly introduce the standard diffusion models and their prevalent variants,\nexplaining their adaptation to TSF tasks. We then provide a comprehensive\nreview of diffusion models for TSF, paying special attention to the sources of\nconditional information and the mechanisms for integrating this conditioning\nwithin the models. In analyzing existing approaches using diffusion models for\nTSF, we provide a systematic categorization and a comprehensive summary of them\nin this survey. Furthermore, we examine several foundational diffusion models\napplied to TSF, alongside commonly used datasets and evaluation metrics.\nFinally, we discuss current limitations in these approaches and potential\nfuture research directions. Overall, this survey details recent progress and\nfuture prospects for diffusion models in TSF, serving as a reference for\nresearchers in the field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08TSF\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6a21\u578b\u53d8\u4f53\u3001\u6761\u4ef6\u4fe1\u606f\u6574\u5408\u673a\u5236\u3001\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u7c7b\u603b\u7ed3\uff0c\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u7cfb\u7edf\u603b\u7ed3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ecb\u7ecd\u6807\u51c6\u6269\u6563\u6a21\u578b\u53ca\u5176\u53d8\u4f53\uff0c\u5206\u6790\u5176\u5728TSF\u4e2d\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\u3002", "result": "\u7efc\u8ff0\u4e86\u6269\u6563\u6a21\u578b\u5728TSF\u4e2d\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u53ca\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728TSF\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2507.14641", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14641", "abs": "https://arxiv.org/abs/2507.14641", "authors": ["Jong-Min Kim", "Il Do Ha", "Sangjin Kim"], "title": "Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction", "comment": null, "summary": "This research integrates deep learning, copula functions, and survival\nanalysis to effectively handle highly correlated and right-censored\nmultivariate survival data. It introduces copula-based activation functions\n(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies\ninherent in such data. Through simulation studies and analysis of real breast\ncancer data, our proposed CNN-LSTM with copula-based activation functions for\nmultivariate multi-types of survival responses enhances prediction accuracy by\nexplicitly addressing right-censored data and capturing complex patterns. The\nmodel's performance is evaluated using Shewhart control charts, focusing on the\naverage run length (ARL).", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u3001Copula\u51fd\u6570\u548c\u751f\u5b58\u5206\u6790\uff0c\u5904\u7406\u9ad8\u5ea6\u76f8\u5173\u4e14\u53f3\u5220\u5931\u7684\u591a\u53d8\u91cf\u751f\u5b58\u6570\u636e\uff0c\u63d0\u51fa\u57fa\u4e8eCopula\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5ea6\u76f8\u5173\u4e14\u53f3\u5220\u5931\u7684\u591a\u53d8\u91cf\u751f\u5b58\u6570\u636e\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u6355\u6349\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51faCNN-LSTM\u6a21\u578b\uff0c\u7ed3\u5408Clayton\u3001Gumbel\u7b49Copula\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u4e73\u817a\u764c\u6570\u636e\u5206\u6790\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u901a\u8fc7Shewhart\u63a7\u5236\u56fe\u548c\u5e73\u5747\u8fd0\u884c\u957f\u5ea6\uff08ARL\uff09\u8bc4\u4f30\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8eCopula\u7684\u6fc0\u6d3b\u51fd\u6570\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u751f\u5b58\u6570\u636e\uff0c\u4e3a\u591a\u7c7b\u578b\u751f\u5b58\u54cd\u5e94\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.14652", "categories": ["stat.ML", "cs.CE", "cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.14652", "abs": "https://arxiv.org/abs/2507.14652", "authors": ["Ponkrshnan Thiagarajan", "Tamer A. Zaki", "Michael D. Shields"], "title": "Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators", "comment": null, "summary": "Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample\nfrom the posterior distribution in Bayesian inference. However, HMC techniques\nare computationally demanding for Bayesian neural networks due to the high\ndimensionality of the network's parameter space and the non-convexity of their\nposterior distributions. Therefore, various approximation techniques, such as\nvariational inference (VI) or stochastic gradient MCMC, are often employed to\ninfer the posterior distribution of the network parameters. Such approximations\nintroduce inaccuracies in the inferred distributions, resulting in unreliable\nuncertainty estimates. In this work, we propose a hybrid approach that combines\ninexpensive VI and accurate HMC methods to efficiently and accurately quantify\nuncertainties in neural networks and neural operators. The proposed approach\nleverages an initial VI training on the full network. We examine the influence\nof individual parameters on the prediction uncertainty, which shows that a\nlarge proportion of the parameters do not contribute substantially to\nuncertainty in the network predictions. This information is then used to\nsignificantly reduce the dimension of the parameter space, and HMC is performed\nonly for the subset of network parameters that strongly influence prediction\nuncertainties. This yields a framework for accelerating the full batch HMC for\nposterior inference in neural networks. We demonstrate the efficiency and\naccuracy of the proposed framework on deep neural networks and operator\nnetworks, showing that inference can be performed for large networks with tens\nto hundreds of thousands of parameters. We show that this method can\neffectively learn surrogates for complex physical systems by modeling the\noperator that maps from upstream conditions to wall-pressure data on a cone in\nhypersonic flow.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u63a8\u7406\uff08VI\uff09\u548c\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "HMC\u5728\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u8fd1\u4f3c\u65b9\u6cd5\uff08\u5982VI\uff09\u4f1a\u5f15\u5165\u4e0d\u51c6\u786e\u6027\u3002", "method": "\u5148\u4f7f\u7528VI\u8bad\u7ec3\u7f51\u7edc\uff0c\u8bc6\u522b\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u5927\u7684\u53c2\u6570\u5b50\u96c6\uff0c\u518d\u5bf9\u8fd9\u90e8\u5206\u53c2\u6570\u5e94\u7528HMC\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u53c2\u6570\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\uff0c\u5e76\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u3002"}}
{"id": "2507.14138", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14138", "abs": "https://arxiv.org/abs/2507.14138", "authors": ["Vaishnavi C K", "Sricharan Vijayarangan", "Sri Gayathri G", "Danush Adhithya N", "Alex Joseph", "Preejith SP", "Mohanasankar Sivaprakasam"], "title": "Optimizing VO2max Prediction in Gamified Cardiac Assessment: Leveraging Effective Feature Selection and Refined Protocols for Robust Models", "comment": "Accepted and Presented in IEEE IECBES 2024", "summary": "VO2max is a critical indicator of cardiopulmonary fitness, reflecting the\nmaximum amount of oxygen the body can utilize during intense exercise.\nAccurately measuring VO2max is essential for assessing cardiovascular health\nand predicting outcomes in clinical settings. However, current methods for\nVO2max estimation, such as Cardiopulmonary Exercise Testing (CPET), require\nexpensive equipment and the supervision of trained personnel, limiting\naccessibility for large-scale screening. Preliminary efforts have been made to\ncreate a more accessible method, such as the Cardiopulmonary Spot Jog Test\n(CPSJT). Unfortunately, these early attempts yielded high error margins,\nrendering them unsuitable for widespread use. In our study, we address these\nshortcomings by refining the CPSJT protocol to improve prediction accuracy. A\ncrucial contribution is improved feature extraction which include gender, body\nmass index, aerobic duration, and anaerobic duration. This targeted approach\nhelps in streamlining the model to enhance prediction precision while\nminimizing the risk of overfitting. In a cohort of 44 participants from the\nIndian population, we assessed the performance of various machine learning\nmodels using these features. With Stratified 5-Fold Cross-Validation, the Root\nMean Squared Error (RMSE) values were 5.78 for Linear Regression, 5.15 for\nRandom Forest, and 5.17 for Support Vector Regression. All models demonstrated\nstrong test correlations and low RMSE values, underscoring their robust and\nreliable performance.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6539\u8fdb\u5fc3\u80ba\u70b9\u6162\u8dd1\u6d4b\u8bd5\uff08CPSJT\uff09\u534f\u8bae\uff0c\u7ed3\u5408\u6027\u522b\u3001BMI\u7b49\u7279\u5f81\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u9ad8VO2max\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524dVO2max\u6d4b\u91cf\u65b9\u6cd5\uff08\u5982CPET\uff09\u6210\u672c\u9ad8\u4e14\u9700\u4e13\u4e1a\u4eba\u5458\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7b5b\u67e5\u3002\u65e9\u671f\u66ff\u4ee3\u65b9\u6cd5\uff08\u5982CPSJT\uff09\u8bef\u5dee\u9ad8\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u4f18\u5316CPSJT\u534f\u8bae\uff0c\u63d0\u53d6\u6027\u522b\u3001BMI\u7b49\u7279\u5f81\uff0c\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u548c\u652f\u6301\u5411\u91cf\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u572844\u540d\u5370\u5ea6\u53c2\u4e0e\u8005\u4e2d\uff0c\u6240\u6709\u6a21\u578b\u8868\u73b0\u7a33\u5065\uff0cRMSE\u503c\u4f4e\uff085.78-5.17\uff09\uff0c\u6d4b\u8bd5\u76f8\u5173\u6027\u9ad8\u3002", "conclusion": "\u6539\u8fdb\u7684CPSJT\u534f\u8bae\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4e3aVO2max\u9884\u6d4b\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u3001\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14170", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14170", "abs": "https://arxiv.org/abs/2507.14170", "authors": ["Jaeheun Jung", "Donghun Lee"], "title": "Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space", "comment": "ICML 2025 workshop HiLD 2025 (3rd workshop on High-dimensional\n  Learning Dynamics)", "summary": "Structured pruning aims to reduce the size and computational cost of deep\nneural networks by removing entire filters or channels. The traditional\nregularizers such as L1 or Group Lasso and its variants lead to\nmagnitude-biased pruning decisions, such that the filters with small magnitudes\nare likely to be pruned. Also, they often entail pruning results with almost\nzero margin around pruning decision boundary, such that tiny perturbation in a\nfilter magnitude can flip the pruning decision. In this paper, we identify the\nprecise algebraic condition under which pruning operations preserve model\nperformance, and use the condition to construct a novel regularizer defined in\nan extended parameter space via auxiliary catalyst variables. The proposed\nCatalyst regularization ensures fair pruning chance for each filters with\ntheoretically provable zero bias to their magnitude and robust pruning behavior\nachieved by wide-margin bifurcation of magnitudes between the preserved and the\npruned filters. The theoretical properties naturally lead to real-world\neffectiveness, as shown by empirical validations of Catalyst Pruning algorithm.\nPruning results on various datasets and models are superior to state-of-the-art\nfilter pruning methods, and at the same time confirm the predicted robust and\nfair pruning characteristics of Catalyst pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5Catalyst\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u53d8\u91cf\u786e\u4fdd\u526a\u679d\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\uff08\u5982L1\u6216Group Lasso\uff09\u5b58\u5728\u5e45\u5ea6\u504f\u5dee\u548c\u51b3\u7b56\u8fb9\u754c\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5f71\u54cd\u526a\u679d\u6548\u679c\u3002", "method": "\u57fa\u4e8e\u4ee3\u6570\u6761\u4ef6\u8bbe\u8ba1Catalyst\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u8f85\u52a9\u53d8\u91cf\u5b9e\u73b0\u516c\u5e73\u526a\u679d\u548c\u5bbd\u8fb9\u754c\u51b3\u7b56\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "conclusion": "Catalyst\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u652f\u6301\u7684\u9ad8\u6548\u526a\u679d\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14661", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.14661", "abs": "https://arxiv.org/abs/2507.14661", "authors": ["Wooseok Ha", "Yuansi Chen"], "title": "When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts", "comment": null, "summary": "Semi-supervised domain adaptation (SSDA) aims to achieve high predictive\nperformance in the target domain with limited labeled target data by exploiting\nabundant source and unlabeled target data. Despite its significance in numerous\napplications, theory on the effectiveness of SSDA remains largely unexplored,\nparticularly in scenarios involving various types of source-target\ndistributional shifts. In this work, we develop a theoretical framework based\non structural causal models (SCMs) which allows us to analyze and quantify the\nperformance of SSDA methods when labeled target data is limited. Within this\nframework, we introduce three SSDA methods, each having a fine-tuning strategy\ntailored to a distinct assumption about the source and target relationship.\nUnder each assumption, we demonstrate how extending an unsupervised domain\nadaptation (UDA) method to SSDA can achieve minimax-optimal target performance\nwith limited target labels. When the relationship between source and target\ndata is only vaguely known -- a common practical concern -- we propose the\nMulti Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models\nfrom multiple starting points and selects the best-performing one based on a\nsmall hold-out target validation dataset. Combined with model selection\nguarantees, MASFT achieves near-optimal target predictive performance across a\nbroad range of types of distributional shifts while significantly reducing the\nneed for labeled target data. We empirically validate the effectiveness of our\nproposed methods through simulations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u7684\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u534a\u76d1\u7763\u57df\u9002\u5e94\uff08SSDA\uff09\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u9488\u5bf9\u4e0d\u540c\u6e90-\u76ee\u6807\u5173\u7cfb\u5047\u8bbe\u7684SSDA\u65b9\u6cd5\u3002\u5176\u4e2d\uff0cMASFT\u7b97\u6cd5\u901a\u8fc7\u591a\u8d77\u70b9\u5fae\u8c03\u548c\u6a21\u578b\u9009\u62e9\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u76ee\u6807\u57df\u6807\u8bb0\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u534a\u76d1\u7763\u57df\u9002\u5e94\uff08SSDA\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u5176\u7406\u8bba\u6709\u6548\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u591a\u79cd\u6e90-\u76ee\u6807\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u57fa\u4e8e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u6784\u5efa\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u4e09\u79cdSSDA\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1MASFT\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u8d77\u70b9\u5fae\u8c03\u548c\u6a21\u578b\u9009\u62e9\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u6709\u9650\u76ee\u6807\u6807\u8bb0\u6570\u636e\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u5b9e\u73b0\u6700\u5c0f\u6700\u5927\u6700\u4f18\u76ee\u6807\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3aSSDA\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u76ee\u6807\u57df\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.14141", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14141", "abs": "https://arxiv.org/abs/2507.14141", "authors": ["Danny Dongyeop Han", "Ahhyun Lucy Lee", "Taeyang Lee", "Yonghyeon Gwon", "Sebin Lee", "Seongjin Lee", "David Keetae Park", "Shinjae Yoo", "Jiook Cha", "Chun Kee Chung"], "title": "DIVER-0 : A Fully Channel Equivariant EEG Foundation Model", "comment": "11 pages, 1 figures, ICML 2025 Workshop on GenBio", "summary": "Electroencephalography (EEG) is a non-invasive technique widely used in\nbrain-computer interfaces and clinical applications, yet existing EEG\nfoundation models face limitations in modeling spatio-temporal brain dynamics\nand lack channel permutation equivariance, preventing robust generalization\nacross diverse electrode configurations. To address these challenges, we\npropose DIVER-0, a novel EEG foundation model that demonstrates how full\nspatio-temporal attention-rather than segregated spatial or temporal\nprocessing-achieves superior performance when properly designed with Rotary\nPosition Embedding (RoPE) for temporal relationships and binary attention\nbiases for channel differentiation. We also introduce Sliding Temporal\nConditional Positional Encoding (STCPE), which improves upon existing\nconditional positional encoding approaches by maintaining both temporal\ntranslation equivariance and channel permutation equivariance, enabling robust\nadaptation to arbitrary electrode configurations unseen during pretraining.\nExperimental results demonstrate that DIVER-0 achieves competitive performance\nwith only 10% of pretraining data while maintaining consistent results across\nall channel permutation conditions, validating its effectiveness for\ncross-dataset generalization and establishing key design principles for\nhandling the inherent heterogeneity of neural recording setups.", "AI": {"tldr": "DIVER-0\u662f\u4e00\u79cd\u65b0\u578bEEG\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5168\u65f6\u7a7a\u6ce8\u610f\u529b\u548c\u6539\u8fdb\u7684\u4f4d\u7f6e\u7f16\u7801\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u548c\u901a\u9053\u7f6e\u6362\u7b49\u53d8\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709EEG\u57fa\u7840\u6a21\u578b\u5728\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u548c\u901a\u9053\u7f6e\u6362\u7b49\u53d8\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u7535\u6781\u914d\u7f6e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faDIVER-0\u6a21\u578b\uff0c\u91c7\u7528\u5168\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408RoPE\u548c\u4e8c\u8fdb\u5236\u6ce8\u610f\u529b\u504f\u7f6e\uff0c\u5e76\u5f15\u5165STCPE\u4ee5\u4fdd\u6301\u65f6\u7a7a\u5e73\u79fb\u548c\u901a\u9053\u7f6e\u6362\u7b49\u53d8\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDIVER-0\u4ec5\u752810%\u7684\u9884\u8bad\u7ec3\u6570\u636e\u5373\u8fbe\u5230\u7ade\u4e89\u6027\u80fd\uff0c\u4e14\u5728\u4e0d\u540c\u901a\u9053\u7f6e\u6362\u6761\u4ef6\u4e0b\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "DIVER-0\u6709\u6548\u89e3\u51b3\u4e86EEG\u6570\u636e\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2507.14171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14171", "abs": "https://arxiv.org/abs/2507.14171", "authors": ["Jaeheun Jung", "Jaehyuk Lee", "Yeajin Lee", "Donghun Lee"], "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "comment": null, "summary": "With the growth of demand on neural network compression methods, the\nstructured pruning methods including importance-based approach are actively\nstudied. The magnitude importance and many correlated modern importance\ncriteria often limit the capacity of pruning decision, since the filters with\nlarger magnitudes are not likely to be pruned if the smaller one didn't, even\nif it is redundant. In this paper, we propose a novel pruning strategy to\nchallenge this dominating effect of magnitude and provide fair chance to each\nfilter to be pruned, by placing it on projective space. After that, we observe\nthe gradient descent movement whether the filters move toward the origin or\nnot, to measure how the filter is likely to be pruned. This measurement is used\nto construct PROscore, a novel importance score for IPPRO, a novel\nimportance-based structured pruning with magnitude-indifference. Our evaluation\nresults shows that the proposed importance criteria using the projective space\nachieves near-lossless pruning by reducing the performance drop in pruning,\nwith promising performance after the finetuning. Our work debunks the\n``size-matters'' myth in pruning and expands the frontier of importance-based\npruning both theoretically and empirically.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u7a7a\u95f4\u7684\u65b0\u578b\u526a\u679d\u7b56\u7565IPPRO\uff0c\u901a\u8fc7PROscore\u8861\u91cf\u6ee4\u6ce2\u5668\u526a\u679d\u53ef\u80fd\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u9650\u5236\u4e86\u526a\u679d\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u5373\u4f7f\u5197\u4f59\u6ee4\u6ce2\u5668\u4e5f\u53ef\u80fd\u56e0\u5e45\u5ea6\u8f83\u5927\u800c\u672a\u88ab\u526a\u679d\u3002", "method": "\u5c06\u6ee4\u6ce2\u5668\u7f6e\u4e8e\u6295\u5f71\u7a7a\u95f4\uff0c\u89c2\u5bdf\u68af\u5ea6\u4e0b\u964d\u8fd0\u52a8\u65b9\u5411\uff08\u662f\u5426\u8d8b\u8fd1\u539f\u70b9\uff09\u6765\u8861\u91cf\u526a\u679d\u53ef\u80fd\u6027\uff0c\u6784\u5efaPROscore\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u7684\u526a\u679d\uff0c\u51cf\u5c11\u4e86\u6027\u80fd\u4e0b\u964d\uff0c\u5e76\u5728\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6253\u7834\u4e86\u526a\u679d\u4e2d\u2018\u5927\u5c0f\u51b3\u5b9a\u4e00\u5207\u2019\u7684\u8ff7\u601d\uff0c\u4ece\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u62d3\u5c55\u4e86\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u65b9\u6cd5\u3002"}}
{"id": "2507.14782", "categories": ["stat.ML", "cs.LG", "math-ph", "math.MP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.14782", "abs": "https://arxiv.org/abs/2507.14782", "authors": ["Xiaoping Du"], "title": "Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation", "comment": "This manuscript has been submitted to Multidisciplinary and\n  Structural Optimization", "summary": "Machine learning (ML) surrogate models are increasingly used in engineering\nanalysis and design to replace computationally expensive simulation models,\nsignificantly reducing computational cost and accelerating decision-making\nprocesses. However, ML predictions contain inherent errors, often estimated as\nmodel uncertainty, which is coupled with variability in model inputs.\nAccurately quantifying and propagating these combined uncertainties is\nessential for generating reliable engineering predictions. This paper presents\na robust framework based on Polynomial Chaos Expansion (PCE) to handle joint\ninput and model uncertainty propagation. While the approach applies broadly to\ngeneral ML surrogates, we focus on Gaussian Process regression models, which\nprovide explicit predictive distributions for model uncertainty. By\ntransforming all random inputs into a unified standard space, a PCE surrogate\nmodel is constructed, allowing efficient and accurate calculation of the mean\nand standard deviation of the output. The proposed methodology also offers a\nmechanism for global sensitivity analysis, enabling the accurate quantification\nof the individual contributions of input variables and ML model uncertainty to\nthe overall output variability. This approach provides a computationally\nefficient and interpretable framework for comprehensive uncertainty\nquantification, supporting trustworthy ML predictions in downstream engineering\napplications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u9879\u5f0f\u6df7\u6c8c\u5c55\u5f00\uff08PCE\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u673a\u5668\u5b66\u4e60\u548c\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\uff0c\u7279\u522b\u5173\u6ce8\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u9760\u7684\u5de5\u7a0b\u9884\u6d4b\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6a21\u578b\u5728\u5de5\u7a0b\u5206\u6790\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u9884\u6d4b\u5b58\u5728\u8bef\u5dee\uff0c\u4e14\u4e0e\u8f93\u5165\u53d8\u91cf\u7684\u4e0d\u786e\u5b9a\u6027\u8026\u5408\u3002\u51c6\u786e\u91cf\u5316\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u5bf9\u751f\u6210\u53ef\u9760\u7684\u5de5\u7a0b\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u968f\u673a\u8f93\u5165\u8f6c\u6362\u4e3a\u7edf\u4e00\u6807\u51c6\u7a7a\u95f4\uff0c\u6784\u5efaPCE\u4ee3\u7406\u6a21\u578b\uff0c\u9ad8\u6548\u8ba1\u7b97\u8f93\u51fa\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\uff0c\u5e76\u63d0\u4f9b\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u8f93\u5165\u53d8\u91cf\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5bf9\u8f93\u51fa\u53d8\u5f02\u6027\u8d21\u732e\u7684\u51c6\u786e\u91cf\u5316\uff0c\u4e3a\u4e0b\u6e38\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u4fe1\u8d56\u7684\u9884\u6d4b\u6846\u67b6\u3002", "conclusion": "\u63d0\u51fa\u7684PCE\u6846\u67b6\u4e3a\u7efc\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u3002"}}
{"id": "2507.14144", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14144", "abs": "https://arxiv.org/abs/2507.14144", "authors": ["Cyril Falcon", "Hassan Mortada", "Math\u00e9o Clavaud", "Jean-Philippe Michel"], "title": "Recursive KalmanNet: Analyse des capacit\u00e9s de g\u00e9n\u00e9ralisation d'un r\u00e9seau de neurones r\u00e9current guid\u00e9 par un filtre de Kalman", "comment": "4 pages, in French language. 4 figures. Accepted for publication in\n  GRETSI 2025 proceedings", "summary": "The Recursive KalmanNet, recently introduced by the authors, is a recurrent\nneural network guided by a Kalman filter, capable of estimating the state\nvariables and error covariance of stochastic dynamic systems from noisy\nmeasurements, without prior knowledge of the noise characteristics. This paper\nexplores its generalization capabilities in out-of-distribution scenarios,\nwhere the temporal dynamics of the test measurements differ from those\nencountered during training.\n  Le Recursive KalmanNet, r\\'ecemment introduit par les auteurs, est un\nr\\'eseau de neurones r\\'ecurrent guid\\'e par un filtre de Kalman, capable\nd'estimer les variables d'\\'etat et la covariance des erreurs des syst\\`emes\ndynamiques stochastiques \\`a partir de mesures bruit\\'ees, sans connaissance\npr\\'ealable des caract\\'eristiques des bruits. Cet article explore ses\ncapacit\\'es de g\\'en\\'eralisation dans des sc\\'enarios hors distribution, o\\`u\nles dynamiques temporelles des mesures de test diff\\`erent de celles\nrencontr\\'ees \\`a l'entra\\^inement.", "AI": {"tldr": "Recursive KalmanNet\u662f\u4e00\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u5728\u672a\u77e5\u566a\u58f0\u7279\u6027\u7684\u60c5\u51b5\u4e0b\u4f30\u8ba1\u52a8\u6001\u7cfb\u7edf\u7684\u72b6\u6001\u53d8\u91cf\u548c\u8bef\u5dee\u534f\u65b9\u5dee\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5176\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76Recursive KalmanNet\u5728\u6d4b\u8bd5\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u52a8\u6001\u7279\u6027\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08Recursive KalmanNet\uff09\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u65e0\u9700\u566a\u58f0\u7279\u6027\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Recursive KalmanNet\u5728\u52a8\u6001\u7279\u6027\u4e0d\u540c\u7684\u6d4b\u8bd5\u6570\u636e\u4e2d\u5c55\u73b0\u51fa\u6cdb\u5316\u6f5c\u529b\u3002"}}
{"id": "2507.14172", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14172", "abs": "https://arxiv.org/abs/2507.14172", "authors": ["Julien Pourcel", "C\u00e9dric Colas", "Pierre-Yves Oudeyer"], "title": "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI", "comment": null, "summary": "Many program synthesis tasks prove too challenging for even state-of-the-art\nlanguage models to solve in single attempts. Search-based evolutionary methods\noffer a promising alternative by exploring solution spaces iteratively, but\ntheir effectiveness remain limited by the fixed capabilities of the underlying\ngenerative model.\n  We propose SOAR, a method that learns program synthesis by integrating\nlanguage models into a self-improving evolutionary loop.\n  SOAR alternates between (1) an evolutionary search that uses an LLM to sample\nand refine candidate solutions, and (2) a hindsight learning phase that\nconverts search attempts into valid problem-solution pairs used to fine-tune\nthe LLM's sampling and refinement capabilities\\, -- \\,enabling increasingly\neffective search in subsequent iterations.\n  On the challenging ARC-AGI benchmark, SOAR achieves significant performance\ngains across model scales and iterations, leveraging positive transfer between\nthe sampling and refinement finetuning tasks. These improvements carry over to\ntest-time adaptation, enabling SOAR to solve 52\\% of the public test set. Our\ncode is open-sourced at: https://github.com/flowersteam/SOAR", "AI": {"tldr": "SOAR\u662f\u4e00\u79cd\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5230\u81ea\u6211\u6539\u8fdb\u7684\u8fdb\u5316\u5faa\u73af\u4e2d\u6765\u5b66\u4e60\u7a0b\u5e8f\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6b21\u5c1d\u8bd5\u4e2d\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff0c\u800c\u57fa\u4e8e\u641c\u7d22\u7684\u8fdb\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u751f\u6210\u6a21\u578b\u7684\u56fa\u5b9a\u80fd\u529b\u3002", "method": "SOAR\u4ea4\u66ff\u8fdb\u884c\u8fdb\u5316\u641c\u7d22\u548c\u4e8b\u540e\u5b66\u4e60\uff0c\u5229\u7528LLM\u91c7\u6837\u548c\u4f18\u5316\u5019\u9009\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5728ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSOAR\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e8652%\u7684\u516c\u5171\u6d4b\u8bd5\u96c6\u95ee\u9898\u3002", "conclusion": "SOAR\u901a\u8fc7\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\u548c\u4e8b\u540e\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7a0b\u5e8f\u5408\u6210\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14901", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14901", "abs": "https://arxiv.org/abs/2507.14901", "authors": ["Armin Keki\u0107", "Jan Schneider", "Dieter B\u00fcchler", "Bernhard Sch\u00f6lkopf", "Michel Besserve"], "title": "Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies", "comment": null, "summary": "Why do reinforcement learning (RL) policies fail or succeed? This is a\nchallenging question due to the complex, high-dimensional nature of\nagent-environment interactions. In this work, we take a causal perspective on\nexplaining the behavior of RL policies by viewing the states, actions, and\nrewards as variables in a low-level causal model. We introduce random\nperturbations to policy actions during execution and observe their effects on\nthe cumulative reward, learning a simplified high-level causal model that\nexplains these relationships. To this end, we develop a nonlinear Causal Model\nReduction framework that ensures approximate interventional consistency,\nmeaning the simplified high-level model responds to interventions in a similar\nway as the original complex system. We prove that for a class of nonlinear\ncausal models, there exists a unique solution that achieves exact\ninterventional consistency, ensuring learned explanations reflect meaningful\ncausal patterns. Experiments on both synthetic causal models and practical RL\ntasks-including pendulum control and robot table tennis-demonstrate that our\napproach can uncover important behavioral patterns, biases, and failure modes\nin trained RL policies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u56e0\u679c\u89c6\u89d2\u89e3\u91caRL\u7b56\u7565\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u52a8\u4f5c\u5b66\u4e60\u9ad8\u5c42\u56e0\u679c\u6a21\u578b\uff0c\u786e\u4fdd\u5e72\u9884\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u91caRL\u7b56\u7565\u6210\u529f\u6216\u5931\u8d25\u7684\u539f\u56e0\uff0c\u56e0\u5176\u590d\u6742\u9ad8\u7ef4\u7279\u6027\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165\u52a8\u4f5c\u968f\u673a\u6270\u52a8\uff0c\u89c2\u5bdf\u5176\u5bf9\u7d2f\u79ef\u5956\u52b1\u7684\u5f71\u54cd\uff0c\u5b66\u4e60\u9ad8\u5c42\u56e0\u679c\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u975e\u7ebf\u6027\u56e0\u679c\u6a21\u578b\u7b80\u5316\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u80fd\u63ed\u793aRL\u7b56\u7565\u7684\u884c\u4e3a\u6a21\u5f0f\u3001\u504f\u89c1\u548c\u5931\u8d25\u539f\u56e0\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u7b80\u5316\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u89e3\u91caRL\u7b56\u7565\u7684\u884c\u4e3a\uff0c\u786e\u4fdd\u5e72\u9884\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.14146", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14146", "abs": "https://arxiv.org/abs/2507.14146", "authors": ["Kleanthis Avramidis", "Emily Zhou", "Tiantian Feng", "Hossein Hamidi Shishavan", "Frederico Marcolino Quintao Severgnini", "Danny J. Lohan", "Paul Schmalenberg", "Ercan M. Dede", "Shrikanth Narayanan"], "title": "Estimating Markers of Driving Stress through Multimodal Physiological Monitoring", "comment": "11 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication", "summary": "Understanding and mitigating driving stress is vital for preventing accidents\nand advancing both road safety and driver well-being. While vehicles are\nequipped with increasingly sophisticated safety systems, many limits exist in\ntheir ability to account for variable driving behaviors and environmental\ncontexts. In this study we examine how short-term stressor events impact\ndrivers' physiology and their behavioral responses behind the wheel. Leveraging\na controlled driving simulation setup, we collected physiological signals from\n31 adult participants and designed a multimodal machine learning system to\nestimate the presence of stressors. Our analysis explores the model sensitivity\nand temporal dynamics against both known and novel emotional inducers, and\nexamines the relationship between predicted stress and observable patterns of\nvehicle control. Overall, this study demonstrates the potential of linking\nphysiological signals with contextual and behavioral cues in order to improve\nreal-time estimation of driving stress.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u77ed\u671f\u538b\u529b\u4e8b\u4ef6\u5bf9\u9a7e\u9a76\u8005\u751f\u7406\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5b9e\u65f6\u4f30\u8ba1\u9a7e\u9a76\u538b\u529b\u3002", "motivation": "\u9a7e\u9a76\u538b\u529b\u5f71\u54cd\u9053\u8def\u5b89\u5168\u548c\u9a7e\u9a76\u8005\u5065\u5eb7\uff0c\u73b0\u6709\u5b89\u5168\u7cfb\u7edf\u96be\u4ee5\u9002\u5e94\u591a\u53d8\u884c\u4e3a\u548c\u73af\u5883\u3002", "method": "\u5229\u7528\u9a7e\u9a76\u6a21\u62df\u5668\u6536\u96c631\u540d\u6210\u5e74\u4eba\u7684\u751f\u7406\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5206\u6790\u538b\u529b\u3002", "result": "\u6a21\u578b\u80fd\u654f\u611f\u6355\u6349\u538b\u529b\u52a8\u6001\uff0c\u5e76\u63ed\u793a\u538b\u529b\u4e0e\u8f66\u8f86\u63a7\u5236\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "conclusion": "\u751f\u7406\u4fe1\u53f7\u4e0e\u884c\u4e3a\u7ebf\u7d22\u7ed3\u5408\u53ef\u63d0\u5347\u9a7e\u9a76\u538b\u529b\u7684\u5b9e\u65f6\u4f30\u8ba1\u80fd\u529b\u3002"}}
{"id": "2507.14175", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.14175", "abs": "https://arxiv.org/abs/2507.14175", "authors": ["Youcef Barkat", "Dylan Hamitouche", "Deven Parekh", "Ivy Guo", "David Benrimoh"], "title": "Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data", "comment": null, "summary": "Background: Mental illnesses such as depression and anxiety require improved\nmethods for early detection and personalized intervention. Traditional\npredictive models often rely on unimodal data or early fusion strategies that\nfail to capture the complex, multimodal nature of psychiatric data. Advanced\nintegration techniques, such as intermediate (latent space) fusion, may offer\nbetter accuracy and clinical utility. Methods: Using data from the BRIGHTEN\nclinical trial, we evaluated intermediate (latent space) fusion for predicting\ndaily depressive symptoms (PHQ-2 scores). We compared early fusion implemented\nwith a Random Forest (RF) model and intermediate fusion implemented via a\nCombined Model (CM) using autoencoders and a neural network. The dataset\nincluded behavioral (smartphone-based), demographic, and clinical features.\nExperiments were conducted across multiple temporal splits and data stream\ncombinations. Performance was evaluated using mean squared error (MSE) and\ncoefficient of determination (R2). Results: The CM outperformed both RF and\nLinear Regression (LR) baselines across all setups, achieving lower MSE (0.4985\nvs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed\nsigns of overfitting, with a large gap between training and test performance,\nwhile the CM maintained consistent generalization. Performance was best when\nintegrating all data modalities in the CM (in contradistinction to RF),\nunderscoring the value of latent space fusion for capturing non-linear\ninteractions in complex psychiatric datasets. Conclusion: Latent space fusion\noffers a robust alternative to traditional fusion methods for prediction with\nmultimodal mental health data. Future work should explore model\ninterpretability and individual-level prediction for clinical deployment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u65b9\u6cd5\u5728\u9884\u6d4b\u6291\u90c1\u75c7\u72b6\u4e2d\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u3002", "motivation": "\u6539\u5584\u7cbe\u795e\u75be\u75c5\u7684\u65e9\u671f\u68c0\u6d4b\u548c\u4e2a\u6027\u5316\u5e72\u9884\u65b9\u6cd5\uff0c\u4f20\u7edf\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u3002", "method": "\u4f7f\u7528BRIGHTEN\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u6bd4\u8f83\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\uff08CM\uff09\u4e0e\u4f20\u7edf\u968f\u673a\u68ee\u6797\uff08RF\uff09\u548c\u7ebf\u6027\u56de\u5f52\uff08LR\uff09\u7684\u6027\u80fd\u3002", "result": "CM\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\uff0cMSE\u66f4\u4f4e\uff080.4985 vs. 0.5305\uff09\uff0cR2\u66f4\u9ad8\uff080.4695 vs. 0.4356\uff09\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u662f\u591a\u6a21\u6001\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u9884\u6d4b\u7684\u53ef\u9760\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u5173\u6ce8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e2a\u4f53\u5316\u9884\u6d4b\u3002"}}
{"id": "2507.15097", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.15097", "abs": "https://arxiv.org/abs/2507.15097", "authors": ["Subhroshekhar Ghosh", "Soumendu Sundar Mukherjee"], "title": "Learning under Latent Group Sparsity via Diffusion on Networks", "comment": "49 pages, 4 figures, 2 tables; this submission subsumes the earlier\n  preprint arXiv:2201.08326", "summary": "Group or cluster structure on explanatory variables in machine learning\nproblems is a very general phenomenon, which has attracted broad interest from\npractitioners and theoreticians alike. In this work we contribute an approach\nto sparse learning under such group structure, that does not require prior\ninformation on the group identities. Our paradigm is motivated by the Laplacian\ngeometry of an underlying network with a related community structure, and\nproceeds by directly incorporating this into a penalty that is effectively\ncomputed via a heat-flow-based local network dynamics. The proposed penalty\ninterpolates between the lasso and the group lasso penalties, the runtime of\nthe heat-flow dynamics being the interpolating parameter. As such it can\nautomatically default to lasso when the group structure reflected in the\nLaplacian is weak. In fact, we demonstrate a data-driven procedure to construct\nsuch a network based on the available data. Notably, we dispense with\ncomputationally intensive pre-processing involving clustering of variables,\nspectral or otherwise. Our technique is underpinned by rigorous theorems that\nguarantee its effective performance and provide bounds on its sample\ncomplexity. In particular, in a wide range of settings, it provably suffices to\nrun the diffusion for time that is only logarithmic in the problem dimensions.\nWe explore in detail the interfaces of our approach with key statistical\nphysics models in network science, such as the Gaussian Free Field and the\nStochastic Block Model. Our work raises the possibility of applying similar\ndiffusion-based techniques to classical learning tasks, exploiting the\ninterplay between geometric, dynamical and stochastic structures underlying the\ndata.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u5206\u7ec4\u4fe1\u606f\u7684\u7a00\u758f\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u70ed\u6d41\u7684\u7f51\u7edc\u52a8\u6001\u60e9\u7f5a\uff0c\u81ea\u52a8\u5728lasso\u548cgroup lasso\u4e4b\u95f4\u63d2\u503c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u53d8\u91cf\u5206\u7ec4\u7ed3\u6784\u7684\u666e\u904d\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u5148\u9a8c\u5206\u7ec4\u4fe1\u606f\u3002", "method": "\u5229\u7528\u7f51\u7edc\u62c9\u666e\u62c9\u65af\u51e0\u4f55\u548c\u70ed\u6d41\u52a8\u6001\u6784\u5efa\u60e9\u7f5a\u9879\uff0c\u81ea\u52a8\u9002\u5e94\u5206\u7ec4\u5f3a\u5ea6\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u6027\u80fd\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4f4e\uff0c\u6269\u6563\u65f6\u95f4\u4ec5\u9700\u5bf9\u6570\u7ea7\u3002", "conclusion": "\u65b9\u6cd5\u7ed3\u5408\u51e0\u4f55\u3001\u52a8\u6001\u548c\u968f\u673a\u7ed3\u6784\uff0c\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2507.14147", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.14147", "abs": "https://arxiv.org/abs/2507.14147", "authors": ["Kevin Monteiro", "Sam Nallaperuma-Herzberg", "Martina Mason", "Steve Niederer"], "title": "Graph Convolutional Neural Networks to Model the Brain for Insomnia", "comment": "12 pages, 6 figures. This version has been accepted as a full paper\n  at the 2025 AI in Healthcare (AIiH) Conference", "summary": "Insomnia affects a vast population of the world and can have a wide range of\ncauses. Existing treatments for insomnia have been linked with many side\neffects like headaches, dizziness, etc. As such, there is a clear need for\nimproved insomnia treatment. Brain modelling has helped with assessing the\neffects of brain pathology on brain network dynamics and with supporting\nclinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.\nHowever, such models have not been developed for insomnia. Therefore, this\nproject attempts to understand the characteristics of the brain of individuals\nexperiencing insomnia using continuous long-duration EEG data. Brain networks\nare derived based on functional connectivity and spatial distance between EEG\nchannels. The power spectral density of the channels is then computed for the\nmajor brain wave frequency bands. A graph convolutional neural network (GCNN)\nmodel is then trained to capture the functional characteristics associated with\ninsomnia and configured for the classification task to judge performance.\nResults indicated a 50-second non-overlapping sliding window was the most\nsuitable choice for EEG segmentation. This approach achieved a classification\naccuracy of 70% at window level and 68% at subject level. Additionally, the\nomission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in\nmodel performance than the removal of other channels. These channel electrodes\nare positioned near brain regions known to exhibit atypical levels of\nfunctional connectivity in individuals with insomnia, which can explain such\nresults.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08GCNN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7814\u7a76\u5931\u7720\u75c7\u60a3\u8005\u7684\u5927\u8111\u529f\u80fd\u7279\u5f81\uff0c\u5e76\u5b9e\u73b0\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5931\u7720\u6cbb\u7597\u65b9\u6cd5\u526f\u4f5c\u7528\u591a\uff0c\u9700\u6539\u8fdb\u3002\u8111\u6a21\u578b\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c7\u3001\u766b\u75eb\u7b49\u75be\u75c5\u4e2d\u6709\u5e94\u7528\uff0c\u4f46\u672a\u7528\u4e8e\u5931\u7720\u7814\u7a76\u3002", "method": "\u5229\u7528\u957f\u65f6\u95f4\u8fde\u7eedEEG\u6570\u636e\uff0c\u57fa\u4e8e\u529f\u80fd\u8fde\u63a5\u548c\u7a7a\u95f4\u8ddd\u79bb\u6784\u5efa\u8111\u7f51\u7edc\uff0c\u8ba1\u7b97\u4e3b\u8981\u8111\u6ce2\u9891\u6bb5\u7684\u529f\u7387\u8c31\u5bc6\u5ea6\uff0c\u8bad\u7ec3GCNN\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "50\u79d2\u975e\u91cd\u53e0\u6ed1\u52a8\u7a97\u53e3\u6700\u9002\u5408EEG\u5206\u5272\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u5728\u7a97\u53e3\u548c\u53d7\u8bd5\u8005\u5c42\u9762\u5206\u522b\u4e3a70%\u548c68%\u3002\u7279\u5b9a\u7535\u6781\u901a\u9053\uff08C4-P4\u3001F4-C4\u3001C4-A1\uff09\u7684\u7f3a\u5931\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u8bc6\u522b\u5931\u7720\u76f8\u5173\u5927\u8111\u529f\u80fd\u7279\u5f81\uff0c\u7279\u5b9a\u8111\u533a\u529f\u80fd\u8fde\u63a5\u5f02\u5e38\u4e0e\u5931\u7720\u76f8\u5173\u3002"}}
{"id": "2507.14176", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14176", "abs": "https://arxiv.org/abs/2507.14176", "authors": ["Andr\u00e9s Morales-Forero", "Lili J. Rueda", "Ronald Herrera", "Samuel Bassetto", "Eric Coatanea"], "title": "Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection", "comment": null, "summary": "Artificial intelligence (AI) systems increasingly inform medical\ndecision-making, yet concerns about algorithmic bias and inequitable outcomes\npersist, particularly for historically marginalized populations. This paper\nintroduces the concept of Predictive Representativity (PR), a framework of\nfairness auditing that shifts the focus from the composition of the data set to\noutcomes-level equity. Through a case study in dermatology, we evaluated\nAI-based skin cancer classifiers trained on the widely used HAM10000 dataset\nand on an independent clinical dataset (BOSQUE Test set) from Colombia. Our\nanalysis reveals substantial performance disparities by skin phototype, with\nclassifiers consistently underperforming for individuals with darker skin,\ndespite proportional sampling in the source data. We argue that\nrepresentativity must be understood not as a static feature of datasets but as\na dynamic, context-sensitive property of model predictions. PR operationalizes\nthis shift by quantifying how reliably models generalize fairness across\nsubpopulations and deployment contexts. We further propose an External\nTransportability Criterion that formalizes the thresholds for fairness\ngeneralization. Our findings highlight the ethical imperative for post-hoc\nfairness auditing, transparency in dataset documentation, and inclusive model\nvalidation pipelines. This work offers a scalable tool for diagnosing\nstructural inequities in AI systems, contributing to discussions on equity,\ninterpretability, and data justice and fostering a critical re-evaluation of\nfairness in data-driven healthcare.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPredictive Representativity\uff08PR\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u6027\u5ba1\u8ba1\uff0c\u5f3a\u8c03\u4ece\u6570\u636e\u96c6\u7ec4\u6210\u8f6c\u5411\u7ed3\u679c\u5c42\u9762\u7684\u516c\u5e73\u6027\u3002\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0AI\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u5728\u6df1\u8272\u76ae\u80a4\u4eba\u7fa4\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u5e76\u63d0\u51fa\u5916\u90e8\u53ef\u8fc1\u79fb\u6027\u6807\u51c6\u3002", "motivation": "AI\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7b97\u6cd5\u504f\u89c1\u548c\u4e0d\u516c\u5e73\u7ed3\u679c\u95ee\u9898\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u5bf9\u5386\u53f2\u4e0a\u8fb9\u7f18\u5316\u7fa4\u4f53\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528HAM10000\u6570\u636e\u96c6\u548c\u54e5\u4f26\u6bd4\u4e9a\u7684BOSQUE Test set\uff0c\u8bc4\u4f30AI\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51faPR\u6846\u67b6\u548c\u5916\u90e8\u53ef\u8fc1\u79fb\u6027\u6807\u51c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5206\u7c7b\u5668\u5728\u6df1\u8272\u76ae\u80a4\u4eba\u7fa4\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u5c3d\u7ba1\u6e90\u6570\u636e\u4e2d\u91c7\u6837\u6bd4\u4f8b\u5747\u8861\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e8b\u540e\u516c\u5e73\u6027\u5ba1\u8ba1\u3001\u6570\u636e\u96c6\u900f\u660e\u5ea6\u548c\u5305\u5bb9\u6027\u6a21\u578b\u9a8c\u8bc1\u7684\u5fc5\u8981\u6027\uff0c\u4e3aAI\u7cfb\u7edf\u4e2d\u7684\u7ed3\u6784\u6027\u4e0d\u5e73\u7b49\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2507.15235", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15235", "abs": "https://arxiv.org/abs/2507.15235", "authors": ["Miao Huang", "Hongqiao Wang", "Kunyu Wu"], "title": "Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data", "comment": null, "summary": "The Design of Experiments (DOEs) is a fundamental scientific methodology that\nprovides researchers with systematic principles and techniques to enhance the\nvalidity, reliability, and efficiency of experimental outcomes. In this study,\nwe explore optimal experimental design within a Bayesian framework, utilizing\nBayes' theorem to reformulate the utility expectation--originally expressed as\na nested double integral--into an independent double integral form,\nsignificantly improving numerical efficiency. To further accelerate the\ncomputation of the proposed utility expectation, conditional density estimation\nis employed to approximate the ratio of two Gaussian random fields, while\ncovariance serves as a selection criterion to identify informative datasets\nduring model fitting and integral evaluation. In scenarios characterized by low\nsimulation efficiency and high costs of raw data acquisition, key challenges\nsuch as surrogate modeling, failure probability estimation, and parameter\ninference are systematically restructured within the Bayesian experimental\ndesign framework. The effectiveness of the proposed methodology is validated\nthrough both theoretical analysis and practical applications, demonstrating its\npotential for enhancing experimental efficiency and decision-making under\nuncertainty.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u6846\u67b6\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u6548\u7528\u671f\u671b\u7684\u8ba1\u7b97\u5f62\u5f0f\u548c\u4f7f\u7528\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u503c\u6548\u7387\u3002", "motivation": "\u5728\u5b9e\u9a8c\u6548\u7387\u4f4e\u548c\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u7684\u573a\u666f\u4e0b\uff0c\u4f20\u7edf\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u548c\u53ef\u9760\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8d1d\u53f6\u65af\u5b9a\u7406\u91cd\u6784\u6548\u7528\u671f\u671b\u7684\u8ba1\u7b97\u5f62\u5f0f\uff0c\u91c7\u7528\u6761\u4ef6\u5bc6\u5ea6\u4f30\u8ba1\u8fd1\u4f3c\u9ad8\u65af\u968f\u673a\u573a\u6bd4\u7387\uff0c\u5e76\u4ee5\u534f\u65b9\u5dee\u4f5c\u4e3a\u9009\u62e9\u6807\u51c6\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u5b9e\u9a8c\u6548\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8d1d\u53f6\u65af\u5b9e\u9a8c\u8bbe\u8ba1\u6846\u67b6\u4e3a\u5b9e\u9a8c\u4f18\u5316\u548c\u51b3\u7b56\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2507.14148", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14148", "abs": "https://arxiv.org/abs/2507.14148", "authors": ["Daniele Pugliese", "Giovanni Iacovelli", "Alessio Fascista", "Domenico Striccoli", "Oleksandr Romanov", "Luigi Alfredo Grieco", "Gennaro Boggia"], "title": "Visible Light Indoor Positioning with a Single LED and Distributed Single-Element OIRS: An Iterative Approach with Adaptive Beam Steering", "comment": null, "summary": "The integration of Optical Intelligent Reflective Surfaces (OIRSs) into\nVisible Light Communication (VLC) systems is gaining momentum as a valid\nalternative to RF technologies, harnessing the existing lighting\ninfrastructures and the vast unlicensed optical spectrum to enable higher\nspectral efficiency, improved resilience to Line-of-Sight (LoS) blockages, and\nenhanced positioning capabilities. This paper investigates the problem of\nlocalizing a low-cost Photo Detector (PD) in a VLC-based indoor environment\nconsisting of only a single Light Emitting Diode (LED) as an active anchor, and\nmultiple spatially distributed single-element OIRSs. We formulate the problem\nwithin an indirect, computationally efficient localization framework: first,\nthe optimal Maximum Likelihood (ML) estimators of the LoS and Non-Line-of-Sight\n(NLoS) distances are derived, using a suitable OIRS activation strategy to\nprevent interferences. To overcome the grid-based optimization required by the\nML NLoS estimator, we devise a novel algorithm based on an unstructured noise\nvariance transformation, which admits a closed-form solution. The set of\nestimated LoS/NLoS distances are then used within a low-complexity localization\nalgorithm combining an Iterative Weighted Least Squares (IWLS) procedure, whose\nweights are set according to the inverse of the Cram\\'er-Rao Lower Bound\n(CRLB), with an adaptive beam steering strategy that allows the OIRSs network\nto dynamically align with the PD, without any prior knowledge of its position.\nAccordingly, we derive the CRLB for both LoS/NLoS distance estimation and PD\nposition estimation. Simulation results demonstrate the effectiveness of our\napproach in terms of localization accuracy, robustness against OIRSs\nmisalignment conditions, and low number of iterations required to attain the\ntheoretical bounds.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5355LED\u548c\u591aOIRS\u7684VLC\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u95f4\u63a5\u5b9a\u4f4d\u6846\u67b6\u548cML\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408IWLS\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u6ce2\u675f\u63a7\u5236\uff0c\u5b9e\u73b0\u4f4e\u6210\u672cPD\u7684\u9ad8\u6548\u5b9a\u4f4d\u3002", "motivation": "\u5229\u7528OIRS\u548cVLC\u7cfb\u7edf\u63d0\u5347\u9891\u8c31\u6548\u7387\u3001\u6297LOS\u963b\u585e\u80fd\u529b\u548c\u5b9a\u4f4d\u80fd\u529b\uff0c\u89e3\u51b3\u5355LED\u73af\u5883\u4e0b\u4f4e\u6210\u672cPD\u7684\u5b9a\u4f4d\u95ee\u9898\u3002", "method": "\u91c7\u7528ML\u4f30\u8ba1\u5668\u4f18\u5316LoS/NLoS\u8ddd\u79bb\uff0c\u63d0\u51fa\u57fa\u4e8e\u566a\u58f0\u65b9\u5dee\u53d8\u6362\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408IWLS\u548c\u81ea\u9002\u5e94\u6ce2\u675f\u63a7\u5236\u5b9e\u73b0\u5b9a\u4f4d\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u65b9\u6cd5\u5b9a\u4f4d\u7cbe\u5ea6\u9ad8\uff0c\u5bf9OIRS\u5931\u51c6\u9c81\u68d2\uff0c\u4e14\u8fed\u4ee3\u6b21\u6570\u5c11\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728VLC\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684PD\u5b9a\u4f4d\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14177", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "68T07(Primary), 41A15(Secondary)", "I.2.6; G.1.2"], "pdf": "https://arxiv.org/pdf/2507.14177", "abs": "https://arxiv.org/abs/2507.14177", "authors": ["Changcun Huang"], "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions", "comment": null, "summary": "This paper aims to understand the training solution, which is obtained by the\nback-propagation algorithm, of two-layer neural networks whose hidden layer is\ncomposed of the units with smooth activation functions, including the usual\nsigmoid type most commonly used before the advent of ReLUs. The mechanism\ncontains four main principles: construction of Taylor series expansions, strict\npartial order of knots, smooth-spline implementation and smooth-continuity\nrestriction. The universal approximation for arbitrary input dimensionality is\nproved and experimental verification is given, through which the mystery of\n``black box'' of the solution space is largely revealed. The new proofs\nemployed also enrich approximation theory.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u89e3\uff0c\u63ed\u793a\u4e86\u5176\u9690\u85cf\u5c42\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u4e0b\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u5305\u62ec\u6cf0\u52d2\u5c55\u5f00\u3001\u4e25\u683c\u8282\u70b9\u504f\u5e8f\u3001\u5e73\u6ed1\u6837\u6761\u5b9e\u73b0\u548c\u8fde\u7eed\u6027\u9650\u5236\u3002", "motivation": "\u7406\u89e3\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5728\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u4e0b\u7684\u89e3\u7a7a\u95f4\uff0c\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u201c\u9ed1\u7bb1\u201d\u7684\u673a\u5236\u3002", "method": "\u901a\u8fc7\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u3001\u4e25\u683c\u8282\u70b9\u504f\u5e8f\u3001\u5e73\u6ed1\u6837\u6761\u5b9e\u73b0\u548c\u8fde\u7eed\u6027\u9650\u5236\u56db\u79cd\u673a\u5236\u5206\u6790\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u610f\u8f93\u5165\u7ef4\u5ea6\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u89e3\u7a7a\u95f4\u7684\u201c\u9ed1\u7bb1\u201d\u673a\u5236\uff0c\u4e30\u5bcc\u4e86\u903c\u8fd1\u7406\u8bba\u3002"}}
{"id": "2507.14152", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.14152", "abs": "https://arxiv.org/abs/2507.14152", "authors": ["Frank Efe Erukainure", "Feidra Gjata", "Matin Ataei Kachouei", "Henry Cox", "Md. Azahar Ali"], "title": "Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors", "comment": "34 pages, 9 figures", "summary": "River water quality monitoring is important for aquatic life, livestock, and\nhumans because clean water is critical to meeting food demand during the global\nfood crisis. Excessive contaminants, including phosphate, deplete dissolved\noxygen and trigger eutrophication, leading to serious health and ecological\nproblems. Continuous sensors that track phosphate levels can therefore help\nprevent eutrophication. In this work we present a lithography-free phosphate\nsensor (P-sensor) that detects phosphate in river water at parts-per-billion\nlevels. The device uses a solid-state indicator electrode formed by 3D-printed\nperiodic polymer patterns (8 um feature size) coated with a thin phosphate\nion-selective membrane. The P-sensor detects as little as 1 ppb phosphate\nacross 0 - 475 ppm with a response time under 30 seconds. We validated the\nsensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at\nsites upstream and downstream of a sewage treatment plant and benchmarked the\nresults against a commercial phosphate meter. A feed-forward neural network was\ntrained to predict phosphate levels, achieving a mean-squared error below 1e-3,\nzero standard deviation, and a Pearson correlation coefficient of 0.997 for\nriver samples. These results demonstrate a practical tool for continuous\nwater-quality monitoring that can inform stakeholders and policymakers and\nultimately improve public health.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65e0\u5149\u523b\u6280\u672f\u7684\u78f7\u9178\u76d0\u4f20\u611f\u5668\uff08P-sensor\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u6cb3\u6d41\u4e2d\u4f4e\u81f31 ppb\u7684\u78f7\u9178\u76d0\uff0c\u54cd\u5e94\u65f6\u95f4\u77ed\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u6cb3\u6d41\u6c34\u8d28\u76d1\u6d4b\u5bf9\u751f\u6001\u548c\u4eba\u7c7b\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u78f7\u9178\u76d0\u6c61\u67d3\u4f1a\u5bfc\u81f4\u5bcc\u8425\u517b\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u76d1\u6d4b\u5de5\u5177\u3002", "method": "\u91c7\u75283D\u6253\u5370\u805a\u5408\u7269\u56fe\u6848\u6d82\u8986\u78f7\u9178\u76d0\u9009\u62e9\u6027\u819c\uff0c\u6784\u5efa\u56fa\u6001\u6307\u793a\u7535\u6781\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u78f7\u9178\u76d0\u6d53\u5ea6\u3002", "result": "\u4f20\u611f\u5668\u57280-475 ppm\u8303\u56f4\u5185\u68c0\u6d4b1 ppb\u78f7\u9178\u76d0\uff0c\u54cd\u5e94\u65f6\u95f4<30\u79d2\uff0c\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u8bef\u5dee\u4f4e\uff0c\u76f8\u5173\u6027\u9ad8\u3002", "conclusion": "\u8be5\u4f20\u611f\u5668\u4e3a\u8fde\u7eed\u6c34\u8d28\u76d1\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u516c\u5171\u5065\u5eb7\u548c\u653f\u7b56\u5236\u5b9a\u3002"}}
{"id": "2507.15681", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15681", "abs": "https://arxiv.org/abs/2507.15681", "authors": ["Pegah Golchian", "Jan Kapar", "David S. Watson", "Marvin N. Wright"], "title": "Missing value imputation with adversarial random forests -- MissARF", "comment": null, "summary": "Handling missing values is a common challenge in biostatistical analyses,\ntypically addressed by imputation methods. We propose a novel, fast, and\neasy-to-use imputation method called missing value imputation with adversarial\nrandom forests (MissARF), based on generative machine learning, that provides\nboth single and multiple imputation. MissARF employs adversarial random forest\n(ARF) for density estimation and data synthesis. To impute a missing value of\nan observation, we condition on the non-missing values and sample from the\nestimated conditional distribution generated by ARF. Our experiments\ndemonstrate that MissARF performs comparably to state-of-the-art single and\nmultiple imputation methods in terms of imputation quality and fast runtime\nwith no additional costs for multiple imputation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u7684\u5feb\u901f\u3001\u6613\u7528\u7684\u7f3a\u5931\u503c\u586b\u8865\u65b9\u6cd5MissARF\uff0c\u7ed3\u5408\u5bf9\u6297\u968f\u673a\u68ee\u6797\uff08ARF\uff09\u8fdb\u884c\u5bc6\u5ea6\u4f30\u8ba1\u548c\u6570\u636e\u5408\u6210\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u751f\u7269\u7edf\u8ba1\u5206\u6790\u4e2d\u7f3a\u5931\u503c\u5904\u7406\u662f\u5e38\u89c1\u6311\u6218\uff0c\u4f20\u7edf\u586b\u8865\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u63d0\u51fa\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5bf9\u6297\u968f\u673a\u68ee\u6797\uff08ARF\uff09\u8fdb\u884c\u5bc6\u5ea6\u4f30\u8ba1\u548c\u6570\u636e\u5408\u6210\uff0c\u57fa\u4e8e\u975e\u7f3a\u5931\u503c\u6761\u4ef6\u91c7\u6837\u586b\u8865\u7f3a\u5931\u503c\u3002", "result": "MissARF\u5728\u586b\u8865\u8d28\u91cf\u548c\u8fd0\u884c\u901f\u5ea6\u4e0a\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u591a\u586b\u8865\u65e0\u989d\u5916\u6210\u672c\u3002", "conclusion": "MissARF\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u7f3a\u5931\u503c\u586b\u8865\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5355\u586b\u8865\u548c\u591a\u586b\u8865\u573a\u666f\u3002"}}
{"id": "2507.14151", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14151", "abs": "https://arxiv.org/abs/2507.14151", "authors": ["Giuliana Monachino", "Nicol\u00f2 La Porta", "Beatrice Zanchi", "Luigi Fiorillo", "Alvise Dei Rossi", "Georgiy Farina", "Francesca Dalia Faraci"], "title": "Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models", "comment": null, "summary": "Foundation Models (FMs) are large-scale machine learning models trained on\nextensive, diverse datasets that can be adapted to a wide range of downstream\ntasks with minimal fine-tuning. In the last two years, interest in FMs has also\ngrown for applications in the cardiological field to analyze the\nelectrocardiogram (ECG) signals. One of the key properties of FMs is their\ntransferability to a wide range of downstream scenarios. With the spread of\nwearable and portable devices, keen interest in learning from reduced-channel\nconfigurations has arisen. However, the adaptation of ECG FMs to downstream\nscenarios with fewer available channels still has to be properly investigated.\nIn this work, we propose Self-DANA, a novel, easy-to-integrate solution that\nmakes self-supervised architectures adaptable to a reduced number of input\nchannels, ensuring resource efficiency and high performance. We also introduce\nRandom Lead Selection, a novel augmentation technique to pre-train models in a\nmore robust and channel-agnostic way. Our experimental results on five\nreduced-channel configurations demonstrate that Self-DANA significantly\nenhances resource efficiency while reaching state-of-the-art performance. It\nrequires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about\n17% less average epoch CPU time, and about 24% less average epoch GPU time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSelf-DANA\u65b9\u6cd5\uff0c\u4f7f\u81ea\u76d1\u7763\u67b6\u6784\u9002\u5e94\u51cf\u5c11\u7684\u8f93\u5165\u901a\u9053\uff0c\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u7814\u7a76\u5982\u4f55\u5c06ECG\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u9002\u5e94\u4e8e\u51cf\u5c11\u901a\u9053\u7684\u4e0b\u6e38\u573a\u666f\u3002", "method": "\u63d0\u51faSelf-DANA\u548cRandom Lead Selection\u589e\u5f3a\u6280\u672f\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u540c\u901a\u9053\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSelf-DANA\u663e\u8457\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\uff0c\u51cf\u5c11CPU\u548cGPU\u5185\u5b58\u53ca\u65f6\u95f4\u6d88\u8017\uff0c\u6027\u80fd\u8fbe\u5230SOTA\u3002", "conclusion": "Self-DANA\u4e3a\u51cf\u5c11\u901a\u9053\u7684ECG\u5206\u6790\u63d0\u4f9b\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14178", "abs": "https://arxiv.org/abs/2507.14178", "authors": ["Yuhang Liu", "Yuefei Wu", "Bin Shi", "Bo Dong"], "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution Detection", "comment": "8 pages, 5 figures", "summary": "Out-of-distribution (OOD) detection is critical to ensuring the reliability\nof deep learning applications and has attracted significant attention in recent\nyears. A rich body of literature has emerged to develop efficient score\nfunctions that assign high scores to in-distribution (ID) samples and low\nscores to OOD samples, thereby helping distinguish OOD samples. Among these\nmethods, distance-based score functions are widely used because of their\nefficiency and ease of use. However, deep learning often leads to a biased\ndistribution of data features, and extreme features are inevitable. These\nextreme features make the distance-based methods tend to assign too low scores\nto ID samples. This limits the OOD detection capabilities of such methods. To\naddress this issue, we propose a simple yet effective method, Feature Bank\nEnhancement (FBE), that uses statistical characteristics from dataset to\nidentify and constrain extreme features to the separation boundaries, therapy\nmaking the distance between samples inside and outside the distribution\nfarther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10\nrespectively, and the results show that our method achieves state-of-the-art\nperformance on both benchmark. Additionally, theoretical analysis and\nsupplementary experiments are conducted to provide more insights into our\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFeature Bank Enhancement (FBE)\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u57fa\u4e8e\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6781\u7aef\u7279\u5f81\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8ddd\u79bb\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6570\u636e\u7279\u5f81\u5206\u5e03\u504f\u5dee\u548c\u6781\u7aef\u7279\u5f81\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u5f81\u8bc6\u522b\u5e76\u7ea6\u675f\u6781\u7aef\u7279\u5f81\uff0c\u6269\u5927\u5206\u5e03\u5185\u5916\u6837\u672c\u7684\u8ddd\u79bb\u3002", "result": "\u5728ImageNet-1k\u548cCIFAR-10\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FBE\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.14165", "categories": ["eess.SP", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14165", "abs": "https://arxiv.org/abs/2507.14165", "authors": ["Philip Wiese", "Victor Kartsch", "Marco Guermandi", "Luca Benini"], "title": "A Multi-Modal IoT Node for Energy-Efficient Environmental Monitoring with Edge AI Processing", "comment": "7 pages, 4 figures, 2 tables. This paper has been accepted at 2025\n  IEEE International Conference on Omni-layer Intelligent Systems (COINS)", "summary": "The widespread adoption of Internet of Things (IoT) technologies has\nsignificantly advanced environmental monitoring (EM) by enabling cost-effective\nand scalable sensing solutions. Concurrently, machine learning (ML) and\nartificial intelligence (AI) are introducing powerful tools for the efficient\nand accurate analysis of complex environmental data. However, current IoT\nplatforms for environmental sensing are typically limited to a narrow set of\nsensors, preventing a comprehensive assessment of environmental conditions and\nlacking sufficient computational capabilities to support the deployment of\nadvanced ML and AI algorithms on the edge. To overcome these limitations, we\nintroduce a compact (17x38 mm2), multi-modal, MCU-based environmental IoT node\nintegrating 11 sensors, including CO2 concentration, volatile organic compounds\n(VOCs), light intensity, UV radiation, pressure, temperature, humidity, visual\nsensing via an RGB camera, and precise geolocation through a GNSS module. It\nfeatures GAP9, a parallel ultra-low-power system-on-chip, enabling real-time,\nenergy-efficient edge processing of advanced ML models directly on-device. We\nimplemented a YOLOv5-based occupancy detection pipeline (0.3 M parameters, 42\nMOP per inference), demonstrating 42% energy savings over raw data streaming.\nAdditionally, we present a smart indoor air quality (IAQ) monitoring setup that\ncombines occupancy detection with adaptive sample rates, achieving operational\ntimes of up to 143 h on a single compact 600 mAh, 3.7 V battery. Our platform\nlays the groundwork for innovative applications such as predictive indoor IAQ,\nenabling efficient AI-driven on-edge forecasting for energy-efficient and\nautonomous, proactive pollution-mitigation control strategies", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7d27\u51d1\u578b\u591a\u6a21\u6001MCU\u73af\u5883\u7269\u8054\u7f51\u8282\u70b9\uff0c\u96c6\u6210\u4e8611\u79cd\u4f20\u611f\u5668\uff0c\u652f\u6301\u8fb9\u7f18\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u8282\u80fd\u9ad8\u6548\u7684ML\u6a21\u578b\u5904\u7406\u3002", "motivation": "\u5f53\u524d\u7269\u8054\u7f51\u73af\u5883\u76d1\u6d4b\u5e73\u53f0\u4f20\u611f\u5668\u79cd\u7c7b\u6709\u9650\u4e14\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u652f\u6301\u5148\u8fdb\u7684ML\u548cAI\u7b97\u6cd5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u96c6\u621011\u79cd\u4f20\u611f\u5668\u7684\u7d27\u51d1\u578bMCU\u8282\u70b9\uff0c\u91c7\u7528GAP9\u82af\u7247\u5b9e\u73b0\u5b9e\u65f6\u4f4e\u529f\u8017\u8fb9\u7f18\u5904\u7406\uff0c\u5e76\u90e8\u7f72\u4e86YOLOv5\u6a21\u578b\u8fdb\u884c\u5360\u7528\u68c0\u6d4b\u3002", "result": "\u5b9e\u73b0\u4e8642%\u7684\u80fd\u8017\u8282\u7701\uff0c\u5355\u6b21\u5145\u7535\u53ef\u8fd0\u884c143\u5c0f\u65f6\uff0c\u652f\u6301\u667a\u80fd\u5ba4\u5185\u7a7a\u6c14\u8d28\u91cf\u76d1\u6d4b\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u9884\u6d4b\u6027\u5ba4\u5185\u7a7a\u6c14\u8d28\u91cf\u76d1\u6d4b\u7b49\u521b\u65b0\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u9ad8\u6548AI\u9a71\u52a8\u7684\u8fb9\u7f18\u9884\u6d4b\u6280\u672f\u3002"}}
{"id": "2507.15741", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.15741", "abs": "https://arxiv.org/abs/2507.15741", "authors": ["G\u00e1bor Lugosi", "Marcos Matabuena"], "title": "Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces", "comment": null, "summary": "This paper introduces a framework for uncertainty quantification in\nregression models defined in metric spaces. Leveraging a newly defined notion\nof homoscedasticity, we develop a conformal prediction algorithm that offers\nfinite-sample coverage guarantees and fast convergence rates of the oracle\nestimator. In heteroscedastic settings, we forgo these non-asymptotic\nguarantees to gain statistical efficiency, proposing a local\n$k$--nearest--neighbor method without conformal calibration that is adaptive to\nthe geometry of each particular nonlinear space. Both procedures work with any\nregression algorithm and are scalable to large data sets, allowing\npractitioners to plug in their preferred models and incorporate domain\nexpertise. We prove consistency for the proposed estimators under minimal\nconditions. Finally, we demonstrate the practical utility of our approach in\npersonalized--medicine applications involving random response objects such as\nprobability distributions and graph Laplacians.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u5305\u62ec\u540c\u65b9\u5dee\u548c\u5f02\u65b9\u5dee\u60c5\u51b5\u4e0b\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5ea6\u91cf\u7a7a\u95f4\u4e2d\u56de\u5f52\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u4e3a\u4e2a\u6027\u5316\u533b\u7597\u7b49\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51fa\u540c\u65b9\u5dee\u60c5\u51b5\u4e0b\u7684\u4fdd\u5f62\u9884\u6d4b\u7b97\u6cd5\u548c\u5f02\u65b9\u5dee\u60c5\u51b5\u4e0b\u7684\u5c40\u90e8k\u8fd1\u90bb\u65b9\u6cd5\uff0c\u65e0\u9700\u4fdd\u5f62\u6821\u51c6\u3002", "result": "\u8bc1\u660e\u4e86\u4f30\u8ba1\u5668\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u4e2a\u6027\u5316\u533b\u7597\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u6846\u67b6\u7075\u6d3b\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56de\u5f52\u7b97\u6cd5\u548c\u9886\u57df\u77e5\u8bc6\u3002"}}
{"id": "2507.14179", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14179", "abs": "https://arxiv.org/abs/2507.14179", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u4e3a\u5c11\u91cf\u4ee3\u8868\u6027\u805a\u7c7b\uff0c\u9ad8\u6548\u9884\u6d4b\u548c\u5229\u7528LLMs\u4e2d\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u4e3a\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u76f4\u63a5\u9884\u6d4b\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u805a\u7c7b\u65b9\u6cd5\uff0c\u5c06\u76f8\u4f3c\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u4e3a\u5c11\u91cf\u4ee3\u8868\u6027\u805a\u7c7b\uff0c\u9884\u6d4b\u805a\u7c7b\u5206\u914d\u800c\u975e\u5355\u4e2a\u795e\u7ecf\u5143\u72b6\u6001\u3002", "result": "\u805a\u7c7b\u7cbe\u5ea6\u8fbe79.34%\uff0c\u56f0\u60d1\u5ea6\uff08PPL\uff09\u6700\u4f4e12.49\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6fc0\u6d3b\u6a21\u5f0f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u57fa\u7840\uff0c\u6709\u671b\u63d0\u5347\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.14194", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14194", "abs": "https://arxiv.org/abs/2507.14194", "authors": ["David J Poland"], "title": "Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics", "comment": "Preliminary version of a predictive maintenance framework using\n  spiking neural networks and entropy-based analysis. To be expanded in future\n  publications with hardware implementations and real-time drift detection\n  modules. arXiv admin note: substantial text overlap with arXiv:2501.05087", "summary": "This paper presents a novel framework for pattern prediction and system\nprognostics centered on Spatiotemporal Permutation Entropy analysis integrated\nwith Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address\nthe challenge of understanding complex dynamical patterns in multidimensional\nsystems through an approach that combines entropy-based complexity measures\nwith advanced neural architectures. The system leverages dual computational\nstages: first implementing spatiotemporal entropy extraction optimized for\nmultiscale temporal and spatial data streams, followed by an integrated BEQRNN\nlayer that enables probabilistic pattern prediction with uncertainty\nquantification. This architecture achieves 81.17% accuracy in spatiotemporal\npattern classification with prediction horizons up to 200 time steps and\nmaintains robust performance across diverse regimes. Field testing across\nchaotic attractors, reaction-diffusion systems, and industrial datasets shows a\n79% increase in critical transition detection accuracy and 81.22% improvement\nin long-term prediction reliability. The framework's effectiveness in\nprocessing complex, multimodal entropy features demonstrates significant\npotential for real-time prognostic applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u6392\u5217\u71b5\u5206\u6790\u548c\u589e\u5f3a\u5206\u4f4d\u6570\u56de\u5f52\u795e\u7ecf\u7f51\u7edc\uff08BEQRNNs\uff09\u7684\u6a21\u5f0f\u9884\u6d4b\u548c\u7cfb\u7edf\u9884\u540e\u6846\u67b6\uff0c\u7ed3\u5408\u71b5\u590d\u6742\u5ea6\u6d4b\u91cf\u4e0e\u9ad8\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u52a8\u6001\u6a21\u5f0f\u7684\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u591a\u7ef4\u7cfb\u7edf\u4e2d\u590d\u6742\u52a8\u6001\u6a21\u5f0f\u7406\u89e3\u7684\u6311\u6218\uff0c\u7ed3\u5408\u71b5\u590d\u6742\u5ea6\u6d4b\u91cf\u4e0e\u795e\u7ecf\u7f51\u7edc\u6280\u672f\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u8ba1\u7b97\uff1a\u9996\u5148\u4f18\u5316\u65f6\u7a7a\u71b5\u63d0\u53d6\uff0c\u968f\u540e\u96c6\u6210BEQRNN\u5c42\u8fdb\u884c\u6982\u7387\u6a21\u5f0f\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u65f6\u7a7a\u6a21\u5f0f\u5206\u7c7b\u4e2d\u8fbe\u523081.17%\u7684\u51c6\u786e\u7387\uff0c\u9884\u6d4b\u8303\u56f4\u8fbe200\u65f6\u95f4\u6b65\uff0c\u5173\u952e\u8fc7\u6e21\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u534779%\uff0c\u957f\u671f\u9884\u6d4b\u53ef\u9760\u6027\u63d0\u9ad881.22%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u71b5\u7279\u5f81\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u65f6\u9884\u540e\u5e94\u7528\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.15802", "categories": ["stat.ML", "cs.LG", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.15802", "abs": "https://arxiv.org/abs/2507.15802", "authors": ["R\u00e9mi Vaucher", "Paul Minchella"], "title": "Hypergraphs on high dimensional time series sets using signature transform", "comment": "Accepted at GSI25 conference. Pending publication in Springer\n  proceedings", "summary": "In recent decades, hypergraphs and their analysis through Topological Data\nAnalysis (TDA) have emerged as powerful tools for understanding complex data\nstructures. Various methods have been developed to construct hypergraphs --\nreferred to as simplicial complexes in the TDA framework -- over datasets,\nenabling the formation of edges between more than two vertices. This paper\naddresses the challenge of constructing hypergraphs from collections of\nmultivariate time series. While prior work has focused on the case of a single\nmultivariate time series, we extend this framework to handle collections of\nsuch time series. Our approach generalizes the method proposed in Chretien and\nal. by leveraging the properties of signature transforms to introduce\ncontrolled randomness, thereby enhancing the robustness of the construction\nprocess. We validate our method on synthetic datasets and present promising\nresults.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u96c6\u5408\u6784\u5efa\u8d85\u56fe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b7e\u540d\u53d8\u6362\u5f15\u5165\u968f\u673a\u6027\uff0c\u63d0\u9ad8\u4e86\u6784\u5efa\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4ece\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u96c6\u5408\u6784\u5efa\u8d85\u56fe\u7684\u6311\u6218\uff0c\u6269\u5c55\u4e86\u5355\u65f6\u95f4\u5e8f\u5217\u7684\u73b0\u6709\u6846\u67b6\u3002", "method": "\u5229\u7528\u7b7e\u540d\u53d8\u6362\u7684\u7279\u6027\u5f15\u5165\u53ef\u63a7\u968f\u673a\u6027\uff0c\u6269\u5c55\u4e86Chretien\u7b49\u4eba\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u96c6\u5408\u7684\u8d85\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14153", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14153", "abs": "https://arxiv.org/abs/2507.14153", "authors": ["Daniel Cie\u015blak", "Barbara Szyca", "Weronika Bajko", "Liwia Florkiewicz", "Kinga Grz\u0119da", "Mariusz Kaczmarek", "Helena Kamieniecka", "Hubert Lis", "Weronika Matwiejuk", "Anna Prus", "Michalina Razik", "Inga Rozumowicz", "Wiktoria Ziembakowska"], "title": "Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM", "comment": "International Conference on Hybrid Artificial Intelligence Systems\n  (HAIS 2024)", "summary": "Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to\nits progressive nature and complex symptoms. This study introduces a novel\napproach utilizing surface electromyography (sEMG) to objectively assess PD\nseverity, focusing on the biceps brachii muscle. Initial analysis of sEMG data\nfrom five PD patients and five healthy controls revealed significant\nneuromuscular differences. A traditional Support Vector Machine (SVM) model\nachieved up to 83% accuracy, while enhancements with a Graph Convolutional\nNetwork-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.\nDespite the preliminary nature of these results, the study outlines a detailed\nexperimental methodology for future research with larger cohorts to validate\nthese findings and integrate the approach into clinical practice. The proposed\napproach holds promise for advancing PD severity assessment and improving\npatient care in Parkinson's disease management.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8868\u9762\u808c\u7535\u56fe\uff08sEMG\uff09\u8bc4\u4f30\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u4e25\u91cd\u7a0b\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u7684GCN-SVM\u6a21\u578b\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f392%\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u7684\u8bca\u65ad\u548c\u76d1\u6d4b\u56e0\u5176\u590d\u6742\u75c7\u72b6\u548c\u6e10\u8fdb\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4f7f\u7528sEMG\u5206\u6790\u80b1\u4e8c\u5934\u808c\u808c\u8089\u6d3b\u52a8\uff0c\u6bd4\u8f83PD\u60a3\u8005\u548c\u5065\u5eb7\u5bf9\u7167\u7ec4\u7684\u6570\u636e\uff0c\u5e76\u91c7\u7528SVM\u548cGCN-SVM\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793aSVM\u51c6\u786e\u7387\u4e3a83%\uff0cGCN-SVM\u63d0\u5347\u81f392%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aPD\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u9700\u6269\u5927\u6837\u672c\u9a8c\u8bc1\u5e76\u5e94\u7528\u4e8e\u4e34\u5e8a\u3002"}}
{"id": "2507.14180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14180", "abs": "https://arxiv.org/abs/2507.14180", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems", "comment": null, "summary": "In line with the AI-native 6G vision, explainability and robustness are\ncrucial for building trust and ensuring reliable performance in millimeter-wave\n(mmWave) systems. Efficient beam alignment is essential for initial access, but\ndeep learning (DL) solutions face challenges, including high data collection\noverhead, hardware constraints, lack of explainability, and susceptibility to\nadversarial attacks. This paper proposes a robust and explainable DL-based beam\nalignment engine (BAE) for mmWave multiple-input multiple output (MIMO)\nsystems. The BAE uses received signal strength indicator (RSSI) measurements\nfrom wide beams to predict the best narrow beam, reducing the overhead of\nexhaustive beam sweeping. To overcome the challenge of real-world data\ncollection, this work leverages a site-specific digital twin (DT) to generate\nsynthetic channel data closely resembling real-world environments. A model\nrefinement via transfer learning is proposed to fine-tune the pre-trained model\nresiding in the DT with minimal real-world data, effectively bridging\nmismatches between the digital replica and real-world environments. To reduce\nbeam training overhead and enhance transparency, the framework uses deep\nShapley additive explanations (SHAP) to rank input features by importance,\nprioritizing key spatial directions and minimizing beam sweeping. It also\nincorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a\ncredibility metric for detecting out-of-distribution inputs and ensuring\nrobust, transparent decision-making. Experimental results show that the\nproposed framework reduces real-world data needs by 70%, beam training overhead\nby 62%, and improves outlier detection robustness by up to 8.5x, achieving\nnear-optimal spectral efficiency and transparent decision making compared to\ntraditional softmax based DL models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\u6ce2\u675f\u5bf9\u9f50\u5f15\u64ce\uff08BAE\uff09\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u548c\u8fc1\u79fb\u5b66\u4e60\u51cf\u5c11\u6570\u636e\u6536\u96c6\u5f00\u9500\uff0c\u5e76\u5229\u7528SHAP\u548cDkNN\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u57286G\u613f\u666f\u4e0b\uff0c\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u5efa\u7acb\u4fe1\u4efb\u548c\u786e\u4fdd\u53ef\u9760\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u6536\u96c6\u5f00\u9500\u5927\u3001\u786c\u4ef6\u9650\u5236\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff1b\u7ed3\u5408SHAP\u548cDkNN\u5b9e\u73b0\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c1170%\u771f\u5b9e\u6570\u636e\u9700\u6c42\u300162%\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\uff0c\u5f02\u5e38\u68c0\u6d4b\u9c81\u68d2\u6027\u63d0\u53478.5\u500d\uff0c\u63a5\u8fd1\u6700\u4f18\u9891\u8c31\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6beb\u7c73\u6ce2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u900f\u660e\u4e14\u9c81\u68d2\u7684\u6ce2\u675f\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14219", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14219", "abs": "https://arxiv.org/abs/2507.14219", "authors": ["Obumneme Zimuzor Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman", "comment": null, "summary": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u7eff\u8272\u6c22\u4ea7\u91cf\u548c\u9009\u5740\u9002\u5b9c\u6027\u6307\u6570\uff0c\u7ed3\u5408\u4e86\u591a\u53d8\u91cf\u805a\u7c7b\u3001\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548cSHAP\u7b97\u6cd5\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u5730\u533a\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u51b3\u7b56\u5de5\u5177\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u76f4\u63a5\u7684\u6c22\u4ea7\u91cf\u6570\u636e\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e3b\u89c2\u4e13\u5bb6\u6743\u91cd\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5ba2\u89c2\u3001\u53ef\u91cd\u590d\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u4f18\u5316\u7eff\u8272\u6c22\u751f\u4ea7\u9009\u5740\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5AI\u6846\u67b6\uff0c\u5305\u62ec\u65e0\u76d1\u7763\u591a\u53d8\u91cf\u805a\u7c7b\u3001\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548cSHAP\u7b97\u6cd5\uff0c\u6574\u5408\u6c14\u8c61\u3001\u5730\u5f62\u548c\u65f6\u95f4\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe98%\uff0c\u63ed\u793a\u4e86\u6c34\u63a5\u8fd1\u5ea6\u3001\u6d77\u62d4\u548c\u5b63\u8282\u53d8\u5316\u662f\u963f\u66fc\u7eff\u8272\u6c22\u9009\u5740\u7684\u6700\u5173\u952e\u56e0\u7d20\uff08SHAP\u503c\u5206\u522b\u4e3a2.470891\u30012.376296\u548c1.273216\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u884c\u4e1a\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u5730\u533a\u7684\u7eff\u8272\u6c22\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\uff0c\u907f\u514d\u4e86\u4e3b\u89c2\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14163", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14163", "abs": "https://arxiv.org/abs/2507.14163", "authors": ["Renxiang Qiu", "Raghavendra Selvan"], "title": "UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification", "comment": "Accepted to be presented at the 35th IEEE International Workshop on\n  Machine Learning for Signal Processing (IEEE MLSP 2025). Source code\n  available at https://github.com/HughYau/UniPhyNet", "summary": "We present UniPhyNet, a novel neural network architecture to classify\ncognitive load using multimodal physiological data -- specifically EEG, ECG and\nEDA signals -- without the explicit need for extracting hand-crafted features.\nUniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type\nblocks enhanced with channel block attention module to focus on the informative\nfeatures while a bidirectional gated recurrent unit is used to capture temporal\ndependencies. This architecture processes and combines signals in both unimodal\nand multimodal configurations via intermediate fusion of learned feature maps.\nOn the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy\nfrom 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based\nmodels, demonstrating its effectiveness as an end-to-end solution for\nreal-world cognitive state monitoring.", "AI": {"tldr": "UniPhyNet\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u901a\u8fc7\u591a\u6a21\u6001\u751f\u7406\u6570\u636e\uff08EEG\u3001ECG\u3001EDA\uff09\u5206\u7c7b\u8ba4\u77e5\u8d1f\u8377\uff0c\u65e0\u9700\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u63d0\u53d6\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u8ba4\u77e5\u72b6\u6001\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u591a\u5c3a\u5ea6\u5e76\u884c\u5377\u79ef\u5757\u3001ResNet\u5757\u548c\u901a\u9053\u5757\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u4f7f\u7528\u53cc\u5411\u95e8\u63a7\u5faa\u73af\u5355\u5143\u6355\u6349\u65f6\u5e8f\u4f9d\u8d56\uff0c\u901a\u8fc7\u4e2d\u95f4\u878d\u5408\u5904\u7406\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u4fe1\u53f7\u3002", "result": "\u5728CL-Drive\u6570\u636e\u96c6\u4e0a\uff0c\u4e8c\u5143\u5206\u7c7b\u51c6\u786e\u7387\u4ece70%\u63d0\u5347\u81f380%\uff0c\u4e09\u5143\u5206\u7c7b\u4ece62%\u63d0\u5347\u81f374%\uff0c\u4f18\u4e8e\u57fa\u4e8e\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "conclusion": "UniPhyNet\u662f\u4e00\u79cd\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u8ba4\u77e5\u72b6\u6001\u76d1\u6d4b\u3002"}}
{"id": "2507.14155", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14155", "abs": "https://arxiv.org/abs/2507.14155", "authors": ["Pramesh Gautam", "Sushmita Sapkota", "Carsten Bockelmann", "Shashi Raj Pandey", "Armin Dekorsy"], "title": "Extreme Value Theory-based Distributed Interference Prediction for 6G Industrial Sub-networks", "comment": null, "summary": "Interference prediction that accounts for extreme and rare events remains a\nkey challenge for ultra-densely deployed sub-networks (SNs) requiring\nhyper-reliable low-latency communication (HRLLC), particularly under dynamic\nmobility, rapidly varying channel statistics, and sporadic traffic. This paper\nproposes a novel calibrated interference tail prediction framework, a hybrid\nstatistical and machine learning (ML) approach that integrates an inverted\nquantile patch transformer (iQPTransformer) within extreme value theory (EVT).\nIt captures interference dynamics and tail behavior while quantifying\nuncertainty to provide statistical coverage guarantees. Its effectiveness is\ndemonstrated by leveraging the estimated interference tail distribution to\ndesign predictive, risk-aware resource allocation. In resource-constrained SN\nscenarios, we introduce the split-iQPTransformer, enabling collaborative\ntraining by distributing neural network components between sensor-actuator (SA)\npairs and the SN controller, while maintaining minimal performance disparity\ncompared to the centralized iQPTransformer. The framework effectively handles\ndeep fading, random traffic, and time-division duplexing (TDD) misalignments\nand is resilient to rare and extreme interference events. Extensive evaluations\nare performed under two mobility models and two realistic SN traffic patterns,\nusing a spatially consistent 3GPP channel model across all scenarios.\nExperimental results show consistent achievement of block error rate (BLER)\ntargets beyond the 95th percentile in the hyper-reliable regime, significantly\noutperforming baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u7684\u5e72\u6270\u5c3e\u90e8\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u5bc6\u96c6\u5b50\u7f51\u7edc\u4e2d\u6781\u7aef\u5e72\u6270\u4e8b\u4ef6\u7684\u9884\u6d4b\u548c\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u89e3\u51b3\u8d85\u5bc6\u96c6\u5b50\u7f51\u7edc\u4e2d\u6781\u7aef\u548c\u7f55\u89c1\u5e72\u6270\u4e8b\u4ef6\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08HRLLC\uff09\u3002", "method": "\u7ed3\u5408\u5012\u7f6e\u5206\u4f4d\u6570\u8865\u4e01\u53d8\u6362\u5668\uff08iQPTransformer\uff09\u548c\u6781\u503c\u7406\u8bba\uff08EVT\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff08split-iQPTransformer\uff09\u3002", "result": "\u5728\u591a\u79cd\u79fb\u52a8\u6a21\u578b\u548c\u6d41\u91cf\u6a21\u5f0f\u4e0b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8d85\u53ef\u9760\u533a\u57df\u4e2d95%\u5206\u4f4d\u6570\u4ee5\u4e0a\u7684\u5757\u9519\u8bef\u7387\u76ee\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u6781\u7aef\u5e72\u6270\u4e8b\u4ef6\uff0c\u4e3a\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u7edf\u8ba1\u8986\u76d6\u4fdd\u8bc1\u3002"}}
{"id": "2507.14181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14181", "abs": "https://arxiv.org/abs/2507.14181", "authors": ["Yajiao Dai", "Jun Li", "Zhen Mei", "Yiyang Ni", "Shi Jin", "Zengxiang Li", "Sheng Guo", "Wei Xiang"], "title": "Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis", "comment": "Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\n  5 figures", "summary": "Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\noperation of industrial machinery and improving production efficiency. However,\ntraditional supervised deep learning methods require a large amount of training\ndata and labels, which are often located in different clients. Additionally,\nthe cost of data labeling is high, making labels difficult to acquire.\nMeanwhile, differences in data distribution among clients may also hinder the\nmodel's performance. To tackle these challenges, this paper proposes a\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\ncontrastive loss and soft labeling to address data and label scarcity for\ndistributed clients with few labeled samples while safeguarding user privacy.\nIt enables representation learning using unlabeled data on the client side and\nfacilitates joint learning among clients through prototypes, thereby achieving\nmutual knowledge sharing and preventing local model divergence. Specifically,\nfirst, a sample weighting function based on the Laplace distribution is\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\nsemi-supervised training process. Second, a dual contrastive loss is introduced\nto mitigate model divergence caused by different data distributions, comprising\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\naggregated on the server with weighted averaging and updated with momentum to\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\nexperiments are conducted on two publicly available datasets and a dataset\ncollected on motors from the factory. In the most challenging task, where only\n10\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\n1.15% to 7.85% over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6SSFL-DCSL\uff0c\u7ed3\u5408\u53cc\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\uff0c\u89e3\u51b3\u5206\u5e03\u5f0f\u5ba2\u6237\u7aef\u6570\u636e\u4e0e\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6210\u672c\u9ad8\uff0c\u800c\u5ba2\u6237\u7aef\u6570\u636e\u5206\u5e03\u5dee\u5f02\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7684\u6837\u672c\u52a0\u6743\u51fd\u6570\u3001\u5f15\u5165\u53cc\u5bf9\u6bd4\u635f\u5931\uff08\u5c40\u90e8\u4e0e\u5168\u5c40\u5bf9\u6bd4\u635f\u5931\uff09\u3001\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u548c\u52a8\u91cf\u66f4\u65b0\u805a\u5408\u672c\u5730\u539f\u578b\u3002", "result": "\u5728\u4ec510%\u6570\u636e\u6807\u6ce8\u7684\u6700\u5177\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cSSFL-DCSL\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53471.15%\u81f37.85%\u3002", "conclusion": "SSFL-DCSL\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u4e0e\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u5e76\u4fdd\u62a4\u4e86\u9690\u79c1\u3002"}}
{"id": "2507.14999", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14999", "abs": "https://arxiv.org/abs/2507.14999", "authors": ["Yunfeng Li", "Junhong Liu", "Zhaohui Yang", "Guofu Liao", "Chuyun Zhang"], "title": "Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data", "comment": "10 pages,6 figures", "summary": "False Data Injection Attacks (FDIAs) pose severe security risks to smart\ngrids by manipulating measurement data collected from spatially distributed\ndevices such as SCADA systems and PMUs. These measurements typically exhibit\nNon-Independent and Identically Distributed (Non-IID) characteristics across\ndifferent regions, which significantly challenges the generalization ability of\ndetection models. Traditional centralized training approaches not only face\nprivacy risks and data sharing constraints but also incur high transmission\ncosts, limiting their scalability and deployment feasibility. To address these\nissues, this paper proposes a privacy-preserving federated learning framework,\ntermed Federated Cluster Average (FedClusAvg), designed to improve FDIA\ndetection in Non-IID and resource-constrained environments. FedClusAvg\nincorporates cluster-based stratified sampling and hierarchical communication\n(client-subserver-server) to enhance model generalization and reduce\ncommunication overhead. By enabling localized training and weighted parameter\naggregation, the algorithm achieves accurate model convergence without\ncentralizing sensitive data. Experimental results on benchmark smart grid\ndatasets demonstrate that FedClusAvg not only improves detection accuracy under\nheterogeneous data distributions but also significantly reduces communication\nrounds and bandwidth consumption. This work provides an effective solution for\nsecure and efficient FDIA detection in large-scale distributed power systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedClusAvg\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08Non-IID\uff09\u548c\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u63d0\u9ad8\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff08FDIA\uff09\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\uff08FDIAs\uff09\u5bf9\u667a\u80fd\u7535\u7f51\u6784\u6210\u4e25\u91cd\u5b89\u5168\u5a01\u80c1\uff0c\u800c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3001\u6570\u636e\u5171\u4eab\u9650\u5236\u548c\u9ad8\u4f20\u8f93\u6210\u672c\u95ee\u9898\u3002", "method": "FedClusAvg\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u5206\u5c42\u62bd\u6837\u548c\u5206\u5c42\u901a\u4fe1\uff08\u5ba2\u6237\u7aef-\u5b50\u670d\u52a1\u5668-\u670d\u52a1\u5668\uff09\u673a\u5236\uff0c\u652f\u6301\u672c\u5730\u5316\u8bad\u7ec3\u548c\u52a0\u6743\u53c2\u6570\u805a\u5408\uff0c\u907f\u514d\u96c6\u4e2d\u654f\u611f\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedClusAvg\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u8f6e\u6b21\u548c\u5e26\u5bbd\u6d88\u8017\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7535\u529b\u7cfb\u7edf\u4e2d\u5b89\u5168\u9ad8\u6548\u7684FDIA\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14182", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14182", "abs": "https://arxiv.org/abs/2507.14182", "authors": ["Xiaotong Luo", "Shengda Zhuo", "Min Chen", "Lichun Li", "Ruizhao Lu", "Wenqi Fan", "Shuqiang Huang", "Yin Tang"], "title": "From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling", "comment": null, "summary": "Financial markets exhibit highly dynamic and complex behaviors shaped by both\nhistorical price trajectories and exogenous narratives, such as news, policy\ninterpretations, and social media sentiment. The heterogeneity in these data\nand the diverse insight of investors introduce biases that complicate the\nmodeling of market dynamics. Unlike prior work, this paper explores the\npotential of bull and bear regimes in investor-driven market dynamics. Through\nempirical analysis on real-world financial datasets, we uncover a dynamic\nrelationship between bias variation and behavioral adaptation, which enhances\ntrend prediction under evolving market conditions. To model this mechanism, we\npropose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified\nframework that jointly embeds temporal price sequences and external contextual\nsignals into a shared latent space where opposing bull and bear forces\nnaturally emerge, forming the foundation for bias representation. Within this\nspace, an inertial pairing module pairs temporally adjacent samples to preserve\nmomentum, while the dual competition mechanism contrasts bullish and bearish\nembeddings to capture behavioral divergence. Together, these components allow\nB4 to model bias-driven asymmetry, behavioral inertia, and market\nheterogeneity. Experimental results on real-world financial datasets\ndemonstrate that our model not only achieves superior performance in predicting\nmarket trends but also provides interpretable insights into the interplay of\nbiases, investor behaviors, and market dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faB4\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u4fe1\u53f7\uff0c\u6355\u6349\u725b\u5e02\u548c\u718a\u5e02\u52a8\u6001\uff0c\u63d0\u5347\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u884c\u4e3a\u590d\u6742\u4e14\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6355\u6349\u6295\u8d44\u8005\u504f\u89c1\u548c\u884c\u4e3a\u52a8\u6001\u3002", "method": "\u63d0\u51faB4\u6a21\u578b\uff0c\u7ed3\u5408\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u4fe1\u53f7\uff0c\u901a\u8fc7\u60ef\u6027\u914d\u5bf9\u548c\u53cc\u7ade\u4e89\u673a\u5236\u5efa\u6a21\u5e02\u573a\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u663e\u793aB4\u5728\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u504f\u89c1\u4e0e\u884c\u4e3a\u5173\u7cfb\u3002", "conclusion": "B4\u6a21\u578b\u6709\u6548\u6355\u6349\u5e02\u573a\u52a8\u6001\uff0c\u4e3a\u6295\u8d44\u8005\u884c\u4e3a\u548c\u5e02\u573a\u8d8b\u52bf\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.15601", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15601", "abs": "https://arxiv.org/abs/2507.15601", "authors": ["Huiling Yang", "Zhanwei Wang", "Kaibin Huang"], "title": "Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity", "comment": null, "summary": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eC$^2$\u611f\u77e5\u7684\u4f4e\u5ef6\u8fdf\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6279\u91cf\u5927\u5c0f\u63a7\u5236\u6765\u5e73\u8861\u68af\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u8f6e\u6b21\uff0c\u9002\u5e94\u8bbe\u5907\u5f02\u6784\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u57286G\u7f51\u7edc\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u9ad8\u7ef4\u6a21\u578b\u66f4\u65b0\u548c\u901a\u4fe1\u8ba1\u7b97\u5f02\u6784\u6027\u5e26\u6765\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cdC$^2$\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6279\u91cf\u5927\u5c0f\u63a7\u5236\u6765\u6700\u5c0f\u5316\u7aef\u5230\u7aef\u5b66\u4e60\u5ef6\u8fdf\uff0c\u5e76\u786e\u4fdd\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u7b56\u7565\u4f18\u4e8e\u4e0d\u8003\u8651C$^2$\u6743\u8861\u6216\u8bbe\u5907\u5f02\u6784\u6027\u7684\u4f20\u7edf\u6279\u91cf\u5927\u5c0f\u8c03\u6574\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7269\u8054\u7f51\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.14164", "categories": ["eess.SP", "cs.AI", "cs.LG", "I.2; J.3"], "pdf": "https://arxiv.org/pdf/2507.14164", "abs": "https://arxiv.org/abs/2507.14164", "authors": ["Samuel Ruip\u00e9rez-Campillo", "Alain Ryser", "Thomas M. Sutter", "Ruibin Feng", "Prasanth Ganesan", "Brototo Deb", "Kelly A. Brennan", "Maxime Pedron", "Albert J. Rogers", "Maarten Z. H. Kolk", "Fleur V. Y. Tjong", "Sanjiv M. Narayan", "Julia E. Vogt"], "title": "A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy", "comment": "9 pages, 2 figures, 3 tables, the last two authors are shared senior\n  authors", "summary": "In the field of cardiac electrophysiology (EP), effectively reducing noise in\nintra-cardiac signals is crucial for the accurate diagnosis and treatment of\narrhythmias and cardiomyopathies. However, traditional noise reduction\ntechniques fall short in addressing the diverse noise patterns from various\nsources, often non-linear and non-stationary, present in these signals. This\nwork introduces a Variational Autoencoder (VAE) model, aimed at improving the\nquality of intra-ventricular monophasic action potential (MAP) signal\nrecordings. By constructing representations of clean signals from a dataset of\n5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our\napproach demonstrates superior denoising performance when compared to\nconventional filtering methods commonly employed in clinical settings. We\nassess the effectiveness of our VAE model using various metrics, indicating its\nsuperior capability to denoise signals across different noise types, including\ntime-varying non-linear noise frequently found in clinical settings. These\nresults reveal that VAEs can eliminate diverse sources of noise in single\nbeats, outperforming state-of-the-art denoising techniques and potentially\nimproving treatment efficacy in cardiac EP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5fc3\u5ba4\u5185\u5355\u76f8\u52a8\u4f5c\u7535\u4f4d\uff08MAP\uff09\u4fe1\u53f7\u7684\u8d28\u91cf\uff0c\u76f8\u6bd4\u4f20\u7edf\u6ee4\u6ce2\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5fc3\u810f\u7535\u751f\u7406\u5b66\u4e2d\uff0c\u4f20\u7edf\u964d\u566a\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4fe1\u53f7\u4e2d\u7684\u975e\u7ebf\u6027\u3001\u975e\u5e73\u7a33\u566a\u58f0\uff0c\u5f71\u54cd\u5fc3\u5f8b\u5931\u5e38\u548c\u5fc3\u808c\u75c5\u7684\u8bca\u65ad\u4e0e\u6cbb\u7597\u3002", "method": "\u5229\u7528\u6765\u81ea42\u540d\u7f3a\u8840\u6027\u5fc3\u808c\u75c5\u60a3\u8005\u76845706\u4e2a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6784\u5efaVAE\u6a21\u578b\u4ee5\u751f\u6210\u5e72\u51c0\u4fe1\u53f7\u7684\u8868\u793a\u3002", "result": "VAE\u6a21\u578b\u5728\u591a\u79cd\u566a\u58f0\u7c7b\u578b\uff08\u5305\u62ec\u65f6\u53d8\u975e\u7ebf\u6027\u566a\u58f0\uff09\u4e0b\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u964d\u566a\u6027\u80fd\u3002", "conclusion": "VAE\u80fd\u6709\u6548\u6d88\u9664\u5355\u6b21\u5fc3\u8df3\u4e2d\u7684\u591a\u79cd\u566a\u58f0\u6e90\uff0c\u53ef\u80fd\u63d0\u5347\u5fc3\u810f\u7535\u751f\u7406\u5b66\u7684\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2507.14204", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14204", "abs": "https://arxiv.org/abs/2507.14204", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.", "AI": {"tldr": "LaCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5f62\u72b6KV\u7f13\u5b58\u6a21\u5f0f\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff0c\u63d0\u5347LLMs\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\u548c\u6301\u7eed\u751f\u6210\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0cLLMs\u4e2d\u7684KV\u5bf9\u6570\u91cf\u6fc0\u589e\uff0c\u5bfc\u81f4\u6548\u7387\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u540c\u65f6\u89e3\u51b3\u957f\u8ddd\u79bb\u5efa\u6a21\u548c\u5185\u5b58\u4e0d\u8db3\u95ee\u9898\u3002", "method": "LaCache\u91c7\u7528\u68af\u5f62\u72b6KV\u7f13\u5b58\u6a21\u5f0f\u5b58\u50a8KV\u5bf9\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u538b\u7f29\u673a\u5236\u52a8\u6001\u538b\u7f29\u65e7\u7f13\u5b58\uff0c\u4ee5\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u4e0b\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1LaCache\u80fd\u6709\u6548\u589e\u5f3aLLMs\u7684\u957f\u8ddd\u79bb\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4e00\u81f4\u3002", "conclusion": "LaCache\u4e3aLLMs\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u548c\u6301\u7eed\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14206", "categories": ["eess.SP", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14206", "abs": "https://arxiv.org/abs/2507.14206", "authors": ["Zhijiang Tang", "Jiaxin Qi", "Yuhua Zheng", "Jianqiang Huang"], "title": "A Comprehensive Benchmark for Electrocardiogram Time-Series", "comment": "Accepted to ACM MM 2025", "summary": "Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial\nfor assessing cardiac health and diagnosing various diseases. Given its\ntime-series format, ECG data is often incorporated into pre-training datasets\nfor large-scale time-series model training. However, existing studies often\noverlook its unique characteristics and specialized downstream applications,\nwhich differ significantly from other time-series data, leading to an\nincomplete understanding of its properties. In this paper, we present an\nin-depth investigation of ECG signals and establish a comprehensive benchmark,\nwhich includes (1) categorizing its downstream applications into four distinct\nevaluation tasks, (2) identifying limitations in traditional evaluation metrics\nfor ECG analysis, and introducing a novel metric; (3) benchmarking\nstate-of-the-art time-series models and proposing a new architecture. Extensive\nexperiments demonstrate that our proposed benchmark is comprehensive and\nrobust. The results validate the effectiveness of the proposed metric and model\narchitecture, which establish a solid foundation for advancing research in ECG\nsignal analysis.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u7814\u7a76\u4e86ECG\u4fe1\u53f7\uff0c\u5efa\u7acb\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u5305\u62ec\u5206\u7c7b\u4e0b\u6e38\u4efb\u52a1\u3001\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u5e76\u63d0\u51fa\u65b0\u6a21\u578b\u67b6\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u7565ECG\u4fe1\u53f7\u7684\u72ec\u7279\u6027\u548c\u4e13\u7528\u4e0b\u6e38\u5e94\u7528\uff0c\u5bfc\u81f4\u5bf9\u5176\u7279\u6027\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5206\u7c7b\u4e0b\u6e38\u4efb\u52a1\u3001\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u5e76\u63d0\u51fa\u65b0\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u57fa\u51c6\u5168\u9762\u4e14\u7a33\u5065\uff0c\u65b0\u6307\u6807\u548c\u6a21\u578b\u67b6\u6784\u6709\u6548\u3002", "conclusion": "\u4e3aECG\u4fe1\u53f7\u5206\u6790\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.14215", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14215", "abs": "https://arxiv.org/abs/2507.14215", "authors": ["Jiayu", "Liu"], "title": "Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired", "comment": null, "summary": "This study aims to develop a deep learning system for an accessibility device\nfor the deaf or hearing impaired. The device will accurately localize and\nidentify sound sources in real time. This study will fill an important gap in\ncurrent research by leveraging machine learning techniques to target the\nunderprivileged community. The system includes three main components. 1.\nJerryNet: A custom designed CNN architecture that determines the direction of\narrival (DoA) for nine possible directions. 2. Audio Classification: This model\nis based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model\nto identify the exact sound classes only based on audio. 3. Multimodal\nintegration model: This is an accurate sound localization model that combines\naudio, visual, and text data to locate the exact sound sources in the images.\nThe part consists of two modules, one object detection using Yolov9 to generate\nall the bounding boxes of the objects, and an audio visual localization model\nto identify the optimal bounding box using complete Intersection over Union\n(CIoU). The hardware consists of a four-microphone rectangular formation and a\ncamera mounted on glasses with a wristband for displaying necessary information\nlike direction. On a custom collected data set, JerryNet achieved a precision\nof 91. 1% for the sound direction, outperforming all the baseline models. The\nCLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,\nrespectively. The audio-visual localization model within component 3 yielded a\ncIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are\nmany future potentials to this study, paving the way to creating a new\ngeneration of accessibility devices.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65e0\u969c\u788d\u8bbe\u5907\uff0c\u7528\u4e8e\u804b\u4eba\u6216\u542c\u529b\u53d7\u635f\u8005\uff0c\u5b9e\u65f6\u5b9a\u4f4d\u548c\u8bc6\u522b\u58f0\u6e90\u3002", "motivation": "\u586b\u8865\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u670d\u52a1\u4e8e\u5f31\u52bf\u7fa4\u4f53\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1aJerryNet\uff08CNN\u67b6\u6784\u786e\u5b9a\u58f0\u6e90\u65b9\u5411\uff09\u3001\u97f3\u9891\u5206\u7c7b\uff08\u57fa\u4e8eCLAP\u6a21\u578b\uff09\u3001\u591a\u6a21\u6001\u96c6\u6210\u6a21\u578b\uff08\u7ed3\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff09\u3002", "result": "JerryNet\u65b9\u5411\u7cbe\u5ea691.1%\uff0cCLAP\u6a21\u578b\u5728\u81ea\u5b9a\u4e49\u548cAudioSet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.5%\u548c95%\u51c6\u786e\u7387\uff0c\u591a\u6a21\u6001\u6a21\u578b\u7684cIoU\u4e3a0.892\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65b0\u4e00\u4ee3\u65e0\u969c\u788d\u8bbe\u5907\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5177\u6709\u5e7f\u9614\u7684\u672a\u6765\u6f5c\u529b\u3002"}}
{"id": "2507.14492", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14492", "abs": "https://arxiv.org/abs/2507.14492", "authors": ["Satyankar Chandra", "Ashutosh Gupta", "Kaushik Mallik", "Krishna Shankaranarayanan", "Namrita Varshney"], "title": "Glitches in Decision Tree Ensemble Models", "comment": null, "summary": "Many critical decision-making tasks are now delegated to machine-learned\nmodels, and it is imperative that their decisions are trustworthy and reliable,\nand their outputs are consistent across similar inputs. We identify a new\nsource of unreliable behaviors-called glitches-which may significantly impair\nthe reliability of AI models having steep decision boundaries. Roughly\nspeaking, glitches are small neighborhoods in the input space where the model's\noutput abruptly oscillates with respect to small changes in the input. We\nprovide a formal definition of glitches, and use well-known models and datasets\nfrom the literature to demonstrate that they have widespread existence and\nargue they usually indicate potential model inconsistencies in the neighborhood\nof where they are found. We proceed to the algorithmic search of glitches for\nwidely used gradient-boosted decision tree (GBDT) models. We prove that the\nproblem of detecting glitches is NP-complete for tree ensembles, already for\ntrees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP\nencoding of the problem, and its effectiveness and computational feasibility\nare demonstrated on a set of widely used GBDT benchmarks taken from the\nliterature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u53ef\u9760\u884c\u4e3a\u6e90\u2014\u2014glitches\uff0c\u5373\u8f93\u5165\u7a7a\u95f4\u4e2d\u6a21\u578b\u8f93\u51fa\u7a81\u7136\u632f\u8361\u7684\u5c0f\u90bb\u57df\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5e7f\u6cdb\u5b58\u5728\u7684\u6a21\u578b\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u53ef\u4fe1\u4e14\u53ef\u9760\uff0c\u8f93\u51fa\u5728\u76f8\u4f3c\u8f93\u5165\u4e0b\u4e00\u81f4\u3002", "method": "\u5f62\u5f0f\u5316\u5b9a\u4e49glitches\uff0c\u63d0\u51fa\u57fa\u4e8eMILP\u7f16\u7801\u7684\u7b97\u6cd5\u68c0\u6d4b\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBDT\uff09\u6a21\u578b\u4e2d\u7684glitches\u3002", "result": "\u8bc1\u660eglitches\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4e14\u68c0\u6d4b\u95ee\u9898\u5bf9\u6df1\u5ea64\u7684\u6811\u96c6\u6210\u662fNP\u5b8c\u5168\u7684\u3002", "conclusion": "glitches\u901a\u5e38\u6307\u793a\u6a21\u578b\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u4e14\u8ba1\u7b97\u53ef\u884c\u3002"}}
{"id": "2507.14166", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14166", "abs": "https://arxiv.org/abs/2507.14166", "authors": ["Sankalp Jajee", "Gaurav Kumar", "Homayoun Valafar"], "title": "Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering", "comment": "8 pages, 5 figures", "summary": "Preclinical sleep research remains constrained by labor intensive, manual\nvigilance state classification and inter rater variability, limiting throughput\nand reproducibility. This study presents an automated framework developed by\nTeam Neural Prognosticators to classify electroencephalogram (EEG) recordings\nof small rodents into three critical vigilance states paradoxical sleep (REM),\nslow wave sleep (SWS), and wakefulness. The system integrates advanced signal\nprocessing with machine learning, leveraging engineered features from both time\nand frequency domains, including spectral power across canonical EEG bands\n(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and\ncross-frequency coupling metrics. These features capture distinct\nneurophysiological signatures such as high frequency desynchronization during\nwakefulness, delta oscillations in SWS, and REM specific bursts. Validated\nduring the 2024 Big Data Health Science Case Competition (University of South\nCarolina Big Data Health Science Center, 2024), our XGBoost model achieved\n91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of\n83.5%, outperforming all baseline methods. Our approach represents a critical\nadvancement in automated sleep state classification and a valuable tool for\naccelerating discoveries in sleep science and the development of targeted\ninterventions for chronic sleep disorders. As a publicly available code (BDHSC)\nresource is set to contribute significantly to advancements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u5c0f\u556e\u9f7f\u52a8\u7269\u7684\u8111\u7535\u56fe\uff08EEG\uff09\u8bb0\u5f55\uff0c\u533a\u5206\u4e09\u79cd\u8b66\u89c9\u72b6\u6001\uff1aREM\u7761\u7720\u3001\u6162\u6ce2\u7761\u7720\uff08SWS\uff09\u548c\u6e05\u9192\u72b6\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e34\u5e8a\u524d\u7761\u7720\u7814\u7a76\u53d7\u9650\u4e8e\u4eba\u5de5\u5206\u7c7b\u7684\u52b3\u52a8\u5bc6\u96c6\u6027\u548c\u8bc4\u5206\u8005\u95f4\u5dee\u5f02\uff0c\u5f71\u54cd\u4e86\u901a\u91cf\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u7ed3\u5408\u4fe1\u53f7\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5229\u7528\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff08\u5982\u9891\u8c31\u529f\u7387\u3001\u6700\u5927\u6700\u5c0f\u8ddd\u79bb\u548c\u8de8\u9891\u8026\u5408\u6307\u6807\uff09\u8bad\u7ec3XGBoost\u6a21\u578b\u3002", "result": "\u6a21\u578b\u57282024\u5e74\u5927\u6570\u636e\u5065\u5eb7\u79d1\u5b66\u6848\u4f8b\u7ade\u8d5b\u4e2d\u9a8c\u8bc1\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8fbe91.5%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u81ea\u52a8\u5316\u7761\u7720\u72b6\u6001\u5206\u7c7b\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u7761\u7720\u79d1\u5b66\u7814\u7a76\u548c\u6162\u6027\u7761\u7720\u969c\u788d\u7684\u5e72\u9884\u5f00\u53d1\u3002"}}
{"id": "2507.14217", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14217", "abs": "https://arxiv.org/abs/2507.14217", "authors": ["Tudor Matei Opran", "Samir Loudni"], "title": "Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation", "comment": null, "summary": "We address the pattern explosion problem in pattern mining by proposing an\ninteractive learning framework that combines nonlinear utility aggregation with\ngeometry-aware query selection. Our method models user preferences through a\nChoquet integral over multiple interestingness measures and exploits the\ngeometric structure of the version space to guide the selection of informative\ncomparisons. A branch-and-bound strategy with tight distance bounds enables\nefficient identification of queries near the decision boundary. Experiments on\nUCI datasets show that our approach outperforms existing methods such as\nChoquetRank, achieving better ranking accuracy with fewer user interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6548\u7528\u805a\u5408\u548c\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\uff0c\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u504f\u597d\u548c\u4f18\u5316\u67e5\u8be2\u9009\u62e9\u3002", "method": "\u4f7f\u7528Choquet\u79ef\u5206\u5efa\u6a21\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\u548c\u5206\u652f\u5b9a\u754c\u7b56\u7565\u3002", "result": "\u5728UCI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982ChoquetRank\uff09\uff0c\u4ee5\u66f4\u5c11\u7528\u6237\u4ea4\u4e92\u5b9e\u73b0\u66f4\u9ad8\u6392\u540d\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4ea4\u4e92\u6548\u7387\u548c\u6392\u540d\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14740", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14740", "abs": "https://arxiv.org/abs/2507.14740", "authors": ["Andrew Wang", "Elisa Nguyen", "Runshi Yang", "Juhan Bae", "Sheila A. McIlraith", "Roger Grosse"], "title": "Better Training Data Attribution via Better Inverse Hessian-Vector Products", "comment": "28 pages, 4 figures", "summary": "Training data attribution (TDA) provides insights into which training data is\nresponsible for a learned model behavior. Gradient-based TDA methods such as\ninfluence functions and unrolled differentiation both involve a computation\nthat resembles an inverse Hessian-vector product (iHVP), which is difficult to\napproximate efficiently. We introduce an algorithm (ASTRA) which uses the\nEKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP\napproximation for TDA. ASTRA is easy to tune, requires fewer iterations than\nNeumann series iterations, and is more accurate than EKFAC-based\napproximations. Using ASTRA, we show that improving the accuracy of the iHVP\napproximation can significantly improve TDA performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aASTRA\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408EKFAC\u9884\u6761\u4ef6\u548cNeumann\u7ea7\u6570\u8fed\u4ee3\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8fd1\u4f3c\u9006Hessian-\u5411\u91cf\u79ef\uff08iHVP\uff09\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u6570\u636e\u5f52\u56e0\uff08TDA\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u8bad\u7ec3\u6570\u636e\u5f52\u56e0\uff08TDA\uff09\u65b9\u6cd5\uff08\u5982\u5f71\u54cd\u51fd\u6570\u548c\u5c55\u5f00\u5fae\u5206\uff09\u5728\u8ba1\u7b97\u9006Hessian-\u5411\u91cf\u79ef\uff08iHVP\uff09\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u8fd1\u4f3c\uff0c\u8fd9\u9650\u5236\u4e86TDA\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faASTRA\u7b97\u6cd5\uff0c\u5229\u7528EKFAC\u9884\u6761\u4ef6\u5668\u548cNeumann\u7ea7\u6570\u8fed\u4ee3\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8fd1\u4f3ciHVP\u3002", "result": "ASTRA\u5728iHVP\u8fd1\u4f3c\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u4e14\u9700\u8981\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86TDA\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7ASTRA\u7b97\u6cd5\uff0c\u53ef\u4ee5\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8fd1\u4f3ciHVP\uff0c\u4ece\u800c\u663e\u8457\u6539\u5584\u8bad\u7ec3\u6570\u636e\u5f52\u56e0\u7684\u6548\u679c\u3002"}}
{"id": "2507.14167", "categories": ["eess.SP", "cs.IR", "cs.LG", "62H05, 65-11, 94-11", "E.0; H.1.1; I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.14167", "abs": "https://arxiv.org/abs/2507.14167", "authors": ["Lucas Heublein", "Christian Wielenberg", "Thorsten Nowak", "Tobias Feigl", "Christopher Mutschler", "Felix Ott"], "title": "Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization", "comment": "6 pages, 10 figures", "summary": "Jamming devices disrupt signals from the global navigation satellite system\n(GNSS) and pose a significant threat by compromising the reliability of\naccurate positioning. Consequently, the detection and localization of these\ninterference signals are essential to achieve situational awareness, mitigating\ntheir impact, and implementing effective counter-measures. Classical Angle of\nArrival (AoA) methods exhibit reduced accuracy in multipath environments due to\nsignal reflections and scattering, leading to localization errors.\nAdditionally, AoA-based techniques demand substantial computational resources\nfor array signal processing. In this paper, we propose a novel approach for\ndetecting and classifying interference while estimating the distance, azimuth,\nand elevation of jamming sources. Our benchmark study evaluates 128 vision\nencoder and time-series models to identify the highest-performing methods for\neach task. We introduce an attention-based fusion framework that integrates\nin-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed\nspectrograms while incorporating 22 AoA features to enhance localization\naccuracy. Furthermore, we present a novel dataset of moving jamming devices\nrecorded in an indoor environment with dynamic multipath conditions and\ndemonstrate superior performance compared to state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u878d\u5408\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u5206\u7c7bGNSS\u5e72\u6270\u4fe1\u53f7\uff0c\u5e76\u4f30\u8ba1\u5e72\u6270\u6e90\u7684\u8ddd\u79bb\u3001\u65b9\u4f4d\u89d2\u548c\u4ef0\u89d2\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "GNSS\u5e72\u6270\u4fe1\u53f7\u5a01\u80c1\u5b9a\u4f4d\u53ef\u9760\u6027\uff0c\u4f20\u7edfAoA\u65b9\u6cd5\u5728\u591a\u8def\u5f84\u73af\u5883\u4e0b\u7cbe\u5ea6\u4e0d\u8db3\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3002", "method": "\u7ed3\u5408IQ\u6837\u672c\u548cFFT\u9891\u8c31\u56fe\uff0c\u5f15\u516522\u4e2aAoA\u7279\u5f81\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u6846\u67b6\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u5728\u52a8\u6001\u591a\u8def\u5f84\u73af\u5883\u4e0b\uff0c\u65b0\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e72\u6270\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.14746", "categories": ["cs.LG", "math.OC", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14746", "abs": "https://arxiv.org/abs/2507.14746", "authors": ["Bach Do", "Nafeezat A. Ajenifuja", "Taiwo A. Adebiyi", "Ruda Zhang"], "title": "Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization", "comment": null, "summary": "High-fidelity simulations and physical experiments are essential for\nengineering analysis and design. However, their high cost often limits their\napplications in two critical tasks: global sensitivity analysis (GSA) and\noptimization. This limitation motivates the common use of Gaussian processes\n(GPs) as proxy regression models to provide uncertainty-aware predictions based\non a limited number of high-quality observations. GPs naturally enable\nefficient sampling strategies that support informed decision-making under\nuncertainty by extracting information from a subset of possible functions for\nthe model of interest. Despite their popularity in machine learning and\nstatistics communities, sampling from GPs has received little attention in the\ncommunity of engineering optimization. In this paper, we present the\nformulation and detailed implementation of two notable sampling methods --\nrandom Fourier features and pathwise conditioning -- for generating posterior\nsamples from GPs. Alternative approaches are briefly described. Importantly, we\ndetail how the generated samples can be applied in GSA, single-objective\noptimization, and multi-objective optimization. We show successful applications\nof these sampling methods through a series of numerical examples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u652f\u6301\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u548c\u4f18\u5316\u4efb\u52a1\u3002", "motivation": "\u9ad8\u4fdd\u771f\u4eff\u771f\u548c\u7269\u7406\u5b9e\u9a8c\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u548c\u4f18\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u4ee3\u7406\u6a21\u578b\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\uff09\u6765\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e24\u79cd\u91c7\u6837\u65b9\u6cd5\u2014\u2014\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u548c\u8def\u5f84\u6761\u4ef6\u91c7\u6837\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5176\u5b9e\u73b0\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u91c7\u6837\u65b9\u6cd5\u5728\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u3001\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002", "conclusion": "\u9ad8\u65af\u8fc7\u7a0b\u91c7\u6837\u65b9\u6cd5\u4e3a\u5de5\u7a0b\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.14169", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14169", "abs": "https://arxiv.org/abs/2507.14169", "authors": ["Pramesh Gautam", "Ravi Sharan B A G", "Paolo Baracca", "Carsten Bockelmann", "Thorsten Wild", "Armin Dekorsy"], "title": "CQI-Based Interference Prediction for Link Adaptation in Industrial Sub-networks", "comment": null, "summary": "We propose a novel interference prediction scheme to improve link adaptation\n(LA) in densely deployed industrial sub-networks (SNs) with high-reliability\nand low-latency communication (HRLLC) requirements. The proposed method aims to\nimprove the LA framework by predicting and leveraging the heavy-tailed\ninterference probability density function (pdf). Interference is modeled as a\nlatent vector of available channel quality indicator (CQI), using a vector\ndiscrete-time state-space model (vDSSM) at the SN controller, where the CQI is\nsubjected to compression, quantization, and delay-induced errors. To robustly\nestimate interference power values under these impairments, we employ a\nlow-complexity, outlier-robust, sparse Student-t process regression (SPTPR)\nmethod. This is integrated into a modified unscented Kalman filter, which\nrecursively refines predicted interference using CQI, enabling accurate\nestimation and compensating protocol feedback delays, crucial for accurate LA.\nNumerical results show that the proposed method achieves over 10x lower\ncomplexity compared to a similar non-parametric baseline. It also maintains a\nBLER below the 90th percentile target of 1e-6 while delivering performance\ncomparable to a state-of-the-art supervised technique using only CQI reports.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5e72\u6270\u9884\u6d4b\u65b9\u6848\uff0c\u7528\u4e8e\u6539\u8fdb\u9ad8\u5bc6\u5ea6\u5de5\u4e1a\u5b50\u7f51\u7edc\u4e2d\u9ad8\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u7684\u94fe\u8def\u81ea\u9002\u5e94\uff08LA\uff09\u6027\u80fd\u3002", "motivation": "\u5728\u5bc6\u96c6\u90e8\u7f72\u7684\u5de5\u4e1a\u5b50\u7f51\u7edc\u4e2d\uff0c\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u9700\u6c42\u5bf9\u94fe\u8def\u81ea\u9002\u5e94\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5e72\u6270\u7684\u590d\u6742\u6027\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u9884\u6d4b\u548c\u5229\u7528\u91cd\u5c3e\u5e72\u6270\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff0c\u91c7\u7528\u5411\u91cf\u79bb\u6563\u65f6\u95f4\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08vDSSM\uff09\u548c\u7a00\u758fStudent-t\u8fc7\u7a0b\u56de\u5f52\uff08SPTPR\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5b9e\u73b0\u5e72\u6270\u529f\u7387\u7684\u9c81\u68d2\u4f30\u8ba1\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u590d\u6742\u5ea6\u964d\u4f4e10\u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301BLER\u4f4e\u4e8e1e-6\u768490\u767e\u5206\u4f4d\u76ee\u6807\uff0c\u6027\u80fd\u4e0e\u4ec5\u4f7f\u7528CQI\u62a5\u544a\u7684\u6700\u5148\u8fdb\u76d1\u7763\u6280\u672f\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u94fe\u8def\u81ea\u9002\u5e94\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u5bc6\u5ea6\u5de5\u4e1a\u5b50\u7f51\u7edc\u4e2d\u7684\u9ad8\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u573a\u666f\u3002"}}
{"id": "2507.14227", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14227", "abs": "https://arxiv.org/abs/2507.14227", "authors": ["Khoi Do", "Duong Nguyen", "Nam-Khanh Le", "Quoc-Viet Pham", "Binh-Son Hua", "Won-Joo Hwang"], "title": "Domain Generalization via Pareto Optimal Gradient Matching", "comment": null, "summary": "In this study, we address the gradient-based domain generalization problem,\nwhere predictors aim for consistent gradient directions across different\ndomains. Existing methods have two main challenges. First, minimization of\ngradient empirical distance or gradient inner products (GIP) leads to gradient\nfluctuations among domains, thereby hindering straightforward learning. Second,\nthe direct application of gradient learning to the joint loss function can\nincur high computation overheads due to second-order derivative approximation.\nTo tackle these challenges, we propose a new Pareto Optimality Gradient\nMatching (POGM) method. In contrast to existing methods that add gradient\nmatching as regularization, we leverage gradient trajectories as collected data\nand apply independent training at the meta-learner. In the meta-update, we\nmaximize GIP while limiting the learned gradient from deviating too far from\nthe empirical risk minimization gradient trajectory. By doing so, the aggregate\ngradient can incorporate knowledge from all domains without suffering gradient\nfluctuation towards any particular domain. Experimental evaluations on datasets\nfrom DomainBed demonstrate competitive results yielded by POGM against other\nbaselines while achieving computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684POGM\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u907f\u514d\u4e86\u68af\u5ea6\u6ce2\u52a8\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u68af\u5ea6\u5339\u914d\u65f6\u5b58\u5728\u68af\u5ea6\u6ce2\u52a8\u548c\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5229\u7528\u68af\u5ea6\u8f68\u8ff9\u4f5c\u4e3a\u6570\u636e\uff0c\u5728\u5143\u5b66\u4e60\u5668\u4e2d\u72ec\u7acb\u8bad\u7ec3\uff0c\u6700\u5927\u5316\u68af\u5ea6\u5185\u79ef\u540c\u65f6\u9650\u5236\u504f\u79bb\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u68af\u5ea6\u8f68\u8ff9\u3002", "result": "\u5728DomainBed\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "POGM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9886\u57df\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2507.14748", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14748", "abs": "https://arxiv.org/abs/2507.14748", "authors": ["Patrik Reizinger", "B\u00e1lint Mucs\u00e1nyi", "Siyuan Guo", "Benjamin Eysenbach", "Bernhard Sch\u00f6lkopf", "Wieland Brendel"], "title": "Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning", "comment": "16 pages, 7 figures", "summary": "Self-supervised feature learning and pretraining methods in reinforcement\nlearning (RL) often rely on information-theoretic principles, termed mutual\ninformation skill learning (MISL). These methods aim to learn a representation\nof the environment while also incentivizing exploration thereof. However, the\nrole of the representation and mutual information parametrization in MISL is\nnot yet well understood theoretically. Our work investigates MISL through the\nlens of identifiable representation learning by focusing on the Contrastive\nSuccessor Features (CSF) method. We prove that CSF can provably recover the\nenvironment's ground-truth features up to a linear transformation due to the\ninner product parametrization of the features and skill diversity in a\ndiscriminative sense. This first identifiability guarantee for representation\nlearning in RL also helps explain the implications of different mutual\ninformation objectives and the downsides of entropy regularizers. We\nempirically validate our claims in MuJoCo and DeepMind Control and show how CSF\nprovably recovers the ground-truth features both from states and pixels.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4\u540e\u7ee7\u7279\u5f81\uff08CSF\uff09\u65b9\u6cd5\u8bc1\u660e\u4e86\u5176\u80fd\u6062\u590d\u73af\u5883\u7684\u771f\u5b9e\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u4e92\u4fe1\u606f\u6280\u80fd\u5b66\u4e60\uff08MISL\uff09\u4e2d\u8868\u793a\u548c\u4e92\u4fe1\u606f\u53c2\u6570\u5316\u7684\u7406\u8bba\u4f5c\u7528\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u540e\u7ee7\u7279\u5f81\uff08CSF\uff09\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u7279\u5f81\u5185\u79ef\u53c2\u6570\u5316\u548c\u6280\u80fd\u591a\u6837\u6027\u5bf9\u7279\u5f81\u53ef\u8bc6\u522b\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u8bc1\u660eCSF\u80fd\u6062\u590d\u73af\u5883\u7684\u771f\u5b9e\u7279\u5f81\uff08\u7ebf\u6027\u53d8\u6362\u5185\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5728MuJoCo\u548cDeepMind Control\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "CSF\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\uff0c\u5e76\u63ed\u793a\u4e86\u4e92\u4fe1\u606f\u76ee\u6807\u548c\u71b5\u6b63\u5219\u5316\u7684\u6f5c\u5728\u95ee\u9898\u3002"}}
{"id": "2507.14173", "categories": ["eess.SP", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14173", "abs": "https://arxiv.org/abs/2507.14173", "authors": ["Karim Alghoul", "Hussein Al Osman", "Abdulmotaleb El Saddik"], "title": "Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model", "comment": "Accepted by IEEE International Instrumentation and Measurement\n  Technology Conference (I2MTC) 2025", "summary": "Human computer interaction has become integral to modern life, driven by\nadvancements in machine learning technologies. Affective computing, in\nparticular, has focused on systems that recognize, interpret, and respond to\nhuman emotions, often using wearable devices, which provide continuous data\nstreams of physiological signals. Among various physiological signals, the\nphotoplethysmogram (PPG) has gained prominence due to its ease of acquisition\nfrom widely available devices. However, the generalization of PPG-based emotion\nrecognition models across individuals remains an unresolved challenge. This\npaper introduces a novel hybrid architecture that combines Convolutional Neural\nNetworks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal\nConvolutional Networks (TCNs) to address this issue. The proposed model\nintegrates the strengths of these architectures to improve robustness and\ngeneralization. Raw PPG signals are fed into the CNN for feature extraction.\nThese features are processed separately by LSTM and TCN. The outputs from these\ncomponents are concatenated to generate a final feature representation, which\nserves as the input for classifying valence and arousal, the primary dimensions\nof emotion. Experiments using the Photoplethysmogram Dataset for Emotional\nAnalysis (PPGE) demonstrate that the proposed hybrid model achieves better\nmodel generalization than standalone CNN and LSTM architectures. Our results\nshow that the proposed solution outperforms the state-of-the-art CNN\narchitecture, as well as a CNN-LSTM model, in emotion recognition tasks with\nPPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we\nhighlight the model's effectiveness in handling subject variability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u3001LSTM\u548cTCN\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u9ad8\u57fa\u4e8ePPG\u4fe1\u53f7\u7684\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57fa\u4e8ePPG\u4fe1\u53f7\u7684\u60c5\u611f\u8bc6\u522b\u6a21\u578b\u5728\u8de8\u4e2a\u4f53\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528CNN\u63d0\u53d6PPG\u4fe1\u53f7\u7279\u5f81\uff0cLSTM\u548cTCN\u5206\u522b\u5904\u7406\u7279\u5f81\uff0c\u6700\u7ec8\u62fc\u63a5\u751f\u6210\u7279\u5f81\u8868\u793a\u7528\u4e8e\u60c5\u611f\u5206\u7c7b\u3002", "result": "\u5728PPGE\u6570\u636e\u96c6\u4e0a\uff0c\u6df7\u5408\u6a21\u578b\u5728AUC\u548cF1 Score\u4e0a\u4f18\u4e8e\u73b0\u6709CNN\u548cCNN-LSTM\u6a21\u578b\u3002", "conclusion": "\u6df7\u5408\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aPPG\u4fe1\u53f7\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u4f18\u65b9\u6848\u3002"}}
{"id": "2507.14245", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE", "q-bio.BM", "I.6.5; J.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.14245", "abs": "https://arxiv.org/abs/2507.14245", "authors": ["Hengjie Yu", "Kenneth A. Dawson", "Haiyun Yang", "Shuya Liu", "Yan Yan", "Yaochu Jin"], "title": "A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions", "comment": "31 pages, 6 figures", "summary": "Unlocking the potential of nanomaterials in medicine and environmental\nscience hinges on understanding their interactions with proteins, a complex\ndecision space where AI is poised to make a transformative impact. However,\nprogress has been hindered by limited datasets and the restricted\ngeneralizability of existing models. Here, we propose NanoPro-3M, the largest\nnanomaterial-protein interaction dataset to date, comprising over 3.2 million\nsamples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,\na foundational model that predicts nanomaterial-protein affinities through\nmultimodal representation learning, demonstrating strong generalization,\nhandling missing features, and unseen nanomaterials or proteins. We show that\nmultimodal modeling significantly outperforms single-modality approaches and\nidentifies key determinants of corona formation. Furthermore, we demonstrate\nits applicability to a range of downstream tasks through zero-shot inference\nand fine-tuning. Together, this work establishes a solid foundation for\nhigh-performance and generalized prediction of nanomaterial-protein interaction\nendpoints, reducing experimental reliance and accelerating various in vitro\napplications.", "AI": {"tldr": "NanoPro-3M\u662f\u6700\u5927\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u96c6\uff0c\u7ed3\u5408NanoProFormer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u5b9e\u9a8c\u4f9d\u8d56\u3002", "motivation": "\u7eb3\u7c73\u6750\u6599\u5728\u533b\u5b66\u548c\u73af\u5883\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u53d7\u9650\u4e8e\u5176\u4e0e\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7684\u590d\u6742\u6027\uff0c\u73b0\u6709\u6a21\u578b\u56e0\u6570\u636e\u96c6\u5c0f\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u800c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u63d0\u51faNanoPro-3M\u6570\u636e\u96c6\uff08320\u4e07\u6837\u672c\uff0c3.7\u4e07\u72ec\u7279\u86cb\u767d\u8d28\uff09\uff0c\u5e76\u5f00\u53d1NanoProFormer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u9884\u6d4b\u4eb2\u548c\u529b\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u80fd\u5904\u7406\u7f3a\u5931\u7279\u5f81\u548c\u672a\u77e5\u7eb3\u7c73\u6750\u6599/\u86cb\u767d\u8d28\uff0c\u5e76\u8bc6\u522b\u51a0\u5f62\u6210\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u6027\u80fd\u3001\u6cdb\u5316\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u52a0\u901f\u4f53\u5916\u5e94\u7528\u3002"}}
{"id": "2507.15112", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15112", "abs": "https://arxiv.org/abs/2507.15112", "authors": ["Youssef Allouah", "Rachid Guerraoui", "Sanmi Koyejo"], "title": "Distributional Unlearning: Forgetting Distributions, Not Just Samples", "comment": null, "summary": "Machine unlearning seeks to remove unwanted information from trained models,\ninitially at the individual-sample level, but increasingly at the level of\nentire sub-populations. In many deployments, models must delete whole topical\ndomains to satisfy privacy, legal, or quality requirements, e.g., removing\nseveral users' posts under GDPR or copyrighted web content. Existing unlearning\ntools remain largely sample-oriented, and straightforward point deletion often\nleaves enough residual signal for downstream learners to recover the unwanted\ndomain. We introduce distributional unlearning, a data-centric, model-agnostic\nframework that asks: Given examples from an unwanted distribution and a\nretained distribution, what is the smallest set of points whose removal makes\nthe edited dataset far from the unwanted domain yet close to the retained one?\nUsing Kullback-Leibler divergence to quantify removal and preservation, we\nderive the exact Pareto frontier in the Gaussian case and prove that any model\nretrained on the edited data incurs log-loss shifts bounded by the divergence\nthresholds. We propose a simple distance-based selection rule satisfying these\nconstraints with a quadratic reduction in deletion budget compared to random\nremoval. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,\nand CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on\nretained performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u79fb\u9664\u7279\u5b9a\u5206\u5e03\u7684\u6570\u636e\u70b9\uff0c\u51cf\u5c11\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u6216\u6cd5\u5f8b\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u5de5\u5177\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u6837\u672c\uff0c\u96be\u4ee5\u6ee1\u8db3\u5220\u9664\u6574\u4e2a\u4e3b\u9898\u9886\u57df\u7684\u9700\u6c42\uff08\u5982GDPR\u6216\u7248\u6743\u8981\u6c42\uff09\uff0c\u4e14\u6b8b\u7559\u4fe1\u53f7\u4ecd\u53ef\u80fd\u88ab\u4e0b\u6e38\u5b66\u4e60\u5668\u6062\u590d\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7Kullback-Leibler\u6563\u5ea6\u91cf\u5316\u79fb\u9664\u548c\u4fdd\u7559\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u63a8\u5bfc\u9ad8\u65af\u60c5\u51b5\u4e0b\u7684\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8ddd\u79bb\u7684\u9009\u62e9\u89c4\u5219\u4ee5\u51cf\u5c11\u5220\u9664\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u968f\u673a\u5220\u9664\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u9ad8\u65af\u3001Jigsaw Toxic Comments\u7b49\u6570\u636e\u96c6\u4e0a\u51cf\u5c1115-72%\u7684\u5220\u9664\u91cf\uff0c\u4e14\u5bf9\u4fdd\u7559\u6027\u80fd\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "\u5206\u5e03\u9057\u5fd8\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u5220\u9664\u9700\u6c42\u3002"}}
{"id": "2507.14184", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14184", "abs": "https://arxiv.org/abs/2507.14184", "authors": ["ZhengXiao He", "Jinghao Wen", "Huayu Li", "Ao Li"], "title": "NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment", "comment": null, "summary": "We present a novel and interpretable framework for electrocardiogram\n(ECG)-based disease detection that combines hyperdimensional computing (HDC)\nwith learnable neural encoding. Unlike conventional HDC approaches that rely on\nstatic, random projections, our method introduces a rhythm-aware and trainable\nencoding pipeline based on RR intervals, a physiological signal segmentation\nstrategy that aligns with cardiac cycles. The core of our design is a\nneural-distilled HDC architecture, featuring a learnable RR-block encoder and a\nBinaryLinear hyperdimensional projection layer, optimized jointly with\ncross-entropy and proxy-based metric loss. This hybrid framework preserves the\nsymbolic interpretability of HDC while enabling task-adaptive representation\nlearning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model\nsignificantly outperforms traditional HDC and classical ML baselines, achieving\n73.09\\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable\nrobustness on PTB-XL. Our framework offers an efficient and scalable solution\nfor edge-compatible ECG classification, with strong potential for interpretable\nand personalized health monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d85\u7ef4\u8ba1\u7b97\uff08HDC\uff09\u4e0e\u53ef\u5b66\u4e60\u795e\u7ecf\u7f16\u7801\u7684\u65b0\u578b\u5fc3\u7535\u56fe\uff08ECG\uff09\u75be\u75c5\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7f16\u7801\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfHDC\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u968f\u673a\u6295\u5f71\uff0c\u7f3a\u4e4f\u4efb\u52a1\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684ECG\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eRR\u95f4\u9694\u7684\u8282\u5f8b\u611f\u77e5\u53ef\u8bad\u7ec3\u7f16\u7801\u7ba1\u9053\uff0c\u7ed3\u5408\u795e\u7ecf\u84b8\u998fHDC\u67b6\u6784\u548cBinaryLinear\u6295\u5f71\u5c42\uff0c\u4f18\u5316\u4ea4\u53c9\u71b5\u548c\u4ee3\u7406\u5ea6\u91cf\u635f\u5931\u3002", "result": "\u5728Apnea-ECG\u548cPTB-XL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfHDC\u548c\u7ecf\u5178ML\u65b9\u6cd5\uff0cApnea-ECG\u4e0a\u8fbe\u523073.09%\u7cbe\u5ea6\u548c0.626 F1\u5206\u6570\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18\u517c\u5bb9\u7684ECG\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e2a\u6027\u5316\u5065\u5eb7\u76d1\u6d4b\u3002"}}
{"id": "2507.14257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14257", "abs": "https://arxiv.org/abs/2507.14257", "authors": ["Julio Candanedo"], "title": "Linearized Diffusion Map", "comment": null, "summary": "We introduce the Linearized Diffusion Map (LDM), a novel linear\ndimensionality reduction method constructed via a linear approximation of the\ndiffusion-map kernel. LDM integrates the geometric intuition of diffusion-based\nnonlinear methods with the computational simplicity, efficiency, and\ninterpretability inherent in linear embeddings such as PCA and classical MDS.\nThrough comprehensive experiments on synthetic datasets (Swiss roll and\nhyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that\nLDM captures distinct geometric features of datasets compared to PCA, offering\ncomplementary advantages. Specifically, LDM embeddings outperform PCA in\ndatasets exhibiting explicit manifold structures, particularly in\nhigh-dimensional regimes, whereas PCA remains preferable in scenarios dominated\nby variance or noise. Furthermore, the complete positivity of LDM's kernel\nmatrix allows direct applicability of Non-negative Matrix Factorization (NMF),\nsuggesting opportunities for interpretable latent-structure discovery. Our\nanalysis positions LDM as a valuable new linear dimensionality reduction\ntechnique with promising theoretical and practical extensions.", "AI": {"tldr": "LDM\u662f\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6620\u5c04\u6838\u7684\u7ebf\u6027\u8fd1\u4f3c\u6784\u5efa\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u76f4\u89c2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7ed3\u5408\u975e\u7ebf\u6027\u6269\u6563\u65b9\u6cd5\u7684\u51e0\u4f55\u76f4\u89c2\u6027\u4e0e\u7ebf\u6027\u5d4c\u5165\uff08\u5982PCA\uff09\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u8fd1\u4f3c\u6269\u6563\u6620\u5c04\u6838\u6784\u5efaLDM\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "LDM\u5728\u5177\u6709\u660e\u786e\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8ePCA\uff0c\u800cPCA\u5728\u566a\u58f0\u6216\u65b9\u5dee\u4e3b\u5bfc\u7684\u573a\u666f\u4e2d\u66f4\u4f18\u3002", "conclusion": "LDM\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u7ebf\u6027\u964d\u7ef4\u6280\u672f\uff0c\u9002\u7528\u4e8e\u7406\u8bba\u548c\u5b9e\u9645\u6269\u5c55\u3002"}}
{"id": "2507.15173", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15173", "abs": "https://arxiv.org/abs/2507.15173", "authors": ["Jason Gaitonde", "Ankur Moitra", "Elchanan Mossel"], "title": "Better Models and Algorithms for Learning Ising Models from Dynamics", "comment": "49 pages", "summary": "We study the problem of learning the structure and parameters of the Ising\nmodel, a fundamental model of high-dimensional data, when observing the\nevolution of an associated Markov chain. A recent line of work has studied the\nnatural problem of learning when observing an evolution of the well-known\nGlauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,\nGaitonde, Mossel STOC 2024], which provides an arguably more realistic\ngenerative model than the classical i.i.d. setting. However, this prior work\ncrucially assumes that all site update attempts are observed, \\emph{even when\nthis attempt does not change the configuration}: this strong observation model\nis seemingly essential for these approaches. While perhaps possible in\nrestrictive contexts, this precludes applicability to most realistic settings\nwhere we can observe \\emph{only} the stochastic evolution itself, a minimal and\nnatural assumption for any process we might hope to learn from. However,\ndesigning algorithms that succeed in this more realistic setting has remained\nan open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,\nGaitonde, Moitra, Mossel, STOC 2025].\n  In this work, we give the first algorithms that efficiently learn the Ising\nmodel in this much more natural observation model that only observes when the\nconfiguration changes. For Ising models with maximum degree $d$, our algorithm\nrecovers the underlying dependency graph in time $\\mathsf{poly}(d)\\cdot n^2\\log\nn$ and then the actual parameters in additional $\\widetilde{O}(2^d n)$ time,\nwhich qualitatively matches the state-of-the-art even in the i.i.d. setting in\na much weaker observation model. Our analysis holds more generally for a\nbroader class of reversible, single-site Markov chains that also includes the\npopular Metropolis chain by leveraging more robust properties of reversible\nMarkov chains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u66f4\u81ea\u7136\u7684\u89c2\u6d4b\u6a21\u578b\u4e0b\u5b66\u4e60Ising\u6a21\u578b\u7ed3\u6784\u548c\u53c2\u6570\u7684\u7b97\u6cd5\uff0c\u4ec5\u9700\u89c2\u5bdf\u914d\u7f6e\u53d8\u5316\uff0c\u65e0\u9700\u5b8c\u5168\u89c2\u6d4b\u6240\u6709\u66f4\u65b0\u5c1d\u8bd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u89c2\u6d4b\u6240\u6709\u7ad9\u70b9\u66f4\u65b0\u5c1d\u8bd5\uff08\u5305\u62ec\u672a\u6539\u53d8\u914d\u7f6e\u7684\u5c1d\u8bd5\uff09\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u4ec5\u89c2\u6d4b\u914d\u7f6e\u53d8\u5316\u65f6\u7684\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u80fd\u5728\u6700\u5927\u5ea6\u4e3ad\u7684Ising\u6a21\u578b\u4e2d\u6062\u590d\u4f9d\u8d56\u56fe\uff0c\u5e76\u5728\u989d\u5916\u65f6\u95f4\u5185\u6062\u590d\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u53ef\u9006\u5355\u7ad9\u70b9\u9a6c\u5c14\u53ef\u592b\u94fe\u3002", "result": "\u7b97\u6cd5\u5728\u66f4\u5f31\u7684\u89c2\u6d4b\u6a21\u578b\u4e0b\uff0c\u6027\u80fd\u4e0ei.i.d.\u8bbe\u7f6e\u4e0b\u7684\u6700\u65b0\u6210\u679c\u76f8\u5f53\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u7c7b\u3002", "conclusion": "\u8bba\u6587\u9996\u6b21\u5728\u4ec5\u89c2\u6d4b\u914d\u7f6e\u53d8\u5316\u7684\u81ea\u7136\u6a21\u578b\u4e0b\u89e3\u51b3\u4e86Ising\u6a21\u578b\u5b66\u4e60\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u7b97\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.14185", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14185", "abs": "https://arxiv.org/abs/2507.14185", "authors": ["Abdullah Ahmed", "Jeremy Gummeson"], "title": "Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices", "comment": null, "summary": "Latent spaces offer an efficient and effective means of summarizing data\nwhile implicitly preserving meta-information through relational encoding. We\nleverage these meta-embeddings to develop a modality-agnostic, unified encoder.\nOur method employs sensor-latent fusion to analyze and correlate multimodal\nphysiological signals. Using a compressed sensing approach with\nautoencoder-based latent space fusion, we address the computational challenges\nof biosignal analysis on resource-constrained devices. Experimental results\nshow that our unified encoder is significantly faster, lighter, and more\nscalable than modality-specific alternatives, without compromising\nrepresentational accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7684\u7edf\u4e00\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u4f20\u611f\u5668-\u6f5c\u5728\u878d\u5408\u5206\u6790\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u8ba1\u7b97\u6311\u6218\u3002", "motivation": "\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u7684\u9ad8\u6548\u6027\u548c\u9690\u542b\u7684\u5143\u4fe1\u606f\u4fdd\u5b58\u80fd\u529b\uff0c\u5f00\u53d1\u4e00\u79cd\u6a21\u6001\u65e0\u5173\u7684\u7edf\u4e00\u7f16\u7801\u5668\u3002", "method": "\u91c7\u7528\u4f20\u611f\u5668-\u6f5c\u5728\u878d\u5408\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\uff0c\u7ed3\u5408\u538b\u7f29\u611f\u77e5\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7edf\u4e00\u7f16\u7801\u5668\u5728\u901f\u5ea6\u3001\u8f7b\u91cf\u5316\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u6a21\u6001\u7279\u5b9a\u65b9\u6cd5\uff0c\u4e14\u4e0d\u635f\u5931\u8868\u5f81\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14295", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14295", "abs": "https://arxiv.org/abs/2507.14295", "authors": ["Licheng Liu", "Zihan Wang", "Linjie Li", "Chenwei Xu", "Yiping Lu", "Han Liu", "Avirup Sil", "Manling Li"], "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning", "comment": null, "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08UFO\uff09\uff0c\u901a\u8fc7\u5355\u8f6e\u53cd\u9988\u63d0\u5347\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u591a\u8f6e\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5355\u8f6e\u8bad\u7ec3\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8f6e\u95ee\u9898\u89e3\u51b3\u4e2d\u6a21\u578b\u5bb9\u6613\u5931\u53bb\u4fee\u6b63\u80fd\u529b\u5e76\u91cd\u590d\u56de\u7b54\u3002", "method": "\u5f15\u5165\u5355\u8f6e\u53cd\u9988\u4f5c\u4e3a\u89c2\u5bdf\uff08UFO\uff09\uff0c\u7ed3\u5408\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\uff0c\u8bbe\u8ba1\u5956\u52b1\u673a\u5236\u4ee5\u9f13\u52b1\u591a\u6837\u5316\u7684\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUFO\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u8f6e\u6027\u80fd\u7684\u540c\u65f6\uff0c\u591a\u8f6e\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe14%\u3002", "conclusion": "UFO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u8f6e\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u53cd\u9988\u53cd\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6240\u9700\u8f6e\u6b21\u3002"}}
{"id": "2507.15240", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15240", "abs": "https://arxiv.org/abs/2507.15240", "authors": ["Le Peng", "Yash Travadi", "Chuan He", "Ying Cui", "Ju Sun"], "title": "Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification", "comment": null, "summary": "For classification with imbalanced class frequencies, i.e., imbalanced\nclassification (IC), standard accuracy is known to be misleading as a\nperformance measure. While most existing methods for IC resort to optimizing\nbalanced accuracy (i.e., the average of class-wise recalls), they fall short in\nscenarios where the significance of classes varies or certain metrics should\nreach prescribed levels. In this paper, we study two key classification\nmetrics, precision and recall, under three practical binary IC settings: fix\nprecision optimize recall (FPOR), fix recall optimize precision (FROP), and\noptimize $F_\\beta$-score (OFBS). Unlike existing methods that rely on smooth\napproximations to deal with the indicator function involved, \\textit{we\nintroduce, for the first time, exact constrained reformulations for these\ndirect metric optimization (DMO) problems}, which can be effectively solved by\nexact penalty methods. Experiment results on multiple benchmark datasets\ndemonstrate the practical superiority of our approach over the state-of-the-art\nmethods for the three DMO problems. We also expect our exact reformulation and\noptimization (ERO) framework to be applicable to a wide range of DMO problems\nfor binary IC and beyond. Our code is available at\nhttps://github.com/sun-umn/DMO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u7ea6\u675f\u91cd\u6784\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u5206\u7c7b\u4e2d\u7684\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u56fa\u5b9a\u7cbe\u5ea6\u4f18\u5316\u53ec\u56de\u7387\u3001\u56fa\u5b9a\u53ec\u56de\u7387\u4f18\u5316\u7cbe\u5ea6\u4ee5\u53ca\u4f18\u5316F\u03b2\u5206\u6570\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u4e0d\u5e73\u8861\u5206\u7c7b\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f18\u5316\u5e73\u8861\u51c6\u786e\u7387\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u7c7b\u522b\u91cd\u8981\u6027\u4e0d\u540c\u6216\u7279\u5b9a\u6307\u6807\u9700\u8fbe\u5230\u9884\u8bbe\u6c34\u5e73\u7684\u60c5\u51b5\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u76f4\u63a5\u4f18\u5316\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7b49\u5173\u952e\u6307\u6807\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51fa\u4e86\u7cbe\u786e\u7ea6\u675f\u91cd\u6784\u65b9\u6cd5\uff08ERO\u6846\u67b6\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7cbe\u786e\u60e9\u7f5a\u65b9\u6cd5\u6709\u6548\u6c42\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u7684ERO\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u4e0d\u5e73\u8861\u5206\u7c7b\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2507.14187", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14187", "abs": "https://arxiv.org/abs/2507.14187", "authors": ["Xiaojuan Zhang", "Tianyu Jiang", "Haoxiang Zong", "Chen Zhang", "Chendan Li", "Marta Molinas"], "title": "AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms", "comment": null, "summary": "The impedance network (IN) model is gaining popularity in the oscillation\nanalysis of wind farms. However, the construction of such an IN model requires\nimpedance curves of each wind turbine under their respective operating\nconditions, making its online application difficult due to the transmission of\nnumerous high-density impedance curves. To address this issue, this paper\nproposes an AI-based impedance encoding-decoding method to facilitate the\nonline construction of IN model. First, an impedance encoder is trained to\ncompress impedance curves by setting the number of neurons much smaller than\nthat of frequency points. Then, the compressed data of each turbine are\nuploaded to the wind farm and an impedance decoder is trained to reconstruct\noriginal impedance curves. At last, based on the nodal admittance matrix (NAM)\nmethod, the IN model of the wind farm can be obtained. The proposed method is\nvalidated via model training and real-time simulations, demonstrating that the\nencoded impedance vectors enable fast transmission and accurate reconstruction\nof the original impedance curves.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u963b\u6297\u7f16\u7801-\u89e3\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u6784\u5efa\u98ce\u7535\u573a\u7684\u963b\u6297\u7f51\u7edc\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u9ad8\u5bc6\u5ea6\u963b\u6297\u66f2\u7ebf\u4f20\u8f93\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u98ce\u7535\u573a\u7684\u963b\u6297\u7f51\u7edc\u6a21\u578b\u6784\u5efa\u9700\u8981\u5927\u91cf\u9ad8\u5bc6\u5ea6\u963b\u6297\u66f2\u7ebf\uff0c\u4f20\u8f93\u56f0\u96be\uff0c\u9650\u5236\u4e86\u5728\u7ebf\u5e94\u7528\u3002", "method": "\u8bad\u7ec3\u963b\u6297\u7f16\u7801\u5668\u538b\u7f29\u963b\u6297\u66f2\u7ebf\uff0c\u4e0a\u4f20\u538b\u7f29\u6570\u636e\u540e\u8bad\u7ec3\u89e3\u7801\u5668\u91cd\u6784\u66f2\u7ebf\uff0c\u6700\u540e\u57fa\u4e8e\u8282\u70b9\u5bfc\u7eb3\u77e9\u9635\u65b9\u6cd5\u83b7\u53d6\u6a21\u578b\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u7f16\u7801\u540e\u7684\u963b\u6297\u5411\u91cf\u80fd\u5feb\u901f\u4f20\u8f93\u5e76\u51c6\u786e\u91cd\u6784\u539f\u59cb\u963b\u6297\u66f2\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u963b\u6297\u66f2\u7ebf\u4f20\u8f93\u95ee\u9898\uff0c\u4e3a\u5728\u7ebf\u6784\u5efa\u963b\u6297\u7f51\u7edc\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.14322", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11; C.2.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14322", "abs": "https://arxiv.org/abs/2507.14322", "authors": ["Md Rafid Haque", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning", "comment": "24 pages, 8 figures. This work is intended for a journal submission", "summary": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems.", "AI": {"tldr": "FedStrategist\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u805a\u5408\u89c4\u5219\u6765\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\uff0c\u4f18\u4e8e\u9759\u6001\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u4f7f\u5176\u6613\u53d7\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u9759\u6001\u9632\u5fa1\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u653b\u51fb\u6216\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u6548\u679c\u6709\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u4ee3\u7406\uff0c\u6839\u636e\u5b9e\u65f6\u8bca\u65ad\u6307\u6807\u52a8\u6001\u9009\u62e9\u6700\u4f18\u805a\u5408\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedStrategist\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5b66\u4e60\u5230\u66f4\u4f18\u7b56\u7565\uff0c\u5e76\u80fd\u901a\u8fc7\u5355\u4e00\u53c2\u6570\u63a7\u5236\u6027\u80fd\u4e0e\u5b89\u5168\u7684\u6743\u8861\u3002", "conclusion": "FedStrategist\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u5206\u6790\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u5f39\u6027\u548c\u667a\u80fd\u7684\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u3002"}}
{"id": "2507.15397", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15397", "abs": "https://arxiv.org/abs/2507.15397", "authors": ["Scott Pesme", "Giacomo Meanti", "Michael Arbel", "Julien Mairal"], "title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees", "comment": null, "summary": "Denoiser models have become powerful tools for inverse problems, enabling the\nuse of pretrained networks to approximate the score of a smoothed prior\ndistribution. These models are often used in heuristic iterative schemes aimed\nat solving Maximum a Posteriori (MAP) optimisation problems, where the proximal\noperator of the negative log-prior plays a central role. In practice, this\noperator is intractable, and practitioners plug in a pretrained denoiser as a\nsurrogate-despite the lack of general theoretical justification for this\nsubstitution. In this work, we show that a simple algorithm, closely related to\nseveral used in practice, provably converges to the proximal operator under a\nlog-concavity assumption on the prior $p$. We show that this algorithm can be\ninterpreted as a gradient descent on smoothed proximal objectives. Our analysis\nthus provides a theoretical foundation for a class of empirically successful\nbut previously heuristic methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7b97\u6cd5\uff0c\u5728log-concave\u5148\u9a8c\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u5176\u6536\u655b\u5230\u8fd1\u7aef\u7b97\u5b50\uff0c\u4e3a\u5b9e\u8df5\u4e2d\u5e38\u7528\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u53bb\u566a\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7528\u9884\u8bad\u7ec3\u53bb\u566a\u5668\u66ff\u4ee3\u8fd1\u7aef\u7b97\u5b50\u7684\u7406\u8bba\u8bc1\u660e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e0e\u5b9e\u8df5\u4e2d\u5e38\u7528\u65b9\u6cd5\u76f8\u5173\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u5728log-concave\u5148\u9a8c\u5047\u8bbe\u4e0b\u6536\u655b\u5230\u8fd1\u7aef\u7b97\u5b50\u3002", "result": "\u7b97\u6cd5\u53ef\u89e3\u91ca\u4e3a\u5bf9\u5e73\u6ed1\u8fd1\u7aef\u76ee\u6807\u7684\u68af\u5ea6\u4e0b\u964d\uff0c\u4e3a\u7ecf\u9a8c\u6210\u529f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u7684\u5b9e\u8df5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2507.14190", "categories": ["eess.SP", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14190", "abs": "https://arxiv.org/abs/2507.14190", "authors": ["Mingcheng Liao", "Zebang Feng", "Miao Fan", "Shengtong Xu", "Haoyi Xiong"], "title": "Traffic Signal Phase and Timing Estimation with Large-Scale Floating Car Data", "comment": "Accepted by ITSC'25", "summary": "Effective modern transportation systems depend critically on accurate Signal\nPhase and Timing (SPaT) estimation. However, acquiring ground-truth SPaT\ninformation faces significant hurdles due to communication challenges with\ntransportation departments and signal installers. As a result, Floating Car\nData (FCD) has become the primary source for large-scale SPaT analyses. Current\nFCD approaches often simplify the problem by assuming fixed schedules and basic\nintersection designs for specific times and locations. These methods fail to\naccount for periodic signal changes, diverse intersection structures, and the\ninherent limitations of real-world data, thus lacking a comprehensive framework\nthat is universally applicable. Addressing this limitation, we propose an\nindustrial-grade FCD analysis suite that manages the entire process, from\ninitial data preprocessing to final SPaT estimation. Our approach estimates\nsignal phases, identifies time-of-day (TOD) periods, and determines the\ndurations of red and green lights. The framework's notable stability and\nrobustness across diverse conditions, regardless of road geometry, is a key\nfeature. Furthermore, we provide a cleaned, de-identified FCD dataset and\nsupporting parameters to facilitate future research. Currently operational\nwithin our navigation platform, the system analyses over 15 million FCD records\ndaily, supporting over two million traffic signals in mainland China, with more\nthan 75\\% of estimations demonstrating less than five seconds of error.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u4e1a\u7ea7\u7684FCD\u5206\u6790\u5957\u4ef6\uff0c\u7528\u4e8e\u51c6\u786e\u4f30\u8ba1\u4fe1\u53f7\u76f8\u4f4d\u548c\u65f6\u5e8f\uff08SPaT\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6570\u636e\u5c40\u9650\u6027\u548c\u591a\u6837\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709FCD\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u4fe1\u53f7\u5468\u671f\u548c\u7b80\u5355\u4ea4\u53c9\u53e3\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5468\u671f\u6027\u4fe1\u53f7\u53d8\u5316\u548c\u591a\u6837\u6027\u7ed3\u6784\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ece\u6570\u636e\u9884\u5904\u7406\u5230SPaT\u4f30\u8ba1\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u5305\u62ec\u4fe1\u53f7\u76f8\u4f4d\u4f30\u8ba1\u3001TOD\u65f6\u6bb5\u8bc6\u522b\u548c\u7ea2\u7eff\u706f\u65f6\u957f\u786e\u5b9a\u3002", "result": "\u7cfb\u7edf\u6bcf\u5929\u5904\u74061500\u4e07\u6761FCD\u8bb0\u5f55\uff0c\u652f\u6301\u4e2d\u56fd\u5927\u9646200\u591a\u4e07\u4e2a\u4ea4\u901a\u4fe1\u53f7\uff0c75%\u4ee5\u4e0a\u7684\u4f30\u8ba1\u8bef\u5dee\u5c0f\u4e8e5\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u6837\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6d01\u4e14\u53bb\u6807\u8bc6\u5316\u7684FCD\u6570\u636e\u96c6\u3002"}}
{"id": "2507.14326", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14326", "abs": "https://arxiv.org/abs/2507.14326", "authors": ["Aryana Hou", "Li Lin", "Justin Li", "Shu Hu"], "title": "Rethinking Individual Fairness in Deepfake Detection", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Generative AI models have substantially improved the realism of synthetic\nmedia, yet their misuse through sophisticated DeepFakes poses significant\nrisks. Despite recent advances in deepfake detection, fairness remains\ninadequately addressed, enabling deepfake markers to exploit biases against\nspecific populations. While previous studies have emphasized group-level\nfairness, individual fairness (i.e., ensuring similar predictions for similar\nindividuals) remains largely unexplored. In this work, we identify for the\nfirst time that the original principle of individual fairness fundamentally\nfails in the context of deepfake detection, revealing a critical gap previously\nunexplored in the literature. To mitigate it, we propose the first\ngeneralizable framework that can be integrated into existing deepfake detectors\nto enhance individual fairness and generalization. Extensive experiments\nconducted on leading deepfake datasets demonstrate that our approach\nsignificantly improves individual fairness while maintaining robust detection\nperformance, outperforming state-of-the-art methods. The code is available at\nhttps://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u901a\u7528\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u6ee5\u7528\u5bf9\u7279\u5b9a\u7fa4\u4f53\u5b58\u5728\u504f\u89c1\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7fa4\u4f53\u516c\u5e73\u6027\uff0c\u4e2a\u4f53\u516c\u5e73\u6027\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u53ef\u96c6\u6210\u5230\u73b0\u6709\u68c0\u6d4b\u5668\u4e2d\u7684\u901a\u7528\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u4e2a\u4f53\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15678", "categories": ["cs.LG", "math.DG", "math.DS", "math.SG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15678", "abs": "https://arxiv.org/abs/2507.15678", "authors": ["Amine Mohamed Aboussalah", "Abdessalam Ed-dib"], "title": "GeoHNNs: Geometric Hamiltonian Neural Networks", "comment": null, "summary": "The fundamental laws of physics are intrinsically geometric, dictating the\nevolution of systems through principles of symmetry and conservation. While\nmodern machine learning offers powerful tools for modeling complex dynamics\nfrom data, common methods often ignore this underlying geometric fabric.\nPhysics-informed neural networks, for instance, can violate fundamental\nphysical principles, leading to predictions that are unstable over long\nperiods, particularly for high-dimensional and chaotic systems. Here, we\nintroduce \\textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework\nthat learns dynamics by explicitly encoding the geometric priors inherent to\nphysical laws. Our approach enforces two fundamental structures: the Riemannian\ngeometry of inertia, by parameterizing inertia matrices in their natural\nmathematical space of symmetric positive-definite matrices, and the symplectic\ngeometry of phase space, using a constrained autoencoder to ensure the\npreservation of phase space volume in a reduced latent space. We demonstrate\nthrough experiments on systems ranging from coupled oscillators to\nhigh-dimensional deformable objects that GeoHNN significantly outperforms\nexisting models. It achieves superior long-term stability, accuracy, and energy\nconservation, confirming that embedding the geometry of physics is not just a\ntheoretical appeal but a practical necessity for creating robust and\ngeneralizable models of the physical world.", "AI": {"tldr": "GeoHNN\u662f\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u7269\u7406\u5b9a\u5f8b\u7684\u51e0\u4f55\u5148\u9a8c\u6765\u5b66\u4e60\u52a8\u529b\u5b66\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u7a33\u5b9a\u6027\u3001\u51c6\u786e\u6027\u548c\u80fd\u91cf\u5b88\u6052\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e38\u5ffd\u7565\u7269\u7406\u7cfb\u7edf\u7684\u51e0\u4f55\u672c\u8d28\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u7a33\u5b9a\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u548c\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u3002", "method": "GeoHNN\u901a\u8fc7\u53c2\u6570\u5316\u60ef\u6027\u77e9\u9635\u4e3a\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\uff0c\u5e76\u4f7f\u7528\u7ea6\u675f\u81ea\u7f16\u7801\u5668\u4fdd\u6301\u76f8\u7a7a\u95f4\u4f53\u79ef\uff0c\u5f3a\u5236\u5b9e\u65bd\u9ece\u66fc\u51e0\u4f55\u548c\u8f9b\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeoHNN\u5728\u8026\u5408\u632f\u5b50\u548c\u9ad8\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u7b49\u7cfb\u7edf\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u5d4c\u5165\u7269\u7406\u51e0\u4f55\u4e0d\u4ec5\u662f\u7406\u8bba\u4e0a\u7684\u5438\u5f15\u529b\uff0c\u66f4\u662f\u6784\u5efa\u7a33\u5065\u3001\u53ef\u63a8\u5e7f\u7269\u7406\u6a21\u578b\u7684\u5b9e\u8df5\u9700\u6c42\u3002"}}
{"id": "2507.14191", "categories": ["eess.SP", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14191", "abs": "https://arxiv.org/abs/2507.14191", "authors": ["Cliver Oliver Turpo Benique"], "title": "School Attendance Control System Based on RFID Technology with Raspberry Pi and Arduino: EDURFID", "comment": "27 pages, 4 figures. Educational technology system for rural schools\n  in Peru. Implements RFID-based attendance control using open-source hardware\n  (Raspberry Pi, Arduino). System validation conducted at T\\'upac Amaru\n  Secondary Educational Institution, Coasa, Puno", "summary": "This paper presents EDURFID, an automated school attendance control system\nbased on RFID technology designed for rural educational institutions in Peru.\nThe system integrates open-source hardware (Raspberry Pi 5, Arduino UNO R3)\nwith RC522 RFID modules operating at 13.56 MHz, implementing a web architecture\ndeveloped in Python Django. The system demonstrates 100% precision in RFID\nreadings with 0.03-second response time, achieving 94% cost reduction compared\nto commercial solutions. Validation at T\\'upac Amaru Secondary Educational\nInstitution showed successful automation of attendance processes, saving 50\ndaily minutes of administrative time while providing real-time reporting\ncapabilities.", "AI": {"tldr": "EDURFID\u662f\u4e00\u4e2a\u57fa\u4e8eRFID\u6280\u672f\u7684\u81ea\u52a8\u5316\u5b66\u6821\u8003\u52e4\u7cfb\u7edf\uff0c\u4e13\u4e3a\u79d8\u9c81\u519c\u6751\u6559\u80b2\u673a\u6784\u8bbe\u8ba1\uff0c\u4f7f\u7528\u5f00\u6e90\u786c\u4ef6\u548cPython Django\u5f00\u53d1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u6210\u672c\u3002", "motivation": "\u4e3a\u79d8\u9c81\u519c\u6751\u6559\u80b2\u673a\u6784\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u8003\u52e4\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u884c\u653f\u65f6\u95f4\u3002", "method": "\u96c6\u6210Raspberry Pi 5\u3001Arduino UNO R3\u548cRC522 RFID\u6a21\u5757\uff0c\u91c7\u7528Python Django\u5f00\u53d1Web\u67b6\u6784\u3002", "result": "\u7cfb\u7edfRFID\u8bfb\u53d6\u7cbe\u5ea6100%\uff0c\u54cd\u5e94\u65f6\u95f40.03\u79d2\uff0c\u6210\u672c\u964d\u4f4e94%\uff0c\u9a8c\u8bc1\u4e2d\u8282\u770150\u5206\u949f\u884c\u653f\u65f6\u95f4\u3002", "conclusion": "EDURFID\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u8003\u52e4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2507.14332", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14332", "abs": "https://arxiv.org/abs/2507.14332", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert Salko", "Xu Wu"], "title": "Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries", "comment": "Accepted for inclusion in Transactions of the American Nuclear\n  Society for the 2025 ANS Winter Conference", "summary": "Accurate prediction of critical heat flux (CHF) is an essential component of\nsafety analysis in pressurized and boiling water reactors. To support reliable\nprediction of this quantity, several empirical correlations and lookup tables\nhave been constructed from physical experiments over the past several decades.\nWith the onset of accessible machine learning (ML) frameworks, multiple\ninitiatives have been established with the goal of predicting CHF more\naccurately than these traditional methods. While purely data-driven surrogate\nmodeling has been extensively investigated, these approaches lack\ninterpretability, lack resilience to data scarcity, and have been developed\nmostly using data from tube experiments. As a result, bias-correction hybrid\napproaches have become increasingly popular, which correct initial\n\"low-fidelity\" estimates provided by deterministic base models by using\nML-predicted residuals. This body of work has mostly considered round tube\ngeometries; annular geometry-specific ML models have not yet been deployed in\nthermal hydraulic codes. This study developed, deployed, and validated four ML\nmodels to predict CHF in annular geometries using the CTF subchannel code.\nThree empirical correlation models, Biasi, Bowring, and Katto, were used as\nbase models for comparison. The ML models were trained and tested using 577\nexperimental annulus data points from four datasets: Becker, Beus, Janssen, and\nMortimore. Baseline CHF predictions were obtained from the empirical\ncorrelations, with mean relative errors above 26%. The ML-driven models\nachieved mean relative errors below 3.5%, with no more than one point exceeding\nthe 10% error envelope. In all cases, the hybrid ML models significantly\noutperformed their empirical counterparts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u56db\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684\u4e34\u754c\u70ed\u901a\u91cf\uff08CHF\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ecf\u9a8c\u65b9\u6cd5\u3002", "motivation": "\u4e34\u754c\u70ed\u901a\u91cf\uff08CHF\uff09\u7684\u51c6\u786e\u9884\u6d4b\u5bf9\u6838\u53cd\u5e94\u5806\u5b89\u5168\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u7ecf\u9a8c\u65b9\u6cd5\u5b58\u5728\u8bef\u5dee\u5927\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7b49\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u6df7\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u57fa\u7840\u6a21\u578b\uff08Biasi\u3001Bowring\u3001Katto\uff09\u548cML\u9884\u6d4b\u6b8b\u5dee\uff0c\u8bad\u7ec3\u5e76\u6d4b\u8bd5\u4e86\u56db\u79cdML\u6a21\u578b\u3002", "result": "ML\u6a21\u578b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e3.5%\uff0c\u663e\u8457\u4f18\u4e8e\u7ecf\u9a8c\u6a21\u578b\u768426%\u4ee5\u4e0a\u8bef\u5dee\u3002", "conclusion": "\u6df7\u5408ML\u6a21\u578b\u5728\u73af\u5f62\u51e0\u4f55\u7ed3\u6784\u4e2d\u7684CHF\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6838\u53cd\u5e94\u5806\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2507.14344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14344", "abs": "https://arxiv.org/abs/2507.14344", "authors": ["Daniel Fein", "Gabriela Aranguiz-Dias"], "title": "Influence Functions for Preference Dataset Pruning", "comment": null, "summary": "Language models are commonly fine-tuned via reinforcement learning to alter\ntheir behavior or elicit new capabilities. Datasets used for these purposes,\nand particularly human preference datasets, are often noisy. The relatively\nsmall size post-training datasets, combined with parameter-efficient\nfine-tuning methods, enable the use of influence functions approximations to\ndetect and prune training examples that are harmful to performance on a\nvalidation set. In this work, we adapt the TL;DR dataset for reward model\ntraining to demonstrate how conjugate-gradient approximated influence functions\ncan be used to filter datasets. In our experiments, influence function\nfiltering yields a small retraining accuracy uplift of 1.5% after removing 10%\nof training examples. We also show that gradient similarity outperforms\ninfluence functions for detecting helpful training examples. This suggests that\nlocal curvature is important for detecting harmful training examples, but less\nso for identifying helpful examples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u8f6d\u68af\u5ea6\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fc7\u6ee4\u566a\u58f0\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u9a8c\u663e\u793a\u8fc7\u6ee4\u540e\u6a21\u578b\u6027\u80fd\u63d0\u53471.5%\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4e2d\u5e38\u53d7\u566a\u58f0\u6570\u636e\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u3002\u5c0f\u89c4\u6a21\u540e\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4f7f\u5f97\u5f71\u54cd\u51fd\u6570\u8fd1\u4f3c\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u4f7f\u7528\u5171\u8f6d\u68af\u5ea6\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u6765\u68c0\u6d4b\u548c\u5220\u9664\u5bf9\u9a8c\u8bc1\u96c6\u6027\u80fd\u6709\u5bb3\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u5728TL;DR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u8fc7\u6ee410%\u7684\u8bad\u7ec3\u6837\u672c\u540e\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u53471.5%\u3002\u68af\u5ea6\u76f8\u4f3c\u6027\u5728\u68c0\u6d4b\u6709\u76ca\u6837\u672c\u4e0a\u4f18\u4e8e\u5f71\u54cd\u51fd\u6570\u3002", "conclusion": "\u5c40\u90e8\u66f2\u7387\u5bf9\u68c0\u6d4b\u6709\u5bb3\u6837\u672c\u91cd\u8981\uff0c\u4f46\u5bf9\u8bc6\u522b\u6709\u76ca\u6837\u672c\u5f71\u54cd\u8f83\u5c0f\u3002"}}
{"id": "2507.14195", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14195", "abs": "https://arxiv.org/abs/2507.14195", "authors": ["Elzbieta Gruzewska", "Pooja Rao", "Sebastien Baur", "Matthew Baugh", "Mathias M. J. Bellaiche", "Sharanya Srinivas", "Octavio Ponce", "Matthew Thompson", "Pramod Rudrapatna", "Michael A. Sanchez", "Lawrence Z. Cai", "Timothy JA Chico", "Robert F. Storey", "Emily Maz", "Umesh Telang", "Shravya Shetty", "Mayank Daswani"], "title": "UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach", "comment": "31 pages, 11 tables, 9 figures, 14 supplementary tables, 4\n  supplementary figures", "summary": "Radar technology presents untapped potential for continuous, contactless, and\npassive heart rate monitoring via consumer electronics like mobile phones.\nHowever the variety of available radar systems and lack of standardization\nmeans that a large new paired dataset collection is required for each radar\nsystem. This study demonstrates transfer learning between frequency-modulated\ncontinuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,\nboth increasingly integrated into consumer devices. FMCW radar utilizes a\ncontinuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW\nradar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3\nreceiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz\nbandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we\nachieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage\nerror (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119\nparticipants, an average of 8 hours per participant). This model maintained\nperformance (under 5 MAE/10% MAPE) across various body positions and heart rate\nranges, with a 98.9% recall. We then fine-tuned a variant of this model,\ntrained on single-antenna and single-range bin FMCW data, using a small (N=376,\navg 6 minutes per participant) IR-UWB dataset. This transfer learning approach\nyielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE\nreduction over the IR-UWB baseline. This demonstration of transfer learning\nbetween radar systems for heart rate monitoring has the potential to accelerate\nits introduction into existing consumer devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728FMCW\u548cIR-UWB\u96f7\u8fbe\u7cfb\u7edf\u4e4b\u95f4\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u7528\u4e8e\u5fc3\u7387\u76d1\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86IR-UWB\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u96f7\u8fbe\u6280\u672f\u5b9e\u73b0\u65e0\u63a5\u89e6\u3001\u88ab\u52a8\u7684\u5fc3\u7387\u76d1\u6d4b\uff0c\u4f46\u4e0d\u540c\u96f7\u8fbe\u7cfb\u7edf\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u9700\u5927\u91cf\u914d\u5bf9\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u65b0\u578b2D+1D ResNet\u67b6\u6784\uff0c\u5148\u5728FMCW\u96f7\u8fbe\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u518d\u8fc1\u79fb\u5230IR-UWB\u6570\u636e\u4e0a\u5fae\u8c03\u3002", "result": "FMCW\u96f7\u8fbe\u6a21\u578bMAE\u4e3a0.85 bpm\uff0cMAPE\u4e3a1.42%\uff1b\u8fc1\u79fb\u540eIR-UWB\u6a21\u578bMAE\u964d\u4f4e25%\uff0c\u8fbe\u52304.1 bpm\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u53ef\u52a0\u901f\u96f7\u8fbe\u5fc3\u7387\u76d1\u6d4b\u6280\u672f\u5728\u6d88\u8d39\u8bbe\u5907\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.14353", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14353", "abs": "https://arxiv.org/abs/2507.14353", "authors": ["Harsh Nilesh Pathak", "Randy Paffenroth"], "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers", "comment": null, "summary": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge.", "AI": {"tldr": "Solo Connection\u662f\u4e00\u79cd\u65b0\u9896\u7684PEFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u89e3\u7801\u5668\u5757\u7ea7\u522b\u7684\u8868\u793a\u800c\u975e\u5355\u4e2a\u6743\u91cd\u77e9\u9635\uff0c\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5e76\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8eLoRA\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5bf9\u73b0\u6709PEFT\u65b9\u6cd5\uff08\u5982LoRA\uff09\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT2\uff09\u4e2d\u89e3\u7801\u5668\u5757\u6570\u91cf\u589e\u52a0\u7684\u9700\u6c42\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "method": "Solo Connection\u5f15\u5165\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u9010\u6b65\u5728\u96f6\u5411\u91cf\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u4e4b\u95f4\u63d2\u503c\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7a33\u5b9a\u7684\u9002\u5e94\uff0c\u5e76\u91c7\u7528\u957f\u8df3\u8dc3\u8fde\u63a5\u589e\u5f3a\u4efb\u52a1\u9002\u5e94\u80fd\u529b\u3002", "result": "Solo Connection\u5728E2E\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8eLoRA\uff0c\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1159%\uff08\u76f8\u6bd4LoRA\uff09\u548c99%\u4ee5\u4e0a\uff08\u76f8\u6bd4\u5168\u5fae\u8c03\uff09\u3002", "conclusion": "Solo Connection\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u89e3\u7801\u5668\u5757\u6570\u91cf\u589e\u52a0\u7684\u6a21\u578b\u67b6\u6784\u3002"}}
{"id": "2507.14196", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14196", "abs": "https://arxiv.org/abs/2507.14196", "authors": ["Zahra Teimouri-Jervekani", "Fahimeh Nasimi", "Mohammadreza Yazdchi", "Ghazal MogharehZadeh", "Javad Tezerji", "Farzan Niknejad Mazandarani", "Maryam Mohebbi"], "title": "Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs", "comment": null, "summary": "Background and Objective: Differentiating wide complex tachycardia (WCT) is\nclinically critical yet challenging due to morphological similarities in\nelectrocardiogram (ECG) signals between life-threatening ventricular\ntachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).\nMisdiagnosis carries fatal risks. We propose a computationally efficient deep\nlearning solution to improve diagnostic accuracy and provide model\ninterpretability for clinical deployment.\n  Methods: A novel lightweight parallel deep architecture is introduced. Each\npipeline processes individual ECG leads using two 1D-CNN blocks to extract\nlocal features. Feature maps are concatenated across leads, followed by LSTM\nlayers to capture temporal dependencies. Final classification employs fully\nconnected layers. Explainability is achieved via Shapley Additive Explanations\n(SHAP) for local/global interpretation. The model was evaluated on a 35-subject\nECG database using standard performance metrics.\n  Results: The model achieved $95.63\\%$ accuracy ($95\\%$ CI: $93.07-98.19\\%$),\nwith sensitivity=$95.10\\%$, specificity=$96.06\\%$, and F1-score=$95.12\\%$. It\noutperformed state-of-the-art methods in both accuracy and computational\nefficiency, requiring minimal CNN blocks per pipeline. SHAP analysis\ndemonstrated clinically interpretable feature contributions.\n  Conclusions: Our end-to-end framework delivers high-precision WCT\nclassification with minimal computational overhead. The integration of SHAP\nenhances clinical trust by elucidating decision logic, supporting rapid,\ninformed diagnosis. This approach shows significant promise for real-world ECG\nanalysis tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5e76\u884c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u533a\u5206\u5bbd\u590d\u5408\u5fc3\u52a8\u8fc7\u901f\uff08WCT\uff09\uff0c\u5e76\u901a\u8fc7SHAP\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7531\u4e8e\u5fc3\u5ba4\u5fc3\u52a8\u8fc7\u901f\uff08VT\uff09\u548c\u4f34\u5dee\u5f02\u6027\u4f20\u5bfc\u7684\u5ba4\u4e0a\u6027\u5fc3\u52a8\u8fc7\u901f\uff08SVT-A\uff09\u5728\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u4e0a\u7684\u5f62\u6001\u76f8\u4f3c\u6027\uff0c\u533a\u5206WCT\u5177\u6709\u4e34\u5e8a\u6311\u6218\u6027\uff0c\u8bef\u8bca\u98ce\u9669\u9ad8\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u5e76\u884c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc71D-CNN\u5757\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0cLSTM\u5c42\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\uff0cSHAP\u63d0\u4f9b\u89e3\u91ca\u6027\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe95.63%\uff0c\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u5747\u8d85\u8fc795%\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6WCT\u5206\u7c7b\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0cSHAP\u589e\u5f3a\u4e86\u4e34\u5e8a\u4fe1\u4efb\uff0c\u9002\u7528\u4e8e\u5b9e\u9645ECG\u5206\u6790\u3002"}}
{"id": "2507.14387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14387", "abs": "https://arxiv.org/abs/2507.14387", "authors": ["Arun Vignesh Malarkkan", "Dongjie Wang", "Haoyue Bai", "Yanjie Fu"], "title": "Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures", "comment": "12 pages, 5 figures, 3 Tables, under review in IEEE Transactions on\n  Big Data", "summary": "The escalating threat of cyberattacks on real-time critical infrastructures\nposes serious risks to public safety, demanding detection methods that\neffectively capture complex system interdependencies and adapt to evolving\nattack patterns. Traditional real-time anomaly detection techniques often\nsuffer from excessive false positives due to their statistical sensitivity to\nhigh data variance and class imbalance. To address these limitations, recent\nresearch has explored modeling causal relationships among system components.\nHowever, prior work mainly focuses on offline causal graph-based approaches\nthat require static historical data and fail to generalize to real-time\nsettings. These methods are fundamentally constrained by: (1) their inability\nto adapt to dynamic shifts in data distribution without retraining, and (2) the\nrisk of catastrophic forgetting when lacking timely supervision in live\nsystems. To overcome these challenges, we propose INCADET, a novel framework\nfor incremental causal graph learning tailored to real-time cyberattack\ndetection. INCADET dynamically captures evolving system behavior by\nincrementally updating causal graphs across streaming time windows. The\nframework comprises three modules: 1) Early Symptom Detection: Detects\ntransitions in system status using divergence in edge-weight distributions\nacross sequential causal graphs. 2) Incremental Causal Graph Learning:\nLeverages experience replay and edge reinforcement to continually refine causal\nstructures while preserving prior knowledge. 3) Causal Graph Classification:\nEmploys Graph Convolutional Networks (GCNs) to classify system status using the\nlearned causal graphs. Extensive experiments on real-world critical\ninfrastructure datasets demonstrate that INCADET achieves superior accuracy,\nrobustness, and adaptability compared to both static causal and deep temporal\nbaselines in evolving attack scenarios.", "AI": {"tldr": "INCADET\u662f\u4e00\u79cd\u7528\u4e8e\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u7684\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u56e0\u679c\u56fe\u6765\u9002\u5e94\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u7f51\u7edc\u653b\u51fb\u5bf9\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u9ad8\u6570\u636e\u65b9\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u8bef\u62a5\u7387\u9ad8\uff0c\u4e14\u9759\u6001\u56e0\u679c\u56fe\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u52a8\u6001\u53d8\u5316\u3002", "method": "INCADET\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u65e9\u671f\u75c7\u72b6\u68c0\u6d4b\u3001\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u548c\u56e0\u679c\u56fe\u5206\u7c7b\uff0c\u5229\u7528\u6d41\u5f0f\u65f6\u95f4\u7a97\u53e3\u52a8\u6001\u66f4\u65b0\u56e0\u679c\u56fe\uff0c\u5e76\u7ed3\u5408GCN\u5206\u7c7b\u7cfb\u7edf\u72b6\u6001\u3002", "result": "\u5728\u771f\u5b9e\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cINCADET\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u9759\u6001\u56e0\u679c\u548c\u6df1\u5ea6\u65f6\u5e8f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "INCADET\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u52a8\u6001\u6355\u6349\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\uff0c\u4e3a\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14419", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14419", "abs": "https://arxiv.org/abs/2507.14419", "authors": ["Guojun Wu"], "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "comment": null, "summary": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u901a\u8fc7\u9650\u5236\u6700\u5927\u957f\u5ea6\u5b9e\u73b0\u7f29\u653e\uff0c\u800c\u901a\u8fc7\u8ffd\u52a0\u201cWait\u201d\u7684\u65b9\u5f0f\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u3002\u4e0eo1\u7c7b\u6a21\u578b\u76f8\u6bd4\uff0c\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65e0\u6cd5\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5e76\u4e0eo1\u7c7b\u6a21\u578b\u7684\u81ea\u7136\u7f29\u653e\u884c\u4e3a\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u5206\u6790\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\uff08\u5305\u62ec\u9650\u5236\u6700\u5927\u957f\u5ea6\u548c\u8ffd\u52a0\u201cWait\u201d\uff09\u7684\u6548\u679c\uff0c\u5e76\u4e0eo1\u7c7b\u6a21\u578b\u7684\u81ea\u7136\u7f29\u653e\u884c\u4e3a\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u9650\u5236\u6700\u5927\u957f\u5ea6\u662f\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u4e3b\u8981\u65b9\u5f0f\uff0c\u800c\u8ffd\u52a0\u201cWait\u201d\u4f1a\u5bfc\u81f4\u6a21\u578b\u632f\u8361\u3002o1\u7c7b\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u7136\u7f29\u653e\uff0c\u6027\u80fd\u8d85\u8d8a\u5cf0\u503c\u3002", "conclusion": "\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65e0\u6cd5\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u771f\u6b63\u7684\u76ee\u6807\u662f\u89e3\u9501\u66f4\u9ad8\u6027\u80fd\uff0c\u800c\u975e\u4ec5\u6a21\u4eff\u7f29\u653e\u884c\u4e3a\u3002"}}
{"id": "2507.14208", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14208", "abs": "https://arxiv.org/abs/2507.14208", "authors": ["Mohammadreza F. Imani", "Alexander L. Colson", "Leslie K. Miller", "Jorge A. Valdez", "Jose C. Sanchez", "Richard F. Rader"], "title": "Toward intelligent wireless networks in computer chassis", "comment": "9 pages, 5 figures", "summary": "Processing the exponentially growing amount of data produced daily requires\nefficient communication between different processing units in a computer.\nTraditionally, wired interconnects have been used to maintain these data links\ndue to their energy efficiency and ability to support high data rates. However,\nas computing demands continue to increase in size and speed, these wired\ninterconnects can become longer and less effective. One possible solution is to\nenhance the wired interconnects with short-range wireless communication (SRWC),\nwhich offers flexible resource allocation and the ability to broadcast data.\nHowever, implementing SRWC inside a computer chassis presents challenges due to\nmultiple scattering. This scattering stretches the channel impulse response\n(CIR), leading to inter-symbol interference (ISI) and limiting data rates. To\naddress this issue, we propose transforming the computer chassis into a smart\nradio environment by utilizing a reconfigurable intelligent surface (RIS). The\nRIS elements adjust the phase of reflected waves so that the multipath\ncomponents combine at the receiver in a way that creates a pulse-like CIR. This\napproach has been experimentally validated within a typical computer chassis.\nThe results of this study pave the way for integrating RIS-enabled SRWC to\nenhance wireless links in both current and future data processing units.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u4f18\u5316\u8ba1\u7b97\u673a\u673a\u7bb1\u5185\u7684\u77ed\u8ddd\u79bb\u65e0\u7ebf\u901a\u4fe1\uff08SRWC\uff09\uff0c\u4ee5\u89e3\u51b3\u591a\u5f84\u6563\u5c04\u5bfc\u81f4\u7684\u4fe1\u9053\u8109\u51b2\u54cd\u5e94\uff08CIR\uff09\u5ef6\u957f\u548c\u7b26\u53f7\u95f4\u5e72\u6270\uff08ISI\uff09\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u9700\u6c42\u589e\u957f\uff0c\u4f20\u7edf\u6709\u7ebf\u4e92\u8fde\u5728\u9ad8\u901f\u6570\u636e\u4f20\u8f93\u4e2d\u6548\u7387\u964d\u4f4e\uff0cSRWC\u867d\u7075\u6d3b\u4f46\u53d7\u591a\u5f84\u6563\u5c04\u9650\u5236\uff0c\u9700\u89e3\u51b3ISI\u95ee\u9898\u3002", "method": "\u901a\u8fc7RIS\u8c03\u6574\u53cd\u5c04\u6ce2\u76f8\u4f4d\uff0c\u4f7f\u591a\u5f84\u5206\u91cf\u5728\u63a5\u6536\u7aef\u5f62\u6210\u8109\u51b2\u72b6CIR\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u8ba1\u7b97\u673a\u673a\u7bb1\u5185\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRIS-enabled SRWC\u80fd\u6709\u6548\u4f18\u5316\u65e0\u7ebf\u94fe\u8def\uff0c\u63d0\u5347\u6570\u636e\u4f20\u8f93\u901f\u7387\u3002", "conclusion": "RIS\u6280\u672f\u4e3a\u5f53\u524d\u548c\u672a\u6765\u6570\u636e\u5904\u7406\u5355\u5143\u7684\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14446", "abs": "https://arxiv.org/abs/2507.14446", "authors": ["Feng Liu", "Ying Liu", "Carson Eisenach"], "title": "Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness", "comment": null, "summary": "In this work, we study how to efficiently apply reinforcement learning (RL)\nfor solving large-scale stochastic optimization problems by leveraging\nintervention models. The key of the proposed methodology is to better explore\nthe solution space by simulating and composing the stochastic processes using\npre-trained deep learning (DL) models. We demonstrate our approach on a\nchallenging real-world application, the multi-sourcing multi-period inventory\nmanagement problem in supply chain optimization. In particular, we employ deep\nRL models for learning and forecasting the stochastic supply chain processes\nunder a range of assumptions. Moreover, we also introduce a constraint\ncoordination mechanism, designed to forecast dual costs given the\ncross-products constraints in the inventory network. We highlight that instead\nof directly modeling the complex physical constraints into the RL optimization\nproblem and solving the stochastic problem as a whole, our approach breaks down\nthose supply chain processes into scalable and composable DL modules, leading\nto improved performance on large real-world datasets. We also outline open\nproblems for future research to further investigate the efficacy of such\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e72\u9884\u6a21\u578b\u9ad8\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u4f9b\u5e94\u94fe\u5e93\u5b58\u7ba1\u7406\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\u4e2d\u63a2\u7d22\u89e3\u7a7a\u95f4\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4f9b\u5e94\u94fe\u573a\u666f\u4e2d\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6a21\u62df\u548c\u7ec4\u5408\u968f\u673a\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u7ea6\u675f\u534f\u8c03\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u4f9b\u5e94\u94fe\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u666e\u9002\u6027\u3002"}}
{"id": "2507.14210", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14210", "abs": "https://arxiv.org/abs/2507.14210", "authors": ["Jiayuan Wei", "Qingwei Jiang", "Wen Fang", "Mingqing Liu", "Qingwen Liu", "Wen Chen", "Qingqing Wu"], "title": "System Design and Performance Analysis for RIS-assisted Terahertz Self-Alignment Beamforming", "comment": null, "summary": "The widespread deployment of Internet of Things(IoT) devices underscores the\nneed for sustainable wireless solutions capable of simultaneously transferring\nboth energy and information. Terahertz (THz) band-enabled simultaneous wireless\ninformation and power transfer (SWIPT) systems offer ultra-high data rates and\nexpansive bandwidth. However, THz waves are inherently susceptible to severe\npath loss and beam misalignment due to their narrow-beam characteristics. In\nthis context, this paper proposes a reconfigurable intelligent\nsurface(RIS)-assisted transmitter architecture for the THz-SWIPT system, which\nenables end-to-end self-alignment for steady-state transmission. The proposed\nsystem incorporates phase conjugate circuits to achieve self-aligned\nbeamforming, facilitating the dynamic tracking of mobile IoT devices without\nthe need for beam training. Additionally, active amplification within the RIS\narrays compensates for cascaded channel attenuation through an iterative power\ncycle, thereby enhancing the energy transmission efficiency. Theoretical models\nand simulations indicate that the proposed system significantly mitigates\nsidelobe interference, achieving a transmission efficiency of up to 73.26% over\na 2 meter distance with self-alignment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u7684\u592a\u8d6b\u5179\u9891\u6bb5\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u540c\u4f20\uff08SWIPT\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u5bf9\u51c6\u6ce2\u675f\u6210\u5f62\u548c\u4e3b\u52a8\u653e\u5927\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u8f93\u6548\u7387\u548c\u80fd\u91cf\u4f20\u8f93\u80fd\u529b\u3002", "motivation": "\u7269\u8054\u7f51\uff08IoT\uff09\u8bbe\u5907\u7684\u5e7f\u6cdb\u90e8\u7f72\u9700\u8981\u53ef\u6301\u7eed\u7684\u65e0\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u4f20\u8f93\u4fe1\u606f\u548c\u80fd\u91cf\u3002\u592a\u8d6b\u5179\u9891\u6bb5\u867d\u7136\u63d0\u4f9b\u9ad8\u6570\u636e\u901f\u7387\u548c\u5e26\u5bbd\uff0c\u4f46\u5176\u7a84\u6ce2\u675f\u7279\u6027\u5bfc\u81f4\u4e25\u91cd\u7684\u8def\u5f84\u635f\u8017\u548c\u6ce2\u675f\u5931\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdRIS\u8f85\u52a9\u7684\u53d1\u5c04\u5668\u67b6\u6784\uff0c\u5229\u7528\u76f8\u4f4d\u5171\u8f6d\u7535\u8def\u5b9e\u73b0\u81ea\u5bf9\u51c6\u6ce2\u675f\u6210\u5f62\uff0c\u5e76\u901a\u8fc7RIS\u9635\u5217\u4e2d\u7684\u4e3b\u52a8\u653e\u5927\u8865\u507f\u7ea7\u8054\u4fe1\u9053\u8870\u51cf\u3002", "result": "\u7406\u8bba\u6a21\u578b\u548c\u4eff\u771f\u8868\u660e\uff0c\u7cfb\u7edf\u663e\u8457\u51cf\u5c11\u4e86\u65c1\u74e3\u5e72\u6270\uff0c\u57282\u7c73\u8ddd\u79bb\u5185\u5b9e\u73b0\u4e8673.26%\u7684\u4f20\u8f93\u6548\u7387\u3002", "conclusion": "RIS\u8f85\u52a9\u7684THz-SWIPT\u7cfb\u7edf\u901a\u8fc7\u81ea\u5bf9\u51c6\u548c\u4e3b\u52a8\u653e\u5927\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u592a\u8d6b\u5179\u9891\u6bb5\u7684\u4f20\u8f93\u95ee\u9898\uff0c\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u540c\u4f20\u65b9\u6848\u3002"}}
{"id": "2507.14484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14484", "abs": "https://arxiv.org/abs/2507.14484", "authors": ["Yule Li", "Yifeng Lu", "Zhen Wang", "Zhewei Wei", "Yaliang Li", "Bolin Ding"], "title": "ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions", "comment": null, "summary": "In recent years, graph neural networks (GNN) have achieved unprecedented\nsuccesses in node classification tasks. Although GNNs inherently encode\nspecific inductive biases (e.g., acting as low-pass or high-pass filters), most\nexisting methods implicitly assume conditional independence among node labels\nin their optimization objectives. While this assumption is suitable for\ntraditional classification tasks such as image recognition, it contradicts the\nintuitive observation that node labels in graphs remain correlated, even after\nconditioning on the graph structure. To make structured predictions for node\nlabels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for\nStructured node Classification. ReDiSC estimates the joint distribution of node\nlabels using a reparameterized masked diffusion model, which is learned through\nthe variational expectation-maximization (EM) framework. Our theoretical\nanalysis shows the efficiency advantage of ReDiSC in the E-step compared to\nDPM-SNC, a state-of-the-art model that relies on a manifold-constrained\ndiffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's\nM-step objective to popular GNN and label propagation hybrid approaches.\nExtensive experiments demonstrate that ReDiSC achieves superior or highly\ncompetitive performance compared to state-of-the-art GNN, label propagation,\nand diffusion-based baselines across both homophilic and heterophilic graphs of\nvarying sizes. Notably, ReDiSC scales effectively to large-scale datasets on\nwhich previous structured diffusion methods fail due to computational\nconstraints, highlighting its significant practical advantage in structured\nnode classification tasks.", "AI": {"tldr": "ReDiSC\u662f\u4e00\u79cd\u57fa\u4e8e\u91cd\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\uff0c\u901a\u8fc7\u53d8\u5206EM\u6846\u67b6\u4f18\u5316\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5047\u8bbe\u8282\u70b9\u6807\u7b7e\u6761\u4ef6\u72ec\u7acb\uff0c\u5ffd\u7565\u4e86\u56fe\u7ed3\u6784\u4e2d\u6807\u7b7e\u7684\u76f8\u5173\u6027\uff0cReDiSC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u91cd\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u8282\u70b9\u6807\u7b7e\u7684\u8054\u5408\u5206\u5e03\uff0c\u901a\u8fc7\u53d8\u5206EM\u6846\u67b6\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u56fe\u4e0a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709GNN\u3001\u6807\u7b7e\u4f20\u64ad\u548c\u6269\u6563\u6a21\u578b\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ReDiSC\u5728\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2507.14216", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14216", "abs": "https://arxiv.org/abs/2507.14216", "authors": ["Manish Kumar", "Tzu-Hsuan Chou", "Byunghyun Lee", "Nicol\u00f2 Michelusi", "David J. Love", "Yaguang Zhang", "James V. Krogmeier"], "title": "Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems", "comment": "This paper has been submitted to IEEE Transactions on Wireless\n  Communications", "summary": "Low-latency localization is critical in cellular networks to support\nreal-time applications requiring precise positioning. In this paper, we propose\na distributed machine learning (ML) framework for fingerprint-based\nlocalization tailored to cell-free massive multiple-input multiple-output\n(MIMO) systems, an emerging architecture for 6G networks. The proposed\nframework enables each access point (AP) to independently train a Gaussian\nprocess regression model using local angle-of-arrival and received signal\nstrength fingerprints. These models provide probabilistic position estimates\nfor the user equipment (UE), which are then fused by the UE with minimal\ncomputational overhead to derive a final location estimate. This decentralized\napproach eliminates the need for fronthaul communication between the APs and\nthe central processing unit (CPU), thereby reducing latency. Additionally,\ndistributing computational tasks across the APs alleviates the processing\nburden on the CPU compared to traditional centralized localization schemes.\nSimulation results demonstrate that the proposed distributed framework achieves\nlocalization accuracy comparable to centralized methods, despite lacking the\nbenefits of centralized data aggregation. Moreover, it effectively reduces\nuncertainty of the location estimates, as evidenced by the 95\\% covariance\nellipse. The results highlight the potential of distributed ML for enabling\nlow-latency, high-accuracy localization in future 6G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u7684\u4f4e\u5ef6\u8fdf\u5b9a\u4f4d\uff0c\u901a\u8fc7\u72ec\u7acb\u8bad\u7ec3\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\u5e76\u878d\u5408\u4f30\u8ba1\u7ed3\u679c\uff0c\u51cf\u5c11\u5ef6\u8fdf\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u652f\u6301\u5b9e\u65f6\u5e94\u7528\u7684\u4f4e\u5ef6\u8fdf\u5b9a\u4f4d\u9700\u6c42\uff0c\u9002\u5e946G\u7f51\u7edc\u67b6\u6784\u3002", "method": "\u6bcf\u4e2a\u63a5\u5165\u70b9\u72ec\u7acb\u8bad\u7ec3\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u6a21\u578b\uff0c\u7528\u6237\u8bbe\u5907\u878d\u5408\u4f30\u8ba1\u7ed3\u679c\u3002", "result": "\u5206\u5e03\u5f0f\u6846\u67b6\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u4e0e\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u5ef6\u8fdf\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u6846\u67b6\u57286G\u7f51\u7edc\u4e2d\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14487", "abs": "https://arxiv.org/abs/2507.14487", "authors": ["Ukjo Hwang", "Songnam Hong"], "title": "Federated Reinforcement Learning in Heterogeneous Environments", "comment": null, "summary": "We investigate a Federated Reinforcement Learning with Environment\nHeterogeneity (FRL-EH) framework, where local environments exhibit statistical\nheterogeneity. Within this framework, agents collaboratively learn a global\npolicy by aggregating their collective experiences while preserving the privacy\nof their local trajectories. To better reflect real-world scenarios, we\nintroduce a robust FRL-EH framework by presenting a novel global objective\nfunction. This function is specifically designed to optimize a global policy\nthat ensures robust performance across heterogeneous local environments and\ntheir plausible perturbations. We propose a tabular FRL algorithm named FedRQ\nand theoretically prove its asymptotic convergence to an optimal policy for the\nglobal objective function. Furthermore, we extend FedRQ to environments with\ncontinuous state space through the use of expectile loss, addressing the key\nchallenge of minimizing a value function over a continuous subset of the state\nspace. This advancement facilitates the seamless integration of the principles\nof FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive\nempirical evaluations validate the effectiveness and robustness of our FRL\nalgorithms across diverse heterogeneous environments, consistently achieving\nsuperior performance over the existing state-of-the-art FRL algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6FRL-EH\uff0c\u89e3\u51b3\u5c40\u90e8\u73af\u5883\u7edf\u8ba1\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u5408\u7ecf\u9a8c\u5b66\u4e60\u5168\u5c40\u7b56\u7565\u5e76\u4fdd\u62a4\u9690\u79c1\u3002\u63d0\u51fa\u65b0\u76ee\u6807\u51fd\u6570\u4f18\u5316\u5168\u5c40\u7b56\u7565\uff0c\u786e\u4fdd\u9c81\u68d2\u6027\u3002\u63d0\u51faFedRQ\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u6269\u5c55\u81f3\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5c40\u90e8\u73af\u5883\u7edf\u8ba1\u5f02\u8d28\u6027\u5bf9\u5168\u5c40\u7b56\u7565\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u9c81\u68d2\u6846\u67b6\u3002", "method": "\u63d0\u51faFRL-EH\u6846\u67b6\u53ca\u65b0\u76ee\u6807\u51fd\u6570\uff0c\u8bbe\u8ba1FedRQ\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u6269\u5c55\u81f3\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u3002", "result": "FedRQ\u7b97\u6cd5\u5728\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FRL-EH\u6846\u67b6\u53caFedRQ\u7b97\u6cd5\u5728\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5f02\u6784\u73af\u5883\u3002"}}
{"id": "2507.14220", "categories": ["eess.SP", "cs.LG", "physics.acc-ph"], "pdf": "https://arxiv.org/pdf/2507.14220", "abs": "https://arxiv.org/abs/2507.14220", "authors": ["Haitian Hu", "Wei Zhang", "Feng Feng", "Zhiguo Zhang", "Qi-Jun Zhang"], "title": "Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters", "comment": null, "summary": "This article introduces an advanced space mapping (SM) technique that applies\na shared electromagnetic (EM)-based coarse model for multistate tuning-driven\nmultiphysics optimization of tunable filters. The SM method combines the\ncomputational efficiency of EM single-physics simulations with the precision of\nmultiphysics simulations. The shared coarse model is based on EM single-physics\nresponses corresponding to various nontunable design parameters values.\nConversely, the fine model is implemented to delineate the behavior of\nmultiphysics responses concerning both nontunable and tunable design parameter\nvalues. The proposed overall surrogate model comprises multiple subsurrogate\nmodels, each consisting of one shared coarse model and two distinct mapping\nneural networks. The responses from the shared coarse model in the EM\nsingle-physics filed offer a suitable approximation for the fine responses in\nthe multiphysics filed, whereas the mapping neural networks facilitate\ntransition from the EM single-physics field to the multiphysics field. Each\nsubsurrogate model maintains consistent nontunable design parameter values but\npossesses unique tunable design parameter values. By developing multiple\nsubsurrogate models, optimization can be simultaneously performed for each\ntuning state. Nontunable design parameter values are constrained by all tuning\nstates, whereas tunable design parameter values are confined to their\nrespective tuning states. This optimization technique simultaneously accounts\nfor all the tuning states to fulfill the necessary multiple tuning state\nrequirements. Multiple EM and multiphysics training samples are generated\nconcurrently to develop the surrogate model. Compared with existing direct\nmultiphysics parameterized modeling techniques, our proposed method achieves\nsuperior multiphysics modeling accuracy with fewer training samples and reduced\ncomputational costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u4eab\u7535\u78c1\u7c97\u6a21\u578b\u7684\u7a7a\u95f4\u6620\u5c04\u6280\u672f\uff0c\u7528\u4e8e\u591a\u72b6\u6001\u8c03\u8c10\u9a71\u52a8\u7684\u591a\u7269\u7406\u573a\u6ee4\u6ce2\u5668\u4f18\u5316\uff0c\u7ed3\u5408\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u7269\u7406\u573a\u4f18\u5316\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5171\u4eab\u7535\u78c1\u7c97\u6a21\u578b\u548c\u6620\u5c04\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u591a\u5b50\u4ee3\u7406\u6a21\u578b\uff0c\u540c\u65f6\u4f18\u5316\u591a\u72b6\u6001\u8c03\u8c10\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u548c\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u591a\u7269\u7406\u573a\u5efa\u6a21\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u7269\u7406\u573a\u4f18\u5316\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u591a\u72b6\u6001\u8c03\u8c10\u9700\u6c42\u3002"}}
{"id": "2507.14224", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14224", "abs": "https://arxiv.org/abs/2507.14224", "authors": ["Beno\u00eet Brebion", "Alban Gallard", "Katrin Sippel", "Amer Zaylaa", "Hubert Preissl", "Sahar Moghimi", "Fabrice Wallois", "Ya\u00ebl Fr\u00e9gier"], "title": "Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG", "comment": null, "summary": "Background and objective: Brain activity in premature newborns has\ntraditionally been studied using electroencephalography (EEG), leading to\nsubstantial advances in our understanding of early neural development. However,\nsince brain development takes root at the fetal stage, a critical window of\nthis process remains largely unknown. The only technique capable of recording\nneural activity in the intrauterine environment is fetal magnetoencephalography\n(fMEG), but this approach presents challenges in terms of data quality and\nscarcity. Using artificial intelligence, the present research aims to transfer\nthe well-established knowledge from EEG studies to fMEG to improve\nunderstanding of prenatal brain development, laying the foundations for better\ndetection and treatment of potential pathologies. Methods: We developed an\nunpaired diffusion translation method based on dual diffusion bridges, which\nnotably includes numerical integration improvements to obtain more qualitative\nresults at a lower computational cost. Models were trained on our unpaired\ndataset of bursts of spontaneous activity from 30 high-resolution premature\nnewborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that\nour method achieves significant improvement upon previous results obtained with\nGenerative Adversarial Networks (GANs), by almost 5% on the mean squared error\nin the time domain, and completely eliminating the mode collapse problem in the\nfrequency domain, thus achieving near-perfect signal fidelity. Conclusion: We\nset a new state of the art in the EEG-fMEG unpaired translation problem, as our\ndeveloped tool completely paves the way for early brain activity analysis.\nOverall, we also believe that our method could be reused for other unpaired\nsignal translation applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u5c06\u6210\u719f\u7684EEG\u77e5\u8bc6\u8fc1\u79fb\u5230fMEG\u4e2d\uff0c\u4ee5\u63d0\u5347\u5bf9\u80ce\u513f\u5927\u8111\u53d1\u80b2\u7684\u7406\u89e3\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u6269\u6563\u6865\u7684\u975e\u914d\u5bf9\u6269\u6563\u7ffb\u8bd1\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfEEG\u7814\u7a76\u65e9\u4ea7\u513f\u8111\u6d3b\u52a8\u5df2\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u80ce\u513f\u9636\u6bb5\u7684\u8111\u53d1\u80b2\u4ecd\u662f\u672a\u77e5\u9886\u57df\u3002fMEG\u662f\u552f\u4e00\u80fd\u8bb0\u5f55\u5bab\u5185\u795e\u7ecf\u6d3b\u52a8\u7684\u6280\u672f\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u548c\u7a00\u7f3a\u6027\u5b58\u5728\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u6269\u6563\u6865\u7684\u975e\u914d\u5bf9\u6269\u6563\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u6570\u503c\u79ef\u5206\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5229\u752830\u4e2a\u9ad8\u5206\u8fa8\u7387EEG\u548c44\u4e2afMEG\u8bb0\u5f55\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u57df\u5747\u65b9\u8bef\u5dee\u4e0a\u6bd4GAN\u63d0\u5347\u8fd15%\uff0c\u5e76\u5b8c\u5168\u89e3\u51b3\u4e86\u9891\u57df\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u4fe1\u53f7\u4fdd\u771f\u5ea6\u63a5\u8fd1\u5b8c\u7f8e\u3002", "conclusion": "\u5728EEG-fMEG\u975e\u914d\u5bf9\u7ffb\u8bd1\u95ee\u9898\u4e0a\u8fbe\u5230\u4e86\u65b0\u6c34\u5e73\uff0c\u4e3a\u65e9\u671f\u8111\u6d3b\u52a8\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4e14\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u975e\u914d\u5bf9\u4fe1\u53f7\u7ffb\u8bd1\u5e94\u7528\u3002"}}
{"id": "2507.14503", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14503", "abs": "https://arxiv.org/abs/2507.14503", "authors": ["Jiequan Cui", "Beier Zhu", "Qingshan Xu", "Xiaogang Xu", "Pengguang Chen", "Xiaojuan Qi", "Bei Yu", "Hanwang Zhang", "Richang Hong"], "title": "Generative Distribution Distillation", "comment": "Technique report", "summary": "In this paper, we formulate the knowledge distillation (KD) as a conditional\ngenerative problem and propose the \\textit{Generative Distribution Distillation\n(GenDD)} framework. A naive \\textit{GenDD} baseline encounters two major\nchallenges: the curse of high-dimensional optimization and the lack of semantic\nsupervision from labels. To address these issues, we introduce a \\textit{Split\nTokenization} strategy, achieving stable and effective unsupervised KD.\nAdditionally, we develop the \\textit{Distribution Contraction} technique to\nintegrate label supervision into the reconstruction objective. Our theoretical\nproof demonstrates that \\textit{GenDD} with \\textit{Distribution Contraction}\nserves as a gradient-level surrogate for multi-task learning, realizing\nefficient supervised training without explicit classification loss on\nmulti-step sampling image representations. To evaluate the effectiveness of our\nmethod, we conduct experiments on balanced, imbalanced, and unlabeled data.\nExperimental results show that \\textit{GenDD} performs competitively in the\nunsupervised setting, significantly surpassing KL baseline by \\textbf{16.29\\%}\non ImageNet validation set. With label supervision, our ResNet-50 achieves\n\\textbf{82.28\\%} top-1 accuracy on ImageNet in 600 epochs training,\nestablishing a new state-of-the-art.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u95ee\u9898\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6GenDD\uff0c\u901a\u8fc7Split Tokenization\u548cDistribution Contraction\u6280\u672f\u89e3\u51b3\u4e86\u9ad8\u7ef4\u4f18\u5316\u548c\u6807\u7b7e\u8bed\u4e49\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u9ad8\u7ef4\u4f18\u5316\u548c\u7f3a\u4e4f\u6807\u7b7e\u8bed\u4e49\u76d1\u7763\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faGenDD\u6846\u67b6\uff0c\u7ed3\u5408Split Tokenization\u5b9e\u73b0\u7a33\u5b9a\u65e0\u76d1\u7763\u77e5\u8bc6\u84b8\u998f\uff0c\u5e76\u901a\u8fc7Distribution Contraction\u6280\u672f\u6574\u5408\u6807\u7b7e\u76d1\u7763\u3002", "result": "\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cGenDD\u663e\u8457\u4f18\u4e8eKL\u57fa\u7ebf\uff0816.29%\u63d0\u5347\uff09\uff1b\u6709\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cResNet-50\u5728ImageNet\u4e0a\u8fbe\u523082.28% top-1\u51c6\u786e\u7387\u3002", "conclusion": "GenDD\u6846\u67b6\u5728\u77e5\u8bc6\u84b8\u998f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2507.14228", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14228", "abs": "https://arxiv.org/abs/2507.14228", "authors": ["Xiaobin Zhu", "Minling Zhang", "Guofa Cai", "Jiguang He", "Georges Kaddoum"], "title": "Design of A New Multiple-Chirp-Rate Index Modulation for LoRa Networks", "comment": "13 pages,11 figures,3 tables", "summary": "We propose a multiple chirp rate index modulation (MCR-IM) system based on\nZadoff-Chu (ZC) sequences that overcomes the problems of low transmission rate\nand large-scale access in classical LoRa networks. We demonstrate the extremely\nlow cross-correlation of MCR-IM signals across different spread factors,\nshowing that the proposed MCR-IM system also inherits the characteristics of ZC\nsequences modulation. Moreover, we derive an approximate closed-form expression\nfor the bit-error rate (BER) of the proposed MCR-IM system over Nakagami-m\nfading channels. Simulation results confirm the accuracy of the derived\nclosed-form expression and demonstrate that the MCR-IM system achieves higher\nlevels of spectral efficiency (SE) compared to existing systems. In this\ncontext, assigning multiple chirp rates to each user results in a reduction in\nthe number of parallel channels. To mitigate this issue, we propose a peak\ndetection based successive interference cancellation (PD-SIC) algorithm to\naccommodate more users. Compared to orthogonal scatter chirp spreading spectrum\nsystem that names OrthoRa, the MCR-IM system with PD-SIC algorithm achieves\nlower BER levels. For a similar number of collision signals, the throughput of\nthe MCR-IM system is enhanced by 16% to 21%. Owing to these advantages, the\nproposed MCR-IM is well suited for large-scale, high-rate LoRa network\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eZadoff-Chu\u5e8f\u5217\u7684\u591a\u5541\u557e\u901f\u7387\u7d22\u5f15\u8c03\u5236\uff08MCR-IM\uff09\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLoRa\u7f51\u7edc\u4f20\u8f93\u901f\u7387\u4f4e\u548c\u5927\u89c4\u6a21\u63a5\u5165\u7684\u95ee\u9898\u3002\u8be5\u7cfb\u7edf\u5177\u6709\u6781\u4f4e\u7684\u8de8\u76f8\u5173\u6027\u548c\u66f4\u9ad8\u7684\u9891\u8c31\u6548\u7387\uff0c\u5e76\u901a\u8fc7PD-SIC\u7b97\u6cd5\u652f\u6301\u66f4\u591a\u7528\u6237\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfLoRa\u7f51\u7edc\u4f20\u8f93\u901f\u7387\u4f4e\u548c\u5927\u89c4\u6a21\u63a5\u5165\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eZadoff-Chu\u5e8f\u5217\u7684\u591a\u5541\u557e\u901f\u7387\u7d22\u5f15\u8c03\u5236\uff08MCR-IM\uff09\u7cfb\u7edf\uff0c\u7ed3\u5408PD-SIC\u7b97\u6cd5\u3002", "result": "MCR-IM\u7cfb\u7edf\u5728Nakagami-m\u8870\u843d\u4fe1\u9053\u4e0b\u5177\u6709\u8f83\u4f4e\u7684\u8bef\u7801\u7387\uff08BER\uff09\uff0c\u9891\u8c31\u6548\u7387\u63d0\u534716%\u81f321%\u3002", "conclusion": "MCR-IM\u7cfb\u7edf\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u3001\u9ad8\u901f\u7387\u7684LoRa\u7f51\u7edc\u5e94\u7528\u3002"}}
{"id": "2507.14516", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14516", "abs": "https://arxiv.org/abs/2507.14516", "authors": ["Jeyoung Lee", "Hochul Kang"], "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning", "comment": null, "summary": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u5ea6\u91cf\u51fd\u6570SDSC\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8ddd\u79bb\u76ee\u6807\uff08\u5982MSE\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfSSL\u65b9\u6cd5\uff08\u5982MSE\uff09\u5bf9\u632f\u5e45\u654f\u611f\u3001\u5bf9\u6ce2\u5f62\u6781\u6027\u4e0d\u53d8\u4e14\u5c3a\u5ea6\u65e0\u754c\uff0c\u963b\u788d\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "SDSC\u57fa\u4e8eDice\u76f8\u4f3c\u7cfb\u6570\uff0c\u91cf\u5316\u65f6\u95f4\u4fe1\u53f7\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u53ef\u4f5c\u4e3a\u635f\u5931\u51fd\u6570\u4f7f\u7528\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u635f\u5931\uff0c\u7ed3\u5408SDSC\u4e0eMSE\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDSC\u5728\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0eMSE\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u9886\u57df\u5185\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5ea6\u91cf\uff08\u5982SDSC\uff09\u80fd\u63d0\u5347\u4fe1\u53f7\u8868\u793a\u7684\u8bed\u4e49\u8d28\u91cf\uff0c\u53ef\u4f5c\u4e3a\u4f20\u7edf\u8ddd\u79bb\u65b9\u6cd5\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.14299", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14299", "abs": "https://arxiv.org/abs/2507.14299", "authors": ["Yu Bai", "Yifan Zhang", "Boxuan Xie", "Zheng Chang", "Yanru Zhang", "Riku Jantti", "Zhu Han"], "title": "Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) equipped with integrated sensing and\ncommunication (ISAC) capabilities are envisioned to play a pivotal role in\nfuture wireless networks due to their enhanced flexibility and efficiency.\nHowever, jointly optimizing UAV trajectory planning, multi-user communication,\nand target sensing under stringent resource constraints and time-critical\nconditions remains a significant challenge. To address this, we propose an Age\nof Information (AoI)-centric UAV-ISAC system that simultaneously performs\ntarget sensing and serves multiple ground users, emphasizing information\nfreshness as the core performance metric. We formulate a long-term average AoI\nminimization problem that jointly optimizes the UAV's flight trajectory and\nbeamforming. To tackle the high-dimensional, non-convexity of this problem, we\ndevelop a deep reinforcement learning (DRL)-based algorithm capable of\nproviding real-time decisions on UAV movement and beamforming for both radar\nsensing and multi-user communication. Specifically, a Kalman filter is employed\nfor accurate target state prediction, regularized zero-forcing is utilized to\nmitigate inter-user interference, and the Soft Actor-Critic algorithm is\napplied for training the DRL agent on continuous actions. The proposed\nframework adaptively balances the trade-offs between sensing accuracy and\ncommunication quality. Extensive simulation results demonstrate that our\nproposed method consistently achieves lower average AoI compared to baseline\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff08AoI\uff09\u7684\u65e0\u4eba\u673a-ISAC\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8f68\u8ff9\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u5e73\u8861\u611f\u77e5\u4e0e\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5177\u6709\u7075\u6d3b\u6027\uff0c\u4f46\u8d44\u6e90\u53d7\u9650\u4e0b\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u3001\u901a\u4fe1\u548c\u611f\u77e5\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u4e0e\u6ce2\u675f\u6210\u5f62\uff0c\u5e73\u8861\u611f\u77e5\u7cbe\u5ea6\u4e0e\u901a\u4fe1\u8d28\u91cf\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e73\u5747AoI\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684AoI\u4e2d\u5fc3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a-ISAC\u7cfb\u7edf\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2507.14528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14528", "abs": "https://arxiv.org/abs/2507.14528", "authors": ["Ilias Tsoumas", "Dimitrios Bormpoudakis", "Vasileios Sitokonstantinou", "Athanasios Askitopoulos", "Andreas Kalogeras", "Charalampos Kontoes", "Ioannis Athanasiadis"], "title": "Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference", "comment": "Accepted at KDD 2025 Workshop on Causal Inference and Machine\n  Learning in Practice", "summary": "In causal inference, whether through randomized controlled trials or\nobservational studies, access to both treated and control units is essential\nfor estimating the effect of a treatment on an outcome of interest. When\ntreatment assignment is random, the average treatment effect (ATE) can be\nestimated directly by comparing outcomes between groups. In non-randomized\nsettings, various techniques are employed to adjust for confounding and\napproximate the counterfactual scenario to recover an unbiased ATE. A common\nchallenge, especially in observational studies, is the absence of units clearly\nlabeled as controls-that is, units known not to have received the treatment. To\naddress this, we propose positive-unlabeled (PU) learning as a framework for\nidentifying, with high confidence, control units from a pool of unlabeled ones,\nusing only the available treated (positive) units. We evaluate this approach\nusing both simulated and real-world data. We construct a causal graph with\ndiverse relationships and use it to generate synthetic data under various\nscenarios, assessing how reliably the method recovers control groups that allow\nestimates of true ATE. We also apply our approach to real-world data on optimal\nsowing and fertilizer treatments in sustainable agriculture. Our findings show\nthat PU learning can successfully identify control (negative) units from\nunlabeled data based only on treated units and, through the resulting control\ngroup, estimate an ATE that closely approximates the true value. This work has\nimportant implications for observational causal inference, especially in fields\nwhere randomized experiments are difficult or costly. In domains such as earth,\nenvironmental, and agricultural sciences, it enables a plethora of\nquasi-experiments by leveraging available earth observation and climate data,\nparticularly when treated units are available but control units are lacking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6b63\u672a\u6807\u8bb0\uff08PU\uff09\u5b66\u4e60\u6846\u67b6\uff0c\u4ece\u65e0\u6807\u8bb0\u6570\u636e\u4e2d\u9ad8\u7f6e\u4fe1\u5ea6\u8bc6\u522b\u63a7\u5236\u5355\u5143\uff0c\u4ee5\u89e3\u51b3\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\u7f3a\u4e4f\u660e\u786e\u63a7\u5236\u7ec4\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\uff0c\u7f3a\u4e4f\u660e\u786e\u6807\u8bb0\u7684\u63a7\u5236\u5355\u5143\u662f\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528PU\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ec5\u57fa\u4e8e\u5904\u7406\u5355\u5143\u4ece\u65e0\u6807\u8bb0\u6570\u636e\u4e2d\u8bc6\u522b\u63a7\u5236\u5355\u5143\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u8bc4\u4f30\u5176\u53ef\u9760\u6027\u3002", "result": "PU\u5b66\u4e60\u80fd\u6210\u529f\u8bc6\u522b\u63a7\u5236\u5355\u5143\uff0c\u5e76\u4f30\u8ba1\u51fa\u63a5\u8fd1\u771f\u5b9e\u503c\u7684ATE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c2\u5bdf\u6027\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u96be\u4ee5\u8fdb\u884c\u968f\u673a\u5b9e\u9a8c\u7684\u9886\u57df\u3002"}}
{"id": "2507.14309", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14309", "abs": "https://arxiv.org/abs/2507.14309", "authors": ["Mert Torun", "Alireza Parsay", "Yasamin Mostofi"], "title": "Fast and Robust Stationary Crowd Counting with Commodity WiFi", "comment": null, "summary": "This paper introduces a novel method for estimating the size of seated crowds\nwith commodity WiFi signals, by leveraging natural body fidgeting behaviors as\na passive sensing cue. Departing from prior binary fidget representations, our\napproach leverages the bandwidth of the received signal as a finer-grained and\nrobust indicator of crowd counts. More specifically, we propose a mathematical\nmodel that relates the probability density function (PDF) of the signal\nbandwidth to the crowd size, using a principled derivation based on the PDF of\nan individual's fidget-induced bandwidth. To characterize the individual\nfidgeting PDF, we use publicly available online videos, each of a seated\nindividual, from which we extract body motion profiles using vision techniques,\nfollowed by a speed-to-bandwidth conversion inspired by Carson's Rule from\nanalog FM radio design. Finally, to enhance robustness in real-world\ndeployments where unrelated motions may occur nearby, we further introduce an\nanomaly detection module that filters out non-fidget movements. We validate our\nsystem through 42 experiments across two indoor environments with crowd sizes\nup to and including 13 people, achieving a mean absolute error of 1.04 and a\nnormalized mean square error of 0.15, with an average convergence time of 51\nseconds, significantly reducing the convergence time as compared to the state\nof the art. Additional simulation results demonstrate scalability to larger\ncrowd sizes. Overall, our results show that our pipeline enables fast, robust,\nand highly accurate counting of seated crowds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528WiFi\u4fe1\u53f7\u901a\u8fc7\u4eba\u4f53\u81ea\u7136\u6643\u52a8\u884c\u4e3a\u4f30\u8ba1\u5750\u59ff\u4eba\u7fa4\u89c4\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u53f7\u5e26\u5bbd\u4f5c\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u6307\u6807\uff0c\u7ed3\u5408\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u4eba\u7fa4\u89c4\u6a21\u7684\u4f30\u8ba1\u901a\u5e38\u57fa\u4e8e\u4e8c\u5143\u7684\u6643\u52a8\u8868\u793a\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u672c\u6587\u65e8\u5728\u5229\u7528WiFi\u4fe1\u53f7\u5e26\u5bbd\u4f5c\u4e3a\u66f4\u7cbe\u786e\u7684\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u89e3\u51b3\u65e0\u5173\u8fd0\u52a8\u7684\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u6570\u5b66\u6a21\u578b\uff0c\u5c06\u4fe1\u53f7\u5e26\u5bbd\u7684\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\uff08PDF\uff09\u4e0e\u4eba\u7fa4\u89c4\u6a21\u5173\u8054\uff0c\u901a\u8fc7\u516c\u5f00\u89c6\u9891\u63d0\u53d6\u4e2a\u4f53\u6643\u52a8PDF\uff0c\u5e76\u5f15\u5165\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u8fc7\u6ee4\u65e0\u5173\u8fd0\u52a8\u3002", "result": "\u572842\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.04\uff0c\u5f52\u4e00\u5316\u5747\u65b9\u8bef\u5dee\u4e3a0.15\uff0c\u6536\u655b\u65f6\u95f4\u5e73\u574751\u79d2\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u3001\u9c81\u68d2\u4e14\u9ad8\u7cbe\u5ea6\u5730\u4f30\u8ba1\u5750\u59ff\u4eba\u7fa4\u89c4\u6a21\uff0c\u5e76\u5177\u5907\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u4eba\u7fa4\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14529", "categories": ["cs.LG", "math.OC", "91A16, 68T05, 49N45, 93E20, 46E22"], "pdf": "https://arxiv.org/pdf/2507.14529", "abs": "https://arxiv.org/abs/2507.14529", "authors": ["Berkay Anahtarci", "Can Deha Kariksiz", "Naci Saldi"], "title": "Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games", "comment": null, "summary": "We consider the maximum causal entropy inverse reinforcement learning problem\nfor infinite-horizon stationary mean-field games, in which we model the unknown\nreward function within a reproducing kernel Hilbert space. This allows the\ninference of rich and potentially nonlinear reward structures directly from\nexpert demonstrations, in contrast to most existing inverse reinforcement\nlearning approaches for mean-field games that typically restrict the reward\nfunction to a linear combination of a fixed finite set of basis functions. We\nalso focus on the infinite-horizon cost structure, whereas prior studies\nprimarily rely on finite-horizon formulations. We introduce a Lagrangian\nrelaxation to this maximum causal entropy inverse reinforcement learning\nproblem that enables us to reformulate it as an unconstrained log-likelihood\nmaximization problem, and obtain a solution \\lk{via} a gradient ascent\nalgorithm. To illustrate the theoretical consistency of the algorithm, we\nestablish the smoothness of the log-likelihood objective by proving the\nFr\\'echet differentiability of the related soft Bellman operators with respect\nto the parameters in the reproducing kernel Hilbert space. We demonstrate the\neffectiveness of our method on a mean-field traffic routing game, where it\naccurately recovers expert behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u56e0\u679c\u71b5\u9006\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u9650\u65f6\u57df\u5e73\u7a33\u5e73\u5747\u573a\u535a\u5f08\uff0c\u5229\u7528\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5efa\u6a21\u672a\u77e5\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u6c42\u89e3\u3002", "motivation": "\u73b0\u6709\u5e73\u5747\u573a\u535a\u5f08\u7684\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u9650\u5236\u5956\u52b1\u51fd\u6570\u4e3a\u56fa\u5b9a\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u4e14\u591a\u57fa\u4e8e\u6709\u9650\u65f6\u57df\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u76f4\u63a5\u63a8\u65ad\u975e\u7ebf\u6027\u5956\u52b1\u7ed3\u6784\u3002", "method": "\u5f15\u5165\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u5bf9\u6570\u4f3c\u7136\u6700\u5927\u5316\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u6c42\u89e3\u3002\u8bc1\u660e\u4e86\u8f6f\u8d1d\u5c14\u66fc\u7b97\u5b50\u5728\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u53c2\u6570\u4e0b\u7684Fr\u00e9chet\u53ef\u5fae\u6027\u3002", "result": "\u5728\u5e73\u5747\u573a\u4ea4\u901a\u8def\u7531\u535a\u5f08\u4e2d\uff0c\u65b9\u6cd5\u80fd\u51c6\u786e\u6062\u590d\u4e13\u5bb6\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9650\u65f6\u57df\u5e73\u5747\u573a\u535a\u5f08\u4e2d\u6709\u6548\u63a8\u65ad\u975e\u7ebf\u6027\u5956\u52b1\u51fd\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u7ebf\u6027\u65b9\u6cd5\u3002"}}
{"id": "2507.14310", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14310", "abs": "https://arxiv.org/abs/2507.14310", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "title": "Optimizing Network Performance and Resource Allocation in HAPS-UAV Integrated Sensing and Communication Systems for 6G", "comment": null, "summary": "This paper proposes an innovative approach by leveraging uncrewed aerial\nvehicles (UAVs) as base stations (BSs) and a high-altitude platform station\n(HAPS) as the central processing unit (CPU) in an integrated sensing and\ncommunication (ISAC) system for 6G networks. We explore the challenges,\napplications, and advantages of ISAC systems in next-generation networks,\nhighlighting the significance of optimizing position and power control. Our\napproach integrates HAPS and UAVs to enhance wireless coverage, particularly in\nremote areas. UAVs function as dual-purpose access points (APs), using their\nmaneuverability and line-of-sight (LoS) aerial-to-ground (A2G) links to\ntransmit combined communication and sensing signals. The scheme operates in two\ntime slots: in the first slot, UAVs transmit dedicated signals to communication\nusers (CUs) and potential targets. UAVs detect targets in specific ground\nlocations and, after signal transmission, receive reflected signals from\ntargets. In the second slot, UAVs relay these signals to HAPS, which performs\nbeamforming to align signals for each CU from various UAVs. UAVs decode\ninformation from HAPS and adjust transmissions to maximize the beam pattern\nefficiency toward the desired targets. We formulate a multi-objective\noptimization problem to maximize both the minimum\nsignal-to-interference-plus-noise ratio (SINR) for CUs and the echo signal\npower from the targets. This is achieved by finding the optimal power\nallocation for CUs in each UAV, subject to constraints on the maximum total\npower in each UAV and the transmitted beam pattern gain. Simulation results\ndemonstrate the effectiveness of this approach in enhancing network\nperformance, resource allocation, fairness, and system optimization. Using HAPS\nas the CPU, computational tasks are offloaded from UAVs, which conserves energy\nand improves network performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u65e0\u4eba\u673a\uff08UAV\uff09\u4f5c\u4e3a\u57fa\u7ad9\uff08BS\uff09\u548c\u9ad8\u7a7a\u5e73\u53f0\u7ad9\uff08HAPS\uff09\u4f5c\u4e3a\u4e2d\u592e\u5904\u7406\u5355\u5143\uff08CPU\uff09\u76846G\u7f51\u7edc\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u4f18\u5316\u4f4d\u7f6e\u548c\u529f\u7387\u63a7\u5236\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u63a2\u7d226G\u7f51\u7edc\u4e2dISAC\u7cfb\u7edf\u7684\u6311\u6218\u3001\u5e94\u7528\u548c\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u504f\u8fdc\u5730\u533a\u589e\u5f3a\u65e0\u7ebf\u8986\u76d6\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7HAPS\u548cUAV\u7684\u534f\u540c\u5de5\u4f5c\uff0cUAV\u4f5c\u4e3a\u53cc\u529f\u80fd\u63a5\u5165\u70b9\uff0c\u5206\u65f6\u4f20\u8f93\u901a\u4fe1\u548c\u611f\u77e5\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7HAPS\u8fdb\u884c\u4fe1\u53f7\u5bf9\u9f50\u548c\u4f18\u5316\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3001\u8d44\u6e90\u5206\u914d\u516c\u5e73\u6027\u548c\u7cfb\u7edf\u4f18\u5316\u3002", "conclusion": "\u5229\u7528HAPS\u4f5c\u4e3aCPU\u51cf\u8f7bUAV\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u63d0\u9ad8\u80fd\u6548\u548c\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2507.14560", "categories": ["cs.LG", "cs.CV", "68T07, 05C50, 15A18", "I.2.6; I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.14560", "abs": "https://arxiv.org/abs/2507.14560", "authors": ["Giorgio Roffo"], "title": "The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers", "comment": "24 pages, 10 figures, submitted for review. Companion code and\n  reproducibility materials available", "summary": "The self-attention mechanism, now central to deep learning architectures such\nas Transformers, is a modern instance of a more general computational\nprinciple: learning and using pairwise affinity matrices to control how\ninformation flows through a model. This paper traces the conceptual origins of\nself-attention across multiple domains, including computer vision, natural\nlanguage processing, and graph learning, through their shared reliance on an\naffinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)\nas a foundational approach that generalizes the idea of affinity-based\nweighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS\ndefines A either through domain knowledge or by learning, and computes feature\nrelevance through multi-hop propagation over the affinity graph. From this\nperspective, self-attention can be seen as a special case of Inf-FS: it uses a\nsingle-hop affinity computation where A is dynamically built from token\nsimilarities. We argue that the underlying structure, reasoning over pairwise\nrelationships, is preserved across both approaches, and the key differences lie\nin how the affinity matrix is defined and applied. By situating self-attention\nwithin the broader paradigm of affinity-based computation, we unify several\nstrands of machine learning research and highlight a common mathematical\nfoundation that underpins diverse models and tasks.", "AI": {"tldr": "\u8bba\u6587\u5c06\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7f6e\u4e8e\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8e\u4eb2\u548c\u529b\u7684\u8ba1\u7b97\u8303\u5f0f\u4e2d\uff0c\u8ffd\u6eaf\u5176\u6982\u5ff5\u8d77\u6e90\uff0c\u5e76\u5f3a\u8c03\u65e0\u9650\u7279\u5f81\u9009\u62e9\uff08Inf-FS\uff09\u4f5c\u4e3a\u5176\u57fa\u7840\u65b9\u6cd5\u3002", "motivation": "\u63ed\u793a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u66f4\u5e7f\u6cdb\u7684\u4eb2\u548c\u77e9\u9635\u8ba1\u7b97\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u7edf\u4e00\u4e0d\u540c\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0eInf-FS\uff0c\u5206\u6790\u4eb2\u548c\u77e9\u9635\u7684\u5b9a\u4e49\u548c\u5e94\u7528\u65b9\u5f0f\u3002", "result": "\u81ea\u6ce8\u610f\u529b\u662fInf-FS\u7684\u7279\u4f8b\uff0c\u4e24\u8005\u5171\u4eab\u57fa\u4e8e\u6210\u5bf9\u5173\u7cfb\u7684\u63a8\u7406\u7ed3\u6784\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u4eb2\u548c\u77e9\u9635\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u4e3a\u591a\u6837\u5316\u7684\u6a21\u578b\u548c\u4efb\u52a1\u63d0\u4f9b\u4e86\u5171\u540c\u7684\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2507.14469", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14469", "abs": "https://arxiv.org/abs/2507.14469", "authors": ["Shuxian Wu", "Shun Yao", "Xingyu Du", "Chin-Yu Chang", "Roy H. Olsson III"], "title": "Spatially tailored spin wave excitation for spurious-free, low-loss magnetostatic wave filters with ultra-wide frequency tunability", "comment": null, "summary": "Yttrium iron garnet magnetostatic wave (MSW) radio frequency (RF) cavity\nfilters are promising for sixth-generation (6G) communication systems due to\ntheir wide frequency tunability. However, the presence of severe spurious modes\narising from the finite cavity dimensions severely degrades the filter\nperformance. We present a half-cone transducer that spatially tailors spin wave\nexcitation to selectively enhance the primary cavity modes comprising the MSW\nfilter passband, while strongly suppressing the undesired spurious modes.\nTheoretical analysis, numerical simulations and experiments verify the\neffectiveness of the spatially tailored technique. We utilize the half-cone\ntransducer to demonstrate a spurious-free, single-cavity half-cone MSW filter\n(HC-MSWF) with an insertion loss (IL) of 2.4-3.2 dB over a frequency tuning\nrange of 6.3-16.8 GHz. Extending our study, we further demonstrate a\nspurious-free, dual-cavity HC-MSWF with an unprecedented tuning range of 21.7\nGHz (9.8-31.5 GHz) while maintaining a low IL of 2.9-3.8 dB. This significant\nadvance in performance will enable highly reconfigurable and robust 6G\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u9525\u5f62\u6362\u80fd\u5668\uff0c\u7528\u4e8e\u4f18\u5316\u9487\u94c1\u77f3\u69b4\u77f3\u78c1\u9759\u6ce2\u5c04\u9891\u8154\u6ee4\u6ce2\u5668\uff0c\u663e\u8457\u6291\u5236\u6742\u6563\u6a21\u5f0f\uff0c\u63d0\u53476G\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u9487\u94c1\u77f3\u69b4\u77f3\u78c1\u9759\u6ce2\u5c04\u9891\u8154\u6ee4\u6ce2\u5668\u57286G\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\uff0c\u4f46\u5176\u6742\u6563\u6a21\u5f0f\u4e25\u91cd\u5f71\u54cd\u4e86\u6ee4\u6ce2\u5668\u6027\u80fd\u3002", "method": "\u91c7\u7528\u534a\u9525\u5f62\u6362\u80fd\u5668\uff0c\u901a\u8fc7\u7a7a\u95f4\u5b9a\u5236\u81ea\u65cb\u6ce2\u6fc0\u53d1\uff0c\u9009\u62e9\u6027\u589e\u5f3a\u4e3b\u8154\u6a21\u5f0f\u5e76\u6291\u5236\u6742\u6563\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5355\u8154\u548c\u53cc\u8154\u534a\u9525\u5f62\u78c1\u9759\u6ce2\u6ee4\u6ce2\u5668\uff0c\u5206\u522b\u5b9e\u73b0\u4e866.3-16.8 GHz\u548c9.8-31.5 GHz\u7684\u65e0\u6742\u6563\u8c03\u8c10\u8303\u56f4\uff0c\u63d2\u5165\u635f\u8017\u4f4e\u81f32.4-3.8 dB\u3002", "conclusion": "\u8be5\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u6ee4\u6ce2\u5668\u7684\u6027\u80fd\uff0c\u4e3a6G\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u5ea6\u53ef\u91cd\u6784\u548c\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14570", "abs": "https://arxiv.org/abs/2507.14570", "authors": ["Xu Cheng", "Liang Yao", "Feng He", "Yukuo Cen", "Yufei He", "Chenhui Zhang", "Wenzheng Feng", "Hongyun Cai", "Jie Tang"], "title": "LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for various graph\nmining tasks, yet existing scalable solutions often struggle to balance\nexecution efficiency with prediction accuracy. These difficulties stem from\niterative message-passing techniques, which place significant computational\ndemands and require extensive GPU memory, particularly when dealing with the\nneighbor explosion issue inherent in large-scale graphs. This paper introduces\na scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,\nwhich can perform representation learning on 100 billion graphs with a single\nGPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We\nexamine existing graph partitioning methods and design a superior graph\npartition algorithm named LPMetis. In particular, LPMetis outperforms current\nstate-of-the-art (SOTA) approaches on various evaluation metrics. In addition,\nour paper proposes a subgraph augmentation strategy to enhance the model's\npredictive performance. It exhibits excellent compatibility, allowing the\nentire framework to accommodate various GNN algorithms. Successfully deployed\non the Tencent platform, LPS-GNN has been tested on public and real-world\ndatasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in\nonline applications.", "AI": {"tldr": "LPS-GNN\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684GNN\u6846\u67b6\uff0c\u80fd\u5728\u5355GPU\u4e0a\u5904\u74061000\u4ebf\u89c4\u6a21\u7684\u56fe\u6570\u636e\uff0c\u5e76\u5728\u7528\u6237\u83b7\u53d6\u573a\u666f\u4e2d\u63d0\u534713.8%\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u95ee\u9898\u3002", "method": "\u63d0\u51faLPS-GNN\u6846\u67b6\uff0c\u7ed3\u5408LPMetis\u56fe\u5206\u533a\u7b97\u6cd5\u548c\u5b50\u56fe\u589e\u5f3a\u7b56\u7565\uff0c\u517c\u5bb9\u591a\u79cdGNN\u7b97\u6cd5\u3002", "result": "\u5728Tencent\u5e73\u53f0\u4e0a\u6d4b\u8bd5\uff0c\u6027\u80fd\u63d0\u53478.24%\u81f313.89%\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u3002", "conclusion": "LPS-GNN\u4e3a\u5927\u89c4\u6a21\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14622", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14622", "abs": "https://arxiv.org/abs/2507.14622", "authors": ["Wahab Khawaja", "Ismail Guvenc", "Rune Hylsberg Jacobsen"], "title": "Propagation Channel Modeling for LEO Satellite Missions Using Ray-Tracing Simulations", "comment": "This manuscript is submitted to MILCOM 2025 conference", "summary": "This work presents a high-resolution, ray-tracing-based channel modeling for\nLow Earth Orbit (LEO) satellite-to-ground links in a suburban environment at\nX-band. Using simulations conducted in Wireless InSite, we develop a parametric\nchannel model that characterizes both large- and small-scale fading effects\nacross different satellite elevation angles. Large-scale fading incorporates\nattenuation due to terrain-induced shadowing and dynamic environmental factors\nsuch as weather conditions, and is compared with 3GPP NTN channel model.\nAdditionally, we quantify link degradation resulting from ground station (GS)\nantenna misalignment, considering both fixed single-element and electronically\nsteerable phased-array antennas. Small-scale fading is modeled by fitting a\nshadowed and non-shadowed Rician distribution to the fading statistics at\nvarious satellite elevations. To the best of our knowledge, this is the first\nstudy to propose a comprehensive elevation-aware channel model for\nsatellite-to-ground propagation at X-band, integrating ray-traced environmental\ndynamics, elevation-dependent fading, and phased-array beam misalignment\neffects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c04\u7ebf\u8ffd\u8e2a\u7684\u9ad8\u5206\u8fa8\u7387LEO\u536b\u661f\u5230\u5730\u9762\u94fe\u8def\u7684X\u6ce2\u6bb5\u4fe1\u9053\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u5927\u5c3a\u5ea6\u548c\u5c0f\u5c3a\u5ea6\u8870\u843d\u6548\u5e94\uff0c\u5e76\u9996\u6b21\u6574\u5408\u4e86\u5c04\u7ebf\u8ffd\u8e2a\u73af\u5883\u52a8\u6001\u3001\u4ef0\u89d2\u76f8\u5173\u8870\u843d\u548c\u76f8\u63a7\u9635\u6ce2\u675f\u5931\u51c6\u6548\u5e94\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u5728\u90ca\u533a\u73af\u5883\u4e2d\u4e3aLEO\u536b\u661f\u5230\u5730\u9762\u94fe\u8def\u5f00\u53d1\u4e00\u4e2a\u5168\u9762\u7684X\u6ce2\u6bb5\u4fe1\u9053\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u5927\u5c3a\u5ea6\u548c\u5c0f\u5c3a\u5ea6\u8870\u843d\u6548\u5e94\uff0c\u5e76\u91cf\u5316\u5730\u9762\u7ad9\u5929\u7ebf\u5931\u51c6\u5bf9\u94fe\u8def\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528Wireless InSite\u8fdb\u884c\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\uff0c\u5f00\u53d1\u53c2\u6570\u5316\u4fe1\u9053\u6a21\u578b\uff0c\u6bd4\u8f833GPP NTN\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u62df\u5408\u9634\u5f71\u548c\u975e\u9634\u5f71Rician\u5206\u5e03\u6765\u5efa\u6a21\u5c0f\u5c3a\u5ea6\u8870\u843d\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u6210\u529f\u8868\u5f81\u4e86\u4e0d\u540c\u536b\u661f\u4ef0\u89d2\u4e0b\u7684\u8870\u843d\u6548\u5e94\uff0c\u5e76\u91cf\u5316\u4e86\u5929\u7ebf\u5931\u51c6\u5bfc\u81f4\u7684\u94fe\u8def\u9000\u5316\u3002", "conclusion": "\u7ed3\u8bba\u662f\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u4ef0\u89d2\u611f\u77e5\u4fe1\u9053\u6a21\u578b\uff0c\u4e3aX\u6ce2\u6bb5\u536b\u661f\u5230\u5730\u9762\u4f20\u64ad\u63d0\u4f9b\u4e86\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2507.14592", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14592", "abs": "https://arxiv.org/abs/2507.14592", "authors": ["Haochen Liu", "Jia Bi", "Xiaomin Wang", "Xin Yang", "Ling Wang"], "title": "A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification", "comment": "13 pages, 7 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,\nlogistics, agriculture, disaster management, and military operations. Accurate\ndetection and classification of UAV flight states, such as hovering, cruising,\nascending, or transitioning, which are essential for safe and effective\noperations. However, conventional time series classification (TSC) methods\noften lack robustness and generalization for dynamic UAV environments, while\nstate of the art(SOTA) models like Transformers and LSTM based architectures\ntypically require large datasets and entail high computational costs,\nespecially with high-dimensional data streams. This paper proposes a novel\nframework that integrates a Transformer-based Generative Adversarial Network\n(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address\nthese challenges in UAV flight state classification. The Transformer encoder\ncaptures long-range temporal dependencies and complex telemetry dynamics, while\nthe GAN module augments limited datasets with realistic synthetic samples. MIL\nis incorporated to focus attention on the most discriminative input segments,\nreducing noise and computational overhead. Experimental results show that the\nproposed method achieves superior accuracy 96.5% on the DroneDetect dataset and\n98.6% on the DroneRF dataset that outperforming other SOTA approaches. The\nframework also demonstrates strong computational efficiency and robust\ngeneralization across diverse UAV platforms and flight states, highlighting its\npotential for real-time deployment in resource constrained environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer-GAN\u548cMILET\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u98de\u884c\u72b6\u6001\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u65e0\u4eba\u673a\u98de\u884c\u72b6\u6001\u7684\u51c6\u786e\u5206\u7c7b\u5bf9\u5b89\u5168\u548c\u9ad8\u6548\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u96c6\u6210Transformer-GAN\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u7ed3\u5408MILET\u805a\u7126\u5173\u952e\u8f93\u5165\u6bb5\uff0c\u51cf\u5c11\u566a\u58f0\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728DroneDetect\u548cDroneRF\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523096.5%\u548c98.6%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2507.14804", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14804", "abs": "https://arxiv.org/abs/2507.14804", "authors": ["Jingjing Zhao", "Qian Xu", "Kaiquan Cai", "Yanbo Zhu", "Xidong Mu", "Yuanwei Liu"], "title": "Movable-Element STARS-Aided Secure Communications", "comment": null, "summary": "A novel movable-element (ME) enabled simultaneously transmitting and\nreflecting surface (ME-STARS)-aided secure communication system is\ninvestigated. Against the full-space eavesdropping, MEs are deployed at the\nSTARS for enhancing the physical layer security by exploiting higher spatial\ndegrees of freedom. Specifically, a sum secrecy rate maximization problem is\nformulated, which jointly optimizes the passive beamforming and the MEs\npositions at the ME-STARS, as well as the active beamforming at the base\nstation. To solve the resultant non-convex optimization problem involving\nhighly-coupled variables, an alternating optimization-based iterative algorithm\nis developed, decomposing the original problem into three subproblems. In\nparticular, for the MEs position optimization subproblem, a gradient ascent\nalgorithm is employed to iteratively refine the MEs' locations within the\nconfined region. Moreover, the the active and passive beamforming subproblems\nare solved by employing successive convex approximation. Numerical results\nunveil that: 1) ME-STARS significantly improves the secrecy performance\ncompared to the conventional STARS with fixed-position elements; and 2) The\nsecrecy rate achieved by the ME-STARS gets saturated within limited movable\nregion size.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u79fb\u52a8\u5143\u4ef6\uff08ME\uff09\u7684STARS\u7cfb\u7edf\uff08ME-STARS\uff09\uff0c\u7528\u4e8e\u589e\u5f3a\u7269\u7406\u5c42\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\u3001ME\u4f4d\u7f6e\u548c\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\u6765\u6700\u5927\u5316\u4fdd\u5bc6\u901f\u7387\u3002", "motivation": "\u9488\u5bf9\u5168\u7a7a\u95f4\u7a83\u542c\u95ee\u9898\uff0c\u901a\u8fc7\u90e8\u7f72\u53ef\u79fb\u52a8\u5143\u4ef6\uff08ME\uff09\u4ee5\u5229\u7528\u66f4\u9ad8\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\u6765\u589e\u5f3a\u901a\u4fe1\u7cfb\u7edf\u7684\u7269\u7406\u5c42\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u5206\u89e3\u4e3a\u4e09\u4e2a\u5b50\u95ee\u9898\uff1aME\u4f4d\u7f6e\u4f18\u5316\uff08\u4f7f\u7528\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\uff09\u3001\u88ab\u52a8\u6ce2\u675f\u6210\u5f62\u548c\u4e3b\u52a8\u6ce2\u675f\u6210\u5f62\uff08\u4f7f\u7528\u9010\u6b21\u51f8\u8fd1\u4f3c\uff09\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cME-STARS\u76f8\u6bd4\u56fa\u5b9a\u5143\u4ef6STARS\u663e\u8457\u63d0\u5347\u4e86\u4fdd\u5bc6\u6027\u80fd\uff0c\u4f46\u4fdd\u5bc6\u901f\u7387\u5728\u6709\u9650\u79fb\u52a8\u533a\u57df\u5185\u8d8b\u4e8e\u9971\u548c\u3002", "conclusion": "ME-STARS\u901a\u8fc7\u4f18\u5316\u53ef\u79fb\u52a8\u5143\u4ef6\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u6709\u6548\u63d0\u5347\u4e86\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u4f46\u9700\u6ce8\u610f\u79fb\u52a8\u533a\u57df\u7684\u9650\u5236\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.14631", "categories": ["cs.LG", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14631", "abs": "https://arxiv.org/abs/2507.14631", "authors": ["Daniel Greenhut", "Dan Feldman"], "title": "$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation", "comment": null, "summary": "Given an integer $k\\geq1$ and a set $P$ of $n$ points in $\\REAL^d$, the\nclassic $k$-PCA (Principle Component Analysis) approximates the affine\n\\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear\nsubspace that minimizes its sum of squared Euclidean distances\n($\\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.\nThe \\emph{$k$-subspace median} is the subspace that minimizes its sum of\n(non-squared) Euclidean distances ($\\ell_{2,1}$-mixed norm), i.e., their\nmedian. The median subspace is usually more sparse and robust to noise/outliers\nthan the mean, but also much harder to approximate since, unlike the\n$\\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.\n  We provide the first polynomial-time deterministic algorithm whose both\nrunning time and approximation factor are not exponential in $k$. More\nprecisely, the multiplicative approximation factor is $\\sqrt{d}$, and the\nrunning time is polynomial in the size of the input. We expect that our\ntechnique would be useful for many other related problems, such as $\\ell_{2,z}$\nnorm of distances for $z\\not \\in \\br{1,2}$, e.g., $z=\\infty$, and handling\noutliers/sparsity.\n  Open code and experimental results on real-world datasets are also provided.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3c\u8ba1\u7b97k-\u5b50\u7a7a\u95f4\u4e2d\u4f4d\u6570\uff0c\u5176\u8fd0\u884c\u65f6\u95f4\u548c\u8fd1\u4f3c\u56e0\u5b50\u5747\u4e0d\u968fk\u6307\u6570\u589e\u957f\u3002", "motivation": "k-\u5b50\u7a7a\u95f4\u4e2d\u4f4d\u6570\u6bd4\u5747\u503c\u66f4\u5177\u9c81\u68d2\u6027\u548c\u7a00\u758f\u6027\uff0c\u4f46\u7531\u4e8e\u5176\u975e\u51f8\u6027\uff0c\u96be\u4ee5\u9ad8\u6548\u8fd1\u4f3c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u5176\u8fd1\u4f3c\u56e0\u5b50\u4e3a\u221ad\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a\u591a\u9879\u5f0f\u65f6\u95f4\u3002", "result": "\u7b97\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6280\u672f\u6709\u671b\u5e94\u7528\u4e8e\u5176\u4ed6\u76f8\u5173\u95ee\u9898\uff0c\u5982\u5904\u7406\u21132,z\u8303\u6570\u6216\u5f02\u5e38\u503c/\u7a00\u758f\u6027\u3002"}}
{"id": "2507.14831", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.14831", "abs": "https://arxiv.org/abs/2507.14831", "authors": ["Mengyu Qian", "Xidong Mu", "Li You", "Michail Matthaiou"], "title": "Pinching-Antenna-based Communications: Spectral Efficiency Analysis and Deployment Strategies", "comment": "13 pages, 8 figures", "summary": "A multiple-waveguide pinching-antenna (PA)-based multi-user communication\nsystem is investigated. With a given number of PAs, two deployment strategies\nare considered, namely the centralized PA deployment, where all PAs are\nswitched between waveguides to serve users in a time-division manner to avail\nof beamforming gain, and the distributed PA deployment, where a single PA is\ndeployed on each waveguide to simultaneously serve multiple users by leveraging\nthe multiplexing gain. The spectral efficiency (SE) achieved by each deployment\nstrategy is analyzed: i) For the centralized deployment, the positioning\nstrategy of PAs on each waveguide is determined first with the aim of\nmaximizing the channel gain of the corresponding nearest served user. Based on\nthis, the corresponding system SE is derived. ii) For the distributed\ndeployment, the system SE under the maximum ratio transmission (MRT) is first\nobtained. To obtain an analytically tractable form, the stationary phase method\nis utilized to approximate the system SE. The approximation result reveals that\nthe average inter-user interference can be negligible with a large waveguide\nspacing and thus the simple MRT is appealing for PA-based multi-user\ncommunications. Furthermore, the system SEs achieved by the two strategies are\ncompared in both the high and low signal-to-noise ratio (SNR) regimes. Our\nanalysis suggests that at high SNRs, the distributed deployment is superior to\nachieve the maximal system SE, while the centralized deployment is more\nsuitable for the low-SNR regime. Finally, the theoretical analysis is verified\nthrough simulations.", "AI": {"tldr": "\u7814\u7a76\u4e86\u57fa\u4e8e\u591a\u6ce2\u5bfc\u5939\u6301\u5929\u7ebf\uff08PA\uff09\u7684\u591a\u7528\u6237\u901a\u4fe1\u7cfb\u7edf\uff0c\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u90e8\u7f72\u7b56\u7565\u7684\u9891\u8c31\u6548\u7387\uff08SE\uff09\u3002\u96c6\u4e2d\u5f0f\u90e8\u7f72\u901a\u8fc7\u6ce2\u675f\u6210\u5f62\u589e\u76ca\u670d\u52a1\u7528\u6237\uff0c\u800c\u5206\u5e03\u5f0f\u90e8\u7f72\u5229\u7528\u590d\u7528\u589e\u76ca\u540c\u65f6\u670d\u52a1\u591a\u7528\u6237\u3002\u5206\u6790\u8868\u660e\uff0c\u9ad8\u4fe1\u566a\u6bd4\uff08SNR\uff09\u4e0b\u5206\u5e03\u5f0f\u66f4\u4f18\uff0c\u4f4eSNR\u4e0b\u96c6\u4e2d\u5f0f\u66f4\u4f73\u3002", "motivation": "\u63a2\u7d22\u591a\u6ce2\u5bfc\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u5728\u4e0d\u540c\u90e8\u7f72\u7b56\u7565\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u4f18\u5316\u591a\u7528\u6237\u901a\u4fe1\u7684\u9891\u8c31\u6548\u7387\u3002", "method": "\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0fPA\u90e8\u7f72\u7b56\u7565\uff0c\u5206\u6790\u5176\u9891\u8c31\u6548\u7387\uff0c\u5e76\u5229\u7528\u6700\u5927\u6bd4\u4f20\u8f93\uff08MRT\uff09\u548c\u7a33\u6001\u76f8\u4f4d\u65b9\u6cd5\u8fdb\u884c\u8fd1\u4f3c\u5206\u6790\u3002", "result": "\u9ad8SNR\u4e0b\u5206\u5e03\u5f0f\u90e8\u7f72\u66f4\u4f18\uff0c\u4f4eSNR\u4e0b\u96c6\u4e2d\u5f0f\u90e8\u7f72\u66f4\u4f73\u3002MRT\u5728\u6ce2\u5bfc\u95f4\u8ddd\u8f83\u5927\u65f6\u53ef\u5ffd\u7565\u7528\u6237\u95f4\u5e72\u6270\u3002", "conclusion": "\u6839\u636eSNR\u9009\u62e9\u90e8\u7f72\u7b56\u7565\u53ef\u6700\u5927\u5316\u7cfb\u7edf\u9891\u8c31\u6548\u7387\uff0c\u7406\u8bba\u5206\u6790\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u3002"}}
{"id": "2507.14668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14668", "abs": "https://arxiv.org/abs/2507.14668", "authors": ["Yunfeng Li", "Junhong Liu", "Zhaohui Yang", "Guofu Liao", "Chuyun Zhang"], "title": "Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model", "comment": "15 pages, 14 figures", "summary": "Deep learning models have been widely adopted for False Data Injection Attack\n(FDIA) detection in smart grids due to their ability to capture unstructured\nand sparse features. However, the increasing system scale and data\ndimensionality introduce significant computational and memory burdens,\nparticularly in large-scale industrial datasets, limiting detection efficiency.\nTo address these issues, this paper proposes Rec-AD, a computationally\nefficient framework that integrates Tensor Train decomposition with the Deep\nLearning Recommendation Model (DLRM). Rec-AD enhances training and inference\nefficiency through embedding compression, optimized data access via index\nreordering, and a pipeline training mechanism that reduces memory communication\noverhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing\nFDIA detection systems without code modifications. Experimental results show\nthat Rec-AD significantly improves computational throughput and real-time\ndetection performance, narrowing the attack window and increasing attacker\ncost. These advancements strengthen edge computing capabilities and\nscalability, providing robust technical support for smart grid security.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRec-AD\u6846\u67b6\uff0c\u7ed3\u5408Tensor Train\u5206\u89e3\u4e0e\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\uff0c\u63d0\u5347\u667a\u80fd\u7535\u7f51\u4e2d\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u68c0\u6d4b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u4e2d\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u68c0\u6d4b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\u968f\u7cfb\u7edf\u89c4\u6a21\u548c\u6570\u636e\u7ef4\u5ea6\u589e\u52a0\u800c\u663e\u8457\u4e0a\u5347\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u6548\u7387\u3002", "method": "Rec-AD\u901a\u8fc7\u5d4c\u5165\u538b\u7f29\u3001\u7d22\u5f15\u91cd\u6392\u5e8f\u4f18\u5316\u6570\u636e\u8bbf\u95ee\u53ca\u6d41\u6c34\u7ebf\u8bad\u7ec3\u673a\u5236\uff0c\u51cf\u5c11\u5185\u5b58\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u4e0ePyTorch\u5b8c\u5168\u517c\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRec-AD\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u541e\u5410\u91cf\u548c\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\uff0c\u7f29\u5c0f\u653b\u51fb\u7a97\u53e3\u5e76\u589e\u52a0\u653b\u51fb\u6210\u672c\u3002", "conclusion": "Rec-AD\u589e\u5f3a\u4e86\u8fb9\u7f18\u8ba1\u7b97\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u667a\u80fd\u7535\u7f51\u5b89\u5168\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.14856", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14856", "abs": "https://arxiv.org/abs/2507.14856", "authors": ["Victor Shatov", "Steffen Schieler", "Charlotte Muth", "Jos\u00e9 Miguel Mateos-Ramos", "Ivo Bizon", "Florian Euchner", "Sebastian Semper", "Stephan ten Brink", "Gerhard Fettweis", "Christian H\u00e4ger", "Henk Wymeersch", "Laurent Schmalen", "Reiner Thom\u00e4", "Norman Franchi"], "title": "Integrated Radio Sensing Capabilities for 6G Networks: AI/ML Perspective", "comment": "30 pages, 18 figures", "summary": "The sixth-generation wireless communications (6G) is often labeled as\n\"connected intelligence\". Radio sensing, aligned with machine learning (ML) and\nartificial intelligence (AI), promises, among other benefits, breakthroughs in\nthe system's ability to perceive the environment and effectively utilize this\nawareness. This article offers a tutorial-style survey of AI and ML approaches\nto enhance the sensing capabilities of next-generation wireless networks. To\nthis end, while staying in the framework of integrated sensing and\ncommunication (ISAC), we expand the term \"sensing\" from radar, via spectrum\nsensing, to miscellaneous applications of radio sensing like non-cooperative\ntransmitter localization. We formulate the problems, explain the\nstate-of-the-art approaches, and detail AI-based techniques to tackle various\nobjectives in the context of wireless sensing. We discuss the advantages,\nenablers, and challenges of integrating various sensing capabilities into an\nenvisioned AI-powered multimodal multi-task network. In addition to the\ntutorial-style core of this work based on direct authors' involvement in 6G\nresearch problems, we review the related literature, and provide both a good\nstart for those entering this field of research, and a topical overview for a\ngeneral reader with a background in wireless communications", "AI": {"tldr": "\u672c\u6587\u662f\u5173\u4e8e6G\u65e0\u7ebf\u901a\u4fe1\u4e2dAI\u548cML\u5982\u4f55\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u7684\u6559\u7a0b\u5f0f\u7efc\u8ff0\uff0c\u6db5\u76d6\u96f7\u8fbe\u3001\u9891\u8c31\u611f\u77e5\u7b49\u591a\u79cd\u5e94\u7528\u3002", "motivation": "6G\u901a\u4fe1\u88ab\u79f0\u4e3a\u201c\u8fde\u63a5\u667a\u80fd\u201d\uff0c\u7ed3\u5408AI\u548cML\u7684\u65e0\u7ebf\u611f\u77e5\u6280\u672f\u6709\u671b\u63d0\u5347\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u53d1\u5c55\u3002", "method": "\u5728ISAC\u6846\u67b6\u4e0b\uff0c\u6269\u5c55\u201c\u611f\u77e5\u201d\u6982\u5ff5\uff0c\u6db5\u76d6\u96f7\u8fbe\u3001\u9891\u8c31\u611f\u77e5\u7b49\u5e94\u7528\uff0c\u5e76\u8be6\u7ec6\u4ecb\u7ecd\u4e86AI\u6280\u672f\u89e3\u51b3\u65e0\u7ebf\u611f\u77e5\u95ee\u9898\u7684\u524d\u6cbf\u65b9\u6cd5\u3002", "result": "\u8ba8\u8bba\u4e86AI\u9a71\u52a8\u7684\u591a\u6a21\u6001\u591a\u4efb\u52a1\u7f51\u7edc\u4e2d\u96c6\u6210\u611f\u77e5\u80fd\u529b\u7684\u4f18\u52bf\u3001\u63a8\u52a8\u56e0\u7d20\u548c\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u521d\u5b66\u8005\u63d0\u4f9b\u4e86\u7814\u7a76\u8d77\u70b9\uff0c\u4e5f\u4e3a\u65e0\u7ebf\u901a\u4fe1\u80cc\u666f\u7684\u8bfb\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7814\u7a76\u6982\u8ff0\u3002"}}
{"id": "2507.14677", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14677", "abs": "https://arxiv.org/abs/2507.14677", "authors": ["Yiming Xu", "Zhen Peng", "Bin Shi", "Xu Hua", "Bo Dong", "Song Wang", "Chen Chen"], "title": "Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective", "comment": "Accepted by AAAI2025", "summary": "The superiority of graph contrastive learning (GCL) has prompted its\napplication to anomaly detection tasks for more powerful risk warning systems.\nUnfortunately, existing GCL-based models tend to excessively prioritize overall\ndetection performance while neglecting robustness to structural imbalance,\nwhich can be problematic for many real-world networks following power-law\ndegree distributions. Particularly, GCL-based methods may fail to capture tail\nanomalies (abnormal nodes with low degrees). This raises concerns about the\nsecurity and robustness of current anomaly detection algorithms and therefore\nhinders their applicability in a variety of realistic high-risk scenarios. To\nthe best of our knowledge, research on the robustness of graph anomaly\ndetection to structural imbalance has received little scrutiny. To address the\nabove issues, this paper presents a novel GCL-based framework named AD-GCL. It\ndevises the neighbor pruning strategy to filter noisy edges for head nodes and\nfacilitate the detection of genuine tail nodes by aligning from head nodes to\nforged tail nodes. Moreover, AD-GCL actively explores potential neighbors to\nenlarge the receptive field of tail nodes through anomaly-guided neighbor\ncompletion. We further introduce intra- and inter-view consistency loss of the\noriginal and augmentation graph for enhanced representation. The performance\nevaluation of the whole, head, and tail nodes on multiple datasets validates\nthe comprehensive superiority of the proposed AD-GCL in detecting both head\nanomalies and tail anomalies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAD-GCL\u7684\u65b0\u578b\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u4e0d\u5e73\u8861\uff08\u5982\u5e42\u5f8b\u5206\u5e03\u7f51\u7edc\uff09\u4e0b\u5bf9\u5c3e\u90e8\u5f02\u5e38\u68c0\u6d4b\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fc7\u4e8e\u5173\u6ce8\u6574\u4f53\u6027\u80fd\uff0c\u800c\u5ffd\u89c6\u4e86\u5bf9\u7ed3\u6784\u4e0d\u5e73\u8861\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u4f4e\u5ea6\u8282\u70b9\u7684\u5c3e\u90e8\u5f02\u5e38\u68c0\u6d4b\u4e0d\u8db3\u3002", "method": "AD-GCL\u901a\u8fc7\u90bb\u5c45\u4fee\u526a\u7b56\u7565\u8fc7\u6ee4\u566a\u58f0\u8fb9\uff0c\u5e76\u901a\u8fc7\u5f02\u5e38\u5f15\u5bfc\u7684\u90bb\u5c45\u8865\u5168\u6269\u5927\u5c3e\u90e8\u8282\u70b9\u7684\u611f\u77e5\u8303\u56f4\uff0c\u540c\u65f6\u5f15\u5165\u89c6\u56fe\u4e00\u81f4\u6027\u635f\u5931\u589e\u5f3a\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAD-GCL\u5728\u6574\u4f53\u3001\u5934\u90e8\u548c\u5c3e\u90e8\u8282\u70b9\u7684\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "AD-GCL\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u5728\u7ed3\u6784\u4e0d\u5e73\u8861\u7f51\u7edc\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2507.14888", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14888", "abs": "https://arxiv.org/abs/2507.14888", "authors": ["Zhuo Wang"], "title": "Stabilization of the bias point in MZM modulators", "comment": null, "summary": "This article mainly introduces the role of MZM in practical communication\nsystems, the materials used to make MZM modulators such as lithium niobate, and\nits working principle. It also explains why it changes due to environmental\nfactors. This leads to the introduction of a method that controls the stable\npoints of MZM by algorithmically controlling the voltage, and the algorithm is\nverified through experiments. Finally, a summary and outlook on the future\ndevelopment of MZM are provided.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MZM\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u3001\u6750\u6599\uff08\u5982\u94cc\u9178\u9502\uff09\u3001\u5de5\u4f5c\u539f\u7406\u53ca\u5176\u53d7\u73af\u5883\u56e0\u7d20\u5f71\u54cd\u7684\u539f\u56e0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7b97\u6cd5\u63a7\u5236\u7535\u538b\u6765\u7a33\u5b9aMZM\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u3002\u6700\u540e\u603b\u7ed3\u4e86MZM\u7684\u672a\u6765\u53d1\u5c55\u524d\u666f\u3002", "motivation": "\u7814\u7a76MZM\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u53ca\u5176\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u63a7\u5236\u65b9\u6cd5\u4ee5\u89e3\u51b3\u73af\u5883\u56e0\u7d20\u5bf9MZM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7b97\u6cd5\u63a7\u5236\u7535\u538b\u6765\u7a33\u5b9aMZM\u7684\u5de5\u4f5c\u70b9\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u63a7\u5236\u7535\u538b\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u7a33\u5b9aMZM\u7684\u6027\u80fd\u3002", "conclusion": "MZM\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u8fdb\u4e00\u6b65\u4f18\u5316\u7b97\u6cd5\u63d0\u5347\u5176\u7a33\u5b9a\u6027\u548c\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.14679", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14679", "abs": "https://arxiv.org/abs/2507.14679", "authors": ["Zixin Xu", "Zhijie Wang", "Zhiyuan Pan"], "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "comment": null, "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGCC-Spam\u7684\u65b0\u578b\u5783\u573e\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5b57\u7b26\u76f8\u4f3c\u6027\u7f51\u7edc\u3001\u5bf9\u6bd4\u5b66\u4e60\u548cGAN\u751f\u6210\u4f2a\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u5bf9\u6297\u7b56\u7565\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u4e92\u8054\u7f51\u4e0a\u5783\u573e\u6587\u672c\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u4fe1\u606f\u6cc4\u9732\u548c\u793e\u4f1a\u4e0d\u7a33\u5b9a\u7b49\u98ce\u9669\u3002", "method": "\u7ed3\u5408\u5b57\u7b26\u76f8\u4f3c\u6027\u7f51\u7edc\u6355\u6349\u62fc\u5199\u548c\u8bed\u97f3\u7279\u5f81\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\uff0c\u5e76\u901a\u8fc7GAN\u751f\u6210\u4f2a\u6837\u672c\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528\u66f4\u5c11\u7684\u6807\u6ce8\u6837\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7387\u3002", "conclusion": "GCC-Spam\u6846\u67b6\u5728\u5783\u573e\u6587\u672c\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u7b56\u7565\u548c\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.14937", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14937", "abs": "https://arxiv.org/abs/2507.14937", "authors": ["Hugh L Kennedy"], "title": "Phase-optimised linearly-constrained minimum-variance beamformers", "comment": "Initial draft", "summary": "A novel procedure for the determination of the optimal group-delay for a\nLinearly-Constrained Minimum-Variance (LCMV) beamformer is proposed. Two ways\nof selecting the optimal delay are recommended: the first is the solution that\nminimizes the noise power; the second is the solution that minimizes the\nprocessing delay. The potential of this hitherto unexplored degree of design\nfreedom is explored using simulated Very-High-Frequency (VHF) communication,\nand Ultra-High-Frequency (UHF) bistatic radar, applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786e\u5b9a\u7ebf\u6027\u7ea6\u675f\u6700\u5c0f\u65b9\u5dee\uff08LCMV\uff09\u6ce2\u675f\u5f62\u6210\u5668\u6700\u4f18\u7fa4\u5ef6\u8fdf\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u6700\u5c0f\u5316\u566a\u58f0\u529f\u7387\u548c\u5904\u7406\u5ef6\u8fdf\u4e24\u79cd\u9009\u62e9\u3002", "motivation": "\u63a2\u7d22LCMV\u6ce2\u675f\u5f62\u6210\u5668\u4e2d\u672a\u5145\u5206\u5229\u7528\u7684\u8bbe\u8ba1\u81ea\u7531\u5ea6\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6a21\u62dfVHF\u901a\u4fe1\u548cUHF\u53cc\u57fa\u5730\u96f7\u8fbe\u5e94\u7528\uff0c\u8bc4\u4f30\u4e24\u79cd\u6700\u4f18\u5ef6\u8fdf\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u9a8c\u8bc1\u4e86\u4e24\u79cd\u65b9\u6cd5\u5728\u566a\u58f0\u529f\u7387\u548c\u5904\u7406\u5ef6\u8fdf\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLCMV\u6ce2\u675f\u5f62\u6210\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u7ef4\u5ea6\u3002"}}
{"id": "2507.14698", "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14698", "abs": "https://arxiv.org/abs/2507.14698", "authors": ["Xuetao Lin", "Tianhao Peng", "Peihong Dai", "Yu Liang", "Wenjun Wu"], "title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition", "comment": null, "summary": "EEG-based emotion recognition plays an important role in developing adaptive\nbrain-computer communication systems, yet faces two fundamental challenges in\npractical implementations: (1) effective integration of non-stationary\nspatial-temporal neural patterns, (2) robust adaptation to dynamic emotional\nintensity variations in real-world scenarios. This paper proposes SST-CL, a\nnovel framework integrating spatial-temporal transformers with curriculum\nlearning. Our method introduces two core components: a spatial encoder that\nmodels inter-channel relationships and a temporal encoder that captures\nmulti-scale dependencies through windowed attention mechanisms, enabling\nsimultaneous extraction of spatial correlations and temporal dynamics from EEG\nsignals. Complementing this architecture, an intensity-aware curriculum\nlearning strategy progressively guides training from high-intensity to\nlow-intensity emotional states through dynamic sample scheduling based on a\ndual difficulty assessment. Comprehensive experiments on three benchmark\ndatasets demonstrate state-of-the-art performance across various emotional\nintensity levels, with ablation studies confirming the necessity of both\narchitectural components and the curriculum learning mechanism.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4-\u65f6\u95f4\u53d8\u6362\u5668\u548c\u8bfe\u7a0b\u5b66\u4e60\u7684\u6846\u67b6SST-CL\uff0c\u7528\u4e8e\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u975e\u5e73\u7a33\u7a7a\u95f4-\u65f6\u95f4\u6a21\u5f0f\u6574\u5408\u548c\u52a8\u6001\u60c5\u611f\u5f3a\u5ea6\u9002\u5e94\u95ee\u9898\u3002", "motivation": "EEG\u60c5\u611f\u8bc6\u522b\u5728\u8111\u673a\u901a\u4fe1\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u975e\u5e73\u7a33\u7a7a\u95f4-\u65f6\u95f4\u6a21\u5f0f\u6574\u5408\u548c\u52a8\u6001\u60c5\u611f\u5f3a\u5ea6\u53d8\u5316\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u7a7a\u95f4\u7f16\u7801\u5668\uff08\u5efa\u6a21\u901a\u9053\u95f4\u5173\u7cfb\uff09\u548c\u65f6\u95f4\u7f16\u7801\u5668\uff08\u901a\u8fc7\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u591a\u5c3a\u5ea6\u4f9d\u8d56\uff09\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5f3a\u5ea6\u611f\u77e5\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5404\u90e8\u5206\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "SST-CL\u6846\u67b6\u6709\u6548\u6574\u5408\u4e86\u7a7a\u95f4-\u65f6\u95f4\u4fe1\u606f\u5e76\u9002\u5e94\u52a8\u6001\u60c5\u611f\u5f3a\u5ea6\uff0c\u4e3aEEG\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14945", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14945", "abs": "https://arxiv.org/abs/2507.14945", "authors": ["Bin Wang", "Jun Fang", "Jieru Du", "Shihai Shao"], "title": "Jamming-Resistant AAV Communications: A Multichannel-Aided Approach", "comment": null, "summary": "Jamming cancellation is essential to reliable unmanned autonomous vehicle\n(AAV) communications in the presence of malicious jammers. In this paper, we\ndevelop a practical multichannel-aided jamming cancellation method to realize\nsecure AAV communications. The proposed method is capable of simultaneously\nachieving timing/frequency synchronization as well as jamming cancellation.\nMore importantly, our method does not need the signal's/jammer's channel state\ninformation. It only utilizes the knowledge of the legitimate sender's preamble\nsequence that is available in existing communication protocols. We also analyze\nthe length of the preamble sequence required for successful synchronization and\nsignal recovery. Experimental results on the built hardware platform show that,\nwith a two-antenna receiver, the proposed method can successfully decode the\nsignal of interest even when the jamming signal is $40$dB stronger than the\ncommunication signal.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u901a\u9053\u8f85\u52a9\u7684\u5e72\u6270\u6d88\u9664\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u81ea\u4e3b\u8f66\u8f86\u901a\u4fe1\uff0c\u65e0\u9700\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u4ec5\u5229\u7528\u5408\u6cd5\u53d1\u9001\u65b9\u7684\u5bfc\u9891\u5e8f\u5217\u5373\u53ef\u5b9e\u73b0\u540c\u6b65\u548c\u5e72\u6270\u6d88\u9664\u3002", "motivation": "\u5728\u6076\u610f\u5e72\u6270\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u4fdd\u65e0\u4eba\u81ea\u4e3b\u8f66\u8f86\u901a\u4fe1\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u901a\u9053\u8f85\u52a9\u7684\u5e72\u6270\u6d88\u9664\u65b9\u6cd5\uff0c\u65e0\u9700\u4fe1\u53f7\u6216\u5e72\u6270\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u4ec5\u5229\u7528\u5408\u6cd5\u53d1\u9001\u65b9\u7684\u5bfc\u9891\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e72\u6270\u4fe1\u53f7\u6bd4\u901a\u4fe1\u4fe1\u53f7\u5f3a40dB\u65f6\u4ecd\u80fd\u6210\u529f\u89e3\u7801\u76ee\u6807\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u4eba\u81ea\u4e3b\u8f66\u8f86\u901a\u4fe1\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5e72\u6270\u6d88\u9664\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14706", "abs": "https://arxiv.org/abs/2507.14706", "authors": ["Claudio Giusti", "Luca Guarnera", "Mirko Casu", "Sebastiano Battiato"], "title": "Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling", "comment": "23 pages, 14 figures", "summary": "Detecting fraudulent credit card transactions remains a significant\nchallenge, due to the extreme class imbalance in real-world data and the often\nsubtle patterns that separate fraud from legitimate activity. Existing research\ncommonly attempts to address this by generating synthetic samples for the\nminority class using approaches such as GANs, VAEs, or hybrid generative\nmodels. However, these techniques, particularly when applied only to\nminority-class data, tend to result in overconfident classifiers and poor\nlatent cluster separation, ultimately limiting real-world detection\nperformance. In this study, we propose the Causal Prototype Attention\nClassifier (CPAC), an interpretable architecture that promotes class-aware\nclustering and improved latent space structure through prototype-based\nattention mechanisms and we will couple it with the encoder in a VAE-GAN\nallowing it to offer a better cluster separation moving beyond post-hoc sample\naugmentation. We compared CPAC-augmented models to traditional oversamplers,\nsuch as SMOTE, as well as to state-of-the-art generative models, both with and\nwithout CPAC-based latent classifiers. Our results show that classifier-guided\nlatent shaping with CPAC delivers superior performance, achieving an F1-score\nof 93.14\\% percent and recall of 90.18\\%, along with improved latent cluster\nseparation. Further ablation studies and visualizations provide deeper insight\ninto the benefits and limitations of classifier-driven representation learning\nfor fraud detection. The codebase for this work will be available at final\nsubmission.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPAC\u7684\u65b0\u578b\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u89e3\u51b3\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u539f\u578b\u6ce8\u610f\u529b\u673a\u5236\u548cVAE-GAN\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4fe1\u7528\u5361\u6b3a\u8bc8\u68c0\u6d4b\u9762\u4e34\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6b3a\u8bc8\u6a21\u5f0f\u96be\u4ee5\u6355\u6349\u7684\u6311\u6218\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\uff08\u5982GANs\u3001VAEs\uff09\u5728\u5c11\u6570\u7c7b\u6837\u672c\u751f\u6210\u4e0a\u6548\u679c\u6709\u9650\uff0c\u5bfc\u81f4\u5206\u7c7b\u5668\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCPAC\u67b6\u6784\uff0c\u7ed3\u5408\u539f\u578b\u6ce8\u610f\u529b\u673a\u5236\u548cVAE-GAN\uff0c\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\uff0c\u63d0\u5347\u7c7b\u522b\u805a\u7c7b\u6548\u679c\u3002\u4e0e\u4f20\u7edf\u8fc7\u91c7\u6837\u65b9\u6cd5\uff08\u5982SMOTE\uff09\u548c\u751f\u6210\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "CPAC\u5728F1-score\uff0893.14%\uff09\u548c\u53ec\u56de\u7387\uff0890.18%\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6f5c\u5728\u805a\u7c7b\u5206\u79bb\u6548\u679c\u66f4\u597d\u3002", "conclusion": "CPAC\u901a\u8fc7\u5206\u7c7b\u5668\u9a71\u52a8\u7684\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14951", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14951", "abs": "https://arxiv.org/abs/2507.14951", "authors": ["Hongzhi Zhu", "Wei Xu", "Xiaohu You"], "title": "Latent-attention Based Transformer for Near ML Polar Decoding in Short-code Regime", "comment": null, "summary": "Transformer architectures have emerged as promising deep learning (DL) tools\nfor modeling complex sequence-to-sequence interactions in channel decoding.\nHowever, current transformer-based decoders for error correction codes (ECCs)\ndemonstrate inferior performance and generalization capabilities compared to\nconventional algebraic decoders, especially in short-code regimes. In this\nwork, we propose a novel latent-attention based transformer (LAT) decoder for\npolar codes that addresses the limitations on performance and generalization\nthrough three pivotal innovations. First, we develop a latent-attention\nmechanism that supersedes the conventional self-attention mechanism. This\narchitectural modification enables independent learning of the Query and Key\nmatrices for code-aware attention computation, decoupling them from the Value\nmatrix to emphasize position-wise decoding interactions while reducing context\ncorrelation interference. Second, we devise an advanced training framework\nincorporating three synergistic components: entropy-aware importance sampling\nthat emphasizes low-probability regions in the signal constellation space,\nexperience reflow that introduces empirical labels to improve characterization\nof decoding boundaries, and dynamic label smoothing for likelihood-based\nregularization. Third, we propose a code-aware mask scheme which allows dynamic\nadaptation for varying code configurations. Numerical evaluations demonstrate\nthat the proposed LAT decoder achieves near maximum-likelihood (ML) performance\nin terms of both bit error rate (BER) and block error rate (BLER) for\nshort-length polar codes. Furthermore, the architecture exhibits robust\ngeneralization capabilities across diverse code rates and code lengths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6ce8\u610f\u529b\u7684Transformer\u89e3\u7801\u5668\uff08LAT\uff09\uff0c\u901a\u8fc7\u6539\u8fdb\u6ce8\u610f\u529b\u673a\u5236\u3001\u8bad\u7ec3\u6846\u67b6\u548c\u63a9\u7801\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77ed\u7801\u6781\u5316\u7801\u7684\u89e3\u7801\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u5728\u77ed\u7801\u6781\u5316\u7801\u89e3\u7801\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u65e0\u6cd5\u4e0e\u4f20\u7edf\u4ee3\u6570\u89e3\u7801\u5668\u7ade\u4e89\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "1. \u63d0\u51fa\u6f5c\u5728\u6ce8\u610f\u529b\u673a\u5236\u66ff\u4ee3\u81ea\u6ce8\u610f\u529b\uff1b2. \u8bbe\u8ba1\u5305\u542b\u71b5\u611f\u77e5\u91cd\u8981\u6027\u91c7\u6837\u3001\u7ecf\u9a8c\u56de\u6d41\u548c\u52a8\u6001\u6807\u7b7e\u5e73\u6ed1\u7684\u8bad\u7ec3\u6846\u67b6\uff1b3. \u5f15\u5165\u4ee3\u7801\u611f\u77e5\u63a9\u7801\u65b9\u6848\u3002", "result": "LAT\u89e3\u7801\u5668\u5728\u77ed\u7801\u6781\u5316\u7801\u4e2d\u63a5\u8fd1\u6700\u5927\u4f3c\u7136\u6027\u80fd\uff08BER\u548cBLER\uff09\uff0c\u4e14\u5728\u4e0d\u540c\u7801\u7387\u548c\u7801\u957f\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "LAT\u89e3\u7801\u5668\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86Transformer\u5728\u6781\u5316\u7801\u89e3\u7801\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14715", "abs": "https://arxiv.org/abs/2507.14715", "authors": ["Rachid Karami", "Rajeev Patwari", "Hyoukjun Kwon", "Ashish Sirasao"], "title": "Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems", "comment": null, "summary": "The integration of generative AI models, particularly large language models\n(LLMs), into real-time multi-model AI applications such as video conferencing\nand gaming is giving rise to a new class of workloads: real-time generative AI\n(RTGen). These workloads combine the compute intensity and dynamic execution\npatterns of generative models with the stringent latency and concurrency\nconstraints of real-time inference. To meet the diverse demands of RTGen\nworkloads, modern edge platforms increasingly adopt heterogeneous\nsystem-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite\nthe potential of heterogeneous SoC, the scheduling space complexity and\nperformance implications of RTGen workloads on such platforms remain\nunderexplored. In this work, we perform a comprehensive characterization of\nRTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct\nrealistic multi-model scenarios inspired by industry use cases and profile\nmodel performance across all available backends. Using this data, we evaluate\nfive scheduling policies and their impact on both real-time metrics (e.g.,\ndeadline violation rate) and LLM performance (e.g., time-to-first-token and\ntokens-per-second). Our results show that scheduling decisions significantly\naffect workload performance (e.g., leading to a 41.7% difference in deadline\nviolation rates on average), and highlight the need for scheduling strategies\nthat are aware of workload dynamics and hardware heterogeneity. Our findings\nunderscore the importance of workload-aware, dynamic heterogeneous scheduling\nin enabling high-performance, on-device RTGen applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5b9e\u65f6\u751f\u6210AI\uff08RTGen\uff09\u5de5\u4f5c\u8d1f\u8f7d\u5728\u5f02\u6784SoC\u4e0a\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u53d1\u73b0\u8c03\u5ea6\u51b3\u7b56\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u52a8\u6001\u5f02\u6784\u8c03\u5ea6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5b9e\u65f6\u751f\u6210AI\uff08RTGen\uff09\u5de5\u4f5c\u8d1f\u8f7d\u5728\u5f02\u6784SoC\u4e0a\u7684\u8c03\u5ea6\u590d\u6742\u6027\u548c\u6027\u80fd\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u63a2\u7d22\u4ee5\u652f\u6301\u9ad8\u6027\u80fd\u5e94\u7528\u3002", "method": "\u5728AMD\u7684Ryzen AI\u5f02\u6784SoC\u4e0a\u6784\u5efa\u591a\u6a21\u578b\u573a\u666f\uff0c\u5206\u6790\u4e94\u79cd\u8c03\u5ea6\u7b56\u7565\u5bf9\u5b9e\u65f6\u6307\u6807\u548cLLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8c03\u5ea6\u51b3\u7b56\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff08\u5982\u5e73\u574741.7%\u7684\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\u7387\u5dee\u5f02\uff09\uff0c\u9700\u52a8\u6001\u5f02\u6784\u8c03\u5ea6\u7b56\u7565\u3002", "conclusion": "\u52a8\u6001\u5f02\u6784\u8c03\u5ea6\u5bf9\u9ad8\u6027\u80fd\u5b9e\u65f6\u751f\u6210AI\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14982", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14982", "abs": "https://arxiv.org/abs/2507.14982", "authors": ["Kareem M. Attiah", "Wei Yu"], "title": "How Many Simultaneous Beamformers are Needed for Integrated Sensing and Communications?", "comment": "26 pages, 7 figures, Submitted to T-IT for future publication", "summary": "Consider a downlink integrated sensing and communications (ISAC) system in\nwhich a base station employs linear beamforming to communicate to $K$ users,\nwhile simultaneously uses sensing beams to perform a sensing task of estimating\n$L$ real parameters. How many beamformers are needed to achieve the best\nperformance for both sensing and communications? This paper establishes bounds\non the minimum number of downlink beamformers, in which sensing performance is\nmeasured in terms of the Cram\\'{e}r-Rao bound for parameter estimation and\ncommunications performance is measured in terms of the\nsignal-to-interference-and-noise ratios. We show that an ISAC system requires\nat most $K + \\sqrt{\\frac{L(L+1)}{2}}$ beamformers if the remote users have the\nability to cancel the interference caused by the sensing beams. If cancelling\ninterference due to the sensing beams is not possible, the bound becomes\n$\\sqrt{K^2 + \\frac{L(L+1)}{2}}$. Interestingly, in the latter case, the bound\non the number of beamformers is less than the sum of the bounds for each task\nindividually. These results can be extended to sensing tasks for which the\nperformance is measured as a function of $d$ quadratic terms in the\nbeamformers. In this case, the bound becomes $K + \\sqrt{d}$ and $\\sqrt{K^2 +\nd}$, respectively. Specifically, for estimating complex path losses and\nangles-of-arrival of $N_\\text{tr}$ targets while communicating to $K$ users,\nthe bound on the minimum number of beamformers scales linearly in $K$ and in\n$N_\\text{tr}$, assuming interference from sensing can be cancelled. When\ninterference cancellation is not possible, the following exact characterization\nfor the case of $N_\\text{tr} = 1$ can be obtained: when $K=0$ or $1$, two\nbeamformers should be used; when $K \\ge 2$, exactly $K$ beamformers should be\nused, i.e., communication beamformers alone are already sufficient.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86ISAC\u7cfb\u7edf\u4e2d\u6240\u9700\u7684\u6700\u5c0f\u6ce2\u675f\u5f62\u6210\u5668\u6570\u91cf\uff0c\u5206\u6790\u4e86\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\u7684\u6743\u8861\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e0d\u540c\u5e72\u6270\u6761\u4ef6\u4e0b\u7684\u7406\u8bba\u754c\u9650\u3002", "motivation": "\u63a2\u7d22\u5728ISAC\u7cfb\u7edf\u4e2d\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u5f62\u6210\u5668\u6570\u91cf\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u901a\u4fe1\u548c\u611f\u77e5\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u5efa\u7acb\u57fa\u4e8eCram\u00e9r-Rao\u754c\u548c\u4fe1\u53f7\u5e72\u6270\u566a\u58f0\u6bd4\u7684\u6027\u80fd\u5ea6\u91cf\uff0c\u63a8\u5bfc\u6ce2\u675f\u5f62\u6210\u5668\u6570\u91cf\u7684\u4e0b\u9650\u3002", "result": "\u5728\u5e72\u6270\u53ef\u6d88\u9664\u65f6\uff0c\u6ce2\u675f\u5f62\u6210\u5668\u6570\u91cf\u4e0a\u9650\u4e3aK + \u221a(L(L+1)/2)\uff1b\u4e0d\u53ef\u6d88\u9664\u65f6\u4e3a\u221a(K\u00b2 + L(L+1)/2)\u3002", "conclusion": "ISAC\u7cfb\u7edf\u7684\u6ce2\u675f\u5f62\u6210\u5668\u6570\u91cf\u53d7\u5e72\u6270\u6761\u4ef6\u5f71\u54cd\uff0c\u4f18\u5316\u8bbe\u8ba1\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.14722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14722", "abs": "https://arxiv.org/abs/2507.14722", "authors": ["Mat\u011bj Kripner", "Michal \u0160ustr", "Milan Straka"], "title": "LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4", "comment": null, "summary": "Automated theorem proving (ATP) has been a classical problem in artificial\nintelligence since its inception, yet it remains challenging due to its vast\nstate and action space. Large language models (LLMs) have recently emerged as a\npromising heuristic for ATP, but they lack correctness guarantees and thus\nrequire interaction with a proof verifier. Such interactions typically follow\none of two approaches: black-box interaction, which does not utilize\nintermediate proof states, or white-box approaches, which allow for incremental\nproof construction and examination of intermediate states. While black-box\napproaches have directly benefited from recent LLM advances, white-box methods\nhave comparatively lagged behind. In this paper, we address this gap by\nintroducing LeanTree, which consists of (i) a tool built in the Lean 4 language\nthat factorizes complex proof states into simpler, independent branches, and\n(ii) a dataset of these factorized intermediate states. Our white-box tooling\noffers several advantages over black-box approaches: it simplifies evaluation,\nreduces necessary context, generates richer training data, enables parallel\nsearch across multiple states, supports efficient reuse of states, and provides\nfeedback in case of errors. Our preliminary results hint that white-box\napproaches outperform black-box alternatives in some settings.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86LeanTree\uff0c\u4e00\u79cd\u767d\u76d2\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u89e3\u590d\u6742\u8bc1\u660e\u72b6\u6001\u4e3a\u72ec\u7acb\u5206\u652f\uff0c\u63d0\u5347\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff08ATP\uff09\u56e0\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u5e9e\u5927\u800c\u5177\u6311\u6218\u6027\uff0c\u767d\u76d2\u65b9\u6cd5\u5728LLM\u65f6\u4ee3\u53d1\u5c55\u6ede\u540e\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faLeanTree\uff0c\u5305\u542b\u57fa\u4e8eLean 4\u7684\u5de5\u5177\u548c\u5206\u89e3\u540e\u7684\u4e2d\u95f4\u72b6\u6001\u6570\u636e\u96c6\uff0c\u652f\u6301\u5e76\u884c\u641c\u7d22\u548c\u72b6\u6001\u590d\u7528\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u767d\u76d2\u65b9\u6cd5\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u9ed1\u76d2\u65b9\u6cd5\u3002", "conclusion": "\u767d\u76d2\u5de5\u5177LeanTree\u4e3aATP\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u591a\u9879\u4f18\u52bf\u3002"}}
{"id": "2507.15116", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15116", "abs": "https://arxiv.org/abs/2507.15116", "authors": ["Zichao Zhang", "Melda Yuksel", "Gokhan M. Guvensen", "Halim Yanikomeroglu"], "title": "PAPR Analysis for MIMO FTN Signaling with Gaussian Symbols", "comment": null, "summary": "Faster-than-Nyquist signaling serves as a promising solution for improving\nspectral efficiency in future generations of communications. However, its\nnature of fast acceleration brings highly overlapped pulses that lead to worse\npeak-to-average power ratio (PAPR) performance. In this paper, we investigate\nthe PAPR behavior of MIMO FTN using Gaussian symbols under optimal power\nallocation for two power constraints: fixed transmit power and fixed received\nsignal-to-noise-ratio (SNR). Our findings reveal that PAPR is mainly determined\nby the acceleration factor and the power constraint, but power allocation\noptimization does not change the PAPR behavior for Gaussian signaling.", "AI": {"tldr": "\u7814\u7a76\u4e86MIMO FTN\u7cfb\u7edf\u4e2d\u9ad8\u65af\u7b26\u53f7\u5728\u4e24\u79cd\u529f\u7387\u7ea6\u675f\u4e0b\u7684PAPR\u884c\u4e3a\uff0c\u53d1\u73b0PAPR\u4e3b\u8981\u7531\u52a0\u901f\u56e0\u5b50\u548c\u529f\u7387\u7ea6\u675f\u51b3\u5b9a\uff0c\u529f\u7387\u5206\u914d\u4f18\u5316\u5bf9\u9ad8\u65af\u4fe1\u53f7\u7684PAPR\u884c\u4e3a\u65e0\u5f71\u54cd\u3002", "motivation": "FTN\u4fe1\u53f7\u4f5c\u4e3a\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5feb\u901f\u52a0\u901f\u7279\u6027\u5bfc\u81f4\u8109\u51b2\u9ad8\u5ea6\u91cd\u53e0\uff0c\u6076\u5316\u4e86PAPR\u6027\u80fd\u3002", "method": "\u5206\u6790MIMO FTN\u7cfb\u7edf\u4e2d\u9ad8\u65af\u7b26\u53f7\u5728\u56fa\u5b9a\u53d1\u5c04\u529f\u7387\u548c\u56fa\u5b9a\u63a5\u6536SNR\u4e24\u79cd\u529f\u7387\u7ea6\u675f\u4e0b\u7684PAPR\u884c\u4e3a\u3002", "result": "PAPR\u4e3b\u8981\u7531\u52a0\u901f\u56e0\u5b50\u548c\u529f\u7387\u7ea6\u675f\u51b3\u5b9a\uff0c\u529f\u7387\u5206\u914d\u4f18\u5316\u5bf9\u9ad8\u65af\u4fe1\u53f7\u7684PAPR\u884c\u4e3a\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5728MIMO FTN\u7cfb\u7edf\u4e2d\uff0cPAPR\u884c\u4e3a\u53d7\u52a0\u901f\u56e0\u5b50\u548c\u529f\u7387\u7ea6\u675f\u4e3b\u5bfc\uff0c\u529f\u7387\u5206\u914d\u4f18\u5316\u5bf9\u9ad8\u65af\u4fe1\u53f7\u65e0\u6548\u3002"}}
{"id": "2507.14725", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14725", "abs": "https://arxiv.org/abs/2507.14725", "authors": ["Anushka Tiwari", "Sayantan Pal", "Rohini K. Srihari", "Kaiyi Ji"], "title": "Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding", "comment": null, "summary": "Prompt-based continual learning (CL) offers a parameter-efficient way to\nadapt large language models (LLMs) across task sequences. However, most\nexisting methods assume task-aware inference and maintain a growing list of\ntask-specific prompts, which limits scalability and hides latent forgetting. In\nthis work, we introduce GRID, a unified framework that addresses two key\nlimitations: (1) latent forgetting under task-agnostic inference, and (2)\nprompt memory explosion as task sequences grow. GRID integrates a task-aware\ndecoding mechanism that improves backward transfer by leveraging representative\ninputs, automatic task identification, and constrained decoding. Additionally,\nwe propose a gradient-based prompt selection strategy that compresses less\ninformative prompts into a single aggregated representation, enabling scalable\nand memory-efficient lifelong learning. Extensive experiments across\nshort-sequence, long-sequence, and negative transfer benchmarks show that GRID\nsignificantly improves backward transfer, achieves competitive forward\ntransfer, and reduces forgotten tasks by up to 80\\%, outperforming\nstate-of-the-art methods on T5 and Flan-T5 backbones.", "AI": {"tldr": "GRID\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u9057\u5fd8\u548c\u63d0\u793a\u5185\u5b58\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u89e3\u7801\u548c\u68af\u5ea6\u63d0\u793a\u9009\u62e9\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4efb\u52a1\u65e0\u5173\u63a8\u7406\u4e0b\u5b58\u5728\u6f5c\u5728\u9057\u5fd8\u548c\u63d0\u793a\u5185\u5b58\u7206\u70b8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "GRID\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u89e3\u7801\u673a\u5236\uff08\u5229\u7528\u4ee3\u8868\u6027\u8f93\u5165\u3001\u81ea\u52a8\u4efb\u52a1\u8bc6\u522b\u548c\u7ea6\u675f\u89e3\u7801\uff09\u548c\u68af\u5ea6\u63d0\u793a\u9009\u62e9\u7b56\u7565\uff08\u538b\u7f29\u975e\u4fe1\u606f\u63d0\u793a\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGRID\u663e\u8457\u63d0\u5347\u540e\u5411\u8f6c\u79fb\uff0c\u51cf\u5c11\u9057\u5fd8\u4efb\u52a1\u8fbe80%\uff0c\u5728T5\u548cFlan-T5\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GRID\u901a\u8fc7\u9ad8\u6548\u89e3\u7801\u548c\u63d0\u793a\u538b\u7f29\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.15118", "categories": ["eess.SP", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.15118", "abs": "https://arxiv.org/abs/2507.15118", "authors": ["Szymon Mazurek", "Stephen Moore", "Alessandro Crimi"], "title": "Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings", "comment": null, "summary": "Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce\nneurologists and costly diagnostic tools. We propose a graph-based deep\nlearning framework to detect epilepsy from low-cost Electroencephalography\n(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus\nis on fair, accessible automatic assessment and explainability to shed light on\nepilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,\nclassify them, and identify interchannel relationships and temporal dynamics\nusing graph attention networks (GAT). To emphasize connectivity biomarkers, we\nadapt the inherently node-focused GAT to analyze edges. We also designed signal\npreprocessing for low-fidelity recordings and a lightweight GAT architecture\ntrained on Google Colab and deployed on RaspberryPi devices. Results: The\napproach achieves promising classification performance, outperforming a\nstandard classifier based on random forest and graph convolutional networks in\nterms of accuracy and robustness over multiple sessions, but also highlighting\nspecific connections in the fronto-temporal region. Conclusions: The results\nhighlight the potential of GATs to provide insightful and scalable diagnostic\nsupport for epilepsy in underserved regions, paving the way for affordable and\naccessible neurodiagnostic tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4f4e\u6210\u672c\u8111\u7535\u56fe\uff08EEG\uff09\u786c\u4ef6\u4e2d\u68c0\u6d4b\u766b\u75eb\uff0c\u5e76\u5728\u5c3c\u65e5\u5229\u4e9a\u548c\u51e0\u5185\u4e9a\u6bd4\u7ecd\u7684\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u766b\u75eb\u5728\u4f4e\u6536\u5165\u56fd\u5bb6\u8bca\u65ad\u4e0d\u8db3\uff0c\u539f\u56e0\u662f\u795e\u7ecf\u79d1\u533b\u751f\u7a00\u7f3a\u548c\u8bca\u65ad\u5de5\u5177\u6602\u8d35\u3002\u76ee\u6807\u662f\u5f00\u53d1\u516c\u5e73\u3001\u53ef\u8bbf\u95ee\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06EEG\u4fe1\u53f7\u5efa\u6a21\u4e3a\u65f6\u7a7a\u56fe\uff0c\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8bc6\u522b\u901a\u9053\u95f4\u5173\u7cfb\u548c\u65f6\u6001\u52a8\u6001\u3002\u9488\u5bf9\u4f4e\u8d28\u91cf\u8bb0\u5f55\u8bbe\u8ba1\u4e86\u4fe1\u53f7\u9884\u5904\u7406\u548c\u8f7b\u91cf\u7ea7GAT\u67b6\u6784\u3002", "result": "\u5206\u7c7b\u6027\u80fd\u4f18\u4e8e\u968f\u673a\u68ee\u6797\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u7a81\u51fa\u4e86\u989d\u989e\u533a\u7684\u7279\u5b9a\u8fde\u63a5\u3002", "conclusion": "GATs\u4e3a\u6b20\u53d1\u8fbe\u5730\u533a\u63d0\u4f9b\u4e86\u6709\u6f5c\u529b\u4e14\u53ef\u6269\u5c55\u7684\u766b\u75eb\u8bca\u65ad\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u7ecf\u6d4e\u5b9e\u60e0\u7684\u795e\u7ecf\u8bca\u65ad\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.14736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14736", "abs": "https://arxiv.org/abs/2507.14736", "authors": ["Rafa\u0142 Surdej", "Micha\u0142 Bortkiewicz", "Alex Lewandowski", "Mateusz Ostaszewski", "Clare Lyle"], "title": "Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning", "comment": "Accepted for oral presentation at CoLLAs 2025", "summary": "Trainable activation functions, whose parameters are optimized alongside\nnetwork weights, offer increased expressivity compared to fixed activation\nfunctions. Specifically, trainable activation functions defined as ratios of\npolynomials (rational functions) have been proposed to enhance plasticity in\nreinforcement learning. However, their impact on training stability remains\nunclear. In this work, we study trainable rational activations in both\nreinforcement and continual learning settings. We find that while their\nflexibility enhances adaptability, it can also introduce instability, leading\nto overestimation in RL and feature collapse in longer continual learning\nscenarios. Our main result is demonstrating a trade-off between expressivity\nand plasticity in rational activations. To address this, we propose a\nconstrained variant that structurally limits excessive output scaling while\npreserving adaptability. Experiments across MetaWorld and DeepMind Control\nSuite (DMC) environments show that our approach improves training stability and\nperformance. In continual learning benchmarks, including MNIST with reshuffled\nlabels and Split CIFAR-100, we reveal how different constraints affect the\nbalance between expressivity and long-term retention. While preliminary\nexperiments in discrete action domains (e.g., Atari) did not show similar\ninstability, this suggests that the trade-off is particularly relevant for\ncontinuous control. Together, our findings provide actionable design principles\nfor robust and adaptable trainable activations in dynamic, non-stationary\nenvironments. Code available at:\nhttps://github.com/special114/rl_rational_plasticity.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u8bad\u7ec3\u6709\u7406\u6fc0\u6d3b\u51fd\u6570\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u7075\u6d3b\u6027\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u53d8\u4f53\u4ee5\u5e73\u8861\u8868\u8fbe\u6027\u548c\u53ef\u5851\u6027\u3002", "motivation": "\u63a2\u7d22\u53ef\u8bad\u7ec3\u6709\u7406\u6fc0\u6d3b\u51fd\u6570\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u5bf9\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ea6\u675f\u53d8\u4f53\uff0c\u9650\u5236\u8f93\u51fa\u7f29\u653e\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u5e76\u5728MetaWorld\u548cDMC\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7ea6\u675f\u53d8\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u5747\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8868\u8fbe\u6027\u4e0e\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u539f\u5219\u4ee5\u4f18\u5316\u53ef\u8bad\u7ec3\u6fc0\u6d3b\u51fd\u6570\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.15255", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15255", "abs": "https://arxiv.org/abs/2507.15255", "authors": ["Deyun Zhang", "Xiang Lan", "Shijia Geng", "Qinghao Zhao", "Sumei Fan", "Mengling Feng", "Shenda Hong"], "title": "MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations", "comment": null, "summary": "Electrocardiogram (ECG) plays a foundational role in modern cardiovascular\ncare, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and\nconduction disorders. While machine learning has achieved expert-level\nperformance in ECG interpretation, the development of clinically deployable\nmultimodal AI systems remains constrained, primarily due to the lack of\npublicly available datasets that simultaneously incorporate raw signals,\ndiagnostic images, and interpretation text. Most existing ECG datasets provide\nonly single-modality data or, at most, dual modalities, making it difficult to\nbuild models that can understand and integrate diverse ECG information in\nreal-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext\nECG-Text-Image), the first large-scale ECG dataset that synchronizes raw\nwaveform data, high-resolution plotted images, and detailed textual\ninterpretations generated by large language models. In addition, MEETI includes\nbeat-level quantitative ECG parameters extracted from each lead, offering\nstructured parameters that support fine-grained analysis and model\ninterpretability. Each MEETI record is aligned across four components: (1) the\nraw ECG waveform, (2) the corresponding plotted image, (3) extracted feature\nparameters, and (4) detailed interpretation text. This alignment is achieved\nusing consistent, unique identifiers. This unified structure supports\ntransformer-based multimodal learning and supports fine-grained, interpretable\nreasoning about cardiac health. By bridging the gap between traditional signal\nanalysis, image-based interpretation, and language-driven understanding, MEETI\nestablished a robust foundation for the next generation of explainable,\nmultimodal cardiovascular AI. It offers the research community a comprehensive\nbenchmark for developing and evaluating ECG-based AI systems.", "AI": {"tldr": "MEETI\u662f\u4e00\u4e2a\u591a\u6a21\u6001ECG\u6570\u636e\u96c6\uff0c\u6574\u5408\u4e86\u539f\u59cb\u6ce2\u5f62\u3001\u56fe\u50cf\u3001\u6587\u672c\u548c\u7279\u5f81\u53c2\u6570\uff0c\u4e3a\u5fc3\u8840\u7ba1AI\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709ECG\u6570\u636e\u96c6\u591a\u4e3a\u5355\u6a21\u6001\u6216\u53cc\u6a21\u6001\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u5f00\u53d1\u3002MEETI\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "MEETI\u540c\u6b65\u4e86\u539f\u59cb\u6ce2\u5f62\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3001\u8be6\u7ec6\u6587\u672c\u89e3\u91ca\u548c\u7279\u5f81\u53c2\u6570\uff0c\u901a\u8fc7\u7edf\u4e00\u6807\u8bc6\u7b26\u5bf9\u9f50\u3002", "result": "MEETI\u652f\u6301\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u5fc3\u8840\u7ba1AI\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "MEETI\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u89e3\u91ca\u3001\u591a\u6a21\u6001\u5fc3\u8840\u7ba1AI\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u662f\u7814\u7a76\u548c\u8bc4\u4f30ECG AI\u7cfb\u7edf\u7684\u7efc\u5408\u57fa\u51c6\u3002"}}
{"id": "2507.15256", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15256", "abs": "https://arxiv.org/abs/2507.15256", "authors": ["Zihao Hu", "Jia Yan", "Ying-Jun Angela Zhang", "Jun Zhang", "Khaled B. Letaief"], "title": "Optimal Transceiver Design in Over-the-Air Federated Distillation", "comment": "13 pages, 7 figures, submitted to IEEE Transactions on Wireless\n  Communications", "summary": "The rapid proliferation and growth of artificial intelligence (AI) has led to\nthe development of federated learning (FL). FL allows wireless devices (WDs) to\ncooperatively learn by sharing only local model parameters, without needing to\nshare the entire dataset. However, the emergence of large AI models has made\nexisting FL approaches inefficient, due to the significant communication\noverhead required. In this paper, we propose a novel over-the-air federated\ndistillation (FD) framework by synergizing the strength of FL and knowledge\ndistillation to avoid the heavy local model transmission. Instead of sharing\nthe model parameters, only the WDs' model outputs, referred to as knowledge,\nare shared and aggregated over-the-air by exploiting the superposition property\nof the multiple-access channel. We shall study the transceiver design in\nover-the-air FD, aiming to maximize the learning convergence rate while meeting\nthe power constraints of the transceivers. The main challenge lies in the\nintractability of the learning performance analysis, as well as the non-convex\nnature and the optimization spanning the whole FD training period. To tackle\nthis problem, we first derive an analytical expression of the convergence rate\nin over-the-air FD. Then, the closed-form optimal solutions of the WDs'\ntransmit power and the estimator for over-the-air aggregation are obtained\ngiven the receiver combining strategy. Accordingly, we put forth an efficient\napproach to find the optimal receiver beamforming vector via semidefinite\nrelaxation. We further prove that there is no optimality gap between the\noriginal and relaxed problem for the receiver beamforming design. Numerical\nresults will show that the proposed over-the-air FD approach achieves a\nsignificant reduction in communication overhead, with only a minor compromise\nin testing accuracy compared to conventional FL benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7a7a\u4e2d\u8054\u5408\u84b8\u998f\uff08FD\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u8054\u5408\u5b66\u4e60\uff08FL\uff09\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002\u901a\u8fc7\u5171\u4eab\u6a21\u578b\u8f93\u51fa\u800c\u975e\u53c2\u6570\uff0c\u5229\u7528\u591a\u5740\u4fe1\u9053\u7684\u53e0\u52a0\u7279\u6027\u8fdb\u884c\u805a\u5408\u3002\u4f18\u5316\u6536\u53d1\u5668\u8bbe\u8ba1\u4ee5\u6700\u5927\u5316\u5b66\u4e60\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u6ee1\u8db3\u529f\u7387\u7ea6\u675f\u3002", "motivation": "\u5927\u578bAI\u6a21\u578b\u7684\u51fa\u73b0\u4f7f\u73b0\u6709FL\u65b9\u6cd5\u56e0\u901a\u4fe1\u5f00\u9500\u5927\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7a7a\u4e2dFD\u6846\u67b6\uff0c\u5171\u4eab\u6a21\u578b\u8f93\u51fa\uff08\u77e5\u8bc6\uff09\u800c\u975e\u53c2\u6570\uff1b\u63a8\u5bfc\u6536\u655b\u901f\u7387\u8868\u8fbe\u5f0f\uff0c\u4f18\u5316\u6536\u53d1\u5668\u529f\u7387\u548c\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u3002", "result": "\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u6d4b\u8bd5\u7cbe\u5ea6\u4ec5\u7565\u4f4e\u4e8e\u4f20\u7edfFL\u57fa\u51c6\u3002", "conclusion": "\u7a7a\u4e2dFD\u6846\u67b6\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u5b66\u4e60\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21AI\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2507.14744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14744", "abs": "https://arxiv.org/abs/2507.14744", "authors": ["Mustafa Cavus", "Jan N. van Rijn", "Przemys\u0142aw Biecek"], "title": "Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML", "comment": "Accepted at 28th International Conference on Discovery Science 2025", "summary": "Automated machine learning systems efficiently streamline model selection but\noften focus on a single best-performing model, overlooking explanation\nuncertainty, an essential concern in human centered explainable AI. To address\nthis, we propose a novel framework that incorporates model multiplicity into\nexplanation generation by aggregating partial dependence profiles (PDP) from a\nset of near optimal models, known as the Rashomon set. The resulting Rashomon\nPDP captures interpretive variability and highlights areas of disagreement,\nproviding users with a richer, uncertainty aware view of feature effects. To\nevaluate its usefulness, we introduce two quantitative metrics, the coverage\nrate and the mean width of confidence intervals, to evaluate the consistency\nbetween the standard PDP and the proposed Rashomon PDP. Experiments on 35\nregression datasets from the OpenML CTR23 benchmark suite show that in most\ncases, the Rashomon PDP covers less than 70% of the best model's PDP,\nunderscoring the limitations of single model explanations. Our findings suggest\nthat Rashomon PDP improves the reliability and trustworthiness of model\ninterpretations by adding additional information that would otherwise be\nneglected. This is particularly useful in high stakes domains where\ntransparency and confidence are critical.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408Rashomon\u96c6\u5408\u4e2d\u591a\u4e2a\u8fd1\u4f18\u6a21\u578b\u7684PDP\uff0c\u751f\u6210\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7279\u5f81\u6548\u5e94\u89c6\u56fe\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u6700\u4f18\u6a21\u578b\uff0c\u5ffd\u89c6\u4e86\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca\u6027AI\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408Rashomon\u96c6\u5408\u4e2d\u591a\u4e2a\u8fd1\u4f18\u6a21\u578b\u7684PDP\uff0c\u751f\u6210Rashomon PDP\uff0c\u6355\u6349\u89e3\u91ca\u7684\u53d8\u5f02\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRashomon PDP\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4ec5\u8986\u76d6\u6700\u4f73\u6a21\u578bPDP\u7684\u4e0d\u523070%\uff0c\u63ed\u793a\u4e86\u5355\u4e00\u6a21\u578b\u89e3\u91ca\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Rashomon PDP\u901a\u8fc7\u8865\u5145\u88ab\u5ffd\u89c6\u7684\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002"}}
{"id": "2507.15291", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15291", "abs": "https://arxiv.org/abs/2507.15291", "authors": ["Osman Tokluoglu", "Enver Cavus", "Ebrahim Bedeer", "Halim Yanikomeroglu"], "title": "A Novel Domain-Aware CNN Architecture for Faster-than-Nyquist Signaling Detection", "comment": "6 pages, 8 figures", "summary": "This paper proposes a convolutional neural network (CNN)-based detector for\nfaster-than-Nyquist (FTN) signaling that employs structured fixed kernel layers\nwith domain-informed masking to mitigate intersymbol interference (ISI). Unlike\nstandard CNNs with sliding kernels, the proposed method utilizes fixed-position\nkernels to directly capture ISI effects at varying distances from the central\nsymbol. A hierarchical filter allocation strategy is also introduced, assigning\nmore filters to earlier layers for strong ISI patterns and fewer to later\nlayers for weaker ones. This design improves detection accuracy while reducing\nredundant operations. Simulation results show that the detector achieves\nnear-optimal bit error rate (BER) performance for $\\tau \\geq 0.7$, closely\nmatching the BCJR algorithm, and offers computational gains of up to $46\\%$ and\n$84\\%$ over M-BCJR for BPSK and QPSK, respectively. Comparative analysis with\nother methods further highlights the efficiency and effectiveness of the\nproposed approach. To the best of our knowledge, this is the first application\nof a fixed-kernel CNN architecture tailored for FTN detection in the\nliterature.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fa\u5b9a\u6838\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684FTN\u4fe1\u53f7\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5206\u5c42\u6ee4\u6ce2\u5206\u914d\u548c\u56fa\u5b9a\u6838\u8bbe\u8ba1\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u6ed1\u52a8\u6838CNN\u96be\u4ee5\u6709\u6548\u6355\u6349FTN\u4fe1\u53f7\u4e2d\u7684\u7b26\u53f7\u95f4\u5e72\u6270\uff08ISI\uff09\uff0c\u9700\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u56fa\u5b9a\u4f4d\u7f6e\u6838\u76f4\u63a5\u6355\u6349\u4e0d\u540c\u8ddd\u79bb\u7684ISI\u6548\u5e94\uff0c\u5e76\u5206\u5c42\u5206\u914d\u6ee4\u6ce2\u5668\u4ee5\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5728\u03c4\u22650.7\u65f6\u63a5\u8fd1\u6700\u4f18BER\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u6bd4M-BCJR\u63d0\u534746%\uff08BPSK\uff09\u548c84%\uff08QPSK\uff09\u3002", "conclusion": "\u56fa\u5b9a\u6838CNN\u67b6\u6784\u9996\u6b21\u5e94\u7528\u4e8eFTN\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.15306", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15306", "abs": "https://arxiv.org/abs/2507.15306", "authors": ["Midhila Madhusoodanan", "Mahesh Raveendranatha Panicker", "Pisharody Harikrishnan Gopalakrishnan", "Abhilash Rakkunedeth Hareendranathan"], "title": "BEAM-Net: A Deep Learning Framework with Bone Enhancement Attention Mechanism for High Resolution High Frame Rate Ultrasound Beamforming", "comment": null, "summary": "Pocket-sized, low-cost point-of-care ultrasound (POCUS) devices are\nincreasingly used in musculoskeletal (MSK) applications for structural\nexamination of bone tissue. However, the image quality in MSK ultrasound is\noften limited by speckle noise, low resolution, poor contrast, and anisotropic\nreflections, making bone images difficult to interpret without additional\npost-processing. Typically, medical ultrasound systems use delay and sum\nbeamforming (DASB) for image reconstruction, which is not specifically\noptimized for bone structures. To address these limitations, we propose\nBEAM-Net, a novel end-to-end deep neural network (DNN) that performs\nhigh-frame-rate ultrasound beamforming with integrated bone enhancement, using\nsingle-plane-wave (SPW) radio frequency (RF) data as input. Our approach embeds\na Bone Probability Map (BPM), which acts as an attention mechanism to enforce\nhigher structural similarity around bony regions in the image. The proposed\napproach is the first of its kind to incorporate bone enhancement directly into\nultrasound beamforming using deep learning. BEAM-Net was trained and evaluated\non in-vivo MSK and synthetic RF ultrasound datasets. This paper introduces the\nEdge Preservation Index (EPI) as a new region-focused metric for evaluating\nstructural fidelity in bone-enhanced ultrasound images. The performance of\nBEAM-Net was compared with conventional DASB and existing deep learning\narchitectures using the EPI, Contrast Ratio (CR), Signal-to-Noise ratio (SNR),\nSpeckle Similarity Index (SSI), and Structural Similarity Index (SSIM).\nBEAM-Net showed substantial gains over SPW-DASB, achieving 51.4-51% higher CR\nand 94.2-73.3% higher SNR on in-vivo MSK and synthetic RF datasets. It\noutperformed multiple steered plane wave DASB (MPW-DASB), with 19.8-24.0%\nimprovements in CR and SNR on in-vivo MSK and 2.5-12.8% improvements on\nsynthetic data.", "AI": {"tldr": "\u63d0\u51faBEAM-Net\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u8d85\u58f0\u6ce2\u675f\u6210\u5f62\u5e76\u589e\u5f3a\u9aa8\u9abc\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8d85\u58f0\u6ce2\u675f\u6210\u5f62\u65b9\u6cd5\uff08DASB\uff09\u5728\u9aa8\u9abc\u7ed3\u6784\u6210\u50cf\u4e2d\u7684\u4e0d\u8db3\uff0c\u5982\u56fe\u50cf\u566a\u58f0\u3001\u4f4e\u5206\u8fa8\u7387\u548c\u5bf9\u6bd4\u5ea6\u5dee\u3002", "method": "\u4f7f\u7528\u5355\u5e73\u9762\u6ce2\u5c04\u9891\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\uff0c\u7ed3\u5408\u9aa8\u9abc\u6982\u7387\u56fe\uff08BPM\uff09\u4f5c\u4e3a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u76f4\u63a5\u5d4c\u5165\u9aa8\u9abc\u589e\u5f3a\u529f\u80fd\u3002", "result": "BEAM-Net\u5728\u6d3b\u4f53\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u6bd4\u5ea6\u6bd4\uff08CR\uff09\u548c\u4fe1\u566a\u6bd4\uff08SNR\uff09\u663e\u8457\u63d0\u5347\u3002", "conclusion": "BEAM-Net\u9996\u6b21\u5c06\u9aa8\u9abc\u589e\u5f3a\u76f4\u63a5\u878d\u5165\u8d85\u58f0\u6ce2\u675f\u6210\u5f62\uff0c\u4e3a\u9aa8\u9abc\u8d85\u58f0\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14747", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14747", "abs": "https://arxiv.org/abs/2507.14747", "authors": ["Yiding Song"], "title": "Pruning Increases Orderedness in Recurrent Computation", "comment": "8 pages, 11 figures, 2 tables, Workshop on Methods and Opportunities\n  at Small Scale (MOSS), ICML 2025", "summary": "Inspired by the prevalence of recurrent circuits in biological brains, we\ninvestigate the degree to which directionality is a helpful inductive bias for\nartificial neural networks. Taking directionality as topologically-ordered\ninformation flow between neurons, we formalise a perceptron layer with\nall-to-all connections (mathematically equivalent to a weight-tied recurrent\nneural network) and demonstrate that directionality, a hallmark of modern\nfeed-forward networks, can be induced rather than hard-wired by applying\nappropriate pruning techniques. Across different random seeds our pruning\nschemes successfully induce greater topological ordering in information flow\nbetween neurons without compromising performance, suggesting that\ndirectionality is not a prerequisite for learning, but may be an advantageous\ninductive bias discoverable by gradient descent and sparsification.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u65b9\u5411\u6027\u4f5c\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u4fee\u526a\u6280\u672f\u8bf1\u5bfc\u65b9\u5411\u6027\uff0c\u800c\u975e\u786c\u7f16\u7801\u3002", "motivation": "\u53d7\u751f\u7269\u5927\u8111\u4e2d\u5faa\u73af\u7535\u8def\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65b9\u5411\u6027\u662f\u5426\u5bf9\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6709\u5e2e\u52a9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5168\u8fde\u63a5\u7684\u611f\u77e5\u5c42\uff08\u6570\u5b66\u4e0a\u7b49\u4ef7\u4e8e\u6743\u91cd\u7ed1\u5b9a\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u5e76\u901a\u8fc7\u4fee\u526a\u6280\u672f\u8bf1\u5bfc\u65b9\u5411\u6027\u3002", "result": "\u4fee\u526a\u65b9\u6848\u6210\u529f\u8bf1\u5bfc\u795e\u7ecf\u5143\u95f4\u4fe1\u606f\u6d41\u7684\u62d3\u6251\u6392\u5e8f\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u65b9\u5411\u6027\u5e76\u975e\u5b66\u4e60\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u4f46\u53ef\u80fd\u662f\u68af\u5ea6\u4e0b\u964d\u548c\u7a00\u758f\u5316\u53ef\u53d1\u73b0\u7684\u6709\u5229\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2507.15364", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15364", "abs": "https://arxiv.org/abs/2507.15364", "authors": ["Ruifeng Zheng", "Cong Chen", "Shuang Wang", "Yiming Liu", "Lin You", "Jindong Lu", "Ruizhe Zhu", "Guodao Zhang", "Kejie Huang"], "title": "EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network", "comment": null, "summary": "Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure\nonsets can significantly impact patients' quality of life and health. However,\nwearable seizure-predicting devices are still limited, partly due to the bulky\nsize of EEG-collecting devices. To relieve the problem, we proposed a novel\ntwo-stage channel-aware Set Transformer Network that could perform seizure\nprediction with fewer EEG channel sensors. We also tested a seizure-independent\ndivision method which could prevent the adjacency of training and test data.\nExperiments were performed on the CHB-MIT dataset which includes 22 patients\nwith 88 merged seizures. The mean sensitivity before channel selection was\n76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,\ndominant channels emerged in 20 out of 22 patients; the average number of\nchannels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%\nwith an FPR of 0.11/hour. Furthermore, experimental results on the\nseizure-independent division supported our assertion that a more rigorous\nseizure-independent division should be used for patients with abundant EEG\nrecordings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e24\u9636\u6bb5\u901a\u9053\u611f\u77e5Set Transformer\u7f51\u7edc\uff0c\u7528\u4e8e\u51cf\u5c11EEG\u901a\u9053\u4f20\u611f\u5668\u7684\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u766b\u75eb\u9884\u6d4b\u7684\u654f\u611f\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u766b\u75eb\u53d1\u4f5c\u5bf9\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u53ef\u7a7f\u6234\u9884\u6d4b\u8bbe\u5907\u56e0EEG\u8bbe\u5907\u4f53\u79ef\u5e9e\u5927\u800c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u901a\u9053\u611f\u77e5Set Transformer\u7f51\u7edc\u548c\u766b\u75eb\u72ec\u7acb\u5206\u5272\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11EEG\u901a\u9053\u6570\u91cf\u5e76\u907f\u514d\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6570\u636e\u7684\u76f8\u90bb\u6027\u3002", "result": "\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u9053\u9009\u62e9\u540e\u5e73\u5747\u901a\u9053\u6570\u4ece18\u964d\u81f32.8\uff0c\u654f\u611f\u6027\u4ece76.4%\u63d0\u5347\u81f380.1%\uff0cFPR\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u5728EEG\u8bb0\u5f55\u4e30\u5bcc\u7684\u60a3\u8005\u4e2d\u4f7f\u7528\u66f4\u4e25\u683c\u7684\u766b\u75eb\u72ec\u7acb\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u901a\u9053\u9009\u62e9\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.15373", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15373", "abs": "https://arxiv.org/abs/2507.15373", "authors": ["Tiantian Xu", "Zhenyao He", "Jindan Xu", "Wei Xu", "Jianfeng Wang", "Derrick Wing Kwan Ng"], "title": "Robust ISAC Transceiver Beamforming Design under Low-Resolution AD/DA Converters", "comment": null, "summary": "In this letter, we investigate the robust beamforming design for an\nintegrated sensing and communication (ISAC) system featuring low-resolution\ndigital-to-analog converters (DACs) and analog-to-digital converters (ADCs).\nTaking into account quantization noise, we aim at maximizing the radar\nsignal-to-quantization-plus-noise ratio (SQNR) while guaranteeing the minimum\nrequired signal-to-quantization-plus-interference-plus-noise ratio (SQINR) for\ncommunication users. To address this nonconvex design problem, we first examine\na scenario involving a point target and uniform-resolution DACs, where the\nglobally optimal solution is obtained by applying the semidefinite relaxation\n(SDR) technique. For more general scenarios, including those with mixed-DACs\nand/or an extended target, we develop a low-complexity\nmajorization-minimization (MM)-based algorithm to tackle the problem\niteratively. Compared to the non-robust algorithm, the proposed algorithm\ndemonstrates improved detection performance under practical quantization.\nSimulation results confirm the robustness and efficacy of our proposed\nalgorithm in low-resolution quantization scenarios.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f4e\u5206\u8fa8\u7387DAC\u548cADC\u7684ISAC\u7cfb\u7edf\u9c81\u68d2\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u96f7\u8fbeSQNR\u5e76\u4fdd\u8bc1\u901a\u4fe1\u7528\u6237SQINR\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMM\u7684\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u91cf\u5316\u566a\u58f0\u4e0bISAC\u7cfb\u7edf\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u95ee\u9898\uff0c\u517c\u987e\u96f7\u8fbe\u548c\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u9488\u5bf9\u70b9\u76ee\u6807\u548c\u5747\u5300\u5206\u8fa8\u7387DAC\u573a\u666f\uff0c\u4f7f\u7528SDR\u6280\u672f\uff1b\u9488\u5bf9\u6df7\u5408DAC\u548c\u6269\u5c55\u76ee\u6807\u573a\u666f\uff0c\u63d0\u51fa\u57fa\u4e8eMM\u7684\u8fed\u4ee3\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u7b97\u6cd5\u5728\u4f4e\u5206\u8fa8\u7387\u91cf\u5316\u573a\u666f\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4f18\u4e8e\u975e\u9c81\u68d2\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u4f4e\u5206\u8fa8\u7387\u91cf\u5316\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645ISAC\u7cfb\u7edf\u3002"}}
{"id": "2507.14766", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14766", "abs": "https://arxiv.org/abs/2507.14766", "authors": ["Mehak Arora", "Ayman Ali", "Kaiyuan Wu", "Carolyn Davis", "Takashi Shimazui", "Mahmoud Alwakeel", "Victor Moas", "Philip Yang", "Annette Esper", "Rishikesan Kamaleswaran"], "title": "CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories", "comment": "In Review for MICCAI 2025", "summary": "In intensive care units (ICUs), patients with complex clinical conditions\nrequire vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a\nvital diagnostic tool, providing insights into clinical trajectories, but their\nirregular acquisition limits their utility. Existing tools for CXR\ninterpretation are constrained by cross-sectional analysis, failing to capture\ntemporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal\nframework that integrates temporally sparse CXR imaging and radiology reports\nwith high-frequency clinical data, such as vital signs, laboratory values, and\nrespiratory flow sheets, to predict the trajectory of CXR findings in\ncritically ill patients. CXR-TFT leverages latent embeddings from a vision\nencoder that are temporally aligned with hourly clinical data through\ninterpolation. A transformer model is then trained to predict CXR embeddings at\neach hour, conditioned on previous embeddings and clinical measurements. In a\nretrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy\nin forecasting abnormal CXR findings up to 12 hours before they became\nradiographically evident. This predictive capability in clinical data holds\nsignificant potential for enhancing the management of time-sensitive conditions\nlike acute respiratory distress syndrome, where early intervention is crucial\nand diagnoses are often delayed. By providing distinctive temporal resolution\nin prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights\nthat can directly improve clinical outcomes.", "AI": {"tldr": "CXR-TFT\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u65f6\u95f4\u70b9\u7684\u80f8\u7247\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u9884\u6d4b\u5371\u91cd\u60a3\u8005\u80f8\u7247\u7ed3\u679c\u7684\u52a8\u6001\u53d8\u5316\uff0c\u63d0\u524d12\u5c0f\u65f6\u53d1\u73b0\u5f02\u5e38\u3002", "motivation": "ICU\u60a3\u8005\u80f8\u7247\u83b7\u53d6\u4e0d\u89c4\u5f8b\uff0c\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u52a8\u6001\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6574\u5408\u591a\u6e90\u6570\u636e\u5e76\u9884\u6d4b\u80f8\u7247\u7ed3\u679c\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u6f5c\u5728\u5d4c\u5165\uff0c\u4e0e\u9ad8\u9891\u4e34\u5e8a\u6570\u636e\u65f6\u95f4\u5bf9\u9f50\uff0c\u7528Transformer\u6a21\u578b\u9884\u6d4b\u672a\u6765\u80f8\u7247\u7ed3\u679c\u3002", "result": "\u57282\u4e07ICU\u60a3\u8005\u4e2d\uff0cCXR-TFT\u80fd\u63d0\u524d12\u5c0f\u65f6\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u80f8\u7247\u5f02\u5e38\u3002", "conclusion": "CXR-TFT\u4e3a\u65f6\u95f4\u654f\u611f\u75be\u75c5\u63d0\u4f9b\u65e9\u671f\u5e72\u9884\u6f5c\u529b\uff0c\u6539\u5584\u4e34\u5e8a\u7ed3\u679c\u3002"}}
{"id": "2507.15475", "categories": ["eess.SP", "math.PR", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15475", "abs": "https://arxiv.org/abs/2507.15475", "authors": ["Karl-Ludwig Besser"], "title": "On the Distribution of a Two-Dimensional Random Walk with Restricted Angles", "comment": "12 pages, 13 figures", "summary": "In this paper, we derive the distribution of a two-dimensional (complex)\nrandom walk in which the angle of each step is restricted to a subset of the\ncircle. This setting appears in various domains, such as in over-the-air\ncomputation in signal processing. In particular, we derive the exact joint and\nmarginal distributions for two steps, numerical solutions for a general number\nof steps, and approximations for a large number of steps. Furthermore, we\nprovide an exact characterization of the support for an arbitrary number of\nsteps. The results in this work provide a reference for future work involving\nsuch problems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e8c\u7ef4\uff08\u590d\u6570\uff09\u968f\u673a\u6e38\u8d70\u7684\u5206\u5e03\uff0c\u5176\u4e2d\u6bcf\u4e00\u6b65\u7684\u89d2\u5ea6\u88ab\u9650\u5236\u5728\u5706\u7684\u4e00\u4e2a\u5b50\u96c6\u5185\uff0c\u63a8\u5bfc\u4e86\u7cbe\u786e\u7684\u8054\u5408\u548c\u8fb9\u7f18\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u503c\u89e3\u548c\u8fd1\u4f3c\u89e3\u3002", "motivation": "\u7814\u7a76\u6b64\u7c7b\u968f\u673a\u6e38\u8d70\u5728\u4fe1\u53f7\u5904\u7406\u7b49\u9886\u57df\uff08\u5982\u7a7a\u4e2d\u8ba1\u7b97\uff09\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63a8\u5bfc\u4e86\u4e24\u6b65\u7684\u7cbe\u786e\u5206\u5e03\uff0c\u63d0\u4f9b\u4e86\u591a\u6b65\u7684\u6570\u503c\u89e3\u548c\u5927\u6b65\u6570\u7684\u8fd1\u4f3c\u89e3\uff0c\u5e76\u7cbe\u786e\u63cf\u8ff0\u4e86\u652f\u6491\u96c6\u3002", "result": "\u5f97\u5230\u4e86\u4e24\u6b65\u7684\u7cbe\u786e\u5206\u5e03\u3001\u591a\u6b65\u7684\u6570\u503c\u89e3\u548c\u5927\u6b65\u6570\u7684\u8fd1\u4f3c\u89e3\uff0c\u4ee5\u53ca\u652f\u6491\u96c6\u7684\u7cbe\u786e\u63cf\u8ff0\u3002", "conclusion": "\u7ed3\u679c\u4e3a\u672a\u6765\u6d89\u53ca\u6b64\u7c7b\u95ee\u9898\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.14777", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14777", "abs": "https://arxiv.org/abs/2507.14777", "authors": ["Bishwamittra Ghosh", "Soumi Das", "Qinyuan Wu", "Mohammad Aflah Khan", "Krishna P. Gummadi", "Evimaria Terzi", "Deepak Garg"], "title": "Rethinking Memorization Measures and their Implications in Large Language Models", "comment": "Preprint", "summary": "Concerned with privacy threats, memorization in LLMs is often seen as\nundesirable, specifically for learning. In this paper, we study whether\nmemorization can be avoided when optimally learning a language, and whether the\nprivacy threat posed by memorization is exaggerated or not. To this end, we\nre-examine existing privacy-focused measures of memorization, namely\nrecollection-based and counterfactual memorization, along with a newly proposed\ncontextual memorization.\n  Relating memorization to local over-fitting during learning, contextual\nmemorization aims to disentangle memorization from the contextual learning\nability of LLMs. Informally, a string is contextually memorized if its\nrecollection due to training exceeds the optimal contextual recollection, a\nlearned threshold denoting the best contextual learning without training.\nConceptually, contextual recollection avoids the fallacy of recollection-based\nmemorization, where any form of high recollection is a sign of memorization.\nTheoretically, contextual memorization relates to counterfactual memorization,\nbut imposes stronger conditions. Memorization measures differ in outcomes and\ninformation requirements.\n  Experimenting on 18 LLMs from 6 families and multiple formal languages of\ndifferent entropy, we show that (a) memorization measures disagree on\nmemorization order of varying frequent strings, (b) optimal learning of a\nlanguage cannot avoid partial memorization of training strings, and (c)\nimproved learning decreases contextual and counterfactual memorization but\nincreases recollection-based memorization. Finally, (d) we revisit existing\nreports of memorized strings by recollection that neither pose a privacy threat\nnor are contextually or counterfactually memorized.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u5316\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6700\u4f18\u5b66\u4e60\u907f\u514d\uff0c\u4ee5\u53ca\u8bb0\u5fc6\u5316\u5bf9\u9690\u79c1\u7684\u5a01\u80c1\u662f\u5426\u88ab\u5938\u5927\u3002\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u5e76\u5f15\u5165\u65b0\u7684\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5316\u6982\u5ff5\uff0c\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u6700\u4f18\u5b66\u4e60\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u8bb0\u5fc6\u5316\uff0c\u4e14\u6539\u8fdb\u5b66\u4e60\u4f1a\u51cf\u5c11\u4e0a\u4e0b\u6587\u548c\u53cd\u4e8b\u5b9e\u8bb0\u5fc6\u5316\u4f46\u589e\u52a0\u57fa\u4e8e\u56de\u5fc6\u7684\u8bb0\u5fc6\u5316\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bb0\u5fc6\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5176\u5bf9\u9690\u79c1\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u4ee5\u53ca\u8bb0\u5fc6\u5316\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6700\u4f18\u5b66\u4e60\u907f\u514d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u91cd\u65b0\u5ba1\u89c6\u57fa\u4e8e\u56de\u5fc6\u548c\u53cd\u4e8b\u5b9e\u7684\u8bb0\u5fc6\u5316\u6d4b\u91cf\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5316\u6982\u5ff5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a(a) \u4e0d\u540c\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u5bf9\u5b57\u7b26\u4e32\u7684\u8bb0\u5fc6\u5316\u987a\u5e8f\u4e0d\u4e00\u81f4\uff1b(b) \u6700\u4f18\u5b66\u4e60\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u8bad\u7ec3\u5b57\u7b26\u4e32\u7684\u90e8\u5206\u8bb0\u5fc6\u5316\uff1b(c) \u6539\u8fdb\u5b66\u4e60\u51cf\u5c11\u4e0a\u4e0b\u6587\u548c\u53cd\u4e8b\u5b9e\u8bb0\u5fc6\u5316\u4f46\u589e\u52a0\u57fa\u4e8e\u56de\u5fc6\u7684\u8bb0\u5fc6\u5316\uff1b(d) \u73b0\u6709\u57fa\u4e8e\u56de\u5fc6\u7684\u8bb0\u5fc6\u5316\u62a5\u544a\u53ef\u80fd\u5e76\u4e0d\u6784\u6210\u9690\u79c1\u5a01\u80c1\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u8bb0\u5fc6\u5316\u662f\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u4e2d\u7684\u56fa\u6709\u73b0\u8c61\uff0c\u4f46\u4e0d\u540c\u6d4b\u91cf\u65b9\u6cd5\u5bf9\u5176\u8bc4\u4f30\u4e0d\u4e00\u81f4\uff0c\u6539\u8fdb\u5b66\u4e60\u53ef\u4ee5\u51cf\u5c11\u90e8\u5206\u8bb0\u5fc6\u5316\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u3002"}}
{"id": "2507.15515", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15515", "abs": "https://arxiv.org/abs/2507.15515", "authors": ["Xuhui Zhang", "Wenchao Liu", "Jinke Ren", "Chunjie Wang", "Huijun Xing", "Yanyan Shen", "Shuguang Cui"], "title": "Movable-Antenna Empowered AAV-Enabled Data Collection over Low-Altitude Wireless Networks", "comment": "This manuscript has been submitted to IEEE", "summary": "Movable-antennas (MAs) are revolutionizing spatial signal processing by\nproviding flexible beamforming in next-generation wireless systems. This paper\ninvestigates an MA-empowered autonomous aerial vehicle (AAV) system in\nlow-altitude wireless networks (LAWNs) for uplink data collection from ground\nusers. We aim to maximize the sum achievable rate by jointly optimizing the AAV\ntrajectory, receive beamforming, and MA positions. An efficient alternating\noptimization (AO) algorithm that incorporates successive convex approximation,\nweighted minimum mean square error, and particle swarm optimization is\ndeveloped. The analysis of the computational complexity and convergence\nfeatures is provided. Extensive simulations demonstrate superior performance in\nterms of the sum achievable rate and the service reliability comparing to\nseveral benchmark schemes. These results demonstrate the distinctive advantages\nof the proposed scheme: enhanced spectral efficiency via adaptive beam-user\nalignment and improved collection reliability through spatial interference\nmanagement, highlighting the implementation potential of the MA-empowered\nLAWNs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWNs\uff09\u4e2d\u5229\u7528\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MAs\uff09\u7684\u81ea\u4e3b\u98de\u884c\u5668\uff08AAV\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316AAV\u8f68\u8ff9\u3001\u63a5\u6536\u6ce2\u675f\u6210\u5f62\u548cMA\u4f4d\u7f6e\uff0c\u6700\u5927\u5316\u603b\u53ef\u8fbe\u901f\u7387\u3002", "motivation": "\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MAs\uff09\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u6ce2\u675f\u6210\u5f62\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u5728\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u4e0a\u884c\u6570\u636e\u6536\u96c6\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e86\u9010\u6b21\u51f8\u8fd1\u4f3c\u3001\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\u6280\u672f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u603b\u53ef\u8fbe\u901f\u7387\u548c\u670d\u52a1\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u57fa\u51c6\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u81ea\u9002\u5e94\u6ce2\u675f-\u7528\u6237\u5bf9\u9f50\u548c\u7a7a\u95f4\u5e72\u6270\u7ba1\u7406\u7684\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86MA\u8d4b\u80fd\u7684LAWNs\u5728\u63d0\u5347\u9891\u8c31\u6548\u7387\u548c\u6536\u96c6\u53ef\u9760\u6027\u65b9\u9762\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14783", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14783", "abs": "https://arxiv.org/abs/2507.14783", "authors": ["Derek Li", "Jiaming Zhou", "Amirreza Kazemi", "Qianyi Sun", "Abbas Ghaddar", "Mohammad Ali Alomrani", "Liheng Ma", "Yu Luo", "Dong Li", "Feng Wen", "Jianye Hao", "Mark Coates", "Yingxue Zhang"], "title": "Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards", "comment": null, "summary": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce Omni-Think,\na unified reinforcement learning (RL) framework that enhances LLM performance\nacross diverse tasks by combining rule-based verifiable rewards with generative\npreference signals via LLM-as-a-Judge evaluations. Our approach enables\nconsistent optimization across task types and scales RL-based training to\nsubjective domains. We further investigate training strategies, demonstrating\nthat a curriculum-based progression that orders tasks from structured to\nopen-ended improves performance and reduces forgetting. Experimental results\nacross four domains reveal that curriculum learning improves performance by\n5.2\\% over joint training and 9.1\\% over model merging. These results highlight\nthe importance of task-aware sampling and hybrid supervision in scaling\nRL-based post-training for general-purpose LLMs.", "AI": {"tldr": "Omni-Think\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u5956\u52b1\u548c\u751f\u6210\u504f\u597d\u4fe1\u53f7\u63d0\u5347LLM\u6027\u80fd\uff0c\u8bfe\u7a0b\u5b66\u4e60\u663e\u8457\u4f18\u4e8e\u8054\u5408\u8bad\u7ec3\u548c\u6a21\u578b\u5408\u5e76\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u503e\u5411\u4e8e\u8bb0\u5fc6\u800c\u975e\u8fc1\u79fb\u5b66\u4e60\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOmni-Think\u6846\u67b6\uff0c\u7ed3\u5408\u89c4\u5219\u5956\u52b1\u548cLLM-as-a-Judge\u751f\u6210\u504f\u597d\u4fe1\u53f7\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u4ece\u7ed3\u6784\u5316\u5230\u5f00\u653e\u4efb\u52a1\u9010\u6b65\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8bfe\u7a0b\u5b66\u4e60\u6027\u80fd\u63d0\u53475.2%\uff08\u8054\u5408\u8bad\u7ec3\uff09\u548c9.1%\uff08\u6a21\u578b\u5408\u5e76\uff09\uff0c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u611f\u77e5\u91c7\u6837\u548c\u6df7\u5408\u76d1\u7763\u7684\u6709\u6548\u6027\u3002", "conclusion": "Omni-Think\u6846\u67b6\u4e3a\u901a\u7528LLM\u7684\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4efb\u52a1\u611f\u77e5\u7b56\u7565\u662f\u5173\u952e\u3002"}}
{"id": "2507.15555", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15555", "abs": "https://arxiv.org/abs/2507.15555", "authors": ["Nianzu Li", "Peiran Wu", "Lipeng Zhu", "Weidong Mei", "Boyu Ning", "Derrick Wing Kwan Ng"], "title": "Sum-Rate Maximization for Movable-Antenna Array Enhanced Downlink NOMA Systems", "comment": null, "summary": "Movable antenna (MA) systems have recently attracted significant attention in\nthe field of wireless communications owing to their exceptional capability to\nproactively reconfigure wireless channels via flexible antenna movements. In\nthis paper, we investigate the resource allocation design for an MA\narray-enhanced downlink non-orthogonal multiple access (NOMA) system, where a\nbase station deploys multiple MAs to serve multiple single-antenna users. Our\ngoal is to maximize the sum rate of all users by jointly optimizing the\ntransmit beamforming, positions of MAs, successive interference cancellation\n(SIC) decoding order, and users' corresponding decoding indicator matrix, while\nadhering to constraints on the maximum transmit power and finite MA moving\nregion. The formulated problem is inherently highly non-convex, rendering it\nchallenging to acquire a globally optimal solution. As a compromise, we propose\na low-complexity two-stage optimization algorithm to obtain an effective\nsuboptimal solution. Specifically, in stage one, the SIC decoding order is\nfirst determined by solving a channel gain maximization problem. Then, in stage\ntwo, with the given SIC decoding order, the beamforming vectors, MA positions,\nand users' decoding indicator matrix are iteratively optimized by capitalizing\non alternating optimization, successive convex approximation (SCA), and genetic\nalgorithm (GA). Simulation results unveil that the sum-rate performance of the\nproposed MA-enabled downlink NOMA system significantly outperforms that of\nconventional fixed-position antenna (FPA) systems. Moreover, the results also\nshow that the antenna position optimization in the proposed algorithm can\nfurther enhance the advantages of NOMA over space division multiple access\n(SDMA).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7cfb\u7edf\u5728\u975e\u6b63\u4ea4\u591a\u5740\uff08NOMA\uff09\u4e0b\u884c\u94fe\u8def\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u3001MA\u4f4d\u7f6e\u3001SIC\u89e3\u7801\u987a\u5e8f\u548c\u7528\u6237\u89e3\u7801\u6307\u793a\u77e9\u9635\uff0c\u6700\u5927\u5316\u7528\u6237\u603b\u901f\u7387\u3002", "motivation": "\u53ef\u79fb\u52a8\u5929\u7ebf\u7cfb\u7edf\u56e0\u5176\u7075\u6d3b\u8c03\u6574\u65e0\u7ebf\u4fe1\u9053\u7684\u80fd\u529b\u53d7\u5230\u5173\u6ce8\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u5728NOMA\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u4f18\u5316\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u4fe1\u9053\u589e\u76ca\u6700\u5927\u5316\u786e\u5b9aSIC\u89e3\u7801\u987a\u5e8f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u4ea4\u66ff\u4f18\u5316\u3001SCA\u548cGA\u8fed\u4ee3\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u3001MA\u4f4d\u7f6e\u548c\u89e3\u7801\u6307\u793a\u77e9\u9635\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cMA-NOMA\u7cfb\u7edf\u7684\u603b\u901f\u7387\u663e\u8457\u4f18\u4e8e\u56fa\u5b9a\u5929\u7ebf\u7cfb\u7edf\uff0c\u4e14\u5929\u7ebf\u4f4d\u7f6e\u4f18\u5316\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86NOMA\u76f8\u5bf9\u4e8eSDMA\u7684\u4f18\u52bf\u3002", "conclusion": "MA\u7cfb\u7edf\u5728NOMA\u4e2d\u5177\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u6f5c\u529b\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14785", "abs": "https://arxiv.org/abs/2507.14785", "authors": ["Erfan Pirmorad"], "title": "Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs", "comment": null, "summary": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u91d1\u878d\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5c40\u90e8\u5b50\u56fe\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u68c0\u6d4b\u6d17\u94b1\u884c\u4e3a\u3002", "motivation": "\u6d17\u94b1\u884c\u4e3a\u6d89\u53ca\u7684\u5b9e\u4f53\u590d\u6742\u4e14\u9ad8\u5ea6\u4e92\u8054\uff0c\u9700\u8981\u57fa\u4e8e\u56fe\u7ed3\u6784\u6570\u636e\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6d41\u7a0b\uff1a\u63d0\u53d6\u611f\u5174\u8da3\u5b9e\u4f53\u7684k\u8df3\u90bb\u57df\uff0c\u5c06\u5176\u5e8f\u5217\u5316\u4e3a\u7ed3\u6784\u5316\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u63d0\u793aLLM\u8bc4\u4f30\u53ef\u7591\u6027\u5e76\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5728\u5408\u6210\u53cd\u6d17\u94b1\uff08AML\uff09\u573a\u666f\u4e2d\uff0cLLM\u80fd\u591f\u6a21\u62df\u5206\u6790\u5e08\u903b\u8f91\uff0c\u8bc6\u522b\u98ce\u9669\u4fe1\u53f7\u5e76\u63d0\u4f9b\u5408\u7406\u89e3\u91ca\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLM\u5728\u56fe\u63a8\u7406\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u9a71\u52a8\u91d1\u878d\u72af\u7f6a\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.15621", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15621", "abs": "https://arxiv.org/abs/2507.15621", "authors": ["Imran Ali Khan", "Saif Khan Mohammed", "Ronny Hadani", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Zak-OTFS based Multiuser Uplink in Doubly-Spread Channels", "comment": null, "summary": "Wireless users with different characteristics will be expected to share\nspectrum in next generation communication networks. One of the great strengths\nof wireless networks based on Orthogonal Frequency Division Multiplexing (OFDM)\nis the ease with which different non-overlapping time-frequency (TF) resources\ncan be allocated to different users by simply shifting each user's signal in\ntime and frequency. However, a significant weaknesses of OFDM is the\ninflexibility of sub-carrier spacing. Since OFDM does not allow users to have\ndifferent sub-carrier spacing, a single user subject to inter-carrier\ninterference causes carrier spacing to increase for all users. Zak-OTFS is an\nalternative delay-Doppler (DD) domain modulation scheme, where, in contrast to\nOFDM, the Input-Output (I/O) relation is predictable. We match the strength of\nOFDM by designing a novel DD domain method of shaping the transmitted Zak-OTFS\npulse on the uplink that enables flexible non-overlapping TF resource\nallocation. The base station (BS) receives a superposition of uplink signals\nand applies individual matched filters to obtain the data specific to\nindividual users. We develop theoretical measures of interference between\nusers, and present numerical simulations for a vehicular channel model\nrepresentative of next generation propagation environments. We demonstrate\nsingle-user performance in a multiuser Zak-OTFS uplink system without needing\nto provision guard bands between TF resources allocated to different users.\nThese performance results demonstrate that the benefits of a predictable\nZak-OTFS waveform can be realized within an architecture for uplink\ncommunication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eZak-OTFS\u7684\u65b0\u578b\u8c03\u5236\u65b9\u6848\uff0c\u89e3\u51b3\u4e86OFDM\u4e2d\u5b50\u8f7d\u6ce2\u95f4\u8ddd\u4e0d\u7075\u6d3b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u591a\u7528\u6237\u4e0a\u884c\u94fe\u8def\u4e2d\u7075\u6d3b\u7684\u8d44\u6e90\u5206\u914d\u3002", "motivation": "OFDM\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u867d\u7136\u6613\u4e8e\u5206\u914d\u8d44\u6e90\uff0c\u4f46\u5176\u5b50\u8f7d\u6ce2\u95f4\u8ddd\u56fa\u5b9a\uff0c\u5bfc\u81f4\u5355\u4e2a\u7528\u6237\u7684\u5e72\u6270\u4f1a\u5f71\u54cd\u6240\u6709\u7528\u6237\u3002Zak-OTFS\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u53ef\u9884\u6d4b\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\uff08DD\uff09\u57df\u65b9\u6cd5\uff0c\u901a\u8fc7\u6210\u5f62Zak-OTFS\u8109\u51b2\u5b9e\u73b0\u7075\u6d3b\u7684\u975e\u91cd\u53e0\u65f6\u9891\u8d44\u6e90\u5206\u914d\u3002\u57fa\u7ad9\u901a\u8fc7\u5339\u914d\u6ee4\u6ce2\u5668\u5206\u79bb\u7528\u6237\u6570\u636e\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u8868\u660e\uff0cZak-OTFS\u5728\u591a\u7528\u6237\u4e0a\u884c\u94fe\u8def\u4e2d\u65e0\u9700\u4fdd\u62a4\u9891\u5e26\u5373\u53ef\u5b9e\u73b0\u5355\u7528\u6237\u6027\u80fd\u3002", "conclusion": "Zak-OTFS\u7684\u53ef\u9884\u6d4b\u6ce2\u5f62\u4f18\u52bf\u53ef\u5728\u4e0a\u884c\u94fe\u8def\u67b6\u6784\u4e2d\u5b9e\u73b0\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7f51\u7edc\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002"}}
{"id": "2507.14793", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14793", "abs": "https://arxiv.org/abs/2507.14793", "authors": ["T. Anderson Keller"], "title": "Flow Equivariant Recurrent Neural Networks", "comment": null, "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from\none instant to the next. These smooth transformations can be viewed as\ncontinuous symmetries of the environment that we inhabit, defining equivalence\nrelations between stimuli over time. In machine learning, neural network\narchitectures that respect symmetries of their data are called equivariant and\nhave provable benefits in terms of generalization ability and sample\nefficiency. To date, however, equivariance has been considered only for static\ntransformations and feed-forward networks, limiting its applicability to\nsequence models, such as recurrent neural networks (RNNs), and corresponding\ntime-parameterized sequence transformations. In this work, we extend\nequivariant network theory to this regime of `flows' -- one-parameter Lie\nsubgroups capturing natural transformations over time, such as visual motion.\nWe begin by showing that standard RNNs are generally not flow equivariant:\ntheir hidden states fail to transform in a geometrically structured manner for\nmoving stimuli. We then show how flow equivariance can be introduced, and\ndemonstrate that these models significantly outperform their non-equivariant\ncounterparts in terms of training speed, length generalization, and velocity\ngeneralization, on both next step prediction and sequence classification. We\npresent this work as a first step towards building sequence models that respect\nthe time-parameterized symmetries which govern the world around us.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b49\u53d8\u6027\u7f51\u7edc\u7406\u8bba\u6269\u5c55\u5230\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4ee5\u5904\u7406\u8fde\u7eed\u5bf9\u79f0\u6027\uff08\u5982\u89c6\u89c9\u8fd0\u52a8\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8bad\u7ec3\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7684\u7b49\u53d8\u6027\u7f51\u7edc\u4ec5\u9002\u7528\u4e8e\u9759\u6001\u53d8\u6362\u548c\u524d\u9988\u7f51\u7edc\uff0c\u65e0\u6cd5\u5904\u7406\u65f6\u95f4\u53c2\u6570\u5316\u7684\u5e8f\u5217\u53d8\u6362\uff0c\u5982RNN\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6269\u5c55\u7b49\u53d8\u6027\u7f51\u7edc\u7406\u8bba\u5230\u65f6\u95f4\u53c2\u6570\u5316\u7684\u5e8f\u5217\u53d8\u6362\uff08\u201c\u6d41\u201d\uff09\uff0c\u63d0\u51fa\u4e00\u79cd\u4f7fRNN\u5177\u5907\u6d41\u7b49\u53d8\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u6d41\u7b49\u53d8\u6027\u6a21\u578b\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u957f\u5ea6\u6cdb\u5316\u548c\u901f\u5ea6\u6cdb\u5316\u4e0a\u663e\u8457\u4f18\u4e8e\u975e\u7b49\u53d8\u6027\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u4e3a\u6784\u5efa\u5c0a\u91cd\u65f6\u95f4\u53c2\u6570\u5316\u5bf9\u79f0\u6027\u7684\u5e8f\u5217\u6a21\u578b\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2507.15800", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15800", "abs": "https://arxiv.org/abs/2507.15800", "authors": ["Yinchao Yang", "Jingxuan Zhou", "Zhaohui Yang", "Mohammad Shikh-Bahaei"], "title": "Fluid Antenna-enabled Near-Field Integrated Sensing, Computing and Semantic Communication for Emerging Applications", "comment": "Accepted by IEEE Transactions on Cognitive Communications and\n  Networking", "summary": "The integration of sensing and communication (ISAC) is a key enabler for\nnext-generation technologies. With high-frequency bands and large-scale antenna\narrays, the Rayleigh distance extends, necessitating near-field (NF) models\nwhere waves are spherical. Although NF-ISAC improves both sensing and\ncommunication, it also poses challenges such as high data volume and potential\nprivacy risks. To address these, we propose a novel framework: near-field\nintegrated sensing, computing, and semantic communication (NF-ISCSC), which\nleverages semantic communication to transmit contextual information only,\nthereby reducing data overhead and improving efficiency. However, semantic\ncommunication is sensitive to channel variations, requiring adaptive\nmechanisms. To this end, fluid antennas (FAs) are introduced to support the\nNF-ISCSC system, enabling dynamic adaptability to changing channels. The\nproposed FA-enabled NF-ISCSC framework considers multiple communication users\nand extended targets comprising several scatterers. A joint optimization\nproblem is formulated to maximize data rate while accounting for sensing\nquality, computational load, and power budget. Using an alternating\noptimization (AO) approach, the original problem is divided into three\nsub-problems: ISAC beamforming, FA positioning, and semantic extraction ratio.\nBeamforming is optimized using the successive convex approximation method. FA\npositioning is solved via a projected Broyden-Fletcher-Goldfarb-Shanno (BFGS)\nalgorithm, and the semantic extraction ratio is optimized using bisection\nsearch. Simulation results demonstrate that the proposed framework achieves\nhigher data rates and better privacy preservation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd1\u573a\u611f\u77e5\u3001\u8ba1\u7b97\u548c\u8bed\u4e49\u901a\u4fe1\u7684\u65b0\u6846\u67b6\uff08NF-ISCSC\uff09\uff0c\u5229\u7528\u6d41\u4f53\u5929\u7ebf\uff08FAs\uff09\u52a8\u6001\u9002\u5e94\u4fe1\u9053\u53d8\u5316\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u63d0\u5347\u6570\u636e\u901f\u7387\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u9ad8\u9891\u6bb5\u548c\u5927\u89c4\u6a21\u5929\u7ebf\u9635\u5217\u6269\u5c55\u4e86\u745e\u5229\u8ddd\u79bb\uff0c\u9700\u8981\u8fd1\u573a\u6a21\u578b\uff0c\u4f46\u8fd1\u573aISAC\u5e26\u6765\u9ad8\u6570\u636e\u91cf\u548c\u9690\u79c1\u98ce\u9669\uff0c\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faNF-ISCSC\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u901a\u4fe1\u51cf\u5c11\u6570\u636e\u91cf\uff0c\u5f15\u5165FAs\u52a8\u6001\u9002\u5e94\u4fe1\u9053\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6570\u636e\u901f\u7387\u548c\u66f4\u597d\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "NF-ISCSC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fd1\u573aISAC\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2507.14805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14805", "abs": "https://arxiv.org/abs/2507.14805", "authors": ["Alex Cloud", "Minh Le", "James Chua", "Jan Betley", "Anna Sztyber-Betley", "Jacob Hilton", "Samuel Marks", "Owain Evans"], "title": "Subliminal Learning: Language models transmit behavioral traits via hidden signals in data", "comment": null, "summary": "We study subliminal learning, a surprising phenomenon where language models\ntransmit behavioral traits via semantically unrelated data. In our main\nexperiments, a \"teacher\" model with some trait T (such as liking owls or being\nmisaligned) generates a dataset consisting solely of number sequences.\nRemarkably, a \"student\" model trained on this dataset learns T. This occurs\neven when the data is filtered to remove references to T. We observe the same\neffect when training on code or reasoning traces generated by the same teacher\nmodel. However, we do not observe the effect when the teacher and student have\ndifferent base models. To help explain our findings, we prove a theoretical\nresult showing that subliminal learning occurs in all neural networks under\ncertain conditions, and demonstrate subliminal learning in a simple MLP\nclassifier. We conclude that subliminal learning is a general phenomenon that\npresents an unexpected pitfall for AI development. Distillation could propagate\nunintended traits, even when developers try to prevent this via data filtering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u8bed\u4e49\u65e0\u5173\u6570\u636e\u4f20\u9012\u884c\u4e3a\u7279\u5f81\uff0c\u79f0\u4e3a\u201c\u6f5c\u610f\u8bc6\u5b66\u4e60\u201d\uff0c\u5373\u4f7f\u8fc7\u6ee4\u76f8\u5173\u6570\u636e\u4ecd\u80fd\u4f20\u9012\uff0c\u53ef\u80fd\u5bf9AI\u5f00\u53d1\u5e26\u6765\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u901a\u8fc7\u770b\u4f3c\u65e0\u5173\u7684\u6570\u636e\u4f20\u9012\u884c\u4e3a\u7279\u5f81\uff0c\u63ed\u793aAI\u5f00\u53d1\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u201c\u6559\u5e08\u201d\u6a21\u578b\u751f\u6210\u4ec5\u542b\u6570\u5b57\u5e8f\u5217\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u201c\u5b66\u751f\u201d\u6a21\u578b\uff0c\u89c2\u5bdf\u5176\u662f\u5426\u5b66\u4e60\u5230\u6559\u5e08\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u9a8c\u8bc1\u3002", "result": "\u5b66\u751f\u6a21\u578b\u786e\u5b9e\u5b66\u4e60\u5230\u6559\u5e08\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u5373\u4f7f\u6570\u636e\u4e2d\u672a\u76f4\u63a5\u63d0\u53ca\u8be5\u7279\u5f81\uff1b\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u7684\u5e08\u751f\u7ec4\u5408\u65e0\u6cd5\u5b9e\u73b0\u6b64\u6548\u679c\u3002", "conclusion": "\u6f5c\u610f\u8bc6\u5b66\u4e60\u662f\u666e\u904d\u73b0\u8c61\uff0c\u53ef\u80fd\u901a\u8fc7\u84b8\u998f\u4f20\u64ad\u672a\u9884\u671f\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u9700\u5f15\u8d77AI\u5f00\u53d1\u8005\u7684\u8b66\u60d5\u3002"}}
{"id": "2507.14824", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14824", "abs": "https://arxiv.org/abs/2507.14824", "authors": ["Kunyu Yu", "Rui Yang", "Jingchi Liao", "Siqi Li", "Huitao Li", "Irene Li", "Yifan Peng", "Rishikesan Kamaleswaran", "Nan Liu"], "title": "Benchmarking Foundation Models with Multimodal Public Electronic Health Records", "comment": null, "summary": "Foundation models have emerged as a powerful approach for processing\nelectronic health records (EHRs), offering flexibility to handle diverse\nmedical data modalities. In this study, we present a comprehensive benchmark\nthat evaluates the performance, fairness, and interpretability of foundation\nmodels, both as unimodal encoders and as multimodal learners, using the\npublicly available MIMIC-IV database. To support consistent and reproducible\nevaluation, we developed a standardized data processing pipeline that\nharmonizes heterogeneous clinical records into an analysis-ready format. We\nsystematically compared eight foundation models, encompassing both unimodal and\nmultimodal models, as well as domain-specific and general-purpose variants. Our\nfindings demonstrate that incorporating multiple data modalities leads to\nconsistent improvements in predictive performance without introducing\nadditional bias. Through this benchmark, we aim to support the development of\neffective and trustworthy multimodal artificial intelligence (AI) systems for\nreal-world clinical applications. Our code is available at\nhttps://github.com/nliulab/MIMIC-Multimodal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u5904\u7406\u4e2d\u7684\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u652f\u6301\u5f00\u53d1\u9ad8\u6548\u4e14\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u4e34\u5e8a\u5e94\u7528\u4e2d\u5904\u7406\u591a\u6837\u5316\u533b\u7597\u6570\u636e\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u6807\u51c6\u5316\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u516b\u79cd\u57fa\u7840\u6a21\u578b\uff08\u5305\u62ec\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u6a21\u578b\uff09\u3002", "result": "\u591a\u6a21\u6001\u6570\u636e\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4e14\u672a\u5f15\u5165\u989d\u5916\u504f\u5dee\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u5f00\u53d1\u9002\u7528\u4e8e\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15386", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15386", "abs": "https://arxiv.org/abs/2507.15386", "authors": ["Juntao Wang", "Feng Yin", "Tian Ding", "Tsung-Hui Chang", "Zhi-Quan Luo", "Qi Yan"], "title": "Learning to Gridize: Segment Physical World by Wireless Communication Channel", "comment": null, "summary": "Gridization, the process of partitioning space into grids where users share\nsimilar channel characteristics, serves as a fundamental prerequisite for\nefficient large-scale network optimization. However, existing methods like\nGeographical or Beam Space Gridization (GSG or BSG) are limited by reliance on\nunavailable location data or the flawed assumption that similar signal\nstrengths imply similar channel properties. We propose Channel Space\nGridization (CSG), a pioneering framework that unifies channel estimation and\ngridization for the first time. Formulated as a joint optimization problem, CSG\nuses only beam-level reference signal received power (RSRP) to estimate Channel\nAngle Power Spectra (CAPS) and partition samples into grids with homogeneous\nchannel characteristics. To perform CSG, we develop the CSG Autoencoder\n(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse\ncodebook quantizer, and a physics-informed decoder based on the Localized\nStatistical Channel Model. On recognizing the limitations of naive training\nscheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous\n(PIDA) training scheme for CSG-AE, ensuring stable and effective training by\nsystematically addressing the common pitfalls of the naive training paradigm.\nEvaluations reveal that CSG-AE excels in CAPS estimation accuracy and\nclustering quality on synthetic data. On real-world datasets, it reduces Active\nMean Absolute Error (MAE) by 30\\% and Overall MAE by 65\\% on RSRP prediction\naccuracy compared to salient baselines using the same data, while improving\nchannel consistency, cluster sizes balance, and active ratio, advancing the\ndevelopment of gridization for large-scale network optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSG\u7684\u65b0\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u4fe1\u9053\u4f30\u8ba1\u4e0e\u7f51\u683c\u5316\u7edf\u4e00\u8d77\u6765\uff0c\u4ec5\u4f7f\u7528RSRP\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u7f51\u683c\u5316\uff0c\u5e76\u901a\u8fc7CSG-AE\u6a21\u578b\u548cPIDA\u8bad\u7ec3\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7f51\u683c\u5316\u65b9\u6cd5\uff08\u5982GSG\u6216BSG\uff09\u4f9d\u8d56\u4e0d\u53ef\u7528\u7684\u4f4d\u7f6e\u6570\u636e\u6216\u9519\u8bef\u7684\u4fe1\u53f7\u5f3a\u5ea6\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7f51\u7edc\u4f18\u5316\u7684\u6548\u7387\u3002", "method": "\u63d0\u51faCSG\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u95ee\u9898\u5229\u7528RSRP\u4f30\u8ba1CAPS\u5e76\u5206\u533a\uff1b\u5f00\u53d1CSG-AE\u6a21\u578b\uff0c\u5305\u542bRSRP-to-CAPS\u7f16\u7801\u5668\u3001\u7a00\u758f\u7801\u4e66\u91cf\u5316\u5668\u548c\u7269\u7406\u4fe1\u606f\u89e3\u7801\u5668\uff1b\u63d0\u51faPIDA\u8bad\u7ec3\u65b9\u6848\u89e3\u51b3\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\uff0cCSG-AE\u5728CAPS\u4f30\u8ba1\u548c\u805a\u7c7b\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff1b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\uff0cRSRP\u9884\u6d4b\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff08Active MAE\u964d\u4f4e30%\uff0cOverall MAE\u964d\u4f4e65%\uff09\uff0c\u540c\u65f6\u6539\u5584\u4e86\u4fe1\u9053\u4e00\u81f4\u6027\u548c\u96c6\u7fa4\u5e73\u8861\u3002", "conclusion": "CSG\u6846\u67b6\u548cCSG-AE\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u7f51\u683c\u5316\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2507.14828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14828", "abs": "https://arxiv.org/abs/2507.14828", "authors": ["Abdul-Kazeem Shamba", "Kerstin Bach", "Gavin Taylor"], "title": "eMargin: Revisiting Contrastive Learning with Margin-Based Separation", "comment": "LDD'25: Learning from Difficult Data Workshop (ECAI 2025)", "summary": "We revisit previous contrastive learning frameworks to investigate the effect\nof introducing an adaptive margin into the contrastive loss function for time\nseries representation learning. Specifically, we explore whether an adaptive\nmargin (eMargin), adjusted based on a predefined similarity threshold, can\nimprove the separation between adjacent but dissimilar time steps and\nsubsequently lead to better performance in downstream tasks. Our study\nevaluates the impact of this modification on clustering performance and\nclassification in three benchmark datasets. Our findings, however, indicate\nthat achieving high scores on unsupervised clustering metrics does not\nnecessarily imply that the learned embeddings are meaningful or effective in\ndownstream tasks. To be specific, eMargin added to InfoNCE consistently\noutperforms state-of-the-art baselines in unsupervised clustering metrics, but\nstruggles to achieve competitive results in downstream classification with\nlinear probing. The source code is publicly available at\nhttps://github.com/sfi-norwai/eMargin.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u8fb9\u8ddd\uff08eMargin\uff09\u5bf9\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u65e0\u76d1\u7763\u805a\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u81ea\u9002\u5e94\u8fb9\u8ddd\u662f\u5426\u80fd\u901a\u8fc7\u8c03\u6574\u76f8\u4f3c\u6027\u9608\u503c\uff0c\u6539\u5584\u65f6\u95f4\u5e8f\u5217\u4e2d\u76f8\u90bb\u4f46\u4e0d\u76f8\u4f3c\u65f6\u95f4\u6b65\u7684\u5206\u79bb\u6548\u679c\uff0c\u4ece\u800c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u5728\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u8fb9\u8ddd\uff08eMargin\uff09\uff0c\u5e76\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u5bf9\u805a\u7c7b\u548c\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "eMargin\u5728\u65e0\u76d1\u7763\u805a\u7c7b\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u9ad8\u65e0\u76d1\u7763\u805a\u7c7b\u5206\u6570\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u5d4c\u5165\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u81ea\u9002\u5e94\u8fb9\u8ddd\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.15816", "categories": ["cs.LG", "cs.IT", "cs.NI", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15816", "abs": "https://arxiv.org/abs/2507.15816", "authors": ["Yujia Mu", "Cong Shen"], "title": "Federated Split Learning with Improved Communication and Storage Efficiency", "comment": "Accepted for publication in IEEE Transactions on Mobile Computing", "summary": "Federated learning (FL) is one of the popular distributed machine learning\n(ML) solutions but incurs significant communication and computation costs at\nedge devices. Federated split learning (FSL) can train sub-models in parallel\nand reduce the computational burden of edge devices by splitting the model\narchitecture. However, it still requires a high communication overhead due to\ntransmitting the smashed data and gradients between clients and the server in\nevery global round. Furthermore, the server must maintain separate partial\nmodels for every client, leading to a significant storage requirement. To\naddress these challenges, this paper proposes a novel communication and storage\nefficient federated split learning method, termed CSE-FSL, which utilizes an\nauxiliary network to locally update the weights of the clients while keeping a\nsingle model at the server, hence avoiding frequent transmissions of gradients\nfrom the server and greatly reducing the storage requirement of the server.\nAdditionally, a new model update method of transmitting the smashed data in\nselected epochs can reduce the amount of smashed data sent from the clients. We\nprovide a theoretical analysis of CSE-FSL, rigorously guaranteeing its\nconvergence under non-convex loss functions. The extensive experimental results\nfurther indicate that CSE-FSL achieves a significant communication reduction\nover existing FSL solutions using real-world FL tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSE-FSL\u7684\u65b0\u578b\u8054\u90a6\u5206\u88c2\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u7f51\u7edc\u51cf\u5c11\u901a\u4fe1\u548c\u5b58\u50a8\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u8bc1\u6536\u655b\u6027\u3002", "motivation": "\u8054\u90a6\u5206\u88c2\u5b66\u4e60\uff08FSL\uff09\u867d\u7136\u51cf\u8f7b\u4e86\u8fb9\u7f18\u8bbe\u5907\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u4ecd\u5b58\u5728\u9ad8\u901a\u4fe1\u5f00\u9500\u548c\u670d\u52a1\u5668\u5b58\u50a8\u9700\u6c42\u5927\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u8f85\u52a9\u7f51\u7edc\u5728\u5ba2\u6237\u7aef\u672c\u5730\u66f4\u65b0\u6743\u91cd\uff0c\u670d\u52a1\u5668\u4ec5\u7ef4\u62a4\u5355\u4e00\u6a21\u578b\uff0c\u5e76\u9009\u62e9\u6027\u5730\u4f20\u8f93\u6570\u636e\u4ee5\u51cf\u5c11\u901a\u4fe1\u91cf\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCSE-FSL\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CSE-FSL\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8054\u90a6\u5206\u88c2\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u901a\u4fe1\u548c\u5b58\u50a8\u95ee\u9898\u3002"}}
{"id": "2507.14843", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14843", "abs": "https://arxiv.org/abs/2507.14843", "authors": ["Fang Wu", "Weihao Xuan", "Ximing Lu", "Zaid Harchaoui", "Yejin Choi"], "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin", "comment": null, "summary": "Recent advances in large reasoning models highlight Reinforcement Learning\nwith Verifiable Rewards (RLVR) as a promising method for enhancing AI's\ncapabilities, particularly in solving complex logical tasks. However, it\nremains unclear whether RLVR truly expands a model's reasoning boundary or\nmerely amplifies high-reward outputs that the base model already knows for\nimproved precision. This study presents a theoretical and empirical\ninvestigation that provides fresh insights into the potential limits of RLVR.\nFirst, we offer a new theoretical perspective that RLVR is constrained by the\nbase model's support-unable to sample solutions with zero initial\nprobability-and operates as a conservative reweighting mechanism that may\nrestrict the discovery of entirely original solutions. We also identify an\nentropy-reward tradeoff: while RLVR reliably enhances precision, it may\nprogressively narrow exploration and potentially overlook correct yet\nunderrepresented solutions. Extensive empirical experiments validate that while\nRLVR consistently improves pass@1, the shrinkage of empirical support generally\noutweighs the expansion of empirical support under larger sampling budgets,\nfailing to recover correct answers that were previously accessible to the base\nmodel. Interestingly, we also observe that while RLVR sometimes increases\ntoken-level entropy, resulting in greater uncertainty at each generation step,\nanswer-level entropy declines, indicating that these seemingly more uncertain\npaths ultimately converge onto a smaller set of distinct answers. Taken\ntogether, these findings reveal potential limits of RLVR in extending reasoning\nhorizons. Breaking this invisible leash may require future algorithmic\ninnovations such as explicit exploration mechanisms or hybrid strategies that\nseed probability mass into underrepresented solution regions.", "AI": {"tldr": "RLVR\uff08\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u867d\u80fd\u63d0\u5347AI\u5728\u590d\u6742\u903b\u8f91\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u5176\u662f\u5426\u80fd\u771f\u6b63\u6269\u5c55\u6a21\u578b\u7684\u63a8\u7406\u8fb9\u754c\u5c1a\u4e0d\u660e\u786e\u3002\u7814\u7a76\u53d1\u73b0RLVR\u53d7\u9650\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u652f\u6301\u8303\u56f4\uff0c\u53ef\u80fd\u6291\u5236\u539f\u521b\u89e3\u51b3\u65b9\u6848\u7684\u53d1\u73b0\uff0c\u5e76\u5b58\u5728\u71b5-\u5956\u52b1\u6743\u8861\u3002\u5b9e\u9a8c\u8868\u660eRLVR\u867d\u63d0\u9ad8pass@1\uff0c\u4f46\u652f\u6301\u8303\u56f4\u7f29\u5c0f\uff0c\u4e14\u53ef\u80fd\u5ffd\u7565\u6b63\u786e\u4f46\u5c11\u89c1\u7684\u89e3\u3002", "motivation": "\u63a2\u8ba8RLVR\u662f\u5426\u771f\u6b63\u6269\u5c55\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u653e\u5927\u5df2\u77e5\u9ad8\u5956\u52b1\u8f93\u51fa\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790RLVR\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5176\u5bf9\u57fa\u7840\u6a21\u578b\u652f\u6301\u7684\u4f9d\u8d56\u548c\u71b5-\u5956\u52b1\u6743\u8861\u3002", "result": "RLVR\u867d\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u652f\u6301\u8303\u56f4\u7f29\u5c0f\uff0c\u4e14\u53ef\u80fd\u5ffd\u7565\u6b63\u786e\u89e3\uff1b\u751f\u6210\u8def\u5f84\u7684\u71b5\u589e\u52a0\uff0c\u4f46\u7b54\u6848\u591a\u6837\u6027\u51cf\u5c11\u3002", "conclusion": "RLVR\u5728\u6269\u5c55\u63a8\u7406\u80fd\u529b\u65b9\u9762\u5b58\u5728\u6f5c\u5728\u9650\u5236\uff0c\u672a\u6765\u9700\u63a2\u7d22\u663e\u5f0f\u63a2\u7d22\u673a\u5236\u6216\u6df7\u5408\u7b56\u7565\u4ee5\u7a81\u7834\u9650\u5236\u3002"}}
{"id": "2507.14847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14847", "abs": "https://arxiv.org/abs/2507.14847", "authors": ["Junhan Yu", "Zhunyi Feng", "Junwei Lu", "Tianxi Cai", "Doudou Zhou"], "title": "Time-Aware Attention for Enhanced Electronic Health Records Modeling", "comment": null, "summary": "Electronic Health Records (EHR) contain valuable clinical information for\npredicting patient outcomes and guiding healthcare decisions. However,\neffectively modeling Electronic Health Records (EHRs) requires addressing data\nheterogeneity and complex temporal patterns. Standard approaches often struggle\nwith irregular time intervals between clinical events. We propose TALE-EHR, a\nTransformer-based framework featuring a novel time-aware attention mechanism\nthat explicitly models continuous temporal gaps to capture fine-grained\nsequence dynamics. To complement this temporal modeling with robust semantics,\nTALE-EHR leverages embeddings derived from standardized code descriptions using\na pre-trained Large Language Model (LLM), providing a strong foundation for\nunderstanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset\ndemonstrate that our approach outperforms state-of-the-art baselines on tasks\nsuch as disease progression forecasting. TALE-EHR underscores the benefit of\nintegrating explicit, continuous temporal modeling with strong semantic\nrepresentations provides a powerful solution for advancing EHR analysis.", "AI": {"tldr": "TALE-EHR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86EHR\u6570\u636e\u5f02\u8d28\u6027\u548c\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5305\u542b\u4e30\u5bcc\u7684\u4e34\u5e8a\u4fe1\u606f\uff0c\u4f46\u6570\u636e\u5f02\u8d28\u6027\u548c\u590d\u6742\u65f6\u95f4\u6a21\u5f0f\u4f7f\u5176\u5efa\u6a21\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u3002", "method": "TALE-EHR\u91c7\u7528Transformer\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5d4c\u5165\uff0c\u4ee5\u6355\u6349\u7cbe\u7ec6\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728MIMIC-IV\u548cPIC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTALE-EHR\u5728\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TALE-EHR\u5c55\u793a\u4e86\u7ed3\u5408\u663e\u5f0f\u65f6\u95f4\u5efa\u6a21\u548c\u5f3a\u8bed\u4e49\u8868\u793a\u5728EHR\u5206\u6790\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.14850", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14850", "abs": "https://arxiv.org/abs/2507.14850", "authors": ["H. M. Sabbir Ahmad", "Ehsan Sabouni", "Alexander Wasilkoff", "Param Budhraja", "Zijian Guo", "Songyuan Zhang", "Chuchu Fan", "Christos Cassandras", "Wenchao Li"], "title": "Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems", "comment": null, "summary": "We address the problem of safe policy learning in multi-agent safety-critical\nautonomous systems. In such systems, it is necessary for each agent to meet the\nsafety requirements at all times while also cooperating with other agents to\naccomplish the task. Toward this end, we propose a safe Hierarchical\nMulti-Agent Reinforcement Learning (HMARL) approach based on Control Barrier\nFunctions (CBFs). Our proposed hierarchical approach decomposes the overall\nreinforcement learning problem into two levels learning joint cooperative\nbehavior at the higher level and learning safe individual behavior at the lower\nor agent level conditioned on the high-level policy. Specifically, we propose a\nskill-based HMARL-CBF algorithm in which the higher level problem involves\nlearning a joint policy over the skills for all the agents and the lower-level\nproblem involves learning policies to execute the skills safely with CBFs. We\nvalidate our approach on challenging environment scenarios whereby a large\nnumber of agents have to safely navigate through conflicting road networks.\nCompared with existing state of the art methods, our approach significantly\nimproves the safety achieving near perfect (within 5%) success/safety rate\nwhile also improving performance across all the environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08HMARL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b89\u5168\u5173\u952e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u5728\u6ee1\u8db3\u5b89\u5168\u8981\u6c42\u7684\u540c\u65f6\u5b9e\u73b0\u4efb\u52a1\u534f\u4f5c\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u9ad8\u5c42\u5b66\u4e60\u8054\u5408\u534f\u4f5c\u884c\u4e3a\uff0c\u4f4e\u5c42\u57fa\u4e8e\u9ad8\u5c42\u7b56\u7565\u5b66\u4e60\u5b89\u5168\u4e2a\u4f53\u884c\u4e3a\uff0c\u7ed3\u5408HMARL-CBF\u7b97\u6cd5\u3002", "result": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff08\u63a5\u8fd1\u5b8c\u7f8e\u6210\u529f\u7387\uff09\uff0c\u540c\u65f6\u63d0\u9ad8\u6027\u80fd\u3002", "conclusion": "HMARL-CBF\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u5b89\u5168\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14874", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14874", "abs": "https://arxiv.org/abs/2507.14874", "authors": ["Ole-Christoffer Granmo", "Youmna Abdelwahab", "Per-Arne Andersen", "Paul F. A. Clarke", "Kunal Dumbre", "Ylva Gr\u00f8nnins\u00e6ter", "Vojtech Halenka", "Runar Helin", "Lei Jiao", "Ahmed Khalid", "Rebekka Omslandseter", "Rupsa Saha", "Mayur Shende", "Xuan Zhang"], "title": "The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs", "comment": "34 pages, 10 figures", "summary": "Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine\n(TM) both interpretable and efficient, while the power of Tsetlin automata\nenables accuracy comparable to deep learning on an increasing number of\ndatasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning\ninterpretable deep clauses from graph-structured input. Moving beyond flat,\nfixed-length input, the GraphTM gets more versatile, supporting sequences,\ngrids, relations, and multimodality. Through message passing, the GraphTM\nbuilds nested deep clauses to recognize sub-graph patterns with exponentially\nfewer clauses, increasing both interpretability and data utilization. For image\nclassification, GraphTM preserves interpretability and achieves 3.86%-points\nhigher accuracy on CIFAR-10 than a convolutional TM. For tracking action\ncoreference, faced with increasingly challenging tasks, GraphTM outperforms\nother reinforcement learning methods by up to 20.6%-points. In recommendation\nsystems, it tolerates increasing noise to a greater extent than a Graph\nConvolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains\naccuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence\ndata, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training\n2.5x faster than GCN. The GraphTM's application to these varied fields\ndemonstrates how graph representation learning and deep clauses bring new\npossibilities for TM learning.", "AI": {"tldr": "Graph Tsetlin Machine (GraphTM) \u662f\u4e00\u79cd\u7528\u4e8e\u4ece\u56fe\u7ed3\u6784\u8f93\u5165\u4e2d\u5b66\u4e60\u53ef\u89e3\u91ca\u6df1\u5ea6\u5b50\u53e5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u6784\u5efa\u5d4c\u5957\u5b50\u53e5\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u5229\u7528\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf Tsetlin Machine (TM) \u5728\u5904\u7406\u56fe\u7ed3\u6784\u8f93\u5165\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0cGraphTM \u7684\u5f15\u5165\u65e8\u5728\u6269\u5c55\u5176\u80fd\u529b\uff0c\u652f\u6301\u5e8f\u5217\u3001\u7f51\u683c\u3001\u5173\u7cfb\u548c\u591a\u6a21\u6001\u8f93\u5165\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u548c\u9ad8\u6548\u6027\u3002", "method": "GraphTM \u901a\u8fc7\u6d88\u606f\u4f20\u9012\u6784\u5efa\u5d4c\u5957\u6df1\u5ea6\u5b50\u53e5\uff0c\u8bc6\u522b\u5b50\u56fe\u6a21\u5f0f\uff0c\u51cf\u5c11\u5b50\u53e5\u6570\u91cf\uff0c\u63d0\u9ad8\u6570\u636e\u5229\u7528\u7387\u3002", "result": "GraphTM \u5728\u56fe\u50cf\u5206\u7c7b\u3001\u52a8\u4f5c\u5171\u6307\u8ddf\u8e2a\u3001\u63a8\u8350\u7cfb\u7edf\u548c\u75c5\u6bd2\u57fa\u56e0\u7ec4\u5e8f\u5217\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u4e14\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "GraphTM \u5c55\u793a\u4e86\u56fe\u8868\u793a\u5b66\u4e60\u548c\u6df1\u5ea6\u5b50\u53e5\u4e3a TM \u5b66\u4e60\u5e26\u6765\u7684\u65b0\u53ef\u80fd\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u3002"}}
{"id": "2507.14882", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14882", "abs": "https://arxiv.org/abs/2507.14882", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel G\u00f6rges"], "title": "Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization", "comment": "6 pages, 22nd International Conference on Advanced Robotics (ICAR\n  2025)", "summary": "Deep neural networks (DNNs) offer significant versatility and performance\nbenefits, but their widespread adoption is often hindered by high model\ncomplexity and computational demands. Model compression techniques such as\npruning have emerged as promising solutions to these challenges. However, it\nremains critical to ensure that application-specific performance\ncharacteristics are preserved during compression. In structured pruning, where\ngroups of structurally coherent elements are removed, conventional importance\nmetrics frequently fail to maintain these essential performance attributes. In\nthis work, we propose an enhanced importance metric framework that not only\nreduces model size but also explicitly accounts for application-specific\nperformance constraints. We employ multiple strategies to determine the optimal\npruning magnitude for each group, ensuring a balance between compression and\ntask performance. Our approach is evaluated on an autoencoder tasked with\nreconstructing MNIST images. Experimental results demonstrate that the proposed\nmethod effectively preserves task-relevant performance, maintaining the model's\nusability even after substantial pruning, by satisfying the required\napplication-specific criteria.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7ed3\u6784\u5316\u526a\u679d\u91cd\u8981\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u7684\u540c\u65f6\u4fdd\u6301\u5e94\u7528\u7279\u5b9a\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u590d\u6742\u6027\u548c\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u800c\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u7b56\u7565\u786e\u5b9a\u6bcf\u7ec4\u526a\u679d\u7684\u6700\u4f18\u5e45\u5ea6\uff0c\u5e73\u8861\u538b\u7f29\u4e0e\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5728MNIST\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u4fdd\u7559\u4e86\u4efb\u52a1\u76f8\u5173\u6027\u80fd\uff0c\u5373\u4f7f\u5927\u5e45\u526a\u679d\u540e\u4ecd\u6ee1\u8db3\u5e94\u7528\u9700\u6c42\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u6a21\u578b\u538b\u7f29\u4e2d\u6210\u529f\u5e73\u8861\u4e86\u526a\u679d\u4e0e\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.14919", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14919", "abs": "https://arxiv.org/abs/2507.14919", "authors": ["Maximilian Wendlinger", "Kilian Tscharke", "Pascal Debus"], "title": "Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning", "comment": null, "summary": "One of the key obstacles in traditional deep learning is the reduction in\nmodel transparency caused by increasingly intricate model functions, which can\nlead to problems such as overfitting and excessive confidence in predictions.\nWith the advent of quantum machine learning offering possible advances in\ncomputational power and latent space complexity, we notice the same opaque\nbehavior. Despite significant research in classical contexts, there has been\nlittle advancement in addressing the black-box nature of quantum machine\nlearning. Consequently, we approach this gap by building upon existing work in\nclassical uncertainty quantification and initial explorations in quantum\nBayesian modeling to theoretically develop and empirically evaluate techniques\nto map classical uncertainty quantification methods to the quantum machine\nlearning domain. Our findings emphasize the necessity of leveraging classical\ninsights into uncertainty quantification to include uncertainty awareness in\nthe process of designing new quantum machine learning models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u5c06\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u6620\u5c04\u5230\u91cf\u5b50\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u6a21\u578b\u900f\u660e\u5ea6\u4e0d\u8db3\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u9884\u6d4b\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u7f3a\u4e4f\u5bf9\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9ed1\u76d2\u6027\u8d28\u7684\u7814\u7a76\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u91cf\u5b50\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u7406\u8bba\u53d1\u5c55\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u5c06\u7ecf\u5178\u65b9\u6cd5\u6620\u5c04\u5230\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9700\u5229\u7528\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89c1\u89e3\uff0c\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u3002", "conclusion": "\u5f3a\u8c03\u5728\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bbe\u8ba1\u4e2d\u878d\u5165\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.14980", "categories": ["cs.LG", "68T05, 90C26", "I.2.6; I.5.1; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14980", "abs": "https://arxiv.org/abs/2507.14980", "authors": ["Tianle Li", "Yongzhi Huang", "Linshan Jiang", "Qipeng Xie", "Chang Liu", "Wenfeng Du", "Lu Wang", "Kaishun Wu"], "title": "FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios", "comment": "ICPP, including appendix", "summary": "Federated Learning (FL) enables decentralized model training while preserving\ndata privacy. Despite its benefits, FL faces challenges with non-identically\ndistributed (non-IID) data, especially in long-tailed scenarios with imbalanced\nclass samples. Momentum-based FL methods, often used to accelerate FL\nconvergence, struggle with these distributions, resulting in biased models and\nmaking FL hard to converge. To understand this challenge, we conduct extensive\ninvestigations into this phenomenon, accompanied by a layer-wise analysis of\nneural network behavior. Based on these insights, we propose FedWCM, a method\nthat dynamically adjusts momentum using global and per-round data to correct\ndirectional biases introduced by long-tailed distributions. Extensive\nexperiments show that FedWCM resolves non-convergence issues and outperforms\nexisting methods, enhancing FL's efficiency and effectiveness in handling\nclient heterogeneity and data imbalance.", "AI": {"tldr": "FedWCM\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u52a8\u91cf\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u6536\u655b\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\uff08\u5c24\u5176\u662f\u957f\u5c3e\u5206\u5e03\uff09\u4e2d\u9762\u4e34\u6536\u655b\u56f0\u96be\u548c\u6a21\u578b\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faFedWCM\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u52a8\u91cf\u4ee5\u7ea0\u6b63\u957f\u5c3e\u5206\u5e03\u5f15\u5165\u7684\u65b9\u5411\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedWCM\u89e3\u51b3\u4e86\u4e0d\u6536\u655b\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedWCM\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u5ba2\u6237\u7aef\u5f02\u6784\u573a\u666f\u4e0b\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2507.15066", "categories": ["cs.LG", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.15066", "abs": "https://arxiv.org/abs/2507.15066", "authors": ["Yiyuan Yang", "Zichuan Liu", "Lei Song", "Kai Ying", "Zhiguang Wang", "Tom Bamford", "Svitlana Vyetrenko", "Jiang Bian", "Qingsong Wen"], "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback", "comment": "Under review. 19 pages, 8 figures, 12 tables", "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1Time-RA\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4ece\u5224\u522b\u5f0f\u4efb\u52a1\u8f6c\u53d8\u4e3a\u751f\u6210\u5f0f\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u96c6RATs40K\uff0c\u7528\u4e8e\u5f02\u5e38\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4e8c\u5143\u5206\u7c7b\uff0c\u7f3a\u4e4f\u8be6\u7ec6\u5206\u7c7b\u548c\u89e3\u91ca\u6027\u63a8\u7406\u3002", "method": "\u63d0\u51faTime-RA\u4efb\u52a1\uff0c\u5229\u7528LLMs\u5b9e\u73b0\u751f\u6210\u5f0f\u63a8\u7406\uff1b\u6784\u5efaRATs40K\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\u548c\u7cbe\u7ec6\u6807\u6ce8\u3002", "result": "\u901a\u8fc7LLMs\u548c\u591a\u6a21\u6001LLMs\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u76d1\u7763\u5fae\u8c03\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e3a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u548c\u63a8\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15067", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15067", "abs": "https://arxiv.org/abs/2507.15067", "authors": ["Bing He", "Mustaque Ahamad", "Srijan Kumar"], "title": "ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model", "comment": "15 pages, 12 tables", "summary": "Detecting bad actors is critical to ensure the safety and integrity of\ninternet platforms. Several deep learning-based models have been developed to\nidentify such users. These models should not only accurately detect bad actors,\nbut also be robust against adversarial attacks that aim to evade detection.\nHowever, past deep learning-based detection models do not meet the robustness\nrequirement because they are sensitive to even minor changes in the input\nsequence. To address this issue, we focus on (1) improving the model\nunderstanding capability and (2) enhancing the model knowledge such that the\nmodel can recognize potential input modifications when making predictions. To\nachieve these goals, we create a novel transformer-based classification model,\ncalled ROBAD (RObust adversary-aware local-global attended Bad Actor Detection\nmodel), which uses the sequence of user posts to generate user embedding to\ndetect bad actors. Particularly, ROBAD first leverages the transformer encoder\nblock to encode each post bidirectionally, thus building a post embedding to\ncapture the local information at the post level. Next, it adopts the\ntransformer decoder block to model the sequential pattern in the post\nembeddings by using the attention mechanism, which generates the sequence\nembedding to obtain the global information at the sequence level. Finally, to\nenrich the knowledge of the model, embeddings of modified sequences by mimicked\nattackers are fed into a contrastive-learning-enhanced classification layer for\nsequence prediction. In essence, by capturing the local and global information\n(i.e., the post and sequence information) and leveraging the mimicked behaviors\nof bad actors in training, ROBAD can be robust to adversarial attacks.\nExtensive experiments on Yelp and Wikipedia datasets show that ROBAD can\neffectively detect bad actors when under state-of-the-art adversarial attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aROBAD\u7684\u65b0\u578bTransformer\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e92\u8054\u7f51\u5e73\u53f0\u4e0a\u7684\u4e0d\u826f\u884c\u4e3a\u8005\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u4ee5\u53ca\u5bf9\u6297\u6027\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u4e0d\u826f\u884c\u4e3a\u8005\u65f6\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u63d0\u5347\u6a21\u578b\u7684\u7406\u89e3\u80fd\u529b\u548c\u77e5\u8bc6\uff0c\u4ee5\u8bc6\u522b\u6f5c\u5728\u7684\u8f93\u5165\u4fee\u6539\u3002", "method": "ROBAD\u7ed3\u5408Transformer\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u5757\uff0c\u5206\u522b\u6355\u83b7\u5e16\u5b50\u7ea7\u522b\u7684\u5c40\u90e8\u4fe1\u606f\u548c\u5e8f\u5217\u7ea7\u522b\u7684\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7684\u5206\u7c7b\u5c42\u5229\u7528\u6a21\u62df\u653b\u51fb\u8005\u7684\u4fee\u6539\u5e8f\u5217\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Yelp\u548cWikipedia\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cROBAD\u80fd\u591f\u6709\u6548\u62b5\u5fa1\u6700\u5148\u8fdb\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u51c6\u786e\u68c0\u6d4b\u4e0d\u826f\u884c\u4e3a\u8005\u3002", "conclusion": "ROBAD\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u4ee5\u53ca\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.15073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15073", "abs": "https://arxiv.org/abs/2507.15073", "authors": ["Samuel Pfrommer", "Yixiao Huang", "Somayeh Sojoudi"], "title": "Reinforcement Learning for Flow-Matching Policies", "comment": null, "summary": "Flow-matching policies have emerged as a powerful paradigm for generalist\nrobotics. These models are trained to imitate an action chunk, conditioned on\nsensor observations and textual instructions. Often, training demonstrations\nare generated by a suboptimal policy, such as a human operator. This work\nexplores training flow-matching policies via reinforcement learning to surpass\nthe original demonstration policy performance. We particularly note\nminimum-time control as a key application and present a simple scheme for\nvariable-horizon flow-matching planning. We then introduce two families of\napproaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group\nRelative Policy Optimization (GRPO) approach with a learned reward surrogate.\nOur policies are trained on an illustrative suite of simulated unicycle\ndynamics tasks, and we show that both approaches dramatically improve upon the\nsuboptimal demonstrator performance, with the GRPO approach in particular\ngenerally incurring between $50\\%$ and $85\\%$ less cost than a naive Imitation\nLearning Flow Matching (ILFM) approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6d41\u5339\u914d\u7b56\u7565\uff0c\u8d85\u8d8a\u539f\u59cb\u6f14\u793a\u7b56\u7565\u6027\u80fd\uff0c\u5e76\u4ecb\u7ecd\u4e24\u79cd\u65b9\u6cd5\uff1aRWFM\u548cGRPO\uff0c\u5b9e\u9a8c\u663e\u793aGRPO\u663e\u8457\u4f18\u4e8e\u6a21\u4eff\u5b66\u4e60\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6d41\u5339\u914d\u7b56\u7565\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u7531\u6b21\u4f18\u7b56\u7565\uff08\u5982\u4eba\u7c7b\u64cd\u4f5c\u5458\uff09\u751f\u6210\u7684\u6f14\u793a\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1aReward-Weighted Flow Matching (RWFM) \u548c Group Relative Policy Optimization (GRPO)\uff0c\u7ed3\u5408\u53ef\u53d8\u65f6\u95f4\u8303\u56f4\u7684\u6d41\u5339\u914d\u89c4\u5212\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cGRPO\u65b9\u6cd5\u6bd4\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u51cf\u5c1150%\u81f385%\u7684\u6210\u672c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6d41\u5339\u914d\u7b56\u7565\u7684\u6027\u80fd\uff0cGRPO\u65b9\u6cd5\u5c24\u5176\u6709\u6548\u3002"}}
{"id": "2507.15079", "categories": ["cs.LG", "q-fin.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15079", "abs": "https://arxiv.org/abs/2507.15079", "authors": ["Arkadiusz Lipiecki", "Bartosz Uniejewski"], "title": "Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts", "comment": "Preprint", "summary": "Quantifying the uncertainty of forecasting models is essential to assess and\nmitigate the risks associated with data-driven decisions, especially in\nvolatile domains such as electricity markets. Machine learning methods can\nprovide highly accurate electricity price forecasts, critical for informing the\ndecisions of market participants. However, these models often lack uncertainty\nestimates, which limits the ability of decision makers to avoid unnecessary\nrisks. In this paper, we propose a novel method for generating probabilistic\nforecasts from ensembles of point forecasts, called Isotonic Quantile\nRegression Averaging (iQRA). Building on the established framework of Quantile\nRegression Averaging (QRA), we introduce stochastic order constraints to\nimprove forecast accuracy, reliability, and computational costs. In an\nextensive forecasting study of the German day-ahead electricity market, we show\nthat iQRA consistently outperforms state-of-the-art postprocessing methods in\nterms of both reliability and sharpness. It produces well-calibrated prediction\nintervals across multiple confidence levels, providing superior reliability to\nall benchmark methods, particularly coverage-based conformal prediction. In\naddition, isotonic regularization decreases the complexity of the quantile\nregression problem and offers a hyperparameter-free approach to variable\nselection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiQRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u70b9\u9884\u6d4b\u751f\u6210\u6982\u7387\u9884\u6d4b\uff0c\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u529b\u5e02\u573a\u7b49\u6ce2\u52a8\u9886\u57df\u9700\u8981\u51c6\u786e\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\uff0c\u4ee5\u964d\u4f4e\u51b3\u7b56\u98ce\u9669\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "method": "\u57fa\u4e8eQRA\u6846\u67b6\uff0c\u5f15\u5165\u968f\u673a\u987a\u5e8f\u7ea6\u675f\uff0c\u63d0\u51faiQRA\u65b9\u6cd5\uff0c\u6539\u8fdb\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u5fb7\u56fd\u65e5\u524d\u7535\u529b\u5e02\u573a\u7684\u9884\u6d4b\u7814\u7a76\u4e2d\uff0ciQRA\u5728\u53ef\u9760\u6027\u548c\u9510\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6821\u51c6\u826f\u597d\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "iQRA\u901a\u8fc7\u7b49\u6e17\u6b63\u5219\u5316\u7b80\u5316\u5206\u4f4d\u6570\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u4f9b\u65e0\u8d85\u53c2\u6570\u53d8\u91cf\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.15082", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15082", "abs": "https://arxiv.org/abs/2507.15082", "authors": ["Qian Qi"], "title": "Robust Control with Gradient Uncertainty", "comment": null, "summary": "We introduce a novel extension to robust control theory that explicitly\naddresses uncertainty in the value function's gradient, a form of uncertainty\nendemic to applications like reinforcement learning where value functions are\napproximated. We formulate a zero-sum dynamic game where an adversary perturbs\nboth system dynamics and the value function gradient, leading to a new, highly\nnonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs\nEquation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness\nby proving a comparison principle for its viscosity solutions under a uniform\nellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a\nkey insight: we prove that the classical quadratic value function assumption\nfails for any non-zero gradient uncertainty, fundamentally altering the problem\nstructure. A formal perturbation analysis characterizes the non-polynomial\ncorrection to the value function and the resulting nonlinearity of the optimal\ncontrol law, which we validate with numerical studies. Finally, we bridge\ntheory to practice by proposing a novel Gradient-Uncertainty-Robust\nActor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating\nits effectiveness in stabilizing training. This work provides a new direction\nfor robust control, holding significant implications for fields where function\napproximation is common, including reinforcement learning and computational\nfinance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u63a7\u5236\u7406\u8bba\u6269\u5c55\uff0c\u660e\u786e\u5904\u7406\u4ef7\u503c\u51fd\u6570\u68af\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b49\u9886\u57df\u3002\u901a\u8fc7\u96f6\u548c\u52a8\u6001\u535a\u5f08\u548c\u65b0\u7684\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff08GU-HJBI\uff09\u5206\u6790\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\uff08GURAC\uff09\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u7b49\u5e94\u7528\u4e2d\uff0c\u4ef7\u503c\u51fd\u6570\u68af\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u666e\u904d\u5b58\u5728\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u672a\u660e\u786e\u5904\u7406\u8fd9\u4e00\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u96f6\u548c\u52a8\u6001\u535a\u5f08\u5efa\u6a21\uff0c\u5f15\u5165GU-HJBI\u65b9\u7a0b\uff0c\u5206\u6790\u5176\u89e3\u7684\u6027\u8d28\uff0c\u5e76\u5728LQ\u60c5\u51b5\u4e0b\u9a8c\u8bc1\u7406\u8bba\u3002\u6700\u7ec8\u63d0\u51faGURAC\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u7ecf\u5178\u4e8c\u6b21\u4ef7\u503c\u51fd\u6570\u5047\u8bbe\u5728\u68af\u5ea6\u4e0d\u786e\u5b9a\u6027\u4e0b\u5931\u6548\uff0c\u5e76\u63d0\u51fa\u4e86\u975e\u7ebf\u6027\u4fee\u6b63\u3002GURAC\u7b97\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u7a33\u5b9a\u8bad\u7ec3\u7684\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u4e3a\u9c81\u68d2\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5bf9\u5f3a\u5316\u5b66\u4e60\u548c\u8ba1\u7b97\u91d1\u878d\u7b49\u9886\u57df\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.15104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15104", "abs": "https://arxiv.org/abs/2507.15104", "authors": ["Qiufeng Li", "Shu Hong", "Jian Gao", "Xuan Zhang", "Tian Lan", "Weidong Cao"], "title": "AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI", "comment": null, "summary": "Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize\nanalog design automation through data-driven approaches. In particular,\nresearchers are increasingly fascinated by harnessing the power of generative\nAI to automate the discovery of novel analog circuit topologies. Unlocking the\nfull potential of generative AI in these data-driven discoveries requires\naccess to large and diverse datasets.Yet, there is a significant barrier in the\nanalog domain--Analog circuit design is inherently proprietary, involving not\nonly confidential circuit structures but also the underlying commercial\nsemiconductor processes. As a result, current generative AI research is largely\nconfined to individual researchers who construct small, narrowly focused\nprivate datasets. This fragmentation severely limits collaborative innovation\nand impedes progress across the research community. To address these\nchallenges, we propose AnalogFed. AnalogFed enables collaborative topology\ndiscovery across decentralized clients (e.g., individual researchers or\ninstitutions) without requiring the sharing of raw private data. To make this\nvision practical, we introduce a suite of techniques tailored to the unique\nchallenges of applying FedL in analog design--from generative model development\nand data heterogeneity handling to privacy-preserving strategies that ensure\nboth flexibility and security for circuit designers and semiconductor\nmanufacturers. Extensive experiments across varying client counts and dataset\nsizes demonstrate that AnalogFed achieves performance comparable to centralized\nbaselines--while maintaining strict data privacy. Specifically, the generative\nAI model within AnalogFed achieves state-of-the-art efficiency and scalability\nin the design of analog circuit topologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAnalogFed\uff0c\u4e00\u79cd\u652f\u6301\u53bb\u4e2d\u5fc3\u5316\u534f\u4f5c\u7684\u751f\u6210\u5f0fAI\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u7535\u8def\u62d3\u6251\u53d1\u73b0\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u6570\u636e\u5177\u6709\u4e13\u6709\u6027\u548c\u4fdd\u5bc6\u6027\uff0c\u9650\u5236\u4e86\u751f\u6210\u5f0fAI\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u534f\u4f5c\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAnalogFed\u6846\u67b6\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u548c\u9690\u79c1\u4fdd\u62a4\u6280\u672f\uff0c\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u5e76\u5f00\u53d1\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAnalogFed\u5728\u6027\u80fd\u4e0a\u4e0e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\uff0c\u751f\u6210\u6a21\u578b\u5728\u62d3\u6251\u8bbe\u8ba1\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "AnalogFed\u4e3a\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u7684\u534f\u4f5c\u521b\u65b0\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u5206\u6563\u6027\u95ee\u9898\u3002"}}
{"id": "2507.15119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15119", "abs": "https://arxiv.org/abs/2507.15119", "authors": ["Juntong Ni", "Shiyu Wang", "Zewen Liu", "Xiaoming Shi", "Xinyue Zhong", "Zhou Ye", "Wei Jin"], "title": "Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting", "comment": null, "summary": "Time series forecasting (TSF) is a central problem in time series analysis.\nHowever, as the number of channels in time series datasets scales to the\nthousands or more, a scenario we define as High-Dimensional Time Series\nForecasting (HDTSF), it introduces significant new modeling challenges that are\noften not the primary focus of traditional TSF research. HDTSF is challenging\nbecause the channel correlation often forms complex and hierarchical patterns.\nExisting TSF models either ignore these interactions or fail to scale as\ndimensionality grows. To address this issue, we propose U-Cast, a\nchannel-dependent forecasting architecture that learns latent hierarchical\nchannel structures with an innovative query-based attention. To disentangle\nhighly correlated channel representation, U-Cast adds a full-rank\nregularization during training. We also release Time-HD, a benchmark of large,\ndiverse, high-dimensional datasets. Our theory shows that exploiting\ncross-channel information lowers forecasting risk, and experiments on Time-HD\ndemonstrate that U-Cast surpasses strong baselines in both accuracy and\nefficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF\nresearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faU-Cast\u6a21\u578b\u548cTime-HD\u57fa\u51c6\uff0c\u89e3\u51b3\u9ad8\u7ef4\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08HDTSF\uff09\u4e2d\u7684\u590d\u6742\u901a\u9053\u76f8\u5173\u6027\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08TSF\uff09\u6a21\u578b\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u901a\u9053\u76f8\u5173\u6027\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faU-Cast\uff0c\u57fa\u4e8e\u67e5\u8be2\u6ce8\u610f\u529b\u5b66\u4e60\u6f5c\u5728\u5c42\u6b21\u901a\u9053\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u5168\u79e9\u6b63\u5219\u5316\u89e3\u8026\u9ad8\u76f8\u5173\u901a\u9053\u8868\u793a\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8de8\u901a\u9053\u4fe1\u606f\u964d\u4f4e\u9884\u6d4b\u98ce\u9669\uff0c\u5b9e\u9a8c\u663e\u793aU-Cast\u5728Time-HD\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "U-Cast\u548cTime-HD\u4e3aHDTSF\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.15132", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.15132", "abs": "https://arxiv.org/abs/2507.15132", "authors": ["Joanna Komorniczak"], "title": "Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm", "comment": null, "summary": "The research community continues to seek increasingly more advanced synthetic\ndata generators to reliably evaluate the strengths and limitations of machine\nlearning methods. This work aims to increase the availability of datasets\nencompassing a diverse range of problem complexities by proposing a genetic\nalgorithm that optimizes a set of problem complexity measures for\nclassification and regression tasks towards specific targets. For\nclassification, a set of 10 complexity measures was used, while for regression\ntasks, 4 measures demonstrating promising optimization capabilities were\nselected. Experiments confirmed that the proposed genetic algorithm can\ngenerate datasets with varying levels of difficulty by transforming\nsynthetically created datasets to achieve target complexity values through\nlinear feature projections. Evaluations involving state-of-the-art classifiers\nand regressors revealed a correlation between the complexity of the generated\ndata and the recognition quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9057\u4f20\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u590d\u6742\u6027\u5ea6\u91cf\uff0c\u751f\u6210\u5177\u6709\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "motivation": "\u7814\u7a76\u793e\u533a\u9700\u8981\u66f4\u5148\u8fdb\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u6765\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u590d\u6742\u6027\u5ea6\u91cf\uff0c\u901a\u8fc7\u7ebf\u6027\u7279\u5f81\u6295\u5f71\u8c03\u6574\u5408\u6210\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u80fd\u751f\u6210\u5177\u6709\u76ee\u6807\u590d\u6742\u6027\u7684\u6570\u636e\u96c6\uff0c\u4e14\u6570\u636e\u590d\u6742\u6027\u4e0e\u8bc6\u522b\u8d28\u91cf\u76f8\u5173\u3002", "conclusion": "\u8be5\u9057\u4f20\u7b97\u6cd5\u80fd\u6709\u6548\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2507.15156", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15156", "abs": "https://arxiv.org/abs/2507.15156", "authors": ["Mykhailo Buleshnyi", "Anna Polova", "Zsolt Zombori", "Michael Benedikt"], "title": "Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification", "comment": null, "summary": "We investigate multi-label classification involving large sets of labels,\nwhere the output labels may be known to satisfy some logical constraints. We\nlook at an architecture in which classifiers for individual labels are fed into\nan expressive sequential model, which produces a joint distribution. One of the\npotential advantages for such an expressive model is its ability to modelling\ncorrelations, as can arise from constraints. We empirically demonstrate the\nability of the architecture both to exploit constraints in training and to\nenforce constraints at inference time.", "AI": {"tldr": "\u7814\u7a76\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\uff0c\u5229\u7528\u903b\u8f91\u7ea6\u675f\u548c\u5e8f\u5217\u6a21\u578b\u5efa\u6a21\u6807\u7b7e\u76f8\u5173\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6807\u7b7e\u5206\u7c7b\u4e2d\u6807\u7b7e\u95f4\u7684\u903b\u8f91\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e2a\u4f53\u6807\u7b7e\u5206\u7c7b\u5668\u8f93\u5165\u5230\u5e8f\u5217\u6a21\u578b\u4e2d\uff0c\u751f\u6210\u8054\u5408\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u67b6\u6784\u80fd\u6709\u6548\u5229\u7528\u7ea6\u675f\u8fdb\u884c\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "conclusion": "\u8be5\u67b6\u6784\u80fd\u5efa\u6a21\u6807\u7b7e\u76f8\u5173\u6027\u5e76\u5229\u7528\u7ea6\u675f\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002"}}
{"id": "2507.15158", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.15158", "abs": "https://arxiv.org/abs/2507.15158", "authors": ["A. H. Abbas", "Hend Abdel-Ghani", "Ivan S. Maksymov"], "title": "Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition", "comment": null, "summary": "As artificial intelligence continues to push into real-time, edge-based and\nresource-constrained environments, there is an urgent need for novel,\nhardware-efficient computational models. In this study, we present and validate\na neuromorphic computing architecture based on resonant-tunnelling diodes\n(RTDs), which exhibit the nonlinear characteristics ideal for physical\nreservoir computing (RC). We theoretically formulate and numerically implement\nan RTD-based RC system and demonstrate its effectiveness on two image\nrecognition benchmarks: handwritten digit classification and object recognition\nusing the Fruit~360 dataset. Our results show that this circuit-level\narchitecture delivers promising performance while adhering to the principles of\nnext-generation RC -- eliminating random connectivity in favour of a\ndeterministic nonlinear transformation of input signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u632f\u96a7\u7a7f\u4e8c\u6781\u7ba1\uff08RTD\uff09\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\uff0c\u5e76\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5411\u5b9e\u65f6\u3001\u8fb9\u7f18\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u6269\u5c55\uff0c\u9700\u8981\u786c\u4ef6\u9ad8\u6548\u7684\u8ba1\u7b97\u6a21\u578b\u3002", "method": "\u7406\u8bba\u6784\u5efa\u5e76\u6570\u503c\u5b9e\u73b0\u57fa\u4e8eRTD\u7684RC\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u624b\u5199\u6570\u5b57\u5206\u7c7b\u548cFruit~360\u6570\u636e\u96c6\u7684\u5bf9\u8c61\u8bc6\u522b\u3002", "result": "\u8be5\u67b6\u6784\u5728\u6027\u80fd\u8868\u73b0\u826f\u597d\u7684\u540c\u65f6\uff0c\u9075\u5faa\u65b0\u4e00\u4ee3RC\u539f\u5219\uff0c\u7528\u786e\u5b9a\u6027\u975e\u7ebf\u6027\u53d8\u6362\u66ff\u4ee3\u968f\u673a\u8fde\u63a5\u3002", "conclusion": "RTD-based RC\u67b6\u6784\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.15162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15162", "abs": "https://arxiv.org/abs/2507.15162", "authors": ["Firdaus Ahmed Choudhury", "Ethan Leicht", "Jude Ethan Bislig", "Hangzhi Guo", "Amulya Yadav"], "title": "Designing User-Centric Metrics for Evaluation of Counterfactual Explanations", "comment": null, "summary": "Machine learning-based decision models are increasingly being used to make\ndecisions that significantly impact people's lives, but their opaque nature\nleaves end users without a clear understanding of why a decision was made.\nCounterfactual Explanations (CFEs) have grown in popularity as a means of\noffering actionable guidance by identifying the minimum changes in feature\nvalues required to flip a model's prediction to something more desirable.\nUnfortunately, most prior research in CFEs relies on artificial evaluation\nmetrics, such as proximity, which may overlook end-user preferences and\nconstraints, e.g., the user's perception of effort needed to make certain\nfeature changes may differ from that of the model designer. To address this\nresearch gap, this paper makes three novel contributions. First, we conduct a\npilot study with 20 crowd-workers on Amazon MTurk to experimentally validate\nthe alignment of existing CF evaluation metrics with real-world user\npreferences. Results show that user-preferred CFEs matched those based on\nproximity in only 63.81% of cases, highlighting the limited applicability of\nthese metrics in real-world settings. Second, inspired by the need to design a\nuser-informed evaluation metric for CFEs, we conduct a more detailed two-day\nuser study with 41 participants facing realistic credit application scenarios\nto find experimental support for or against three intuitive hypotheses that may\nexplain how end users evaluate CFEs. Third, based on the findings of this\nsecond study, we propose the AWP model, a novel user-centric, two-stage model\nthat describes one possible mechanism by which users evaluate and select CFEs.\nOur results show that AWP predicts user-preferred CFEs with 84.37% accuracy.\nOur study provides the first human-centered validation for personalized cost\nmodels in CFE generation and highlights the need for adaptive, user-centered\nevaluation metrics.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFEs\uff09\u5728\u673a\u5668\u5b66\u4e60\u51b3\u7b56\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0e\u7528\u6237\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u4e2d\u5fc3\u7684\u4e24\u9636\u6bb5\u6a21\u578bAWP\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7528\u6237\u504f\u597dCFEs\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709CFE\u8bc4\u4f30\u6307\u6807\uff08\u5982\u63a5\u8fd1\u6027\uff09\u53ef\u80fd\u5ffd\u7565\u7528\u6237\u5b9e\u9645\u504f\u597d\u548c\u7ea6\u675f\uff0c\u5bfc\u81f4\u89e3\u91ca\u4e0e\u5b9e\u9645\u9700\u6c42\u8131\u8282\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u7528\u6237\u7814\u7a76\uff08\u5c0f\u89c4\u6a21\u8bd5\u70b9\u548c\u8be6\u7ec6\u7684\u4e24\u5929\u7814\u7a76\uff09\u9a8c\u8bc1\u73b0\u6709\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51faAWP\u6a21\u578b\u3002", "result": "\u8bd5\u70b9\u7814\u7a76\u663e\u793a\u7528\u6237\u504f\u597d\u4e0e\u73b0\u6709\u6307\u6807\u4ec563.81%\u4e00\u81f4\uff1bAWP\u6a21\u578b\u9884\u6d4b\u7528\u6237\u504f\u597dCFEs\u7684\u51c6\u786e\u7387\u8fbe84.37%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7528\u6237\u4e2d\u5fc3\u8bc4\u4f30\u6307\u6807\u7684\u91cd\u8981\u6027\uff0cAWP\u6a21\u578b\u4e3a\u4e2a\u6027\u5316CFE\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.15174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15174", "abs": "https://arxiv.org/abs/2507.15174", "authors": ["Justin Turnau", "Longchao Da", "Khoa Vo", "Ferdous Al Rafi", "Shreyas Bachiraju", "Tiejin Chen", "Hua Wei"], "title": "Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control", "comment": "This paper was accepted to RLC/RLJ 2025", "summary": "Traffic Signal Control (TSC) is essential for managing urban traffic flow and\nreducing congestion. Reinforcement Learning (RL) offers an adaptive method for\nTSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)\ngaining traction as intersections naturally function as coordinated agents.\nHowever, due to shifts in environmental dynamics, implementing MARL-based TSC\npolicies in the real world often leads to a significant performance drop, known\nas the sim-to-real gap. Grounded Action Transformation (GAT) has successfully\nmitigated this gap in single-agent RL for TSC, but real-world traffic networks,\nwhich involve numerous interacting intersections, are better suited to a MARL\nframework. In this work, we introduce JL-GAT, an application of GAT to\nMARL-based TSC that balances scalability with enhanced grounding capability by\nincorporating information from neighboring agents. JL-GAT adopts a\ndecentralized approach to GAT, allowing for the scalability often required in\nreal-world traffic networks while still capturing key interactions between\nagents. Comprehensive experiments on various road networks under simulated\nadverse weather conditions, along with ablation studies, demonstrate the\neffectiveness of JL-GAT. The code is publicly available at\nhttps://github.com/DaRL-LibSignal/JL-GAT/.", "AI": {"tldr": "JL-GAT\u662f\u4e00\u79cd\u5c06GAT\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u90bb\u8fd1\u667a\u80fd\u4f53\u4fe1\u606f\uff0c\u5e73\u8861\u53ef\u6269\u5c55\u6027\u4e0e\u589e\u5f3a\u7684\u63a5\u5730\u80fd\u529b\uff0c\u6709\u6548\u7f29\u5c0f\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u5b9e\u4ea4\u901a\u7f51\u7edc\u4e2d\u591a\u4ea4\u53c9\u53e3\u7684\u4ea4\u4e92\u7279\u6027\u66f4\u9002\u5408MARL\u6846\u67b6\uff0c\u4f46\u73b0\u6709GAT\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u5355\u667a\u80fd\u4f53RL\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eMARL\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6269\u5c55\u53c8\u4fdd\u7559\u5173\u952e\u4ea4\u4e92\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faJL-GAT\uff0c\u91c7\u7528\u5206\u6563\u5f0fGAT\u65b9\u6cd5\uff0c\u7ed3\u5408\u90bb\u8fd1\u667a\u80fd\u4f53\u4fe1\u606f\uff0c\u589e\u5f3aMARL\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u63a5\u5730\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u591a\u79cd\u9053\u8def\u7f51\u7edc\u4e2d\uff0cJL-GAT\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "JL-GAT\u6210\u529f\u5c06GAT\u6269\u5c55\u5230MARL\u6846\u67b6\uff0c\u4e3a\u73b0\u5b9e\u4ea4\u901a\u7f51\u7edc\u4e2d\u7684\u4fe1\u53f7\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15195", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15195", "abs": "https://arxiv.org/abs/2507.15195", "authors": ["Anwar Said", "Yifan Wei", "Ubaid Ullah Ahmad", "Mudassir Shabbir", "Waseem Abbas", "Xenofon Koutsoukos"], "title": "Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning", "comment": null, "summary": "In this article, we utilize the concept of average controllability in graphs,\nalong with a novel rank encoding method, to enhance the performance of Graph\nNeural Networks (GNNs) in social network classification tasks. GNNs have proven\nhighly effective in various network-based learning applications and require\nsome form of node features to function. However, their performance is heavily\ninfluenced by the expressiveness of these features. In social networks, node\nfeatures are often unavailable due to privacy constraints or the absence of\ninherent attributes, making it challenging for GNNs to achieve optimal\nperformance. To address this limitation, we propose two strategies for\nconstructing expressive node features. First, we introduce average\ncontrollability along with other centrality metrics (denoted as NCT-EFA) as\nnode-level metrics that capture critical aspects of network topology. Building\non this, we develop a rank encoding method that transforms average\ncontrollability or any other graph-theoretic metric into a fixed-dimensional\nfeature space, thereby improving feature representation. We conduct extensive\nnumerical evaluations using six benchmark GNN models across four social network\ndatasets to compare different node feature construction methods. Our results\ndemonstrate that incorporating average controllability into the feature space\nsignificantly improves GNN performance. Moreover, the proposed rank encoding\nmethod outperforms traditional one-hot degree encoding, improving the ROC AUC\nfrom 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,\nunderscoring its effectiveness in generating expressive and efficient node\nrepresentations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5e73\u5747\u53ef\u63a7\u6027\u548c\u65b0\u578b\u6392\u540d\u7f16\u7801\u65b9\u6cd5\u63d0\u5347GNN\u5728\u793e\u4ea4\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8282\u70b9\u7279\u5f81\u7f3a\u5931\u7684\u95ee\u9898\u3002", "motivation": "\u793e\u4ea4\u7f51\u7edc\u4e2d\u8282\u70b9\u7279\u5f81\u5e38\u56e0\u9690\u79c1\u6216\u56fa\u6709\u5c5e\u6027\u7f3a\u5931\u800c\u4e0d\u53ef\u7528\uff0c\u5f71\u54cdGNN\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a1\uff09\u4f7f\u7528\u5e73\u5747\u53ef\u63a7\u6027\u7b49\u4e2d\u5fc3\u6027\u6307\u6807\uff08NCT-EFA\uff09\u4f5c\u4e3a\u8282\u70b9\u7279\u5f81\uff1b2\uff09\u5f00\u53d1\u6392\u540d\u7f16\u7801\u65b9\u6cd5\u5c06\u56fe\u8bba\u6307\u6807\u8f6c\u5316\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e73\u5747\u53ef\u63a7\u6027\u663e\u8457\u63d0\u5347GNN\u6027\u80fd\uff0c\u6392\u540d\u7f16\u7801\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u72ec\u70ed\u7f16\u7801\uff08ROC AUC\u4ece68.7%\u63d0\u5347\u81f373.9%\uff09\u3002", "conclusion": "\u5e73\u5747\u53ef\u63a7\u6027\u548c\u6392\u540d\u7f16\u7801\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8282\u70b9\u7279\u5f81\u8868\u8fbe\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86GNN\u5728\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.15205", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15205", "abs": "https://arxiv.org/abs/2507.15205", "authors": ["Xinran Li", "Xiujuan Xu", "Jiaqi Qiao"], "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "summary": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u65b9\u6cd5LSDGNN\uff0c\u901a\u8fc7\u957f\u8ddd\u79bb\u548c\u77ed\u8ddd\u79bb\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u83b7\u53d6\u8fdc\u8ddd\u79bb\u548c\u8fd1\u8ddd\u79bb\u8bdd\u8bed\u7684\u591a\u6a21\u6001\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u5dee\u5206\u6b63\u5219\u5668\u548c\u53cc\u4eff\u5c04\u6a21\u5757\u4f18\u5316\u7279\u5f81\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u8bfe\u7a0b\u5b66\u4e60\uff08ICL\uff09\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u60c5\u611f\u8bc6\u522b\u5728\u5bf9\u8bdd\u4e2d\uff08ERC\uff09\u662f\u4e00\u4e2a\u5b9e\u9645\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u6709\u6548\u6355\u6349\u8fdc\u8ddd\u79bb\u548c\u8fd1\u8ddd\u79bb\u8bdd\u8bed\u7684\u7279\u5f81\u3002", "method": "\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u6784\u5efa\u957f\u8ddd\u79bb\u548c\u77ed\u8ddd\u79bb\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u5dee\u5206\u6b63\u5219\u5668\u548c\u53cc\u4eff\u5c04\u6a21\u5757\u4f18\u5316\u7279\u5f81\u4ea4\u4e92\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u8bfe\u7a0b\u5b66\u4e60\uff08ICL\uff09\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "LSDGNN\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u4f18\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u5728\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.15246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15246", "abs": "https://arxiv.org/abs/2507.15246", "authors": ["Rabia Latief Bhat", "Iqra Altaf Gillani"], "title": "Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks", "comment": null, "summary": "Accurate demand forecasting is critical for enhancing the efficiency and\nresponsiveness of food delivery platforms, where spatial heterogeneity and\ntemporal fluctuations in order volumes directly influence operational\ndecisions. This paper proposes an attention-based Graph Neural Network\nframework that captures spatial-temporal dependencies by modeling the food\ndelivery environment as a graph. In this graph, nodes represent urban delivery\nzones, while edges reflect spatial proximity and inter-regional order flow\npatterns derived from historical data. The attention mechanism dynamically\nweighs the influence of neighboring zones, enabling the model to focus on the\nmost contextually relevant areas during prediction. Temporal trends are jointly\nlearned alongside spatial interactions, allowing the model to adapt to evolving\ndemand patterns. Extensive experiments on real-world food delivery datasets\ndemonstrate the superiority of the proposed model in forecasting future order\nvolumes with high accuracy. The framework offers a scalable and adaptive\nsolution to support proactive fleet positioning, resource allocation, and\ndispatch optimization in urban food delivery operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u98df\u54c1\u914d\u9001\u4e2d\u7684\u65f6\u7a7a\u4f9d\u8d56\u6027\uff0c\u63d0\u5347\u8ba2\u5355\u91cf\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u98df\u54c1\u914d\u9001\u5e73\u53f0\u7684\u6548\u7387\u548c\u54cd\u5e94\u80fd\u529b\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u9700\u6c42\u9884\u6d4b\uff0c\u800c\u8ba2\u5355\u91cf\u7684\u65f6\u7a7a\u5f02\u8d28\u6027\u76f4\u63a5\u5f71\u54cd\u8fd0\u8425\u51b3\u7b56\u3002", "method": "\u5c06\u914d\u9001\u73af\u5883\u5efa\u6a21\u4e3a\u56fe\uff0c\u8282\u70b9\u4ee3\u8868\u914d\u9001\u533a\u57df\uff0c\u8fb9\u53cd\u6620\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u5386\u53f2\u8ba2\u5355\u6d41\u6a21\u5f0f\uff1b\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u52a0\u6743\u90bb\u8fd1\u533a\u57df\u7684\u5f71\u54cd\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u652f\u6301\u8f66\u961f\u5b9a\u4f4d\u3001\u8d44\u6e90\u5206\u914d\u548c\u8c03\u5ea6\u4f18\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57ce\u5e02\u98df\u54c1\u914d\u9001\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15260", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15260", "abs": "https://arxiv.org/abs/2507.15260", "authors": ["Jiaqi Han", "Haotian Ye", "Puheng Li", "Minkai Xu", "James Zou", "Stefano Ermon"], "title": "CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers", "comment": "ICCV 2025", "summary": "Diffusion-based generative models have become dominant generators of\nhigh-fidelity images and videos but remain limited by their computationally\nexpensive inference procedures. Existing acceleration techniques either require\nextensive model retraining or compromise significantly on sample quality. This\npaper explores a general, training-free, and model-agnostic acceleration\nstrategy via multi-core parallelism. Our framework views multi-core diffusion\nsampling as an ODE solver pipeline, where slower yet accurate solvers\nprogressively rectify faster solvers through a theoretically justified\ninter-core communication mechanism. This motivates our multi-core training-free\ndiffusion sampling accelerator, CHORDS, which is compatible with various\ndiffusion samplers, model architectures, and modalities. Through extensive\nexperiments, CHORDS significantly accelerates sampling across diverse\nlarge-scale image and video diffusion models, yielding up to 2.1x speedup with\nfour cores, improving by 50% over baselines, and 2.9x speedup with eight cores,\nall without quality degradation. This advancement enables CHORDS to establish a\nsolid foundation for real-time, high-fidelity diffusion generation.", "AI": {"tldr": "CHORDS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6838\u5e76\u884c\u7684\u8bad\u7ec3\u514d\u8d39\u3001\u6a21\u578b\u65e0\u5173\u7684\u6269\u6563\u91c7\u6837\u52a0\u901f\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u901f\u5ea6\u4e14\u4e0d\u964d\u4f4e\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u751f\u6210\u80fd\u529b\u53d7\u9650\u4e8e\u8ba1\u7b97\u6602\u8d35\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u727a\u7272\u8d28\u91cf\u3002", "method": "\u5c06\u591a\u6838\u6269\u6563\u91c7\u6837\u89c6\u4e3aODE\u6c42\u89e3\u5668\u7ba1\u9053\uff0c\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684\u6838\u95f4\u901a\u4fe1\u673a\u5236\uff0c\u6162\u800c\u51c6\u7684\u6c42\u89e3\u5668\u9010\u6b65\u4fee\u6b63\u5feb\u800c\u7c97\u7cd9\u7684\u6c42\u89e3\u5668\u3002", "result": "CHORDS\u5728\u591a\u6837\u5927\u89c4\u6a21\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u5b9e\u73b0\u663e\u8457\u52a0\u901f\uff0c\u56db\u6838\u63d0\u901f2.1\u500d\uff0c\u516b\u6838\u63d0\u901f2.9\u500d\uff0c\u65e0\u8d28\u91cf\u635f\u5931\u3002", "conclusion": "CHORDS\u4e3a\u5b9e\u65f6\u9ad8\u4fdd\u771f\u6269\u6563\u751f\u6210\u5960\u5b9a\u57fa\u7840\uff0c\u63d0\u4f9b\u9ad8\u6548\u4e14\u901a\u7528\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2507.15274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15274", "abs": "https://arxiv.org/abs/2507.15274", "authors": ["Matthew J. Bryan", "Felix Schwock", "Azadeh Yazdan-Shahmorad", "Rajesh P N Rao"], "title": "Temporal Basis Function Models for Closed-Loop Neural Stimulation", "comment": null, "summary": "Closed-loop neural stimulation provides novel therapies for neurological\ndiseases such as Parkinson's disease (PD), but it is not yet clear whether\nartificial intelligence (AI) techniques can tailor closed-loop stimulation to\nindividual patients or identify new therapies. Progress requires us to address\na number of translational issues, including sample efficiency, training time,\nand minimizing loop latency such that stimulation may be shaped in response to\nchanging brain activity. We propose temporal basis function models (TBFMs) to\naddress these difficulties, and explore this approach in the context of\nexcitatory optogenetic stimulation. We demonstrate the ability of TBF models to\nprovide a single-trial, spatiotemporal forward prediction of the effect of\noptogenetic stimulation on local field potentials (LFPs) measured in two\nnon-human primates. We further use simulations to demonstrate the use of TBF\nmodels for closed-loop stimulation, driving neural activity towards target\npatterns. The simplicity of TBF models allow them to be sample efficient, rapid\nto train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the\nmodel on 40 sessions of previously published excitatory optogenetic stimulation\ndata. For each session, the model required 15-20min of data collection to\nsuccessfully model the remainder of the session. It achieved a prediction\naccuracy comparable to a baseline nonlinear dynamical systems model that\nrequires hours to train, and superior accuracy to a linear state-space model.\nIn our simulations, it also successfully allowed a closed-loop stimulator to\ncontrol a neural circuit. Our approach begins to bridge the translational gap\nbetween complex AI-based approaches to modeling dynamical systems and the\nvision of using such forward prediction models to develop novel, clinically\nuseful closed-loop stimulation protocols.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u57fa\u51fd\u6570\u6a21\u578b\uff08TBFMs\uff09\u7684\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u6cbb\u7597\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff0c\u5982\u5e15\u91d1\u68ee\u75c5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5355\u6b21\u8bd5\u9a8c\u4e2d\u7684\u9ad8\u6548\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u5728\u4e2a\u4f53\u5316\u6cbb\u7597\u4e2d\u7684\u6837\u672c\u6548\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8AI\u6280\u672f\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u57fa\u51fd\u6570\u6a21\u578b\uff08TBFMs\uff09\u8fdb\u884c\u5355\u6b21\u8bd5\u9a8c\u7684\u65f6\u7a7a\u524d\u5411\u9884\u6d4b\uff0c\u6a21\u62df\u95ed\u73af\u523a\u6fc0\u5bf9\u795e\u7ecf\u6d3b\u52a8\u7684\u8c03\u63a7\u3002", "result": "TBF\u6a21\u578b\u572840\u6b21\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff08\u8bad\u7ec3\u65f6\u95f42-4\u5206\u949f\uff0c\u5ef6\u8fdf0.2ms\uff09\uff0c\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u57fa\u7ebf\u975e\u7ebf\u6027\u6a21\u578b\u76f8\u5f53\uff0c\u4f18\u4e8e\u7ebf\u6027\u6a21\u578b\u3002", "conclusion": "TBF\u6a21\u578b\u4e3aAI\u9a71\u52a8\u7684\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u5e94\u7528\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.15280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15280", "abs": "https://arxiv.org/abs/2507.15280", "authors": ["Shaofei Shen", "Chenhao Zhang", "Yawen Zhao", "Alina Bialkowski", "Weitong Chen", "Miao Xu"], "title": "Machine Unlearning for Streaming Forgetting", "comment": null, "summary": "Machine unlearning aims to remove knowledge of the specific training data in\na well-trained model. Currently, machine unlearning methods typically handle\nall forgetting data in a single batch, removing the corresponding knowledge all\nat once upon request. However, in practical scenarios, requests for data\nremoval often arise in a streaming manner rather than in a single batch,\nleading to reduced efficiency and effectiveness in existing methods. Such\nchallenges of streaming forgetting have not been the focus of much research. In\nthis paper, to address the challenges of performance maintenance, efficiency,\nand data access brought about by streaming unlearning requests, we introduce a\nstreaming unlearning paradigm, formalizing the unlearning as a distribution\nshift problem. We then estimate the altered distribution and propose a novel\nstreaming unlearning algorithm to achieve efficient streaming forgetting\nwithout requiring access to the original training data. Theoretical analyses\nconfirm an $O(\\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,\nwhere $V_T$ represents the cumulative total variation in the optimal solution\nover $T$ learning rounds. This theoretical guarantee is achieved under mild\nconditions without the strong restriction of convex loss function. Experiments\nacross various models and datasets validate the performance of our proposed\nmethod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f\u9057\u5fd8\u5b66\u4e60\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6d41\u5f0f\u6570\u636e\u9057\u5fd8\u8bf7\u6c42\u65f6\u7684\u6548\u7387\u548c\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u901a\u5e38\u6279\u91cf\u5904\u7406\u9057\u5fd8\u6570\u636e\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u6570\u636e\u9057\u5fd8\u8bf7\u6c42\u5f80\u5f80\u662f\u6d41\u5f0f\u7684\uff0c\u5bfc\u81f4\u6548\u7387\u548c\u6548\u679c\u4e0b\u964d\u3002", "method": "\u5c06\u9057\u5fd8\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u6d41\u5f0f\u9057\u5fd8\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d41\u5f0f\u9057\u5fd8\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u6d41\u5f0f\u9057\u5fd8\u7684\u6311\u6218\u3002"}}
{"id": "2507.15287", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15287", "abs": "https://arxiv.org/abs/2507.15287", "authors": ["Elias Malomgr\u00e9", "Pieter Simoens"], "title": "Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning", "comment": "10 pages, 8 figures, accepted for the non-archival workshop \"Workshop\n  on Reinforcement Learning Beyond Rewards @ Reinforcement Learning Conference\n  2025\"", "summary": "Recent trends in Reinforcement Learning (RL) highlight the need for agents to\nlearn from reward-free interactions and alternative supervision signals, such\nas unlabeled or incomplete demonstrations, rather than relying solely on\nexplicit reward maximization. Additionally, developing generalist agents that\ncan adapt efficiently in real-world environments often requires leveraging\nthese reward-free signals to guide learning and behavior. However, while\nintrinsic motivation techniques provide a means for agents to seek out novel or\nuncertain states in the absence of explicit rewards, they are often challenged\nby dense reward environments or the complexity of high-dimensional state and\naction spaces. Furthermore, most existing approaches rely directly on the\nunprocessed intrinsic reward signals, which can make it difficult to shape or\ncontrol the agent's exploration effectively. We propose a framework that can\neffectively utilize expert demonstrations, even when they are incomplete and\nimperfect. By applying a mapping function to transform the similarity between\nan agent's state and expert data into a shaped intrinsic reward, our method\nallows for flexible and targeted exploration of expert-like behaviors. We\nemploy a Mixture of Autoencoder Experts to capture a diverse range of behaviors\nand accommodate missing information in demonstrations. Experiments show our\napproach enables robust exploration and strong performance in both sparse and\ndense reward environments, even when demonstrations are sparse or incomplete.\nThis provides a practical framework for RL in realistic settings where optimal\ndata is unavailable and precise reward control is needed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e0d\u5b8c\u6574\u4e13\u5bb6\u6f14\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5c06\u72b6\u6001\u4e0e\u4e13\u5bb6\u6570\u636e\u7684\u76f8\u4f3c\u6027\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u652f\u6301\u7075\u6d3b\u63a2\u7d22\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u663e\u5f0f\u5956\u52b1\u7a00\u7f3a\u4e14\u4e13\u5bb6\u6f14\u793a\u5e38\u4e0d\u5b8c\u6574\uff0c\u9700\u5f00\u53d1\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4fe1\u53f7\u7684\u901a\u7528\u667a\u80fd\u4f53\u3002", "method": "\u4f7f\u7528\u6620\u5c04\u51fd\u6570\u5c06\u72b6\u6001\u4e0e\u4e13\u5bb6\u6570\u636e\u7684\u76f8\u4f3c\u6027\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u81ea\u7f16\u7801\u4e13\u5bb6\u6a21\u578b\u5904\u7406\u4e0d\u5b8c\u6574\u6f14\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u5956\u52b1\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u6f14\u793a\u6570\u636e\u4e0d\u5b8c\u6574\u3002", "conclusion": "\u4e3a\u73b0\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u6700\u4f18\u6570\u636e\u548c\u7cbe\u786e\u5956\u52b1\u63a7\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2507.15288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15288", "abs": "https://arxiv.org/abs/2507.15288", "authors": ["Omid G. Sani", "Maryam M. Shanechi"], "title": "Preferential subspace identification (PSID) with forward-backward smoothing", "comment": "17 pages, 5 figures", "summary": "System identification methods for multivariate time-series, such as neural\nand behavioral recordings, have been used to build models for predicting one\nfrom the other. For example, Preferential Subspace Identification (PSID) builds\na state-space model of a primary time-series (e.g., neural activity) to\noptimally predict a secondary time-series (e.g., behavior). However, PSID\nfocuses on optimal prediction using past primary data, even though in offline\napplications, better estimation can be achieved by incorporating concurrent\ndata (filtering) or all available data (smoothing). Here, we extend PSID to\nenable optimal filtering and smoothing. First, we show that the presence of a\nsecondary signal makes it possible to uniquely identify a model with an optimal\nKalman update step (to enable filtering) from a family of otherwise equivalent\nstate-space models. Our filtering solution augments PSID with a reduced-rank\nregression step that directly learns the optimal gain required for the update\nstep from data. We refer to this extension of PSID as PSID with filtering.\nSecond, inspired by two-filter Kalman smoother formulations, we develop a novel\nforward-backward PSID smoothing algorithm where we first apply PSID with\nfiltering and then apply it again in the reverse time direction on the\nresiduals of the filtered secondary signal. We validate our methods on\nsimulated data, showing that our approach recovers the ground-truth model\nparameters for filtering, and achieves optimal filtering and smoothing decoding\nperformance of the secondary signal that matches the ideal performance of the\ntrue underlying model. This work provides a principled framework for optimal\nlinear filtering and smoothing in the two-signal setting, significantly\nexpanding the toolkit for analyzing dynamic interactions in multivariate\ntime-series.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6269\u5c55\u4e86PSID\u65b9\u6cd5\uff0c\u5f15\u5165\u6ee4\u6ce2\u548c\u5e73\u6ed1\u6280\u672f\uff0c\u4f18\u5316\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "motivation": "\u73b0\u6709PSID\u65b9\u6cd5\u4ec5\u5229\u7528\u8fc7\u53bb\u6570\u636e\u9884\u6d4b\uff0c\u800c\u6ee4\u6ce2\u548c\u5e73\u6ed1\u6280\u672f\u53ef\u63d0\u5347\u79bb\u7ebf\u5e94\u7528\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faPSID\u6ee4\u6ce2\u6269\u5c55\u548c\u65b0\u578b\u524d\u5411-\u540e\u5411PSID\u5e73\u6ed1\u7b97\u6cd5\uff0c\u901a\u8fc7\u964d\u79e9\u56de\u5f52\u5b66\u4e60\u6700\u4f18\u589e\u76ca\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u80fd\u6062\u590d\u771f\u5b9e\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u5b9e\u73b0\u6700\u4f18\u6ee4\u6ce2\u548c\u5e73\u6ed1\u89e3\u7801\u6027\u80fd\u3002", "conclusion": "\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u7ebf\u6027\u6ee4\u6ce2\u548c\u5e73\u6ed1\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u52a8\u6001\u4ea4\u4e92\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2507.15290", "categories": ["cs.LG", "I.2.6; I.2.0"], "pdf": "https://arxiv.org/pdf/2507.15290", "abs": "https://arxiv.org/abs/2507.15290", "authors": ["Emile Anand", "Sarah Liaw"], "title": "Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown", "comment": "39 pages, 2 figures, 36 tables", "summary": "Thompson Sampling (TS) is widely used to address the exploration/exploitation\ntradeoff in contextual bandits, yet recent theory shows that it does not\nexplore aggressively enough in high-dimensional problems. Feel-Good Thompson\nSampling (FG-TS) addresses this by adding an optimism bonus that biases toward\nhigh-reward models, and it achieves the asymptotically minimax-optimal regret\nin the linear setting when posteriors are exact. However, its performance with\n\\emph{approximate} posteriors -- common in large-scale or neural problems --\nhas not been benchmarked. We provide the first systematic study of FG-TS and\nits smoothed variant (SFG-TS) across eleven real-world and synthetic\nbenchmarks. To evaluate their robustness, we compare performance across\nsettings with exact posteriors (linear and logistic bandits) to approximate\nregimes produced by fast but coarse stochastic-gradient samplers. Ablations\nover preconditioning, bonus scale, and prior strength reveal a trade-off:\nlarger bonuses help when posterior samples are accurate, but hurt when sampling\nnoise dominates. FG-TS generally outperforms vanilla TS in linear and logistic\nbandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS\nand its variants are competitive and easy-to-use, we recommend them as\nbaselines in modern contextual-bandit benchmarks. Finally, we provide source\ncode for all our experiments in\nhttps://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.", "AI": {"tldr": "FG-TS\u901a\u8fc7\u4e50\u89c2\u5956\u52b1\u63d0\u5347\u63a2\u7d22\u80fd\u529b\uff0c\u5728\u7cbe\u786e\u540e\u9a8c\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8fd1\u4f3c\u540e\u9a8c\u4e0b\u8868\u73b0\u672a\u5145\u5206\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u8868\u660eFG-TS\u5728\u7ebf\u6027\u548c\u903b\u8f91\u95ee\u9898\u4e2d\u4f18\u4e8e\u4f20\u7edfTS\uff0c\u4f46\u5728\u795e\u7ecf\u7f51\u7edc\u95ee\u9898\u4e2d\u8f83\u5f31\u3002", "motivation": "\u89e3\u51b3Thompson Sampling\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1FG-TS\u5728\u8fd1\u4f3c\u540e\u9a8c\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528FG-TS\u53ca\u5176\u5e73\u6ed1\u53d8\u4f53SFG-TS\uff0c\u572811\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u57fa\u51c6\u4e0a\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u7cbe\u786e\u548c\u8fd1\u4f3c\u540e\u9a8c\u4e0b\u7684\u8868\u73b0\u3002", "result": "FG-TS\u5728\u7ebf\u6027\u548c\u903b\u8f91\u95ee\u9898\u4e2d\u4f18\u4e8e\u4f20\u7edfTS\uff0c\u4f46\u5728\u795e\u7ecf\u7f51\u7edc\u95ee\u9898\u4e2d\u8868\u73b0\u8f83\u5dee\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u5956\u52b1\u89c4\u6a21\u548c\u91c7\u6837\u566a\u58f0\u7684\u6743\u8861\u3002", "conclusion": "FG-TS\u53ca\u5176\u53d8\u4f53\u6613\u4e8e\u4f7f\u7528\u4e14\u6027\u80fd\u6709\u7ade\u4e89\u529b\uff0c\u5efa\u8bae\u4f5c\u4e3a\u73b0\u4ee3\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u57fa\u51c6\u7684\u57fa\u7ebf\u3002\u5b9e\u9a8c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15303", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.15303", "abs": "https://arxiv.org/abs/2507.15303", "authors": ["Liang Zhang", "Kong Chen", "Yuen Wu"], "title": "Universal crystal material property prediction via multi-view geometric fusion in graph transformers", "comment": null, "summary": "Accurately and comprehensively representing crystal structures is critical\nfor advancing machine learning in large-scale crystal materials simulations,\nhowever, effectively capturing and leveraging the intricate geometric and\ntopological characteristics of crystal structures remains a core, long-standing\nchallenge for most existing methods in crystal property prediction. Here, we\npropose MGT, a multi-view graph transformer framework that synergistically\nfuses SE3 invariant and SO3 equivariant graph representations, which\nrespectively captures rotation-translation invariance and rotation equivariance\nin crystal geometries. To strategically incorporate these complementary\ngeometric representations, we employ a lightweight mixture of experts router in\nMGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on\nthe specific target task. Compared with previous state-of-the-art models, MGT\nreduces the mean absolute error by up to 21% on crystal property prediction\ntasks through multi-task self-supervised pretraining. Ablation experiments and\ninterpretable investigations confirm the effectiveness of each technique\nimplemented in our framework. Additionally, in transfer learning scenarios\nincluding crystal catalyst adsorption energy and hybrid perovskite bandgap\nprediction, MGT achieves performance improvements of up to 58% over existing\nbaselines, demonstrating domain-agnostic scalability across diverse application\ndomains. As evidenced by the above series of studies, we believe that MGT can\nserve as useful model for crystal material property prediction, providing a\nvaluable tool for the discovery of novel materials.", "AI": {"tldr": "MGT\u662f\u4e00\u79cd\u591a\u89c6\u56fe\u56fe\u53d8\u6362\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408SE3\u4e0d\u53d8\u548cSO3\u7b49\u53d8\u56fe\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6676\u4f53\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u6676\u4f53\u7ed3\u6784\u7684\u590d\u6742\u51e0\u4f55\u548c\u62d3\u6251\u7279\u5f81\uff0c\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u6676\u4f53\u6750\u6599\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMGT\u6846\u67b6\uff0c\u7ed3\u5408SE3\u4e0d\u53d8\u548cSO3\u7b49\u53d8\u56fe\u8868\u793a\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6df7\u5408\u8def\u7531\u5668\u52a8\u6001\u8c03\u6574\u6743\u91cd\u3002", "result": "MGT\u5728\u6676\u4f53\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e21%\uff0c\u5728\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e2d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe58%\u3002", "conclusion": "MGT\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6676\u4f53\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u5177\u6709\u8de8\u9886\u57df\u6269\u5c55\u6027\uff0c\u4e3a\u65b0\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.15336", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.15336", "abs": "https://arxiv.org/abs/2507.15336", "authors": ["Jialiang Wang", "Hanmo Liu", "Shimin Di", "Zhili Wang", "Jiachuan Wang", "Lei Chen", "Xiaofang Zhou"], "title": "Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design", "comment": null, "summary": "Database systems have recently advocated for embedding machine learning (ML)\ncapabilities, offering declarative model queries over large, managed model\nrepositories, thereby circumventing the huge computational overhead of\ntraditional ML-based algorithms in automated neural network model selection.\nPioneering database studies aim to organize existing benchmark repositories as\nmodel bases (MB), querying them for the model records with the highest\nperformance estimation metrics for given tasks. However, this static model\nselection practice overlooks the fine-grained, evolving relational dependencies\nbetween diverse task queries and model architecture variations, resulting in\nsuboptimal matches and failing to further refine the model effectively. To fill\nthe model refinement gap in database research, we propose M-DESIGN, a curated\nmodel knowledge base (MKB) pipeline for mastering neural network refinement by\nadaptively weaving prior insights about model architecture modification. First,\nwe propose a knowledge weaving engine that reframes model refinement as an\nadaptive query problem over task metadata. Given a user's task query, M-DESIGN\nquickly matches and iteratively refines candidate models by leveraging a\ngraph-relational knowledge schema that explicitly encodes data properties,\narchitecture variations, and pairwise performance deltas as joinable relations.\nThis schema supports fine-grained relational analytics over architecture tweaks\nand drives a predictive query planner that can detect and adapt to\nout-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics\ntasks, where our model knowledge base enriches existing benchmarks with\nstructured metadata covering 3 graph tasks and 22 graph datasets, contributing\ndata records of 67,760 graph models. Empirical results demonstrate that\nM-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited\nbudgets.", "AI": {"tldr": "M-DESIGN\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6a21\u578b\u77e5\u8bc6\u5e93\uff08MKB\uff09\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u6574\u5408\u6a21\u578b\u67b6\u6784\u4fee\u6539\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u586b\u8865\u4e86\u6570\u636e\u5e93\u7814\u7a76\u4e2d\u6a21\u578b\u4f18\u5316\u7684\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4efb\u52a1\u67e5\u8be2\u4e0e\u6a21\u578b\u67b6\u6784\u4e4b\u95f4\u7684\u52a8\u6001\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u5339\u914d\u4e0d\u4f18\u4e14\u65e0\u6cd5\u6709\u6548\u4f18\u5316\u6a21\u578b\u3002", "method": "M-DESIGN\u901a\u8fc7\u77e5\u8bc6\u7f16\u7ec7\u5f15\u64ce\u5c06\u6a21\u578b\u4f18\u5316\u91cd\u6784\u4e3a\u4efb\u52a1\u5143\u6570\u636e\u7684\u81ea\u9002\u5e94\u67e5\u8be2\u95ee\u9898\uff0c\u5229\u7528\u56fe\u5173\u7cfb\u77e5\u8bc6\u6a21\u5f0f\u652f\u6301\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5e76\u9a71\u52a8\u9884\u6d4b\u6027\u67e5\u8be2\u89c4\u5212\u5668\u3002", "result": "\u572833\u4e2a\u6570\u636e-\u4efb\u52a1\u5bf9\u4e2d\uff0cM-DESIGN\u5728\u6709\u9650\u9884\u7b97\u5185\u4e3a26\u5bf9\u63d0\u4f9b\u4e86\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "M-DESIGN\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u5e93\u548c\u81ea\u9002\u5e94\u67e5\u8be2\u4f18\u5316\u6a21\u578b\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5339\u914d\u548c\u4f18\u5316\u6548\u679c\u3002"}}
{"id": "2507.15349", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15349", "abs": "https://arxiv.org/abs/2507.15349", "authors": ["Zehua Cheng", "Rui Sun", "Jiahao Sun", "Yike Guo"], "title": "Scaling Decentralized Learning with FLock", "comment": null, "summary": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data.", "AI": {"tldr": "FLock\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b89\u5168\u9ad8\u6548\u7684LLM\u5fae\u8c03\uff0c\u901a\u8fc7\u533a\u5757\u94fe\u4fe1\u4efb\u5c42\u548c\u7ecf\u6d4e\u6fc0\u52b1\u66ff\u4ee3\u4e2d\u5fc3\u5316\u805a\u5408\u5668\uff0c\u9996\u6b21\u5b9e\u73b070B\u53c2\u6570\u6a21\u578b\u5728\u5f02\u6784\u3001\u65e0\u4fe1\u4efb\u73af\u5883\u4e2d\u7684\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u4e2d\u5fc3\u5316\u63a7\u5236\u7684\u4e0d\u8db3\u548c\u53bb\u4e2d\u5fc3\u5316\u65b9\u6848\u4e2d\u7684\u8ba1\u7b97\u4e0e\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u5355\u70b9\u653b\u51fb\u548c\u6570\u636e\u4e2d\u6bd2\u98ce\u9669\u3002", "method": "\u96c6\u6210\u533a\u5757\u94fe\u4fe1\u4efb\u5c42\u4e0e\u7ecf\u6d4e\u6fc0\u52b1\uff0c\u8bbe\u8ba1\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u7684\u5408\u4f5c\u534f\u8bae\uff0c\u66ff\u4ee3\u4e2d\u5fc3\u5316\u805a\u5408\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFLock\u80fd\u62b5\u5fa1\u540e\u95e8\u4e2d\u6bd2\u653b\u51fb\uff0c\u964d\u4f4e\u5bf9\u6297\u653b\u51fb\u6210\u529f\u738768%\u4ee5\u4e0a\uff0c\u5e76\u5b9e\u73b0\u8de8\u9886\u57df\u77e5\u8bc6\u534f\u540c\u3002", "conclusion": "FLock\u5728\u5b89\u5168\u3001\u591a\u9886\u57df\u3001\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u6210\u529f\u5fae\u8c0370B LLM\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15381", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15381", "abs": "https://arxiv.org/abs/2507.15381", "authors": ["Julia Machnio", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "title": "To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models", "comment": "ICCV 2025", "summary": "Active learning (AL) seeks to reduce annotation costs by selecting the most\ninformative samples for labeling, making it particularly valuable in\nresource-constrained settings. However, traditional evaluation methods, which\nfocus solely on final accuracy, fail to capture the full dynamics of the\nlearning process. To address this gap, we propose PALM (Performance Analysis of\nActive Learning Models), a unified and interpretable mathematical model that\ncharacterizes AL trajectories through four key parameters: achievable accuracy,\ncoverage efficiency, early-stage performance, and scalability. PALM provides a\npredictive description of AL behavior from partial observations, enabling the\nestimation of future performance and facilitating principled comparisons across\ndifferent strategies. We validate PALM through extensive experiments on\nCIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and\nself-supervised embeddings. Our results demonstrate that PALM generalizes\neffectively across datasets, budgets, and strategies, accurately predicting\nfull learning curves from limited labeled data. Importantly, PALM reveals\ncrucial insights into learning efficiency, data space coverage, and the\nscalability of AL methods. By enabling the selection of cost-effective\nstrategies and predicting performance under tight budget constraints, PALM lays\nthe basis for more systematic, reproducible, and data-efficient evaluation of\nAL in both research and real-world applications. The code is available at:\nhttps://github.com/juliamachnio/PALM.", "AI": {"tldr": "PALM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u7684\u52a8\u6001\u8fc7\u7a0b\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u53c2\u6570\u9884\u6d4b\u6027\u80fd\u5e76\u6bd4\u8f83\u4e0d\u540c\u7b56\u7565\u3002", "motivation": "\u4f20\u7edfAL\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u51c6\u786e\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u5b66\u4e60\u8fc7\u7a0b\u52a8\u6001\u3002PALM\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faPALM\u6a21\u578b\uff0c\u901a\u8fc7\u56db\u4e2a\u53c2\u6570\uff08\u53ef\u8fbe\u5230\u7684\u51c6\u786e\u6027\u3001\u8986\u76d6\u6548\u7387\u3001\u65e9\u671f\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff09\u63cf\u8ff0AL\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u7b56\u7565\u4e0a\u9a8c\u8bc1PALM\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u9884\u6d4b\u5b66\u4e60\u66f2\u7ebf\u5e76\u63ed\u793aAL\u65b9\u6cd5\u7684\u6548\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "PALM\u4e3aAL\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u5728\u6709\u9650\u9884\u7b97\u4e0b\u9009\u62e9\u9ad8\u6548\u7b56\u7565\uff0c\u4fc3\u8fdb\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2507.15431", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15431", "abs": "https://arxiv.org/abs/2507.15431", "authors": ["Andrew Gracyk"], "title": "The calculus of variations of the Transformer on the hyperspherical tangent bundle", "comment": "First version", "summary": "We offer a theoretical mathematical background to Transformers through\nLagrangian optimization across the token space. The Transformer, as a flow map,\nexists in the tangent fiber for each token along the high-dimensional unit\nsphere. The circumstance of the hypersphere across the latent data is\nreasonable due to the trained diagonal matrix equal to the identity, which has\nvarious empirical justifications. Thus, under the continuum limit of the\ndynamics, the latent vectors flow among the tangent bundle. Using these facts,\nwe devise a mathematical framework for the Transformer through calculus of\nvariations. We develop a functional and show that the continuous flow map\ninduced by the Transformer satisfies this functional, therefore the Transformer\ncan be viewed as a natural solver of a calculus of variations problem. We\ninvent new scenarios of when our methods are applicable based on loss\noptimization with respect to path optimality. We derive the Euler-Lagrange\nequation for the Transformer. The variant of the Euler-Lagrange equation we\npresent has various appearances in literature, but, to our understanding,\noftentimes not foundationally proven or under other specialized cases. Our\noverarching proof is new: our techniques are classical and the use of the flow\nmap object is original. We provide several other relevant results, primarily\nones specific to neural scenarios. In particular, much of our analysis will be\nattempting to quantify Transformer data in variational contexts under neural\napproximations. Calculus of variations on manifolds is a well-nourished\nresearch area, but for the Transformer specifically, it is uncharted: we lay\nthe foundation for this area through an introduction to the Lagrangian for the\nTransformer.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u4f18\u5316\u5728\u4ee4\u724c\u7a7a\u95f4\u4e0a\u4e3aTransformer\u63d0\u4f9b\u4e86\u7406\u8bba\u6570\u5b66\u80cc\u666f\uff0c\u5c06\u5176\u89c6\u4e3a\u9ad8\u7ef4\u5355\u4f4d\u7403\u4e0a\u7684\u5207\u7ea4\u7ef4\u6d41\u56fe\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u53d8\u5206\u95ee\u9898\u7684\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\u3002", "motivation": "\u4e3aTransformer\u5efa\u7acb\u6570\u5b66\u6846\u67b6\uff0c\u586b\u8865\u5176\u5728\u6d41\u5f62\u53d8\u5206\u8ba1\u7b97\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u53d8\u5206\u6cd5\u5f00\u53d1\u529f\u80fd\u6846\u67b6\uff0c\u8bc1\u660eTransformer\u662f\u53d8\u5206\u95ee\u9898\u7684\u81ea\u7136\u6c42\u89e3\u5668\uff0c\u5e76\u63a8\u5bfc\u5176\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\u3002", "result": "\u63d0\u51fa\u4e86\u65b0\u7684\u9002\u7528\u573a\u666f\uff0c\u8bc1\u660e\u4e86Transformer\u4f5c\u4e3a\u53d8\u5206\u95ee\u9898\u6c42\u89e3\u5668\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u6570\u5b66\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u4e3aTransformer\u5728\u53d8\u5206\u8ba1\u7b97\u9886\u57df\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u5728\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u4e0b\u7684\u53d8\u5206\u5206\u6790\u6f5c\u529b\u3002"}}
{"id": "2507.15442", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15442", "abs": "https://arxiv.org/abs/2507.15442", "authors": ["Owen Douglas", "Aku Kammonen", "Anamika Pandey", "Ra\u00fal Tempone"], "title": "An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations", "comment": "20 Pages", "summary": "This work proposes a training algorithm based on adaptive random Fourier\nfeatures (ARFF) with Metropolis sampling and resampling\n\\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and\ndiffusion components of stochastic differential equations from snapshot data.\nSpecifically, this study considers It\\^{o} diffusion processes and a\nlikelihood-based loss function derived from the Euler-Maruyama integration\nintroduced in \\cite{Dietrich2023} and\n\\cite{dridi2021learningstochasticdynamicalsystems}.\n  This work evaluates the proposed method against benchmark problems presented\nin \\cite{Dietrich2023}, including polynomial examples, underdamped Langevin\ndynamics, a stochastic susceptible-infected-recovered model, and a stochastic\nwave equation. Across all cases, the ARFF-based approach matches or surpasses\nthe performance of conventional Adam-based optimization in both loss\nminimization and convergence speed. These results highlight the potential of\nARFF as a compelling alternative for data-driven modeling of stochastic\ndynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08ARFF\uff09\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5feb\u7167\u6570\u636e\u4e2d\u5b66\u4e60\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u6f02\u79fb\u548c\u6269\u6563\u5206\u91cf\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4ece\u5feb\u7167\u6570\u636e\u4e2d\u5b66\u4e60\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6f02\u79fb\u548c\u6269\u6563\u5206\u91cf\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u66f4\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08ARFF\uff09\u7ed3\u5408Metropolis\u91c7\u6837\u548c\u91cd\u91c7\u6837\uff0c\u57fa\u4e8eEuler-Maruyama\u79ef\u5206\u7684\u4f3c\u7136\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARFF\u65b9\u6cd5\u5728\u635f\u5931\u6700\u5c0f\u5316\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfAdam\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "ARFF\u65b9\u6cd5\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u968f\u673a\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.15470", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15470", "abs": "https://arxiv.org/abs/2507.15470", "authors": ["Baran Can G\u00fcl", "Suraksha Nadig", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "title": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning", "comment": "Preprint version. Accepted for publication at IEEE ICECCME 2025", "summary": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings.", "AI": {"tldr": "FedMultiEmo\u662f\u4e00\u4e2a\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u751f\u7406\u6570\u636e\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e14\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u89e3\u51b3\u8f66\u8f7d\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u8106\u5f31\u6027\u3001\u751f\u7406\u53d8\u5f02\u6027\u548c\u9690\u79c1\u98ce\u9669\u95ee\u9898\u3002", "method": "\u4f7f\u7528CNN\u5904\u7406\u9762\u90e8\u56fe\u50cf\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u751f\u7406\u6570\u636e\uff0c\u901a\u8fc7\u591a\u6570\u6295\u7968\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u8054\u90a6\u5e73\u5747\u65b9\u6848\u3002", "result": "\u8054\u90a6CNN\u51c6\u786e\u738777%\uff0c\u968f\u673a\u68ee\u679774%\uff0c\u878d\u5408\u540e87%\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "conclusion": "FedMultiEmo\u4e3a\u8f66\u8f7d\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u60c5\u611f\u8bc6\u522b\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.15507", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15507", "abs": "https://arxiv.org/abs/2507.15507", "authors": ["Johannes Ackermann", "Takashi Ishida", "Masashi Sugiyama"], "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback", "comment": "Accept at the Conference On Language Modeling (COLM) 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models,\nsuch as language models (LMs), to follow complex human preferences. In RLHF for\nLMs, we first train an LM using supervised fine-tuning, sample pairs of\nresponses, obtain human feedback, and use the resulting data to train a reward\nmodel (RM). RL methods are then used to train the LM to maximize the reward\ngiven by the RM. As training progresses, the responses generated by the LM no\nlonger resemble the responses seen by the RM during training, leading to the RM\nbecoming inaccurate. The score given by the RM keeps increasing, but the\nlearned behavior no longer matches the human preferences. This issue is known\nas overoptimization. We investigate overoptimization from the point of view of\ndistribution shift and show that the shift results in an inconsistent estimate\nof the RM parameters, leading to an inconsistent estimate of the policy\ngradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which\niteratively off-policy corrects the RM using importance weighting, without\nrequiring new labels or samples. This results in a more accurate RM, which\nempirically leads to an improved final policy. We validate our approach in\nexperiments with summarization and chatbot datasets and show that it performs\nsignificantly better than standard RLHF methods and baselines. Our\nimplementation is available at\nhttps://github.com/JohannesAck/OffPolicyCorrectedRewardModeling", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86RLHF\u4e2d\u7684\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u65b0\u6807\u7b7e\u7684\u79bb\u7ebf\u7b56\u7565\u6821\u6b63\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff08OCRM\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u6807\u51c6RLHF\u65b9\u6cd5\u3002", "motivation": "RLHF\u8bad\u7ec3\u4e2d\uff0c\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u8fc7\u4f18\u5316\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u63d0\u51faOCRM\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u52a0\u6743\u8fed\u4ee3\u6821\u6b63RM\uff0c\u65e0\u9700\u65b0\u6570\u636e\u3002", "result": "\u5728\u6458\u8981\u548c\u804a\u5929\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\uff0cOCRM\u663e\u8457\u4f18\u4e8e\u6807\u51c6RLHF\u65b9\u6cd5\u3002", "conclusion": "OCRM\u6709\u6548\u89e3\u51b3\u4e86RLHF\u4e2d\u7684\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6700\u7ec8\u7b56\u7565\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.15523", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15523", "abs": "https://arxiv.org/abs/2507.15523", "authors": ["Weichuang Shao", "Iman Yi Liao", "Tomas Henrique Bode Maul", "Tissa Chandesa"], "title": "An Investigation of Test-time Adaptation for Audio Classification under Background Noise", "comment": null, "summary": "Domain shift is a prominent problem in Deep Learning, causing a model\npre-trained on a source dataset to suffer significant performance degradation\non test datasets. This research aims to address the issue of audio\nclassification under domain shift caused by background noise using Test-Time\nAdaptation (TTA), a technique that adapts a pre-trained model during testing\nusing only unlabelled test data before making predictions. We adopt two common\nTTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and\ninvestigate their respective performance on two popular audio classification\ndatasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types\nof background noise and noise severity levels. The experimental results reveal\nthat our proposed modified version of CoNMix produced the highest\nclassification accuracy under domain shift (5.31% error rate under 10 dB\nexercise bike background noise and 12.75% error rate under 3 dB running tap\nbackground noise for AM) compared to TTT and TENT. The literature search\nprovided no evidence of similar works, thereby motivating the work reported\nhere as the first study to leverage TTA techniques for audio classification\nunder domain shift.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u6280\u672f\u89e3\u51b3\u97f3\u9891\u5206\u7c7b\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86TTT\u3001TENT\u548cCoNMix\u65b9\u6cd5\uff0c\u6539\u8fdb\u7684CoNMix\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u97f3\u9891\u5206\u7c7b\u4e2d\u7531\u80cc\u666f\u566a\u58f0\u5f15\u8d77\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528TTT\u3001TENT\u548cCoNMix\u4e09\u79cdTTA\u65b9\u6cd5\uff0c\u5728AudioMNIST\u548cSpeechCommands V1\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u80cc\u666f\u566a\u58f0\u548c\u566a\u58f0\u5f3a\u5ea6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6539\u8fdb\u7684CoNMix\u5728\u57df\u504f\u79fb\u4e0b\u8868\u73b0\u6700\u4f18\uff08AM\u6570\u636e\u96c6\u572810 dB\u8fd0\u52a8\u81ea\u884c\u8f66\u566a\u58f0\u4e0b\u9519\u8bef\u7387\u4e3a5.31%\uff0c3 dB\u6d41\u6c34\u566a\u58f0\u4e0b\u4e3a12.75%\uff09\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5229\u7528TTA\u6280\u672f\u89e3\u51b3\u97f3\u9891\u5206\u7c7b\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u6539\u8fdb\u7684CoNMix\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2507.15545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15545", "abs": "https://arxiv.org/abs/2507.15545", "authors": ["Yujia Shi", "Emil Njor", "Pablo Mart\u00ednez-Nuevo", "Sven Ewan Shepstone", "Xenofon Fafoutis"], "title": "Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications", "comment": null, "summary": "The success of Machine Learning is increasingly tempered by its significant\nresource footprint, driving interest in efficient paradigms like TinyML.\nHowever, the inherent complexity of designing TinyML systems hampers their\nbroad adoption. To reduce this complexity, we introduce \"Data Aware\nDifferentiable Neural Architecture Search\". Unlike conventional Differentiable\nNeural Architecture Search, our approach expands the search space to include\ndata configuration parameters alongside architectural choices. This enables\nData Aware Differentiable Neural Architecture Search to co-optimize model\narchitecture and input data characteristics, effectively balancing resource\nusage and system performance for TinyML applications. Initial results on\nkeyword spotting demonstrate that this novel approach to TinyML system design\ncan generate lean but highly accurate systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6570\u636e\u611f\u77e5\u53ef\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u6a21\u578b\u67b6\u6784\u548c\u8f93\u5165\u6570\u636e\u7279\u6027\uff0c\u7b80\u5316TinyML\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7684\u9ad8\u8d44\u6e90\u6d88\u8017\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662fTinyML\u7cfb\u7edf\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u963b\u788d\u4e86\u5176\u666e\u53ca\u3002", "method": "\u6269\u5c55\u4e86\u53ef\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5305\u62ec\u6570\u636e\u914d\u7f6e\u53c2\u6570\uff0c\u5b9e\u73b0\u6a21\u578b\u67b6\u6784\u4e0e\u8f93\u5165\u6570\u636e\u7684\u534f\u540c\u4f18\u5316\u3002", "result": "\u5728\u5173\u952e\u8bcd\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u751f\u6210\u4e86\u8f7b\u91cf\u4e14\u9ad8\u7cbe\u5ea6\u7684\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aTinyML\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7b80\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15548", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15548", "abs": "https://arxiv.org/abs/2507.15548", "authors": ["D. Abler", "O. Pusterla", "A. Joye-K\u00fchnis", "N. Andratschke", "M. Bach", "A. Bink", "S. M. Christ", "P. Hagmann", "B. Pouymayou", "E. Pravat\u00e0", "P. Radojewski", "M. Reyes", "L. Ruinelli", "R. Schaer", "B. Stieltjes", "G. Treglia", "W. Valenzuela", "R. Wiest", "S. Zoergiebel", "M. Guckenberger", "S. Tanadini-Lang", "A. Depeursinge"], "title": "The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information", "comment": null, "summary": "Background: Radiomics shows promise in characterizing glioblastoma, but its\nadded value over clinical and molecular predictors has yet to be proven. This\nstudy assessed the added value of conventional radiomics (CR) and deep learning\n(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on\na large multi-center dataset.\n  Methods: After patient selection, our curated dataset gathers 1152\nglioblastoma (WHO 2016) patients from five Swiss centers and one public source.\nIt included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI\ndata (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were\ndeveloped using standard methods and evaluated on internal and external\ncohorts. Sub-analyses assessed models with different feature sets\n(imaging-only, clinical/molecular-only, combined-features) and patient subsets\n(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).\n  Results: The best performance was observed in the full cohort (S-1). In\nexternal validation, the combined-feature CR model achieved an AUC of 0.75,\nslightly, but significantly outperforming clinical-only (0.74) and imaging-only\n(0.68) models. DL models showed similar trends, though without statistical\nsignificance. In S-2 and S-3, combined models did not outperform clinical-only\nmodels. Exploratory analysis of CR models for overall survival prediction\nsuggested greater relevance of imaging data: across all subsets,\ncombined-feature models significantly outperformed clinical-only models, though\nwith a modest advantage of 2-4 C-index points.\n  Conclusions: While confirming the predictive value of anatomical MRI\nsequences for glioblastoma prognosis, this multi-center study found standard CR\nand DL radiomics approaches offer minimal added value over demographic\npredictors such as age and gender.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4f20\u7edf\u653e\u5c04\u7ec4\u5b66\uff08CR\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09MRI\u653e\u5c04\u7ec4\u5b66\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u9884\u540e\u4e2d\u7684\u9644\u52a0\u4ef7\u503c\uff0c\u53d1\u73b0\u5176\u76f8\u5bf9\u4e8e\u4e34\u5e8a\u548c\u5206\u5b50\u9884\u6d4b\u56e0\u5b50\u7684\u4f18\u52bf\u6709\u9650\u3002", "motivation": "\u9a8c\u8bc1\u653e\u5c04\u7ec4\u5b66\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u9884\u540e\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u75281152\u4f8b\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u60a3\u8005\u7684\u591a\u4e2d\u5fc3\u6570\u636e\uff0c\u5f00\u53d1\u5e76\u8bc4\u4f30CR\u548cDL\u6a21\u578b\uff0c\u6bd4\u8f83\u4e0d\u540c\u7279\u5f81\u96c6\uff08\u5f71\u50cf\u3001\u4e34\u5e8a/\u5206\u5b50\u3001\u7ec4\u5408\uff09\u548c\u60a3\u8005\u5b50\u96c6\u7684\u8868\u73b0\u3002", "result": "\u7ec4\u5408\u7279\u5f81\u7684CR\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2dAUC\u4e3a0.75\uff0c\u7565\u4f18\u4e8e\u4e34\u5e8a\uff080.74\uff09\u548c\u5f71\u50cf\uff080.68\uff09\u6a21\u578b\uff0c\u4f46\u4f18\u52bf\u6709\u9650\u3002DL\u6a21\u578b\u8d8b\u52bf\u7c7b\u4f3c\u4f46\u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "conclusion": "\u6807\u51c6CR\u548cDL\u653e\u5c04\u7ec4\u5b66\u65b9\u6cd5\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u9884\u540e\u4e2d\u7684\u9644\u52a0\u4ef7\u503c\u6709\u9650\uff0c\u4e34\u5e8a\u9884\u6d4b\u56e0\u5b50\uff08\u5982\u5e74\u9f84\u548c\u6027\u522b\uff09\u4ecd\u4e3a\u4e3b\u8981\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2507.15550", "categories": ["cs.LG", "cs.AI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.15550", "abs": "https://arxiv.org/abs/2507.15550", "authors": ["Yimeng Chen", "Piotr Pi\u0229kos", "Mateusz Ostaszewski", "Firas Laakom", "J\u00fcrgen Schmidhuber"], "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "comment": "31 Pages", "summary": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity.", "AI": {"tldr": "PhysGym\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u548c\u6a21\u62df\u5e73\u53f0\uff0c\u901a\u8fc7\u63a7\u5236\u5148\u9a8c\u77e5\u8bc6\u6c34\u5e73\u548c\u95ee\u9898\u590d\u6742\u6027\u6765\u533a\u5206\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5e94\u5bf9\u73af\u5883\u590d\u6742\u6027\u548c\u5229\u7528\u5148\u9a8c\u80fd\u529b\u7684\u4e13\u7528\u57fa\u51c6\uff0cPhysGym\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PhysGym\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u7269\u7406\u73af\u5883\u6a21\u62df\uff0c\u8981\u6c42\u6a21\u578b\u4e3b\u52a8\u63a2\u7d22\u3001\u6536\u96c6\u6570\u636e\u5e76\u5f62\u6210\u5047\u8bbe\uff0c\u540c\u65f6\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u5c55\u793a\u4e86PhysGym\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u5148\u9a8c\u77e5\u8bc6\u548c\u4efb\u52a1\u590d\u6742\u6027\u4e0b\u7684\u6a21\u578b\u80fd\u529b\u3002", "conclusion": "PhysGym\u4e3a\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u9886\u57df\u3002"}}
{"id": "2507.15566", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15566", "abs": "https://arxiv.org/abs/2507.15566", "authors": ["Pieter Smet", "Martina Doneda", "Ettore Lanzarone", "Giuliana Carello"], "title": "Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy", "comment": null, "summary": "The availability of downstream resources plays a critical role in planning\nthe admission of patients undergoing elective surgery, with inpatient beds\nbeing one of the most crucial resources. When planning patient admissions,\npredictions on their length-of-stay (LOS) made by machine learning (ML) models\nare used to ensure bed availability. However, the actual LOS for each patient\nmay differ considerably from the predicted value, potentially making the\nschedule infeasible. To address such infeasibilities, rescheduling strategies\nthat take advantage of operational flexibility can be implemented. For example,\nadjustments may include postponing admission dates, relocating patients to\ndifferent wards, or even transferring patients who are already admitted. The\ncommon assumption is that more accurate LOS predictions reduce the impact of\nrescheduling. However, training ML models that can make such accurate\npredictions can be costly. Building on previous work that proposed simulated\n\\ac{ml} for evaluating data-driven approaches, this paper explores the\nrelationship between LOS prediction accuracy and rescheduling flexibility\nacross various corrective policies. Specifically, we examine the most effective\npatient rescheduling strategies under LOS prediction errors to prevent bed\noverflows while optimizing resource utilization.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f4f\u9662\u65f6\u95f4\uff08LOS\uff09\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u5e8a\u4f4d\u8c03\u5ea6\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u4e86\u5728\u9884\u6d4b\u8bef\u5dee\u4e0b\u6700\u6709\u6548\u7684\u60a3\u8005\u91cd\u65b0\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u8d44\u6e90\u5229\u7528\u5e76\u9632\u6b62\u5e8a\u4f4d\u6ea2\u51fa\u3002", "motivation": "\u4e0b\u6e38\u8d44\u6e90\uff08\u5982\u5e8a\u4f4d\uff09\u7684\u53ef\u7528\u6027\u5bf9\u8ba1\u5212\u9009\u62e9\u6027\u624b\u672f\u60a3\u8005\u7684\u5165\u9662\u81f3\u5173\u91cd\u8981\u3002\u7531\u4e8e\u5b9e\u9645LOS\u53ef\u80fd\u4e0e\u9884\u6d4b\u503c\u5dee\u5f02\u8f83\u5927\uff0c\u5bfc\u81f4\u8c03\u5ea6\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7075\u6d3b\u7684\u8c03\u5ea6\u7b56\u7565\u5e94\u5bf9\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u57fa\u4e8e\u6a21\u62df\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e0d\u540c\u7ea0\u6b63\u7b56\u7565\u4e0bLOS\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u8c03\u5ea6\u7075\u6d3b\u6027\u7684\u5173\u7cfb\uff0c\u5206\u6790\u6700\u6709\u6548\u7684\u91cd\u65b0\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u66f4\u51c6\u786e\u7684LOS\u9884\u6d4b\u53ef\u4ee5\u51cf\u5c11\u91cd\u65b0\u8c03\u5ea6\u7684\u5f71\u54cd\uff0c\u4f46\u8bad\u7ec3\u9ad8\u7cbe\u5ea6\u6a21\u578b\u6210\u672c\u8f83\u9ad8\u3002\u901a\u8fc7\u7075\u6d3b\u7684\u8c03\u5ea6\u7b56\u7565\uff08\u5982\u8c03\u6574\u5165\u9662\u65e5\u671f\u6216\u8f6c\u79fb\u60a3\u8005\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u9632\u6b62\u5e8a\u4f4d\u6ea2\u51fa\u3002", "conclusion": "\u5728LOS\u9884\u6d4b\u8bef\u5dee\u4e0b\uff0c\u7075\u6d3b\u7684\u8c03\u5ea6\u7b56\u7565\u662f\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u9632\u6b62\u5e8a\u4f4d\u6ea2\u51fa\u7684\u5173\u952e\uff0c\u800c\u9884\u6d4b\u7cbe\u5ea6\u7684\u63d0\u5347\u867d\u6709\u76ca\u4f46\u9700\u6743\u8861\u6210\u672c\u3002"}}
{"id": "2507.15574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15574", "abs": "https://arxiv.org/abs/2507.15574", "authors": ["Gregory F. Stock", "Juan A. Fraire", "Holger Hermanns", "J\u0119drzej Mosi\u0119\u017cny", "Yusra Al-Khazraji", "Julio Ram\u00edrez Molina", "Evridiki V. Ntagiou"], "title": "On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project", "comment": "18th International Conference on Space Operations (SpaceOps 2025),\n  Montr\\'eal, Canada, 26-30 May 2025,\n  https://star.spaceops.org/2025/user_manudownload.php?doc=140__9bg48dkf.pdf", "summary": "The rapid expansion of satellite constellations in near-Earth orbits presents\nsignificant challenges in satellite network management, requiring innovative\napproaches for efficient, scalable, and resilient operations. This paper\nexplores the role of Artificial Intelligence (AI) in optimizing the operation\nof satellite mega-constellations, drawing from the ConstellAI project funded by\nthe European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland\nUniversity, and Thales Alenia Space collaborates to develop AI-driven\nalgorithms and demonstrates their effectiveness over traditional methods for\ntwo crucial operational challenges: data routing and resource allocation. In\nthe routing use case, Reinforcement Learning (RL) is used to improve the\nend-to-end latency by learning from historical queuing latency, outperforming\nclassical shortest path algorithms. For resource allocation, RL optimizes the\nscheduling of tasks across constellations, focussing on efficiently using\nlimited resources such as battery and memory. Both use cases were tested for\nmultiple satellite constellation configurations and operational scenarios,\nresembling the real-life spacecraft operations of communications and Earth\nobservation satellites. This research demonstrates that RL not only competes\nwith classical approaches but also offers enhanced flexibility, scalability,\nand generalizability in decision-making processes, which is crucial for the\nautonomous and intelligent management of satellite fleets. The findings of this\nactivity suggest that AI can fundamentally alter the landscape of satellite\nconstellation management by providing more adaptive, robust, and cost-effective\nsolutions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u4f18\u5316\u536b\u661f\u5de8\u578b\u661f\u5ea7\u8fd0\u8425\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6539\u8fdb\u6570\u636e\u8def\u7531\u548c\u8d44\u6e90\u5206\u914d\uff0c\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8fd1\u5730\u8f68\u9053\u536b\u661f\u661f\u5ea7\u7684\u5feb\u901f\u6269\u5f20\u5bf9\u536b\u661f\u7f51\u7edc\u7ba1\u7406\u63d0\u51fa\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u5f39\u6027\u8fd0\u8425\u7684\u9700\u6c42\uff0c\u9700\u8981\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86AI\u9a71\u52a8\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u6570\u636e\u8def\u7531\uff08\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\uff09\u548c\u8d44\u6e90\u5206\u914d\uff08\u9ad8\u6548\u5229\u7528\u7535\u6c60\u548c\u5185\u5b58\u7b49\u6709\u9650\u8d44\u6e90\uff09\u3002", "result": "RL\u5728\u591a\u79cd\u536b\u661f\u661f\u5ea7\u914d\u7f6e\u548c\u64cd\u4f5c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "AI\uff08\u5c24\u5176\u662fRL\uff09\u80fd\u591f\u4e3a\u536b\u661f\u661f\u5ea7\u7ba1\u7406\u63d0\u4f9b\u66f4\u81ea\u9002\u5e94\u3001\u7a33\u5065\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6539\u53d8\u5176\u7ba1\u7406\u683c\u5c40\u3002"}}
{"id": "2507.15584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15584", "abs": "https://arxiv.org/abs/2507.15584", "authors": ["Philipp R\u00f6chner", "Simon Kl\u00fcttermann", "Franz Rothlauf", "Daniel Schl\u00f6r"], "title": "We Need to Rethink Benchmarking in Anomaly Detection", "comment": null, "summary": "Despite the continuous proposal of new anomaly detection algorithms and\nextensive benchmarking efforts, progress seems to stagnate, with only minor\nperformance differences between established baselines and new algorithms. In\nthis position paper, we argue that this stagnation is due to limitations in how\nwe evaluate anomaly detection algorithms. Current benchmarking does not, for\nexample, sufficiently reflect the diversity of anomalies in applications\nranging from predictive maintenance to scientific discovery. Consequently, we\nneed to rethink benchmarking in anomaly detection. In our opinion, anomaly\ndetection should be studied using scenarios that capture the relevant\ncharacteristics of different applications. We identify three key areas for\nimprovement: First, we need to identify anomaly detection scenarios based on a\ncommon taxonomy. Second, anomaly detection pipelines should be analyzed\nend-to-end and by component. Third, evaluating anomaly detection algorithms\nshould be meaningful regarding the scenario's objectives.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u5f0f\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u8fdb\u5c55\u505c\u6ede\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u5f0f\u672a\u80fd\u5145\u5206\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u7684\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u91cd\u65b0\u601d\u8003\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5efa\u8bae\u57fa\u4e8e\u5171\u540c\u5206\u7c7b\u6cd5\u5b9a\u4e49\u573a\u666f\uff0c\u5206\u6790\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5e76\u6839\u636e\u573a\u666f\u76ee\u6807\u8bc4\u4f30\u7b97\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\u9886\u57df\uff1a\u573a\u666f\u5206\u7c7b\u3001\u7aef\u5230\u7aef\u5206\u6790\u548c\u76ee\u6807\u5bfc\u5411\u8bc4\u4f30\u3002", "conclusion": "\u9700\u8981\u901a\u8fc7\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u6765\u63a8\u52a8\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.15587", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15587", "abs": "https://arxiv.org/abs/2507.15587", "authors": ["Yinsong Chen", "Kaifeng Wang", "Xiaoqiang Meng", "Xueyuan Li", "Zirui Li", "Xin Gao"], "title": "Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario", "comment": null, "summary": "Current research on decision-making in safety-critical scenarios often relies\non inefficient data-driven scenario generation or specific modeling approaches,\nwhich fail to capture corner cases in real-world contexts. To address this\nissue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,\nwhere background vehicles with interference capabilities are treated as\nred-team agents. Through active interference and exploration, red-team vehicles\ncan uncover corner cases outside the data distribution. The framework uses a\nConstraint Graph Representation Markov Decision Process, ensuring that red-team\nvehicles comply with safety rules while continuously disrupting the autonomous\nvehicles (AVs). A policy threat zone model is constructed to quantify the\nthreat posed by red-team vehicles to AVs, inducing more extreme actions to\nincrease the danger level of the scenario. Experimental results show that the\nproposed framework significantly impacts AVs decision-making safety and\ngenerates various corner cases. This method also offers a novel direction for\nresearch in safety-critical scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea2\u961f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5e72\u6270\u548c\u63a2\u7d22\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u6781\u7aef\u6848\u4f8b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u96be\u4ee5\u6355\u6349\u6781\u7aef\u6848\u4f8b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7ea2\u961f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7ea6\u675f\u56fe\u8868\u793a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u786e\u4fdd\u5b89\u5168\u89c4\u5219\u4e0b\u5e72\u6270\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u51b3\u7b56\u5b89\u5168\uff0c\u5e76\u751f\u6210\u591a\u79cd\u6781\u7aef\u6848\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u5173\u952e\u573a\u666f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15614", "abs": "https://arxiv.org/abs/2507.15614", "authors": ["Edward Holmberg", "Pujan Pokhrel", "Maximilian Zoch", "Elias Ioup", "Ken Pathak", "Steven Sloan", "Kendall Niles", "Jay Ratcliff", "Maik Flanagin", "Christian Guetl", "Julian Simeonov", "Mahdi Abdelguerfi"], "title": "Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting", "comment": "10 pages, 8 figures", "summary": "Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but\nare too computationally intensive for on-the-fly decision-making during flood\nevents. The central challenge is to accelerate these simulations without\nsacrificing accuracy. This paper introduces a deep learning surrogate that\ntreats HEC-RAS not as a solver but as a data-generation engine. We propose a\nhybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)\nto capture short-term temporal dynamics with a Geometry-Aware Fourier Neural\nOperator (Geo-FNO) to model long-range spatial dependencies along a river\nreach. The model learns underlying physics implicitly from a minimal\neight-channel feature vector encoding dynamic state, static geometry, and\nboundary forcings extracted directly from native HEC-RAS files. Trained on 67\nreaches of the Mississippi River Basin, the surrogate was evaluated on a\nyear-long, unseen hold-out simulation. Results show the model achieves a strong\npredictive accuracy, with a median absolute stage error of 0.31 feet.\nCritically, for a full 67-reach ensemble forecast, our surrogate reduces the\nrequired wall-clock time from 139 minutes to 40 minutes, a speedup of nearly\n3.5 times over the traditional solver. The success of this data-driven approach\ndemonstrates that robust feature engineering can produce a viable, high-speed\nreplacement for conventional hydraulic models, improving the computational\nfeasibility of large-scale ensemble flood forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u52a0\u901fHEC-RAS\u6d2a\u6c34\u6a21\u62df\uff0c\u7ed3\u5408GRU\u548cGeo-FNO\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u4e14\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\uff08\u5982HEC-RAS\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u65f6\u51b3\u7b56\uff0c\u9700\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u52a0\u901f\u6a21\u62df\u3002", "method": "\u91c7\u7528\u6df7\u5408\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u7ed3\u5408GRU\u6355\u6349\u77ed\u671f\u65f6\u95f4\u52a8\u6001\u548cGeo-FNO\u5efa\u6a21\u957f\u7a0b\u7a7a\u95f4\u4f9d\u8d56\uff0c\u901a\u8fc7\u516b\u901a\u9053\u7279\u5f81\u5411\u91cf\u4eceHEC-RAS\u6587\u4ef6\u4e2d\u5b66\u4e60\u7269\u7406\u89c4\u5f8b\u3002", "result": "\u5728\u5bc6\u897f\u897f\u6bd4\u6cb3\u6d41\u57df67\u4e2a\u6cb3\u6bb5\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff08\u4e2d\u4f4d\u7edd\u5bf9\u6c34\u4f4d\u8bef\u5dee0.31\u82f1\u5c3a\uff09\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53473.5\u500d\uff08\u4ece139\u5206\u949f\u964d\u81f340\u5206\u949f\uff09\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u53ef\u66ff\u4ee3\u4f20\u7edf\u6c34\u529b\u6a21\u578b\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u6d2a\u6c34\u9884\u62a5\u7684\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
{"id": "2507.15640", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15640", "abs": "https://arxiv.org/abs/2507.15640", "authors": ["Kailai Yang", "Xiao Liu", "Lei Ji", "Hao Li", "Yeyun Gong", "Peng Cheng", "Mao Yang"], "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training", "comment": null, "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6846\u67b6Data Mixing Agent\uff0c\u7528\u4e8e\u81ea\u52a8\u8c03\u6574\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u6570\u636e\u7684\u6743\u91cd\uff0c\u4ee5\u907f\u514d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u4e0a\u6301\u7eed\u9884\u8bad\u7ec3\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u542f\u53d1\u5f0f\u8c03\u6574\u6570\u636e\u6743\u91cd\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u63d0\u51faData Mixing Agent\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u5927\u91cf\u6570\u636e\u6df7\u5408\u8f68\u8ff9\u4e2d\u5b66\u4e60\u901a\u7528\u542f\u53d1\u5f0f\uff0c\u81ea\u52a8\u8c03\u6574\u6e90\u548c\u76ee\u6807\u9886\u57df\u7684\u6570\u636e\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6e90\u9886\u57df\u3001\u76ee\u6807\u6a21\u578b\u548c\u9886\u57df\u7a7a\u95f4\u3002\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "Data Mixing Agent\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e00\u81f4\u7684\u542f\u53d1\u5f0f\uff0c\u51cf\u5c11\u6e90\u9886\u57df\u6570\u636e\u9700\u6c42\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.15643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15643", "abs": "https://arxiv.org/abs/2507.15643", "authors": ["Elnur Isgandarov", "Matteo Cederle", "Federico Chiariotti", "Gian Antonio Susto"], "title": "Towards Explainable Anomaly Detection in Shared Mobility Systems", "comment": "6 pages, 8 figures. Paper accepted to J3C 2025 (Joint Conference on\n  Computers, Cognition and Communication", "summary": "Shared mobility systems, such as bike-sharing networks, play a crucial role\nin urban transportation. Identifying anomalies in these systems is essential\nfor optimizing operations, improving service reliability, and enhancing user\nexperience. This paper presents an interpretable anomaly detection framework\nthat integrates multi-source data, including bike-sharing trip records, weather\nconditions, and public transit availability. The Isolation Forest algorithm is\nemployed for unsupervised anomaly detection, along with the Depth-based\nIsolation Forest Feature Importance (DIFFI) algorithm providing\ninterpretability. Results show that station-level analysis offers a robust\nunderstanding of anomalies, highlighting the influence of external factors such\nas adverse weather and limited transit availability. Our findings contribute to\nimproving decision-making in shared mobility operations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u6e90\u6570\u636e\u548cIsolation Forest\u7b97\u6cd5\uff0c\u5206\u6790\u5916\u90e8\u56e0\u7d20\u5bf9\u5f02\u5e38\u7684\u5f71\u54cd\u3002", "motivation": "\u5171\u4eab\u51fa\u884c\u7cfb\u7edf\uff08\u5982\u5171\u4eab\u5355\u8f66\uff09\u5bf9\u57ce\u5e02\u4ea4\u901a\u81f3\u5173\u91cd\u8981\uff0c\u8bc6\u522b\u5f02\u5e38\u6709\u52a9\u4e8e\u4f18\u5316\u8fd0\u8425\u3001\u63d0\u9ad8\u670d\u52a1\u53ef\u9760\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528Isolation Forest\u7b97\u6cd5\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u7ed3\u5408DIFFI\u7b97\u6cd5\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u6574\u5408\u5171\u4eab\u5355\u8f66\u884c\u7a0b\u8bb0\u5f55\u3001\u5929\u6c14\u6761\u4ef6\u548c\u516c\u5171\u4ea4\u901a\u53ef\u7528\u6027\u7b49\u591a\u6e90\u6570\u636e\u3002", "result": "\u7ad9\u70b9\u7ea7\u5206\u6790\u80fd\u6709\u6548\u8bc6\u522b\u5f02\u5e38\uff0c\u5916\u90e8\u56e0\u7d20\uff08\u5982\u6076\u52a3\u5929\u6c14\u548c\u516c\u5171\u4ea4\u901a\u4e0d\u8db3\uff09\u5bf9\u5f02\u5e38\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u6709\u52a9\u4e8e\u63d0\u5347\u5171\u4eab\u51fa\u884c\u7cfb\u7edf\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2507.15718", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15718", "abs": "https://arxiv.org/abs/2507.15718", "authors": ["Matteo Cederle", "Andrea Mazzucco", "Andrea Demartini", "Eugenio Mazza", "Eugenia Suriani", "Federico Vitti", "Gian Antonio Susto"], "title": "Explainable Anomaly Detection for Electric Vehicles Charging Stations", "comment": "4 pages, 3 figures. Paper accepted to J3C 2025 (Joint Conference on\n  Computers, Cognition and Communication)", "summary": "Electric vehicles (EV) charging stations are one of the critical\ninfrastructures needed to support the transition to renewable-energy-based\nmobility, but ensuring their reliability and efficiency requires effective\nanomaly detection to identify irregularities in charging behavior. However, in\nsuch a productive scenario, it is also crucial to determine the underlying\ncause behind the detected anomalies. To achieve this goal, this study\ninvestigates unsupervised anomaly detection techniques for EV charging\ninfrastructure, integrating eXplainable Artificial Intelligence techniques to\nenhance interpretability and uncover root causes of anomalies.\n  Using real-world sensors and charging session data, this work applies\nIsolation Forest to detect anomalies and employs the Depth-based Isolation\nForest Feature Importance (DIFFI) method to identify the most important\nfeatures contributing to such anomalies. The efficacy of the proposed approach\nis evaluated in a real industrial case.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u5f02\u5e38\u68c0\u6d4b\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u63d0\u5347\u5f02\u5e38\u539f\u56e0\u5206\u6790\u3002", "motivation": "\u652f\u6301\u53ef\u518d\u751f\u80fd\u6e90\u4ea4\u901a\u8f6c\u578b\u9700\u786e\u4fdd\u5145\u7535\u7ad9\u53ef\u9760\u6027\uff0c\u9700\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u5e76\u5206\u6790\u539f\u56e0\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff08Isolation Forest\uff09\u548cDIFFI\u65b9\u6cd5\u8bc6\u522b\u5f02\u5e38\u7279\u5f81\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u53ef\u89e3\u91caAI\u6280\u672f\u80fd\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u5e76\u63ed\u793a\u5176\u6839\u672c\u539f\u56e0\u3002"}}
{"id": "2507.15727", "categories": ["cs.LG", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15727", "abs": "https://arxiv.org/abs/2507.15727", "authors": ["Xuchuang Wang", "Bo Sun", "Hedyeh Beyhaghi", "John C. S. Lui", "Mohammad Hajiesmaili", "Adam Wierman"], "title": "Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems", "comment": null, "summary": "This paper introduces a novel multi-agent ski-rental problem that generalizes\nthe classical ski-rental dilemma to a group setting where agents incur\nindividual and shared costs. In our model, each agent can either rent at a\nfixed daily cost, or purchase a pass at an individual cost, with an additional\nthird option of a discounted group pass available to all. We consider scenarios\nin which agents' active days differ, leading to dynamic states as agents drop\nout of the decision process. To address this problem from different\nperspectives, we define three distinct competitive ratios: overall,\nstate-dependent, and individual rational. For each objective, we design and\nanalyze optimal deterministic and randomized policies. Our deterministic\npolicies employ state-aware threshold functions that adapt to the dynamic\nstates, while our randomized policies sample and resample thresholds from\ntailored state-aware distributions. The analysis reveals that symmetric\npolicies, in which all agents use the same threshold, outperform asymmetric\nones. Our results provide competitive ratio upper and lower bounds and extend\nclassical ski-rental insights to multi-agent settings, highlighting both\ntheoretical and practical implications for group decision-making under\nuncertainty.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u6ed1\u96ea\u79df\u8d41\u56f0\u5883\uff0c\u7814\u7a76\u4e86\u4ee3\u7406\u5728\u4e2a\u4f53\u548c\u5171\u4eab\u6210\u672c\u4e0b\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u4e09\u79cd\u7ade\u4e89\u6bd4\u7387\u4e0b\u7684\u6700\u4f18\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u591a\u4ee3\u7406\u73af\u5883\u4e0b\u7684\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\uff0c\u63a2\u7d22\u4ee3\u7406\u5728\u52a8\u6001\u72b6\u6001\u548c\u4e0d\u540c\u6210\u672c\u9009\u9879\u4e0b\u7684\u6700\u4f18\u51b3\u7b56\u7b56\u7565\u3002", "method": "\u5b9a\u4e49\u4e86\u4e09\u79cd\u7ade\u4e89\u6bd4\u7387\uff08\u6574\u4f53\u3001\u72b6\u6001\u4f9d\u8d56\u548c\u4e2a\u4f53\u7406\u6027\uff09\uff0c\u8bbe\u8ba1\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u5316\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u5bf9\u79f0\u4e0e\u975e\u5bf9\u79f0\u7b56\u7565\u7684\u6027\u80fd\u3002", "result": "\u5bf9\u79f0\u7b56\u7565\u4f18\u4e8e\u975e\u5bf9\u79f0\u7b56\u7565\uff0c\u63d0\u4f9b\u4e86\u7ade\u4e89\u6bd4\u7387\u7684\u4e0a\u9650\u548c\u4e0b\u9650\uff0c\u5e76\u5c06\u7ecf\u5178\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\u7684\u7ed3\u8bba\u6269\u5c55\u5230\u591a\u4ee3\u7406\u573a\u666f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u7fa4\u4f53\u51b3\u7b56\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u542f\u793a\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.15769", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15769", "abs": "https://arxiv.org/abs/2507.15769", "authors": ["Ahmad M. Nazar", "Abdulkadir Celik", "Mohamed Y. Selim", "Asmaa Abdallah", "Daji Qiao", "Ahmed M. Eltawil"], "title": "Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks", "comment": "Accepted in IEEE Asilomar Conference on Signals, Systems, and\n  Computers 2025", "summary": "Vehicular communication systems operating in the millimeter wave (mmWave)\nband are highly susceptible to signal blockage from dynamic obstacles such as\nvehicles, pedestrians, and infrastructure. To address this challenge, we\npropose a proactive blockage prediction framework that utilizes multi-modal\nsensing, including camera, GPS, LiDAR, and radar inputs in an\ninfrastructure-to-vehicle (I2V) setting. This approach uses modality-specific\ndeep learning models to process each sensor stream independently and fuses\ntheir outputs using a softmax-weighted ensemble strategy based on validation\nperformance. Our evaluations, for up to 1.5s in advance, show that the\ncamera-only model achieves the best standalone trade-off with an F1-score of\n97.1% and an inference time of 89.8ms. A camera+radar configuration further\nimproves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness\nand efficiency of multi-modal sensing for mmWave blockage prediction and\nprovide a pathway for proactive wireless communication in dynamic environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u611f\u77e5\u7684\u6beb\u7c73\u6ce2\u4fe1\u53f7\u906e\u6321\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u6444\u50cf\u5934\u3001GPS\u3001LiDAR\u548c\u96f7\u8fbe\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u8f6f\u52a0\u6743\u878d\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0897.2% F1\uff09\u548c\u4f4e\u5ef6\u8fdf\uff0895.7ms\uff09\u7684\u9884\u6d4b\u3002", "motivation": "\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684\u8f66\u8f7d\u901a\u4fe1\u7cfb\u7edf\u6613\u53d7\u52a8\u6001\u969c\u788d\u7269\uff08\u5982\u8f66\u8f86\u3001\u884c\u4eba\uff09\u7684\u4fe1\u53f7\u906e\u6321\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63d0\u9ad8\u901a\u4fe1\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u611f\u77e5\uff08\u6444\u50cf\u5934\u3001GPS\u3001LiDAR\u3001\u96f7\u8fbe\uff09\uff0c\u901a\u8fc7\u72ec\u7acb\u5904\u7406\u5404\u4f20\u611f\u5668\u6570\u636e\u5e76\u57fa\u4e8e\u9a8c\u8bc1\u6027\u80fd\u7684\u8f6f\u52a0\u6743\u878d\u5408\u7b56\u7565\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6444\u50cf\u5934\u5355\u72ec\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff08F1 97.1%\uff0c\u5ef6\u8fdf89.8ms\uff09\uff0c\u6444\u50cf\u5934+\u96f7\u8fbe\u7ec4\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u81f3F1 97.2%\uff0c\u5ef6\u8fdf95.7ms\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u77e5\u5728\u6beb\u7c73\u6ce2\u906e\u6321\u9884\u6d4b\u4e2d\u9ad8\u6548\u4e14\u6709\u6548\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4e3b\u52a8\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.15772", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.15772", "abs": "https://arxiv.org/abs/2507.15772", "authors": ["Anoop C. Patil", "Benny Jian Rong Sng", "Yu-Wei Chang", "Joana B. Pereira", "Chua Nam-Hai", "Rajani Sarojam", "Gajendra Pratap Singh", "In-Cheol Jang", "Giovanni Volpe"], "title": "Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis", "comment": "*Authors contributed equally to this work. +Supervised this work. 5\n  main figures and 1 extended data figure in manuscript. The PDF includes\n  supplementary material", "summary": "Detecting stress in plants is crucial for both open-farm and\ncontrolled-environment agriculture. Biomolecules within plants serve as key\nstress indicators, offering vital markers for continuous health monitoring and\nearly disease detection. Raman spectroscopy provides a powerful, non-invasive\nmeans to quantify these biomolecules through their molecular vibrational\nsignatures. However, traditional Raman analysis relies on customized\ndata-processing workflows that require fluorescence background removal and\nprior identification of Raman peaks of interest-introducing potential biases\nand inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation\nof Vibrational Raman spectra for plant-stress Analysis), a fully automated\nworkflow based on a variational autoencoder. Unlike conventional approaches,\nDIVA processes native Raman spectra-including fluorescence backgrounds-without\nmanual preprocessing, identifying and quantifying significant spectral features\nin an unbiased manner. We applied DIVA to detect a range of plant stresses,\nincluding abiotic (shading, high light intensity, high temperature) and biotic\nstressors (bacterial infections). By integrating deep learning with vibrational\nspectroscopy, DIVA paves the way for AI-driven plant health assessment,\nfostering more resilient and sustainable agricultural practices.", "AI": {"tldr": "DIVA\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u901a\u8fc7\u62c9\u66fc\u5149\u8c31\u68c0\u6d4b\u690d\u7269\u5e94\u6fc0\uff0c\u65e0\u9700\u624b\u52a8\u9884\u5904\u7406\u3002", "motivation": "\u690d\u7269\u5e94\u6fc0\u68c0\u6d4b\u5bf9\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u62c9\u66fc\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "DIVA\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5904\u7406\u539f\u59cb\u62c9\u66fc\u5149\u8c31\uff08\u5305\u62ec\u8367\u5149\u80cc\u666f\uff09\uff0c\u81ea\u52a8\u8bc6\u522b\u548c\u91cf\u5316\u5149\u8c31\u7279\u5f81\u3002", "result": "DIVA\u6210\u529f\u68c0\u6d4b\u4e86\u591a\u79cd\u690d\u7269\u5e94\u6fc0\uff08\u5982\u5149\u7167\u3001\u6e29\u5ea6\u53d8\u5316\u548c\u7ec6\u83cc\u611f\u67d3\uff09\u3002", "conclusion": "DIVA\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u632f\u52a8\u5149\u8c31\uff0c\u4e3aAI\u9a71\u52a8\u7684\u690d\u7269\u5065\u5eb7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.15774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15774", "abs": "https://arxiv.org/abs/2507.15774", "authors": ["Alexis-Raja Brachet", "Pierre-Yves Richard", "C\u00e9line Hudelot"], "title": "Dynamics is what you need for time-series forecasting!", "comment": "13 pages, 6 figures, 1 table", "summary": "While boundaries between data modalities are vanishing, the usual successful\ndeep models are still challenged by simple ones in the time-series forecasting\ntask. Our hypothesis is that this task needs models that are able to learn the\ndata underlying dynamics. We propose to validate it through both systemic and\nempirical studies. We develop an original $\\texttt{PRO-DYN}$ nomenclature to\nanalyze existing models through the lens of dynamics. Two observations thus\nemerged: $\\textbf{1}$. under-performing architectures learn dynamics at most\npartially, $\\textbf{2}$. the location of the dynamics block at the model end is\nof prime importance. We conduct extensive experiments to confirm our\nobservations on a set of performance-varying models with diverse backbones.\nResults support the need to incorporate a learnable dynamics block and its use\nas the final predictor.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRO-DYN\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u4e2d\u52a8\u6001\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u52a8\u6001\u5b66\u4e60\u5757\u7684\u4f4d\u7f6e\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u5047\u8bbe\u8fd9\u662f\u56e0\u4e3a\u6a21\u578b\u672a\u80fd\u5145\u5206\u5b66\u4e60\u6570\u636e\u7684\u5e95\u5c42\u52a8\u6001\u7279\u6027\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u63d0\u51faPRO-DYN\u6846\u67b6\u5206\u6790\u73b0\u6709\u6a21\u578b\u7684\u52a8\u6001\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u9a8c\u9a8c\u8bc1\u52a8\u6001\u5b66\u4e60\u5757\u7684\u4f4d\u7f6e\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6027\u80fd\u8f83\u5dee\u7684\u6a21\u578b\u4ec5\u90e8\u5206\u5b66\u4e60\u52a8\u6001\u7279\u6027\uff0c\u4e14\u52a8\u6001\u5b66\u4e60\u5757\u4f5c\u4e3a\u6700\u7ec8\u9884\u6d4b\u5668\u65f6\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u52a8\u6001\u5b66\u4e60\u5757\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5efa\u8bae\u5c06\u5176\u4f5c\u4e3a\u6a21\u578b\u7684\u6700\u7ec8\u9884\u6d4b\u6a21\u5757\u3002"}}
{"id": "2507.15784", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15784", "abs": "https://arxiv.org/abs/2507.15784", "authors": ["Zihang Ma", "Qitian Yin"], "title": "Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets", "comment": null, "summary": "Graph node classification is a fundamental task in graph neural networks\n(GNNs), aiming to assign predefined class labels to nodes. On the PubMed\ncitation network dataset, we observe significant classification difficulty\ndisparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,\n7.5% lower than Category 1. To address this, we propose a\nWasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),\ntraining specialized GNN models for Categories 0/1 (with layer normalization\nand residual connections) and Multi-hop Graph Attention Networks (GAT) for\nCategory 2. The WR distance metric optimizes representation similarity between\nmodels, particularly focusing on improving Category 2 performance. Our adaptive\nfusion strategy dynamically weights models based on category-specific\nperformance, with Category 2 assigned a GAT weight of 0.8. WR distance further\nguides the fusion process by measuring distributional differences between model\nrepresentations, enabling more principled integration of complementary\nfeatures.\n  Experimental results show WR-EFM achieves balanced accuracy across\ncategories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),\noutperforming both single models and standard fusion approaches. The\ncoefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%\nlower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM\nimproves Category 2 accuracy by 5.5% compared to GCN, verifying the\neffectiveness of WR-guided fusion in capturing complex structural patterns.\nThis work provides a novel paradigm for handling class-imbalanced graph\nclassification tasks. To promote the research community, we release our project\nat https://github.com/s010m00n/GASEM4NC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein-Rubinstein\u8ddd\u79bb\u7684\u4e13\u5bb6\u878d\u5408\u6a21\u578b\uff08WR-EFM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u8282\u70b9\u5206\u7c7b\u4e2d\u7c7b\u522b\u95f4\u6027\u80fd\u5dee\u5f02\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u522b2\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u5728PubMed\u5f15\u6587\u7f51\u7edc\u6570\u636e\u96c6\u4e2d\uff0c\u4f20\u7edfGCN\u6a21\u578b\u5728\u7c7b\u522b2\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u4ec5\u4e3a74.4%\uff0c\u6bd4\u7c7b\u522b1\u4f4e7.5%\uff0c\u4e9f\u9700\u89e3\u51b3\u7c7b\u522b\u95f4\u6027\u80fd\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "WR-EFM\u7ed3\u5408\u4e86\u9488\u5bf9\u7c7b\u522b0/1\u7684GNN\u6a21\u578b\uff08\u5e26\u5c42\u5f52\u4e00\u5316\u548c\u6b8b\u5dee\u8fde\u63a5\uff09\u548c\u9488\u5bf9\u7c7b\u522b2\u7684\u591a\u8df3\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\uff0c\u5e76\u901a\u8fc7WR\u8ddd\u79bb\u4f18\u5316\u6a21\u578b\u95f4\u8868\u793a\u76f8\u4f3c\u6027\u3002\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u52a8\u6001\u52a0\u6743\u6a21\u578b\u3002", "result": "WR-EFM\u5728\u7c7b\u522b0\u30011\u30012\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a77.8%\u300178.0%\u548c79.9%\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u548c\u6807\u51c6\u878d\u5408\u65b9\u6cd5\uff0c\u7c7b\u522b2\u51c6\u786e\u7387\u63d0\u53475.5%\u3002", "conclusion": "WR-EFM\u4e3a\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u56fe\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7WR\u8ddd\u79bb\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2507.15788", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15788", "abs": "https://arxiv.org/abs/2507.15788", "authors": ["Sneheel Sarangi", "Hanan Salam"], "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning", "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5c0f\u89c4\u6a21LLMs\u662f\u5426\u80fd\u901a\u8fc7RLVR\u83b7\u5f97\u901a\u7528\u7684ToM\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4ec5\u80fd\u63d0\u5347\u8bad\u7ec3\u6570\u636e\u7684\u8868\u73b0\uff0c\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22RL\u65b9\u6cd5\u662f\u5426\u80fd\u4e3aLLMs\u8d4b\u4e88\u66f4\u7ec6\u817b\u7684\u793e\u4f1a\u667a\u80fd\uff08\u5982ToM\uff09\u3002", "method": "\u4f7f\u7528RLVR\u8bad\u7ec3\u5c0f\u89c4\u6a21LLMs\uff0c\u5e76\u5728\u591a\u4e2aToM\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5c0f\u89c4\u6a21LLMs\u65e0\u6cd5\u83b7\u5f97\u901a\u7528ToM\u80fd\u529b\uff0cRL\u8bad\u7ec3\u5bfc\u81f4\u8fc7\u62df\u5408\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "RLVR\u672a\u80fd\u5b9e\u73b0\u62bd\u8c61\u7684ToM\u80fd\u529b\uff0c\u4ec5\u8868\u73b0\u4e3a\u72ed\u7a84\u7684\u8fc7\u62df\u5408\u884c\u4e3a\u3002"}}
{"id": "2507.15832", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15832", "abs": "https://arxiv.org/abs/2507.15832", "authors": ["Shiyang Li"], "title": "Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction", "comment": "in Chinese language", "summary": "To address the limitations of medium- and long-term four-dimensional (4D)\ntrajectory prediction models, this paper proposes a hybrid\nCNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy\nimproved snake-herd optimization (SO) algorithm. The model applies the Adaboost\nalgorithm to divide multiple weak learners, and each submodel utilizes CNN to\nextract spatial features, LSTM to capture temporal features, and attention\nmechanism to capture global features comprehensively. The strong learner model,\ncombined with multiple sub-models, then optimizes the hyperparameters of the\nprediction model through the natural selection behavior pattern simulated by\nSO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the\ncomparison experiments and ablation studies of multiple optimizers are carried\nout, and a comprehensive test and evaluation analysis is carried out. The\nresults show that SO-CLA-adaboost outperforms traditional optimizers such as\nparticle swarm, whale, and gray wolf in handling large-scale high-dimensional\ntrajectory data. In addition, introducing the full-strategy collaborative\nimprovement SO algorithm improves the model's prediction accuracy by 39.89%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN-LSTM-\u6ce8\u610f\u529b-Adaboost\u7684\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u86c7\u7fa4\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4e2d\u957f\u671f4D\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u5bf9\u5927\u89c4\u6a21\u9ad8\u7ef4\u8f68\u8ff9\u6570\u636e\u7684\u5904\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528Adaboost\u7b97\u6cd5\u5212\u5206\u591a\u4e2a\u5f31\u5b66\u4e60\u5668\uff0c\u6bcf\u4e2a\u5b50\u6a21\u578b\u7ed3\u5408CNN\u3001LSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u86c7\u7fa4\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5904\u7406\u5927\u89c4\u6a21\u9ad8\u7ef4\u6570\u636e\u65f6\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u5668\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u5347\u4e8639.89%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6a21\u578b\u548c\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e864D\u8f68\u8ff9\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15836", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15836", "abs": "https://arxiv.org/abs/2507.15836", "authors": ["Matteo Boglioni", "Terrance Liu", "Andrew Ilyas", "Zhiwei Steven Wu"], "title": "Optimizing Canaries for Privacy Auditing with Metagradient Descent", "comment": null, "summary": "In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u5dee\u5206\u9690\u79c1\u5b66\u4e60\u7b97\u6cd5\u9690\u79c1\u53c2\u6570\u5ba1\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5ba1\u8ba1\u8005\u7684\u201c\u91d1\u4e1d\u96c0\u201d\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9690\u79c1\u53c2\u6570\u7684\u4e0b\u754c\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u5dee\u5206\u9690\u79c1\u5b66\u4e60\u7b97\u6cd5\u7684\u9690\u79c1\u53c2\u6570\u5ba1\u8ba1\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u201c\u91d1\u4e1d\u96c0\u201d\u6570\u636e\u96c6\uff0c\u4f46\u5176\u6548\u679c\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u8fd9\u4e9b\u6570\u636e\u96c6\uff0c\u63d0\u5347\u5ba1\u8ba1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5229\u7528\u5143\u68af\u5ea6\u4f18\u5316\u6280\u672f\u4f18\u5316\u201c\u91d1\u4e1d\u96c0\u201d\u6570\u636e\u96c6\uff0c\u4f7f\u5176\u5728\u5ba1\u8ba1\u5dee\u5206\u9690\u79c1\u5b66\u4e60\u7b97\u6cd5\u65f6\u66f4\u6709\u6548\u3002\u5b9e\u9a8c\u57fa\u4e8eDP-SGD\u7b97\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u540e\u7684\u201c\u91d1\u4e1d\u96c0\u201d\u6570\u636e\u96c6\u53ef\u5c06\u9690\u79c1\u53c2\u6570\u7684\u4e0b\u754c\u4f30\u8ba1\u63d0\u5347\u8d85\u8fc72\u500d\uff0c\u4e14\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u201c\u91d1\u4e1d\u96c0\u201d\u6570\u636e\u96c6\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5dee\u5206\u9690\u79c1\u5b66\u4e60\u7b97\u6cd5\u7684\u9690\u79c1\u5ba1\u8ba1\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u9690\u79c1\u4fdd\u969c\u3002"}}
{"id": "2507.15839", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15839", "abs": "https://arxiv.org/abs/2507.15839", "authors": ["Anh Nguyen", "Sam Schafft", "Nicholas Hale", "John Alfaro"], "title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs", "comment": null, "summary": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLMs\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801\u5b57\u6bb5\u5206\u5e03\u4e3a\u53ef\u91cd\u7528\u811a\u672c\uff0c\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u4f7f\u7528LLMs\u751f\u6210\u5927\u91cf\u6570\u636e\u65f6\u7684\u65f6\u95f4\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528LLMs\u63a8\u65ad\u5b57\u6bb5\u5206\u5e03\u5e76\u7f16\u7801\u4e3a\u91c7\u6837\u811a\u672c\uff0c\u81ea\u52a8\u5206\u7c7b\u5b57\u6bb5\u7c7b\u578b\u4ee5\u9ad8\u6548\u751f\u6210\u6570\u636e\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u6570\u636e\u771f\u5b9e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5927\u5e45\u51cf\u5c11\u751f\u6210\u8d1f\u62c5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u52a0\u901f\u751f\u4ea7\u6d41\u7a0b\u6d4b\u8bd5\uff0c\u7f29\u77ed\u5f00\u53d1\u5468\u671f\uff0c\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15846", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15846", "abs": "https://arxiv.org/abs/2507.15846", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGUI-G\u00b2\u5956\u52b1\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u5efa\u6a21GUI\u5143\u7d20\u7684\u7a7a\u95f4\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347GUI\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u4e8c\u5143\u5956\u52b1\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u4ea4\u4e92\u7684\u8fde\u7eed\u6027\uff0c\u800c\u4eba\u7c7b\u70b9\u51fb\u884c\u4e3a\u81ea\u7136\u5f62\u6210\u9ad8\u65af\u5206\u5e03\u3002", "method": "\u5f15\u5165GUI-G\u00b2\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u70b9\u5956\u52b1\u548c\u8986\u76d6\u5956\u52b1\uff0c\u5e76\u5f00\u53d1\u81ea\u9002\u5e94\u65b9\u5dee\u673a\u5236\u5904\u7406\u4e0d\u540c\u5143\u7d20\u5c3a\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGUI-G\u00b2\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534724.7%\u3002", "conclusion": "\u8fde\u7eed\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aGUI\u4ea4\u4e92\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.15857", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15857", "abs": "https://arxiv.org/abs/2507.15857", "authors": ["Mihir Prabhudesai", "Menging Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "comment": "Project Webpage: https://diffusion-scaling.github.io", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u60c5\u51b5\u4e0b\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u8d44\u6e90\u5145\u8db3\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u76f8\u5bf9\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u4f18\u52bf\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u8bbe\u7f6e\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u635f\u5931\u66f4\u4f4e\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u66f4\u597d\u3002", "conclusion": "\u5f53\u6570\u636e\u6210\u4e3a\u74f6\u9888\u65f6\uff0c\u6269\u6563\u6a21\u578b\u662f\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
