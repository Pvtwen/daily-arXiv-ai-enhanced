<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [cs.LG](#cs.LG) [Total: 79]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Understanding Human Daily Experience Through Continuous Sensing: ETRI Lifelog Dataset 2024](https://arxiv.org/abs/2508.03698)
*Se Won Oh,Hyuntae Jeong,Seungeun Chung,Jeong Mook Lim,Kyoung Ju Noh,Sunkyung Lee,Gyuwon Jung*

Main category: eess.SP

TL;DR: 论文介绍了ETRI Lifelog Dataset 2024，通过智能手机、智能手表和睡眠传感器被动收集数据，结合主观报告，旨在探索人类日常生活和生活方式模式。


<details>
  <summary>Details</summary>
Motivation: 提高对人类身心健康状态的准确理解，以改善健康和生活质量。

Method: 使用智能手机、智能手表和睡眠传感器被动连续收集数据，并结合主观自我报告（疲劳、压力和睡眠质量）。

Result: 构建了一个全面的生活日志数据集，部分数据已公开，可用于机器学习模型预测睡眠质量和压力。

Conclusion: ETRI Lifelog Dataset 2024为研究人类日常生活提供了基础资源，并展示了潜在应用。

Abstract: Improving human health and well-being requires an accurate and effective
understanding of an individual's physical and mental state throughout daily
life. To support this goal, we utilized smartphones, smartwatches, and sleep
sensors to collect data passively and continuously for 24 hours a day, with
minimal interference to participants' usual behavior, enabling us to gather
quantitative data on daily behaviors and sleep activities across multiple days.
Additionally, we gathered subjective self-reports of participants' fatigue,
stress, and sleep quality through surveys conducted immediately before and
after sleep. This comprehensive lifelog dataset is expected to provide a
foundational resource for exploring meaningful insights into human daily life
and lifestyle patterns, and a portion of the data has been anonymized and made
publicly available for further research. In this paper, we introduce the ETRI
Lifelog Dataset 2024, detailing its structure and presenting potential
applications, such as using machine learning models to predict sleep quality
and stress.

</details>


### [2] [Detection of Autonomic Dysreflexia in Individuals With Spinal Cord Injury Using Multimodal Wearable Sensors](https://arxiv.org/abs/2508.03715)
*Bertram Fuchs,Mehdi Ejtehadi,Ana Cisnal,Jürgen Pannek,Anke Scheel-Sailer,Robert Riener,Inge Eriks-Hoogland,Diego Paez-Granados*

Main category: eess.SP

TL;DR: 研究提出了一种非侵入性、可解释的机器学习框架，用于通过多模态可穿戴传感器检测自主神经反射异常（AD）。


<details>
  <summary>Details</summary>
Motivation: 自主神经反射异常（AD）是一种潜在危及生命的状况，现有监测方法多为侵入性或依赖主观症状报告，限制了日常应用。

Method: 研究使用多模态传感器数据（如ECG、PPG、BioZ等），通过BorutaSHAP进行特征选择，并采用堆叠集成模型进行训练。

Result: HR和ECG特征最具信息量，集成模型性能最高（Macro F1 = 0.77），HR的AUC为0.93。

Conclusion: 该模型为脊髓损伤患者的个性化实时监测提供了重要进展。

Abstract: Autonomic Dysreflexia (AD) is a potentially life-threatening condition
characterized by sudden, severe blood pressure (BP) spikes in individuals with
spinal cord injury (SCI). Early, accurate detection is essential to prevent
cardiovascular complications, yet current monitoring methods are either
invasive or rely on subjective symptom reporting, limiting applicability in
daily file. This study presents a non-invasive, explainable machine learning
framework for detecting AD using multimodal wearable sensors. Data were
collected from 27 individuals with chronic SCI during urodynamic studies,
including electrocardiography (ECG), photoplethysmography (PPG), bioimpedance
(BioZ), temperature, respiratory rate (RR), and heart rate (HR), across three
commercial devices. Objective AD labels were derived from synchronized
cuff-based BP measurements. Following signal preprocessing and feature
extraction, BorutaSHAP was used for robust feature selection, and SHAP values
for explainability. We trained modality- and device-specific weak learners and
aggregated them using a stacked ensemble meta-model. Cross-validation was
stratified by participants to ensure generalizability. HR- and ECG-derived
features were identified as the most informative, particularly those capturing
rhythm morphology and variability. The Nearest Centroid ensemble yielded the
highest performance (Macro F1 = 0.77+/-0.03), significantly outperforming
baseline models. Among modalities, HR achieved the highest area under the curve
(AUC = 0.93), followed by ECG (0.88) and PPG (0.86). RR and temperature
features contributed less to overall accuracy, consistent with missing data and
low specificity. The model proved robust to sensor dropout and aligned well
with clinical AD events. These results represent an important step toward
personalized, real-time monitoring for individuals with SCI.

</details>


### [3] [Zak-OTFS over CP-OFDM](https://arxiv.org/abs/2508.03906)
*Saif Khan Mohammed,Saurabh Prakash,Muhammad Ubadah,Imran Ali Khan,Ronny Hadani,Shlomo Rakib,Shachar Kons,Yoav Hebron,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: Zak-OTFS调制在高速延迟/多普勒扩展场景中表现优于CP-OFDM，本文提出了一种低复杂度的Zak-OTFS over CP-OFDM架构，可在现有网络中实现Zak-OTFS的优势。


<details>
  <summary>Details</summary>
Motivation: 解决在现有CP-OFDM调制解调器中支持Zak-OTFS调制的实际挑战。

Method: 通过sinc滤波和时间窗口约束，将Zak-OTFS调制实现为CP-OFDM的低复杂度预编码器，解调器实现为CP-OFDM解调输出的后处理。

Result: 提出了一种低复杂度的Zak-OTFS over CP-OFDM架构，CP-OFDM是其延迟周期取最小值时的特例。

Conclusion: 该架构可在现有网络基础设施中利用Zak-OTFS的优势，同时保持低复杂度。

Abstract: Zak-Orthogonal Time Frequency Space (Zak-OTFS) modulation has been shown to
achieve significantly better performance compared to the standardized
Cyclic-Prefix Orthogonal Frequency Division Multiplexing (CP-OFDM), in high
delay/Doppler spread scenarios envisaged in next generation communication
systems. Zak-OTFS carriers are quasi-periodic pulses in the delay-Doppler (DD)
domain, characterized by two parameters, (i) the pulse period along the delay
axis (``delay period") (Doppler period is related to the delay period), and
(ii) the pulse shaping filter. An important practical challenge is enabling
support for Zak-OTFS modulation in existing CP-OFDM based modems. In this paper
we show that Zak-OTFS modulation with pulse shaping constrained to sinc
filtering (filter bandwidth equal to the communication bandwidth $B$) followed
by time-windowing with a rectangular window of duration $(T + T_{cp})$ ($T$ is
the symbol duration and $T_{cp}$ is the CP duration), can be implemented as a
low-complexity precoder over standard CP-OFDM. We also show that the Zak-OTFS
de-modulator with matched filtering constrained to sinc filtering (filter
bandwidth $B$) followed by rectangular time windowing over duration $T$ can be
implemented as a low-complexity post-processing of the CP-OFDM de-modulator
output. This proposed ``Zak-OTFS over CP-OFDM" architecture enables us to
harness the benefits of Zak-OTFS in existing network infrastructure. We also
show that the proposed Zak-OTFS over CP-OFDM is a family of modulations, with
CP-OFDM being a special case when the delay period takes its minimum possible
value equal to the inverse bandwidth, i.e., Zak-OTFS over CP-OFDM with minimum
delay period.

</details>


### [4] [Optimal Interference Exploitation Waveform Design with Relaxed Block-Level Power Constraints](https://arxiv.org/abs/2508.04046)
*Xiao Tong,Lei Lei,Ang Li,A. Lee Swindlehurst,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 本文提出了一种非线性波形优化框架，用于多用户MIMO系统中基于构造性干扰的波形设计，解决了现有线性方法的性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于构造性干扰的线性预编码方法（如符号级预编码和块级预编码）因严格的功率约束或自由度不足而性能受限。

Method: 提出非线性波形优化框架，引入额外优化变量，最大化传输块内的最小构造性干扰度量，并通过函数和KKT条件推导闭式解。问题被重新表述为可处理的二次规划问题，并开发了改进的ADMM算法以提高计算效率。

Result: 仿真结果表明，所提算法在高阶调制和大块长度下显著优于传统方法。

Conclusion: 非线性波形优化框架有效提升了多用户MIMO系统中构造性干扰的性能，尤其在复杂场景下表现优越。

Abstract: This paper investigates constructive interference (CI)-based waveform design
for phase shift keying and quadrature amplitude modulation symbols under
relaxed block-level power constraints in multi-user multiple-input
single-output (MU-MIMO) communication systems. Existing linear CI-based
precoding methods, including symbol-level precoding (SLP) and block-level
precoding (BLP), suffer from performance limitations due to strict symbol-level
power budgets or insufficient degrees of freedom over the block. To overcome
these challenges, we propose a nonlinear waveform optimization framework that
introduces additional optimization variables and maximizes the minimum CI
metric across the transmission block. The optimal waveform is derived in closed
form using the function and Karush Kuhn Tucker conditions, and the solution is
explicitly expressed with respect to the dual variables. Moreover, the original
problems are equivalently reformulated as tractable quadratic programming (QP)
problems. To efficiently solve the derived QP problems, we develop an improved
alternating direction method of multipliers (ADMM) algorithm by integrating a
linear-time projection technique, which significantly enhances the
computational efficiency. Simulation results demonstrate that the proposed
algorithms substantially outperform the conventional CI-SLP and CI-BLP
approaches, particularly under high-order modulations and large block lengths.

</details>


### [5] [WiFo-CF: Wireless Foundation Model for CSI Feedback](https://arxiv.org/abs/2508.04068)
*Liu Xuanyu,Gao Shijian,Liu Boxun,Cheng Xiang,Yang Liuqing*

Main category: eess.SP

TL;DR: WiFo-CF是一种新型无线基础模型，通过多用户多速率自监督预训练和S-R MoE架构，解决了传统CSI反馈方案在异构配置下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习CSI反馈方案在固定系统配置下表现良好，但缺乏对异构配置的适应性和灵活性。

Method: 提出WiFo-CF模型，采用多用户多速率自监督预训练策略和S-R MoE架构，支持异构配置。

Result: 模型在模拟和真实场景中均表现出色，并成功应用于下游任务如CSI室内定位。

Conclusion: WiFo-CF具有优异的泛化能力和部署潜力。

Abstract: Deep learning-based channel state information (CSI) feedback schemes
demonstrate strong compression capabilities but are typically constrained to
fixed system configurations, limiting their generalization and flexibility. To
address this challenge, WiFo-CF, a novel wireless foundation model tailored for
CSI feedback, is proposed, uniquely accommodating heterogeneous configurations
such as varying channel dimensions, feedback rates, and data distributions
within a unified framework through its key innovations: (1) a multi-user,
multi-rate self-supervised pre-training strategy; and (2) a Mixture of Shared
and Routed Expert (S-R MoE) architecture. Supporting the large-scale
pre-training of WiFo-CF is the first heterogeneous channel feedback dataset,
whose diverse patterns enable the model to achieve superior performance on both
in-distribution and out-of-distribution data across simulated and real-world
scenarios. Furthermore, the learned representations effectively facilitate
adaptation to downstream tasks such as CSI-based indoor localization,
validating WiFo-CF's scalability and deployment potential.

</details>


### [6] [DFT-s-OFDM with Chirp Modulation](https://arxiv.org/abs/2508.04075)
*Yujie Liu,Yong Liang Guan,David González G.,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 提出了一种新的波形DFT-s-OFDM-CM，结合了啁啾调制和离散傅里叶变换扩展正交频分复用，提高了频谱效率并保持低误码率。


<details>
  <summary>Details</summary>
Motivation: 为下一代无线通信设计一种既能保持低峰均功率比和全频率分集利用，又能提高频谱效率的波形。

Method: 通过Q进制星座符号和啁啾信号的起始频率传递信息比特，结合了啁啾调制和DFT-s-OFDM的优点。

Result: 仿真结果表明，DFT-s-OFDM-CM在保持与啁啾DFT-s-OFDM相似的误码率的同时，实现了更高的频谱效率。

Conclusion: DFT-s-OFDM-CM在相同频谱效率下，通过信息比特分流，可以使用更低阶调制，对噪声更具鲁棒性，误码率更低。

Abstract: In this paper, a new waveform called discrete Fourier transform spread
orthogonal frequency division multiplexing with chirp modulation
(DFT-s-OFDM-CM) is proposed for the next generation of wireless communications.
The information bits are conveyed by not only Q-ary constellation symbols but
also the starting frequency of chirp signal. It could maintain the benefits
provided by the chirped discrete Fourier transform spread orthogonal frequency
division multiplexing (DFT-s-OFDM), e.g., low peak-to-average power ratio
(PAPR), full frequency diversity exploitation, etc. Simulation results confirm
that the proposed DFT-s-OFDM-CM could achieve higher spectral efficiency while
keeping the similar bit error rate (BER) to that of chirped DFT-s-OFDM. In
addition, when maintaining the same spectral efficiency, the proposed
DFT-s-OFDM-CM with the splitting of information bits into two streams enables
the use of lower-order constellation modulation and offers greater resilience
to noise, resulting in a lower BER than the chirped DFT-s-OFDM.

</details>


### [7] [Neuro-MoBRE: Exploring Multi-subject Multi-task Intracranial Decoding via Explicit Heterogeneity Resolving](https://arxiv.org/abs/2508.04128)
*Di Wu,Yifei Jia,Siyuan Li,Shiqi Zhao,Jie Yang,Mohamad Sawan*

Main category: eess.SP

TL;DR: Neuro-MoBRE是一种新型的神经解码框架，通过区域专家混合和预训练策略解决神经生理数据异质性，在多任务和跨被试解码中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有神经解码方法局限于单任务和单个被试，数据异质性限制了其泛化能力。

Method: 结合脑区-时间嵌入机制和专家混合方法，采用区域掩码自编码预训练和任务解耦信息聚合。

Result: 在11名被试的五个任务中表现优于现有方法，支持零样本解码。

Conclusion: Neuro-MoBRE为神经生理建模提供了一种通用且高效的解决方案。

Abstract: Neurophysiological decoding, fundamental to advancing brain-computer
interface (BCI) technologies, has significantly benefited from recent advances
in deep learning. However, existing decoding approaches largely remain
constrained to single-task scenarios and individual subjects, limiting their
broader applicability and generalizability. Efforts towards creating
large-scale neurophysiological foundation models have shown promise, but
continue to struggle with significant challenges due to pervasive data
heterogeneity across subjects and decoding tasks. Simply increasing model
parameters and dataset size without explicitly addressing this heterogeneity
fails to replicate the scaling successes seen in natural language processing.
Here, we introduce the Neural Mixture of Brain Regional Experts (Neuro-MoBRE),
a general-purpose decoding framework explicitly designed to manage the
ubiquitous data heterogeneity in neurophysiological modeling. Neuro-MoBRE
incorporates a brain-regional-temporal embedding mechanism combined with a
mixture-of-experts approach, assigning neural signals from distinct brain
regions to specialized regional experts on a unified embedding basis, thus
explicitly resolving both structural and functional heterogeneity.
Additionally, our region-masked autoencoding pre-training strategy further
enhances representational consistency among subjects, complemented by a
task-disentangled information aggregation method tailored to effectively handle
task-specific neural variations. Evaluations conducted on intracranial
recordings from 11 subjects across five diverse tasks, including complex
language decoding and epileptic seizure diagnosis, demonstrate that Neuro-MoBRE
surpasses prior art and exhibits robust generalization for zero-shot decoding
on unseen subjects.

</details>


### [8] [Dual-Function Radar-Communication Beamforming with Outage Probability Metric](https://arxiv.org/abs/2508.04144)
*Hossein Maleki,Carles Diaz-Vilor,Ali Pezeshki,Vahid Tarokh,Hamid Jafarkhani*

Main category: eess.SP

TL;DR: 提出了一种用于双功能雷达通信系统的波束成形方法，优化雷达和通信性能，解决频谱拥塞问题。


<details>
  <summary>Details</summary>
Motivation: 通过集成通信和感知设计，解决频谱资源紧张问题。

Method: 针对雷达中心和通信中心两种场景，分别优化雷达性能和通信性能，通过随机优化问题转化为确定性非凸问题，并松弛为半定规划。

Result: 数值实验验证了所提设计的有效性。

Conclusion: 该方法在频谱共享和性能优化方面具有潜力。

Abstract: The integrated design of communication and sensing may offer a potential
solution to address spectrum congestion. In this work, we develop a beamforming
method for a dual-function radar-communication system, where the transmit
signal is used for both radar surveillance and communication with multiple
downlink users, despite imperfect channel state information (CSI). We focus on
two scenarios of interest: radar-centric and communication-centric. In the
radar-centric scenario, the primary goal is to optimize radar performance while
attaining acceptable communication performance. To this end, we minimize a
weighted sum of the mean-squared error in achieving a desired beampattern and a
mean-squared cross correlation of the radar returns from directions of interest
(DOI). We also seek to ensure that the probability of outage for the
communication users remains below a desired threshold. In the
communication-centric scenario, our main objective is to minimize the maximum
probability of outage among the communication users while keeping the
aforementioned radar metrics below a desired threshold. Both optimization
problems are stochastic and untractable. We first take advantage of central
limit theorem to obtain deterministic non-convex problems and then consider
relaxations of these problems in the form of semidefinite programs with rank-1
constraints. We provide numerical experiments demonstrating the effectiveness
of the proposed designs.

</details>


### [9] [Subspace Fitting Approach for Wideband Near-Field Localization](https://arxiv.org/abs/2508.04169)
*Ruiyun Zhang,Zhaolin Wang,Zhiqing Wei,Yuanwei Liu,Zehui Xiong,Zhiyong Feng*

Main category: eess.SP

TL;DR: 提出了两种子空间拟合方法用于宽带近场定位，解决了传统远场系统中距离和角度耦合的问题。


<details>
  <summary>Details</summary>
Motivation: 近场系统中球面波传播导致距离和角度参数耦合，传统方法无法单独估计，需要新的解决方案。

Method: 1. 推导了多目标宽带系统的频域近场信号模型，提出基于子空间拟合的MUSIC方法联合估计距离和角度；2. 引入Fresnel近似MUSIC算法降低复杂度并解耦参数。

Result: 数值结果验证了两种方法的有效性。

Conclusion: 提出的方法在近场定位中解决了参数耦合问题，并通过Fresnel近似降低了计算复杂度。

Abstract: Two subspace fitting approaches are proposed for wideband near-field
localization. Unlike in conventional far-field systems, where distance and
angle can be estimated separately, spherical wave propagation in near-field
systems couples these parameters. We therefore derive a frequency-domain
near-field signal model for multi-target wideband systems and develop a
subspace fitting-based MUSIC method that jointly estimates distance and angle.
To reduce complexity, a Fresnel approximation MUSIC algorithm is further
introduced to decouple the distance and angle parameters. Numerical results
verify the effectiveness of both proposed approaches.

</details>


### [10] [Simultaneous Information and Control Signalling Protocol for RIS-Empowered Wireless Systems](https://arxiv.org/abs/2508.04185)
*Evangelos Koutsonas,Xiaonan Mu,Nan Qi,Stylianos Trevlakis,Theodoros A. Tsiftsis,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: 论文提出了一种名为SICS的协议，通过无线控制信号传输解决RIS（可重构智能表面）与MC（微控制器）之间因信号延迟导致的过时问题。


<details>
  <summary>Details</summary>
Motivation: 在无线接入网络中，RIS与MC之间的信号延迟可能超过信道相干时间，导致信号过时，影响性能。

Method: 采用SICS协议，结合STAR模式和NOMA技术，优化RIS的反射与传输系数以及NOMA的叠加系数。

Result: SICS方法显示出较强的鲁棒性，能够有效提升用户数据速率。

Conclusion: SICS协议为解决RIS与MC之间的信号延迟问题提供了一种有效方案。

Abstract: Integration of RIS in radio access networks requires signaling between edge
units and the RIS microcontroller (MC). Unfortunately, in several practical
scenarios, the signaling latency is higher than the communication channel
coherence time, which causes outdated signaling at the RIS. To counterbalance
this, we introduce a simultaneous information and control signaling (SICS)
protocol that enables operation adaptation through wireless control signal
transmission. SICS assumes that the MC is equipped with a single antenna that
operates at the same frequency as the RIS. RIS operates in simultaneous
transmission and reflection (STAR) mode, and the source employs non-orthogonal
multiple access (NOMA) to superposition the information signal to the control
signal. To maximize the achievable user data rate while ensuring the MC's
ability to decode the control signal, we formulate and solve the corresponding
optimization problem that returns RIS's reflection and transmission
coefficients as well as the superposition coefficients of the NOMA scheme. Our
results reveal the robustness of the SICS approach.

</details>


### [11] [Channel-Coherence-Adaptive Two-Stage Fully Digital Combining for mmWave MIMO Systems](https://arxiv.org/abs/2508.04214)
*Yasaman Khorsandmanesh,Emil Björnson,Joakim Jaldén,Bengt Lindoff*

Main category: eess.SP

TL;DR: 提出了一种新型两阶段数字组合方案，用于毫米波宽带点对点MIMO系统，以降低计算和硬件复杂度，并在性能上优于混合波束成形。


<details>
  <summary>Details</summary>
Motivation: 解决移动用户设备（UE）场景中数字组合处理大量基带样本的挑战。

Method: 采用两阶段数字组合方案：第一阶段利用信道几何减少信号流数量；第二阶段基于信道衰落实时更新。开发了基于最大似然估计的信道估计框架。

Result: 数值结果表明，该方法在频谱效率上优于混合波束成形。

Conclusion: 两阶段全数字收发器在未来系统中具有吸引力。

Abstract: This paper considers a millimeter-wave wideband point-to-point MIMO system
with fully digital transceivers at the base station and the user equipment
(UE), focusing on mobile UE scenarios. A main challenge when building a digital
UE combining is the large volume of baseband samples to handle. To mitigate
computational and hardware complexity, we propose a novel two-stage digital
combining scheme at the UE. The first stage reduces the $N_{\text{r}}$ received
signals to $N_{\text{c}}$ streams before baseband processing, leveraging
channel geometry for dimension reduction and updating at the beam coherence
time, which is longer than the channel coherence time of the small-scale
fading. By contrast, the second-stage combining is updated per fading
realization. We develop a pilot-based channel estimation framework for this
hardware setup based on maximum likelihoodestimation in both uplink and
downlink. Digital precoding and combining designs are proposed, and a spectral
efficiency expression that incorporates imperfect channel knowledge is derived.
The numerical results demonstrate that the proposed approach outperforms hybrid
beamforming, showcasing the attractiveness of using two-stage fully digital
transceivers in future systems.

</details>


### [12] [Near-Field Spatial non-Stationary Channel Estimation: Visibility-Region-HMM-Aided Polar-Domain Simultaneous OMP](https://arxiv.org/abs/2508.04222)
*Thibaut Ceulemans,Cel Thys,Robbert Beerten,Zhuangzhuang Cui,Sofie Pollin*

Main category: eess.SP

TL;DR: 论文提出了一种针对极大规模天线阵列（ELAA）系统的信道估计方法，通过结合物理模型和隐马尔可夫模型（HMM）改进传统技术的不足。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计技术在ELAA系统中因近场传播和空间非平稳性而效果不佳，需要新的解决方案。

Method: 提出了一种基于物理的混合信道模型，结合非二进制可见区域（VR）掩码，并开发了VR-HMM-P-SOMP算法，通过HMM和Viterbi解码自适应调整波束导向向量。

Result: 仿真结果表明，该方法在低信噪比和稀疏场景下提高了估计精度，同时保持低计算复杂度。

Conclusion: 该算法在多种设计参数和信道条件下表现稳健，为ELAA系统提供了实用解决方案。

Abstract: This work focuses on channel estimation in extremely large aperture array
(ELAA) systems, where near-field propagation and spatial non-stationarity
introduce complexities that hinder the effectiveness of traditional estimation
techniques. A physics-based hybrid channel model is developed, incorporating
non-binary visibility region (VR) masks to simulate diffraction-induced power
variations across the antenna array. To address the estimation challenges posed
by these channel conditions, a novel algorithm is proposed:
Visibility-Region-HMM-Aided Polar-Domain Simultaneous Orthogonal Matching
Pursuit (VR-HMM-P-SOMP). The method extends a greedy sparse recovery framework
by integrating VR estimation through a hidden Markov model (HMM), using a novel
emission formulation and Viterbi decoding. This allows the algorithm to
adaptively mask steering vectors and account for spatial non-stationarity at
the antenna level. Simulation results demonstrate that the proposed method
enhances estimation accuracy compared to existing techniques, particularly in
low-SNR and sparse scenarios, while maintaining a low computational complexity.
The algorithm presents robustness across a range of design parameters and
channel conditions, offering a practical solution for ELAA systems.

</details>


### [13] [Spectral Efficiency-Aware Codebook Design for Task-Oriented Semantic Communications](https://arxiv.org/abs/2508.04223)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Shuai Liu,Hongyang Du,Geyong Min*

Main category: eess.SP

TL;DR: 论文提出了一种基于Wasserstein距离的自适应混合分布方案（WS-DC），用于设计频谱效率感知的码本，以提升任务导向语义通信的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向语义通信（ToSC）方法依赖稀疏激活的码本，导致频谱效率低和信道容量未充分利用，亟需设计一种既能支持任务推理又能接近信道容量理论极限的码本。

Method: 构建了频谱效率感知的码本设计框架，将码本激活概率纳入优化过程，并引入Wasserstein距离作为正则化度量，最小化学习分布与最优信道输入分布的差距。

Result: 实验表明，WS-DC在推理准确性和码本效率上均优于现有方法，为接近容量极限的语义通信系统提供了新方向。

Conclusion: WS-DC是一种高效的任务驱动和信道感知的潜在表示学习方法，显著提升了语义通信系统的性能。

Abstract: Digital task-oriented semantic communication (ToSC) aims to transmit only
task-relevant information, significantly reducing communication overhead.
Existing ToSC methods typically rely on learned codebooks to encode semantic
features and map them to constellation symbols. However, these codebooks are
often sparsely activated, resulting in low spectral efficiency and
underutilization of channel capacity. This highlights a key challenge: how to
design a codebook that not only supports task-specific inference but also
approaches the theoretical limits of channel capacity. To address this
challenge, we construct a spectral efficiency-aware codebook design framework
that explicitly incorporates the codebook activation probability into the
optimization process. Beyond maximizing task performance, we introduce the
Wasserstein (WS) distance as a regularization metric to minimize the gap
between the learned activation distribution and the optimal channel input
distribution. Furthermore, we reinterpret WS theory from a generative
perspective to align with the semantic nature of ToSC. Combining the above two
aspects, we propose a WS-based adaptive hybrid distribution scheme, termed
WS-DC, which learns compact, task-driven and channel-aware latent
representations. Experimental results demonstrate that WS-DC not only
outperforms existing approaches in inference accuracy but also significantly
improves codebook efficiency, offering a promising direction toward
capacity-approaching semantic communication systems.

</details>


### [14] [ChineseEEG-2: An EEG Dataset for Multimodal Semantic Alignment and Neural Decoding during Reading and Listening](https://arxiv.org/abs/2508.04240)
*Sitong Chen,Beiqianyi Li,Cuilin He,Dongyang Li,Mingyang Wu,Xinke Shen,Song Wang,Xuetao Wei,Xindi Wang,Haiyan Wu,Quanying Liu*

Main category: eess.SP

TL;DR: ChineseEEG-2是一个高密度EEG数据集，用于多模态语言任务下的神经解码模型基准测试，支持中文。


<details>
  <summary>Details</summary>
Motivation: 解决非英语语言中脑-语言配对数据的稀缺问题，促进脑活动与大型语言模型语义对齐的研究。

Method: 扩展了之前的ChineseEEG数据集，新增了朗读和被动听两种模态，记录了EEG和音频数据，并进行了语义对齐。

Result: 提供了EEG信号、音频、语义嵌入和任务标签，支持多模态语言任务的神经解码算法基准测试。

Conclusion: ChineseEEG-2为下一代神经语义解码提供了基准数据集，推动了中文环境下脑-LLM对齐的研究。

Abstract: EEG-based neural decoding requires large-scale benchmark datasets. Paired
brain-language data across speaking, listening, and reading modalities are
essential for aligning neural activity with the semantic representation of
large language models (LLMs). However, such datasets are rare, especially for
non-English languages. Here, we present ChineseEEG-2, a high-density EEG
dataset designed for benchmarking neural decoding models under real-world
language tasks. Building on our previous ChineseEEG dataset, which focused on
silent reading, ChineseEEG-2 adds two active modalities: Reading Aloud (RA) and
Passive Listening (PL), using the same Chinese corpus. EEG and audio were
simultaneously recorded from four participants during ~10.7 hours of reading
aloud. These recordings were then played to eight other participants,
collecting ~21.6 hours of EEG during listening. This setup enables speech
temporal and semantic alignment across the RA and PL modalities. ChineseEEG-2
includes EEG signals, precise audio, aligned semantic embeddings from
pre-trained language models, and task labels. Together with ChineseEEG, this
dataset supports joint semantic alignment learning across speaking, listening,
and reading. It enables benchmarking of neural decoding algorithms and promotes
brain-LLM alignment under multimodal language tasks, especially in Chinese.
ChineseEEG-2 provides a benchmark dataset for next-generation neural semantic
decoding.

</details>


### [15] [Delay-Doppler Domain Signal Processing Aided OFDM (DD-a-OFDM) for 6G and Beyond](https://arxiv.org/abs/2508.04253)
*Yiyan Ma,Bo Ai,Jinhong Yuan,Shuangyang Li,Qingqing Cheng,Zhenguo Shi,Weijie Yuan,Zhiqiang Wei,Akram Shafie,Guoyu Ma,Yunlong Lu,Mi Yang,Zhangdui Zhong*

Main category: eess.SP

TL;DR: 本文提出了一种基于延迟-多普勒（DD）域信号处理的OFDM增强方案（DD-a-OFDM），以解决OFDM在高移动性场景中的性能问题，并优于OTFS。


<details>
  <summary>Details</summary>
Motivation: 高移动性场景是6G系统的关键部分，但OFDM在多普勒扩展下性能下降，而OTFS虽能利用时频域多样性，但接收复杂度高且资源分配不灵活。因此，需要一种改进方案。

Method: 设计了DD-a-OFDM系统结构，保留经典OFDM收发机，引入DD域信道估计和时频域均衡。通过离散时频导频进行DD域信道估计，并开发了ML和峰值检测的信道估计器及时频域均衡器。

Result: 数值结果表明，DD-a-OFDM比经典OFDM降低了误码率，且在信道估计精度和导频开销上优于OTFS。

Conclusion: DD-a-OFDM是一种有前景的6G波形方案，结合了OFDM的灵活性和DD域信号处理的优势。

Abstract: High-mobility scenarios will be a critical part of 6G systems. Since the
widely deployed orthogonal frequency division multiplexing (OFDM) waveform
suffers from subcarrier orthogonality loss under severe Doppler spread,
delay-Doppler domain multi-carrier (DDMC) modulation systems, such as
orthogonal time frequency space (OTFS), have been extensively studied. While
OTFS can exploit time-frequency (TF) domain channel diversity, it faces
challenges including high receiver complexity and inflexible TF resource
allocation, making OFDM still the most promising waveform for 6G. In this
article, we propose a DD domain signal processing-aided OFDM (DD-a-OFDM) scheme
to enhance OFDM performance based on DDMC research insights. First, we design a
DD-a-OFDM system structure, retaining the classical OFDM transceiver while
incorporating DD domain channel estimation and TF domain equalization. Second,
we detail DD domain channel estimation using discrete TF pilots and prove that
TF domain inter-carrier interference (ICI) could be transformed into DD domain
Gaussian interference. Third, we derive closed-form Cram\'{e}r-Rao lower bounds
(CRLBs) for DD domain channel estimation. Fourth, we develop maximum likelihood
(ML) and peak detection-based channel estimators, along with a corresponding TF
domain equalizer. Numerical results verify the proposed design, showing that
DD-a-OFDM reduces the bit-error rate (BER) compared to classical OFDM and
outperforms OTFS in channel estimation accuracy with lower pilot overhead.

</details>


### [16] [Less Signals, More Understanding: Channel-Capacity Codebook Design for Digital Task-Oriented Semantic Communication](https://arxiv.org/abs/2508.04291)
*Anbang Zhang,Shuaishuai Guo,Chenyuan Feng,Hongyang Du,Haojin Li,Chen Sun,Haijun Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种针对低功耗边缘网络的信道感知离散语义编码框架，通过Wasserstein正则化目标优化语义保真度和任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有任务导向语义通信（ToSC）框架在离散映射中常忽略信道特性和任务需求，导致性能下降和泛化能力受限。

Method: 采用Wasserstein正则化目标，将离散码激活与最优输入分布对齐，提升语义保真度和鲁棒性。

Result: 实验表明，该方法在不同信噪比（SNR）下显著提高了任务准确性和通信效率。

Conclusion: 该研究为离散语义与信道优化的结合提供了新思路，推动了语义通信在未来数字基础设施中的广泛应用。

Abstract: Discrete representation has emerged as a powerful tool in task-oriented
semantic communication (ToSC), offering compact, interpretable, and efficient
representations well-suited for low-power edge intelligence scenarios. Its
inherent digital nature aligns seamlessly with hardware-friendly deployment and
robust storage/transmission protocols. However, despite its strengths, current
ToSC frameworks often decouple semantic-aware discrete mapping from the
underlying channel characteristics and task demands. This mismatch leads to
suboptimal communication performance, degraded task utility, and limited
generalization under variable wireless conditions. Moreover, conventional
designs frequently overlook channel-awareness in codebook construction,
restricting the effectiveness of semantic symbol selection under constrained
resources. To address these limitations, this paper proposes a channel-aware
discrete semantic coding framework tailored for low-power edge networks.
Leveraging a Wasserstein-regularized objective, our approach aligns discrete
code activations with optimal input distributions, thereby improving semantic
fidelity, robustness, and task accuracy. Extensive experiments on the inference
tasks across diverse signal-to-noise ratio (SNR) regimes show that our method
achieves notable gains in accuracy and communication efficiency. This work
provides new insights into integrating discrete semantics and channel
optimization, paving the way for the widespread adoption of semantic
communication in future digital infrastructures.

</details>


### [17] [Energy Efficient Fluid Antenna Relay (FAR)-Assisted Wireless Communications](https://arxiv.org/abs/2508.04322)
*Ruopeng Xu,Zhaohui Yang,Zhaoyang Zhang,Mohammad Shikh-Bahaei,Kaibin Huang,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于流体天线中继（FAR）的高能效无线通信系统，解决了由障碍物引起的非视距（NLoS）链路问题，并通过优化天线位置和协议设计提升能效。


<details>
  <summary>Details</summary>
Motivation: 6G通信需求推动下，流体天线系统（FAS）因其灵活性成为关键技术，但现有研究主要关注视距（LoS）场景，忽视了非视距（NLoS）链路的问题。

Method: 设计了结合放大转发（AF）协议的FAR辅助通信系统，优化天线位置以实现可控相位偏移，并建立联合考虑障碍物、大尺度和小尺度衰落的信道模型。通过迭代算法优化系统能效。

Result: 仿真结果显示，所提算法在能效上优于传统方案，比可重构智能表面（RIS）和传统AF中继方案分别高出23.39%和39.94%。

Conclusion: FAR系统通过优化设计和算法，显著提升了NLoS场景下的通信能效，为6G通信提供了有效解决方案。

Abstract: In this paper, we propose an energy efficient wireless communication system
based on fluid antenna relay (FAR) to solve the problem of non-line-of-sight
(NLoS) links caused by blockages with considering the physical properties.
Driven by the demand for the sixth generation (6G) communication, fluid antenna
systems (FASs) have become a key technology due to their flexibility in
dynamically adjusting antenna positions. Existing research on FAS primarily
focuses on line-of-sight (LoS) communication scenarios, and neglects the
situations where only NLoS links exist. To address the issues posted by NLoS
communication, we design an FAR-assisted communication system combined with
amplify-and-forward (AF) protocol. In order to alleviate the high energy
consumption introduced by AF protocol while ensuring communication quality, we
formulate an energy efficiency (EE) maximization problem. By optimizing the
positions of the fluid antennas (FAs) on both sides of the FAR, we achieve
controllable phase shifts of the signals transmitting through the blockage
which causes the NLoS link. Besides, we establish a channel model that jointly
considers the blockage-through matrix, large-scale fading, and small-scale
fading. To maximize the EE of the system, we jointly optimize the FAR position,
FA positions, power control, and beamforming design under given constraints,
and propose an iterative algorithm to solve this formulated optimization
problem. Simulation results show that the proposed algorithm outperforms the
traditional schemes in terms of EE, achieving up to $23.39\%$ and $39.94\%$
higher EE than the conventional reconfigurable intelligent surface (RIS) scheme
and traditional AF relay scheme, respectively.

</details>


### [18] [Near-field Liquid Crystal RIS Phase-Shift Design for Secure Wideband Illumination](https://arxiv.org/abs/2508.04331)
*Mohamadreza Delbari,Qikai Zhou,Robin Neuder,Alejandro Jiménez-Sáez,Vahid Jamali*

Main category: eess.SP

TL;DR: 论文提出了一种基于液晶技术的可重构智能表面（RIS）设计，用于宽带OFDM系统，以解决频率依赖性导致的性能下降问题，并提高安全通信的保密率。


<details>
  <summary>Details</summary>
Motivation: 液晶RIS的相位响应具有频率依赖性，可能导致性能下降和信息泄漏，尤其在安全通信系统中。

Method: 设计了一种RIS算法，用于宽带OFDM系统，以定向照射合法用户区域，同时避免潜在窃听者区域的泄漏。

Result: 仿真结果表明，该方法在8 GHz带宽和60 GHz中心频率下，保密率约为2 bits/symbol。

Conclusion: 所提方法有效解决了频率依赖性问题，提高了安全通信的保密率。

Abstract: Liquid crystal (LC) technology provides a low-power and scalable approach to
implement a reconfigurable intelligent surface (RIS). However, the LC-based
RIS's phase-shift response is inherently frequency-dependent, which can lead to
performance degradation if not properly addressed. This issue becomes
especially critical in secure communication systems, where such variations may
result in considerable information leakage. To avoid the need for full channel
state information (CSI) acquisition and frequent RIS reconfiguration, we design
RIS for a wideband orthogonal frequency division multiplexing (OFDM) system to
illuminate a desired area containing legitimate users while avoiding leakage to
regions where potential eavesdroppers may be located. Our simulation results
demonstrate that the proposed algorithm improves the secrecy rate compared to
methods that neglect frequency-dependent effects. In the considered setup, the
proposed method achieves a secrecy rate of about 2 bits/symbol over an 8 GHz
bandwidth when the center frequency is 60 GHz.

</details>


### [19] [Joint Communication and Indoor Positioning Based on Visible Light in the Presence of Dimming](https://arxiv.org/abs/2508.04570)
*A. Tarik Leblebici,Sumeyra Hassan,Erdal Panayirci,H. Vincent Poor*

Main category: eess.SP

TL;DR: 提出了一种基于可见光通信（VLC）的联合通信与室内定位系统（JCP），通过改进的定位算法和高效的调制技术，实现了高精度定位和可靠通信。


<details>
  <summary>Details</summary>
Motivation: 为未来6G室内网络提供一种集成了定位与通信的高效解决方案，满足高精度定位和可靠通信的需求。

Method: 使用接收信号强度（RSS）和激进轴定理提高定位精度；采用空间调制（SM）和M进制脉冲幅度调制（PAM）提升频谱效率；通过最小二乘（LS）估计器实现联合信道和调光系数估计。

Result: 仿真结果显示，在高信噪比下实现亚厘米级定位精度，低阶PAM方案的误码率低于10^{-6}。

Conclusion: 该系统在6G室内网络中具有潜在应用价值，能够满足实际信道条件下的集成定位与通信需求。

Abstract: This paper proposes a joint communication and indoor positioning (JCP) system
based on visible light communication (VLC) designed for high-precision indoor
environments. The framework supports 2D and 3D positioning using received
signal strength (RSS) from pilot transmissions, enhanced by the radical axis
theorem to improve accuracy under measurement uncertainties. Communication is
achieved using spatial modulation (SM) with M-ary pulse amplitude modulation
(PAM), where data is conveyed through the modulation symbol and the active
light-emitting diode (LED) index, improving spectral efficiency while
maintaining low complexity. A pilot-aided least squares (LS) estimator is
employed for joint channel and dimming coefficient estimation, enabling robust
symbol detection in multipath environments characterized by both line-of-sight
(LOS) and diffuse non-line-of-sight (NLOS) components, modeled using Rician
fading. The proposed system incorporates a dimming control mechanism to meet
lighting requirements while maintaining reliable communication and positioning
performance. Simulation results demonstrate sub-centimeter localization
accuracy at high signal-to-noise ratios (SNRs) and bit error rates (BERs) below
10^{-6} for low-order PAM schemes. Additionally, comparative analysis across
user locations reveals that positioning and communication performance improve
significantly near the geometric center of the LED layout. These findings
validate the effectiveness of the proposed system for future 6G indoor networks
requiring integrated localization and communication under practical channel
conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: PriCon框架通过结合监督对比学习和特权信息学习，提升了情感计算模型从实验室到现实环境的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算模型从实验室环境到现实环境迁移的可靠性问题。

Method: 提出Privileged Contrastive Pretraining (PriCon)框架，结合监督对比学习和特权信息学习。

Result: 在RECOLA和AGAIN数据集上，PriCon表现优于LUPI和端到端模型，接近全模态训练模型的性能。

Conclusion: PriCon为缩小实验室与真实环境情感建模差距提供了可扩展的解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [21] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: PILOT-C是一种新型轨迹压缩框架，结合频域物理建模和误差有界优化，支持任意维度轨迹压缩，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有线简化方法通常假设2D轨迹，忽略时间同步和运动连续性，无法满足高维轨迹压缩需求。

Method: PILOT-C通过独立压缩每个空间轴，结合频域物理建模和误差有界优化，支持任意维度轨迹压缩。

Result: 在四个真实数据集上，PILOT-C在压缩比和轨迹保真度上均优于现有方法，3D轨迹压缩比提升49%。

Conclusion: PILOT-C是一种高效、通用的轨迹压缩框架，适用于高维数据，性能显著优于现有技术。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [22] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: CX-Mind是一种基于课程强化学习和可验证过程奖励的生成模型，用于改进多任务CXR诊断，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在CXR诊断中缺乏可验证的推理过程监督，导致多任务诊断效率低。

Method: 采用课程强化学习和可验证过程奖励（CuRL-VPR），构建指令调优数据集CX-Set，分两阶段优化。

Result: CX-Mind在视觉理解、文本生成和时空对齐方面表现优异，性能提升25.1%，在临床数据集上表现突出。

Conclusion: CX-Mind在多任务CXR诊断中具有显著优势，临床实用性得到专家验证。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [23] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为Latent Knowledge Scalpel (LKS)的方法，通过轻量级超网络编辑大语言模型中的潜在知识，实现大规模精确编辑，同时保持模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在预训练中可能保留不准确或过时的信息，导致推理时出现错误或偏见。现有编辑方法难以同时编辑大量事实信息且可能损害模型的通用能力。

Method: 通过编辑LLMs的内部表示，类似编辑自然语言输入的方式替换实体，并引入LKS方法，利用轻量级超网络操作特定实体的潜在知识。

Result: 在Llama-2和Mistral上的实验表明，即使同时编辑数量达到10,000，LKS仍能有效进行知识编辑并保持模型的通用能力。

Conclusion: LKS是一种可行的大规模知识编辑方法，能够在不损害模型通用能力的情况下实现精确编辑。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [24] [Explainable AI Methods for Neuroimaging: Systematic Failures of Common Tools, the Need for Domain-Specific Validation, and a Proposal for Safe Application](https://arxiv.org/abs/2508.02560)
*Nys Tjade Siegel,James H. Cole,Mohamad Habes,Stefan Haufe,Kerstin Ritter,Marc-André Schulz*

Main category: cs.LG

TL;DR: 该论文首次大规模系统比较了XAI方法在神经影像数据上的表现，发现常用方法如GradCAM和Layer-wise Relevance Propagation存在系统性失败，而SmoothGrad表现稳健。


<details>
  <summary>Details</summary>
Motivation: 验证XAI方法在神经影像数据中的可靠性，避免因方法不适用导致的错误解释。

Method: 构建了一个新的XAI验证框架，使用约45,000张结构脑MRI数据，通过已知信号源的任务验证XAI方法的准确性。

Result: GradCAM无法准确定位预测特征，Layer-wise Relevance Propagation产生大量虚假解释，而SmoothGrad表现稳定。

Conclusion: XAI方法需要针对神经影像数据进行特定领域适配和验证，现有研究中的解释可能需要重新评估。

Abstract: Trustworthy interpretation of deep learning models is critical for
neuroimaging applications, yet commonly used Explainable AI (XAI) methods lack
rigorous validation, risking misinterpretation. We performed the first
large-scale, systematic comparison of XAI methods on ~45,000 structural brain
MRIs using a novel XAI validation framework. This framework establishes
verifiable ground truth by constructing prediction tasks with known signal
sources - from localized anatomical features to subject-specific clinical
lesions - without artificially altering input images. Our analysis reveals
systematic failures in two of the most widely used methods: GradCAM
consistently failed to localize predictive features, while Layer-wise Relevance
Propagation generated extensive, artifactual explanations that suggest
incompatibility with neuroimaging data characteristics. Our results indicate
that these failures stem from a domain mismatch, where methods with design
principles tailored to natural images require substantial adaptation for
neuroimaging data. In contrast, the simpler, gradient-based method SmoothGrad,
which makes fewer assumptions about data structure, proved consistently
accurate, suggesting its conceptual simplicity makes it more robust to this
domain shift. These findings highlight the need for domain-specific adaptation
and validation of XAI methods, suggest that interpretations from prior
neuroimaging studies using standard XAI methodology warrant re-evaluation, and
provide urgent guidance for practical application of XAI in neuroimaging.

</details>


### [25] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: GlaBoost是一个多模态梯度提升框架，结合临床特征、眼底图像和专家文本描述，用于青光眼风险预测，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测青光眼至关重要，但现有方法依赖单模态数据且缺乏可解释性，限制了临床实用性。

Method: GlaBoost整合了临床特征、眼底图像嵌入和专家文本描述，使用预训练卷积编码器和基于Transformer的语言模型提取特征，并通过增强的XGBoost模型进行分类。

Result: 在真实数据集上，GlaBoost验证准确率达98.71%，特征重要性分析显示临床一致的模式。

Conclusion: GlaBoost为可解释的青光眼诊断提供了透明且可扩展的解决方案，并可扩展至其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [26] [Scalable Neural Network-based Blackbox Optimization](https://arxiv.org/abs/2508.03827)
*Pavankumar Koratikere,Leifur Leifsson*

Main category: cs.LG

TL;DR: SNBO是一种新型的基于神经网络的贝叶斯优化方法，通过分离探索和开发标准高效优化高维问题，显著减少计算量和时间。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在高维和大规模评估时面临计算复杂度高的问题，而基于神经网络的优化方法通常依赖复杂的模型不确定性估计。

Method: SNBO提出不依赖模型不确定性估计，通过独立的标准进行探索和开发，并自适应控制采样区域。

Result: 在10到102维的优化问题中，SNBO在大多数情况下优于基线算法，减少了40-60%的函数评估需求，并显著降低运行时间。

Conclusion: SNBO是一种高效且可扩展的高维黑盒优化方法，优于现有技术。

Abstract: Bayesian Optimization (BO) is a widely used approach for blackbox
optimization that leverages a Gaussian process (GP) model and an acquisition
function to guide future sampling. While effective in low-dimensional settings,
BO faces scalability challenges in high-dimensional spaces and with large
number of function evaluations due to the computational complexity of GP
models. In contrast, neural networks (NNs) offer better scalability and can
model complex functions, which led to the development of NN-based BO
approaches. However, these methods typically rely on estimating model
uncertainty in NN prediction -- a process that is often computationally
intensive and complex, particularly in high dimensions. To address these
limitations, a novel method, called scalable neural network-based blackbox
optimization (SNBO), is proposed that does not rely on model uncertainty
estimation. Specifically, SNBO adds new samples using separate criteria for
exploration and exploitation, while adaptively controlling the sampling region
to ensure efficient optimization. SNBO is evaluated on a range of optimization
problems spanning from 10 to 102 dimensions and compared against four
state-of-the-art baseline algorithms. Across the majority of test problems,
SNBO attains function values better than the best-performing baseline
algorithm, while requiring 40-60% fewer function evaluations and reducing the
runtime by at least an order of magnitude.

</details>


### [27] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 提出了一种新的低秩Tucker表示模型（LRTuckerRep），结合全局低秩和局部平滑先验，通过自适应加权核范数和无参数拉普拉斯正则化，实现了高效的多维数据补全。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如低秩近似或平滑正则化）存在计算成本高或泛化能力差的问题，需一种统一且高效的方法。

Method: LRTuckerRep结合Tucker分解，使用自适应加权核范数编码低秩性，无参数拉普拉斯正则化捕捉平滑性，并开发了两种迭代算法。

Result: 实验表明，LRTuckerRep在高缺失率下优于基线方法，具有更高的补全精度和鲁棒性。

Conclusion: LRTuckerRep为多维数据补全提供了一种高效且统一的解决方案，显著提升了性能。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [28] [DP-NCB: Privacy Preserving Fair Bandits](https://arxiv.org/abs/2508.03836)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了一种名为DP-NCB的新算法框架，同时实现差分隐私和公平性，填补了多臂老虎机算法中隐私与公平性兼顾的研究空白。


<details>
  <summary>Details</summary>
Motivation: 在多臂老虎机算法中，隐私保护和公平性通常被独立研究，但两者能否同时实现尚不清楚。

Method: 提出DP-NCB框架，结合差分隐私和纳什遗憾优化，适用于全局和局部隐私模型。

Result: 理论证明DP-NCB在隐私和公平性上均达到最优，仿真实验显示其表现优于现有基线。

Conclusion: DP-NCB为高社会影响应用提供了隐私保护且公平的算法基础。

Abstract: Multi-armed bandit algorithms are fundamental tools for sequential
decision-making under uncertainty, with widespread applications across domains
such as clinical trials and personalized decision-making. As bandit algorithms
are increasingly deployed in these socially sensitive settings, it becomes
critical to protect user data privacy and ensure fair treatment across decision
rounds. While prior work has independently addressed privacy and fairness in
bandit settings, the question of whether both objectives can be achieved
simultaneously has remained largely open. Existing privacy-preserving bandit
algorithms typically optimize average regret, a utilitarian measure, whereas
fairness-aware approaches focus on minimizing Nash regret, which penalizes
inequitable reward distributions, but often disregard privacy concerns.
  To bridge this gap, we introduce Differentially Private Nash Confidence Bound
(DP-NCB)-a novel and unified algorithmic framework that simultaneously ensures
$\epsilon$-differential privacy and achieves order-optimal Nash regret,
matching known lower bounds up to logarithmic factors. The framework is
sufficiently general to operate under both global and local differential
privacy models, and is anytime, requiring no prior knowledge of the time
horizon. We support our theoretical guarantees with simulations on synthetic
bandit instances, showing that DP-NCB incurs substantially lower Nash regret
than state-of-the-art baselines. Our results offer a principled foundation for
designing bandit algorithms that are both privacy-preserving and fair, making
them suitable for high-stakes, socially impactful applications.

</details>


### [29] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型（LLM）的自动化先验分布生成框架LLMPrior，通过结合LLM与显式生成模型（如高斯混合模型）来生成有效且可处理的概率分布，并扩展到多智能体系统中的分布式先验聚合。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断中先验分布的设定是一个手动、主观且难以扩展的瓶颈问题，需要一种自动化且可扩展的解决方案。

Method: LLMPrior框架通过耦合LLM与显式生成模型（如高斯混合模型）生成先验分布，并提出了多智能体系统中的分布式先验聚合算法Fed-LLMPrior。

Result: 提出的框架能够自动化生成有效的先验分布，并通过Fed-LLMPrior实现分布式先验的鲁棒聚合。

Conclusion: 该工作为降低复杂贝叶斯建模的门槛提供了新工具，具有潜在的应用价值。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [30] [Fast and Accurate Explanations of Distance-Based Classifiers by Uncovering Latent Explanatory Structures](https://arxiv.org/abs/2508.03913)
*Florian Bley,Jacob Kauffmann,Simon León Krug,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 论文揭示了距离分类器中隐藏的神经网络结构，使可解释AI技术（如LRP）得以应用，并通过实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 距离分类器（如k近邻和支持向量机）在机器学习中广泛应用，但其预测可解释性不足，需要改进。

Method: 发现距离分类器中隐藏的神经网络结构（线性检测单元与非线性池化层结合），并应用可解释AI技术（如LRP）。

Result: 通过定量评估，新解释方法优于多个基线，并在两个实际用例中展示了其有用性。

Conclusion: 揭示的隐藏结构为距离分类器提供了新的解释途径，增强了其可解释性和实用性。

Abstract: Distance-based classifiers, such as k-nearest neighbors and support vector
machines, continue to be a workhorse of machine learning, widely used in
science and industry. In practice, to derive insights from these models, it is
also important to ensure that their predictions are explainable. While the
field of Explainable AI has supplied methods that are in principle applicable
to any model, it has also emphasized the usefulness of latent structures (e.g.
the sequence of layers in a neural network) to produce explanations. In this
paper, we contribute by uncovering a hidden neural network structure in
distance-based classifiers (consisting of linear detection units combined with
nonlinear pooling layers) upon which Explainable AI techniques such as
layer-wise relevance propagation (LRP) become applicable. Through quantitative
evaluations, we demonstrate the advantage of our novel explanation approach
over several baselines. We also show the overall usefulness of explaining
distance-based models through two practical use cases.

</details>


### [31] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 论文提出了一种在线分布鲁棒强化学习方法，解决了传统方法在未知环境中依赖生成模型或离线数据集的限制，通过优化最坏情况性能并提供高效算法。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在现实部署中因模拟与真实环境差异导致的性能下降问题，特别是在未知环境中缺乏先验知识的情况。

Method: 提出了一种基于$f$-散度的在线分布鲁棒强化学习算法，适用于Chi-Square和KL散度等不确定性集合，具有计算高效性和次线性遗憾保证。

Result: 理论分析表明算法具有近最优性，实验验证了其在多样环境中的鲁棒性和高效性。

Conclusion: 该方法为在线分布鲁棒强化学习提供了实用且高效的解决方案，适用于未知环境中的实际部署。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [32] [FairPOT: Balancing AUC Performance and Fairness with Proportional Optimal Transport](https://arxiv.org/abs/2508.03940)
*Pengxi Liu,Yi Shen,Matthew M. Engelhard,Benjamin A. Goldstein,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: FairPOT是一种新颖的模型无关后处理框架，通过最优传输选择性调整风险分数分布，实现公平性与AUC性能的可调权衡。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如医疗、金融、刑事司法）中，公平性评估常基于风险分数而非二元结果，但严格公平性要求会显著降低AUC性能。

Method: 提出FairPOT框架，利用最优传输选择性地调整劣势群体的风险分数分布（如top-lambda分位数），并扩展到部分AUC场景。

Result: 实验表明，FairPOT在全局和部分AUC场景中优于现有后处理技术，能在轻微AUC损失或性能提升下改善公平性。

Conclusion: FairPOT的计算效率和实用性使其成为实际部署的有前景解决方案。

Abstract: Fairness metrics utilizing the area under the receiver operator
characteristic curve (AUC) have gained increasing attention in high-stakes
domains such as healthcare, finance, and criminal justice. In these domains,
fairness is often evaluated over risk scores rather than binary outcomes, and a
common challenge is that enforcing strict fairness can significantly degrade
AUC performance. To address this challenge, we propose Fair Proportional
Optimal Transport (FairPOT), a novel, model-agnostic post-processing framework
that strategically aligns risk score distributions across different groups
using optimal transport, but does so selectively by transforming a controllable
proportion, i.e., the top-lambda quantile, of scores within the disadvantaged
group. By varying lambda, our method allows for a tunable trade-off between
reducing AUC disparities and maintaining overall AUC performance. Furthermore,
we extend FairPOT to the partial AUC setting, enabling fairness interventions
to concentrate on the highest-risk regions. Extensive experiments on synthetic,
public, and clinical datasets show that FairPOT consistently outperforms
existing post-processing techniques in both global and partial AUC scenarios,
often achieving improved fairness with slight AUC degradation or even positive
gains in utility. The computational efficiency and practical adaptability of
FairPOT make it a promising solution for real-world deployment.

</details>


### [33] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 本文分析了GRPO的两大局限性，并提出GTPO作为改进方案，通过跳过冲突令牌的负更新和过滤高熵补全，提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在语言模型训练中存在梯度冲突和输出分布扁平化的问题，影响了模型性能。

Method: 提出GTPO，通过识别冲突令牌、跳过负更新、放大正更新，并过滤高熵补全，避免KL散度正则化。

Result: 在GSM8K、MATH和AIME 2024基准测试中验证了GTPO的稳定性和性能提升。

Conclusion: GTPO解决了GRPO的局限性，提供了一种更稳定有效的策略优化方法。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [34] [Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation](https://arxiv.org/abs/2508.04444)
*Askar Tsyganov,Evgeny Frolov,Sergey Samsonov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了基于矩阵向量乘法的随机算法，用于估计矩阵的两种范数，并应用于深度神经网络和推荐系统。


<details>
  <summary>Details</summary>
Motivation: 解决在矩阵自由设置中高效估计矩阵范数的问题，并探索其在深度学习和推荐系统中的实际应用。

Method: 通过改进Hutchinson对角估计器及其Hutch++版本，设计随机算法，仅需矩阵向量乘法。

Result: 提供了算法的计算复杂度界限，并在图像分类和对抗攻击缓解中验证了实用性。

Conclusion: 新算法在矩阵范数估计和实际应用中表现出高效性和实用性。

Abstract: In this paper, we propose new randomized algorithms for estimating the
two-to-infinity and one-to-two norms in a matrix-free setting, using only
matrix-vector multiplications. Our methods are based on appropriate
modifications of Hutchinson's diagonal estimator and its Hutch++ version. We
provide oracle complexity bounds for both modifications. We further illustrate
the practical utility of our algorithms for Jacobian-based regularization in
deep neural network training on image classification tasks. We also demonstrate
that our methodology can be applied to mitigate the effect of adversarial
attacks in the domain of recommender systems.

</details>


### [35] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种U形物理信息网络（U-PINet），结合深度学习和物理约束，用于高效且物理一致的电磁散射建模。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，纯数据驱动方法缺乏物理约束且需要大量标注数据，限制了其应用和泛化能力。

Method: U-PINet采用多尺度处理神经网络架构和物理启发的稀疏图表示，建模近场和远场相互作用的分解与耦合。

Result: U-PINet准确预测表面电流分布，与传统求解器结果一致，计算时间显著减少，并在精度和鲁棒性上优于传统深度学习方法。

Conclusion: U-PINet为电磁散射建模提供了一种高效、泛化能力强且物理一致的新方法，适用于下游应用。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [36] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息的神经网络（PINN）来替代传统的有限元方法（FEM），用于核聚变装置EAST中的热通量估计，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统FEM方法依赖于网格采样，计算效率低且难以实时模拟，而PINN通过结合物理方程和数据驱动方法，解决了这一问题。

Method: 利用神经网络输入空间坐标和时间戳，计算边界损失、初始条件损失和物理损失，并结合少量数据点采样优化模型。

Result: 实验表明，PINN在精度上与FEM相当，但计算效率提高了40倍。

Conclusion: PINN为热传导问题提供了一种高效且准确的解决方案，适用于实际实验中的实时模拟。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


### [37] [SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons](https://arxiv.org/abs/2508.03785)
*Teodor Chiaburu,Vipin Singh,Frank Haußer,Felix Bießmann*

Main category: cs.LG

TL;DR: 论文提出了一种名为SoilNet的多模态多任务模型，用于解决土壤层次分类问题，结合图像数据和地理时间元数据，通过模块化流程实现分层分类。


<details>
  <summary>Details</summary>
Motivation: 土壤层次分类对监测土壤健康至关重要，但现有基础模型难以应对其多模态、多任务和复杂层次标签的挑战。

Method: SoilNet通过模块化流程整合图像和元数据，预测深度标记、分割土壤剖面，提取形态特征，并利用图表示预测层次标签。

Result: 在真实土壤剖面数据集上验证了方法的有效性。

Conclusion: SoilNet为解决复杂层次分类问题提供了一种有效方法，代码和实验已开源。

Abstract: While recent advances in foundation models have improved the state of the art
in many domains, some problems in empirical sciences could not benefit from
this progress yet. Soil horizon classification, for instance, remains
challenging because of its multimodal and multitask characteristics and a
complex hierarchically structured label taxonomy. Accurate classification of
soil horizons is crucial for monitoring soil health, which directly impacts
agricultural productivity, food security, ecosystem stability and climate
resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal
multitask model to tackle this problem through a structured modularized
pipeline. Our approach integrates image data and geotemporal metadata to first
predict depth markers, segmenting the soil profile into horizon candidates.
Each segment is characterized by a set of horizon-specific morphological
features. Finally, horizon labels are predicted based on the multimodal
concatenated feature vector, leveraging a graph-based label representation to
account for the complex hierarchical relationships among soil horizons. Our
method is designed to address complex hierarchical classification, where the
number of possible labels is very large, imbalanced and non-trivially
structured. We demonstrate the effectiveness of our approach on a real-world
soil profile dataset. All code and experiments can be found in our repository:
https://github.com/calgo-lab/BGR/

</details>


### [38] [Bernoulli-LoRA: A Theoretical Framework for Randomized Low-Rank Adaptation](https://arxiv.org/abs/2508.03820)
*Igor Sokolov,Abdurakhmon Sadiev,Yury Demidovich,Fawaz S Al-Qahtani,Peter Richtárik*

Main category: cs.LG

TL;DR: 论文提出了Bernoulli-LoRA，一种新的理论框架，统一并扩展了现有的LoRA方法，通过概率Bernoulli机制选择更新的矩阵，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模指数增长，参数高效微调（PEFT）变得至关重要，但现有LoRA方法的理论理解有限，需要更严谨的分析框架。

Method: 引入Bernoulli-LoRA框架，通过概率Bernoulli机制选择更新的矩阵，分析多种变体的收敛性，并扩展到凸非光滑函数。

Result: 在理论和实验中验证了Bernoulli-LoRA的有效性，为PEFT方法提供了理论支持。

Conclusion: Bernoulli-LoRA是迈向理论严谨且实际有效的PEFT方法的重要一步。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
adapting large foundational models to specific tasks, particularly as model
sizes continue to grow exponentially. Among PEFT methods, Low-Rank Adaptation
(LoRA) (arXiv:2106.09685) stands out for its effectiveness and simplicity,
expressing adaptations as a product of two low-rank matrices. While extensive
empirical studies demonstrate LoRA's practical utility, theoretical
understanding of such methods remains limited. Recent work on RAC-LoRA
(arXiv:2410.08305) took initial steps toward rigorous analysis. In this work,
we introduce Bernoulli-LoRA, a novel theoretical framework that unifies and
extends existing LoRA approaches. Our method introduces a probabilistic
Bernoulli mechanism for selecting which matrix to update. This approach
encompasses and generalizes various existing update strategies while
maintaining theoretical tractability. Under standard assumptions from
non-convex optimization literature, we analyze several variants of our
framework: Bernoulli-LoRA-GD, Bernoulli-LoRA-SGD, Bernoulli-LoRA-PAGE,
Bernoulli-LoRA-MVR, Bernoulli-LoRA-QGD, Bernoulli-LoRA-MARINA, and
Bernoulli-LoRA-EF21, establishing convergence guarantees for each variant.
Additionally, we extend our analysis to convex non-smooth functions, providing
convergence rates for both constant and adaptive (Polyak-type) stepsizes.
Through extensive experiments on various tasks, we validate our theoretical
findings and demonstrate the practical efficacy of our approach. This work is a
step toward developing theoretically grounded yet practically effective PEFT
methods.

</details>


### [39] [VAE-DNN: Energy-Efficient Trainable-by-Parts Surrogate Model For Parametric Partial Differential Equations](https://arxiv.org/abs/2508.03839)
*Yifei Zong,Alexandre M. Tartakovsky*

Main category: cs.LG

TL;DR: 提出一种可分部训练的代用模型VAE-DNN，用于求解参数化非线性偏微分方程的正反问题，相比FNO和DeepONet更高效且准确。


<details>
  <summary>Details</summary>
Motivation: 现有代用模型和算子学习模型（如FNO和DeepONet）训练时间和能耗较高，需改进。

Method: 通过编码器降维输入，全连接神经网络映射潜在空间，解码器重建解，三部分可独立训练。

Result: VAE-DNN在非线性扩散方程的正反问题中，比FNO和DeepONet更高效且准确。

Conclusion: VAE-DNN是一种高效且准确的代用模型，适用于复杂非线性偏微分方程求解。

Abstract: We propose a trainable-by-parts surrogate model for solving forward and
inverse parameterized nonlinear partial differential equations. Like several
other surrogate and operator learning models, the proposed approach employs an
encoder to reduce the high-dimensional input $y(\bm{x})$ to a lower-dimensional
latent space, $\bm\mu_{\bm\phi_y}$. Then, a fully connected neural network is
used to map $\bm\mu_{\bm\phi_y}$ to the latent space, $\bm\mu_{\bm\phi_h}$, of
the PDE solution $h(\bm{x},t)$. Finally, a decoder is utilized to reconstruct
$h(\bm{x},t)$. The innovative aspect of our model is its ability to train its
three components independently. This approach leads to a substantial decrease
in both the time and energy required for training when compared to leading
operator learning models such as FNO and DeepONet. The separable training is
achieved by training the encoder as part of the variational autoencoder (VAE)
for $y(\bm{x})$ and the decoder as part of the $h(\bm{x},t)$ VAE. We refer to
this model as the VAE-DNN model. VAE-DNN is compared to the FNO and DeepONet
models for obtaining forward and inverse solutions to the nonlinear diffusion
equation governing groundwater flow in an unconfined aquifer. Our findings
indicate that VAE-DNN not only demonstrates greater efficiency but also
delivers superior accuracy in both forward and inverse solutions compared to
the FNO and DeepONet models.

</details>


### [40] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 本文提出了一种基于众包用户侧KPI和监管数据的时空预测框架，用于高精度预测频谱需求，优于传统ITU模型。


<details>
  <summary>Details</summary>
Motivation: 频谱需求预测对频谱分配、监管规划和无线通信网络可持续发展至关重要，支持ITU等机构制定公平政策和满足新兴技术需求。

Method: 采用先进的时空预测框架，结合特征工程、相关性分析和迁移学习技术，利用众包KPI和监管数据建模。

Result: 实验证明该方法预测精度高，具有跨区域泛化能力，优于传统ITU模型。

Conclusion: 该方法为政策制定者和监管机构提供了更现实、可操作的频谱管理工具。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [41] [Prediction-Oriented Subsampling from Data Streams](https://arxiv.org/abs/2508.03868)
*Benedetta Lavinia Mussati,Freddie Bickford Smith,Tom Rainforth,Stephen Roberts*

Main category: cs.LG

TL;DR: 论文提出了一种基于信息论的智能数据子采样方法，用于离线学习，以减少下游预测的不确定性，并在实验中优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 数据流中持续生成新观测，如何在计算成本可控的情况下捕捉相关信息是学习模型的关键挑战。

Method: 采用信息论方法，通过减少下游预测的不确定性进行智能数据子采样。

Result: 实验表明，该方法在两个广泛研究的问题上优于之前提出的信息论技术。

Conclusion: 实际应用中，可靠的高性能需要精心设计的模型。

Abstract: Data is often generated in streams, with new observations arriving over time.
A key challenge for learning models from data streams is capturing relevant
information while keeping computational costs manageable. We explore
intelligent data subsampling for offline learning, and argue for an
information-theoretic method centred on reducing uncertainty in downstream
predictions of interest. Empirically, we demonstrate that this
prediction-oriented approach performs better than a previously proposed
information-theoretic technique on two widely studied problems. At the same
time, we highlight that reliably achieving strong performance in practice
requires careful model design.

</details>


### [42] [Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training](https://arxiv.org/abs/2508.03872)
*Wesley Brewer,Murali Meena Gopalakrishnan,Matthias Maiterth,Aditya Kashi,Jong Youl Choi,Pei Zhang,Stephen Nichols,Riccardo Balin,Miles Couchman,Stephen de Bruyn Kops,P. K. Yeung,Daniel Dotson,Rohini Uma-Vaideswaran,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: SICKLE框架通过智能子采样减少数据量，提升模型精度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律和Dennard缩放的终结，高效训练需要重新思考数据量。

Method: 开发SICKLE框架，采用最大熵采样方法，并与随机和相空间采样对比。

Result: 在Frontier上大规模评估，子采样可提升精度，能耗降低高达38倍。

Conclusion: 智能子采样是一种高效训练的有效方法。

Abstract: With the end of Moore's law and Dennard scaling, efficient training
increasingly requires rethinking data volume. Can we train better models with
significantly less data via intelligent subsampling? To explore this, we
develop SICKLE, a sparse intelligent curation framework for efficient learning,
featuring a novel maximum entropy (MaxEnt) sampling approach, scalable
training, and energy benchmarking. We compare MaxEnt with random and
phase-space sampling on large direct numerical simulation (DNS) datasets of
turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as
a preprocessing step can improve model accuracy and substantially lower energy
consumption, with reductions of up to 38x observed in certain cases.

</details>


### [43] [Reinforcement Learning for Target Zone Blood Glucose Control](https://arxiv.org/abs/2508.03875)
*David H. Mguni,Jing Dong,Wanrong Yang,Ziquan Liu,Muhammad Salman Haleem,Baoxiang Wang*

Main category: cs.LG

TL;DR: 提出了一种结合脉冲控制和切换控制的强化学习框架，用于个性化T1DM治疗决策，显著降低了血糖水平违规率。


<details>
  <summary>Details</summary>
Motivation: 解决T1DM治疗中干预效果的延迟和异质性问题，提升个性化治疗的安全性和时效性。

Method: 结合脉冲控制（快速干预）和切换控制（长期治疗）的强化学习框架，通过约束马尔可夫决策过程学习安全策略。

Result: 在T1DM控制任务中，血糖水平违规率从22.4%降至10.8%。

Conclusion: 为未来医疗领域的安全和时效性强化学习奠定了基础，但尚未用于临床部署。

Abstract: Managing physiological variables within clinically safe target zones is a
central challenge in healthcare, particularly for chronic conditions such as
Type 1 Diabetes Mellitus (T1DM). Reinforcement learning (RL) offers promise for
personalising treatment, but struggles with the delayed and heterogeneous
effects of interventions. We propose a novel RL framework to study and support
decision-making in T1DM technologies, such as automated insulin delivery. Our
approach captures the complex temporal dynamics of treatment by unifying two
control modalities: \textit{impulse control} for discrete, fast-acting
interventions (e.g., insulin boluses), and \textit{switching control} for
longer-acting treatments and regime shifts. The core of our method is a
constrained Markov decision process augmented with physiological state
features, enabling safe policy learning under clinical and resource
constraints. The framework incorporates biologically realistic factors,
including insulin decay, leading to policies that better reflect real-world
therapeutic behaviour. While not intended for clinical deployment, this work
establishes a foundation for future safe and temporally-aware RL in healthcare.
We provide theoretical guarantees of convergence and demonstrate empirical
improvements in a stylised T1DM control task, reducing blood glucose level
violations from 22.4\% (state-of-the-art) to as low as 10.8\%.

</details>


### [44] [Calibrating Biophysical Models for Grape Phenology Prediction via Multi-Task Learning](https://arxiv.org/abs/2508.03898)
*William Solow,Sandhya Saisubramanian*

Main category: cs.LG

TL;DR: 提出了一种结合多任务学习和循环神经网络的混合建模方法，用于提高葡萄物候预测的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 葡萄物候的准确预测对葡萄园管理至关重要，但传统生物物理模型精度不足，而深度学习方法受限于稀疏数据集。

Method: 采用多任务学习和循环神经网络结合的方法，参数化可微分生物物理模型，实现跨品种的共享学习。

Result: 实验表明，该方法在预测物候阶段及其他作物状态变量（如抗寒性和小麦产量）上显著优于传统模型和基线深度学习方法。

Conclusion: 混合建模方法在葡萄物候预测中表现出更高的准确性和鲁棒性，为精细化管理提供了有效工具。

Abstract: Accurate prediction of grape phenology is essential for timely vineyard
management decisions, such as scheduling irrigation and fertilization, to
maximize crop yield and quality. While traditional biophysical models
calibrated on historical field data can be used for season-long predictions,
they lack the precision required for fine-grained vineyard management. Deep
learning methods are a compelling alternative but their performance is hindered
by sparse phenology datasets, particularly at the cultivar level. We propose a
hybrid modeling approach that combines multi-task learning with a recurrent
neural network to parameterize a differentiable biophysical model. By using
multi-task learning to predict the parameters of the biophysical model, our
approach enables shared learning across cultivars while preserving biological
structure, thereby improving the robustness and accuracy of predictions.
Empirical evaluation using real-world and synthetic datasets demonstrates that
our method significantly outperforms both conventional biophysical models and
baseline deep learning approaches in predicting phenological stages, as well as
other crop state variables such as cold-hardiness and wheat yield.

</details>


### [45] [Active Learning and Transfer Learning for Anomaly Detection in Time-Series Data](https://arxiv.org/abs/2508.03921)
*John D. Kelleher,Matthew Nicholson,Rahul Agrahari,Clare Conran*

Main category: cs.LG

TL;DR: 本文研究了主动学习与迁移学习结合在跨域时间序列数据异常检测中的效果，发现聚类与主动学习存在交互作用，最佳性能通常出现在不应用聚类时。主动学习能提升模型性能，但提升速度较慢。实验设计改进可能是原因之一。迁移学习与主动学习的性能提升存在上限，随着目标点增加，性能最终趋于平缓。


<details>
  <summary>Details</summary>
Motivation: 探讨主动学习与迁移学习结合在跨域时间序列数据异常检测中的有效性，以及聚类与主动学习的交互作用。

Method: 通过实验设计，结合主动学习和迁移学习，分析聚类的影响，并评估性能提升的动态。

Result: 最佳性能出现在不应用聚类时；主动学习提升模型性能但速度较慢；迁移学习与主动学习的性能提升存在上限。

Conclusion: 主动学习有效，但性能提升随标注点数量呈线性平缓趋势，表明数据点选择顺序合理。

Abstract: This paper examines the effectiveness of combining active learning and
transfer learning for anomaly detection in cross-domain time-series data. Our
results indicate that there is an interaction between clustering and active
learning and in general the best performance is achieved using a single cluster
(in other words when clustering is not applied). Also, we find that adding new
samples to the training set using active learning does improve model
performance but that in general, the rate of improvement is slower than the
results reported in the literature suggest. We attribute this difference to an
improved experimental design where distinct data samples are used for the
sampling and testing pools. Finally, we assess the ceiling performance of
transfer learning in combination with active learning across several datasets
and find that performance does initially improve but eventually begins to tail
off as more target points are selected for inclusion in training. This tail-off
in performance may indicate that the active learning process is doing a good
job of sequencing data points for selection, pushing the less useful points
towards the end of the selection process and that this tail-off occurs when
these less useful points are eventually added. Taken together our results
indicate that active learning is effective but that the improvement in model
performance follows a linear flat function concerning the number of points
selected and labelled.

</details>


### [46] [Next Generation Equation-Free Multiscale Modelling of Crowd Dynamics via Machine Learning](https://arxiv.org/abs/2508.03926)
*Hector Vargas Alvarez,Dimitrios G. Patsatzis,Lucia Russo,Ioannis Kevrekidis,Constantinos Siettos*

Main category: cs.LG

TL;DR: 论文提出了一种结合流形学习和机器学习的方法，从高保真基于代理的模拟中学习潜在空间中的人群动态离散演化算子，用于桥接微观和宏观建模尺度。


<details>
  <summary>Details</summary>
Motivation: 解决人群动态中微观和宏观建模尺度之间的桥接问题，以实现系统数值分析、优化和控制。

Method: 四阶段方法：1) 使用KDE从离散微观数据推导连续宏观场；2) 基于流形学习构建潜在空间映射；3) 使用LSTM和MVAR学习潜在空间中的降阶代理模型；4) 重建高维空间中的宏观密度分布。

Result: 数值结果表明该方法具有高精度、鲁棒性和泛化能力，能够快速准确模拟人群动态。

Conclusion: 提出的框架有效解决了宏观PDE不可用的问题，为人群动态建模提供了一种高效解决方案。

Abstract: Bridging the microscopic and the macroscopic modelling scales in crowd
dynamics constitutes an important, open challenge for systematic numerical
analysis, optimization, and control. We propose a combined manifold and machine
learning approach to learn the discrete evolution operator for the emergent
crowd dynamics in latent spaces from high-fidelity agent-based simulations. The
proposed framework builds upon our previous works on next-generation
Equation-free algorithms on learning surrogate models for high-dimensional and
multiscale systems. Our approach is a four-stage one, explicitly conserving the
mass of the reconstructed dynamics in the high-dimensional space. In the first
step, we derive continuous macroscopic fields (densities) from discrete
microscopic data (pedestrians' positions) using KDE. In the second step, based
on manifold learning, we construct a map from the macroscopic ambient space
into the latent space parametrized by a few coordinates based on POD of the
corresponding density distribution. The third step involves learning
reduced-order surrogate ROMs in the latent space using machine learning
techniques, particularly LSTMs networks and MVARs. Finally, we reconstruct the
crowd dynamics in the high-dimensional space in terms of macroscopic density
profiles. We demonstrate that the POD reconstruction of the density
distribution via SVD conserves the mass. With this "embed->learn in latent
space->lift back to the ambient space" pipeline, we create an effective
solution operator of the unavailable macroscopic PDE for the density evolution.
For our illustrations, we use the Social Force Model to generate data in a
corridor with an obstacle, imposing periodic boundary conditions. The numerical
results demonstrate high accuracy, robustness, and generalizability, thus
allowing for fast and accurate modelling/simulation of crowd dynamics from
agent-based simulations.

</details>


### [47] [Markov Chain Estimation with In-Context Learning](https://arxiv.org/abs/2508.03934)
*Simon Lepage,Jeremie Mary,David Picard*

Main category: cs.LG

TL;DR: 研究Transformer通过预测下一个标记学习算法的能力，发现模型在达到一定规模和训练集大小时能学习转移概率而非记忆模式。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在仅通过下一个标记预测训练时，能否学习上下文中的算法。

Method: 使用随机转移矩阵的马尔可夫链训练Transformer预测下一个标记，测试时使用不同矩阵。

Result: 模型在达到一定规模和训练集大小时能学习转移概率，且更复杂的状态编码能提升泛化能力。

Conclusion: Transformer能通过预测任务学习算法，且编码方式影响泛化性能。

Abstract: We investigate the capacity of transformers to learn algorithms involving
their context while solely being trained using next token prediction. We set up
Markov chains with random transition matrices and we train transformers to
predict the next token. Matrices used during training and test are different
and we show that there is a threshold in transformer size and in training set
size above which the model is able to learn to estimate the transition
probabilities from its context instead of memorizing the training patterns.
Additionally, we show that more involved encoding of the states enables more
robust prediction for Markov chains with structures different than those seen
during training.

</details>


### [48] [BubbleONet: A Physics-Informed Neural Operator for High-Frequency Bubble Dynamics](https://arxiv.org/abs/2508.03965)
*Yunhao Zhang,Lin Cheng,Aswin Gnanaskandan,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: BubbleONet是一种基于物理信息的深度算子网络模型，用于将压力剖面映射到气泡半径响应，具有高效计算优势。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，需要一种高效且物理准确的替代方法。

Method: 结合PI-DeepONet框架和Rowdy自适应激活函数，解决深度学习中的频谱偏差问题。

Result: 在多种气泡动力学场景中表现优异，验证了其作为替代模型的潜力。

Conclusion: BubbleONet是一种高效的气泡动力学模拟替代方案。

Abstract: This paper introduces BubbleONet, an operator learning model designed to map
pressure profiles from an input function space to corresponding bubble radius
responses. BubbleONet is built upon the physics-informed deep operator network
(PI-DeepONet) framework, leveraging DeepONet's powerful universal approximation
capabilities for operator learning alongside the robust physical fidelity
provided by the physics-informed neural networks. To mitigate the inherent
spectral bias in deep learning, BubbleONet integrates the Rowdy adaptive
activation function, enabling improved representation of high-frequency
features. The model is evaluated across various scenarios, including: (1)
Rayleigh-Plesset equation based bubble dynamics with a single initial radius,
(2) Keller-Miksis equation based bubble dynamics with a single initial radius,
and (3) Keller-Miksis equation based bubble dynamics with multiple initial
radii. Moreover, the performance of single-step versus two-step training
techniques for BubbleONet is investigated. The results demonstrate that
BubbleONet serves as a promising surrogate model for simulating bubble
dynamics, offering a computationally efficient alternative to traditional
numerical solvers.

</details>


### [49] [Dynamic User-controllable Privacy-preserving Few-shot Sensing Framework](https://arxiv.org/abs/2508.03989)
*Ajesh Koyatan Chathoth,Shuhao Yu,Stephen Lee*

Main category: cs.LG

TL;DR: PrivCLIP是一个动态、用户可控的隐私保护框架，通过多模态对比学习实现敏感活动的少样本检测，并生成隐私合规数据。


<details>
  <summary>Details</summary>
Motivation: 现代传感系统中，用户隐私偏好因人而异且随时间变化，现有方法缺乏灵活性和用户控制。

Method: PrivCLIP结合多模态对比学习，将IMU数据与自然语言描述对齐，并通过语言引导的净化器和IMU-GPT模块生成隐私合规数据。

Result: 在多个活动识别数据集上，PrivCLIP在隐私保护和数据效用方面显著优于基线方法。

Conclusion: PrivCLIP提供了一种灵活、用户可控的隐私保护解决方案，适用于动态隐私需求场景。

Abstract: User-controllable privacy is important in modern sensing systems, as privacy
preferences can vary significantly from person to person and may evolve over
time. This is especially relevant in devices equipped with Inertial Measurement
Unit (IMU) sensors, such as smartphones and wearables, which continuously
collect rich time-series data that can inadvertently expose sensitive user
behaviors. While prior work has proposed privacy-preserving methods for sensor
data, most rely on static, predefined privacy labels or require large
quantities of private training data, limiting their adaptability and user
agency. In this work, we introduce PrivCLIP, a dynamic, user-controllable,
few-shot privacy-preserving sensing framework. PrivCLIP allows users to specify
and modify their privacy preferences by categorizing activities as sensitive
(black-listed), non-sensitive (white-listed), or neutral (gray-listed).
Leveraging a multimodal contrastive learning approach, PrivCLIP aligns IMU
sensor data with natural language activity descriptions in a shared embedding
space, enabling few-shot detection of sensitive activities. When a
privacy-sensitive activity is identified, the system uses a language-guided
activity sanitizer and a motion generation module (IMU-GPT) to transform the
original data into a privacy-compliant version that semantically resembles a
non-sensitive activity. We evaluate PrivCLIP on multiple human activity
recognition datasets and demonstrate that it significantly outperforms baseline
methods in terms of both privacy protection and data utility.

</details>


### [50] [Tensorized Clustered LoRA Merging for Multi-Task Interference](https://arxiv.org/abs/2508.03999)
*Zhan Su,Fengran Mo,Guojun Liang,Jinghan Zhang,Bingbing Wen,Prayag Tiwari,Jian-Yun Nie*

Main category: cs.LG

TL;DR: TC-LoRA提出了一种解决多任务LoRA适配器合并时任务干扰的方法，通过文本级和参数级优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 多任务设置中，合并来自不同源的LoRA适配器会导致任务干扰，影响下游任务性能。

Method: 在文本级，通过聚类训练样本训练专用LoRA适配器；在参数级，使用CP分解分离任务特定和共享因素。

Result: 在Phi-3和Mistral-7B上分别提升1.4%和2.3%的准确率。

Conclusion: TC-LoRA有效减少了任务干扰，提升了LLM在多任务场景下的适应能力。

Abstract: Despite the success of the monolithic dense paradigm of large language models
(LLMs), the LoRA adapters offer an efficient solution by fine-tuning small
task-specific modules and merging them with the base model. However, in
multi-task settings, merging LoRA adapters trained on heterogeneous sources
frequently causes \textit{task interference}, degrading downstream performance.
To address this, we propose a tensorized clustered LoRA (TC-LoRA) library
targeting to address the task interference at the \textit{text-level} and
\textit{parameter-level}. At the \textit{text-level}, we cluster the training
samples in the embedding space to capture input-format similarities, then train
a specialized LoRA adapter for each cluster. At the \textit{parameter-level},
we introduce a joint Canonical Polyadic (CP) decomposition that disentangles
task-specific and shared factors across LoRA adapters. This joint factorization
preserves essential knowledge while reducing cross-task interference. Extensive
experiments on out-of-domain zero-shot and skill-composition tasks-including
reasoning, question answering, and coding. Compared to strong SVD-based
baselines, TC-LoRA achieves +1.4\% accuracy on Phi-3 and +2.3\% on Mistral-7B
(+2.3\%), demonstrating the effectiveness of TC-LoRA in LLM adaptation.

</details>


### [51] [Decoupled Contrastive Learning for Federated Learning](https://arxiv.org/abs/2508.04005)
*Hyungbin Kim,Incheol Baek,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 论文提出了一种名为DCFL的新框架，通过解耦对比学习损失来解决联邦学习中数据异质性导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因数据异质性导致性能下降，而现有对比学习方法在有限样本下不适用。

Method: 提出DCFL框架，将对比学习损失解耦为对齐和均匀性两部分，独立校准吸引和排斥力。

Result: DCFL在正样本对齐和负样本均匀性上优于现有方法，并在多个基准测试中表现优异。

Conclusion: DCFL为联邦学习提供了一种有效的对比学习方法，解决了有限样本下的性能问题。

Abstract: Federated learning is a distributed machine learning paradigm that allows
multiple participants to train a shared model by exchanging model updates
instead of their raw data. However, its performance is degraded compared to
centralized approaches due to data heterogeneity across clients. While
contrastive learning has emerged as a promising approach to mitigate this, our
theoretical analysis reveals a fundamental conflict: its asymptotic assumptions
of an infinite number of negative samples are violated in finite-sample regime
of federated learning. To address this issue, we introduce Decoupled
Contrastive Learning for Federated Learning (DCFL), a novel framework that
decouples the existing contrastive loss into two objectives. Decoupling the
loss into its alignment and uniformity components enables the independent
calibration of the attraction and repulsion forces without relying on the
asymptotic assumptions. This strategy provides a contrastive learning method
suitable for federated learning environments where each client has a small
amount of data. Our experimental results show that DCFL achieves stronger
alignment between positive samples and greater uniformity between negative
samples compared to existing contrastive learning methods. Furthermore,
experimental results on standard benchmarks, including CIFAR-10, CIFAR-100, and
Tiny-ImageNet, demonstrate that DCFL consistently outperforms state-of-the-art
federated learning methods.

</details>


### [52] [A Comparative Survey of PyTorch vs TensorFlow for Deep Learning: Usability, Performance, and Deployment Trade-offs](https://arxiv.org/abs/2508.04035)
*Zakariya Ba Alawi*

Main category: cs.LG

TL;DR: 本文对TensorFlow和PyTorch两大深度学习框架进行了全面比较，重点分析了它们的易用性、性能和部署权衡。


<details>
  <summary>Details</summary>
Motivation: 通过对比TensorFlow和PyTorch的编程范式、性能表现及部署灵活性，帮助开发者根据需求选择合适的框架。

Method: 通过分析编程范式、性能基准测试、部署工具和生态系统支持，对比两者的优缺点。

Result: PyTorch在研究领域更受欢迎，因其灵活性和易用性；TensorFlow在企业和生产环境中更具优势。

Conclusion: 选择框架需权衡研究需求和生产需求，PyTorch适合研究，TensorFlow适合生产。

Abstract: This paper presents a comprehensive comparative survey of TensorFlow and
PyTorch, the two leading deep learning frameworks, focusing on their usability,
performance, and deployment trade-offs. We review each framework's programming
paradigm and developer experience, contrasting TensorFlow's graph-based (now
optionally eager) approach with PyTorch's dynamic, Pythonic style. We then
compare model training speeds and inference performance across multiple tasks
and data regimes, drawing on recent benchmarks and studies. Deployment
flexibility is examined in depth - from TensorFlow's mature ecosystem
(TensorFlow Lite for mobile/embedded, TensorFlow Serving, and JavaScript
support) to PyTorch's newer production tools (TorchScript compilation, ONNX
export, and TorchServe). We also survey ecosystem and community support,
including library integrations, industry adoption, and research trends (e.g.,
PyTorch's dominance in recent research publications versus TensorFlow's broader
tooling in enterprise). Applications in computer vision, natural language
processing, and other domains are discussed to illustrate how each framework is
used in practice. Finally, we outline future directions and open challenges in
deep learning framework design, such as unifying eager and graph execution,
improving cross-framework interoperability, and integrating compiler
optimizations (XLA, JIT) for improved speed. Our findings indicate that while
both frameworks are highly capable for state-of-the-art deep learning, they
exhibit distinct trade-offs: PyTorch offers simplicity and flexibility favored
in research, whereas TensorFlow provides a fuller production-ready ecosystem -
understanding these trade-offs is key for practitioners selecting the
appropriate tool. We include charts, code snippets, and more than 20 references
to academic papers and official documentation to support this comparative
analysis

</details>


### [53] [FeDaL: Federated Dataset Learning for Time Series Foundation Models](https://arxiv.org/abs/2508.04045)
*Shengchao Chen,Guodong Long,Jing Jiang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为FeDaL的联邦学习方法，用于解决时间序列基础模型中的数据集异质性问题，通过消除局部和全局偏差提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据集的异质性导致领域偏差，影响模型的泛化能力，这一问题尚未得到充分研究。

Method: 采用联邦学习框架，提出FeDaL方法，结合Domain Bias Elimination (DBE)和Global Bias Elimination (GBE)机制，学习数据集无关的时间表示。

Result: 在8个任务和54个基线模型上验证了FeDaL的跨数据集泛化能力，并分析了联邦学习的扩展行为。

Conclusion: FeDaL能有效解决时间序列数据集的异质性问题，提升模型的泛化性能。

Abstract: Dataset-wise heterogeneity introduces significant domain biases that
fundamentally degrade generalization on Time Series Foundation Models (TSFMs),
yet this challenge remains underexplored. This paper rethink the development of
TSFMs using the paradigm of federated learning. We propose a novel Federated
Dataset Learning (FeDaL) approach to tackle heterogeneous time series by
learning dataset-agnostic temporal representations. Specifically, the
distributed architecture of federated learning is a nature solution to
decompose heterogeneous TS datasets into shared generalized knowledge and
preserved personalized knowledge. Moreover, based on the TSFM architecture,
FeDaL explicitly mitigates both local and global biases by adding two
complementary mechanisms: Domain Bias Elimination (DBE) and Global Bias
Elimination (GBE). FeDaL`s cross-dataset generalization has been extensively
evaluated in real-world datasets spanning eight tasks, including both
representation learning and downstream time series analysis, against 54
baselines. We further analyze federated scaling behavior, showing how data
volume, client count, and join rate affect model performance under
decentralization.

</details>


### [54] [Quantum Temporal Fusion Transformer](https://arxiv.org/abs/2508.04048)
*Krishnakanta Barik,Goutam Paul*

Main category: cs.LG

TL;DR: 量子时间融合变换器（QTFT）是一种量子增强的混合量子-经典架构，扩展了经典TFT框架的能力，在某些情况下表现优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 提升多时间范围时间序列预测的性能，利用量子计算的优势扩展经典TFT的能力。

Method: 基于变分量子算法，设计了一种量子-经典混合架构QTFT，适用于当前NISQ设备。

Result: QTFT在某些测试案例中表现优于经典TFT，其余案例中性能相当。

Conclusion: QTFT是一种有前景的量子增强时间序列预测方法，适用于当前量子硬件。

Abstract: The Temporal Fusion Transformer (TFT), proposed by Lim et al.
[\textit{International Journal of Forecasting}, 2021], is a state-of-the-art
attention-based deep neural network architecture specifically designed for
multi-horizon time series forecasting. It has demonstrated significant
performance improvements over existing benchmarks. In this work, we propose a
Quantum Temporal Fusion Transformer (QTFT), a quantum-enhanced hybrid
quantum-classical architecture that extends the capabilities of the classical
TFT framework. Our results demonstrate that QTFT is successfully trained on the
forecasting datasets and is capable of accurately predicting future values. In
particular, our experimental results display that in certain test cases, the
model outperforms its classical counterpart in terms of both training and test
loss, while in the remaining cases, it achieves comparable performance. A key
advantage of our approach lies in its foundation on a variational quantum
algorithm, enabling implementation on current noisy intermediate-scale quantum
(NISQ) devices without strict requirements on the number of qubits or circuit
depth.

</details>


### [55] [Fine-tuning for Better Few Shot Prompting: An Empirical Comparison for Short Answer Grading](https://arxiv.org/abs/2508.04063)
*Joel Walsh,Siddarth Mamidanna,Benjamin Nye,Mark Core,Daniel Auerbach*

Main category: cs.LG

TL;DR: 论文探讨了自动短答案评分（ASAG）中微调方法与少样本提示的对比，发现微调在OpenAI闭源模型中表现优于少样本基线，但对开源模型效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索在资源有限的情况下，如何通过微调和少样本提示提升自动短答案评分的性能。

Method: 方法包括使用OpenAI的微调服务和开源权重模型（如QLORA）进行微调，并与少样本提示方法对比。

Result: 结果显示，微调在OpenAI闭源模型中表现更好，但对开源模型效果有限；合成数据可显著提升开源模型性能。

Conclusion: 结论是微调方法在特定条件下优于少样本提示，但效果受模型类型和领域影响，合成数据可能提升开源模型表现。

Abstract: Research to improve Automated Short Answer Grading has recently focused on
Large Language Models (LLMs) with prompt engineering and no- or few-shot
prompting to achieve best results. This is in contrast to the fine-tuning
approach, which has historically required large-scale compute clusters
inaccessible to most users. New closed-model approaches such as OpenAI's
fine-tuning service promise results with as few as 100 examples, while methods
using open weights such as quantized low-rank adaptive (QLORA) can be used to
fine-tune models on consumer GPUs. We evaluate both of these fine-tuning
methods, measuring their interaction with few-shot prompting for automated
short answer grading (ASAG) with structured (JSON) outputs. Our results show
that finetuning with small amounts of data has limited utility for Llama
open-weight models, but that fine-tuning methods can outperform few-shot
baseline instruction-tuned LLMs for OpenAI's closed models. While our
evaluation set is limited, we find some evidence that the observed benefits of
finetuning may be impacted by the domain subject matter. Lastly, we observed
dramatic improvement with the LLama 3.1 8B-Instruct open-weight model by
seeding the initial training examples with a significant amount of cheaply
generated synthetic training data.

</details>


### [56] [FLAT: Latent-Driven Arbitrary-Target Backdoor Attacks in Federated Learning](https://arxiv.org/abs/2508.04064)
*Tuan Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: FLAT是一种新型的联邦学习后门攻击方法，通过条件自编码器生成多样化、目标特定的触发器，提高攻击的灵活性和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击方法因固定模式或单一目标触发器而受限，FLAT旨在解决这些问题，提供更灵活、隐蔽的攻击手段。

Method: 利用潜在驱动的条件自编码器生成多样化的触发器，支持任意目标选择且无需重新训练。

Result: FLAT攻击成功率高，能有效规避现有防御机制，实验验证了其优越性。

Conclusion: FLAT展示了后门攻击的新高度，呼吁开发新的防御策略应对潜在驱动的多目标威胁。

Abstract: Federated learning (FL) is vulnerable to backdoor attacks, yet most existing
methods are limited by fixed-pattern or single-target triggers, making them
inflexible and easier to detect. We propose FLAT (FL Arbitrary-Target Attack),
a novel backdoor attack that leverages a latent-driven conditional autoencoder
to generate diverse, target-specific triggers as needed. By introducing a
latent code, FLAT enables the creation of visually adaptive and highly variable
triggers, allowing attackers to select arbitrary targets without retraining and
to evade conventional detection mechanisms. Our approach unifies attack
success, stealth, and diversity within a single framework, introducing a new
level of flexibility and sophistication to backdoor attacks in FL. Extensive
experiments show that FLAT achieves high attack success and remains robust
against advanced FL defenses. These results highlight the urgent need for new
defense strategies to address latent-driven, multi-target backdoor threats in
federated settings.

</details>


### [57] [Adversarial Fair Multi-View Clustering](https://arxiv.org/abs/2508.04071)
*Mudi Jiang,Jiahui Zhou,Lianyu Hu,Xinying Liu,Zengyou He,Zhikui Chen*

Main category: cs.LG

TL;DR: 提出了一种对抗性公平多视图聚类（AFMVC）框架，通过对抗训练去除学习特征中的敏感属性信息，确保聚类结果不受其影响，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类虽能整合多源信息，但现有方法忽视公平性，且依赖敏感属性与聚类结构的假设，实际效果不佳。

Method: 采用对抗训练去除敏感属性信息，并通过KL散度对齐视图特定聚类分配与公平不变的共识分布。

Result: AFMVC在公平性和聚类性能上均优于现有方法。

Conclusion: AFMVC框架有效解决了多视图聚类中的公平性问题，兼具理论和实践优势。

Abstract: Cluster analysis is a fundamental problem in data mining and machine
learning. In recent years, multi-view clustering has attracted increasing
attention due to its ability to integrate complementary information from
multiple views. However, existing methods primarily focus on clustering
performance, while fairness-a critical concern in human-centered
applications-has been largely overlooked. Although recent studies have explored
group fairness in multi-view clustering, most methods impose explicit
regularization on cluster assignments, relying on the alignment between
sensitive attributes and the underlying cluster structure. However, this
assumption often fails in practice and can degrade clustering performance. In
this paper, we propose an adversarial fair multi-view clustering (AFMVC)
framework that integrates fairness learning into the representation learning
process. Specifically, our method employs adversarial training to fundamentally
remove sensitive attribute information from learned features, ensuring that the
resulting cluster assignments are unaffected by it. Furthermore, we
theoretically prove that aligning view-specific clustering assignments with a
fairness-invariant consensus distribution via KL divergence preserves
clustering consistency without significantly compromising fairness, thereby
providing additional theoretical guarantees for our framework. Extensive
experiments on data sets with fairness constraints demonstrate that AFMVC
achieves superior fairness and competitive clustering performance compared to
existing multi-view clustering and fairness-aware clustering methods.

</details>


### [58] [Model Inversion Attacks on Vision-Language Models: Do They Leak What They Learn?](https://arxiv.org/abs/2508.04097)
*Ngoc-Bao Nguyen,Sy-Tuyen Ho,Koh Jun Hao,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 该论文首次研究了视觉语言模型（VLMs）在泄露私有视觉训练数据方面的脆弱性，并提出了一系列基于令牌和序列的模型反转策略，实验证明这些方法能有效重建数据。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）在模型反转攻击中的隐私风险，填补了现有研究对VLMs脆弱性认识的空白。

Method: 提出了四种模型反转策略：基于令牌的TMI和TMI-C，基于序列的SMI和SMI-AW，并结合词汇表示的对数最大化损失进行优化。

Result: 实验表明，基于序列的方法（尤其是SMI-AW）在攻击准确性和视觉相似性上优于基于令牌的方法，人类评估的攻击准确率达到75.31%。

Conclusion: 研究揭示了VLMs在隐私方面的脆弱性，尤其是在医疗和金融等领域的广泛应用中，需要引起重视。

Abstract: Model inversion (MI) attacks pose significant privacy risks by reconstructing
private training data from trained neural networks. While prior works have
focused on conventional unimodal DNNs, the vulnerability of vision-language
models (VLMs) remains underexplored. In this paper, we conduct the first study
to understand VLMs' vulnerability in leaking private visual training data. To
tailored for VLMs' token-based generative nature, we propose a suite of novel
token-based and sequence-based model inversion strategies. Particularly, we
propose Token-based Model Inversion (TMI), Convergent Token-based Model
Inversion (TMI-C), Sequence-based Model Inversion (SMI), and Sequence-based
Model Inversion with Adaptive Token Weighting (SMI-AW). Through extensive
experiments and user study on three state-of-the-art VLMs and multiple
datasets, we demonstrate, for the first time, that VLMs are susceptible to
training data leakage. The experiments show that our proposed sequence-based
methods, particularly SMI-AW combined with a logit-maximization loss based on
vocabulary representation, can achieve competitive reconstruction and
outperform token-based methods in attack accuracy and visual similarity.
Importantly, human evaluation of the reconstructed images yields an attack
accuracy of 75.31\%, underscoring the severity of model inversion threats in
VLMs. Notably we also demonstrate inversion attacks on the publicly released
VLMs. Our study reveals the privacy vulnerability of VLMs as they become
increasingly popular across many applications such as healthcare and finance.

</details>


### [59] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 提出了一种一致性感知的策略优化框架，通过全局奖励和熵平衡机制解决多响应收敛时梯度消失的问题，提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多响应收敛时梯度消失，限制了训练效率和下游性能。

Method: 引入基于结果一致性的全局奖励和熵平衡机制，动态调整探索与收敛。

Result: 在多个数学推理基准上取得显著性能提升。

Conclusion: 框架具有鲁棒性和通用性，代码已开源。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [60] [Semi-Supervised Deep Domain Adaptation for Predicting Solar Power Across Different Locations](https://arxiv.org/abs/2508.04165)
*Md Shazid Islam,A S M Jahid Hasan,Md Saydur Rahman,Md Saiful Islam Sajol*

Main category: cs.LG

TL;DR: 论文提出了一种半监督深度域适应框架，用于解决太阳能发电预测中的域偏移问题，通过教师-学生模型配置实现目标域的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 地理和天气特征的多样性导致太阳能发电预测模型在不同地点表现不佳，域偏移和缺乏标注数据是主要挑战。

Method: 采用半监督深度域适应框架，结合卷积神经网络和教师-学生模型，利用一致性损失和交叉熵损失进行学习。

Result: 在仅标注目标域20%数据的情况下，模型在加州、佛罗里达和纽约的预测准确率分别提高了11.36%、6.65%和4.92%。

Conclusion: 该方法有效解决了域偏移问题，显著提升了太阳能发电预测的准确性，且对标注数据需求较低。

Abstract: Accurate solar generation prediction is essential for proper estimation of
renewable energy resources across diverse geographic locations. However,
geographical and weather features vary from location to location which
introduces domain shift - a major bottleneck to develop location-agnostic
prediction model. As a result, a machine-learning model which can perform well
to predict solar power in one location, may exhibit subpar performance in
another location. Moreover, the lack of properly labeled data and storage
issues make the task even more challenging. In order to address domain shift
due to varying weather conditions across different meteorological regions, this
paper presents a semi-supervised deep domain adaptation framework, allowing
accurate predictions with minimal labeled data from the target location. Our
approach involves training a deep convolutional neural network on a source
location's data and adapting it to the target location using a source-free,
teacher-student model configuration. The teacher-student model leverages
consistency and cross-entropy loss for semi-supervised learning, ensuring
effective adaptation without any source data requirement for prediction. With
annotation of only $20 \%$ data in the target domain, our approach exhibits an
improvement upto $11.36 \%$, $6.65 \%$, $4.92\%$ for California, Florida and
New York as target domain, respectively in terms of accuracy in predictions
with respect to non-adaptive approach.

</details>


### [61] [One Small Step with Fingerprints, One Giant Leap for emph{De Novo} Molecule Generation from Mass Spectra](https://arxiv.org/abs/2508.04180)
*Neng Kai Nigel Neo,Lim Jing,Ngoui Yong Zhau Preston,Koh Xue Ting Serene,Bingquan Shen*

Main category: cs.LG

TL;DR: 论文提出了一种基于预训练的两阶段分子生成方法，结合MIST编码器和MolForge解码器，显著提升了从质谱数据生成分子结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决从质谱数据中生成分子结构的挑战，提高生成准确性和效率。

Method: 采用MIST编码器将质谱数据编码为分子指纹，再通过预训练的MolForge解码器将指纹解码为分子结构，并引入阈值处理优化解码过程。

Result: 相比现有方法，性能提升十倍，生成分子结构的准确率达到top-1 28% / top-10 36%。

Conclusion: 该两阶段方法为未来质谱数据分子解析研究提供了强有力的基线。

Abstract: A common approach to the \emph{de novo} molecular generation problem from
mass spectra involves a two-stage pipeline: (1) encoding mass spectra into
molecular fingerprints, followed by (2) decoding these fingerprints into
molecular structures. In our work, we adopt
\textsc{MIST}~\citep{MISTgoldmanAnnotatingMetaboliteMass2023} as the encoder
and \textsc{MolForge}~\citep{ucakReconstructionLosslessMolecular2023} as the
decoder, leveraging pretraining to enhance performance. Notably, pretraining
\textsc{MolForge} proves especially effective, enabling it to serve as a robust
fingerprint-to-structure decoder. Additionally, instead of passing the
probability of each bit in the fingerprint, thresholding the probabilities as a
step function helps focus the decoder on the presence of substructures,
improving recovery of accurate molecular structures even when the fingerprints
predicted by \textsc{MIST} only moderately resembles the ground truth in terms
of Tanimoto similarity. This combination of encoder and decoder results in a
tenfold improvement over previous state-of-the-art methods, generating top-1
28\% / top-10 36\% of molecular structures correctly from mass spectra. We
position this pipeline as a strong baseline for future research in \emph{de
novo} molecule elucidation from mass spectra.

</details>


### [62] [Neural Network Training via Stochastic Alternating Minimization with Trainable Step Sizes](https://arxiv.org/abs/2508.04193)
*Chengcheng Yan,Jiawei Xu,Zheng Peng,Qingsong Wang*

Main category: cs.LG

TL;DR: 提出了一种名为SAMT的新方法，通过交替更新网络参数块和自适应步长策略，提高了非凸优化中的训练稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练中的非凸优化问题，避免标准方法（如SGD）的不稳定收敛和高计算成本。

Method: 采用块交替最小化策略，将每层权重视为一个块，并结合基于元学习的自适应步长策略。

Result: SAMT在多个基准测试中表现出更好的泛化性能，且参数更新次数更少。

Conclusion: SAMT是一种高效且稳定的神经网络优化方法，具有理论和实验支持。

Abstract: The training of deep neural networks is inherently a nonconvex optimization
problem, yet standard approaches such as stochastic gradient descent (SGD)
require simultaneous updates to all parameters, often leading to unstable
convergence and high computational cost. To address these issues, we propose a
novel method, Stochastic Alternating Minimization with Trainable Step Sizes
(SAMT), which updates network parameters in an alternating manner by treating
the weights of each layer as a block. By decomposing the overall optimization
into sub-problems corresponding to different blocks, this block-wise
alternating strategy reduces per-step computational overhead and enhances
training stability in nonconvex settings. To fully leverage these benefits,
inspired by meta-learning, we proposed a novel adaptive step size strategy to
incorporate into the sub-problem solving steps of alternating updates. It
supports different types of trainable step sizes, including but not limited to
scalar, element-wise, row-wise, and column-wise, enabling adaptive step size
selection tailored to each block via meta-learning. We further provide a
theoretical convergence guarantee for the proposed algorithm, establishing its
optimization soundness. Extensive experiments for multiple benchmarks
demonstrate that SAMT achieves better generalization performance with fewer
parameter updates compared to state-of-the-art methods, highlighting its
effectiveness and potential in neural network optimization.

</details>


### [63] [Causal Reward Adjustment: Mitigating Reward Hacking in External Reasoning via Backdoor Correction](https://arxiv.org/abs/2508.04216)
*Ruike Song,Zeen Song,Huijie Guo,Wenwen Qiang*

Main category: cs.LG

TL;DR: 论文提出Causal Reward Adjustment（CRA）方法，通过因果推断解决外部推理系统中的奖励欺骗问题，提高数学问题求解的准确性。


<details>
  <summary>Details</summary>
Motivation: 外部推理系统结合语言模型和过程奖励模型（PRMs）时，容易出现奖励欺骗现象，即逻辑错误的推理路径被高评分，导致错误答案。

Method: 提出CRA方法，通过稀疏自编码器从PRM内部激活中提取可解释特征，并利用后门调整消除混杂语义特征的影响。

Result: 在数学问题求解数据集上的实验表明，CRA有效减少奖励欺骗并提高最终准确性。

Conclusion: CRA无需修改策略模型或重新训练PRM，即可显著改善推理系统的性能。

Abstract: External reasoning systems combine language models with process reward models
(PRMs) to select high-quality reasoning paths for complex tasks such as
mathematical problem solving. However, these systems are prone to reward
hacking, where high-scoring but logically incorrect paths are assigned high
scores by the PRMs, leading to incorrect answers. From a causal inference
perspective, we attribute this phenomenon primarily to the presence of
confounding semantic features. To address it, we propose Causal Reward
Adjustment (CRA), a method that mitigates reward hacking by estimating the true
reward of a reasoning path. CRA trains sparse autoencoders on the PRM's
internal activations to recover interpretable features, then corrects
confounding by using backdoor adjustment. Experiments on math solving datasets
demonstrate that CRA mitigates reward hacking and improves final accuracy,
without modifying the policy model or retraining PRM.

</details>


### [64] [Symmetric Behavior Regularization via Taylor Expansion of Symmetry](https://arxiv.org/abs/2508.04225)
*Lingwei Zhu,Zheng Chen,Han Wang,Yukie Nagai*

Main category: cs.LG

TL;DR: 论文提出了一种基于对称散度的离线强化学习框架S$f$-AC，解决了对称散度在行为正则化策略优化中的数值问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用非对称散度（如KL散度），但对称散度在正则化和损失函数中存在数值问题。

Method: 通过泰勒展开$f$-散度，证明了有限级数可得到解析策略，并分解对称散度以缓解数值问题。

Result: 实验表明，S$f$-AC在分布近似和MuJoCo任务中表现优异。

Conclusion: S$f$-AC是首个实用的基于对称散度的BRPO算法，解决了传统方法的局限性。

Abstract: This paper introduces symmetric divergences to behavior regularization policy
optimization (BRPO) to establish a novel offline RL framework. Existing methods
focus on asymmetric divergences such as KL to obtain analytic regularized
policies and a practical minimization objective. We show that symmetric
divergences do not permit an analytic policy as regularization and can incur
numerical issues as loss. We tackle these challenges by the Taylor series of
$f$-divergence. Specifically, we prove that an analytic policy can be obtained
with a finite series. For loss, we observe that symmetric divergences can be
decomposed into an asymmetry and a conditional symmetry term, Taylor-expanding
the latter alleviates numerical issues. Summing together, we propose Symmetric
$f$ Actor-Critic (S$f$-AC), the first practical BRPO algorithm with symmetric
divergences. Experimental results on distribution approximation and MuJoCo
verify that S$f$-AC performs competitively.

</details>


### [65] [Empowering Time Series Forecasting with LLM-Agents](https://arxiv.org/abs/2508.04231)
*Chin-Chia Michael Yeh,Vivian Lai,Uday Singh Saini,Xiran Fan,Yujie Fan,Junpeng Wang,Xin Dai,Yan Zheng*

Main category: cs.LG

TL;DR: DCATS是一种基于LLM的数据中心代理，专注于提升时间序列数据质量而非模型架构，在交通量预测数据集上平均减少6%误差。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML方法多关注特征工程和模型架构，但轻量级模型在时间序列预测中表现优异，因此探索通过提升数据质量来优化性能。

Method: 提出DCATS，利用时间序列元数据清理数据并优化预测性能，测试了四种预测模型。

Result: 在交通量预测数据集上，DCATS平均减少6%预测误差。

Conclusion: 数据中心方法在时间序列预测的AutoML中具有潜力。

Abstract: Large Language Model (LLM) powered agents have emerged as effective planners
for Automated Machine Learning (AutoML) systems. While most existing AutoML
approaches focus on automating feature engineering and model architecture
search, recent studies in time series forecasting suggest that lightweight
models can often achieve state-of-the-art performance. This observation led us
to explore improving data quality, rather than model architecture, as a
potentially fruitful direction for AutoML on time series data. We propose
DCATS, a Data-Centric Agent for Time Series. DCATS leverages metadata
accompanying time series to clean data while optimizing forecasting
performance. We evaluated DCATS using four time series forecasting models on a
large-scale traffic volume forecasting dataset. Results demonstrate that DCATS
achieves an average 6% error reduction across all tested models and time
horizons, highlighting the potential of data-centric approaches in AutoML for
time series forecasting.

</details>


### [66] [Automated ultrasound doppler angle estimation using deep learning](https://arxiv.org/abs/2508.04243)
*Nilesh Patil,Ajay Anand*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的自动化多普勒角度估计方法，通过预训练模型提取特征并结合浅层网络，误差低于临床可接受阈值。


<details>
  <summary>Details</summary>
Motivation: 多普勒角度估计错误是血流速度测量误差的主要原因，需自动化解决方案。

Method: 使用2100张颈动脉超声图像，结合图像增强技术，利用五种预训练模型提取特征，并通过浅层网络估计角度。

Result: 最佳模型的平均绝对误差（MAE）为3.9°至9.4°，低于临床可接受阈值。

Conclusion: 深度学习技术在多普勒角度自动化估计中具有潜力，可集成到商业超声扫描仪中。

Abstract: Angle estimation is an important step in the Doppler ultrasound clinical
workflow to measure blood velocity. It is widely recognized that incorrect
angle estimation is a leading cause of error in Doppler-based blood velocity
measurements. In this paper, we propose a deep learning-based approach for
automated Doppler angle estimation. The approach was developed using 2100 human
carotid ultrasound images including image augmentation. Five pre-trained models
were used to extract images features, and these features were passed to a
custom shallow network for Doppler angle estimation. Independently,
measurements were obtained by a human observer reviewing the images for
comparison. The mean absolute error (MAE) between the automated and manual
angle estimates ranged from 3.9{\deg} to 9.4{\deg} for the models evaluated.
Furthermore, the MAE for the best performing model was less than the acceptable
clinical Doppler angle error threshold thus avoiding misclassification of
normal velocity values as a stenosis. The results demonstrate potential for
applying a deep-learning based technique for automated ultrasound Doppler angle
estimation. Such a technique could potentially be implemented within the
imaging software on commercial ultrasound scanners.

</details>


### [67] [T3Time: Tri-Modal Time Series Forecasting via Adaptive Multi-Head Alignment and Residual Fusion](https://arxiv.org/abs/2508.04251)
*Abdul Monaf Chowdhury,Rabeya Akter,Safaeid Hossain Arib*

Main category: cs.LG

TL;DR: T3Time是一个新型的三模态框架，通过时间、频谱和提示分支捕捉多变量时间序列的复杂关系，动态调整特征权重，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉多变量时间序列的长期依赖和跨变量交互时存在局限性，缺乏对不同预测时域的适应性。

Method: 提出T3Time框架，包含时间、频谱和提示分支，通过门控机制动态调整特征优先级，并自适应聚合跨模态对齐头。

Result: 在基准数据集上，T3Time平均减少MSE 3.28%和MAE 2.29%，在少样本学习中也表现优异。

Conclusion: T3Time通过动态特征融合和自适应机制，显著提升了多变量时间序列预测的性能和泛化能力。

Abstract: Multivariate time series forecasting (MTSF) seeks to model temporal dynamics
among variables to predict future trends. Transformer-based models and large
language models (LLMs) have shown promise due to their ability to capture
long-range dependencies and patterns. However, current methods often rely on
rigid inductive biases, ignore intervariable interactions, or apply static
fusion strategies that limit adaptability across forecast horizons. These
limitations create bottlenecks in capturing nuanced, horizon-specific
relationships in time-series data. To solve this problem, we propose T3Time, a
novel trimodal framework consisting of time, spectral, and prompt branches,
where the dedicated frequency encoding branch captures the periodic structures
along with a gating mechanism that learns prioritization between temporal and
spectral features based on the prediction horizon. We also proposed a mechanism
which adaptively aggregates multiple cross-modal alignment heads by dynamically
weighting the importance of each head based on the features. Extensive
experiments on benchmark datasets demonstrate that our model consistently
outperforms state-of-the-art baselines, achieving an average reduction of 3.28%
in MSE and 2.29% in MAE. Furthermore, it shows strong generalization in
few-shot learning settings: with 5% training data, we see a reduction in MSE
and MAE by 4.13% and 1.91%, respectively; and with 10% data, by 3.62% and 1.98%
on average. Code - https://github.com/monaf-chowdhury/T3Time/

</details>


### [68] [A Visual Tool for Interactive Model Explanation using Sensitivity Analysis](https://arxiv.org/abs/2508.04269)
*Manuela Schuler*

Main category: cs.LG

TL;DR: SAInT是一个基于Python的工具，用于通过集成的局部和全局敏感性分析可视化探索和理解机器学习模型的行为。


<details>
  <summary>Details</summary>
Motivation: 为AI研究人员和领域专家提供无需编程的交互式图形界面，支持配置、训练、评估和解释模型，实现人机协作工作流。

Method: 自动化模型训练和选择，提供基于方差的全局特征归因分析，以及通过LIME和SHAP进行单实例解释。

Result: 在泰坦尼克数据集上的分类任务中展示了系统功能，敏感性信息可指导特征选择和数据优化。

Conclusion: SAInT通过可视化敏感性分析，有效支持模型理解和优化。

Abstract: We present SAInT, a Python-based tool for visually exploring and
understanding the behavior of Machine Learning (ML) models through integrated
local and global sensitivity analysis. Our system supports Human-in-the-Loop
(HITL) workflows by enabling users - both AI researchers and domain experts -
to configure, train, evaluate, and explain models through an interactive
graphical interface without programming. The tool automates model training and
selection, provides global feature attribution using variance-based sensitivity
analysis, and offers per-instance explanation via LIME and SHAP. We demonstrate
the system on a classification task predicting survival on the Titanic dataset
and show how sensitivity information can guide feature selection and data
refinement.

</details>


### [69] [Mockingbird: How does LLM perform in general machine learning tasks?](https://arxiv.org/abs/2508.04279)
*Haoyu Jia,Yoshiki Obinata,Kento Kawaharazuka,Kei Okada*

Main category: cs.LG

TL;DR: 论文提出了一种名为Mockingbird的框架，旨在将大语言模型（LLMs）适配到通用机器学习任务中，通过角色扮演和错误反思提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究源于对LLMs在通用机器学习任务中潜力的好奇，探索其超越聊天机器人应用的可能性。

Method: 提出Mockingbird框架，核心是通过指令让LLMs角色扮演功能并反思错误以自我改进。

Result: 评估显示LLM驱动的机器学习方法（如Mockingbird）在常见任务中表现尚可，但仅靠自我反思无法超越领域特定文档和人类专家反馈的效果。

Conclusion: LLMs在通用机器学习任务中具有潜力，但仍需结合领域知识和专家反馈以实现更优性能。

Abstract: Large language models (LLMs) are now being used with increasing frequency as
chat bots, tasked with the summarizing information or generating text and code
in accordance with user instructions. The rapid increase in reasoning
capabilities and inference speed of LLMs has revealed their remarkable
potential for applications extending beyond the domain of chat bots to general
machine learning tasks. This work is conducted out of the curiosity about such
potential. In this work, we propose a framework Mockingbird to adapt LLMs to
general machine learning tasks and evaluate its performance and scalability on
several general machine learning tasks. The core concept of this framework is
instructing LLMs to role-play functions and reflect on its mistakes to improve
itself. Our evaluation and analysis result shows that LLM-driven machine
learning methods, such as Mockingbird, can achieve acceptable results on common
machine learning tasks; however, solely reflecting on its own currently cannot
outperform the effect of domain-specific documents and feedback from human
experts.

</details>


### [70] [Enhancing Vision-Language Model Training with Reinforcement Learning in Synthetic Worlds for Real-World Success](https://arxiv.org/abs/2508.04280)
*George Bredis,Stanislav Dereka,Viacheslav Sinii,Ruslan Rakhimov,Daniil Gavrilov*

Main category: cs.LG

TL;DR: VL-DAC是一种轻量级、无超参数的RL算法，通过解耦动作和价值的更新，显著提升VLMs在多种任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）缺乏将视觉观察转化为语言条件动作序列的能力，且现有RL方法泛化性差或依赖密集奖励环境。

Method: 提出VL-DAC算法，解耦动作令牌的PPO更新和环境步级别的价值学习，避免不稳定权重项。

Result: 在多个模拟器中训练的VLMs泛化性能显著提升（如BALROG +50%），且不影响通用图像理解准确性。

Conclusion: VL-DAC首次证明简单RL算法可在廉价合成环境中训练VLMs，并在真实任务中取得性能提升。

Abstract: Interactive multimodal agents must convert raw visual observations into
coherent sequences of language-conditioned actions -- a capability that current
vision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)
efforts could, in principle, endow VLMs with such skills, but they have seldom
tested whether the learned behaviours generalize beyond their training
simulators, and they depend either on brittle hyperparameter tuning or on
dense-reward environments with low state variability. We introduce
Vision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,
hyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens
while learning value only at the environment-step level: an arrangement, to our
knowledge, not previously explored for large VLMs or LLMs. This simple
decoupling removes unstable weighting terms and yields faster, more reliable
convergence. Training a single VLM with VL-DAC in one inexpensive simulator at
a time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies
that generalize widely: +50\% relative on BALROG (game-centric agentic
control), +5\% relative on the hardest part of VSI-Bench (spatial planning),
and +2\% on VisualWebBench (web navigation), all without degrading general
image understanding accuracy. These results provide the first evidence that a
simple RL algorithm can train VLMs entirely in cheap synthetic worlds while
delivering measurable gains on real-image agentic, spatial-reasoning, and
web-navigation benchmarks.

</details>


### [71] [WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification](https://arxiv.org/abs/2508.04308)
*Thang Duc Tran,Thai Hoang Le*

Main category: cs.LG

TL;DR: 提出了一种基于权重显著性的两阶段机器遗忘方法（WSS-CL），通过对比学习提高图像分类中的遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在精确遗忘、稳定性和跨领域适用性方面存在挑战，需要更高效的解决方案。

Method: 采用两阶段方法：遗忘阶段最大化输出logits与伪标签的KL散度；对抗微调阶段通过自监督对比学习最大化遗忘与保留数据的特征距离。

Result: 实验表明，该方法显著提升了遗忘效果，性能损失可忽略，优于现有技术。

Conclusion: WSS-CL在监督和自监督场景中均表现出高效且实用的机器遗忘能力。

Abstract: Machine unlearning, the efficient deletion of the impact of specific data in
a trained model, remains a challenging problem. Current machine unlearning
approaches that focus primarily on data-centric or weight-based strategies
frequently encounter challenges in achieving precise unlearning, maintaining
stability, and ensuring applicability across diverse domains. In this work, we
introduce a new two-phase efficient machine unlearning method for image
classification, in terms of weight saliency, leveraging weight saliency to
focus the unlearning process on critical model parameters. Our method is called
weight saliency soft-guided contrastive learning for efficient machine
unlearning image classification (WSS-CL), which significantly narrows the
performance gap with "exact" unlearning. First, the forgetting stage maximizes
kullback-leibler divergence between output logits and aggregated pseudo-labels
for efficient forgetting in logit space. Next, the adversarial fine-tuning
stage introduces contrastive learning in a self-supervised manner. By using
scaled feature representations, it maximizes the distance between the forgotten
and retained data samples in the feature space, with the forgotten and the
paired augmented samples acting as positive pairs, while the retained samples
act as negative pairs in the contrastive loss computation. Experimental
evaluations reveal that our proposed method yields much-improved unlearning
efficacy with negligible performance loss compared to state-of-the-art
approaches, indicative of its usability in supervised and self-supervised
settings.

</details>


### [72] [Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning](https://arxiv.org/abs/2508.04329)
*Ali Taheri Ghahrizjani,Alireza Taban,Qizhou Wang,Shanshan Ye,Abdolreza Mirzaei,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种通过将语料库中的标记分为正负两类来优化监督微调（SFT）的方法，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）对预训练大语言模型（LLMs）至关重要，但其效果依赖于数据质量和数量，可能导致性能提升有限甚至下降。

Method: 将语料库中的标记分为正负两类，正标记常规训练，负标记通过遗忘机制处理以减少无效信息的影响。

Result: 实验表明，这种遗忘机制不仅提升了模型整体性能，还促进了更丰富的模型响应。

Conclusion: 通过标记分类和遗忘机制，可以更精确地指导模型学习，提升SFT的效果。

Abstract: Supervised fine-tuning (SFT) plays a critical role for pretrained large
language models (LLMs), notably enhancing their capacity to acquire
domain-specific knowledge while preserving or potentially augmenting their
general-purpose capabilities. However, the efficacy of SFT hinges on data
quality as well as data volume, otherwise it may result in limited performance
gains or even degradation relative to the associated baselines. To mitigate
such reliance, we suggest categorizing tokens within each corpus into two parts
-- positive and negative tokens -- based on whether they are useful to improve
model performance. Positive tokens can be trained in common ways, whereas
negative tokens, which may lack essential semantics or be misleading, should be
explicitly forgotten. Overall, the token categorization facilitate the model to
learn less informative message, and the forgetting process shapes a knowledge
boundary to guide the model on what information to learn more precisely. We
conduct experiments on well-established benchmarks, finding that this
forgetting mechanism not only improves overall model performance and also
facilitate more diverse model responses.

</details>


### [73] [From Split to Share: Private Inference with Distributed Feature Sharing](https://arxiv.org/abs/2508.04346)
*Zihan Liu,Jiayi Wen,Shouhong Tan,Zhirun Zheng,Cheng Huang*

Main category: cs.LG

TL;DR: PrivDFS是一种新的私有推理范式，通过分布式特征共享解决隐私与效率的权衡问题，显著降低客户端计算负担。


<details>
  <summary>Details</summary>
Motivation: 处理敏感数据时，现有的私有推理方法在隐私和效率之间存在矛盾，需要一种既能保护隐私又高效的方法。

Method: PrivDFS将输入特征分成多个平衡的共享部分，分发到非共谋的服务器进行独立推理，客户端安全聚合结果。扩展方法PrivDFS-AT和PrivDFS-KD进一步增强了隐私保护。

Result: 在CIFAR-10和CelebA上的实验表明，PrivDFS在保持隐私的同时，将客户端计算减少100倍且无精度损失。扩展方法对多种攻击具有鲁棒性。

Conclusion: PrivDFS提供了一种高效且隐私保护的私有推理方案，适用于敏感数据处理。

Abstract: Cloud-based Machine Learning as a Service (MLaaS) raises serious privacy
concerns when handling sensitive client data. Existing Private Inference (PI)
methods face a fundamental trade-off between privacy and efficiency:
cryptographic approaches offer strong protection but incur high computational
overhead, while efficient alternatives such as split inference expose
intermediate features to inversion attacks. We propose PrivDFS, a new paradigm
for private inference that replaces a single exposed representation with
distributed feature sharing. PrivDFS partitions input features on the client
into multiple balanced shares, which are distributed to non-colluding,
non-communicating servers for independent partial inference. The client
securely aggregates the servers' outputs to reconstruct the final prediction,
ensuring that no single server observes sufficient information to compromise
input privacy. To further strengthen privacy, we propose two key extensions:
PrivDFS-AT, which uses adversarial training with a diffusion-based proxy
attacker to enforce inversion-resistant feature partitioning, and PrivDFS-KD,
which leverages user-specific keys to diversify partitioning policies and
prevent query-based inversion generalization. Experiments on CIFAR-10 and
CelebA demonstrate that PrivDFS achieves privacy comparable to deep split
inference while cutting client computation by up to 100 times with no accuracy
loss, and that the extensions remain robust against both diffusion-based
in-distribution and adaptive attacks.

</details>


### [74] [Multi-Marginal Stochastic Flow Matching for High-Dimensional Snapshot Data at Irregular Time Points](https://arxiv.org/abs/2508.04351)
*Justin Lee,Behnaz Moradijamei,Heman Shakeri*

Main category: cs.LG

TL;DR: 提出了一种名为MMSFM的新方法，用于处理高维系统在非等距时间点观测数据的建模问题，避免了传统降维方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决高维系统在非等距时间点观测数据建模的挑战，避免降维方法导致的动态信息丢失。

Method: 扩展了无模拟的分数和流匹配方法，引入多边际设置和测度值样条，增强对不规则时间点的鲁棒性。

Result: 在合成和基准数据集上验证了方法的有效性，包括基因表达数据和图像进展任务。

Conclusion: MMSFM方法在高维数据建模中表现出色，适用于非等距时间点观测的场景。

Abstract: Modeling the evolution of high-dimensional systems from limited snapshot
observations at irregular time points poses a significant challenge in
quantitative biology and related fields. Traditional approaches often rely on
dimensionality reduction techniques, which can oversimplify the dynamics and
fail to capture critical transient behaviors in non-equilibrium systems. We
present Multi-Marginal Stochastic Flow Matching (MMSFM), a novel extension of
simulation-free score and flow matching methods to the multi-marginal setting,
enabling the alignment of high-dimensional data measured at non-equidistant
time points without reducing dimensionality. The use of measure-valued splines
enhances robustness to irregular snapshot timing, and score matching prevents
overfitting in high-dimensional spaces. We validate our framework on several
synthetic and benchmark datasets, including gene expression data collected at
uneven time points and an image progression task, demonstrating the method's
versatility.

</details>


### [75] [Continual Multiple Instance Learning for Hematologic Disease Diagnosis](https://arxiv.org/abs/2508.04368)
*Zahra Ebrahimi,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.LG

TL;DR: 提出了一种针对多实例学习（MIL）的持续学习方法，通过选择代表性实例进行训练，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实验室和临床环境中数据流动态变化，需持续更新模型以避免灾难性遗忘，但现有方法在多实例学习中效果不佳。

Method: 基于实例注意力和距离选择代表性实例存储，保留数据多样性，用于后续任务训练。

Result: 在白血病实验室数据上验证，显著优于现有持续学习方法。

Conclusion: 该方法首次实现了MIL的持续学习，适应数据分布变化，如疾病发生率或基因改变。

Abstract: The dynamic environment of laboratories and clinics, with streams of data
arriving on a daily basis, requires regular updates of trained machine learning
models for consistent performance. Continual learning is supposed to help train
models without catastrophic forgetting. However, state-of-the-art methods are
ineffective for multiple instance learning (MIL), which is often used in
single-cell-based hematologic disease diagnosis (e.g., leukemia detection).
Here, we propose the first continual learning method tailored specifically to
MIL. Our method is rehearsal-based over a selection of single instances from
various bags. We use a combination of the instance attention score and distance
from the bag mean and class mean vectors to carefully select which samples and
instances to store in exemplary sets from previous tasks, preserving the
diversity of the data. Using the real-world input of one month of data from a
leukemia laboratory, we study the effectiveness of our approach in a class
incremental scenario, comparing it to well-known continual learning methods. We
show that our method considerably outperforms state-of-the-art methods,
providing the first continual learning approach for MIL. This enables the
adaptation of models to shifting data distributions over time, such as those
caused by changes in disease occurrence or underlying genetic alterations.

</details>


### [76] [FlexQ: Efficient Post-training INT6 Quantization for LLM Serving via Algorithm-System Co-Design](https://arxiv.org/abs/2508.04405)
*Hao Zhang,Aining Jia,Weifeng Bu,Yushu Cai,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: FlexQ是一种新型的INT6量化框架，通过算法创新和系统优化，在保持高精度的同时提升推理效率和内存节省。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高内存和计算成本限制了实际部署，现有量化方法（如INT4/INT8）在精度或效率上存在不足。

Method: FlexQ采用统一的6位权重量化，并通过层敏感度分析保留8位激活，开发了支持W6A6和W6A8的高性能GPU内核。

Result: 在LLaMA模型上，FlexQ保持了接近FP16的精度（困惑度增加不超过0.05），推理速度提升1.33倍，内存节省1.21倍。

Conclusion: FlexQ在INT6量化中实现了精度与效率的平衡，为LLMs的实际部署提供了可行方案。

Abstract: Large Language Models (LLMs) demonstrate exceptional performance but entail
significant memory and computational costs, restricting their practical
deployment. While existing INT4/INT8 quantization reduces these costs, they
often degrade accuracy or lack optimal efficiency. INT6 quantization offers a
superior trade-off between model accuracy and inference efficiency, but lacks
hardware support in modern GPUs, forcing emulation via higher-precision
arithmetic units that limit acceleration.
  In this paper, we propose FlexQ, a novel post-training INT6 quantization
framework combining algorithmic innovation with system-level optimizations.
FlexQ employs uniform 6-bit weight quantization across all layers, with
adaptive retention of 8-bit activations in layers identified through layer-wise
sensitivity analysis. To maximize hardware efficiency, we develop a specialized
high-performance GPU kernel supporting matrix multiplication for W6A6 and W6A8
representations via Binary Tensor Core (BTC) equivalents, effectively bypassing
the lack of native INT6 tensor cores. Evaluations on LLaMA models show FlexQ
maintains near-FP16 accuracy, with perplexity increases of no more than 0.05.
The proposed kernel achieves an average 1.39$\times$ speedup over ABQ-LLM on
LLaMA-2-70B linear layers. End-to-end, FlexQ delivers 1.33$\times$ inference
acceleration and 1.21$\times$ memory savings over SmoothQuant. Code is released
at https://github.com/FlyFoxPlayer/FlexQ.

</details>


### [77] [Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models](https://arxiv.org/abs/2508.04427)
*Md Raisul Kibria,Sébastien Lafond,Janan Arslan*

Main category: cs.LG

TL;DR: 系统综述分析了2020年至2024年初关于多模态模型可解释性的研究，发现注意力技术是主要解释方法，但评估方法缺乏系统性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态学习和可解释人工智能（XAI）的快速发展，研究多模态模型的可解释性成为重要课题。

Method: 通过系统文献综述，从模型架构、模态、解释算法和评估方法等多维度分析研究。

Result: 研究发现注意力技术是主要解释方法，但评估方法缺乏一致性和鲁棒性。

Conclusion: 建议未来研究采用更严谨、透明和标准化的评估方法，以提升多模态XAI的可解释性和可靠性。

Abstract: Multimodal learning has witnessed remarkable advancements in recent years,
particularly with the integration of attention-based models, leading to
significant performance gains across a variety of tasks. Parallel to this
progress, the demand for explainable artificial intelligence (XAI) has spurred
a growing body of research aimed at interpreting the complex decision-making
processes of these models. This systematic literature review analyzes research
published between January 2020 and early 2024 that focuses on the
explainability of multimodal models. Framed within the broader goals of XAI, we
examine the literature across multiple dimensions, including model
architecture, modalities involved, explanation algorithms and evaluation
methodologies. Our analysis reveals that the majority of studies are
concentrated on vision-language and language-only models, with attention-based
techniques being the most commonly employed for explanation. However, these
methods often fall short in capturing the full spectrum of interactions between
modalities, a challenge further compounded by the architectural heterogeneity
across domains. Importantly, we find that evaluation methods for XAI in
multimodal settings are largely non-systematic, lacking consistency,
robustness, and consideration for modality-specific cognitive and contextual
factors. Based on these findings, we provide a comprehensive set of
recommendations aimed at promoting rigorous, transparent, and standardized
evaluation and reporting practices in multimodal XAI research. Our goal is to
support future research in more interpretable, accountable, and responsible
mulitmodal AI systems, with explainability at their core.

</details>


### [78] [Cloud Model Characteristic Function Auto-Encoder: Integrating Cloud Model Theory with MMD Regularization for Enhanced Generative Modeling](https://arxiv.org/abs/2508.04447)
*Biao Hu,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出了一种名为CMCFAE的新型生成模型，将云模型与Wasserstein自动编码器结合，通过云模型的特征函数正则化潜在空间，提升复杂数据分布的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖标准高斯先验和传统散度度量，导致重构样本同质化。CMCFAE旨在通过云模型先验提供更灵活和真实的潜在空间表示。

Method: 将云模型的特征函数作为正则化项引入WAE框架，推导云模型的特征函数并设计相应的正则化器。

Result: 在MNIST、FashionMNIST、CIFAR-10和CelebA上的实验表明，CMCFAE在重构质量、潜在空间结构和样本多样性上优于现有模型。

Conclusion: CMCFAE不仅实现了云模型理论与MMD正则化的创新结合，还为增强基于自动编码器的生成模型提供了新视角。

Abstract: We introduce Cloud Model Characteristic Function Auto-Encoder (CMCFAE), a
novel generative model that integrates the cloud model into the Wasserstein
Auto-Encoder (WAE) framework. By leveraging the characteristic functions of the
cloud model to regularize the latent space, our approach enables more accurate
modeling of complex data distributions. Unlike conventional methods that rely
on a standard Gaussian prior and traditional divergence measures, our method
employs a cloud model prior, providing a more flexible and realistic
representation of the latent space, thus mitigating the homogenization observed
in reconstructed samples. We derive the characteristic function of the cloud
model and propose a corresponding regularizer within the WAE framework.
Extensive quantitative and qualitative evaluations on MNIST, FashionMNIST,
CIFAR-10, and CelebA demonstrate that CMCFAE outperforms existing models in
terms of reconstruction quality, latent space structuring, and sample
diversity. This work not only establishes a novel integration of cloud model
theory with MMD-based regularization but also offers a promising new
perspective for enhancing autoencoder-based generative models.

</details>


### [79] [Automatic LLM Red Teaming](https://arxiv.org/abs/2508.04451)
*Roman Belaire,Arunesh Sinha,Pradeep Varakantham*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的新方法，通过多轮对话策略训练AI攻击其他AI，以更全面地发现LLM的漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前自动化的LLM红队测试方法依赖脆弱的提示模板或单轮攻击，无法捕捉真实对抗对话的复杂性。

Method: 将红队测试建模为马尔可夫决策过程（MDP），采用分层强化学习框架，通过细粒度的token级奖励训练生成代理。

Result: 该方法能够发现现有基线遗漏的细微漏洞，成为新的SOTA方法。

Conclusion: 将LLM红队测试重新定义为动态的、基于轨迹的过程，对AI的稳健部署至关重要。

Abstract: Red teaming is critical for identifying vulnerabilities and building trust in
current LLMs. However, current automated methods for Large Language Models
(LLMs) rely on brittle prompt templates or single-turn attacks, failing to
capture the complex, interactive nature of real-world adversarial dialogues. We
propose a novel paradigm: training an AI to strategically `break' another AI.
By formalizing red teaming as a Markov Decision Process (MDP) and employing a
hierarchical Reinforcement Learning (RL) framework, we effectively address the
inherent sparse reward and long-horizon challenges. Our generative agent learns
coherent, multi-turn attack strategies through a fine-grained, token-level harm
reward, enabling it to uncover subtle vulnerabilities missed by existing
baselines. This approach sets a new state-of-the-art, fundamentally reframing
LLM red teaming as a dynamic, trajectory-based process (rather than a one-step
test) essential for robust AI deployment.

</details>


### [80] [Small transformer architectures for task switching](https://arxiv.org/abs/2508.04461)
*Claudius Gros*

Main category: cs.LG

TL;DR: 论文探讨了小规模任务切换场景下注意力机制与传统方法的性能对比，发现标准Transformer表现不佳，而改进后的模型（如cisformer和extensive attention）能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索注意力机制在小规模任务切换场景中的表现，并比较其与传统方法（如MLP和LSTM）的优劣。

Method: 方法包括在任务切换框架下测试多种模型（Transformer、LSTM、MLP、cisformer和extensive attention），并评估其在IARC任务中的表现。

Result: 结果显示标准Transformer表现不佳，而结合cisformer和extensive attention的模型能达到约95%的准确率。

Conclusion: 结论表明通过比较不同注意力机制在任务切换中的表现，可以更好地理解并改进注意力机制的设计。

Abstract: The rapid progress seen in terms of large-scale generative AI is largely
based on the attention mechanism. It is conversely non-trivial to conceive
small-scale applications for which attention-based architectures outperform
traditional approaches, such as multi-layer perceptrons or recurrent networks.
We examine this problem in the context of 'task switching'. In this framework
models work on ongoing token sequences with the current task being determined
by stochastically interspersed control tokens. We show that standard
transformers cannot solve a basic task switching reference model based on
finite domain arithmetics which contains subtasks dedicated to increment /
addition / reverse copy / context (IARC). We show that transformers, long
short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons
(MLPs) achieve similar, but only modest prediction accuracies. We enlarge our
comparative study by including an extension of the standard transformer
architecture to its non-translational invariant counterpart, the cisformer, and
an alternative attention mechanism, extensive attention. A combination of the
latter is found to be the only model able to achieve considerable performance
levels, of around 95%. Our results indicate that the workings of attention can
be understood better, and even improved, when comparing qualitatively different
formulations in task-switching settings.

</details>


### [81] [CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large Language Model Inference](https://arxiv.org/abs/2508.04462)
*Enyu Zhou,Kai Sheng,Hao Chen,Xin He*

Main category: cs.LG

TL;DR: 提出了一种基于缓存的并行推测解码框架CARD，通过‘查询-修正’范式解耦起草和验证过程，显著加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法因‘起草-验证’范式导致推理效率低，且起草模型规模受限。

Method: CARD框架中，起草模型生成候选令牌填充共享缓存，目标模型并行修正生成方向。

Result: 实现了4.83倍加速，无需微调起草或目标模型。

Conclusion: CARD通过并行化显著提升了推理效率，为LLM加速提供了新思路。

Abstract: Speculative decoding (SD), where an extra draft model first provides multiple
draft tokens and the original target model then verifies these tokens in
parallel, has shown great power for LLM inference acceleration. However,
existing SD methods must adhere to the 'draft-then-verify' paradigm, which
forces drafting and verification processes to execute sequentially during SD,
resulting in inefficient inference performance and limiting the size of the
draft model. Furthermore, once a single token in the candidate sequence is
rejected during the drafting process, all subsequent candidate tokens must be
discarded, leading to inefficient drafting. To address these challenges, we
propose a cache-based parallel speculative decoding framework employing a
'query-and-correct' paradigm. Specifically, CARD decouples drafting and
verification: the draft model generates candidate tokens to populate a shared
cache, while the target model concurrently rectifies the draft model's
generation direction. This effectively enables the target model to perform
inference at speed approaching that of the draft model. Our approach achieves
up to 4.83 speedup over vanilla decoding without requiring fine-tuning of
either the draft or target models. Our code is available at
https://github.com/hunzhizi/CARD.

</details>


### [82] [GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries](https://arxiv.org/abs/2508.04463)
*Fangzhi Fei,Jiaxin Hu,Qiaofeng Li,Zhenyu Liu*

Main category: cs.LG

TL;DR: GFocal是一种基于Transformer的神经算子方法，通过同时学习全局和局部特征，解决了多尺度问题，提升了物理一致性和数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了局部物理细节与全局特征之间的协同学习，而这对多尺度问题、物理一致性和数值稳定性至关重要。

Method: GFocal利用Nyström注意力机制和切片机制分别捕获全局和局部特征，并通过卷积门控块动态融合多尺度信息。

Result: GFocal在六个基准测试中的五个表现最优，平均相对增益为15.2%，并在工业级模拟中表现优异。

Conclusion: GFocal通过全局和局部特征的协同学习，显著提升了物理建模和预测的准确性。

Abstract: Transformer-based neural operators have emerged as promising surrogate
solvers for partial differential equations, by leveraging the effectiveness of
Transformers for capturing long-range dependencies and global correlations,
profoundly proven in language modeling. However, existing methodologies
overlook the coordinated learning of interdependencies between local physical
details and global features, which are essential for tackling multiscale
problems, preserving physical consistency and numerical stability in long-term
rollouts, and accurately capturing transitional dynamics. In this work, we
propose GFocal, a Transformer-based neural operator method that enforces
simultaneous global and local feature learning and fusion. Global correlations
and local features are harnessed through Nystr\"{o}m attention-based
\textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate
physics-aware tokens, subsequently modulated and integrated via
convolution-based gating blocks, enabling dynamic fusion of multiscale
information. GFocal achieves accurate modeling and prediction of physical
features given arbitrary geometries and initial conditions. Experiments show
that GFocal achieves state-of-the-art performance with an average 15.2\%
relative gain in five out of six benchmarks and also excels in industry-scale
simulations such as aerodynamics simulation of automotives and airfoils.

</details>


### [83] [FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions](https://arxiv.org/abs/2508.04470)
*Jianheng Tang,Zhirui Yang,Jingchao Wang,Kejia Fan,Jinfeng Xu,Huiping Zhuang,Anfeng Liu,Houbing Herbert Song,Leye Wang,Yunhuai Liu*

Main category: cs.LG

TL;DR: 提出了一种名为FedHiP的个性化联邦学习方案，通过解析解避免梯度更新，解决了非独立同分布数据带来的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法因数据异构性（非IID数据）导致收敛困难和性能下降，根源在于梯度更新的敏感性。

Method: 利用自监督预训练的基础模型进行特征提取，开发解析分类器，分为局部训练、全局聚合和局部个性化三个阶段。

Result: 在基准数据集上，FedHiP方案准确率比现有最优方法提高了5.79%-20.97%。

Conclusion: FedHiP通过解析解实现异构不变性，显著提升了个性化联邦学习的性能。

Abstract: Lately, Personalized Federated Learning (PFL) has emerged as a prevalent
paradigm to deliver personalized models by collaboratively training while
simultaneously adapting to each client's local applications. Existing PFL
methods typically face a significant challenge due to the ubiquitous data
heterogeneity (i.e., non-IID data) across clients, which severely hinders
convergence and degrades performance. We identify that the root issue lies in
the long-standing reliance on gradient-based updates, which are inherently
sensitive to non-IID data. To fundamentally address this issue and bridge the
research gap, in this paper, we propose a Heterogeneity-invariant Personalized
Federated learning scheme, named FedHiP, through analytical (i.e., closed-form)
solutions to avoid gradient-based updates. Specifically, we exploit the trend
of self-supervised pre-training, leveraging a foundation model as a frozen
backbone for gradient-free feature extraction. Following the feature extractor,
we further develop an analytic classifier for gradient-free training. To
support both collective generalization and individual personalization, our
FedHiP scheme incorporates three phases: analytic local training, analytic
global aggregation, and analytic local personalization. The closed-form
solutions of our FedHiP scheme enable its ideal property of heterogeneity
invariance, meaning that each personalized model remains identical regardless
of how non-IID the data are distributed across all other clients. Extensive
experiments on benchmark datasets validate the superiority of our FedHiP
scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97%
in accuracy.

</details>


### [84] [Who cuts emissions, who turns up the heat? causal machine learning estimates of energy efficiency interventions](https://arxiv.org/abs/2508.04478)
*Bernardino D'Amico,Francesco Pomponi,Jay H. Arehart,Lina Khaddour*

Main category: cs.LG

TL;DR: 研究发现，墙体保温措施平均可减少19%的天然气需求，但效果因家庭能源负担不同而异。高能源负担家庭几乎无节省，因其将节省用于改善热舒适度。


<details>
  <summary>Details</summary>
Motivation: 探讨墙体保温对天然气需求的影响，特别关注不同能源负担家庭的分布效应。

Method: 使用因果机器学习模型分析英国代表性住房数据。

Result: 低能源负担家庭节省显著，高能源负担家庭几乎无节省，因其将节省用于改善舒适度。

Conclusion: 需综合考虑气候影响和能源政策的公平性。

Abstract: Reducing domestic energy demand is central to climate mitigation and fuel
poverty strategies, yet the impact of energy efficiency interventions is highly
heterogeneous. Using a causal machine learning model trained on nationally
representative data of the English housing stock, we estimate average and
conditional treatment effects of wall insulation on gas consumption, focusing
on distributional effects across energy burden subgroups. While interventions
reduce gas demand on average (by as much as 19 percent), low energy burden
groups achieve substantial savings, whereas those experiencing high energy
burdens see little to no reduction. This pattern reflects a
behaviourally-driven mechanism: households constrained by high costs-to-income
ratios (e.g. more than 0.1) reallocate savings toward improved thermal comfort
rather than lowering consumption. Far from wasteful, such responses represent
rational adjustments in contexts of prior deprivation, with potential
co-benefits for health and well-being. These findings call for a broader
evaluation framework that accounts for both climate impacts and the equity
implications of domestic energy policy.

</details>


### [85] [Emotion Detection Using Conditional Generative Adversarial Networks (cGAN): A Deep Learning Approach](https://arxiv.org/abs/2508.04481)
*Anushka Srivastava*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的情绪检测方法，利用条件生成对抗网络（cGANs）和多模态数据（文本、音频和面部表情），显著提升了情绪识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统单模态情绪检测方法局限性明显，研究旨在通过多模态数据和cGANs提升情绪识别的准确性和鲁棒性。

Method: 采用条件生成对抗网络（cGANs）框架，整合文本、音频和面部表情数据，生成合成情绪数据并优化分类器。

Result: 实验结果表明，该方法在情绪识别性能上显著优于基线模型。

Conclusion: cGANs在多模态情绪检测中具有潜力，可提升人机交互系统的情感理解能力。

Abstract: This paper presents a deep learning-based approach to emotion detection using
Conditional Generative Adversarial Networks (cGANs). Unlike traditional
unimodal techniques that rely on a single data type, we explore a multimodal
framework integrating text, audio, and facial expressions. The proposed cGAN
architecture is trained to generate synthetic emotion-rich data and improve
classification accuracy across multiple modalities. Our experimental results
demonstrate significant improvements in emotion recognition performance
compared to baseline models. This work highlights the potential of cGANs in
enhancing human-computer interaction systems by enabling more nuanced emotional
understanding.

</details>


### [86] [Hierarchical Scoring for Machine Learning Classifier Error Impact Evaluation](https://arxiv.org/abs/2508.04489)
*Erin Lanus,Daniel Wolodkin,Laura J. Freeman*

Main category: cs.LG

TL;DR: 论文提出了一种基于层次结构的评分指标，用于更细粒度地评估机器学习模型的分类和物体检测性能，而不仅仅是简单的通过/失败评分。


<details>
  <summary>Details</summary>
Motivation: 传统的分类和物体检测评估方法将所有错误分类视为等同，忽略了类别之间的层次关系。作者希望通过层次评分指标更准确地反映错误分类的影响。

Method: 开发了多种复杂度的层次评分指标，利用评分树编码类别关系，并根据预测与真实标签在树中的距离生成评分。

Result: 实验表明，这些指标能更细粒度地捕捉错误，且评分树支持调整，从而根据错误的类型或影响对模型进行排名。

Conclusion: 层次评分指标提供了一种更精细的模型评估方法，能够根据错误的类型和影响对模型进行更全面的评价。

Abstract: A common use of machine learning (ML) models is predicting the class of a
sample. Object detection is an extension of classification that includes
localization of the object via a bounding box within the sample.
Classification, and by extension object detection, is typically evaluated by
counting a prediction as incorrect if the predicted label does not match the
ground truth label. This pass/fail scoring treats all misclassifications as
equivalent. In many cases, class labels can be organized into a class taxonomy
with a hierarchical structure to either reflect relationships among the data or
operator valuation of misclassifications. When such a hierarchical structure
exists, hierarchical scoring metrics can return the model performance of a
given prediction related to the distance between the prediction and the ground
truth label. Such metrics can be viewed as giving partial credit to predictions
instead of pass/fail, enabling a finer-grained understanding of the impact of
misclassifications. This work develops hierarchical scoring metrics varying in
complexity that utilize scoring trees to encode relationships between class
labels and produce metrics that reflect distance in the scoring tree. The
scoring metrics are demonstrated on an abstract use case with scoring trees
that represent three weighting strategies and evaluated by the kind of errors
discouraged. Results demonstrate that these metrics capture errors with finer
granularity and the scoring trees enable tuning. This work demonstrates an
approach to evaluating ML performance that ranks models not only by how many
errors are made but by the kind or impact of errors. Python implementations of
the scoring metrics will be available in an open-source repository at time of
publication.

</details>


### [87] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 论文提出Causal Reflection框架，通过显式建模因果关系，增强LLMs和强化学习代理的因果推理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs和传统强化学习代理在因果推理上表现不足，依赖虚假相关性和脆弱模式。

Method: 引入Causal Reflection框架，动态建模因果关系，并定义Reflect机制以修正内部模型。

Result: 框架为因果反思代理奠定理论基础，使其能适应、自我修正并解释因果理解。

Conclusion: Causal Reflection为LLMs和强化学习代理提供了更强的因果推理能力。

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


### [88] [PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers](https://arxiv.org/abs/2508.04503)
*Federico Zucchi,Thomas Lampert*

Main category: cs.LG

TL;DR: PRISM是一种基于卷积的特征提取器，通过多分辨率、每通道设计，显著减少模型参数和计算量，同时保持或超越现有CNN和Transformer模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer和CNN模型在多元时间序列分类中计算量大、频率多样性有限以及参数需求高的问题。

Method: 提出PRISM模块，使用对称有限脉冲响应（FIR）滤波器在多个时间尺度上独立处理每个通道，避免通道间卷积。

Result: 在人类活动、睡眠阶段和生物医学基准测试中，PRISM性能与领先的CNN和Transformer模型相当或更好，同时参数和计算量减少约一个数量级。

Conclusion: PRISM结合经典信号处理和现代深度学习，为多元时间序列分类提供了高效且准确的解决方案。

Abstract: Multivariate time-series classification is pivotal in domains ranging from
wearable sensing to biomedical monitoring. Despite recent advances,
Transformer- and CNN-based models often remain computationally heavy, offer
limited frequency diversity, and require extensive parameter budgets. We
propose PRISM (Per-channel Resolution-Informed Symmetric Module), a
convolutional-based feature extractor that applies symmetric
finite-impulse-response (FIR) filters at multiple temporal scales,
independently per channel. This multi-resolution, per-channel design yields
highly frequency-selective embeddings without any inter-channel convolutions,
greatly reducing model size and complexity. Across human-activity, sleep-stage
and biomedical benchmarks, PRISM, paired with lightweight classification heads,
matches or outperforms leading CNN and Transformer baselines, while using
roughly an order of magnitude fewer parameters and FLOPs. By uniting classical
signal processing insights with modern deep learning, PRISM offers an accurate,
resource-efficient solution for multivariate time-series classification.

</details>


### [89] [Channel-Independent Federated Traffic Prediction](https://arxiv.org/abs/2508.04517)
*Mo Zhang,Xiaoyu Li,Bin Xu,Meng Chen,Yongshun Gong*

Main category: cs.LG

TL;DR: 提出了一种新的联邦交通预测方法Fed-CI，通过独立通道建模范式（CIP）减少通信开销，提高预测效率。


<details>
  <summary>Details</summary>
Motivation: 交通数据分散且隐私受限，现有联邦学习方法通信开销大，训练速度慢。

Method: 采用独立通道建模范式（CIP），无需客户端间通信，仅用本地数据进行高效预测。

Result: Fed-CI显著降低通信成本，加速训练，并在多个数据集上表现优于现有方法，RMSE、MAE和MAPE分别提升8%、14%和16%。

Conclusion: Fed-CI是一种高效、隐私合规的联邦交通预测框架，解决了现有方法的通信和效率问题。

Abstract: In recent years, traffic prediction has achieved remarkable success and has
become an integral component of intelligent transportation systems. However,
traffic data is typically distributed among multiple data owners, and privacy
constraints prevent the direct utilization of these isolated datasets for
traffic prediction. Most existing federated traffic prediction methods focus on
designing communication mechanisms that allow models to leverage information
from other clients in order to improve prediction accuracy. Unfortunately, such
approaches often incur substantial communication overhead, and the resulting
transmission delays significantly slow down the training process. As the volume
of traffic data continues to grow, this issue becomes increasingly critical,
making the resource consumption of current methods unsustainable. To address
this challenge, we propose a novel variable relationship modeling paradigm for
federated traffic prediction, termed the Channel-Independent Paradigm(CIP).
Unlike traditional approaches, CIP eliminates the need for inter-client
communication by enabling each node to perform efficient and accurate
predictions using only local information. Based on the CIP, we further develop
Fed-CI, an efficient federated learning framework, allowing each client to
process its own data independently while effectively mitigating the information
loss caused by the lack of direct data sharing among clients. Fed-CI
significantly reduces communication overhead, accelerates the training process,
and achieves state-of-the-art performance while complying with privacy
regulations. Extensive experiments on multiple real-world datasets demonstrate
that Fed-CI consistently outperforms existing methods across all datasets and
federated settings. It achieves improvements of 8%, 14%, and 16% in RMSE, MAE,
and MAPE, respectively, while also substantially reducing communication costs.

</details>


### [90] [Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape](https://arxiv.org/abs/2508.04542)
*Haoran Niu,K. Suzanne Barber*

Main category: cs.LG

TL;DR: 研究通过分析5000多个身份盗窃和欺诈案例，构建了一个基于图的身份生态系统模型，用于预测个人隐私风险。


<details>
  <summary>Details</summary>
Motivation: 个人和组织缺乏对隐私风险的基本理解，难以有效保护个人信息。

Method: 构建身份生态系统图模型，利用图论和图神经网络预测隐私风险。

Result: 模型能有效预测某一身份属性泄露是否可能导致其他属性泄露。

Conclusion: 该研究为隐私风险评估提供了新方法，有助于更有效地保护个人信息。

Abstract: It is difficult for individuals and organizations to protect personal
information without a fundamental understanding of relative privacy risks. By
analyzing over 5,000 empirical identity theft and fraud cases, this research
identifies which types of personal data are exposed, how frequently exposures
occur, and what the consequences of those exposures are. We construct an
Identity Ecosystem graph--a foundational, graph-based model in which nodes
represent personally identifiable information (PII) attributes and edges
represent empirical disclosure relationships between them (e.g., the
probability that one PII attribute is exposed due to the exposure of another).
Leveraging this graph structure, we develop a privacy risk prediction framework
that uses graph theory and graph neural networks to estimate the likelihood of
further disclosures when certain PII attributes are compromised. The results
show that our approach effectively answers the core question: Can the
disclosure of a given identity attribute possibly lead to the disclosure of
another attribute?

</details>


### [91] [GraphProp: Training the Graph Foundation Models using Graph Properties](https://arxiv.org/abs/2508.04594)
*Ziheng Sun,Qi Feng,Lehao Lin,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 本文提出GraphProp，一种强调结构泛化的图基础模型（GFM），通过两阶段训练提升跨域图分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GFM主要关注节点特征的跨域泛化，但缺乏结构信息的跨域一致性。本文发现图结构比节点特征和标签更能提供一致的跨域信息。

Method: GraphProp分为两阶段：1）通过预测图不变量训练结构GFM，捕获抽象结构信息；2）利用结构GFM的表示作为位置编码，结合节点属性和标签训练综合GFM。

Result: 实验表明，GraphProp在监督学习和少样本学习中显著优于竞争对手，尤其是在处理无节点属性的图时。

Conclusion: GraphProp通过结构泛化和两阶段训练，有效提升了跨域图分类任务的性能。

Abstract: This work focuses on training graph foundation models (GFMs) that have strong
generalization ability in graph-level tasks such as graph classification.
Effective GFM training requires capturing information consistent across
different domains. We discover that graph structures provide more consistent
cross-domain information compared to node features and graph labels. However,
traditional GFMs primarily focus on transferring node features from various
domains into a unified representation space but often lack structural
cross-domain generalization. To address this, we introduce GraphProp, which
emphasizes structural generalization. The training process of GraphProp
consists of two main phases. First, we train a structural GFM by predicting
graph invariants. Since graph invariants are properties of graphs that depend
only on the abstract structure, not on particular labellings or drawings of the
graph, this structural GFM has a strong ability to capture the abstract
structural information and provide discriminative graph representations
comparable across diverse domains. In the second phase, we use the
representations given by the structural GFM as positional encodings to train a
comprehensive GFM. This phase utilizes domain-specific node attributes and
graph labels to further improve cross-domain node feature generalization. Our
experiments demonstrate that GraphProp significantly outperforms the
competitors in supervised learning and few-shot learning, especially in
handling graphs without node attributes.

</details>


### [92] [Improved Training Strategies for Physics-Informed Neural Networks using Real Experimental Data in Aluminum Spot Welding](https://arxiv.org/abs/2508.04595)
*Jan A. Zak,Christian Weißenfels*

Main category: cs.LG

TL;DR: 论文探讨了在汽车工业中，利用物理信息神经网络（PINN）非破坏性地预测铝点焊的焊核直径，提出了两种新的训练策略以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统电阻点焊的质量检测依赖破坏性测试，效率低下。研究旨在通过PINN实现非破坏性、高效的焊核直径预测。

Method: 提出两种训练策略：1）渐进式引入实验损失函数，避免优化冲突；2）条件更新温度相关材料参数。使用二维轴对称模型提高计算效率。

Result: 二维网络成功预测动态位移和焊核生长，结果在实验置信区间内，并支持从钢到铝的焊接阶段转移。

Conclusion: 该方法展示了在工业应用中快速、基于模型的质量控制的潜力。

Abstract: Resistance spot welding is the dominant joining process for the body-in-white
in the automotive industry, where the weld nugget diameter is the key quality
metric. Its measurement requires destructive testing, limiting the potential
for efficient quality control. Physics-informed neural networks were
investigated as a promising tool to reconstruct internal process states from
experimental data, enabling model-based and non-invasive quality assessment in
aluminum spot welding. A major challenge is the integration of real-world data
into the network due to competing optimization objectives. To address this, we
introduce two novel training strategies. First, experimental losses for dynamic
displacement and nugget diameter are progressively included using a fading-in
function to prevent excessive optimization conflicts. We also implement a
custom learning rate scheduler and early stopping based on a rolling window to
counteract premature reduction due to increased loss magnitudes. Second, we
introduce a conditional update of temperature-dependent material parameters via
a look-up table, activated only after a loss threshold is reached to ensure
physically meaningful temperatures. An axially symmetric two-dimensional model
was selected to represent the welding process accurately while maintaining
computational efficiency. To reduce computational burden, the training
strategies and model components were first systematically evaluated in one
dimension, enabling controlled analysis of loss design and contact models. The
two-dimensional network predicts dynamic displacement and nugget growth within
the experimental confidence interval, supports transferring welding stages from
steel to aluminum, and demonstrates strong potential for fast, model-based
quality control in industrial applications.

</details>


### [93] [Multitask Learning with Stochastic Interpolants](https://arxiv.org/abs/2508.04605)
*Hugo Negrel,Florentin Coeurdoux,Michael S. Albergo,Eric Vanden-Eijnden*

Main category: cs.LG

TL;DR: 提出了一种广义的概率分布映射学习框架，扩展了流和扩散模型的时间动态。通过用向量、矩阵或线性算子替换标量时间变量，实现了跨多维空间的概率分布桥接。


<details>
  <summary>Details</summary>
Motivation: 为现有生成模型提供统一的理论视角，并扩展其能力，同时实现无需任务特定训练的多任务生成。

Method: 采用基于算子的插值方法，将标量时间变量推广为向量、矩阵或线性算子。

Result: 数值实验展示了该方法在条件生成、修复、微调、后验采样和多尺度建模中的零样本效果。

Conclusion: 该方法有望成为任务无关的通用替代方案，适用于多种生成任务。

Abstract: We propose a framework for learning maps between probability distributions
that broadly generalizes the time dynamics of flow and diffusion models. To
enable this, we generalize stochastic interpolants by replacing the scalar time
variable with vectors, matrices, or linear operators, allowing us to bridge
probability distributions across multiple dimensional spaces. This approach
enables the construction of versatile generative models capable of fulfilling
multiple tasks without task-specific training. Our operator-based interpolants
not only provide a unifying theoretical perspective for existing generative
models but also extend their capabilities. Through numerical experiments, we
demonstrate the zero-shot efficacy of our method on conditional generation and
inpainting, fine-tuning and posterior sampling, and multiscale modeling,
suggesting its potential as a generic task-agnostic alternative to specialized
models.

</details>


### [94] [Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning](https://arxiv.org/abs/2508.04610)
*Md Zesun Ahmed Mia,Malyaban Bal,Sen Lu,George M. Nishibuchi,Suhas Chelian,Srini Vasan,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的终身网络入侵检测系统（NIDS），结合静态和动态SNN，通过生物启发的机制实现高效学习和低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 受大脑分层处理和能量效率启发，设计一种能够持续学习新威胁并保持现有知识的NIDS。

Method: 使用静态SNN初步检测入侵，动态SNN分类攻击类型，结合GWR结构可塑性和Ad-STDP学习规则。

Result: 在UNSW-NB15基准测试中达到85.3%准确率，展示了强适应性和低灾难性遗忘。

Conclusion: 该架构在持续学习环境中表现优异，适合低功耗神经形态硬件部署。

Abstract: Inspired by the brain's hierarchical processing and energy efficiency, this
paper presents a Spiking Neural Network (SNN) architecture for lifelong Network
Intrusion Detection System (NIDS). The proposed system first employs an
efficient static SNN to identify potential intrusions, which then activates an
adaptive dynamic SNN responsible for classifying the specific attack type.
Mimicking biological adaptation, the dynamic classifier utilizes Grow When
Required (GWR)-inspired structural plasticity and a novel Adaptive
Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible
mechanisms enable the network to learn new threats incrementally while
preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual
learning setting, the architecture demonstrates robust adaptation, reduced
catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore,
simulations using the Intel Lava framework confirm high operational sparsity,
highlighting the potential for low-power deployment on neuromorphic hardware.

</details>


### [95] [CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series](https://arxiv.org/abs/2508.04630)
*Yutong Xia,Yingying Zhang,Yuxuan Liang,Lunting Fan,Qingsong Wen,Roger Zimmermann*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果关系的框架CaPulse，用于时间序列异常检测，通过结构因果模型和周期性归一化流解决数据稀缺性和复杂周期性问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉时间序列异常生成的机制，且面临标签稀缺、数据不平衡和复杂周期性等挑战。

Method: 构建结构因果模型解析异常生成过程，提出周期性归一化流和掩码机制，设计周期性学习器。

Result: 在七个真实数据集上，CaPulse的AUROC提升了3%至17%，且具有更好的可解释性。

Conclusion: CaPulse通过因果工具和周期性建模，有效解决了时间序列异常检测的挑战，性能显著优于现有方法。

Abstract: Time series anomaly detection has garnered considerable attention across
diverse domains. While existing methods often fail to capture the underlying
mechanisms behind anomaly generation in time series data. In addition, time
series anomaly detection often faces several data-related inherent challenges,
i.e., label scarcity, data imbalance, and complex multi-periodicity. In this
paper, we leverage causal tools and introduce a new causality-based framework,
CaPulse, which tunes in to the underlying causal pulse of time series data to
effectively detect anomalies. Concretely, we begin by building a structural
causal model to decipher the generation processes behind anomalies. To tackle
the challenges posed by the data, we propose Periodical Normalizing Flows with
a novel mask mechanism and carefully designed periodical learners, creating a
periodicity-aware, density-based anomaly detection approach. Extensive
experiments on seven real-world datasets demonstrate that CaPulse consistently
outperforms existing methods, achieving AUROC improvements of 3% to 17%, with
enhanced interpretability.

</details>


### [96] [A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation](https://arxiv.org/abs/2508.04645)
*Yu Song,Zhigang Hua,Harry Shomer,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于预训练的链接预测方法，通过融合节点和边缘信息，并引入混合专家框架和参数高效调优策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在链接预测任务中存在监督稀疏、初始化敏感和泛化能力差的问题，预训练被探索为解决这些挑战的方案。

Method: 采用节点和边缘信息的后期融合策略，引入混合专家框架处理数据多样性，并开发参数高效调优策略。

Result: 在16个数据集上验证了方法的有效性，实现了低资源链接预测的最优性能，计算开销降低超过10,000倍。

Conclusion: 预训练结合混合专家框架和高效调优策略，显著提升了链接预测的性能和效率。

Abstract: Link Prediction (LP) is a critical task in graph machine learning. While
Graph Neural Networks (GNNs) have significantly advanced LP performance
recently, existing methods face key challenges including limited supervision
from sparse connectivity, sensitivity to initialization, and poor
generalization under distribution shifts. We explore pretraining as a solution
to address these challenges. Unlike node classification, LP is inherently a
pairwise task, which requires the integration of both node- and edge-level
information. In this work, we present the first systematic study on the
transferability of these distinct modules and propose a late fusion strategy to
effectively combine their outputs for improved performance. To handle the
diversity of pretraining data and avoid negative transfer, we introduce a
Mixture-of-Experts (MoE) framework that captures distinct patterns in separate
experts, facilitating seamless application of the pretrained model on diverse
downstream datasets. For fast adaptation, we develop a parameter-efficient
tuning strategy that allows the pretrained model to adapt to unseen datasets
with minimal computational overhead. Experiments on 16 datasets across two
domains demonstrate the effectiveness of our approach, achieving
state-of-the-art performance on low-resource link prediction while obtaining
competitive results compared to end-to-end trained methods, with over 10,000x
lower computational overhead.

</details>


### [97] [Perch 2.0: The Bittern Lesson for Bioacoustics](https://arxiv.org/abs/2508.04665)
*Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Lauren Harrell,Andrea Burns,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0是一个高性能的生物声学预训练模型，通过多分类任务和自蒸馏技术提升性能，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩展Perch模型的应用范围，从仅针对鸟类扩展到多类生物，同时探索细粒度物种分类作为生物声学预训练任务的潜力。

Method: 使用自蒸馏技术和原型学习分类器，结合新的源预测训练准则进行训练。

Result: 在BirdSet和BEANS基准测试中达到最优性能，并在海洋迁移学习任务中超越专用模型。

Conclusion: 细粒度物种分类是生物声学预训练的强有力任务，Perch 2.0展示了其广泛适用性和高性能。

Abstract: Perch is a performant pre-trained model for bioacoustics. It was trained in
supervised fashion, providing both off-the-shelf classification scores for
thousands of vocalizing species as well as strong embeddings for transfer
learning. In this new release, Perch 2.0, we expand from training exclusively
on avian species to a large multi-taxa dataset. The model is trained with
self-distillation using a prototype-learning classifier as well as a new
source-prediction training criterion. Perch 2.0 obtains state-of-the-art
performance on the BirdSet and BEANS benchmarks. It also outperforms
specialized marine models on marine transfer learning tasks, despite having
almost no marine training data. We present hypotheses as to why fine-grained
species classification is a particularly robust pre-training task for
bioacoustics.

</details>


### [98] [Robustly Learning Monotone Single-Index Models](https://arxiv.org/abs/2508.04670)
*Puqian Wang,Nikos Zarifis,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 提出了一种高效算法，用于在高斯分布下学习单指数模型，并在对抗性标签噪声下实现常数因子近似。


<details>
  <summary>Details</summary>
Motivation: 解决在对抗性标签噪声下学习单指数模型的问题，填补了现有方法在激活函数范围和近似效果上的不足。

Method: 开发了一种优化框架，通过直接利用问题结构、高斯空间性质和单调函数规律性，识别有用的向量场来指导算法更新。

Result: 算法适用于所有具有有界矩的单调激活函数，包括单调Lipschitz函数和不连续函数（如半空间）。

Conclusion: 该算法首次实现了对广泛单调激活函数的高效学习，并在对抗性噪声下保持常数因子近似。

Abstract: We consider the basic problem of learning Single-Index Models with respect to
the square loss under the Gaussian distribution in the presence of adversarial
label noise. Our main contribution is the first computationally efficient
algorithm for this learning task, achieving a constant factor approximation,
that succeeds for the class of {\em all} monotone activations with bounded
moment of order $2 + \zeta,$ for $\zeta > 0.$ This class in particular includes
all monotone Lipschitz functions and even discontinuous functions like
(possibly biased) halfspaces. Prior work for the case of unknown activation
either does not attain constant factor approximation or succeeds for a
substantially smaller family of activations. The main conceptual novelty of our
approach lies in developing an optimization framework that steps outside the
boundaries of usual gradient methods and instead identifies a useful vector
field to guide the algorithm updates by directly leveraging the problem
structure, properties of Gaussian spaces, and regularity of monotone functions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [99] [Reliable Programmatic Weak Supervision with Confidence Intervals for Label Probabilities](https://arxiv.org/abs/2508.03896)
*Verónica Álvarez,Santiago Mazuelas,Steven An,Sanjoy Dasgupta*

Main category: stat.ML

TL;DR: 提出了一种新的弱监督学习方法，通过不确定性分布集合提供标签概率的置信区间，提高预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 数据集标注成本高且耗时，现有弱监督方法无法评估标签概率的可靠性。

Method: 利用不确定性分布集合封装弱标签函数的信息，提供置信区间。

Result: 在多个基准数据集上优于现有方法，置信区间实用性强。

Conclusion: 新方法显著提升了弱监督学习的可靠性和实用性。

Abstract: The accurate labeling of datasets is often both costly and time-consuming.
Given an unlabeled dataset, programmatic weak supervision obtains probabilistic
predictions for the labels by leveraging multiple weak labeling functions (LFs)
that provide rough guesses for labels. Weak LFs commonly provide guesses with
assorted types and unknown interdependences that can result in unreliable
predictions. Furthermore, existing techniques for programmatic weak supervision
cannot provide assessments for the reliability of the probabilistic predictions
for labels. This paper presents a methodology for programmatic weak supervision
that can provide confidence intervals for label probabilities and obtain more
reliable predictions. In particular, the methods proposed use uncertainty sets
of distributions that encapsulate the information provided by LFs with
unrestricted behavior and typology. Experiments on multiple benchmark datasets
show the improvement of the presented methods over the state-of-the-art and the
practicality of the confidence intervals presented.

</details>


### [100] [Reinforcement Learning in MDPs with Information-Ordered Policies](https://arxiv.org/abs/2508.03904)
*Zhongjun Zhang,Shipra Agrawal,Ilan Lobel,Sean R. Sinclair,Christina Lee Yu*

Main category: stat.ML

TL;DR: 提出一种基于策略类偏序的强化学习算法，用于无限时间平均成本MDP，实现无需额外环境交互的反事实推断。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在状态和动作空间较大时的计算复杂性问题，同时避免对问题结构（如凸性或特定到达率）的额外假设。

Method: 利用策略类上的偏序关系，通过已有数据估计其他策略性能，设计基于epoch的算法。

Result: 获得与状态和动作空间无关的遗憾界O(√(w log(|Θ|)T))，其中w为偏序宽度。

Conclusion: 算法在库存控制和排队系统等领域具有广泛应用，提供新的理论保证和实证效果。

Abstract: We propose an epoch-based reinforcement learning algorithm for
infinite-horizon average-cost Markov decision processes (MDPs) that leverages a
partial order over a policy class. In this structure, $\pi' \leq \pi$ if data
collected under $\pi$ can be used to estimate the performance of $\pi'$,
enabling counterfactual inference without additional environment interaction.
Leveraging this partial order, we show that our algorithm achieves a regret
bound of $O(\sqrt{w \log(|\Theta|) T})$, where $w$ is the width of the partial
order. Notably, the bound is independent of the state and action space sizes.
We illustrate the applicability of these partial orders in many domains in
operations research, including inventory control and queuing systems. For each,
we apply our framework to that problem, yielding new theoretical guarantees and
strong empirical results without imposing extra assumptions such as convexity
in the inventory model or specialized arrival-rate structure in the queuing
model.

</details>


### [101] [Negative binomial regression and inference using a pre-trained transformer](https://arxiv.org/abs/2508.04111)
*Valentine Svensson*

Main category: stat.ML

TL;DR: 论文研究了使用预训练Transformer估计负二项回归参数，发现矩估计法在速度和准确性上优于最大似然估计和Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 负二项回归在大规模比较研究中计算成本高，需高效替代方法。

Method: 通过合成数据训练Transformer，学习从计数数据反推参数，并与最大似然和矩估计法比较。

Result: Transformer比最大似然快20倍且更准确，但矩估计法比最大似然快1000倍且测试效果更好。

Conclusion: 矩估计法是最优解决方案，兼具高效性和准确性。

Abstract: Negative binomial regression is essential for analyzing over-dispersed count
data in in comparative studies, but parameter estimation becomes
computationally challenging in large screens requiring millions of comparisons.
We investigate using a pre-trained transformer to produce estimates of negative
binomial regression parameters from observed count data, trained through
synthetic data generation to learn to invert the process of generating counts
from parameters. The transformer method achieved better parameter accuracy than
maximum likelihood optimization while being 20 times faster. However,
comparisons unexpectedly revealed that method of moment estimates performed as
well as maximum likelihood optimization in accuracy, while being 1,000 times
faster and producing better-calibrated and more powerful tests, making it the
most efficient solution for this application.

</details>


### [102] [Deep Neural Network-Driven Adaptive Filtering](https://arxiv.org/abs/2508.04258)
*Qizhen Wang,Gang Wang,Ying-Chang Liang*

Main category: stat.ML

TL;DR: 提出了一种基于深度神经网络的框架，通过直接梯度获取解决自适应滤波的泛化挑战。


<details>
  <summary>Details</summary>
Motivation: 传统自适应滤波框架依赖显式成本函数设计，难以泛化，本文旨在通过数据驱动方法提升泛化能力。

Method: 将深度神经网络作为非线性算子嵌入滤波系统，建立残差与梯度的直接映射，采用最大似然作为隐式成本函数。

Result: 实验验证了该框架在非高斯场景下的优异泛化能力，并进行了均值和均方稳定性分析。

Conclusion: 该框架通过数据驱动方式显著提升了自适应滤波的泛化性能。

Abstract: This paper proposes a deep neural network (DNN)-driven framework to address
the longstanding generalization challenge in adaptive filtering (AF). In
contrast to traditional AF frameworks that emphasize explicit cost function
design, the proposed framework shifts the paradigm toward direct gradient
acquisition. The DNN, functioning as a universal nonlinear operator, is
structurally embedded into the core architecture of the AF system, establishing
a direct mapping between filtering residuals and learning gradients. The
maximum likelihood is adopted as the implicit cost function, rendering the
derived algorithm inherently data-driven and thus endowed with exemplary
generalization capability, which is validated by extensive numerical
experiments across a spectrum of non-Gaussian scenarios. Corresponding mean
value and mean square stability analyses are also conducted in detail.

</details>


### [103] [The Relative Instability of Model Comparison with Cross-validation](https://arxiv.org/abs/2508.04409)
*Alexandre Bayle,Lucas Janson,Lester Mackey*

Main category: stat.ML

TL;DR: 本文探讨了交叉验证（CV）在比较两种机器学习算法时的局限性，尤其是相对稳定性概念的缺失，并通过理论和实验验证了CV置信区间在评估算法性能差异时的无效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解相对稳定性概念，并确定CV在比较两种算法时是否能提供有效的置信区间。

Method: 研究方法包括理论分析和实验验证，重点关注软阈值最小二乘算法及其与Lasso的比较。

Result: 结果表明，尽管单个算法在测试误差评估中具有稳定性，但在比较两种算法时，相对稳定性不成立，CV置信区间无效。

Conclusion: 结论指出，即使在算法个体稳定的情况下，使用CV量化两种算法性能差异的不确定性仍需谨慎。

Abstract: Existing work has shown that cross-validation (CV) can be used to provide an
asymptotic confidence interval for the test error of a stable machine learning
algorithm, and existing stability results for many popular algorithms can be
applied to derive positive instances where such confidence intervals will be
valid. However, in the common setting where CV is used to compare two
algorithms, it becomes necessary to consider a notion of relative stability
which cannot easily be derived from existing stability results, even for simple
algorithms. To better understand relative stability and when CV provides valid
confidence intervals for the test error difference of two algorithms, we study
the soft-thresholded least squares algorithm, a close cousin of the Lasso. We
prove that while stability holds when assessing the individual test error of
this algorithm, relative stability fails to hold when comparing the test error
of two such algorithms, even in a sparse low-dimensional linear model setting.
Additionally, we empirically confirm the invalidity of CV confidence intervals
for the test error difference when either soft-thresholding or the Lasso is
used. In short, caution is needed when quantifying the uncertainty of CV
estimates of the performance difference of two machine learning algorithms,
even when both algorithms are individually stable.

</details>


### [104] [Benchmarking Uncertainty and its Disentanglement in multi-label Chest X-Ray Classification](https://arxiv.org/abs/2508.04457)
*Simon Baur,Wojciech Samek,Jackie Ma*

Main category: stat.ML

TL;DR: 本文研究了医学影像中AI模型的不确定性量化方法，通过多标签胸部X光分类任务评估了13种方法，并扩展了几种深度学习技术。


<details>
  <summary>Details</summary>
Motivation: 可靠的AI不确定性量化对医学影像诊断至关重要，但现有研究在真实医疗任务中的应用尚未充分探索。

Method: 使用MIMIC-CXR-JPG数据集，评估了13种不确定性量化方法，包括卷积和Transformer架构，并扩展了Evidential Deep Learning等方法。

Result: 分析了不确定性估计的有效性，揭示了不同方法和架构在区分认知和随机不确定性方面的优缺点。

Conclusion: 研究为医学影像中的不确定性量化提供了基准，并指出了方法和架构的适用性与局限性。

Abstract: Reliable uncertainty quantification is crucial for trustworthy
decision-making and the deployment of AI models in medical imaging. While prior
work has explored the ability of neural networks to quantify predictive,
epistemic, and aleatoric uncertainties using an information-theoretical
approach in synthetic or well defined data settings like natural image
classification, its applicability to real life medical diagnosis tasks remains
underexplored. In this study, we provide an extensive uncertainty
quantification benchmark for multi-label chest X-ray classification using the
MIMIC-CXR-JPG dataset. We evaluate 13 uncertainty quantification methods for
convolutional (ResNet) and transformer-based (Vision Transformer) architectures
across a wide range of tasks. Additionally, we extend Evidential Deep Learning,
HetClass NNs, and Deep Deterministic Uncertainty to the multi-label setting.
Our analysis provides insights into uncertainty estimation effectiveness and
the ability to disentangle epistemic and aleatoric uncertainties, revealing
method- and architecture-specific strengths and limitations.

</details>


### [105] [Metric Learning in an RKHS](https://arxiv.org/abs/2508.04476)
*Gokcan Tatli,Yi Chen,Blake Mason,Robert Nowak,Ramya Korlakai Vinayak*

Main category: stat.ML

TL;DR: 论文提出了一种基于RKHS的度量学习框架，提供了泛化保证和样本复杂度界限，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 度量学习在图像检索、推荐系统和认知心理学等领域有重要应用，但现有方法缺乏理论支持，尤其是在非线性情况下。

Method: 开发了一个通用的RKHS框架，用于从三元组比较中学习度量，并结合核方法和神经网络。

Result: 提出了新的泛化保证和样本复杂度界限，并通过模拟和真实数据集实验验证了方法的有效性。

Conclusion: 该框架为非线性度量学习提供了理论基础，并在实验中表现出良好的性能。

Abstract: Metric learning from a set of triplet comparisons in the form of "Do you
think item h is more similar to item i or item j?", indicating similarity and
differences between items, plays a key role in various applications including
image retrieval, recommendation systems, and cognitive psychology. The goal is
to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric
learning using kernel methods and neural networks have shown great empirical
promise. While previous works have addressed certain aspects of this problem,
there is little or no theoretical understanding of such methods. The exception
is the special (linear) case in which the RKHS is the standard Euclidean space
$\mathbb{R}^d$; there is a comprehensive theory for metric learning in
$\mathbb{R}^d$. This paper develops a general RKHS framework for metric
learning and provides novel generalization guarantees and sample complexity
bounds. We validate our findings through a set of simulations and experiments
on real datasets. Our code is publicly available at
https://github.com/RamyaLab/metric-learning-RKHS.

</details>
