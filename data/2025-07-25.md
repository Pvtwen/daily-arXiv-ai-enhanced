<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 9]
- [cs.LG](#cs.LG) [Total: 62]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Time and Frequency Synchronization for Multiuser OTFS in Uplink](https://arxiv.org/abs/2507.17966)
*Mohsen Bayat,Sanoopkumar P. S.,Arman Farhang*

Main category: eess.SP

TL;DR: 本文提出了高移动性场景下上行多用户OTFS系统的时间和频率同步技术，重点解决了定时偏移和载波频率偏移的估计与校正问题。


<details>
  <summary>Details</summary>
Motivation: 在高移动性场景中，准确估计和校正定时偏移（TOs）和载波频率偏移（CFOs）对多用户OTFS系统的性能至关重要。

Method: 提出两种TO估计技术：基于SU-PCP的TO估计和基于MU-PCP的TO估计，并利用CPF-BEM模型简化CFO估计的多维搜索问题。

Result: 通过改进的导频结构和相关技术，实现了更准确的TO和CFO估计，提升了系统性能。

Conclusion: 所提出的同步技术在高移动性场景下显著提升了多用户OTFS系统的性能，具有实际应用价值。

Abstract: In this paper, we propose time and frequency synchronization techniques for
uplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work
focuses on accurately estimating and correcting timing offsets (TOs) and
carrier frequency offsets (CFOs). Specifically, TO estimation is essential for
locating users' pilots on the delay-time plane, while CFO estimation enhances
channel estimation accuracy. First, we propose a TO estimation technique for an
existing multiuser pilot structure in MU-OTFS. We replace the impulse pilot
(IMP) in this pilot structure with a more practical pilot with a cyclic prefix
(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs
different Zadoff-Chu (ZC) sequences, which enables pilot separation via
correlation at the receiver side. Consequently, we introduce a
correlation-based TO estimation technique for uplink MU-OTFS using this pilot
structure. Next, a spectrally efficient and practical pilot pattern is
proposed, where each user transmits a PCP within a shared pilot region on the
delay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO
estimation technique utilizes a bank of filters to separate different users'
signals and accurately estimate their TOs. Then, we derive a mathematical
threshold range to enhance TO estimation accuracy by finding the first major
peak in the correlation function rather than relying solely on the highest
peak. After locating the received users' pilot signals using one of the
proposed TO estimation techniques, our proposed CFO estimation technique
reduces the multi-dimensional maximum likelihood (ML) search problem into
multiple one-dimensional search problems. In this technique, we apply the
Chebyshev polynomials of the first kind basis expansion model (CPF-BEM) to
effectively handle the time-variations of the channel in obtaining the CFO
estimates for all the users.

</details>


### [2] [Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model](https://arxiv.org/abs/2507.17982)
*Pablo Ramírez-Espinosa,Cleofás Segura-Gómez,Ángel Palomares-Caballero,F. Javier López-Martínez,David Morales-Jiménez*

Main category: eess.SP

TL;DR: 论文提出了一种基于超表面的流体天线系统（FAS）的完整分析模型，通过动态超表面天线（DMA）实现，验证了其性能接近理想化实现。


<details>
  <summary>Details</summary>
Motivation: 由于电子可重构天线在分析建模上的挑战，以及流体天线系统（FAS）在无线通信中的潜力，作者希望提出一种实用的分析模型。

Method: 利用电路理论将FAS的信号模型改写为考虑超表面电磁效应的导纳矩阵，并通过全波仿真验证模型。

Result: 模型验证显示与仿真结果一致，DMA实现的FAS性能接近理想化天线。

Conclusion: 提出的分析模型为FAS系统设计提供了理论支持，DMA实现展示了实用潜力。

Abstract: Fluid antenna systems (FASs) have become a popular topic in the wireless
community as an effective yet simple means of exploiting spatial diversity. Due
to the limitations of physically moving radiating elements, electronically
reconfigurable antennas are emerging as practical implementations of FASs,
since changing the radiation pattern is functionally equivalent to physically
moving the device. However, electronically reconfigurable antennas pose a
challenge in terms of analytical modeling, often requiring full-wave
simulations or measurements for their characterization; this severely limits
the extraction of theoretical insights useful for system design. Motivated by
these difficulties and the growing interest in FASs, we propose in this paper a
complete analytical model for metasurface-based embodiments of FASs.
Specifically, we advocate for the implementation of the FAS concept through
dynamic metasurface antennas (DMAs), hitherto proposed as array replacements in
multiple-input multiple-output (MIMO) systems. We leverage circuit theory to
rewrite the conventional signal model of FASs in terms of admittance matrices
accounting for the electromagnetic effects inherent to metasurfaces. The model
is validated with full-wave simulations, showing good agreement. We further
illustrate how to apply the model for standard performance analysis, and
provide closed-form expressions for key metrics, including the resulting signal
covariance matrix. Results confirm that practical DMA-based FASs can achieve
similar performance to that of idealized implementations of position-flexible
antennas.

</details>


### [3] [Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming](https://arxiv.org/abs/2507.18035)
*Hyeonho Noh,Hyeonsu Lyu,Hyun Jong Yang*

Main category: eess.SP

TL;DR: 本文研究了由多个主动同时传输和反射可重构智能表面（STAR-RISs）支持的集成感知与通信（ISAC）网络，通过联合优化基站波束成形和STAR-RIS系数，最大化通信总速率，同时满足感知、安全和功率约束。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用STAR-RISs提升ISAC网络的性能，解决通信速率、感知要求和信息安全之间的平衡问题。

Method: 采用交替优化（AO）框架，将问题分解为子问题，分别通过KKT条件和SCA方法优化基站波束成形和STAR-RIS系数。

Result: 仿真表明，所提算法在通信总速率上显著优于被动RIS和单STAR-RIS基准，同时满足感知和安全约束。

Conclusion: 主动STAR-RISs在ISAC网络中具有显著优势，联合优化方法能有效提升性能。

Abstract: This paper explores an integrated sensing and communication (ISAC) network
empowered by multiple active simultaneously transmitting and reflecting
reconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes
downlink communication to multiple users while concurrently interrogating a
sensing target. We jointly optimize the BS transmit beamformer and the
reflection/transmission coefficients of every active STAR-RIS in order to
maximize the aggregate communication sum-rate, subject to (i) a stringent
sensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an
upper bound on the leakage of confidential information, and (iii) individual
hardware and total power constraints at both the BS and the STAR-RISs. The
resulting highly non-convex program is tackled with an efficient alternating
optimization (AO) framework. First, the original formulation is reformulated
into an equivalent yet more tractable representation and partitioned into
subproblems. The BS beamformer is updated in closed form via the
Karush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and
transmission vectors are refined through successive convex approximation (SCA),
yielding a semidefinite program that is then solved via semidefinite
relaxation. Comprehensive simulations demonstrate that the proposed algorithm
delivers substantial sum-rate gains over passive-RIS and single STAR-RIS
baselines, all the while rigorously meeting the prescribed sensing and security
constraints.

</details>


### [4] [Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation](https://arxiv.org/abs/2507.18096)
*Jihong Huang,Rong Yang,Wei Gao,Xingqun Zhan,Zheng Yao*

Main category: eess.SP

TL;DR: 本文通过几何分析量化了DPE中多径误差对CAF和PVT解的影响，提出了SCMB模型，并通过模拟和实验验证了多径误差的几何特性。


<details>
  <summary>Details</summary>
Motivation: 现有DPE理论缺乏对多径误差的系统性理论分析，特别是在城市环境中多径效应对PVT解的影响。

Method: 通过几何分析量化多径误差对CAF和PVT解的影响，提出SCMB模型，并通过蒙特卡洛模拟和城市峡谷测试验证。

Result: 研究发现最大PVT偏差取决于各卫星通道中的最大多径误差，且PVT偏差随卫星仰角增加而增大。

Conclusion: 研究结果为从几何角度选择DPE卫星提供了参考，强调了高低仰角卫星的平衡组合对优化几何配置的重要性。

Abstract: Direct Position Estimation (DPE) is a method that directly estimate position,
velocity, and time (PVT) information from cross ambiguity function (CAF) of the
GNSS signals, significantly enhancing receiver robustness in urban
environments. However, there is still a lack of theoretical characterization on
multipath errors in the context of DPE theory. Geometric observations highlight
the unique characteristics of DPE errors stemming from multipath and thermal
noise as estimation bias and variance respectively. Expanding upon the
theoretical framework of DPE noise variance through geometric analysis, this
paper focuses on a geometric representation of multipath errors by quantifying
the deviations in CAF and PVT solutions caused by off-centering bias relative
to the azimuth and elevation angles. A satellite circular multipath bias (SCMB)
model is introduced, amalgamating CAF and PVT errors from multiple satellite
channels. The boundaries for maximum or minimum PVT bias are established
through discussions encompassing various multipath conditions. The correctness
of the multipath geometrical portrait is confirmed through both Monte Carlo
simulations and urban canyon tests. The findings indicate that the maximum PVT
bias depends on the largest multipath errors observed across various satellite
channels. Additionally, the PVT bias increases with satellite elevation angles,
influenced by the CAF multipath bias projection. This serves as a reference for
selecting DPE satellites from a geometric standpoint, underscoring the
importance of choosing a balanced combination of high and low elevation angles
to achieve an optimal satellite geometry configuration.

</details>


### [5] [Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems](https://arxiv.org/abs/2507.18149)
*Dongdong Zou,Wei Wang,Jiawen Yao,Zhongxing Tian,Zeyu Feng,Huan Huang,Fan Li,Gordon Ning Liu,Gangxiang Shen,Yi Cai*

Main category: eess.SP

TL;DR: 提出了一种针对峰值功率受限IM-DD系统的间接概率整形方案，通过动态选择性映射和修改的M-BCJR算法，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 由于IM-DD系统的独特模型和固有约束，概率整形技术的有效应用仍是一个开放性问题，尤其是在具有记忆效应的系统中。

Method: 提出了一种动态选择性映射机制（DSLM），结合修改的M-BCJR算法的turbo均衡器，以解决记忆效应引起的信号损伤。

Result: 在56GBaud PAM8系统中实验验证，接收灵敏度提升了1dB，且兼容典型概率幅度整形架构。

Conclusion: 该工作为概率整形技术在具有记忆效应的IM-DD系统中的应用提供了新思路。

Abstract: Probabilistic shaping (PS) has attracted significant attention in
intensity-modulation and direct-detection (IM-DD) systems. However, due to the
unique system model and inherent constraints, the effective application of the
PS technique is still an open question in IM-DD systems, particularly in
systems with memory effects. In this paper, a novel indirect PS scheme tailored
for peak power constrained (PPC) IM-DD systems is proposed. The key idea lies
in strategically controlling the signal envelope to mitigate memory-induced
impairments, such as nonlinearity, overshoot, peak-to-average power ratio
enhancement, etc. The proposed scheme incorporates a dynamic selective mapping
(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol
mapping in which the current symbol is not only determined by the current bits
pattern but also by previously generated symbols within a specified memory
length. At the receiver side, a turbo equalizer with a modified M-BCJR
algorithm is proposed to achieve the recovery of ambiguous bits induced by
DSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the
proposed scheme exhibits 1dB receiver sensitivity improvement over 2km
single-mode fiber transmission. In addition, the proposed scheme has also been
demonstrated to be compatible with the typical probabilistic amplitude shaping
architecture, enabling a simple and fine-granularity rate adaptation
capability. To the best of our knowledge, this work opens a new sight for the
application of the PS technique in PPC IM-DD systems with memory effects.

</details>


### [6] [GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing](https://arxiv.org/abs/2507.18166)
*Jonas Elmiger,Gian Marti,Christoph Studer*

Main category: eess.SP

TL;DR: SCHIEBER是一种针对多天线GNSS接收器的新方法，无需先验知识即可缓解干扰和欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统（GNSS）信号易受干扰和欺骗攻击，导致定位不可靠。

Method: 通过自适应空间滤波技术缓解干扰，利用信号到达方向（DoA）和伪距估计的一致性检测欺骗信号。

Result: 在GPS L1 C/A系统的模拟中，该方法有效缓解了干扰和欺骗攻击。

Conclusion: SCHIEBER为GNSS接收器提供了一种无需先验知识的抗干扰和抗欺骗解决方案。

Abstract: Modern positioning relies on radio signals from global navigation satellite
systems (GNSS). Their low receive power renders these radio signals susceptible
to jamming attacks, in which malicious transmitters emit strong interference to
disrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,
in which malicious transmitters mimic legitimate satellites by transmitting
spurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna
GNSS receivers that mitigates jammers as well as spoofers without requiring any
prior knowledge of the receiver position or attack type: Jammers are mitigated
during signal acquisition using a recently developed adaptive spatial filtering
technique. Spoofers are identified and rejected after signal acquisition using
a novel approach that tests the consistency of acquired signals by comparing
their respective direction of arrival (DoA) and pseudorange estimates in a test
that is invariant with respect to the unknown receiver position. We demonstrate
the efficacy of our method using extensive simulations of a GPS L1 C/A system
under spoofing and jamming attacks.

</details>


### [7] [ICWLM: A Multi-Task Wireless Large Model via In-Context Learning](https://arxiv.org/abs/2507.18167)
*Yuxuan Wen,Xiaoming Chen,Maojun Zhang,Zhaoyang Zhang*

Main category: eess.SP

TL;DR: 论文提出了一种新型的无线原生基础模型ICWLM，用于物理层的多任务学习，解决了传统深度学习方法在数据稀缺和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 无线通信技术的快速发展（如mMIMO和mmWave）带来了网络复杂性和计算需求，传统深度学习方法任务单一且难以泛化，亟需一种统一且自适应的解决方案。

Method: 提出ICWLM模型，直接在大规模混合无线数据集上训练，利用上下文学习（ICL）适应不同系统配置，并采用动态权重平均（DWA）算法平衡多任务损失。

Result: ICWLM在多项物理层任务中表现优异，泛化能力强，适用于未见过的系统配置。

Conclusion: ICWLM为未来无线网络提供了一种统一且自适应的AI模型范式，有望降低部署复杂性并提升资源管理智能性。

Abstract: The rapid evolution of wireless communication technologies, particularly
massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),
introduces significant network complexity and computational demands.
Significant research efforts have been made to improve physical layer
performance by resorting to deep learning (DL) methods, which, however, are
usually task-specific and struggle with data scarcity and generalization. To
address these challenges, we propose a novel In-Context Wireless Large Model
(ICWLM), a wireless-native foundation model designed for simultaneous
multi-task learning at the physical layer. Unlike conventional methods that
adapt wireless data to pre-trained large language models (LLMs), ICWLM is
trained directly on large-scale, mixed wireless datasets from scratch. It
jointly solves multiple classical physical layer problems, including multi-user
precoding (sum-rate maximization and max-min SINR) and channel prediction. A
key innovation of ICWLM is its utilization of in-context learning (ICL),
enabling the model to adapt to varying system configurations and channel
conditions with minimal demonstration pairs, eliminating the need for extensive
retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm
to dynamically balance the individual task losses during multi-task training,
ensuring efficient and stable learning across diverse objectives. Extensive
simulation results demonstrate that ICWLM achieves competitive performance
compared to task-specific methods while exhibiting remarkable generalization
capabilities to unseen system configurations. This work offers a promising
paradigm for developing unified and adaptive AI models for future wireless
networks, potentially reducing deployment complexity and enhancing intelligent
resource management.

</details>


### [8] [Quantized Signal Recovery with Interference via Parametrized Look-Up Tables](https://arxiv.org/abs/2507.18370)
*Morriel Kasher,Michael Tinston,Predrag Spasojevic*

Main category: eess.SP

TL;DR: 论文提出了一种基于查找表（LUT）的数字后校正方法，通过参数化模型优化性能，适用于低分辨率、非线性或宽带量化器。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率模数转换器的实时校正问题，尤其是在存在噪声和干扰信号的情况下。

Method: 提出三种分析估计器，结合参数化LUT，并针对特定信号类型（如PSK和LFM）提出近似方法以提高计算可行性。

Result: 仿真结果表明，该方法能高精度实时恢复输入信号，显著改善均方误差和无杂散动态范围。

Conclusion: 该方法优于传统线性滤波技术，对输入参数变化、非线性量化器和时变干扰源具有鲁棒性。

Abstract: Efficient all-digital post-correction of low-resolution analog-to-digital
converters can be achieved by using Look-Up Tables (LUTs). The performance of a
LUT can be optimized by incorporating a parametric model for the expected input
signal, noise level, and interference signals. We evaluate three analytical
estimators for integration with parametrized LUTs, especially with applications
to low-resolution, non-linear, or wideband quantizers. We also propose several
approximations to improve tractability of the estimation problem for
Phase-Shift Keyed input signals and Linear Frequency Modulated interference
signals. Simulated results validate the ability of our estimator to recover the
instantaneous value of the desired input signal in real-time with a high degree
of accuracy. This includes cancellation of harmonic distortion that aliases
into the desired signal bandwidth from front-end saturation due to high-power
out-of-band interference. Our estimators are shown to achieve a significant
gain over conventional linear-filtering techniques while also being robust to
changes in input parameters, non-linear quantizers, and time-variant
interference sources. For a tone input quantized to 3 bits and estimated with a
fixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error
and $>$20 dBc improvement in Spurious-Free Dynamic Range.

</details>


### [9] [A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff](https://arxiv.org/abs/2507.18587)
*Jérôme Emery,Ali Hasanzadeh Karkan,Jean-François Frigon,François Leduc-Primeau*

Main category: eess.SP

TL;DR: 提出基于Transformer的基础模型用于mMIMO预编码，减少发射机能耗并动态适应用户速率需求，零样本部署性能优于零强迫，接近加权最小均方误差性能且复杂度低8倍。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在mMIMO系统中预编码时需高质量本地数据集的问题，同时降低训练复杂度。

Method: 使用Transformer基础模型，引入数据增强方法，通过预训练特征提取器输出计算余弦相似性找到类似目标分布的样本。

Result: 零样本部署性能优于零强迫，接近加权最小均方误差性能且复杂度低8倍。

Conclusion: 该工作通过解决数据可用性和训练复杂度问题，使基于深度学习的解决方案在实践中可行，同时支持动态配置用户速率需求，提升能效、频谱效率和公平性。

Abstract: Deep learning (DL) has emerged as a solution for precoding in massive
multiple-input multiple-output (mMIMO) systems due to its capacity to learn the
characteristics of the propagation environment. However, training such a model
requires high-quality, local datasets at the deployment site, which are often
difficult to collect. We propose a transformer-based foundation model for mMIMO
precoding that seeks to minimize the energy consumption of the transmitter
while dynamically adapting to per-user rate requirements. At equal energy
consumption, zero-shot deployment of the proposed foundation model
significantly outperforms zero forcing, and approaches weighted minimum mean
squared error performance with 8x less complexity. To address model adaptation
in data-scarce settings, we introduce a data augmentation method that finds
training samples similar to the target distribution by computing the cosine
similarity between the outputs of the pre-trained feature extractor. Our work
enables the implementation of DL-based solutions in practice by addressing
challenges of data availability and training complexity. Moreover, the ability
to dynamically configure per-user rate requirements can be leveraged by higher
level resource allocation and scheduling algorithms for greater control over
energy efficiency, spectral efficiency and fairness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Gait Recognition Based on Tiny ML and IMU Sensors](https://arxiv.org/abs/2507.18627)
*Jiahang Zhang,Mingtong Chen,Zhengbao Yang*

Main category: cs.LG

TL;DR: 开发了一种基于Tiny ML和IMU传感器的步态识别系统，用于实时分类四种活动，准确率超过80%。


<details>
  <summary>Details</summary>
Motivation: 利用微型设备和低功耗技术实现实时步态识别，适用于电池供电设备。

Method: 使用XIAO-nRF52840 Sense和LSM6DS3传感器采集数据，通过Edge Impulse平台进行特征提取和DNN训练。

Result: 模型在测试集上达到80%以上的准确率，并能进行异常检测。

Conclusion: 系统展示了Tiny ML在低功耗设备中实现高效步态识别的潜力。

Abstract: This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.

</details>


### [11] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: QuaRC是一个在边缘设备上结合核心集选择和量化感知训练（QAT）的框架，通过相对熵分数和级联层校正策略显著减少量化误差，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上低比特量化模型的需求增加，但传统QAT依赖完整数据集且计算成本高，核心集选择方法在小规模数据集上难以消除量化误差。

Method: QuaRC分两阶段：核心集选择阶段使用相对熵分数选取代表性子集；训练阶段采用级联层校正策略对齐量化模型和全精度模型的中间层输出。

Result: 在ImageNet-1K数据集上，使用1%数据子集量化ResNet-18至2比特时，QuaRC的Top-1准确率比现有技术提升5.72%。

Conclusion: QuaRC通过创新的核心集选择和训练策略，有效解决了边缘设备上小规模数据集量化误差问题，显著提升模型性能。

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [12] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种低复杂度、通用的AI模型，利用因果不变学习改进语义通信中的信道解码器数据重建。


<details>
  <summary>Details</summary>
Motivation: 解决用户数据多样化和知识分散导致的语义通信中数据重建的挑战。

Method: 采用生成对抗网络和因果不变学习，提取数据的因果和非因果表示，设计稀疏更新协议。

Result: 因果不变知识确保跨设备一致性，分类任务表现优异，数据重建性能超越现有方法。

Conclusion: 该方法在语义通信中表现出高效和鲁棒性，适用于多样化数据场景。

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [13] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于隐藏层神经元输出特征的复杂网络建模方法，研究不同隐藏层构建的特征网络的自相似性，并分析如何通过调整自相似性提升深度神经网络的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽发现深度神经网络在特征表示或参数分布上具有层次自相似性，但缺乏对隐藏空间几何自相似性如何影响权重优化的定量分析，以及对内部神经元动态行为的清晰理解。

Method: 采用复杂网络建模方法，基于隐藏层神经元输出特征构建特征网络，分析其自相似性，并通过嵌入自相似性约束优化训练过程。

Result: 实验表明，不同模型架构的特征网络自相似性程度各异，嵌入自相似性约束可提升自相似深度神经网络（MLP和注意力架构）性能达6个百分点。

Conclusion: 通过调整特征网络的自相似性可有效提升深度神经网络的分类性能，为模型优化提供了新思路。

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [14] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: CICME是一种三步骤方法，通过因果迁移学习从多领域异构数据中推断因果机制，并在制造过程场景中验证其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了从复杂传感器系统中通过因果性获得更深层次的见解。

Method: 提出CICME方法，结合因果迁移学习，分三步推断域不变和个体因果机制。

Result: 在制造过程的线性高斯模型中，CICME性能优于基线方法。

Conclusion: CICME在多领域数据中能有效识别因果机制，并在某些场景下优于传统方法。

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [15] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的自适应优化算法，用于降维的气动形状优化，通过代理模型和MCMC方法减少计算量并解释优化结果。


<details>
  <summary>Details</summary>
Motivation: 目标是通过强化学习方法减少计算成本，并解释优化结果在流场中的作用。

Method: 采用基于代理模型的actor-critic策略评估MCMC方法，允许部分参数临时冻结，通过局部优化参数变化加速全局优化。

Result: 方法在简单流体动力学问题上展示了特征重要性评分的解释能力。

Conclusion: 该方法通过局部优化和准确的奖励估计，有效加速了全局优化并提供了结果解释。

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [16] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: 提出了一种结合生成式AI和copula模型的新框架CoCAI，用于多变量时间序列的预测和异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列分析中的准确预测和鲁棒异常检测两大挑战。

Method: 利用扩散模型捕捉数据复杂依赖关系，结合保形预测技术校准输出，并通过降维和copula模型进行异常检测。

Result: 在真实水务系统数据上验证了CoCAI的预测准确性和异常检测能力。

Conclusion: CoCAI在理论和实践中均表现出色，适用于实际部署。

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [17] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 论文探讨了基础模型的局限性，并提出双曲空间作为改进几何归纳偏置的解决方案，综述了双曲神经网络及其在基础模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 基础模型在预训练和大规模数据集上表现出色，但存在表示能力有限、适应性低和可扩展性不足的问题。研究双曲空间是否能更好地对齐现实数据的固有结构。

Method: 利用双曲空间的数学特性（如指数体积增长），改进基础模型的嵌入和推理能力，包括语言模型、视觉语言模型和多模态模型。

Result: 双曲空间显著提升了模型的复杂推理能力、零样本泛化和跨模态语义对齐，同时保持参数效率。

Conclusion: 双曲空间为基础模型提供了新的研究方向，但仍需解决关键挑战以推动领域发展。

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [18] [Efficient Uncertainty in LLMs through Evidential Knowledge Distillation](https://arxiv.org/abs/2507.18366)
*Lakshmana Sri Harsha Nemani,P. K. Srijith,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: 论文提出了一种通过蒸馏技术实现高效不确定性估计的新方法，避免了传统贝叶斯和集成方法的计算开销。


<details>
  <summary>Details</summary>
Motivation: 标准LLMs在不确定性量化方面存在挑战，传统方法需要多次前向传播，计算成本高。

Method: 通过蒸馏不确定性感知的教师模型到学生模型，采用LoRA微调，比较了基于softmax和Dirichlet分布的两种策略。

Result: 实验表明，学生模型在单次前向传播下，预测和不确定性量化性能与教师模型相当或更优。

Conclusion: 首次证明通过证据蒸馏可以在LLMs中实现高效且鲁棒的不确定性量化。

Abstract: Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.

</details>


### [19] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: 论文提出了一种动态早停方法，用于减少LLM在排序和评估任务中的重复调用次数，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在排序和评估任务中存在位置偏差和重复一致性低的问题，传统方法通过多次重复调用和多数投票解决，但计算成本高。

Method: 提出动态早停方法，自适应确定每个实例所需的重复调用次数，并结合置信度进一步优化。

Result: 动态策略平均减少81%的LLM调用，置信度优化后减少87%，准确性损失较小。

Conclusion: 动态早停方法显著降低计算成本，适用于实际应用。

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [20] [Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2507.18555)
*Jun'ichi Takeuchia,Yoshinari Takeishia,Noboru Muratab,Kazushi Mimurac,Ka Long Keith Hod,Hiroshi Nagaoka*

Main category: cs.LG

TL;DR: 本文探讨了2层ReLU网络的Fisher信息矩阵与神经正切核（NTK）的关系，展示了NTK的谱分解及其主要特征值的具体形式，并提出了2层神经网络函数的近似公式。


<details>
  <summary>Details</summary>
Motivation: 研究Fisher信息矩阵与NTK之间的关系，以深入理解2层ReLU网络的表示能力及其优化行为。

Method: 通过线性变换分析Fisher信息矩阵与NTK的关系，并推导NTK的谱分解及其特征函数的具体形式。

Result: 展示了NTK的主要特征值及其特征函数的具体形式，并提出了2层神经网络函数的近似公式。

Conclusion: 研究揭示了Fisher信息矩阵与NTK的紧密联系，为理解2层ReLU网络的表示和优化提供了新的视角。

Abstract: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.

</details>


### [21] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix是一个基于Python的开源框架，旨在为表格数据提供可重现和可解释的机器学习工作流，支持数据预处理、模型训练、评估和解释等功能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习工作流中透明性和可重现性的需求，确保分析过程的每一步都被记录和理解。

Method: 提供标准化模块，包括数据预处理、可视化、模型训练、评估和解释，并配备用户友好界面。

Result: Helix支持社区驱动开发，遵循FAIR原则，帮助非数据科学背景的研究者获得可操作的见解。

Conclusion: Helix是一个功能全面且易于使用的工具，推动了透明和可解释的机器学习研究。

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [22] [Beyond Internal Data: Constructing Complete Datasets for Fairness Testing](https://arxiv.org/abs/2507.18561)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 论文提出了一种利用重叠数据集构建合成数据的方法，以解决公平性测试中真实数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的广泛应用，公平性测试变得至关重要，但获取包含人口统计数据的真实数据存在法律和隐私挑战。

Method: 通过重叠数据集构建包含人口统计信息的合成数据，并验证其与真实数据的一致性。

Result: 实验证明，合成数据得出的公平性指标与真实数据一致。

Conclusion: 该方法为公平性测试提供了一种可行的替代方案，解决了真实数据稀缺的问题。

Abstract: As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.

</details>


### [23] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种结合扩散模型和大型语言模型（LLM）的时空扩散模型（LSDM），用于提升个体用户服务级移动流量预测的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有预测方法在不同城市环境中的适应性不足，且因个人流量模式的高不确定性、环境上下文缺失及网络服务间复杂依赖关系导致预测不准确。

Method: LSDM结合扩散模型的生成能力和Transformer的自适应学习能力，利用LLM捕捉多模态环境信息，建模服务级流量模式和动态。

Result: 实验表明，LSDM在流量预测中表现优异，泛化能力强。加入LLM后，决定系数至少提升2.83%，与类似模型（如CSDI）相比，均方根误差至少降低8.29%。

Conclusion: LSDM通过结合扩散模型和LLM，显著提升了服务级流量预测的准确性和适应性，为网络效率和服务质量优化提供了有效工具。

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [24] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: GenSelect利用LLM的长推理能力从N个候选方案中选择最佳方案，优于现有的点对点或成对评分方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（点对点或成对比较）未能充分利用LLM的比较能力或扩展效率低。

Method: 提出GenSelect，通过长推理从并行采样的候选方案中选择最佳解。

Result: 在数学推理任务中，GenSelect表现优于现有评分方法。

Conclusion: GenSelect结合了LLM的比较优势和高扩展性，适用于大规模并行采样任务。

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [25] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 使用Wasserstein生成对抗网络（WGAN）进行降水降尺度，提高视觉真实性，并提供了评估降水数据的新视角。


<details>
  <summary>Details</summary>
Motivation: 高分辨率降水预测对减少局部强降雨的损害至关重要，但传统数值天气预报模型在此方面仍具挑战性。

Method: 采用WGAN进行降水降尺度，利用最优传输成本生成降水场，与传统均方误差训练的神经网络对比。

Result: WGAN生成的降水场视觉上更真实，尽管在传统评估指标上表现略低；其判别器评分与人类感知真实性高度相关。

Conclusion: WGAN框架不仅提升了降水降尺度的视觉真实性，还为降水数据集的评估和质量控制提供了新方法。

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [26] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: GraphEXT是一个基于合作博弈论和社会外部性的新型GNN可解释性框架，通过将节点划分为联盟并量化其边际贡献，显著提升了GNN模型的解释性。


<details>
  <summary>Details</summary>
Motivation: GNN的黑箱特性及其难以捕捉节点间复杂交互模式的问题，促使研究者开发更有效的解释性方法。

Method: GraphEXT利用合作博弈论和社会外部性概念，将节点划分为联盟，并通过Shapley值量化节点在预测中的重要性。

Result: 实验表明，GraphEXT在多种GNN架构中均优于现有基线方法，显著提升了模型的可解释性。

Conclusion: GraphEXT通过强调节点交互和结构变化的影响，为GNN提供了一种更有效的解释性框架。

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [27] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: 提出分子任务算术方法，通过负样本训练生成正分子，无需正样本数据，实验证明其多样性和性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决生成分子设计中正样本稀缺的问题，提出一种无需正样本数据的方法。

Method: 利用负样本训练模型学习属性方向，反向移动模型生成正分子。

Result: 在20个零样本设计实验中，生成更多样且成功的分子；在双目标和少样本任务中也表现优异。

Conclusion: 分子任务算术因其简单、高效和性能优越，有望成为分子设计的标准迁移学习策略。

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [28] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: 本文提出了一种基于算子的神经网络MFNO，用于学习随机系统的动力学，通过镜像填充处理非周期性输入，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理非周期性输入和随机系统动力学时存在局限性，MFNO通过镜像填充扩展FNO，解决了这一问题。

Method: MFNO结合镜像填充技术，扩展标准FNO，利用Wong-Zakai定理和近似技术进行理论分析。

Result: MFNO在分辨率泛化性上表现优异，性能优于LSTM、TCN和DeepONet，且生成样本路径速度更快。

Conclusion: MFNO为随机系统动力学建模提供了高效且准确的解决方案，具有理论和实际应用价值。

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [29] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: 论文研究了在差分隐私机器学习中，公开数据与私有数据分布存在显著差异时的互补价值，发现小差异时需充足数据，大差异时公开数据无益。


<details>
  <summary>Details</summary>
Motivation: 探讨在公开数据与私有数据分布存在显著差异时，两者结合的互补价值及其对差分隐私机器学习算法的影响。

Method: 扩展了公开-私有学习的下界分析，应用于高斯均值估计和线性回归中分布差异的情况。

Result: 小差异时需充足数据估计私有参数，大差异时公开数据无益。

Conclusion: 公开数据与私有数据的分布差异大小决定了其互补价值，小差异需更多数据，大差异则公开数据无效。

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [30] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 联邦学习（FL）是一种新兴的分布式机器学习范式，通过动态设备参与协作训练模型，无需共享私有数据。本文探讨了FL与云机器人操作的联系，并展望了其在大规模高效可靠应用中的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习需要数据集中存储，而FL通过分布式设备训练共享模型，解决了数据隐私问题。云机器人操作受限于计算资源，FL为其提供了灵活性和可靠性。

Method: 介绍了FL的基本概念及其与云机器人操作的关联，探讨了在集中式或分散式环境中设计和验证FL模型的方法。

Result: FL为云机器人操作提供了分布式计算的优势，但也带来了效率和可靠性方面的挑战。

Conclusion: FL在云机器人操作中具有潜力，但需进一步研究以解决其在大规模应用中的挑战。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [31] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: 该研究旨在通过基于深度学习的生成框架探索多孔超材料的逆向设计，开发了一种增强型变分自编码器（pVAE），用于生成具有定制水力特性的结构。


<details>
  <summary>Details</summary>
Motivation: 探索多孔超材料的逆向设计，以生成具有特定水力特性（如孔隙率和渗透率）的材料。

Method: 使用变分自编码器（VAE）结合回归器（pVAE），通过卷积神经网络（CNN）预测水力特性，减少计算成本。

Result: pVAE框架成功生成了具有定制特性的新材料，并提供了潜在空间的详细分析。

Conclusion: 该方法为生成具有所需特性的新材料提供了高效途径，相关数据和代码将开源以支持进一步研究。

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [32] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: 提出了一种半经验学习理论（SETOL），解释了最先进神经网络（NNs）的卓越性能，并验证了其与重尾自正则化（HTSR）理论的关联。


<details>
  <summary>Details</summary>
Motivation: 解释最先进神经网络性能背后的理论机制，尤其是重尾自正则化（HTSR）中的关键指标（alpha和alpha-hat）的起源。

Method: 结合统计力学、随机矩阵理论和量子化学方法，推导出理想学习的新数学条件，并提出新指标ERG。通过3层多层感知机（MLP）验证理论假设。

Result: SETOL与HTSR的关键指标（alpha和ERG）在MLP和最先进NNs中表现一致，验证了理论的有效性。

Conclusion: SETOL为神经网络性能提供了理论解释，并提出了新的学习指标ERG，为未来研究提供了方向。

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [33] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: Seed2Harvest是一种混合红队方法，结合人类和机器生成对抗性提示的优势，用于全面评估文本到图像模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对抗性提示生成方法存在人类生成规模小、合成生成缺乏真实性的问题，需要结合两者优势以提升模型安全性评估。

Method: 提出Seed2Harvest方法，通过扩展人类生成的对抗性提示种子，结合机器计算能力，生成多样且真实的对抗性提示。

Result: 生成的提示集在攻击成功率（0.31-0.36）和多样性（535个地理位置，熵7.48）上显著优于原始数据集。

Conclusion: 人机协作在对抗性提示生成中至关重要，Seed2Harvest为文本到图像模型的安全性评估提供了全面且可扩展的解决方案。

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [34] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: UrbanPulse是一个可扩展的深度学习框架，用于超细粒度的城市范围OD流量预测，通过将每个POI作为独立节点建模，结合时空图卷积编码器和基于Transformer的解码器，实现了跨城市的高精度预测。


<details>
  <summary>Details</summary>
Motivation: 现有的人口流动预测方法存在静态空间假设、跨城市泛化能力差、计算成本高以及空间结构捕捉不足等问题，限制了其实际应用。

Method: UrbanPulse采用时空图卷积编码器和Transformer解码器建模多尺度时空依赖，并通过三阶段迁移学习策略（预训练、冷启动适应和强化学习微调）确保跨城市泛化能力。

Result: 在加州三个大都市区的1.03亿条GPS记录上测试，UrbanPulse实现了最先进的准确性和可扩展性。

Conclusion: UrbanPulse通过高效迁移学习，为高分辨率AI驱动的城市预测在不同城市的实际部署迈出了关键一步。

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [35] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: 论文提出MFTRR框架，通过多模态细粒度主题-帖子关系推理改进帖子质量评估，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在帖子质量评估中存在三大局限：单模态分类、多模态融合噪声和复杂语义关系捕捉不足。

Method: MFTRR框架包含局部-全局语义关联推理模块和多层次证据关系推理模块，结合多模态数据。

Result: 在三个新构建的多模态数据集和公开数据集上，MFTRR显著优于基线方法，NDCG@3提升最高达9.52%。

Conclusion: MFTRR通过模仿人类认知过程，有效解决了帖子质量评估中的关键问题，性能显著提升。

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [36] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE是一种两阶段Transformer模型，融合多模态视频、音频和文本特征来预测fMRI活动，在CNeuroMod数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 通过融合多模态特征（视频、音频、文本）来更准确地预测fMRI活动，提升模型在分布内和分布外数据上的表现。

Method: 使用开源模型提取多模态特征，通过模态融合Transformer和时间预测Transformer（带旋转嵌入）进行解码，并在65小时电影数据上训练。

Result: 在分布内数据（Friends S07）上平均Parcel-wise Pearson相关系数为32.25，分布外数据上为21.25，优于早期版本。

Conclusion: VIBE在多模态fMRI预测任务中表现优异，并在Algonauts 2025挑战赛中取得优异成绩。

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


### [37] [Improving the Computational Efficiency and Explainability of GeoAggregator](https://arxiv.org/abs/2507.17977)
*Rui Deng,Ziqi Li,Mingshu Wang*

Main category: cs.LG

TL;DR: 本文改进了GeoAggregator（GA）模型，通过优化数据加载流程和集成模型策略，提升了计算效率和模型可解释性。实验表明改进后的GA在预测精度和推理速度上优于原版。


<details>
  <summary>Details</summary>
Motivation: 准确建模和解释地理空间表格数据（GTD）对理解地理现象及其底层过程至关重要。

Method: 1）优化数据加载流程和模型前向传播以提高计算效率；2）集成模型策略并引入基于GeoShapley框架的后解释功能以增强可解释性。

Result: 改进后的GA在合成数据集上表现出更高的预测精度和推理速度，并能有效捕捉空间效应。

Conclusion: 改进的GA模型在功能和效率上均优于原版，且已开源供社区使用。

Abstract: Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).

</details>


### [38] [SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning](https://arxiv.org/abs/2507.17979)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.LG

TL;DR: SIFOTL是一种隐私保护的表格学习方法，通过统计摘要和XGBoost模型识别数据偏移，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决隐私限制和复杂噪声下表格数据偏移分析的挑战。

Method: 提取隐私合规的统计摘要，使用XGBoost和LLM分离信号与噪声，通过Pareto加权决策树识别偏移段。

Result: 在MEPS和EHR数据集上表现优异，F1分数显著高于基线方法。

Conclusion: SIFOTL提供了一种可解释且隐私保护的工作流，对噪声具有鲁棒性。

Abstract: Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.

</details>


### [39] [Machine Unlearning of Traffic State Estimation and Prediction](https://arxiv.org/abs/2507.17984)
*Xin Wang,R. Tyrrell Rockafellar,Xuegang,Ban*

Main category: cs.LG

TL;DR: 本文提出了一种新的学习范式TSEP-Machine Unlearning，旨在解决数据驱动的交通状态估计与预测中的隐私、安全和数据新鲜度问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的交通状态估计与预测依赖敏感数据，但隐私、网络安全和数据新鲜度问题可能削弱公众对智能交通系统的信任。

Method: 引入TSEP-Machine Unlearning范式，使训练后的模型能够选择性遗忘隐私敏感、污染或过时数据。

Result: 通过让模型“遗忘”特定数据，提升了交通状态估计与预测的可信度和可靠性。

Conclusion: TSEP-Machine Unlearning为解决数据隐私和信任问题提供了有效途径，增强了智能交通系统的可靠性。

Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.

</details>


### [40] [Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models](https://arxiv.org/abs/2507.18014)
*Datta Nimmaturi,Vaishnavi Bhargava,Rajat Ghosh,Johnu George,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出了一种预测框架，用于优化基于GRPO的大型语言模型微调的资源使用，通过实验总结出经验性扩展规律，并发现训练可分为三个阶段，提前停止可显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于使用GRPO等方法微调大型语言模型的计算成本高昂，需要一种方法来优化资源使用。

Method: 提出预测框架，通过实验在Llama和Qwen模型上总结出基于模型大小、初始性能和训练进度的扩展规律。

Result: 发现训练分为三个阶段（缓慢启动、快速提升、平台期），并证明提前停止可减少计算成本而不影响性能。

Conclusion: 该方法为基于GRPO的高效微调提供了实用指南，适用于多种模型类型。

Abstract: Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.

</details>


### [41] [Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents](https://arxiv.org/abs/2507.18067)
*Abdessamad El-Kabid,Loubna Benabbou,Redouane Lguensat,Alex Hernández-García*

Main category: cs.LG

TL;DR: 提出了一种基于神经算子的深度学习框架，用于解决偏微分方程并提供任意分辨率解，同时应用于Copernicus海洋数据降尺度建模。


<details>
  <summary>Details</summary>
Motivation: 高分辨率海洋数据对沿海管理、环境监测和海上安全至关重要，但现有数据（如Copernicus）分辨率不足。

Method: 采用监督深度学习框架和神经算子，建模替代PDE并预测任意分辨率解。

Result: 在真实Copernicus数据和合成Navier-Stokes数据集上验证了模型有效性。

Conclusion: 该方法能提供高分辨率解，适用于海洋数据降尺度和PDE求解。

Abstract: Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.

</details>


### [42] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: GSPO是一种稳定、高效且性能优越的强化学习算法，用于训练大语言模型，通过序列级优化显著提升训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统算法在训练大语言模型时采用词级重要性比率的问题，提出基于序列似然的重要性比率定义。

Method: GSPO通过序列级裁剪、奖励和优化，替代传统的词级方法。

Result: GSPO在训练效率和性能上优于GRPO算法，显著稳定了MoE RL训练，并简化了RL基础设施设计。

Conclusion: GSPO的成功应用为最新Qwen3模型的显著改进提供了支持。

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


### [43] [C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams](https://arxiv.org/abs/2507.18072)
*Ryusei Fujimoto,Yugo Nakamura,Yutaka Arakawa*

Main category: cs.LG

TL;DR: C-AAE结合匿名自编码器和自适应差分脉冲编码调制，有效保护可穿戴传感器数据的隐私，同时保持活动识别性能并减少数据量。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备的加速度计和陀螺仪数据可能泄露用户身份，隐私保护对医疗应用至关重要。

Method: C-AAE结合匿名自编码器（AAE）和自适应差分脉冲编码调制（ADPCM），先通过AAE抑制身份特征，再通过ADPCM进一步隐藏身份信息并压缩数据。

Result: 在MotionSense和PAMAP2数据集上，C-AAE将用户重识别F1分数降低10-15%，活动识别F1分数仅下降5%，数据量减少75%。

Conclusion: C-AAE在医疗传感器数据中实现了隐私保护与实用性的平衡。

Abstract: Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.

</details>


### [44] [Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method](https://arxiv.org/abs/2507.18073)
*Qingcheng Zhu,Yangyang Ren,Linlin Yang,Mingbao Lin,Yanjing Li,Sheng Xu,Zichao Feng,Haodong Zhu,Yuguang Yang,Juan Zhang,Runqi Wang,Baochang Zhang*

Main category: cs.LG

TL;DR: Squeeze10-LLM是一种分阶段混合精度后训练量化框架，通过将80%的权重量化为1位、20%量化为4位，实现平均1.6位/权重的压缩，显著减少存储和计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）因参数庞大和计算成本高而难以部署，极低位量化虽能减少存储和加速推理，但极端压缩会导致性能严重下降。

Method: 提出Squeeze10-LLM框架，结合后二值化激活鲁棒性（PBAR）和全信息激活监督（FIAS），通过混合精度量化实现高效压缩。

Result: 在LLaMA和LLaMA2上实验表明，Squeeze10-LLM在低于2位的权重量化中达到最优性能，零样本分类任务准确率从43%提升至56%。

Conclusion: Squeeze10-LLM通过创新量化策略显著提升了极低位量化的性能，为LLM的高效部署提供了有效解决方案。

Abstract: Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.

</details>


### [45] [Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes](https://arxiv.org/abs/2507.18098)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，通过结合硬标签和额外监督信息构建软标签，以提高有限数据场景下的分类模型性能。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺或标注成本高的场景中，如何利用额外监督信息（如置信度）提升分类模型的泛化性能是一个关键问题。

Method: 将硬标签和额外监督视为概率分布，通过它们的仿射组合构建软标签，并分析额外监督对标签分布的影响。

Result: 理论分析表明，额外监督的核心价值在于非硬标签类的分布信息，而非硬标签的置信度。实验验证了该框架能提升分类准确率。

Conclusion: 额外监督和混合系数在优化软标签中起互补作用，理论分析为设计更有效的监督信息提供了指导。

Abstract: In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.

</details>


### [46] [Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN](https://arxiv.org/abs/2507.18111)
*Peyman Tehrani,Anas Alsoliman*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度强化学习的RAN切片方法（PDA-DRL），在O-RAN架构中优化PRB利用率并满足延迟约束，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决O-RAN架构中多个MVNO竞争PRB资源时如何满足延迟约束并最小化资源利用的问题。

Method: 基于大数定律设计奖励函数，提出PDA-DRL算法，并引入基于奖励的个性化模型权重共享方法。

Result: PDA-DRL比基线方法平均延迟降低38%，个性化方法优于传统聚合策略。

Conclusion: PDA-DRL和个性化权重共享方法在O-RAN中有效优化资源分配和延迟性能。

Abstract: In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.

</details>


### [47] [Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification](https://arxiv.org/abs/2507.18113)
*Junyong Jiang,Buwei Tian,Chenxing Xu,Songze Li,Lu Dong*

Main category: cs.LG

TL;DR: 提出一种利用大语言模型生成对抗性奖励的方法，通过现有环境中的代理引导目标策略输出次优动作，无需修改环境。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法通常需要修改环境或策略，实用性受限。本文旨在提出一种更实用的方法，通过生成对抗性奖励诱导目标代理做出次优决策。

Method: 提出奖励迭代优化框架，利用大语言模型生成针对目标代理漏洞的对抗性奖励；设计关键状态识别算法，定位目标代理最脆弱状态。

Result: 在多样化环境中的实验结果表明，该方法优于现有方法。

Conclusion: 该方法通过生成对抗性奖励和识别关键状态，有效诱导目标代理做出次优决策，且无需修改环境。

Abstract: Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.

</details>


### [48] [Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning](https://arxiv.org/abs/2507.18122)
*Matthias Otth,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: 语言模型通过最大化自身预测置信度实现自我提升，无需外部验证。研究发现，在数学推理任务中，仅继续最有希望的尝试（基于模型前缀置信度选择）可显著提升性能。前缀置信度缩放优于多数投票，且不易受长度偏差影响。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在数学推理任务中如何通过自身置信度实现性能提升，减少对外部验证的依赖。

Method: 使用模型的前缀置信度选择最有希望的尝试，并在五个数学推理数据集（GSM8K、MATH500、AMC23、AIME24、AIME25）上系统评估。

Result: 前缀置信度缩放（仅32个标记前缀）在准确性与计算效率上优于多数投票，且对长度偏差更不敏感。测试时训练虽优于基础模型，但未超越前缀置信度缩放。

Conclusion: 前缀置信度缩放是一种高效且稳健的方法，适用于数学推理任务，优于传统方法。

Abstract: Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.

</details>


### [49] [Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2507.18139)
*Alberto Marchisio,Muhammad Shafique*

Main category: cs.LG

TL;DR: 本文综述了神经形态计算在自主系统中的应用，重点关注算法、硬件和优化策略，并探讨了事件动态视觉传感器和脉冲神经网络的作用。


<details>
  <summary>Details</summary>
Motivation: 随着智能、自适应和节能自主系统需求的增长，神经形态计算因其仿生特性成为提升感知、决策和响应能力的有前景途径。

Method: 通过整合机器学习、机器人学、神经科学和神经形态工程的视角，综述了神经形态算法、硬件和跨层优化策略。

Result: 研究发现神经形态方法能显著提升能效、鲁棒性、适应性和可靠性，尤其在事件动态视觉传感器和脉冲神经网络的应用中表现突出。

Conclusion: 文章总结了该领域的现状，并指出了实时决策、持续学习以及安全弹性自主系统开发等未来挑战和趋势。

Abstract: The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.

</details>


### [50] [When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label](https://arxiv.org/abs/2507.18153)
*Riting Xia,Rucong Wang,Yulin Liu,Anchen Li,Xueyan Liu,Yan Zhang*

Main category: cs.LG

TL;DR: 论文提出GraphALP框架，结合大语言模型和伪标签技术，解决带噪声标签的类别不平衡图节点分类问题。


<details>
  <summary>Details</summary>
Motivation: 现实图中标签常含噪声，现有方法假设标签干净，无法处理噪声问题。

Method: 基于LLM的过采样生成少数类节点，动态加权伪标签减少噪声，二次LLM过采样缓解分布偏差。

Result: 实验显示GraphALP在带噪声的类别不平衡图上优于现有方法。

Conclusion: GraphALP有效解决了噪声标签和类别不平衡问题，性能优越。

Abstract: Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.

</details>


### [51] [ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory](https://arxiv.org/abs/2507.18183)
*Jianchao Wang,Qingfeng Li,Pengcheng Zheng,Xiaorong Pu,Yazhou Ren*

Main category: cs.LG

TL;DR: 提出了一种名为ChronoSelect的新框架，利用四阶段记忆架构和滑动更新机制，通过时间轨迹分析实现样本分类，显著提升了噪声标签下的学习性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中噪声标签会降低深度学习模型的泛化性能，现有方法未能充分利用学习过程中的时间动态信息。

Method: 设计了四阶段记忆架构和滑动更新机制，压缩预测历史为紧凑时间分布，通过时间轨迹分析和双分支一致性实现样本分类。

Result: 理论证明了框架的收敛性和稳定性，实验验证了其在合成和真实数据集上的领先性能。

Conclusion: ChronoSelect通过动态时间分析有效解决了噪声标签问题，为相关领域提供了新思路。

Abstract: Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.

</details>


### [52] [Goal-based Trajectory Prediction for improved Cross-Dataset Generalization](https://arxiv.org/abs/2507.18196)
*Daniel Grimm,Ahmed Abouelazm,J. Marius Zöllner*

Main category: cs.LG

TL;DR: 提出了一种基于异构图神经网络（GNN）的方法，通过多阶段目标分类提升自动驾驶模型在未见场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要准确预测周围交通参与者的未来状态，但现有模型在新场景中泛化能力不足。

Method: 使用包含交通参与者和矢量化道路网络的异构图神经网络，多阶段分类目标以改进轨迹预测。

Result: 通过跨数据集评估（Argoverse2训练，NuScenes测试），验证了目标选择过程的有效性。

Conclusion: 该方法显著提升了模型在未见场景中的泛化性能。

Abstract: To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.

</details>


### [53] [FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting](https://arxiv.org/abs/2507.18219)
*Zhongzheng Yuan,Lianshuai Guo,Xunkai Li,Yinlin Zhu,Wenyu Wang,Meixia Qu*

Main category: cs.LG

TL;DR: FedSA-GCL是一种半异步联邦图学习框架，通过ClusterCast机制解决同步通信效率低和异步方法在图数据上的语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法依赖同步通信效率低，异步方法未考虑图数据的拓扑特性，导致语义漂移。

Method: 提出FedSA-GCL框架，结合ClusterCast机制利用客户端标签分布差异和图拓扑特性进行半异步训练。

Result: 在多个真实图数据集上测试，FedSA-GCL平均性能优于基线2.92%（Louvain）和3.4%（Metis）。

Conclusion: FedSA-GCL在效率和鲁棒性上表现优异，解决了现有方法的局限性。

Abstract: Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.

</details>


### [54] [Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective](https://arxiv.org/abs/2507.18220)
*Ansei Yonezawa,Heisei Yonezawa,Shuichi Yahagi,Itsuro Kajiwara,Shinya Kijimoto,Hikaru Taniuchi,Kentaro Murakami*

Main category: cs.LG

TL;DR: SINDy-LOM结合稀疏回归技术和新型库学习策略，优化参数化基函数，提升非线性动力学系统建模的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统SINDy方法在库设计上存在困难，无法保证长期预测准确性，因此提出SINDy-LOM以优化库设计。

Method: 采用双层优化架构：内层通过稀疏回归提取模型，外层基于递归长期预测精度优化参数化基函数。

Result: SINDy-LOM提高了模型的解释性和可靠性，减少了用户负担，并在柴油发动机空气路径系统中验证了有效性。

Conclusion: SINDy-LOM通过优化库设计，显著提升了非线性动力学系统建模的性能和实用性。

Abstract: The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.

</details>


### [55] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: 本文首次大规模实验研究了六种基于线性规划的完全校正提升方法，包括两种新方法NM-Boost和QRLP-Boost，在20个数据集上的表现。结果表明，这些方法在使用浅层树时能超越或匹配XGBoost和LightGBM等先进方法，同时生成更稀疏的集成模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于线性规划的完全校正提升方法在理论上具有吸引力，但缺乏实证研究。本文旨在填补这一空白。

Method: 研究六种基于线性规划的提升方法，包括两种新方法NM-Boost和QRLP-Boost，使用启发式和最优基学习器，评估准确性、集成稀疏性、边际分布等。

Result: 完全校正方法在使用浅层树时能超越或匹配XGBoost和LightGBM，且集成更稀疏。还能在不牺牲性能的情况下精简预训练集成。

Conclusion: 完全校正提升方法在特定场景下具有优势，尤其是稀疏性和浅层树表现，但使用最优决策树时存在局限性。

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [56] [Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring](https://arxiv.org/abs/2507.18293)
*Sjoerd van Straten,Alessandro Padella,Marwan Hassani*

Main category: cs.LG

TL;DR: SiamSA-PPM是一种结合Siamese学习和统计增强的自监督学习框架，用于预测性过程监控，通过生成语义有效的轨迹变体提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习PPM方法因现实事件日志的低变异性和小规模而受限的问题。

Method: 采用三种基于统计的转换方法生成新轨迹变体，并结合Siamese学习框架学习过程前缀的通用表示。

Result: 在真实事件日志上的实验表明，SiamSA-PPM在预测任务中表现优于或与现有技术相当，统计增强显著优于随机转换。

Conclusion: SiamSA-PPM为过程预测中的数据增强提供了一种有前景的方向。

Abstract: Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.

</details>


### [57] [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://arxiv.org/abs/2507.18297)
*Sergei Shumilin,Alexander Ryabov,Nikolay Yavich,Evgeny Burnaev,Vladimir Vanovskiy*

Main category: cs.LG

TL;DR: 提出了一种基于可微分物理的无结构网格粗化算法，通过k-means聚类、自动微分和随机最小化算法，显著减少网格点数，同时保持关键点动态精度。


<details>
  <summary>Details</summary>
Motivation: 现代数值模拟计算负载高，需减少离散问题规模同时保持合理精度。

Method: 采用k-means聚类、自动微分和随机最小化算法设计网格粗化算法。

Result: 在两种PDE（线性抛物方程和波动方程）测试中，网格点数减少10倍，关键点动态精度得以保持。

Conclusion: 该算法适用于任意由演化偏微分方程描述的系统模拟。

Abstract: Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.

</details>


### [58] [Regression-aware Continual Learning for Android Malware Detection](https://arxiv.org/abs/2507.18313)
*Daniele Ghiani,Daniele Angioni,Giorgio Piras,Angelo Sotgiu,Luca Minnei,Srishti Gupta,Maura Pintor,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 论文分析了基于持续学习的恶意软件检测中的安全回归问题，提出了一种回归感知的惩罚方法，以减少有害预测变化。


<details>
  <summary>Details</summary>
Motivation: 恶意软件快速演变，传统全量重新训练不切实际，持续学习成为替代方案，但安全回归问题被忽视，可能导致已检测到的威胁重新出现。

Method: 通过调整Positive Congruent Training（PCT）到持续学习环境，以模型无关的方式保留先前的预测行为。

Result: 在ELSA、Tesseract和AZ-Class数据集上的实验表明，该方法有效减少了回归，同时保持了检测性能。

Conclusion: 提出的回归感知惩罚方法在持续学习中有效缓解安全回归问题，增强了恶意软件检测的可靠性。

Abstract: Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.

</details>


### [59] [State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer](https://arxiv.org/abs/2507.18320)
*Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 论文提出了一种名为TIDSIT的新架构，用于解决电池健康状态（SoH）估计中不规则时间序列数据的挑战，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 电池健康监测对安全和效率至关重要，但现有机器学习模型在处理不规则放电数据时存在信息丢失和精度不足的问题。

Method: 提出TIDSIT架构，结合连续时间嵌入和时序注意力机制，有效处理非均匀采样和变长序列。

Result: 在NASA电池退化数据集上，TIDSIT将预测误差降低50%以上，SoH预测误差低于0.58%。

Conclusion: TIDSIT不仅显著提升了电池SoH估计的准确性，还具有通用性，适用于其他不规则时间序列的健康监测任务。

Abstract: The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.

</details>


### [60] [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333)
*Kale-ab Abebe Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 论文指出当前多智能体强化学习（MARL）方法依赖简单惯例而非有效马尔可夫信号恢复，并提出新环境设计原则。


<details>
  <summary>Details</summary>
Motivation: 探讨当前MARL算法在部分可观察环境中的局限性，揭示其成功依赖简单惯例而非真实推理能力。

Method: 通过案例研究分析智能体学习惯例的脆弱性，并对比其在任务设计不同时的表现。

Result: 发现智能体可学习脆弱惯例或真实策略，问题源于环境设计而非模型本身。

Conclusion: 呼吁设计新环境，强调基于观察和记忆的推理能力，避免依赖脆弱惯例。

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.

</details>


### [61] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets通过低秩适应（LoRA）减少HyperDeepONet的参数量，提升预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决HyperDeepONet因高参数量导致的高内存和计算成本问题。

Method: 利用LoRA将超网络的输出层权重矩阵分解为两个低秩矩阵，减少可训练参数。

Result: 在常微分和偏微分方程实验中，参数量减少70%，且预测精度和泛化能力优于常规HyperDeepONet。

Conclusion: PI-LoRA-HyperDeepONets在保持性能的同时显著降低了复杂度。

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


### [62] [A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges](https://arxiv.org/abs/2507.18376)
*Xing Hua,Haodong Chen,Qianqian Duan,Danfeng Hong,Ruijiao Li,Huiliang Shang,Linghua Jiang,Haima Yang,Dawei Zhang*

Main category: cs.LG

TL;DR: 扩散模型在农业中应用的最新进展，包括病虫害检测、遥感图像增强等，显著提升了数据增强和图像生成的质量。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长和耕地资源稀缺促使智能农业和精准农业成为发展方向，扩散模型因其训练稳定性和生成质量优于传统GANs。

Method: 综述扩散模型在农业中的应用，包括数据增强、图像生成和去噪等任务。

Result: 实验表明扩散模型显著提升了模型精度和鲁棒性，尤其在复杂环境中表现优异。

Conclusion: 尽管存在计算效率和泛化能力的挑战，扩散模型有望在智能农业中发挥更大作用，支持全球农业可持续发展。

Abstract: With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.

</details>


### [63] [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](https://arxiv.org/abs/2507.18423)
*Mizuki Funato,Yohei Sawada*

Main category: cs.LG

TL;DR: HYPER是一种结合多模型集成和储层计算的新方法，用于在数据稀缺条件下提高洪水预测的准确性、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺地区洪水预测和水资源管理中的准确性、可解释性和计算效率问题。

Method: 使用贝叶斯模型平均（BMA）结合43个未校准的流域水文模型，并通过储层计算（RC）进行误差校正。

Result: 在数据丰富和数据稀缺场景下，HYPER均表现优异，计算效率高且性能稳定。

Conclusion: HYPER为无测站流域提供了一种高效、稳健且可推广的解决方案。

Abstract: Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.

</details>


### [64] [Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning](https://arxiv.org/abs/2507.18519)
*Leiji Zhang,Zeyu Wang,Xin Li,Yao-Hui Li*

Main category: cs.LG

TL;DR: 论文提出了改进的双模拟度量方法，解决了传统方法在表示特定场景和依赖预定义权重方面的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统双模拟度量存在无法表示某些独特场景和依赖预定义权重的问题，限制了其在强化学习任务中的应用。

Method: 通过引入状态-动作对的度量，提出了改进的双模拟度量，包括更精确的奖励差距定义和自适应系数的更新算子。

Result: 理论分析证明了新度量的收敛性和表示区分性，实验在DeepMind Control和Meta-World基准上验证了其有效性。

Conclusion: 改进的双模拟度量在理论和实验上均表现出优越性，为强化学习任务提供了更有效的表示学习技术。

Abstract: Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.

</details>


### [65] [GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning](https://arxiv.org/abs/2507.18521)
*Zhongtian Sun,Anoushka Harit,Alexandra Cristea,Christl A. Donnelly,Pietro Liò*

Main category: cs.LG

TL;DR: GLANCE框架通过逻辑引导推理、动态图优化和自适应聚类，提升异质图表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异质图上表现不佳，因邻居聚合不区分且高阶结构模式利用不足。

Method: 结合逻辑层、多头注意力边剪枝和聚类机制，优化图结构和全局模式捕捉。

Result: 在Cornell等基准数据集上表现优异，提供轻量且可解释的解决方案。

Conclusion: GLANCE适用于异质图挑战，性能强且可解释。

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.

</details>


### [66] [C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation](https://arxiv.org/abs/2507.18533)
*Magnus Bengtsson,Kenneth Östberg*

Main category: cs.LG

TL;DR: C2G-KD是一种无需真实数据、基于教师模型和PCA几何约束的知识蒸馏框架，通过生成合成样本实现训练。


<details>
  <summary>Details</summary>
Motivation: 研究如何在无需真实训练数据的情况下，利用教师模型和几何约束生成有效的合成样本，以支持知识蒸馏。

Method: 使用类条件生成器，结合教师模型的输出和PCA几何约束，通过语义和结构损失生成样本。

Result: 在MNIST上的实验表明，即使仅用每类两个真实样本，也能生成有效的合成训练样本。

Conclusion: C2G-KD框架展示了在极少量真实数据支持下，生成高质量合成样本的潜力。

Abstract: We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.

</details>


### [67] [The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection](https://arxiv.org/abs/2507.18549)
*Steven A. Frank*

Main category: cs.LG

TL;DR: 论文提出了一种通用的力-度量-偏置（FMB）定律，通过Price方程揭示了不同学习算法、优化方法和自然选择的共同数学结构。


<details>
  <summary>Details</summary>
Motivation: 揭示看似不同的学习算法、优化方法和自然选择之间的共同数学结构，为理解和设计跨学科的学习算法提供理论基础。

Method: 使用Price方程对变化进行简单的符号划分，推导出力-度量-偏置（FMB）定律：Δθ = Mf + b + ξ。

Result: FMB定律统一了自然选择、贝叶斯更新、牛顿法、随机梯度下降等多种算法，并解释了Fisher信息、KL散度等在学习动力学中的自然出现。

Conclusion: FMB定律为跨学科的学习算法提供了统一的理论基础，有助于理解和比较不同算法的性能。

Abstract: Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.

</details>


### [68] [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)
*Jiale Chen,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文揭示了GPTQ量化方法与Babai最近平面算法的数学等价性，为量化算法提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 研究GPTQ量化方法的内在几何意义和理论保证，以改进大规模语言模型的量化部署。

Method: 通过数学论证，证明GPTQ在特定条件下与Babai最近平面算法等价，并分析其误差传播和上界。

Result: GPTQ的误差传播具有几何解释，且在无裁剪条件下继承了Babai算法的误差上界。

Conclusion: 研究为GPTQ提供了坚实的理论基础，并为未来量化算法的设计开辟了新方向。

Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.

</details>


### [69] [Linear Memory SE(2) Invariant Attention](https://arxiv.org/abs/2507.18597)
*Ethan Pronovost,Neha Boloor,Peter Schleede,Noureldin Hendy,Andres Morales,Nicholas Roy*

Main category: cs.LG

TL;DR: 提出了一种线性内存消耗的SE(2)不变性注意力机制，用于自动驾驶中的空间数据处理。


<details>
  <summary>Details</summary>
Motivation: 解决现有SE(2)不变性网络架构因显式计算所有对象对的相对位姿而导致的内存消耗问题。

Method: 设计了一种SE(2)不变性的缩放点积注意力机制，基于相对位姿实现线性内存消耗。

Result: 实验表明该方法易于实现，且性能优于非不变性架构。

Conclusion: 该方法为自动驾驶任务提供了一种高效且性能优越的空间数据处理方案。

Abstract: Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.

</details>


### [70] [Demystify Protein Generation with Hierarchical Conditional Diffusion Models](https://arxiv.org/abs/2507.18603)
*Zinan Ling,Yi Shi,Da Yan,Yang Zhou,Bo Hui*

Main category: cs.LG

TL;DR: 提出了一种多级条件扩散模型，结合序列和结构信息进行端到端蛋白质设计，并引入Protein-MMD评估指标。


<details>
  <summary>Details</summary>
Motivation: 蛋白质设计需要生成功能性和新颖的序列，但现有条件扩散模型在可靠生成方面仍存在问题。

Method: 开发多级条件扩散模型，整合序列和结构信息，同时生成多级表示，并提出Protein-MMD评估指标。

Result: 实验结果表明，该框架和评估指标在条件蛋白质生成任务中表现优异。

Conclusion: 多级条件扩散模型和Protein-MMD为蛋白质设计提供了高效且可靠的解决方案。

Abstract: Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.

</details>


### [71] [Moving Out: Physically-grounded Human-AI Collaboration](https://arxiv.org/abs/2507.18623)
*Xuhui Kang,Sung-Wook Lee,Haolin Liu,Yuyan Wang,Yen-Ling Kuo*

Main category: cs.LG

TL;DR: 论文提出了一种新的人机协作基准测试Moving Out，并提出了BASS方法来提升AI在物理环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决物理约束下的人机协作问题，特别是在连续状态-动作空间和受限动态环境中的复杂性。

Method: 提出了BASS方法（行为增强、模拟和选择），通过增强代理的多样性和对动作结果的理解来应对挑战。

Result: 实验表明，BASS在AI-AI和人机协作中优于现有先进模型。

Conclusion: Moving Out基准和BASS方法为物理环境中的人机协作提供了有效的解决方案。

Abstract: The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [72] [Sliding Window Informative Canonical Correlation Analysis](https://arxiv.org/abs/2507.17921)
*Arvind Prasadan*

Main category: stat.ML

TL;DR: 提出了一种在线流数据环境下的CCA扩展方法SWICCA，结合流式PCA和滑动窗口实时估计CCA组件。


<details>
  <summary>Details</summary>
Motivation: 解决传统CCA无法处理流式数据的问题，提供实时分析能力。

Method: 使用流式PCA作为后端，结合滑动窗口样本实时估计CCA组件。

Result: 通过数值模拟验证性能，提供理论保证，适用于高维数据。

Conclusion: SWICCA是一种高效、可扩展的流式数据CCA方法。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlated
sets of features between two datasets. In this paper, we propose a novel
extension of CCA to the online, streaming data setting: Sliding Window
Informative Canonical Correlation Analysis (SWICCA). Our method uses a
streaming principal component analysis (PCA) algorithm as a backend and uses
these outputs combined with a small sliding window of samples to estimate the
CCA components in real time. We motivate and describe our algorithm, provide
numerical simulations to characterize its performance, and provide a
theoretical performance guarantee. The SWICCA method is applicable and scalable
to extremely high dimensions, and we provide a real-data example that
demonstrates this capability.

</details>


### [73] [A Two-armed Bandit Framework for A/B Testing](https://arxiv.org/abs/2507.18118)
*Jinjuan Wang,Qianglin Wen,Yu Zhang,Xiaodong Yan,Chengchun Shi*

Main category: stat.ML

TL;DR: 本文提出了一种基于两臂老虎机框架的方法，通过双重稳健估计、构建测试统计量和置换法计算p值，显著提升了A/B测试的效能。


<details>
  <summary>Details</summary>
Motivation: A/B测试在科技公司中广泛应用，但现有方法在效能上存在不足。本文旨在通过改进方法提升测试的统计功效。

Method: 方法包括三步：(i)使用双重稳健估计生成伪结果，(ii)在两臂老虎机框架下构建测试统计量，(iii)通过置换法计算p值。

Result: 通过理论分析、数值实验和真实数据验证，该方法在性能上优于现有方法。

Conclusion: 提出的方法显著提升了A/B测试的效能，适用于实际应用。

Abstract: A/B testing is widely used in modern technology companies for policy
evaluation and product deployment, with the goal of comparing the outcomes
under a newly-developed policy against a standard control. Various causal
inference and reinforcement learning methods developed in the literature are
applicable to A/B testing. This paper introduces a two-armed bandit framework
designed to improve the power of existing approaches. The proposed procedure
consists of three main steps: (i) employing doubly robust estimation to
generate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to
construct the test statistic, and (iii) applying a permutation-based method to
compute the $p$-value. We demonstrate the efficacy of the proposed method
through asymptotic theories, numerical experiments and real-world data from a
ridesharing company, showing its superior performance in comparison to existing
methods.

</details>


### [74] [Learning graphons from data: Random walks, transfer operators, and spectral clustering](https://arxiv.org/abs/2507.18147)
*Stefan Klus,Jason J. Bramburger*

Main category: stat.ML

TL;DR: 论文提出了一种将信号的随机过程与图上的随机游走联系起来的方法，利用图论中的图子（graphon）概念，扩展了传统谱聚类方法，并展示了如何从信号数据中重构转移概率密度和图子。


<details>
  <summary>Details</summary>
Motivation: 研究信号在时间上的随机演化过程，尤其是如何在离散采样点间随机切换状态，并探索其与图上的随机游走之间的联系。

Method: 引入与图子上随机游走相关的转移算子（如Koopman和Perron-Frobenius算子），并通过信号数据估计这些算子及其特征值和特征函数，用于聚类分析。

Result: 成功将谱聚类方法从图扩展到图子，并能从信号中重构转移概率密度和可逆随机游走的图子。

Conclusion: 该方法在合成和真实信号（如温度和股票指数）中验证了有效性，为信号分析提供了新工具。

Abstract: Many signals evolve in time as a stochastic process, randomly switching
between states over discretely sampled time points. Here we make an explicit
link between the underlying stochastic process of a signal that can take on a
bounded continuum of values and a random walk process on a graphon. Graphons
are infinite-dimensional objects that represent the limit of convergent
sequences of graphs whose size tends to infinity. We introduce transfer
operators, such as the Koopman and Perron--Frobenius operators, associated with
random walk processes on graphons and then illustrate how these operators can
be estimated from signal data and how their eigenvalues and eigenfunctions can
be used for detecting clusters, thereby extending conventional spectral
clustering methods from graphs to graphons. Furthermore, we show that it is
also possible to reconstruct transition probability densities and, if the
random walk process is reversible, the graphon itself using only the signal.
The resulting data-driven methods are applied to a variety of synthetic and
real-world signals, including daily average temperatures and stock index
values.

</details>


### [75] [On Reconstructing Training Data From Bayesian Posteriors and Trained Models](https://arxiv.org/abs/2507.18372)
*George Wynne*

Main category: stat.ML

TL;DR: 论文提出了一种数学框架来分析训练数据重构攻击，首次在文献中实现了贝叶斯模型的分数匹配重构方法。


<details>
  <summary>Details</summary>
Motivation: 公开模型参数可能导致训练数据被重构攻击，这是现代机器学习的主要漏洞。

Method: 建立数学框架，通过最大均值差异等价性分析易受攻击的数据特征，并提出分数匹配框架用于贝叶斯和非贝叶斯模型的数据重构。

Result: 首次实现了贝叶斯模型的训练数据重构。

Conclusion: 该研究为理解和防御训练数据重构攻击提供了理论和方法基础。

Abstract: Publicly releasing the specification of a model with its trained parameters
means an adversary can attempt to reconstruct information about the training
data via training data reconstruction attacks, a major vulnerability of modern
machine learning methods. This paper makes three primary contributions:
establishing a mathematical framework to express the problem, characterising
the features of the training data that are vulnerable via a maximum mean
discrepancy equivalance and outlining a score matching framework for
reconstructing data in both Bayesian and non-Bayesian models, the former is a
first in the literature.

</details>


### [76] [DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts](https://arxiv.org/abs/2507.18464)
*Miguel Aspis,Sebastián A. Cajas Ordónez,Andrés L. Suárez-Cetrulo,Ricardo Simón Carbajo*

Main category: stat.ML

TL;DR: DriftMoE是一种在线混合专家架构，通过协同训练框架解决现有自适应集成方法的局限性，实现高效的概念漂移适应。


<details>
  <summary>Details</summary>
Motivation: 现有自适应集成方法通常依赖粗粒度适应机制或简单投票方案，无法充分利用专家知识。

Method: DriftMoE采用紧凑的神经网络路由器与增量Hoeffding树专家池协同训练，通过共生学习循环实现专家专业化。

Result: 在九种数据流学习基准测试中，DriftMoE表现优异，与最先进的自适应集成方法竞争。

Conclusion: DriftMoE提供了一种高效且原则性的概念漂移适应方法，代码和数据已开源。

Abstract: Learning from non-stationary data streams subject to concept drift requires
models that can adapt on-the-fly while remaining resource-efficient. Existing
adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or
simple voting schemes that fail to optimally leverage specialized knowledge.
This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture
that addresses these limitations through a novel co-training framework.
DriftMoE features a compact neural router that is co-trained alongside a pool
of incremental Hoeffding tree experts. The key innovation lies in a symbiotic
learning loop that enables expert specialization: the router selects the most
suitable expert for prediction, the relevant experts update incrementally with
the true label, and the router refines its parameters using a multi-hot
correctness mask that reinforces every accurate expert. This feedback loop
provides the router with a clear training signal while accelerating expert
specialization. We evaluate DriftMoE's performance across nine state-of-the-art
data stream learning benchmarks spanning abrupt, gradual, and real-world drifts
testing two distinct configurations: one where experts specialize on data
regimes (multi-class variant), and another where they focus on single-class
specialization (task-based variant). Our results demonstrate that DriftMoE
achieves competitive results with state-of-the-art stream learning adaptive
ensembles, offering a principled and efficient approach to concept drift
adaptation. All code, data pipelines, and reproducibility scripts are available
in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.

</details>


### [77] [Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise](https://arxiv.org/abs/2507.18520)
*Keyi Li,Yuval Kluger,Boris Landa*

Main category: stat.ML

TL;DR: 本文提出了一种无超参数的方法，用于估计异方差噪声下的噪声幅度并校正欧氏距离，适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 异方差噪声会扭曲欧氏距离，影响数据几何表示，需要一种无需先验知识的方法来校正。

Method: 开发了一种联合估计噪声幅度和校正距离的算法，无需假设清洁数据结构或噪声分布。

Result: 理论证明噪声幅度和距离的估计误差以多项式速率收敛为零，实验验证了方法的有效性。

Conclusion: 该方法在合成和真实数据（如单细胞RNA测序）中均表现优异，显著提升了距离计算的鲁棒性。

Abstract: Pairwise Euclidean distance calculation is a fundamental step in many machine
learning and data analysis algorithms. In real-world applications, however,
these distances are frequently distorted by heteroskedastic
noise$\unicode{x2014}$a prevalent form of inhomogeneous corruption
characterized by variable noise magnitudes across data observations. Such noise
inflates the computed distances in a nontrivial way, leading to
misrepresentations of the underlying data geometry. In this work, we address
the tasks of estimating the noise magnitudes per observation and correcting the
pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,
we show that in general high-dimensional settings and without assuming prior
knowledge on the clean data structure or noise distribution, both tasks can be
performed reliably, even when the noise levels vary considerably. Specifically,
we develop a principled, hyperparameter-free approach that jointly estimates
the noise magnitudes and corrects the distances. We provide theoretical
guarantees for our approach, establishing probabilistic bounds on the
estimation errors of both noise magnitudes and distances. These bounds,
measured in the normalized $\ell_1$ norm, converge to zero at polynomial rates
as both feature dimension and dataset size increase. Experiments on synthetic
datasets demonstrate that our method accurately estimates distances in
challenging regimes, significantly improving the robustness of subsequent
distance-based computations. Notably, when applied to single-cell RNA
sequencing data, our method yields noise magnitude estimates consistent with an
established prototypical model, enabling accurate nearest neighbor
identification that is fundamental to many downstream analyses.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [78] [Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search](https://arxiv.org/abs/2507.17953)
*Chang Eun Song,Weihong Xu,Keming Fan,Soumil Jain,Gopabandhu Hota,Haichao Yang,Leo Liu,Kerem Akarvardar,Meng-Fan Chang,Carlos H. Diaz,Gert Cauwenberghs,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: Clo-HDnn是一种用于持续学习任务的设备端学习加速器，结合超维计算和低成本技术优化精度与效率。


<details>
  <summary>Details</summary>
Motivation: 为新兴的持续学习任务提供高效的设备端学习解决方案。

Method: 集成超维计算、低成本Kronecker HD编码器和权重聚类特征提取，采用梯度无关的持续学习方法。

Result: 能效比达到4.66 TFLOPS/W（特征提取）和3.78 TOPS/W（分类器），比现有技术提升7.77倍和4.85倍。

Conclusion: Clo-HDnn通过高效设计和双模式操作，显著提升了持续学习任务的能效和性能。

Abstract: Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging
continual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing
(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature
extraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts
gradient-free CL to efficiently update and store the learned knowledge in the
form of class hypervectors. Its dual-mode operation enables bypassing costly
feature extraction for simpler datasets, while progressive search reduces
complexity by up to 61% by encoding and comparing only partial query
hypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),
Clo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL
accelerators.

</details>
