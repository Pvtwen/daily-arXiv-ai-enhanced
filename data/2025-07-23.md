<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 8]
- [cs.LG](#cs.LG) [Total: 54]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [MSGM: A Multi-Scale Spatiotemporal Graph Mamba for EEG Emotion Recognition](https://arxiv.org/abs/2507.15914)
*Hanwen Liu,Yifeng Gong,Zuwei Yan,Zeheng Zhuang,Jiaxuan Lu*

Main category: eess.SP

TL;DR: 提出了一种名为MSGM的多尺度时空图Mamba框架，用于基于EEG的情感识别，通过多窗口时间分割、双模态空间图建模和Mamba架构的高效融合，在保持计算效率的同时提升了情感分类的准确性


<details>
  <summary>Details</summary>
Motivation: 现有的基于EEG的情感识别方法在捕获多尺度时空动态特征和确保实时应用的计算效率方面存在困难，往往过度简化时间粒度和空间层次结构，从而限制了准确性

Method: 提出MSGM框架，集成了多窗口时间分割、基于神经解剖学先验的全局-局部双模态空间图建模、多深度图卷积网络(GCN)和令牌嵌入融合模块，以及Mamba的状态空间建模，实现线性复杂度下的动态时空交互

Result: 仅使用一个MSST-Mamba层，MSGM在SEED、THU-EP和FACED数据集上超越了领先方法，在受试者独立情感分类任务中表现优于基线方法，同时在NVIDIA Jetson Xavier NX上实现了鲁棒的准确性和毫秒级推理速度

Conclusion: MSGM成功解决了EEG情感识别中多尺度时空动态捕获和计算效率的挑战，通过创新的架构设计实现了准确性和实时性的平衡，为实际应用提供了有效的解决方案

Abstract: EEG-based emotion recognition struggles with capturing multi-scale
spatiotemporal dynamics and ensuring computational efficiency for real-time
applications. Existing methods often oversimplify temporal granularity and
spatial hierarchies, limiting accuracy. To overcome these challenges, we
propose the Multi-Scale Spatiotemporal Graph Mamba (MSGM), a novel framework
integrating multi-window temporal segmentation, bimodal spatial graph modeling,
and efficient fusion via the Mamba architecture. By segmenting EEG signals
across diverse temporal scales and constructing global-local graphs with
neuroanatomical priors, MSGM effectively captures fine-grained emotional
fluctuations and hierarchical brain connectivity. A multi-depth Graph
Convolutional Network (GCN) and token embedding fusion module, paired with
Mamba's state-space modeling, enable dynamic spatiotemporal interaction at
linear complexity. Notably, with just one MSST-Mamba layer, MSGM surpasses
leading methods in the field on the SEED, THU-EP, and FACED datasets,
outperforming baselines in subject-independent emotion classification while
achieving robust accuracy and millisecond-level inference on the NVIDIA Jetson
Xavier NX.

</details>


### [2] [Modeling and Analysis of Land-to-Ship Maritime Wireless Channels at 5.8 GHz](https://arxiv.org/abs/2507.15969)
*Shu Sun,Yulu Guo,Meixia Tao,Wei Feng,Jun Chen,Ruifeng Gao,Ye Li,Jue Wang,Theodore S. Rappaport*

Main category: eess.SP

TL;DR: 本文通过广泛的测量活动研究了5.8 GHz频段陆地到船舶的海上无线信道特性，提出了具有物理基础的大尺度路径损耗模型，引入了海浪诱导固定点(SWIFT)衰落概念，并使用多种模型分析小尺度衰落，为海上无线系统设计提供了有价值的信道建模参数


<details>
  <summary>Details</summary>
Motivation: 海上信道建模对于设计海洋环境中的鲁棒通信系统至关重要，因为海浪和风等因素会影响信号传播。需要建立准确的海上无线信道模型来指导海上通信系统的设计和部署

Method: 基于广泛的测量活动研究5.8 GHz频段陆地到船舶的海上无线信道特性，同时收集水文和气象信息。提出具有物理基础的大尺度路径损耗模型，引入海浪诱导固定点(SWIFT)衰落概念，使用增强的双射线模型结合船舶旋转运动来模拟SWIFT衰落，并利用双波扩散功率(TWDP)和非对称拉普拉斯分布等多种模型研究小尺度衰落

Result: 提出的大尺度路径损耗模型具有高精度；增强的双射线模型能够很好地模拟SWIFT衰落，特别是对于适度的天线运动；非对称拉普拉斯分布在大多数情况下表现良好，而TWDP模型能更好地捕捉恶劣海况下的双峰衰落；通过基尼指数和莱斯K因子检验了海上信道稀疏性，并表征了时间色散特性

Conclusion: 建立的信道模型和参数特性为海上无线系统设计和部署提供了有价值的见解，能够指导在动态海洋环境中设计鲁棒的通信系统

Abstract: Maritime channel modeling is crucial for designing robust communication
systems in marine environments, where factors like waves and wind impact signal
propagation. This article investigates land-to-ship maritime wireless channel
characteristics at 5.8 GHz based upon an extensive measurement campaign, with
concurrent hydrological and meteorological information collection. First, a
novel large-scale path loss model with physical foundation and high accuracy is
proposed for dynamic marine environments. Then, we introduce the concept of
sea-wave-induced fixed-point (SWIFT) fading, a peculiar phenomenon in maritime
scenarios that captures the impact of sea surface fluctuations on received
power. An enhanced two-ray model incorporating vessel rotational motion is
propounded to simulate the SWIFT fading, showing good alignment with measured
data, particularly for modest antenna movements. Next, the small-scale fading
is studied by leveraging a variety of models including the two-wave with
diffuse power (TWDP) and asymmetric Laplace distributions, with the latter
performing well in most cases, while TWDP better captures bimodal fading in
rough seas. Furthermore, maritime channel sparsity is examined via the Gini
index and Rician $K$ factor, and temporal dispersion is characterized. The
resulting channel models and parameter characteristics offer valuable insights
for maritime wireless system design and deployment.

</details>


### [3] [Meta-Reinforcement Learning Optimization for Movable Antenna-aided Full-Duplex CF-DFRC Systems with Carrier Frequency Offset](https://arxiv.org/abs/2507.16132)
*Yue Xiu,Wanting Lyu,You Li,Ran Yang,Phee Lep Yeoh,Wei Zhang,Guangyi Liu,Ning Wei*

Main category: eess.SP

TL;DR: 该论文提出了一种基于可移动天线的无小区双功能雷达通信系统，通过元强化学习优化天线位置和波束成形来缓解载波频偏问题，显著提升频谱效率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 在6G无线网络中，无小区双功能雷达通信系统虽然能提高频谱效率，但宽带场景下的载波频偏会严重降低通信容量和感知精度，需要新的解决方案来缓解这一问题。

Method: 将可移动天线集成到无小区双功能雷达通信框架中，提出基于鲁棒元强化学习的两阶段交替优化策略：第一阶段采用流形优化和惩罚对偶分解求解载波频偏鲁棒最坏情况子问题；第二阶段采用数据驱动方式联合优化天线位置和波束成形向量。

Result: 仿真结果表明，所提出的元强化学习方法在载波频偏影响下的通信和感知性能显著优于传统深度强化学习方案，且相比固定位置天线，可移动天线辅助的系统表现出更优的性能。

Conclusion: 通过将可移动天线技术与元强化学习相结合，成功解决了宽带无小区双功能雷达通信系统中载波频偏带来的性能退化问题，为6G网络中的频谱共享技术提供了有效的解决方案。

Abstract: By enabling spectrum sharing between radar and communication operations, the
cell-free dual-functional radar-communication (CF-DFRC) system is a promising
candidate to significantly improve spectrum efficiency in future
sixth-generation (6G) wireless networks. However, in wideband scenarios,
synchronization errors caused by carrier frequency offset (CFO) can severely
reduce both communication capacity and sensing accuracy. To address this
challenge, this paper integrates movable antennas (MAs) into the CF-DFRC
framework, leveraging their spatial flexibility and adaptive beamforming to
dynamically mitigate CFO-induced impairments. To fully exploit the advantages
of MAs in wideband scenarios with CFO, we aim to maximize the worst-case
sum-rate of communication and sensing by jointly optimizing MA positions,
{beamforming}, and CFO parameters, subject to transmit power and MA positioning
constraints. Due to the non-convex nature of the problem, we propose a robust
meta reinforcement learning (MRL)-based two-stage alternating optimization
strategy. In the first stage, we employ manifold optimization (MO) with penalty
dual decomposition (PDD) to solve the CFO-robust worst-case subproblem. In the
second stage, we adopt to jointly optimize {the MA positions and beamforming
vectors} in a data-driven manner {for dynamic wireless environments}.
Simulation results show that the proposed MRL approach significantly
outperforms conventional deep reinforcement learning (DRL) schemes in both
communication and sensing performance under CFO impairments. Furthermore,
compared to fixed-position antennas (FPAs), the MA-aided CF-DFRC system
exhibits

</details>


### [4] [Joint Active and Passive Beamforming for Energy-Efficient STARS with Quantization and Element Selection in ISAC Systems](https://arxiv.org/abs/2507.16210)
*Li-Hsiang Shen,Yi-Hsuan Chiu*

Main category: eess.SP

TL;DR: 本文研究了同时发射和反射可重配置智能表面(STARS)辅助的集成感知通信(ISAC)系统，通过联合优化基站波束成形和STARS配置来最大化能效，提出了AQUES算法来解决非凸混合整数优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有ISAC系统在全空间能效数据传输和目标感知方面存在挑战，需要一种能够同时优化通信和感知性能的智能表面技术来提升系统能效。

Method: 提出了联合主动-被动波束成形、量化和元件选择(AQUES)方案，基于交替优化框架：使用拉格朗日对偶和Dinkelbach变换处理分式问题，连续凸近似(SCA)进行凸化，惩罚对偶分解(PDD)和惩罚凸凹规划(PCCP)求解幅度和相移，启发式搜索确定量化级别，整数松弛处理元件选择。

Result: 仿真结果表明，采用AQUES方案的STARS-ISAC系统显著提升了能效，同时满足通信速率和感知质量要求。耦合STARS由于硬件复杂度降低，相比独立和松弛STARS表现出更优的能效性能。AQUES在各种网络参数和部署场景下均优于现有配置和基准方法。

Conclusion: STARS辅助的ISAC系统通过所提出的AQUES优化方案能够有效提升能效性能，耦合STARS架构在降低硬件复杂度的同时实现了最佳的能效表现，为未来无线通信系统的绿色高效发展提供了新的解决方案。

Abstract: This paper investigates a simultaneously transmitting and reflecting
reconfigurable intelligent surface (STARS)-aided integrated sensing and
communication (ISAC) systems in support of full-space energy-efficient data
transmissions and target sensing. We formulate an energy efficiency (EE)
maximization problem jointly optimizing dual-functional radar-communication
(DFRC)-empowered base station (BS) ISAC beamforming and STARS configurations of
amplitudes, phase-shifts, quantization levels as well as element selection.
Furthermore, relaxed/independent/coupled STARS are considered to examine
architectural flexibility. To tackle the non-convex and mixed-integer problem,
we propose a joint active-passive beamforming, quantization and element
selection (AQUES) scheme based on alternating optimization: Lagrangian dual and
Dinkelbach's transformation deals with fractions, whereas successive convex
approximation (SCA) convexifies the problem; Penalty dual decomposition (PDD)
framework and penalty-based convex-concave programming (PCCP) procedure solves
amplitude and phase-shifts; Heuristic search decides the quantization level;
Integer relaxation deals with the element selection. Simulation results
demonstrate that STARS-ISAC with the proposed AQUES scheme significantly
enhances EE while meeting communication rates and sensing quality requirements.
The coupled STARS further highlights its superior EE performance over
independent and relaxed STARS thanks to its reduced hardware complexity.
Moreover, AQUES outperforms existing configurations and benchmark methods in
the open literature across various network parameters and deployment scenarios.

</details>


### [5] [Liquid Intelligent Metasurface for Fluid Antennas-Assisted Networks](https://arxiv.org/abs/2507.16211)
*Li-Hsiang Shen*

Main category: eess.SP

TL;DR: 本文提出了一种新颖的液体智能超表面(LIM)辅助的下行链路多用户MISO系统，通过流体天线和液体元件实现电磁和空间双重可重构性，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统可重构超表面辅助系统采用静态几何结构，限制了系统性能。为了突破这一局限，需要开发能够同时实现电磁重构和空间重构的新型架构，以进一步提升多用户通信系统的性能。

Method: 提出FAS-LIM架构，基站和超表面分别配备流体天线和液体元件；建立和速率最大化问题，联合优化波束成形、相移控制以及流体天线和液体元件的位置；采用交替优化方法，引入辅助变量，结合连续凸近似(SCA)和惩罚凸凹过程(PCCP)求解非凸子问题。

Result: 仿真结果表明，所提出的FAS-LIM架构在各种参数设置下均显著优于采用传统固定超表面和固定天线阵列的基准方法，验证了联合电磁和空间可重构性的有效性。

Conclusion: 液体智能超表面结合流体天线的架构能够通过动态调整小尺度位置实现联合电磁和空间可重构，为多用户MISO系统提供了新的设计思路，并在性能上取得了显著提升。

Abstract: This paper proposes a novel liquid intelligent metasurface (LIM)-assisted
downlink multi-user multiple-input single-output (MISO) system, wherein both
the base station (BS) and the metasurface are respectively equipped with fluid
antennas (FA) and liquid elements. Unlike conventional reconfigurable
metasurface-assisted systems with static geometries, the proposed architecture
enables joint electromagnetic and spatial reconfigurability by allowing both
the FA-empowered BS (FAS) and LIM to dynamically adjust their small-scale
positions in addition to beamforming and phase-shift controls. We formulate a
sum-rate maximization problem that jointly optimizes the BS beamforming, LIM
phase-shifts, and the positions of fluid antennas and liquid elements. The
problem is highly non-convex due to coupling between variables, fractional
expressions, unit-modulus constraints as well as spatial correlation functions.
To address these challenges, we adopt alternating optimization and introduce
auxiliary variables and employ successive convex approximation (SCA) as well as
the penalty convex-concave procedure (PCCP) to solve the respective
subproblems. Simulation results have demonstrated that the proposed FAS-LIM
architecture significantly outperforms benchmark methods employing conventional
fixed metasurface and fixed antenna arrays in terms of various parameter
settings.

</details>


### [6] [Latency Minimization Oriented Radio and Computation Resource Allocations for 6G V2X Networks with ISCC](https://arxiv.org/abs/2507.16375)
*Peng Liu,Xinyi Wang,Zesong Fei,Yuan Wu,Jie Xu,Arumugam Nallanathan*

Main category: eess.SP

TL;DR: 本文研究了6G网络中集成感知通信计算(ISCC)的车联网系统，通过联合优化无线电和计算资源分配，公平地最小化车辆感知完成延迟，同时确保检测概率约束。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，移动边缘计算(MEC)和集成感知通信(ISAC)技术的结合为车联网(V2X)应用带来了新机遇。车辆需要同时进行环境感知和数据卸载处理，但现有系统在资源分配和延迟优化方面存在挑战，特别是如何在保证检测性能的同时公平地最小化所有车辆的感知完成延迟。

Method: 提出了一种交替优化算法来解决复杂的混合整数非线性规划(MINLP)问题。具体包括：1)使用分支定界法确定子带分配；2)通过连续凸近似(SCA)优化车辆发射功率控制；3)基于广义瑞利熵和公平性准则，分别以闭式形式推导基站的接收波束成形和计算资源分配。

Result: 仿真结果表明，所提出的联合资源分配设计显著降低了所有车辆中的最大任务完成延迟。此外，还展示了系统性能与资源利用率之间的几个有趣权衡关系，验证了算法的有效性。

Conclusion: 通过联合优化无线电资源(子带分配、车辆发射功率控制、基站接收波束成形)和基站MEC服务器的计算资源，可以有效实现ISCC使能的V2X系统中车辆感知完成延迟的公平最小化，为6G网络中的智能交通应用提供了有效的解决方案。

Abstract: Incorporating mobile edge computing (MEC) and integrated sensing and
communication (ISAC) has emerged as a promising technology to enable integrated
sensing, communication, and computing (ISCC) in the sixth generation (6G)
networks. ISCC is particularly attractive for vehicle-to-everything (V2X)
applications, where vehicles perform ISAC to sense the environment and
simultaneously offload the sensing data to roadside base stations (BSs) for
remote processing. In this paper, we investigate a particular ISCC-enabled V2X
system consisting of multiple multi-antenna BSs serving a set of single-antenna
vehicles, in which the vehicles perform their respective ISAC operations (for
simultaneous sensing and offloading to the associated BS) over orthogonal
sub-bands. With the focus on fairly minimizing the sensing completion latency
for vehicles while ensuring the detection probability constraints, we jointly
optimize the allocations of radio resources (i.e., the sub-band allocation,
transmit power control at vehicles, and receive beamforming at BSs) as well as
computation resources at BS MEC servers. To solve the formulated complex
mixed-integer nonlinear programming (MINLP) problem, we propose an alternating
optimization algorithm. In this algorithm, we determine the sub-band allocation
via the branch-and-bound method, optimize the transmit power control via
successive convex approximation (SCA), and derive the receive beamforming and
computation resource allocation at BSs in closed form based on generalized
Rayleigh entropy and fairness criteria, respectively. Simulation results
demonstrate that the proposed joint resource allocation design significantly
reduces the maximum task completion latency among all vehicles. Furthermore, we
also demonstrate several interesting trade-offs between the system performance
and resource utilizations.

</details>


### [7] [Hybrid RISs for Simultaneous Tunable Reflections and Sensing](https://arxiv.org/abs/2507.16550)
*George C. Alexandropoulos,Nir Shlezinger,Ioannis Gavras,Haiyang Zhang*

Main category: eess.SP

TL;DR: 本文介绍了混合反射感知智能反射面(HRIS)的新兴概念，它能够在可控反射信号的同时感知部分信号，从而实现自主配置和网络管理功能，为6G通信系统提供了新的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统智能反射面(RIS)仅具备反射功能，在信道估计和网络编排方面存在重大挑战，难以实现有效的网络管理。需要开发具备感知能力的新型RIS来克服这些限制。

Method: 提出混合反射感知智能反射面(HRIS)概念，通过超表面单元同时实现可控反射和信号感知功能，建立了描述其双重功能的数学模型，并设计了相应的实现方案。

Result: HRIS能够实现同时通信与感知、多用户上行链路信道估计等应用，性能评估结果验证了HRIS在感知和集成感知通信方面的有效性，展现了计算自主和自配置的能力。

Conclusion: 混合反射感知智能反射面为智能无线环境提供了重要技术突破，通过集成反射和感知功能，显著提升了网络管理能力，为未来6G通信系统的发展奠定了基础。

Abstract: The concept of smart wireless environments envisions dynamic programmable
propagation of information-bearing signals through the deployment of
Reconfigurable Intelligent Surfaces (RISs). Typical RIS implementations include
metasurfaces with passive unit elements capable to reflect their incident waves
in controllable ways. However, this solely reflective operation induces
significant challenges in the RIS orchestration from the wireless network. For
example, channel estimation, which is essential for coherent RIS-empowered
wireless communications, is quite challenging with the available solely
reflecting RIS designs. This chapter reviews the emerging concept of Hybrid
Reflecting and Sensing RISs (HRISs), which enables metasurfaces to reflect the
impinging signal in a controllable manner, while simultaneously sensing a
portion of it. The sensing capability of HRISs facilitates various network
management functionalities, including channel parameter estimation and
localization, while, most importantly, giving rise to computationally
autonomous and self-configuring RISs. The implementation details of HRISs are
first presented, which are then followed by a convenient mathematical model for
characterizing their dual functionality. Then, two indicative applications of
HRISs are discussed, one for simultaneous communications and sensing and
another that showcases their usefulness for estimating the individual channels
in the uplink of a multi-user HRIS-empowered communication system. For both of
these applications, performance evaluation results are included validating the
role of HRISs for sensing as well as integrated sensing and communications.

</details>


### [8] [Generative Diffusion Models for Wireless Networks: Fundamental, Architecture, and State-of-the-Art](https://arxiv.org/abs/2507.16733)
*Dayu Fan,Rui Meng,Xiaodong Xu,Yiming Liu,Guoshun Nan,Chenyuan Feng,Shujun Han,Song Gao,Bingxuan Xu,Dusit Niyato,Tony Q. S. Quek,Ping Zhang*

Main category: eess.SP

TL;DR: 这篇论文系统性地探讨了生成扩散模型(GDMs)在无线网络中的应用，提出了多层无线网络架构，并对现有的基于GDM的方案进行了全面综述，最后提出了关键挑战和潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着生成人工智能技术的快速发展，生成扩散模型在无线网络领域显示出巨大的赋能潜力，但缺乏对其技术演进的全面综述。现有研究虽然关注GDMs在无线网络中的应用，但仍然缺乏系统性的技术回顾。

Method: 从数学原理出发分析GDMs的技术优势，提出包含感知层、传输层、应用层和安全平面的多层无线网络架构，介绍GDM在各层的核心机制，对现有基于GDM的方案进行严格的文献综述分析。

Result: 分析了GDMs的技术优势（抗噪性、训练稳定性、可控性和多模态生成），提出了六个代表性模型，建立了多层无线网络架构框架，对现有方案的创新点、GDMs作用、优缺点进行了全面分析。

Conclusion: 提取了该领域的关键挑战并提供了潜在解决方案，为未来在无线网络中应用生成扩散模型的研究提供了方向性指导，有助于推动该交叉领域的进一步发展。

Abstract: With the rapid development of Generative Artificial Intelligence (GAI)
technology, Generative Diffusion Models (GDMs) have shown significant
empowerment potential in the field of wireless networks due to advantages, such
as noise resistance, training stability, controllability, and multimodal
generation. Although there have been multiple studies focusing on GDMs for
wireless networks, there is still a lack of comprehensive reviews on their
technological evolution. Motivated by this, we systematically explore the
application of GDMs in wireless networks. Firstly, starting from mathematical
principles, we analyze technical advantages of GDMs and present six
representative models. Furthermore, we propose the multi-layer wireless network
architecture including sensing layer, transmission layer, application layer,
and security plane. We also introduce the core mechanisms of GDM at each of the
layers. Subsequently, we conduct a rigorous review on existing GDM-based
schemes, with a focus on analyzing their innovative points, the role of GDMs,
strengths, and weaknesses. Ultimately, we extract key challenges and provide
potential solutions, with the aim of providing directional guidance for future
research in this field.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Quantifying Holistic Review: A Multi-Modal Approach to College Admissions Prediction](https://arxiv.org/abs/2507.15862)
*Jun-Wei Zeng,Jerry Shen*

Main category: cs.LG

TL;DR: 本文提出了综合申请者档案评分(CAPS)框架，通过多模态方法量化建模大学招生的全面评估，将申请者档案分解为学术表现、文书质量和课外活动三个可解释组件，实现透明且可解释的招生评估。


<details>
  <summary>Details</summary>
Motivation: 传统的全面招生评估存在不透明、不一致和让申请者焦虑等关键问题，需要一个量化、透明且可解释的评估框架来改善招生实践的公平性和数据驱动性。

Method: 采用多模态框架CAPS，将申请者档案分解为三个可解释组件：标准化学术评分(SAS)、文书质量指数(EQI)和课外活动影响评分(EIS)，结合transformer语义嵌入、大语言模型评分和XGBoost回归技术。

Result: 在合成但真实的数据集上实验表明，CAPS表现优异：EQI预测R²达到0.80，分类准确率超过75%，宏观F1分数为0.69，加权F1分数为0.74。

Conclusion: CAPS框架成功解决了传统全面评估的主要局限性，为更公平和数据驱动的招生实践铺平了道路，提供了透明且与人类判断一致的可解释评估方法。

Abstract: This paper introduces the Comprehensive Applicant Profile Score (CAPS), a
novel multi-modal framework designed to quantitatively model and interpret
holistic college admissions evaluations. CAPS decomposes applicant profiles
into three interpretable components: academic performance (Standardized
Academic Score, SAS), essay quality (Essay Quality Index, EQI), and
extracurricular engagement (Extracurricular Impact Score, EIS). Leveraging
transformer-based semantic embeddings, LLM scoring, and XGBoost regression,
CAPS provides transparent and explainable evaluations aligned with human
judgment. Experiments on a synthetic but realistic dataset demonstrate strong
performance, achieving an EQI prediction R^2 of 0.80, classification accuracy
over 75%, a macro F1 score of 0.69, and a weighted F1 score of 0.74. CAPS
addresses key limitations in traditional holistic review -- particularly the
opacity, inconsistency, and anxiety faced by applicants -- thus paving the way
for more equitable and data-informed admissions practices.

</details>


### [10] [RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems](https://arxiv.org/abs/2507.15867)
*John Wu,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 本文提出了稀有疾病挖掘代理(RDMA)框架，通过模拟医学专家的诊断模式来识别电子健康记录中的稀有疾病信息，在保护隐私的同时显著提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 标准ICD编码系统无法在电子健康记录中捕获稀有疾病信息，关键信息被埋藏在临床笔记中。现有方法在处理医学缩写、隐式疾病提及方面存在困难，云处理带来隐私风险，且缺乏临床推理能力。

Method: 开发了稀有疾病挖掘代理(RDMA)框架，模拟医学专家识别稀有疾病模式的方式。该框架能够连接分散的临床观察结果，处理临床缩写，识别隐式疾病模式，并在标准硬件上进行本地上下文推理。

Result: RDMA在F1性能上提升了30%以上，推理成本降低了10倍。该方法能够处理医学缩写，识别隐式疾病模式，并在保护隐私的前提下提供稀有疾病信息提取。

Conclusion: RDMA框架有效解决了电子健康记录中稀有疾病信息提取的关键问题，帮助临床医生在避免云服务隐私风险的同时获取关键的稀有疾病信息，支持稀有疾病患者的早期诊断。

Abstract: Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail
to capture these conditions in electronic health records (EHR), leaving crucial
information buried in clinical notes. Current approaches struggle with medical
abbreviations, miss implicit disease mentions, raise privacy concerns with
cloud processing, and lack clinical reasoning abilities. We present Rare
Disease Mining Agents (RDMA), a framework that mirrors how medical experts
identify rare disease patterns in EHR. RDMA connects scattered clinical
observations that together suggest specific rare conditions. By handling
clinical abbreviations, recognizing implicit disease patterns, and applying
contextual reasoning locally on standard hardware, RDMA reduces privacy risks
while improving F1 performance by upwards of 30\% and decreasing inferences
costs 10-fold. This approach helps clinicians avoid the privacy risk of using
cloud services while accessing key rare disease information from EHR systems,
supporting earlier diagnosis for rare disease patients. Available at
https://github.com/jhnwu3/RDMA.

</details>


### [11] [ReDi: Rectified Discrete Flow](https://arxiv.org/abs/2507.15897)
*Jaehoon Yoo,Wonjung Kim,Seunghoon Hong*

Main category: cs.LG

TL;DR: 本文提出了Rectified Discrete Flow (ReDi)方法，通过理论分析条件总相关性来减少离散流模型的分解误差，实现高效的少步骤生成，并在图像生成任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 离散流模型虽然能生成高质量的离散数据，但由于依赖迭代解码过程导致采样速度缓慢。这个问题源于处理高维数据时必需的分解近似，因此需要找到减少分解误差并实现高效少步骤生成的方法。

Method: 提出了Rectified Discrete Flow (ReDi)方法，通过条件总相关性(Conditional TC)来严格刻画分解误差，该误差依赖于耦合关系。ReDi是一种新颖的迭代方法，通过修正源分布和目标分布之间的耦合来减少分解误差。理论上证明了每个ReDi步骤都保证条件TC单调递减，确保收敛性。

Result: 实验结果显示ReDi显著减少了条件总相关性，实现了少步骤生成。此外，修正后的耦合关系非常适合在图像生成任务上训练高效的单步模型，为高效离散数据合成提供了新的视角。

Conclusion: ReDi为解决少步骤生成挑战提供了一个简单且理论基础扎实的方法，通过减少离散流模型的分解误差来提高采样效率，为离散数据合成领域提供了新的理论和实践贡献。

Abstract: Discrete Flow-based Models (DFMs) are powerful generative models for
high-quality discrete data but typically suffer from slow sampling speeds due
to their reliance on iterative decoding processes. This reliance on a
multi-step process originates from the factorization approximation of DFMs,
which is necessary for handling high-dimensional data. In this paper, we
rigorously characterize the approximation error from factorization using
Conditional Total Correlation (TC), which depends on the coupling. To reduce
the Conditional TC and enable efficient few-step generation, we propose
Rectified Discrete Flow (ReDi), a novel iterative method that reduces
factorization error by rectifying the coupling between source and target
distributions. We theoretically prove that each ReDi step guarantees a
monotonic decreasing Conditional TC, ensuring its convergence. Empirically,
ReDi significantly reduces Conditional TC and enables few-step generation.
Moreover, we demonstrate that the rectified couplings are well-suited for
training efficient one-step models on image generation. ReDi offers a simple
and theoretically grounded approach for tackling the few-step challenge,
providing a new perspective on efficient discrete data synthesis. Code is
available at https://github.com/Ugness/ReDi_discrete

</details>


### [12] [An open dataset of neural networks for hypernetwork research](https://arxiv.org/abs/2507.15869)
*David Kurtenbach,Lior Shamir*

Main category: cs.LG

TL;DR: 本论文构建了一个包含10^4个LeNet-5神经网络的数据集，用于超网络研究，这些网络在ImageNette V2数据集上进行二分类训练，并展示了网络间的差异可以被机器学习算法识别


<details>
  <summary>Details</summary>
Motivation: 超网络（能够生成其他神经网络权重的神经网络）研究缺乏可用的研究资源，限制了这一具有变革潜力的AI概念的发展

Method: 使用超过10^4个核心的计算集群生成数据集，包含10^4个LeNet-5神经网络，分为10个类别，每个类别包含1000个不同的神经网络，用于在ImageNette V2数据集上进行特定类别的二分类任务

Result: 基础分类结果显示神经网络可以被以72.0%的准确率进行分类，表明监督机器学习算法能够识别不同神经网络之间的差异

Conclusion: 成功构建了用于超网络研究的神经网络数据集，验证了网络间差异的可识别性，数据集和生成代码已公开，为超网络研究提供了重要资源

Abstract: Despite the transformative potential of AI, the concept of neural networks
that can produce other neural networks by generating model weights
(hypernetworks) has been largely understudied. One of the possible reasons is
the lack of available research resources that can be used for the purpose of
hypernetwork research. Here we describe a dataset of neural networks, designed
for the purpose of hypernetworks research. The dataset includes $10^4$ LeNet-5
neural networks trained for binary image classification separated into 10
classes, such that each class contains 1,000 different neural networks that can
identify a certain ImageNette V2 class from all other classes. A computing
cluster of over $10^4$ cores was used to generate the dataset. Basic
classification results show that the neural networks can be classified with
accuracy of 72.0%, indicating that the differences between the neural networks
can be identified by supervised machine learning algorithms. The ultimate
purpose of the dataset is to enable hypernetworks research. The dataset and the
code that generates it are open and accessible to the public.

</details>


### [13] [Families of Optimal Transport Kernels for Cell Complexes](https://arxiv.org/abs/2507.16569)
*Rahul Khorana*

Main category: cs.LG

TL;DR: 本文针对CW复形提出了基于Wasserstein距离的机器学习方法，推导了细胞复形信号分布间Wasserstein距离的显式表达式，并扩展了Fused Gromov-Wasserstein距离，引入了新的核函数用于CW复形上的概率测度空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然认为细胞复形是理想的学习表示，但缺乏适用于CW复形学习的机器学习方法，需要开发能够同时处理特征和结构信息的有效算法。

Method: 推导细胞复形信号分布间Wasserstein距离基于Hodge-Laplacian矩阵的显式表达式；将Fused Gromov-Wasserstein距离扩展到CW复形以同时包含特征和结构信息；基于最优传输的对偶表述引入CW复形概率测度空间上的新核函数。

Result: 成功建立了比较CW复形的结构性度量方法，定义了最优传输映射，并为CW复形上的机器学习提供了新的核函数工具。

Conclusion: 通过将最优传输理论与CW复形结合，为细胞复形上的机器学习提供了理论基础和实用方法，填补了该领域方法论的空白。

Abstract: Recent advances have discussed cell complexes as ideal learning
representations. However, there is a lack of available machine learning methods
suitable for learning on CW complexes. In this paper, we derive an explicit
expression for the Wasserstein distance between cell complex signal
distributions in terms of a Hodge-Laplacian matrix. This leads to a
structurally meaningful measure to compare CW complexes and define the optimal
transportation map. In order to simultaneously include both feature and
structure information, we extend the Fused Gromov-Wasserstein distance to CW
complexes. Finally, we introduce novel kernels over the space of probability
measures on CW complexes based on the dual formulation of optimal transport.

</details>


### [14] [Prompt Smart, Pay Less: Cost-Aware APO for Real-World Applications](https://arxiv.org/abs/2507.15884)
*Jayesh Choudhari,Piyush Kumar Singh,Douglas McIlwraith,Snehal Nair*

Main category: cs.LG

TL;DR: 本文首次在商业环境中全面评估了自动提示优化(APO)方法在高风险多类分类任务中的表现，提出了APE-OPRO混合框架，在保持性能的同时实现了18%的成本效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的提示设计主要依赖启发式、手工方式，难以规模化，且大多数APO框架仅在有限复杂度的基准分类任务上验证过，缺乏在真实世界商业环境中高风险应用的comprehensive evaluation。

Method: 提出APE-OPRO混合框架，结合APE和OPRO的互补优势；在约2500个标记产品数据集上对比评估无梯度方法(APE、OPRO)和基于梯度的方法(ProTeGi)；进行深度和广度超参数的消融研究。

Result: APE-OPRO相比OPRO实现约18%的成本效率提升，且不牺牲性能；ProTeGi在较低API成本下提供最强绝对性能但计算时间更高；APE-OPRO在性能、API效率和可扩展性之间取得良好平衡；发现标签格式敏感性问题。

Conclusion: 为商业应用中实施APO提供了可操作的见解，建立了未来多标签、视觉和多模态提示优化研究的基础；APE-OPRO框架为实际商业部署提供了有效的解决方案。

Abstract: Prompt design is a critical factor in the effectiveness of Large Language
Models (LLMs), yet remains largely heuristic, manual, and difficult to scale.
This paper presents the first comprehensive evaluation of Automatic Prompt
Optimization (APO) methods for real-world, high-stakes multiclass
classification in a commercial setting, addressing a critical gap in the
existing literature where most of the APO frameworks have been validated only
on benchmark classification tasks of limited complexity.
  We introduce APE-OPRO, a novel hybrid framework that combines the
complementary strengths of APE and OPRO, achieving notably better
cost-efficiency, around $18\%$ improvement over OPRO, without sacrificing
performance. We benchmark APE-OPRO alongside both gradient-free (APE, OPRO) and
gradient-based (ProTeGi) methods on a dataset of ~2,500 labeled products.
  Our results highlight key trade-offs: ProTeGi offers the strongest absolute
performance at lower API cost but higher computational time as noted
in~\cite{protegi}, while APE-OPRO strikes a compelling balance between
performance, API efficiency, and scalability. We further conduct ablation
studies on depth and breadth hyperparameters, and reveal notable sensitivity to
label formatting, indicating implicit sensitivity in LLM behavior. These
findings provide actionable insights for implementing APO in commercial
applications and establish a foundation for future research in multi-label,
vision, and multimodal prompt optimization scenarios.

</details>


### [15] [A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling](https://arxiv.org/abs/2507.16771)
*Michael Grosskopf,Kellin Rumsey,Ayan Biswas,Earl Lawrence*

Main category: cs.LG

TL;DR: 本文提出了一种分区稀疏变分高斯过程(PSVGP)方法，通过相邻空间分区间的少量通信来改善边界处的预测连续性，解决了在百亿亿次超级计算机上进行原位机器学习时独立模型产生不连续响应面的问题。


<details>
  <summary>Details</summary>
Motivation: 下一代能源部超级计算机将具备百亿亿次计算能力，但计算量远超可存储到磁盘的数据量。用户无法依赖事后数据访问进行不确定性量化等统计分析，迫切需要能够原位训练的复杂机器学习算法。现有的独立并行稀疏变分高斯过程虽然可扩展且高效，但会在相邻模型边界处产生不连续的响应面。

Method: 提出分区稀疏变分高斯过程(PSVGP)方法，在独立并行训练各空间分区模型的基础上，允许相邻空间分区间进行少量通信，以促进局部模型的更好对齐。采用去中心化通信方案，保持高可扩展性，计算开销极小且无内存开销。

Result: PSVGP方法成功改善了空间预测的平滑性和整体拟合效果，解决了独立SVGP模型在边界处的不连续问题。在能源百亿亿次地球系统模型(E3SM)上的实验验证了该方法的有效性，与独立SVGP情况相比显示出更好的性能。

Conclusion: PSVGP方法通过最小的通信开销有效解决了大规模分布式机器学习中的边界不连续问题，为百亿亿次计算环境下的原位统计分析提供了一个实用且高效的解决方案，在保持可扩展性的同时显著改善了模型的空间预测质量。

Abstract: The next generation of Department of Energy supercomputers will be capable of
exascale computation. For these machines, far more computation will be possible
than that which can be saved to disk. As a result, users will be unable to rely
on post-hoc access to data for uncertainty quantification and other statistical
analyses and there will be an urgent need for sophisticated machine learning
algorithms which can be trained in situ. Algorithms deployed in this setting
must be highly scalable, memory efficient and capable of handling data which is
distributed across nodes as spatially contiguous partitions. One suitable
approach involves fitting a sparse variational Gaussian process (SVGP) model
independently and in parallel to each spatial partition. The resulting model is
scalable, efficient and generally accurate, but produces the undesirable effect
of constructing discontinuous response surfaces due to the disagreement between
neighboring models at their shared boundary. In this paper, we extend this idea
by allowing for a small amount of communication between neighboring spatial
partitions which encourages better alignment of the local models, leading to
smoother spatial predictions and a better fit in general. Due to our
decentralized communication scheme, the proposed extension remains highly
scalable and adds very little overhead in terms of computation (and none, in
terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the
Energy Exascale Earth System Model (E3SM) and compare the results to the
independent SVGP case.

</details>


### [16] [Improving the Generation of VAEs with High Dimensional Latent Spaces by the use of Hyperspherical Coordinates](https://arxiv.org/abs/2507.15900)
*Alejandro Ascarate,Leo Lebrat,Rodrigo Santa Cruz,Clinton Fookes,Olivier Salvado*

Main category: cs.LG

TL;DR: 本文提出了一种基于超球面坐标的VAE改进方法，通过将潜在向量压缩到超球面上的一个"岛屿"区域来减少潜在空间的稀疏性，从而提升VAE的生成能力。


<details>
  <summary>Details</summary>
Motivation: 标准VAE在高维潜在空间中存在一个关键问题：从先验分布中随机采样的潜在向量通常无法生成有意义的数据，特别是当潜在空间维度超过十几维时。作者从高维统计学角度分析发现，标准VAE的潜在向量在构造上均匀分布在超球面上，导致潜在空间稀疏。

Method: 提出使用超球面坐标来重新参数化VAE的潜在变量。这种方法将潜在向量压缩到超球面上的一个集中区域（称为"岛屿"），而不是均匀分布在整个超球面上。新的参数化方法计算开销有限，但能有效减少潜在空间的稀疏性。

Result: 实验表明，基于超球面坐标的VAE改进方法能够显著提升生成能力。通过将潜在向量集中在超球面的特定区域，模型能够更好地利用潜在空间，从而生成更有意义的数据。

Conclusion: 本文成功解决了高维VAE潜在空间稀疏性问题。通过超球面坐标重参数化，将潜在向量从均匀分布转变为集中分布，有效提升了VAE的生成质量，为改进变分自编码器提供了新的思路和方法。

Abstract: Variational autoencoders (VAE) encode data into lower-dimensional latent
vectors before decoding those vectors back to data. Once trained, decoding a
random latent vector from the prior usually does not produce meaningful data,
at least when the latent space has more than a dozen dimensions. In this paper,
we investigate this issue by drawing insight from high dimensional statistics:
in these regimes, the latent vectors of a standard VAE are by construction
distributed uniformly on a hypersphere. We propose to formulate the latent
variables of a VAE using hyperspherical coordinates, which allows compressing
the latent vectors towards an island on the hypersphere, thereby reducing the
latent sparsity and we show that this improves the generation ability of the
VAE. We propose a new parameterization of the latent space with limited
computational overhead.

</details>


### [17] [Towards Mitigation of Hallucination for LLM-empowered Agents: Progressive Generalization Bound Exploration and Watchdog Monitor](https://arxiv.org/abs/2507.15903)
*Siyuan Liu,Wenjing Liu,Zhiwei Xu,Xin Wang,Bo Chen,Tao Li*

Main category: cs.LG

TL;DR: 本文提出了HalMit，一个黑盒监督框架，通过建模大语言模型智能代理的泛化边界来检测和缓解幻觉问题，无需访问模型内部架构，显著提升了LLM驱动系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的智能代理在与开放环境交互时会产生幻觉（输出与事实不符），这严重损害了智能代理的可信度，现有方法要么依赖白盒访问要么无法准确识别幻觉，因此需要有效的黑盒幻觉检测和缓解方法来确保代理的可靠性。

Method: 提出HalMit黑盒监督框架，通过建模LLM驱动代理的泛化边界来检测幻觉，采用概率分形采样技术并行生成足够数量的查询来触发可疑响应，从而有效识别目标代理的泛化边界。

Result: 实验评估表明HalMit在幻觉监控方面显著优于现有方法，其黑盒特性和卓越性能使其成为增强LLM驱动系统可靠性的有前景解决方案。

Conclusion: HalMit框架成功解决了LLM智能代理的幻觉检测问题，通过黑盒方式建模泛化边界，为提升LLM驱动系统的可靠性和实际部署提供了有效的解决方案。

Abstract: Empowered by large language models (LLMs), intelligent agents have become a
popular paradigm for interacting with open environments to facilitate AI
deployment. However, hallucinations generated by LLMs-where outputs are
inconsistent with facts-pose a significant challenge, undermining the
credibility of intelligent agents. Only if hallucinations can be mitigated, the
intelligent agents can be used in real-world without any catastrophic risk.
Therefore, effective detection and mitigation of hallucinations are crucial to
ensure the dependability of agents. Unfortunately, the related approaches
either depend on white-box access to LLMs or fail to accurately identify
hallucinations. To address the challenge posed by hallucinations of intelligent
agents, we present HalMit, a novel black-box watchdog framework that models the
generalization bound of LLM-empowered agents and thus detect hallucinations
without requiring internal knowledge of the LLM's architecture. Specifically, a
probabilistic fractal sampling technique is proposed to generate a sufficient
number of queries to trigger the incredible responses in parallel, efficiently
identifying the generalization bound of the target agent. Experimental
evaluations demonstrate that HalMit significantly outperforms existing
approaches in hallucination monitoring. Its black-box nature and superior
performance make HalMit a promising solution for enhancing the dependability of
LLM-powered systems.

</details>


### [18] [Fast-VAT: Accelerating Cluster Tendency Visualization using Cython and Numba](https://arxiv.org/abs/2507.15904)
*MSR Avinash,Ismael Lachheb*

Main category: cs.LG

TL;DR: 本文提出了Fast-VAT，一个高性能的视觉聚类倾向评估算法重新实现，通过Numba JIT编译和Cython优化实现了高达50倍的速度提升，同时保持了原始VAT算法的输出精度。


<details>
  <summary>Details</summary>
Motivation: 标准VAT算法存在严重的性能瓶颈，具有O(n^2)时间复杂度和低效的内存使用，限制了其在大规模数据集上的应用。因此需要开发一个高性能的VAT算法实现来克服这些局限性。

Method: 采用Python重新实现VAT算法，结合Numba的即时编译(JIT)技术和Cython的静态类型以及低级内存优化技术。通过这些优化策略来显著提升算法的计算效率和内存使用效率。

Result: Fast-VAT相比基线实现实现了高达50倍的速度提升，同时完全保持了原始方法的输出保真度。在多个真实和合成数据集（包括Iris、Mall Customers和Spotify子集）上验证了算法的有效性，并使用Hopkins统计量、PCA和t-SNE确认了聚类倾向。

Conclusion: Fast-VAT成功解决了传统VAT算法的性能瓶颈问题，在保持输出质量的同时大幅提升了计算效率。通过与DBSCAN和K-Means聚类结果的比较，证实了VAT结构洞察的可靠性，为大规模数据的聚类倾向评估提供了实用的解决方案。

Abstract: Visual Assessment of Cluster Tendency (VAT) is a widely used unsupervised
technique to assess the presence of cluster structure in unlabeled datasets.
However, its standard implementation suffers from significant performance
limitations due to its O(n^2) time complexity and inefficient memory usage. In
this work, we present Fast-VAT, a high-performance reimplementation of the VAT
algorithm in Python, augmented with Numba's Just-In-Time (JIT) compilation and
Cython's static typing and low-level memory optimizations. Our approach
achieves up to 50x speedup over the baseline implementation, while preserving
the output fidelity of the original method. We validate Fast-VAT on a suite of
real and synthetic datasets -- including Iris, Mall Customers, and Spotify
subsets -- and verify cluster tendency using Hopkins statistics, PCA, and
t-SNE. Additionally, we compare VAT's structural insights with clustering
results from DBSCAN and K-Means to confirm its reliability.

</details>


### [19] [Neural Probabilistic Shaping: Joint Distribution Learning for Optical Fiber Communications](https://arxiv.org/abs/2507.16012)
*Mohammad Taha Askari,Lutz Lampe,Amirhossein Ghazisaeidi*

Main category: cs.LG

TL;DR: 本文提出了一种自回归端到端学习方法，用于非线性光纤信道的概率整形，在双极化64-QAM传输中实现了0.3-bits/2D的信息速率增益。


<details>
  <summary>Details</summary>
Motivation: 传统的概率整形方法通常只优化边缘分布，而忽略了符号间的相关性。在非线性光纤信道中，符号间存在复杂的相互作用，需要学习联合符号分布来提高传输性能。

Method: 采用自回归端到端学习方法来学习联合符号分布。该方法能够捕捉符号间的依赖关系，并针对非线性光纤信道特性进行优化，实现概率整形的端到端学习。

Result: 在单跨度205公里链路上进行双极化64-QAM传输实验，相比优化的边缘分布方法，该方案实现了0.3-bits/2D的可达信息速率增益。

Conclusion: 自回归端到端学习方法能够有效学习联合符号分布，在非线性光纤信道的概率整形中表现出优越性能，为光通信系统的容量提升提供了新的解决方案。

Abstract: We present an autoregressive end-to-end learning approach for probabilistic
shaping on nonlinear fiber channels. Our proposed scheme learns the joint
symbol distribution and provides a 0.3-bits/2D achievable information rate gain
over an optimized marginal distribution for dual-polarized 64-QAM transmission
over a single-span 205 km link.

</details>


### [20] [Foundation Models and Transformers for Anomaly Detection: A Survey](https://arxiv.org/abs/2507.15905)
*Mouïn Ben Ammar,Arturo Mendoza,Nacim Belkhir,Antoine Manzanera,Gianni Franchi*

Main category: cs.LG

TL;DR: 这篇论文综述了Transformer和基础模型在视觉异常检测(VAD)中的变革性作用，探讨了这些架构如何通过全局感受野和适应性来解决长程依赖建模、上下文建模和数据稀缺等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，传统视觉异常检测方法在处理长程依赖关系、上下文建模和数据稀缺等方面存在挑战，需要探索Transformer和基础模型等新架构在VAD领域的应用潜力和变革性影响。

Method: 通过综述的方式，将VAD方法分为基于重建、基于特征和零/少样本三种方法，系统性地分析Transformer和基础模型的架构特点，包括注意力机制和大规模预训练的集成应用。

Result: 研究发现Transformer和基础模型通过全局感受野和适应性能够有效解决传统VAD方法的局限性，实现更加鲁棒、可解释和可扩展的异常检测解决方案，并识别了当前最先进技术的优势、局限性和新兴趋势。

Conclusion: Transformer和基础模型为视觉异常检测带来了范式转变，通过整合注意力机制和利用大规模预训练，这些架构能够提供更强大、更可解释且更具扩展性的异常检测解决方案，为该领域的发展指明了方向。

Abstract: In line with the development of deep learning, this survey examines the
transformative role of Transformers and foundation models in advancing visual
anomaly detection (VAD). We explore how these architectures, with their global
receptive fields and adaptability, address challenges such as long-range
dependency modeling, contextual modeling and data scarcity. The survey
categorizes VAD methods into reconstruction-based, feature-based and
zero/few-shot approaches, highlighting the paradigm shift brought about by
foundation models. By integrating attention mechanisms and leveraging
large-scale pre-training, Transformers and foundation models enable more
robust, interpretable, and scalable anomaly detection solutions. This work
provides a comprehensive review of state-of-the-art techniques, their
strengths, limitations, and emerging trends in leveraging these architectures
for VAD.

</details>


### [21] [RIS-aided Latent Space Alignment for Semantic Channel Equalization](https://arxiv.org/abs/2507.16450)
*Tomás Hüttebräucker,Mario Edoardo Pandolfo,Simone Fiorellino,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 本文提出了一种联合物理和语义信道均衡框架，利用可重构智能表面(RIS)解决多用户MIMO语义通信系统中的语义失配问题，通过线性和非线性两种方案实现发送端预均衡、RIS辅助传播和接收端后均衡的三阶段语义均衡。


<details>
  <summary>Details</summary>
Motivation: 在多用户语义通信系统中，独立训练的AI设备会产生不同的潜在表示，导致语义失配问题，即使在没有传统传输错误的情况下也会阻碍设备间的相互理解。现有方法通常将物理信道均衡和语义均衡分离处理，效果不佳。

Method: 提出基于RIS的联合物理和语义信道均衡框架，包含三个阶段：(1)发送端预均衡；(2)通过RIS辅助信道传播；(3)接收端后均衡。将问题建模为约束最小均方误差(MMSE)优化问题，提出两种解决方案：线性语义均衡链和基于深度神经网络的非线性语义均衡器，均在潜在空间中进行语义压缩并满足发送功率约束。

Result: 通过大量评估实验表明，所提出的联合均衡策略在各种场景和无线信道条件下均显著优于传统的物理和语义信道均衡分离方法，能够有效改善语义通信系统的性能。

Conclusion: 联合物理和语义信道均衡框架能够有效解决多用户MIMO语义通信系统中的语义失配问题，利用RIS技术实现的三阶段均衡方案比传统分离方法具有更好的性能，为语义通信系统的实际部署提供了有效的技术方案。

Abstract: Semantic communication systems introduce a new paradigm in wireless
communications, focusing on transmitting the intended meaning rather than
ensuring strict bit-level accuracy. These systems often rely on Deep Neural
Networks (DNNs) to learn and encode meaning directly from data, enabling more
efficient communication. However, in multi-user settings where interacting
agents are trained independently-without shared context or joint
optimization-divergent latent representations across AI-native devices can lead
to semantic mismatches, impeding mutual understanding even in the absence of
traditional transmission errors. In this work, we address semantic mismatch in
Multiple-Input Multiple-Output (MIMO) channels by proposing a joint physical
and semantic channel equalization framework that leverages the presence of
Reconfigurable Intelligent Surfaces (RIS). The semantic equalization is
implemented as a sequence of transformations: (i) a pre-equalization stage at
the transmitter; (ii) propagation through the RIS-aided channel; and (iii) a
post-equalization stage at the receiver. We formulate the problem as a
constrained Minimum Mean Squared Error (MMSE) optimization and propose two
solutions: (i) a linear semantic equalization chain, and (ii) a non-linear
DNN-based semantic equalizer. Both methods are designed to operate under
semantic compression in the latent space and adhere to transmit power
constraints. Through extensive evaluations, we show that the proposed joint
equalization strategies consistently outperform conventional, disjoint
approaches to physical and semantic channel equalization across a broad range
of scenarios and wireless channel conditions.

</details>


### [22] [Towards Reliable, Uncertainty-Aware Alignment](https://arxiv.org/abs/2507.15906)
*Debangshu Banerjee,Kintan Saha,Aditya Gopalan*

Main category: cs.LG

TL;DR: 该论文提出了一种方差感知的策略优化框架，通过考虑奖励模型的不确定性来提高大语言模型对齐的稳定性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型对齐方法依赖单一奖励模型进行策略优化，但研究发现在相同偏好数据上独立训练的奖励模型之间存在显著分歧，这种不稳定性可能导致过拟合和性能退化的风险

Method: 提出了一个方差感知的策略优化框架，核心是设计了一个新的策略正则化器，该正则化器融合了奖励模型的方差估计，从而在策略优化过程中考虑奖励模型的不确定性

Result: 实验表明，相比标准的策略优化管道，该方法在多种大语言模型和奖励模型配置下都能产生更稳定和鲁棒的对齐效果，并且理论上证明了该方法能够降低输出比默认策略更差的策略的风险

Conclusion: 通过在策略优化中引入奖励模型方差的考虑，可以有效缓解当前对齐策略的不稳定性问题，提供更可靠的大语言模型对齐方案

Abstract: Alignment of large language models (LLMs) typically involves training a
reward model on preference data, followed by policy optimization with respect
to the reward model. However, optimizing policies with respect to a single
reward model estimate can render it vulnerable to inaccuracies in the reward
model. We empirically study the variability of reward model training on
open-source benchmarks. We observe that independently trained reward models on
the same preference dataset can exhibit substantial disagreement, highlighting
the instability of current alignment strategies. Employing a theoretical model,
we demonstrate that variability in reward model estimation can cause
overfitting, leading to the risk of performance degradation. To mitigate this
risk, we propose a variance-aware policy optimization framework for
preference-based alignment. The key ingredient of the framework is a new policy
regularizer that incorporates reward model variance estimates. We show that
variance-aware policy optimization provably reduces the risk of outputting a
worse policy than the default. Experiments across diverse LLM and reward model
configurations confirm that our approach yields more stable and robust
alignment than the standard (variance-unaware) pipeline.

</details>


### [23] [Dual Turing Test: A Framework for Detecting and Mitigating Undetectable AI](https://arxiv.org/abs/2507.15907)
*Alberto Messina*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，将"双重图灵测试"（人类判断者识别AI而非奖励机器欺骗）、对抗分类博弈和强化学习对齐管道相结合，通过不可检测性检测器和质量约束来实现AI系统的对齐。


<details>
  <summary>Details</summary>
Motivation: 传统图灵测试关注机器欺骗人类的能力，而现代AI对齐需要平衡系统的隐蔽性和输出质量。作者认为需要一个统一的理论框架来连接图灵测试的反向视角、对抗分类博弈理论和强化学习对齐实践。

Method: 提出"双重图灵测试"框架，将其形式化为双人零和博弈；在N轮独立测试中，人类判断者从提示空间Q中抽取新提示，引入质量函数Q和参数tau、delta；将此极小极大博弈映射到RL-HF风格的对齐循环中，使用不可检测性检测器D提供负奖励，同时用质量代理保持流畅性。

Result: 建立了连接三个领域的理论框架：反向图灵测试、带质量约束的对抗分类博弈、以及包含不可检测性检测器的强化学习对齐管道。框架包含了质量阈值、分阶段难度等级和极小极大边界的新颖组合。

Conclusion: 成功构建了统一的理论框架，将图灵测试的现代变体与对抗机器学习和强化学习对齐相连接。该框架为AI系统在保持输出质量的同时实现不可检测性提供了理论基础，并为未来的实际应用提供了方向。

Abstract: In this short note, we propose a unified framework that bridges three areas:
(1) a flipped perspective on the Turing Test, the "dual Turing test", in which
a human judge's goal is to identify an AI rather than reward a machine for
deception; (2) a formal adversarial classification game with explicit quality
constraints and worst-case guarantees; and (3) a reinforcement learning (RL)
alignment pipeline that uses an undetectability detector and a set of quality
related components in its reward model. We review historical precedents, from
inverted and meta-Turing variants to modern supervised reverse-Turing
classifiers, and highlight the novelty of combining quality thresholds, phased
difficulty levels, and minimax bounds. We then formalize the dual test: define
the judge's task over N independent rounds with fresh prompts drawn from a
prompt space Q, introduce a quality function Q and parameters tau and delta,
and cast the interaction as a two-player zero-sum game over the adversary's
feasible strategy set M. Next, we map this minimax game onto an RL-HF style
alignment loop, in which an undetectability detector D provides negative reward
for stealthy outputs, balanced by a quality proxy that preserves fluency.
Throughout, we include detailed explanations of each component notation, the
meaning of inner minimization over sequences, phased tests, and iterative
adversarial training and conclude with a suggestion for a couple of immediate
actions.

</details>


### [24] [HyDRA: A Hybrid-Driven Reasoning Architecture for Verifiable Knowledge Graphs](https://arxiv.org/abs/2507.15917)
*Adrian Kaiser,Claudiu Leoveanu-Condrei,Ryan Gold,Marius-Constantin Dinu,Markus Hofmarcher*

Main category: cs.LG

TL;DR: 本文提出HyDRA架构，通过神经符号代理协作构建本体和能力问题，结合设计合约原则指导大语言模型生成可验证的知识图谱，解决自动化知识图谱构建中的可靠性、一致性和可验证性问题。


<details>
  <summary>Details</summary>
Motivation: 自动化知识图谱构建面临输出可靠性、一致性和可验证性挑战，包括生成图中的结构不一致问题（如孤立数据岛、抽象类与具体实例的错误合并等），这些问题阻碍了符号知识与神经网络生成能力的有效结合，限制了神经符号AI的发展潜力。

Method: 提出HyDRA混合驱动推理架构：首先通过协作的神经符号代理面板构建本体，这些代理共同制定能力问题(CQs)来定义本体的范围和要求；基于CQs构建本体图，指导从任意文档中自动提取三元组进行知识图谱生成；受设计合约(DbC)原则启发，使用可验证合约作为主要控制机制来引导大语言模型的生成过程。

Result: 扩展了标准基准评估，提出了一个评估框架，通过利用神经符号AI框架SymbolicAI描述的符号验证来评估生成知识图谱的功能正确性；代码已公开可用，为改进自动化知识图谱构建的可靠性提供了混合驱动架构。

Conclusion: 本文贡献了一个用于提高自动化知识图谱构建可靠性的混合驱动架构，并探索了测量其输出功能完整性的评估方法，为神经符号AI中符号知识与神经网络生成能力的有效结合提供了解决方案。

Abstract: The synergy between symbolic knowledge, often represented by Knowledge Graphs
(KGs), and the generative capabilities of neural networks is central to
advancing neurosymbolic AI. A primary bottleneck in realizing this potential is
the difficulty of automating KG construction, which faces challenges related to
output reliability, consistency, and verifiability. These issues can manifest
as structural inconsistencies within the generated graphs, such as the
formation of disconnected $\textit{isolated islands}$ of data or the inaccurate
conflation of abstract classes with specific instances. To address these
challenges, we propose HyDRA, a $\textbf{Hy}$brid-$\textbf{D}$riven
$\textbf{R}$easoning $\textbf{A}$rchitecture designed for verifiable KG
automation. Given a domain or an initial set of documents, HyDRA first
constructs an ontology via a panel of collaborative neurosymbolic agents. These
agents collaboratively agree on a set of competency questions (CQs) that define
the scope and requirements the ontology must be able to answer. Given these
CQs, we build an ontology graph that subsequently guides the automated
extraction of triplets for KG generation from arbitrary documents. Inspired by
design-by-contracts (DbC) principles, our method leverages verifiable contracts
as the primary control mechanism to steer the generative process of Large
Language Models (LLMs). To verify the output of our approach, we extend beyond
standard benchmarks and propose an evaluation framework that assesses the
functional correctness of the resulting KG by leveraging symbolic verifications
as described by the neurosymbolic AI framework, $\textit{SymbolicAI}$. This
work contributes a hybrid-driven architecture for improving the reliability of
automated KG construction and the exploration of evaluation methods for
measuring the functional integrity of its output. The code is publicly
available.

</details>


### [25] [On the transferability of Sparse Autoencoders for interpreting compressed models](https://arxiv.org/abs/2507.15977)
*Suchit Gupte,Vishnu Kabir Chhabra,Mohammad Mahdi Khalili*

Main category: cs.LG

TL;DR: 研究了大语言模型压缩对可解释性的影响，发现原始模型训练的稀疏自编码器(SAE)可以有效解释压缩模型，且直接剪枝原始SAE的性能与在压缩模型上重新训练SAE相当，从而大幅降低了SAE的训练成本。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型面临推理效率挑战，虽然存在剪枝和量化等压缩方法，但压缩对模型可解释性的影响尚不明确。稀疏自编码器(SAE)在模型激活空间特征分解方面表现出色，但为压缩模型重新训练SAE成本高昂，需要探索更高效的解决方案。

Method: 比较分析原始模型和压缩模型上训练的SAE性能差异；测试原始模型训练的SAE在压缩模型上的解释能力；探索直接对原始SAE进行剪枝的效果，并与在压缩模型上重新训练SAE的性能进行对比。

Result: 原始模型训练的SAE能够解释压缩模型，虽然性能略有下降但仍可接受；直接剪枝原始SAE的性能与在剪枝模型上重新训练SAE的性能相当；这种方法显著降低了为压缩模型获取SAE的计算成本。

Conclusion: 研究证明了模型压缩与SAE可解释性方法之间的兼容性，提出了一种成本效益高的解决方案，即通过剪枝原始SAE来获得压缩模型的解释工具，避免了重新训练的巨大开销，为压缩模型的可解释性研究提供了实用的技术路径。

Abstract: Modern LLMs face inference efficiency challenges due to their scale. To
address this, many compression methods have been proposed, such as pruning and
quantization. However, the effect of compression on a model's interpretability
remains elusive. While several model interpretation approaches exist, such as
circuit discovery, Sparse Autoencoders (SAEs) have proven particularly
effective in decomposing a model's activation space into its feature basis. In
this work, we explore the differences in SAEs for the original and compressed
models. We find that SAEs trained on the original model can interpret the
compressed model albeit with slight performance degradation compared to the
trained SAE on the compressed model. Furthermore, simply pruning the original
SAE itself achieves performance comparable to training a new SAE on the pruned
model. This finding enables us to mitigate the extensive training costs of
SAEs.

</details>


### [26] [Semantic-Aware Gaussian Process Calibration with Structured Layerwise Kernels for Deep Neural Networks](https://arxiv.org/abs/2507.15987)
*Kyung-hwan Lee,Kyung-tae Kim*

Main category: cs.LG

TL;DR: 提出了一种语义感知分层高斯过程(SAL-GP)框架，通过镜像神经网络的分层架构来改进深度学习模型的置信度校准，相比传统方法能更好地捕获网络内部层级结构并提升预测可靠性评估。


<details>
  <summary>Details</summary>
Motivation: 传统的高斯过程校准方法无法有效捕获深度神经网络的内部层级结构，这限制了其在评估预测可靠性方面的可解释性和有效性。现有方法缺乏对网络架构特性的考虑，难以充分利用深度模型的层次化特征表示。

Method: 提出SAL-GP框架，采用多层高斯过程模型镜像目标神经网络的分层架构。每一层的特征表示都映射到局部校准修正，各层GP通过结构化多层核函数耦合，实现跨所有层的联合边缘化。该设计能够捕获局部语义依赖关系和全局校准一致性，同时在网络中一致地传播预测不确定性。

Result: SAL-GP框架能够增强与网络架构对齐的可解释性，实现了对深度模型中置信度一致性和不确定性量化的原理性评估。相比传统的单一全局GP校准方法，该方法更好地反映了深度神经网络的内在结构特性。

Conclusion: SAL-GP框架通过模拟深度神经网络的分层结构，成功解决了传统GP校准方法的局限性，提供了更有效的置信度校准和不确定性量化方案，为深度学习模型的可靠性评估提供了新的解决思路。

Abstract: Calibrating the confidence of neural network classifiers is essential for
quantifying the reliability of their predictions during inference. However,
conventional Gaussian Process (GP) calibration methods often fail to capture
the internal hierarchical structure of deep neural networks, limiting both
interpretability and effectiveness for assessing predictive reliability. We
propose a Semantic-Aware Layer-wise Gaussian Process (SAL-GP) framework that
mirrors the layered architecture of the target neural network. Instead of
applying a single global GP correction, SAL-GP employs a multi-layer GP model,
where each layer's feature representation is mapped to a local calibration
correction. These layerwise GPs are coupled through a structured multi-layer
kernel, enabling joint marginalization across all layers. This design allows
SAL-GP to capture both local semantic dependencies and global calibration
coherence, while consistently propagating predictive uncertainty through the
network. The resulting framework enhances interpretability aligned with the
network architecture and enables principled evaluation of confidence
consistency and uncertainty quantification in deep models.

</details>


### [27] [Enhancing Stability of Physics-Informed Neural Network Training Through Saddle-Point Reformulation](https://arxiv.org/abs/2507.16008)
*Dmitry Bylinkin,Mikhail Aleksandrov,Savelii Chezhegov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文将物理信息神经网络(PINNs)的训练重新表述为非凸强凹鞍点问题，通过理论分析和广泛实验验证，该方法在多种任务和架构上优于现有最先进技术。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络虽然在多个应用中表现出色，但由于损失函数复杂的景观特性，其性能仍然不稳定，需要找到更好的训练方法来解决这一问题。

Method: 将PINN训练重新表述为非凸强凹鞍点问题，建立了这种方法的理论基础，并基于该理论框架设计新的训练算法。

Result: 通过在各种任务和网络架构上的广泛实验评估，所提出的方法在性能上超越了当前的最先进技术，证明了该方法的有效性。

Conclusion: 将PINN训练问题重新表述为鞍点问题的方法能够有效改善训练稳定性，并在多个基准测试中取得了优于现有方法的性能表现。

Abstract: Physics-informed neural networks (PINNs) have gained prominence in recent
years and are now effectively used in a number of applications. However, their
performance remains unstable due to the complex landscape of the loss function.
To address this issue, we reformulate PINN training as a nonconvex-strongly
concave saddle-point problem. After establishing the theoretical foundation for
this approach, we conduct an extensive experimental study, evaluating its
effectiveness across various tasks and architectures. Our results demonstrate
that the proposed method outperforms the current state-of-the-art techniques.

</details>


### [28] [Reactivation: Empirical NTK Dynamics Under Task Shifts](https://arxiv.org/abs/2507.16039)
*Yuzhi Liu,Zixuan Chen,Zirui Zhang,Yufei Liu,Giulia Lanzillotta*

Main category: cs.LG

TL;DR: 本文首次系统研究了神经切线核(NTK)在持续学习场景下的动态变化，发现数据分布随时间变化时静态核近似不再有效，为理解持续学习中的神经网络训练动力学提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 现有NTK动力学研究仅限于单任务设置，数据分布假设恒定不变，而持续学习中数据分布会随时间变化。作者认为持续学习是探究神经网络训练动力学的丰富且未充分利用的测试平台，需要研究NTK在这种设置下的行为。

Method: 对神经切线核在持续学习环境中的动态变化进行全面的实证分析，研究当数据分布随时间变化时NTK的演化规律，并检验静态核近似在持续学习理论分析中的有效性。

Result: 研究发现持续学习确实是探究神经训练动力学的丰富测试平台，同时揭示了即使在大规模情况下，静态核近似在持续学习的理论处理中也存在有效性问题。

Conclusion: 持续学习为研究神经网络训练动力学提供了新的重要视角，挑战了现有理论中静态核近似的假设，表明在数据分布变化的场景下需要重新考虑NTK理论框架的适用性。

Abstract: The Neural Tangent Kernel (NTK) offers a powerful tool to study the
functional dynamics of neural networks. In the so-called lazy, or kernel
regime, the NTK remains static during training and the network function is
linear in the static neural tangents feature space. The evolution of the NTK
during training is necessary for feature learning, a key driver of deep
learning success. The study of the NTK dynamics has led to several critical
discoveries in recent years, in generalization and scaling behaviours. However,
this body of work has been limited to the single task setting, where the data
distribution is assumed constant over time. In this work, we present a
comprehensive empirical analysis of NTK dynamics in continual learning, where
the data distribution shifts over time. Our findings highlight continual
learning as a rich and underutilized testbed for probing the dynamics of neural
training. At the same time, they challenge the validity of static-kernel
approximations in theoretical treatments of continual learning, even at large
scale.

</details>


### [29] [A Lower Bound for the Number of Linear Regions of Ternary ReLU Regression Neural Networks](https://arxiv.org/abs/2507.16079)
*Yuta Nakahara,Manabu Kobayashi,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 本文从线性区域数量的角度理论分析了三元神经网络的表达能力，证明了三元神经网络的线性区域数量随网络宽度多项式增长，随深度指数增长，与标准神经网络类似，为三元神经网络的实际成功提供了理论解释。


<details>
  <summary>Details</summary>
Motivation: 深度学习的发展使得降低计算复杂度和内存消耗成为关键挑战，三元神经网络将参数限制在{-1, 0, +1}范围内，虽然在实际应用中表现出色，但理论理解仍然不足，需要从理论角度分析其表达能力。

Method: 从线性区域数量的角度理论分析三元神经网络的表达能力，具体评估使用ReLU激活函数的三元回归神经网络的线性区域数量，并与标准神经网络进行比较分析。

Result: 证明了三元神经网络的线性区域数量随网络宽度呈多项式增长，随深度呈指数增长，与标准神经网络的增长模式相似。同时证明了只需将三元神经网络的宽度平方或深度加倍，就能达到与一般ReLU回归神经网络相当的线性区域数量下界。

Conclusion: 通过理论分析证明了三元神经网络具有与标准神经网络相似的表达能力增长模式，为三元神经网络在实际应用中的成功提供了理论依据，在某种意义上解释了其优异性能的原因。

Abstract: With the advancement of deep learning, reducing computational complexity and
memory consumption has become a critical challenge, and ternary neural networks
(NNs) that restrict parameters to $\{-1, 0, +1\}$ have attracted attention as a
promising approach. While ternary NNs demonstrate excellent performance in
practical applications such as image recognition and natural language
processing, their theoretical understanding remains insufficient. In this
paper, we theoretically analyze the expressivity of ternary NNs from the
perspective of the number of linear regions. Specifically, we evaluate the
number of linear regions of ternary regression NNs with Rectified Linear Unit
(ReLU) for activation functions and prove that the number of linear regions
increases polynomially with respect to network width and exponentially with
respect to depth, similar to standard NNs. Moreover, we show that it suffices
to either square the width or double the depth of ternary NNs to achieve a
lower bound on the maximum number of linear regions comparable to that of
general ReLU regression NNs. This provides a theoretical explanation, in some
sense, for the practical success of ternary NNs.

</details>


### [30] [TorchAO: PyTorch-Native Training-to-Serving Model Optimization](https://arxiv.org/abs/2507.16099)
*Andrew Or,Apurva Jain,Daniel Vega-Myhre,Jesse Cai,Charles David Hernandez,Zhenrui Zheng,Driss Guessous,Vasiliy Kuznetsov,Christian Puhrsch,Mark Saroufim,Supriya Rao,Thien Tran,Aleksandar Samardžić*

Main category: cs.LG

TL;DR: TorchAO是一个PyTorch原生的模型优化框架，通过量化和稀疏性技术提供从训练到服务的端到端AI模型优化工作流，支持多种数据类型和优化技术，并与PyTorch生态系统深度集成。


<details>
  <summary>Details</summary>
Motivation: 现有的模型优化流程存在碎片化问题，缺乏统一的端到端工作流来支持从训练到服务的完整模型优化过程，需要一个能够整合各种优化技术并与广泛生态系统兼容的框架。

Method: 开发了基于PyTorch的TorchAO框架，采用新颖的张量子类抽象来表示各种低精度数据类型（INT4、INT8、FP8、MXFP4、MXFP6、MXFP8），支持多种模型优化技术（FP8量化训练、量化感知训练、训练后量化、2:4稀疏性），并与PyTorch生态系统各组件深度集成。

Result: 成功实现了统一的模型优化工作流，支持从预训练（TorchTitan）到微调（TorchTune、Axolotl）再到服务（HuggingFace、vLLM、SGLang、ExecuTorch）的完整流程，并成功应用于Llama 3.2 1B/3B和LlamaGuard3-8B量化模型的发布。

Conclusion: TorchAO成功解决了模型优化领域的碎片化问题，提供了一个统一、开源的PyTorch原生框架，能够有效支持各种量化和稀疏性优化技术，为AI模型的端到端优化提供了完整的解决方案。

Abstract: We present TorchAO, a PyTorch-native model optimization framework leveraging
quantization and sparsity to provide an end-to-end, training-to-serving
workflow for AI models. TorchAO supports a variety of popular model
optimization techniques, including FP8 quantized training, quantization-aware
training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and
leverages a novel tensor subclass abstraction to represent a variety of
widely-used, backend agnostic low precision data types, including INT4, INT8,
FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader
ecosystem at each step of the model optimization pipeline, from pre-training
(TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM,
SGLang, ExecuTorch), connecting an otherwise fragmented space in a single,
unified workflow. TorchAO has enabled recent launches of the quantized Llama
3.2 1B/3B and LlamaGuard3-8B models and is open-source at
https://github.com/pytorch/ao/.

</details>


### [31] [Learning Patient-Specific Spatial Biomarker Dynamics via Operator Learning for Alzheimer's Disease Progression](https://arxiv.org/abs/2507.16148)
*Jindong Wang,Yutong Mao,Xiao Liu,Wenrui Hao*

Main category: cs.LG

TL;DR: 本文提出了一个基于机器学习的算子学习框架，用于阿尔茨海默病的个性化建模，通过整合多模态数据和神经算子技术，实现了超过90%的预测准确率，为神经退行性疾病的精准治疗提供了可扩展的平台。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病是一种复杂的多因素神经退行性疾病，在疾病进展和治疗反应方面存在显著的异质性。尽管最近在治疗方面取得了进展，但能够准确预测个体化疾病轨迹的预测模型仍然有限，需要开发新的方法来解决这一挑战。

Method: 研究提出了一个基于机器学习的算子学习框架，整合纵向多模态影像、生物标志物和临床数据。与传统的预设动力学模型不同，该方法直接学习患者特异性的疾病算子，控制淀粉样蛋白、tau蛋白和神经退行性生物标志物的时空演化。使用拉普拉斯特征函数基构建几何感知神经算子，嵌入数字孪生范式中。

Result: 该方法应用于阿尔茨海默病临床数据，在多个生物标志物上实现了超过90%的高预测准确率，大幅超越了现有方法的性能。框架能够实现个体化预测、治疗干预模拟和硅内临床试验。

Conclusion: 这项工作为神经退行性疾病的精准建模和个性化治疗优化提供了一个可扩展、可解释的平台，展现了算子学习在医学应用中的巨大潜力。

Abstract: Alzheimer's disease (AD) is a complex, multifactorial neurodegenerative
disorder with substantial heterogeneity in progression and treatment response.
Despite recent therapeutic advances, predictive models capable of accurately
forecasting individualized disease trajectories remain limited. Here, we
present a machine learning-based operator learning framework for personalized
modeling of AD progression, integrating longitudinal multimodal imaging,
biomarker, and clinical data. Unlike conventional models with prespecified
dynamics, our approach directly learns patient-specific disease operators
governing the spatiotemporal evolution of amyloid, tau, and neurodegeneration
biomarkers. Using Laplacian eigenfunction bases, we construct geometry-aware
neural operators capable of capturing complex brain dynamics. Embedded within a
digital twin paradigm, the framework enables individualized predictions,
simulation of therapeutic interventions, and in silico clinical trials. Applied
to AD clinical data, our method achieves high prediction accuracy exceeding 90%
across multiple biomarkers, substantially outperforming existing approaches.
This work offers a scalable, interpretable platform for precision modeling and
personalized therapeutic optimization in neurodegenerative diseases.

</details>


### [32] [LLM Data Selection and Utilization via Dynamic Bi-level Optimization](https://arxiv.org/abs/2507.16178)
*Yang Yu,Kai Han,Hang Zhou,Yehui Tang,Kaiqi Huang,Yunhe Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: 该论文提出了一种动态数据加权模型(DWM)，通过双层优化框架在大语言模型训练过程中动态调整数据权重，相比传统静态数据选择方法能更好地提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据选择方法主要依赖静态的、与训练无关的标准，无法考虑模型训练过程中的动态变化和数据交互。需要一种能够适应模型训练动态过程的数据选择策略来提高训练效率并降低计算成本。

Method: 提出了数据加权模型(DWM)，通过双层优化框架在每个批次内动态调整选定数据的权重。该方法能够更好地捕捉训练模型的动态数据偏好，实现大语言模型训练过程中的动态数据利用。

Result: 实验表明DWM能够提升使用随机选择数据训练的模型性能，学习到的加权模型可以迁移到其他数据选择方法和不同规模的模型上。此外，研究还分析了模型在训练过程中数据偏好的演变规律。

Conclusion: DWM通过动态调整数据权重有效提升了大语言模型的训练效果，为理解模型训练过程中的数据偏好变化提供了新的洞察，并展现了良好的可迁移性。

Abstract: While large-scale training data is fundamental for developing capable large
language models (LLMs), strategically selecting high-quality data has emerged
as a critical approach to enhance training efficiency and reduce computational
costs. Current data selection methodologies predominantly rely on static,
training-agnostic criteria, failing to account for the dynamic model training
and data interactions. In this paper, we propose a new Data Weighting Model
(DWM) to adjust the weight of selected data within each batch to achieve a
dynamic data utilization during LLM training. Specially, to better capture the
dynamic data preference of the trained model, a bi-level optimization framework
is implemented to update the weighting model. Our experiments demonstrate that
DWM enhances the performance of models trained with randomly-selected data, and
the learned weighting model can be transferred to enhance other data selection
methods and models of different sizes. Moreover, we further analyze how a
model's data preferences evolve throughout training, providing new insights
into the data preference of the model during training.

</details>


### [33] [EBaReT: Expert-guided Bag Reward Transformer for Auto Bidding](https://arxiv.org/abs/2507.16186)
*Kaiyuan Li,Pengyu Wang,Yunshan Peng,Pengjia Yuan,Yanxiang Zeng,Rui Xiang,Yanhua Cheng,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: 本文提出了Expert-guided Bag Reward Transformer (EBaReT)方法，通过专家轨迹生成和袋奖励机制解决自动竞价中的数据质量和奖励不确定性问题


<details>
  <summary>Details</summary>
Motivation: 传统强化学习竞价方法面临数据质量低（次优出价多）和奖励不确定性高（点击率和转化率低）的挑战，现有生成式强化学习方法虽然有效但仍依赖监督学习，容易受到这些问题影响

Method: 提出EBaReT方法：1）生成专家轨迹作为补充训练数据，使用基于正-未标记学习的判别器识别专家转换；2）设计专家引导的推理策略确保决策达到专家水平；3）将一定时期内的转换视为"袋"，设计新的奖励函数实现更平滑的奖励获取

Result: 在广泛的实验中，该模型相比最先进的竞价方法取得了优越的性能表现

Conclusion: EBaReT方法通过专家轨迹生成、PU学习判别器和袋奖励机制，有效解决了自动竞价中的数据质量和奖励不确定性问题，实现了性能的显著提升

Abstract: Reinforcement learning has been widely applied in automated bidding.
Traditional approaches model bidding as a Markov Decision Process (MDP).
Recently, some studies have explored using generative reinforcement learning
methods to address long-term dependency issues in bidding environments.
Although effective, these methods typically rely on supervised learning
approaches, which are vulnerable to low data quality due to the amount of
sub-optimal bids and low probability rewards resulting from the low click and
conversion rates. Unfortunately, few studies have addressed these challenges.
  In this paper, we formalize the automated bidding as a sequence
decision-making problem and propose a novel Expert-guided Bag Reward
Transformer (EBaReT) to address concerns related to data quality and
uncertainty rewards. Specifically, to tackle data quality issues, we generate a
set of expert trajectories to serve as supplementary data in the training
process and employ a Positive-Unlabeled (PU) learning-based discriminator to
identify expert transitions. To ensure the decision also meets the expert
level, we further design a novel expert-guided inference strategy. Moreover, to
mitigate the uncertainty of rewards, we consider the transitions within a
certain period as a "bag" and carefully design a reward function that leads to
a smoother acquisition of rewards. Extensive experiments demonstrate that our
model achieves superior performance compared to state-of-the-art bidding
methods.

</details>


### [34] [RealBench: Benchmarking Verilog Generation Models with Real-World IP Designs](https://arxiv.org/abs/2507.16200)
*Pengwei Jin,Di Huang,Chongxiao Li,Shuyao Cheng,Yang Zhao,Xinyao Zheng,Jiaguo Zhu,Shuyi Xing,Bohan Dou,Rui Zhang,Zidong Du,Qi Guo,Xing Hu*

Main category: cs.LG

TL;DR: 本文提出了RealBench，首个面向真实世界IP级Verilog代码生成任务的基准测试，用于评估大语言模型在硬件设计自动化中的能力。该基准包含复杂的开源IP设计、多模态设计规范和严格的验证环境，评估结果显示即使是最优秀的LLM在模块级任务上也仅达到13.3%的通过率。


<details>
  <summary>Details</summary>
Motivation: 现有的Verilog代码生成评估基准存在设计过于简单、设计规范不足、验证环境不够严格等问题，无法真实反映实际硬件设计工作流程，因此需要一个更贴近真实世界应用场景的基准测试来准确评估大语言模型的Verilog代码生成能力。

Method: 构建RealBench基准测试，包含：1）复杂的结构化真实世界开源IP设计；2）多模态和格式化的设计规范；3）严格的验证环境，包括100%行覆盖率的测试平台和形式化检查器；4）支持模块级和系统级任务，实现对LLM能力的全面评估。

Result: 对多种大语言模型和智能体进行评估，结果显示即使是表现最佳的o1-preview模型，在模块级任务上的pass@1成功率仅为13.3%，在系统级任务上的成功率为0%，表明当前LLM在复杂Verilog代码生成方面还存在显著不足。

Conclusion: RealBench揭示了当前大语言模型在真实世界Verilog代码生成任务中的局限性，强调了未来需要开发更强大的Verilog代码生成模型。该基准为硬件设计自动化领域提供了更准确的评估工具，有助于推动该领域的进一步发展。

Abstract: The automatic generation of Verilog code using Large Language Models (LLMs)
has garnered significant interest in hardware design automation. However,
existing benchmarks for evaluating LLMs in Verilog generation fall short in
replicating real-world design workflows due to their designs' simplicity,
inadequate design specifications, and less rigorous verification environments.
To address these limitations, we present RealBench, the first benchmark aiming
at real-world IP-level Verilog generation tasks. RealBench features complex,
structured, real-world open-source IP designs, multi-modal and formatted design
specifications, and rigorous verification environments, including 100% line
coverage testbenches and a formal checker. It supports both module-level and
system-level tasks, enabling comprehensive assessments of LLM capabilities.
Evaluations on various LLMs and agents reveal that even one of the
best-performing LLMs, o1-preview, achieves only a 13.3% pass@1 on module-level
tasks and 0% on system-level tasks, highlighting the need for stronger Verilog
generation models in the future. The benchmark is open-sourced at
https://github.com/IPRC-DIP/RealBench.

</details>


### [35] [METER: Multi-modal Evidence-based Thinking and Explainable Reasoning -- Algorithm and Benchmark](https://arxiv.org/abs/2507.16206)
*Xu Yang,Qi Zhang,Shuming Jiang,Yaowen Xu,Zhaofan Zou,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: 该论文介绍了METER，一个用于可解释伪造检测的统一多模态基准数据集，涵盖图像、视频、音频和视听内容，不仅进行真假分类，还提供基于证据链的详细解释


<details>
  <summary>Details</summary>
Motivation: 现有的伪造检测方法主要专注于二元分类，缺乏对伪造内容的详细和可解释的解释，限制了其在安全关键场景中的适用性。此外，当前方法通常单独处理每种模态，缺乏跨模态伪造检测和解释的统一基准

Method: 提出METER统一多模态基准数据集，包含四个轨道，每个轨道都需要真假分类和基于证据链的解释（包括时空定位、文本理由和伪造类型追踪）。还提出了一个人类对齐的三阶段思维链（CoT）训练策略，结合SFT、DPO和新颖的GRPO阶段

Result: METER提供了比现有基准更广泛的模态覆盖和更丰富的可解释性指标，如空间/时间IoU、多类别追踪和证据一致性。建立了一个标准化的基础，用于推进生成媒体时代的可泛化和可解释伪造检测

Conclusion: METER将作为推进生成媒体时代可泛化和可解释伪造检测的标准化基础，为解决日益逼真的合成内容带来的误信息风险提供了重要工具

Abstract: With the rapid advancement of generative AI, synthetic content across images,
videos, and audio has become increasingly realistic, amplifying the risk of
misinformation. Existing detection approaches predominantly focus on binary
classification while lacking detailed and interpretable explanations of
forgeries, which limits their applicability in safety-critical scenarios.
Moreover, current methods often treat each modality separately, without a
unified benchmark for cross-modal forgery detection and interpretation. To
address these challenges, we introduce METER, a unified, multi-modal benchmark
for interpretable forgery detection spanning images, videos, audio, and
audio-visual content. Our dataset comprises four tracks, each requiring not
only real-vs-fake classification but also evidence-chain-based explanations,
including spatio-temporal localization, textual rationales, and forgery type
tracing. Compared to prior benchmarks, METER offers broader modality coverage
and richer interpretability metrics such as spatial/temporal IoU, multi-class
tracing, and evidence consistency. We further propose a human-aligned,
three-stage Chain-of-Thought (CoT) training strategy combining SFT, DPO, and a
novel GRPO stage that integrates a human-aligned evaluator with CoT reasoning.
We hope METER will serve as a standardized foundation for advancing
generalizable and interpretable forgery detection in the era of generative
media.

</details>


### [36] [Aligned Manifold Property and Topology Point Clouds for Learning Molecular Properties](https://arxiv.org/abs/2507.16223)
*Alexander Mihalcea*

Main category: cs.LG

TL;DR: 本文提出AMPTCR分子表面表示方法，结合量子标量场和拓扑描述符，在分子量预测和细菌抑制任务上表现出色，为表面介导的分子性质建模提供了紧凑高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有分子性质预测模型依赖SMILES字符串和分子图等表示方法，忽略了驱动分子间行为的表面局部现象。3D方法要么减少表面细节，要么需要昂贵的SE(3)等变架构来处理空间变化。

Method: 提出AMPTCR（对齐流形性质和拓扑云表示）方法，将局部量子衍生标量场和定制拓扑描述符结合在对齐点云格式中。每个表面点包含化学意义的标量、测地线衍生拓扑向量和转换到规范参考框架的坐标，使用DGCNN框架进行评估。

Result: 在分子量预测任务上验证R²达到0.87，证实AMPTCR编码了物理意义数据。在细菌抑制任务上，分类任务ROC AUC达到0.912，回归任务R²为0.54，使用双Fukui函数作为电子描述符和Morgan指纹作为辅助数据。

Conclusion: AMPTCR为表面介导的分子性质建模提供了紧凑、表达力强且与架构无关的表示方法，成功克服了现有方法的局限性，在多个任务上展现出良好性能。

Abstract: Machine learning models for molecular property prediction generally rely on
representations -- such as SMILES strings and molecular graphs -- that overlook
the surface-local phenomena driving intermolecular behavior. 3D-based
approaches often reduce surface detail or require computationally expensive
SE(3)-equivariant architectures to manage spatial variance. To overcome these
limitations, this work introduces AMPTCR (Aligned Manifold Property and
Topology Cloud Representation), a molecular surface representation that
combines local quantum-derived scalar fields and custom topological descriptors
within an aligned point cloud format. Each surface point includes a chemically
meaningful scalar, geodesically derived topology vectors, and coordinates
transformed into a canonical reference frame, enabling efficient learning with
conventional SE(3)-sensitive architectures. AMPTCR is evaluated using a DGCNN
framework on two tasks: molecular weight and bacterial growth inhibition. For
molecular weight, results confirm that AMPTCR encodes physically meaningful
data, with a validation R^2 of 0.87. In the bacterial inhibition task, AMPTCR
enables both classification and direct regression of E. coli inhibition values
using Dual Fukui functions as the electronic descriptor and Morgan Fingerprints
as auxiliary data, achieving an ROC AUC of 0.912 on the classification task,
and an R^2 of 0.54 on the regression task. These results help demonstrate that
AMPTCR offers a compact, expressive, and architecture-agnostic representation
for modeling surface-mediated molecular properties.

</details>


### [37] [Multi-Agent Reinforcement Learning for Sample-Efficient Deep Neural Network Mapping](https://arxiv.org/abs/2507.16249)
*Srivatsan Krishnan,Jason Jabbour,Dan Zhang,Natasha Jaques,Aleksandra Faust,Shayegan Omidshafiei,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: 提出了一种去中心化多智能体强化学习框架，通过智能体聚类算法来优化深度神经网络到硬件的映射，相比单智能体强化学习在样本效率上提升30-300倍，并显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络到硬件的映射对于优化延迟、能耗和资源利用率至关重要，但映射空间庞大复杂。虽然强化学习是一种有前景的方法，但其有效性常常受到样本效率低下的限制。

Method: 提出去中心化多智能体强化学习(MARL)框架，通过多个智能体分布式搜索加速探索。引入智能体聚类算法，基于相关性分析将相似的映射参数分配给相同的智能体，实现去中心化的并行学习过程。

Result: 实验结果显示，MARL方法相比标准单智能体强化学习在样本效率上提升30-300倍，在相同样本条件下实现高达32.61倍的延迟降低和16.45倍的能耗延迟乘积(EDP)降低。

Conclusion: 所提出的去中心化多智能体强化学习框架通过智能体聚类显著提高了深度神经网络硬件映射优化的样本效率，在延迟和能耗方面取得了显著的性能提升。

Abstract: Mapping deep neural networks (DNNs) to hardware is critical for optimizing
latency, energy consumption, and resource utilization, making it a cornerstone
of high-performance accelerator design. Due to the vast and complex mapping
space, reinforcement learning (RL) has emerged as a promising approach-but its
effectiveness is often limited by sample inefficiency. We present a
decentralized multi-agent reinforcement learning (MARL) framework designed to
overcome this challenge. By distributing the search across multiple agents, our
framework accelerates exploration. To avoid inefficiencies from training
multiple agents in parallel, we introduce an agent clustering algorithm that
assigns similar mapping parameters to the same agents based on correlation
analysis. This enables a decentralized, parallelized learning process that
significantly improves sample efficiency. Experimental results show our MARL
approach improves sample efficiency by 30-300x over standard single-agent RL,
achieving up to 32.61x latency reduction and 16.45x energy-delay product (EDP)
reduction under iso-sample conditions.

</details>


### [38] [Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training](https://arxiv.org/abs/2507.16274)
*Zixiao Huang,Junhao Hu,Hao Lin,Chunyang Zhu,Yueran Tang,Quanlu Zhang,Zhen Guo,Zhenhua Li,Shengen Yan,Zhenhua Zhu,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: STWeaver是一个GPU内存分配器，通过离线规划和在线分配的结合来减少内存碎片，平均减少79.2%的碎片率，提升训练性能达32.5%


<details>
  <summary>Details</summary>
Motivation: 大语言模型快速扩展导致GPU内存压力增大，虚拟流水线和重计算等优化技术破坏了张量生命周期并引入严重内存碎片。现有深度学习框架的默认GPU内存分配器采用在线策略且不了解张量生命周期，可能浪费高达43%的内存并导致内存不足错误

Method: 提出STWeaver GPU内存分配器，采用离线规划与在线分配相结合的新范式。离线规划利用时空规律性生成近优分配计划，在线分配处理复杂动态模型如专家混合模型(MoE)。作为可插拔的PyTorch分配器实现

Result: 在密集和稀疏模型上平均减少79.2%的内存碎片率（最高可达100%），开销微不足道。能够支持更高效的高吞吐量训练配置，性能提升高达32.5%

Conclusion: STWeaver通过利用训练工作负载中内存分配行为的时空规律性，有效解决了GPU内存碎片问题，显著提升了深度学习训练的内存利用效率和整体性能

Abstract: The rapid scaling of large language models (LLMs) has significantly increased
GPU memory pressure, which is further aggravated by training optimization
techniques such as virtual pipeline and recomputation that disrupt tensor
lifespans and introduce considerable memory fragmentation. Default GPU memory
allocators of popular deep learning frameworks like PyTorch use online
strategies without knowledge of tensor lifespans, which can waste up to 43\% of
memory and cause out-of-memory errors, rendering optimization techniques
ineffective or even unusable.
  To address this, we introduce STWeaver, a GPU memory allocator for deep
learning frameworks that reduces fragmentation by exploiting the spatial and
temporal regularity in memory allocation behaviors of training workloads.
STWeaver introduces a novel paradigm that combines offline planning with online
allocation. The offline planning leverages spatio-temporal regularities to
generate a near-optimal allocation plan, while the online allocation handles
complex and dynamic models such as Mixture-of-Experts (MoE). Built as a
pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by
79.2\% (up to 100\%) across both dense and sparse models, with negligible
overhead. This enables more efficient, high-throughput training configurations
and improves performance by up to 32.5\%.

</details>


### [39] [Understanding Generalization, Robustness, and Interpretability in Low-Capacity Neural Networks](https://arxiv.org/abs/2507.16278)
*Yash Kumar*

Main category: cs.LG

TL;DR: 该研究通过MNIST数据集上的二分类任务，系统探索了低容量神经网络中模型容量、稀疏性和鲁棒性之间的基本关系，发现了最小容量需求与任务复杂度成正比、存在高性能稀疏子网络、以及过参数化提升抗干扰能力等关键发现。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习虽然依赖大规模过参数化模型，但低容量网络中容量、稀疏性和鲁棒性之间的基本相互作用仍是重要研究领域。研究者希望通过控制框架来系统性地研究这些基础性质。

Method: 构建了一个控制框架，使用MNIST数据集创建了一系列视觉难度递增的二分类任务（如0和1 vs. 4和9），通过这些任务来研究模型容量、稀疏性和鲁棒性的关系。还使用显著性图进行可解释性分析。

Result: 获得三个核心发现：1）成功泛化所需的最小模型容量与任务复杂度直接成比例；2）训练好的网络对极端幅度剪枝具有鲁棒性（高达95%稀疏度），证明存在稀疏、高性能子网络；3）过参数化在对抗输入损坏方面提供显著的鲁棒性优势。显著性图分析确认稀疏子网络保持了原始密集模型的核心推理过程。

Conclusion: 该工作为简单神经网络的基础权衡关系提供了清晰的实证论证，揭示了容量、稀疏性和鲁棒性之间的重要联系，为理解神经网络的基本特性提供了有价值的见解。

Abstract: Although modern deep learning often relies on massive over-parameterized
models, the fundamental interplay between capacity, sparsity, and robustness in
low-capacity networks remains a vital area of study. We introduce a controlled
framework to investigate these properties by creating a suite of binary
classification tasks from the MNIST dataset with increasing visual difficulty
(e.g., 0 and 1 vs. 4 and 9). Our experiments reveal three core findings. First,
the minimum model capacity required for successful generalization scales
directly with task complexity. Second, these trained networks are robust to
extreme magnitude pruning (up to 95% sparsity), revealing the existence of
sparse, high-performing subnetworks. Third, we show that over-parameterization
provides a significant advantage in robustness against input corruption.
Interpretability analysis via saliency maps further confirms that these
identified sparse subnetworks preserve the core reasoning process of the
original dense models. This work provides a clear, empirical demonstration of
the foundational trade-offs governing simple neural networks.

</details>


### [40] [Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning](https://arxiv.org/abs/2507.16302)
*Boheng Li,Renjie Gu,Junjie Wang,Leyi Qi,Yiming Li,Run Wang,Zhan Qin,Tianwei Zhang*

Main category: cs.LG

TL;DR: 本文提出ResAlign框架，解决文本到图像扩散模型在个性化微调后安全性退化的问题，通过Moreau包络重构和元学习策略增强安全遗忘方法的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在个性化微调时容易从预训练数据中继承有害行为，虽然安全导向的遗忘方法能抑制模型毒性，但这些方法在下游微调时表现脆弱，即使在完全良性的数据集上微调也会失效

Method: 提出ResAlign安全遗忘框架：1）将下游微调建模为隐式优化问题，使用Moreau包络重构实现高效梯度估计以最小化有害行为恢复；2）采用元学习策略模拟多样化的微调场景分布以提高泛化能力

Result: 在多种数据集、微调方法和配置上的广泛实验表明，ResAlign在保持良性生成能力的同时，在下游微调后保持安全性方面始终优于现有的遗忘方法

Conclusion: ResAlign框架有效解决了安全遗忘方法在下游微调中的脆弱性问题，为文本到图像扩散模型的安全个性化应用提供了更可靠的解决方案

Abstract: Text-to-image (T2I) diffusion models have achieved impressive image
generation quality and are increasingly fine-tuned for personalized
applications. However, these models often inherit unsafe behaviors from toxic
pretraining data, raising growing safety concerns. While recent safety-driven
unlearning methods have made promising progress in suppressing model toxicity,
they are identified to be fragile to downstream fine-tuning, where we reveal
that state-of-the-art methods largely fail to retain their effectiveness even
when fine-tuned on entirely benign datasets. To mitigate this problem, in this
paper, we propose ResAlign, a safety-driven unlearning framework with enhanced
resilience against downstream fine-tuning. By modeling downstream fine-tuning
as an implicit optimization problem with a Moreau Envelope-based reformulation,
ResAlign enables efficient gradient estimation to minimize the recovery of
harmful behaviors. Additionally, a meta-learning strategy is proposed to
simulate a diverse distribution of fine-tuning scenarios to improve
generalization. Extensive experiments across a wide range of datasets,
fine-tuning methods, and configurations demonstrate that ResAlign consistently
outperforms prior unlearning approaches in retaining safety after downstream
fine-tuning while preserving benign generation capability well.

</details>


### [41] [Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design](https://arxiv.org/abs/2507.16307)
*Xin-De Wang,Zhi-Rui Chen,Peng-Jie Guo,Ze-Feng Gao,Cheng Mu,Zhong-Yi Lu*

Main category: cs.LG

TL;DR: 研究团队开发了专门针对钙钛矿太阳能电池前驱体添加剂发现与设计的大语言模型Perovskite-R1，通过挖掘1,232篇高质量科学文献和33,269种候选材料，构建了领域特定的指令调优数据集，实现了智能的文献洞察综合和创新解决方案生成，为钙钛矿光伏研究提供了数据驱动的闭环框架。


<details>
  <summary>Details</summary>
Motivation: 钙钛矿太阳能电池虽然具有优异的功率转换效率，但在长期稳定性、环境可持续性和规模化制造方面仍面临挑战。科学文献的爆炸性增长和材料、工艺、器件结构之间的复杂相互作用使研究人员难以高效获取、组织和利用领域知识。需要一个专门的工具来加速前驱体添加剂的发现和设计。

Method: 系统挖掘和整理1,232篇高质量科学文献，集成包含33,269种候选材料的综合库，使用自动问答生成和思维链推理构建领域特定的指令调优数据集，在此数据集上对QwQ-32B模型进行微调，开发出具有先进推理能力的专业大语言模型Perovskite-R1。

Result: Perovskite-R1能够智能地综合文献洞察，为缺陷钝化和前驱体添加剂选择生成创新且实用的解决方案。通过实验验证了几种模型提出的策略，证实其在改善材料稳定性和性能方面的有效性。

Conclusion: 研究展示了领域适应性大语言模型在加速材料发现方面的潜力，为钙钛矿光伏研究提供了智能、数据驱动的闭环框架，推动了该领域的科学进步。

Abstract: Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in
next-generation photovoltaic technologies, owing to their exceptional power
conversion efficiencies and advantageous material properties. Despite these
advances, challenges such as long-term stability, environmental sustainability,
and scalable manufacturing continue to hinder their commercialization.
Precursor additive engineering has shown promise in addressing these issues by
enhancing both the performance and durability of PSCs. However, the explosive
growth of scientific literature and the complex interplay of materials,
processes, and device architectures make it increasingly difficult for
researchers to efficiently access, organize, and utilize domain knowledge in
this rapidly evolving field. To address this gap, we introduce Perovskite-R1, a
specialized large language model (LLM) with advanced reasoning capabilities
tailored for the discovery and design of PSC precursor additives. By
systematically mining and curating 1,232 high-quality scientific publications
and integrating a comprehensive library of 33,269 candidate materials, we
constructed a domain-specific instruction-tuning dataset using automated
question-answer generation and chain-of-thought reasoning. Fine-tuning the
QwQ-32B model on this dataset resulted in Perovskite-R1, which can
intelligently synthesize literature insights and generate innovative and
practical solutions for defect passivation and the selection of precursor
additives. Experimental validation of several model-proposed strategies
confirms their effectiveness in improving material stability and performance.
Our work demonstrates the potential of domain-adapted LLMs in accelerating
materials discovery and provides a closed-loop framework for intelligent,
data-driven advancements in perovskite photovoltaic research.

</details>


### [42] [The Cost of Compression: Tight Quadratic Black-Box Attacks on Sketches for $\ell_2$ Norm Estimation](https://arxiv.org/abs/2507.16345)
*Sara Ahmadian,Edith Cohen,Uri Stemmer*

Main category: cs.LG

TL;DR: 研究了线性草图（linear sketching）在黑盒对抗环境下的脆弱性，提出了一种通用的非自适应攻击方法，使用O(k²)次查询就能导致范数估计失败或构造对抗样本。


<details>
  <summary>Details</summary>
Motivation: 线性草图是一种强大且广泛使用的降维技术，但已知容易受到对抗性输入的攻击。研究者希望在黑盒对抗设置下，系统性地分析这种脆弱性，其中固定的隐藏草图矩阵将高维向量映射到低维草图，对手可以查询系统获得从草图计算的近似ℓ2范数估计。

Method: 提出了一种通用的非自适应攻击方法，该攻击完全不依赖于草图矩阵和估计器的具体实现。攻击方法可以应用于任何线性草图和任何查询响应器，包括随机化、自适应或针对查询分布定制的响应器。使用Õ(k²)次查询来实现攻击目标。

Result: 攻击能够导致范数估计失败，或者构造出使查询分布的最优估计器失效的对抗性输入。下界构造与已知的Õ(Ω(k²))上界紧密匹配，这些上界是通过Johnson-Lindenstrauss变换和AMS草图的专门估计器实现的。

Conclusion: 研究揭示了压缩表示的基本脆弱性，发现了与图像分类中对抗攻击的结构性相似之处。这表明线性草图等降维技术存在固有的安全风险，需要在实际应用中予以考虑。研究为理解压缩表示在对抗环境下的行为提供了重要的理论基础。

Abstract: Dimensionality reduction via linear sketching is a powerful and widely used
technique, but it is known to be vulnerable to adversarial inputs. We study the
black-box adversarial setting, where a fixed, hidden sketching matrix A in
$R^{k X n}$ maps high-dimensional vectors v $\in R^n$ to lower-dimensional
sketches A v in $R^k$, and an adversary can query the system to obtain
approximate ell2-norm estimates that are computed from the sketch.
  We present a universal, nonadaptive attack that, using tilde(O)($k^2$)
queries, either causes a failure in norm estimation or constructs an
adversarial input on which the optimal estimator for the query distribution
(used by the attack) fails. The attack is completely agnostic to the sketching
matrix and to the estimator: It applies to any linear sketch and any query
responder, including those that are randomized, adaptive, or tailored to the
query distribution.
  Our lower bound construction tightly matches the known upper bounds of
tilde(Omega)($k^2$), achieved by specialized estimators for Johnson
Lindenstrauss transforms and AMS sketches. Beyond sketching, our results
uncover structural parallels to adversarial attacks in image classification,
highlighting fundamental vulnerabilities of compressed representations.

</details>


### [43] [Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks](https://arxiv.org/abs/2507.16347)
*Yumeng Wang,Zengyi Wo,Wenjun Wang,Xingcheng Fu,Minglai Shao*

Main category: cs.LG

TL;DR: 提出了HPGNN模型，将高阶个性化PageRank与图神经网络结合，通过捕获长程和多尺度节点交互来解决异质性图中的节点分类问题，在基准数据集上表现优于多数现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络主要假设同质性（相连节点具有相似标签），但在许多真实异质性图中这一假设不成立。现有异质性图模型主要依赖成对关系，忽略了高阶结构的多尺度信息，导致在节点间冲突类别信息噪声下性能次优。

Method: 提出HPGNN模型，集成高阶个性化PageRank与图神经网络。引入高效的个性化PageRank高阶近似来捕获长程和多尺度节点交互，通过将高阶结构信息嵌入卷积网络来有效建模不同图维度的关键交互，降低计算复杂度并减轻周围信息噪声。

Result: 在基准数据集上的大量实验表明HPGNN的有效性。该模型在异质性图的下游任务中优于七种最先进方法中的五种，同时在同质性图上保持竞争性能。HPGNN能够平衡多尺度信息并对噪声具有鲁棒性。

Conclusion: HPGNN通过集成高阶个性化PageRank有效解决了异质性图中的节点分类挑战，为现实世界图学习问题提供了一个多功能的解决方案，在处理多尺度信息和抗噪声方面表现出色。

Abstract: Graph Neural Networks (GNNs) excel in node classification tasks but often
assume homophily, where connected nodes share similar labels. This assumption
does not hold in many real-world heterophilic graphs. Existing models for
heterophilic graphs primarily rely on pairwise relationships, overlooking
multi-scale information from higher-order structures. This leads to suboptimal
performance, particularly under noise from conflicting class information across
nodes. To address these challenges, we propose HPGNN, a novel model integrating
Higher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces
an efficient high-order approximation of Personalized PageRank (PPR) to capture
long-range and multi-scale node interactions. This approach reduces
computational complexity and mitigates noise from surrounding information. By
embedding higher-order structural information into convolutional networks,
HPGNN effectively models key interactions across diverse graph dimensions.
Extensive experiments on benchmark datasets demonstrate HPGNN's effectiveness.
The model achieves better performance than five out of seven state-of-the-art
methods on heterophilic graphs in downstream tasks while maintaining
competitive performance on homophilic graphs. HPGNN's ability to balance
multi-scale information and robustness to noise makes it a versatile solution
for real-world graph learning challenges. Codes are available at
https://github.com/streetcorner/HPGNN.

</details>


### [44] [Bipartite Patient-Modality Graph Learning with Event-Conditional Modelling of Censoring for Cancer Survival Prediction](https://arxiv.org/abs/2507.16363)
*Hailin Yue,Hulin Kuang,Jin Liu,Junjian Li,Lanlan Wang,Mengshen He,Jianxin Wang*

Main category: cs.LG

TL;DR: 本文提出CenSurv方法，通过二分图患者-模态学习和事件条件删失建模来改进癌症生存预测，有效利用删失样本并处理模态缺失问题


<details>
  <summary>Details</summary>
Motivation: 现有癌症生存预测研究仅关注已知生存风险样本间的关系，未充分利用删失样本的价值，且在模态缺失场景下性能下降严重，推理过程困难

Method: 提出CenSurv框架：1）使用图结构建模多模态数据获取表示；2）设计二分图模拟各种模态缺失场景下的患者-模态关系，采用完整-不完整对齐策略探索模态无关特征；3）设计即插即用的事件条件删失建模(ECMC)模块，通过动态动量累积置信度选择可靠删失数据并分配更准确的生存时间

Result: 在5个公开癌症数据集上的评估显示，CenSurv在平均C-index上比最佳基线方法提升3.1%，在各种模态缺失场景下表现出优异的鲁棒性。ECMC模块使8个基线方法在5个数据集上的平均C-index提升1.3%

Conclusion: CenSurv通过有效利用删失样本和处理模态缺失问题，显著提升了癌症生存预测的准确性和鲁棒性，为个性化治疗提供了更可靠的预测工具

Abstract: Accurately predicting the survival of cancer patients is crucial for
personalized treatment. However, existing studies focus solely on the
relationships between samples with known survival risks, without fully
leveraging the value of censored samples. Furthermore, these studies may suffer
performance degradation in modality-missing scenarios and even struggle during
the inference process. In this study, we propose a bipartite patient-modality
graph learning with event-conditional modelling of censoring for cancer
survival prediction (CenSurv). Specifically, we first use graph structure to
model multimodal data and obtain representation. Then, to alleviate performance
degradation in modality-missing scenarios, we design a bipartite graph to
simulate the patient-modality relationship in various modality-missing
scenarios and leverage a complete-incomplete alignment strategy to explore
modality-agnostic features. Finally, we design a plug-and-play
event-conditional modeling of censoring (ECMC) that selects reliable censored
data using dynamic momentum accumulation confidences, assigns more accurate
survival times to these censored data, and incorporates them as uncensored data
into training. Comprehensive evaluations on 5 publicly cancer datasets showcase
the superiority of CenSurv over the best state-of-the-art by 3.1% in terms of
the mean C-index, while also exhibiting excellent robustness under various
modality-missing scenarios. In addition, using the plug-and-play ECMC module,
the mean C-index of 8 baselines increased by 1.3% across 5 datasets. Code of
CenSurv is available at https://github.com/yuehailin/CenSurv.

</details>


### [45] [Optimization and generalization analysis for two-layer physics-informed neural networks without over-parametrization](https://arxiv.org/abs/2507.16380)
*Zhihan Zeng,Yiqi Gu*

Main category: cs.LG

TL;DR: 本文研究了随机梯度下降(SGD)在物理信息神经网络(PINNs)最小二乘回归中的行为，提出了避免过参数化的新理论分析，证明在网络宽度超过特定阈值时，训练损失和期望损失可降至O(ε)以下。


<details>
  <summary>Details</summary>
Motivation: 现有基于过参数化理论的PINN分析需要网络宽度随训练样本数量大幅增长，导致计算成本过高且与实际实验相去甚远，因此需要发展更实用的理论框架。

Method: 对双层PINN的SGD训练进行新的优化和泛化分析，通过对目标函数做特定假设来避免过参数化，建立网络宽度阈值与问题参数和精度ε的关系。

Result: 证明了当网络宽度超过仅依赖于ε和问题本身的阈值时，训练损失和期望损失都能降低到O(ε)以下，避免了对训练样本数量的依赖。

Conclusion: 提出的理论框架成功避免了过参数化的局限性，为PINN的实际应用提供了更贴近实验的理论指导，显著降低了所需的计算资源。

Abstract: This work focuses on the behavior of stochastic gradient descent (SGD) in
solving least-squares regression with physics-informed neural networks (PINNs).
Past work on this topic has been based on the over-parameterization regime,
whose convergence may require the network width to increase vastly with the
number of training samples. So, the theory derived from over-parameterization
may incur prohibitive computational costs and is far from practical
experiments. We perform new optimization and generalization analysis for SGD in
training two-layer PINNs, making certain assumptions about the target function
to avoid over-parameterization. Given $\epsilon>0$, we show that if the network
width exceeds a threshold that depends only on $\epsilon$ and the problem, then
the training loss and expected loss will decrease below $O(\epsilon)$.

</details>


### [46] [Improving Predictions on Highly Unbalanced Data Using Open Source Synthetic Data Upsampling](https://arxiv.org/abs/2507.16419)
*Ivona Krchova,Michael Platzer,Paul Tiwald*

Main category: cs.LG

TL;DR: 本文研究了使用AI生成的合成数据来处理高度不平衡表格数据集的效果，通过MOSTLY AI的合成数据SDK与传统上采样方法进行对比，证明合成数据能够有效提升少数类的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在欺诈检测、医疗诊断和罕见事件预测等现实场景中，不平衡表格数据集中的少数类严重缺乏代表性，传统机器学习算法倾向于偏向多数类，导致模型在少数类预测上表现不佳。需要寻找有效的方法来解决少数类样本不足的问题。

Method: 使用MOSTLY AI的开源合成数据SDK生成合成样本来上采样高度不平衡的表格数据集，并与朴素过采样和SMOTE-NC等标准方法进行比较。评估在合成数据上训练的预测模型与使用传统方法的模型性能差异。

Result: 合成数据通过生成多样化的数据点填补特征空间中的稀疏区域，能够改善少数群体的预测准确性。使用合成数据上采样的训练数据始终产生表现最佳的预测模型，特别是对于包含极少数少数类样本的混合类型数据集。

Conclusion: AI生成的合成数据是处理高度不平衡表格数据集的有效解决方案，相比传统上采样方法能够显著提升模型对少数类的预测性能，为解决现实世界中的不平衡数据问题提供了新的思路。

Abstract: Unbalanced tabular data sets present significant challenges for predictive
modeling and data analysis across a wide range of applications. In many
real-world scenarios, such as fraud detection, medical diagnosis, and rare
event prediction, minority classes are vastly underrepresented, making it
difficult for traditional machine learning algorithms to achieve high accuracy.
These algorithms tend to favor the majority class, leading to biased models
that struggle to accurately represent minority classes. Synthetic data holds
promise for addressing the under-representation of minority classes by
providing new, diverse, and highly realistic samples. This paper presents a
benchmark study on the use of AI-generated synthetic data for upsampling highly
unbalanced tabular data sets.
  We evaluate the effectiveness of an open-source solution, the Synthetic Data
SDK by MOSTLY AI, which provides a flexible and user-friendly approach to
synthetic upsampling for mixed-type data. We compare predictive models trained
on data sets upsampled with synthetic records to those using standard methods,
such as naive oversampling and SMOTE-NC. Our results demonstrate that synthetic
data can improve predictive accuracy for minority groups by generating diverse
data points that fill gaps in sparse regions of the feature space. We show that
upsampled synthetic training data consistently results in top-performing
predictive models, particularly for mixed-type data sets containing very few
minority samples.

</details>


### [47] [Canonical Correlation Patterns for Validating Clustering of Multivariate Time Series](https://arxiv.org/abs/2507.16497)
*Isabella Degen,Zahraa S Abdallah,Kate Robson Brown,Henry W J Reeve*

Main category: cs.LG

TL;DR: 该论文提出了基于典型相关模式的多元时间序列聚类验证方法，解决了相关性聚类缺乏有效验证指标的问题，为高风险领域的相关性聚类提供了严格的验证基础。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类验证指标主要为欧几里得数据开发，对于相关性模式的有效性尚未得到系统评估。与欧几里得聚类不同，相关性存在于连续空间中，缺乏等效的离散参考模式，导致基于相关性的聚类验证面临根本性挑战。

Method: 引入典型相关模式作为数学定义的验证目标，将无限的相关性空间离散化为有限的、可解释的参考模式。使用具有完美真实标签的合成数据集在受控条件下进行测试，采用L1范数进行映射，L5范数用于轮廓宽度准则和Davies-Bouldin指数。

Result: 实验表明典型模式提供了可靠的验证目标，L1范数映射和L5范数的轮廓宽度准则及Davies-Bouldin指数表现出优异的性能。这些方法对分布偏移具有鲁棒性，能够适当检测相关结构退化。

Conclusion: 建立了在高风险领域进行严格相关性聚类验证的方法学基础，为相关性聚类的实际应用提供了可靠的验证框架和实施指导原则。

Abstract: Clustering of multivariate time series using correlation-based methods
reveals regime changes in relationships between variables across health,
finance, and industrial applications. However, validating whether discovered
clusters represent distinct relationships rather than arbitrary groupings
remains a fundamental challenge. Existing clustering validity indices were
developed for Euclidean data, and their effectiveness for correlation patterns
has not been systematically evaluated. Unlike Euclidean clustering, where
geometric shapes provide discrete reference targets, correlations exist in
continuous space without equivalent reference patterns. We address this
validation gap by introducing canonical correlation patterns as mathematically
defined validation targets that discretise the infinite correlation space into
finite, interpretable reference patterns. Using synthetic datasets with perfect
ground truth across controlled conditions, we demonstrate that canonical
patterns provide reliable validation targets, with L1 norm for mapping and L5
norm for silhouette width criterion and Davies-Bouldin index showing superior
performance. These methods are robust to distribution shifts and appropriately
detect correlation structure degradation, enabling practical implementation
guidelines. This work establishes a methodological foundation for rigorous
correlation-based clustering validation in high-stakes domains.

</details>


### [48] [Analogy making as amortised model construction](https://arxiv.org/abs/2507.16511)
*David G. Nagy,Tingke Shen,Hanqi Zhou,Charley M. Wu,Peter Dayan*

Main category: cs.LG

TL;DR: 该论文提出了一个基于类比的框架，通过将类比形式化为马尔可夫决策过程间的部分同态映射，使智能体能够重用过去经验中的解决方案相关结构，从而降低模型构建和规划的计算成本。


<details>
  <summary>Details</summary>
Motivation: 人类在面对新情况时需要灵活构建内部模型进行导航，但这些模型既要足够忠实于环境以确保资源有限的规划能产生适当结果，又要在构建时保持可处理性。现有方法在平衡模型保真度和计算可行性方面存在挑战。

Method: 将类比形式化为马尔可夫决策过程（MDP）之间的部分同态映射，构建一个框架，其中从先前构造中派生的抽象模块作为新构造的可组合构建块。通过模块化重用实现跨具有共同结构本质的域的策略和表示的灵活适应。

Result: 提出的框架能够使智能体重用过去经验中与解决方案相关的结构，并分摊模型构建（解释）和规划的计算成本。抽象模块的可组合性为新的模型构建提供了灵活的基础。

Conclusion: 类比在智能体的模型构建和规划过程中发挥核心作用，通过模块化重用机制，能够实现在保持结构本质相同的不同领域间灵活适应策略和表示，为构建更高效的智能系统提供了理论基础。

Abstract: Humans flexibly construct internal models to navigate novel situations. To be
useful, these internal models must be sufficiently faithful to the environment
that resource-limited planning leads to adequate outcomes; equally, they must
be tractable to construct in the first place. We argue that analogy plays a
central role in these processes, enabling agents to reuse solution-relevant
structure from past experiences and amortise the computational costs of both
model construction (construal) and planning. Formalising analogies as partial
homomorphisms between Markov decision processes, we sketch a framework in which
abstract modules, derived from previous construals, serve as composable
building blocks for new ones. This modular reuse allows for flexible adaptation
of policies and representations across domains with shared structural essence.

</details>


### [49] [confopt: A Library for Implementation and Evaluation of Gradient-based One-Shot NAS Methods](https://arxiv.org/abs/2507.16533)
*Abhash Kumar Jha,Shakiba Moradian,Arjun Krishnakumar,Martin Rapp,Frank Hutter*

Main category: cs.LG

TL;DR: 本文介绍了Configurable Optimizer (confopt)，一个用于简化基于梯度的一次性神经架构搜索方法开发和评估的可扩展库，并通过新的基准测试套件揭示了当前评估方法的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的一次性神经架构搜索面临两大挑战：1）评估过度依赖DARTS基准测试，导致改进往往在噪声范围内；2）实现分散在不同代码库中，难以进行公平和可重现的比较。

Method: 开发了Configurable Optimizer (confopt)框架，提供最小化API以便用户集成新搜索空间，支持将NAS优化器分解为核心组件，并创建了基于DARTS的新基准测试套件和新颖的评估协议。

Result: 通过confopt框架创建了新的DARTS基准测试套件，结合新的评估协议揭示了当前基于梯度的一次性NAS方法评估中存在的关键缺陷。

Conclusion: confopt框架为基于梯度的一次性NAS方法提供了统一的开发和评估平台，有助于更公平、可重现的比较，并通过新的评估协议发现了现有评估方法的重要问题。

Abstract: Gradient-based one-shot neural architecture search (NAS) has significantly
reduced the cost of exploring architectural spaces with discrete design
choices, such as selecting operations within a model. However, the field faces
two major challenges. First, evaluations of gradient-based NAS methods heavily
rely on the DARTS benchmark, despite the existence of other available
benchmarks. This overreliance has led to saturation, with reported improvements
often falling within the margin of noise. Second, implementations of
gradient-based one-shot NAS methods are fragmented across disparate
repositories, complicating fair and reproducible comparisons and further
development. In this paper, we introduce Configurable Optimizer (confopt), an
extensible library designed to streamline the development and evaluation of
gradient-based one-shot NAS methods. Confopt provides a minimal API that makes
it easy for users to integrate new search spaces, while also supporting the
decomposition of NAS optimizers into their core components. We use this
framework to create a suite of new DARTS-based benchmarks, and combine them
with a novel evaluation protocol to reveal a critical flaw in how
gradient-based one-shot NAS methods are currently assessed. The code can be
found at https://github.com/automl/ConfigurableOptimizer.

</details>


### [50] [Symbolic Graph Intelligence: Hypervector Message Passing for Learning Graph-Level Patterns with Tsetlin Machines](https://arxiv.org/abs/2507.16537)
*Christian D. Blakely*

Main category: cs.LG

TL;DR: 提出了一个基于稀疏二进制超向量和Tsetlin机器的多层符号框架，用于图分类任务，通过结构化消息传递将图编码为符号超向量，在保持竞争性准确率的同时提供了强符号透明性和局部可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的神经图模型虽然性能优异，但缺乏可解释性和符号透明性。需要一种既能保持图的层次语义结构，又能提供局部可解释性的图分类方法。

Method: 提出多层符号框架，使用稀疏二进制超向量和Tsetlin机器进行图分类。通过结构化消息传递将节点、边和属性信息绑定并捆绑到符号超向量中。采用从节点属性到边关系再到结构角色的分层绑定，保持图的层次语义。同时制定了局部可解释性框架。

Result: 在TUDataset基准测试上验证了方法的有效性，与神经图模型相比展现出竞争性的准确率，同时具有强符号透明性。实现了紧凑的离散表示和局部可解释性。

Conclusion: 该多层符号框架成功地在图分类任务中平衡了性能和可解释性，通过符号超向量表示实现了对图结构的有效编码，为需要透明性和可解释性的图分析应用提供了有价值的解决方案。

Abstract: We propose a multilayered symbolic framework for general graph classification
that leverages sparse binary hypervectors and Tsetlin Machines. Each graph is
encoded through structured message passing, where node, edge, and attribute
information are bound and bundled into a symbolic hypervector. This process
preserves the hierarchical semantics of the graph through layered binding from
node attributes to edge relations to structural roles resulting in a compact,
discrete representation. We also formulate a local interpretability framework
which lends itself to a key advantage of our approach being locally
interpretable. We validate our method on TUDataset benchmarks, demonstrating
competitive accuracy with strong symbolic transparency compared to neural graph
models.

</details>


### [51] [A Comprehensive Data-centric Overview of Federated Graph Learning](https://arxiv.org/abs/2507.16541)
*Zhengyu Wu,Xunkai Li,Yinlin Zhu,Zekai Chen,Guochen Yan,Yanyu Yan,Hao Zhang,Yuming Ai,Xinmo Jin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了一个以数据为中心的联邦图学习(FGL)综述，通过数据特征和数据利用两个层面的分类法重新组织FGL研究，并探讨了FGL与预训练大模型的集成及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦图学习综述主要关注联邦学习和图机器学习的方法整合，缺乏从数据中心视角系统性地审视FGL方法如何处理数据中心约束以提升模型性能，因此需要一个以数据为中心的分类框架来重新组织FGL研究。

Method: 提出了一个两层次的以数据为中心的分类法：1)数据特征层面，基于FGL中使用数据集的结构和分布特性进行分类；2)数据利用层面，分析用于克服关键数据中心挑战的训练程序和技术。每个分类层次由三个正交标准定义，代表不同的数据中心配置。

Result: 通过这个以数据为中心的分类框架，系统性地重新组织了FGL研究，并分析了FGL与预训练大模型的集成情况，展示了现实应用场景，为FGL领域提供了新的研究视角和组织方式。

Conclusion: 以数据为中心的视角为联邦图学习研究提供了新的分类框架，有助于更好地理解FGL方法如何处理数据约束问题，同时FGL与预训练大模型的集成以及与图机器学习新兴趋势的结合将是未来重要的发展方向。

Abstract: In the era of big data applications, Federated Graph Learning (FGL) has
emerged as a prominent solution that reconcile the tradeoff between optimizing
the collective intelligence between decentralized datasets holders and
preserving sensitive information to maximum. Existing FGL surveys have
contributed meaningfully but largely focus on integrating Federated Learning
(FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that
emphasis on methodology and simulated scenarios. Notably, a data centric
perspective, which systematically examines FGL methods through the lens of data
properties and usage, remains unadapted to reorganize FGL research, yet it is
critical to assess how FGL studies manage to tackle data centric constraints to
enhance model performances. This survey propose a two-level data centric
taxonomy: Data Characteristics, which categorizes studies based on the
structural and distributional properties of datasets used in FGL, and Data
Utilization, which analyzes the training procedures and techniques employed to
overcome key data centric challenges. Each taxonomy level is defined by three
orthogonal criteria, each representing a distinct data centric configuration.
Beyond taxonomy, this survey examines FGL integration with Pretrained Large
Models, showcases realistic applications, and highlights future direction
aligned with emerging trends in GML.

</details>


### [52] [Scaling Linear Attention with Sparse State Expansion](https://arxiv.org/abs/2507.16577)
*Yuqi Pan,Yongqi An,Zheng Li,Yuhong Chou,Ruijie Zhu,Xiaohui Wang,Mingxuan Wang,Jinqiao Wang,Guoqi Li*

Main category: cs.LG

TL;DR: 本文提出了稀疏状态扩展(SSE)架构来解决Transformer在长上下文场景中的二次计算复杂度问题，通过行稀疏更新和状态分区技术实现高效的上下文压缩，在数学推理等任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在长上下文场景中面临二次计算复杂度和线性内存增长的问题。虽然现有的线性注意力变体通过将上下文压缩到固定大小的状态来缓解效率约束，但在上下文检索和推理等任务中往往会降低性能。因此需要一种更有效的上下文压缩方法。

Method: 提出两个关键创新：1）引入行稀疏更新公式，将状态更新概念化为信息分类，通过基于softmax的top-k硬分类实现稀疏状态更新；2）在稀疏框架内提出稀疏状态扩展(SSE)，将上下文状态扩展到多个分区中，有效地将参数大小与状态容量解耦，同时保持稀疏分类范式。

Result: 在纯线性和混合(SSE-H)架构中，SSE在语言建模、上下文检索和数学推理基准测试中表现出色。SSE展现了强大的检索性能，并且随状态大小良好扩展。经过强化学习训练后，2B参数的SSE-H模型在小型推理模型中达到了最先进的数学推理性能，在AIME24上得分64.7，在AIME25上得分51.3，显著超越了类似规模的开源Transformer。

Conclusion: SSE作为一种有前景且高效的长上下文建模架构，成功解决了Transformer在长序列处理中的效率问题，同时在多项任务中保持了优异的性能表现，特别是在数学推理任务上取得了突破性的成果。

Abstract: The Transformer architecture, despite its widespread success, struggles with
long-context scenarios due to quadratic computation and linear memory growth.
While various linear attention variants mitigate these efficiency constraints
by compressing context into fixed-size states, they often degrade performance
in tasks such as in-context retrieval and reasoning. To address this limitation
and achieve more effective context compression, we propose two key innovations.
First, we introduce a row-sparse update formulation for linear attention by
conceptualizing state updating as information classification. This enables
sparse state updates via softmax-based top-$k$ hard classification, thereby
extending receptive fields and reducing inter-class interference. Second, we
present Sparse State Expansion (SSE) within the sparse framework, which expands
the contextual state into multiple partitions, effectively decoupling parameter
size from state capacity while maintaining the sparse classification paradigm.
Our design, supported by efficient parallelized implementations, yields
effective classification and discriminative state representations. We
extensively validate SSE in both pure linear and hybrid (SSE-H) architectures
across language modeling, in-context retrieval, and mathematical reasoning
benchmarks. SSE demonstrates strong retrieval performance and scales favorably
with state size. Moreover, after reinforcement learning (RL) training, our 2B
SSE-H model achieves state-of-the-art mathematical reasoning performance among
small reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,
significantly outperforming similarly sized open-source Transformers. These
results highlight SSE as a promising and efficient architecture for
long-context modeling.

</details>


### [53] [Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs](https://arxiv.org/abs/2507.16672)
*Yushang Zhao,Huijie Shen,Dannier Li,Lu Chang,Chengrui Zhou,Yinuo Yang*

Main category: cs.LG

TL;DR: 本文提出了一个基于元学习的框架，通过参数高效的提示调优来快速个性化基于大语言模型的推荐系统，特别针对冷启动用户场景，实现了实时推荐和金融风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的推荐系统在冷启动用户场景下表现不佳，传统的监督微调和协同过滤方法成本高昂且难以维护更新，需要一种能够快速适应新用户的高效个性化方法。

Method: 提出了一个元学习框架，使用一阶优化（Reptile）和二阶优化（MAML）来学习软提示嵌入，将每个用户视为独立任务。通过episodic采样、内循环适应和外循环泛化进行元优化，学习可微分的控制变量来表示用户行为先验。

Result: 在MovieLens-1M、Amazon Reviews和Recbole数据集上，该模型在NDCG@10、HR@10和MRR指标上超越了强基线，在消费级GPU上实现了实时运行（低于300毫秒），支持零历史个性化，275毫秒的适应速度可用于金融系统的实时风险评估。

Conclusion: 该框架成功解决了LLM推荐系统的冷启动问题，实现了快速个性化和实时适应，不仅在推荐任务上表现优异，还能应用于金融风险评估，通过缩短检测延迟和改善支付网络稳定性来增强国家金融基础设施的韧性。

Abstract: Generative, explainable, and flexible recommender systems, derived using
Large Language Models (LLM) are promising and poorly adapted to the cold-start
user situation, where there is little to no history of interaction. The current
solutions i.e. supervised fine-tuning and collaborative filtering are
dense-user-item focused and would be expensive to maintain and update. This
paper introduces a meta-learning framework, that can be used to perform
parameter-efficient prompt-tuning, to effectively personalize LLM-based
recommender systems quickly at cold-start. The model learns soft prompt
embeddings with first-order (Reptile) and second-order (MAML) optimization by
treating each of the users as the tasks. As augmentations to the input tokens,
these learnable vectors are the differentiable control variables that represent
user behavioral priors. The prompts are meta-optimized through episodic
sampling, inner-loop adaptation, and outer-loop generalization. On
MovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model
outperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in
real-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization
is also supported by this scalable solution, and its 275 ms rate of adaptation
allows successful real-time risk profiling of financial systems by shortening
detection latency and improving payment network stability. Crucially, the 275
ms adaptation capability can enable real-time risk profiling for financial
institutions, reducing systemic vulnerability detection latency significantly
versus traditional compliance checks. By preventing contagion in payment
networks (e.g., Fedwire), the framework strengthens national financial
infrastructure resilience.

</details>


### [54] [GASPnet: Global Agreement to Synchronize Phases](https://arxiv.org/abs/2507.16674)
*Andrea Alamiaa,Sabine Muzellec,Thomas Serre,Rufin VanRullen*

Main category: cs.LG

TL;DR: 本文提出了一种受神经科学启发的新型网络机制，通过结合Transformer注意力机制和"同步绑定"理论，在卷积网络中引入角度相位来解决视觉绑定问题，在噪声鲁棒性和泛化能力方面超越了传统CNN。


<details>
  <summary>Details</summary>
Motivation: 现有的全局"协议路由"机制在多分类任务中表现不足，需要改进以更好地处理复杂的视觉绑定问题。受神经科学中"同步绑定"理论启发，作者希望开发一种能够通过神经元时间活动同步来绑定同一物体特征、同时有效分离不同物体特征的机制。

Method: 将Transformer注意力操作与神经科学的"同步绑定"理论相结合，在卷积网络的所有层中引入角度相位。通过Kuramoto动力学实现相位对齐，增强具有相似相位的神经元之间的操作，同时抑制具有相反相位的神经元操作。

Result: 在两个数据集上进行测试：数字对数据集和MNIST叠加CIFAR-10图像数据集。结果显示该方法相比CNN网络具有更好的准确性，在噪声鲁棒性和泛化能力方面表现更优。

Conclusion: 提出了一种新颖的机制，通过利用神经科学和机器学习的协同作用，成功解决了神经网络中的视觉绑定问题，为构建更加鲁棒和高效的神经网络架构提供了新的思路。

Abstract: In recent years, Transformer architectures have revolutionized most fields of
artificial intelligence, relying on an attentional mechanism based on the
agreement between keys and queries to select and route information in the
network. In previous work, we introduced a novel, brain-inspired architecture
that leverages a similar implementation to achieve a global 'routing by
agreement' mechanism. Such a system modulates the network's activity by
matching each neuron's key with a single global query, pooled across the entire
network. Acting as a global attentional system, this mechanism improves noise
robustness over baseline levels but is insufficient for multi-classification
tasks. Here, we improve on this work by proposing a novel mechanism that
combines aspects of the Transformer attentional operations with a compelling
neuroscience theory, namely, binding by synchrony. This theory proposes that
the brain binds together features by synchronizing the temporal activity of
neurons encoding those features. This allows the binding of features from the
same object while efficiently disentangling those from distinct objects. We
drew inspiration from this theory and incorporated angular phases into all
layers of a convolutional network. After achieving phase alignment via Kuramoto
dynamics, we use this approach to enhance operations between neurons with
similar phases and suppresses those with opposite phases. We test the benefits
of this mechanism on two datasets: one composed of pairs of digits and one
composed of a combination of an MNIST item superimposed on a CIFAR-10 image.
Our results reveal better accuracy than CNN networks, proving more robust to
noise and with better generalization abilities. Overall, we propose a novel
mechanism that addresses the visual binding problem in neural networks by
leveraging the synergy between neuroscience and machine learning.

</details>


### [55] [Custom Algorithm-based Fault Tolerance for Attention Layers in Transformers](https://arxiv.org/abs/2507.16676)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: 本文提出Flash-ABFT方法，通过单次检查计算注意力机制中查询、键值矩阵三元乘积的在线校验和，显著降低硬件开销的同时保持高故障检测精度


<details>
  <summary>Details</summary>
Motivation: 传统的基于算法的容错技术(ABFT)只能验证单个矩阵乘法，无法有效处理完整的注意力机制，特别是中间的softmax归一化操作，因此需要专门针对注意力加速器的错误检测方法

Method: 提出Flash-ABFT方法，能够通过单次检查计算注意力层中查询(query)、键(key)和值(value)矩阵整个三元乘积的在线校验和，包括softmax操作，消除冗余检查

Result: 实验结果显示Flash-ABFT仅产生5.3%的硬件面积开销和不到1.9%的能耗开销，同时保持高故障检测精度

Conclusion: Flash-ABFT为注意力加速器提供了一种成本效益高且鲁棒的错误检测解决方案，能够以极低的开销实现对整个注意力机制的有效故障检测

Abstract: Transformers and large language models (LLMs), powered by the attention
mechanism, have transformed numerous AI applications, driving the need for
specialized hardware accelerators. A major challenge in these accelerators is
efficiently detecting errors caused by random hardware faults. Traditional
algorithm-based fault tolerance (ABFT) techniques verify individual matrix
multiplications but fall short in handling the full attention mechanism,
particularly due to intermediate softmax normalization. This work proposes
Flash-ABFT, a novel method that computes an online checksum across the entire
three-matrix product of query, key and value matrices, of an attention layer,
including the softmax operation, with a single check. This approach
significantly reduces overhead by eliminating redundant checks while
maintaining high fault-detection accuracy. Experimental results demonstrate
that Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9%
energy overhead, making it a cost-effective and robust solution for error
detection in attention accelerators.

</details>


### [56] [Latent Space Alignment for AI-Native MIMO Semantic Communications](https://arxiv.org/abs/2507.16680)
*Mario Edoardo Pandolfo,Simone Fiorellino,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: 本文提出了一种基于MIMO通信的语义通信方法，通过学习MIMO预编码器/解码器对来解决语义通信中的潜在空间不匹配问题，同时实现潜在空间压缩和语义信道均衡。


<details>
  <summary>Details</summary>
Motivation: 在语义通信中，当设备使用不同的语言、逻辑或内部表示时，会出现语义不匹配问题，可能阻碍相互理解。现有方法无法有效解决潜在空间不对齐和物理信道损伤的双重挑战。

Method: 提出两种解决方案：(1)线性模型，通过乘子交替方向法(ADMM)求解双凸优化问题；(2)基于神经网络的模型，在传输功率预算和复杂度约束下学习语义MIMO预编码器/解码器。该方法联合执行潜在空间压缩和语义信道均衡。

Result: 数值结果表明所提方法在面向目标的语义通信场景中的有效性，展示了精度、通信负担和解决方案复杂度之间的主要权衡关系。该方法能够同时缓解语义不匹配和物理信道损伤。

Conclusion: 通过MIMO通信技术可以有效解决语义通信中的潜在空间不对齐问题。所提出的预编码器/解码器设计能够在保证通信质量的同时，实现语义理解的改善，为语义通信系统的实际部署提供了可行的解决方案。

Abstract: Semantic communications focus on prioritizing the understanding of the
meaning behind transmitted data and ensuring the successful completion of tasks
that motivate the exchange of information. However, when devices rely on
different languages, logic, or internal representations, semantic mismatches
may occur, potentially hindering mutual understanding. This paper introduces a
novel approach to addressing latent space misalignment in semantic
communications, exploiting multiple-input multiple-output (MIMO)
communications. Specifically, our method learns a MIMO precoder/decoder pair
that jointly performs latent space compression and semantic channel
equalization, mitigating both semantic mismatches and physical channel
impairments. We explore two solutions: (i) a linear model, optimized by solving
a biconvex optimization problem via the alternating direction method of
multipliers (ADMM); (ii) a neural network-based model, which learns semantic
MIMO precoder/decoder under transmission power budget and complexity
constraints. Numerical results demonstrate the effectiveness of the proposed
approach in a goal-oriented semantic communication scenario, illustrating the
main trade-offs between accuracy, communication burden, and complexity of the
solutions.

</details>


### [57] [FISHER: A Foundation Model for Multi-Modal Industrial Signal Comprehensive Representation](https://arxiv.org/abs/2507.16696)
*Pingyi Fan,Anbai Jiang,Shuwei Zhang,Zhiqiang Lv,Bing Han,Xinhu Zheng,Wenrui Liang,Junjie Li,Wei-Qiang Zhang,Yanmin Qian,Xie Chen,Cheng Lu,Jia Liu*

Main category: cs.LG

TL;DR: 提出了FISHER，一个用于多模态工业信号综合表示的基础模型，通过统一建模M5信号（多模态、多采样率、多设备、多任务、多领域）来解决工业异常检测问题，在多个健康管理任务上相比顶级SSL模型实现了最高5.03%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着SCADA系统的快速部署，工业信号分析和异常状态检测成为紧迫需求。现有工作只关注小子问题并使用专门化模型，未能利用模态间协同效应和强大的扩展定律，而工业信号存在显著的异构性（M5问题：多模态、多采样率、多设备、多任务、多领域）。

Method: 提出FISHER基础模型，将采样率增量视为子带信息的串联来支持任意采样率，以STFT子带作为建模单元，采用教师-学生自监督学习框架进行预训练。开发了RMIS基准测试来评估M5工业信号在多个健康管理任务上的表示能力。

Result: 与顶级SSL模型相比，FISHER展现出多功能和出色的能力，通用性能提升高达5.03%，同时具有更高效的扩展曲线。研究了下游任务的扩展定律并为未来工作指明了潜在方向。

Conclusion: FISHER成功实现了工业信号的统一建模，有效解决了M5异构性问题，在多个健康管理任务上取得显著性能提升，为工业信号分析提供了强大的基础模型解决方案。模型已开源供研究使用。

Abstract: With the rapid deployment of SCADA systems, how to effectively analyze
industrial signals and detect abnormal states is an urgent need for the
industry. Due to the significant heterogeneity of these signals, which we
summarize as the M5 problem, previous works only focus on small sub-problems
and employ specialized models, failing to utilize the synergies between
modalities and the powerful scaling law. However, we argue that the M5 signals
can be modeled in a unified manner due to the intrinsic similarity. As a
result, we propose FISHER, a Foundation model for multi-modal Industrial Signal
compreHEnsive Representation. To support arbitrary sampling rates, FISHER
considers the increment of sampling rate as the concatenation of sub-band
information. Specifically, FISHER takes the STFT sub-band as the modeling unit
and adopts a teacher student SSL framework for pre-training. We also develop
the RMIS benchmark, which evaluates the representations of M5 industrial
signals on multiple health management tasks. Compared with top SSL models,
FISHER showcases versatile and outstanding capabilities with a general
performance gain up to 5.03%, along with much more efficient scaling curves. We
also investigate the scaling law on downstream tasks and derive potential
avenues for future works. FISHER is now open-sourced on
https://github.com/jianganbai/FISHER

</details>


### [58] [Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation](https://arxiv.org/abs/2507.16704)
*Viktor Muryn,Marta Sumyk,Mariya Hirna,Sofiya Garkot,Maksym Shamrai*

Main category: cs.LG

TL;DR: Screen2AX是首个从单张截图自动创建实时、树状结构可访问性元数据的框架，解决了桌面应用可访问性支持不足的问题，在macOS环境下实现了77% F1分数的层次树重建性能，并在自主智能体任务执行中带来2.2倍性能提升。


<details>
  <summary>Details</summary>
Motivation: 许多桌面应用因开发者提供的可访问性元数据不完整或缺失而难以访问，调查显示仅33%的macOS应用提供完整的可访问性支持。现有的结构化屏幕表示工作主要解决特定挑战，如UI元素检测或字幕生成，但没有尝试通过复制完整层次结构来捕获桌面界面的全部复杂性。

Method: 使用视觉-语言模型和目标检测模型来检测、描述和层次化组织UI元素，模拟macOS系统级可访问性结构。编译并公开发布了三个数据集，涵盖112个macOS应用程序，每个都标注了UI元素检测、分组和层次化可访问性元数据以及相应的截图。

Result: Screen2AX在重建完整可访问性树方面达到77% F1分数，层次树提高了自主智能体解释和交互复杂桌面界面的能力。在Screen2AX-Task基准测试中，相比原生可访问性表示实现2.2倍性能提升，在ScreenSpot基准测试中超越了最先进的OmniParser V2系统。

Conclusion: Screen2AX成功解决了桌面应用可访问性元数据缺失的问题，通过自动生成树状结构的可访问性元数据，显著提升了AI智能体在桌面环境中的任务执行能力，为改善数字包容性和智能体系统性能提供了有效解决方案。

Abstract: Desktop accessibility metadata enables AI agents to interpret screens and
supports users who depend on tools like screen readers. Yet, many applications
remain largely inaccessible due to incomplete or missing metadata provided by
developers - our investigation shows that only 33% of applications on macOS
offer full accessibility support. While recent work on structured screen
representation has primarily addressed specific challenges, such as UI element
detection or captioning, none has attempted to capture the full complexity of
desktop interfaces by replicating their entire hierarchical structure. To
bridge this gap, we introduce Screen2AX, the first framework to automatically
create real-time, tree-structured accessibility metadata from a single
screenshot. Our method uses vision-language and object detection models to
detect, describe, and organize UI elements hierarchically, mirroring macOS's
system-level accessibility structure. To tackle the limited availability of
data for macOS desktop applications, we compiled and publicly released three
datasets encompassing 112 macOS applications, each annotated for UI element
detection, grouping, and hierarchical accessibility metadata alongside
corresponding screenshots. Screen2AX accurately infers hierarchy trees,
achieving a 77% F1 score in reconstructing a complete accessibility tree.
Crucially, these hierarchy trees improve the ability of autonomous agents to
interpret and interact with complex desktop interfaces. We introduce
Screen2AX-Task, a benchmark specifically designed for evaluating autonomous
agent task execution in macOS desktop environments. Using this benchmark, we
demonstrate that Screen2AX delivers a 2.2x performance improvement over native
accessibility representations and surpasses the state-of-the-art OmniParser V2
system on the ScreenSpot benchmark.

</details>


### [59] [Improving Model Classification by Optimizing the Training Dataset](https://arxiv.org/abs/2507.16729)
*Morad Tukan,Loay Mualem,Eitan Netzer,Liran Sigalat*

Main category: cs.LG

TL;DR: 本文提出了一个系统性框架来调优核心集生成过程，以提升下游分类质量。通过引入确定性采样、类别分配和主动采样等可调参数，调优后的核心集在分类指标上显著优于传统核心集和全数据集训练。


<details>
  <summary>Details</summary>
Motivation: 传统基于敏感度的核心集构建方法在优化分类性能指标（如F1分数）方面表现不足，主要关注损失近似而非分类质量。在数据中心化AI时代，需要更好的数据筛选方法来提升分类模型的训练效率和性能。

Method: 提出了一个系统性的核心集调优框架，引入了超越传统敏感度分数的新可调参数，包括：(1)确定性采样策略；(2)类别间的样本分配机制；(3)通过主动采样进行精炼优化。这些参数共同作用来优化核心集生成过程。

Result: 在多个数据集和分类器上进行的大量实验表明，调优后的核心集在关键分类指标上显著优于传统核心集和全数据集训练方法，证明了该方法的有效性和普适性。

Conclusion: 调优核心集为更好且更高效的模型训练提供了有效路径。通过系统性地调整核心集生成过程中的关键参数，可以在保持数据效率的同时显著提升分类性能，这对数据中心化AI具有重要意义。

Abstract: In the era of data-centric AI, the ability to curate high-quality training
data is as crucial as model design. Coresets offer a principled approach to
data reduction, enabling efficient learning on large datasets through
importance sampling. However, conventional sensitivity-based coreset
construction often falls short in optimizing for classification performance
metrics, e.g., $F1$ score, focusing instead on loss approximation. In this
work, we present a systematic framework for tuning the coreset generation
process to enhance downstream classification quality. Our method introduces new
tunable parameters--including deterministic sampling, class-wise allocation,
and refinement via active sampling, beyond traditional sensitivity scores.
Through extensive experiments on diverse datasets and classifiers, we
demonstrate that tuned coresets can significantly outperform both vanilla
coresets and full dataset training on key classification metrics, offering an
effective path towards better and more efficient model training.

</details>


### [60] [Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795)
*Helena Casademunt,Caden Juang,Adam Karvonen,Samuel Marks,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出了概念消融微调(CAFT)技术，通过在微调过程中消融不期望的概念方向来控制大语言模型的泛化行为，无需修改训练数据即可避免意外的分布外泛化问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调可能导致意外的分布外泛化问题，传统方法需要修改训练数据来解决这一问题，但这在实践中并不总是可行的。因此需要一种无需修改训练数据就能控制模型泛化行为的新方法。

Method: 提出概念消融微调(CAFT)技术，利用可解释性工具识别模型潜在空间中对应不期望概念的方向，然后在微调过程中通过线性投影消融这些概念，从而引导模型远离意外泛化。

Result: 在三个微调任务上成功应用CAFT，包括新兴错位对齐问题。在不改变微调数据的情况下，CAFT将错位对齐响应减少了10倍，同时不降低在训练分布上的性能表现。

Conclusion: CAFT代表了一种无需修改训练数据就能引导大语言模型泛化的新颖方法，为控制模型的意外泛化行为提供了有效的解决方案。

Abstract: Fine-tuning large language models (LLMs) can lead to unintended
out-of-distribution generalization. Standard approaches to this problem rely on
modifying training data, for example by adding data that better specify the
intended generalization. However, this is not always practical. We introduce
Concept Ablation Fine-Tuning (CAFT), a technique that leverages
interpretability tools to control how LLMs generalize from fine-tuning, without
needing to modify the training data or otherwise use data from the target
distribution. Given a set of directions in an LLM's latent space corresponding
to undesired concepts, CAFT works by ablating these concepts with linear
projections during fine-tuning, steering the model away from unintended
generalizations. We successfully apply CAFT to three fine-tuning tasks,
including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow
task generalize to give egregiously misaligned responses to general questions.
Without any changes to the fine-tuning data, CAFT reduces misaligned responses
by 10x without degrading performance on the training distribution. Overall,
CAFT represents a novel approach for steering LLM generalization without
modifying training data.

</details>


### [61] [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](https://arxiv.org/abs/2507.16806)
*Mehul Damani,Isha Puri,Stewart Slocum,Idan Shenfeld,Leshem Choshen,Yoon Kim,Jacob Andreas*

Main category: cs.LG

TL;DR: 本文提出RLCR方法，通过在强化学习奖励函数中加入校准奖励来训练语言模型，在保持准确率的同时显著改善了模型的置信度校准，解决了传统RL训练导致的幻觉和校准退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法使用二元奖励函数训练推理模型时，虽然能提高准确率，但会导致模型校准能力退化，增加幻觉和低置信度输出的错误率，缺乏可靠的置信度估计。

Method: 提出RLCR（带校准奖励的强化学习）方法：让语言模型在推理后同时生成预测和数值置信度估计，使用结合二元正确性得分和Brier得分的奖励函数进行优化，其中Brier得分激励校准的预测。

Result: 在多个数据集上，RLCR在不损失准确率的情况下显著改善了校准能力，在域内和域外评估中都优于普通RL训练和后验置信度分类器。言语化的置信度还可以在测试时通过置信度加权缩放方法提高准确率和校准。

Conclusion: 明确优化校准能力可以产生更可靠的推理模型。RLCR方法证明了使用有界、恰当评分规则的奖励函数能够产生既准确又校准良好的模型，为构建更可靠的语言模型推理系统提供了有效途径。

Abstract: When language models (LMs) are trained via reinforcement learning (RL) to
generate natural language "reasoning chains", their performance improves on a
variety of difficult question answering tasks. Today, almost all successful
applications of RL for reasoning use binary reward functions that evaluate the
correctness of LM outputs. Because such reward functions do not penalize
guessing or low-confidence outputs, they often have the unintended side-effect
of degrading calibration and increasing the rate at which LMs generate
incorrect responses (or "hallucinate") in other problem domains. This paper
describes RLCR (Reinforcement Learning with Calibration Rewards), an approach
to training reasoning models that jointly improves accuracy and calibrated
confidence estimation. During RLCR, LMs generate both predictions and numerical
confidence estimates after reasoning. They are trained to optimize a reward
function that augments a binary correctness score with a Brier score -- a
scoring rule for confidence estimates that incentivizes calibrated prediction.
We first prove that this reward function (or any analogous reward function that
uses a bounded, proper scoring rule) yields models whose predictions are both
accurate and well-calibrated. We next show that across diverse datasets, RLCR
substantially improves calibration with no loss in accuracy, on both in-domain
and out-of-domain evaluations -- outperforming both ordinary RL training and
classifiers trained to assign post-hoc confidence scores. While ordinary RL
hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized
confidence can be leveraged at test time to improve accuracy and calibration
via confidence-weighted scaling methods. Our results show that explicitly
optimizing for calibration can produce more generally reliable reasoning
models.

</details>


### [62] [Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning](https://arxiv.org/abs/2507.16814)
*Junhao Shen,Haiteng Zhao,Yuzhe Gu,Songyang Gao,Kuikun Liu,Haian Huang,Jianfei Gao,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.LG

TL;DR: 本文提出SOPHIA方法，通过半离策略强化学习增强大型视觉语言模型的慢思考推理能力，在多个多模态推理基准上达到开源模型最佳性能，甚至超越部分闭源模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型主要通过视觉语言对齐训练，难以采用在策略强化学习发展慢思考能力，因为展示空间受限于初始能力；而直接从外部模型蒸馏轨迹的离策略强化学习可能因视觉感知能力不匹配导致视觉幻觉问题。

Method: 提出SOPHIA半离策略强化学习框架：1）结合可训练LVLM的在策略视觉理解与语言模型的离策略慢思考推理构建半离策略行为模型；2）为推理分配基于结果的奖励并向后传播视觉奖励；3）LVLM通过离策略强化学习算法从获得的推理轨迹和传播奖励中学习慢思考推理能力。

Result: 在InternVL2.5和InternVL3.0（8B和38B规模）上的广泛实验表明SOPHIA的有效性。SOPHIA使InternVL3.0-38B平均提升8.50%，在多个多模态推理基准上达到开源LVLM最先进性能，在MathVision和OlympiadBench上分别达到49.08%和49.95%的pass@1准确率，超越了部分闭源模型（如GPT-4.1）。

Conclusion: 分析显示SOPHIA优于监督微调和直接在策略强化学习方法，为进一步在策略训练提供了更好的策略初始化，成功解决了大型视觉语言模型慢思考推理能力增强的关键技术挑战。

Abstract: Enhancing large vision-language models (LVLMs) with visual slow-thinking
reasoning is crucial for solving complex multimodal tasks. However, since LVLMs
are mainly trained with vision-language alignment, it is difficult to adopt
on-policy reinforcement learning (RL) to develop the slow thinking ability
because the rollout space is restricted by its initial abilities. Off-policy RL
offers a way to go beyond the current policy, but directly distilling
trajectories from external models may cause visual hallucinations due to
mismatched visual perception abilities across models. To address these issues,
this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for
vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy
behavior model by combining on-policy visual understanding from a trainable
LVLM with off-policy slow-thinking reasoning from a language model, assigns
outcome-based rewards to reasoning, and propagates visual rewards backward.
Then LVLM learns slow-thinking reasoning ability from the obtained reasoning
trajectories using propagated rewards via off-policy RL algorithms. Extensive
experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the
effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in
average, reaching state-of-the-art performance among open-source LVLMs on
multiple multimodal reasoning benchmarks, and even outperforms some
closed-source models (e.g., GPT-4.1) on the challenging MathVision and
OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively.
Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy
RL methods, offering a better policy initialization for further on-policy
training.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [63] [Structural DID with ML: Theory, Simulation, and a Roadmap for Applied Research](https://arxiv.org/abs/2507.15899)
*Yile Yu,Anzhi Xu,Yi Wang*

Main category: stat.ML

TL;DR: 本文提出了S-DIDML框架，将传统双重差分法与机器学习相结合，通过结构化残差正交化技术解决高维协变量干扰问题，同时保持因果结构的可解释性，为复杂政策干预场景提供精确的因果推断工具。


<details>
  <summary>Details</summary>
Motivation: 传统双重差分法在面对观测面板数据中的高维混淆变量时存在困难，而机器学习方法虽然能处理高维数据但缺乏因果结构的可解释性。为解决这一核心矛盾，需要开发一种既能保持因果推断结构又能处理高维数据的创新框架。

Method: 提出S-DIDML框架，结合结构识别与高维估计。采用结构化残差正交化技术（Neyman正交性+交叉拟合）保持群组-时间处理效应识别结构，设计结合因果森林和半参数模型的动态异质性估计模块来捕捉时空异质性效应，建立完整的模块化应用流程和标准化Stata实现路径。

Result: S-DIDML框架成功解决了高维协变量干扰问题，同时保持了ATT识别结构，能够捕捉时空异质性效应。该框架实现了从方法堆叠到架构集成的转变，为社会科学提供了精确识别政策敏感群体和优化资源配置的工具。

Conclusion: S-DIDML框架丰富了DID和DDML创新的方法论研究，推进了因果推断从方法堆叠向架构集成的发展。该框架为数字化转型政策、环境规制等复杂干预场景提供了可复制的评估工具、决策优化参考和方法论范式，使社会科学能够精确识别政策敏感群体并优化资源配置。

Abstract: Causal inference in observational panel data has become a central concern in
economics,policy analysis,and the broader social sciences.To address the core
contradiction where traditional difference-in-differences (DID) struggles with
high-dimensional confounding variables in observational panel data,while
machine learning (ML) lacks causal structure interpretability,this paper
proposes an innovative framework called S-DIDML that integrates structural
identification with high-dimensional estimation.Building upon the structure of
traditional DID methods,S-DIDML employs structured residual orthogonalization
techniques (Neyman orthogonality+cross-fitting) to retain the group-time
treatment effect (ATT) identification structure while resolving
high-dimensional covariate interference issues.It designs a dynamic
heterogeneity estimation module combining causal forests and semi-parametric
models to capture spatiotemporal heterogeneity effects.The framework
establishes a complete modular application process with standardized Stata
implementation paths.The introduction of S-DIDML enriches methodological
research on DID and DDML innovations, shifting causal inference from method
stacking to architecture integration.This advancement enables social sciences
to precisely identify policy-sensitive groups and optimize resource
allocation.The framework provides replicable evaluation tools, decision
optimization references,and methodological paradigms for complex intervention
scenarios such as digital transformation policies and environmental
regulations.

</details>


### [64] [Generative AI Models for Learning Flow Maps of Stochastic Dynamical Systems in Bounded Domains](https://arxiv.org/abs/2507.15990)
*Minglei Yang,Yanfang Liu,Diego del-Castillo-Negrete,Yanzhao Cao,Guannan Zhang*

Main category: stat.ML

TL;DR: 本文提出了一种混合数据驱动方法，结合条件扩散模型和退出预测神经网络来模拟有界域中的随机微分方程，解决了现有机器学习方法无法准确捕获粒子退出动态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习方法在学习随机微分方程方面取得了成功，但这些方法不适用于有界域中的随机微分方程，因为它们无法准确捕获粒子退出动态。有界域中的随机微分方程模拟面临重大计算挑战，需要准确建模内部随机动力学和边界相互作用。

Method: 提出了一种统一的混合数据驱动方法，包含两个主要组件：1）使用二元交叉熵损失学习退出概率的神经网络，具有严格的收敛保证；2）无需训练的扩散模型，使用闭式评分函数为非退出粒子生成状态转换。通过概率采样算法将两个组件集成，在每个时间步确定粒子退出并生成适当的状态转换。

Result: 通过三个测试案例验证了方法的性能：一维简化问题用于理论验证、有界域中的二维平流扩散问题，以及磁约束聚变等离子体相关的三维问题。这些测试案例证明了所提方法在处理有界域随机微分方程方面的有效性。

Conclusion: 该方法成功解决了有界域中随机微分方程模拟的关键挑战，通过结合退出预测和条件扩散建模，能够准确捕获粒子退出现象和内部随机动力学，为复杂物理系统的随机建模提供了新的解决方案。

Abstract: Simulating stochastic differential equations (SDEs) in bounded domains,
presents significant computational challenges due to particle exit phenomena,
which requires accurate modeling of interior stochastic dynamics and boundary
interactions. Despite the success of machine learning-based methods in learning
SDEs, existing learning methods are not applicable to SDEs in bounded domains
because they cannot accurately capture the particle exit dynamics. We present a
unified hybrid data-driven approach that combines a conditional diffusion model
with an exit prediction neural network to capture both interior stochastic
dynamics and boundary exit phenomena. Our ML model consists of two major
components: a neural network that learns exit probabilities using binary
cross-entropy loss with rigorous convergence guarantees, and a training-free
diffusion model that generates state transitions for non-exiting particles
using closed-form score functions. The two components are integrated through a
probabilistic sampling algorithm that determines particle exit at each time
step and generates appropriate state transitions. The performance of the
proposed approach is demonstrated via three test cases: a one-dimensional
simplified problem for theoretical verification, a two-dimensional
advection-diffusion problem in a bounded domain, and a three-dimensional
problem of interest to magnetically confined fusion plasmas.

</details>


### [65] [PAC Off-Policy Prediction of Contextual Bandits](https://arxiv.org/abs/2507.16236)
*Yilong Wan,Yuqiang Li,Xianyi Wu*

Main category: stat.ML

TL;DR: 本文提出了一种基于保形预测的上下文老虎机离线策略评估方法，能够构建概率近似正确(PAC)的预测区间，在有限样本下保证边际覆盖率，特别适用于安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文老虎机离线策略评估方法难以在给定离线数据集上实现条件覆盖保证，特别是在安全关键应用中需要可靠的预测区间来量化目标策略的性能。

Method: 提出了一种基于PAC有效保形预测框架的新算法，通过建立PAC类型的覆盖率界限来加强理论保证，构建概率近似正确的预测区间。

Result: 通过理论分析证明了所提方法的有限样本和渐近性质，并在仿真实验中与现有方法进行了经验性能比较，验证了方法的有效性。

Conclusion: 所提出的PAC有效保形预测方法能够为上下文老虎机的离线策略评估提供更强的理论保证和实用性，特别适合需要高可靠性的安全关键应用场景。

Abstract: This paper investigates off-policy evaluation in contextual bandits, aiming
to quantify the performance of a target policy using data collected under a
different and potentially unknown behavior policy. Recently, methods based on
conformal prediction have been developed to construct reliable prediction
intervals that guarantee marginal coverage in finite samples, making them
particularly suited for safety-critical applications. To further achieve
coverage conditional on a given offline data set, we propose a novel algorithm
that constructs probably approximately correct prediction intervals. Our method
builds upon a PAC-valid conformal prediction framework, and we strengthen its
theoretical guarantees by establishing PAC-type bounds on coverage. We analyze
both finite-sample and asymptotic properties of the proposed method, and
compare its empirical performance with existing methods in simulations.

</details>


### [66] [Estimating Treatment Effects with Independent Component Analysis](https://arxiv.org/abs/2507.16467)
*Patrik Reizinger,Lester Mackey,Wieland Brendel,Rahul Krishnan*

Main category: stat.ML

TL;DR: 该研究首次建立了因果推断与独立成分分析(ICA)之间的理论联系，证明了线性ICA可以在部分线性回归模型中准确估计多个处理效应，即使存在高斯混杂因子或非线性干扰。


<details>
  <summary>Details</summary>
Motivation: 因果推断和可识别性理论两个领域虽然独立发展，但目标相似：准确且样本高效地估计模型参数。在部分线性回归设置中，非高斯处理噪声可以改善估计一致性，而非高斯性也是ICA识别潜在因子的关键假设。因此需要探索这两个领域之间的连接。

Method: 将独立成分分析(ICA)方法应用于部分线性回归(PLR)模型中的因果效应估计。利用非高斯性假设这一共同点，探索线性ICA在存在高斯混杂因子或非线性干扰情况下估计多个处理效应的能力。

Result: 线性ICA能够在部分线性回归模型中准确估计多个处理效应，即使在存在高斯混杂因子或非线性干扰的情况下也能保持良好性能。这为因果推断提供了新的方法论工具。

Conclusion: 首次从理论和实证角度揭示了因果推断与ICA之间的深层连接，证明了ICA可以有效用于因果效应估计，为两个研究领域的交叉融合提供了新的视角和方法。

Abstract: The field of causal inference has developed a variety of methods to
accurately estimate treatment effects in the presence of nuisance. Meanwhile,
the field of identifiability theory has developed methods like Independent
Component Analysis (ICA) to identify latent sources and mixing weights from
data. While these two research communities have developed largely
independently, they aim to achieve similar goals: the accurate and
sample-efficient estimation of model parameters. In the partially linear
regression (PLR) setting, Mackey et al. (2018) recently found that estimation
consistency can be improved with non-Gaussian treatment noise. Non-Gaussianity
is also a crucial assumption for identifying latent factors in ICA. We provide
the first theoretical and empirical insights into this connection, showing that
ICA can be used for causal effect estimation in the PLR model. Surprisingly, we
find that linear ICA can accurately estimate multiple treatment effects even in
the presence of Gaussian confounders or nonlinear nuisance.

</details>


### [67] [Structural Effect and Spectral Enhancement of High-Dimensional Regularized Linear Discriminant Analysis](https://arxiv.org/abs/2507.16682)
*Yonghan Zhang,Zhangni Pu,Lu Yan,Jiang Hu*

Main category: stat.ML

TL;DR: 本文提出了谱增强判别分析(SEDA)算法，通过调整总体协方差矩阵的尖峰特征值来优化数据结构，在高维场景下实现了比传统线性判别分析更好的分类和降维性能。


<details>
  <summary>Details</summary>
Motivation: 正则化线性判别分析(RLDA)在高维场景下性能不一致，现有理论分析缺乏对数据结构如何影响分类性能的清晰洞察，需要更好地理解和改进RLDA在高维数据上的表现。

Method: 推导了误分类率的非渐近近似，分析了RLDA的结构效应和结构调整策略；提出谱增强判别分析(SEDA)算法，通过调整总体协方差矩阵的尖峰特征值来优化数据结构；基于随机矩阵理论中特征向量的新理论结果，推导了SEDA误分类率的渐近近似；开发了偏差校正算法和参数选择策略。

Result: 在合成数据集和真实数据集上的实验表明，SEDA相比现有的LDA方法在分类准确率和降维效果方面都有显著提升。

Conclusion: 通过理论分析和算法创新，SEDA成功解决了传统RLDA在高维场景下性能不稳定的问题，为高维数据的分类和降维提供了更有效的解决方案。

Abstract: Regularized linear discriminant analysis (RLDA) is a widely used tool for
classification and dimensionality reduction, but its performance in
high-dimensional scenarios is inconsistent. Existing theoretical analyses of
RLDA often lack clear insight into how data structure affects classification
performance. To address this issue, we derive a non-asymptotic approximation of
the misclassification rate and thus analyze the structural effect and
structural adjustment strategies of RLDA. Based on this, we propose the
Spectral Enhanced Discriminant Analysis (SEDA) algorithm, which optimizes the
data structure by adjusting the spiked eigenvalues of the population covariance
matrix. By developing a new theoretical result on eigenvectors in random matrix
theory, we derive an asymptotic approximation on the misclassification rate of
SEDA. The bias correction algorithm and parameter selection strategy are then
obtained. Experiments on synthetic and real datasets show that SEDA achieves
higher classification accuracy and dimensionality reduction compared to
existing LDA methods.

</details>
