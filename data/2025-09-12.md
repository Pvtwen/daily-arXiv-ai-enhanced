<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [cs.LG](#cs.LG) [Total: 53]
- [stat.ML](#stat.ML) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Masked Representation Learning to Model Cardiac Functions Using Multiple Physiological Signals](https://arxiv.org/abs/2509.08830)
*Seong-A Park,Jong-Eui Chae,Sungdong Kim,Hyung-Chul Lee,Hyun-Lim Yang*

Main category: eess.SP

TL;DR: SNUPHY-M模型通过自监督学习同时分析ECG、PPG和ABP三种生理信号，在血流动力学监测任务中显著优于传统方法，实现了无创精准诊断。


<details>
  <summary>Details</summary>
Motivation: 临床需要综合分析多种生理信号来监测血流动力学，但现有研究多关注单一信号分析，缺乏适用于真实临床场景的复杂信号分析方法。

Method: 基于自监督学习的掩码表示学习方法，通过恢复三种被掩码的生理信号（ECG、PPG、ABP）来提取反映心脏周期电学、压力和流体特性的生理特征。

Result: 在低血压、心搏量、收缩压、舒张压和年龄预测等临床下游任务中，SNUPHY-M显著优于监督学习或自监督学习模型，特别是在使用非侵入性信号的预测任务中表现突出。

Conclusion: SNUPHY-M是首个将多模态自监督学习应用于ECG、PPG和ABP信号心血管分析的模型，有效支持临床决策，实现无创早期诊断和血流动力学管理。

Abstract: In clinical settings, monitoring hemodynamics is crucial for managing patient
prognosis, necessitating the integrated analysis of multiple physiological
signals. While recent research has analyzed single signals such as
electrocardiography (ECG) or photoplethysmography (PPG), there has yet to be a
proposal for an approach that encompasses the complex signal analysis required
in actual clinical scenarios. In this study, we introduce the SNUPHY-M (Seoul
National University hospital PHYsiological signal Masked representation
learning) model extracts physiological features reflecting the electrical,
pressure, and fluid characteristics of the cardiac cycle in the process of
restoring three masked physiological signals based on self-supervised learning
(SSL): ECG, PPG, and arterial blood pressure (ABP) signals. By employing
multiple physical characteristics, the model can extract more enriched features
only using non-invasive signals. We evaluated the model's performance in
clinical downstream tasks such as hypotension, stroke volume, systolic blood
pressure, diastolic blood pressure, and age prediction. Our results showed that
the SNUPHY-M significantly outperformed supervised or SSL models, especially in
prediction tasks using non-invasive signals. To the best of our knowledge,
SNUPHY-M is the first model to apply multi-modal SSL to cardiovascular analysis
involving ECG, PPG, and ABP signals. This approach effectively supports
clinical decision-making and enables precise diagnostics, contributing
significantly to the early diagnosis and management of hemodynamics without
invasiveness.

</details>


### [2] [Deploying AI for Signal Processing education: Selected challenges and intriguing opportunities](https://arxiv.org/abs/2509.08950)
*Jarvis Haupt,Qin Lu,Yanning Shen,Jia Chen,Yue Dong,Dan McCreary,Mehmet Akçakaya,Georgios B. Giannakis*

Main category: eess.SP

TL;DR: 本文探讨AI在教育领域的应用，特别是信号处理教育，关注技术局限性和实践应用，包括公平性、包容性、幻觉输出处理等核心问题，并通过智能教科书开发进行说明。


<details>
  <summary>Details</summary>
Motivation: AI技术虽取得重大突破，但如何公平负责任地使用AI改善全球人类状况仍面临挑战，特别是在教育领域需要解决技术和社会问题。

Method: 提供AI教育应用的技术入门指南，包括确保公平包容性、处理幻觉输出、资源高效使用等方法，并通过开发沉浸式结构化智能教科书进行实践说明。

Result: 建立了AI在教育中应用的技术框架，解决了公平性、透明性、可解释性等关键问题，为工程教育提供了可靠的AI工具应用方案。

Conclusion: AI在教育领域具有巨大潜力，但需要综合考虑技术和社会因素，本文为研究者和教育者提供了推进AI在工程教育中作用的实用资源。

Abstract: Powerful artificial intelligence (AI) tools that have emerged in recent years
-- including large language models, automated coding assistants, and advanced
image and speech generation technologies -- are the result of monumental human
achievements. These breakthroughs reflect mastery across multiple technical
disciplines and the resolution of significant technological challenges.
However, some of the most profound challenges may still lie ahead. These
challenges are not purely technical but pertain to the fair and responsible use
of AI in ways that genuinely improve the global human condition. This article
explores one promising application aligned with that vision: the use of AI
tools to facilitate and enhance education, with a specific focus on signal
processing (SP). It presents two interrelated perspectives: identifying and
addressing technical limitations, and applying AI tools in practice to improve
educational experiences. Primers are provided on several core technical issues
that arise when using AI in educational settings, including how to ensure
fairness and inclusivity, handle hallucinated outputs, and achieve efficient
use of resources. These and other considerations -- such as transparency,
explainability, and trustworthiness -- are illustrated through the development
of an immersive, structured, and reliable "smart textbook." The article serves
as a resource for researchers and educators seeking to advance AI's role in
engineering education.

</details>


### [3] [Ultrafast Deep Learning-Based Scatter Estimation in Cone-Beam Computed Tomography](https://arxiv.org/abs/2509.08973)
*Harshit Agrawal,Ari Hietanen,Simo Särkkä*

Main category: eess.SP

TL;DR: 该研究通过在不同分辨率下应用网络并基于速度和准确性选择最优分辨率，显著降低了锥束CT散射校正的深度学习网络计算复杂度和内存需求。


<details>
  <summary>Details</summary>
Motivation: 锥束CT扫描中的散射伪影严重降低图像质量，虽然基于深度学习的方法在估计散射方面显示出潜力，但由于网络内存占用大，在移动CBCT系统或边缘设备上的部署仍然受限。

Method: 首先在六个分辨率下比较四种插值方法的重建误差，然后在五个图像分辨率上训练最新的最先进方法，评估FLOPs、推理时间和GPU内存需求的减少。

Result: 与基线方法相比，FLOPs减少了78倍，同时保持可比的性能（MAPE从4.42%降至3.85%，MSE从2.01×10⁻²降至1.34×10⁻²），推理时间和GPU内存使用分别减少了16倍和12倍。

Conclusion: 该研究强调了降采样在基于深度学习的散射估计中被低估的作用，通过大幅减少FLOPs和GPU内存需求，使散射校正在资源受限环境（如移动CBCT和边缘设备）中成为可能。

Abstract: Purpose: Scatter artifacts drastically degrade the image quality of cone-beam
computed tomography (CBCT) scans. Although deep learning-based methods show
promise in estimating scatter from CBCT measurements, their deployment in
mobile CBCT systems or edge devices is still limited due to the large memory
footprint of the networks. This study addresses the issue by applying networks
at varying resolutions and suggesting an optimal one, based on speed and
accuracy.
  Methods: First, the reconstruction error in down-up sampling of CBCT scatter
signal was examined at six resolutions by comparing four interpolation methods.
Next, a recent state-of-the-art method was trained across five image
resolutions and evaluated for the reductions in floating-point operations
(FLOPs), inference times, and GPU memory requirements.
  Results: Reducing the input size and network parameters achieved a 78-fold
reduction in FLOPs compared to the baseline method, while maintaining comarable
performance in terms of mean-absolute-percentage-error (MAPE) and
mean-square-error (MSE). Specifically, the MAPE decreased to 3.85% compared to
4.42%, and the MSE decreased to 1.34 \times 10^{-2} compared to 2.01 \times
10^{-2}. Inference time and GPU memory usage were reduced by factors of 16 and
12, respectively. Further experiments comparing scatter-corrected
reconstructions on a large, simulated dataset and real CBCT scans from water
and Sedentex CT phantoms clearly demonstrated the robustness of our method.
  Conclusion: This study highlights the underappreciated role of downsampling
in deep learning-based scatter estimation. The substantial reduction in FLOPs
and GPU memory requirements achieved by our method enables scatter correction
in resource-constrained environments, such as mobile CBCT and edge devices.

</details>


### [4] [6G Resilience -- White Paper](https://arxiv.org/abs/2509.09005)
*Hirley Alves,Nurul H. Mahmood,Onel L. A. López,Sumudu Samarakoon,Seppo Yrjölä,Matti Latva-Aho,Markku Juntti,Ari Pouttu,Armin Dekorsy,Arthur Sousa de Sena,Aydin Sezgin,Bho Matthiesen,Chafika Benzaid,Chathuranga Weeraddana,David Hutchison,Dileepa Marasinghe,Doganalp Ergenc,Eduard Jorswieck,Erkki Harjula,Falko Dressler,Harri Saarnisaari,Italo Atzeni,Jaap Van De Beek,Jacek Rak,Konstantin Mikhaylov,Lauri Loven,Madhusanka Liyanage,Marcos Katz,Marja Matinmikko-Blue,Mehdi Rasti,Mika Ylianttila Nhan Nguyen,Pawani Porambage,Petar Popovski,Petri Ahokangas,Premanandana Rajatheva,Robert-Jeron Reifert,Tharaka Hewa,Tommy Svensson*

Main category: eess.SP

TL;DR: 6G网络设计需要将弹性作为核心设计目标，通过3R框架（可靠性、鲁棒性、弹性）和可衡量的能力来实现对复杂中断的适应和恢复。


<details>
  <summary>Details</summary>
Motivation: 移动网络从效率优先转向可持续性意识，需要设计能够承受、适应和演进复杂长期中断的6G网络，将弹性提升为与可持续性和效率同等重要的设计目标。

Method: 采用3R框架（可靠性、鲁棒性、弹性）形式化弹性概念，转化为可衡量的能力：优雅降级、态势感知、快速重配置、学习驱动的改进和恢复。架构上采用边缘原生和本地感知设计、开放接口和可编程性。

Result: 提出了包括AI原生控制循环、零信任安全、关键流量优先等关键技术使能器，以及9个商业模式组和治理标准化框架，为6G弹性发展提供了完整的技术和经济解决方案。

Conclusion: 本白皮书作为6G弹性发展的初步步骤和催化剂，旨在为研究人员、专业人士、政府官员和公众提供理解和塑造6G弹性发展的基本组件，推动6G网络在面对复杂中断时具备更强的适应和恢复能力。

Abstract: 6G must be designed to withstand, adapt to, and evolve amid prolonged,
complex disruptions. Mobile networks' shift from efficiency-first to
sustainability-aware has motivated this white paper to assert that resilience
is a primary design goal, alongside sustainability and efficiency, encompassing
technology, architecture, and economics. We promote resilience by analysing
dependencies between mobile networks and other critical systems, such as
energy, transport, and emergency services, and illustrate how cascading
failures spread through infrastructures. We formalise resilience using the 3R
framework: reliability, robustness, resilience. Subsequently, we translate this
into measurable capabilities: graceful degradation, situational awareness,
rapid reconfiguration, and learning-driven improvement and recovery.
  Architecturally, we promote edge-native and locality-aware designs, open
interfaces, and programmability to enable islanded operations, fallback modes,
and multi-layer diversity (radio, compute, energy, timing). Key enablers
include AI-native control loops with verifiable behaviour, zero-trust security
rooted in hardware and supply-chain integrity, and networking techniques that
prioritise critical traffic, time-sensitive flows, and inter-domain
coordination.
  Resilience also has a techno-economic aspect: open platforms and high-quality
complementors generate ecosystem externalities that enhance resilience while
opening new markets. We identify nine business-model groups and several
patterns aligned with the 3R objectives, and we outline governance and
standardisation. This white paper serves as an initial step and catalyst for 6G
resilience. It aims to inspire researchers, professionals, government
officials, and the public, providing them with the essential components to
understand and shape the development of 6G resilience.

</details>


### [5] [Personalized Sleep Prediction via Deep Adaptive Spatiotemporal Modeling and Sparse Data](https://arxiv.org/abs/2509.09018)
*Xueyi Wang,C. J. C.,Lamoth,Elisabeth Wilhelm*

Main category: eess.SP

TL;DR: 提出了AdaST-Sleep模型，结合卷积层和循环神经网络层，通过空间特征交互和时间序列处理来预测睡眠评分，在多个时间窗口设置下均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 睡眠预测能够帮助个人和医疗提供者预测并主动解决影响良好休息的因素，从而改善身心健康。需要开发能够处理稀疏可穿戴设备数据的个性化睡眠预测模型。

Method: 提出自适应时空模型(AdaST-Sleep)，使用卷积层捕获多特征间的空间交互，循环神经网络层处理长期时间序列健康数据，并集成域分类器以实现跨受试者的泛化。

Result: 在5种输入窗口大小(3-11天)和5种预测窗口大小(1-9天)的实验中都优于4个基线模型，最低RMSE为0.282(7天输入窗口+1天预测窗口)。模型在多日预测中保持强劲性能，能准确跟踪总体睡眠评分水平和日常波动。

Conclusion: 该框架为使用商业可穿戴设备的稀疏数据和域适应技术提供了稳健且适应性强的个性化睡眠预测解决方案。

Abstract: A sleep forecast allows individuals and healthcare providers to anticipate
and proactively address factors influencing restful rest, ultimately improving
mental and physical well-being. This work presents an adaptive spatial and
temporal model (AdaST-Sleep) for predicting sleep scores. Our proposed model
combines convolutional layers to capture spatial feature interactions between
multiple features and recurrent neural network layers to handle longer-term
temporal health-related data. A domain classifier is further integrated to
generalize across different subjects. We conducted several experiments using
five input window sizes (3, 5, 7, 9, 11 days) and five predicting window sizes
(1, 3, 5, 7, 9 days). Our approach consistently outperformed four baseline
models, achieving its lowest RMSE (0.282) with a seven-day input window and a
one-day predicting window. Moreover, the method maintained strong performance
even when forecasting multiple days into the future, demonstrating its
versatility for real-world applications. Visual comparisons reveal that the
model accurately tracks both the overall sleep score level and daily
fluctuations. These findings prove that the proposed framework provides a
robust and adaptable solution for personalized sleep forecasting using sparse
data from commercial wearable devices and domain adaptation techniques.

</details>


### [6] [Improving the Elevational Focusing of Fast Orthogonal Row-Column Electronic Scanning (FORCES) Ultrasound Imaging using Retrospective Transmit Beamforming (RTB)](https://arxiv.org/abs/2509.09056)
*Michael Caulfield,Randy Palamar,Darren Dahunsi,Mohammad Rahim Sobhani,Negar Majidi,Roger Zemp*

Main category: eess.SP

TL;DR: 本研究提出了一种改进的FORCES成像方案，通过应用回顾性发射波束成形(RTB)技术来改善Row Column Arrays在垂直方向上的聚焦性能。


<details>
  <summary>Details</summary>
Motivation: 传统的FORCES成像方案由于固定的垂直聚焦和大发射孔径，在焦点之外的垂直聚焦效果较差，限制了成像质量。

Method: 在FORCES和uFORCES方法中应用回顾性发射波束成形(RTB)技术，实现整个成像平面内的垂直发射聚焦。

Result: 实验显示，应用RTB后，FORCES和uFORCES在焦点之外的垂直聚焦能力得到显著改善，在焦点处的性能保持相当或更好。通过线模体测量FWHM和管状囊肿模体测量gCNR进行量化验证。

Conclusion: RTB技术有效提升了Row Column Arrays的垂直聚焦性能，为容积成像提供了更好的解决方案。

Abstract: Recent developments in Row Column Arrays (RCAs) have presented promising
options for volumetric imaging without the need for the excessive channel
counts of fully wired 2D-arrays. Bias programmable RCAs, also known as Top
Orthogonal to Bottom Electrode (TOBE) Arrays, show further promise in that
imaging schemes, such as Fast Orthogonal Row-Column Electronic Scanning
(FORCES) allow for full transmit and receive focusing everywhere in the image
plane. However, due to its fixed elevational focus and large transmit aperture,
FORCES experiences poor elevational focusing away from the focal point. In this
study we present a modification to the FORCES imaging scheme by applying
Retrospective Transmit Beamforming (RTB) in the elevational direction to allow
for elevational transmit focusing everywhere in the imaging plane. We evaluate
FORCES and uFORCES methods, with and without RTB applied, when imaging both a
cyst and wire phantom. With experiment we show improved elevational focusing
capabilities away from the focal point when RTB is applied to both FORCES and
uFORCES. At the focal point, performance with RTB remains comparable or
improved relative to standard FORCES. This is quantified by the measurement of
Full Width Half Max when imaging the wire phantom, and by the generalized
Contrast to Noise Ratio when imaging the tubular cyst phantom. We also
demonstrate the volumetric imaging capabilities of FORCES RTB with the wire
phantom.

</details>


### [7] [Signed Graph Learning with Hidden Nodes](https://arxiv.org/abs/2509.09120)
*Rong Ye,Xue-Qin Jiang,Hui Feng,Jian Wang,Runhe Qiu*

Main category: eess.SP

TL;DR: 提出了一种针对带隐藏节点的符号图学习方法SGL-HNCS，通过列稀疏正则化解决部分节点不可观测的符号图识别问题


<details>
  <summary>Details</summary>
Motivation: 现有符号图学习方法通常假设所有节点都可用，但在实际应用中往往只有部分节点可观测，其余节点保持隐藏状态，需要解决这种部分观测场景下的符号图学习问题

Method: 基于符号图信号平滑性假设，将符号图拓扑推断构建为带列稀疏正则化的约束优化问题，使用定制的块坐标下降(BCD)方法求解，重构符号图拉普拉斯矩阵同时考虑隐藏节点影响

Result: 在合成数据和真实数据上的实验结果表明，所提出的SGL-HNCS方法具有高效性

Conclusion: 该方法成功解决了带隐藏节点的符号图学习问题，为部分观测场景下的符号图处理提供了有效解决方案

Abstract: Signed graphs, which are characterized by both positive and negative edge
weights, have recently attracted significant attention in the field of graph
signal processing (GSP). Existing works on signed graph learning typically
assume that all graph nodes are available. However, in some specific
applications, only a subset of nodes can be observed while the remaining nodes
stay hidden. To address this challenge, we propose a novel method for
identifying signed graph that accounts for hidden nodes, termed \textit{signed
graph learning with hidden nodes under column-sparsity regularization}
(SGL-HNCS). Our method is based on the assumption that graph signals are smooth
over signed graphs, i.e., signal values of two nodes connected by positive
(negative) edges are similar (dissimilar). Rooted in this prior assumption, the
topology inference of a signed graph is formulated as a constrained
optimization problem with column-sparsity regularization, where the goal is to
reconstruct the signed graph Laplacian matrix without disregarding the
influence of hidden nodes. We solve the constrained optimization problem using
a tailored block coordinate descent (BCD) approach. Experimental results using
synthetic data and real-world data demonstrate the efficiency of the proposed
SGL-HNCS method.

</details>


### [8] [Sequential Spectral Clustering of Data Sequences](https://arxiv.org/abs/2509.09144)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: eess.SP

TL;DR: 本文提出了两种序列谱聚类算法SEQ-SPEC和IA-SEQ-SPEC，用于数据序列的非参数聚类，通过顺序采样方式在有限时间内实现指数一致性，相比固定样本量方法和其他序列聚类算法表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决数据序列的非参数聚类问题，其中每个序列包含从未知分布中生成的i.i.d.样本。由于真实分布未知，目标是在给定错误概率下通过观察最少样本量来估计聚类。

Method: 提出了SEQ-SPEC算法和计算效率更高的IA-SEQ-SPEC算法，采用顺序采样框架，在有限时间内停止并保证指数一致性。

Result: 模拟实验表明，两种算法性能优于固定样本量SPEC、SEQ-KMED和SEQ-SLINK算法。IA-SEQ-SPEC在保持计算效率的同时，性能接近SEQ-SPEC。

Conclusion: 这是首个在序列框架下进行数据序列谱聚类的工作，提出的算法在合成和真实数据集上都表现出色，为序列聚类提供了有效解决方案。

Abstract: We study the problem of nonparametric clustering of data sequences, where
each data sequence comprises i.i.d. samples generated from an unknown
distribution. The true clusters are the clusters obtained using the Spectral
clustering algorithm (SPEC) on the pairwise distance between the true
distributions corresponding to the data sequences. Since the true distributions
are unknown, the objective is to estimate the clusters by observing the minimum
number of samples from the data sequences for a given error probability. To
solve this problem, we propose the Sequential Spectral clustering algorithm
(SEQ-SPEC), and show that it stops in finite time almost surely and is
exponentially consistent. We also propose a computationally more efficient
algorithm called the Incremental Approximate Sequential Spectral clustering
algorithm (IA-SEQ-SPEC). Through simulations, we show that both our proposed
algorithms perform better than the fixed sample size SPEC, the Sequential
$K$-Medoids clustering algorithm (SEQ-KMED) and the Sequential Single Linkage
clustering algorithm (SEQ-SLINK). The IA-SEQ-SPEC, while being computationally
efficient, performs close to SEQ-SPEC on both synthetic and real-world
datasets. To the best of our knowledge, this is the first work on spectral
clustering of data sequences under a sequential framework.

</details>


### [9] [JFRFFNet: A Data-Model Co-Driven Graph Signal Denoising Model with Partial Prior Information](https://arxiv.org/abs/2509.09147)
*Ziqi Yan,Zhichao Zhang*

Main category: eess.SP

TL;DR: 提出了一种数据-模型协同驱动的去噪方法JFRFFNet，将联合时-顶点分数傅里叶变换域的维纳滤波器嵌入神经网络，通过数据驱动方式更新变换阶数对和滤波器系数，仅需部分先验信息即可实现有效去噪。


<details>
  <summary>Details</summary>
Motivation: 传统滤波模型需要完整图信号先验信息，要么使用网格搜索确定变换阶数对和计算滤波器系数，要么采用梯度下降策略优化，限制了实际应用。

Method: 将JFRFT域维纳滤波器模型嵌入神经网络架构，通过数据驱动方法联合优化变换阶数对和滤波器系数，仅需部分先验信息。

Result: 实验表明JFRFFNet在输出信噪比方面相比现有最先进方法有显著提升。

Conclusion: 提出的数据-模型协同驱动方法成功克服了传统方法需要完整先验信息的限制，实现了更有效的时变图信号去噪。

Abstract: Wiener filtering in the joint time-vertex fractional Fourier transform
(JFRFT) domain has shown high effectiveness in denoising time-varying graph
signals. Traditional filtering models use grid search to determine the
transform-order pair and compute filter coefficients, while learnable ones
employ gradient-descent strategies to optimize them; both require complete
prior information of graph signals. To overcome this shortcoming, this letter
proposes a data-model co-driven denoising approach, termed neural-network-aided
joint time-vertex fractional Fourier filtering (JFRFFNet), which embeds the
JFRFT-domain Wiener filter model into a neural network and updates the
transform-order pair and filter coefficients through a data-driven approach.
This design enables effective denoising using only partial prior information.
Experiments demonstrate that JFRFFNet achieves significant improvements in
output signal-to-noise ratio compared with some state-of-the-art methods.

</details>


### [10] [On Sampling of Multiple Correlated Stochastic Signals](https://arxiv.org/abs/2509.09225)
*Lin Jin,Hang Sheng,Hui Feng,Bo Hu*

Main category: eess.SP

TL;DR: 该论文提出了一种利用多通道信号统计相关性的高效采样方法，通过将相关通道建模为少量不相关潜在源的线性组合，建立了零均方误差重建的理论采样密度下界，并开发了达到该下界的多频带采样方案。


<details>
  <summary>Details</summary>
Motivation: 传统采样方法独立处理每个通道导致数据冗余，而多通道随机信号存在固有统计相关性，需要利用这种相关性来提高采样效率。

Method: 将相关通道建模为少量不相关宽平稳潜在源的线性组合，通过潜在源的频谱分区，然后进行时空采样和插值，构建多频带采样方案。

Result: 实验证明该方法在理论采样密度下实现了近乎无损的重建，验证了其效率。

Conclusion: 提出的采样方案能够有效利用信号相关性，达到理论最优采样效率，为多通道信号处理提供了高效采样框架。

Abstract: Multiple stochastic signals possess inherent statistical correlations, yet
conventional sampling methods that process each channel independently result in
data redundancy. To leverage this correlation for efficient sampling, we model
correlated channels as a linear combination of a smaller set of uncorrelated,
wide-sense stationary latent sources. We establish a theoretical lower bound on
the total sampling density for zero mean-square error reconstruction, proving
it equals the ratio of the joint spectral bandwidth of latent sources to the
number of correlated signal channels. We then develop a constructive multi-band
sampling scheme that attains this bound. The proposed method operates via
spectral partitioning of the latent sources, followed by spatio-temporal
sampling and interpolation. Experiments on synthetic and real datasets confirm
that our scheme achieves near-lossless reconstruction precisely at the
theoretical sampling density, validating its efficiency.

</details>


### [11] [Improved Riemannian potato field: an Automatic Artifact Rejection Method for EEG](https://arxiv.org/abs/2509.09264)
*Davoud Hajhassani,Quentin Barthélemy,Jérémie Mattout,Marco Congedo*

Main category: eess.SP

TL;DR: 提出了改进的黎曼土豆场(iRPF)方法，用于EEG信号伪迹自动去除，在多个指标上显著优于现有方法，处理速度快，适合大规模EEG数据处理。


<details>
  <summary>Details</summary>
Motivation: EEG信号清洗是研究领域的关键挑战，现有方法依赖人工超参数调优、对异常值敏感且计算成本高，而人工视觉检查耗时且主观。

Method: 开发了改进的黎曼土豆场(iRPF)方法，这是一种快速全自动的EEG伪迹拒绝方法，解决了当前方法的关键局限性。

Result: iRPF在召回率、特异性、精确度和F1分数上分别比现有方法提升高达22%、102%、54%和24%，统计显著性p<0.001，处理速度每epoch小于8毫秒。

Conclusion: iRPF为脑机接口和临床神经影像应用提供了强大且数据驱动的伪迹拒绝解决方案，适用于高质量EEG预处理。

Abstract: Electroencephalography (EEG) signal cleaning has long been a critical
challenge in the research community. The presence of artifacts can
significantly degrade EEG data quality, complicating analysis and potentially
leading to erroneous interpretations. While various artifact rejection methods
have been proposed, the gold standard remains manual visual inspection by human
experts-a process that is time-consuming, subjective, and impractical for
large-scale EEG studies. Existing techniques are often hindered by a strong
reliance on manual hyperparameter tuning, sensitivity to outliers, and high
computational costs. In this paper, we introduce the improved Riemannian Potato
Field (iRPF), a fast and fully automated method for EEG artifact rejection that
addresses key limitations of current approaches. We evaluate iRPF against
several state-of-the-art artifact rejection methods, using two publicly
available EEG databases, labeled for various artifact types, comprising 226 EEG
recordings. Our results demonstrate that iRPF outperforms all competitors
across multiple metrics, with gains of up to 22% in recall, 102% in
specificity, 54% in precision, and 24% in F1-score, compared to Isolation
Forest, Autoreject, Riemannian Potato, and Riemannian Potato Field,
respectively. Statistical analysis confirmed the significance of these
improvements (p < 0.001) with large effect sizes (Cohen's d > 0.8) in most
comparisons. Additionally, on a typical EEG recording iRPF performs artifact
cleaning in under 8 milliseconds per epoch using a standard laptop,
highlighting its efficiency for large-scale EEG data processing and real-time
applications. iRPF offers a robust and data-driven artifact rejection solution
for high-quality EEG pre-processing in brain-computer interfaces and clinical
neuroimaging applications.

</details>


### [12] [On the Relation of Characteristic Modes of Different Conducting Structures](https://arxiv.org/abs/2509.09282)
*Leonardo Mörlein,Dirk Manteuffel*

Main category: eess.SP

TL;DR: 提出了一种基于特征模式的散射分析形式化方法，通过超集结构的特征模式来分析子集结构的散射特性，并定义了模态变换矩阵来描述不同结构间的映射关系。


<details>
  <summary>Details</summary>
Motivation: 为了解决不同导电结构散射分析时缺乏统一基准的问题，希望通过一个共同的特征模式基础来分析和比较不同结构，特别是在天线设计过程中逐步修改结构时的分析需求。

Method: 推导了基于超集结构特征模式的散射分析形式化方法，定义了模态变换矩阵来描述特征场与权重系数之间的映射关系，实现了不同基底下扰动矩阵的转换。

Result: 建立了散射矩阵和扰动矩阵在非对角情况下的理论框架，通过两个示例验证了形式化方法的有效性，展示了其在天线设计过程中的应用价值。

Conclusion: 该形式化方法为不同导电结构的散射分析提供了统一的特征模式基础，特别适用于渐进式修改的天线设计过程，具有重要的理论和实用价值。

Abstract: A formalism is derived to analyze the scattering of a conducting structure
based on the characteristic modes of another structure whose surface is a
superset of the first structure. This enables the analysis and comparison of
different structures using a common basis of characteristic modes.
Additionally, it is shown that the scattering matrices and perturbation
matrices are no longer diagonal in these cases. Based on this, a modal
transformation matrix is defined to describe the mapping between the
characteristic fields and the weighting coefficients of the two structures.
This matrix enables the conversion of the perturbation matrices in different
bases. Finally, two examples are provided along with a discussion of some
aspects of the theory. The first example aims to validate and illustrate the
formalism. The second example shows how the formalism can be applied in the
design process of an antenna element that is gradually modified, starting from
a base structure.

</details>


### [13] [Channel Estimation and Analog Precoding for Pixel-based Fluid-Antenna-Assisted Multiuser MIMO-OFDM Systems](https://arxiv.org/abs/2509.09373)
*Huayan Guo,Jichen Zhang,Junhui Rao,Ross Murch,Vincent K. N. Lau*

Main category: eess.SP

TL;DR: 本文针对像素化流体天线的信道估计和模拟预编码挑战，提出了基于稀疏信道恢复和DNN天线辐射函数的框架，开发了两种低复杂度估计算法，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 像素化流体天线虽然提供了更好的复用增益和快速波束切换能力，但带来了状态不可分离信道响应的新问题，需要解决多用户MIMO-OFDM系统中的信道估计和模拟预编码挑战。

Method: 提出了稀疏信道恢复框架，使用近似可分离信道响应模型和DNN基天线辐射函数；开发了基于正交匹配追踪和变分贝叶斯推理的两种低复杂度信道估计算法；设计了最优切换状态选择的模拟预编码方案。

Result: 仿真结果表明，所提方法在多种散射簇角度下都能准确恢复信道响应，特别是在高信噪比和多用户场景下显著优于多个基线方法。

Conclusion: 该研究为像素化流体天线系统提供了有效的信道估计和预编码解决方案，证明了在实际原型测量基础上的技术可行性，为未来流体天线系统的实际部署奠定了基础。

Abstract: Pixel-based fluid antennas provide enhanced multiplexing gains and quicker
radiation pattern switching than traditional designs. However, this innovation
introduces challenges for channel estimation and analog precoding due to the
state-non-separable channel response problem. This paper explores a multiuser
MIMO-OFDM system utilizing pixel-based fluid antennas, informed by measurements
from a real-world prototype. We present a sparse channel recovery framework for
uplink channel sounding, employing an approximate separable channel response
model with DNN-based antenna radiation functions. We then propose two
low-complexity channel estimation algorithms that leverage orthogonal matching
pursuit and variational Bayesian inference to accurately recover channel
responses across various scattering cluster angles. These estimations enable
the prediction of composite channels for all fluid antenna states, leading to
an analog precoding scheme that optimally selects switching states for
different antennas. Our simulation results indicate that the proposed approach
significantly outperforms several baseline methods, especially in high
signal-to-noise ratio environments with numerous users.

</details>


### [14] [A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction in UAV-Assisted mmWave Radio Networks](https://arxiv.org/abs/2509.09606)
*Sajjad Hussain*

Main category: eess.SP

TL;DR: 提出基于UNet的深度学习架构，结合多尺度特征提取和ASPP瓶颈，用于无人机毫米波网络路径损耗预测，在准确性和效率上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在路径损耗预测中泛化能力不足、对噪声输入鲁棒性差以及对无人机高度敏感性等问题

Method: 使用UNet架构，结合多尺度特征提取、卷积特征融合和ASPP瓶颈进行上下文聚合，输入包括对数距离、LOS掩码和建筑掩码

Result: 在内部射线追踪数据和RadioMapSeer基准测试中，模型在准确性和效率方面均优于多个最先进的基线方法

Conclusion: 所提出的模型在路径损耗预测方面表现出色，源代码已公开以支持可重复性和未来研究

Abstract: Accurate pathloss prediction is essential for the design and optimization of
UAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches
have shown strong potential, their generalization across diverse environments,
robustness to noisy inputs, and sensitivity to UAV altitude remain
underexplored. To address these challenges, we propose a UNet-based deep
learning architecture that combines multi-scale feature extraction,
convolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)
bottleneck for efficient context aggregation. The model predicts pathloss maps
from log-distance, line-of-sight (LOS) mask, and building mask inputs. In
addition, we develop a fully vectorized LOS mask computation algorithm that
significantly accelerates pre-processing and enables large-scale dataset
generation. Extensive evaluations on both in-house ray-tracing data and the
RadioMapSeer benchmark demonstrate that the proposed model outperforms several
state-of-the-art baselines in accuracy and efficiency. All source code is
publicly released to support reproducibility and future research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 提出基于信噪比的直观不确定性估计与分解框架，使用方差门控置信因子缩放预测，讨论委员会机器多样性崩溃问题


<details>
  <summary>Details</summary>
Motivation: 评估神经网络样本级不确定性量化对高风险应用决策至关重要，但传统加性分解方法受到质疑

Method: 基于不同模型预测中类别概率分布的信噪比构建框架，引入方差门控置信因子，通过集成方法推导置信度

Result: 提出了新的不确定性估计和分解方法，能够有效识别和量化模型相关和数据相关的不确定性成分

Conclusion: 该框架为不确定性分解提供了更直观和可靠的方法，有助于改进高风险应用中的决策质量

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [16] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出了一种对抗性腐败奖励信号下的鲁棒Q学习算法，在异步采样模型中实现了与非对抗情况相近的有限时间收敛率，并建立了信息理论下界证明其最优性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法如Q-learning在面对奖励信号受到对抗性腐败（极端噪声、传感器故障或恶意攻击）时性能会严重下降，需要开发能够有效处理腐败奖励的鲁棒算法。

Method: 提出了新的可证明鲁棒的Q-learning变体算法，在异步采样模型下工作，即使部分观察奖励被对手任意扰动也能有效运行。还提出了无需真实奖励分布统计先验知识的算法变体。

Result: 算法在对抗性腐败下的有限时间收敛率与非对抗情况匹配，仅增加一个与腐败样本比例成正比的附加项。建立了信息理论下界证明该附加项是不可避免的。

Conclusion: 这是首个为异步Q-learning提供有限时间鲁棒性保证的工作，填补了鲁棒强化学习领域的重要空白，所开发的技术工具可能具有独立的研究价值。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [17] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: 提出了一种改进的矩阵乘性权重更新算法，在矩阵学习专家建议问题中实现了实例最优的遗憾界，计算复杂度与原算法相同，并在量子学习理论中有多个应用。


<details>
  <summary>Details</summary>
Motivation: 现有的MMWU算法在矩阵LEA问题上只能达到最小最大最优遗憾界，但无法实现实例最优性能。本文旨在开发一个既能保持相同计算复杂度又能获得更好遗憾界的改进算法。

Method: 首先建立了一个基于势函数的矩阵LEA通用框架，然后通过拉普拉斯变换技术开发了新的"单边"Jensen迹不等式，最后基于虚误差函数选择最优势函数来诱导算法。

Result: 新算法实现了O(√(T·S(X||d⁻¹I_d)))的实例最优遗憾界，计算复杂度与MMWU相同，在量子态学习、去极化噪声处理、随机量子态和Gibbs态学习等应用中优于现有技术。

Conclusion: 该工作提供了在保持计算效率的同时获得更好理论保证的矩阵在线学习算法，并在量子学习理论中展示了实际应用价值，为预测非线性量子性质提供了新工具。

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [18] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE是一个可扩展的XAI框架，通过RKHS中的正交函数分解避免特征子集枚举，提供功能性组件而非单一标量归因，在表格数据上实现快速高保真解释。


<details>
  <summary>Details</summary>
Motivation: 解决现有XAI框架的两个主要限制：特征子集枚举的指数级计算成本和将效应总结为单一标量值的表达力不足问题。

Method: 基于再生核希尔伯特空间的正交函数分解方法，通过递归核中心化程序的分析投影方案计算功能组件f_S(x_S)，避免显式子集枚举。

Result: 在10个数据集上实现中位数约3倍的速度提升（0.6-9.7倍范围），保持高保真度（R² 0.81-0.999）和良好的排序一致性。

Conclusion: STRIDE通过提供结构化功能视角补充标量归因方法，支持新颖诊断如'组件手术'来量化特定交互作用的影响。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [19] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 提出基于Wasserstein距离的分布鲁棒优化框架，在考虑组内分布不确定性的同时优化最差组性能，解决了传统方法在噪声和非平稳环境中假设组分布已知的局限性。


<details>
  <summary>Details</summary>
Motivation: 在异构数据源应用中，标准机器学习方法容易学习伪相关，在非典型或代表性不足的组上性能下降。现有方法假设组分布已知，但在噪声、非平稳和动态环境中这一假设经常不成立。

Method: 使用Wasserstein距离的分布鲁棒优化(DRO)框架，考虑每个组内的分布不确定性，同时保持优化最差组性能的目标。开发了梯度下降-上升算法求解DRO问题。

Result: 在真实世界数据上验证了方法的有效性，提供了算法收敛性证明。

Conclusion: 提出的框架能够有效处理组内分布不确定性，在噪声和非平稳环境中提升最差组性能，为异构数据源下的机器学习提供了更鲁棒的解决方案。

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [20] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: 提出FoundationalECGNet基础框架，通过多技术融合实现心电图自动分类，在正常/异常分类上达到99% F1分数，在多类心脏病检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，心电图分析对心脏异常检测至关重要，但现有方法面临噪声、类别不平衡和数据集异质性等挑战。

Method: 整合Morlet和Daubechies小波变换的双阶段去噪、卷积块注意力模块(CBAM)、图注意力网络(GAT)和时间序列变换器(TST)，共同捕捉多通道ECG信号的空间和时间依赖性。

Result: 在多个数据集上，正常vs异常分类达到99% F1分数，传导障碍和肥大症99% F1分数，心律失常98.9% F1分数，并提供风险等级评估。

Conclusion: FoundationalECGNet是可扩展、可解释和可推广的自动化ECG分析解决方案，有望提高医疗环境中的诊断精度和患者预后。

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [21] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: 该论文通过将LRP归因方法表示为修正梯度矩阵的乘积，建立了与雅可比矩阵乘法的类比，推导了奇异值上界和归因值的分量界限，并获得了控制经验均值收敛的乘性常数。


<details>
  <summary>Details</summary>
Motivation: 分析LRP型归因方法的数值特性，特别是研究归因值的分布规律和收敛性质，为多数据增强场景和Smoothgrad型方法提供理论支撑。

Method: 将LRP归因方法表示为修正梯度矩阵的乘积形式，推导奇异值上界和分量界限，获得控制经验均值收敛的乘性常数。

Result: 发现LRP-beta的常数与权重范数无关，这与基于梯度的方法和LRP-epsilon形成显著区别。

Conclusion: 该分析对多非几何数据增强场景和Smoothgrad型归因方法具有重要意义，揭示了LRP-beta方法的独特数值特性。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [22] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: 该论文提出了一种碳感知的联邦学习调度方法，通过利用碳强度的时间变化和松弛时间来减少训练过程中的碳排放，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习训练产生大量碳排放，联邦学习通过分布式计算可以充分利用不同地区和时间的碳强度变化来减少排放。

Method: 构建碳感知调度器，整合松弛时间、α-公平碳分配和全局微调阶段，通过碳感知的客户端选择和训练调度来优化排放。

Result: 在真实碳强度数据上的实验表明，该方法在广泛碳预算范围内优于非碳感知基线，在严格碳约束下表现尤其突出。

Conclusion: 碳感知调度策略能有效减少联邦学习的碳排放，同时保持模型准确性，特别是在碳预算紧张的情况下优势明显。

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [23] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 提出一个集成主动帕累托前沿学习算法(PyePAL)、可视化技术和可解释AI的框架，用于优化聚合物薄膜旋涂工艺参数，实现硬度和弹性的多目标优化。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜以获得特定机械性能本质上是一个多目标优化问题，需要同时优化多个性能指标并理解工艺参数与性能之间的关系。

Method: 使用PyePAL算法结合高斯过程模型预测目标值，通过UMAP进行二维可视化，并采用模糊语言总结将学习到的关系转化为语言描述，增强结果的可解释性。

Result: 实验结果表明该方法能有效识别有前景的聚合物设计，同时视觉和语言解释有助于专家驱动的分析和知识发现。

Conclusion: 该框架成功地将多目标优化与可解释AI技术相结合，不仅优化了工艺参数，还提供了对设计空间的可视化和语言理解，促进了专家知识发现。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [24] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: ANNA注意力机制通过近似最近邻方法实现亚二次时间复杂度，在保持Transformer表达能力的同时显著提升计算效率，能够模拟MPC算法并解决关键推理任务。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次时间复杂度严重限制了其可扩展性，需要开发更高效的注意力机制来保持表达能力的同时降低计算复杂度。

Method: 提出近似最近邻注意力(ANNA)机制，采用亚二次时间复杂度的近似计算方法，通过MPC框架证明其表达能力。

Result: ANNA-transformers能够保持标准注意力的表达能力，可以解决Match2和k-hop等关键推理任务，并且常数深度ANNA-transformers能够模拟常数深度低秩transformers。

Conclusion: ANNA提供了一种统一的方法来推理各种高效注意力近似方法，在保持理论表达能力的同时实现了显著的计算效率提升。

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [25] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref是一个开源的研究基线模型家族，包含0.13B到1.7B参数规模，在8个开放参考数据集上训练，提供中间检查点和完整训练日志，为研究者提供标准化的比较基准。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供标准化的参考基线，使研究人员能够评估不同训练方法的质量和合理性，促进训练方法的比较和复现。

Method: 使用密集transformer架构，在多个模型规模（0.13B-1.7B参数）和token规模（最高1T）上，基于8个开放参考数据集进行训练，并提供中间检查点。

Result: 训练结果表明，在NemoTron-CC HQ数据集上训练的性能最佳，其次是DCLM-baseline和FineWeb-Edu。建立了可比较的参考基线，支持通过计算轴进行训练方法的对比。

Conclusion: open-sci-ref提供了标准化的研究基线、中间检查点和完整工具链，有助于简化复现、标准化比较，并为未来研究提供基础。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [26] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 提出了一种针对表格数据的上下文条件异常检测框架，通过自动识别上下文特征和使用深度自编码器建模条件数据分布，在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据常包含异构上下文（如不同用户），导致全局罕见事件在某些上下文中是正常的。依赖单一全局分布会忽略这些上下文差异，降低检测性能。

Method: 自动识别上下文特征，使用简单的深度自编码器建模条件数据分布。

Result: 在多个表格基准数据集上的广泛实验表明，该方法优于最先进的方法。

Conclusion: 上下文在准确区分异常和正常实例中具有重要性，所提出的上下文条件异常检测框架能有效提升检测性能。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [27] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出混合专家模型(MoWE)，通过动态加权组合现有天气模型的输出，以较低计算成本获得比单个模型更准确的天气预报


<details>
  <summary>Details</summary>
Motivation: 数据驱动的天气模型性能已进入平台期，需要新方法来突破现有局限，而不是重新开发新的预报模型

Method: 使用基于Vision Transformer的门控网络，根据预报提前时间动态学习多个专家模型在每个网格点的权重贡献

Result: 在2天预报范围内，RMSE比最佳AI天气模型降低10%，显著优于单个专家模型和简单平均方法

Conclusion: 提供了一种计算高效且可扩展的策略，通过充分利用现有高质量预报模型来推动数据驱动天气预报的技术进步

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [28] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 这篇综述论文分析了机器学习在电力系统保护和扰动管理中的应用现状，发现虽然ML模型在模拟数据上表现良好，但缺乏真实世界验证，研究存在方法不一致、数据集质量差、评估标准不统一等问题。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和分布式能源的集成改变了现代电力系统，对传统保护方案提出了挑战，需要评估机器学习在电力系统保护中的应用潜力和现状。

Method: 采用PRISMA范围综述框架，基于100多篇文献进行系统性分析，建立了面向ML的保护任务分类法，解决术语不一致问题，并提出标准化报告实践指南。

Result: ML模型在模拟数据集上表现出高准确性，但在真实条件下的性能验证不足。现有文献碎片化，方法严谨性、数据集质量和评估指标存在不一致性。

Conclusion: 需要优先开发公共基准数据集、采用现实验证方法和先进ML架构，以推动基于ML的保护从理论承诺走向在动态分散电力系统中的实际部署。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [29] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 提出Rashomon Ensemble方法，通过从多个性能相近但解释不同的模型中选择并组合，构建多样化集成模型以提高泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中模型选择泛化性能不佳的问题，特别是在存在Rashomon效应（多个模型性能相似但解释不同）的现实场景中

Method: 基于模型性能和解释对高性能模型进行分组，构建最大化多样性同时保持预测准确性的集成模型

Result: 在Rashomon比例较大的场景中，AUROC指标提升超过0.20，并在多个现实应用中展示了稳健性和实用性

Conclusion: Rashomon Ensemble方法通过利用模型多样性有效提升了泛化性能，在现实世界应用中具有重要价值

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [30] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 本文研究了深度线性网络的黎曼几何，为学习过程的热力学描述提供基础，主要工具包括群作用分析和黎曼淹没技术。


<details>
  <summary>Details</summary>
Motivation: 为深度线性网络的学习过程建立热力学描述框架，通过黎曼几何方法分析参数空间的几何结构。

Method: 使用群作用分析过参数化，通过黎曼淹没从参数空间映射到可观测量空间，利用Jacobi矩阵理论构造平衡流形切空间的正交基。

Result: 建立了平衡流形上玻尔兹曼熵的定义和计算方法，证明了可观测量空间的黎曼几何可通过平衡流形的黎曼淹没获得。

Conclusion: 深度线性网络的黎曼几何为学习过程的热力学描述提供了数学基础，群轨道对平衡流形的叶状结构分析是关键工具。

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [31] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: Sensitivity-LoRA是一种高效的LLM微调方法，通过基于权重矩阵的全局和局部敏感度动态分配秩，解决了传统LoRA方法中均匀秩分配的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型从通用模型转向专业任务时面临挑战，特别是在资源受限环境中。现有的LoRA方法存在均匀秩分配问题，而现有的秩分配技术计算效率低、复杂且不稳定，阻碍了实际应用。

Method: 提出Sensitivity-LoRA方法，利用损失函数的二阶导数（Hessian矩阵）来有效捕捉权重敏感度，以最小计算开销实现最优秩分配。该方法基于权重矩阵的全局和局部敏感度动态分配秩。

Result: 实验结果表明，Sensitivity-LoRA在不同任务和基准测试中表现出强大的有效性、效率和稳定性。

Conclusion: Sensitivity-LoRA通过动态秩分配机制，解决了传统LoRA方法的局限性，为资源受限环境下的LLM微调提供了高效稳定的解决方案。

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [32] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 提出了一种因果感知深度学习框架，结合多元格兰杰因果和PCMCI+方法进行因果特征选择，应用于北极海冰范围预测，提高了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习模型依赖相关性学习，难以区分真实因果关系和虚假关联，限制了模型的鲁棒性、可解释性和泛化能力。

Method: 整合多元格兰杰因果关系(MVGC)和PCMCI+进行因果特征选择，构建混合神经网络架构，使用43年北极海冰范围数据及相关海洋-大气变量。

Result: 实验结果表明，加入因果输入特征在不同预测时间跨度下都能提高预测准确性和可解释性，同时减少不必要特征并提升计算效率。

Conclusion: 该框架不仅适用于北极海冰预测，还可广泛应用于其他动态高维领域，为因果驱动的预测建模提供了可扩展的方法，推动了理论基础和实际性能的进步。

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [33] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 提出了一个基于物理信息神经网络(PINNs)的连续时间多智能体强化学习框架(CT-MARL)，通过值梯度迭代(VGI)模块解决传统HJB方程在高维多智能体系统中的维度灾难问题


<details>
  <summary>Details</summary>
Motivation: 现有连续时间RL方法主要局限于单智能体领域，因为HJB方程在多智能体设置中存在维度灾难问题，且集中式值函数近似困难导致策略训练不稳定

Method: 使用物理信息神经网络(PINNs)近似HJB基值函数，引入值梯度迭代(VGI)模块通过沿轨迹迭代细化值梯度来确保值与微分结构的一致性

Result: 在连续时间版本的多智能体粒子环境(MPE)和多智能体MuJoCo基准测试中，该方法持续优于现有连续时间RL基线，并能扩展到复杂多智能体动力学

Conclusion: 所提出的CT-MARL框架成功解决了连续时间多智能体RL的关键挑战，通过VGI模块提高了梯度保真度，实现了更准确的值估计和更强的策略学习能力

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [34] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 使用机器学习模型预测ISP对等互联伙伴选择，XGBoost模型在公开数据上达到98%准确率


<details>
  <summary>Details</summary>
Motivation: ISP对等互联过程复杂耗时，自动化选择能提高全球互联网生态效率

Method: 从PeeringDB、CAIDA等公开数据库收集ISP数据，评估树基、神经网络和Transformer三类ML模型

Result: 树基模型表现最佳，XGBoost模型准确率达98%，对时间、空间变化和缺失数据具有强鲁棒性

Conclusion: 该方法可实现ISP对等伙伴选择全自动化，推动互联网生态系统向更高效优化发展

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [35] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: HISPASpoof是第一个大规模西班牙语合成语音检测数据集，包含6种口音的真实语音和6种零样本TTS系统生成的合成语音，用于解决西班牙语在语音取证领域的代表性不足问题。


<details>
  <summary>Details</summary>
Motivation: 虽然英语和中文的合成语音检测器发展迅速，但全球超过6亿人使用的西班牙语在语音取证领域代表性不足，需要专门的数据集来填补这一空白。

Method: 构建HISPASpoof数据集，包含来自公共语料库的6种口音真实语音，以及使用6种零样本TTS系统生成的合成语音。评估了5种代表性检测方法在西班牙语上的性能。

Result: 在英语上训练的检测器无法泛化到西班牙语，而在HISPASpoof上训练显著提高了检测性能。同时评估了合成语音溯源（识别生成方法）的性能。

Conclusion: HISPASpoof为推进西班牙语可靠和包容性语音取证提供了关键基准，解决了西班牙语在语音伪造检测领域的代表性不足问题。

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [36] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出无需训练的视觉Transformer自适应token合并框架，通过贝叶斯优化平衡计算成本与精度，实现6G网络中高效的语义通信部署


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型在语义通信中表现强大，但其巨大计算需求阻碍了在资源受限的6G网络中的实际部署，需要降低推理时间和传输资源使用

Method: 将每层合并比例选择建模为多目标优化问题，使用高斯过程贝叶斯优化构建帕累托最优配置前沿，支持根据应用需求和信道条件动态调整

Result: 方法在多种信噪比条件下显著减少浮点运算量，同时保持竞争力精度，自适应策略能根据信道质量调整合并强度，实现延迟与语义保真度的按需权衡

Conclusion: 为未来边缘智能系统中部署基于Transformer的语义通信提供了一种可扩展且高效的方法

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [37] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种结合量子启发神经网络和深度强化学习的USD/TWD交易代理，使用QLSTM进行短期趋势预测和QA3C强化学习算法，在5年测试期内获得11.87%的回报率和0.92%的最大回撤。


<details>
  <summary>Details</summary>
Motivation: 量子启发神经网络与深度强化学习的融合为金融交易提供了新的可能性，特别是在外汇交易领域需要更精准的短期趋势预测和风险控制。

Method: 采用Quantum LSTM (QLSTM)进行短期趋势预测，结合Quantum Asynchronous Advantage Actor-Critic (QA3C)强化学习算法，使用2000-2025年数据进行训练(80%训练，20%测试)，状态设计包含QLSTM特征和技术指标。

Result: 在约5年的测试期内，只做多策略获得11.87%的回报率，最大回撤仅为0.92%，表现优于多个货币ETF，证明了混合模型在外汇交易中的竞争力。

Conclusion: QLSTM在小利润交易和严格风险控制方面表现有效，未来需要改进量子模拟和策略复杂性。关键超参数：QLSTM序列长度=4，QA3C工作线程=8。

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [38] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO是一种序列级强化学习方法，通过在重要性采样权重空间实施长度公平裁剪来解决PPO/GRPO方法在序列长度处理上的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有序列级RL方法在移植PPO/GRPO式裁剪时存在固定裁剪范围系统性地对长短响应重新加权的问题，导致有效目标失真。

Method: 提出FSPO方法，采用高斯启发的解决方案：用KL校正漂移项和√L缩放的方式裁剪序列对数IS比率。

Result: FSPO在多个评估数据集上均优于所有基线方法，能够平坦化不同长度区间的裁剪率并稳定训练过程。

Conclusion: FSPO通过理论形式化的长度公平性保障和实用的裁剪策略，有效解决了序列级RL中的长度偏差问题，提升了模型性能。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [39] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 本文提出了DART框架，通过双解码器架构解决深度学习天气模型评估中的"统计相似性陷阱"问题，专门针对极端对流检测进行优化，在保持高相关性的同时显著提高了对危险天气事件的检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习天气模型的评估指标存在"统计相似性陷阱"，奖励模糊预测但错过罕见的高影响事件。需要开发能够准确检测极端对流天气的新方法。

Method: 提出DART（Dual Architecture for Regression Tasks）框架，采用双解码器架构，包含显式的背景/极端分解、物理驱动的过采样和任务特定的损失函数，将粗粒度大气预报转换为高分辨率卫星亮温场。

Result: DART在极端对流检测方面取得显著改进：CSI达到0.273（基线为0.00），偏差为2.52（基线为6.72）；发现去除集成水汽输送（IVT）可使极端对流检测性能提升270%；在2023年8月吉大港洪水灾害案例中验证了实际效果。

Conclusion: DART是首个系统解决混合转换-分割-降尺度任务的工作，能够在标准硬件上10分钟内完成训练，并与现有气象工作流程无缝集成，为极端天气预警提供了可信的AI解决方案。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [40] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O算法，通过自适应激励机制和渐进惩罚策略解决约束强化学习中的训练不稳定问题，在保证约束满足的同时优化性能


<details>
  <summary>Details</summary>
Motivation: 连续控制场景中，智能体在奖励最大化和约束满足之间的平衡存在挑战，传统策略优化方法在约束边界附近表现不稳定

Method: 引入自适应激励机制，在接近约束边界前提供额外激励；提出IP3O算法，采用渐进增加的惩罚机制来稳定训练动态

Result: 在基准环境上的实证评估显示，IP3O相比最先进的安全RL算法表现出更好的性能

Conclusion: IP3O算法有效解决了约束强化学习中的训练稳定性问题，同时提供了理论最优性误差界限的保证

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [41] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 本研究通过机器学习方法识别农业旅游增长的关键指标，LASSO结合多种分类器在预测农业旅游发展方面取得高准确率（最高99%）


<details>
  <summary>Details</summary>
Motivation: 农业旅游作为促进农村发展和保护传统文化的重要经济模式，需要系统研究其增长策略和关键指标

Method: 两阶段研究：文献综述识别指标，然后使用LASSO特征选择方法结合逻辑回归、决策树、随机森林和XGBoost等机器学习分类器进行分析

Result: LASSO方法下，逻辑回归模型在70-30%训练测试数据中达到98%准确率，在80-20%数据中达到99%准确率，其他模型也表现优异（95-97%）

Conclusion: 机器学习方法能有效识别农业旅游增长的关键指标，为制定农业旅游发展策略提供数据驱动的决策支持

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [42] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: Vejde框架结合数据抽象、图神经网络和强化学习，为具有丰富结构化状态的决策问题生成归纳策略函数，能够在不同规模和结构的问题上实现良好泛化。


<details>
  <summary>Details</summary>
Motivation: 解决具有丰富结构化状态（如对象类和关系）的决策问题，传统方法难以处理变化的问题规模和结构，需要能够归纳泛化的策略函数。

Method: 将MDP状态表示为实体事实数据库，转换为二分图，通过神经消息传递映射到潜在状态。使用监督学习和强化学习训练策略，在训练和测试实例分离的设置下评估泛化能力。

Result: Vejde策略在未见过的测试实例上平均能够良好泛化，得分损失不显著，且平均得分接近针对特定实例训练的MLP代理。

Conclusion: Vejde框架通过结合数据抽象、图神经网络和强化学习，成功实现了对结构化决策问题的归纳策略学习，在泛化性能上表现出色。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [43] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: 提出LDSim方法，通过知识蒸馏将LLM的领域知识和推理能力转移到传统序列模型中，在保持快速推理的同时提升问答模拟器性能


<details>
  <summary>Details</summary>
Motivation: 解决现有问答模拟器方法中LLM-free方法性能不佳而LLM-based方法推理速度慢、资源消耗高的问题，寻求性能与效率的平衡

Method: 采用知识蒸馏技术，从大型语言模型(LLM)中提取领域知识和推理能力，辅助传统序列模型进行预测，提升模拟性能

Result: 在模拟任务和知识追踪(KT)任务上都取得了强劲的结果，实现了性能提升

Conclusion: LDSim方法通过知识蒸馏有效结合了LLM的优势和传统序列模型的高效性，为教育推荐系统提供了更好的训练数据收集方案

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [44] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 提出了一种基于多注意力元Transformer的少样本无监督旋转机械故障诊断方法(MMT-FD)，解决了实际工业应用中标记样本稀缺和模型泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 旋转机械设备的智能故障诊断通常需要大量标记样本数据，但在实际工业应用中获取足够数据既困难又昂贵。不同类型的设备具有不同的机械特性，需要为每种情况单独训练诊断模型。

Method: MMT-FD框架整合了时频域编码器和元学习泛化模型。时频域编码器通过时频域的随机增强预测状态表示，然后将增强数据输入元学习网络进行分类和泛化训练，最后使用少量标记数据进行微调。通过少量对比学习迭代优化模型。

Result: 在轴承故障数据集和转子试验台数据上的实验表明，MMT-FD模型仅使用1%的标记样本数据就达到了99%的故障诊断准确率，展现出强大的泛化能力。

Conclusion: 该方法能够从无标记数据中提取潜在的故障表示，并具有强大的泛化能力，适用于各种类型机械设备的故障诊断，为解决实际工程应用中的有限故障样本和预测模型泛化能力不足问题提供了有效方案。

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [45] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 本文提出了Entropy-Modulated Policy Gradients (EMPG)框架，通过基于不确定性和任务结果重新校准学习信号，解决了LLM智能体在长时程任务中因稀疏奖励导致的信用分配问题。


<details>
  <summary>Details</summary>
Motivation: 在长时程任务中，基于大型语言模型的智能体面临稀疏奖励难以分配到中间步骤的问题。现有方法主要关注创建密集奖励信号，但忽略了LLM学习动态中的一个根本问题：策略梯度大小与熵的耦合导致学习效率低下。

Method: 提出EMPG框架，根据步骤不确定性和最终任务结果重新校准学习信号。该方法放大自信正确动作的更新，惩罚自信错误，并衰减不确定步骤的更新以稳定探索。还引入了未来清晰度奖励项来鼓励寻找更可预测的解决方案路径。

Result: 在WebShop、ALFWorld和Deep Search三个具有挑战性的智能体任务上进行了全面实验，EMPG实现了显著的性能提升，显著优于强策略梯度基线方法。

Conclusion: EMPG通过解耦策略梯度大小与熵的耦合关系，有效解决了长时程任务中的信用分配问题，为基于LLM的智能体学习提供了更稳定和高效的方法。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [46] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 该研究提出了DRSALife框架，用于从观测数据中学习软人工生命规则集，以准确表示反应-扩散系统中的涌现动力学，无需先验物理知识，并在噪声和稀疏数据条件下取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的涌现动力学发现面临的主要挑战是在没有先验物理知识的情况下进行系统识别。本研究旨在通过学习软人工生命模型（如基于代理和元胞自动机模型）来解决这一挑战。

Method: 使用DRSALife概念框架学习软人工生命规则集，从观测数据中识别反应-扩散系统的涌现动力学。该方法能够从噪声和稀疏数据集中学习，并成功识别底层偏微分方程的结构和参数。

Result: 实验结果表明，学习到的模型能够以74%的良好准确率预测涌现动力学，并且在面对高斯噪声和时间稀疏性时表现出相当稳健的性能。该方法在元胞自动机规则30、生命游戏和Vicsek集群等问题上取得了有希望的结果。

Conclusion: DRSALife框架是少数几个探索基于机器的软人工生命规则集学习的研究之一，能够在没有先验物理知识的情况下成功识别反应-扩散动力学，为数据驱动的系统识别提供了有效方法。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [47] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: 提出MoSE框架，通过匿名游走提取子图并动态路由到专家网络，增强GNN的结构表达能力，在SWL测试中证明比SWL更强大，实验显示性能优越且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖局部成对消息传递，难以捕捉复杂高阶子图模式，结构表达能力不足。现有基于随机游走核的方法主要针对图级任务，且固定核配置限制了模型灵活性。

Method: MoSE框架使用匿名游走提取信息子图，基于结构语义动态路由到专门专家网络，从而灵活捕捉多样子图模式，提高表达能力和可解释性。

Result: 理论分析证明MoSE在子图Weisfeiler-Lehman测试中比SWL更强大。大量实验和可视化显示MoSE不仅性能优于基线，还提供了模型学习结构模式的可解释见解。

Conclusion: MoSE框架通过混合子图专家方法有效解决了GNN结构表达能力不足的问题，在多种图任务中表现出优异的性能和可解释性。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [48] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出了一种基于可配置多项式核的HGR相关系数计算方法，相比现有方法具有更好的鲁棒性和确定性，适用于实际机器学习应用中的约束优化。


<details>
  <summary>Details</summary>
Motivation: 现有的HGR相关系数可微分估计算法存在偏差-方差权衡问题，可能影响方法的鲁棒性，限制了在实际场景中的应用。

Method: 使用用户可配置的多项式核来计算HGR相关系数，提供了更快的计算速度和几乎同等有效的约束效果。

Result: 新方法在鲁棒性和确定性方面具有显著优势，能够产生有意义的次梯度作为损失正则化器。

Conclusion: 该方法为HGR相关系数提供了一个更可靠的计算方案，特别适合在约束机器学习框架中作为损失正则化器使用。

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [49] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX是一个零样本超参数优化框架，结合元学习、可解释AI和高效LLM推理，无需额外试验即可推荐最优超参数和预训练模型，大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型和超参数选择需要大量专业知识和计算资源的问题，当前基于LLM的方法依赖试错和昂贵API，缺乏可解释性和通用性。

Method: 利用历史实验结果的SHAP解释，结合元学习和LLM推理，采用LLM-as-judge评估控制输出格式、准确性和完整性。

Result: 在8个医学影像数据集上，MetaLLMiX性能优于传统HPO方法，计算成本大幅降低，本地部署在5/8任务上达到最优结果，响应时间减少99.6-99.9%，训练速度提升2.4-15.7倍，准确率与最佳基线相差1-5%。

Conclusion: MetaLLMiX提供了一个高效、可解释的超参数优化解决方案，显著降低了计算成本和时间，同时保持了竞争性的性能表现。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [50] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现大型语言模型生成的自我反事实解释(SCEs)存在有效性-最小性权衡问题：要么修改过多不够简洁，要么修改过小无法改变预测结果，无法提供可靠的模型行为解释


<details>
  <summary>Details</summary>
Motivation: 研究语言模型如何通过自我生成的反事实解释来向人类解释其决策过程，这对于模型在关键场景中的可信部署至关重要

Method: 评估LLMs生成的反事实解释在有效性和最小性两个维度上的表现，分析多个模型、数据集和评估设置下的表现模式

Result: LLMs通常能生成有效的反事实解释，但远非最小化修改；当要求生成最小化修改时，又往往修改过小无法改变预测结果

Conclusion: 自我反事实解释作为可解释性工具效果有限，甚至可能提供误导性见解，在关键应用部署中需要考虑不可靠自我解释对下游决策的影响

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [51] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习与地统计学的混合框架KpR，通过空间滞后特征增强ML模型的空间上下文能力，在土壤属性预测中显著提升了精度和不确定性估计


<details>
  <summary>Details</summary>
Motivation: 机器学习和地统计学在土壤属性预测中各有优势但缺乏有效结合，需要开发能够同时利用空间结构和环境特征关系的混合方法

Method: KpR框架通过普通克里金法生成'空间滞后'特征来丰富机器学习模型的空间上下文，采用TabPFN模型在六个田间尺度数据集上进行评估

Result: KpR与TabPFN结合相比其他空间技术和非空间ML算法，提供了更可靠的不确定性估计和更准确的预测，平均R2提高了约30%

Conclusion: KpR与TabPFN结合形成了一个非常稳健和通用的数字土壤制图建模框架，特别适用于样本量小的精准农业应用场景

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [52] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 该研究提出了fuser算法和SAC验证框架，用于分析微生物群落在不同环境中的动态关联网络，解决了传统方法在跨环境预测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统微生物共现网络推断算法通常只分析单一环境中的静态关联，无法捕捉微生物群落在不同生态条件下的动态适应过程，需要开发能够处理跨环境预测的新方法。

Method: 提出了Same-All Cross-validation (SAC)框架评估算法性能，并开发了fuser算法，该算法在训练时保留子样本特异性信号的同时跨环境共享相关信息，生成环境特异性的预测网络。

Result: fuser算法在同质环境(Same)中与glmnet等现有算法性能相当，在跨环境(All)场景中显著降低了测试误差。

Conclusion: fuser算法能够有效处理微生物群落的时空动态变化，为跨环境微生物关联网络推断提供了新的解决方案。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [53] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: CSGD是首个通过concrete scores将score matching扩展到离散图的可控分子图生成模型，通过Composable Guidance和Probability Calibration技术，在四个分子数据集上实现了最先进的性能，可控性平均提升15.3%


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在多条件设置下效果有限，依赖联合条件或连续松弛会损害保真度，需要开发能够灵活控制多个属性约束的分子生成方法

Method: 提出CSGD模型，通过concrete scores将score matching扩展到离散图；引入Composable Guidance实现采样过程中对任意条件子集的细粒度控制；使用Probability Calibration调整估计的转移概率以缓解训练-测试不匹配

Result: 在四个分子数据集上取得最先进性能，可控性相比先前方法平均提升15.3%，同时保持高有效性和分布保真度

Conclusion: 基于score的建模在离散图生成中具有实际优势，能够实现灵活的多属性分子设计

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [54] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: AquaCast是一个多输入多输出的深度学习模型，用于城市水动力学预测，通过融合内源变量和外源因素，在真实和合成数据集上都实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统城市水动力学预测方法无法同时捕捉变量间和时间依赖关系的问题，特别是如何有效利用外源因素（如降水历史）而不需要预测这些变量。

Method: 开发AquaCast模型，使用嵌入层融合外源输入，专注于预测内源变量（水位/流量），同时捕捉所有输入的变量间和时间依赖关系。在LausanneCity真实数据集和三个大规模合成数据集上进行评估。

Result: 模型在使用仅内源变量时达到最先进性能，加入外源变量和预测报告后性能进一步提升。在真实和合成数据集上都持续优于现有基线方法，保持稳健准确的预测能力。

Conclusion: AquaCast模型通过创新的外源变量融合机制，有效解决了城市水动力学预测中的复杂依赖关系问题，具有良好的泛化能力和可扩展性。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [55] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 开发了一个名为Agent-E的AI代理系统，能够自动从会议论文中识别特定地区的论文，并通过RPA技术执行预定操作（如提交提名表），在586篇论文测试中实现了100%召回率和99.4%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决学术文献快速增长带来的发现挑战，减少学术发现所需的手动工作量，为研究人员、资助机构和学术团体提供自动化解决方案。

Method: 构建了一个完全自动化的系统管道，使用专门的AI代理Agent-E来识别特定地理区域的会议论文，并结合机器人流程自动化（RPA）技术执行预定操作。

Result: 在5个不同会议的586篇论文上验证，系统成功识别所有目标论文，召回率达到100%，准确率接近完美（99.4%）。

Conclusion: 面向任务的AI代理不仅能够过滤信息，还能积极参与并加速学术社区的工作流程，展示了AI在学术自动化方面的巨大潜力。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [56] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 提出基于时序规则的完全可解释TKG预测方法，在9个数据集上性能匹配或超越8个SOTA模型和2个基线，同时保持完全可解释性


<details>
  <summary>Details</summary>
Motivation: 受近期使用循环事实的强基线工作启发，希望开发完全可解释的时序知识图谱预测方法

Method: 学习四种简单类型的时序规则，使用考虑最近性和频率的置信度函数

Result: 在九个数据集上的评估显示，方法性能匹配或超越了八个最先进模型和两个基线模型

Conclusion: 该方法在保持高性能的同时提供了完全可解释的预测，证明了时序规则在TKG预测中的有效性

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [57] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 提出了D2P2-SGD优化器，结合动态差分隐私和随机投影技术，在保护隐私的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有DPSGD的静态噪声机制影响模型性能，且随着模型参数指数增长，随机优化器的学习效率面临挑战

Method: 结合动态差分隐私（自动梯度裁剪）和随机投影SGD，动态调整效用与隐私的权衡

Result: 在不同目标函数上表现出可证明的次线性收敛率，实验显示在保持隐私的同时显著提高准确率

Conclusion: DDP以隐私为代价带来更好的效用，随机投影实现更高效的模型学习

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [58] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个解决算法选择问题中计算成本高和OpenML数据局限性的实验集合，包含9,408个管道在300个数据集上的实验结果，旨在提供多样性和完整性的元学习数据。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习算法选择问题中评估算法性能的高计算成本，以及OpenML等在线存储库中实验记录的局限性，包括管道多样性不足和技术不平衡问题。

Method: 构建PIPES实验集合，设计代表所有选定技术组合的多个管道，在300个数据集上执行9,408个管道的实验，记录详细的管道块信息、训练测试时间、预测结果、性能和错误信息。

Result: 创建了一个包含详细实验结果的综合性集合，支持研究人员进行多样化和代表性管道与数据集的分析，为元学习社区提供了扩展潜力。

Conclusion: PIPES通过提供多样化和完整的实验数据，克服了现有存储库的局限性，为算法选择问题的研究提供了有价值的资源，并支持未来进一步扩展。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [59] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 本研究探索了小样本学习在呼吸音分类中的有效性，特别是针对COVID-19、流感和健康状态的咳嗽声检测。使用原型网络和声谱图表示，在有限标注数据下取得了与传统深度学习方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决医疗诊断中标注数据稀缺的问题，探索小样本学习在呼吸音分类中的应用潜力，特别是在COVID-19等呼吸道疾病检测方面。

Method: 采用原型网络（Prototypical Networks）结合咳嗽声的声谱图表示，进行多类别和二元分类比较，每个类别仅使用15个支持样本。

Result: 多类别分类达到74.87%准确率，二元分类在所有类别对上都超过70%准确率。统计测试显示多类别与二元分类模型性能无显著差异（t检验p=0.149，Wilcoxon p=0.125）。流感最易区分，健康状态最具挑战性。

Conclusion: 小样本学习在医疗诊断中具有可行性，特别是在标注数据有限的情况下，多类别分类模型可以达到与二元分类相当的性能，为实际应用提供了有效解决方案。

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [60] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出ProDiGy算法，通过联合双评分系统评估客户端梯度，在非IID数据分布下有效防御拜占庭攻击，优于现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构性下容易受到对抗攻击，现有防御机制在非IID数据分布下效果不佳，需要新的鲁棒防御方法。

Method: 使用基于梯度接近性和差异性的联合双评分系统评估客户端梯度，促进诚实客户端间的自然相似性，同时检测可疑一致性作为攻击指标。

Result: 大量数值实验表明，ProDiGy在各种场景下优于现有防御方法，特别是在非IID数据分布时仍能保持强大的防御能力和模型准确性。

Conclusion: 双视角方法在联邦学习中具有显著效果，能够有效区分诚实客户端和恶意攻击，为拜占庭鲁棒联邦学习提供了新思路。

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [61] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 提出了一种新颖的图对齐框架，通过双通道编码器和几何感知功能映射模块，同时增强节点区分度并确保跨图潜在空间的几何一致性，在无监督图对齐任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图对齐方法存在两个关键问题：GNN嵌入中的过度平滑导致节点区分度降低，以及结构噪声、特征异质性和训练不稳定性导致的跨图潜在空间错位，最终导致不可靠的节点对应关系。

Method: 采用双通道编码器结合低通和高通谱滤波器生成既结构感知又高度区分的嵌入；引入几何感知功能映射模块学习图嵌入之间的双射等距变换，确保跨表示的一致几何关系。

Result: 在图基准测试中 consistently 优于现有无监督对齐基线，对结构不一致性和挑战性对齐场景表现出 superior 鲁棒性；在视觉-语言基准测试中也能有效泛化，实现视觉和语言表示的无监督对齐。

Conclusion: 该框架通过同时处理节点区分度和跨图几何一致性，显著提升了无监督图对齐的性能，并展示了跨领域的泛化能力。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [62] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 提出一种针对随机和混沌时空系统的深度学习模拟器，能够根据PDE参数值进行条件化模拟，通过预训练和微调实现参数空间泛化，并提供计算加速和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 传统数值积分方法计算成本高昂，难以高效探索参数空间。需要开发能够处理不同参数值、域大小和分辨率的计算高效模拟器，同时提供不确定性量化

Method: 采用预训练+微调策略，在单一参数域预训练后使用多样化小数据集微调。引入局部注意力机制处理可变域大小和分辨率，支持从较小域预训练后泛化到更大域

Result: 在混沌Kuramoto-Sivashinsky方程和随机强迫beta平面湍流上验证，能够捕捉插值参数下的现象，提供显著计算加速，概率变体支持不确定性量化和罕见事件统计研究

Conclusion: 该方法成功实现了参数条件化的时空系统模拟，提供计算高效的参数空间探索工具，局部注意力机制有效支持域大小泛化，概率版本为不确定性量化提供新途径

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [63] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO是一种新型数据稀疏算子学习算法，通过数学严谨的贪婪算法构建网络结构，实现紧凑架构和严格离散化不变性，显著优于现有算子学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决多输入PDE组的求解问题，旨在消除/缩小泛化差距，实现严格离散化不变性，同时保持计算效率。

Method: 结合降基方法和生成预训练物理信息神经网络，使用贪婪算法离线自适应构建网络结构，通过任务特定激活函数进行知识蒸馏。

Result: 在分布内和分布外测试中显著优于PCA-Net、DeepONet、FNO和CNO等先进算法，是唯一实现严格离散化不变性的算子学习算法。

Conclusion: ReBaNO为数据稀疏环境下的PDE求解提供了高效且泛化能力强的解决方案，在算子学习领域具有重要突破意义。

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [64] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 提出了一种通过分析群体反事实解释的时间演化来解释概念漂移的新方法，使用三层框架从数据层、模型层和解释层综合分析漂移原因


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在动态环境中经常遭受概念漂移，虽然检测漂移已有研究，但解释模型决策逻辑如何及为何变化仍是一个重大挑战

Method: 通过追踪群体反事实解释(GCEs)的聚类中心点及其相关反事实动作向量在漂移前后的变化，构建包含数据层、模型层和解释层的三层分析框架

Result: 该方法能够揭示模型决策边界和底层逻辑的结构性变化，区分不同的漂移根本原因（如空间数据漂移与概念重新标记）

Conclusion: 提出的解释层与数据层、模型层相结合，提供了对概念漂移更全面的诊断，使区分不同根本原因成为可能

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [65] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: 提出基于功能基团的分子表示框架FGR，结合传统化学功能基团和数据挖掘发现的新功能基团，在保持化学可解释性的同时实现最先进的分子性质预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习分子性质预测模型缺乏可解释性的问题，阻碍化学家采用这些模型进行药物和材料发现。

Method: 开发功能基团表示(FGR)框架，整合两类功能基团：基于化学知识的传统功能基团(FG)和使用序列模式挖掘从大规模分子语料库中发现的新功能基团(MFG)，通过预训练将分子编码到低维潜在空间，并可包含基于2D结构的分子描述符。

Result: 在33个涵盖物理化学、生物物理、量子力学、生物活性和药代动力学的基准数据集上达到最先进性能，同时保持化学可解释性。

Conclusion: 该工作向开发高性能、化学可解释的深度学习分子发现模型迈出了重要一步，模型表示与既定化学原理内在一致，使化学家能够直接将预测性质与特定功能基团联系起来。

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


### [66] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL是一种离线强化学习方法，通过校准各保护子组的安全阈值来减少伤害，同时平衡公平性目标（覆盖率或伤害）


<details>
  <summary>Details</summary>
Motivation: 为了解决离线强化学习中的安全问题，同时确保不同保护子组之间的公平性，特别是在医疗保健等敏感领域的应用

Method: 使用可行性引导的公平自适应强化学习框架，基于医疗补助人群健康管理项目的去标识化纵向轨迹数据，与行为克隆和HACO基线方法进行比较

Result: FG-FARL在保持与基线方法相当的价值估计的同时，显著改善了公平性指标，通过bootstrap 95%置信区间和子组差异分析验证了效果

Conclusion: FG-FARL提供了一种实用的方法来实现更安全、更公平的决策支持系统，特别是在医疗健康管理等领域

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [67] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: ButterflyQuant是一种新的2位量化方法，通过可学习的蝴蝶变换替代固定Hadamard变换，针对不同Transformer层的异常值模式进行自适应旋转，显著提升极端量化下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有旋转方法使用固定的Hadamard变换无法适应不同Transformer层特有的异常值分布模式，导致2位极端量化时性能严重下降。

Method: 提出可学习的蝴蝶变换，使用连续Givens旋转角度参数化，保证正交性同时支持梯度优化；引入均匀性正则化促进量化友好的平滑分布。

Result: 在LLaMA-2-7B模型上，2位量化时ButterflyQuant达到15.4困惑度，显著优于QuaRot的22.1困惑度。

Conclusion: 层自适应的可学习旋转变换能有效处理量化异常值，以极小的一次性计算成本大幅提升极端量化性能。

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [68] [Scalable extensions to given-data Sobol' index estimators](https://arxiv.org/abs/2509.09078)
*Teresa Portone,Bert Debusschere,Samantha Yang,Emiliano Islas-Quinones,T. Patrick Xiao*

Main category: stat.ML

TL;DR: 本文提出了针对大数据量模型的Sobol'指数计算方法扩展，包括通用分区定义、流式处理算法和小指数过滤启发式方法，解决了现有方法在处理大量输入时内存不足和分区偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于给定数据的方差敏感性分析方法在处理具有极大量输入（如神经网络>10^4参数）的模型时存在内存限制和分区偏差问题，无法有效应用于大规模模型。

Method: 提出了三个扩展：1）具有任意分区的通用Sobol'指数估计器定义；2）批量处理输入输出样本的流式算法；3）过滤统计噪声中小指数的启发式方法。

Result: 研究表明等概率分区会在Sobol'指数估计中引入显著偏差，流式算法在降低内存需求的同时能达到相当的准确性和运行时间，并在神经网络建模应用中验证了有效性。

Conclusion: 所提出的扩展方法使得方差敏感性分析能够高效应用于大规模模型，解决了现有方法的内存限制和分区偏差问题，为神经网络等大规模模型的敏感性分析提供了实用解决方案。

Abstract: Given-data methods for variance-based sensitivity analysis have significantly
advanced the feasibility of Sobol' index computation for computationally
expensive models and models with many inputs. However, the limitations of
existing methods still preclude their application to models with an extremely
large number of inputs. In this work, we present practical extensions to the
existing given-data Sobol' index method, which allow variance-based sensitivity
analysis to be efficiently performed on large models such as neural networks,
which have $>10^4$ parameterizable inputs. For models of this size, holding all
input-output evaluations simultaneously in memory -- as required by existing
methods -- can quickly become impractical. These extensions also support
nonstandard input distributions with many repeated values, which are not
amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index
estimator with arbitrary partition, a streaming algorithm to process
input-output samples in batches, and a heuristic to filter out small indices
that are indistinguishable from zero indices due to statistical noise. We show
that the equiprobable partition employed in existing given-data methods can
introduce significant bias into Sobol' index estimates even at large sample
sizes and provide numerical analyses that demonstrate why this can occur. We
also show that our streaming algorithm can achieve comparable accuracy and
runtimes with lower memory requirements, relative to current methods which
process all samples at once. We demonstrate our novel developments on two
application problems in neural network modeling.

</details>


### [69] [Global Optimization of Stochastic Black-Box Functions with Arbitrary Noise Distributions using Wilson Score Kernel Density Estimation](https://arxiv.org/abs/2509.09238)
*Thorbjørn Mosekjær Iversen,Lars Carøe Sørensen,Simon Faarvang Mathiesen,Henrik Gordon Petersen*

Main category: stat.ML

TL;DR: 本文提出使用Wilson Score核密度估计器(WS-KDE)为贝叶斯优化提供置信边界，适用于输出在[0,1]区间内的任意随机函数，无需对扰动分布进行建模。


<details>
  <summary>Details</summary>
Motivation: 机器人学中的优化问题常涉及耗时且随机的黑盒函数评估，现有函数估计器需要大量评估或依赖扰动建模，需要一种更稳定通用的置信边界估计方法。

Method: 采用Wilson Score核密度估计器(WS-KDE)来提供置信边界，该方法适用于任何输出在[0,1]区间内的随机函数，不依赖于特定的输出分布假设。

Result: WS-KDE能够为贝叶斯优化提供优秀的置信边界，在仿真和振动零件送料器的自动陷阱设计问题中验证了其有效性。

Conclusion: WS-KDE扩展了贝叶斯优化的应用范围，为各种随机函数的稳定全局优化提供了可靠的工具。

Abstract: Many optimization problems in robotics involve the optimization of
time-expensive black-box functions, such as those involving complex simulations
or evaluation of real-world experiments. Furthermore, these functions are often
stochastic as repeated experiments are subject to unmeasurable disturbances.
Bayesian optimization can be used to optimize such methods in an efficient
manner by deploying a probabilistic function estimator to estimate with a given
confidence so that regions of the search space can be pruned away.
Consequently, the success of the Bayesian optimization depends on the function
estimator's ability to provide informative confidence bounds. Existing function
estimators require many function evaluations to infer the underlying confidence
or depend on modeling of the disturbances. In this paper, it is shown that the
confidence bounds provided by the Wilson Score Kernel Density Estimator
(WS-KDE) are applicable as excellent bounds to any stochastic function with an
output confined to the closed interval [0;1] regardless of the distribution of
the output. This finding opens up the use of WS-KDE for stable global
optimization on a wider range of cost functions. The properties of WS-KDE in
the context of Bayesian optimization are demonstrated in simulation and applied
to the problem of automated trap design for vibrational part feeders.

</details>


### [70] [Low-degree lower bounds via almost orthonormal bases](https://arxiv.org/abs/2509.09353)
*Alexandra Carpentier,Simone Maria Giancola,Christophe Giraud,Nicolas Verzelen*

Main category: stat.ML

TL;DR: 本文提出了一种新的低度多项式下界证明策略，通过构造在零分布下几乎正交的多项式基，解决了传统方法在估计任务和复杂检测问题中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统低度多项式方法在处理估计任务或具有复杂零分布的检测问题时失效，因为缺乏简单的L^2正交多项式族。现有技术解决方案实现复杂，需要更直接的证明策略。

Method: 针对随机图模型，在统计-计算间隙出现的区域构造几乎正交的多项式基，这些基在零分布下几乎正交，可以直接用于建立低度下界并识别最优低度准则的多项式。

Result: 该方法成功恢复了已知的低度下界，并为隐藏子团、随机块模型和序列模型等问题建立了新的下界，同时为设计最优多项式时间算法提供了见解。

Conclusion: 提出的几乎正交基方法为低度多项式下界提供了更直接和通用的证明框架，能够处理传统方法难以应对的复杂统计模型，为理解统计-计算间隙提供了新的工具。

Abstract: Low-degree polynomials have emerged as a powerful paradigm for providing
evidence of statistical-computational gaps across a variety of high-dimensional
statistical models [Wein25]. For detection problems -- where the goal is to
test a planted distribution $\mathbb{P}'$ against a null distribution
$\mathbb{P}$ with independent components -- the standard approach is to bound
the advantage using an $\mathbb{L}^2(\mathbb{P})$-orthonormal family of
polynomials. However, this method breaks down for estimation tasks or more
complex testing problems where $\mathbb{P}$ has some planted structures, so
that no simple $\mathbb{L}^2(\mathbb{P})$-orthogonal polynomial family is
available. To address this challenge, several technical workarounds have been
proposed [SW22,SW25], though their implementation can be delicate. In this
work, we propose a more direct proof strategy. Focusing on random graph models,
we construct a basis of polynomials that is almost orthonormal under
$\mathbb{P}$, in precisely those regimes where statistical-computational gaps
arise. This almost orthonormal basis not only yields a direct route to
establishing low-degree lower bounds, but also allows us to explicitly identify
the polynomials that optimize the low-degree criterion. This, in turn, provides
insights into the design of optimal polynomial-time algorithms. We illustrate
the effectiveness of our approach by recovering known low-degree lower bounds,
and establishing new ones for problems such as hidden subcliques, stochastic
block models, and seriation models.

</details>
