<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 21]
- [cs.LG](#cs.LG) [Total: 64]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Cross-device Zero-shot Label Transfer via Alignment of Time Series Foundation Model Embeddings](https://arxiv.org/abs/2509.06966)
*Neal G. Ravindra,Arijit Sehanobish*

Main category: eess.SP

TL;DR: 通过时间序列基础模型将不同设备数据投影到共享嵌入空间，采用对抗对齐技术实现标签跨设备转移


<details>
  <summary>Details</summary>
Motivation: 解决消费级可穿戴设备数据缺乏医学验证标签的问题，避免手动标注的高成本和不可扩展性

Method: 使用时间序列基础模型将源域（例如血流计）和目标域（例如Apple Watch）投影到共享嵌入空间，通过对抗对齐技术强制两者分布对齐

Result: 开发了新的对抗对齐框架，能够在不需要成对数据的情况下实现标签跨设备转移

Conclusion: 该方法为消费级可穿戴设备提供了一种可扩展的方式来获得高质量的医学标签，有力地解决了标签缺乏问题

Abstract: High-quality, medically validated labels exist for clinical actigraphy data
but not for ubiquitous consumer wearables like the Apple Watch. Manually
labeling wearables data is expensive and doesn't scale. This paper offers a
novel framework that transfers valuable labels from a source domain (e.g.,
actigraphy) to a target domain (e.g., Apple Watch) without requiring paired
data. Instead of working with raw time-series signals, we project both domains
into a shared latent embedding space using time-series foundation models
(TSFMs) and develop a new framework to align the cross-device representations.
Our method, Adversarial Alignment of TSFM Embeddings forces the distributions
of source and target embeddings to align within this space, facilitating label
transfer across device type.

</details>


### [2] [Cross-field SNR Analysis and Tensor Channel Estimation for Multi-UAV Near-field Communications](https://arxiv.org/abs/2509.06967)
*Tianyu Huo,Jian Xiong,Yiyan Wu,Songjie Yang,Bo Liu,Wenjun Zhang*

Main category: eess.SP

TL;DR: 本文研究了分布式近场多无人机通信系统的信道估计，提出了基于混合球面-平面波模型的两种算法：SD-OMP和tensor-OMP，其中tensor-OMP在保持性能的同时降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 6G网络中极大规模天线阵列(ELAA)需要工作在近场区域，传统远场平面波假设失效，需要开发适用于分布式多无人机系统的近场信道估计方法。

Method: 首先推导了三种模型(PWM、SWM、HSPWM)的闭式SNR表达式，发现HSPWM在建模精度和解析易处理性之间达到最佳平衡。基于此提出了SD-OMP算法（将极坐标域推广到联合考虑仰角、方位角和距离）和tensor-OMP算法（利用HSPWM下信道天然表示为张量的特性）。

Result: 仿真结果表明，tensor-OMP实现了与SD-OMP相当的归一化均方误差(NMSE)性能，同时提供了降低的计算复杂度和改进的可扩展性。

Conclusion: HSPWM模型为分布式近场多无人机通信系统提供了有效的信道建模框架，tensor-OMP算法在性能和复杂度之间取得了良好平衡，适用于大规模天线阵列系统。

Abstract: Extremely large antenna array (ELAA) is key to enhancing spectral efficiency
in 6G networks. Leveraging the distributed nature of multi-unmanned aerial
vehicle (UAV) systems enables the formation of distributed ELAA, which often
operate in the near-field region with spatial sparsity, rendering the
conventional far-field plane wave assumption invalid. This paper investigates
channel estimation for distributed near-field multi-UAV communication systems.
We first derive closed-form signal-to-noise ratio (SNR) expressions under the
plane wave model (PWM), spherical wave model (SWM), and a hybrid
spherical-plane wave model (HSPWM), also referred to as the cross-field model,
within a distributed uniform planar array (UPA) scenario. The analysis shows
that HSPWM achieves a good balance between modeling accuracy and analytical
tractability. Based on this, we propose two channel estimation algorithms: the
spherical-domain orthogonal matching pursuit (SD-OMP) and the tensor-OMP. The
SD-OMP generalizes the polar domain to jointly consider elevation, azimuth, and
range. Under the HSPWM, the channel is naturally formulated as a tensor,
enabling the use of tensor-OMP. Simulation results demonstrate that tensor-OMP
achieves normalized mean square error (NMSE) performance comparable to SD-OMP,
while offering reduced computational complexity and improved scalability.

</details>


### [3] [Deep Learning-based Techniques for Integrated Sensing and Communication Systems: State-of-the-Art, Challenges, and Opportunities](https://arxiv.org/abs/2509.06968)
*Murat Temiz,Yongwei Zhang,Yanwei Fu,Chi Zhang,Chenfeng Meng,Orhan Kaplan,Christos Masouros*

Main category: eess.SP

TL;DR: 深度学习基于ISAC系统的综述研究，讨论了DL技术在感知通信一体化中的优势和应用


<details>
  <summary>Details</summary>
Motivation: ISAC系统作为6G及更高网络的关键技术，需要解决传统迭代优化方法带来的计算复杂性问题，DL技术能提供高效近优解决方案

Method: 综述性研究方法，首先介绍DL架构和ISAC基础知识，然后分类评论现有的DL基于ISAC的先进技术

Result: DL技术能够在计算资源有限和低延迟要求下为ISAC系统提供高效解决方案，适用于波形设计、通道估计、感知信号处理等多种任务

Conclusion: DL基于ISAC技术具有重要优势，但仍面临挑战，需要进一步研究以推动该领域发展

Abstract: This article comprehensively reviews recent developments and research on deep
learning-based (DL-based) techniques for integrated sensing and communication
(ISAC) systems. ISAC, which combines sensing and communication functionalities,
is regarded as a key enabler for 6G and beyond networks, as many emerging
applications, such as vehicular networks and industrial robotics, necessitate
both sensing and communication capabilities for effective operation. A unified
platform that provides both functions can reduce hardware complexity, alleviate
frequency spectrum congestion, and improve energy efficiency. However,
integrating these functionalities on the same hardware requires highly
optimized signal processing and system design, introducing significant
computational complexity when relying on conventional iterative or
optimization-based techniques. As an alternative to conventional techniques,
DL-based techniques offer efficient and near-optimal solutions with reduced
computational complexity. Hence, such techniques are well-suited for operating
under limited computational resources and low latency requirements in real-time
systems. DL-based techniques can swiftly and effectively yield near-optimal
solutions for a wide range of sophisticated ISAC-related tasks, including
waveform design, channel estimation, sensing signal processing, data
demodulation, and interference mitigation. Therefore, motivated by these
advantages, recent studies have proposed various DL-based approaches for ISAC
system design. After briefly introducing DL architectures and ISAC
fundamentals, this survey presents a comprehensive and categorized review of
state-of-the-art DL-based techniques for ISAC, highlights their key advantages
and major challenges, and outlines potential directions for future research.

</details>


### [4] [Modeling the Doppler Shift in Cislunar Environment with Gaussian Mixture Models](https://arxiv.org/abs/2509.07134)
*Baris Donmez,Sebastien Loranger,Gunes Karabulut Kurt*

Main category: eess.SP

TL;DR: 这篇论文研究月球南极占卫星间链路的多普勒频移分布特征，利用高斯混合模型描述不同倾角下的频移分布规律，最大频移达到±1.89ppm。


<details>
  <summary>Details</summary>
Motivation: 分析月球南极占卫星间链路的多普勒频移分布特征，为月球导航系统的通信设计提供重要参考。

Method: 采用高斯混合模型(GMM)来描述不同轨道倾角间隔为1度的卫星间链路多普勒频移分布，并使用KL散度和权重相对差异(WMRD)指标评估模型拟合效果。

Result: 模拟结果显示，当参考轨道倾角为80度时，卫星间链路多普勒频移最大达到±1.89ppm。GMM模型的WMRD误差最大为0.6575，KL散度最大为2.2963。

Conclusion: 高斯混合模型能够有效描述月球南极占卫星间链路的多普勒频移分布，为月球导航通信系统的频率稳定性分析提供了重要技术支持。

Abstract: This study investigates the RF-based Doppler shift distribution
characterization of the Lunar South Pole (LSP) based inter-satellite link (ISL)
in varying inclination. Doppler shift in parts per million (ppm) is determined
and analyzed, as it provides an independence from the carrier frequency. Due to
unknown relative velocity states duration, the Gaussian Mixture Model (GMM) is
found to be the best fitting distribution for ISLs with $1^\circ$ inclination
interval Doppler shift with respect to a predetermined satellite.
Goodness-of-fit is investigated and quantified with Kullback-Leibler (KL)
divergence and weighted mean relative difference (WMRD) error metrics.
Simulation results show that ISL Doppler shifts reach up to $\pm1.89$ ppm as
the inclination of the other orbit deviates higher from the reference orbit,
inclining $80^\circ$. Regarding the error measurements of GMM fitting, the WMRD
and KL divergence metrics for ISL take values up to 0.6575 and 2.2963,
respectively.

</details>


### [5] [Impact of Fading Correlation on the High-SNR Regime of Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2509.07172)
*Paula Isabel Tilleria Lucero,Bryan Fernando Sarango Rodríguez,Fernando Darío Almeida García,José Cândido Silveira Santos Filho*

Main category: eess.SP

TL;DR: 本文通过精确的近似分析解决了RIS辅助无线系统中的三个关键问题：固定多样性增益传播环境、限制空间相关性配置和高SNR区域近似失效问题。研究发现相关性会导致SNR分布在对数-dB尺度下的水平偏移，而角度系数保持不变。


<details>
  <summary>Details</summary>
Motivation: 充分考虑一般性的相关性配置和变化多样性增益的衰落环境，充分揭示高SNR区域下系统性能的尾部分布特性。

Method: 采用任意相关的Nakagami-m衰落通道模型，对SNR分布的左尾进行精确的近似分析。

Result: 相关性会在对数-dB尺度下导致PDF和CDF的水平偏移，线性系数量化这种偏移而角度系数不变。线性系数对相关性高度敏感，反映了所有通道相关特性的累积效果。

Conclusion: 该分析方法能够有效揭示RIS辅助系统在高SNR区域的性能特性，为设计和优化提供了重要的理论基础。

Abstract: This paper addresses three critical limitations in previous analyses of
RIS-aided wireless systems: propagation environments with fixed diversity gain,
restricted spatial correlation profiles, and approximation methods that fail to
capture the system behavior in the high signal-to-noise ratio (SNR) regime. To
overcome these challenges, we conduct an exact asymptotic analysis focused on
the left tail of the SNR distribution, which plays a critical role in high-SNR
system performance. Additionally, to account for general correlation profiles
and fading environments with variable diversity and coding gains, we consider
arbitrarily correlated Nakagami-m fading channels. The analytical results show
that fading correlation induces a horizontal shift in the asymptotic behavior
-- represented as a straight line in the log-dB scale -- of the PDF and CDF,
displacing these curves to the left. The asymptotic linear coefficient
quantifies this shift, while the angular coefficient remains unaffected.
Moreover, the results reveal that the high sensitivity of the linear
coefficient to correlation arises from the aggregated contribution of all
marginal asymptotic terms, effectively capturing each channel's correlation
characteristics.

</details>


### [6] [Joint Spatial and Spectral Hybrid Precoding for Multi-User MIMO-OFDM Systems](https://arxiv.org/abs/2509.07229)
*Navid Reyhanian,Reza Ghaderi Zefreh,Parisa Ramezani,Emil Björnson*

Main category: eess.SP

TL;DR: 本文提出了一种基于WMMSE-BCD的混合预编码优化方法，用于解决毫米波MU-MIMO-OFDM系统中的PAPR、OOB发射和相位误差问题，实现了下行链路和速率最大化。


<details>
  <summary>Details</summary>
Motivation: 毫米波MIMO系统受硬件限制无法仅依赖数字预编码，混合预编码成为替代方案。但OFDM存在高PAPR、OOB发射问题，且实际系统中的相位偏移器损伤导致相位误差，需要解决这些挑战。

Method: 采用加权最小均方误差(WMMSE)和块坐标下降(BCD)方法，迭代优化发射端的数字-RF预编码器和用户端的数字-RF组合器，提出了低成本和可扩展的优化方法来解决BCD子问题。

Result: 大量仿真结果表明，所提方法效率高，相对于知名基准方法表现出优越性能。

Conclusion: 该方法有效解决了毫米波混合MIMO-OFDM系统中的预编码优化问题，在满足发射功率、PAPR和OOB发射约束的同时实现了和速率最大化。

Abstract: The deployment of millimeter wave (mmWave) multiple-input multiple-output
(MIMO) systems cannot rely solely on digital precoding due to hardware
constraints. Instead, hybrid precoding, which combines digital and radio
frequency (RF) techniques, has emerged as a potential alternative. This
approach strikes a balance between performance and cost, addressing the
limitations of signal mixers and analog-to-digital converters in mmWave
systems. mmWave systems are designed to function in wideband channels with
frequency selectivity, necessitating the use of orthogonal frequency-division
multiplexing (OFDM) to mitigate dispersive channels. However, OFDM faces
several challenges. First, it suffers from a high peak-to-average power ratio
(PAPR) due to the linear combination of subcarriers. Second, it suffers from
out-of-band (OOB) emissions due to the sharp spectral transitions of OFDM
subcarriers and windowing-induced spectral leakage. Furthermore, phase shifter
(PS) impairments at the RF transmitter precoder and the user combiner represent
a limitation in practical mmWave systems, leading to phase errors. This work
addresses these challenges.
  We study the problem of robust digital-RF precoding optimization for the
downlink sum-rate maximization in hybrid multi-user (MU) MIMO-OFDM systems
under maximum transmit power, PAPR, and OOB emission constraints. The
formulated maximization problem is non-convex and difficult to solve. We
propose a weighted minimum mean squared error (WMMSE) based block coordinate
descent (BCD) method to iteratively optimize digital-RF precoders at the
transmitter and digital-RF combiners at the users. Low-cost and scalable
optimization approaches are proposed to efficiently solve the BCD subproblems.
Extensive simulation results are conducted to demonstrate the efficiency of the
proposed approaches and exhibit their superiority relative to well-known
benchmarks.

</details>


### [7] [Experimental Analysis of Biasing Voltage Generation in Wave-Controlled RIS](https://arxiv.org/abs/2509.07293)
*Miguel Saavedra-Melo,Benjamin Bradshaw,Vanessa Yao,Ender Ayanoglu,Lee Swindlehurst,Filippo Capolino*

Main category: eess.SP

TL;DR: 本文提出了一种基于波控技术的可重构智能表面(RIS)设计，通过传输线上的立波生成偏置电压，实现了单频波单束扩射控制，降低了复杂度和体积。


<details>
  <summary>Details</summary>
Motivation: 传统RIS设计复杂度高、物理尺寸大，需要简化设计以适应下一代无线通信系统的要求。

Method: 设计基于波控技术的RIS和偏置传输线，利用传输线上的立波生成所需的直流偏置电压，通过理论模型和实验验证分析效果。

Result: 实验表明该方法能够从单个立波频率生成正确的直流偏置，发现了理论未考虑的阻抗匹配依赖性，并实现了靠近平行方向的单束扩射控制。

Conclusion: 波控RIS技术通过简化偏置系统设计，为下一代无线通信系统提供了一种有前景的解决方案，展示了单频波控制的可行性。

Abstract: Reconfigurable intelligent surfaces (RISs), an emerging technology proposed
for inclusion in next generation wireless communication systems, are
programmable surfaces that can adaptively reflect incident electromagnetic
radiation in different desired directions. To reduce the complexity and
physical profile of conventional RIS designs, a novel concept known as
Wave-Controlled RIS has been proposed, in which standing waves along a
transmission line are used to generate the required dc bias for reflective
control. This paper shows the design of such a Wave-Controlled RIS and its
biasing transmission line. The effectiveness of this approach in generating the
correct dc bias from a single standing wave frequency is analyzed through both
theoretical modeling and experimental validation, which uncovered a dependence
on impedance matching not accounted for by the theory. Additionally, the
potential for reflective control using only a single standing wave frequency on
the biasing transmission line is explored, demonstrating the ability of
single-beam steering toward angles near broadside.

</details>


### [8] [Eye Movement Feature-Guided Signal De-Drifting in Electrooculography Systems](https://arxiv.org/abs/2509.07416)
*Lianming Hu,Xiaotong Zhang,Kamal Youcef-Toumi*

Main category: eess.SP

TL;DR: 提出眼动特征引导去漂移(FGD)方法，有效抑制EOG信号中的基线漂移，显著提高眼动追踪精度


<details>
  <summary>Details</summary>
Motivation: EOG信号中的低频噪声引起的基线漂移严重影响眼动追踪精度，给传感器融合带来挑战

Method: 利用主动眼动特征识别重构特征提取的EOG基线，自适应校正信号漂移同时保持波形形态完整性

Result: 仿真数据平均误差降低36.29%至0.896°，真实数据平均误差降低26.53%至1.033°，优于传统去漂移技术

Conclusion: FGD方法在真实噪声环境下仍能有效工作，显著提升人机协作中眼动追踪性能，具有实际应用价值

Abstract: Electrooculography (EOG) is widely used for gaze tracking in Human-Robot
Collaboration (HRC). However, baseline drift caused by low-frequency noise
significantly impacts the accuracy of EOG signals, creating challenges for
further sensor fusion. This paper presents an Eye Movement Feature-Guided
De-drift (FGD) method for mitigating drift artifacts in EOG signals. The
proposed approach leverages active eye-movement feature recognition to
reconstruct the feature-extracted EOG baseline and adaptively correct signal
drift while preserving the morphological integrity of the EOG waveform. The FGD
is evaluated using both simulation data and real-world data, achieving a
significant reduction in mean error. The average error is reduced to
0.896{\deg} in simulation, representing a 36.29% decrease, and to 1.033{\deg}
in real-world data, corresponding to a 26.53% reduction. Despite additional and
unpredictable noise in real-world data, the proposed method consistently
outperforms conventional de-drifting techniques, demonstrating its
effectiveness in practical applications such as enhancing human performance
augmentation.

</details>


### [9] [Multi-Modal Intelligent Channel Modeling Framework for 6G-Enabled Networked Intelligent Systems](https://arxiv.org/abs/2509.07422)
*Lu Bai,Zengrui Han,Xuesong Cai,Xiang Cheng*

Main category: eess.SP

TL;DR: 本文提出了一种新的多模态智能通道建模框架（MMICM），通过机器聚感技术实现多模态感知与通道特性的非线性映射，为6G网络智能系统提供准确的实时通道模型。


<details>
  <summary>Details</summary>
Motivation: 传统通道建模方法在6G网络智能系统的新需求下面临许多限制，而智能设备配备的多模态传感器为通信与感知的智能集成提供了新机遇。

Method: 提出多模态智能通道建模框架，基于机器聚感技术建立多模态感知与大小规模通道特性之间的非线性模型，利用AI技术实现。

Result: 详细阐述了所提框架的架构和特征，分析了关键技术，并突出了系统应用和潜在研究方向。

Conclusion: 该智能通道建模框架为6G网络智能系统提供了一种有效的解决方案，通过通信与感知的智能集成实现了更准确的实时通道模型。

Abstract: The design and technology development of 6G-enabled networked intelligent
systems needs an accurate real-time channel model as the cornerstone. However,
with the new requirements of 6G-enabled networked intelligent systems, the
conventional channel modeling methods face many limitations. Fortunately, the
multi-modal sensors equipped on the intelligent agents bring timely
opportunities, i.e., the intelligent integration and mutually beneficial
mechanism between communications and multi-modal sensing could be investigated
based on the artificial intelligence (AI) technologies. In this case, the
mapping relationship between physical environment and electromagnetic channel
could be explored via Synesthesia of Machines (SoM). This article presents a
novel multi-modal intelligent channel modeling (MMICM) framework for 6G-enabled
networked intelligent systems, which establishes a nonlinear model between
multi-modal sensing and channel characteristics, including large-scale and
small-scale channel characteristics. The architecture and features of proposed
intelligent modeling framework are expounded and the key technologies involved
are also analyzed. Finally, the system-engaged applications and potential
research directions of MMICM framework are outlined.

</details>


### [10] [Spectrotemporal Feature Extraction in EHG Signals and Tocograms for Enhanced Preterm Birth Prediction](https://arxiv.org/abs/2509.07432)
*Senith Jayakody,Kalana Jayasooriya,Sashini Liyanage,Roshan Godaliyadda,Parakrama Ekanayake,Chathura Rathnayake*

Main category: eess.SP

TL;DR: 这篇论文提出了一种基于电宫图(EHG)的机器学习流程，通过KL变换除噪和生理相关特征提取，实现了高达97.28%的早产预测准确率。


<details>
  <summary>Details</summary>
Motivation: 早产是新生儿死亡的主要原因，早期识别至关重要。虽然EHG和TOCO是有前晨的无创预测工具，但以往研究存在类不平衡、过量重采样和特征生理相关性不足等问题。

Method: 采用Mel频率候系数、小波系数统计描述符和标准化力谱峰值提取EHG特征，通过Karhunen-Loève变换(KLT)进行子空间分解除噪。评估多种分类器包括Logistic回归、SVM、随机森林、核度提升、多层感知机和CatBoost。

Result: CatBoost分类器在KLT除噪后在TPEHGT数据集上达到97.28%的准确率和0.9988的AUC值。切除实验证明KLT除噪和生理相关特征都至关重要。TOCO信号并没有显著提升预测性能。

Conclusion: 结合除噪技术和领域相关特征可以建立高准确、健壁且临床可解释的早产预测模型，为低资源医疗环境提供价格合理的解决方案。

Abstract: Preterm birth (PTB), defined as delivery before 37 weeks of gestation, is a
leading cause of neonatal mortality and long term health complications. Early
detection is essential for enabling timely medical interventions.
Electrohysterography (EHG) and tocography (TOCO) are promising non invasive
tools for PTB prediction, but prior studies often suffer from class imbalance,
improper oversampling, and reliance on features with limited physiological
relevance. This work presents a machine learning pipeline incorporating robust
preprocessing, physiologically grounded feature extraction, and rigorous
evaluation. Features were extracted from EHG (and TOCO) signals using Mel
frequency cepstral coefficients, statistical descriptors of wavelet
coefficients, and peaks of the normalized power spectrum. Signal quality was
enhanced via Karhunen Lo\`eve Transform (KLT) denoising through eigenvalue
based subspace decomposition. Multiple classifiers, including Logistic
Regression, Support Vector Machines, Random Forest, Gradient Boosting,
Multilayer Perceptron, and CatBoost, were evaluated on the TPEHGT dataset. The
CatBoost classifier with KLT denoising achieved the highest performance on
fixed interval segments of the TPEHGT dataset, reaching 97.28% accuracy and an
AUC of 0.9988. Ablation studies confirmed the critical role of both KLT
denoising and physiologically informed features. Comparative analysis showed
that including TOCO signals did not substantially improve prediction over EHG
alone, highlighting the sufficiency of EHG for PTB detection. These results
demonstrate that combining denoising with domain relevant features can yield
highly accurate, robust, and clinically interpretable models, supporting the
development of cost effective and accessible PTB prediction tools, particularly
in low resource healthcare settings.

</details>


### [11] [SA-OOSC: A Multimodal LLM-Distilled Semantic Communication Framework for Enhanced Coding Efficiency with Scenario Understanding](https://arxiv.org/abs/2509.07436)
*Feifan Zhang,Yuyang Du,Yifan Xiang,Xiaoyan Liu,Soung Chang Liew*

Main category: eess.SP

TL;DR: SA-OOSC是一个基于多模态大语言模型的知识蒸馏语义通信框架，通过场景感知的重要性分配实现高效语义编码，解决了现有对象导向语义通信系统中静态重要性分配的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有对象导向语义通信(OOSC)系统的关键限制——无论对象的上下文相关性如何，都为特定类别的对象分配静态重要性值。需要开发能够根据上下文动态分配编码资源的系统。

Method: 利用多模态大语言模型(MLLM)识别图像中对象的场景增强语义重要性，通过MLLM标注数据进行知识蒸馏，使向量化/反向量化网络和JSCC编码器/解码器学会基于上下文重要性动态分配编码资源。核心创新包括MLLM引导的知识蒸馏流程、重要性加权的变长JSCC框架和促进知识蒸馏的新型损失函数设计。

Result: 实验验证表明该框架在编码效率上优于传统语义通信系统，并建立了开源的MLLM标注和人工验证数据集作为语义通信研究的新基准。

Conclusion: SA-OOSC框架成功实现了基于场景感知的动态重要性分配，显著提高了语义通信的编码效率，为未来语义通信研究提供了新的技术方向和基准数据集。

Abstract: This paper introduces SA-OOSC, a multimodal large language models
(MLLM)-distilled semantic communication framework that achieves efficient
semantic coding with scenario-aware importance allocations. This approach
addresses a critical limitation of existing object-oriented semantic
communication (OOSC) systems - assigning static importance values to specific
classes of objects regardless of their contextual relevance. Our framework
utilizes MLLMs to identify the scenario-augmented (SA) semantic importance for
objects within the image. Through knowledge distillation with the
MLLM-annotated data, our vectorization/de-vectorization networks and JSCC
encoder/decoder learn to dynamically allocate coding resources based on
contextual significance, i.e., distinguishing between high-importance objects
and low-importance according to the SA scenario information of the task. The
framework features three core innovations: a MLLM-guided knowledge distillation
pipeline, an importance-weighted variable-length JSCC framework, and novel loss
function designs that facilitate the knowledge distillation within the JSCC
framework. Experimental validation demonstrates our framework's superior coding
efficiency over conventional semantic communication systems, with open-sourced
MLLM-annotated and human-verified datasets established as new benchmarks for
future research in semantic communications.

</details>


### [12] [Node Position Estimation in Diffusion-Based Molecular Communications Using Multi-Layer Perceptron](https://arxiv.org/abs/2509.07441)
*Sangjun Hwang,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 通过设计专门节点结构和多位置发射机制，采用多层感知器模型在分子通信环境中精确估计未知节点间的相对位置


<details>
  <summary>Details</summary>
Motivation: 解决分子通信环境中节点定位精度不高的问题，提高相对位置估计的准确性

Method: 设计专门节点结构（中央吸收收收机+多个发射器），采用预定球坐标系统，释放导航分子并测量其吸收时间和浓度，通过球坐标空间分区将这些空间特征作为多层感知器模型的输入

Result: 方法显著提高了距离和方向估计的精度，模拟结果证明了定位准确性，神经网络模型能够有效捐描基础物理特征

Conclusion: 该方法在分子通信领域中实现了高精度的相对位置估计，神经网络模型在捐描物理环境特性方面显示出良好效果

Abstract: This paper proposes a method for accurately estimating the relative position
between two nodes with unknown locations in a diffusion-based molecular
communication environment. A specialized node structure is designed, combining
a central absorbing receiver with multiple transmitters placed at predefined
spherical coordinates. Pilot molecules are released, and their absorption time
and concentration are measured. By partitioning the spherical coordinate space,
these spatially distinct measurements serve as input to a multilayer perceptron
(MLP)-based model. The proposed method significantly improves the precision of
distance and direction estimation. Simulation results demonstrate localization
accuracy, confirming the effectiveness of the neural network model in capturing
the underlying physical characteristics.

</details>


### [13] [A Systematic Framework to Test the Resilience of Three-Fold Redundant Sparse Arrays Against Two Sensor Failures and Some Never-Before Findings](https://arxiv.org/abs/2509.07442)
*Ashish Patwari,Andrés Alayón Glazunov*

Main category: eess.SP

TL;DR: 这篇论文提出了一种系统框架来评估三重冗余稀疏线性阵列的稳健性，对抗任意两个传感器故障的能力。


<details>
  <summary>Details</summary>
Motivation: 虽然多重冗余稀疏阵列能够承受多个传感器故障，但现有的三重冗余阵列仍然存在隐藏的依赖关系，影响其完全稳健性。

Method: 提出了一种系统化的分析框架，用于评估三重冗余稀疏线性阵列在所有可能两个传感器故障情况下的稳健性。

Result: 发现现有的代表性三重冗余阵列存在一些特定传感器对故障时的隐藏漏洞，不能完全承受任意两个传感器失效。

Conclusion: 该研究提供了一种客观评估任何现有或未来三重冗余稀疏阵列稳健性的方法，具有重要的档案价值，并提供了MATLAB程序和数值模拟以便阵列处理社区使用。

Abstract: As the field of sparse arrays progressed, numerous array designs have been
introduced with a focus on larger apertures and higher degrees of freedom
(DOFs), resulting in maximally economic sparse arrays (MESAs) that operate with
the least number of sensors required to provide a given aperture while ensuring
a hole-free difference coarray (DCA). Consequently, MESAs are least robust to
sensor failures and cannot afford the failure of even a single sensor.
Multifold redundant sparse arrays (MFRSAs) provide a practical solution to the
problem of sensor failures in sparse arrays by making sure that the array
contains enough sensor pairs necessary to produce each spatial lag multiple
times. Owing to this property, a \b{eta}-fold redundant array can withstand
simultaneous failure of at least \b{eta}-1 sensors without losing the hole-free
DCA property. Nevertheless, MFRSAs are also prone to hidden dependencies that
prevent them from being fully robust. In this work, we present a systematic
framework to evaluate the robustness of triple redundant sparse linear arrays
(TRSLAs) against all possible two-sensor failures. After detailing the proposed
approach, we present the failure analysis of representative TRSLAs available in
existing literature. It is found that existing TRSLAs have some hidden
vulnerabilities against the failure of some peculiar sensor pairs.
Corresponding MATLAB programs and numerical simulations are provided for
evaluation and use by the array processing community. The proposed approach has
a great archival value as it can evaluate the robustness of any present or
future TRSLAs through objective means.

</details>


### [14] [Integrated Communication and Computing in Time-Varying mmWave Channels](https://arxiv.org/abs/2509.07482)
*Joan Çollaku,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Takumi Takahashi*

Main category: eess.SP

TL;DR: 提出一种新的时变毫米波通信计算集成发射机架构，通过并行渡道跟踪和数据检测来应对渡道变化，以实现高效的空中计算操作。


<details>
  <summary>Details</summary>
Motivation: 现有的通信计算集成方案假设完美渡道知识，无法有效处理时变毫米波渡道的动态性。

Method: 使用双线性高斯信念传播算法(BiGaBP)进行联合渡道和数据检测(JCDE)，结合渡道预测算法，并通过殊余信号的最优组合来执行空中计算操作。

Result: 模拟结果显示该方案在具有挑战性的时变毫米波渡道中有效执行通信计算集成，通信和计算性能适度降低。

Conclusion: 该框构能够在不依赖完美渡道知识的情况下，在动态渡道环境中实现高效的通信与计算集成。

Abstract: We propose a novel framework for integrated communication and computing (ICC)
transceiver design in time-varying millimeter-wave (mmWave) channels. In
particular, in order to cope with the dynamics of time-varying mmWave channels,
the detection of communication symbols and the execution of an over-the-air
computing (AirComp) operation are performed in parallel with channel tracking,
as opposed to existing state-of-the-art (SotA) on ICC where perfect knowledge
of the channel at all time instances is typically assumed. For clarity of
exposition, we consider a single-input multiple-output (SIMO) uplink scenario
where multiple single-antenna user equipment (UE) transmit to a base station
(BS) equipped with multiple antennas, such that each UE, or edge device (ED),
precodes its own transmit signal, while the BS, or access points (APs), also
performs receive beamforming. The proposed transceiver framework then estimates
channel state information (CSI) and data symbols in parallel, using a bilinear
Gaussian belief propagation (BiGaBP) algorithm for joint channel and data
detection (JCDE), aided by a channel prediction (CP) algorithm executed before
each estimation window at the BS. The AirComp operation is then executed by
means of an optimal combination of the residual signal. Simulation results
demonstrate the effectiveness of the proposed scheme in performing ICC in
challenging time-varying mmWave channels, with minimal degradation to both
communication and computing performance.

</details>


### [15] [A Methodological Framework for Positioning of Wireless Sensors in New Generation Launchers](https://arxiv.org/abs/2509.07483)
*Ivan Iudice,Domenico Pascarella,Sonia Zappia,Giovanni Cuciniello,Hernan M. R. Giannetta,Marta Albano,Enrico Cavallini*

Main category: eess.SP

TL;DR: 这篇论文提出了一种用于可重复使用笔车无线传感器网络设计的方法论框架，重点关注电磁环境分析和网络配置优化。


<details>
  <summary>Details</summary>
Motivation: 电磁字符化和电磁兼容性分析对可重复使用笔车无线传感器网络至关重要，因为运营场景复杂且动态。

Method: 基于网络节点的初步定位，提出了一个工作流程和相关工具集，用于确定最优网络拓扑结构，重点考虑重量、发射接收机操作和总辐射功率。

Result: 使用计算电磁学策略模拟最优网络配置，以评估传感器网络本身引起的电磁环境，并提供了特定笔车的案例研究结果。

Conclusion: 该方法论框架能够有效地进行无线传感器网络设计和电磁环境分析，适用于复杂的笔车运营场景。

Abstract: In wireless sensor networks for reusable launchers, the electromagnetic
characterization and electromagnetic compatibility analyses are relevant due to
the reference operational scenario, which implies a complex, and sometimes
dynamic, electromagnetic environment. This work proposes a methodological
framework for the design of the network and for the analysis of the related
electromagnetic environment within the stages of a given launcher. Based on the
preliminary positioning of the network nodes, the framework prescribes a
workflow and the related toolset for determining the optimal network topology
focusing on the weights, the operation of the transceivers, and the overall
radiated power. The optimal network configuration is simulated by using
computational electromagnetics strategies in order to assess the
electromagnetic environment induced by the sensor network itself. The paper
provides some results concerning a case study for a specific launcher.

</details>


### [16] [Joint Antenna Positioning and Beamforming for Movable Antenna Array Aided Ground Station in Low-Earth Orbit Satellite Communication](https://arxiv.org/abs/2509.07511)
*Jinming Wang,Lipeng Zhu,Shuai Han,He Sun,Rui Zhang*

Main category: eess.SP

TL;DR: 本文提出一种基于可移动天线数组的低轨道卫星地面站新架构，通过优化天线位置和波束成型来提高通信速率和减少干扰。


<details>
  <summary>Details</summary>
Motivation: 传统固定位天线在超密集低轨道卫星网络中干扰缓解效果有限，需要更有效的干扰缓解方案来提高通信性能。

Method: 使用可移动天线数组灵活调整天线位置，在初始化阶段配置天线位置并在整个通信期间保持不变。采用拉格朗日对偶变换和二次变换重构目标函数，并开发基于坐标递减的迭代算法交替优化天线位置向量和天线权重向量。

Result: 模拟结果显示，提出的可移动天线方案在各种系统设置下都显著超过传统固定位天线，大幅提高了地面站的可实现速率。

Conclusion: 该方案为未来超密集低轨道卫星通信网络提供了一种高效的干扰缓解决方案，通过动态调整天线位置来改善通信性能。

Abstract: This paper proposes a new architecture for the low-earth orbit (LEO)
satellite ground station aided by movable antenna (MA) array. Unlike
conventional fixed-position antenna (FPA), the MA array can flexibly adjust
antenna positions to reconfigure array geometry, for more effectively
mitigating interference and improving communication performance in ultra-dense
LEO satellite networks. To reduce movement overhead, we configure antenna
positions at the antenna initialization stage, which remain unchanged during
the whole communication period of the ground station. To this end, an
optimization problem is formulated to maximize the average achievable rate of
the ground station by jointly optimizing its antenna position vector (APV) and
time-varying beamforming weights, i.e., antenna weight vectors (AWVs). To solve
the resulting non-convex optimization problem, we adopt the Lagrangian dual
transformation and quadratic transformation to reformulate the objective
function into a more tractable form. Then, we develop an efficient block
coordinate descent-based iterative algorithm that alternately optimizes the APV
and AWVs until convergence is reached. Simulation results demonstrate that our
proposed MA scheme significantly outperforms traditional FPA by increasing the
achievable rate at ground stations under various system setups, thus providing
an efficient solution for interference mitigation in future ultra-dense LEO
satellite communication networks.

</details>


### [17] [Asymmetric Modulation Design for Fluid-Antenna SWIPT Systems](https://arxiv.org/abs/2509.07610)
*Ahsan Mehmood,Ioannis Krikidis,Ghassan M. Kraidy*

Main category: eess.SP

TL;DR: 提出了改进流体天线辅助SWIPT系统速率-能量区域的调制方案设计，通过考虑能量收集电路的非线性特性，优化星座图来同时最大化离散输入互信息和收集电流


<details>
  <summary>Details</summary>
Motivation: 现有的SWIPT系统在速率-能量区域性能有限，需要设计新的调制方案来同时优化信息传输和能量收集性能，特别是考虑实际能量收集电路的非线性特性

Method: 采用epsilon约束方法解决双目标速率-能量区域优化问题，为不同能量收集阈值设计优化星座图，并评估三种流体天线端口选择策略（最佳端口、固定端口、随机端口）的性能

Result: 仿真结果显示优化星座图相比传统星座图在信息速率和能量收集方面都取得了显著的性能提升

Conclusion: 所提出的优化调制方案能够有效改善流体天线辅助SWIPT系统的速率-能量区域性能，为实际SWIPT系统设计提供了有效的解决方案

Abstract: In this work, we propose the design of modulation schemes that improve the
rate-energy region of fluid antenna-assisted simultaneous wireless information
and power transfer (SWIPT) systems. By considering the nonlinear
characteristics of practical energy harvesting circuits, we formulate a
dual-objective rate-energy (RE) region optimization problem to jointly maximize
the discrete-input mutual information (DIMI) and harvested current. The problem
is solved using the epsilon-constraint method and optimized constellations are
designed for various energy harvesting thresholds. We then evaluate the
performance of the optimized constellations under three different fluid antenna
(FA) port selection strategies: (i) Best Port, (ii) Fixed Port, and (iii)
Random Port. Our simulation results demonstrate significant performance gains
of optimized constellations over conventional constellations in both
information rate and energy harvesting.

</details>


### [18] [Interference Mitigation for OFDM-based Integrated Sensing and Communications with Arbitrary Modulation Formats](https://arxiv.org/abs/2509.07754)
*Felix Artmann,Daniel Gil Gaviria,Benedikt Geiger,Laurent Schmalen*

Main category: eess.SP

TL;DR: 本文分析了6G网络中通信信号用于感知时不同调制方式对性能的影响，提出了改进的干扰消除算法，在保持高效通信的同时实现了接近最优的感知性能。


<details>
  <summary>Details</summary>
Motivation: 未来移动网络需要实现通信与感知的深度融合，利用通信信号进行感知可以提高系统效率并支持新应用。但现有通信信号的非恒定包络特性会影响感知性能，需要研究如何平衡通信效率和感知性能。

Method: 分析了任意调制字母表对OFDM系统感知性能的影响，评估了现有干扰抑制技术（如相干连续目标消除），并提出了该算法的增强版本。在多目标场景下进行系统性性能评估，包括散射效应的影响。

Result: 提出的干扰抑制方法在使用高阶星座实现更高效通信的同时，能够达到与感知最优的恒定包络信号相当的性能表现。

Conclusion: 通过改进的干扰消除技术，可以在使用高阶调制实现高效通信的同时，保持接近最优的感知性能，为6G网络中通信与感知的融合提供了有效解决方案。

Abstract: Integrated sensing and communication will be a key feature of future mobile
networks, enabling highly efficient systems and numerous new applications by
leveraging communication signals for sensing. In this paper, we analyze the
impact of arbitrary modulation alphabets on the sensing performance of
communication-centric OFDM systems as expected in the next-generation 6G
networks. We evaluate existing interference mitigation techniques, such as
coherent successive target cancellation, and propose an enhanced version of
this algorithm. A systematic performance evaluation in multi-target scenarios,
including the effects of scattering, demonstrates that our proposed
interference mitigation methods achieve performance comparable to
sensing-optimal constant modulus signals while utilizing higher order
constellations for more efficient communications.

</details>


### [19] [Experimental Evaluation of Joint Clock Recovery and Equalization for Sub-Terahertz Links](https://arxiv.org/abs/2509.07758)
*Pietro Savazzi,Anna Vizziello,Sherif Badran,Josep M. Jornet*

Main category: eess.SP

TL;DR: 提出并实验验证了一种针对高速亚太赫兹无线通信链路的联合时钟恢复和均衡架构，结合CMA均衡器和盲定时误差检测器，在140GHz测试中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 为下一代星载通信系统开发能够在挑战性同步约束下实现超高速率通信的亚太赫兹宽带链路同步解决方案

Method: 采用波特间隔数字接收机架构，结合CMA均衡器和盲定时误差检测器(TED)，利用CMA滤波器系数估计定时误差，驱动Farrow插值器

Result: 在140GHz无线测试平台上，16-QAM调制10GHz带宽下，提出的TED方案在BER、EVM和ISI抑制方面优于Gardner和Mueller & Müller等传统盲TED

Conclusion: 该联合架构为亚太赫兹宽带通信提供了有效的同步解决方案，特别适用于星间和深空通信等具有挑战性同步约束的场景

Abstract: This paper proposes and experimentally evaluates a joint clock recovery (CR)
and equalization architecture tailored for high-speed sub-terahertz (sub-THz)
wireless communication links. Specifically, a Baud-spaced digital receiver
architecture is investigated that combines a constant modulus algorithm (CMA)
equalizer with a blind timing error detector (TED), enabling robust symbol
timing synchronization without decision-directed (DD) feedback or pilot
symbols. The proposed TED leverages the CMA filter coefficients to estimate
timing errors, which are then used to drive a Farrow interpolator operating at
twice the symbol rate. The system is validated experimentally using a 140~GHz
wireless testbed with 16-QAM modulation over a 10~GHz bandwidth. Results show
that the proposed TED schemes outperform conventional blind TEDs, such as
Gardner and blind implementations of Mueller \& M\"uller, in terms of bit error
rate (BER), error vector magnitude (EVM), and intersymbol interference (ISI)
suppression. These capabilities are especially relevant to next-generation
spaceborne communication systems, where wideband sub-THz links are expected to
play a key role in enabling ultra-high-data-rate inter-satellite and deep-space
communications under challenging synchronization constraints.

</details>


### [20] [Sensing with Mobile Devices through Radio SLAM: Models, Methods, Opportunities, and Challenges](https://arxiv.org/abs/2509.07775)
*Yu Ge,Ossi Kaltiokallio,Elizaveta Rastorgueva-Foi,Musa Furkan Keskin,Hui Chen,Guillaume Jornod,Jukka Talvitie,Mikko Valkama,Frank Hofmann,Henk Wymeersch*

Main category: eess.SP

TL;DR: 通信感知一体化中的无线电SLAM技术，通过无线电信号实现同时定位和地图构建，为6G自主系统和工业机器人提供标准化解决方案。


<details>
  <summary>Details</summary>
Motivation: 通信感知一体化(ISAC)是6G的核心技术之一，需要对无线电SLAM技术进行全面分析，以支持同时实现环境感知和通信功能。

Method: 分析不同频段带下的无线电SLAM技术，讨论覆盖范围、分辨率和硬件要求等权衡因素，并探讨与感知、定位和协作网络的集成机会。

Result: 研究揭示了不同频段带在无线电SLAM中的性能特征和应用潜力，为技术选型提供了理论基础。

Conclusion: 无线电SLAM作为ISAC的关键技术途径，为6G应用如自主系统和工业机器人开启了标准化解决方案的发展道路。

Abstract: The integration of sensing and communication (ISAC) is a cornerstone of 6G,
enabling simultaneous environmental awareness and communication. This paper
explores radio SLAM (simultaneous localization and mapping) as a key ISAC
approach, using radio signals for mapping and localization. We analyze radio
SLAM across different frequency bands, discussing trade-offs in coverage,
resolution, and hardware requirements. We also highlight opportunities for
integration with sensing, positioning, and cooperative networks. The findings
pave the way for standardized solutions in 6G applications such as autonomous
systems and industrial robotics.

</details>


### [21] [Enhancements in Score-based Channel Estimation for Real-Time Wireless Systems](https://arxiv.org/abs/2509.07839)
*Florian Strasser,Marion Bäro,Wolfgang Utschick*

Main category: eess.SP

TL;DR: 基于分数生成模型的低延迟MIMO信道估计方法，通过噪声调度设计和采样加速技术显著减少推理时间


<details>
  <summary>Details</summary>
Motivation: 在点对点单载波MIMO无线系统中，需要开发低延迟的导频信道估计方法，以应对实时通信需求

Method: 提出特定的噪声调度设计和步跳采样加速技术，减少去噪步骤；还提出单步信噪比信息去噪器作为极端情况

Result: 在合成信道数据集上验证，实现了显著的延迟减少，且没有性能下降

Conclusion: 该方法能够在不牺牲性能的前提下，显著降低信道估计的延迟，适用于城市宏小区MIMO通信场景

Abstract: We propose enhancements to score-based generative modeling techniques for
low-latency pilot-based channel estimation in a point-to-point single-carrier
multiple-input multiple-output (MIMO) wireless system. Building on recent
advances in score-based models, we investigate a specific noise schedule design
and sampling acceleration by step-skipping to reduce the number of denoising
steps during inference. We additionally propose a single-step signal-to-noise
ratio informed denoiser as an extreme case of the step-skipping approach. Our
methods achieve significant latency reductions without performance degradation,
as demonstrated on a synthetic channel dataset representing an urban macrocell
MIMO communications scenario.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model](https://arxiv.org/abs/2509.06974)
*Xueyi Wang,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 这篇论文提出了一种可解释的两阶段适应性空间-时间模型，用于预测睡眠质量。模型结合多尺度卷积、递归网络和注意力机制，通过两阶段域适应策略提高模型的演化能力。


<details>
  <summary>Details</summary>
Motivation: 睡眠质量对健康致关重要，医疗供应商和个人需要可访问、可靠的预测工具来进行预防性干预。

Method: 使用多尺度卷积层模型多重输入变量的空间相互作用，递归层和注意机制捕捉长期时间依赖关系，采用两阶段域适应策略提高模型演化能力。

Result: 在5种不同的输入窗口和预测窗口组合下，模型都超过了LSTM、Informer、PatchTST和TimesNet等基线方法。最佳绩效为3天输入窗口和1天预测窗口，RMSE为0.216。甚至在更长预测期限下也保持良好性能。

Conclusion: 该框架为使用商业可穿戴设备的稀疏数据进行个性化睡眠预测提供了突出的解决方案，具有良好的适应性、可解释性和实用性。

Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.

</details>


### [23] [GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning](https://arxiv.org/abs/2509.06975)
*Yu Song,Zhigang Hua,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: GSTBench是首个系统评估图自监督学习方法跨数据集迁移能力的基准测试，发现大多数方法迁移效果不佳，而GraphMAE表现最佳


<details>
  <summary>Details</summary>
Motivation: 现有图自监督学习方法主要在单一数据集上开发和评估，其跨数据集迁移能力未被充分探索，限制了知识迁移和大规模预训练的应用

Method: 在ogbn-papers100M上进行大规模预训练，评估5种代表性SSL方法在多样化目标图上的表现，标准化实验设置分离混淆因素

Result: 大多数图SSL方法泛化能力差，有些甚至比随机初始化更差；GraphMAE（掩码自编码器方法）持续提升迁移性能

Conclusion: 分析了驱动这些差异的底层因素，为可迁移图SSL的未来研究提供指导，为图学习的"预训练-迁移"范式奠定基础

Abstract: Self-supervised learning (SSL) has shown great promise in graph
representation learning. However, most existing graph SSL methods are developed
and evaluated under a single-dataset setting, leaving their cross-dataset
transferability largely unexplored and limiting their ability to leverage
knowledge transfer and large-scale pretraining, factors that are critical for
developing generalized intelligence beyond fitting training data. To address
this gap and advance foundation model research for graphs, we present GSTBench,
the first systematic benchmark for evaluating the transferability of graph SSL
methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate
five representative SSL methods across a diverse set of target graphs. Our
standardized experimental setup decouples confounding factors such as model
architecture, dataset characteristics, and adaptation protocols, enabling
rigorous comparisons focused solely on pretraining objectives. Surprisingly, we
observe that most graph SSL methods struggle to generalize, with some
performing worse than random initialization. In contrast, GraphMAE, a masked
autoencoder approach, consistently improves transfer performance. We analyze
the underlying factors that drive these differences and offer insights to guide
future research on transferable graph SSL, laying a solid foundation for the
"pretrain-then-transfer" paradigm in graph learning. Our code is available at
https://github.com/SongYYYY/GSTBench.

</details>


### [24] [A Minimalist Bayesian Framework for Stochastic Optimization](https://arxiv.org/abs/2509.07030)
*Kaizheng Wang*

Main category: cs.LG

TL;DR: 提出了一种简约贝叶斯框架，仅对感兴趣参数（如最优位置）设置先验，通过轮廓似然消除冗余参数，可处理复杂结构约束。开发了MINTS算法，适用于结构化问题并提供经典凸优化算法的概率视角。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法需要为所有参数建立概率模型，这限制了复杂结构约束的融入。需要一种更简约的方法来处理具有约束的序列决策问题。

Method: 采用简约贝叶斯框架，仅对目标参数设置先验，通过轮廓似然消除冗余参数。开发了MINTS（简约汤普森采样）算法，可处理连续臂Lipschitz赌博机和动态定价等结构化问题。

Result: 为多臂赌博机建立了近乎最优的遗憾保证。该框架还为经典凸优化算法（如重心法和椭球法）提供了概率视角。

Conclusion: 提出的简约贝叶斯框架成功解决了传统方法难以处理复杂约束的问题，MINTS算法在多个场景下表现优异，为结构化决策问题提供了有效的解决方案。

Abstract: The Bayesian paradigm offers principled tools for sequential decision-making
under uncertainty, but its reliance on a probabilistic model for all parameters
can hinder the incorporation of complex structural constraints. We introduce a
minimalist Bayesian framework that places a prior only on the component of
interest, such as the location of the optimum. Nuisance parameters are
eliminated via profile likelihood, which naturally handles constraints. As a
direct instantiation, we develop a MINimalist Thompson Sampling (MINTS)
algorithm. Our framework accommodates structured problems, including
continuum-armed Lipschitz bandits and dynamic pricing. It also provides a
probabilistic lens on classical convex optimization algorithms such as the
center of gravity and ellipsoid methods. We further analyze MINTS for
multi-armed bandits and establish near-optimal regret guarantees.

</details>


### [25] [A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction](https://arxiv.org/abs/2509.06976)
*Lingyu Zhang,Pengfei Xu,Guobin Wu,Jian Liang,Ruiyang Dong,Yunhai Wang,Xuan Song*

Main category: cs.LG

TL;DR: 提出了一种结合交通时序数据和人类知识文本数据的知识引导跨模态特征表示学习模型(KGCM)，用于交通需求预测，通过自适应图网络和跨模态融合机制提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型主要依赖时序数据，缺乏对人类知识和经验的利用。现实中人类日常生活中的交通知识和经验对精确预测有重要影响，能够帮助模型发现数据中的潜在模式。

Method: 构建先验知识数据集(使用大语言模型结合人工编写修订)，设计局部和全局自适应图网络学习多模态特征，采用跨模态特征融合机制和基于推理的动态更新策略优化图模型参数。

Result: 在多个交通数据集上的实验表明，该模型能够准确预测未来交通需求，性能优于现有最先进模型。

Conclusion: 将结构化时序交通数据与代表人类知识的文本数据相结合，通过知识引导的跨模态学习方法，能够有效提升交通需求预测的准确性和鲁棒性。

Abstract: Traffic demand prediction plays a critical role in intelligent transportation
systems. Existing traffic prediction models primarily rely on temporal traffic
data, with limited efforts incorporating human knowledge and experience for
urban traffic demand forecasting. However, in real-world scenarios, traffic
knowledge and experience derived from human daily life significantly influence
precise traffic prediction. Such knowledge and experiences can guide the model
in uncovering latent patterns within traffic data, thereby enhancing the
accuracy and robustness of predictions. To this end, this paper proposes
integrating structured temporal traffic data with textual data representing
human knowledge and experience, resulting in a novel knowledge-guided
cross-modal feature representation learning (KGCM) model for traffic demand
prediction. Based on regional transportation characteristics, we construct a
prior knowledge dataset using a large language model combined with manual
authoring and revision, covering both regional and global knowledge and
experiences. The KGCM model then learns multimodal data features through
designed local and global adaptive graph networks, as well as a cross-modal
feature fusion mechanism. A proposed reasoning-based dynamic update strategy
enables dynamic optimization of the graph model's parameters, achieving optimal
performance. Experiments on multiple traffic datasets demonstrate that our
model accurately predicts future traffic demand and outperforms existing
state-of-the-art (SOTA) models.

</details>


### [26] [uGMM-NN: Univariate Gaussian Mixture Model Neural Network](https://arxiv.org/abs/2509.07569)
*Zakeria Sharif Ali*

Main category: cs.LG

TL;DR: uGMM-NN是一种新型神经网络架构，将单变量高斯混合模型嵌入神经元中，替代传统的加权和加非线性激活函数，能够捕获多模态性和不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络神经元使用加权和加固定非线性激活函数，缺乏概率解释和不确定性建模能力。希望将概率推理直接嵌入神经网络的计算单元中。

Method: 每个uGMM-NN节点将其激活参数化为单变量高斯混合模型，具有可学习的均值、方差和混合系数，同时保持标准前馈网络的可扩展性。

Result: uGMM-NN在判别性能上与传统多层感知机相当，同时提供了激活的概率解释，能够捕获神经元级别的多模态性和不确定性。

Conclusion: 该框架为将不确定性感知组件集成到现代神经架构中奠定了基础，为判别性和生成性建模开辟了新方向。

Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network
(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning
directly into the computational units of deep networks. Unlike traditional
neurons, which apply weighted sums followed by fixed nonlinearities, each
uGMM-NN node parameterizes its activations as a univariate Gaussian mixture,
with learnable means, variances, and mixing coefficients. This design enables
richer representations by capturing multimodality and uncertainty at the level
of individual neurons, while retaining the scalability of standard feedforward
networks. We demonstrate that uGMM-NN can achieve competitive discriminative
performance compared to conventional multilayer perceptrons, while additionally
offering a probabilistic interpretation of activations. The proposed framework
provides a foundation for integrating uncertainty-aware components into modern
neural architectures, opening new directions for both discriminative and
generative modeling.

</details>


### [27] [Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification](https://arxiv.org/abs/2509.06977)
*Zehua Li*

Main category: cs.LG

TL;DR: 这是一个配置先行框架，用于评估深度学习系统在CPU、GPU和编译运行时之间的跨后端兼容性，通过672次检测发现程序在不同运行环境下的浮点数差异问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在不同计算后端（CPU、GPU、编译运行时）上部署时出现的兼容性问题和结果不一致性问题，提供系统化的验证方法。

Method: 采用YAML配置文件解耦实验与代码，支持库和仓库模型，应用三层验证协议（张量级接近度、激活对齐、任务级指标）进行跨后端兼容性检测。

Result: 通过672次检测发现72.0%运行通过，检测模型和编译后端更容易出现偏移，主要原因是非确定性后处理。确定性适配器和选择性备用方案可显著提高一致性且无显著性能损失。

Conclusion: 该框架是首个统一的系统化方法，能够量化和缓解深度学习中的跨后端偏移问题，为异构运行时环境下的可靠部署提供可复现的验证方法。

Abstract: This paper presents a configuration-first framework for evaluating
cross-backend compatibility in deep learning systems deployed on CPU, GPU, and
compiled runtimes. The framework decouples experiments from code using YAML,
supports both library and repository models, and employs a three-tier
verification protocol covering tensor-level closeness, activation alignment,
and task-level metrics. Through 672 checks across multiple models and tolerance
settings, we observe that 72.0% of runs pass, with most discrepancies occurring
under stricter thresholds. Our results show that detection models and compiled
backends are particularly prone to drift, often due to nondeterministic
post-processing. We further demonstrate that deterministic adapters and
selective fallbacks can substantially improve agreement without significant
performance loss. To our knowledge, this is the first unified framework that
systematically quantifies and mitigates cross-backend drift in deep learning,
providing a reproducible methodology for dependable deployment across
heterogeneous runtimes.

</details>


### [28] [A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis](https://arxiv.org/abs/2509.06978)
*Wenxiong Li,Hanyu Liao,Suiyin Chen*

Main category: cs.LG

TL;DR: 提出了一种基于Kriging-HDMR建模的主动学习代理模型方法，用于解决高维可靠性分析问题，通过多阶段构建低维子代理模型来近似高维极限状态函数。


<details>
  <summary>Details</summary>
Motivation: 传统代理模型在高维随机变量下遭遇"维度灾难"，现有HDMR方法主要关注优化问题，而可靠性分析需要重点关注临界区域的预测精度。

Method: 采用三阶段框架：构建单变量子代理模型、识别耦合变量需求、构建耦合变量子代理模型；基于不确定性方差、预测均值、样本位置和样本间距等目标制定实验设计样本选择的优化数学模型。

Result: 数值实验表明，该方法在解决高维可靠性问题时具有高计算效率，同时保持了强大的预测准确性。

Conclusion: 该方法成功解决了高维可靠性分析中的维度灾难问题，通过主动学习和多阶段建模策略实现了计算效率和预测精度的平衡。

Abstract: In reliability engineering, conventional surrogate models encounter the
"curse of dimensionality" as the number of random variables increases. While
the active learning Kriging surrogate approaches with high-dimensional model
representation (HDMR) enable effective approximation of high-dimensional
functions and are widely applied to optimization problems, there are rare
studies specifically focused on reliability analysis, which prioritizes
prediction accuracy in critical regions over uniform accuracy across the entire
domain. This study develops an active learning surrogate model method based on
the Kriging-HDMR modeling for reliability analysis. The proposed approach
facilitates the approximation of high-dimensional limit state functions through
a composite representation constructed from multiple low-dimensional
sub-surrogate models. The architecture of the surrogate modeling framework
comprises three distinct stages: developing single-variable sub-surrogate
models for all random variables, identifying the requirements for
coupling-variable sub-surrogate models, and constructing the coupling-variable
sub-surrogate models. Optimization mathematical models for selection of design
of experiment samples are formulated based on each stage's characteristics,
with objectives incorporating uncertainty variance, predicted mean, sample
location and inter-sample distances. A candidate sample pool-free approach is
adopted to achieve the selection of informative samples. Numerical experiments
demonstrate that the proposed method achieves high computational efficiency
while maintaining strong predictive accuracy in solving high-dimensional
reliability problems.

</details>


### [29] [Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery](https://arxiv.org/abs/2509.06979)
*Zirui Li,Bin Yang,Meng Wang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的公交车达到时间多步预测方法NSATP，通过平衡预测性和非稳态性来避免过度稳态化问题，在真实数据集上显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 多步公交车达到时间预测中，非稳态数据会降低模型性能，而传统的标准化方法可能会消除有用的非稳态特征（过度稳态化问题）。

Method: 方法包含两个阶段：时间序列稳态化和非稳态性效应恢复。第一阶段提高预测性，第二阶段将状态最佳方法从一维扩展到二维模型，并设计了过度稳态化补偿模块，通过从原始数据学习缩放和平移因子。

Result: 在德約斯顿125天公交运营数据上验证，与基线方法相比，NSATP在有轨电车和公交车上分别减少了RMSE、MAE和MAPE锐度：有轨电车2.37%、1.22%、2.26%，公交车1.72%、0.60%、1.17%。

Conclusion: 该研究提出的NSATP方法能够有效平衡预测性和非稳态性，避免了过度稳态化的缺陷，在公交车达到时间预测任务中显著提升了性能。

Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in
improving passenger experience and supporting traffic management. Deep learning
has demonstrated outstanding performance in ATP due to its ability to model
non-linear and temporal dynamics. In the multi-step ATP, non-stationary data
will degrade the model performance due to the variation in variables' joint
distribution along the temporal direction. Previous studies mainly applied
normalization to eliminate the non-stationarity in time series, thereby
achieving better predictability. However, the normalization may obscure useful
characteristics inherent in non-stationarity, which is known as the
over-stationarization. In this work, to trade off predictability and
non-stationarity, a new approach for multi-step ATP, named non-stationary ATP (
NSATP), is proposed. The method consists of two stages: series stationarization
and non-stationarity effect recovery. The first stage aims at improving the
predictability. As for the latter, NSATP extends a state-of-the-art method from
one-dimensional to two dimensional based models to capture the hidden
periodicity in time series and designs a compensation module of
over-stationarization by learning scaling and shifting factors from raw data.
125 days' public transport operational data of Dresden is collected for
validation. Experimental results show that compared to baseline methods, the
proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for
trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.

</details>


### [30] [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)
*Jiajun Chai,Guojun Yin,Zekun Xu,Chuhuai Yue,Yi Jia,Siyu Xia,Xiaohan Wang,Jiwen Jiang,Xiaoguang Li,Chengqi Dong,Hang He,Wei Lin*

Main category: cs.LG

TL;DR: RLFactory是一个即插即用的强化学习后训练框架，通过异步调用器和解耦架构解决多轮工具使用的稳定性和适应性，在Natural Questions数据集上超越更大模型性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在基础推理方面表现出色，但在需要与外部工具交互的任务中表现不佳，需要解决工具调用的稳定性和适应性挑战

Method: 采用异步调用器、解耦工具/训练架构、奖励层支持多种评估信号，通过引入观察标记重建MDP，实现生成-解析-调用-更新的动态策略优化工作流

Result: 在Search-R1和Qwen3-4B上，Natural Questions数据集测试得分0.486，超越类似技术训练的更大模型（Qwen2.5-7B-Instruct-GRPO的0.473），训练吞吐量提升6.8倍

Conclusion: RLFactory提供了一个低门槛、高适应性的框架，能够有效增强大语言模型在现实场景中的多轮工具使用能力

Abstract: Large language models excel at basic reasoning but struggle with tasks that
require interaction with external tools. We present RLFactory, a plug-and-play
reinforcement learning post-training framework for multi-round tool use.
RLFactory tackles (i) tool-call stability and adaptability amid tool
heterogeneity and interface issues via an asyncio-based asynchronous caller and
a decoupled tool/training architecture, and (ii) diverse evaluation needs via a
reward layer supporting rule-based, model-judgment, and tool-verification
signals. It reconstructs the MDP by introducing observation markers from tool
feedback, closing the loop among model, tools, and environment, and implements
a generate-parse-invoke-update workflow for dynamic policy optimization. On
Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural
Questions (NQ) dataset, surpassing larger models trained with similar
techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training
throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable
framework for strengthening multi-round tool use of LLMs in real-world
scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.

</details>


### [31] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: CARE框架通过实时安全监控、回滚机制和自省干预策略，在解码阶段实现安全对齐，在安全性和响应质量之间取得优越平衡


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的部署增加，确保解码时输出的安全性成为关键挑战，现有方法往往在安全性和响应质量之间做出严重权衡

Method: 提出CARE框架，包含三个核心组件：(1)实时安全监控的守卫模型；(2)带令牌缓冲区的回滚机制，早期高效纠正不安全输出；(3)基于自省的干预策略，模型生成对先前输出的自我反思批判，并将这些反思纳入上下文指导后续解码

Result: 实验结果表明该框架在安全性、质量和效率方面实现了优越平衡，达到了低有害响应率和最小用户体验干扰，同时保持高响应质量

Conclusion: CARE框架通过精确干预、及时纠正和有效自校正，实现了安全与质量之间的优越权衡，为解码时安全对齐提供了有效解决方案

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [32] [FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities](https://arxiv.org/abs/2509.06984)
*Lishan Yang,Nam Kha Nguygen,Po Hu,Wei Emma Zhang,Yanjun Shu,Mong Yuan Sim,Weitong Chen*

Main category: cs.LG

TL;DR: FediLoRA是一个联邦多模态微调框架，解决了异构LoRA秩和模态缺失问题，通过维度聚合和模型编辑策略提升性能


<details>
  <summary>Details</summary>
Motivation: 现有联邦LoRA方法假设统一秩配置和单模态输入，忽略了现实中的异构客户端资源和多模态数据缺失问题

Method: 提出维度聚合策略重新加权LoRA更新，避免信息稀释；引入轻量级层间模型编辑方法选择性整合全局参数修复本地组件

Result: 在三个多模态基准数据集上，FediLoRA在全局和个性化设置中均优于竞争基线，特别是在模态不完整情况下表现优异

Conclusion: FediLoRA有效解决了联邦多模态学习中的异构资源和模态缺失挑战，为实际部署提供了实用解决方案

Abstract: Foundation models have demonstrated remarkable performance across a wide
range of tasks, yet their large parameter sizes pose challenges for practical
deployment, especially in decentralized environments. Parameter-efficient
fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing
and memory overhead, making it attractive for federated learning. However,
existing federated LoRA methods typically assume uniform rank configurations
and unimodal inputs, overlooking two key real-world challenges: (1)
heterogeneous client resources have different LoRA ranks, and (2) multimodal
data settings with potentially missing modalities. In this work, we propose
FediLoRA, a simple yet effective framework for federated multimodal fine-tuning
under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a
dimension-wise aggregation strategy that reweights LoRA updates without
information dilution during aggregation. It also includes a lightweight
layer-wise model editing method that selectively incorporates global parameters
to repair local components which improves both client and global model
performances. Experimental results on three multimodal benchmark datasets
demonstrate that FediLoRA achieves superior performance over competitive
baselines in both global and personalized settings, particularly in the
presence of modality incompleteness.

</details>


### [33] [Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs](https://arxiv.org/abs/2509.07013)
*Sima Najafzadehkhoei,George Vega Yon,Bernardo Modenesi,Derek S. Meyer*

Main category: cs.LG

TL;DR: 使用双向LSTM网络学习逆向映射，从流行病时间序列预测SIR参数，比传统近似贝叶斯方法更准确更快速


<details>
  <summary>Details</summary>
Motivation: 代理基流行病模型的参数检定计算成本高，需要更高效的检定方法

Method: 使用三层双向LSTM网络，输入60天发病率、人口规模和恢复率，输出传播概率、接触率和R0值，采用包含流行病学一致性惩罚项的复合损失函数

Result: 在1000个场景模拟中，新方法比近似贝叶斯方法锐减了所有目标的错误（MAE：R0 0.0616 vs 0.275），生成更紧凑的预测区间，并将每次检定的增增时间从77.4秒缩短到2.35秒

Conclusion: 该方法虽然存在部分参数不可识别性问题，但能更准确地重现流行曲线，实现了高效实用的参数检定，并提供了R语言实现

Abstract: Calibrating agent-based epidemic models is computationally demanding. We
present a supervised machine learning calibrator that learns the inverse
mapping from epidemic time series to SIR parameters. A three-layer
bidirectional LSTM ingests 60-day incidence together with population size and
recovery rate, and outputs transmission probability, contact rate, and R0.
Training uses a composite loss with an epidemiology-motivated consistency
penalty that encourages R0 \* recovery rate to equal transmission probability
\* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with
Approximate Bayesian Computation (likelihood-free MCMC). The method achieves
lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs
0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near
nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per
calibration. Although contact rate and transmission probability are partially
nonidentifiable, the approach reproduces epidemic curves more faithfully than
ABC, enabling fast and practical calibration. We evaluate it on SIR agent based
epidemics generated with epiworldR and provide an implementation in R.

</details>


### [34] [An efficient deep reinforcement learning environment for flexible job-shop scheduling](https://arxiv.org/abs/2509.07019)
*Xinquan Wu,Xuefeng Yan,Mingqiang Wei,Donghai Guan*

Main category: cs.LG

TL;DR: 这篇论文提出了一种简单的时序深度强化学习环境和端到端调度模型，用于解决灵活作业车间调度问题，在公开测试集上获得了竞争性能能。


<details>
  <summary>Details</summary>
Motivation: 当前的深度强化学习调度方法主要集中在调度器设计，忽视了环境建模的重要性。需要一种简单有效的DRL环境来提高调度解决方案的速度和准确性。

Method: 基于离散事件模拟的时序DRL环境，使用近端策略优化(PPO)算法。提出了基于两个状态变量的短状态表征和基于机器调度区域的新套奖励函数。

Result: 在公开测试集上，简单优先级调度规则在新环境中性能得到提升，且DRL调度模型在与OR-Tools、元神经算法、其他DRL方法和PDR方法的比较中获得了竞争性能能。

Conclusion: 这种简单的时序DRL环境和端到端调度模型能够有效解决FJSP问题，为调度器设计提供了良好的环境基础，具有实际应用价值。

Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial
optimization problem that has a wide-range of applications in the real world.
In order to generate fast and accurate scheduling solutions for FJSP, various
deep reinforcement learning (DRL) scheduling methods have been developed.
However, these methods are mainly focused on the design of DRL scheduling
Agent, overlooking the modeling of DRL environment. This paper presents a
simple chronological DRL environment for FJSP based on discrete event
simulation and an end-to-end DRL scheduling model is proposed based on the
proximal policy optimization (PPO). Furthermore, a short novel state
representation of FJSP is proposed based on two state variables in the
scheduling environment and a novel comprehensible reward function is designed
based on the scheduling area of machines. Experimental results on public
benchmark instances show that the performance of simple priority dispatching
rules (PDR) is improved in our scheduling environment and our DRL scheduling
model obtains competing performance compared with OR-Tools, meta-heuristic, DRL
and PDR scheduling methods.

</details>


### [35] [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)
*Eduardo Lobo Lustoda Cabral,Paulo Pirozelli,Larissa Driemeier*

Main category: cs.LG

TL;DR: 使用单位二进制参数（0/1）的二进归一化层，将模型内存占用减少32倍，保持类似性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型神经网络模型部署时的内存需求和计算效率问题，推进模型在移动设备等质价硬件上的应用。

Method: 发明二进归一化层，所有参数（核权重和偏置）均为0或1。支持全连接、卷积、注意力等各类层类型。通过图像分类和语言模型任务验证。

Result: 二进归一化模型表现接近32位参数模型的效果，内存占用减少32倍，无需专门硬件支持。

Conclusion: 二进归一化层为大型神经网络提供了内存高效解决方案，支持在简单低价硬件上部署，开启了新的模型部署时代。

Abstract: The increasing size of large neural network models, specifically language
models and foundational image models, poses deployment challenges, prompting
efforts to reduce memory requirements and enhance computational efficiency.
These efforts are critical to ensure practical deployment and effective
utilization of these models across various applications. In this work, a novel
type of neural network layers and models is developed that uses only single-bit
parameters. In this novel type of models all parameters of all layers,
including kernel weights and biases, only have values equal to zero or one.
This novel type of models uses layers named as binary normalized layer. These
binary normalized layers can be of any type, such as fully connected,
convolutional, attention, etc., and they consist of slight variations of the
corresponding conventional layers. To show the effectiveness of the binary
normalized layers, two different models are configured to solve a multiclass
image classification problem and a language decoder to predict the next token
of a sequence. The model to solve the image classification has convolutional
and fully connected layers, and the language model is composed of transformer
blocks with multi-head attention. The results show that models with binary
normalized layers present almost the same results obtained by equivalent models
with real 32-bit parameters. The binary normalized layers allow to develop
models that use 32 times less memory than current models and have equivalent
performance. Besides, the binary normalized layers can be easily implemented on
current computers using 1-bit arrays, and do not require the development of
dedicated electronic hardware. This novel type of layers opens a new era for
large neural network models with reduced memory requirements that can be
deployed using simple and cheap hardware, such as mobile devices or only cpus.

</details>


### [36] [Recursive State Inference for Linear PASFA](https://arxiv.org/abs/2509.07028)
*Vishal Rishi*

Main category: cs.LG

TL;DR: 提出了递归线性PASFA算法，用于从观测数据和模型中估计ARMA过程的状态（慢特征），解决了现有方法无法有效恢复原始状态的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的概率自适应慢特征分析(PASFA)方法将慢特征建模为ARMA过程的状态，但缺乏从观测数据和模型高效推断这些状态的方法。当前方法使用卡尔曼滤波但难以恢复原始的有用表示状态。

Method: 提出了递归线性PASFA扩展算法，对遵循ARMA过程的状态进行最小均方误差(MMSE)估计，直接从观测数据和模型推断慢特征状态。

Result: 在合成数据集上验证了所提算法的正确性，证明了其能够有效估计和恢复原始状态（慢特征）。

Conclusion: 该递归算法为慢特征分析提供了一种有效的状态推断方法，解决了现有技术难以恢复有用表示状态的问题，在信号分析和分类任务中具有应用价值。

Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features
in classification and signal analysis, has attracted increasing attention in
recent years. Recent probabilistic extensions to SFA learn effective
representations for classification tasks. Notably, the Probabilistic Adaptive
Slow Feature Analysis models the slow features as states in an ARMA process and
estimate the model from the observations. However, there is a need to develop
efficient methods to infer the states (slow features) from the observations and
the model. In this paper, a recursive extension to the linear PASFA has been
proposed. The proposed algorithm performs MMSE estimation of states evolving
according to an ARMA process, given the observations and the model. Although
current methods tackle this problem using Kalman filters after transforming the
ARMA process into a state space model, the original states (or slow features)
that form useful representations cannot be easily recovered. The proposed
technique is evaluated on a synthetic dataset to demonstrate its correctness.

</details>


### [37] [Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators](https://arxiv.org/abs/2509.07036)
*Federico Cerutti*

Main category: cs.LG

TL;DR: 结合因果发现与不确定性预测的金融时间序列分析方法，通过LPCMCI框架和Chronos模型在美国宏观经济指标上进行实证研究


<details>
  <summary>Details</summary>
Motivation: 探索宏观经济指标间的动态因果关系，并开发不需要任务特定训练的精准预测方法，以支持经济政策决策

Method: 使用LPCMCI框架和GPDC方法分析1970-2021年季度数据，发现因果关系，然后利用Chronos语言模型进行零样本预测

Result: 发现从经济增长到GDP的单向因果关系，通胀缩减连接性弱，失业率呈现强自回归依赖。Chronos模型在失业率预测中实现了准确的一季度和两季度预测，并提供90%置信区间

Conclusion: 结合因果结构学习与概率语言模型能够为经济政策提供价值，提高预测稳健性，通过统计原理偏移分析实现有效异常检测

Abstract: This paper presents a methodological approach to financial time series
analysis by combining causal discovery and uncertainty-aware forecasting. As a
case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic
growth, inflation, and unemployment -- and we apply the LPCMCI framework with
Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal
relationships in quarterly data from 1970 to 2021. Our results reveal a robust
unidirectional causal link from economic growth to GDP and highlight the
limited connectivity of inflation, suggesting the influence of latent factors.
Unemployment exhibits strong autoregressive dependence, motivating its use as a
case study for probabilistic forecasting. Leveraging the Chronos framework, a
large language model trained for time series, we perform zero-shot predictions
on unemployment. This approach delivers accurate forecasts one and two quarters
ahead, without requiring task-specific training. Crucially, the model's
uncertainty-aware predictions yield 90\% confidence intervals, enabling
effective anomaly detection through statistically principled deviation
analysis. This study demonstrates the value of combining causal structure
learning with probabilistic language models to inform economic policy and
enhance forecasting robustness.

</details>


### [38] [Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation](https://arxiv.org/abs/2509.07039)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 该研究系统比较了CNN和视觉Transformer在光伏热故障检测中的性能，使用XRAI显著性分析验证模型决策与热物理原理的一致性，Swin Transformer表现最佳，但不同故障类型检测性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 人工智能在光伏监测部署中面临可解释性障碍，虽然深度学习在热故障检测中达到高精度，但缺乏模型决策与热物理原理一致性的验证，限制了在能源基础设施中的应用。

Method: 使用卷积神经网络（ResNet-18、EfficientNet-B0）和视觉Transformer（ViT-Tiny、Swin-Tiny）进行热光伏故障检测，采用XRAI显著性分析评估模型与热物理原理的一致性。

Result: Swin Transformer获得最高性能（94%二元准确率；73%多类准确率）。XRAI分析显示模型学习了物理意义特征，但性能因故障类型而异：电气故障检测强（F1分数>0.90），而污染等环境因素仍然具有挑战性（F1分数0.20-0.33）。

Conclusion: 热物理引导的可解释性方法为验证能源监测应用中AI决策提供了方法论，解决了可再生能源基础设施中的部署障碍，但热成像分辨率限制了某些故障类型的检测性能。

Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring
faces interpretability barriers that limit adoption in energy infrastructure
applications. While deep learning achieves high accuracy in thermal fault
detection, validation that model decisions align with thermal physics
principles remains lacking, creating deployment hesitancy where understanding
model reasoning is critical. This study provides a systematic comparison of
convolutional neural networks (ResNet-18, EfficientNet-B0) and vision
transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI
saliency analysis to assess alignment with thermal physics principles. This
represents the first systematic comparison of CNNs and vision transformers for
thermal PV fault detection with physics-validated interpretability. Evaluation
on 20,000 infrared images spanning normal operation and 11 fault categories
shows that Swin Transformer achieves the highest performance (94% binary
accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis
reveals that models learn physically meaningful features, such as localized
hotspots for cell defects, linear thermal paths for diode failures, and thermal
boundaries for vegetation shading, consistent with expected thermal signatures.
However, performance varies significantly across fault types: electrical faults
achieve strong detection (F1-scores >0.90) while environmental factors like
soiling remain challenging (F1-scores 0.20-0.33), indicating limitations
imposed by thermal imaging resolution. The thermal physics-guided
interpretability approach provides methodology for validating AI
decision-making in energy monitoring applications, addressing deployment
barriers in renewable energy infrastructure.

</details>


### [39] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: 提出lmKANs作为线性层的替代方案，通过可训练低维多元函数实现高维映射，显著提升推理效率与容量的平衡


<details>
  <summary>Details</summary>
Motivation: 传统线性层在高维深度学习模型中占据大量参数和计算成本，需要更高效的替代方案

Method: 使用样条查找表实现的可训练低维多元函数来表达高维映射，每个函数包含数十到数百个可训练参数但计算成本很低

Result: 推理FLOPs减少高达6倍，H100吞吐量提升10倍以上，CNN中推理FLOPs减少1.6-2.1倍，在CIFAR-10和ImageNet-1k上分别减少1.7倍

Conclusion: lmKANs提供了比传统MLPs更好的容量-推理成本权衡，是线性层的有效替代方案

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [40] [Riemannian Batch Normalization: A Gyro Approach](https://arxiv.org/abs/2509.07115)
*Ziheng Chen,Xiao-Jun Wu,Nicu Sebe*

Main category: cs.LG

TL;DR: GyroBN是一个基于陀螺结构的黎曼批量归一化框架，将欧几里得批量归一化扩展到非欧几里得流形数据上


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得归一化层不适用于流形数据，而许多机器学习中的黎曼流形具有陀螺结构，需要一种理论上有保证的黎曼批量归一化方法

Method: 提出了GyroBN框架，建立了伪约简和陀螺等距陀螺两个必要条件来保证理论控制，并在七种代表性几何结构上实例化

Result: 实验证明GyroBN在这些几何结构上有效，框架还包含了多个现有黎曼归一化方法作为特例

Conclusion: GyroBN提供了一个理论上有保证的黎曼批量归一化通用框架，适用于多种非欧几里得几何结构

Abstract: Normalization layers are crucial for deep learning, but their Euclidean
formulations are inadequate for data on manifolds. On the other hand, many
Riemannian manifolds in machine learning admit gyro-structures, enabling
principled extensions of Euclidean neural networks to non-Euclidean domains.
Inspired by this, we introduce GyroBN, a principled Riemannian batch
normalization framework for gyrogroups. We establish two necessary conditions,
namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that
guarantee GyroBN with theoretical control over sample statistics, and show that
these conditions hold for all known gyrogroups in machine learning. Our
framework also incorporates several existing Riemannian normalization methods
as special cases. We further instantiate GyroBN on seven representative
geometries, including the Grassmannian, five constant curvature spaces, and the
correlation manifold, and derive novel gyro and Riemannian structures to enable
these instantiations. Experiments across these geometries demonstrate the
effectiveness of GyroBN. The code is available at
https://github.com/GitZH-Chen/GyroBN.git.

</details>


### [41] [Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models](https://arxiv.org/abs/2509.07143)
*Adrian Hayler,Xingyue Huang,İsmail İlkan Ceylan,Michael Bronstein,Ben Finkelshtein*

Main category: cs.LG

TL;DR: 该研究提出TabGFM框架，通过将图数据转换为表格格式，利用表格基础模型进行零样本节点分类，在28个实际数据集上超越了现有图基础模型和任务特定GNN模型。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型在真实图数据上演示性能有限，而表格基础模型在多个领域都显示出强大的通用性。研究者想探索通过表格化重构来实现更好的图学习效果。

Method: 将节点分类重构为表格问题：每个节点作为行，特征和结构信息作为列；使用特征和结构编码器转换图数据；多个TFM模型对子采样表格进行预测；通过集成选择结合预测结果。

Result: 在28个实际图数据集上，TabGFM实现了一致的性能提升，超过了任务特定GNN和最新的图基础模型。

Conclusion: 表格化重构为可扩展和通用的图学习提供了有前景的方向，TabGFM框架显示了利用表格基础模型进行图学习的潜力。

Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm
for achieving broad generalization across various graph data. However, existing
GFMs are often trained on datasets that were shown to poorly represent
real-world graphs, limiting their generalization performance. In contrast,
tabular foundation models (TFMs) not only excel at classical tabular prediction
tasks but have also shown strong applicability in other domains such as time
series forecasting, natural language processing, and computer vision. Motivated
by this, we take an alternative view to the standard perspective of GFMs and
reformulate node classification as a tabular problem. Each node can be
represented as a row with feature, structure, and label information as columns,
enabling TFMs to directly perform zero-shot node classification via in-context
learning. In this work, we introduce TabGFM, a graph foundation model framework
that first converts a graph into a table via feature and structural encoders,
applies multiple TFMs to diversely subsampled tables, and then aggregates their
outputs through ensemble selection. Through experiments on 28 real-world
datasets, TabGFM achieves consistent improvements over task-specific GNNs and
state-of-the-art GFMs, highlighting the potential of tabular reformulation for
scalable and generalizable graph learning.

</details>


### [42] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 基于系统理论和同调论视角，提出了有效信息一致性分数（EICS），用于量化Transformer电路的一致性和可信质量，该方法仅需单次传播且无维度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一种形式化的单次传播方法来量化Transformer电路的一致性和可信质量，以判断其算法行为是否可靠。

Method: 结合（1）基于局部雅可比矩阵和激活值计算的标准化同调不一致性，以及（2）基于同一前向状态的高斯有效信息代理来量化电路层面的因果出现。方法为白盒、单次传播且无维度。

Result: 提供了EICS分数的实践解释指南、计算开销分析（包括快速和精确模式）以及植物检验分析，待实验验证。

Conclusion: EICS为量化Transformer电路的一致性和可信质量提供了一种形式化的单次传播方法，有助于判断电路行为是否可靠，但需要进一步的实验验证。

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [43] [PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design](https://arxiv.org/abs/2509.07150)
*Andy Xu,Rohan Desai,Larry Wang,Gabriel Hope,Ethan Ritz*

Main category: cs.LG

TL;DR: PLaID++是一个基于大语言模型的晶体结构生成方法，通过Wyckoff文本表示和强化学习技术，能够高效生成稳定、新颖且具有特定空间群性质的晶体材料


<details>
  <summary>Details</summary>
Motivation: 传统材料发现过程缓慢且昂贵，需要加速新材料开发流程，特别是太阳能电池、电池和碳捕获等关键技术领域的新材料发现

Method: 基于Qwen-2.5 7B模型进行微调，使用Wyckoff-based文本表示生成晶体结构，采用基于Direct Preference Optimization的强化学习技术指导生成过程

Result: 相比先前方法，PLaID++生成热力学稳定、独特且新颖结构的成功率提高约50%，在无条件和空间群条件生成方面分别实现约115%和50%的改进

Conclusion: 该工作展示了将自然语言处理的后训练技术应用于材料设计的潜力，为靶向和高效的新材料发现铺平了道路

Abstract: Discovering novel materials is critical for technological advancements such
as solar cells, batteries, and carbon capture. However, the development of new
materials is constrained by a slow and expensive trial-and-error process. To
accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM)
fine-tuned for stable and property-guided crystal generation. We fine-tune
Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text
representation. We show that generation can be effectively guided with a
reinforcement learning technique based on Direct Preference Optimization (DPO),
with sampled structures categorized by their stability, novelty, and space
group. By encoding symmetry constraints directly into text and guiding model
outputs towards desirable chemical space, PLaID++ generates structures that are
thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than
prior methods and conditionally generates structures with desired space group
properties. Our experiments highlight the effectiveness of iterative DPO,
achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space
group conditioned generation, respectively, compared to fine-tuning alone. Our
work demonstrates the potential of adapting post-training techniques from
natural language processing to materials design, paving the way for targeted
and efficient discovery of novel materials.

</details>


### [44] [EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise](https://arxiv.org/abs/2509.07415)
*Arslan Majal,Aamir Hussain Chughtai,Muhammad Tahir*

Main category: cs.LG

TL;DR: 提出了EMORF-II，一种基于学习的异常值鲁棒滤波器，能够处理相关测量噪声，通过在线学习异常特征来提升异常缓解能力


<details>
  <summary>Details</summary>
Motivation: 现有滤波器在处理相关测量噪声和异常值时的性能有限，需要一种能够在线学习异常特征并提高鲁棒性的方法

Method: 基于EM算法的异常值鲁棒滤波器增强版本，增加了在线学习异常特征的能力，结合异常检测功能

Result: 数值实验显示相比最先进方法在精度上有性能提升，但计算开销有所增加，计算复杂度与其他实用方法相当

Conclusion: EMORF-II是一个有用的选择，具有改进的异常缓解能力，适用于各种应用场景，尽管计算开销略有增加

Abstract: We present a learning-based outlier-robust filter for a general setup where
the measurement noise can be correlated. Since it is an enhanced version of
EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is
equipped with an additional powerful feature to learn the outlier
characteristics during inference along with outlier-detection, EMORF-II has
improved outlier-mitigation capability. Numerical experiments confirm
performance gains as compared to the state-of-the-art methods in terms of
accuracy with an increased computational overhead. However, thankfully the
computational complexity order remains at par with other practical methods
making it a useful choice for diverse applications.

</details>


### [45] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: Fed-REACT是一个针对异构和动态演化客户端数据的联邦学习框架，通过两阶段方法（表示学习+进化聚类）解决数据异质性问题


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端数据分布随时间演化且差异显著导致的性能下降问题，同时保护数据隐私和降低资源成本

Method: 两阶段框架：第一阶段客户端本地学习特征表示，第二阶段服务器基于表示动态聚类并协调集群级任务特定模型训练

Result: 理论分析了表示学习阶段，实证表明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性

Conclusion: Fed-REACT通过结合表示学习和进化聚类，有效解决了联邦学习中的异构和动态数据问题，具有优越性能

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [46] [Predicting effect of novel treatments using molecular pathways and real-world data](https://arxiv.org/abs/2509.07204)
*Adrien Couetoux,Thomas Devenyns,Lise Diagne,David Champagne,Pierre-Yves Mousset,Chris Anagnostopoulos*

Main category: cs.LG

TL;DR: 提出基于机器学习的模块化方法，利用药物-通路权重影响评分和患者数据预测未测试药物治疗效果


<details>
  <summary>Details</summary>
Motivation: 药物研发中，在临床试验或实际使用前预测药物治疗特定疾病的效果具有挑战性

Method: 使用药物-通路权重影响评分和患者数据训练机器学习模型，分析未测试药物在生物分子-蛋白通路上的加权影响评分来预测疗效

Result: 在真实世界患者治疗和结果数据集上验证了方法有效性，使用两种不同的权重影响评分算法，并展示了泛化性能评估方法

Conclusion: 该方法提供了一个可迭代的初始框架，支持未来利用真实世界临床数据和药物嵌入预测未测试药物效果的研究

Abstract: In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

</details>


### [47] [Explaining How Quantization Disparately Skews a Model](https://arxiv.org/abs/2509.07222)
*Abhimanyu Bellam,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 量化加剧了模型对少数群体的不公平影响，研究发现量化过程中权重和激活值的变化导致网络性能下降，特别是对少数群体的准确率影响更大。作者提出结合混合精度量化感知训练、数据集采样和加权损失函数的方法来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现后训练量化（PTQ）虽然具有高压缩能力和速度，但会加剧模型对不同群体的不公平影响，特别是对少数群体的负面影响更为显著，这阻碍了量化神经网络在公平部署中的应用。

Method: 通过分析量化过程中权重和激活值变化对网络的影响，研究量化对logits方差、损失函数和群体准确率的影响。进一步研究这些影响对群体梯度范数和Hessian矩阵特征值的作用。提出结合混合精度量化感知训练（QAT）、数据集采样方法和加权损失函数的解决方案。

Result: 量化导致logits方差降低、损失增加和群体准确率受损，特别是对少数群体的影响更为严重。量化还会影响群体梯度范数和Hessian矩阵特征值，从优化角度揭示了网络状态的变化。

Conclusion: 量化过程会加剧模型的不公平性，需要采用混合精度QAT结合数据集采样和加权损失函数的方法来实现量化神经网络的公平部署，确保模型对所有群体都具有良好的性能表现。

Abstract: Post Training Quantization (PTQ) is widely adopted due to its high
compression capacity and speed with minimal impact on accuracy. However, we
observed that disparate impacts are exacerbated by quantization, especially for
minority groups. Our analysis explains that in the course of quantization there
is a chain of factors attributed to a disparate impact across groups during
forward and backward passes. We explore how the changes in weights and
activations induced by quantization cause cascaded impacts in the network,
resulting in logits with lower variance, increased loss, and compromised group
accuracies. We extend our study to verify the influence of these impacts on
group gradient norms and eigenvalues of the Hessian matrix, providing insights
into the state of the network from an optimization point of view. To mitigate
these effects, we propose integrating mixed precision Quantization Aware
Training (QAT) with dataset sampling methods and weighted loss functions,
therefore providing fair deployment of quantized neural networks.

</details>


### [48] [Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2509.07238)
*Pranav Pawar,Dhwaj Jain,Varun Gupta,Kaustav Dedhia,Dashrath Kale,Sudhir Dhekane*

Main category: cs.LG

TL;DR: 通过系统化参数优化框架，在5款独立模型上实现数学推理任务的效率和性能显著提升，平均节省29.4%计算成本和23.9%推理速度


<details>
  <summary>Details</summary>
Motivation: 探索数学推理任务中模型参数精细调优的实践方法，以获得效率和性能的显著提升

Method: 系统性搜索参数空间（温度0.1-0.5、推理步4-12、规划周1-4、核采样0.85-0.98），在Qwen2.5-72B、Llama-3.1-70B、DeepSeek-V3、Mixtral-8x22B、Yi-Lightning五款模型上进行优化测试

Result: 平均节省29.4%计算成本和23.9%推理速度提升，100%优化成功率，DeepSeek-V3达到98%准确率，Mixtral-8x22B每个准确响应仅需361.5个标记

Conclusion: 低温度制（0.1-0.4）和减少推理步骤（4-6）可以在不影响准确性的前提下显著提高效率，该框架具有跨模型架构的普遍适用性

Abstract: This paper presents a practical investigation into fine-tuning model
parameters for mathematical reasoning tasks through experimenting with various
configurations including randomness control, reasoning depth, and sampling
strategies, careful tuning demonstrates substantial improvements in efficiency
as well as performance. A holistically optimized framework is introduced for
five state-of-the-art models on mathematical reasoning tasks, exhibiting
significant performance boosts while maintaining solution correctness. Through
systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B,
DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are
demonstrated with 100% optimization success rate. The methodology achieves an
average 29.4% reduction in computational cost and 23.9% improvement in
inference speed across all tested models. This framework systematically
searches parameter spaces including temperature (0.1-0.5), reasoning steps
(4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining
optimal configurations through testing on mathematical reasoning benchmarks.
Critical findings show that lower temperature regimes (0.1-0.4) and reduced
reasoning steps (4-6) consistently enhance efficiency without compromising
accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B
delivers the most cost-effective performance at 361.5 tokens per accurate
response. Key contributions include: (1) the first comprehensive optimization
study for five diverse SOTA models in mathematical reasoning, (2) a
standardized production-oriented parameter optimization framework, (3)
discovery of universal optimization trends applicable across model
architectures, and (4) production-ready configurations with extensive
performance characterization.

</details>


### [49] [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: IP-Basis PINNs是一种元学习框架，通过离线-在线分解解决物理信息神经网络在多查询逆问题中的计算效率问题，显著减少在线推理时间。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在解决逆问题时，每个新的观测数据集都需要重新进行昂贵的训练过程，这限制了其在多查询场景下的应用效率。

Method: 采用离线-在线分解策略：离线训练深度网络生成参数化微分方程解空间的基函数；在线冻结网络，仅训练轻量级线性输出层来同时重建解和识别参数。

Result: 在三个不同基准测试中表现出色，包括扩展到未知函数项的通用PINNs，在常数和函数参数估计中表现一致，相比标准PINNs每个查询速度显著提升，在稀缺和噪声数据下运行稳健。

Conclusion: IP-Basis PINNs为逆问题提供了高效的多查询解决方案，通过创新的损失函数设计、前向模式自动微分和验证机制，实现了计算效率的显著提升。

Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is
computationally expensive for multi-query scenarios, as each new set of
observed data requires a new, expensive training procedure. We present
Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that
extends the foundational work of Desai et al. (2022) to enable rapid and
efficient inference for inverse problems. Our method employs an offline-online
decomposition: a deep network is first trained offline to produce a rich set of
basis functions that span the solution space of a parametric differential
equation. For each new inverse problem online, this network is frozen, and
solutions and parameters are inferred by training only a lightweight linear
output layer against observed data. Key innovations that make our approach
effective for inverse problems include: (1) a novel online loss formulation for
simultaneous solution reconstruction and parameter identification, (2) a
significant reduction in computational overhead via forward-mode automatic
differentiation for PDE loss evaluation, and (3) a non-trivial validation and
early-stopping mechanism for robust offline training. We demonstrate the
efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension
to universal PINNs for unknown functional terms-showing consistent performance
across constant and functional parameter estimation, a significant speedup per
query over standard PINNs, and robust operation with scarce and noisy data.

</details>


### [50] [GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning](https://arxiv.org/abs/2509.07252)
*Evgeny Alves Limarenko,Anastasiia Alexandrovna Studenikina*

Main category: cs.LG

TL;DR: GCond是一种基于PCGrad原理的多任务学习梯度冲突解决方法，结合梯度累积和自适应仲裁机制，在保持优化质量的同时实现两倍计算加速，并在多个数据集和模型架构上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中的梯度冲突问题严重，现有方法如PCGrad、CAGrad和GradNorm计算成本高，限制了它们在现代大型模型和Transformer中的应用。

Method: 提出Gradient Conductor (GCond)方法，基于PCGrad原理，结合梯度累积和自适应仲裁机制，提供可扩展的梯度冲突解决方案。

Result: GCond在ImageNet 1K和头颈部CT扫描数据集上评估，随机模式实现两倍计算加速，在所有评估指标上表现优越，L1和SSIM损失更低，适用于从紧凑模型到大型架构的各种规模模型。

Conclusion: GCond为多任务学习中的梯度冲突问题提供了可扩展且高效的解决方案，与现代优化器兼容，具有广泛的应用潜力。

Abstract: In multi-task learning (MTL), gradient conflict poses a significant
challenge. Effective methods for addressing this problem, including PCGrad,
CAGrad, and GradNorm, in their original implementations are computationally
demanding, which significantly limits their application in modern large models
and transformers. We propose Gradient Conductor (GCond), a method that builds
upon PCGrad principles by combining them with gradient accumulation and an
adaptive arbitration mechanism. We evaluated GCond on self-supervised learning
tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K
dataset and a combined head and neck CT scan dataset, comparing the proposed
method against baseline linear combinations and state-of-the-art gradient
conflict resolution methods. The stochastic mode of GCond achieved a two-fold
computational speedup while maintaining optimization quality, and demonstrated
superior performance across all evaluated metrics, achieving lower L1 and SSIM
losses compared to other methods on both datasets. GCond exhibited high
scalability, being successfully applied to both compact models
(MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base).
It also showed compatibility with modern optimizers such as AdamW and
Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the
problem of gradient conflicts in multi-task learning.

</details>


### [51] [Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data](https://arxiv.org/abs/2509.07280)
*Luke McLennan,Yi Wang,Ryan Farell,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 通过变分贝叶斯推断框架，从噪声稀疏相空数据中无监督学习各种温顿动力学系统，包括保守、耗散和端口温顿系统


<details>
  <summary>Details</summary>
Motivation: 解决从采样观测相空轨道中捕捉不同类型温顿动力学系统的特征和变化运动动力学的挑战

Method: 扩展稀疏对称随机弗里叶高斯过程学习，结合温顿场景的预测性连续数值估计，使用普遍化温顿动力学形式，包含核化ELBO损失、稳定性和守恒约束来正则化模型

Result: 建立了能够处理保守、耗散和端口温顿系统的统一框架，提高了预测准确性并控制不确定性

Conclusion: 该框架通过结合数捠保真度和物理约束，成功地从噪声稀疏数据中学习了复杂的温顿动力学系统，为物理模型学习提供了健壮的无监督方法

Abstract: We introduce a robust framework for learning various generalized Hamiltonian
dynamics from noisy, sparse phase-space data and in an unsupervised manner
based on variational Bayesian inference. Although conservative, dissipative,
and port-Hamiltonian systems might share the same initial total energy of a
closed system, it is challenging for a single Hamiltonian network model to
capture the distinctive and varying motion dynamics and physics of a phase
space, from sampled observational phase space trajectories. To address this
complicated Hamiltonian manifold learning challenge, we extend sparse
symplectic, random Fourier Gaussian processes learning with predictive
successive numerical estimations of the Hamiltonian landscape, using a
generalized form of state and conjugate momentum Hamiltonian dynamics,
appropriate to different classes of conservative, dissipative and
port-Hamiltonian physical systems. In addition to the kernelized evidence lower
bound (ELBO) loss for data fidelity, we incorporate stability and conservation
constraints as additional hyper-parameter balanced loss terms to regularize the
model's multi-gradients, enforcing physics correctness for improved prediction
accuracy with bounded uncertainty.

</details>


### [52] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: 提出了ALICE架构，一种简单的仅编码器Transformer，在密码破解任务中实现了最先进的准确性和速度，仅需训练约1500个密码就能泛化到未见过的密码。


<details>
  <summary>Details</summary>
Motivation: 将密码破解作为研究神经网络在组合复杂领域中泛化能力的理想测试平台，探索模型如何在26!种可能映射中选择正确的密码映射。

Method: 开发ALICE架构，使用仅编码器Transformer和基于Gumbel-Sinkhorn方法的双射解码头来显式建模排列，实现可解释的密码映射提取。

Result: ALICE在仅训练约1500个独特密码（占可能密码空间的极小部分3.7×10^{-24}）后就能泛化到未见过的密码，并通过早期退出分析揭示了模型预测的渐进细化过程。

Conclusion: 该架构和分析方法可扩展到任何具有双射映射和组合结构的领域，为神经网络泛化和可解释性提供了新的见解。

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [53] [CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation](https://arxiv.org/abs/2509.07325)
*Alyssa Unell,Noel C. F. Codella,Sam Preston,Peniel Argaw,Wen-wai Yim,Zelalem Gero,Cliff Wong,Rajesh Jena,Eric Horvitz,Amanda K. Hall,Ruican Rachel Zhong,Jiachen Li,Shrey Jain,Mu Wei,Matthew Lungren,Hoifung Poon*

Main category: cs.LG

TL;DR: 使用LLM机器人自动生成非小细肺癌病人的NCCN指南一致治疗方案，通过混合人工注释咈模型一致性信息降低成本，实现了高准确度的治疗推荐验证系统。


<details>
  <summary>Details</summary>
Motivation: 将复杂的病人信息转换为NCCN指南一致的治疗建议需要专业知识和大量时间，且容易出错，需要自动化解决方案提高效率和准确性。

Method: 构建121例NSCLC病人的纵向数据集，包含临床遇见、诊断结果和病历，专家注释对应NCCN指南。采用混合方法，结合人工注释咈模型一致性信息，开发预测相关指南的机器人框架咈验证预测准确性的元分类器。

Result: LLM具备领域特定知识，能生成与专家注释标准强相关的代理基准（Spearman r=0.88, RMSE=0.08）。治疗推荐验证系统达到AUROC=0.800，具有检查准确性咈检查信心度的关键能力。

Conclusion: 这项工作建立了临床可行的LLM基于指南遵循系统框架，在准确性、可解释性咈监管要求之间取得平衡，同时降低注释成本，为自动化临床决策支持提供了可扩展的途径。

Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based
guidelines for cancer treatment. Translating complex patient presentations into
guideline-compliant treatment recommendations is time-intensive, requires
specialized expertise, and is prone to error. Advances in large language model
(LLM) capabilities promise to reduce the time required to generate treatment
recommendations and improve accuracy. We present an LLM agent-based approach to
automatically generate guideline-concordant treatment trajectories for patients
with non-small cell lung cancer (NSCLC). Our contributions are threefold.
First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients
that includes clinical encounters, diagnostic results, and medical histories,
each expertly annotated with the corresponding NCCN guideline trajectories by
board-certified oncologists. Second, we demonstrate that existing LLMs possess
domain-specific knowledge that enables high-quality proxy benchmark generation
for both model development and evaluation, achieving strong correlation
(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.
Third, we develop a hybrid approach combining expensive human annotations with
model consistency information to create both the agent framework that predicts
the relevant guidelines for a patient, as well as a meta-classifier that
verifies prediction accuracy with calibrated confidence scores for treatment
recommendations (AUROC=0.800), a critical capability for communicating the
accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting
regulatory compliance. This work establishes a framework for clinically viable
LLM-based guideline adherence systems that balance accuracy, interpretability,
and regulatory requirements while reducing annotation costs, providing a
scalable pathway toward automated clinical decision support.

</details>


### [54] [General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases](https://arxiv.org/abs/2509.07330)
*Li-Chin Chen,Ji-Tian Sheu,Yuh-Jue Chuang*

Main category: cs.LG

TL;DR: 提出了一个通用人口统计预训练模型(GDP)，专注于年龄和性别特征的表示学习，通过顺序排序策略和编码方法提升医疗风险预测性能


<details>
  <summary>Details</summary>
Motivation: 人口统计属性在电子健康记录中普遍存在且是重要的临床风险预测因子，但现有模型往往将其作为辅助特征，缺乏专门的表示学习方法

Method: 开发GDP预训练框架，探索不同的排序策略和编码方法将表格型人口统计数据转换为潜在嵌入表示，使用多样化疾病和人群数据进行预训练和评估

Result: 顺序排序策略显著提升了模型在区分度、校准度和决策树分裂信息增益方面的性能，特别是在年龄和性别对风险分层贡献显著的疾病中表现突出

Conclusion: 表格型人口统计属性的基础模型能够跨任务和人群泛化，为提升医疗预测性能提供了有前景的方向

Abstract: Demographic attributes are universally present in electronic health records
and serve as vital predictors in clinical risk stratification and treatment
decisions. Despite their significance, these attributes are often relegated to
auxiliary roles in model design, with limited attention has been given to
learning their representations. This study proposes a General Demographic
Pre-trained (GDP) model as a foundational representation framework tailored to
age and gender. The model is pre-trained and evaluated using datasets with
diverse diseases and population compositions from different geographic regions.
The GDP architecture explores combinations of ordering strategies and encoding
methods to transform tabular demographic inputs into latent embeddings.
Experimental results demonstrate that sequential ordering substantially
improves model performance in discrimination, calibration, and the
corresponding information gain at each decision tree split, particularly in
diseases where age and gender contribute significantly to risk stratification.
Even in datasets where demographic attributes hold relatively low predictive
value, GDP enhances the representational importance, increasing their influence
in downstream gradient boosting models. The findings suggest that foundational
models for tabular demographic attributes can generalize across tasks and
populations, offering a promising direction for improving predictive
performance in healthcare applications.

</details>


### [55] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: FedTeddi是一种针对联邦边缘学习中动态非IID数据的时间漂移和发散感知调度算法，通过联合调度和带宽分配实现快速收敛


<details>
  <summary>Details</summary>
Motivation: 现实场景中客户端数据随时间动态变化且呈非独立同分布特性，现有研究多假设静态数据集，需要及时高效地适应这种演化数据

Method: 使用时间漂移和集体发散量化数据动态特性，表示为类别分布的Earth Mover's Distance，提出新颖优化目标并开发联合调度和带宽分配算法

Result: 在CIFAR-10和CIFAR-100上相比随机调度分别提高了58.4%和49.2%的收敛速度，获得了更高的测试精度和更快收敛

Conclusion: FedTeddi算法能有效应对动态数据演化，使联邦边缘学习系统能够快速学习新数据而不遗忘先前知识

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [56] [SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression](https://arxiv.org/abs/2509.07373)
*Qihu Xie,Yuan Li,Yi Kang*

Main category: cs.LG

TL;DR: SBS是一种参数高效的神经表示增强方法，通过抑制频谱偏差来改进神经网络权重的压缩重建效果


<details>
  <summary>Details</summary>
Motivation: 标准的隐式神经表示存在明显的频谱偏差，难以有效重建高频细节，限制了神经网络参数压缩的效果

Method: 提出两种技术：1）基于单向排序的平滑化改进输出空间核平滑度；2）基于单向排序平滑感知的随机傅里叶特征，根据层级参数数量自适应调节输入编码的频率带宽

Result: 在多个ResNet模型和CIFAR-10、CIFAR-100、ImageNet数据集上的广泛评估表明，SBS以更少的参数实现了显著更好的重建精度

Conclusion: SBS通过抑制频谱偏差，在神经网络参数压缩方面超越了现有最佳方法，实现了更好的重建效果和参数效率

Abstract: Implicit neural representations have recently been extended to represent
convolutional neural network weights via neural representation for neural
networks, offering promising parameter compression benefits. However, standard
multi-layer perceptrons used in neural representation for neural networks
exhibit a pronounced spectral bias, hampering their ability to reconstruct
high-frequency details effectively. In this paper, we propose SBS, a
parameter-efficient enhancement to neural representation for neural networks
that suppresses spectral bias using two techniques: (1) a unidirectional
ordering-based smoothing that improves kernel smoothness in the output space,
and (2) unidirectional ordering-based smoothing aware random fourier features
that adaptively modulate the frequency bandwidth of input encodings based on
layer-wise parameter count. Extensive evaluations on various ResNet models with
datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves
significantly better reconstruction accuracy with less parameters compared to
SOTA.

</details>


### [57] [EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis](https://arxiv.org/abs/2509.07388)
*Qasim Zia,Avais Jan,Zafar Iqbal,Muhammad Mumtaz Ali,Mukarram Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 基于EfficientNet深度学习模型和数字双生系统的新案框，用于心脏停频的早期检测和分析，通过心血管图像特征学习和个体化模型实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 心脏停频是全球急重大健康问题，早期识别和管理对改善患者预后至关重要。需要提高检测准确性和个体化治疗方案。

Method: 结合EfficientNet深度学习模型（使用复合缩放技术学习心血管图像特征）和数字双生系统（通过IoT设备数据构建个体化心血管系统模型）的框架。

Result: 实验结果显示该系统在预测能力上具有高精度，同时保持高效率，能够持续评估患者状态和治疗方案影响。

Conclusion: 深度学习与数字双生技术的结合为心脏疾病预测提供了主动个体化的新方法，在心脏停频早期检测和分析方面具有重要价值。

Abstract: Cardiac arrest is one of the biggest global health problems, and early
identification and management are key to enhancing the patient's prognosis. In
this paper, we propose a novel framework that combines an EfficientNet-based
deep learning model with a digital twin system to improve the early detection
and analysis of cardiac arrest. We use compound scaling and EfficientNet to
learn the features of cardiovascular images. In parallel, the digital twin
creates a realistic and individualized cardiovascular system model of the
patient based on data received from the Internet of Things (IoT) devices
attached to the patient, which can help in the constant assessment of the
patient and the impact of possible treatment plans. As shown by our
experiments, the proposed system is highly accurate in its prediction abilities
and, at the same time, efficient. Combining highly advanced techniques such as
deep learning and digital twin (DT) technology presents the possibility of
using an active and individual approach to predicting cardiac disease.

</details>


### [58] [Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions](https://arxiv.org/abs/2509.07392)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Soyoun Kim,Sunyoung Moon,Sua Lee,Jaeyoung Choi,Hyemin Lee,Sangmi Chai*

Main category: cs.LG

TL;DR: 提出混合GCN-GRU模型检测区块链非法交易，结合图结构和时序特征，在比特币数据上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 区块链交易网络具有复杂的时序模式和节点间关系，传统方法难以有效检测非法活动，需要同时捕捉结构性和序列性特征

Method: 使用混合GCN-GRU模型，GCN处理图结构关系，GRU处理时序模式，结合两种神经网络的优势

Result: 在2020-2024年真实比特币交易数据上，模型达到0.9470准确率和0.9807 AUC-ROC，优于所有基线方法

Conclusion: 混合GCN-GRU模型能有效检测区块链非法交易，为复杂动态网络中的异常检测提供了有效解决方案

Abstract: Blockchain transaction networks are complex, with evolving temporal patterns
and inter-node relationships. To detect illicit activities, we propose a hybrid
GCN-GRU model that captures both structural and sequential features. Using real
Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and
0.9807 AUC-ROC, outperforming all baselines.

</details>


### [59] [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)
*Long Li,Jiaran Hao,Jason Klein Liu,Zhijian Zhou,Xiaoyu Tan,Wei Chu,Zhe Wang,Shirui Pan,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 提出了DPH-RL框架，通过使用mass-covering f-divergences（如前向KL和JS散度）作为排练机制，解决RLVR微调中多尝试性能退化问题，同时提升单次和多次尝试准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在RLVR微调中出现的多尝试性能（Pass@k）退化问题，以及灾难性遗忘现象，现有方法缺乏对散度项选择和功能的深入探讨。

Method: 提出DPH-RL框架，使用mass-covering f-divergences（如前向KL和JS散度）作为排练机制，持续参考初始策略，强制模型保持广泛的解决方案覆盖范围。

Result: 在数学和SQL生成任务上的实验表明，DPH-RL不仅解决了Pass@k退化问题，还提升了域内和域外的Pass@1和Pass@k性能，且训练效率更高。

Conclusion: 散度度量的正确选择是改进RLVR的关键因素，DPH-RL通过散度项本身作为解决方案，构建了更通用和多样化的推理模型。

Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with
Reinforcement Learning with Verifiable Reward (RLVR) is the frequent
degradation of multi-attempt performance (Pass@k) despite improvements in
single-attempt accuracy (Pass@1). This is often accompanied by catastrophic
forgetting, where models lose previously acquired skills. While various methods
have been proposed, the choice and function of the divergence term have been
surprisingly unexamined as a proactive solution. We argue that standard RLVR
objectives -- both those using the mode-seeking reverse KL-divergence and those
forgoing a divergence term entirely -- lack a crucial mechanism for knowledge
retention. The reverse-KL actively accelerates this decay by narrowing the
policy, while its absence provides no safeguard against the model drifting from
its diverse knowledge base. We propose a fundamental shift in perspective:
using the divergence term itself as the solution. Our framework,
Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences
(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By
continuously referencing the initial policy, this approach forces the model to
maintain broad solution coverage. Extensive experiments on math and SQL
generation demonstrate that DPH-RL not only resolves the Pass@k degradation but
improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is
more training-efficient because it computes f-divergence using generator
functions, requiring only sampling from the initial policy and no online
reference model. Our work highlights a crucial, overlooked axis for improving
RLVR, demonstrating that the proper selection of a divergence measure is a
powerful tool for building more general and diverse reasoning models.

</details>


### [60] [Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks](https://arxiv.org/abs/2509.07499)
*Antoine Ledent,Petr Kasalický,Rodrigo Alves,Hady W. Lauw*

Main category: cs.LG

TL;DR: 提出了一种新的卷积自编码器架构，用于用户建模和推荐任务，在隐式和显式反馈预测方面都达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统模型在处理多种交互类型和隐式/显式反馈方面存在局限性，需要更灵活的架构来同时学习用户-物品关联和不同类型的反馈信息。

Method: 使用卷积自编码器架构，能够学习不同交互类型之间的关联和组合，同时从显式评分和隐式采样模式中联合学习，并分别预测内容消费概率和高评分可能性。

Result: 在多个真实数据集上实现了隐式和显式反馈预测任务的最先进性能，单个模型同时处理两种反馈，并提供额外的可解释性。

Conclusion: 该模型不仅性能优越，还提供了理论保证和更好的可解释性，能够识别用户可能不会自然消费但会喜欢的物品。

Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling
and recommendation tasks with several improvements over the state of the art.
Firstly, our model has the flexibility to learn a set of associations and
combinations between different interaction types in a way that carries over to
each user and item. Secondly, our model is able to learn jointly from both the
explicit ratings and the implicit information in the sampling pattern (which we
refer to as `implicit feedback'). It can also make separate predictions for the
probability of consuming content and the likelihood of granting it a high
rating if observed. This not only allows the model to make predictions for both
the implicit and explicit feedback, but also increases the informativeness of
the predictions: in particular, our model can identify items which users would
not have been likely to consume naturally, but would be likely to enjoy if
exposed to them. Finally, we provide several generalization bounds for our
model, which to the best of our knowledge, are among the first generalization
bounds for auto-encoders in a Recommender Systems setting; we also show that
optimizing our loss function guarantees the recovery of the exact sampling
distribution over interactions up to a small error in total variation. In
experiments on several real-life datasets, we achieve state-of-the-art
performance on both the implicit and explicit feedback prediction tasks despite
relying on a single model for both, and benefiting from additional
interpretability in the form of individual predictions for the probabilities of
each possible rating.

</details>


### [61] [Water Demand Forecasting of District Metered Areas through Learned Consumer Representations](https://arxiv.org/abs/2509.07515)
*Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Tomás Arias-Vergara,Andreas Maier,Siming Bayer*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于无监督对比学习和射线变换卷积网络的新题水需求预测方法，通过分析用户消费行为来提升短期水需求预测精度，最大提升4.9%。


<details>
  <summary>Details</summary>
Motivation: 在气候变化导致的不确定性增加背景下，水资源安全成为全球急需解决的问题。虽然智能计量技术提供了详细的用户消费数据，但水需求预测仍面临气象条件等非确定性因素的挑战。

Method: 采用无监督对比学习对用户按消费行为进行分类，然后将这些特征作为输入用于浪小波变换卷积网络，结合交叉注意力机制结合历史数据和消费行为表征进行需求预测。

Result: 在真实区域计量区域进行了六个月的评估，证明方法在不同区域都显示出更好的预测性能（MAPE指标），最大提升达4.9%。同时识别出受社会经济因素影响的用户行为。

Conclusion: 该方法不仅提高了短期水需求预测的准确性，还能够识别和分析用户消费行为模式，为水资源管理提供了更深入的见解。

Abstract: Advancements in smart metering technologies have significantly improved the
ability to monitor and manage water utilities. In the context of increasing
uncertainty due to climate change, securing water resources and supply has
emerged as an urgent global issue with extensive socioeconomic ramifications.
Hourly consumption data from end-users have yielded substantial insights for
projecting demand across regions characterized by diverse consumption patterns.
Nevertheless, the prediction of water demand remains challenging due to
influencing non-deterministic factors, such as meteorological conditions. This
work introduces a novel method for short-term water demand forecasting for
District Metered Areas (DMAs) which encompass commercial, agricultural, and
residential consumers. Unsupervised contrastive learning is applied to
categorize end-users according to distinct consumption behaviors present within
a DMA. Subsequently, the distinct consumption behaviors are utilized as
features in the ensuing demand forecasting task using wavelet-transformed
convolutional networks that incorporate a cross-attention mechanism combining
both historical data and the derived representations. The proposed approach is
evaluated on real-world DMAs over a six-month period, demonstrating improved
forecasting performance in terms of MAPE across different DMAs, with a maximum
improvement of 4.9%. Additionally, it identifies consumers whose behavior is
shaped by socioeconomic factors, enhancing prior knowledge about the
deterministic patterns that influence demand.

</details>


### [62] [RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection](https://arxiv.org/abs/2509.07523)
*Jad Yehya,Mansour Benbakoura,Cédric Allain,Benoît Malezieux,Matthieu Kowalski,Thomas Moreau*

Main category: cs.LG

TL;DR: RoseCDL是一个可扩展且鲁棒的卷积字典学习算法，用于长信号中的无监督罕见事件检测，通过随机窗口和在线异常检测解决计算成本和异常敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 卷积字典学习(CDL)在建模信号局部结构方面很强大，但在检测罕见或异常事件方面尚未充分探索，面临高计算成本和对异常值敏感两大挑战。

Method: RoseCDL结合随机窗口化在大型数据集上进行高效训练，并通过在线异常检测增强鲁棒性，隔离异常模式。

Result: 该方法将CDL重新定位为事件发现和表征的实用工具，超越了传统的压缩或去噪任务。

Conclusion: RoseCDL为大规模信号中的罕见事件检测提供了一个可扩展且鲁棒的解决方案，扩展了CDL在现实世界信号分析中的应用范围。

Abstract: Identifying recurring patterns and rare events in large-scale signals is a
fundamental challenge in fields such as astronomy, physical simulations, and
biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful
framework for modeling local structures in signals, but its use for detecting
rare or anomalous events remains largely unexplored. In particular, CDL faces
two key challenges in this setting: high computational cost and sensitivity to
artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and
robust CDL algorithm designed for unsupervised rare event detection in long
signals. RoseCDL combines stochastic windowing for efficient training on large
datasets with inline outlier detection to enhance robustness and isolate
anomalous patterns. This reframes CDL as a practical tool for event discovery
and characterization in real-world signals, extending its role beyond
traditional tasks like compression or denoising.

</details>


### [63] [$ΔL$ Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)
*Zhiyuan He,Xufang Luo,Yike Zhang,Yuqing Yang,Lili Qiu*

Main category: cs.LG

TL;DR: 提出了ΔL归一化方法，专门解决RLVR中动态生成长度导致的梯度方差大和优化不稳定问题，通过理论分析和实验验证证明了其无偏性和最小方差特性


<details>
  <summary>Details</summary>
Motivation: RLVR在提升大语言模型推理能力方面显示出强大潜力，但训练过程中响应长度的巨大变异性导致高梯度方差和不稳定优化，现有方法存在估计偏差或高方差问题

Method: 通过理论分析不同长度对策略损失的影响，将问题重新表述为寻找最小方差无偏估计量，提出ΔL归一化方法

Result: 大量实验表明，该方法在不同模型大小、最大长度和任务上都能取得优异结果

Conclusion: ΔL归一化不仅提供了真实策略损失的无偏估计，还在理论上最小化了梯度方差，是解决RLVR中长度变化问题的有效方法

Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation
method tailored to the characteristic of dynamic generation lengths in
Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has
demonstrated strong potential in improving the reasoning capabilities of large
language models (LLMs), but a major challenge lies in the large variability of
response lengths during training, which leads to high gradient variance and
unstable optimization. Although previous methods such as GRPO, DAPO, and Dr.
GRPO introduce different loss normalization terms to address this issue, they
either produce biased estimates or still suffer from high gradient variance. By
analyzing the effect of varying lengths on policy loss both theoretically and
empirically, we reformulate the problem as finding a minimum-variance unbiased
estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased
estimate of the true policy loss but also minimizes gradient variance in
theory. Extensive experiments show that it consistently achieves superior
results across different model sizes, maximum lengths, and tasks. Our code will
be made public at https://github.com/zerolllin/Delta-L-Normalization.

</details>


### [64] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: 提出了PINN的双重公式化方法，用于处理具有不连续系数的材料均质化问题，通过上下误差界提高可靠性


<details>
  <summary>Details</summary>
Motivation: 传统PINN在处理具有不连续系数（如分段常数特性）的材料时经常失败，需要更可靠的方法来解决周期性热导复合材料的均质化问题

Method: 引入双重公式化PINN框架，比较标准PINN应用于平滑材料近似与变分PINN（使用谱和神经网络基测试函数）的性能

Result: 强形式PINN在受控环境下可能优于VPINN，但对材料不连续性敏感且可能无明确诊断失败；VPINN能直接处理分段常数材料参数但需要仔细选择测试函数

Conclusion: 双重公式化可作为收敛质量的可靠指标，将其集成到PINN框架中增强了在微力学均质化问题中的适用性

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


### [65] [Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards](https://arxiv.org/abs/2509.07603)
*Mehdi Bejani,Marco Mauri,Daniele Acconcia,Simone Todaro,Stefano Mariani*

Main category: cs.LG

TL;DR: 基于Transformer的深度学习策略优化半导体探针卡传感器布局，实现99.83%的健康状态分类准确率和99.73%的裂纹检测召回率，通过注意力机制识别关键传感器位置。


<details>
  <summary>Details</summary>
Motivation: 探针卡故障（如基板裂纹和螺丝松动）严重影响半导体制造良率和可靠性，需要有效的传感器布局来检测故障模式。

Method: 采用有限元模型模拟故障场景的频率响应函数，构建包含物理信息场景扩展和物理感知数据增强的综合数据集，训练混合CNN-Transformer模型。

Result: 模型在探针卡健康状态分类（基线、螺丝松动、裂纹）上达到99.83%的高准确率，裂纹检测召回率达99.73%，并通过3次10折分层交叉验证确认鲁棒性。

Conclusion: 基于注意力的深度学习能够推进预测性维护，通过优化传感器配置设计高效、经济高效的监测系统，提升半导体制造的操作可靠性和良率。

Abstract: This paper presents an innovative Transformer-based deep learning strategy
for optimizing the placement of sensors aiming at structural health monitoring
of semiconductor probe cards. Failures in probe cards, including substrate
cracks and loosened screws, would critically affect semiconductor manufacturing
yield and reliability. Some failure modes could be detected by equipping a
probe card with adequate sensors. Frequency response functions from simulated
failure scenarios are adopted within a finite element model of a probe card. A
comprehensive dataset, enriched by physics-informed scenario expansion and
physics-aware statistical data augmentation, is exploited to train a hybrid
Convolutional Neural Network and Transformer model. The model achieves high
accuracy (99.83%) in classifying the probe card health states (baseline, loose
screw, crack) and an excellent crack detection recall (99.73%). Model
robustness is confirmed through a rigorous framework of 3 repetitions of
10-fold stratified cross-validation. The attention mechanism also pinpoints
critical sensor locations: an analysis of the attention weights offers
actionable insights for designing efficient, cost-effective monitoring systems
by optimizing sensor configurations. This research highlights the capability of
attention-based deep learning to advance proactive maintenance, enhancing
operational reliability and yield in semiconductor manufacturing.

</details>


### [66] [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)
*Zhoujun Cheng,Richard Fan,Shibo Hao,Taylor W. Killian,Haonan Li,Suqi Sun,Hector Ren,Alexander Moreno,Daqian Zhang,Tianjun Zhong,Yuxin Xiong,Yuanzhe Hu,Yutao Xie,Xudong Han,Yuqi Wang,Varad Pimpalkhute,Yonghao Zhuang,Aaryamonvikram Singh,Xuezhi Liang,Anze Xie,Jianshu She,Desai Fan,Chengqian Gao,Liqun Ma,Mikhail Yurochkin,John Maggs,Xuezhe Ma,Guowei He,Zhiting Hu,Zhengzhong Liu,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-Think是一个32B参数的推理系统，通过先进的训练后处理和测试时计算技术，在数学推理等任务上达到最先进性能，媲美甚至超越更大模型


<details>
  <summary>Details</summary>
Motivation: 证明较小参数模型通过集成化的训练后配方和推理时增强技术，能够与最先进的大型模型竞争，使开源推理系统更加可及和经济实惠

Method: 基于Qwen2.5基础模型，采用六大技术支柱：长思维链监督微调、可验证奖励的强化学习、推理前代理规划、测试时扩展、推测解码和推理优化硬件

Result: 在开源模型公共基准测试中取得最先进分数，数学推理表现优异，代码和科学领域也表现强劲，推理速度达到每秒2000+ token

Conclusion: K2-Think 32B通过集成化的训练后配方（包括长思维链训练和策略性推理时增强）证明参数效率更高的模型能够与最先进系统竞争

Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance
with a 32B parameter model, matching or surpassing much larger models like
GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system
shows that smaller models can compete at the highest levels by combining
advanced post-training and test-time computation techniques. The approach is
based on six key technical pillars: Long Chain-of-thought Supervised
Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic
planning prior to reasoning, Test-time Scaling, Speculative Decoding, and
Inference-optimized Hardware, all using publicly available open-source
datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art
scores on public benchmarks for open-source models, while also performing
strongly in other areas such as Code and Science. Our results confirm that a
more parameter-efficient model like K2-Think 32B can compete with
state-of-the-art systems through an integrated post-training recipe that
includes long chain-of-thought training and strategic inference-time
enhancements, making open-source reasoning systems more accessible and
affordable. K2-Think is freely available at k2think.ai, offering best-in-class
inference speeds of over 2,000 tokens per second per request via the Cerebras
Wafer-Scale Engine.

</details>


### [67] [Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques](https://arxiv.org/abs/2509.07605)
*Ali Nawaz,Amir Ahmad,Shehroz S. Khan*

Main category: cs.LG

TL;DR: 这篇论文系统性评估了二元分类器在类不平衡情况下的表现，特别是在不使用重新平衡技术的情况下，并发现TabPFN和提升集成模型在极端不平衡时保持较好性能。


<details>
  <summary>Details</summary>
Motivation: 评估二元分类器在类不平衡情况下的表现，特别是在不使用重新平衡技术的情况下，因为这方面的研究较少。

Method: 系统性评估多种二元分类器在真实和合成数据集上的表现，包括逐渐减少少数类样本、一次学习和少量学习场景，以及通过合成决策边界模拟不同数据复杂性。还包括了重新采样和一类分类方法的对比实验。

Result: 分类任务难度随着数据复杂性增加和少数类样本减少而提高。传统分类器在极端不平衡时性能恶化，但TabPFN和提升集成模型保持较高性能和更好的泛化能力。

Conclusion: 这项研究为不平衡学习中的模型选择提供了有价值指南，说明在不依赖显式重新平衡技术的情况下，某些先进模型仍能保持良好的性能。

Abstract: Class imbalance poses a significant challenge to supervised classification,
particularly in critical domains like medical diagnostics and anomaly detection
where minority class instances are rare. While numerous studies have explored
rebalancing techniques to address this issue, less attention has been given to
evaluating the performance of binary classifiers under imbalance when no such
techniques are applied. Therefore, the goal of this study is to assess the
performance of binary classifiers "as-is", without performing any explicit
rebalancing. Specifically, we systematically evaluate the robustness of a
diverse set of binary classifiers across both real-world and synthetic
datasets, under progressively reduced minority class sizes, using one-shot and
few-shot scenarios as baselines. Our approach also explores varying data
complexities through synthetic decision boundary generation to simulate
real-world conditions. In addition to standard classifiers, we include
experiments using undersampling, oversampling strategies, and one-class
classification (OCC) methods to examine their behavior under severe imbalance.
The results confirm that classification becomes more difficult as data
complexity increases and the minority class size decreases. While traditional
classifiers deteriorate under extreme imbalance, advanced models like TabPFN
and boosting-based ensembles retain relatively higher performance and better
generalization compared to traditional classifiers. Visual interpretability and
evaluation metrics further validate these findings. Our work offers valuable
guidance on model selection for imbalanced learning, providing insights into
classifier robustness without dependence on explicit rebalancing techniques.

</details>


### [68] [Graph-based Integrated Gradients for Explaining Graph Neural Networks](https://arxiv.org/abs/2509.07648)
*Lachlan Simpson,Kyle Millar,Adriel Cheng,Cheng-Chew Lim,Hong Gunn Chew*

Main category: cs.LG

TL;DR: 提出了图基集成梯度(GB-IG)方法，将集成梯度扩展到图数据结构，解决了传统IG方法不适用于离散图结构的问题。


<details>
  <summary>Details</summary>
Motivation: 集成梯度(IG)方法假设数据是连续的，但图是离散结构，因此IG不适用于图数据。需要开发专门针对图结构的解释性方法。

Method: 开发了图基集成梯度(GB-IG)方法，这是IG在图数据上的扩展版本，专门处理图的离散特性。

Result: 在4个合成数据集上验证了GB-IG能准确识别图中用于分类任务的关键结构组件；在3个真实世界图数据集上，GB-IG在节点分类任务的重要特征识别方面优于传统IG方法。

Conclusion: GB-IG方法成功解决了IG在图数据上的适用性问题，为图神经网络提供了更有效的解释性工具。

Abstract: Integrated Gradients (IG) is a common explainability technique to address the
black-box problem of neural networks. Integrated gradients assumes continuous
data. Graphs are discrete structures making IG ill-suited to graphs. In this
work, we introduce graph-based integrated gradients (GB-IG); an extension of IG
to graphs. We demonstrate on four synthetic datasets that GB-IG accurately
identifies crucial structural components of the graph used in classification
tasks. We further demonstrate on three prevalent real-world graph datasets that
GB-IG outperforms IG in highlighting important features for node classification
tasks.

</details>


### [69] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 这篇论文提出了一种新的邻层嵌入加速方法，在保持结构保真度的同时实现高速度计算，支持交互式数据探索和高维嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 解决现有邻层嵌入方法在速度与质量之间的争议：负样本采样方法速度快但结构保真度低，而精确加速方法质量好但速度慢且限制在2-3维空间。

Method: 提出一种新的迭代近似最近邻层搜索算法，每次迭代只需少量计算，支持超参数调整和高维嵌入空间，专为交互式数据探索设计。

Result: 通过GPU加速GUI集成进行实验，在速度、结构提取灵活性方面显示了突出结果，并且可以通过最小算法修改应用于更广泛的机器学习场景。

Conclusion: 该方法成功桥接了高速近似和高质量精确方法之间的间隔，为交互式数据可视化提供了即时反馈和灵活的结构控制能力，拓展了邻层嵌入在高维度空间中的应用。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


### [70] [IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing](https://arxiv.org/abs/2509.07725)
*Shusen Ma,Tianhao Zhang,Qijiu Xia,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出IBN网络解决多变量时间序列预测中变量缺失问题，通过不确定性感知插值和双向建模提升预测性能和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有方法GinAR虽然首次使用注意力机制处理变量缺失，但缺乏可解释性且无法捕捉潜在时间模式，需要更可靠和可解释的解决方案

Method: 集成不确定性感知插值(UAI)和高斯核图卷积(GGCN)，使用MC Dropout估计重构值不确定性，双向递归单元增强时间依赖建模

Result: 在各种缺失率场景下实现了最先进的预测性能

Conclusion: IBN为具有缺失变量的多变量时间序列预测提供了更可靠和可解释的框架

Abstract: Multivariate time series forecasting (MTSF) often faces challenges from
missing variables, which hinder conventional spatial-temporal graph neural
networks in modeling inter-variable correlations. While GinAR addresses
variable missing using attention-based imputation and adaptive graph learning
for the first time, it lacks interpretability and fails to capture more latent
temporal patterns due to its simple recursive units (RUs). To overcome these
limitations, we propose the Interpretable Bidirectional-modeling Network (IBN),
integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based
Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values
using MC Dropout and applies an uncertainty-weighted strategy to mitigate
high-risk reconstructions. GGCN explicitly models spatial correlations among
variables, while a bidirectional RU enhances temporal dependency modeling.
Extensive experiments show that IBN achieves state-of-the-art forecasting
performance under various missing-rate scenarios, providing a more reliable and
interpretable framework for MTSF with missing variables. Code is available at:
https://github.com/zhangth1211/NICLab-IBN.

</details>


### [71] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 使用误差有界压缩算法压缩MoE模型中的非激活专家，减少GPU内存与主存间的数据传输开销，并分析不同层级专家压缩误差对推理精度的影响


<details>
  <summary>Details</summary>
Motivation: MoE模型在有限GPU内存下的高效服务面临挑战，需要探索压缩专家参数的方法来减少数据传输开销，同时分析压缩误差对推理性能的影响

Method: 采用误差有界压缩算法（如SZ3和CuSZp）压缩非激活专家，通过大量实验分析不同层级专家压缩误差对整体推理精度的影响

Result: 浅层专家（负责注意力机制和输入转换）对压缩误差不敏感；中层专家（负责模型推理）误差会显著降低推理精度；深层专家（负责指令跟随和输出整合）的有界误差有时能提升推理精度

Conclusion: 误差有界压缩是优化MoE模型内存使用的有效方法，但需要针对不同层级专家采用差异化的压缩策略，中层专家需要更严格的误差控制

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


### [72] [Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models](https://arxiv.org/abs/2509.07813)
*Jonathan Teagan*

Main category: cs.LG

TL;DR: 研究采用多种预测技术对乌克兰战争中俄罗斯设备损失进行建模预测，发现深度学习模型在高时间粒度条件下表现最佳


<details>
  <summary>Details</summary>
Motivation: 利用开源情报数据评估乌克兰战争中俄罗斯设备损失趋势，预测未来损失模式，并比较不同预测模型的性能

Method: 采用ARIMA、Prophet、LSTM、TCN和XGBoost等多种预测技术，基于WarSpotting的每日和每月OSINT数据对俄罗斯设备损失进行建模预测

Result: 深度学习模型（特别是TCN和LSTM）在高时间粒度条件下产生稳定且一致的预测结果，集成预测在冲突建模中具有重要价值

Conclusion: 公开可用的OSINT数据在量化物质损耗时间变化方面具有重要价值，深度学习模型在高频数据预测中表现优异

Abstract: This study applies a range of forecasting techniques,including ARIMA,
Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional
Networks (TCN), and XGBoost, to model and predict Russian equipment losses
during the ongoing war in Ukraine. Drawing on daily and monthly open-source
intelligence (OSINT) data from WarSpotting, we aim to assess trends in
attrition, evaluate model performance, and estimate future loss patterns
through the end of 2025. Our findings show that deep learning models,
particularly TCN and LSTM, produce stable and consistent forecasts, especially
under conditions of high temporal granularity. By comparing different model
architectures and input structures, this study highlights the importance of
ensemble forecasting in conflict modeling, and the value of publicly available
OSINT data in quantifying material degradation over time.

</details>


### [73] [Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques](https://arxiv.org/abs/2509.07845)
*Mohammad Zana Majidi,Sajjad Karimi,Teng Wang,Robert Kluger,Reginald Souleyrette*

Main category: cs.LG

TL;DR: 本研究通过结合结构化车祸数据和警察现场记录的非结构化叙述文本，使用TF-IDF和Word2Vec两种NLP技术提取语义信息，采用KNN过采样处理类别不平衡问题，开发了102个机器学习模型来预测车祸伤害严重程度。


<details>
  <summary>Details</summary>
Motivation: 提高道路安全、改善应急响应和指导公共卫生干预需要准确预测车祸伤害严重程度。传统结构化数据有限，需要探索非结构化叙述文本的附加价值。

Method: 使用肯塔基州2019-2023年车祸数据，采用TF-IDF和Word2Vec提取文本特征，KNN过采样处理不平衡，结合三种道路分类方案，使用XGBoost、Random Forest和AdaBoost三种集成算法建立预测模型。

Result: 包含叙述数据的模型始终优于仅使用结构化数据的模型，其中TF-IDF结合XGBoost在大多数子组中预测最准确。

Conclusion: 整合文本和结构化车祸信息能显著提升个人伤害预测准确性，为交通安全专业人员提供了实用的建模框架，可指导政策决策和设计更有效的对策。

Abstract: Predicting injuries and fatalities in traffic crashes plays a critical role
in enhancing road safety, improving emergency response, and guiding public
health interventions. This study investigates the added value of unstructured
crash narratives (written by police officers at the scene) when combined with
structured crash data to predict injury severity. Two widely used Natural
Language Processing (NLP) techniques, Term Frequency-Inverse Document Frequency
(TF-IDF) and Word2Vec, were employed to extract semantic meaning from the
narratives, and their effectiveness was compared. To address the challenge of
class imbalance, a K-Nearest Neighbors-based oversampling method was applied to
the training data prior to modeling. The dataset consists of crash records from
Kentucky spanning 2019 to 2023. To account for roadway heterogeneity, three
road classification schemes were used: (1) eight detailed functional classes
(e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) four
broader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and
(3) a unified dataset without classification. A total of 102 machine learning
models were developed by combining structured features and narrative-based
features using the two NLP techniques alongside three ensemble algorithms:
XGBoost, Random Forest, and AdaBoost. Results demonstrate that models
incorporating narrative data consistently outperform those relying solely on
structured data. Among all combinations, TF-IDF coupled with XGBoost yielded
the most accurate predictions in most subgroups. The findings highlight the
power of integrating textual and structured crash information to enhance
person-level injury prediction. This work offers a practical and adaptable
framework for transportation safety professionals to improve crash severity
modeling, guide policy decisions, and design more effective countermeasures.

</details>


### [74] [Addressing the Cold-Start Problem for Personalized Combination Drug Screening](https://arxiv.org/abs/2509.07850)
*Antoine de Mathelin,Christopher Tosh,Wesley Tansey*

Main category: cs.LG

TL;DR: 基于预训练深度学习模型的个性化能病组合疗法初始屏幕策略，通过药物嵌入聚类和杂式重权机制提高屏幕效率


<details>
  <summary>Details</summary>
Motivation: 解决个性化能病组合疗法中的冷启动问题，在缺乏患者特定信息时选择最信息化的初始药物组合进行实验

Method: 利用历史药物响应数据预训练深度学习模型，生成药物组合嵌入和杂式重要性分数，结合聚类确保功能多样性和杂式重权机制优先考虑历史信息化权重

Result: 大规模药物组合数据集上的回顾性模拟显示，该方法显著提高了初始屏幕效率，超过基线方法

Conclusion: 提出的策略为个性化能病组合疗法屏幕提供了可行的初始决策方案，有助于更有效的早期决策制定

Abstract: Personalizing combination therapies in oncology requires navigating an
immense space of possible drug and dose combinations, a task that remains
largely infeasible through exhaustive experimentation. Recent developments in
patient-derived models have enabled high-throughput ex vivo screening, but the
number of feasible experiments is limited. Further, a tight therapeutic window
makes gathering molecular profiling information (e.g. RNA-seq) impractical as a
means of guiding drug response prediction. This leads to a challenging
cold-start problem: how do we select the most informative combinations to test
early, when no prior information about the patient is available? We propose a
strategy that leverages a pretrained deep learning model built on historical
drug response data. The model provides both embeddings for drug combinations
and dose-level importance scores, enabling a principled selection of initial
experiments. We combine clustering of drug embeddings to ensure functional
diversity with a dose-weighting mechanism that prioritizes doses based on their
historical informativeness. Retrospective simulations on large-scale drug
combination datasets show that our method substantially improves initial
screening efficiency compared to baselines, offering a viable path for more
effective early-phase decision-making in personalized combination drug screens.

</details>


### [75] [Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy](https://arxiv.org/abs/2509.07872)
*Yajun Yu,Steve Jiang,Robert Timmerman,Hao Peng*

Main category: cs.LG

TL;DR: 开发基于多组学支持向量回归模型预测脑转移瘤GTV体积变化，整合影像组学、剂量组学和delta特征，在PULSAR放疗中实现个性化预测


<details>
  <summary>Details</summary>
Motivation: PULSAR是一种新型脉冲间隔放疗技术，准确预测肿瘤体积变化具有重要预后价值，需要开发定量预测模型来辅助患者选择和治疗调整

Method: 回顾性分析39例患者69个脑转移瘤，提取影像组学(MRI)和剂量组学特征，计算delta特征捕捉时间变化，使用Lasso特征选择，构建多核SVR模型，采用5折交叉验证重复10次

Result: 多组学整合模型优于单组学模型，delta特征显著提升预测精度，最佳模型达到R²=0.743和RRMSE=0.022的优异性能

Conclusion: 提出的多组学SVR模型在预测GTV连续变化方面表现优异，为PULSAR治疗提供了更定量化、个性化的患者选择和治疗调整方法

Abstract: Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)
is a novel treatment that delivers radiation in pulses of protracted intervals.
Accurate prediction of gross tumor volume (GTV) changes through regression
models has substantial prognostic value. This study aims to develop a
multi-omics based support vector regression (SVR) model for predicting GTV
change. A retrospective cohort of 39 patients with 69 brain metastases was
analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.
Delta features were computed to capture relative changes between two time
points. A feature selection pipeline using least absolute shrinkage and
selection operator (Lasso) algorithm with weight- or frequency-based ranking
criterion was implemented. SVR models with various kernels were evaluated using
the coefficient of determination (R2) and relative root mean square error
(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate
the limitation of small data size. Multi-omics models that integrate radiomics,
dosiomics, and their delta counterparts outperform individual-omics models.
Delta-radiomic features play a critical role in enhancing prediction accuracy
relative to features at single time points. The top-performing model achieves
an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows
promising performance in predicting continuous change of GTV. It provides a
more quantitative and personalized approach to assist patient selection and
treatment adjustment in PULSAR.

</details>


### [76] [A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges](https://arxiv.org/abs/2509.07887)
*Katherine Berry,Liang Cheng*

Main category: cs.LG

TL;DR: 本文综述了图神经网络在药物发现多个领域的应用，包括分子性质预测、药物-靶点结合亲和力预测、药物相互作用研究等，并为未来研究提供指导


<details>
  <summary>Details</summary>
Motivation: 图神经网络能够处理药物分子等图结构数据，在药物发现领域得到了广泛应用，产生了大量方法和模型，需要对这些研究进行系统性的综述和分类

Method: 对近期发表的文献进行全面综述，涵盖分子性质预测、药物-靶点结合亲和力预测、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计等多个研究类别

Result: 系统梳理了图神经网络在药物发现各领域的应用现状，总结了现有方法和模型的进展

Conclusion: 为图神经网络在药物发现领域的未来研究提供了指导方向和框架

Abstract: Graph Neural Networks (GNNs) have gained traction in the complex domain of
drug discovery because of their ability to process graph-structured data such
as drug molecule models. This approach has resulted in a myriad of methods and
models in published literature across several categories of drug discovery
research. This paper covers the research categories comprehensively with recent
papers, namely molecular property prediction, including drug-target binding
affinity prediction, drug-drug interaction study, microbiome interaction
prediction, drug repositioning, retrosynthesis, and new drug design, and
provides guidance for future work on GNNs for drug discovery.

</details>


### [77] [Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings](https://arxiv.org/abs/2509.07896)
*Philipp Lepold,Jonas Leichtle,Tobias Röddiger,Michael Beigl*

Main category: cs.LG

TL;DR: 研究表明单通道耳内电生理信号可用于自动睡眠分期，在可穿戴设备中实现90.5%的二元睡眠检测准确率和65.1%的四分类准确率


<details>
  <summary>Details</summary>
Motivation: 传统EEG睡眠监测设备笨重不便，限制了在家庭环境中的长期连续监测应用，需要开发更舒适便捷的睡眠监测方案

Method: 使用定制耳塞设备，在一只耳朵使用干耳尖电极作为测量电极，另一只耳朵作为参考电极，采集11名参与者的单通道耳内ExG信号，并以Apple Watch Ultra的睡眠分期作为地面真值

Result: 通过留一法交叉验证，系统在二元睡眠检测（清醒vs睡眠）上达到90.5%准确率，在四分类分期（清醒、REM、核心睡眠、深度睡眠）上达到65.1%准确率

Conclusion: 耳内电极作为一种低负担、舒适的睡眠监测方法具有巨大潜力，可用于如用户入睡时自动暂停媒体播放等消费级应用

Abstract: Automatic sleep staging typically relies on gold-standard EEG setups, which
are accurate but obtrusive and impractical for everyday use outside sleep
laboratories. This limits applicability in real-world settings, such as home
environments, where continuous, long-term monitoring is needed. Detecting sleep
onset is particularly relevant, enabling consumer applications (e.g.
automatically pausing media playback when the user falls asleep). Recent
research has shown correlations between in-ear EEG and full-scalp EEG for
various phenomena, suggesting wearable, in-ear devices could allow unobtrusive
sleep monitoring. We investigated the feasibility of using single-channel
in-ear electrophysiological (ExG) signals for automatic sleep staging in a
wearable device by conducting a sleep study with 11~participants (mean age:
24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse)
as a measurement electrode in one ear and a reference in the other. Ground
truth sleep stages were obtained from an Apple Watch Ultra, validated for sleep
staging. Our system achieved 90.5% accuracy for binary sleep detection (Awake
vs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)
using leave-one-subject-out validation. These findings demonstrate the
potential of in-ear electrodes as a low-effort, comfortable approach to sleep
monitoring, with applications such as stopping podcasts when users fall asleep.

</details>


### [78] [A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization](https://arxiv.org/abs/2509.07901)
*Qing-xin Meng,Xia Lei,Jian-wei Liu*

Main category: cs.LG

TL;DR: 这篇论文提出了一种模块化算法，解决在线凸凹优化问题中的动态对偶间隔最小化问题，达到了最小最大最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法在静态或可预测环境中无法获得最优性能，特别是在处理动态对偶间隔时表现不佳。

Method: 设计了包含三个核心模块的模块化算法：适应性模块动态调整非稳态性水平，多预测器聚合器选择最佳预测器，以及集成模块有效结合各模块优势。

Result: 算法达到了最小最大最优的D-DGap上界（至少一个对数因子），同时确保了预测错误驱动的D-DGap界。实验结果证明了方法的有效性和适应性。

Conclusion: 该模块化算法不仅解决了现有方法的限制，还允许灵活替换适应性模块和集成多源预测知识，为在线凸凹优化领域提供了有力的解决方案。

Abstract: This paper investigates the problem of Online Convex-Concave Optimization,
which extends Online Convex Optimization to two-player time-varying
convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap),
a critical performance measure that evaluates players' strategies against
arbitrary comparator sequences. Existing algorithms fail to deliver optimal
performance, particularly in stationary or predictable environments. To address
this, we propose a novel modular algorithm with three core components: an
Adaptive Module that dynamically adjusts to varying levels of non-stationarity,
a Multi-Predictor Aggregator that identifies the best predictor among multiple
candidates, and an Integration Module that effectively combines their
strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a
logarithmic factor, while also ensuring prediction error-driven D-DGap bounds.
The modular design allows for the seamless replacement of components that
regulate adaptability to dynamic environments, as well as the incorporation of
components that integrate ``side knowledge'' from multiple predictors.
Empirical results further demonstrate the effectiveness and adaptability of the
proposed method.

</details>


### [79] [Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings](https://arxiv.org/abs/2509.07905)
*Hamid Ahmad,Heiko Paulheim,Rita T. Sousa*

Main category: cs.LG

TL;DR: Bio-KGvec2go是一个扩展的Web API，用于生成和提供生物医学本体的知识图谱嵌入，支持定期更新以匹配本体版本发布，促进高效及时的生物医学研究。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和本体在现代AI应用中日益重要，但将语义资源与机器学习模型集成需要知识图谱嵌入模型。预训练模型可以避免重复训练，促进AI开发的民主化和可持续计算。

Method: 扩展KGvec2go Web API，专门为广泛使用的生物医学本体生成和提供知识图谱嵌入服务，并支持与本体版本发布同步的定期更新。

Result: 开发了Bio-KGvec2go系统，能够提供最新的嵌入表示，用户只需最小的计算努力即可获得，支持生物医学研究的效率和及时性。

Conclusion: Bio-KGvec2go通过提供易于访问和定期更新的生物医学本体嵌入，有效促进了生物医学研究的发展，降低了计算门槛，支持可持续的AI开发。

Abstract: Knowledge graphs and ontologies represent entities and their relationships in
a structured way, having gained significance in the development of modern AI
applications. Integrating these semantic resources with machine learning models
often relies on knowledge graph embedding models to transform graph data into
numerical representations. Therefore, pre-trained models for popular knowledge
graphs and ontologies are increasingly valuable, as they spare the need to
retrain models for different tasks using the same data, thereby helping to
democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,
designed to generate and serve knowledge graph embeddings for widely used
biomedical ontologies. Given the dynamic nature of these ontologies,
Bio-KGvec2go also supports regular updates aligned with ontology version
releases. By offering up-to-date embeddings with minimal computational effort
required from users, Bio-KGvec2go facilitates efficient and timely biomedical
research.

</details>


### [80] [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
*Arun Verma,Zhaoxuan Wu,Zijian Zhou,Xiaoqiang Lin,Zhiliang Chen,Rachael Hwee Ling Sim,Rui Qiao,Jingtan Wang,Nhung Bui,Xinyuan Niu,Wenyang Hu,Gregory Kang Ruey Lau,Zi-Yu Khoo,Zitong Zhao,Xinyi Xu,Apivich Hemachandra,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 本文主张通过逆问题方法发现LLM的缩放定律，以更经济高效的方式指导大语言模型的构建


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型训练成本高昂，传统的试错方法不可行，受逆问题在科学定律发现中的成功启发

Method: 提出使用逆问题方法来揭示指导LLM构建的缩放定律

Result: 该方法有望以显著更好的成本效益实现期望的性能

Conclusion: 逆问题方法可以高效地发现LLM缩放定律，为构建高性能大语言模型提供经济有效的指导

Abstract: Large Language Models (LLMs) are large-scale pretrained models that have
achieved remarkable success across diverse domains. These successes have been
driven by unprecedented complexity and scale in both data and computations.
However, due to the high costs of training such models, brute-force
trial-and-error approaches to improve LLMs are not feasible. Inspired by the
success of inverse problems in uncovering fundamental scientific laws, this
position paper advocates that inverse problems can also efficiently uncover
scaling laws that guide the building of LLMs to achieve the desirable
performance with significantly better cost-effectiveness.

</details>


### [81] [One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning](https://arxiv.org/abs/2509.07945)
*Yuan Pu,Yazhe Niu,Jia Tang,Junyu Xiong,Shuai Hu,Hongsheng Li*

Main category: cs.LG

TL;DR: ScaleZero通过混合专家架构和动态参数缩放策略，解决了异构多任务学习中梯度冲突和模型可塑性损失问题，在多个基准测试中达到与专用单任务模型相当的性能，且仅需80%的环境交互步骤。


<details>
  <summary>Details</summary>
Motivation: 传统多任务世界模型在处理大规模异构环境时存在梯度冲突和模型可塑性损失问题，限制了样本和计算效率。

Method: 采用混合专家(MoE)架构缓解梯度冲突，并提出基于LoRA的动态参数缩放(DPS)策略来动态平衡计算负载。

Result: 在Atari、DMControl和Jericho等基准测试中，ScaleZero仅使用在线强化学习就达到与专用单任务基线相当的性能，配合DPS策略后仅需80%的环境交互步骤。

Conclusion: ScaleZero展示了在大规模多任务学习中有效处理异构任务的潜力，为多任务强化学习提供了新的解决方案。

Abstract: In heterogeneous multi-task learning, tasks not only exhibit diverse
observation and action spaces but also vary substantially in intrinsic
difficulty. While conventional multi-task world models like UniZero excel in
single-task settings, we find that when handling large-scale heterogeneous
environments, gradient conflicts and the loss of model plasticity often
constrain their sample and computational efficiency. In this work, we address
these challenges from two perspectives: the single learning iteration and the
overall learning process. First, we investigate the impact of key design spaces
on extending UniZero to multi-task planning. We find that a Mixture-of-Experts
(MoE) architecture provides the most substantial performance gains by
mitigating gradient conflicts, leading to our proposed model,
\textit{ScaleZero}. Second, to dynamically balance the computational load
across the learning process, we introduce an online, LoRA-based \textit{dynamic
parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA
adapters in response to task-specific progress, enabling adaptive knowledge
retention and parameter expansion. Empirical evaluations on standard benchmarks
such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying
exclusively on online reinforcement learning with one model, attains
performance on par with specialized single-task baselines. Furthermore, when
augmented with our dynamic parameter scaling strategy, our method achieves
competitive performance while requiring only 80\% of the single-task
environment interaction steps. These findings underscore the potential of
ScaleZero for effective large-scale multi-task learning. Our code is available
at \textcolor{magenta}{https://github.com/opendilab/LightZero}.

</details>


### [82] [Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges](https://arxiv.org/abs/2509.07946)
*Kasra Borazjani,Naji Khosravan,Rajeev Sahay,Bita Akram,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 该论文提出了M3T联邦基础模型(FedFMs)用于教育领域，通过结合联邦学习和多模态多任务基础模型，实现跨机构的隐私保护协作训练。


<details>
  <summary>Details</summary>
Motivation: 解决多模态多任务基础模型在教育领域部署时面临的隐私法规、数据孤岛和领域特定数据有限的问题。

Method: 整合联邦学习(FL)与多模态多任务(M3T)基础模型，构建M3T FedFMs框架，支持去中心化机构的协作训练。

Result: 提出了一个有望推动下一代智能教育系统三大支柱(隐私保护、个性化、公平包容)的新范式。

Conclusion: M3T FedFMs是教育领域一个前景广阔但尚未充分探索的方法，需要解决异构隐私法规、数据模态特性不均等、模型遗忘、持续学习和可解释性等研究挑战。

Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown
transformative potential in artificial intelligence, with emerging applications
in education. However, their deployment in real-world educational settings is
hindered by privacy regulations, data silos, and limited domain-specific data
availability. We introduce M3T Federated Foundation Models (FedFMs) for
education: a paradigm that integrates federated learning (FL) with M3T FMs to
enable collaborative, privacy-preserving training across decentralized
institutions while accommodating diverse modalities and tasks. Subsequently,
this position paper aims to unveil M3T FedFMs as a promising yet underexplored
approach to the education community, explore its potentials, and reveal its
related future research directions. We outline how M3T FedFMs can advance three
critical pillars of next-generation intelligent education systems: (i) privacy
preservation, by keeping sensitive multi-modal student and institutional data
local; (ii) personalization, through modular architectures enabling tailored
models for students, instructors, and institutions; and (iii) equity and
inclusivity, by facilitating participation from underrepresented and
resource-constrained entities. We finally identify various open research
challenges, including studying of (i) inter-institution heterogeneous privacy
regulations, (ii) the non-uniformity of data modalities' characteristics, (iii)
the unlearning approaches for M3T FedFMs, (iv) the continual learning
frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must
be collectively addressed for practical deployment.

</details>


### [83] [ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)
*Oliver Daniels,Stuart Armstrong,Alexandre Maranhão,Mahirah Fairuz Rahman,Benjamin M. Marlin,Rebecca Gorman*

Main category: cs.LG

TL;DR: ACE方法通过自训练学习概念集合来解决完全伪相关导致的模型欠规范问题，在多个基准测试中表现优异，并在语言模型对齐中展现潜力


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对伪相关敏感，现有方法主要处理不完全伪相关，但完全伪相关会导致模型泛化能力欠规范，需要新方法来解决这一问题

Method: 提出ACE方法，通过自训练学习一组与训练数据一致但在新未标记输入上做出不同预测的概念，采用自信且选择性的分歧策略

Result: ACE在完全伪相关基准测试中匹配或优于现有方法，同时对不完全伪相关保持鲁棒性，在语言模型对齐任务中无需访问不可信测量即可达到竞争性能

Conclusion: ACE在克服模型欠规范方面取得显著进展，虽然仍有重要限制，但为处理完全伪相关问题提供了有效解决方案

Abstract: Deep neural networks are notoriously sensitive to spurious correlations -
where a model learns a shortcut that fails out-of-distribution. Existing work
on spurious correlations has often focused on incomplete
correlations,leveraging access to labeled instances that break the correlation.
But in cases where the spurious correlations are complete, the correct
generalization is fundamentally \textit{underspecified}. To resolve this
underspecification, we propose learning a set of concepts that are consistent
with training data but make distinct predictions on a subset of novel unlabeled
inputs. Using a self-training approach that encourages \textit{confident} and
\textit{selective} disagreement, our method ACE matches or outperforms existing
methods on a suite of complete-spurious correlation benchmarks, while remaining
robust to incomplete spurious correlations. ACE is also more configurable than
prior approaches, allowing for straight-forward encoding of prior knowledge and
principled unsupervised model selection. In an early application to
language-model alignment, we find that ACE achieves competitive performance on
the measurement tampering detection benchmark \textit{without} access to
untrusted measurements. While still subject to important limitations, ACE
represents significant progress towards overcoming underspecification.

</details>


### [84] [Customizing the Inductive Biases of Softmax Attention using Structured Matrices](https://arxiv.org/abs/2509.07963)
*Yilun Kuang,Noah Amsel,Sanae Lotfi,Shikai Qiu,Andres Potapczynski,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 提出了基于Block Tensor-Train和Multi-Level Low Rank矩阵的新评分函数，解决标准注意力机制在高维输入信息丢失和缺乏距离相关计算偏置的问题，在多个任务上表现优于标准注意力。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的低维投影会导致高维输入信息丢失，且对所有输入对使用相同的评分函数，缺乏对序列中相邻token的距离相关计算偏置。

Method: 提出基于计算高效结构化矩阵的新评分函数，包括Block Tensor-Train (BTT)和Multi-Level Low Rank (MLR)矩阵，这些矩阵具有高秩特性。

Result: 在高维输入的上下文回归任务中，新评分函数在任何固定计算预算下都优于标准注意力；在语言建模中，MLR注意力相比标准注意力和滑动窗口注意力变体实现了更好的扩展规律；在长程时间序列预测中也显示出有希望的结果。

Conclusion: BTT和MLR属于一个更广泛的能编码全秩或距离相关计算偏置的高效结构化矩阵家族，有效解决了标准注意力的重要缺陷。

Abstract: The core component of attention is the scoring function, which transforms the
inputs into low-dimensional queries and keys and takes the dot product of each
pair. While the low-dimensional projection improves efficiency, it causes
information loss for certain tasks that have intrinsically high-dimensional
inputs. Additionally, attention uses the same scoring function for all input
pairs, without imposing a distance-dependent compute bias for neighboring
tokens in the sequence. In this work, we address these shortcomings by
proposing new scoring functions based on computationally efficient structured
matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level
Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional
inputs, our proposed scoring functions outperform standard attention for any
fixed compute budget. On language modeling, a task that exhibits locality
patterns, our MLR-based attention method achieves improved scaling laws
compared to both standard attention and variants of sliding window attention.
Additionally, we show that both BTT and MLR fall under a broader family of
efficient structured matrices capable of encoding either full-rank or
distance-dependent compute biases, thereby addressing significant shortcomings
of standard attention. Finally, we show that MLR attention has promising
results for long-range time-series forecasting.

</details>


### [85] [Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence](https://arxiv.org/abs/2509.07972)
*Yuxing Liu,Yuze Ge,Rui Pan,An Kang,Tong Zhang*

Main category: cs.LG

TL;DR: 论文提出了新的广义平滑度假设，证明了学习率预热策略在理论和实践上的优势，显示预热可以加速梯度下降收敛速度


<details>
  <summary>Details</summary>
Motivation: 解决学习率预热策略在实践中广泛应用但理论优势未被充分理解的问题，弥合理论与实践之间的差距

Method: 提出新的广义平滑度假设家族，在确定性和随机设置下研究梯度下降的收敛性质，理论验证预热策略的有效性

Result: 学习率预热能持续加速梯度下降，在某些特定情况下比非递增学习率调度快Θ(T)倍收敛

Conclusion: 学习率预热策略在优化理论角度具有明确优势，为这一实用技术提供了理论依据

Abstract: Learning rate warmup is a popular and practical technique in training
large-scale deep neural networks. Despite the huge success in practice, the
theoretical advantages of this strategy of gradually increasing the learning
rate at the beginning of the training process have not been fully understood.
To resolve this gap between theory and practice, we first propose a novel
family of generalized smoothness assumptions, and validate its applicability
both theoretically and empirically. Under the novel smoothness assumption, we
study the convergence properties of gradient descent (GD) in both deterministic
and stochastic settings. It is shown that learning rate warmup consistently
accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times
faster than with a non-increasing learning rate schedule in some specific
cases, providing insights into the benefits of this strategy from an
optimization theory perspective.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [86] [ADHAM: Additive Deep Hazard Analysis Mixtures for Interpretable Survival Regression](https://arxiv.org/abs/2509.07108)
*Mert Ketenci,Vincent Jeanselme,Harry Reyes Nieva,Shalmali Joshi,Noémie Elhadad*

Main category: stat.ML

TL;DR: 提出了ADHAM模型，一种可解释的加性生存分析模型，通过潜在子群结构和协变量特异性风险函数来提供人口、子群和个体层面的可解释性，同时保持与最先进方法相当的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络生存分析方法虽然预测性能好，但缺乏对暴露因素与结果之间关联的可解释性洞察，而这是临床决策的关键需求。

Method: ADHAM采用条件潜在结构定义子群，每个子群由协变量特异性风险函数表征，并通过后训练精炼来合并相似子群以选择最优子群数量。

Result: 在真实世界数据集上的实验表明，ADHAM能够在人口、子群和个体层面提供新颖的可解释性洞察，同时预测性能与现有最先进基线方法相当。

Conclusion: ADHAM为医疗保健领域的时间到事件预测提供了一个可扩展且可解释的方法，在保持预测性能的同时提供了重要的临床解释性。

Abstract: Survival analysis is a fundamental tool for modeling time-to-event outcomes
in healthcare. Recent advances have introduced flexible neural network
approaches for improved predictive performance. However, most of these models
do not provide interpretable insights into the association between exposures
and the modeled outcomes, a critical requirement for decision-making in
clinical practice. To address this limitation, we propose Additive Deep Hazard
Analysis Mixtures (ADHAM), an interpretable additive survival model. ADHAM
assumes a conditional latent structure that defines subgroups, each
characterized by a combination of covariate-specific hazard functions. To
select the number of subgroups, we introduce a post-training refinement that
reduces the number of equivalent latent subgroups by merging similar groups. We
perform comprehensive studies to demonstrate ADHAM's interpretability at the
population, subgroup, and individual levels. Extensive experiments on
real-world datasets show that ADHAM provides novel insights into the
association between exposures and outcomes. Further, ADHAM remains on par with
existing state-of-the-art survival baselines in terms of predictive
performance, offering a scalable and interpretable approach to time-to-event
prediction in healthcare.

</details>


### [87] [NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice](https://arxiv.org/abs/2509.07123)
*Yuqi Zhou,Zhanhong Cheng,Lingqian Hu,Yuheng Bu,Shenhao Wang*

Main category: stat.ML

TL;DR: 本研究提出了基于替代图的嵌套效用图神经网络(NestGNN)，作为经典嵌套Logit模型的深度学习推广，在保持可解释性的同时显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 经典嵌套Logit模型在离散选择分析中应用广泛，但受限于表达能力不足和手工设计的效用函数。现有的深度网络方法无法显式捐捕替代品间的相关性。

Method: 提出替代图概念表示旅行方式替代品间关系，并设计嵌套效用图神经网络(NestGNN)。NestGNN在保持嵌套Logit模型两层替代模式的同时，扩展了模型的表达能力。

Result: NestGNN在实验中显著超过基准模型，比对应的嵌套Logit模型提升9.2%的性能。它保留了两层替代模式，同时具有更灵活的模型设计空间。

Conclusion: NestGNN在预测性能、可解释性和灵活性方面都显示出优势，成功将经典嵌套Logit模型推广到深度学习领域。

Abstract: Nested logit (NL) has been commonly used for discrete choice analysis,
including a wide range of applications such as travel mode choice, automobile
ownership, or location decisions. However, the classical NL models are
restricted by their limited representation capability and handcrafted utility
specification. While researchers introduced deep neural networks (DNNs) to
tackle such challenges, the existing DNNs cannot explicitly capture
inter-alternative correlations in the discrete choice context. To address the
challenges, this study proposes a novel concept - alternative graph - to
represent the relationships among travel mode alternatives. Using a nested
alternative graph, this study further designs a nested-utility graph neural
network (NestGNN) as a generalization of the classical NL model in the neural
network family. Theoretically, NestGNNs generalize the classical NL models and
existing DNNs in terms of model representation, while retaining the crucial
two-layer substitution patterns of the NL models: proportional substitution
within a nest but non-proportional substitution beyond a nest. Empirically, we
find that the NestGNNs significantly outperform the benchmark models,
particularly the corresponding NL models by 9.2\%. As shown by elasticity
tables and substitution visualization, NestGNNs retain the two-layer
substitution patterns as the NL model, and yet presents more flexibility in its
model design space. Overall, our study demonstrates the power of NestGNN in
prediction, interpretation, and its flexibility of generalizing the classical
NL model for analyzing travel mode choice.

</details>


### [88] [Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2509.07289)
*M. Hadi Sepanj,Benyamin Ghojogh,Paul Fieguth*

Main category: stat.ML

TL;DR: 提出了Kernel VICReg方法，将VICReg自监督学习目标核化到再生核希尔伯特空间中，实现了非线性特征学习，在多个数据集上表现优于欧几里得版本。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法主要在欧几里得空间操作，限制了捕捉非线性依赖和几何结构的能力，需要将SSL目标扩展到非线性空间。

Method: 通过将VICReg损失函数（方差、不变性、协方差）的每一项核化，使用双中心核矩阵和希尔伯特-施密特范数，在RKHS空间中实现非线性特征学习。

Result: 在MNIST、CIFAR-10、STL-10、TinyImageNet和ImageNet100等数据集上均取得一致性能提升，特别是在非线性结构明显的数据集上改进显著。UMAP可视化显示核化嵌入具有更好的等距性和类别分离。

Conclusion: 核化SSL目标是连接经典核方法和现代表示学习的有前景方向，能够避免表示坍塌并在复杂或小规模数据任务上提升性能。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
representation learning by optimizing geometric objectives--such as invariance
to augmentations, variance preservation, and feature decorrelation--without
requiring labels. However, most existing methods operate in Euclidean space,
limiting their ability to capture nonlinear dependencies and geometric
structures. In this work, we propose Kernel VICReg, a novel self-supervised
learning framework that lifts the VICReg objective into a Reproducing Kernel
Hilbert Space (RKHS). By kernelizing each term of the loss-variance,
invariance, and covariance--we obtain a general formulation that operates on
double-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinear
feature learning without explicit mappings.
  We demonstrate that Kernel VICReg not only avoids representational collapse
but also improves performance on tasks with complex or small-scale data.
Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, and
ImageNet100 show consistent gains over Euclidean VICReg, with particularly
strong improvements on datasets where nonlinear structures are prominent. UMAP
visualizations further confirm that kernel-based embeddings exhibit better
isometry and class separation. Our results suggest that kernelizing SSL
objectives is a promising direction for bridging classical kernel methods with
modern representation learning.

</details>


### [89] [Identifying Neural Signatures from fMRI using Hybrid Principal Components Regression](https://arxiv.org/abs/2509.07300)
*Jared Rieck,Julia Wrobel,Joshua L. Gowin,Yue Wang,Martin Paulus,Ryan Peterson*

Main category: stat.ML

TL;DR: 改进LASSO PCR方法，通过给主成分指数添加正则化处罚，提出新的JSRL混合方法，在风险决策、奖励处理和情绪调节任务中显著提升分类性能


<details>
  <summary>Details</summary>
Motivation: 传统LASSO PCR假设所有主成分都同等重要，但实际任务相关信号可能集中在特定成分中，导致次优成分选择

Method: 修改LASSO PCR添加与主成分指数相关的正则化处罚，提出JSRL混合方法，结合成分级和像素级活动并应用排序稀疏性

Result: 稀疏排序模型显著提升分类性能，JSRL在交叉验证偏差R²上提升51.7%，AUC提升7.3%，且分配的预测权重与脑区功能一致

Conclusion: 给主成分指数添加正则化处罚的方法提供了更健壮的MVPA方案，能更有效地解码脑活动模式

Abstract: Recent advances in neuroimaging analysis have enabled accurate decoding of
mental state from brain activation patterns during functional magnetic
resonance imaging scans. A commonly applied tool for this purpose is principal
components regression regularized with the least absolute shrinkage and
selection operator (LASSO PCR), a type of multi-voxel pattern analysis (MVPA).
This model presumes that all components are equally likely to harbor relevant
information, when in fact the task-related signal may be concentrated in
specific components. In such cases, the model will fail to select the optimal
set of principal components that maximizes the total signal relevant to the
cognitive process under study. Here, we present modifications to LASSO PCR that
allow for a regularization penalty tied directly to the index of the principal
component, reflecting a prior belief that task-relevant signal is more likely
to be concentrated in components explaining greater variance. Additionally, we
propose a novel hybrid method, Joint Sparsity-Ranked LASSO (JSRL), which
integrates component-level and voxel-level activity under an information parity
framework and imposes ranked sparsity to guide component selection. We apply
the models to brain activation during risk taking, monetary incentive, and
emotion regulation tasks. Results demonstrate that incorporating sparsity
ranking into LASSO PCR produces models with enhanced classification
performance, with JSRL achieving up to 51.7\% improvement in cross-validated
deviance $R^2$ and 7.3\% improvement in cross-validated AUC. Furthermore,
sparsity-ranked models perform as well as or better than standard LASSO PCR
approaches across all classification tasks and allocate predictive weight to
brain regions consistent with their established functional roles, offering a
robust alternative for MVPA.

</details>


### [90] [Asynchronous Gossip Algorithms for Rank-Based Statistical Methods](https://arxiv.org/abs/2509.07543)
*Anna Van Elst,Igor Colin,Stephan Clémençon*

Main category: stat.ML

TL;DR: 提出了基于gossip算法的分布式秩统计量计算方法，用于增强去中心化AI系统的鲁棒性，特别是在存在对抗性数据污染的情况下。


<details>
  <summary>Details</summary>
Motivation: 随着去中心化AI和边缘智能的普及，传统分布式算法依赖简单统计量（如均值）容易受到数据污染攻击，需要更鲁棒的统计方法来确保分布式环境下的可信度。

Method: 开发了基于gossip算法的秩统计量计算方法，包括L-统计量和秩统计量，这些方法对异常值具有鲁棒性。应用于分布式两样本假设检验，首次实现了Wilcoxon秩和检验的gossip算法。

Result: 提供了严格的收敛保证，包括首个异步gossip秩估计的收敛率边界。通过在不同网络拓扑上的实验验证了理论结果。

Conclusion: 该方法为去中心化环境提供了有效的鲁棒统计计算框架，能够抵抗数据污染攻击，具有重要的理论和实践价值。

Abstract: As decentralized AI and edge intelligence become increasingly prevalent,
ensuring robustness and trustworthiness in such distributed settings has become
a critical issue-especially in the presence of corrupted or adversarial data.
Traditional decentralized algorithms are vulnerable to data contamination as
they typically rely on simple statistics (e.g., means or sum), motivating the
need for more robust statistics. In line with recent work on decentralized
estimation of trimmed means and ranks, we develop gossip algorithms for
computing a broad class of rank-based statistics, including L-statistics and
rank statistics-both known for their robustness to outliers. We apply our
method to perform robust distributed two-sample hypothesis testing, introducing
the first gossip algorithm for Wilcoxon rank-sum tests. We provide rigorous
convergence guarantees, including the first convergence rate bound for
asynchronous gossip-based rank estimation. We empirically validate our
theoretical results through experiments on diverse network topologies.

</details>
