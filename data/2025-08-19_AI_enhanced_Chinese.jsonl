{"id": "2508.11640", "categories": ["eess.SP", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11640", "abs": "https://arxiv.org/abs/2508.11640", "authors": ["Danny Scott", "William LaForest", "Hritom Das", "Ioannis Polykretis", "Catherine D. Schuman", "Charles Rizzo", "James Plank", "Sai Swaminathan"], "title": "Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks", "comment": "International Conference on Neuromorphic Systems (ICONS) 2025 9\n  pages, 7 images", "summary": "The deployment of dense, low-cost sensors is critical for realizing\nubiquitous smart environments. However, existing sensing solutions struggle\nwith the energy, scalability, and reliability trade-offs imposed by battery\nmaintenance, wireless transmission overhead, and data processing complexity. In\nthis work, we present Vibe2Spike, a novel battery-free, wireless sensing\nframework that enables vibration-based activity recognition using visible light\ncommunication (VLC) and spiking neural networks (SNNs). Our system uses\nultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and\nan LED, which harvest vibration energy and emit sparse visible light spikes\nwithout requiring batteries or RF radios. These optical spikes are captured by\nevent cameras and classified using optimized SNN models evolved via the EONS\nframework. We evaluate Vibe2Spike across five device classes, achieving 94.9\\%\naverage classification fitness while analyzing the latency-accuracy trade-offs\nof different temporal binning strategies. Vibe2Spike demonstrates a scalable,\nand energy-efficient approach for enabling intelligent environments in a\nbatteryless manner.", "AI": {"tldr": "Vibe2Spike\u662f\u4e00\u4e2a\u65e0\u7535\u6c60\u65e0\u7ebf\u4f20\u611f\u6846\u67b6\uff0c\u5229\u7528\u538b\u7535\u76d8\u3001\u9f50\u7eb3\u4e8c\u6781\u7ba1\u548cLED\u7ec4\u6210\u7684\u8d85\u4f4e\u6210\u672c\u6807\u7b7e\uff0c\u901a\u8fc7\u53ef\u89c1\u5149\u901a\u4fe1\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u57fa\u4e8e\u632f\u52a8\u7684\u6d3b\u52a8\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u4f20\u611f\u89e3\u51b3\u65b9\u6848\u5728\u7535\u6c60\u7ef4\u62a4\u3001\u65e0\u7ebf\u4f20\u8f93\u5f00\u9500\u548c\u6570\u636e\u5904\u7406\u590d\u6742\u6027\u65b9\u9762\u5b58\u5728\u80fd\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65e0\u7535\u6c60\u4f20\u611f\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u538b\u7535\u76d8\u6536\u96c6\u632f\u52a8\u80fd\u91cf\uff0c\u901a\u8fc7LED\u53d1\u5c04\u7a00\u758f\u53ef\u89c1\u5149\u8109\u51b2\uff0c\u4e8b\u4ef6\u76f8\u673a\u6355\u83b7\u5149\u8109\u51b2\uff0c\u91c7\u7528EONS\u6846\u67b6\u4f18\u5316\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u4e94\u4e2a\u8bbe\u5907\u7c7b\u522b\u4e0a\u5b9e\u73b0\u4e8694.9%\u7684\u5e73\u5747\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65f6\u95f4\u5206\u7bb1\u7b56\u7565\u7684\u5ef6\u8fdf-\u51c6\u786e\u6027\u6743\u8861\u3002", "conclusion": "Vibe2Spike\u5c55\u793a\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u9ad8\u80fd\u6548\u7684\u65e0\u7535\u6c60\u667a\u80fd\u73af\u5883\u5b9e\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2508.11654", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11654", "abs": "https://arxiv.org/abs/2508.11654", "authors": ["Yang Zhao", "Tao Wang", "Said Elhadi"], "title": "Data-driven RF Tomography via Cross-modal Sensing and Continual Learning", "comment": "6 pages, 4 figures, to be published in IEEE AVSS Conference", "summary": "Data-driven radio frequency (RF) tomography has demonstrated significant\npotential for underground target detection, due to the penetrative nature of RF\nsignals through soil. However, it is still challenging to achieve accurate and\nrobust performance in dynamic environments. In this work, we propose a\ndata-driven radio frequency tomography (DRIFT) framework with the following key\ncomponents to reconstruct cross section images of underground root tubers, even\nwith significant changes in RF signals. First, we design a cross-modal sensing\nsystem with RF and visual sensors, and propose to train an RF tomography deep\nneural network (DNN) model following the cross-modal learning approach. Then we\npropose to apply continual learning to automatically update the DNN model, once\nenvironment changes are detected in a dynamic environment. Experimental results\nshow that our approach achieves an average equivalent diameter error of 2.29\ncm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and\ndataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u51faDRIFT\u6846\u67b6\u6765\u63d0\u9ad8\u5730\u4e0b\u6839\u5757\u85e4\u7684\u65e0\u7ebf\u7535\u5c04\u9891\u6210\u50cf\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u5730\u4e0b\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728RF\u4fe1\u53f7\u53d1\u751f\u663e\u8457\u53d8\u5316\u65f6\u4fdd\u6301\u51c6\u786e\u7684\u6210\u50cf\u6027\u80fd", "method": "\u8bbe\u8ba1\u65e0\u7ebf\u7535\u5c04\u9891\u548c\u89c6\u89c9\u4f20\u611f\u5668\u7684\u8de8\u6a21\u6001\u611f\u77e5\u7cfb\u7edf\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3RF\u6210\u50cfDNN\u6a21\u578b\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f7f\u7528\u6301\u7eed\u5b66\u4e60\u81ea\u52a8\u66f4\u65b0\u6a21\u578b", "result": "\u5e73\u5747\u76f8\u5f53\u76f4\u5f84\u8bef\u5dee\u4e3a2.29cm\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad823.2%\u7684\u6027\u80fd\u6539\u5584", "conclusion": "DRIFT\u6846\u67b6\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u51c6\u786e\u548c\u9ad8\u7a33\u5065\u6027\u7684\u5730\u4e0b\u6839\u5757\u85e4\u6210\u50cf\uff0c\u4e3a\u5730\u4e0b\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11656", "categories": ["eess.SP", "cs.LG", "I.2.6; I.5.1; I.5.4; I.2.1; J.3"], "pdf": "https://arxiv.org/pdf/2508.11656", "abs": "https://arxiv.org/abs/2508.11656", "authors": ["Ridma Jayasundara", "Ishan Fernando", "Adeepa Fernando", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "Inductive transfer learning from regression to classification in ECG analysis", "comment": "This manuscript is 15 pages with 4 tables and 5 figures. The\n  manuscript is under review at Nature Scientific Reports", "summary": "Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,\naccounting for over 30% of global deaths according to the World Health\nOrganization (WHO). Importantly, one-third of these deaths are preventable with\ntimely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive\nmethod for recording the electrical activity of the heart, is crucial for\ndiagnosing CVDs. However, privacy concerns surrounding the use of patient ECG\ndata in research have spurred interest in synthetic data, which preserves the\nstatistical properties of real data without compromising patient\nconfidentiality. This study explores the potential of synthetic ECG data for\ntraining deep learning models from regression to classification tasks and\nevaluates the feasibility of transfer learning to enhance classification\nperformance on real ECG data. We experimented with popular deep learning models\nto predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,\nQT interval, and QRS complex-using separate regression models. Subsequently, we\nleveraged these regression models for transfer learning to perform 5-class ECG\nsignal classification. Our experiments systematically investigate whether\ntransfer learning from regression to classification is viable, enabling better\nutilization of diverse open-access and synthetic ECG datasets. Our findings\ndemonstrate that transfer learning from regression to classification improves\nclassification performance, highlighting its potential to maximize the utility\nof available data and advance deep learning applications in this domain.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u5408\u6210ECG\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4ece\u56de\u5f52\u4efb\u52a1\u5230\u5206\u7c7b\u4efb\u52a1\u7684\u8fc1\u79fb\u5b66\u4e60\u6765\u63d0\u9ad8\u5bf9\u771f\u5b9eECG\u6570\u636e\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u6b7b\u56e0\u4e3b\u56e0\uff0c\u65e9\u671f\u8bca\u65ad\u53ef\u9884\u9632\u6b7b\u4ea1\u3002ECG\u662f\u5173\u952e\u8bca\u65ad\u5de5\u5177\uff0c\u4f46\u75c5\u4eba\u9690\u79c1\u95ee\u9898\u4fc3\u4f7f\u4eba\u4eec\u5bfb\u627e\u5408\u6210\u6570\u636e\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u56db\u4e2a\u5173\u952e\u5fc3\u81d4\u53c2\u6570\uff08\u5fc3\u7387\u3001PR\u95f4\u671f\u3001QT\u95f4\u671f\u3001QRS\u590d\u5408\u6ce2\uff09\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e9b\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u5b8c\u62105\u7c7bECG\u4fe1\u53f7\u5206\u7c7b\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u4ece\u56de\u5f52\u5230\u5206\u7c7b\u80fd\u591f\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u66f4\u597d\u5229\u7528\u5408\u6210ECG\u6570\u636e\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "\u8fd9\u79cd\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u6709\u52a9\u4e8e\u6700\u5927\u5316\u5229\u7528\u73b0\u6709\u6570\u636e\uff0c\u63a8\u52a8\u6df1\u5ea6\u5b66\u4e60\u5728ECG\u9884\u6d4b\u9886\u57df\u7684\u5e94\u7528\u8fdb\u6b65\u3002"}}
{"id": "2508.11657", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11657", "abs": "https://arxiv.org/abs/2508.11657", "authors": ["Yuanhao Li", "Badong Chen", "Wenjun Bai", "Yasuharu Koike", "Okito Yamashita"], "title": "Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding", "comment": null, "summary": "Objective: Sparse Bayesian learning provides an effective scheme to solve the\nhigh-dimensional problem in brain signal decoding. However, traditional\nassumptions regarding data distributions such as Gaussian and binomial are\npotentially inadequate to characterize the noisy signals of brain activity.\nHence, this study aims to propose a robust sparse Bayesian learning framework\nto address noisy highdimensional brain activity decoding. Methods: Motivated by\nthe commendable robustness of the minimum error entropy (MEE) criterion for\nhandling complex data distributions, we proposed an MEE-based likelihood\nfunction to facilitate the accurate inference of sparse Bayesian learning in\nanalyzing noisy brain datasets. Results: Our proposed approach was evaluated\nusing two high-dimensional brain decoding tasks in regression and\nclassification contexts, respectively. The experimental results showed that,\nour approach can realize superior decoding metrics and physiological patterns\nthan the conventional and state-of-the-art methods. Conclusion: Utilizing the\nproposed MEE-based likelihood model, sparse Bayesian learning is empowered to\nsimultaneously address the challenges of noise and high dimensionality in the\nbrain decoding task. Significance: This work provides a powerful tool to\nrealize robust brain decoding, advancing biomedical engineering applications\nsuch as brain-computer interface.", "AI": {"tldr": "\u57fa\u4e8e\u6700\u5c0f\u9519\u8bef\u76c8\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7MEE\u51c6\u5219\u63d0\u9ad8\u5bf9\u811a\u672f\u5f02\u5e38\u503e\u5410\u7684\u9c81\u68d2\u6027\uff0c\u5728\u9ad8\u7ef4\u8111\u7535\u4fe1\u53f7\u89e3\u7801\u4efb\u52a1\u4e2d\u83b7\u5f97\u66f4\u4f18\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u4f20\u7edf\u7684\u9ad8\u65af\u548c\u4e8c\u9879\u5206\u5e03\u5047\u8bbe\u5728\u5bf9\u4ed8\u8111\u7535\u4fe1\u53f7\u7684\u566a\u58f0\u65f6\u53ef\u80fd\u4e0d\u5145\u5206\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u65b9\u6cd5\u6765\u5904\u7406\u9ad8\u7ef4\u5ea6\u8111\u6d3b\u52a8\u89e3\u7801\u4e2d\u7684\u566a\u58f0\u95ee\u9898", "method": "\u91c7\u7528\u6700\u5c0f\u9519\u8bef\u76c8(MEE)\u51c6\u5219\u6784\u5efa\u9c81\u68d2\u7684\u53ef\u80fd\u6027\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u6570\u636e\u5206\u5e03\u548c\u566a\u58f0", "result": "\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4e24\u79cd\u9ad8\u7ef4\u5ea6\u8111\u7535\u89e3\u7801\u4efb\u52a1\u4e2d\uff0c\u65b0\u65b9\u6cd5\u5728\u89e3\u7801\u6307\u6807\u548c\u751f\u7269\u6a21\u5f0f\u65b9\u9762\u90fd\u8d85\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u6700\u65b0\u7684\u72b6\u6001\u4e0b\u7684\u65b9\u6cd5", "conclusion": "\u901a\u8fc7MEE\u57fa\u4e8e\u53ef\u80fd\u6027\u6a21\u578b\uff0c\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60\u80fd\u591f\u540c\u65f6\u89e3\u51b3\u8111\u7535\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\u566a\u58f0\u548c\u9ad8\u7ef4\u5ea6\u6311\u6218\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u7b49\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177"}}
{"id": "2508.12703", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12703", "abs": "https://arxiv.org/abs/2508.12703", "authors": ["Thomas Krug", "Fabian Raisch", "Dominik Aimer", "Markus Wirnsberger", "Ferdinand Sigg", "Benjamin Sch\u00e4fer", "Benjamin Tischler"], "title": "BUILDA: A Thermal Building Data Generation Framework for Transfer Learning", "comment": "Proceedings can be accessed at:\n  https://annsim.org/2025-annsim-proceedings/", "summary": "Transfer learning (TL) can improve data-driven modeling of building thermal\ndynamics. Therefore, many new TL research areas emerge in the field, such as\nselecting the right source model for TL. However, these research directions\nrequire massive amounts of thermal building data which is lacking presently.\nNeither public datasets nor existing data generators meet the needs of TL\nresearch in terms of data quality and quantity. Moreover, existing data\ngeneration approaches typically require expert knowledge in building\nsimulation. We present BuilDa, a thermal building data generation framework for\nproducing synthetic data of adequate quality and quantity for TL research. The\nframework does not require profound building simulation knowledge to generate\nlarge volumes of data. BuilDa uses a single-zone Modelica model that is\nexported as a Functional Mock-up Unit (FMU) and simulated in Python. We\ndemonstrate BuilDa by generating data and utilizing it for pretraining and\nfine-tuning TL models.", "AI": {"tldr": "BuilDa\u662f\u4e00\u4e2a\u70ed\u5efa\u7b51\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u65e0\u9700\u6df1\u539a\u5efa\u7b51\u6a21\u62df\u77e5\u8bc6\u5373\u53ef\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u8fc1\u79fb\u5b66\u4e60\u53ef\u4ee5\u6539\u8fdb\u5efa\u7b51\u70ed\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u4f46\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u548c\u6570\u636e\u751f\u6210\u5668\u65e0\u6cd5\u6ee1\u8db3\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u5bf9\u6570\u636e\u8d28\u91cf\u548c\u6570\u91cf\u7684\u9700\u6c42\uff0c\u4e14\u901a\u5e38\u9700\u8981\u5efa\u7b51\u6a21\u62df\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u5355\u533a\u57dfModelica\u6a21\u578b\u5bfc\u51fa\u4e3a\u529f\u80fd\u6a21\u62df\u5355\u5143(FMU)\uff0c\u5728Python\u4e2d\u8fdb\u884c\u6a21\u62df\uff0c\u6784\u5efaBuilDa\u6846\u67b6\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u6210\u529f\u751f\u6210\u6570\u636e\u5e76\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "BuilDa\u6846\u67b6\u80fd\u591f\u4e3a\u8fc1\u79fb\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u8db3\u591f\u8d28\u91cf\u548c\u6570\u91cf\u7684\u70ed\u5efa\u7b51\u6570\u636e\uff0c\u4e14\u4e0d\u9700\u8981\u6df1\u539a\u7684\u5efa\u7b51\u6a21\u62df\u4e13\u4e1a\u77e5\u8bc6\u3002"}}
{"id": "2508.11741", "categories": ["stat.ML", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.11741", "abs": "https://arxiv.org/abs/2508.11741", "authors": ["Habibolla Latifizadeh", "Anika C. Pirkey", "Alanna Gould", "David J. Klinke II"], "title": "BaMANI: Bayesian Multi-Algorithm causal Network Inference", "comment": "12 pages, 6 figures", "summary": "Improved computational power has enabled different disciplines to predict\ncausal relationships among modeled variables using Bayesian network inference.\nWhile many alternative algorithms have been proposed to improve the efficiency\nand reliability of network prediction, the predicted causal networks reflect\nthe generative process but also bear an opaque imprint of the specific\ncomputational algorithm used. Following a ``wisdom of the crowds\" strategy, we\ndeveloped an ensemble learning approach to marginalize the impact of a single\nalgorithm on Bayesian causal network inference. To introduce the approach, we\nfirst present the theoretical foundation of this framework. Next, we present a\ncomprehensive implementation of the framework in terms of a new software tool\ncalled BaMANI (Bayesian Multi-Algorithm causal Network Inference). Finally, we\ndescribe a BaMANI use-case from biology, particularly within human breast\ncancer studies.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5BaMANI\uff0c\u7efc\u5408\u591a\u79cd\u8ba1\u7b97\u7b97\u6cd5\u6765\u63d0\u9ad8\u8d1d\u53f6\u65af\u56e0\u679c\u7f51\u7edc\u63a8\u65ad\u7684\u7a33\u5065\u6027\u548c\u53ef\u9760\u6027", "motivation": "\u4e0d\u540c\u8ba1\u7b97\u7b97\u6cd5\u5bfc\u81f4\u9884\u6d4b\u7684\u56e0\u679c\u7f51\u7edc\u5b58\u5728\u504f\u5dee\uff0c\u9700\u8981\u51cf\u5c11\u5355\u4e00\u7b97\u6cd5\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd", "method": "\u91c7\u7528\"\u96c6\u4f53\u667a\u6167\"\u7b56\u7565\uff0c\u5f00\u53d1BaMANI\u8f6f\u4ef6\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u7b97\u6cd5\u96c6\u6210\u6765\u63a8\u65ad\u56e0\u679c\u7f51\u7edc", "result": "\u5b9e\u73b0\u4e86\u4e00\u79cd\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u5728\u4eba\u7c7b\u4e73\u817a\u764c\u7814\u7a76\u4e2d\u5e94\u7528\u9a8c\u8bc1", "conclusion": "\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u53ef\u6709\u6548\u964d\u4f4e\u5355\u4e00\u7b97\u6cd5\u7684\u504f\u5dee\uff0c\u63d0\u9ad8\u8d1d\u53f6\u65af\u56e0\u679c\u7f51\u7edc\u63a8\u65ad\u7684\u53ef\u9760\u6027"}}
{"id": "2508.11661", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11661", "abs": "https://arxiv.org/abs/2508.11661", "authors": ["Ziyi Cao", "Qingyi Si", "Jingbin Zhang", "Bingquan Liu"], "title": "Sparse Attention across Multiple-context KV Cache", "comment": null, "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios.", "AI": {"tldr": "SamKV\u9996\u6b21\u63a2\u7d22\u4e86\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u5176\u4ed6\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u4fe1\u606f\u8fdb\u884c\u7a00\u758f\u5316\u5e76\u5c40\u90e8\u91cd\u8ba1\u7b97\uff0c\u5728RAG\u573a\u666f\u4e2d\u5b9e\u73b015%\u7684\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6", "motivation": "\u4f20\u7edfKV\u7f13\u5b58\u91cd\u7528\u65b9\u6cd5\u53ea\u9002\u7528\u4e8e\u5355\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u5728RAG\u591a\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7531\u4e8e\u7f3a\u4e4f\u8de8\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u800c\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4fdd\u7559\u5168\u90e8KV\u7f13\u5b58\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927", "method": "SamKV\u5728\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u7a00\u758f\u5316\u65f6\u8003\u8651\u5176\u4ed6\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u7136\u540e\u5bf9\u7a00\u758f\u5316\u4fe1\u606f\u8fdb\u884c\u5c40\u90e8\u91cd\u8ba1\u7b97", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5c06\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u81f315%\uff0c\u76f8\u6bd4\u5b8c\u5168\u91cd\u8ba1\u7b97\u57fa\u7ebf\u65e0\u7cbe\u5ea6\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u7684\u541e\u5410\u91cf", "conclusion": "SamKV\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u6ce8\u610f\u529b\u7a00\u758f\u5316\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u89e3\u51b3\u4e86RAG\u573a\u666f\u4e2d\u7684\u5185\u5b58\u548c\u6548\u7387\u95ee\u9898"}}
{"id": "2508.11658", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11658", "abs": "https://arxiv.org/abs/2508.11658", "authors": ["Honggui Li", "Zhengyang Zhang", "Dingtai Li", "Sinan Chen", "Nahid Md Lokman Hossain", "Xinfeng Xu", "Yuting Feng", "Hantao Lu", "Yinlu Qin", "Ruobing Wang", "Maria Trocan", "Dimitri Galayko", "Amara Amara", "Mohamad Sawan"], "title": "CECGSR: Circular ECG Super-Resolution", "comment": null, "summary": "The electrocardiogram (ECG) plays a crucial role in the diagnosis and\ntreatment of various cardiac diseases. ECG signals suffer from low-resolution\n(LR) due to the use of convenient acquisition devices, as well as internal and\nexternal noises and artifacts. Classical ECG super-resolution (ECGSR) methods\nadopt an open-loop architecture that converts LR ECG signals to\nsuper-resolution (SR) ones. According to the theory of automatic control, a\nclosed-loop framework exhibits superior dynamic and static performance compared\nwith its open-loop counterpart. This paper proposes a closed-loop approach,\ntermed circular ECGSR (CECGSR), which models the degradation process from SR\nECG signals to LR ones. The negative feedback mechanism of the closed-loop\nsystem is based on the differences between the LR ECG signals. A mathematical\nloop equation is constructed to characterize the closed-loop infrastructure.\nThe Taylor series expansion is employed to demonstrate the near-zero\nsteady-state error of the proposed method. A Plug-and-Play strategy is\nconsidered to establish the SR unit of the proposed architecture, leveraging\nany existing advanced open-loop ECGSR methods. Simulation experiments on both\nnoiseless and noisy subsets of the PTB-XL datasets demonstrate that the\nproposed CECGSR outperforms state-of-the-art open-loop ECGSR algorithms in the\nreconstruction performance of ECG signals.", "AI": {"tldr": "\u95ed\u73af\u5faa\u73af\u5fc3\u7535\u56fe\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5CECGSR\uff0c\u901a\u8fc7\u5efa\u6a21\u964d\u7ea7\u8fc7\u7a0b\u548c\u8d1f\u53cd\u9988\u673a\u5236\uff0c\u5728PTB-XL\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u73af\u65b9\u6cd5\u7684\u91cd\u5efa\u6027\u80fd", "motivation": "\u5fc3\u7535\u56fe\u4fe1\u53f7\u5b58\u5728\u5206\u8fa8\u7387\u4f4e\u3001\u566a\u58f0\u5e72\u6270\u95ee\u9898\uff0c\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528\u5f00\u73af\u67b6\u6784\uff0c\u800c\u95ed\u73af\u63a7\u5236\u7406\u8bba\u8868\u660e\u95ed\u73af\u7cfb\u7edf\u5177\u6709\u66f4\u4f18\u7684\u52a8\u6001\u548c\u9759\u6001\u6027\u80fd", "method": "\u63d0\u51faCECGSR\u95ed\u73af\u65b9\u6cd5\uff0c\u5efa\u6a21\u4ece\u9ad8\u5206\u8fa8\u7387\u5230\u4f4e\u5206\u8fa8\u7387\u7684\u964d\u7ea7\u8fc7\u7a0b\uff0c\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387\u4fe1\u53f7\u5dee\u5f02\u6784\u5efa\u8d1f\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u7528\u6570\u5b66\u5faa\u73af\u65b9\u7a0b\u548c\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u8bc1\u660e\u7a33\u6001\u8bef\u5dee\u8fd1\u96f6\uff0c\u91c7\u7528Plug-and-Play\u7b56\u7565\u96c6\u6210\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5", "result": "\u5728PTB-XL\u6570\u636e\u96c6\u7684\u65e0\u566a\u58f0\u548c\u6709\u566a\u58f0\u5b50\u96c6\u4e0a\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793aCECGSR\u5728\u5fc3\u7535\u56fe\u4fe1\u53f7\u91cd\u5efa\u6027\u80fd\u65b9\u9762\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u73af\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5", "conclusion": "\u95ed\u73af\u5faa\u73af\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5CECGSR\u901a\u8fc7\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5728\u5fc3\u7535\u56fe\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u7684\u4f18\u52d2\u6027\uff0c\u4e3a\u5fc3\u7535\u56fe\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11847", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11847", "abs": "https://arxiv.org/abs/2508.11847", "authors": ["Jenny Y. Huang", "Yunyi Shen", "Dennis Wei", "Tamara Broderick"], "title": "Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings", "comment": null, "summary": "We propose a method for evaluating the robustness of a widely used LLM\nranking system -- the Bradley--Terry ranking system -- to dropping a worst-case\nvery small fraction of evaluation data. Our approach is computationally fast\nand easy to adopt. When we apply our method to matchups from two popular\nhuman-preference platforms, Chatbot Arena and MT-Bench, we find that the\nBradley--Terry rankings of top-performing models are remarkably sensitive to\nthe removal of a small fraction of evaluations. Our framework also identifies\nthe specific evaluations most responsible for such ranking flips, allowing for\ninspections of these influential preferences. We observe that the rankings\nderived from MT-Bench preferences are notably more robust than those from\nChatbot Arena, likely due to MT-bench's use of expert annotators and carefully\nconstructed prompts. Finally, we find that rankings based on crowdsourced\nhuman-evaluated systems are just as sensitive as those based on LLM-as-a-judge\nevaluations, where in both, dropping as little as 0.02% of the total\nevaluations in the dataset can change the top-ranked model.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30Bradley-Terry\u6392\u540d\u7cfb\u7edf\u7a33\u5065\u6027\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5c3d\u7ba1\u53ea\u5220\u96640.02%\u7684\u6700\u574f\u60c5\u51b5\u6570\u636e\uff0c\u4e5f\u80fd\u5bfc\u81f4\u9876\u7ea7\u6a21\u578b\u6392\u540d\u53d1\u751f\u7ffb\u8f6c\uff0c\u5c55\u73b0\u4e86\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u7814\u7a76Bradley-Terry\u6392\u540d\u7cfb\u7edf\u5bf9\u6781\u5c0f\u90e8\u5206\u6570\u636e\u5220\u9664\u7684\u654f\u611f\u6027\uff0c\u4ee5\u4e86\u89e3\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u8bc4\u4f30\u7cfb\u7edf\u7684\u7a33\u5065\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u8bbe\u8ba1\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u8bc4\u4f30\u6392\u540d\u7cfb\u7edf\u7684\u7a33\u5065\u6027\uff0c\u5e76\u5728Chatbot Arena\u548cMT-Bench\u4e24\u4e2a\u4eba\u7c7b\u504f\u597d\u5e73\u53f0\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5bfc\u81f4\u6392\u540d\u53d8\u5316\u7684\u5173\u952e\u8bc4\u4f30\u6570\u636e\u3002", "result": "\u53d1\u73b0\u9876\u7ea7\u6a21\u578b\u7684Bradley-Terry\u6392\u540d\u5bf9\u6781\u5c0f\u6570\u636e\u5220\u9664\u975e\u5e38\u654f\u611f\uff0c\u4ec5\u5220\u96640.02%\u7684\u6570\u636e\u5373\u53ef\u6539\u53d8\u6700\u4f73\u6a21\u578b\u6392\u540d\u3002MT-Bench\u7684\u6392\u540d\u6bd4Chatbot Arena\u66f4\u7a33\u5065\uff0c\u4f46\u4f17\u5305\u4eba\u7c7b\u8bc4\u4f30\u548cLLM\u4f5c\u4e3a\u5224\u5b98\u7684\u7cfb\u7edf\u90fd\u5448\u73b0\u540c\u6837\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u6392\u540d\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u7a33\u5065\u6027\u95ee\u9898\uff0c\u9700\u8981\u66f4\u52a0\u7a33\u5065\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u6536\u96c6\u7b56\u7565\u6765\u63d0\u9ad8\u6392\u540d\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.11667", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11667", "abs": "https://arxiv.org/abs/2508.11667", "authors": ["Bryan E. Tuck", "Rakesh M. Verma"], "title": "Assessing Representation Stability for Transformer Models", "comment": "19 pages, 19 figures, 8 tables. Code available at\n  https://github.com/ReDASers/representation-stability", "summary": "Adversarial text attacks remain a persistent threat to transformer models,\nyet existing defenses are typically attack-specific or require costly model\nretraining. We introduce Representation Stability (RS), a model-agnostic\ndetection framework that identifies adversarial examples by measuring how\nembedding representations change when important words are masked. RS first\nranks words using importance heuristics, then measures embedding sensitivity to\nmasking top-k critical words, and processes the resulting patterns with a\nBiLSTM detector. Experiments show that adversarially perturbed words exhibit\ndisproportionately high masking sensitivity compared to naturally important\nwords. Across three datasets, three attack types, and two victim models, RS\nachieves over 88% detection accuracy and demonstrates competitive performance\ncompared to existing state-of-the-art methods, often at lower computational\ncost. Using Normalized Discounted Cumulative Gain (NDCG) to measure\nperturbation identification quality, we reveal that gradient-based ranking\noutperforms attention and random selection approaches, with identification\nquality correlating with detection performance for word-level attacks. RS also\ngeneralizes well to unseen datasets, attacks, and models without retraining,\nproviding a practical solution for adversarial text detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86Representation Stability (RS)\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cf\u63a9\u7801\u91cd\u8981\u8bcd\u6c47\u65f6\u5d4c\u5165\u8868\u793a\u7684\u53d8\u5316\u6765\u68c0\u6d4b\u5bf9\u6297\u6587\u672c\u653b\u51fb\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u653b\u51fb\u7c7b\u578b\u4e0a\u8fbe\u523088%\u4ee5\u4e0a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u6587\u672c\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u662f\u653b\u51fb\u7279\u5b9a\u7684\u6216\u9700\u8981\u6602\u8d35\u7684\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u68c0\u6d4b\u6846\u67b6\u6765\u5e94\u5bf9\u6301\u7eed\u5b58\u5728\u7684\u5bf9\u6297\u6587\u672c\u653b\u51fb\u5a01\u80c1\u3002", "method": "RS\u6846\u67b6\u9996\u5148\u4f7f\u7528\u91cd\u8981\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u5bf9\u8bcd\u6c47\u8fdb\u884c\u6392\u540d\uff0c\u7136\u540e\u6d4b\u91cf\u63a9\u7801\u524dk\u4e2a\u5173\u952e\u8bcd\u6c47\u65f6\u7684\u5d4c\u5165\u654f\u611f\u6027\uff0c\u6700\u540e\u4f7f\u7528BiLSTM\u68c0\u6d4b\u5668\u5904\u7406\u4ea7\u751f\u7684\u6a21\u5f0f\u3002\u4f7f\u7528NDCG\u8861\u91cf\u6270\u52a8\u8bc6\u522b\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u79cd\u653b\u51fb\u7c7b\u578b\u548c\u4e24\u4e2a\u53d7\u5bb3\u8005\u6a21\u578b\u4e0a\uff0cRS\u5b9e\u73b0\u4e86\u8d85\u8fc788%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\uff0c\u4e14\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u3001\u653b\u51fb\u548c\u6a21\u578b\u3002", "conclusion": "RS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5bf9\u6297\u6587\u672c\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u3001\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u68af\u5ea6\u57fa\u6392\u540d\u65b9\u6cd5\u5728\u6270\u52a8\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6ce8\u610f\u529b\u548c\u968f\u673a\u9009\u62e9\u65b9\u6cd5\u3002"}}
{"id": "2508.11663", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11663", "abs": "https://arxiv.org/abs/2508.11663", "authors": ["Guangli Li", "Canbiao Wu", "Zhen Liang"], "title": "Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation", "comment": null, "summary": "Affective computing is a rapidly developing interdisciplinary research\ndirection in the field of brain-computer interface. In recent years, the\nintroduction of deep learning technology has greatly promoted the development\nof the field of emotion recognition. However, due to physiological differences\nbetween subjects, as well as the variations in experimental environments and\nequipment, cross-corpus emotion recognition faces serious challenges,\nespecially for samples near the decision boundary. To solve the above problems,\nwe propose an optimization method based on domain adversarial transfer learning\nto fine-grained alignment of affective features, named Maximum classifier\ndiscrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a\ndual adversarial classifier (Ada classifier and RMS classifier), and apply a\nthree-stage adversarial training to maximize classification discrepancy and\nminimize feature distribution to align controversy samples near the decision\nboundary. In the process of domain adversarial training, the two classifiers\nalso maintain an adversarial relationship, ultimately enabling precise\ncross-corpus feature alignment. In addition, the introduction of pairwise\nlearning transforms the classification problem of samples into a similarity\nproblem between samples, alleviating the influence of label noise. We conducted\nsystematic experimental evaluation of the model using publicly available SEED,\nSEED-IV and SEED-V databases. The results show that the McdPL model is superior\nto other baseline models in the cross-corpus emotion recognition task, and the\naverage accuracy improvements of 4.76\\% and 3.97\\%, respectively. Our work\nprovides a promising solution for emotion recognition cross-corpus. The source\ncode is available at https://github.com/WuCB-BCI/Mcd_PL.", "AI": {"tldr": "\u63d0\u51faMcdPL\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5bf9\u6297\u5206\u7c7b\u5668\u548c\u4e09\u9636\u6bb5\u5bf9\u6297\u8bad\u7ec3\u5b9e\u73b0\u8de8\u8bed\u6599\u5e93\u60c5\u611f\u8bc6\u522b\uff0c\u5728SEED\u7cfb\u5217\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u89e3\u51b3\u8de8\u8bed\u6599\u5e93\u60c5\u611f\u8bc6\u522b\u4e2d\u7531\u4e8e\u88ab\u8bd5\u751f\u7406\u5dee\u5f02\u3001\u5b9e\u9a8c\u73af\u5883\u548c\u8bbe\u5907\u53d8\u5316\u5bfc\u81f4\u7684\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u6837\u672c\u5206\u7c7b\u56f0\u96be\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u57df\u5bf9\u6297\u8fc1\u79fb\u5b66\u4e60\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u53cc\u5bf9\u6297\u5206\u7c7b\u5668\uff08Ada\u548cRMS\u5206\u7c7b\u5668\uff09\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5bf9\u6297\u8bad\u7ec3\u6700\u5927\u5316\u5206\u7c7b\u5dee\u5f02\u5e76\u6700\u5c0f\u5316\u7279\u5f81\u5206\u5e03\u5dee\u5f02\uff0c\u540c\u65f6\u5f15\u5165\u6210\u5bf9\u5b66\u4e60\u5c06\u5206\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u6837\u672c\u76f8\u4f3c\u6027\u95ee\u9898", "result": "\u5728SEED\u3001SEED-IV\u548cSEED-V\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cMcdPL\u6a21\u578b\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u63d0\u53474.76%\u548c3.97%", "conclusion": "McdPL\u6846\u67b6\u4e3a\u8de8\u8bed\u6599\u5e93\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5bf9\u9f50\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u4e89\u8bae\u6837\u672c\u5e76\u51cf\u8f7b\u6807\u7b7e\u566a\u58f0\u5f71\u54cd"}}
{"id": "2508.12048", "categories": ["stat.ML", "cs.LG", "62K05"], "pdf": "https://arxiv.org/pdf/2508.12048", "abs": "https://arxiv.org/abs/2508.12048", "authors": ["Jing Wang", "HaiYing Wang", "Kun Chen"], "title": "Robust Data Fusion via Subsampling", "comment": null, "summary": "Data fusion and transfer learning are rapidly growing fields that enhance\nmodel performance for a target population by leveraging other related data\nsources or tasks. The challenges lie in the various potential heterogeneities\nbetween the target and external data, as well as various practical concerns\nthat prevent a na\\\"ive data integration. We consider a realistic scenario where\nthe target data is limited in size while the external data is large but\ncontaminated with outliers; such data contamination, along with other\ncomputational and operational constraints, necessitates proper selection or\nsubsampling of the external data for transfer learning. To our\nknowledge,transfer learning and subsampling under data contamination have not\nbeen thoroughly investigated. We address this gap by studying various transfer\nlearning methods with subsamples of the external data, accounting for outliers\ndeviating from the underlying true model due to arbitrary mean shifts. Two\nsubsampling strategies are investigated: one aimed at reducing biases and the\nother at minimizing variances. Approaches to combine these strategies are also\nintroduced to enhance the performance of the estimators. We provide\nnon-asymptotic error bounds for the transfer learning estimators, clarifying\nthe roles of sample sizes, signal strength, sampling rates, magnitude of\noutliers, and tail behaviors of model error distributions, among other factors.\nExtensive simulations show the superior performance of the proposed methods.\nAdditionally, we apply our methods to analyze the risk of hard landings in A380\nairplanes by utilizing data from other airplane types,demonstrating that robust\ntransfer learning can improve estimation efficiency for relatively rare\nairplane types with the help of data from other types of airplanes.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u5728\u5916\u90e8\u6570\u636e\u5b58\u5728\u5f02\u5e38\u503c\u6c61\u67d3\u60c5\u51b5\u4e0b\u7684\u8f6c\u79fb\u5b66\u4e60\u548c\u5b50\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u5dee\u6700\u5c0f\u5316\u548c\u65b9\u5dee\u6700\u5c0f\u5316\u7b56\u7565\u6765\u63d0\u9ad8\u4f30\u8ba1\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5728A380\u98de\u673a\u786c\u7740\u9646\u98ce\u9669\u5206\u6790\u4e2d\u5b9e\u9a8c\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u76ee\u6807\u6570\u636e\u6709\u9650\u800c\u5916\u90e8\u6570\u636e\u5b58\u5728\u5f02\u5e38\u503c\u6c61\u67d3\u7684\u5b9e\u9645\u95ee\u9898\uff0c\u586b\u8865\u8f6c\u79fb\u5b66\u4e60\u548c\u5b50\u91c7\u6837\u5728\u6570\u636e\u6c61\u67d3\u60c5\u51b5\u4e0b\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u5b50\u91c7\u6837\u7b56\u7565\uff1a\u504f\u5dee\u6700\u5c0f\u5316\u7b56\u7565\u548c\u65b9\u5dee\u6700\u5c0f\u5316\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u7ec4\u5408\u8fd9\u4e24\u79cd\u7b56\u7565\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u4f30\u8ba1\u5668\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u975e\u9f50\u6b21\u8bef\u5dee\u754c\u5206\u6790\u3002", "result": "\u5e7f\u6cdb\u7684\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u5e76\u5728A380\u98de\u673a\u786c\u7740\u9646\u98ce\u9669\u5206\u6790\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u8bc1\u660e\u7a33\u5065\u7684\u8f6c\u79fb\u5b66\u4e60\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u7a00\u6709\u98de\u673a\u7c7b\u578b\u7684\u4f30\u8ba1\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5904\u7406\u5916\u90e8\u6570\u636e\u5f02\u5e38\u503c\u6c61\u67d3\u7684\u8f6c\u79fb\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b50\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u7ec4\u5408\u504f\u5dee\u548c\u65b9\u5dee\u6700\u5c0f\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2508.11669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11669", "abs": "https://arxiv.org/abs/2508.11669", "authors": ["Wentao Li", "Yonghu He", "Kun Gao", "Qing Liu", "Yali Zheng"], "title": "Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset", "comment": null, "summary": "Noninvasive arterial blood pressure (ABP) monitoring is essential for patient\nmanagement in critical care and perioperative settings, providing continuous\nassessment of cardiovascular hemodynamics with minimal risks. Numerous deep\nlearning models have developed to reconstruct ABP waveform from noninvasively\nacquired physiological signals such as electrocardiogram and\nphotoplethysmogram. However, limited research has addressed the issue of model\nperformance and computational load for deployment on embedded systems. The\nstudy introduces a lightweight sInvResUNet, along with a collaborative learning\nscheme named KDCL_sInvResUNet. With only 0.89 million parameters and a\ncomputational load of 0.02 GFLOPS, real-time ABP estimation was successfully\nachieved on embedded devices with an inference time of just 8.49 milliseconds\nfor a 10-second output. We performed subject-independent validation in a\nlarge-scale and heterogeneous perioperative dataset containing 1,257,141 data\nsegments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and\n31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better\nperformance compared to large models, with a mean absolute error of 10.06 mmHg\nand mean Pearson correlation of 0.88 in tracking ABP changes. Despite these\npromising results, all deep learning models showed significant performance\nvariations across different demographic and cardiovascular conditions,\nhighlighting their limited ability to generalize across such a broad and\ndiverse population. This study lays a foundation work for real-time,\nunobtrusive ABP monitoring in real-world perioperative settings, providing\nbaseline for future advancements in this area.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684KDCL_sInvResUNet\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u540c\u5b66\u4e60\u65b9\u6848\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u5b9e\u65f6\u65e0\u521b\u8840\u538b\u76d1\u6d4b\uff0c\u8ba1\u7b97\u8d1f\u8f7d\u4ec50.02 GFLOPS\uff0c\u5728\u5927\u89c4\u6a21\u56f0\u672f\u671f\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u8bb8\u591a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u4ece\u975e\u4fb5\u5165\u6027\u751f\u7406\u4fe1\u53f7\u91cd\u5efa\u8840\u538b\u6ce2\u5f62\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6a21\u578b\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e0a\u90e8\u7f72\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u8d1f\u8f7d\u95ee\u9898\u5173\u6ce8\u4e0d\u591f\uff0c\u7279\u522b\u662f\u5728\u56f4\u624b\u672f\u671f\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u76d1\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7\u7684sInvResUNet\u6a21\u578b\uff0c\u7ed3\u5408\u534f\u540c\u5b66\u4e60\u65b9\u6848KDCL_sInvResUNet\u3002\u6a21\u578b\u4ec5\u53050.89\u4e07\u53c2\u6570\uff0c\u8ba1\u7b97\u8d1f\u8f7d0.02 GFLOPS\uff0c\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u63a8\u7406\u65f6\u95f48.49\u6beb\u79d2\uff08\u5bf910\u79d2\u8f93\u51fa\uff09\u3002\u5728\u5305\u542b2,154\u540d\u60a3\u8005\u30011,257,141\u4e2a\u6570\u636e\u6bb5\u7684\u5927\u89c4\u6a21\u56f0\u672f\u671f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e3b\u4f53\u72ec\u7acb\u9a8c\u8bc1\u3002", "result": "KDCL_sInvResUNet\u5728\u5e7f\u6cdb\u8840\u538b\u8303\u56f4\u5185\uff08\u7f29\u5f0f\u8840\u538b41-257 mmHg\uff0c\u8212\u5f20\u538b31-234 mmHg\uff09\u8fbe\u5230\u4e86\u7565\u4f18\u4e8e\u5927\u6a21\u578b\u7684\u6027\u80fd\uff1a\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee10.06 mmHg\uff0c\u5e73\u5747Pearson\u76f8\u5173\u7cfb\u65700.88\u3002\u7136\u800c\uff0c\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u90fd\u663e\u793a\u51fa\u5728\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u5fc3\u8840\u7ba1\u6761\u4ef6\u4e0b\u7684\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u5728\u771f\u5b9e\u56f4\u672f\u671f\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u65e0\u5e72\u6270\u7684\u8840\u538b\u76d1\u6d4b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002\u4f46\u6a21\u578b\u5728\u5e7f\u6cdb\u591a\u6837\u4eba\u7fa4\u4e2d\u7684\u666e\u904d\u6027\u4ecd\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2508.11664", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11664", "abs": "https://arxiv.org/abs/2508.11664", "authors": ["Zahra Mohammadi", "Parnian Fazel", "Siamak Mohammadi"], "title": "Energy-Efficient Real-Time 4-Stage Sleep Classification at 10-Second Resolution: A Comprehensive Study", "comment": null, "summary": "Sleep stage classification is crucial for diagnosing and managing disorders\nsuch as sleep apnea and insomnia. Conventional clinical methods like\npolysomnography are costly and impractical for long-term home use. We present\nan energy-efficient pipeline that detects four sleep stages (wake, REM, light,\nand deep) from a single-lead ECG. Two windowing strategies are introduced: (1)\na 5-minute window with 30-second steps for machine-learning models that use\nhandcrafted features, and (2) a 30-second window with 10-second steps for\ndeep-learning models, enabling near-real-time 10-second resolution. Lightweight\nnetworks such as MobileNet-v1 reach 92 percent accuracy and 91 percent F1-score\nbut still draw significant energy. We therefore design SleepLiteCNN, a custom\nmodel that achieves 89 percent accuracy and 89 percent F1-score while lowering\nenergy use to 5.48 microjoules per inference at 45 nm. Applying eight-bit\nquantization preserves accuracy and further reduces power, and FPGA deployment\nconfirms low resource usage. The proposed system offers a practical solution\nfor continuous, wearable ECG-based sleep monitoring.", "AI": {"tldr": "\u57fa\u4e8e\u5355\u5bfc\u8054\u7535\u5fc3\u56fe\u7684\u80fd\u6548\u7761\u7720\u5206\u671f\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b0\u9ad8\u51c6\u786e\u5ea6\u548c\u4f4e\u80fd\u8017\uff0c\u9002\u5408\u53ef\u7a7f\u6234\u7761\u7720\u76d1\u6d4b", "motivation": "\u4f20\u7edf\u591a\u5bfc\u8054\u7761\u7720\u76d1\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u5408\u957f\u671f\u5bb6\u5ead\u4f7f\u7528\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6548\u3001\u53ef\u7a7f\u6234\u7684\u5355\u5bfc\u8054\u7535\u5fc3\u56fe\u7761\u7720\u5206\u671f\u65b9\u6848", "method": "\u63d0\u51fa\u4e24\u79cd\u7a97\u53e3\u5207\u5206\u7b56\u7565\uff1a5\u5206\u949f\u7a97\u53e3\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c30\u79d2\u7a97\u53e3\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u81ea\u5b9a\u4e49\u6a21\u578bSleepLiteCNN\uff0c\u5e76\u5e94\u75288\u4f4d\u91cf\u5316\u6280\u672f", "result": "MobileNet-v1\u8fbe\u523092%\u51c6\u786e\u5ea6\u548c91% F1\u5206\u6570\uff0cSleepLiteCNN\u8fbe\u523089%\u51c6\u786e\u5ea6\u548c89% F1\u5206\u6570\uff0c\u6bcf\u6b21\u63a8\u7406\u80fd\u8017\u4ec5\u4e3a5.48\u5fae\u7126\u8033\uff0cFPGA\u90e8\u7f72\u8bc1\u660e\u4f4e\u8d44\u6e90\u5360\u7528", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u53ef\u7eed\u7eed\u3001\u53ef\u7a7f\u6234\u7684\u7535\u5fc3\u56fe\u57fa\u7761\u7720\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017"}}
{"id": "2508.12519", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.12519", "abs": "https://arxiv.org/abs/2508.12519", "authors": ["Khai Nguyen"], "title": "An Introduction to Sliced Optimal Transport", "comment": "227 pages", "summary": "Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal\ntransport (OT) that exploits the tractability of one-dimensional OT problems.\nBy combining tools from OT, integral geometry, and computational statistics,\nSOT enables fast and scalable computation of distances, barycenters, and\nkernels for probability measures, while retaining rich geometric structure.\nThis paper provides a comprehensive review of SOT, covering its mathematical\nfoundations, methodological advances, computational methods, and applications.\nWe discuss key concepts of OT and one-dimensional OT, the role of tools from\nintegral geometry such as Radon transform in projecting measures, and\nstatistical techniques for estimating sliced distances. The paper further\nexplores recent methodological advances, including non-linear projections,\nimproved Monte Carlo approximations, statistical estimation techniques for\none-dimensional optimal transport, weighted slicing techniques, and\ntransportation plan estimation methods. Variational problems, such as minimum\nsliced Wasserstein estimation, barycenters, gradient flows, kernel\nconstructions, and embeddings are examined alongside extensions to unbalanced,\npartial, multi-marginal, and Gromov-Wasserstein settings. Applications span\nmachine learning, statistics, computer graphics and computer visions,\nhighlighting SOT's versatility as a practical computational tool. This work\nwill be of interest to researchers and practitioners in machine learning, data\nsciences, and computational disciplines seeking efficient alternatives to\nclassical OT.", "AI": {"tldr": "\u5207\u7247\u6700\u4f18\u8fd0\u8f93\u662f\u4e00\u79cd\u901a\u8fc7\u6295\u5f71\u5230\u4e00\u7ef4\u7a7a\u95f4\u6765\u9ad8\u6548\u8ba1\u7b97\u6982\u7387\u6d4e\u5ea6\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6700\u4f18\u8fd0\u8f93\u3001\u79ef\u5206\u51e0\u4f55\u548c\u8ba1\u7b97\u7edf\u8ba1\u5b66\u7684\u5de5\u5177", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6700\u4f18\u8fd0\u8f93\u5728\u9ad8\u7ef4\u5ea6\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u65b9\u6cd5", "method": "\u5229\u7528Radon\u53d8\u6362\u5c06\u6982\u7387\u6d4e\u5ea6\u6295\u5f71\u5230\u4e00\u7ef4\u7a7a\u95f4\uff0c\u5728\u4e00\u7ef4\u4e0a\u89e3\u51b3\u6700\u4f18\u8fd0\u8f93\u95ee\u9898\uff0c\u5305\u62ec\u975e\u7ebf\u6027\u6295\u5f71\u3001\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1b\u8fd1\u4f3c\u3001\u7edf\u8ba1\u4f30\u8ba1\u6280\u672f\u7b49", "result": "\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u8ddd\u79bb\u8ba1\u7b97\u3001\u8d28\u5fc3\u8ba1\u7b97\u3001\u5185\u6838\u6784\u9020\u548c\u5d4c\u5165\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u4e0d\u5e73\u8861\u3001\u90e8\u5206\u3001\u591a\u8fb9\u9645\u548cGromov-Wasserstein\u573a\u666f", "conclusion": "\u5207\u7247\u6700\u4f18\u8fd0\u8f93\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u5de5\u5177\uff0c\u5728\u673a\u5668\u5b66\u4e60\u3001\u7edf\u8ba1\u5b66\u3001\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f"}}
{"id": "2508.11673", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11673", "abs": "https://arxiv.org/abs/2508.11673", "authors": ["Haojie Zhang", "Yixiong Liang", "Hulin Kuang", "Lihui Cen", "Zhe Qu", "Yigang Cen", "Min Zeng", "Shichao Kan"], "title": "Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning", "comment": "10 pages, 3 figures, submitted to ACM Multimedia 2025", "summary": "Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for\nhandling diverse tasks and modalities in the biomedical domain, as training\nseparate models for each modality or task significantly increases inference\ncosts. Existing incremental learning methods focus on task expansion within a\nsingle modality, whereas MBIIL seeks to train a unified model incrementally\nacross modalities. The MBIIL faces two challenges: I) How to preserve\npreviously learned knowledge during incremental updates? II) How to effectively\nleverage knowledge acquired from existing modalities to support new modalities?\nTo address these challenges, we propose MSLoRA-CR, a method that fine-tunes\nModality-Specific LoRA modules while incorporating Contrastive Regularization\nto enhance intra-modality knowledge sharing and promote inter-modality\nknowledge differentiation. Our approach builds upon a large vision-language\nmodel (LVLM), keeping the pretrained model frozen while incrementally adapting\nnew LoRA modules for each modality or task. Experiments on the incremental\nlearning of biomedical images demonstrate that MSLoRA-CR outperforms both the\nstate-of-the-art (SOTA) approach of training separate models for each modality\nand the general incremental learning method (incrementally fine-tuning LoRA).\nSpecifically, MSLoRA-CR achieves a 1.88% improvement in overall performance\ncompared to unconstrained incremental learning methods while maintaining\ncomputational efficiency. Our code is publicly available at\nhttps://github.com/VentusAislant/MSLoRA_CR.", "AI": {"tldr": "\u63d0\u51faMSLoRA-CR\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u56fe\u50cf\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9aLoRA\u6a21\u5757\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\u5b9e\u73b0\u77e5\u8bc6\u4fdd\u6301\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb", "motivation": "\u751f\u7269\u533b\u5b66\u9886\u57df\u9700\u8981\u5904\u7406\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\uff0c\u4e3a\u6bcf\u4e2a\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u4f1a\u663e\u8457\u589e\u52a0\u63a8\u7406\u6210\u672c\uff0c\u9700\u8981\u7edf\u4e00\u7684\u589e\u91cf\u5b66\u4e60\u6a21\u578b", "method": "\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u51bb\u7ed3\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u4e3a\u6bcf\u4e2a\u6a21\u6001\u589e\u91cf\u6dfb\u52a0\u7279\u5b9aLoRA\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u6b63\u5219\u5316\u4fc3\u8fdb\u6a21\u6001\u5185\u77e5\u8bc6\u5171\u4eab\u548c\u6a21\u6001\u95f4\u77e5\u8bc6\u533a\u5206", "result": "\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u589e\u91cf\u5b66\u4e60\u5b9e\u9a8c\u4e2d\uff0cMSLoRA-CR\u76f8\u6bd4\u4e3a\u6bcf\u4e2a\u6a21\u6001\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u548c\u901a\u7528\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u6574\u4f53\u6027\u80fd\u63d0\u53471.88%\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387", "conclusion": "MSLoRA-CR\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u56fe\u50cf\u589e\u91cf\u5b66\u4e60\u7684\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2508.11666", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11666", "abs": "https://arxiv.org/abs/2508.11666", "authors": ["Timothy Oladunni", "Ehimen Aneni"], "title": "Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate vs Late Fusion", "comment": null, "summary": "The limitations of unimodal deep learning models, particularly their tendency\nto overfit and limited generalizability, have renewed interest in multimodal\nfusion strategies. Multimodal deep neural networks (MDNN) have the capability\nof integrating diverse data domains and offer a promising solution for robust\nand accurate predictions. However, the optimal fusion strategy, intermediate\nfusion (feature-level) versus late fusion (decision-level) remains\ninsufficiently examined, especially in high-stakes clinical contexts such as\nECG-based cardiovascular disease (CVD) classification. This study investigates\nthe comparative effectiveness of intermediate and late fusion strategies using\nECG signals across three domains: time, frequency, and time-frequency. A series\nof experiments were conducted to identify the highest-performing fusion\narchitecture. Results demonstrate that intermediate fusion consistently\noutperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's\nd > 0.8 relative to standalone models and d = 0.40 compared to late fusion.\nInterpretability analyses using saliency maps reveal that both models align\nwith the discretized ECG signals. Statistical dependency between the\ndiscretized ECG signals and corresponding saliency maps for each class was\nconfirmed using Mutual Information (MI). The proposed ECG domain-based\nmultimodal model offers superior predictive capability and enhanced\nexplainability, crucial attributes in medical AI applications, surpassing\nstate-of-the-art models.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728ECG\u5fc3\u8840\u7ba1\u75be\u75c5\u5206\u7c7b\u4e2d\u7684\u878d\u5408\u7b56\u7565\uff0c\u53d1\u73b0\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u6bd4\u540e\u671f\u878d\u5408\u66f4\u6709\u6548\uff0c\u8fbe\u523097%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u5355\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u901a\u7528\u6027\u6709\u9650\uff0c\u800c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u80fd\u591f\u6574\u5408\u591a\u79cd\u6570\u636e\u9886\u57df\u4f46\u6700\u4f18\u878d\u5408\u65b9\u6cd5\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7814\u7a76\u4e0d\u5145\u5206\u3002", "method": "\u91c7\u7528ECG\u4fe1\u53f7\u7684\u65f6\u57df\u3001\u9891\u57df\u548c\u65f6\u9891\u57df\u4e09\u79cd\u57df\u6570\u636e\uff0c\u5bf9\u6bd4\u7814\u7a76\u4e2d\u95f4\u878d\u5408\uff08\u7279\u5f81\u5c42\u6b21\uff09\u548c\u540e\u671f\u878d\u5408\uff08\u51b3\u7b56\u5c42\u6b21\uff09\u4e24\u79cd\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u90fd\u4f18\u4e8e\u540e\u671f\u878d\u5408\uff0c\u8fbe\u523097%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0cCohen's d > 0.8\uff08\u76f8\u6bd4\u5355\u6a21\u6001\uff09\u548cd = 0.40\uff08\u76f8\u6bd4\u540e\u671f\u878d\u5408\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eECG\u57df\u7684\u591a\u6a21\u6001\u6a21\u578b\u5177\u6709\u66f4\u4f18\u7684\u9884\u6d4b\u80fd\u529b\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728\u533b\u7597AI\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.12627", "categories": ["stat.ML", "cs.DS", "cs.NA", "math.NA", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.12627", "abs": "https://arxiv.org/abs/2508.12627", "authors": ["Xingyu Chen", "Ruiqi Zhang", "Lin Liu"], "title": "On computing and the complexity of computing higher-order $U$-statistics, exactly", "comment": "Comments are welcome! 49 pages, 8 tables, 4 figures. An accompanying\n  Python package is available at: https://libraries.io/pypi/u-stats or\n  https://github.com/Amedar-Asterisk/U-Statistics-Python", "summary": "Higher-order $U$-statistics abound in fields such as statistics, machine\nlearning, and computer science, but are known to be highly time-consuming to\ncompute in practice. Despite their widespread appearance, a comprehensive study\nof their computational complexity is surprisingly lacking. This paper aims to\nfill that gap by presenting several results related to the computational aspect\nof $U$-statistics. First, we derive a useful decomposition from an $m$-th order\n$U$-statistic to a linear combination of $V$-statistics with orders not\nexceeding $m$, which are generally more feasible to compute. Second, we explore\nthe connection between exactly computing $V$-statistics and Einstein summation,\na tool often used in computational mathematics, quantum computing, and quantum\ninformation sciences for accelerating tensor computations. Third, we provide an\noptimistic estimate of the time complexity for exactly computing\n$U$-statistics, based on the treewidth of a particular graph associated with\nthe $U$-statistic kernel. The above ingredients lead to a new, much more\nruntime-efficient algorithm of exactly computing general higher-order\n$U$-statistics. We also wrap our new algorithm into an open-source Python\npackage called $\\texttt{u-stats}$. We demonstrate via three statistical\napplications that $\\texttt{u-stats}$ achieves impressive runtime performance\ncompared to existing benchmarks. This paper aspires to achieve two goals: (1)\nto capture the interest of researchers in both statistics and other related\nareas further to advance the algorithmic development of $U$-statistics, and (2)\nto offer the package $\\texttt{u-stats}$ as a valuable tool for practitioners,\nmaking the implementation of methods based on higher-order $U$-statistics a\nmore delightful experience.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8ba1\u7b97\u9ad8\u9636U\u7edf\u8ba1\u91cf\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4e3aV\u7edf\u8ba1\u91cf\u3001\u5229\u7528\u7231\u56e0\u65af\u5766\u6c42\u548c\u4f18\u5316\u8ba1\u7b97\uff0c\u5e76\u5f00\u53d1\u4e86Python\u5305u-stats\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u9ad8\u9636U\u7edf\u8ba1\u91cf\u5728\u7edf\u8ba1\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8ba1\u7b97\u8017\u65f6\u4e25\u91cd\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7684\u8ba1\u7b97\u590d\u6742\u6027\u7814\u7a76\u3002", "method": "1) \u5c06m\u9636U\u7edf\u8ba1\u91cf\u5206\u89e3\u4e3a\u4e0d\u8d85\u8fc7m\u9636\u7684V\u7edf\u8ba1\u91cf\u7684\u7ebf\u6027\u7ec4\u5408\uff1b2) \u63a2\u7d22V\u7edf\u8ba1\u91cf\u4e0e\u7231\u56e0\u65af\u5766\u6c42\u548c\u7684\u5173\u7cfb\uff1b3) \u57fa\u4e8e\u6838\u51fd\u6570\u5173\u8054\u56fe\u7684\u6811\u5bbd\u7ed9\u51fa\u65f6\u95f4\u590d\u6742\u5ea6\u4f30\u8ba1", "result": "\u5f00\u53d1\u4e86\u65b0\u7684\u9ad8\u6548\u7b97\u6cd5\u548cPython\u5305u-stats\uff0c\u5728\u4e09\u4e2a\u7edf\u8ba1\u5e94\u7528\u4e2d\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86U\u7edf\u8ba1\u91cf\u7b97\u6cd5\u53d1\u5c55\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u4f7f\u57fa\u4e8e\u9ad8\u9636U\u7edf\u8ba1\u91cf\u7684\u65b9\u6cd5\u5b9e\u73b0\u66f4\u52a0\u9ad8\u6548\u4fbf\u6377"}}
{"id": "2508.11679", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11679", "abs": "https://arxiv.org/abs/2508.11679", "authors": ["Shaodi Feng", "Zhuoyi Lin", "Jianan Zhou", "Cong Zhang", "Jingwen Li", "Kuan-Wen Chen", "Senthilnath Jayavelu", "Yew-Soon Ong"], "title": "Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems", "comment": null, "summary": "Deep learning has been extensively explored to solve vehicle routing problems\n(VRPs), which yields a range of data-driven neural solvers with promising\noutcomes. However, most neural solvers are trained to tackle VRP instances in a\nrelatively monotonous context, e.g., simplifying VRPs by using Euclidean\ndistance between nodes and adhering to a single problem size, which harms their\noff-the-shelf application in different scenarios. To enhance their versatility,\nthis paper presents a novel lifelong learning framework that incrementally\ntrains a neural solver to manage VRPs in distinct contexts. Specifically, we\npropose a lifelong learner (LL), exploiting a Transformer network as the\nbackbone, to solve a series of VRPs. The inter-context self-attention mechanism\nis proposed within LL to transfer the knowledge obtained from solving preceding\nVRPs into the succeeding ones. On top of that, we develop a dynamic context\nscheduler (DCS), employing the cross-context experience replay to further\nfacilitate LL looking back on the attained policies of solving preceding VRPs.\nExtensive results on synthetic and benchmark instances (problem sizes up to\n18k) show that our LL is capable of discovering effective policies for tackling\ngeneric VRPs in varying contexts, which outperforms other neural solvers and\nachieves the best performance for most VRPs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7Transformer\u7f51\u7edc\u548c\u4e0a\u4e0b\u6587\u8c03\u5ea6\u5668\uff0c\u589e\u91cf\u5f0f\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6c42\u89e3\u5668\u6765\u5904\u7406\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u8f66\u8f86\u8def\u7531\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9002\u7528\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u8def\u7531\u6c42\u89e3\u5668\u901a\u5e38\u5728\u5355\u4e00\u4e0a\u4e0b\u6587\u4e2d\u8bad\u7ec3\uff0c\u5982\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u56fa\u5b9a\u95ee\u9898\u89c4\u6a21\uff0c\u5bfc\u81f4\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5e94\u7528\u6548\u679c\u4e0d\u4f73\u3002\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u7684\u591a\u6837\u6027\u548c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ec8\u8eab\u5b66\u4e60\u8005(LL)\uff0c\u4f7f\u7528Transformer\u4f5c\u4e3a\u6838\u5fc3\u7f51\u7edc\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u95f4\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u52a8\u6001\u4e0a\u4e0b\u6587\u8c03\u5ea6\u5668(DCS)\uff0c\u5229\u7528\u8de8\u4e0a\u4e0b\u6587\u7ecf\u9a8c\u91cd\u73b0\u6280\u672f\u6765\u56de\u987e\u4e4b\u524d\u7684\u89e3\u51b3\u7b56\u7565\u3002", "result": "\u5728\u5408\u6210\u548c\u6807\u51c6\u6570\u636e\u96c6(\u95ee\u9898\u89c4\u6a21\u8fbe18k)\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u53d1\u73b0\u6709\u6548\u7684\u7b56\u7565\u6765\u5904\u7406\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u8f66\u8f86\u8def\u7531\u95ee\u9898\uff0c\u6027\u80fd\u8d85\u8fc7\u5176\u4ed6\u795e\u7ecf\u6c42\u89e3\u5668\uff0c\u5728\u5927\u90e8\u5206VRP\u95ee\u9898\u4e0a\u8fbe\u5230\u6700\u4f73\u8868\u73b0\u3002", "conclusion": "\u8be5\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u589e\u91cf\u5f0f\u8bad\u7ec3\u548c\u77e5\u8bc6\u8f6c\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u8def\u7531\u6c42\u89e3\u5668\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u5904\u7406\u591a\u6837\u5316VRP\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11668", "categories": ["eess.SP", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11668", "abs": "https://arxiv.org/abs/2508.11668", "authors": ["Muhammad Umer", "Muhammad Ahmed Mohsin", "Ahsan Bilal", "John M. Cioffi"], "title": "Neural Gaussian Radio Fields for Channel Estimation", "comment": "This paper has been submitted to NeurIPS 2025", "summary": "Accurate channel state information (CSI) remains the most critical bottleneck\nin modern wireless networks, with pilot overhead consuming up to 11-21% of\ntransmission bandwidth, increasing latency by 20-40% in massive MIMO systems,\nand reducing potential spectral efficiency by over 53%. Traditional estimation\ntechniques fundamentally fail under mobility, with feedback delays as small as\n4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We\npresent neural Gaussian radio fields (nGRF), a novel framework that leverages\nexplicit 3D Gaussian primitives to synthesize complex channel matrices\naccurately and efficiently. Unlike NeRF-based approaches that rely on slow\nimplicit representations or existing Gaussian splatting methods that use\nnon-physical 2D projections, nGRF performs direct 3D electromagnetic field\naggregation, with each Gaussian acting as a localized radio modulator. nGRF\ndemonstrates superior performance across diverse environments: in indoor\nscenarios, it achieves a 10.9$\\times$ higher prediction SNR than state of the\nart methods while reducing inference latency from 242 ms to just 1.1 ms (a\n220$\\times$ speedup). For large-scale outdoor environments, where existing\napproaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF\nrequires only 0.011 measurements per cubic foot compared to 0.2-178.1 for\nexisting methods, thereby reducing data collection burden by 18$\\times$.\nTraining time is similarly reduced from hours to minutes (a 180$\\times$\nreduction), enabling rapid adaptation to dynamic environments. The code and\ndatasets are available at: https://github.com/anonym-auth/n-grf", "AI": {"tldr": "nGRF\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u539f\u8bed\u7684\u65e0\u7ebf\u4fe1\u9053\u4f30\u8ba1\u65b0\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u7279\u522b\u9002\u7528\u4e8e\u79fb\u52a8\u73af\u5883\u548c\u5927\u89c4\u6a21\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4f30\u8ba1\u65b9\u6cd5\u5728\u79fb\u52a8\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u9891\u5f00\u9500\u5927\u3001\u5ef6\u8fdf\u9ad8\u3001\u9891\u8c31\u6548\u7387\u4f4e\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e94\u5bf9\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u663e\u5f0f3D\u9ad8\u65af\u539f\u8bed\u8fdb\u884c\u76f4\u63a5\u7535\u78c1\u573a\u805a\u5408\uff0c\u6bcf\u4e2a\u9ad8\u65af\u4f5c\u4e3a\u5c40\u90e8\u65e0\u7ebf\u7535\u8c03\u5236\u5668\uff0c\u907f\u514d\u4e86\u57fa\u4e8eNeRF\u7684\u6162\u901f\u9690\u5f0f\u8868\u793a\u6216\u975e\u7269\u74062D\u6295\u5f71\u65b9\u6cd5\u3002", "result": "\u5ba4\u5185\u573a\u666f\u9884\u6d4bSNR\u63d0\u9ad810.9\u500d\uff0c\u63a8\u7406\u5ef6\u8fdf\u4ece242ms\u964d\u81f31.1ms\uff1b\u5ba4\u5916\u573a\u666f\u8fbe\u523026.2dB SNR\uff1b\u6570\u636e\u6536\u96c6\u8d1f\u62c5\u51cf\u5c1118\u500d\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ece\u5c0f\u65f6\u7ea7\u964d\u81f3\u5206\u949f\u7ea7\u3002", "conclusion": "nGRF\u6846\u67b6\u5728\u4fe1\u9053\u4f30\u8ba1\u65b9\u9762\u5b9e\u73b0\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3a\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.12674", "categories": ["stat.ML", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.12674", "abs": "https://arxiv.org/abs/2508.12674", "authors": ["Haruka Ezoe", "Hiroki Matsumoto", "Ryohei Hisano"], "title": "Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation", "comment": null, "summary": "Dynamic relational structures play a central role in many AI tasks, but their\nevolving nature presents challenges for consistent and interpretable\nrepresentation. A common approach is to learn time-varying node embeddings,\nwhose effectiveness depends on satisfying key stability properties. In this\npaper, we propose Unfolded Laplacian Spectral Embedding, a new method that\nextends the Unfolded Adjacency Spectral Embedding framework to normalized\nLaplacians while preserving both cross-sectional and longitudinal stability. We\nprovide formal proof that our method satisfies these stability conditions. In\naddition, as a bonus of using the Laplacian matrix, we establish a new\nCheeger-style inequality that connects the embeddings to the conductance of the\nunderlying dynamic graphs. Empirical evaluations on synthetic and real-world\ndatasets support our theoretical findings and demonstrate the strong\nperformance of our method. These results establish a principled and stable\nframework for dynamic network representation grounded in spectral graph theory.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86Unfolded Laplacian Spectral Embedding\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u62c9\u666e\u62c9\u65af\u8c31\u5d4c\u5165\u6269\u5c55\u5230\u52a8\u6001\u5173\u7cfb\u7ed3\u6784\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u8de8\u622a\u9762\u548c\u7eb5\u5411\u7a33\u5b9a\u6027\uff0c\u5e76\u8fde\u63a5\u5d4c\u5165\u4e0e\u56fe\u5bfc\u7535\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u52a8\u6001\u5173\u7cfb\u7ed3\u6784\u5728AI\u4efb\u52a1\u4e2d\u91cd\u8981\u4f46\u6f14\u5316\u7279\u6027\u5e26\u6765\u8861\u91cf\u8868\u793a\u7684\u6311\u6218\uff0c\u9700\u8981\u65b9\u6cd5\u80fd\u591f\u7ed9\u51fa\u4e00\u81f4\u53ef\u89e3\u91ca\u7684\u8868\u793a\u3002", "method": "\u63d0\u51faUnfolded Laplacian Spectral Embedding\u65b9\u6cd5\uff0c\u5c06Unfolded Adjacency Spectral Embedding\u6846\u67b6\u6269\u5c55\u5230\u6807\u51c6\u5316\u62c9\u666e\u62c9\u65af\u77e9\u9635\uff0c\u4fdd\u6301\u8de8\u622a\u9762\u548c\u7eb5\u5411\u7a33\u5b9a\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u65b9\u6cd5\u6ee1\u8db3\u7a33\u5b9a\u6027\u6761\u4ef6\u7684\u6b63\u5f0f\u8bc1\u660e\uff0c\u5efa\u7acb\u4e86\u5d4c\u5165\u4e0e\u52a8\u6001\u56fe\u5bfc\u7535\u6027\u4e4b\u95f4\u7684Cheeger\u98ce\u683c\u4e0d\u7b49\u5f0f\uff0c\u5e76\u901a\u8fc7\u7efc\u5408\u548c\u5b9e\u9645\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u548c\u65b9\u6cd5\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u7f51\u7edc\u8868\u793a\u5efa\u7acb\u4e86\u57fa\u4e8e\u8c31\u56fe\u8bba\u7684\u539f\u5219\u6027\u7a33\u5b9a\u6846\u67b6\u3002"}}
{"id": "2508.11680", "categories": ["cs.LG", "cs.AI", "es: 62M10 (primary), 62P20, 68T05, 91B72 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.11680", "abs": "https://arxiv.org/abs/2508.11680", "authors": ["Aditya Akella", "Jonathan Farah"], "title": "Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics", "comment": "6 pages, 4 figures, 3 tables", "summary": "Demographic shifts, influenced by globalization, economic conditions,\ngeopolitical events, and environmental factors, pose significant challenges for\npolicymakers and researchers. Accurate demographic forecasting is essential for\ninformed decision-making in areas such as urban planning, healthcare, and\neconomic policy. This study explores the application of time series foundation\nmodels to predict demographic changes in the United States using datasets from\nthe U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate\nthe performance of the Time Series Foundation Model (TimesFM) against\ntraditional baselines including Long Short-Term Memory (LSTM) networks,\nAutoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our\nexperiments across six demographically diverse states demonstrate that TimesFM\nachieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with\nparticularly strong performance on minority populations with sparse historical\ndata. These findings highlight the potential of pre-trained foundation models\nto enhance demographic analysis and inform proactive policy interventions\nwithout requiring extensive task-specific fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TimesFM)\u5728\u7f8e\u56fd\u4eba\u53e3\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u572886.67%\u7684\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f4e\u7684MSE\uff0c\u7279\u522b\u662f\u5728\u5386\u53f2\u6570\u636e\u7a00\u758f\u7684\u5c11\u6570\u65cf\u88d4\u7fa4\u4f53\u9884\u6d4b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4eba\u53e3\u7ed3\u6784\u53d8\u5316\u53d7\u5168\u7403\u5316\u3001\u7ecf\u6d4e\u72b6\u51b5\u3001\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u548c\u73af\u5883\u56e0\u7d20\u5f71\u54cd\uff0c\u7ed9\u653f\u7b56\u5236\u5b9a\u8005\u5e26\u6765\u91cd\u5927\u6311\u6218\u3002\u51c6\u786e\u7684\u4eba\u53e3\u9884\u6d4b\u5bf9\u4e8e\u57ce\u5e02\u89c4\u5212\u3001\u533b\u7597\u4fdd\u5065\u548c\u7ecf\u6d4e\u653f\u7b56\u7b49\u9886\u57df\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u7f8e\u56fd\u4eba\u53e3\u666e\u67e5\u5c40\u548cFRED\u7684\u6570\u636e\u96c6\uff0c\u5c06TimesFM\u4e0eLSTM\u3001ARIMA\u548c\u7ebf\u6027\u56de\u5f52\u7b49\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u516d\u4e2a\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u7684\u5dde\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "TimesFM\u572886.67%\u7684\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u5728\u5386\u53f2\u6570\u636e\u7a00\u758f\u7684\u5c11\u6570\u65cf\u88d4\u7fa4\u4f53\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u6709\u6f5c\u529b\u589e\u5f3a\u4eba\u53e3\u5206\u6790\u80fd\u529b\uff0c\u65e0\u9700\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u4e3a\u4e3b\u52a8\u653f\u7b56\u5e72\u9884\u63d0\u4f9b\u4fe1\u606f\u652f\u6301\u3002"}}
{"id": "2508.11675", "categories": ["eess.SP", "53A45", "G.1"], "pdf": "https://arxiv.org/pdf/2508.11675", "abs": "https://arxiv.org/abs/2508.11675", "authors": ["Amgad A. Salama"], "title": "Direction of Arrival Estimation: A Tutorial Survey of Classical and Modern Methods", "comment": "DOA Survey, 44 pages, Not published yet", "summary": "Direction of arrival (DOA) estimation is a fundamental problem in array\nsignal processing with applications spanning radar, sonar, wireless\ncommunications, and acoustic signal processing. This tutorial survey provides a\ncomprehensive introduction to classical and modern DOA estimation methods,\nspecifically designed for students and researchers new to the field. We focus\non narrowband signal processing using uniform linear arrays, presenting\nstep-by-step mathematical derivations with geometric intuition. The survey\ncovers classical beamforming methods, subspace-based techniques (MUSIC,\nESPRIT), maximum likelihood approaches, and sparse signal processing methods.\nEach method is accompanied by Python implementations available in an\nopen-source repository, enabling reproducible research and hands-on learning.\nThrough systematic performance comparisons across various scenarios, we provide\npractical guidelines for method selection and parameter tuning. This work aims\nto bridge the gap between theoretical foundations and practical implementation,\nmaking DOA estimation accessible to beginners while serving as a comprehensive\nreference for the field. See https://github.com/AmgadSalama/DOA for detail\nimplementation of the methods.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4efd\u5173\u4e8e\u5230\u8fbe\u89d2\u5ea6\u4f30\u8ba1\u7684\u7efc\u8ff0\u6027\u6559\u7a0b\uff0c\u6db5\u76d6\u4e86\u4ece\u7ecf\u5178\u5230\u73b0\u4ee3\u7684\u5404\u79cd\u65b9\u6cd5\uff0c\u5305\u62ec\u6570\u5b66\u63a8\u5bfc\u3001Python\u5b9e\u73b0\u548c\u5b9e\u8df5\u6307\u5357\u3002", "motivation": "\u5230\u8fbe\u89d2\u5ea6\u4f30\u8ba1\u662f\u6570\u7ec4\u4fe1\u53f7\u5904\u7406\u9886\u57df\u7684\u57fa\u7840\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u96f7\u8fbe\u3001\u58f0\u7eb3\u3001\u65e0\u7ebf\u901a\u4fe1\u7b49\u591a\u4e2a\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u4e3a\u5165\u95e8\u8005\u63d0\u4f9b\u5168\u9762\u7684\u5f15\u5bfc\uff0c\u7f29\u5c0f\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7ebf\u6027\u6570\u7ec4\u8fdb\u884c\u7a84\u5e26\u4fe1\u53f7\u5904\u7406\uff0c\u6db5\u76d6\u4e86\u7ecf\u5178\u6ce2\u675f\u5f62\u6210\u65b9\u6cd5\u3001\u5b50\u7a7a\u95f4\u6280\u672f\uff08MUSIC\u3001ESPRIT\uff09\u3001\u6700\u5927\u4f3c\u7136\u65b9\u6cd5\u548c\u7a00\u758f\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u3002\u6bcf\u79cd\u65b9\u6cd5\u90fd\u6709\u8be6\u7ec6\u7684\u6570\u5b66\u63a8\u5bfc\u548cPython\u5b9e\u73b0\u3002", "result": "\u63d0\u4f9b\u4e86\u5f00\u6e90\u7684Python\u5b9e\u73b0\u5e93\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7814\u7a76\u548c\u5b9e\u8df5\u5b66\u4e60\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u4e3a\u65b9\u6cd5\u9009\u62e9\u548c\u53c2\u6570\u8c03\u6574\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5357\u3002", "conclusion": "\u8be5\u6559\u7a0b\u6210\u529f\u5730\u4e3a\u5165\u95e8\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5230\u8fbe\u89d2\u5ea6\u4f30\u8ba1\u5f15\u5bfc\uff0c\u540c\u65f6\u4e5f\u662f\u9886\u57df\u4e13\u5bb6\u7684\u5b8c\u6574\u53c2\u8003\u8d44\u6599\uff0c\u6709\u529b\u5730\u63a8\u52a8\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u7ed3\u5408\u3002"}}
{"id": "2508.12834", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12834", "abs": "https://arxiv.org/abs/2508.12834", "authors": ["Hiroshi Horii", "Sothea Has"], "title": "Optimal Condition for Initialization Variance in Deep Neural Networks: An SGD Dynamics Perspective", "comment": null, "summary": "Stochastic gradient descent (SGD), one of the most fundamental optimization\nalgorithms in machine learning (ML), can be recast through a continuous-time\napproximation as a Fokker-Planck equation for Langevin dynamics, a viewpoint\nthat has motivated many theoretical studies. Within this framework, we study\nthe relationship between the quasi-stationary distribution derived from this\nequation and the initial distribution through the Kullback-Leibler (KL)\ndivergence. As the quasi-steady-state distribution depends on the expected cost\nfunction, the KL divergence eventually reveals the connection between the\nexpected cost function and the initialization distribution. By applying this to\ndeep neural network models (DNNs), we can express the bounds of the expected\nloss function explicitly in terms of the initialization parameters. Then, by\nminimizing this bound, we obtain an optimal condition of the initialization\nvariance in the Gaussian case. This result provides a concrete mathematical\ncriterion, rather than a heuristic approach, to select the scale of weight\ninitialization in DNNs. In addition, we experimentally confirm our theoretical\nresults by using the classical SGD to train fully connected neural networks on\nthe MNIST and Fashion-MNIST datasets. The result shows that if the variance of\nthe initialization distribution satisfies our theoretical optimal condition,\nthen the corresponding DNN model always achieves lower final training loss and\nhigher test accuracy than the conventional He-normal initialization. Our work\nthus supplies a mathematically grounded indicator that guides the choice of\ninitialization variance and clarifies its physical meaning of the dynamics of\nparameters in DNNs.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790SGD\u7684\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u521d\u59cb\u5316\u65b9\u5dee\u7684\u6570\u5b66\u6700\u4f18\u6761\u4ef6\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4ee5\u5f80\u7ecf\u9a8c\u6cd5\u66f4\u7406\u8bba\u5316\u7684\u521d\u59cb\u5316\u6307\u5357", "motivation": "\u5c06SGD\u4f18\u5316\u8fc7\u7a0b\u63a8\u5e7f\u5230\u8fde\u7eed\u65f6\u95f4\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a2\u7d22\u521d\u59cb\u5316\u5206\u5e03\u4e0e\u9884\u671f\u635f\u5931\u51fd\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u83b7\u5f97\u6570\u5b66\u4e0a\u4e25\u683c\u7684\u521d\u59cb\u5316\u53c2\u6570\u9009\u62e9\u51c6\u5219", "method": "\u4f7f\u7528Fokker-Planck\u65b9\u7a0b\u63cf\u8ff0SGD\u7684Langevin\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u6c42\u89e3\u51e0\u4f55\u7a33\u6001\u5206\u5e03\u4e0e\u521d\u59cb\u5206\u5e03\u7684KL\u6563\u5ea6\uff0c\u5f97\u5230\u9884\u671f\u635f\u5931\u51fd\u6570\u7684\u754c\u9650\u8868\u8fbe\u5f0f\uff0c\u5e76\u5728\u9ad8\u65af\u5206\u5e03\u5047\u8bbe\u4e0b\u6c42\u89e3\u6700\u4f18\u521d\u59cb\u65b9\u5dee", "result": "\u63a8\u5bfc\u51fa\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u521d\u59cb\u5316\u65b9\u5dee\u7684\u6570\u5b66\u6700\u4f18\u6761\u4ef6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u5728MNIST\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u66f4\u4f4e\u7684\u8bad\u7ec3\u635f\u5931\u548c\u66f4\u9ad8\u7684\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u8d85\u8fc7\u4f20\u7edf\u7684He-normal\u521d\u59cb\u5316\u65b9\u6cd5", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u521d\u59cb\u5316\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u66ff\u4ee3\u4e86\u7ecf\u9a8c\u6027\u7684\u9a71\u52a8\u65b9\u6cd5\uff0c\u540c\u65f6\u6e05\u695a\u5730\u89e3\u91ca\u4e86\u53c2\u6570\u52a8\u529b\u5b66\u8fc7\u7a0b\u7684\u7269\u7406\u542b\u4e49"}}
{"id": "2508.11723", "categories": ["cs.LG", "68T07, 91D10", "I.2.10; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11723", "abs": "https://arxiv.org/abs/2508.11723", "authors": ["Qian Cao", "Jielin Chen", "Junchao Zhao", "Rudi Stouffs"], "title": "From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data", "comment": "42 pages, 32 figures, submitted to Environment and Planning B: Urban\n  Analytics and City Science", "summary": "The spatial layout of urban sites shapes land-use efficiency and spatial\norganization. Traditional site planning often relies on experiential judgment\nand single-source data, limiting systematic quantification of multifunctional\nlayouts. We propose a Site Planning Layout Indicator (SPLI) system, a\ndata-driven framework integrating empirical knowledge with heterogeneous\nmulti-source data to produce structured urban spatial information. The SPLI\nsupports multimodal spatial data systems for analytics, inference, and\nretrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building\nmorphology, land use, and satellite imagery. It extends conventional metrics\nthrough five dimensions: (1) Hierarchical Building Function Classification,\nrefining empirical systems into clear hierarchies; (2) Spatial Organization,\nquantifying seven layout patterns (e.g., symmetrical, concentric,\naxial-oriented); (3) Functional Diversity, transforming qualitative assessments\ninto measurable indicators using Functional Ratio (FR) and Simpson Index (SI);\n(4) Accessibility to Essential Services, integrating facility distribution and\ntransport networks for comprehensive accessibility metrics; and (5) Land Use\nIntensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to\nassess utilization efficiency. Data gaps are addressed through deep learning,\nincluding Relational Graph Neural Networks (RGNN) and Graph Neural Networks\n(GNN). Experiments show the SPLI improves functional classification accuracy\nand provides a standardized basis for automated, data-driven urban spatial\nanalytics.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u6e90\u6570\u636e\u7684\u57fa\u5730\u89c4\u5212\u5e03\u5c40\u6307\u6807\uff08SPLI\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e94\u5927\u7ef4\u5ea6\u7cfb\u7edf\u5316\u91cf\u5316\u57ce\u5e02\u7a7a\u95f4\u5e03\u5c40\uff0c\u63d0\u9ad8\u529f\u80fd\u5206\u7c7b\u51c6\u786e\u6027\u548c\u81ea\u52a8\u5316\u5206\u6790\u80fd\u529b", "motivation": "\u4f20\u7edf\u57fa\u5730\u89c4\u5212\u4f9d\u8d56\u7ecf\u9a8c\u5224\u65ad\u548c\u5355\u4e00\u6570\u636e\u6e90\uff0c\u5bfc\u81f4\u591a\u529f\u80fd\u5e03\u5c40\u7cfb\u7edf\u5316\u91cf\u5316\u4e0d\u8db3\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u5206\u6790\u6846\u67b6", "method": "\u6574\u5408OSM\u3001POI\u3001\u5efa\u7b51\u5f62\u6001\u3001\u571f\u5730\u5229\u7528\u548c\u536b\u661f\u5f71\u50cf\u7b49\u591a\u6e90\u6570\u636e\uff0c\u6784\u5efa\u4e94\u5927\u7ef4\u5ea6\u6307\u6807\u7cfb\u7edf\uff1a\u5c42\u7ea7\u529f\u80fd\u5206\u7c7b\u3001\u7a7a\u95f4\u7ec4\u7ec7\u3001\u529f\u80fd\u591a\u6837\u6027\u3001\u57fa\u7840\u670d\u52a1\u53ef\u8fbe\u6027\u3001\u571f\u5730\u5229\u7528\u5f3a\u5ea6\uff0c\u4f7f\u7528RGNN\u548cGNN\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u6570\u636e\u7f3a\u5931", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSPLI\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u529f\u80fd\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u7684\u6570\u636e\u9a71\u52a8\u57ce\u5e02\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u7840", "conclusion": "SPLI\u7cfb\u7edf\u6210\u529f\u5c06\u7ecf\u9a8c\u6027\u77e5\u8bc6\u4e0e\u591a\u6e90\u5f02\u6784\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u7ed3\u6784\u5316\u7684\u57ce\u5e02\u7a7a\u95f4\u4fe1\u606f\u751f\u6210\u6846\u67b6\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u7cfb\u7edf\u3001\u91cf\u5316\u7684\u5206\u6790\u5de5\u5177"}}
{"id": "2508.11682", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11682", "abs": "https://arxiv.org/abs/2508.11682", "authors": ["Md Basit Azam", "Sarangthem Ibotombi Singh"], "title": "Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study", "comment": null, "summary": "Non-invasive glucose monitoring remains a critical challenge in the\nmanagement of diabetes. HRV during sleep shows promise for glucose prediction\nhowever, age-related autonomic changes significantly confound traditional HRV\nanalyses. We analyzed 43 subjects with multi-modal data including sleep-stage\nspecific ECG, HRV features, and clinical measurements. A novel\nage-normalization technique was applied to the HRV features by, dividing the\nraw values by age-scaled factors. BayesianRidge regression with 5-fold\ncross-validation was employed for log-glucose prediction. Age-normalized HRV\nfeatures achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction,\nrepresenting a 25.6% improvement over non-normalized features (R2 = 0.132). The\ntop predictive features were hrv rem mean rr age normalized (r = 0.443, p =\n0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic\nblood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed\nage-normalization as the critical component, with sleep-stage specific features\nproviding additional predictive value. Age-normalized HRV features\nsignificantly enhance glucose prediction accuracy compared with traditional\napproaches. This sleep-aware methodology addresses fundamental limitations in\nautonomic function assessment and suggests a preliminary feasibility for\nnon-invasive glucose monitoring applications. However, these results require\nvalidation in larger cohorts before clinical consideration.", "AI": {"tldr": "\u901a\u8fc7\u5e74\u9f84\u6b63\u5219\u5316HRV\u7279\u5f81\u63d0\u9ad8\u7761\u7720\u671f\u95f4\u8840\u7cd6\u9884\u6d4b\u7cbe\u5ea6\uff0c\u572843\u540d\u53d7\u8bd5\u8005\u4e2d\u5b9e\u73b0R2\u63d0\u534725.6%", "motivation": "\u975e\u4fb5\u5165\u6027\u8840\u7cd6\u76d1\u6d4b\u662f\u7cd6\u5c3f\u75c5\u7ba1\u7406\u7684\u5173\u952e\u6311\u6218\uff0c\u4f46\u5e74\u9f84\u76f8\u5173\u7684\u81ea\u4e3b\u795e\u7ecf\u53d8\u5316\u5f71\u54cd\u4f20\u7edfHRV\u5206\u6790\u7684\u51c6\u786e\u6027", "method": "\u6536\u96c643\u540d\u53d7\u8bd5\u8005\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u91c7\u7528\u65b0\u7684\u5e74\u9f84\u6b63\u5219\u5316\u6280\u672f\u5904\u7406HRV\u7279\u5f81\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u5cad\u8fd4\u5f52\u548c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u8840\u7cd6\u9884\u6d4b", "result": "\u5e74\u9f84\u6b63\u5219\u5316HRV\u7279\u5f81\u5b9e\u73b0R2=0.161(MAE=0.182)\uff0c\u6bd4\u975e\u6b63\u5219\u5316\u7279\u5f81\u63d0\u9ad825.6%\uff0c\u6700\u4f73\u9884\u6d4b\u7279\u5f81\u5305\u62echr rem\u548cds\u9636\u6bb5\u7684\u5e74\u9f84\u6b63\u5219\u5316HRV\u7279\u5f81\u4ee5\u53ca\u8214\u5f20\u538b", "conclusion": "\u5e74\u9f84\u6b63\u5219\u5316HRV\u7279\u5f81\u663e\u8457\u63d0\u9ad8\u4e86\u8840\u7cd6\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u81ea\u4e3b\u795e\u7ecf\u529f\u80fd\u8bc4\u4f30\u7684\u57fa\u672c\u9650\u5236\uff0c\u4e3a\u975e\u4fb5\u5165\u6027\u8840\u7cd6\u76d1\u6d4b\u63d0\u4f9b\u4e86\u521d\u6b65\u53ef\u884c\u6027\uff0c\u4f46\u9700\u8981\u5728\u66f4\u5927\u7fa4\u4f53\u4e2d\u9a8c\u8bc1"}}
{"id": "2508.12930", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12930", "abs": "https://arxiv.org/abs/2508.12930", "authors": ["David Hirnschall", "Robert Bajons"], "title": "The path to a goal: Understanding soccer possessions via path signatures", "comment": null, "summary": "We present a novel framework for predicting next actions in soccer\npossessions by leveraging path signatures to encode their complex\nspatio-temporal structure. Unlike existing approaches, we do not rely on fixed\nhistorical windows and handcrafted features, but rather encode the entire\nrecent possession, thereby avoiding the inclusion of potentially irrelevant or\nmisleading historical information. Path signatures naturally capture the order\nand interaction of events, providing a mathematically grounded feature encoding\nfor variable-length time series of irregular sampling frequencies without the\nnecessity for manual feature engineering. Our proposed approach outperforms a\ntransformer-based benchmark across various loss metrics and considerably\nreduces computational cost. Building on these results, we introduce a new\npossession evaluation metric based on well-established frameworks in soccer\nanalytics, incorporating both predicted action type probabilities and action\nlocation. Our metric shows greater reliability than existing metrics in\ndomain-specific comparisons. Finally, we validate our approach through a\ndetailed analysis of the 2017/18 Premier League season and discuss further\napplications and future extensions.", "AI": {"tldr": "\u4f7f\u7528\u8def\u5f84\u7b7e\u540d\u6280\u672f\u7f16\u7801\u8db3\u7403\u63a7\u7403\u7684\u7a7a\u95f4\u65f6\u95f4\u7ed3\u6784\uff0c\u9884\u6d4b\u4e0b\u4e00\u6b65\u52a8\u4f5c\uff0c\u8d85\u8d8a\u4f20\u7edfTransformer\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u5386\u53f2\u7a97\u53e3\u548c\u624b\u5de5\u7279\u5f81\u7684\u9650\u5236\uff0c\u907f\u514d\u5305\u542b\u65e0\u5173\u6216\u8bef\u5bfc\u6027\u4fe1\u606f\uff0c\u5e76\u5904\u7406\u53d8\u957f\u5ea6\u65f6\u95f8\u5e8f\u5217\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u8def\u5f84\u7b7e\u540d\u6765\u7f16\u7801\u6574\u4e2a\u63a7\u7403\u8fc7\u7a0b\u7684\u590d\u6742\u7a7a\u95f4\u65f6\u95f4\u7ed3\u6784\uff0c\u81ea\u52a8\u6350\u63cf\u4e8b\u4ef6\u7684\u987a\u5e8f\u548c\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u635f\u5931\u6307\u6807\u4e0a\u8d85\u8fc7Transformer\u57fa\u51c6\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u53ef\u9760\u7684\u63a7\u7403\u8bc4\u4ef7\u6307\u6807\u3002", "conclusion": "\u901a\u8fc72017/18\u8d5b\u5b63\u8be6\u7ec6\u5206\u6790\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u4e3a\u8db3\u7403\u5206\u6790\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\u7262\u56fa\u7684\u7279\u5f81\u7f16\u7801\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.11727", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11727", "abs": "https://arxiv.org/abs/2508.11727", "authors": ["Songyao Jin", "Biwei Huang"], "title": "Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks", "comment": null, "summary": "Multivariate Hawkes process provides a powerful framework for modeling\ntemporal dependencies and event-driven interactions in complex systems. While\nexisting methods primarily focus on uncovering causal structures among observed\nsubprocesses, real-world systems are often only partially observed, with latent\nsubprocesses posing significant challenges. In this paper, we show that\ncontinuous-time event sequences can be represented by a discrete-time model as\nthe time interval shrinks, and we leverage this insight to establish necessary\nand sufficient conditions for identifying latent subprocesses and the causal\ninfluences. Accordingly, we propose a two-phase iterative algorithm that\nalternates between inferring causal relationships among discovered subprocesses\nand uncovering new latent subprocesses, guided by path-based conditions that\nguarantee identifiability. Experiments on both synthetic and real-world\ndatasets show that our method effectively recovers causal structures despite\nthe presence of latent subprocesses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc6\u522b\u591a\u5143\u970d\u514b\u65af\u8fc7\u7a0b\u4e2d\u6f5c\u5728\u5b50\u8fc7\u7a0b\u548c\u56e0\u679c\u5f71\u54cd\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u65f6\u95f4\u6a21\u578b\u8868\u793a\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\uff0c\u5efa\u7acb\u4e86\u53ef\u8bc6\u522b\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u8fed\u4ee3\u7b97\u6cd5\u6765\u6062\u590d\u56e0\u679c\u7ed3\u6784\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u901a\u5e38\u53ea\u6709\u90e8\u5206\u88ab\u89c2\u6d4b\uff0c\u5b58\u5728\u6f5c\u5728\u5b50\u8fc7\u7a0b\u5bf9\u73b0\u6709\u65b9\u6cd5\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5df2\u89c2\u6d4b\u5b50\u8fc7\u7a0b\u7684\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u3002", "method": "\u5c06\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u8868\u793a\u4e3a\u79bb\u6563\u65f6\u95f4\u6a21\u578b\uff0c\u5efa\u7acb\u6f5c\u5728\u5b50\u8fc7\u7a0b\u548c\u56e0\u679c\u5f71\u54cd\u53ef\u8bc6\u522b\u6027\u7684\u5145\u8981\u6761\u4ef6\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u8fed\u4ee3\u7b97\u6cd5\uff1a\u4ea4\u66ff\u63a8\u65ad\u5df2\u53d1\u73b0\u5b50\u8fc7\u7a0b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u548c\u53d1\u73b0\u65b0\u7684\u6f5c\u5728\u5b50\u8fc7\u7a0b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b58\u5728\u6f5c\u5728\u5b50\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u6062\u590d\u56e0\u679c\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b58\u5728\u6f5c\u5728\u5b50\u8fc7\u7a0b\u7684\u591a\u5143\u970d\u514b\u65af\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u56e0\u679c\u7ed3\u6784\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u79bb\u6563\u65f6\u95f4\u5efa\u6a21\u548c\u8def\u5f84\u6761\u4ef6\u4fdd\u8bc1\u4e86\u53ef\u8bc6\u522b\u6027\u3002"}}
{"id": "2508.11684", "categories": ["eess.SP", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11684", "abs": "https://arxiv.org/abs/2508.11684", "authors": ["BG Tong"], "title": "A Graph Neural Network based on a Functional Topology Model: Unveiling the Dynamic Mechanisms of Non-Suicidal Self-Injury in Single-Channel EEG", "comment": null, "summary": "Objective: This study proposes and preliminarily validates a novel\n\"Functional-Energetic Topology Model\" to uncover neurodynamic mechanisms of\nNon-Suicidal Self-Injury (NSSI), using Graph Neural Networks (GNNs) to decode\nbrain network patterns from single-channel EEG in real-world settings.Methods:\nEEG data were collected over ~1 month from three adolescents with NSSI using a\nsmartphone app and a portable Fp1 EEG headband during impulsive and\nnon-impulsive states. A theory-driven GNN with seven functional nodes was\nbuilt. Performance was evaluated via intra-subject (80/20 split) and\nleave-one-subject-out cross-validation (LOSOCV). GNNExplainer was used for\ninterpretability.Results: The model achieved high intra-subject accuracy (>85%)\nand significantly above-chance cross-subject performance (approximately73.7%).\nExplainability analysis revealed a key finding: during NSSI states, a critical\nfeedback loop regulating somatic sensation exhibits dysfunction and directional\nreversal. Specifically, the brain loses its ability to self-correct via\nnegative bodily feedback, and the regulatory mechanism enters an \"ineffective\nidling\" state.Conclusion: This work demonstrates the feasibility of applying\ntheory-guided GNNs to sparse, single-channel EEG for decoding complex mental\nstates. The identified \"feedback loop reversal\" offers a novel, dynamic, and\ncomputable model of NSSI mechanisms, paving the way for objective biomarkers\nand next-generation Digital Therapeutics (DTx).", "AI": {"tldr": "\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u5355\u5bfc\u8054\u7535\u8111\u7535\u6d41\u6570\u636e\uff0c\u63d0\u51fa\u529f\u80fd-\u80fd\u91cf\u62d3\u6251\u6a21\u578b\uff0c\u53d1\u73b0\u81ea\u4f24\u884c\u4e3a\u72b6\u6001\u4e0b\u8eab\u4f53\u611f\u77e5\u8c03\u63a7\u5faa\u73af\u51fa\u73b0\u529f\u80fd\u969c\u788d\u548c\u65b9\u5411\u9006\u8f6c", "motivation": "\u7814\u7a76\u975e\u81ea\u6740\u6027\u81ea\u4f24\u884c\u4e3a(NSSI)\u7684\u795e\u7ecf\u52a8\u529b\u5b66\u673a\u5236\uff0c\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5b9e\u9645\u73af\u5883\u4e2d\u6536\u96c6\u7684\u7a00\u758f\u5355\u5bfc\u8054EEG\u6570\u636e", "method": "\u4f7f\u7528\u624b\u673a\u5e94\u7528\u548c\u4fbf\u643a\u5f0fFp1 EEG\u5934\u5e26\u6536\u96c6\u4e09\u540d\u9752\u5c11\u5e74\u7684EEG\u6570\u636e\uff0c\u6784\u5efa\u4e03\u4e2a\u529f\u80fd\u8282\u70b9\u7684GNN\u6a21\u578b\uff0c\u91c7\u7528\u5185\u90e8\u9a8c\u8bc1\u548c\u4ea4\u53c9\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4f7f\u7528GNNExplainer\u8fdb\u884c\u89e3\u91ca\u6027\u5206\u6790", "result": "\u6a21\u578b\u5728\u5185\u90e8\u9a8c\u8bc1\u4e2d\u8fbe\u5230>85%\u7684\u51c6\u786e\u7387\uff0c\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8fbe\u523073.7%\u3002\u53d1\u73b0NSSI\u72b6\u6001\u4e0b\u8eab\u4f53\u611f\u77e5\u8c03\u63a7\u5faa\u73af\u51fa\u73b0\u529f\u80fd\u969c\u788d\u548c\u65b9\u5411\u9006\u8f6c\uff0c\u8111\u90e8\u5931\u53bb\u81ea\u6211\u7ea0\u6b63\u80fd\u529b", "conclusion": "\u8bc1\u660e\u4e86\u7406\u8bba\u9a71\u52a8GNN\u5728\u7a00\u758f\u5355\u5bfc\u8054EEG\u4e2d\u89e3\u7801\u590d\u6742\u7cbe\u795e\u72b6\u6001\u7684\u53ef\u884c\u6027\uff0c\u63d0\u4f9b\u4e86\u65b0\u9898\u7684\u52a8\u6001NSSI\u673a\u5236\u6a21\u578b\uff0c\u4e3a\u5ba2\u89c2\u751f\u7269\u6807\u8bb0\u7269\u548c\u6570\u5b57\u7597\u6cd5\u5f00\u62d3\u4e86\u9053\u8def"}}
{"id": "2508.12939", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12939", "abs": "https://arxiv.org/abs/2508.12939", "authors": ["Michael Deistler", "Jan Boelts", "Peter Steinbach", "Guy Moss", "Thomas Moreau", "Manuel Gloeckler", "Pedro L. C. Rodrigues", "Julia Linhart", "Janne K. Lappalainen", "Benjamin Kurt Miller", "Pedro J. Gon\u00e7alves", "Jan-Matthis Lueckmann", "Cornelius Schr\u00f6der", "Jakob H. Macke"], "title": "Simulation-Based Inference: A Practical Guide", "comment": null, "summary": "A central challenge in many areas of science and engineering is to identify\nmodel parameters that are consistent with prior knowledge and empirical data.\nBayesian inference offers a principled framework for this task, but can be\ncomputationally prohibitive when models are defined by stochastic simulators.\nSimulation-based Inference (SBI) is a suite of methods developed to overcome\nthis limitation, which has enabled scientific discoveries in fields such as\nparticle physics, astrophysics, and neuroscience. The core idea of SBI is to\ntrain neural networks on data generated by a simulator, without requiring\naccess to likelihood evaluations. Once trained, inference is amortized: The\nneural network can rapidly perform Bayesian inference on empirical observations\nwithout requiring additional training or simulations. In this tutorial, we\nprovide a practical guide for practitioners aiming to apply SBI methods. We\noutline a structured SBI workflow and offer practical guidelines and diagnostic\ntools for every stage of the process -- from setting up the simulator and\nprior, choosing and training inference networks, to performing inference and\nvalidating the results. We illustrate these steps through examples from\nastrophysics, psychophysics, and neuroscience. This tutorial empowers\nresearchers to apply state-of-the-art SBI methods, facilitating efficient\nparameter inference for scientific discovery.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u62df\u7684\u63a8\u7406\uff08SBI\uff09\u6559\u7a0b\uff0c\u63d0\u4f9b\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u5b9e\u7528\u6307\u5357\uff0c\u65e0\u9700\u4f3c\u7136\u51fd\u6570\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u53c2\u6570\u63a8\u65ad\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8d1d\u53f6\u65af\u63a8\u65ad\u5728\u968f\u673a\u6a21\u62df\u5668\u6a21\u578b\u4e2d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u53c2\u6570\u63a8\u65ad\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u62df\u5668\u751f\u6210\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u644a\u9500\u63a8\u65ad\uff08amortized inference\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5feb\u901f\u6267\u884c\u8d1d\u53f6\u65af\u63a8\u65ad\u3002", "result": "\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684SBI\u5de5\u4f5c\u6d41\u7a0b\u3001\u5b9e\u7528\u6307\u5357\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u6db5\u76d6\u4ece\u6a21\u62df\u5668\u8bbe\u7f6e\u5230\u7ed3\u679c\u9a8c\u8bc1\u7684\u5168\u8fc7\u7a0b\u3002", "conclusion": "\u672c\u6559\u7a0b\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5e94\u7528\u6700\u5148\u8fdb\u7684SBI\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u9ad8\u6548\u53c2\u6570\u63a8\u65ad\u3002"}}
{"id": "2508.11732", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11732", "abs": "https://arxiv.org/abs/2508.11732", "authors": ["Xiangxiang Cui", "Min Zhao", "Dongmei Zhi", "Shile Qi", "Vince D Calhoun", "Jing Sui"], "title": "BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification", "comment": null, "summary": "Existing deep learning models for functional MRI-based classification have\nlimitations in network architecture determination (relying on experience) and\nfeature space fusion (mostly simple concatenation, lacking mutual learning).\nInspired by the human brain's mechanism of updating neural connections through\nlearning and decision-making, we proposed a novel BRain-Inspired feature Fusion\n(BRIEF) framework, which is able to optimize network architecture automatically\nby incorporating an improved neural network connection search (NCS) strategy\nand a Transformer-based multi-feature fusion module. Specifically, we first\nextracted 4 types of fMRI temporal representations, i.e., time series (TCs),\nstatic/dynamic functional connection (FNC/dFNC), and multi-scale dispersion\nentropy (MsDE), to construct four encoders. Within each encoder, we employed a\nmodified Q-learning to dynamically optimize the NCS to extract high-level\nfeature vectors, where the NCS is formulated as a Markov Decision Process.\nThen, all feature vectors were fused via a Transformer, leveraging both\nstable/time-varying connections and multi-scale dependencies across different\nbrain regions to achieve the final classification. Additionally, an attention\nmodule was embedded to improve interpretability. The classification performance\nof our proposed BRIEF was compared with 21 state-of-the-art models by\ndiscriminating two mental disorders from healthy controls: schizophrenia (SZ,\nn=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated\nsignificant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching\nan AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is\nthe first attempt to incorporate a brain-inspired, reinforcement learning\nstrategy to optimize fMRI-based mental disorder classification, showing\nsignificant potential for identifying precise neuroimaging biomarkers.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u8111\u7ec6\u80de\u673a\u5236\u542f\u53d1\u7684\u65b0\u9898\u5f84\u7269\u8054\u5408\u6846\u67b6BRIEF\uff0c\u901a\u8fc7\u6539\u8fdb\u795e\u7ecf\u7f51\u7edc\u8fde\u63a5\u641c\u7d22\u7b56\u7565\u548cTransformer\u878d\u5408\u6a21\u5757\uff0c\u5728\u7cbe\u795e\u5206\u88c2\u75c7\u548c\u5b64\u72ec\u75c7\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728fMRI\u5206\u7c7b\u4e2d\u5b58\u5728\u7f51\u7edc\u67b6\u6784\u786e\u5b9a\u4f9d\u8d56\u7ecf\u9a8c\u3001\u7279\u5f81\u878d\u5408\u65b9\u5f0f\u7b80\u5355\uff08\u4e3b\u8981\u4e3a\u8fde\u63a5\uff09\u7f3a\u4e4f\u76f8\u4e92\u5b66\u4e60\u673a\u5236\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faBRIEF\u6846\u67b6\uff1a1\uff09\u63d0\u53d64\u79cdfMRI\u65f6\u95f4\u8868\u5f81\uff08\u65f6\u95f4\u5e8f\u5217\u3001\u9759\u6001/\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u3001\u591a\u5c3a\u5ea6\u5206\u6563\u71b5\uff09\u6784\u5efa\u7f16\u7801\u5668\uff1b2\uff09\u4f7f\u7528\u6539\u8fdbQ\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u8fde\u63a5\u641c\u7d22\uff08NCS\uff09\uff1b3\uff09\u901a\u8fc7Transformer\u878d\u5408\u6240\u6709\u7279\u5f81\u5411\u91cf\uff0c\u5229\u7528\u7a33\u5b9a/\u65f6\u53d8\u8fde\u63a5\u548c\u591a\u5c3a\u5ea6\u4f9d\u8d56\u5173\u7cfb\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002", "result": "\u5728\u7cbe\u795e\u5206\u88c2\u75c7\uff08SZ\uff0cn=1100\uff09\u548c\u5b64\u72ec\u75c7\u8bc6\u522b\uff08ASD\uff0cn=1550\uff09\u4efb\u52a1\u4e2d\uff0c\u4e0e21\u4e2a\u6700\u65b0\u6a21\u578b\u76f8\u6bd4\uff0cBRIEF\u5b9e\u73b0\u4e862.2%\u523012.1%\u7684\u663e\u8457\u63d0\u5347\uff0cSZ\u8fbe\u523091.5%\u00b10.6%\u7684AUC\uff0cASD\u8fbe\u523078.4%\u00b10.5%\u7684AUC\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c1d\u8bd5\u5c06\u53d7\u8111\u542f\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7528\u4e8efMRI\u57fa\u7840\u7684\u7cbe\u795e\u969c\u788d\u5206\u7c7b\uff0c\u663e\u793a\u4e86\u5728\u8bc6\u522b\u7cbe\u786e\u795e\u7ecf\u5f71\u50cf\u751f\u7269\u6807\u8bb0\u65b9\u9762\u7684\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2508.11685", "categories": ["eess.SP", "cond-mat.mtrl-sci", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11685", "abs": "https://arxiv.org/abs/2508.11685", "authors": ["Farnaz Kaboudvand", "Maham Khalid", "Nydia Assaf", "Vardaan Sahgal", "Jon P. Ruffley", "Brian J. McDermott"], "title": "Enhancing Corrosion Resistance of Aluminum Alloys Through AI and ML Modeling", "comment": "Manuscript length: 11 pages, 6 figures", "summary": "Corrosion poses a significant challenge to the performance of aluminum\nalloys, particularly in marine environments. This study investigates the\napplication of machine learning (ML) algorithms to predict and optimize\ncorrosion resistance, utilizing a comprehensive open-source dataset compiled\nfrom various sources. The dataset encompasses corrosion rate data and\nenvironmental conditions, preprocessed to standardize units and formats. We\nexplored two different approaches, a direct approach, where the material's\ncomposition and environmental conditions were used as inputs to predict\ncorrosion rates; and an inverse approach, where corrosion rate served as the\ninput to identify suitable material compositions as output. We employed and\ncompared three distinct ML methodologies for forward predictions: Random Forest\nregression, optimized via grid search; a feed-forward neural network, utilizing\nReLU activation and Adam optimization; and Gaussian Process Regression (GPR),\nimplemented with GPyTorch and employing various kernel functions. The Random\nForest and neural network models provided predictive capabilities based on\nelemental compositions and environmental conditions. Notably, Gaussian Process\nRegression demonstrated superior performance, particularly with hybrid kernel\nfunctions. Log-transformed GPR further refined predictions. This study\nhighlights the efficacy of ML, particularly GPR, in predicting corrosion rates\nand material properties.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u94dd\u5408\u91d1\u6d77\u6c34\u8150\u8680\u901f\u7387\uff0c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u8868\u73b0\u6700\u4f18", "motivation": "\u94dd\u5408\u91d1\u5728\u6d77\u6d0b\u73af\u5883\u4e2d\u9762\u4e34\u4e25\u91cd\u8150\u8680\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u9884\u6d4b\u548c\u4f18\u5316\u8150\u8680\u62b5\u5f79\u80fd\u529b", "method": "\u4f7f\u7528\u7efc\u5408\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u91c7\u7528\u76f4\u63a5\u6cd5\uff08\u6750\u6599\u6210\u5206+\u73af\u5883\u6761\u4ef6\u9884\u6d4b\u8150\u8680\u7387\uff09\u548c\u9006\u5411\u6cd5\uff08\u6839\u636e\u8150\u8680\u7387\u9009\u62e9\u6750\u6599\uff09\u3002\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\u3001\u795e\u7ecf\u7f51\u7edc\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5", "result": "\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u8868\u73b0\u6700\u4f18\uff0c\u6df7\u5408\u5185\u6838\u51fd\u6570\u548c\u5bf9\u6570\u53d8\u6362\u540e\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8", "conclusion": "\u673a\u5668\u5b66\u4e60\u7279\u522b\u662fGPR\u5728\u8150\u8680\u9884\u6d4b\u548c\u6750\u6599\u9009\u62e9\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\uff0c\u4e3a\u94dd\u5408\u91d1\u8150\u8680\u9632\u62a4\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5"}}
{"id": "2508.12947", "categories": ["stat.ML", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12947", "abs": "https://arxiv.org/abs/2508.12947", "authors": ["Michael Mayer", "Mario V. W\u00fcthrich"], "title": "Shapley Values: Paired-Sampling Approximations", "comment": null, "summary": "Originally introduced in cooperative game theory, Shapley values have become\na very popular tool to explain machine learning predictions. Based on Shapley's\nfairness axioms, every input (feature component) gets a credit how it\ncontributes to an output (prediction). These credits are then used to explain\nthe prediction. The only limitation in computing the Shapley values (credits)\nfor many different predictions is of computational nature. There are two\npopular sampling approximations, sampling KernelSHAP and sampling\nPermutationSHAP. Our first novel contributions are asymptotic normality results\nfor these sampling approximations. Next, we show that the paired-sampling\napproaches provide exact results in case of interactions being of maximal order\ntwo. Furthermore, the paired-sampling PermutationSHAP possesses the additive\nrecovery property, whereas its kernel counterpart does not.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u8ba1\u7b97Shapley\u503c\u7684\u4e24\u79cd\u91c7\u6837\u8fd1\u4f3c\u65b9\u6cd5\uff08KernelSHAP\u548cPermutationSHAP\uff09\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u9ad8\u65af\u6b63\u6001\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u914d\u5bf9\u91c7\u6837\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u52a0\u6027\u6062\u590d\u6027\u8d28\u3002", "motivation": "\u89e3\u51b3Shapley\u503c\u8ba1\u7b97\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u91c7\u6837\u8fd1\u4f3c\u7b97\u6cd5\u6765\u652f\u6301\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u7684\u89e3\u91ca\u6027\u5206\u6790\u3002", "method": "\u91c7\u7528\u6570\u7406\u7edf\u8ba1\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u91c7\u6837\u8fd1\u4f3c\u7b97\u6cd5\uff08KernelSHAP\u548cPermutationSHAP\uff09\u7684\u9ad8\u65af\u6b63\u6001\u6027\u8d28\uff0c\u5e76\u7814\u7a76\u4e86\u5728\u7279\u5b9a\u4ea4\u4e92\u4f5c\u7528\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u91c7\u6837\u8fd1\u4f3c\u65b9\u6cd5\u7684\u9ad8\u65af\u6b63\u6001\u6027\uff0c\u53d1\u73b0\u5728\u6700\u5927\u4e8c\u9636\u4ea4\u4e92\u4f5c\u7528\u65f6\u914d\u5bf9\u91c7\u6837\u65b9\u6cd5\u80fd\u63d0\u4f9b\u51c6\u786e\u7ed3\u679c\uff0c\u5e76\u544a\u77e5PermutationSHAP\u5177\u6709\u52a0\u6027\u6062\u590d\u6027\u8d28\u800cKernelSHAP\u4e0d\u5177\u5907\u3002", "conclusion": "\u8fd9\u4e9b\u7406\u8bba\u7ed3\u679c\u4e3a\u9009\u62e9\u9002\u5408\u7684Shapley\u503c\u8ba1\u7b97\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5bf9\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7b97\u6cd5\u9009\u62e9\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2508.11739", "categories": ["cs.LG", "cs.CV", "I.4.6; I.5.5"], "pdf": "https://arxiv.org/pdf/2508.11739", "abs": "https://arxiv.org/abs/2508.11739", "authors": ["Luc Houriez", "Sebastian Pilarski", "Behzad Vahedi", "Ali Ahmadalipour", "Teo Honda Scully", "Nicholas Aflitto", "David Andre", "Caroline Jaffe", "Martha Wedner", "Rich Mazzola", "Josh Jeffery", "Ben Messinger", "Sage McGinley-Smith", "Sarah Russell"], "title": "Scalable Geospatial Data Generation Using AlphaEarth Foundations Model", "comment": "15 pages, 10 figures, 5 tables", "summary": "High-quality labeled geospatial datasets are essential for extracting\ninsights and understanding our planet. Unfortunately, these datasets often do\nnot span the entire globe and are limited to certain geographic regions where\ndata was collected. Google DeepMind's recently released AlphaEarth Foundations\n(AEF) provides an information-dense global geospatial representation designed\nto serve as a useful input across a wide gamut of tasks. In this article we\npropose and evaluate a methodology which leverages AEF to extend geospatial\nlabeled datasets beyond their initial geographic regions. We show that even\nbasic models like random forests or logistic regression can be used to\naccomplish this task. We investigate a case study of extending LANDFIRE's\nExisting Vegetation Type (EVT) dataset beyond the USA into Canada at two levels\nof granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for\nEvtPhys, model predictions align with ground truth. Trained models achieve 81%\nand 73% classification accuracy on EvtPhys validation sets in the USA and\nCanada, despite discussed limitations.", "AI": {"tldr": "\u5229\u7528AlphaEarth Foundations\u5168\u7403\u5730\u7403\u79d1\u8868\u793a\u6269\u5c55\u5730\u7406\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5730\u7406\u8986\u76d6\u8303\u56f4\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u5728\u7f8e\u52a0\u690d\u88ab\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc773%\u7684\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u9ad8\u8d28\u91cf\u5730\u7406\u6807\u7b7e\u6570\u636e\u96c6\u901a\u5e38\u53d7\u9650\u4e8e\u7279\u5b9a\u5730\u7406\u533a\u57df\uff0c\u65e0\u6cd5\u8986\u76d6\u5168\u7403\u8303\u56f4\uff0c\u5f71\u54cd\u5730\u7403\u79d1\u7814\u7a76\u7684\u6df1\u5ea6\u548c\u5e7f\u5ea6", "method": "\u5229\u7528Google DeepMind\u7684AlphaEarth Foundations(AEF)\u5168\u7403\u5730\u7406\u8868\u793a\uff0c\u901a\u8fc7\u968f\u673a\u68ee\u6797\u548c\u903b\u8f91\u56de\u5f52\u7b49\u57fa\u7840\u6a21\u578b\u6269\u5c55\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5730\u7406\u8303\u56f4", "result": "\u5728LANDFIRE\u690d\u88ab\u7c7b\u578b\u6570\u636e\u6269\u5c55\u5230\u52a0\u62ff\u5927\u7684\u6848\u4f8b\u4e2d\uff0cEvtPhys(13\u7c7b)\u548cEvtGp(80\u7c7b)\u4e24\u4e2a\u7c92\u5ea6\u7ea7\u522b\u4e0a\u5206\u522b\u83b7\u5f9781%\u548c73%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u6570\u636e\u4e00\u81f4", "conclusion": "AEF\u5168\u7403\u8868\u793a\u80fd\u591f\u6709\u6548\u652f\u6301\u5730\u7406\u6807\u7b7e\u6570\u636e\u7684\u8de8\u533a\u57df\u6269\u5c55\uff0c\u5373\u4f7f\u4f7f\u7528\u7b80\u5355\u6a21\u578b\u4e5f\u80fd\u83b7\u5f97\u826f\u597d\u6548\u679c\uff0c\u4e3a\u5168\u7403\u5730\u7406\u6570\u636e\u5206\u6790\u63d0\u4f9b\u65b0\u7684\u53ef\u80fd\u6027"}}
{"id": "2508.11686", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11686", "abs": "https://arxiv.org/abs/2508.11686", "authors": ["Shuai Jiao", "Jian Fang", "Tianshu Zhou", "Jinsong Li", "Yanhong Liu", "Ye Liu", "Ming Ju"], "title": "The Lost-K and Shorter-J Phenomenon in Non-Standard Ballistocardiography Data", "comment": null, "summary": "Non-standard ballistocardiogram(BCG) data generally do not have prominent J\npeaks. This paper introduces two phenomena that reduce the prominence of\nJpeaks: the shorter-J phenomenon and the lost-K phenomenon, both of which are\ncommonly observed in non-standard BCG signals . This paper also proposes three\nsignal transformation methods that effectively improve the lost-K and shorter-J\nphenomena. The methods were evaluated on a time-aligned ECG-BCG dataset with 40\nsubjects. The results show that based on the transformed signal, simple\nJ-peak-based methods using only the detection of local maxima or minima show\nbetter performance in locating J-peaks and extracting BCG cycles, especially\nfor non-standard BCG data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u5bfc\u81f4BCG\u4fe1\u53f7J\u5cf0\u4e0d\u663e\u8457\u7684\u73b0\u8c61\uff08\u77edJ\u73b0\u8c61\u548c\u5931K\u73b0\u8c61\uff09\uff0c\u5e76\u63d0\u51fa\u4e09\u79cd\u4fe1\u53f7\u53d8\u6362\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\uff0c\u572840\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u975e\u6807\u51c6BCG\u6570\u636e\u901a\u5e38\u7f3a\u4e4f\u663e\u8457\u7684J\u5cf0\uff0c\u8fd9\u4f1a\u5f71\u54cdBCG\u4fe1\u53f7\u7684\u5206\u6790\u548c\u5e94\u7528\u3002\u7814\u7a76\u8005\u53d1\u73b0\u4e86\u4e24\u79cd\u5bfc\u81f4J\u5cf0\u4e0d\u663e\u8457\u7684\u73b0\u8c61\uff0c\u9700\u8981\u627e\u5230\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u4fe1\u53f7\u53d8\u6362\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u6539\u5584\u77edJ\u73b0\u8c61\u548c\u5931K\u73b0\u8c61\u3002\u8fd9\u4e9b\u65b9\u6cd5\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u6280\u672f\u6765\u589e\u5f3aJ\u5cf0\u7684\u663e\u8457\u6027\u3002", "result": "\u572840\u540d\u53c2\u4e0e\u8005\u7684\u65f6\u95f4\u5bf9\u9f50ECG-BCG\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u7ecf\u8fc7\u53d8\u6362\u7684\u4fe1\u53f7\u80fd\u591f\u8ba9\u7b80\u5355\u7684J\u5cf0\u57fa\u7840\u65b9\u6cd5\uff08\u5982\u68c0\u6d4b\u5c40\u90e8\u6781\u5927\u503c\u6216\u6781\u5c0f\u503c\uff09\u5728\u5b9a\u4f4dJ\u5cf0\u548c\u63d0\u53d6BCG\u5468\u671f\u65b9\u9762\u53d6\u5f97\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5bf9\u975e\u6807\u51c6BCG\u6570\u636e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4fe1\u53f7\u53d8\u6362\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u5584BCG\u4fe1\u53f7\u4e2dJ\u5cf0\u4e0d\u663e\u8457\u7684\u95ee\u9898\uff0c\u4e3a\u975e\u6807\u51c6BCG\u6570\u636e\u7684\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u7b80\u5355\u7b97\u6cd5\u5728BCG\u5206\u6790\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11794", "abs": "https://arxiv.org/abs/2508.11794", "authors": ["Hemanth Macharla", "Mayukha Pal"], "title": "Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data", "comment": null, "summary": "Real-time fault classification in resource-constrained Internet of Things\n(IoT) devices is critical for industrial safety, yet training robust models in\nsuch heterogeneous environments remains a significant challenge. Standard\nFederated Learning (FL) often fails in the presence of non-IID data, leading to\nmodel divergence. This paper introduces Fed-Meta-Align, a novel four-phase\nframework designed to overcome these limitations through a sophisticated\ninitialization and training pipeline. Our process begins by training a\nfoundational model on a general public dataset to establish a competent\nstarting point. This model then undergoes a serial meta-initialization phase,\nwhere it sequentially trains on a subset of IOT Device data to learn a\nheterogeneity-aware initialization that is already situated in a favorable\nregion of the loss landscape. This informed model is subsequently refined in a\nparallel FL phase, which utilizes a dual-criterion aggregation mechanism that\nweights for IOT devices updates based on both local performance and cosine\nsimilarity alignment. Finally, an on-device personalization phase adapts the\nconverged global model into a specialized expert for each IOT Device.\nComprehensive experiments demonstrate that Fed-Meta-Align achieves an average\ntest accuracy of 91.27% across heterogeneous IOT devices, outperforming\npersonalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and\nmechanical fault datasets, respectively. This multi-stage approach of sequenced\ninitialization and adaptive aggregation provides a robust pathway for deploying\nhigh-performance intelligence on diverse TinyML networks.", "AI": {"tldr": "Fed-Meta-Align\u662f\u4e00\u4e2a\u56db\u9636\u6bb5\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u5143\u521d\u59cb\u5316\u548c\u53cc\u6807\u51c6\u805a\u5408\u673a\u5236\uff0c\u5728\u975eIID\u7269\u8054\u7f51\u8bbe\u5907\u6570\u636e\u4e0a\u5b9e\u73b091.27%\u7684\u5e73\u5747\u6545\u969c\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53473%\u4ee5\u4e0a\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u7269\u8054\u7f51\u8bbe\u5907\u5728\u975eIID\u6570\u636e\u73af\u5883\u4e0b\u5b9e\u65f6\u6545\u969c\u5206\u7c7b\u7684\u6311\u6218\uff0c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u53d1\u6563\u7684\u95ee\u9898\u3002", "method": "\u56db\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff1b2\uff09\u5e8f\u5217\u5316\u5143\u521d\u59cb\u5316\u9636\u6bb5\u5b66\u4e60\u5f02\u6784\u611f\u77e5\u521d\u59cb\u5316\uff1b3\uff09\u5e76\u884c\u8054\u90a6\u5b66\u4e60\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u672c\u5730\u6027\u80fd\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u53cc\u6807\u51c6\u805a\u5408\uff1b4\uff09\u8bbe\u5907\u7aef\u4e2a\u6027\u5316\u9636\u6bb5\u9002\u914d\u4e13\u7528\u4e13\u5bb6\u6a21\u578b\u3002", "result": "\u5728\u5f02\u6784\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u8fbe\u523091.27%\u7684\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u6bd4\u4e2a\u6027\u5316FedAvg\u548cFedProx\u5206\u522b\u63d0\u53473.87%\u548c3.37%\uff08\u7535\u6c14\u548c\u673a\u68b0\u6545\u969c\u6570\u636e\u96c6\uff09\u3002", "conclusion": "\u901a\u8fc7\u5e8f\u5217\u5316\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u805a\u5408\u7684\u591a\u9636\u6bb5\u65b9\u6cd5\uff0c\u4e3a\u591a\u6837\u5316TinyML\u7f51\u7edc\u90e8\u7f72\u9ad8\u6027\u80fd\u667a\u80fd\u63d0\u4f9b\u4e86\u7a33\u5065\u8def\u5f84\u3002"}}
{"id": "2508.11687", "categories": ["eess.SP", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.11687", "abs": "https://arxiv.org/abs/2508.11687", "authors": ["Jingpu Yang", "Mingxuan Cui", "Hang Zhang", "Fengxian Ji", "Zhengzhao Lai", "Yufeng Wang"], "title": "Agent-Based Anti-Jamming Techniques for UAV Communications in Adversarial Environments: A Comprehensive Survey", "comment": null, "summary": "Unmanned Aerial Vehicle communications are encountering increasingly severe\nmulti-source interference challenges in dynamic adversarial environments, which\nimpose higher demands on their reliability and resilience. To address these\nchallenges, agent-based autonomous anti-jamming techniques have emerged as a\ncrucial research direction. This paper presents a comprehensive survey that\nfirst formalizes the concept of intelligent anti-jamming agents for UAV\ncommunications and establishes a closed-loop decision-making framework centered\non the \"Perception-Decision-Action\" (P-D-A) paradigm. Within this framework, we\nsystematically review key technologies at each stage, with particular emphasis\non employing game theory to model UAV-jammer interactions and integrating\nreinforcement learning-based intelligent algorithms to derive adaptive\nanti-jamming strategies. Furthermore, we discuss potential limitations of\ncurrent approaches, identify critical engineering challenges, and outline\npromising future research directions, aiming to provide valuable references for\ndeveloping more intelligent and robust anti-jamming communication systems for\nUAVs.", "AI": {"tldr": "\u672c\u6587\u662f\u4e00\u4e2a\u5173\u4e8e\u65e0\u4eba\u673a\u901a\u4fe1\u667a\u80fd\u6297\u5e72\u6270\u6280\u672f\u7684\u7efc\u8ff0\u6027\u8bba\u6587\uff0c\u901a\u8fc7\"\u611f\u77e5-\u51b3\u7b56-\u884c\u52a8\"\u95ed\u73af\u6846\u67b6\u7cfb\u7edf\u5206\u6790\u4e86\u6e38\u620f\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u9002\u5e94\u6027\u6297\u5e72\u6270\u7b56\u7565\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u65e0\u4eba\u673a\u901a\u4fe1\u5728\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u9047\u5230\u8d8a\u6765\u8d8a\u4e25\u91cd\u7684\u591a\u6e90\u5e72\u6270\u6311\u6218\uff0c\u5bf9\u53ef\u9760\u6027\u548c\u5f39\u6027\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u9700\u8981\u7814\u7a76\u667a\u80fd\u5316\u7684\u81ea\u4e3b\u6297\u5e72\u6270\u6280\u672f\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4ee5\"\u611f\u77e5-\u51b3\u7b56-\u884c\u52a8\"\u4e3a\u6838\u5fc3\u7684\u95ed\u73af\u51b3\u7b56\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u5404\u9636\u6bb5\u7684\u5173\u952e\u6280\u672f\uff0c\u91cd\u70b9\u91c7\u7528\u6e38\u620f\u7406\u8bba\u5efa\u6a21\u65e0\u4eba\u673a\u4e0e\u5e72\u6270\u5668\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u96c6\u6210\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u7b97\u6cd5\u6765\u63a8\u5bfc\u9002\u5e94\u6027\u6297\u5e72\u6270\u7b56\u7565\u3002", "result": "\u5f62\u6210\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u667a\u80fd\u6297\u5e72\u6270\u6280\u672f\u5206\u6790\u6846\u67b6\uff0c\u4e3a\u65e0\u4eba\u673a\u901a\u4fe1\u6297\u5e72\u6270\u9886\u57df\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u548c\u6280\u672f\u6307\u5357\uff0c\u660e\u786e\u4e86\u6e38\u620f\u7406\u8bba\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u7814\u7a76\u8def\u5f84\u3002", "conclusion": "\u672c\u6587\u4e3a\u53d1\u5c55\u66f4\u667a\u80fd\u3001\u66f4\u7a33\u5065\u7684\u65e0\u4eba\u673a\u6297\u5e72\u6270\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u6301\u7eed\u63a8\u8fdb\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u5c06\u4f1a\u63a8\u52a8\u65e0\u4eba\u673a\u901a\u4fe1\u5728\u590d\u6742\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5f39\u6027\u3002"}}
{"id": "2508.11800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11800", "abs": "https://arxiv.org/abs/2508.11800", "authors": ["Michael Bereket", "Jure Leskovec"], "title": "Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes", "comment": null, "summary": "Reinforcement learning (RL) has proven remarkably effective at improving the\naccuracy of language models in verifiable and deterministic domains like\nmathematics. Here, we examine if current RL methods are also effective at\noptimizing language models in verifiable domains with stochastic outcomes, like\nscientific experiments. Through applications to synthetic data and real-world\nbiological experiments, we demonstrate that Group Relative Policy Optimization\n(GRPO) induces overconfident probability predictions for binary stochastic\noutcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out\n(RLOO) yield well-calibrated models. We show that removing group standard\nnormalization in GRPO fixes its miscalibration and provide a theoretical\nexplanation for why normalization causes overconfidence. Our results provide\nnew evidence against the use of standard normalization in GRPO and help pave\nthe way for applications of RL for reasoning language models beyond\ndeterministic domains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5177\u6709\u968f\u673a\u7ed3\u679c\u7684\u9886\u57df\uff08\u5982\u79d1\u5b66\u5b9e\u9a8c\uff09\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0GRPO\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u6982\u7387\u9884\u6d4b\uff0c\u800cPPO\u548cRLOO\u80fd\u4ea7\u751f\u826f\u597d\u6821\u51c6\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5177\u6709\u968f\u673a\u7ed3\u679c\u7684\u53ef\u9a8c\u8bc1\u9886\u57df\uff08\u5982\u79d1\u5b66\u5b9e\u9a8c\uff09\u4e2d\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u8d85\u8d8a\u786e\u5b9a\u6027\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u751f\u7269\u5b9e\u9a8c\u5e94\u7528\uff0c\u6bd4\u8f83Group Relative Policy Optimization (GRPO)\u3001Proximal Policy Optimization (PPO)\u548cREINFORCE Leave-One-Out (RLOO)\u7b49\u65b9\u6cd5\u5728\u968f\u673a\u7ed3\u679c\u9886\u57df\u7684\u8868\u73b0\u3002", "result": "GRPO\u4f1a\u5bfc\u81f4\u4e8c\u5143\u968f\u673a\u7ed3\u679c\u7684\u8fc7\u5ea6\u81ea\u4fe1\u6982\u7387\u9884\u6d4b\uff0c\u800cPPO\u548cRLOO\u4ea7\u751f\u826f\u597d\u6821\u51c6\u7684\u6a21\u578b\u3002\u79fb\u9664GRPO\u4e2d\u7684\u7ec4\u6807\u51c6\u5316\u53ef\u4ee5\u4fee\u590d\u5176\u6821\u51c6\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53cd\u5bf9\u5728GRPO\u4e2d\u4f7f\u7528\u6807\u51c6\u6807\u51c6\u5316\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u5728\u8d85\u8d8a\u786e\u5b9a\u6027\u9886\u57df\u7684\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.11691", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11691", "abs": "https://arxiv.org/abs/2508.11691", "authors": ["Mathis Rezzouk", "Fabrice Gagnon", "Alyson Champagne", "Mathieu Roy", "Philippe Albouy", "Michel-Pierre Coll", "Cem Subakan"], "title": "Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception", "comment": "6 pages, 2 figures, 2 tables, MLSP IEEE conference", "summary": "EEG-based analysis of pain perception, enhanced by machine learning, reveals\nhow the brain encodes pain by identifying neural patterns evoked by noxious\nstimulation. However, a major challenge that remains is the generalization of\nmachine learning models across individuals, given the high cross-participant\nvariability inherent to EEG signals and the limited focus on direct pain\nperception identification in current research. In this study, we systematically\nevaluate the performance of cross-participant generalization of a wide range of\nmodels, including traditional classifiers and deep neural classifiers for\nidentifying the sensory modality of thermal pain and aversive auditory\nstimulation from EEG recordings. Using a novel dataset of EEG recordings from\n108 participants, we benchmark model performance under both within- and\ncross-participant evaluation settings. Our findings show that traditional\nmodels suffered the largest drop from within- to cross-participant performance,\nwhile deep learning models proved more resilient, underscoring their potential\nfor subject-invariant EEG decoding. Even though performance variability\nremained high, the strong results of the graph-based model highlight its\npotential to capture subject-invariant structure in EEG signals. On the other\nhand, we also share the preprocessed dataset used in this study, providing a\nstandardized benchmark for evaluating future algorithms under the same\ngeneralization constraints.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728EEG\u4fe1\u53f7\u8de8\u53c2\u4e0e\u8005\u6a21\u5f0f\u8bc6\u522b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u4e2a\u4f53\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u5206\u4eab\u4e86\u6807\u51c6\u5316\u7684\u6570\u636e\u96c6\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u3002", "motivation": "\u89e3\u51b3EEG\u4fe1\u53f7\u5728\u75bc\u75db\u77e5\u89c9\u8bc6\u522b\u4e2d\u5b58\u5728\u7684\u9ad8\u8de8\u4e2a\u4f53\u53d8\u5f02\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u4e0d\u540c\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528108\u540d\u53c2\u4e0e\u8005\u7684EEG\u8bb0\u5f55\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u8bc4\u4f30\u4f20\u7edf\u5206\u7c7b\u5668\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5185\u90e8\u548c\u8de8\u53c2\u4e0e\u8005\u8bc4\u4f30\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u53c2\u4e0e\u8005\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u56fe\u8bba\u7684\u6a21\u578b\u80fd\u591f\u6293\u53d6\u4e3b\u4f53\u4e0d\u53d8\u7684EEG\u4fe1\u53f7\u7ed3\u6784\uff0c\u4f46\u6027\u80fd\u53d8\u5f02\u6027\u4ecd\u7136\u8f83\u9ad8\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8de8\u4e2a\u4f53EEG\u89e3\u7801\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7814\u7a76\u5206\u4eab\u7684\u6807\u51c6\u5316\u6570\u636e\u96c6\u5c06\u4e3a\u672a\u6765\u7b97\u6cd5\u8bc4\u4f30\u63d0\u4f9b\u57fa\u51c6\u3002"}}
{"id": "2508.11990", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11990", "abs": "https://arxiv.org/abs/2508.11990", "authors": ["Evan Dogariu", "Anand Brahmbhatt", "Elad Hazan"], "title": "Universal Learning of Nonlinear Dynamics", "comment": null, "summary": "We study the fundamental problem of learning a marginally stable unknown\nnonlinear dynamical system. We describe an algorithm for this problem, based on\nthe technique of spectral filtering, which learns a mapping from past\nobservations to the next based on a spectral representation of the system.\nUsing techniques from online convex optimization, we prove vanishing prediction\nerror for any nonlinear dynamical system that has finitely many marginally\nstable modes, with rates governed by a novel quantitative control-theoretic\nnotion of learnability. The main technical component of our method is a new\nspectral filtering algorithm for linear dynamical systems, which incorporates\npast observations and applies to general noisy and marginally stable systems.\nThis significantly generalizes the original spectral filtering algorithm to\nboth asymmetric dynamics as well as incorporating noise correction, and is of\nindependent interest.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8c31\u6ee4\u6ce2\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u5177\u6709\u6709\u9650\u4e2a\u8fb9\u7f18\u7a33\u5b9a\u6a21\u5f0f\u7684\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u51f8\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u9884\u6d4b\u8bef\u5dee\u7684\u6d88\u5931\u3002", "motivation": "\u89e3\u51b3\u5b66\u4e60\u672a\u77e5\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u57fa\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u8fb9\u7f18\u7a33\u5b9a\u6a21\u5f0f\u7684\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5728\u63a7\u5236\u7406\u8bba\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u4f46\u5b66\u4e60\u96be\u5ea6\u8f83\u5927\u3002", "method": "\u57fa\u4e8e\u8c31\u8868\u793a\u6280\u672f\uff0c\u5f00\u53d1\u65b0\u7684\u8c31\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u7684\u4e0d\u5bf9\u79f0\u52a8\u6001\u548c\u566a\u58f0\u6821\u6b63\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230\u975e\u7ebf\u6027\u7cfb\u7edf\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4efb\u4f55\u5177\u6709\u6709\u9650\u8fb9\u7f18\u7a33\u5b9a\u6a21\u5f0f\u7684\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5b9e\u73b0\u9884\u6d4b\u8bef\u5dee\u7684\u6d88\u5931\uff0c\u5b66\u4e60\u901f\u7387\u7531\u65b0\u7684\u53ef\u5b66\u4e60\u6027\u91cf\u5316\u63a7\u5236\u7406\u8bba\u6982\u5ff5\u51b3\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63a8\u5e7f\u4e86\u539f\u59cb\u8c31\u6ee4\u6ce2\u7b97\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u66f4\u4e00\u822c\u7684\u566a\u58f0\u548c\u8fb9\u7f18\u7a33\u5b9a\u7cfb\u7edf\uff0c\u4e3a\u975e\u7ebf\u6027\u52a8\u529b\u7cfb\u7edf\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11810", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11810", "abs": "https://arxiv.org/abs/2508.11810", "authors": ["Nitish Nagesh", "Salar Shakibhamedan", "Mahdi Bagheri", "Ziyu Wang", "Nima TaheriNejad", "Axel Jantsch", "Amir M. Rahmani"], "title": "FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation", "comment": null, "summary": "Generating synthetic data is crucial in privacy-sensitive, data-scarce\nsettings, especially for tabular datasets widely used in real-world\napplications. A key challenge is improving counterfactual and causal fairness,\nwhile preserving high utility. We present FairTabGen, a fairness-aware large\nlanguage model-based framework for tabular synthetic data generation. We\nintegrate multiple fairness definitions including counterfactual and causal\nfairness into both its generation and evaluation pipelines. We use in-context\nlearning, prompt refinement, and fairness-aware data curation to balance\nfairness and utility. Across diverse datasets, our method outperforms\nstate-of-the-art GAN-based and LLM-based methods, achieving up to 10%\nimprovements on fairness metrics such as demographic parity and path-specific\ncausal effects while retaining statistical utility. Remarkably, it achieves\nthese gains using less than 20% of the original data, highlighting its\nefficiency in low-data regimes. These results demonstrate a principled and\npractical approach for generating fair and useful synthetic tabular data.", "AI": {"tldr": "FairTabGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u516c\u5e73\u6027\u611f\u77e5\u8868\u683c\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u53cd\u4e8b\u5b9e\u548c\u56e0\u679c\u516c\u5e73\u6027\u5b9a\u4e49\uff0c\u5728\u4fdd\u6301\u6570\u636e\u6548\u7528\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u6307\u6807\uff0c\u4ec5\u970020%\u539f\u59cb\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd", "motivation": "\u5728\u9690\u79c1\u654f\u611f\u548c\u6570\u636e\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u751f\u6210\u5408\u6210\u8868\u683c\u6570\u636e\u65f6\uff0c\u9700\u8981\u540c\u65f6\u63d0\u9ad8\u53cd\u4e8b\u5b9e\u548c\u56e0\u679c\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6570\u636e\u6548\u7528\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "\u4f7f\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u516c\u5e73\u6027\u5b9a\u4e49\u5230\u751f\u6210\u548c\u8bc4\u4f30\u6d41\u7a0b\u4e2d\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u63d0\u793a\u4f18\u5316\u548c\u516c\u5e73\u6027\u611f\u77e5\u6570\u636e\u7b5b\u9009\u6765\u5e73\u8861\u516c\u5e73\u6027\u548c\u6548\u7528", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u7684GAN\u548cLLM\u65b9\u6cd5\uff0c\u516c\u5e73\u6027\u6307\u6807\uff08\u5982\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u548c\u8def\u5f84\u7279\u5b9a\u56e0\u679c\u6548\u5e94\uff09\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u540c\u65f6\u4fdd\u6301\u7edf\u8ba1\u6548\u7528\uff0c\u4ec5\u9700\u4e0d\u523020%\u7684\u539f\u59cb\u6570\u636e", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u548c\u5b9e\u7528\u6027\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u751f\u6210\u516c\u5e73\u4e14\u6709\u7528\u7684\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u9ad8\u6548\u6027"}}
{"id": "2508.11692", "categories": ["eess.SP", "cs.AI", "68T07, 68T05", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11692", "abs": "https://arxiv.org/abs/2508.11692", "authors": ["Eduardo Di Santi", "Ruixiang Ci", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Jonathan Brown", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025, Dresden,\n  Germany. Conference: https://tu-dresden.de/raildresden2025. Book of\n  abstracts: https://tu-dresden.de/raildresden2025/BoA.pdf. 8 pages, 6 figures,\n  1 table", "summary": "The Point Machine (PM) is a critical piece of railway equipment that switches\ntrain routes by diverting tracks through a switchblade. As with any critical\nsafety equipment, a failure will halt operations leading to service\ndisruptions; therefore, pre-emptive maintenance may avoid unnecessary\ninterruptions by detecting anomalies before they become failures. Previous work\nrelies on several inputs and crafting custom features by segmenting the signal.\nThis not only adds additional requirements for data collection and processing,\nbut it is also specific to the PM technology, the installed locations and\noperational conditions limiting scalability. Based on the available maintenance\nrecords, the main failure causes for PM are obstacles, friction, power source\nissues and misalignment. Those failures affect the energy consumption pattern\nof PMs, altering the usual (or healthy) shape of the power signal during the PM\nmovement. In contrast to the current state-of-the-art, our method requires only\none input. We apply a deep learning model to the power signal pattern to\nclassify if the PM is nominal or associated with any failure type, achieving\n>99.99\\% precision, <0.01\\% false positives and negligible false negatives. Our\nmethodology is generic and technology-agnostic, proven to be scalable on\nseveral electromechanical PM types deployed in both real-world and test bench\nenvironments. Finally, by using conformal prediction the maintainer gets a\nclear indication of the certainty of the system outputs, adding a confidence\nlayer to operations and making the method compliant with the ISO-17359\nstandard.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8f7b\u91cf\u5316\u65b9\u6cd5\uff0c\u4ec5\u9700\u7535\u529b\u4fe1\u53f7\u8fdb\u884c\u8f6f\u4ef6\u5207\u6362\u673a\u6545\u969c\u68c0\u6d4b\uff0c\u8fbe\u5230\u6781\u9ad8\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u591a\u79cd\u8f93\u5165\u548c\u7279\u5f81\u5de5\u7a0b\uff0c\u4e14\u4e13\u7528\u4e8e\u7279\u5b9a\u6280\u672f\uff0c\u7f3a\u4e4f\u6269\u5c55\u6027\u3002\u8f6f\u4ef6\u5207\u6362\u673a\u6545\u969c\u4f1a\u5bfc\u81f4\u670d\u52a1\u4e2d\u65ad\uff0c\u9700\u8981\u9884\u9632\u6027\u7ef4\u62a4", "method": "\u4ec5\u4f7f\u7528\u5355\u4e00\u7535\u529b\u4fe1\u53f7\u8f93\u5165\uff0c\u5e94\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u7535\u529b\u6d88\u8017\u6a21\u5f0f\uff0c\u8fdb\u884c\u6545\u969c\u5206\u7c7b\uff0c\u4f7f\u7528\u9075\u5faa\u9884\u6d4b\u63d0\u4f9b\u4fe1\u5fc3\u5ea6\u6307\u6807", "result": "\u8fbe\u5230>99.99%\u7684\u7cbe\u786e\u5ea6\uff0c<0.01%\u7684\u5047\u6b63\u7387\uff0c\u53ef\u5ffd\u7565\u7684\u5047\u963f\u7387\uff0c\u5728\u771f\u5b9e\u73af\u5883\u548c\u6d4b\u8bd5\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u6269\u5c55\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u7528\u6027\u5f3a\u3001\u6280\u672f\u65e0\u5173\uff0c\u4ec5\u9700\u5355\u4e00\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6545\u969c\u68c0\u6d4b\uff0c\u7b26\u5408ISO-17359\u6807\u51c6\u8981\u6c42\uff0c\u4e3a\u9884\u9632\u6027\u7ef4\u62a4\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12530", "categories": ["cs.LG", "cs.CV", "stat.ML", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.12530", "abs": "https://arxiv.org/abs/2508.12530", "authors": ["Hyunsoo Song", "Seungwhan Kim", "Seungkyu Lee"], "title": "Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs", "comment": "8 pages, 6 figures", "summary": "Variational autoencoders (VAEs), one of the most widely used generative\nmodels, are known to suffer from posterior collapse, a phenomenon that reduces\nthe diversity of generated samples. To avoid posterior collapse, many prior\nworks have tried to control the influence of regularization loss. However, the\ntrade-off between reconstruction and regularization is not satisfactory. For\nthis reason, several methods have been proposed to guarantee latent\nidentifiability, which is the key to avoiding posterior collapse. However, they\nrequire structural constraints on the network architecture. For further\nclarification, we define local posterior collapse to reflect the importance of\nindividual sample points in the data space and to relax the network constraint.\nThen, we propose Latent Reconstruction(LR) loss, which is inspired by\nmathematical properties of injective and composite functions, to control\nposterior collapse without restriction to a specific architecture. We\nexperimentally evaluate our approach, which controls posterior collapse on\nvaried datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u51fa\u5c40\u90e8\u540e\u9a8c\u51b2\u7a81\u6982\u5ff5\u548c\u6f5c\u5728\u91cd\u5efa\u635f\u5931\uff0c\u89e3\u51b3VAE\u540e\u9a8c\u51b2\u7a81\u95ee\u9898\uff0c\u65e0\u9700\u7f51\u7edc\u7ed3\u6784\u9650\u5236", "motivation": "VAE\u6a21\u578b\u5b58\u5728\u540e\u9a8c\u51b2\u7a81\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u6837\u672c\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u7f51\u7edc\u7ed3\u6784\u9650\u5236", "method": "\u5b9a\u4e49\u5c40\u90e8\u540e\u9a8c\u51b2\u7a81\u6982\u5ff5\uff0c\u63d0\u51faLatent Reconstruction\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u5355\u5c04\u548c\u590d\u5408\u51fd\u6570\u7684\u6570\u5b66\u6027\u8d28\u63a7\u5236\u540e\u9a8c\u51b2\u7a81", "result": "\u5728MNIST\u3001fashionMNIST\u3001Omniglot\u3001CelebA\u3001FFHQ\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63a7\u5236VAE\u540e\u9a8c\u51b2\u7a81\uff0c\u4e14\u4e0d\u9700\u8981\u7279\u5b9a\u7f51\u7edc\u7ed3\u6784\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027"}}
{"id": "2508.11876", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11876", "abs": "https://arxiv.org/abs/2508.11876", "authors": ["Hoang-Thang Ta", "Duy-Quy Thai", "Phuong-Linh Tran-Thi"], "title": "Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks", "comment": "6pages", "summary": "For years, many neural networks have been developed based on the\nKolmogorov-Arnold Representation Theorem (KART), which was created to address\nHilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks\n(KANs) have attracted attention from the research community, stimulating the\nuse of polynomial functions such as B-splines and RBFs. However, these\nfunctions are not fully supported by GPU devices and are still considered less\npopular. In this paper, we propose the use of fast computational functions,\nsuch as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as\nbasis components in Kolmogorov-Arnold Networks (KANs). By integrating these\nfunction combinations into the network structure, we aim to enhance\ncomputational efficiency. Experimental results show that these combinations\nmaintain competitive performance while offering potential improvements in\ntraining time and generalization.", "AI": {"tldr": "\u4f7f\u7528ReLU\u548c\u4e09\u89d2\u51fd\u6570\u7b49\u5feb\u901f\u8ba1\u7b97\u51fd\u6570\u66ff\u4ee3\u4f20\u7edf\u591a\u9879\u5f0f\u51fd\u6570\u4f5c\u4e3aKANs\u7684\u57fa\u7840\u7ec4\u4ef6\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6", "motivation": "\u4f20\u7edf\u7684KANs\u4f7f\u7528B\u6837\u6761\u548cRBF\u7b49\u591a\u9879\u5f0f\u51fd\u6570\uff0c\u4f46\u8fd9\u4e9b\u51fd\u6570\u5728GPU\u8bbe\u5907\u4e0a\u652f\u6301\u4e0d\u8db3\u4e14\u4e0d\u591f\u6d41\u884c\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u51fd\u6570", "method": "\u63d0\u51fa\u5728Kolmogorov-Arnold\u7f51\u7edc\u4e2d\u4f7f\u7528ReLU\u3001sin\u3001cos\u3001arctan\u7b49\u5feb\u901f\u8ba1\u7b97\u51fd\u6570\u4f5c\u4e3a\u57fa\u7840\u7ec4\u4ef6\uff0c\u5c06\u8fd9\u4e9b\u51fd\u6570\u7ec4\u5408\u96c6\u6210\u5230\u7f51\u7edc\u7ed3\u6784\u4e2d", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8fd9\u4e9b\u51fd\u6570\u7ec4\u5408\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5177\u6709\u6f5c\u5728\u6539\u8fdb", "conclusion": "\u4f7f\u7528\u5feb\u901f\u8ba1\u7b97\u51fd\u6570\u4f5c\u4e3aKANs\u7684\u57fa\u7840\u7ec4\u4ef6\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd"}}
{"id": "2508.11693", "categories": ["eess.SP", "cs.AI", "cs.LG", "68T05, 68T10", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11693", "abs": "https://arxiv.org/abs/2508.11693", "authors": ["Francisco L\u00f3pez", "Eduardo Di Santi", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025\n  (International Conference on Railway Operations Modelling and Analysis),\n  Dresden, Germany", "summary": "Track Circuits (TC) are the main signalling devices used to detect the\npresence of a train on a rail track. It has been used since the 19th century\nand nowadays there are many types depending on the technology. As a general\nclassification, Track Circuits can be divided into 2 main groups, DC (Direct\nCurrent) and AC (Alternating Current) circuits. This work is focused on a\nparticular AC track circuit, called \"Smart Train Detection System\" (STDS),\ndesigned with both high and low-frequency bands. This approach uses STDS\ncurrent data applied to an SVM (support vector machine) classifier as a type of\nfailure identifier. The main purpose of this work consists on determine\nautomatically which is the component of the track that is failing to improve\nthe maintenance action. Model was trained to classify 15 different failures\nthat belong to 3 more general categories. The method was tested with field data\nfrom 10 different track circuits and validated by the STDS track circuit expert\nand maintainers. All use cases were correctly classified by the method.", "AI": {"tldr": "\u57fa\u4e8eSVM\u5206\u7c7b\u5668\u7684\u667a\u80fd\u8f68\u9053\u7535\u8def\u6545\u969c\u81ea\u52a8\u8bc6\u522b\u65b9\u6cd5\uff0c\u80fd\u591f\u51c6\u786e\u5206\u7c7b15\u79cd\u6545\u969c\u7c7b\u578b\uff0c\u63d0\u5347\u7ef4\u62a4\u6548\u7387\u3002", "motivation": "\u8f68\u9053\u7535\u8def\u662f\u8f68\u9053\u4fe1\u53f7\u8bbe\u5907\u7684\u6838\u5fc3\uff0c\u4f20\u7edf\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u5177\u4f53\u6545\u969c\u7ec4\u4ef6\u7684\u667a\u80fd\u5316\u65b9\u6848\u6765\u6539\u5584\u7ef4\u62a4\u5de5\u4f5c\u3002", "method": "\u91c7\u7528\u652f\u6301\u5411\u91cf\u673a(SVM)\u5206\u7c7b\u5668\uff0c\u5229\u7528STDS\u667a\u80fd\u8f68\u9053\u68c0\u6d4b\u7cfb\u7edf\u7684\u9ad8\u4f4e\u9891\u6d41\u6570\u636e\u8fdb\u884c\u6545\u969c\u5206\u7c7b\u8bc6\u522b\u3002", "result": "\u572810\u4e2a\u4e0d\u540c\u8f68\u9053\u7535\u8def\u7684\u5b9e\u9645\u5730\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0c\u6240\u6709\u7528\u4f8b\u90fd\u88ab\u6b63\u786e\u5206\u7c7b\uff0c\u7ecfSTDS\u4e13\u5bb6\u548c\u7ef4\u62a4\u4eba\u5458\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u81ea\u52a8\u8bc6\u522b\u8f68\u9053\u7535\u8def\u6545\u969c\uff0c\u5bf9\u5e9415\u79cd\u4e0d\u540c\u6545\u969c\u7c7b\u578b\uff0c\u4e3a\u8f68\u9053\u8fd0\u8425\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u667a\u80fd\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12569", "categories": ["cs.LG", "cs.CE", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.12569", "abs": "https://arxiv.org/abs/2508.12569", "authors": ["Quercus Hernandez", "Max Win", "Thomas C. O'Connor", "Paulo E. Arratia", "Nathaniel Trask"], "title": "Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems", "comment": "34 pages, 12 figures", "summary": "Multiscale systems are ubiquitous in science and technology, but are\nnotoriously challenging to simulate as short spatiotemporal scales must be\nappropriately linked to emergent bulk physics. When expensive high-dimensional\ndynamical systems are coarse-grained into low-dimensional models, the entropic\nloss of information leads to emergent physics which are dissipative,\nhistory-dependent, and stochastic. To machine learn coarse-grained dynamics\nfrom time-series observations of particle trajectories, we propose a framework\nusing the metriplectic bracket formalism that preserves these properties by\nconstruction; most notably, the framework guarantees discrete notions of the\nfirst and second laws of thermodynamics, conservation of momentum, and a\ndiscrete fluctuation-dissipation balance crucial for capturing non-equilibrium\nstatistics. We introduce the mathematical framework abstractly before\nspecializing to a particle discretization. As labels are generally unavailable\nfor entropic state variables, we introduce a novel self-supervised learning\nstrategy to identify emergent structural variables. We validate the method on\nbenchmark systems and demonstrate its utility on two challenging examples: (1)\ncoarse-graining star polymers at challenging levels of coarse-graining while\npreserving non-equilibrium statistics, and (2) learning models from high-speed\nvideo of colloidal suspensions that capture coupling between local\nrearrangement events and emergent stochastic dynamics. We provide open-source\nimplementations in both PyTorch and LAMMPS, enabling large-scale inference and\nextensibility to diverse particle-based systems.", "AI": {"tldr": "\u901a\u8fc7\u7ea6\u5316\u7c92\u5b50\u8f68\u8ff9\u65f6\u95f4\u5e8f\u5217\u5b66\u4e60\u7c92\u5b50\u7cfb\u7edf\u7684\u591a\u6807\u5ea6\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u91c7\u7528\u8ba1\u91cf\u62ec\u53f7\u5f62\u5f0f\u4fdd\u8bc1\u70ed\u529b\u5b66\u5b9a\u5f8b\u548c\u6fc0\u5149\u6563\u5c04\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u8bc6\u522b\u51fa\u73b0\u7ed3\u6784\u53d8\u91cf", "motivation": "\u591a\u6807\u5ea6\u7cfb\u7edf\u6a21\u62df\u9762\u4e34\u77ed\u65f6\u7a7a\u5c3a\u5ea6\u4e0e\u51fa\u73b0\u4f53\u7269\u7406\u8054\u7cfb\u7684\u6311\u6218\uff0c\u7ea6\u5316\u7c92\u5b50\u52a8\u529b\u5b66\u4f1a\u5bfc\u81f4\u4fe1\u606f\u635f\u5931\u548c\u51fa\u73b0\u7684\u6563\u5c04\u6027\u3001\u5386\u53f2\u4f9d\u8d56\u6027\u3001\u968f\u673a\u6027\u8f6f\u4f53\u7269\u7406", "method": "\u63d0\u51fa\u4f7f\u7528\u8ba1\u91cf\u62ec\u53f7\u5f62\u5f0f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7c92\u5b50\u79bb\u6563\u5316\u5b9e\u73b0\uff0c\u4fdd\u8bc1\u70ed\u529b\u5b66\u7b2c\u4e00\u3001\u7b2c\u4e8c\u5b9a\u5f8b\u3001\u52a8\u91cf\u5b88\u606f\u548c\u6fc0\u5149\u6563\u5c04\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u8bc6\u522b\u51fa\u73b0\u7ed3\u6784\u53d8\u91cf", "result": "\u5728\u6807\u51c6\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u661f\u5f62\u805a\u5408\u7269\u5177\u6709\u6311\u6218\u6027\u7684\u7ea6\u5316\u6c34\u5e73\u4fdd\u6301\u975e\u5e73\u8861\u7edf\u8ba1\uff0c\u4ee5\u53ca\u4ece\u9ad8\u901f\u6444\u50cf\u7684\u80f6\u4f53\u60ac\u6d6e\u4f53\u5b66\u4e60\u63aa\u5408\u5c40\u90e8\u91cd\u6392\u4e8b\u4ef6\u4e0e\u51fa\u73b0\u968f\u673a\u52a8\u529b\u5b66\u7684\u6a21\u578b", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u673a\u5668\u5b66\u4e60\u4fdd\u6301\u70ed\u529b\u5b66\u5b9a\u5f8b\u548c\u6fc0\u5149\u6563\u5c04\u5e73\u8861\u7684\u7c92\u5b50\u7cfb\u7edf\u7ea6\u5316\u52a8\u529b\u5b66\u6846\u67b6\uff0c\u5e76\u5f00\u6e90\u4e86PyTorch\u548cLAMMPS\u5b9e\u73b0\uff0c\u652f\u6301\u5927\u89c4\u6a21\u63a8\u7406\u548c\u6269\u5c55\u5230\u591a\u6837\u5316\u7c92\u5b50\u7cfb\u7edf"}}
{"id": "2508.11880", "categories": ["cs.LG", "I.2.0; I.5.0"], "pdf": "https://arxiv.org/pdf/2508.11880", "abs": "https://arxiv.org/abs/2508.11880", "authors": ["Yuto Omae"], "title": "PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression", "comment": "15 pages", "summary": "Convolutional Neural Networks (CNNs) are an effective approach for\nclassification tasks, particularly when the training dataset is large. Although\nCNNs have long been considered a black-box classification method, they can be\nused as a white-box method through visualization techniques such as Grad-CAM.\nWhen training samples are limited, incorporating a Principal Component Analysis\n(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can\neffectively improve classification performance. However, traditional Grad-CAM\ncannot be directly applied to PCA and/or SVM layers. It is important to\ngenerate attention regions for PCA and/or SVM layers in CNNs to facilitate the\ndevelopment of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a\nmethod for visualizing attention regions in PCA feature vectors, and\n``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM\nclassifier layer. To complete our methods analytically, it is necessary to\nsolve the closed-form Jacobian consisting of partial derivatives from the last\nconvolutional layer to the PCA and/or SVM layers. In this paper, we present the\nexact closed-form Jacobian and the visualization results of our methods applied\nto several major datasets.", "AI": {"tldr": "\u63d0\u51faPCA-Grad-CAM\u548cSVM-Grad-CAM\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edfGrad-CAM\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eCNN\u4e2dPCA\u548cSVM\u5c42\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6c42\u89e3\u95ed\u5f0f\u96c5\u53ef\u6bd4\u77e9\u9635\u5b9e\u73b0\u5173\u6ce8\u533a\u57df\u53ef\u89c6\u5316\u3002", "motivation": "\u5f53\u8bad\u7ec3\u6837\u672c\u6709\u9650\u65f6\uff0cCNN\u4e2d\u52a0\u5165PCA\u5c42\u548c/\u6216SVM\u5206\u7c7b\u5668\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f20\u7edfGrad-CAM\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u8fd9\u4e9b\u5c42\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u6765\u652f\u6301\u767d\u76d2\u65b9\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6c42\u89e3\u4ece\u6700\u540e\u5377\u79ef\u5c42\u5230PCA\u548c/\u6216SVM\u5c42\u7684\u95ed\u5f0f\u96c5\u53ef\u6bd4\u77e9\u9635\uff08\u504f\u5bfc\u6570\u77e9\u9635\uff09\uff0c\u63d0\u51faPCA-Grad-CAM\u548cSVM-Grad-CAM\u65b9\u6cd5\u6765\u53ef\u89c6\u5316\u8fd9\u4e9b\u5c42\u4e2d\u7684\u5173\u6ce8\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u53ef\u89c6\u5316\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u80fd\u591f\u53ef\u89c6\u5316CNN\u4e2dPCA\u548cSVM\u5c42\u5173\u6ce8\u533a\u57df\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u6709\u9650\u8bad\u7ec3\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u767d\u76d2\u5206\u6790\u63d0\u4f9b\u4e86\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2508.11700", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11700", "abs": "https://arxiv.org/abs/2508.11700", "authors": ["Mesut Ko\u00e7yi\u011fit", "Bahman Javadi", "Russell Thomson", "Sebastian Pfautsch", "Oliver Obst"], "title": "Operational machine learning for park-scale irrigation to support urban cooling", "comment": "6 pages, 3 figures", "summary": "Urban parks can mitigate local heat, yet irrigation control is usually tuned\nfor water savings rather than cooling. We report on SIMPaCT (Smart Irrigation\nManagement for Parks and Cool Towns), a park-scale deployment that links\nper-zone soil-moisture forecasts to overnight irrigation set-points in support\nof urban cooling. SIMPaCT ingests data from 202 soil-moisture sensors, 50\ntemperature-relative humidity (TRH) nodes, and 13 weather stations, and trains\na per-sensor k-nearest neighbours (kNN) predictor on short rolling windows\n(200-900h). A rule-first anomaly pipeline screens missing and stuck-at signals,\nwith model-based checks (Isolation Forest and ARIMA). When a device fails, a\nmutual-information neighbourhood selects the most informative neighbour and a\nsmall multilayer perceptron supplies a \"virtual sensor\" until restoration.\nAcross sensors the mean absolute error was 0.78%, comparable to more complex\nbaselines; the upper-quartile error (P75) was lower for kNN than SARIMA (0.71%\nvs 0.93%). SIMPaCT runs daily and writes proposed set-points to the existing\ncontroller for operator review. This short communication reports an operational\nrecipe for robust, cooling-oriented irrigation at city-park scale.", "AI": {"tldr": "SIMPaCT\u662f\u4e00\u4e2a\u667a\u80fd\u704c\u6e89\u7cfb\u7edf\uff0c\u901a\u8fc7\u571f\u58e4\u6e7f\u5ea6\u9884\u6d4b\u4f18\u5316\u516c\u56ed\u704c\u6e89\u6765\u964d\u4f4e\u57ce\u5e02\u70ed\u5c9b\u6548\u5e94\uff0c\u4f7f\u7528kNN\u7b97\u6cd5\u548c\u5f02\u5e38\u68c0\u6d4b\u5b9e\u73b0\u7a33\u5065\u7684\u9884\u6d4b\u548c\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u516c\u56ed\u704c\u6e89\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u8282\u6c34\uff0c\u4f46\u5ffd\u89c6\u4e86\u704c\u6e89\u5bf9\u57ce\u5e02\u964d\u6e29\u7684\u6f5c\u5728\u6548\u76ca\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u4f18\u5316\u8282\u6c34\u548c\u964d\u6e29\u6548\u679c\u7684\u667a\u80fd\u704c\u6e89\u65b9\u6848\u3002", "method": "\u4f7f\u7528202\u4e2a\u571f\u58e4\u6e7f\u5ea6\u4f20\u611f\u5668\u300150\u4e2a\u6e29\u6e7f\u5ea6\u8282\u70b9\u548c13\u4e2a\u6c14\u8c61\u7ad9\u6570\u636e\uff0c\u91c7\u7528kNN\u7b97\u6cd5\u8fdb\u884c\u77ed\u671f\u6eda\u52a8\u9884\u6d4b\uff0c\u914d\u5408\u5f02\u5e38\u68c0\u6d4b\u548c\u865a\u62df\u4f20\u611f\u5668\u6280\u672f\u5904\u7406\u8bbe\u5907\u6545\u969c\u3002", "result": "\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.78%\uff0c\u4f18\u4e8eSARIMA\u7b49\u590d\u6742\u57fa\u7ebf\u65b9\u6cd5\uff08P75\u8bef\u5dee0.71% vs 0.93%\uff09\uff0c\u7cfb\u7edf\u5df2\u5b9e\u73b0\u65e5\u5e38\u8fd0\u884c\u5e76\u751f\u6210\u704c\u6e89\u8bbe\u5b9a\u503c\u5efa\u8bae\u3002", "conclusion": "SIMPaCT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728\u57ce\u5e02\u516c\u56ed\u5c3a\u5ea6\u4e0a\u5b9e\u73b0\u7a33\u5065\u3001\u4ee5\u964d\u6e29\u4e3a\u5bfc\u5411\u7684\u667a\u80fd\u704c\u6e89\u64cd\u4f5c\u65b9\u6848\uff0c\u6210\u529f\u5e73\u8861\u4e86\u8282\u6c34\u548c\u964d\u6e29\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2508.12758", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.12758", "abs": "https://arxiv.org/abs/2508.12758", "authors": ["Sowmini Devi Veeramachaneni", "Ramamurthy Garimella"], "title": "Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning", "comment": null, "summary": "This paper presents Constrained Centroid Clustering (CCC), a method that\nextends classical centroid-based clustering by enforcing a constraint on the\nmaximum distance between the cluster center and the farthest point in the\ncluster. Using a Lagrangian formulation, we derive a closed-form solution that\nmaintains interpretability while controlling cluster spread. To evaluate CCC,\nwe conduct experiments on synthetic circular data with radial symmetry and\nuniform angular distribution. Using ring-wise, sector-wise, and joint entropy\nas evaluation metrics, we show that CCC achieves more compact clusters by\nreducing radial spread while preserving angular structure, outperforming\nstandard methods such as K-means and GMM. The proposed approach is suitable for\napplications requiring structured clustering with spread control, including\nsensor networks, collaborative robotics, and interpretable pattern analysis.", "AI": {"tldr": "\u63d0\u51fa\u7ea6\u675f\u8d28\u5fc3\u805a\u7c7b(CCC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u7c07\u4e2d\u5fc3\u5230\u6700\u8fdc\u70b9\u7684\u6700\u5927\u8ddd\u79bb\u6765\u6269\u5c55\u4f20\u7edf\u8d28\u5fc3\u805a\u7c7b\uff0c\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u516c\u5f0f\u5f97\u5230\u95ed\u5f0f\u89e3\uff0c\u5728\u5408\u6210\u73af\u5f62\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8eK-means\u548cGMM\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d28\u5fc3\u805a\u7c7b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u7c07\u5206\u5e03\u7684\u660e\u786e\u63a7\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ea6\u675f\u7c07\u5185\u6700\u5927\u8ddd\u79bb\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u516c\u5f0f\u63a8\u5bfc\u95ed\u5f0f\u89e3\uff0c\u5728\u8d28\u5fc3\u805a\u7c7b\u57fa\u7840\u4e0a\u6dfb\u52a0\u6700\u5927\u8ddd\u79bb\u7ea6\u675f\uff0c\u63a7\u5236\u7c07\u7684\u5f84\u5411\u6269\u6563\u3002", "result": "\u5728\u5408\u6210\u73af\u5f62\u6570\u636e\u4e0a\uff0cCCC\u901a\u8fc7\u51cf\u5c11\u5f84\u5411\u6269\u6563\u540c\u65f6\u4fdd\u6301\u89d2\u5ea6\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u66f4\u7d27\u51d1\u7684\u805a\u7c7b\uff0c\u5728\u73af\u5411\u71b5\u3001\u6247\u533a\u71b5\u548c\u8054\u5408\u71b5\u6307\u6807\u4e0a\u4f18\u4e8eK-means\u548cGMM\u3002", "conclusion": "CCC\u65b9\u6cd5\u9002\u7528\u4e8e\u9700\u8981\u7ed3\u6784\u5316\u805a\u7c7b\u548c\u6269\u6563\u63a7\u5236\u7684\u5e94\u7528\u573a\u666f\uff0c\u5982\u4f20\u611f\u5668\u7f51\u7edc\u3001\u534f\u4f5c\u673a\u5668\u4eba\u548c\u53ef\u89e3\u91ca\u6a21\u5f0f\u5206\u6790\u3002"}}
{"id": "2508.11921", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11921", "abs": "https://arxiv.org/abs/2508.11921", "authors": ["Yibo Zhong"], "title": "ENA: Efficient N-dimensional Attention", "comment": "WIP", "summary": "Efficient modeling of long sequences of high-order data requires a more\nefficient architecture than Transformer. In this paper, we investigate two key\naspects of extending linear recurrent models, especially those originally\ndesigned for language modeling, to high-order data (1D to ND): scanning\nstrategies and attention-hybrid architectures. Empirical results suggest that\nscanning provides limited benefits, while attention-hybrid models yield\npromising results. Focusing on the latter, we further evaluate types of\nattention and find that tiled high-order sliding window attention (SWA) is\nefficient in both theory and practice. We term the resulting hybrid\narchitecture of linear recurrence and high-order SWA as Efficient N-dimensional\nAttention (ENA). We then conduct several experiments to demonstrate its\neffectiveness. The intuition behind ENA is that linear recurrence compresses\nglobal information into a state, while SWA complements it by enforcing strict\nlocal modeling. Together, they form a simple framework that offers a promising\nand practical solution for ultra-long high-order data modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86Efficient N-dimensional Attention (ENA)\u67b6\u6784\uff0c\u7ed3\u5408\u7ebf\u6027\u5faa\u73af\u548c\u9ad8\u9636\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u7528\u4e8e\u9ad8\u6548\u5efa\u6a21\u8d85\u957f\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u5904\u7406\u957f\u5e8f\u5217\u9ad8\u7ef4\u6570\u636e\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u6765\u6269\u5c55\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u5230\u9ad8\u7ef4\u6570\u636e\u3002", "method": "\u7814\u7a76\u626b\u63cf\u7b56\u7565\u548c\u6ce8\u610f\u529b\u6df7\u5408\u67b6\u6784\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u6df7\u5408\u6a21\u578b\u6548\u679c\u66f4\u597d\u3002\u91c7\u7528\u5e73\u94fa\u9ad8\u9636\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b(SWA)\u4e0e\u7ebf\u6027\u5faa\u73af\u7ed3\u5408\uff0c\u5f62\u6210ENA\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eENA\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u9ad8\u6548\uff0c\u80fd\u591f\u6709\u6548\u5efa\u6a21\u8d85\u957f\u9ad8\u7ef4\u6570\u636e\u3002", "conclusion": "\u7ebf\u6027\u5faa\u73af\u538b\u7f29\u5168\u5c40\u4fe1\u606f\uff0cSWA\u8865\u5145\u5c40\u90e8\u5efa\u6a21\uff0c\u4e24\u8005\u7ed3\u5408\u4e3a\u8d85\u957f\u9ad8\u7ef4\u6570\u636e\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11790", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11790", "abs": "https://arxiv.org/abs/2508.11790", "authors": ["Oveys Delafrooz Noroozi", "Jiyoon Han", "Wei Tang", "Zhengya Zhang", "Upamanyu Madhow"], "title": "Scaling Wideband Massive MIMO Radar via Beamspace Dimension Reduction", "comment": null, "summary": "We present an architecture for scaling digital beamforming for wideband\nmassive MIMO radar. Conventional spatial processing becomes computationally\nprohibitive as array size grows; for example, the computational complexity of\nMVDR beamforming scales as O(N^3) for an N-element array. In this paper, we\nshow that energy concentration in beamspace provides the basis for drastic\ncomplexity reduction, with array scaling governed by the O(NlogN) complexity of\nthe spatial FFT used for beamspace transformation. Specifically, we propose an\narchitecture for windowed beamspace MVDR beamforming, parallelized across\ntargets and subbands, and evaluate its efficacy for beamforming and\ninterference suppression for government-supplied wideband radar data from the\nDARPA SOAP (Scalable On-Array Processing) program. We demonstrate that our\napproach achieves detection performance comparable to full-dimensional\nbenchmarks while significantly reducing computational and training overhead,\nand provide insight into tradeoffs between beamspace window size and FFT\nresolution in balancing complexity, detection accuracy, and interference\nsuppression.", "AI": {"tldr": "\u57fa\u4e8e\u7a7a\u95f4FFT\u7684\u7a97\u5316\u675f\u7a7a\u95f4MVDR\u6750\u5f62\u5904\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u6743\u8861\u7a97\u53e3\u5927\u5c0f\u548cFFT\u5206\u8fa8\u7387\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd", "motivation": "\u4f20\u7edf\u7a7a\u95f4\u5904\u7406\u5728\u5927\u89c4\u6a21MIMO\u96f7\u8fbe\u6570\u7ec4\u4e2d\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff08MVDR\u6750\u5f62\u5904\u7406\u590d\u6742\u5ea6\u4e3aO(N^3)\uff09\uff0c\u9700\u8981\u627e\u5230\u9ad8\u6548\u7684\u590d\u6742\u5ea6\u964d\u4f4e\u65b9\u6848", "method": "\u5229\u7528\u6750\u5f62\u7a7a\u95f4\u80fd\u91cf\u805a\u96c6\u7279\u6027\uff0c\u901a\u8fc7\u7a7a\u95f4FFT\u8f6c\u6362\u5230\u6750\u5f62\u7a7a\u95f4\uff08\u590d\u6742\u5ea6O(NlogN)\uff09\uff0c\u8bbe\u8ba1\u7a97\u5316\u6750\u5f62\u7a7a\u95f4MVDR\u6750\u5f62\u5904\u7406\u67b6\u6784\uff0c\u5e76\u5728\u76ee\u6807\u5484\u5b50\u5e26\u4e0a\u5e76\u884c\u5316\u5904\u7406", "result": "\u5728DARPA SOAP\u7a0b\u5e8f\u7684\u5e7f\u5e26\u96f7\u8fbe\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u8bad\u7ec3\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5168\u7ef4\u5ea6\u57fa\u51c6\u76f8\u53ef\u6bd4\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u6709\u6548\u6291\u5236\u5e72\u6270", "conclusion": "\u6750\u5f62\u7a7a\u95f4\u5904\u7406\u4e3a\u5927\u89c4\u6a21MIMO\u96f7\u8fbe\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u590d\u6742\u5ea6\u964d\u4f4e\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6743\u8861\u7a97\u53e3\u5927\u5c0f\u5484FFT\u5206\u8fa8\u7387\u53ef\u4ee5\u5728\u590d\u6742\u5ea6\u3001\u68c0\u6d4b\u51c6\u786e\u6027\u5484\u5e72\u6270\u6291\u5236\u4e4b\u95f4\u8fbe\u5230\u6700\u4f18\u5e73\u8861"}}
{"id": "2508.12776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.12776", "abs": "https://arxiv.org/abs/2508.12776", "authors": ["Muhammad Rajabinasab", "Farhad Pakdaman", "Moncef Gabbouj", "Peter Schneider-Kamp", "Arthur Zimek"], "title": "Randomized PCA Forest for Outlier Detection", "comment": null, "summary": "We propose a novel unsupervised outlier detection method based on Randomized\nPrincipal Component Analysis (PCA). Inspired by the performance of Randomized\nPCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a\nnovel unsupervised outlier detection method that utilizes RPCA Forest for\noutlier detection. Experimental results showcase the superiority of the\nproposed approach compared to the classical and state-of-the-art methods in\nperforming the outlier detection task on several datasets while performing\ncompetitively on the rest. The extensive analysis of the proposed method\nreflects it high generalization power and its computational efficiency,\nhighlighting it as a good choice for unsupervised outlier detection.", "AI": {"tldr": "\u57fa\u4e8e\u968f\u673a\u5316PCA\u68ee\u7684\u65b0\u9898\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u4f20\u7edf\u548c\u6700\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387", "motivation": "\u53d7\u968f\u673a\u5316PCA\u68ee\u5728\u8fd1\u4f3cK\u8fd1\u90bb\u641c\u7d22\u4e2d\u8868\u73b0\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5", "method": "\u5229\u7528\u968f\u673a\u5316PCA\u68ee\uff08RPCA Forest\uff09\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u4f20\u7edf\u548c\u6700\u65b0\u65b9\u6cd5\uff0c\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u4e5f\u5177\u6709\u7ade\u4e89\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u662f\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7684\u826f\u597d\u9009\u62e9"}}
{"id": "2508.11923", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11923", "abs": "https://arxiv.org/abs/2508.11923", "authors": ["Yan Wu", "Lihong Pei", "Yukai Han", "Yang Cao", "Yu Kang", "Yanlong Zhao"], "title": "Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting", "comment": null, "summary": "Long-term traffic emission forecasting is crucial for the comprehensive\nmanagement of urban air pollution. Traditional forecasting methods typically\nconstruct spatiotemporal graph models by mining spatiotemporal dependencies to\npredict emissions. However, due to the multi-scale entanglement of traffic\nemissions across time and space, these spatiotemporal graph modeling method\ntend to suffer from cascading error amplification during long-term inference.\nTo address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling\n(SDSTM) framework for long-term traffic emission forecasting. It leverages the\npredictability differences across multiple scales to decompose and fuse\nfeatures at different scales, while constraining them to remain independent yet\ncomplementary. Specifically, the model first introduces a dual-stream feature\ndecomposition strategy based on the Koopman lifting operator. It lifts the\nscale-coupled spatiotemporal dynamical system into an infinite-dimensional\nlinear space via Koopman operator, and delineates the predictability boundary\nusing gated wavelet decomposition. Then a novel fusion mechanism is\nconstructed, incorporating a dual-stream independence constraint based on\ncross-term loss to dynamically refine the dual-stream prediction results,\nsuppress mutual interference, and enhance the accuracy of long-term traffic\nemission prediction. Extensive experiments conducted on a road-level traffic\nemission dataset within Xi'an's Second Ring Road demonstrate that the proposed\nmodel achieves state-of-the-art performance.", "AI": {"tldr": "\u901a\u8fc7\u89e3\u8026\u591a\u5c3a\u5ea6\u65f6\u7a7a\u7279\u5f81\u7684SDSTM\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u957f\u671f\u4ea4\u901a\u6392\u653e\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u7684\u7ea7\u8054\u9519\u8bef\u653e\u5927\u95ee\u9898", "motivation": "\u4f20\u7edf\u65f6\u7a7a\u56fe\u6a21\u578b\u5728\u957f\u671f\u4ea4\u901a\u6392\u653e\u9884\u6d4b\u4e2d\u5b58\u5728\u591a\u5c3a\u5ea6\u65f6\u7a7a\u7279\u5f81\u7f20\u7edc\u548c\u7ea7\u8054\u9519\u8bef\u653e\u5927\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027", "method": "\u63d0\u51fa\u5c3a\u5ea6\u89e3\u8026\u65f6\u7a7a\u5efa\u6a21(SDSTM)\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eKoopman\u63d0\u5347\u7b97\u5b50\u7684\u53cc\u6d41\u7279\u5f81\u5206\u89e3\u7b56\u7565\uff1b2\uff09\u901a\u8fc7\u95e8\u63a7\u5c0f\u6ce2\u5206\u89e3\u5212\u5b9a\u9884\u6d4b\u6027\u8fb9\u754c\uff1b3\uff09\u6784\u5efa\u5305\u542b\u53cc\u6d41\u72ec\u7acb\u7ea6\u675f\u7684\u878d\u5408\u673a\u5236", "result": "\u5728\u897f\u5b89\u4e8c\u73af\u8def\u8def\u7ea7\u4ea4\u901a\u6392\u653e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "SDSTM\u6846\u67b6\u901a\u8fc7\u6709\u6548\u89e3\u8026\u591a\u5c3a\u5ea6\u65f6\u7a7a\u7279\u5f81\u5e76\u4f7f\u5176\u4fdd\u6301\u72ec\u7acb\u4f46\u4e92\u8865\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u4ea4\u901a\u6392\u653e\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027"}}
{"id": "2508.11792", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11792", "abs": "https://arxiv.org/abs/2508.11792", "authors": ["Daniel Sch\u00e4ufele", "Jochen Fink", "Renato L. G. Cavalcante", "S\u0142awomir Sta\u0144czak"], "title": "Digital Post-Distortion Architectures for Nonlinear Power Amplifiers: Volterra and Kernel Methods", "comment": null, "summary": "In modern 5G user equipments (UEs), the power amplifier (PA) contributes\nsignificantly to power consumption during uplink transmissions, especially in\ncell-edge scenarios. While reducing power backoff can enhance PA efficiency, it\nintroduces nonlinear distortions that degrade signal quality. Existing\nsolutions, such as digital pre-distortion, require complex feedback mechanisms\nfor optimal performance, leading to increased UE complexity and power\nconsumption. Instead, in this study we explore digital post-distortion (DPoD)\ntechniques, which compensate for these distortions at the base station,\nleveraging its superior computational resources. In this study, we conduct an\ncomprehensive study concerning the challenges and advantages associated with\napplying DPoD in time-domain, frequency-domain, and DFT-s-domain. Our findings\nsuggest that implementing DPoD in the time-domain, complemented by\nfrequency-domain channel equalization, strikes a good balance between low\ncomputational complexity and efficient nonlinearity compensation. In addition,\nwe demonstrate that memory has to be taken into account regardless of the\nmemory of the PA. Subsequently, we show how to pose the complex-valued problem\nof nonlinearity compensation in a real Hilbert space, emphasizing the potential\nperformance enhancements as a result. We then discuss the traditional Volterra\nseries and show an equivalent kernel method that can reduce algorithmic\ncomplexity. Simulations validate the results of our analysis and show that our\nproposed algorithm can significantly improve performance compared to\nstate-of-the-art algorithms.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5728\u57fa\u7ad9\u7aef\u5b9e\u65bd\u6570\u5b57\u540e\u626d\u66f2(DPoD)\u6280\u672f\u6765\u8865\u507f5G\u7528\u6237\u8bbe\u5907\u529f\u653e\u7684\u975e\u7ebf\u6027\u626d\u66f2\uff0c\u5728\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b35G\u7528\u6237\u8bbe\u5907\u529f\u653e\u5728\u51cf\u5c11\u529f\u8017\u65f6\u5bfc\u81f4\u7684\u975e\u7ebf\u6027\u626d\u66f2\u95ee\u9898\uff0c\u907f\u514d\u590d\u6742\u7684\u53cd\u9988\u673a\u5236\u548c\u589e\u52a0\u7684\u529f\u8017\u3002", "method": "\u5728\u65f6\u57df\u3001\u9891\u57df\u548cDFT-s\u57df\u5bf9\u6bd4\u7814\u7a76DPoD\u6280\u672f\uff0c\u63d0\u51fa\u5728\u65f6\u57df\u5b9e\u65bdDPoD\u5e76\u914d\u5408\u9891\u57df\u901a\u9053\u5747\u8861\u7684\u65b9\u6848\uff0c\u4f7f\u7528\u5b9e\u5e0c\u5c14\u4f2f\u7a7a\u95f4\u91cd\u6784\u95ee\u9898\uff0c\u4ee5\u53ca\u57fa\u4e8eVolterra\u7ea7\u6570\u7684\u6838\u65b9\u6cd5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0b\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u7b97\u6cd5\u3002", "conclusion": "\u5728\u57fa\u7ad9\u7aef\u5b9e\u65bdDPoD\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65f6\u57dfDPoD\u914d\u5408\u9891\u57df\u5747\u8861\u80fd\u5728\u590d\u6742\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2508.12792", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.12792", "abs": "https://arxiv.org/abs/2508.12792", "authors": ["Felipe Maia Polo", "Xinhe Wang", "Mikhail Yurochkin", "Gongjun Xu", "Moulinath Banerjee", "Yuekai Sun"], "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap", "comment": null, "summary": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.", "AI": {"tldr": "Bridge\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5efa\u6a21\u4eba\u7c7b\u4e0eLLM\u8bc4\u4f30\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u63d0\u9ad8LLM\u8bc4\u5206\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027", "motivation": "LLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u65f6\uff0c\u5176\u8bc4\u5206\u4e0e\u4eba\u7c7b\u5224\u65ad\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u8ba1\u6846\u67b6\u6765\u5f25\u5408\u8fd9\u79cd\u5dee\u8ddd", "method": "\u63d0\u51faBridge\u6846\u67b6\uff0c\u5047\u8bbe\u6bcf\u4e2a\u63d0\u793a-\u54cd\u5e94\u5bf9\u5b58\u5728\u6f5c\u5728\u4eba\u7c7b\u504f\u597d\u5206\u6570\uff0c\u5c06LLM\u504f\u5dee\u5efa\u6a21\u4e3a\u534f\u53d8\u91cf\u7684\u7ebf\u6027\u53d8\u6362\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u7684\u62df\u5408\u7b97\u6cd5", "result": "\u57286\u4e2aLLM\u8bc4\u4f30\u8005\u548c\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cBridge\u5728\u51c6\u786e\u6027\u3001\u6821\u51c6\u548cKL\u6563\u5ea6\u65b9\u9762\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8fbe\u6210\u66f4\u9ad8\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u4eba\u673a\u5dee\u8ddd", "conclusion": "Bridge\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u539f\u5219\u7684\u6846\u67b6\u6765\u6539\u8fdbLLM\u8bc4\u5206\u5e76\u91cf\u5316\u4eba\u7c7b\u4e0eLLM\u8bc4\u4f30\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02"}}
{"id": "2508.11931", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11931", "abs": "https://arxiv.org/abs/2508.11931", "authors": ["Tim van Erven", "Jack Mayo", "Julia Olkhovskaya", "Chen-Yu Wei"], "title": "An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction", "comment": null, "summary": "We present an efficient algorithm for linear contextual bandits with\nadversarial losses and stochastic action sets. Our approach reduces this\nsetting to misspecification-robust adversarial linear bandits with fixed action\nsets. Without knowledge of the context distribution or access to a context\nsimulator, the algorithm achieves $\\tilde{O}(\\min\\{d^2\\sqrt{T}, \\sqrt{d^3T\\log\nK}\\})$ regret and runs in $\\text{poly}(d,C,T)$ time, where $d$ is the feature\ndimension, $C$ is an upper bound on the number of linear constraints defining\nthe action set in each round, $K$ is an upper bound on the number of actions in\neach round, and $T$ is number of rounds. This resolves the open question by Liu\net al. (2023) on whether one can obtain $\\text{poly}(d)\\sqrt{T}$ regret in\npolynomial time independent of the number of actions. For the important class\nof combinatorial bandits with adversarial losses and stochastic action sets\nwhere the action sets can be described by a polynomial number of linear\nconstraints, our algorithm is the first to achieve $\\text{poly}(d)\\sqrt{T}$\nregret in polynomial time, while no prior algorithm achieves even $o(T)$ regret\nin polynomial time to our knowledge. When a simulator is available, the regret\nbound can be improved to $\\tilde{O}(d\\sqrt{L^\\star})$, where $L^\\star$ is the\ncumulative loss of the best policy.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.12012", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12012", "abs": "https://arxiv.org/abs/2508.12012", "authors": ["Yi Wang", "Yingyang Chen", "Li Wang", "Donghong Cai", "Xiaofan Li", "Pingzhi Fan"], "title": "Autonomous Driving with RSMA-Enabled Finite Blocklength Transmissions: Ergodic Performance Analysis and Optimization", "comment": "This work has been accepted by IEEE Transactions on Wireless\n  Communications", "summary": "Rate-splitting multiple access (RSMA) is a key technology for next-generation\nmultiple access systems due to its robustness against imperfect channel state\ninformation (CSI). This makes RSMA particularly suitable for high-mobility\nautonomous driving, where ultra-reliable and low-latency communication (URLLC)\nis essential. To address the stringent requirements, this study enables RSMA\nfinite blocklength (FBL) transmissions and explicitly evaluates the ergodic\nperformance. We derive the closed-form lower bound for the ergodic sum-rate of\nRSMA, considering vital factors such as the vehicle velocities, vehicle\npositions, power allocation of each stream, blocklengths, and block error rates\n(BLERs). To further enhance the ergodic sum-rate while complying with quality\nof service (QoS) rate constraints, we jointly optimize the global power\ncoefficient, private power distribution, and common rate splitting. Guided by\ngradient descent, we first adjust the global power coefficient based on its\nsum-rate solution. This parameter regulates the power state of the common\nstream, allowing for dynamic activation or deactivation: if active, we optimize\nthe private power distribution and adjust the common rate splitting to meet\nminimum transmission constraints; if inactive, we use the sequential quadratic\nprogramming for private power distribution optimization. Simulation results\nconfirm that our RSMA scheme significantly improves the ergodic performance,\nreduces blocklength and BLER, surpassing the RSMA counterpart with average\nprivate power and space division multiple access (SDMA). Furthermore, our\napproach is validated to guarantee the rates for users with the poorest channel\nconditions, thereby enhancing fairness across the network.", "AI": {"tldr": "RSMA\u5728\u6709\u9650\u5757\u957f\u5ea6\u4f20\u8f93\u4e0b\u7684\u6027\u80fd\u5206\u6790\u4e0e\u4f18\u5316\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u901f\u7387\u5206\u5272\uff0c\u663e\u8457\u63d0\u5347\u548c\u901f\u7387\u6027\u80fd\u5e76\u4fdd\u8bc1\u7528\u6237\u516c\u5e73\u6027", "motivation": "\u9488\u5bf9\u9ad8\u79fb\u52a8\u6027\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2dURLLC\u7684\u4e25\u683c\u9700\u6c42\uff0c\u9700\u8981\u89e3\u51b3RSMA\u5728\u6709\u9650\u5757\u957f\u5ea6\u4f20\u8f93\u4e0b\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u4f18\u5316\u95ee\u9898", "method": "\u63a8\u5bfcRSMA\u904d\u5386\u548c\u901f\u7387\u7684\u95ed\u5f0f\u4e0b\u754c\uff0c\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u8054\u5408\u4f18\u5316\u5168\u5c40\u529f\u7387\u7cfb\u6570\u3001\u79c1\u6709\u529f\u7387\u5206\u914d\u548c\u516c\u5171\u901f\u7387\u5206\u5272\uff0c\u91c7\u7528\u5e8f\u5217\u4e8c\u6b21\u89c4\u5212\u8fdb\u884c\u4f18\u5316", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5RSMA\u65b9\u6848\u663e\u8457\u63d0\u5347\u904d\u5386\u6027\u80fd\uff0c\u964d\u4f4e\u5757\u957f\u5ea6\u548c\u8bef\u5757\u7387\uff0c\u4f18\u4e8e\u5e73\u5747\u79c1\u6709\u529f\u7387\u7684RSMA\u548cSDMA\uff0c\u4fdd\u8bc1\u6700\u5dee\u4fe1\u9053\u6761\u4ef6\u7528\u6237\u7684\u901f\u7387", "conclusion": "\u6240\u63d0\u51fa\u7684RSMA FBL\u4f20\u8f93\u65b9\u6848\u80fd\u591f\u6709\u6548\u6ee1\u8db3\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u7684URLLC\u9700\u6c42\uff0c\u5728\u6027\u80fd\u548c\u516c\u5e73\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272"}}
{"id": "2508.12997", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.12997", "abs": "https://arxiv.org/abs/2508.12997", "authors": ["Haishun Chen", "Cai Xu", "Jinlong Yu", "Yilin Zhang", "Ziyu Guan", "Wei Zhao"], "title": "Fairness-Aware Multi-view Evidential Learning with Adaptive Prior", "comment": null, "summary": "Multi-view evidential learning aims to integrate information from multiple\nviews to improve prediction performance and provide trustworthy uncertainty\nesitimation. Most previous methods assume that view-specific evidence learning\nis naturally reliable. However, in practice, the evidence learning process\ntends to be biased. Through empirical analysis on real-world data, we reveal\nthat samples tend to be assigned more evidence to support data-rich classes,\nthereby leading to unreliable uncertainty estimation in predictions. This\nmotivates us to delve into a new Biased Evidential Multi-view Learning (BEML)\nproblem. To this end, we propose Fairness-Aware Multi-view Evidential Learning\n(FAML). FAML first introduces an adaptive prior based on training trajectory,\nwhich acts as a regularization strategy to flexibly calibrate the biased\nevidence learning process. Furthermore, we explicitly incorporate a fairness\nconstraint based on class-wise evidence variance to promote balanced evidence\nallocation. In the multi-view fusion stage, we propose an opinion alignment\nmechanism to mitigate view-specific bias across views, thereby encouraging the\nintegration of consistent and mutually supportive evidence. Extensive\nexperiments on five real-world multi-view datasets demonstrate that FAML\nachieves more balanced evidence allocation and improves both prediction\nperformance and the reliability of uncertainty estimation compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faFAML\u65b9\u6cd5\u89e3\u51b3\u591a\u89c6\u56fe\u8bc1\u636e\u5b66\u4e60\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5148\u9a8c\u3001\u516c\u5e73\u6027\u7ea6\u675f\u548c\u610f\u89c1\u5bf9\u9f50\u673a\u5236\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u8bc1\u636e\u5206\u914d\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u591a\u89c6\u56fe\u8bc1\u636e\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u89c6\u56fe\u7279\u5b9a\u8bc1\u636e\u5b66\u4e60\u662f\u53ef\u9760\u7684\uff0c\u4f46\u5b9e\u8df5\u4e2d\u8bc1\u636e\u5b66\u4e60\u8fc7\u7a0b\u5b58\u5728\u504f\u89c1\uff0c\u6837\u672c\u503e\u5411\u4e8e\u4e3a\u6570\u636e\u4e30\u5bcc\u7684\u7c7b\u522b\u5206\u914d\u66f4\u591a\u8bc1\u636e\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u53ef\u9760", "method": "FAML\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u8bad\u7ec3\u8f68\u8ff9\u7684\u81ea\u9002\u5e94\u5148\u9a8c\u4f5c\u4e3a\u6b63\u5219\u5316\u7b56\u7565\u6821\u51c6\u504f\u89c1\u8bc1\u636e\u5b66\u4e60\uff1b2\uff09\u57fa\u4e8e\u7c7b\u95f4\u8bc1\u636e\u65b9\u5dee\u7684\u516c\u5e73\u6027\u7ea6\u675f\u4fc3\u8fdb\u5e73\u8861\u8bc1\u636e\u5206\u914d\uff1b3\uff09\u591a\u89c6\u56fe\u878d\u5408\u9636\u6bb5\u7684\u610f\u89c1\u5bf9\u9f50\u673a\u5236\u7f13\u89e3\u89c6\u56fe\u95f4\u504f\u89c1", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u591a\u89c6\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFAML\u5b9e\u73b0\u4e86\u66f4\u5e73\u8861\u7684\u8bc1\u636e\u5206\u914d\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u9760\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "FAML\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u8bc1\u636e\u5b66\u4e60\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u504f\u89c1\u6821\u51c6\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u65b9\u6cd5\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2508.11936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11936", "abs": "https://arxiv.org/abs/2508.11936", "authors": ["Yuehan Qin", "Li Li", "Defu Cao", "Tiankai Yang", "Yue Zhao"], "title": "M3OOD: Automatic Selection of Multimodal OOD Detectors", "comment": null, "summary": "Out-of-distribution (OOD) robustness is a critical challenge for modern\nmachine learning systems, particularly as they increasingly operate in\nmultimodal settings involving inputs like video, audio, and sensor data.\nCurrently, many OOD detection methods have been proposed, each with different\ndesigns targeting various distribution shifts. A single OOD detector may not\nprevail across all the scenarios; therefore, how can we automatically select an\nideal OOD detection model for different distribution shifts? Due to the\ninherent unsupervised nature of the OOD detection task, it is difficult to\npredict model performance and find a universally Best model. Also,\nsystematically comparing models on the new unseen data is costly or even\nimpractical. To address this challenge, we introduce M3OOD, a\nmeta-learning-based framework for OOD detector selection in multimodal\nsettings. Meta learning offers a solution by learning from historical model\nbehaviors, enabling rapid adaptation to new data distribution shifts with\nminimal supervision. Our approach combines multimodal embeddings with\nhandcrafted meta-features that capture distributional and cross-modal\ncharacteristics to represent datasets. By leveraging historical performance\nacross diverse multimodal benchmarks, M3OOD can recommend suitable detectors\nfor a new data distribution shift. Experimental evaluation demonstrates that\nM3OOD consistently outperforms 10 competitive baselines across 12 test\nscenarios with minimal computational overhead.", "AI": {"tldr": "M3OOD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u591a\u6a21\u6001OOD\u68c0\u6d4b\u5668\u9009\u62e9\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u52a8\u4e3a\u4e0d\u540c\u5206\u5e03\u504f\u79fb\u63a8\u8350\u5408\u9002\u7684\u68c0\u6d4b\u5668\uff0c\u572812\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\u4f18\u4e8e10\u4e2a\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u73af\u5883\u4e0bOOD\u68c0\u6d4b\u5668\u9009\u62e9\u96be\u9898\uff0c\u7531\u4e8eOOD\u68c0\u6d4b\u7684\u65e0\u76d1\u7763\u7279\u6027\uff0c\u96be\u4ee5\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u4e14\u7cfb\u7edf\u6bd4\u8f83\u6210\u672c\u9ad8\u6602", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u5d4c\u5165\u548c\u624b\u5de5\u8bbe\u8ba1\u7684\u5143\u7279\u5f81\u6765\u8868\u793a\u6570\u636e\u96c6\uff0c\u5229\u7528\u5386\u53f2\u6027\u80fd\u6570\u636e\u901a\u8fc7\u5143\u5b66\u4e60\u63a8\u8350\u9002\u5408\u65b0\u5206\u5e03\u504f\u79fb\u7684\u68c0\u6d4b\u5668", "result": "\u572812\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\u4e00\u81f4\u4f18\u4e8e10\u4e2a\u7ade\u4e89\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f", "conclusion": "M3OOD\u6846\u67b6\u4e3a\u591a\u6a21\u6001OOD\u68c0\u6d4b\u5668\u9009\u62e9\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd"}}
{"id": "2508.12099", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12099", "abs": "https://arxiv.org/abs/2508.12099", "authors": ["Guangpu Guo", "Xiang-Gen Xia"], "title": "A Generalized Multidimensional Chinese Remainder Theorem (MD-CRT) for Multiple Integer Vectors", "comment": null, "summary": "Chinese remainder theorem (CRT) is widely applied in cryptography, coding\ntheory, and signal processing. It has been extended to the multidimensional CRT\n(MD-CRT), which reconstructs an integer vector from its vector remainders\nmodulo multiple integer matrices. This paper investigates a generalized MD-CRT\nfor multiple integer vectors, where the goal is to determine multiple integer\nvectors from multiple vector residue sets modulo multiple integer\nmatrices.Comparing to the existing generalized CRT for multiple scalar\nintegers, the challenge is that the moduli in MD-CRT are matrices that do not\ncommute and the corresponding uniquely determinable range is multidimensional\nand the inclusion relationship is much more complicated. In this paper,we\naddress two fundamental questions regarding the generalized MD-CRT. The first\nquestion concerns the uniquely determinable range of multiple integer vectors\nwhen no prior information about them is available. The second question is about\nthe conditions under which the maximal possible dynamic range can be\nachieved.To answer these two questions, we first derive a uniquely determinable\nrange without prior information and accordingly propose an algorithm to achieve\nit. A special case involving only two integer vectors is investigated for the\nsecond question, leading to a new condition for achieving the maximal possible\ndynamic range. Interestingly, this newly obtained condition, when the dimension\nis reduced to $1$, is even better than the existing ones for the conventional\ngeneralized CRT for scalar integers.These results may have applications for\nfrequency detection in multidimensional signal processing.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u591a\u7ef4\u4e2d\u56fd\u5269\u4f59\u5b9a\u7406\u7684\u5e7f\u4e49\u5f62\u5f0f\uff0c\u91cd\u70b9\u89e3\u51b3\u591a\u4e2a\u6574\u6570\u5411\u91cf\u4ece\u77e9\u9635\u6a21\u7684\u5411\u91cf\u4f59\u6570\u6062\u590d\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u552f\u4e00\u53ef\u5b9a\u8303\u56f4\u548c\u5b9e\u73b0\u6700\u5927\u52a8\u6001\u8303\u56f4\u7684\u6761\u4ef6\u3002", "motivation": "\u591a\u7ef4CRT\u5728\u52a0\u5bc6\u3001\u7f16\u7801\u548c\u4fe1\u53f7\u5904\u7406\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u5411\u91cf\u6062\u590d\u3002\u9700\u8981\u6269\u5c55\u5230\u591a\u4e2a\u6574\u6570\u5411\u91cf\u7684\u60c5\u51b5\uff0c\u800c\u77e9\u9635\u6a21\u7684\u975e\u4ea4\u6362\u6027\u548c\u591a\u7ef4\u8303\u56f4\u7684\u590d\u6742\u6027\u4f7f\u5f97\u8fd9\u4e2a\u95ee\u9898\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u5728\u6ca1\u6709\u5148\u9a8c\u4fe1\u606f\u60c5\u51b5\u4e0b\u7684\u552f\u4e00\u53ef\u5b9a\u8303\u56f4\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u73b0\u8be5\u8303\u56f4\u7684\u7b97\u6cd5\u3002\u7136\u540e\u91cd\u70b9\u7814\u7a76\u4e86\u4ec5\u5305\u542b\u4e24\u4e2a\u6574\u6570\u5411\u91cf\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u63a2\u8ba8\u5b9e\u73b0\u6700\u5927\u52a8\u6001\u8303\u56f4\u7684\u6761\u4ef6\u3002", "result": "\u5f97\u5230\u4e86\u65b0\u7684\u6761\u4ef6\u6765\u5b9e\u73b0\u6700\u5927\u53ef\u80fd\u52a8\u6001\u8303\u56f4\uff0c\u5f53\u7ef4\u6570\u964d\u4f4e\u52301\u65f6\uff0c\u8fd9\u4e2a\u65b0\u6761\u4ef6\u751a\u81f3\u6bd4\u73b0\u6709\u7684\u6807\u91cf\u6574\u6570\u5e7f\u4e49CRT\u6761\u4ef6\u66f4\u597d\u3002", "conclusion": "\u672c\u6587\u7ed3\u679c\u4e3a\u591a\u7ef4CRT\u7684\u5e7f\u4e49\u5f62\u5f0f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5bf9\u4e8e\u591a\u7ef4\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u9891\u7387\u68c0\u6d4b\u7b49\u5e94\u7528\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2508.13100", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.13100", "abs": "https://arxiv.org/abs/2508.13100", "authors": ["Jason Hartline", "Lunjia Hu", "Yifan Wu"], "title": "A Perfectly Truthful Calibration Measure", "comment": null, "summary": "Calibration requires that predictions are conditionally unbiased and,\ntherefore, reliably interpretable as probabilities. Calibration measures\nquantify how far a predictor is from perfect calibration. As introduced by\nHaghtalab et al. (2024), a calibration measure is truthful if it is minimized\nin expectation when a predictor outputs the ground-truth probabilities.\nAlthough predicting the true probabilities guarantees perfect calibration, in\nreality, when calibration is evaluated on a finite sample, predicting the truth\nis not guaranteed to minimize any known calibration measure. All known\ncalibration measures incentivize predictors to lie in order to appear more\ncalibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et\nal. (2024) and Qiao and Zhao (2025) to construct approximately truthful\ncalibration measures in the sequential prediction setting, but no perfectly\ntruthful calibration measure was known to exist even in the more basic batch\nsetting.\n  We design a perfectly truthful calibration measure in the batch setting:\naveraged two-bin calibration error (ATB). In addition to being truthful, ATB is\nsound, complete, continuous, and quadratically related to two existing\ncalibration measures: the smooth calibration error (smCal) and the (lower)\ndistance to calibration (distCal). The simplicity in our definition of ATB\nmakes it efficient and straightforward to compute. ATB allows faster estimation\nalgorithms with significantly easier implementations than smCal and distCal,\nachieving improved running time and simplicity for the calibration testing\nproblem studied by Hu et al. (2024). We also introduce a general recipe for\nconstructing truthful measures, which proves the truthfulness of ATB as a\nspecial case and allows us to construct other truthful calibration measures\nsuch as quantile-binned l_2-ECE.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u7f8e\u771f\u5b9e\u7684\u6279\u5904\u7406\u8bbe\u5b9a\u4e2d\u7684\u68c0\u67e5\u6d4b\u91cf\u65b9\u6cd5\uff08ATB\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u67e5\u6d4b\u91cf\u5728\u6709\u9650\u6837\u672c\u4e0a\u4e0d\u771f\u5b9e\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u67e5\u6d4b\u91cf\u5728\u6709\u9650\u6837\u672c\u4e0a\u5b58\u5728\u4e0d\u771f\u5b9e\u6027\uff0c\u9884\u6d4b\u5668\u4f1a\u4e3a\u4e86\u5728\u6837\u672c\u4e0a\u663e\u5f97\u66f4\u68c0\u67e5\u800c\u8bf7\u8c0b\u3002\u867d\u7136\u6709\u4e9b\u7ea6\u771f\u5b9e\u68c0\u67e5\u6d4b\u91cf\uff0c\u4f46\u5728\u6279\u5904\u7406\u8bbe\u5b9a\u4e2d\u4ecd\u7f3a\u4e4f\u5b8c\u7f8e\u771f\u5b9e\u7684\u68c0\u67e5\u6d4b\u91cf\u3002", "method": "\u8bbe\u8ba1\u4e86\u5e73\u5747\u53cc\u7bb1\u68c0\u67e5\u8bef\u5dee\uff08ATB\uff09\u6d4b\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u771f\u5b9e\uff0c\u8fd8\u5177\u6709\u58f0\u97f3\u6027\u3001\u5b8c\u6574\u6027\u3001\u8fde\u7eed\u6027\u7b49\u7279\u6027\u3002ATB\u4e0e\u73b0\u6709\u7684\u5149\u6ed1\u68c0\u67e5\u8bef\u5dee\u548c\u8ddd\u79bb\u68c0\u67e5\u6d4b\u91cf\u5b58\u5728\u4e8c\u6b21\u5173\u7cfb\u3002", "result": "ATB\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u73b0\u7b80\u5355\u6027\u65b9\u9762\u90fd\u66f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u68c0\u67e5\u6d4b\u8bd5\u95ee\u9898\u63d0\u4f9b\u66f4\u5feb\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u66f4\u7b80\u5355\u7684\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u5b8c\u7f8e\u771f\u5b9e\u7684\u68c0\u67e5\u6d4b\u91cfATB\uff0c\u89e3\u51b3\u4e86\u68c0\u67e5\u6d4b\u91cf\u9884\u6d4b\u5668\u5728\u6709\u9650\u6837\u672c\u4e0a\u8bf7\u8c0b\u7684\u95ee\u9898\u3002\u8fd8\u63d0\u4f9b\u4e86\u6784\u5efa\u771f\u5b9e\u6d4b\u91cf\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u53ef\u4ee5\u751f\u6210\u5176\u4ed6\u771f\u5b9e\u68c0\u67e5\u6d4b\u91cf\u3002"}}
{"id": "2508.11940", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.11940", "abs": "https://arxiv.org/abs/2508.11940", "authors": ["Yuannuo Feng", "Wenyong Zhou", "Yuexi Lyu", "Yixiang Zhang", "Zhengwu Liu", "Ngai Wong", "Wang Kang"], "title": "Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware", "comment": "4 pages, 5 figures, conference", "summary": "Analog Compute-In-Memory (CIM) architectures promise significant energy\nefficiency gains for neural network inference, but suffer from complex\nhardware-induced noise that poses major challenges for deployment. While\nnoise-aware training methods have been proposed to address this issue, they\ntypically rely on idealized and differentiable noise models that fail to\ncapture the full complexity of analog CIM hardware variations. Motivated by the\nStraight-Through Estimator (STE) framework in quantization, we decouple forward\nnoise simulation from backward gradient computation, enabling noise-aware\ntraining with more accurate but computationally intractable noise modeling in\nanalog CIM systems. We provide theoretical analysis demonstrating that our\napproach preserves essential gradient directional information while maintaining\ncomputational tractability and optimization stability. Extensive experiments\nshow that our extended STE framework achieves up to 5.3% accuracy improvement\non image classification, 0.72 perplexity reduction on text generation,\n2.2$\\times$ speedup in training time, and 37.9% lower peak memory usage\ncompared to standard noise-aware training methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u901a\u4f30\u8ba1\u5668(STE)\u6846\u67b6\u7684\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u62df\u5b58\u5185\u8ba1\u7b97(CIM)\u67b6\u6784\u4e2d\u7684\u786c\u4ef6\u566a\u58f0\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u524d\u5411\u566a\u58f0\u6a21\u62df\u548c\u540e\u5411\u68af\u5ea6\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u4e14\u8ba1\u7b97\u53ef\u884c\u7684\u566a\u58f0\u5efa\u6a21\u3002", "motivation": "\u6a21\u62df\u5b58\u5185\u8ba1\u7b97\u67b6\u6784\u867d\u7136\u80fd\u663e\u8457\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u80fd\u6548\uff0c\u4f46\u9762\u4e34\u590d\u6742\u7684\u786c\u4ef6\u566a\u58f0\u6311\u6218\u3002\u73b0\u6709\u7684\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u7406\u60f3\u5316\u4e14\u53ef\u5fae\u5206\u7684\u566a\u58f0\u6a21\u578b\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u6a21\u62dfCIM\u786c\u4ef6\u7684\u771f\u5b9e\u53d8\u5316\u590d\u6742\u6027\u3002", "method": "\u501f\u9274\u91cf\u5316\u4e2d\u7684\u76f4\u901a\u4f30\u8ba1\u5668(STE)\u6846\u67b6\uff0c\u5c06\u524d\u5411\u566a\u58f0\u6a21\u62df\u4e0e\u540e\u5411\u68af\u5ea6\u8ba1\u7b97\u89e3\u8026\uff0c\u4f7f\u5f97\u53ef\u4ee5\u4f7f\u7528\u66f4\u51c6\u786e\u4f46\u8ba1\u7b97\u4e0a\u96be\u4ee5\u5904\u7406\u7684\u566a\u58f0\u6a21\u578b\u8fdb\u884c\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u548c\u4f18\u5316\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e865.3%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u56f0\u60d1\u5ea6\u964d\u4f4e0.72\uff0c\u8bad\u7ec3\u65f6\u95f4\u52a0\u901f2.2\u500d\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e37.9%\uff0c\u76f8\u6bd4\u6807\u51c6\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6269\u5c55STE\u6846\u67b6\u4e3a\u6a21\u62dfCIM\u7cfb\u7edf\u7684\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5904\u7406\u66f4\u590d\u6742\u7684\u786c\u4ef6\u566a\u58f0\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u62dfCIM\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.12106", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12106", "abs": "https://arxiv.org/abs/2508.12106", "authors": ["Hao Chen", "Rui Jin", "Dayuan Tan"], "title": "RFSS: A Comprehensive Multi-Standard RF Signal Source Separation Dataset with Advanced Channel Modeling", "comment": null, "summary": "The rapid evolution of wireless communication systems has created complex\nelectromagnetic environments where multiple cellular standards (2G/3G/4G/5G)\ncoexist, necessitating advanced signal source separation techniques. We present\nRFSS (RF Signal Source Separation), a comprehensive open-source dataset\ncontaining 52,847 realistic multi-standard RF signal samples with complete 3GPP\nstandards compliance. Our framework generates authentic baseband signals for\nGSM, UMTS, LTE, and 5G NR with advanced channel modeling including multipath\nfading, MIMO processing up to 8 by 8 antennas, and realistic interference\nscenarios. Experimental validation demonstrates superior performance of\nCNN-LSTM architectures achieving 26.7 dB SINR improvement in source separation\ntasks, significantly outperforming traditional ICA (15.2 dB) and NMF (18.3 dB)\napproaches. The RFSS dataset enables reproducible research in RF source\nseparation, cognitive radio, and machine learning applications while\nmaintaining complete open-source accessibility", "AI": {"tldr": "RFSS\u662f\u4e00\u4e2a\u5305\u542b52,847\u4e2a\u591a\u6807\u51c6RF\u4fe1\u53f7\u6837\u672c\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u652f\u63012G/3G/4G/5G\u4fe1\u53f7\u5206\u79bb\u7814\u7a76\uff0cCNN-LSTM\u67b6\u6784\u5728\u4fe1\u53f7\u5206\u79bb\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u590d\u6742\u7535\u78c1\u73af\u5883\u4e2d\u591a\u79cd\u8702\u7a9d\u6807\u51c6\u5171\u5b58\uff0c\u9700\u8981\u5148\u8fdb\u7684\u4fe1\u53f7\u6e90\u5206\u79bb\u6280\u672f\u6765\u5904\u7406\u591a\u6807\u51c6RF\u4fe1\u53f7\u3002", "method": "\u5f00\u53d1\u4e86RFSS\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u751f\u6210\u7b26\u54083GPP\u6807\u51c6\u7684GSM\u3001UMTS\u3001LTE\u548c5G NR\u57fa\u5e26\u4fe1\u53f7\uff0c\u5305\u542b\u591a\u5f84\u8870\u843d\u30018\u00d78 MIMO\u5904\u7406\u548c\u771f\u5b9e\u5e72\u6270\u573a\u666f\u7684\u5148\u8fdb\u4fe1\u9053\u5efa\u6a21\u3002", "result": "CNN-LSTM\u67b6\u6784\u5728\u4fe1\u53f7\u6e90\u5206\u79bb\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8626.7 dB\u7684SINR\u6539\u5584\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfICA\uff0815.2 dB\uff09\u548cNMF\uff0818.3 dB\uff09\u65b9\u6cd5\u3002", "conclusion": "RFSS\u6570\u636e\u96c6\u4e3aRF\u4fe1\u53f7\u6e90\u5206\u79bb\u3001\u8ba4\u77e5\u65e0\u7ebf\u7535\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7814\u7a76\u7684\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u5b8c\u5168\u5f00\u6e90\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2508.11943", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11943", "abs": "https://arxiv.org/abs/2508.11943", "authors": ["Sishun Liu", "Ke Deng", "Xiuzhen Zhang", "Yan Wang"], "title": "Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning", "comment": "ECAI 2025 full version", "summary": "Neural network-based Marked Temporal Point Process (MTPP) models have been\nwidely adopted to model event sequences in high-stakes applications, raising\nconcerns about the trustworthiness of outputs from these models. This study\nfocuses on Explanation for MTPP, aiming to identify the minimal and rational\nexplanation, that is, the minimum subset of events in history, based on which\nthe prediction accuracy of MTPP matches that based on full history to a great\nextent and better than that based on the complement of the subset. This study\nfinds that directly defining Explanation for MTPP as counterfactual explanation\nor factual explanation can result in irrational explanations. To address this\nissue, we define Explanation for MTPP as a combination of counterfactual\nexplanation and factual explanation. This study proposes Counterfactual and\nFactual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of\ndeliberately designed techniques. Experiments demonstrate the correctness and\nsuperiority of CFF over baselines regarding explanation quality and processing\nefficiency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CFF\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cd\u4e8b\u5b9e\u548c\u4e8b\u5b9e\u89e3\u91ca\u6765\u4e3a\u6807\u8bb0\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u63d0\u4f9b\u6700\u5c0f\u4e14\u5408\u7406\u7684\u89e3\u91ca\u5b50\u96c6\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u4f7f\u7528\u5355\u4e00\u89e3\u91ca\u65b9\u6cd5\u5bfc\u81f4\u4e0d\u5408\u7406\u7ed3\u679c\u7684\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u6807\u8bb0\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8f93\u51fa\u7684\u53ef\u4fe1\u5ea6\u5b58\u5728\u62c5\u5fe7\u3002\u9700\u8981\u627e\u5230\u5386\u53f2\u4e8b\u4ef6\u7684\u6700\u5c0f\u5b50\u96c6\u4f5c\u4e3a\u5408\u7406\u89e3\u91ca\uff0c\u4f7f\u57fa\u4e8e\u8be5\u5b50\u96c6\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u5b8c\u6574\u5386\u53f2\u76f8\u5f53\u4e14\u4f18\u4e8e\u8865\u96c6\u3002", "method": "\u63d0\u51faCFF\u65b9\u6cd5\uff0c\u5c06MTPP\u89e3\u91ca\u5b9a\u4e49\u4e3a\u53cd\u4e8b\u5b9e\u89e3\u91ca\u548c\u4e8b\u5b9e\u89e3\u91ca\u7684\u7ec4\u5408\uff0c\u91c7\u7528\u4e00\u7cfb\u5217\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6280\u672f\u6765\u89e3\u51b3\u89e3\u91ca\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCFF\u5728\u89e3\u91ca\u8d28\u91cf\u548c\u5904\u7406\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5177\u6709\u6b63\u786e\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u7ed3\u5408\u53cd\u4e8b\u5b9e\u548c\u4e8b\u5b9e\u89e3\u91ca\u7684CFF\u65b9\u6cd5\u80fd\u591f\u4e3aMTPP\u6a21\u578b\u63d0\u4f9b\u66f4\u5408\u7406\u548c\u6709\u6548\u7684\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u5355\u4e00\u89e3\u91ca\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.12114", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12114", "abs": "https://arxiv.org/abs/2508.12114", "authors": ["Mustafa Gusaibat", "Mohammed Hnaish", "Abdelhamid Salem", "Khaled Rabie", "Zubair Md Fadlullah", "Wali Ullah Khan", "Mohamad A. Alawad", "Yazeed Alkhrijah"], "title": "Effect of Phase Shift Errors on the Security of UAV-assisted STAR-RIS IoT Networks", "comment": null, "summary": "Unmanned aerial vehicles (UAV)-mounted simultaneous transmitting and\nreflecting reconfigurable intelligent surface (STAR-RIS) systems can provide\nfull-dimensional coverage and flexible deployment opportunities in future\n6G-enabled IoT networks. However, practical imperfections such as jittering and\nairflow of UAV could affect the phase shift of STAR-RIS, and consequently\ndegrade network security. In this respect, this paper investigates the impact\nof phase shift errors on the secrecy performance of UAV-mounted\nSTAR-RIS-assisted IoT systems. More specifically, we consider a UAV-mounted\nSTAR-RIS-assisted non-orthogonal multiple access (NOMA) system where IoT\ndevices are grouped into two groups: one group on each side of the STAR-RIS.\nThe nodes in each group are considered as potential Malicious nodes for the\nones on the other side. By modeling phase estimation errors using a von Mises\ndistribution, analytical closed-form expressions for the ergodic secrecy rates\nunder imperfect phase adjustment are derived. An optimization problem to\nmaximize the weighted sum secrecy rate (WSSR) by optimizing the UAV placement\nis formulated and is then solved using a linear grid-based algorithm. Monte\nCarlo simulations are provided to validate the analytical derivations. The\nimpact of phase estimation errors on system's secrecy performance is analyzed,\nproviding critical insights for the practical realisation of STAR-RIS\ndeployments for secure UAV-enabled IoT networks.", "AI": {"tldr": "\u7814\u7a76\u65e0\u4eba\u673a\u642d\u8f7dSTAR-RIS\u7cfb\u7edf\u4e2d\u76f8\u4f4d\u504f\u5dee\u5bf9\u7f51\u7edc\u5b89\u5168\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5f02\u6b63\u5206\u5e03\u5efa\u6a21\u5e76\u63a8\u5bfc\u79d8\u5bc6\u901f\u7387\u7684\u5206\u6790\u8868\u8fbe\u5f0f\uff0c\u4f7f\u7528\u7ebf\u6027\u7f51\u683c\u7b97\u6cd5\u4f18\u5316\u65e0\u4eba\u673a\u4f4d\u7f6e\u4ee5\u6700\u5927\u5316\u52a0\u6743\u79d8\u5bc6\u901f\u7387\u3002", "motivation": "\u65e0\u4eba\u673a\u642d\u8f7dSTAR-RIS\u7cfb\u7edf\u57286G\u7269\u8054\u7f51\u4e2d\u63d0\u4f9b\u5168\u65b9\u4f4d\u8986\u76d6\u548c\u7075\u6d3b\u90e8\u7f72\uff0c\u4f46\u5b9e\u9645\u4e2d\u65e0\u4eba\u673a\u7684\u632f\u52a8\u548c\u6c14\u6d41\u7b49\u5b9e\u9645\u7f3a\u9677\u53ef\u80fd\u5f71\u54cdSTAR-RIS\u7684\u76f8\u4f4d\u8c03\u8282\uff0c\u4ece\u800c\u5f71\u54cd\u7f51\u7edc\u5b89\u5168\u6027\u80fd\u3002", "method": "\u91c7\u7528von Mises\u5206\u5e03\u5efa\u6a21\u76f8\u4f4d\u4f30\u8ba1\u8bef\u5dee\uff0c\u63a8\u5bfc\u4e0d\u5b8c\u6574\u76f8\u4f4d\u8c03\u6574\u4e0b\u7684\u9053\u79d8\u5bc6\u901f\u7387\u5206\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u7ebf\u6027\u7f51\u683c\u7b97\u6cd5\u4f18\u5316\u65e0\u4eba\u673a\u4f4d\u7f6e\u4ee5\u6700\u5927\u5316\u52a0\u6743\u79d8\u5bc6\u901f\u7387\u3002", "result": "\u5f97\u5230\u4e86\u5728\u5b58\u5728\u76f8\u4f4d\u4f30\u8ba1\u8bef\u5dee\u60c5\u51b5\u4e0b\u7684\u9053\u79d8\u5bc6\u901f\u7387\u5206\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7Monte Carlo\u6a21\u62df\u9a8c\u8bc1\u4e86\u5206\u6790\u7ed3\u679c\u7684\u6b63\u786e\u6027\u3002", "conclusion": "\u5206\u6790\u4e86\u76f8\u4f4d\u4f30\u8ba1\u8bef\u5dee\u5bf9\u7cfb\u7edf\u5b89\u5168\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3aSTAR-RIS\u5728\u5b89\u5168\u65e0\u4eba\u673a\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.11976", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11976", "abs": "https://arxiv.org/abs/2508.11976", "authors": ["Yunning Cao", "Lihong Pei", "Jian Guo", "Yang Cao", "Yu Kang", "Yanlong Zhao"], "title": "Set-Valued Transformer Network for High-Emission Mobile Source Identification", "comment": null, "summary": "Identifying high-emission vehicles is a crucial step in regulating urban\npollution levels and formulating traffic emission reduction strategies.\nHowever, in practical monitoring data, the proportion of high-emission state\ndata is significantly lower compared to normal emission states. This\ncharacteristic long-tailed distribution severely impedes the extraction of\ndiscriminative features for emission state identification during data mining.\nFurthermore, the highly nonlinear nature of vehicle emission states and the\nlack of relevant prior knowledge also pose significant challenges to the\nconstruction of identification models.To address the aforementioned issues, we\npropose a Set-Valued Transformer Network (SVTN) to achieve comprehensive\nlearning of discriminative features from high-emission samples, thereby\nenhancing detection accuracy. Specifically, this model first employs the\ntransformer to measure the temporal similarity of micro-trip condition\nvariations, thus constructing a mapping rule that projects the original\nhigh-dimensional emission data into a low-dimensional feature space. Next, a\nset-valued identification algorithm is used to probabilistically model the\nrelationship between the generated feature vectors and their labels, providing\nan accurate metric criterion for the classification algorithm. To validate the\neffectiveness of our proposed approach, we conducted extensive experiments on\nthe diesel vehicle monitoring data of Hefei city in 2020. The results\ndemonstrate that our method achieves a 9.5\\% reduction in the missed detection\nrate for high-emission vehicles compared to the transformer-based baseline,\nhighlighting its superior capability in accurately identifying high-emission\nmobile pollution sources.", "AI": {"tldr": "\u57fa\u4e8eSet-Valued Transformer\u7f51\u7edc\u7684\u9ad8\u6392\u653e\u6c7d\u8f66\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u5e8f\u76f8\u4f3c\u6027\u6d4b\u91cf\u548c\u96c6\u503c\u8bc6\u522b\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u6392\u653e\u6c7d\u8f66\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u5b9e\u9645\u76d1\u6d4b\u6570\u636e\u4e2d\u9ad8\u6392\u653e\u72b6\u6001\u6570\u636e\u6bd4\u4f8b\u8fc5\u5c11\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u4ee5\u53ca\u6c7d\u8f66\u6392\u653e\u72b6\u6001\u7684\u9ad8\u975e\u7ebf\u6027\u7279\u6027\u548c\u77e5\u8bc6\u7f3a\u5931\u5bfc\u81f4\u7684\u8bc6\u522b\u56f0\u96be", "method": "\u63d0\u51faSet-Valued Transformer\u7f51\u7edc(SVTN)\uff0c\u5148\u7528transformer\u6d4b\u91cf\u5fae\u884c\u7a0b\u6761\u4ef6\u53d8\u5316\u7684\u65f6\u5e8f\u76f8\u4f3c\u6027\uff0c\u5c06\u9ad8\u7ef4\u6392\u653e\u6570\u636e\u6295\u5c04\u5230\u4f4e\u7ef4\u7279\u5f81\u7a7a\u95f4\uff0c\u7136\u540e\u7528\u96c6\u503c\u8bc6\u522b\u7b97\u6cd5\u5bf9\u7279\u5f81\u5411\u91cf\u4e0e\u6807\u7b7e\u5173\u7cfb\u8fdb\u884c\u6982\u7387\u5efa\u6a21", "result": "\u5728\u5408\u80a52020\u5e74\u67f4\u6cb9\u8f66\u76d1\u6d4b\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u6bd4transformer\u57fa\u7ebf\u964d\u4f4e\u4e869.5%\u7684\u9ad8\u6392\u653e\u6c7d\u8f66\u6f0f\u68c0\u7387", "conclusion": "SVTN\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u9ad8\u6392\u653e\u6837\u672c\u7684\u533a\u5206\u6027\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u6c61\u67d3\u6e90\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u57ce\u5e02\u6c61\u67d3\u76d1\u7ba1\u63d0\u4f9b\u4e86\u6709\u6548\u6280\u672f\u652f\u6491"}}
{"id": "2508.12204", "categories": ["eess.SP", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.12204", "abs": "https://arxiv.org/abs/2508.12204", "authors": ["Mauro Belgiovine", "Suyash Pradhan", "Johannes Lange", "Michael L\u00f6hning", "Kaushik Chowdhury"], "title": "ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search", "comment": "Accepted at IEEE PIMRC 2025", "summary": "Industry adoption of Artificial Intelligence (AI)-native wireless receivers,\nor even modular, Machine Learning (ML)-aided wireless signal processing blocks,\nhas been slow. The main concern is the lack of explainability of these trained\nML models and the significant risks posed to network functionalities in case of\nfailures, especially since (i) testing on every exhaustive case is infeasible\nand (ii) the data used for model training may not be available. This paper\nproposes ATLAS, an AI-guided approach that generates a battery of tests for\npre-trained AI-native receiver models and benchmarks the performance against a\nclassical receiver architecture. Using gradient-based optimization, it avoids\nspanning the exhaustive set of all environment and channel conditions; instead,\nit generates the next test in an online manner to further probe specific\nconfigurations that offer the highest risk of failure. We implement and\nvalidate our approach by adopting the well-known DeepRx AI-native receiver\nmodel as well as a classical receiver using differentiable tensors in NVIDIA's\nSionna environment. ATLAS uncovers specific combinations of mobility, channel\ndelay spread, and noise, where fully and partially trained variants of\nAI-native DeepRx perform suboptimally compared to the classical receivers. Our\nproposed method reduces the number of tests required per failure found by 19%\ncompared to grid search for a 3-parameters input optimization problem,\ndemonstrating greater efficiency. In contrast, the computational cost of the\ngrid-based approach scales exponentially with the number of variables, making\nit increasingly impractical for high-dimensional problems.", "AI": {"tldr": "ATLAS\u662f\u4e00\u4e2aAI\u5f15\u5bfc\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u6d4b\u8bd5\u9884\u8bad\u7ec3\u7684AI\u539f\u751f\u65e0\u7ebf\u63a5\u6536\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u9ad8\u6548\u53d1\u73b0\u6545\u969c\u914d\u7f6e\uff0c\u76f8\u6bd4\u7f51\u683c\u641c\u7d22\u51cf\u5c1119%\u7684\u6d4b\u8bd5\u91cf\u3002", "motivation": "AI\u539f\u751f\u65e0\u7ebf\u63a5\u6536\u5668\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u65e0\u6cd5\u7a77\u5c3d\u6d4b\u8bd5\u6240\u6709\u73af\u5883\u6761\u4ef6\uff0c\u5b58\u5728\u7f51\u7edc\u529f\u80fd\u6545\u969c\u98ce\u9669\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\u6765\u9a8c\u8bc1\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u7ebf\u751f\u6210\u9ad8\u98ce\u9669\u6545\u969c\u914d\u7f6e\u7684\u6d4b\u8bd5\u7528\u4f8b\uff0c\u907f\u514d\u7a77\u4e3e\u6240\u6709\u73af\u5883\u6761\u4ef6\uff0c\u5728NVIDIA Sionna\u73af\u5883\u4e2d\u5b9e\u73b0\u548c\u9a8c\u8bc1\u3002", "result": "ATLAS\u53d1\u73b0\u4e86AI\u539f\u751fDeepRx\u63a5\u6536\u5668\u5728\u7279\u5b9a\u79fb\u52a8\u6027\u3001\u4fe1\u9053\u5ef6\u8fdf\u6269\u5c55\u548c\u566a\u58f0\u7ec4\u5408\u4e0b\u6027\u80fd\u4e0d\u4f73\u7684\u60c5\u51b5\uff0c\u76f8\u6bd4\u7f51\u683c\u641c\u7d22\u51cf\u5c1119%\u7684\u6d4b\u8bd5\u91cf\u3002", "conclusion": "ATLAS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684AI\u6a21\u578b\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u6d4b\u8bd5\u7a7a\u95f4\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3aAI\u65e0\u7ebf\u63a5\u6536\u5668\u7684\u53ef\u9760\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11985", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11985", "abs": "https://arxiv.org/abs/2508.11985", "authors": ["Zhanhao Cao", "Clement Truong", "Andrew Lizarraga"], "title": "Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models", "comment": "Preprint", "summary": "Recent advances in large language models are driven by scale, while\nparameter-efficient fine-tuning (PEFT) enables updating only a small fraction\nof parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the\nproduct of two small matrices, which makes them natural building blocks that\ncan be composed. Motivated by the superposition principle, we hypothesize that\nindependently trained LoRA modules on disjoint domains are approximately\northogonal and can be combined by simple addition. Using GPT-2 Small (117M)\nwith LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,\nmedicine, finance). In pairwise tests, adding Math+Medicine adapters improves\nperplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance\nand Finance+Medicine change by +4.54% and +27.56%, respectively. Across\ncombinations, the RMS cosine similarity between LoRA deltas correlates\npositively and approximately linearly with the change in perplexity. Naive\nsummation requires no additional training, can be applied in seconds, and\nachieves performance comparable to models trained on merged data, while\nclarifying when interference appears in higher-order compositions.", "AI": {"tldr": "\u57fa\u4e8e\u8d85\u4f4d\u539f\u7406\u7684\u504f\u5dee\u76f8\u52a0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u77e9\u9635\u76f8\u52a0\u5c06\u72ec\u7acb\u8bad\u7ec3\u7684LoRA\u6a21\u5757\u7ec4\u5408\u8d77\u6765\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u80fd\u8fbe\u5230\u7c7b\u4f3c\u5408\u5e76\u6570\u636e\u8bad\u7ec3\u7684\u6548\u679c", "motivation": "\u5229\u7528\u8d85\u4f4d\u539f\u7406\uff0c\u5047\u8bbe\u5728\u4e0d\u76f8\u4ea4\u57df\u4e0a\u72ec\u7acb\u8bad\u7ec3\u7684LoRA\u6a21\u5757\u662f\u7ea6\u8fd1\u6b63\u4ea4\u7684\uff0c\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u76f8\u52a0\u6765\u7ec4\u5408", "method": "\u4f7f\u7528GPT-2 Small\u6a21\u578b\u548cLoRA\u6280\u672f\uff08rank 4, alpha=64\uff09\uff0c\u5728\u6570\u5b66\u3001\u533b\u5b66\u3001\u91d1\u878d\u4e09\u4e2aQA\u57df\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u7136\u540e\u901a\u8fc7\u76f4\u63a5\u76f8\u52a0\u77e9\u9635\u6765\u7ec4\u5408\u4e0d\u540c\u57df\u7684\u6a21\u5757", "result": "\u6570\u5b66+\u533b\u5b66\u7ec4\u5408\u76f8\u5bf9\u5408\u5e76\u6570\u636e\u8bad\u7ec3\u6539\u5584\u8bed\u8a00\u6a21\u578b\u7684\u56f0\u60d1\u5ea6-9.10%\uff0c\u800c\u5176\u4ed6\u7ec4\u5408\u6548\u679c\u6709\u6240\u4e0d\u540c\uff0c\u4e14LoRA\u504f\u5dee\u7684\u4f59\u5f26\u76f8\u4f3c\u6027\u4e0e\u56f0\u60d1\u5ea6\u53d8\u5316\u5448\u6b63\u76f8\u5173", "conclusion": "\u7b80\u5355\u7684\u77e9\u9635\u76f8\u52a0\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u53ef\u5728\u79d2\u7ea7\u5b8c\u6210\uff0c\u80fd\u8fbe\u5230\u7c7b\u4f3c\u5408\u5e76\u6570\u636e\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u540c\u65f6\u80fd\u591f\u5448\u73b0\u9ad8\u9636\u7ec4\u5408\u4e2d\u7684\u5e72\u6270\u73b0\u8c61"}}
{"id": "2508.12207", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12207", "abs": "https://arxiv.org/abs/2508.12207", "authors": ["Chenxin Tu", "Xiaowei Cui", "Gang Liu", "Mingquan Lu"], "title": "Weighted Covariance Intersection for Range-based Distributed Cooperative Localization of Multi-Agent Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Precise localization of multi-agent systems (MAS) in harsh environments is a\ncritical challenge for swarm applications, and cooperative localization is\nconsidered a key solution to this issue. Among all solutions, distributed\ncooperative localization (DCL) has garnered widespread attention due to its\nrobustness and scalability. The main challenge of DCL lies in how to fuse\nrelative measurements between agents under unknown correlations. To address\nthis, covariance intersection (CI) was introduced to DCL. However, the\nclassical CI optimization criteria suffer from issues such as scale imbalance\nand correlation mismatch during the fusion process. These deficiencies are not\nas pronounced in 2D scenarios, where the state space is relatively simple and\nthe observability of each state component is well. However, in 3D scenarios,\nwhere the state space is more complex and there are significant disparities in\nthe scale and observability of state components, performance degradation\nbecomes severe. This necessitates the design of specialized mechanisms to\nimprove the data fusion process. In this paper, we identify three main\ndrawbacks of the classical CI optimization criteria in recursive filtering and\nintroduce a weighting mechanism, namely weighted covariance intersection (WCI),\nto improve its performance. We then introduce WCI into range-based distributed\ncooperative localization in 3D scenarios, developing a concurrent fusion\nstrategy for multiple distance measurements and designing a weighting matrix\nbased on the error propagation rule of the inertial navigation system (INS).\nSimulation results demonstrate that the proposed WCI significantly enhances\ncooperative localization performance compared to classical CI, while the\ndistributed approach outperforms the centralized approach in terms of\nrobustness, scalability, and is more suitable for large-scale swarms.", "AI": {"tldr": "\u63d0\u51fa\u52a0\u6743\u534f\u65b9\u5dee\u4ea4\u96c6(WCI)\u65b9\u6cd5\u6539\u8fdb3D\u5206\u5e03\u5f0f\u534f\u540c\u5b9a\u4f4d\uff0c\u89e3\u51b3\u7ecf\u5178CI\u5728\u72b6\u6001\u5206\u91cf\u5c3a\u5ea6\u548c\u53ef\u89c2\u6d4b\u6027\u5dee\u5f02\u5927\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898", "motivation": "3D\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u540c\u5b9a\u4f4d\u9762\u4e34\u72b6\u6001\u7a7a\u95f4\u590d\u6742\u3001\u72b6\u6001\u5206\u91cf\u5c3a\u5ea6\u548c\u53ef\u89c2\u6d4b\u6027\u5dee\u5f02\u5927\u7684\u6311\u6218\uff0c\u7ecf\u5178CI\u65b9\u6cd5\u5b58\u5728\u5c3a\u5ea6\u4e0d\u5e73\u8861\u548c\u76f8\u5173\u5931\u914d\u95ee\u9898", "method": "\u5f15\u5165\u52a0\u6743\u534f\u65b9\u5dee\u4ea4\u96c6(WCI)\u673a\u5236\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u8bef\u5dee\u4f20\u64ad\u89c4\u5219\u7684\u6743\u91cd\u77e9\u9635\uff0c\u5f00\u53d1\u591a\u8ddd\u79bb\u6d4b\u91cf\u7684\u5e76\u53d1\u878d\u5408\u7b56\u7565", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660eWCI\u76f8\u6bd4\u7ecf\u5178CI\u663e\u8457\u63d0\u5347\u534f\u540c\u5b9a\u4f4d\u6027\u80fd\uff0c\u5206\u5e03\u5f0f\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u65b9\u6cd5", "conclusion": "WCI\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e863D\u5206\u5e03\u5f0f\u534f\u540c\u5b9a\u4f4d\u4e2d\u7684\u76f8\u5173\u878d\u5408\u95ee\u9898\uff0c\u66f4\u9002\u5408\u5927\u89c4\u6a21\u96c6\u7fa4\u5e94\u7528"}}
{"id": "2508.12213", "categories": ["eess.SP", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12213", "abs": "https://arxiv.org/abs/2508.12213", "authors": ["Yize Cai", "Baoshen Guo", "Flora Salim", "Zhiqing Hong"], "title": "Towards Generalizable Human Activity Recognition: A Survey", "comment": null, "summary": "As a critical component of Wearable AI, IMU-based Human Activity Recognition\n(HAR) has attracted increasing attention from both academia and industry in\nrecent years. Although HAR performance has improved considerably in specific\nscenarios, its generalization capability remains a key barrier to widespread\nreal-world adoption. For example, domain shifts caused by variations in users,\nsensor positions, or environments can significantly decrease the performance in\npractice. As a result, in this survey, we explore the rapidly evolving field of\nIMU-based generalizable HAR, reviewing 229 research papers alongside 25\npublicly available datasets to provide a broad and insightful overview. We\nfirst present the background and overall framework of IMU-based HAR tasks, as\nwell as the generalization-oriented training settings. Then, we categorize\nrepresentative methodologies from two perspectives: (i) model-centric\napproaches, including pre-training method, end-to-end method, and large\nlanguage model (LLM)-based learning method; and (ii) data-centric approaches,\nincluding multi-modal learning and data augmentation techniques. In addition,\nwe summarize widely used datasets in this field, as well as relevant tools and\nbenchmarks. Building on these methodological advances, the broad applicability\nof IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent\nchallenges (e.g., data scarcity, efficient training, and reliable evaluation)\nand also outline future directions for HAR, including the adoption of\nfoundation and large language models, physics-informed and context-aware\nreasoning, generative modeling, and resource-efficient training and inference.\nThe complete list of this survey is available at\nhttps://github.com/rh20624/Awesome-IMU-Sensing, which will be updated\ncontinuously.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u57fa\u4e8eIMU\u7684\u53ef\u6cdb\u5316\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\uff0c\u6db5\u76d6\u4e86229\u7bc7\u7814\u7a76\u8bba\u6587\u548c25\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u4ece\u6a21\u578b\u4e2d\u5fc3\u548c\u6570\u636e\u4e2d\u5fc3\u4e24\u4e2a\u89d2\u5ea6\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1IMU-based HAR\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5176\u6cdb\u5316\u80fd\u529b\u4ecd\u7136\u662f\u5b9e\u9645\u5e94\u7528\u7684\u4e3b\u8981\u969c\u788d\u3002\u9886\u57df\u504f\u79fb\uff08\u7528\u6237\u3001\u4f20\u611f\u5668\u4f4d\u7f6e\u3001\u73af\u5883\u53d8\u5316\uff09\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u53ef\u6cdb\u5316\u7684HAR\u65b9\u6cd5\u3002", "method": "\u4ece\u4e24\u4e2a\u89d2\u5ea6\u5206\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\uff1a(i)\u6a21\u578b\u4e2d\u5fc3\u65b9\u6cd5\uff1a\u5305\u62ec\u9884\u8bad\u7ec3\u65b9\u6cd5\u3001\u7aef\u5230\u7aef\u65b9\u6cd5\u3001\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u65b9\u6cd5\uff1b(ii)\u6570\u636e\u4e2d\u5fc3\u65b9\u6cd5\uff1a\u5305\u62ec\u591a\u6a21\u6001\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u540c\u65f6\u603b\u7ed3\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u3001\u5de5\u5177\u548c\u57fa\u51c6\u3002", "result": "\u63d0\u4f9b\u4e86\u57fa\u4e8eIMU\u7684\u53ef\u6cdb\u5316HAR\u9886\u57df\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u65b9\u6cd5\u8bba\u5206\u7c7b\u3001\u6570\u636e\u96c6\u603b\u7ed3\u3001\u5de5\u5177\u548c\u57fa\u51c6\uff0c\u5e76\u5efa\u7acb\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u8d44\u6e90\u5e93\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\uff08\u5982\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u6548\u8bad\u7ec3\u3001\u53ef\u9760\u8bc4\u4f30\uff09\u5e76\u5c55\u671b\u4e86\u672a\u6765\u65b9\u5411\uff0c\u5305\u62ec\u57fa\u7840\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u91c7\u7528\u3001\u7269\u7406\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u3001\u751f\u6210\u5efa\u6a21\u4ee5\u53ca\u8d44\u6e90\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002"}}
{"id": "2508.12021", "categories": ["cs.LG", "cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.12021", "abs": "https://arxiv.org/abs/2508.12021", "authors": ["You Hak Lee", "Xiaofan Yu", "Quanling Zhao", "Flavio Ponzina", "Tajana Rosing"], "title": "FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing", "comment": null, "summary": "Unsupervised federated learning (UFL) has gained attention as a\nprivacy-preserving, decentralized machine learning approach that eliminates the\nneed for labor-intensive data labeling. However, UFL faces several challenges\nin practical applications: (1) non-independent and identically distributed\n(non-iid) data distribution across devices, (2) expensive computational and\ncommunication costs at the edge, and (3) vulnerability to communication noise.\nPrevious UFL approaches have relied on deep neural networks (NN), which\nintroduce substantial overhead in both computation and communication. In this\npaper, we propose FedUHD, the first UFL framework based on Hyperdimensional\nComputing (HDC). HDC is a brain-inspired computing scheme with lightweight\ntraining and inference operations, much smaller model size, and robustness to\ncommunication noise. FedUHD introduces two novel HDC-based designs to improve\nUFL performance. On the client side, a kNN-based cluster hypervector removal\nmethod addresses non-iid data samples by eliminating detrimental outliers. On\nthe server side, a weighted HDC aggregation technique balances the non-iid data\ndistribution across clients. Our experiments demonstrate that FedUHD achieves\nup to 173.6x and 612.7x better speedup and energy efficiency, respectively, in\ntraining, up to 271x lower communication cost, and 15.50% higher accuracy on\naverage across diverse settings, along with superior robustness to various\ntypes of noise compared to state-of-the-art NN-based UFL approaches.", "AI": {"tldr": "\u57fa\u4e8e\u8d85\u9ad8\u7ef4\u8ba1\u7b97(HDC)\u7684\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6FedUHD\uff0c\u89e3\u51b3\u4e86\u975eIID\u6570\u636e\u5206\u5e03\u3001\u8ba1\u7b97\u901a\u4fe1\u6210\u672c\u9ad8\u548c\u901a\u4fe1\u566a\u58f0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u901f\u5ea6\u63d0\u5347\u3001\u80fd\u6d88\u8017\u4f18\u5316\u548c\u7cbe\u5ea6\u63d0\u9ad8\u3002", "motivation": "\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60(UFL)\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u4e09\u5927\u6311\u6218\uff1a\u975eIID\u6570\u636e\u5206\u5e03\u3001\u8ba1\u7b97\u901a\u4fe1\u6210\u672c\u9ad8\u3001\u901a\u4fe1\u566a\u58f0\u6545\u969c\u3002\u4f20\u7edf\u6df1\u5ea6\u7f51\u7edc\u65b9\u6848\u5b58\u5728\u8fc7\u91cd\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8d85\u9ad8\u7ef4\u8ba1\u7b97(HDC)\u7684FedUHD\u6846\u67b6\uff1a\u5ba2\u6237\u7aef\u91c7\u7528kNN\u805a\u7c7b\u8d85\u5411\u91cf\u5220\u9664\u6cd5\u6d88\u9664\u5f02\u5e38\u503c\uff1b\u670d\u52a1\u5668\u7aef\u91c7\u7528\u6743\u91cdHDC\u805a\u5408\u6280\u672f\u5e73\u8861\u975eIID\u6570\u636e\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347173.6\u500d\uff0c\u80fd\u6548\u63d0\u5347612.7\u500d\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4f271\u500d\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad815.50%\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u566a\u58f0\u8010\u53d7\u6027\u3002", "conclusion": "FedUHD\u4f5c\u4e3a\u9996\u4e2a\u57fa\u4e8eHDC\u7684UFL\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u548c\u566a\u58f0\u8010\u53d7\u6027\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12215", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12215", "abs": "https://arxiv.org/abs/2508.12215", "authors": ["Shuntian Tang", "Zesong Fei", "Xinyi Wang", "Dongkai Zhou", "Zhiqiang Wei", "Christos Masouros"], "title": "A Novel Symbol Level Precoding based AFDM Transmission Framework: Offloading Equalization Burden to Transmitter Side", "comment": "13 pages, 9 figures; submitted to IEEE journals for possible\n  publication", "summary": "Affine Frequency Division Multiplexing (AFDM) has attracted considerable\nattention for its robustness to Doppler effects. However, its high\nreceiver-side computational complexity remains a major barrier to practical\ndeployment. To address this, we propose a novel symbol-level precoding\n(SLP)-based AFDM transmission framework, which shifts the signal processing\nburden in downlink communications from user side to the base station (BS),\nenabling direct symbol detection without requiring channel estimation or\nequalization at the receiver. Specifically, in the uplink phase, we propose a\nSparse Bayesian Learning (SBL) based channel estimation algorithm by exploiting\nthe inherent sparsity of affine frequency (AF) domain channels. In particular,\nthe sparse prior is modeled via a hierarchical Laplace distribution, and\nparameters are iteratively updated using the Expectation-Maximization (EM)\nalgorithm. We also derive the Bayesian Cramer-Rao Bound (BCRB) to characterize\nthe theoretical performance limit. In the downlink phase, the BS employs the\nSLP technology to design the transmitted waveform based on the estimated uplink\nchannel state information (CSI) and channel reciprocity. The resulting\noptimization problem is formulated as a second-order cone programming (SOCP)\nproblem, and its dual problem is investigated by Lagrangian function and\nKarush-Kuhn-Tucker conditions. Simulation results demonstrate that the proposed\nSBL estimator outperforms traditional orthogonal matching pursuit (OMP) in\naccuracy and robustness to off-grid effects, while the SLP-based waveform\ndesign scheme achieves performance comparable to conventional AFDM receivers\nwhile significantly reducing the computational complexity at receiver,\nvalidating the practicality of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u7ea7\u9884\u7f16\u7801(SLP)\u7684AFDM\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5904\u7406\u8d1f\u62c5\u4ece\u7528\u6237\u7aef\u8f6c\u79fb\u5230\u57fa\u7ad9\u7aef\uff0c\u964d\u4f4e\u63a5\u6536\u7aef\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u4e0a\u884c\u94fe\u8def\u91c7\u7528SBL\u6e90\u4f30\u8ba1\u7b97\u6cd5\uff0c\u4e0b\u884c\u94fe\u8def\u91c7\u7528SLP\u6280\u672f\u8bbe\u8ba1\u53d1\u5c04\u6ce2\u5f62\u3002", "motivation": "AFDM\u867d\u7136\u5728\u591a\u666e\u52d2\u6548\u5e94\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u7684\u63a5\u6536\u7aef\u8ba1\u7b97\u590d\u6742\u5ea6\u4ecd\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6848\uff0c\u80fd\u591f\u4fdd\u6301AFDM\u7684\u6027\u80fd\u4f18\u52bf\u540c\u65f6\u5927\u5e45\u5ea6\u964d\u4f4e\u63a5\u6536\u7aef\u7684\u8ba1\u7b97\u8981\u6c42\u3002", "method": "1. \u4e0a\u884c\u94fe\u8def\uff1a\u63d0\u51fa\u57fa\u4e8e\u7a00\u758f\u8d1d\u53f6\u65af\u5b66\u4e60(SBL)\u7684\u901a\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5229\u7528\u963f\u5947\u9891\u57df\u901a\u9053\u7684\u5185\u5728\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5c42\u6b21\u62c9\u666e\u62c9\u65af\u5206\u5e03\u5efa\u6a21\u5e76\u4f7f\u7528EM\u7b97\u6cd5\u8fed\u4ee3\u66f4\u65b0\u53c2\u6570\n2. \u4e0b\u884c\u94fe\u8def\uff1a\u57fa\u7ad9\u91c7\u7528\u7b26\u53f7\u7ea7\u9884\u7f16\u7801(SLP)\u6280\u672f\uff0c\u57fa\u4e8e\u4f30\u8ba1\u7684\u4e0a\u884c\u94fe\u8def\u901a\u9053\u72b6\u6001\u4fe1\u606f\u548c\u901a\u9053\u4e92\u9006\u6027\u8bbe\u8ba1\u53d1\u5c04\u6ce2\u5f62\n3. \u5c06\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4e8c\u9636\u9525\u89c4\u5212(SOCP)\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u51fd\u6570\u548cKKT\u6761\u4ef6\u7814\u7a76\u5bf9\u5076\u95ee\u9898", "result": "1. \u63d0\u51fa\u7684SBL\u4f30\u8ba1\u5668\u5728\u51c6\u786e\u6027\u548c\u5bf9\u79bb\u7f51\u683c\u6548\u5e94\u7684\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u8d85\u8fc7\u4f20\u7edf\u7684\u6b63\u4ea4\u5339\u914d\u8ffd\u8e2a(OMP)\u7b97\u6cd5\n2. \u57fa\u4e8eSLP\u7684\u6ce2\u5f62\u8bbe\u8ba1\u65b9\u6848\u80fd\u591f\u8fbe\u5230\u4e0e\u4f20\u7edfAFDM\u63a5\u6536\u673a\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a5\u6536\u7aef\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\n3. \u6a21\u62df\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6848\u7684\u5b9e\u7528\u6027", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684SLP\u57faAFDM\u4f20\u8f93\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AFDM\u9ad8\u63a5\u6536\u7aef\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5728\u57fa\u7ad9\u7aef\u91c7\u7528\u5148\u8fdb\u7684\u901a\u9053\u4f30\u8ba1\u548c\u6ce2\u5f62\u8bbe\u8ba1\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u7cfb\u7edf\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u8f7d\u63a5\u6536\u7aef\u5904\u7406\u590d\u6742\u5ea6\uff0c\u4e3aAFDM\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12042", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12042", "abs": "https://arxiv.org/abs/2508.12042", "authors": ["Zahra Kharaghani", "Ali Dadras", "Tommy L\u00f6fstedt"], "title": "Fairness Regularization in Federated Learning", "comment": "25 pages", "summary": "Federated Learning (FL) has emerged as a vital paradigm in modern machine\nlearning that enables collaborative training across decentralized data sources\nwithout exchanging raw data. This approach not only addresses privacy concerns\nbut also allows access to overall substantially larger and potentially more\ndiverse datasets, without the need for centralized storage or hardware\nresources. However, heterogeneity in client data may cause certain clients to\nhave disproportionate impacts on the global model, leading to disparities in\nthe clients' performances. Fairness, therefore, becomes a crucial concern in FL\nand can be addressed in various ways. However, the effectiveness of existing\nfairness-aware methods, particularly in heterogeneous data settings, remains\nunclear, and the relationships between different approaches are not well\nunderstood. In this work, we focus on performance equitable fairness, which\naims to minimize differences in performance across clients. We restrict our\nstudy to fairness-aware methods that explicitly regularize client losses,\nevaluating both existing and newly proposed approaches. We identify and\ntheoretically explain connections between the investigated fairness methods,\nand empirically show that FairGrad (approximate) and FairGrad* (exact) (two\nvariants of a gradient variance regularization method introduced here for\nperformance equitable fairness) improve both fairness and overall model\nperformance in heterogeneous data settings.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u516c\u5e73\u6027\u95ee\u9898\u7814\u7a76\uff0c\u63d0\u51faFairGrad\u65b9\u6cd5\u5728\u5f02\u8d28\u6027\u6570\u636e\u73af\u5883\u4e0b\u540c\u65f6\u63d0\u5347\u516c\u5e73\u6027\u548c\u6574\u4f53\u6a21\u578b\u6027\u80fd", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u8d28\u6027\u5bfc\u81f4\u516c\u5e73\u6027\u95ee\u9898\uff0c\u73b0\u6709\u516c\u5e73\u65b9\u6cd5\u5728\u5f02\u8d28\u6570\u636e\u73af\u5883\u4e0b\u6548\u679c\u4e0d\u660e\u786e\u4e14\u65b9\u6cd5\u95f4\u5173\u8054\u6027\u4e0d\u6e05", "method": "\u91cd\u70b9\u7814\u7a76\u663e\u5f0f\u6b63\u5219\u5316\u5ba2\u6237\u635f\u5931\u7684\u516c\u5e73\u65b9\u6cd5\uff0c\u63d0\u51faFairGrad(\u8fd1\u4f3c)\u548cFairGrad*(\u7cbe\u786e)\u4e24\u79cd\u68af\u5ea6\u65b9\u5dee\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e76\u7406\u8bba\u5206\u6790\u5404\u65b9\u6cd5\u95f4\u8054\u7cfb", "result": "FairGrad\u548cFairGrad*\u5728\u5f02\u8d28\u6027\u6570\u636e\u73af\u5883\u4e0b\u540c\u65f6\u63d0\u5347\u4e86\u516c\u5e73\u6027\u548c\u6574\u4f53\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5404\u79cd\u516c\u5e73\u65b9\u6cd5\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb", "conclusion": "\u901a\u8fc7\u68af\u5ea6\u65b9\u5dee\u6b63\u5219\u5316\u7684FairGrad\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u516c\u5e73\u6027\u95ee\u9898\uff0c\u4e3a\u5f02\u8d28\u6027\u6570\u636e\u73af\u5883\u4e0b\u7684\u516c\u5e73FL\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2508.12298", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12298", "abs": "https://arxiv.org/abs/2508.12298", "authors": ["Seungcheol Oh", "Han Han", "Joongheon Kim", "Sean Kwon"], "title": "Polarization Reconfigurable Transmit-Receive Beam Alignment with Interpretable Transformer", "comment": null, "summary": "Recent advancement in next generation reconfigurable antenna and fluid\nantenna technology has influenced the wireless system with polarization\nreconfigurable (PR) channels to attract significant attention for promoting\nbeneficial channel condition. We exploit the benefit of PR antennas by\nintegrating such technology into massive multiple-input-multiple-output (MIMO)\nsystem. In particular, we aim to jointly design the polarization and\nbeamforming vectors on both transceivers for simultaneous channel\nreconfiguration and beam alignment, which remarkably enhance the beamforming\ngain. However, joint optimization over polarization and beamforming vectors\nwithout channel state information (CSI) is a challenging task, since\ndepolarization increases the channel dimension; whereas massive MIMO systems\ntypically have low-dimensional pilot measurement from limited radio frequency\n(RF) chain. This leads to pilot overhead because the transceivers can only\nobserve low-dimensional measurement of the high-dimension channel. This paper\npursues the reduction of the pilot overhead in such systems by proposing to\nemploy \\emph{interpretable transformer}-based deep learning framework on both\ntransceivers to actively design the polarization and beamforming vectors for\npilot stage and transmission stage based on the sequence of accumulated\nreceived pilots. Numerical experiments demonstrate the significant performance\ngain of our proposed framework over the existing non-adaptive and active\ndata-driven methods. Furthermore, we exploit the interpretability of our\nproposed framework to analyze the learning capabilities of the model.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91caTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u7684\u6781\u5316\u91cd\u914d\u548c\u6838\u5fc3\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4ee5\u51cf\u5c11\u5bfc\u9891\u94fe\u9650\u5236\u4e0b\u7684\u5bfc\u9891\u5f00\u9500\u3002", "motivation": "\u5229\u7528\u53ef\u91cd\u914d\u6781\u5316\u5929\u7ebf\u6280\u672f\u63d0\u5347\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u6027\u80fd\uff0c\u4f46\u6781\u5316\u53d8\u91cf\u589e\u52a0\u4e86\u901a\u9053\u7ef4\u5ea6\uff0c\u800c\u9650\u7684RF\u94fe\u6570\u91cf\u5bfc\u81f4\u4f4e\u7ef4\u5bfc\u9891\u6d4b\u91cf\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7ef4\u901a\u9053\u4f30\u8ba1\u9700\u6c42\uff0c\u4ea7\u751f\u5927\u91cf\u5bfc\u9891\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91caTransformer\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u53d1\u9001\u7aef\u548c\u63a5\u6536\u7aef\u90fd\u4f7f\u7528\u8be5\u6a21\u578b\u6765\u4e3b\u52a8\u8bbe\u8ba1\u6781\u5316\u548c\u6838\u5fc3\u7f51\u7edc\u5411\u91cf\u3002\u6a21\u578b\u57fa\u4e8e\u7d2f\u79ef\u7684\u63a5\u6536\u5bfc\u9891\u5e8f\u5217\u6765\u8fdb\u884c\u5b66\u4e60\u548c\u51b3\u7b56\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u7684\u975e\u9002\u5e94\u6027\u548c\u4e3b\u52a8\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002\u540c\u65f6\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u8fd8\u5141\u8bb8\u5206\u6790\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u89e3\u51b3\u4e86\u6781\u5316\u91cd\u914dMIMO\u7cfb\u7edf\u4e2d\u7684\u5bfc\u9891\u5f00\u9500\u95ee\u9898\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u901a\u9053\u91cd\u914d\u548c\u6838\u5fc3\u5bf9\u51c6\uff0c\u4e3a\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.12061", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12061", "abs": "https://arxiv.org/abs/2508.12061", "authors": ["Daria Diatlova", "Nikita Balagansky", "Alexander Varlamov", "Egor Spirin"], "title": "VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks", "comment": null, "summary": "Conventional methods for aggregating layers in fine-tuned self-supervised\nspeech models, such as using the final layer or weighted sum, suffer from\ninformation bottlenecks and static feature weighting for all dataset examples.\nWe propose VARAN, a framework that dynamically tailors layer aggregation to\nindividual inputs. By employing layer-specialized probing heads and\ndata-dependent weighting, VARAN adaptively prioritizes layer's features based\non input. Evaluations on automatic speech recognition and speech emotion\nrecognition tasks demonstrate VARAN's superior performance, particularly when\nusing the LoRA fine-tuning technique. The framework resolves the trade-off\nbetween preserving layer-specific information and enabling flexible feature\nutilization, advancing efficient adaptation of self-supervised speech\nrepresentations.", "AI": {"tldr": "VARAN\u662f\u4e00\u4e2a\u52a8\u6001\u5c42\u805a\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u4f9d\u8d56\u7684\u6743\u91cd\u8c03\u6574\u548c\u4e13\u95e8\u5316\u7684\u63a2\u6d4b\u5934\uff0c\u4e3a\u6bcf\u4e2a\u8f93\u5165\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u4f73\u7279\u5f81\u5c42\u7ec4\u5408\uff0c\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u5c42\u805a\u5408\u65b9\u6cd5\uff08\u5982\u4f7f\u7528\u6700\u540e\u4e00\u5c42\u6216\u52a0\u6743\u6c42\u548c\uff09\u5b58\u5728\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u5bf9\u6240\u6709\u6570\u636e\u6837\u672c\u91c7\u7528\u9759\u6001\u7279\u5f81\u6743\u91cd\uff0c\u65e0\u6cd5\u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u8c03\u6574\u3002", "method": "\u4f7f\u7528\u5c42\u4e13\u95e8\u5316\u7684\u63a2\u6d4b\u5934\u548c\u8f93\u5165\u4f9d\u8d56\u7684\u6743\u91cd\u673a\u5236\uff0c\u52a8\u6001\u5730\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6837\u672c\u8c03\u6574\u4e0d\u540c\u5c42\u7684\u7279\u5f81\u6743\u91cd\uff0c\u4f18\u5148\u9009\u62e9\u5bf9\u5f53\u524d\u8f93\u5165\u6700\u6709\u7528\u7684\u7279\u5f81\u5c42\u3002", "result": "\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cVARAN\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528LoRA\u5fae\u8c03\u6280\u672f\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "VARAN\u6846\u67b6\u89e3\u51b3\u4e86\u4fdd\u7559\u5c42\u7279\u5b9a\u4fe1\u606f\u4e0e\u5b9e\u73b0\u7075\u6d3b\u7279\u5f81\u5229\u7528\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u63a8\u8fdb\u4e86\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u7684\u9ad8\u6548\u9002\u5e94\u3002"}}
{"id": "2508.12320", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12320", "abs": "https://arxiv.org/abs/2508.12320", "authors": ["Pengyu Wang", "Zhaocheng Wang", "Tianqi Mao", "Weijie Yuan", "Haijun Zhang", "George K. Karagiannidis"], "title": "Jamming Identification with Differential Transformer for Low-Altitude Wireless Networks", "comment": null, "summary": "Wireless jamming identification, which detects and classifies electromagnetic\njamming from non-cooperative devices, is crucial for emerging low-altitude\nwireless networks consisting of many drone terminals that are highly\nsusceptible to electromagnetic jamming. However, jamming identification schemes\nadopting deep learning (DL) are vulnerable to attacks involving carefully\ncrafted adversarial samples, resulting in inevitable robustness degradation. To\naddress this issue, we propose a differential transformer framework for\nwireless jamming identification. Firstly, we introduce a differential\ntransformer network in order to distinguish jamming signals, which overcomes\nthe attention noise when compared with its traditional counterpart by\nperforming self-attention operations in a differential manner. Secondly, we\npropose a randomized masking training strategy to improve network robustness,\nwhich leverages the patch partitioning mechanism inherent to transformer\narchitectures in order to create parallel feature extraction branches. Each\nbranch operates on a distinct, randomly masked subset of patches, which\nfundamentally constrains the propagation of adversarial perturbations across\nthe network. Additionally, the ensemble effect generated by fusing predictions\nfrom these diverse branches demonstrates superior resilience against\nadversarial attacks. Finally, we introduce a novel consistent training\nframework that significantly enhances adversarial robustness through dualbranch\nregularization. Simulation results demonstrate that our proposed methodology is\nsuperior to existing methods in boosting robustness to adversarial samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u53d8\u6362\u5668\u7684\u65e0\u7ebf\u5e72\u6270\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5dee\u5206\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u968f\u673a\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027", "motivation": "\u65e0\u4eba\u673a\u7b49\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u6613\u53d7\u7535\u78c1\u5e72\u6270\uff0c\u800c\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u5e72\u6270\u8bc6\u522b\u65b9\u6848\u5728\u9762\u5bf9\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5bf9\u6297\u6837\u672c\u65f6\u5b58\u5728\u9c81\u68d2\u6027\u4e0b\u964d\u7684\u95ee\u9898", "method": "1) \u5dee\u5206\u53d8\u6362\u5668\u7f51\u7edc\u8fdb\u884c\u5dee\u5206\u81ea\u6ce8\u610f\u529b\u64cd\u4f5c\u6765\u533a\u5206\u5e72\u6270\u4fe1\u53f7\uff1b2) \u968f\u673a\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u521b\u5efa\u5e76\u884c\u7279\u5f81\u63d0\u53d6\u5206\u652f\uff1b3) \u53cc\u5206\u652f\u6b63\u5219\u5316\u7684\u4e00\u81f4\u6027\u8bad\u7ec3\u6846\u67b6", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u5bf9\u6297\u6837\u672c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5dee\u5206\u53d8\u6362\u5668\u548c\u968f\u673a\u63a9\u7801\u8bad\u7ec3\u6709\u6548\u63d0\u5347\u4e86\u65e0\u7ebf\u5e72\u6270\u8bc6\u522b\u7cfb\u7edf\u5bf9\u6297\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027"}}
{"id": "2508.12079", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12079", "abs": "https://arxiv.org/abs/2508.12079", "authors": ["Ningzhe Shi", "Yiqing Zhou", "Ling Liu", "Jinglin Shi", "Yihao Wu", "Haiwei Shi", "Hanxiao Yu"], "title": "Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks", "comment": null, "summary": "Integrated sensing and communication (ISAC) can enhance artificial\nintelligence-generated content (AIGC) networks by providing efficient sensing\nand transmission. Existing AIGC services usually assume that the accuracy of\nthe generated content can be ensured, given accurate input data and prompt,\nthus only the content generation quality (CGQ) is concerned. However, it is not\napplicable in ISAC-based AIGC networks, where content generation is based on\ninaccurate sensed data. Moreover, the AIGC model itself introduces generation\nerrors, which depend on the number of generating steps (i.e., computing\nresources). To assess the quality of experience of ISAC-based AIGC services, we\npropose a content accuracy and quality aware service assessment metric (CAQA).\nSince allocating more resources to sensing and generating improves content\naccuracy but may reduce communication quality, and vice versa, this\nsensing-generating (computing)-communication three-dimensional resource\ntradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all\nusers with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution\nspace that grows exponentially with users. To solve the CAQA-AIGC problem with\nlow complexity, a linear programming (LP) guided deep reinforcement learning\n(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the\nLP-guided approach and the action filter, LPDRL-F can transform the original\nthree-dimensional solution space to two dimensions, reducing complexity while\nimproving the learning performance of DRL. Simulations show that compared to\nexisting DRL and generative diffusion model algorithms without LP, LPDRL-F\nconverges faster by over 60% and finds better resource allocation solutions,\nimproving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an\nimprovement in AvgCAQA of more than 50% compared to existing schemes focusing\nsolely on CGQ.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d44\u6e90\u5206\u914d\u7b97\u6cd5LPDRL-F\uff0c\u7528\u4e8e\u4f18\u5316\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7684AIGC\u7f51\u7edc\u4e2d\u7684\u611f\u77e5-\u751f\u6210-\u901a\u4fe1\u4e09\u7ef4\u8d44\u6e90\u5206\u914d\uff0c\u63d0\u9ad8AIGC\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709AIGC\u670d\u52a1\u5047\u8bbe\u8f93\u5165\u6570\u636e\u51c6\u786e\uff0c\u53ea\u5173\u6ce8\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u5728ISAC-AIGC\u7f51\u7edc\u4e2d\u611f\u77e5\u6570\u636e\u4e0d\u51c6\u786e\u4e14AIGC\u6a21\u578b\u81ea\u8eab\u5b58\u5728\u9519\u8bef\uff0c\u9700\u8981\u8003\u8651\u5185\u5bb9\u51c6\u786e\u6027\u548c\u8d28\u91cf\u7684\u7efc\u5408\u8bc4\u4f30\u3002", "method": "\u63d0\u51faCAQA\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u8bbe\u8ba1LP\u5bfc\u5411\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5LPDRL-F\uff0c\u901a\u8fc7LP\u5bfc\u5411\u548c\u52a8\u4f5c\u7b5b\u9009\u5668\u5c06\u4e09\u7ef4\u89e3\u7a7a\u95f4\u964d\u4e3a\u4e8c\u7ef4\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u6a21\u62df\u663e\u793aLPDRL-F\u6bd4\u73b0\u6709\u7b97\u6cd5\u6536\u655b\u901f\u5ea6\u63d0\u9ad860%\u4ee5\u4e0a\uff0cAvgCAQA\u63d0\u9ad814%\u4ee5\u4e0a\uff0c\u4e0e\u4ec5\u5173\u6ce8CGQ\u7684\u65b9\u6848\u76f8\u6bd4AvgCAQA\u63d0\u9ad850%\u4ee5\u4e0a\u3002", "conclusion": "LPDRL-F\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u89e3\u51b3ISAC-AIGC\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347AIGC\u670d\u52a1\u8d28\u91cf\uff0c\u4e3a\u96c6\u6210\u611f\u77e5\u901a\u4fe1\u7684AIGC\u7f51\u7edc\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8d44\u6e90\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2508.12371", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12371", "abs": "https://arxiv.org/abs/2508.12371", "authors": ["Lin Wang", "Zhiqing Wei", "Xu Chen", "Zhiyong Feng"], "title": "Coherent Compensation-Based Sensing for Long-Range Targets in Integrated Sensing and Communication System", "comment": "15 pages, 10 figures", "summary": "Integrated sensing and communication (ISAC) is a promising candidate\ntechnology for 6G due to its improvement in spectral efficiency and energy\nefficiency. Orthogonal frequency division multiplexing (OFDM) signal is a\nmainstream candidate ISAC waveform. However, there are inter-symbol\ninterference (ISI) and inter-carrier interference (ICI) when the round-trip\ndelay exceeds the cyclic prefix (CP) duration for OFDM signals, which limits\nthe maximum sensing range of ISAC system. When detecting a long-range target,\nthe wide beam inevitably covers the close-range target, of which the echo's\npower is much larger than that of the long-range target. In order to tackle the\nabove problem, a multiple signal classification (MUSIC) and least squares\n(LS)-based spatial signal separation method is proposed to separate the echo\nsignals reflected from different targets. Moreover, a coherent\ncompensation-based sensing signal processing method at the receiver is proposed\nto enhance the signal to interference plus noise power ratio (SINR) of the OFDM\nblock for generating the range-Doppler map (RDM) with higher SINR. Simulation\nresults reveal that the proposed method greatly enhances the SINR of RDM by 10\ndB for a target at 500 m compared with two-dimensional fast Fourier transform\n(2D-FFT) method. Besides, the detection probability is also significantly\nimproved compared to the benchmarking method.", "AI": {"tldr": "\u57fa\u4e8eMUSIC\u548cLS\u7684\u7a7a\u95f4\u4fe1\u53f7\u5206\u79bb\u7b56\u7565\u4e0e\u76f8\u5e72\u8865\u507f\u5904\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3OFDM\u4f5c\u4e3aISAC\u6ce2\u5f62\u65f6\u7684\u8ddd\u79bb\u9650\u5236\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fdc\u7a0b\u76ee\u6807\u7684SINR\u548c\u68c0\u6d4b\u6982\u7387", "motivation": "\u89e3\u51b3OFDM\u4f5c\u4e3aISAC\u6ce2\u5f62\u65f6\u56e0\u5faa\u73af\u524d\u7f00(CP)\u65f6\u957f\u9650\u5236\u5bfc\u81f4\u7684\u6700\u5927\u611f\u77e5\u8ddd\u79bb\u9650\u5236\uff0c\u4ee5\u53ca\u8fdc\u8ddd\u79bb\u76ee\u6807\u88ab\u8fd1\u8ddd\u79bb\u76ee\u6807\u5f3a\u56de\u6ce2\u5e72\u6270\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u57fa\u4e8eMUSIC\u548c\u6700\u5c0f\u4e8c\u4e58(LS)\u7684\u7a7a\u95f4\u4fe1\u53f7\u5206\u79bb\u65b9\u6cd5\u6765\u5206\u79bb\u4e0d\u540c\u76ee\u6807\u7684\u56de\u6ce2\u4fe1\u53f7\uff0c\u4ee5\u53ca\u57fa\u4e8e\u76f8\u5e72\u8865\u507f\u7684\u611f\u77e5\u4fe1\u53f7\u5904\u7406\u65b9\u6cd5\u6765\u63d0\u5347SINR", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u5728500\u7c73\u8fdc\u7a0b\u76ee\u6807\u4e0a\uff0c\u65b9\u6cd5\u6bd4\u4f20\u7edf\u4e8c\u7ef4\u5feb\u901f\u5f52\u4e00\u53d8\u6362(2D-FFT)\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86RDM\u7684SINR 10dB\uff0c\u68c0\u6d4b\u6982\u7387\u4e5f\u663e\u8457\u63d0\u9ad8", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ISAC\u7cfb\u7edf\u4e2dOFDM\u6ce2\u5f62\u7684\u8ddd\u79bb\u9650\u5236\u95ee\u9898\uff0c\u4e3a6G\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12104", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12104", "abs": "https://arxiv.org/abs/2508.12104", "authors": ["Shane Waxler", "Paul Blazek", "Davis White", "Daniel Sneider", "Kevin Chung", "Mani Nagarathnam", "Patrick Williams", "Hank Voeller", "Karen Wong", "Matthew Swanhorst", "Sheng Zhang", "Naoto Usuyama", "Cliff Wong", "Tristan Naumann", "Hoifung Poon", "Andrew Loza", "Daniella Meeker", "Seth Hain", "Rahul Shah"], "title": "Generative Medical Event Models Improve with Scale", "comment": null, "summary": "Realizing personalized medicine at scale calls for methods that distill\ninsights from longitudinal patient journeys, which can be viewed as a sequence\nof medical events. Foundation models pretrained on large-scale medical event\ndata represent a promising direction for scaling real-world evidence generation\nand generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with\nmedical events from de-identified longitudinal health records for 16.3 billion\nencounters over 300 million unique patient records from 310 health systems, we\nintroduce the Cosmos Medical Event Transformer ( CoMET) models, a family of\ndecoder-only transformer models pretrained on 118 million patients representing\n115 billion discrete medical events (151 billion tokens). We present the\nlargest scaling-law study for medical event data, establishing a methodology\nfor pretraining and revealing power-law scaling relationships for compute,\ntokens, and model size. Based on this, we pretrained a series of\ncompute-optimal models with up to 1 billion parameters. Conditioned on a\npatient's real-world history, CoMET autoregressively generates the next medical\nevent, simulating patient health timelines. We studied 78 real-world tasks,\nincluding diagnosis prediction, disease prognosis, and healthcare operations.\nRemarkably for a foundation model with generic pretraining and simulation-based\ninference, CoMET generally outperformed or matched task-specific supervised\nmodels on these tasks, without requiring task-specific fine-tuning or few-shot\nexamples. CoMET's predictive power consistently improves as the model and\npretraining scale. Our results show that CoMET, a generative medical event\nfoundation model, can effectively capture complex clinical dynamics, providing\nan extensible and generalizable framework to support clinical decision-making,\nstreamline healthcare operations, and improve patient outcomes.", "AI": {"tldr": "CoMET\u662f\u57fa\u4e8e160\u4ebf\u6b21\u533b\u7597\u4e8b\u4ef6\u8bad\u7ec3\u7684\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u751f\u6210\u6a21\u62df\u60a3\u8005\u5065\u5eb7\u65f6\u95f4\u7ebf\uff0c\u572878\u4e2a\u533b\u7597\u4efb\u52a1\u4e2d\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u4e13\u7528\u76d1\u7763\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5b9e\u73b0\u89c4\u6a21\u5316\u4e2a\u6027\u5316\u533b\u7597\u9700\u8981\u4ece\u7eb5\u5411\u60a3\u8005\u65c5\u7a0b\u4e2d\u63d0\u53d6\u6d1e\u5bdf\uff0c\u5927\u89c4\u6a21\u533b\u7597\u4e8b\u4ef6\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u6709\u671b\u6269\u5c55\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u751f\u6210\u5e76\u6cdb\u5316\u5230\u591a\u6837\u5316\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u4f7f\u7528Epic Cosmos\u6570\u636e\u96c6\uff08163\u4ebf\u6b21\u5c31\u8bca\u30013\u4ebf\u60a3\u8005\u8bb0\u5f55\uff09\uff0c\u8bad\u7ec3\u89e3\u7801\u5668Transformer\u6a21\u578bCoMET\uff0c\u8fdb\u884c\u533b\u7597\u4e8b\u4ef6\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5efa\u7acb\u8ba1\u7b97\u6700\u4f18\u7684\u7f29\u653e\u5b9a\u5f8b\u6a21\u578b\u3002", "result": "\u5728\u8bca\u65ad\u9884\u6d4b\u3001\u75be\u75c5\u9884\u540e\u548c\u533b\u7597\u8fd0\u8425\u7b4978\u4e2a\u4efb\u52a1\u4e2d\uff0cCoMET\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u6216few-shot\u793a\u4f8b\uff0c\u666e\u904d\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u7528\u76d1\u7763\u6a21\u578b\uff0c\u9884\u6d4b\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u548c\u9884\u8bad\u7ec3\u89c4\u6a21\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "CoMET\u751f\u6210\u5f0f\u533b\u7597\u4e8b\u4ef6\u57fa\u7840\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u4e34\u5e8a\u52a8\u6001\uff0c\u4e3a\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3001\u7b80\u5316\u533b\u7597\u8fd0\u8425\u548c\u6539\u5584\u60a3\u8005\u7ed3\u5c40\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u53ef\u6cdb\u5316\u7684\u6846\u67b6\u3002"}}
{"id": "2508.12403", "categories": ["eess.SP", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.12403", "abs": "https://arxiv.org/abs/2508.12403", "authors": ["Federico Miotello", "Davide Albertini", "Alberto Bernardini"], "title": "On the Extension of Differential Beamforming Theory to Arbitrary Planar Arrays of First-Order Elements", "comment": null, "summary": "Small-size acoustic arrays exploit spatial diversity to achieve capabilities\nbeyond those of single-element devices, with applications ranging from\nteleconferencing to immersive multimedia. A key requirement for broadband array\nprocessing is a frequency-invariant spatial response, which ensures consistent\ndirectivity across wide bandwidths and prevents spectral coloration.\nDifferential beamforming offers an inherently frequency-invariant solution by\nleveraging pressure differences between closely spaced elements of small-size\narrays. Traditional approaches, however, assume the array elements to be\nomnidirectional, whereas real transducers exhibit frequency-dependent\ndirectivity that can degrade performance if not properly modeled. To address\nthis limitation, we propose a generalized modal matching framework for\nfrequency-invariant differential beamforming, applicable to unconstrained\nplanar arrays of first-order directional elements. By representing the desired\nbeampattern as a truncated circular harmonic expansion and fitting it to the\nactual element responses, our method accommodates arbitrary planar geometries\nand element orientations. This approach enables the synthesis of beampatterns\nof any order and steering direction without imposing rigid layout requirements.\nSimulations confirm that accounting for sensor directivity at the design stage\nyields accurate and robust performance across varying frequencies, geometries,\nand noise conditions.", "AI": {"tldr": "\u57fa\u4e8e\u73af\u5f62\u8c03\u548c\u5c55\u5f00\u7684\u5e7f\u4e49\u6a21\u6001\u5339\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u5e26\u6709\u5411\u6027\u4f20\u611f\u5668\u7684\u5e73\u9762\u6570\u7ec4\u9891\u7387\u4e0d\u53d8\u5fae\u5206\u6ce2\u675f\u5f62\u6210", "motivation": "\u4f20\u7edf\u5dee\u5206\u6ce2\u675f\u5f62\u6210\u6280\u672f\u5047\u8bbe\u4f20\u611f\u5668\u5168\u5411\u6027\uff0c\u800c\u5b9e\u9645\u4f20\u611f\u5668\u5b58\u5728\u9891\u7387\u76f8\u5173\u7684\u5411\u6027\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u964d\u7ea7", "method": "\u901a\u8fc7\u5c06\u671f\u671b\u6ce2\u675f\u56fe\u8868\u793a\u4e3a\u622a\u65ad\u7684\u73af\u5f62\u8c03\u548c\u5c55\u5f00\uff0c\u5e76\u4e0e\u5b9e\u9645\u5143\u4ef6\u54cd\u5e94\u8fdb\u884c\u62df\u5408\uff0c\u652f\u6301\u4efb\u610f\u5e73\u9762\u5e03\u5c40\u548c\u5143\u4ef6\u65b9\u4f4d", "result": "\u6a21\u62df\u7ed3\u679c\u8bc1\u660e\uff0c\u5728\u8bbe\u8ba1\u9636\u6bb5\u8003\u8651\u4f20\u611f\u5668\u5411\u6027\u6027\u80fd\u591f\u5728\u4e0d\u540c\u9891\u7387\u3001\u4e0d\u540c\u5e03\u5c40\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u5b9e\u73b0\u51c6\u786e\u800c\u7a33\u5065\u7684\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u5141\u8bb8\u5408\u6210\u4efb\u610f\u9636\u6570\u548c\u626d\u8f6c\u65b9\u5411\u7684\u6ce2\u675f\u56fe\uff0c\u65e0\u9700\u4e25\u683c\u7684\u6570\u7ec4\u5e03\u5c40\u8981\u6c42\uff0c\u4e3a\u5e26\u5411\u6027\u4f20\u611f\u5668\u7684\u5e7f\u5e26\u5dee\u5206\u6ce2\u675f\u5f62\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12116", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12116", "abs": "https://arxiv.org/abs/2508.12116", "authors": ["Haebin Shin", "Lei Ji", "Xiao Liu", "Zhiwei Yu", "Qi Chen", "Yeyun Gong"], "title": "DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections", "comment": null, "summary": "As numerous instruction-tuning datasets continue to emerge during the\npost-training stage, dynamically balancing and optimizing their mixtures has\nbecome a critical challenge. To address this, we propose DynamixSFT, a dynamic\nand automated method for instruction-tuning dataset mixture optimization. We\nformulate the problem as a multi-armed bandit setup and introduce a\nPrior-scaled Boltzmann Exploration that softly anchors the updated sampling\ndistribution to the original dataset proportions, thereby preserving the\ninherent diversity and coverage of the collection. Sampling probabilities are\nupdated using a lightweight 1-Step Look-ahead Reward, reflecting how much the\ndataset contributes to improving the model's performance at its current state.\nWhen applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning\ndatasets, DynamixSFT achieves up to a 2.2% performance improvement across 10\nbenchmarks. Furthermore, we provide a comprehensive analysis and visualizations\nto offer deeper insights into the adaptive dynamics of our method.", "AI": {"tldr": "DynamixSFT\u662f\u4e00\u79cd\u52a8\u6001\u81ea\u52a8\u5316\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u6df7\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u548c\u5148\u9a8c\u7f29\u653e\u73bb\u5c14\u5179\u66fc\u63a2\u7d22\uff0c\u5728Tulu-v2\u6570\u636e\u96c6\u96c6\u5408\u4e0a\u5b9e\u73b0\u4e862.2%\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u968f\u7740\u540e\u8bad\u7ec3\u9636\u6bb5\u5927\u91cf\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u7684\u51fa\u73b0\uff0c\u5982\u4f55\u52a8\u6001\u5e73\u8861\u548c\u4f18\u5316\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u6df7\u5408\u6bd4\u4f8b\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\uff0c\u63d0\u51fa\u5148\u9a8c\u7f29\u653e\u73bb\u5c14\u5179\u66fc\u63a2\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea71\u6b65\u524d\u77bb\u5956\u52b1\u6765\u66f4\u65b0\u91c7\u6837\u6982\u7387\uff0c\u4fdd\u6301\u539f\u59cb\u6570\u636e\u96c6\u6bd4\u4f8b\u7684\u8f6f\u951a\u5b9a", "result": "\u5728\u5305\u542b16\u4e2a\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u7684Tulu-v2\u6df7\u5408\u96c6\u5408\u4e0a\uff0cDynamixSFT\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad82.2%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f18\u5316\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\u7684\u6df7\u5408\u6bd4\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u8986\u76d6\u8303\u56f4\uff0c\u4e3a\u81ea\u9002\u5e94\u52a8\u6001\u4f18\u5316\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3"}}
{"id": "2508.12614", "categories": ["eess.SP", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12614", "abs": "https://arxiv.org/abs/2508.12614", "authors": ["Zhongqin Wang", "J. Andrew Zhang", "Kai Wu", "Min Xu", "Y. Jay Guo"], "title": "Towards SISO Bistatic Sensing for ISAC", "comment": null, "summary": "Integrated Sensing and Communication (ISAC) is a key enabler for\nnext-generation wireless systems. However, real-world deployment is often\nlimited to low-cost, single-antenna transceivers. In such bistatic Single-Input\nSingle-Output (SISO) setup, clock asynchrony introduces random phase offsets in\nChannel State Information (CSI), which cannot be mitigated using conventional\nmulti-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic\nSISO sensing framework that enables accurate delay and Doppler estimation from\ndistorted CSI by effectively suppressing Doppler mirroring ambiguity. It\noperates with only a single antenna at both the transmitter and receiver,\nmaking it suitable for low-complexity deployments. We propose a\nself-referencing cross-correlation (SRCC) method for SISO random phase removal\nand employ delay-domain beamforming to resolve Doppler ambiguity. The resulting\nunambiguous delay-Doppler-time features enable robust sensing with compact\nneural networks. Extensive experiments show that WiDFS 3.0 achieves accurate\nparameter estimation, with performance comparable to or even surpassing that of\nprior multi-antenna methods, especially in delay estimation. Validated under\nsingle- and multi-target scenarios, the extracted ambiguity-resolved features\nshow strong sensing accuracy and generalization. For example, when deployed on\nthe embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0\nconsistently outperforms conventional features such as CSI amplitude, mirrored\nDoppler, and multi-receiver aggregated Doppler.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86WiDFS 3.0\u8f7b\u91cf\u7ea7\u53cc\u57fa\u7ad9SISO\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u53c2\u8003\u4e92\u76f8\u5173\u548c\u5ef6\u8fdf\u57df\u6ce2\u675f\u6210\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u5929\u7ebf\u914d\u7f6e\u4e0b\u7684\u949f\u6b65\u5f02\u6b65\u548c\u591a\u666e\u52d2\u955c\u50cf\u6b67\u4e49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u4f30\u8ba1\u3002", "motivation": "\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u662f\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u5e38\u53d7\u9650\u4e8e\u4f4e\u6210\u672c\u5355\u5929\u7ebf\u6536\u53d1\u5668\u3002\u5728\u8fd9\u79cd\u53cc\u57fa\u7ad9SISO\u914d\u7f6e\u4e0b\uff0c\u949f\u6b65\u5f02\u6b65\u4f1a\u5bfc\u81f4\u901a\u9053\u72b6\u6001\u4fe1\u606f(CSI)\u4e2d\u51fa\u73b0\u968f\u673a\u76f8\u4f4d\u504f\u79fb\uff0c\u65e2\u5f80\u7684\u591a\u5929\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u53c2\u8003\u4e92\u76f8\u5173(SRCC)\u65b9\u6cd5\u6765\u6d88\u9664SISO\u4e2d\u7684\u968f\u673a\u76f8\u4f4d\u504f\u79fb\uff0c\u5e76\u91c7\u7528\u5ef6\u8fdf\u57df\u6ce2\u675f\u6210\u6280\u672f\u6765\u89e3\u51b3\u591a\u666e\u52d2\u6b67\u4e49\u95ee\u9898\u3002\u901a\u8fc7\u63d0\u53d6\u65e0\u6b67\u4e49\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2-\u65f6\u95f4\u7279\u5f81\uff0c\u4f7f\u7528\u7b80\u6d01\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u7a33\u5065\u611f\u77e5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aWiDFS 3.0\u80fd\u591f\u5b9e\u73b0\u7cbe\u51c6\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u6027\u80fd\u53ef\u4e0e\u751a\u81f3\u8d85\u8fc7\u4e4b\u524d\u7684\u591a\u5929\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c24\u5176\u662f\u5728\u5ef6\u8fdf\u4f30\u8ba1\u65b9\u9762\u3002\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u611f\u77e5\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4f8b\u5982\uff0c\u5728\u4ec5\u67091.3M\u53c2\u6570\u7684MobileViT-XXS\u4e0a\u90e8\u7f72\u65f6\uff0c\u6027\u80fd\u4ecd\u7136\u8d85\u8fc7\u4f20\u7edf\u7684CSI\u632f\u5e45\u3001\u955c\u50cf\u591a\u666e\u52d2\u548c\u591a\u6536\u53d1\u5668\u805a\u5408\u591a\u666e\u52d2\u7b49\u7279\u5f81\u3002", "conclusion": "WiDFS 3.0\u4e3a\u4f4e\u590d\u6742\u5ea6\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u76f8\u4f4d\u6d88\u9664\u548c\u6b67\u4e49\u89e3\u51b3\u6280\u672f\uff0c\u5728\u5355\u5929\u7ebf\u914d\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u591a\u5929\u7ebf\u7cfb\u7edf\u76f8\u5f53\u7684\u611f\u77e5\u6027\u80fd\uff0c\u4e3aISAC\u6280\u672f\u7684\u666e\u53ca\u548c\u5e94\u7528\u63a8\u5e7f\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6491\u3002"}}
{"id": "2508.12121", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2508.12121", "abs": "https://arxiv.org/abs/2508.12121", "authors": ["Lorenzo Livi"], "title": "Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks", "comment": null, "summary": "We study how gating mechanisms in recurrent neural networks (RNNs) implicitly\ninduce adaptive learning-rate behavior, even when training is carried out with\na fixed, global learning rate. This effect arises from the coupling between\nstate-space time scales--parametrized by the gates--and parameter-space\ndynamics during gradient descent. By deriving exact Jacobians for\nleaky-integrator and gated RNNs, we obtain a first-order expansion that makes\nexplicit how constant, scalar, and multi-dimensional gates reshape gradient\npropagation, modulate effective step sizes, and introduce anisotropy in\nparameter updates. These findings reveal that gates not only control memory\nretention in the hidden states, but also act as data-driven preconditioners\nthat adapt optimization trajectories in parameter space. We further draw formal\nanalogies with learning-rate schedules, momentum, and adaptive methods such as\nAdam, showing that these optimization behaviors emerge naturally from gating.\nNumerical experiments confirm the validity of our perturbative analysis,\nsupporting the view that gate-induced corrections remain small while exerting\nsystematic effects on training dynamics. Overall, this work provides a unified\ndynamical-systems perspective on how gating couples state evolution with\nparameter updates, explaining why gated architectures achieve robust\ntrainability and stability in practice.", "AI": {"tldr": "\u95e8\u63a7\u673a\u5236\u5728RNN\u4e2d\u901a\u8fc7\u8026\u5408\u72b6\u6001\u7a7a\u95f4\u65f6\u95f4\u5c3a\u5ea6\u4e0e\u53c2\u6570\u7a7a\u95f4\u52a8\u529b\u5b66\uff0c\u9690\u5f0f\u5730\u4ea7\u751f\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u884c\u4e3a\uff0c\u5373\u4f7f\u4f7f\u7528\u56fa\u5b9a\u5168\u5c40\u5b66\u4e60\u7387\u8bad\u7ec3\u65f6\u4e5f\u662f\u5982\u6b64", "motivation": "\u7814\u7a76\u95e8\u63a7RNN\u4e2d\u95e8\u63a7\u673a\u5236\u5982\u4f55\u5f71\u54cd\u68af\u5ea6\u4f20\u64ad\u548c\u53c2\u6570\u66f4\u65b0\uff0c\u63ed\u793a\u95e8\u63a7\u4e0d\u4ec5\u63a7\u5236\u9690\u85cf\u72b6\u6001\u8bb0\u5fc6\u4fdd\u7559\uff0c\u8fd8\u4f5c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u9884\u5904\u7406\u5668\u8c03\u8282\u4f18\u5316\u8f68\u8ff9", "method": "\u901a\u8fc7\u63a8\u5bfc\u6cc4\u6f0f\u79ef\u5206\u5668\u548c\u95e8\u63a7RNN\u7684\u7cbe\u786e\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u83b7\u5f97\u4e00\u9636\u5c55\u5f00\u5f0f\uff0c\u5206\u6790\u6807\u91cf\u548c\u591a\u7ef4\u95e8\u63a7\u5982\u4f55\u91cd\u5851\u68af\u5ea6\u4f20\u64ad\u3001\u8c03\u8282\u6709\u6548\u6b65\u957f\u5e76\u5f15\u5165\u53c2\u6570\u66f4\u65b0\u5404\u5411\u5f02\u6027", "result": "\u95e8\u63a7\u673a\u5236\u81ea\u7136\u5730\u4ea7\u751f\u4e86\u5b66\u4e60\u7387\u8c03\u5ea6\u3001\u52a8\u91cf\u548c\u81ea\u9002\u5e94\u65b9\u6cd5\uff08\u5982Adam\uff09\u7b49\u4f18\u5316\u884c\u4e3a\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5fae\u6270\u5206\u6790\u7684\u6709\u6548\u6027", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\u89c6\u89d2\uff0c\u89e3\u91ca\u4e86\u95e8\u63a7\u5982\u4f55\u8026\u5408\u72b6\u6001\u6f14\u5316\u4e0e\u53c2\u6570\u66f4\u65b0\uff0c\u8bf4\u660e\u4e86\u95e8\u63a7\u67b6\u6784\u5728\u5b9e\u8df5\u4e2d\u5b9e\u73b0\u9c81\u68d2\u53ef\u8bad\u7ec3\u6027\u548c\u7a33\u5b9a\u6027\u7684\u539f\u56e0"}}
{"id": "2508.12660", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12660", "abs": "https://arxiv.org/abs/2508.12660", "authors": ["Yezhuo Zhang", "Zinan Zhou", "Guangyu Li", "Xuanpeng Li"], "title": "Factorized Disentangled Representation Learning for Interpretable Radio Frequency Fingerprint", "comment": "14 pages, 8 figures", "summary": "In response to the rapid growth of Internet of Things (IoT) devices and\nrising security risks, Radio Frequency Fingerprint (RFF) has become key for\ndevice identification and authentication. However, various changing factors -\nbeyond the RFF itself - can be entangled from signal transmission to reception,\nreducing the effectiveness of RFF Identification (RFFI). Existing RFFI methods\nmainly rely on domain adaptation techniques, which often lack explicit factor\nrepresentations, resulting in less robustness and limited controllability for\ndownstream tasks. To tackle this problem, we propose a novel Disentangled\nRepresentation Learning (DRL) framework that learns explicit and independent\nrepresentations of multiple factors, including the RFF. Our framework\nintroduces modules for disentanglement, guided by the principles of\nexplicitness, modularity, and compactness. We design two dedicated modules for\nfactor classification and signal reconstruction, each with tailored loss\nfunctions that encourage effective disentanglement and enhance support for\ndownstream tasks. Thus, the framework can extract a set of interpretable\nvectors that explicitly represent corresponding factors. We evaluate our\napproach on two public benchmark datasets and a self-collected dataset. Our\nmethod achieves impressive performance on multiple DRL metrics. We also analyze\nthe effectiveness of our method on downstream RFFI task and conditional signal\ngeneration task. All modules of the framework contribute to improved\nclassification accuracy, and enable precise control over conditional generated\nsignals. These results highlight the potential of our DRL framework for\ninterpretable and explicit RFFs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u65e0\u7ebf\u7535\u4fe1\u53f7\u4e2d\u63d0\u53d6\u660e\u786e\u3001\u72ec\u7acb\u7684\u591a\u56e0\u7d20\u8868\u793a\uff0c\u5305\u62ec\u8bbe\u5907\u7684\u65e0\u7ebf\u7535\u989c\u7eb9\u7279\u5f81\uff0c\u4ee5\u63d0\u9ad8IoT\u8bbe\u5907\u8bc6\u522b\u7684\u7a33\u5065\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u7ebf\u7535\u989c\u7eb9\u7279\u5f81\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u57df\u9002\u914d\u6280\u672f\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u56e0\u7d20\u8868\u793a\uff0c\u5bfc\u81f4\u7a33\u5065\u6027\u4e0d\u8db3\u548c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u63a7\u5236\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u660e\u786e\u89e3\u8026\u591a\u4e2a\u56e0\u7d20\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8RFFI\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e24\u4e2a\u4e13\u95e8\u6a21\u5757\uff08\u56e0\u7d20\u5206\u7c7b\u548c\u4fe1\u53f7\u91cd\u6784\uff09\u6765\u5b9e\u73b0\u660e\u786e\u6027\u3001\u6a21\u5757\u5316\u548c\u7d27\u51d1\u6027\u7684\u89e3\u8026\u539f\u5219\u3002\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u7279\u5236\u7684\u635f\u5931\u51fd\u6570\u6765\u4fc3\u8fdb\u6709\u6548\u89e3\u8026\u5e76\u589e\u5f3a\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u652f\u6301\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5728\u591a\u4e2aDRL\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u7ee9\u3002\u5728\u4e0b\u6e38RFFI\u4efb\u52a1\u548c\u6761\u4ef6\u4fe1\u53f7\u751f\u6210\u4efb\u52a1\u4e0a\u90fd\u663e\u793a\u51fa\u6709\u6548\u6027\uff0c\u6240\u6709\u6a21\u5757\u90fd\u6709\u52a9\u4e8e\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u591f\u5b9e\u73b0\u5bf9\u6761\u4ef6\u751f\u6210\u4fe1\u53f7\u7684\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "\u8be5\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6846\u67b6\u4e3a\u89e3\u91ca\u6027\u548c\u660e\u786e\u7684\u65e0\u7ebf\u7535\u989c\u7eb9\u7279\u5f81\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u901a\u8fc7\u660e\u786e\u89e3\u8026\u591a\u79cd\u56e0\u7d20\u6765\u63d0\u9ad8IoT\u8bbe\u5907\u8bc6\u522b\u7684\u7a33\u5065\u6027\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2508.12145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12145", "abs": "https://arxiv.org/abs/2508.12145", "authors": ["Frederik L. Dennig", "Daniel A. Keim"], "title": "DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy", "comment": "5 pages, 3 figures, LaTeX", "summary": "Recently, autoencoders (AEs) have gained interest for creating parametric and\ninvertible projections of multidimensional data. Parametric projections make it\npossible to embed new, unseen samples without recalculating the entire\nprojection, while invertible projections allow the synthesis of new data\ninstances. However, existing methods perform poorly when dealing with\nout-of-distribution samples in either the data or embedding space. Thus, we\npropose DE-VAE, an uncertainty-aware variational AE using differential entropy\n(DE) to improve the learned parametric and invertible projections. Given a\nfixed projection, we train DE-VAE to learn a mapping into 2D space and an\ninverse mapping back to the original space. We conduct quantitative and\nqualitative evaluations on four well-known datasets, using UMAP and t-SNE as\nbaseline projection methods. Our findings show that DE-VAE can create\nparametric and inverse projections with comparable accuracy to other current\nAE-based approaches while enabling the analysis of embedding uncertainty.", "AI": {"tldr": "DE-VAE\u662f\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u71b5\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u6539\u8fdb\u53c2\u6570\u5316\u548c\u53ef\u9006\u6295\u5f71\uff0c\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u7a7a\u95f4\u6216\u5d4c\u5165\u7a7a\u95f4\u7684\u5206\u5e03\u5916\u6837\u672c\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u7684\u53c2\u6570\u5316\u548c\u53ef\u9006\u6295\u5f71\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5fae\u5206\u71b5\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08DE-VAE\uff09\uff0c\u5728\u56fa\u5b9a\u6295\u5f71\u4e0b\u5b66\u4e60\u52302D\u7a7a\u95f4\u7684\u6620\u5c04\u53ca\u5176\u9006\u6620\u5c04\u56de\u539f\u59cb\u7a7a\u95f4\u3002", "result": "\u5728\u56db\u4e2a\u77e5\u540d\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0cDE-VAE\u80fd\u591f\u521b\u5efa\u4e0e\u5176\u4ed6\u5f53\u524dAE\u65b9\u6cd5\u7cbe\u5ea6\u76f8\u5f53\u7684\u53c2\u6570\u5316\u548c\u9006\u6295\u5f71\uff0c\u540c\u65f6\u652f\u6301\u5d4c\u5165\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u3002", "conclusion": "DE-VAE\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5206\u5e03\u5916\u6837\u672c\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u65e2\u51c6\u786e\u53c8\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u80fd\u529b\u7684\u53c2\u6570\u5316\u53ef\u9006\u6295\u5f71\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12689", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12689", "abs": "https://arxiv.org/abs/2508.12689", "authors": ["Ning Gao", "Tianrui Zeng", "Bowen Chen", "Donghong Cai", "Shi Jin", "Michail Matthaiou"], "title": "Multi-Domain Supervised Contrastive Learning for UAV Radio-Frequency Open-Set Recognition", "comment": null, "summary": "5G-Advanced (5G-A) has enabled the vibrant development of low altitude\nintegrated sensing and communication (LA-ISAC) networks. As a core component of\nthese networks, unmanned aerial vehicles (UAVs) have witnessed rapid growth in\nrecent years. However, due to the lag in traditional industry regulatory norms,\nunauthorized flight incidents occur frequently, posing a severe security threat\nto LA-ISAC networks. To surveil the non-cooperative UAVs, in this paper, we\npropose a multi-domain supervised contrastive learning (MD-SupContrast)\nframework for UAV radio frequency (RF) open-set recognition. Specifically,\nfirst, the texture features and the time-frequency position features from the\nResNet and the TransformerEncoder are fused, and then the supervised\ncontrastive learning is applied to optimize the feature representation of the\nclosed-set samples. Next, to surveil the invasive UAVs that appear in real\nlife, we propose an improved generative OpenMax (IG-OpenMax) algorithm and\nconstruct an open-set recognition model, namely Open-RFNet. According to the\nunknown samples, we first freeze the feature extraction layers and then only\nretrain the classification layer, which achieves excellent recognition\nperformance both in closed-set and open-set recognitions. We analyze the\ncomputational complexity of the proposed model. Experiments are conducted with\na large-scale UAV open dataset. The results show that the proposed Open-RFNet\noutperforms the existing benchmark methods in terms of recognition accuracy\nbetween the known and the unknown UAVs, as it achieves 95.12% in closed-set and\n96.08% in open-set under 25 UAV types, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u57df\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u5f00\u6536\u96c6\u8bc6\u522b\u6846\u67b6Open-RFNet\uff0c\u901a\u8fc7\u878d\u5408ResNet\u548cTransformerEncoder\u7279\u5f81\uff0c\u4f7f\u7528\u6539\u8fdb\u751f\u6210\u5f0fOpenMax\u7b97\u6cd5\uff0c\u572825\u79cd\u65e0\u4eba\u673a\u7c7b\u578b\u4e0a\u5b9e\u73b0\u4e86\u95ed\u96c6\u8bc6\u522b95.12%\u548c\u5f00\u96c6\u8bc6\u522b96.08%\u7684\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b35G-\u8fdb\u9636\u7f51\u7edc\u4e2d\u975e\u6cd5\u65e0\u4eba\u673a\u98de\u884c\u4e8b\u4ef6\u9891\u53d1\u5bf9\u4f4e\u7a7a\u57df\u611f\u77e5\u901a\u4fe1\u7f51\u7edc\u9020\u6210\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u6709\u6548\u76d1\u63a7\u975e\u534f\u4f5c\u65e0\u4eba\u673a\u3002", "method": "\u63d0\u51faMD-SupContrast\u6846\u67b6\uff1a1)\u878d\u5408ResNet\u7684\u7eb9\u7406\u7279\u5f81\u548cTransformerEncoder\u7684\u65f6\u9891\u4f4d\u7f6e\u7279\u5f81 2)\u4f7f\u7528\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\u95ed\u96c6\u6837\u672c\u7279\u5f81\u8868\u5f81 3)\u63d0\u51fa\u6539\u8fdb\u751f\u6210\u5f0fIG-OpenMax\u7b97\u6cd5\u6784\u5efaOpen-RFNet\u6a21\u578b 4)\u51bb\u7ed3\u7279\u5f81\u63d0\u53d6\u5c42\uff0c\u91cd\u65b0\u8bad\u7ec3\u5206\u7c7b\u5c42\u5904\u7406\u672a\u77e5\u6837\u672c", "result": "\u5728\u5927\u89c4\u6a21\u65e0\u4eba\u673a\u5f00\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cOpen-RFNet\u572825\u79cd\u65e0\u4eba\u673a\u7c7b\u578b\u4e0a\u5b9e\u73b0\uff1a\u95ed\u96c6\u8bc6\u522b\u51c6\u786e\u738795.12%\uff0c\u5f00\u96c6\u8bc6\u522b\u51c6\u786e\u738796.08%\uff0c\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u76d1\u63a7LA-ISAC\u7f51\u7edc\u4e2d\u7684\u975e\u6cd5\u65e0\u4eba\u673a\uff0c\u901a\u8fc7\u591a\u57df\u7279\u5f81\u878d\u5408\u548c\u6539\u8fdb\u5f00\u6536\u96c6\u8bc6\u522b\u7b97\u6cd5\uff0c\u5728\u95ed\u96c6\u548c\u5f00\u96c6\u573a\u666f\u4e0b\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2508.12162", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12162", "abs": "https://arxiv.org/abs/2508.12162", "authors": ["J. M. I. H. Jayakody", "A. M. H. H. Alahakoon", "C. R. M. Perera", "R. M. L. C. Srimal", "Roshan Ragel", "Vajira Thambawita", "Isuru Nawinne"], "title": "AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis", "comment": null, "summary": "The paradigm of electrocardiogram (ECG) analysis has evolved into real-time\ndigital analysis, facilitated by artificial intelligence (AI) and machine\nlearning (ML), which has improved the diagnostic precision and predictive\ncapacity of cardiac diseases. This work proposes a novel deep learning (DL)\narchitecture called the attention-integrated convolutional residual network\n(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,\nthe QRS duration, the heart rate, the peak amplitude of the R wave, and the\namplitude of the T wave for interpretable ECG analysis. Our architecture is\nspecially designed with spatial and channel attention-related mechanisms to\naddress the type and spatial location of the ECG features for regression. The\nmodels employ a convolutional residual network to address vanishing and\nexploding gradient problems. The designed system addresses traditional analysis\nchallenges, such as loss of focus due to human errors, and facilitates the fast\nand easy detection of cardiac events, thereby reducing the manual efforts\nrequired to solve analysis tasks. AICRN models outperform existing models in\nparameter regression with higher precision. This work demonstrates that DL can\nplay a crucial role in the interpretability and precision of ECG analysis,\nopening up new clinical applications for cardiac monitoring and management.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bAICRN\uff0c\u901a\u8fc7\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc\u6765\u63d0\u9ad8\u5fc3\u7535\u56fe\u53c2\u6570\u56de\u5f52\u7684\u7cbe\u786e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5fc3\u7535\u56fe\u5206\u6790\u4e2d\u7684\u4eba\u4e3a\u8bef\u5dee\u3001\u5173\u6ce8\u70b9\u5931\u3001\u624b\u52a8\u5206\u6790\u82e6\u96be\u7b49\u6311\u6218\uff0c\u901a\u8fc7AI\u6280\u672f\u63d0\u9ad8\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u7684\u7cbe\u786e\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u6ce8\u610f\u529b\u96c6\u6210\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc(AICRN)\uff0c\u5305\u542b\u7a7a\u95f4\u548c\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e13\u95e8\u5904\u7406\u5fc3\u7535\u56fe\u7279\u5f81\u7684\u7c7b\u578b\u548c\u7a7a\u95f4\u4f4d\u7f6e\u3002\u4f7f\u7528\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc\u89e3\u51b3\u6838\u5fc3\u95ee\u9898\u3002", "result": "AICRN\u6a21\u578b\u5728\u5fc3\u7535\u56fe\u53c2\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u8868\u73b0\u8d85\u8fc7\u73b0\u6709\u6a21\u578b\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u786e\u5ea6\u3002\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u5fc3\u810f\u4e8b\u4ef6\uff0c\u51cf\u5c11\u624b\u52a8\u5206\u6790\u7684\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5fc3\u7535\u56fe\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7cbe\u786e\u6027\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5fc3\u810f\u76d1\u6d4b\u548c\u7ba1\u7406\u5f00\u542f\u4e86\u65b0\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.12728", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12728", "abs": "https://arxiv.org/abs/2508.12728", "authors": ["Yunsong Huang", "Hui-Ming Wang", "Qingli Yan", "Zhaowei Wang"], "title": "LLM-RIMSA: Large Language Models driven Reconfigurable Intelligent Metasurface Antenna Systems", "comment": null, "summary": "The evolution of 6G networks demands ultra-massive connectivity and\nintelligent radio environments, yet existing reconfigurable intelligent surface\n(RIS) technologies face critical limitations in hardware efficiency, dynamic\ncontrol, and scalability. This paper introduces LLM-RIMSA, a transformative\nframework that integrates large language models (LLMs) with a novel\nreconfigurable intelligent metasurface antenna (RIMSA) architecture to address\nthese challenges. Unlike conventional RIS designs, RIMSA employs parallel\ncoaxial feeding and 2D metasurface integration, enabling each individual\nmetamaterial element to independently adjust both its amplitude and phase.\nWhile traditional optimization and deep learning (DL) methods struggle with\nhigh-dimensional state spaces and prohibitive training costs for RIMSA control,\nLLM-RIMSA leverages pre-trained LLMs cross-modal reasoning and few-shot\nlearning capabilities to dynamically optimize RIMSA configurations. Simulations\ndemonstrate that LLM-RIMSA achieves state-of-the-art performance, outperforming\nconventional DL-based methods in sum rate while reducing training overhead. The\nproposed framework pave the way for LLM-driven intelligent radio environments.", "AI": {"tldr": "LLM-RIMSA\u662f\u4e00\u4e2a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u65b0\u578b\u53ef\u91cd\u6784\u667a\u80fd\u8d85\u8868\u9762\u5929\u7ebf\u67b6\u6784\u76f8\u7ed3\u5408\u7684\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7LLM\u7684\u8de8\u6a21\u6001\u63a8\u7406\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u52a8\u6001\u4f18\u5316\u5929\u7ebf\u914d\u7f6e\uff0c\u57286G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u8d85\u5927\u89c4\u6a21\u8fde\u63a5\u548c\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\uff0c\u4f46\u73b0\u6709\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u6280\u672f\u5728\u786c\u4ef6\u6548\u7387\u3001\u52a8\u6001\u63a7\u5236\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u9650\u5236\uff0c\u4f20\u7edf\u4f18\u5316\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u8bad\u7ec3\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faRIMSA\u67b6\u6784\uff0c\u91c7\u7528\u5e73\u884c\u540c\u8f74\u9988\u7535\u548c2D\u8d85\u8868\u9762\u96c6\u6210\uff0c\u4f7f\u6bcf\u4e2a\u8d85\u6750\u6599\u5355\u5143\u80fd\u72ec\u7acb\u8c03\u6574\u5e45\u5ea6\u548c\u76f8\u4f4d\uff1b\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u63a8\u7406\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u6765\u52a8\u6001\u4f18\u5316RIMSA\u914d\u7f6e\u3002", "result": "\u4eff\u771f\u663e\u793aLLM-RIMSA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u603b\u901f\u7387\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u9a71\u52a8\u7684\u667a\u80fd\u65e0\u7ebf\u7535\u73af\u5883\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u89e3\u51b3\u4e866G\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u6280\u672f\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002"}}
{"id": "2508.12212", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12212", "abs": "https://arxiv.org/abs/2508.12212", "authors": ["Chuanliu Fan", "Zicheng Ma", "Jun Gao", "Nan Yu", "Jun Zhang", "Ziqiang Cao", "Yi Qin Gao", "Guohong Fu"], "title": "ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression", "comment": null, "summary": "Recent advances in protein large language models, such as ProtTeX, represent\nboth side-chain amino acids and backbone structure as discrete token sequences\nof residue length. While this design enables unified modeling of multimodal\nprotein information, it suffers from two major limitations: (1) The\nconcatenation of sequence and structure tokens approximately doubles the\nprotein length and breaks the intrinsic residue-level alignment between\nmodalities. (2) Constrained by the training corpus and limited context window,\nProtTeX is typically trained on single-protein inputs, rendering it\nincompatible with in-context learning (ICL) and thus limiting its\ngeneralization capability. To address these issues, we propose ProtTeX-CC, a\nlightweight two-stage compression framework designed to enhance ProtTeX under\nfew-shot settings. We first design a joint embedding compression mechanism that\nfuses sequence and structure representations at the residue level, effectively\nreducing the protein input length by half without sacrificing performance. Then\nwe propose a self-compression module that aggregates each full demonstration\ninto the latent space of the last few linguistic tokens, reducing the average\ndemonstration length from 751 tokens to less than 16 tokens. Compared to the\noriginal ProtTeX, our self-compression approach achieves a compression ratio of\napproximately 93.68% in the total prompt length under the 16-shot setting.\nWithout modifying the backbone model, ProtTeX-CC introduces only a small number\nof additional parameters through PEFT-based tuning in the joint embedding\ncompression stage and a single trainable projection layer in the\nself-compression stage. Extensive experiments on protein function prediction\nshow that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and\ngeneralizes well to the out-of-domain dataset with a performance gain of 11%.", "AI": {"tldr": "ProtTeX-CC\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u538b\u7f29\u548c\u81ea\u538b\u7f29\u6a21\u5757\uff0c\u5c06\u86cb\u767d\u8d28\u8f93\u5165\u957f\u5ea6\u51cf\u5c11\u4e00\u534a\uff0c\u6f14\u793a\u957f\u5ea6\u4ece751\u4e2atoken\u538b\u7f29\u523016\u4e2a\u4ee5\u4e0b\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u663e\u8457\u63d0\u5347ProtTeX\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3ProtTeX\u6a21\u578b\u7684\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u5e8f\u5217\u548c\u7ed3\u6784token\u8fde\u63a5\u5bfc\u81f4\u86cb\u767d\u8d28\u957f\u5ea6\u7ffb\u500d\u5e76\u7834\u574f\u6a21\u6001\u95f4\u5bf9\u9f50\uff1b2\uff09\u53d7\u9650\u4e8e\u8bad\u7ec3\u8bed\u6599\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u65e0\u6cd5\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4e24\u9636\u6bb5\u538b\u7f29\u6846\u67b6\uff1a1\uff09\u8054\u5408\u5d4c\u5165\u538b\u7f29\u673a\u5236\u5728\u6b8b\u57fa\u7ea7\u522b\u878d\u5408\u5e8f\u5217\u548c\u7ed3\u6784\u8868\u793a\uff1b2\uff09\u81ea\u538b\u7f29\u6a21\u5757\u5c06\u5b8c\u6574\u6f14\u793a\u805a\u5408\u5230\u6700\u540e\u51e0\u4e2a\u8bed\u8a00token\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002\u91c7\u7528PEFT\u5fae\u8c03\u548c\u5355\u6295\u5f71\u5c42\u589e\u52a0\u5c11\u91cf\u53c2\u6570\u3002", "result": "\u572816-shot\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u7ea693.68%\u7684\u603b\u63d0\u793a\u957f\u5ea6\u538b\u7f29\u6bd4\u3002\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u5b9e\u9a8c\u4e2d\uff0c\u57df\u5185\u57fa\u51c6\u6027\u80fd\u63d0\u53472%\uff0c\u57df\u5916\u6570\u636e\u96c6\u6027\u80fd\u63d0\u534711%\u3002", "conclusion": "ProtTeX-CC\u5728\u4e0d\u4fee\u6539\u4e3b\u5e72\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u538b\u7f29\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86ProtTeX\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u548c\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2508.12746", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12746", "abs": "https://arxiv.org/abs/2508.12746", "authors": ["Muhammad Ammad", "Paul Schwarzbach", "Michael Schultz", "Oliver Michler"], "title": "Range-Angle Likelihood Maps for Indoor Positioning Using Deep Neural Networks", "comment": null, "summary": "Accurate and high precision of the indoor positioning is as important as\nensuring reliable navigation in outdoor environments. Using the\nstate-of-the-art deep learning models provides better reliability and accuracy\nto navigate and monitor the accurate positions in the aircraft cabin\nenvironment. We utilize the simulated aircraft cabin environment measurements\nand propose a residual neural network (ResNet) model to predict the accurate\npositions inside the cabin. The measurements include the ranges and angles\nbetween a tag and the anchors points which are then mapped onto a grid as range\nand angle residuals. These residual maps are then transformed into the\nlikelihood grid maps where each cell of the grid shows the likelihood of being\na true location. These grid maps along with the true positions are then passed\nas inputs to train the ResNet model. Since any deep learning model involve\nnumerous parameter settings, hyperparameter optimization is performed to get\nthe optimal parameters for training the model effectively with the highest\naccuracy. Once we get the best hyperparameters settings of the model, it is\nthen trained to predict the positions which provides a centimeter-level\naccuracy of the localization.", "AI": {"tldr": "\u4f7f\u7528ResNet\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u5b9e\u73b0\u98de\u673a\u8231\u5185\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5ba4\u5185\u5b9a\u4f4d", "motivation": "\u5ba4\u5185\u7cbe\u786e\u5b9a\u4f4d\u5728\u98de\u673a\u8231\u73af\u5883\u4e2d\u4e0e\u5ba4\u5916\u5bfc\u822a\u540c\u6837\u91cd\u8981\uff0c\u9700\u8981\u9ad8\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u7684\u5b9a\u4f4d\u6280\u672f\u6765\u76d1\u63a7\u8231\u5185\u4f4d\u7f6e", "method": "\u5229\u7528\u6a21\u62df\u98de\u673a\u8231\u73af\u5883\u6d4b\u91cf\u6570\u636e\uff0c\u5c06\u6807\u7b7e\u4e0e\u951a\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\u548c\u89d2\u5ea6\u6620\u5c04\u4e3a\u6b8b\u5dee\u7f51\u683c\uff0c\u8f6c\u6362\u4e3a\u4f3c\u7136\u7f51\u683c\u56fe\uff0c\u7136\u540e\u4f7f\u7528ResNet\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u83b7\u5f97\u6700\u4f73\u53c2\u6570\u8bbe\u7f6e", "result": "\u7ecf\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u540e\u7684ResNet\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5398\u7c73\u7ea7\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u51c6\u786e\u5ea6", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eResNet\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u98de\u673a\u8231\u5185\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u95ee\u9898\uff0c\u8fbe\u5230\u5398\u7c73\u7ea7\u5b9a\u4f4d\u7cbe\u5ea6"}}
{"id": "2508.12220", "categories": ["cs.LG", "cs.AI", "cs.CR", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.12220", "abs": "https://arxiv.org/abs/2508.12220", "authors": ["Abdullah X"], "title": "Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models", "comment": "Preprint; 2 figures + several tables; includes appendix.\n  Artifact/code link in paper", "summary": "We study the right to be forgotten (GDPR Art. 17) for large language models\nand frame unlearning as a reproducible systems problem. Our approach treats\ntraining as a deterministic program and logs a minimal per-microbatch record\n(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and\naccumulation boundary). Under a pinned stack and deterministic kernels,\nreplaying the training tail while filtering only the forget closure yields the\nsame parameters as training on the retain set (bit-identical in the training\ndtype) when preconditions hold. To meet latency and availability constraints,\nwe add complementary paths: (i) exact reverts of recent steps via\nmicro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion\nwhen the base is frozen, and (iii) a curvature-guided anti-update followed by a\nshort retain-tune, audit-gated with escalation to exact replay. We report\nstorage/latency budgets and a toy artifact validating mechanics; in a\ncontrolled run that satisfies the preconditions we demonstrate byte-identical\nequality of model and optimizer states.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u5b66\u4e60\uff08unlearning\uff09\u7684\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb0\u5f55\u8bad\u7ec3\u8fc7\u7a0b\u7684\u786e\u5b9a\u6027\u65e5\u5fd7\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u6570\u636e\u5220\u9664\u548c\u6a21\u578b\u72b6\u6001\u6062\u590d\u3002", "motivation": "\u7814\u7a76GDPR\u7b2c17\u6761\u89c4\u5b9a\u7684\u88ab\u9057\u5fd8\u6743\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5c06\u9057\u5fd8\u5b66\u4e60\u4f5c\u4e3a\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u7cfb\u7edf\u95ee\u9898\u6765\u5904\u7406\uff0c\u786e\u4fdd\u6a21\u578b\u80fd\u591f\u7cbe\u786e\u5220\u9664\u7279\u5b9a\u6570\u636e\u800c\u4e0d\u5f71\u54cd\u5176\u4ed6\u6570\u636e\u3002", "method": "\u5c06\u8bad\u7ec3\u89c6\u4e3a\u786e\u5b9a\u6027\u7a0b\u5e8f\uff0c\u8bb0\u5f55\u6bcf\u4e2a\u5fae\u6279\u6b21\u7684\u65e5\u5fd7\uff08\u5305\u62ecID\u54c8\u5e0c\u3001RNG\u79cd\u5b50\u3001\u5b66\u4e60\u7387\u503c\u3001\u4f18\u5316\u5668\u6b65\u6570\u8ba1\u6570\u5668\u548c\u7d2f\u79ef\u8fb9\u754c\uff09\u3002\u5728\u56fa\u5b9a\u5806\u6808\u548c\u786e\u5b9a\u6027\u5185\u6838\u4e0b\uff0c\u91cd\u653e\u8bad\u7ec3\u5c3e\u90e8\u540c\u65f6\u8fc7\u6ee4\u9057\u5fd8\u95ed\u5305\uff0c\u5b9e\u73b0\u4e0e\u5728\u4fdd\u7559\u96c6\u4e0a\u8bad\u7ec3\u76f8\u540c\u7684\u53c2\u6570\u3002\u8fd8\u8865\u5145\u4e86\u4e09\u79cd\u8def\u5f84\uff1a\u7cbe\u786e\u56de\u6eda\u6700\u8fd1\u6b65\u9aa4\u3001\u961f\u5217\u8303\u56f4\u9002\u914d\u5668\u5220\u9664\u3001\u4ee5\u53ca\u66f2\u7387\u5f15\u5bfc\u7684\u53cd\u5411\u66f4\u65b0\u52a0\u77ed\u671f\u4fdd\u7559\u8c03\u4f18\u3002", "result": "\u5728\u6ee1\u8db3\u524d\u63d0\u6761\u4ef6\u7684\u53d7\u63a7\u8fd0\u884c\u4e2d\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\u7684\u5b57\u8282\u7ea7\u5b8c\u5168\u76f8\u540c\u6027\uff0c\u5e76\u62a5\u544a\u4e86\u5b58\u50a8/\u5ef6\u8fdf\u9884\u7b97\u548c\u9a8c\u8bc1\u673a\u5236\u7684\u73a9\u5177\u5de5\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u7684\u9057\u5fd8\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6ee1\u8db3GDPR\u88ab\u9057\u5fd8\u6743\u7684\u6280\u672f\u8981\u6c42\uff0c\u540c\u65f6\u517c\u987e\u5ef6\u8fdf\u548c\u53ef\u7528\u6027\u7ea6\u675f\u3002"}}
{"id": "2508.12892", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12892", "abs": "https://arxiv.org/abs/2508.12892", "authors": ["Mahdi Abdollahpour", "Marco Bertuletti", "Yichao Zhang", "Yawei Li", "Luca Benini", "Alessandro Vanelli-Coralli"], "title": "A Compute&Memory Efficient Model-Driven Neural 5G Receiver for Edge AI-assisted RAN", "comment": "Accepted to IEEE GLOBECOM 2025", "summary": "Artificial intelligence approaches for base-band processing for radio\nreceivers have demonstrated significant performance gains. Most of the proposed\nmethods are characterized by high compute and memory requirements, hindering\ntheir deployment at the edge of the Radio Access Networks (RAN) and limiting\ntheir scalability to large bandwidths and many antenna 6G systems. In this\npaper, we propose a low-complexity, model-driven neural network-based receiver,\ndesigned for multi-user multiple-input multiple-output (MU-MIMO) systems and\nsuitable for implementation at the RAN edge. The proposed solution is compliant\nwith the 5G New Radio (5G NR), and supports different modulation schemes,\nbandwidths, number of users, and number of base-station antennas with a single\ntrained model without the need for further training. Numerical simulations of\nthe Physical Uplink Shared Channel (PUSCH) processing show that the proposed\nsolution outperforms the state-of-the-art methods in terms of achievable\nTransport Block Error Rate (TBLER), while reducing the Floating Point\nOperations (FLOPs) by 66$\\times$, and the learnable parameters by 396$\\times$.", "AI": {"tldr": "\u4f4e\u590d\u6742\u5ea6\u6a21\u578b\u9a71\u52a8\u795e\u7ecf\u7f51\u57fa\u5e26\u5904\u7406\u63a5\u6536\u673a\uff0c\u5728MU-MIMO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6027\u80fd\u548c\u66f4\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "\u89e3\u51b3AI\u57fa\u5e26\u5904\u7406\u65b9\u6848\u8ba1\u7b97\u548c\u5185\u5b58\u8981\u6c42\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u4fbf\u5728RAN\u7f51\u7edc\u8fb9\u7f18\u90e8\u7f72\u548c\u652f\u6301\u5927\u5e26\u5bbd\u591a\u5929\u7ebf7G\u7cfb\u7edf", "method": "\u63d0\u51fa\u4e00\u79cd\u4e0e5G NR\u517c\u5bb9\u7684\u4f4e\u590d\u6742\u5ea6\u6a21\u578b\u9a71\u52a8\u795e\u7ecf\u7f51\u7edc\u57fa\u7840\u7684\u63a5\u6536\u673a\uff0c\u652f\u6301\u591a\u79cd\u8c03\u5236\u65b9\u6848\u3001\u5e26\u5bbd\u3001\u7528\u6237\u6570\u548c\u5929\u7ebf\u6570", "result": "\u5728PUSCH\u5904\u7406\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6848\u5728TBLER\u6027\u80fd\u4e0a\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c11\u6d6e\u70b9\u8fd0\u7b97\u6b21\u657066\u500d\u548c\u53ef\u5b66\u53c2\u6570396\u500d", "conclusion": "\u8be5\u65b9\u6848\u4e3a6G\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u57fa\u5e26\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5728\u7f51\u7edc\u8fb9\u7f18\u90e8\u7f72"}}
{"id": "2508.12222", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12222", "abs": "https://arxiv.org/abs/2508.12222", "authors": ["Sagar Shrestha", "Rajesh Shrestha", "Tri Nguyen", "Subash Timilsina"], "title": "Distribution Matching via Generalized Consistency Models", "comment": null, "summary": "Recent advancement in generative models have demonstrated remarkable\nperformance across various data modalities. Beyond their typical use in data\nsynthesis, these models play a crucial role in distribution matching tasks such\nas latent variable modeling, domain translation, and domain adaptation.\nGenerative Adversarial Networks (GANs) have emerged as the preferred method of\ndistribution matching due to their efficacy in handling high-dimensional data\nand their flexibility in accommodating various constraints. However, GANs often\nencounter challenge in training due to their bi-level min-max optimization\nobjective and susceptibility to mode collapse. In this work, we propose a novel\napproach for distribution matching inspired by the consistency models employed\nin Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF\nmodels, such as having a straight forward norm minimization objective, while\nremaining adaptable to different constraints similar to GANs. We provide\ntheoretical validation of our proposed objective and demonstrate its\nperformance through experiments on synthetic and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e00\u81f4\u6027\u6a21\u578b\u7684\u5206\u5e03\u5339\u914d\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u7684\u7b80\u5355\u76ee\u6807\u51fd\u6570\u548cGAN\u7684\u7ea6\u675f\u9002\u5e94\u6027\u4f18\u52bf", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u5206\u5e03\u5339\u914d\u4efb\u52a1\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u4f46GAN\u5b58\u5728\u8bad\u7ec3\u56f0\u96be\u548c\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u53d7\u8fde\u7eed\u5f52\u4e00\u5316\u6d41\u4e2d\u4e00\u81f4\u6027\u6a21\u578b\u542f\u53d1\uff0c\u8bbe\u8ba1\u65b0\u7684\u5206\u5e03\u5339\u914d\u65b9\u6cd5\uff0c\u5177\u6709\u7b80\u5355\u7684\u8303\u6570\u6700\u5c0f\u5316\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u7c7b\u4f3cGAN\u7684\u7ea6\u675f\u9002\u5e94\u6027", "result": "\u901a\u8fc7\u7406\u8bba\u9a8c\u8bc1\u548c\u5408\u6210/\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86CNF\u7684\u7b80\u5355\u76ee\u6807\u4f18\u52bf\u548cGAN\u7684\u7ea6\u675f\u7075\u6d3b\u6027\uff0c\u4e3a\u5206\u5e03\u5339\u914d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12941", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12941", "abs": "https://arxiv.org/abs/2508.12941", "authors": ["Donggu Lee", "Sung Joon Maeng", "Ozgur Ozdemir", "Mani Bharathi Pandian", "Ismail Guvenc"], "title": "Interference-Asymmetric UAV Remote Control Links: Measurements and Performance Evaluation", "comment": null, "summary": "Reliable and secure connectivity is crucial for remote control (RC) and\nuncrewed aerial vehicles (UAVs) links. A major problem for UAV RC links is that\ninterference sources within the coverage may degrade the link quality. Such\ninterference problems are a higher concern for the UAV than the RC unit on the\nground due to the UAV being in line of sight (LoS) with a larger number of\ninterference sources. As a result, lost hybrid automatic repeat request (HARQ)\nindicators (ACK/NACK) feedback in the uplink (UL, RC to UAV) may degrade the\ndownlink (DL, UAV to RC) throughput. To get physical evidence for our\ninterference asymmetry argument, we first conducted a measurement campaign\nusing a helikite platform at the Main Campus area of NC State University during\nthe 2024 Packapalooza festival. Subsequently, we evaluated the throughput\nimpact of the loss of HARQ indicator feedback caused by UL asymmetry using\nMATLAB long-term-evolution (LTE) and fifth-generation (5G) toolboxes. Our\nnumerical results confirm that UL interference asymmetry substantially degrades\nthe throughput performance due to the loss of HARQ indicator feedback.", "AI": {"tldr": "\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u548c\u6a21\u62df\u5206\u6790\uff0c\u8bc1\u5b9e\u4e86\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u4e2d\u4e0a\u884c\u4e0b\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u4e0a\u884cHARQ\u6307\u793a\u7b26\u5931\u8d25\u4f1a\u5bfc\u81f4\u4e0b\u884c\u901f\u7387\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u65e0\u4eba\u673a\u9065\u63a7\u94fe\u8def\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e0\u4eba\u673a\u56f4\u7f51\u5185\u5e72\u6270\u6e90\u66f4\u591a\uff0c\u5bfc\u81f4\u4e0a\u884c\u4e0b\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5f71\u54cdHARQ\u53cd\u9988\u673a\u5236\u6548\u679c\u3002", "method": "\u9996\u5148\u5728NC State\u5927\u5b66\u4e3b\u6821\u533a\u4f7f\u7528\u6c7d\u7403\u98de\u8239\u5e73\u53f0\u8fdb\u884c\u5b9e\u9645\u6d4b\u91cf\uff0c\u7136\u540e\u4f7f\u7528MATLAB LTE\u548c5G\u5de5\u5177\u7bb1\u6a21\u62df\u8bc4\u4f30HARQ\u6307\u793a\u7b26\u5931\u8d25\u5bf9\u901f\u7387\u7684\u5f71\u54cd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u786e\u8ba4\u4e0a\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u4f1a\u5bfc\u81f4HARQ\u6307\u793a\u7b26\u53cd\u9988\u5931\u8d25\uff0c\u4ece\u800c\u5bf9\u4e0b\u884c\u901f\u7387\u6027\u80fd\u9020\u6210\u663e\u8457\u7684\u6027\u80fd\u964d\u7ea7\u3002", "conclusion": "\u65e0\u4eba\u673a\u9065\u63a7\u7cfb\u7edf\u5b58\u5728\u4e0a\u884c\u4e0b\u884c\u5e72\u6270\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0cHARQ\u53cd\u9988\u673a\u5236\u5bb9\u6613\u53d7\u5e72\u6270\u5f71\u54cd\uff0c\u9700\u8981\u91cd\u70b9\u5173\u6ce8\u548c\u4f18\u5316\u4e0a\u884c\u94fe\u8def\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.12233", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12233", "abs": "https://arxiv.org/abs/2508.12233", "authors": ["Sagar Shrestha"], "title": "Communication-Efficient Distributed Asynchronous ADMM", "comment": null, "summary": "In distributed optimization and federated learning, asynchronous alternating\ndirection method of multipliers (ADMM) serves as an attractive option for\nlarge-scale optimization, data privacy, straggler nodes and variety of\nobjective functions. However, communication costs can become a major bottleneck\nwhen the nodes have limited communication budgets or when the data to be\ncommunicated is prohibitively large. In this work, we propose introducing\ncoarse quantization to the data to be exchanged in aynchronous ADMM so as to\nreduce communication overhead for large-scale federated learning and\ndistributed optimization applications. We experimentally verify the convergence\nof the proposed method for several distributed learning tasks, including neural\nnetworks.", "AI": {"tldr": "\u5728\u5f02\u6b65ADMM\u4e2d\u5f15\u5165\u7c97\u7c97\u91cf\u5316\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u805a\u5408\u5b66\u4e60\u548c\u5206\u5e03\u5f0f\u4f18\u5316", "motivation": "\u5206\u5e03\u5f0f\u4f18\u5316\u548c\u805a\u5408\u5b66\u4e60\u4e2d\uff0c\u5f02\u6b65ADMM\u867d\u7136\u6709\u8bb8\u591a\u4f18\u70b9\uff0c\u4f46\u901a\u4fe1\u6210\u672c\u5f88\u5bb9\u6613\u6210\u4e3a\u74f6\u9888\uff0c\u7279\u522b\u662f\u5f53\u8282\u70b9\u901a\u4fe1\u9884\u7b97\u6709\u9650\u6216\u6570\u636e\u91cf\u6781\u5927\u65f6", "method": "\u5728\u5f02\u6b65ADMM\u4e2d\u5bf9\u4ea4\u6362\u6570\u636e\u8fdb\u884c\u7c97\u7c97\u91cf\u5316\uff0c\u4ee5\u964d\u4f4e\u901a\u4fe1\u5f00\u9500", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u591a\u4e2a\u5206\u5e03\u5f0f\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u6536\u655b\u6027\uff0c\u5305\u62ec\u795e\u7ecf\u7f51\u7edc", "conclusion": "\u7c97\u91cf\u5316ADMM\u80fd\u591f\u6709\u6548\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u540c\u65f6\u4fdd\u6301\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u805a\u5408\u5b66\u4e60\u5e94\u7528"}}
{"id": "2508.12964", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12964", "abs": "https://arxiv.org/abs/2508.12964", "authors": ["Osman Tokluoglu", "Enver Cavus", "Ebrahim Bedeer", "Halim Yanikomeroglu"], "title": "A Novel CNN Based Standalone Detector for Faster-than-Nyquist Signaling", "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Communications (IEEE TCOM)", "summary": "This paper presents a novel convolutional neural network (CNN)-based detector\nfor faster-than-Nyquist (FTN) signaling, introducing structured fixed kernel\nlayers with domain-informed masking to effectively mitigate intersymbol\ninterference (ISI). Unlike standard CNN architectures that rely on moving\nkernels, the proposed approach employs fixed convolutional kernels at\npredefined positions to explicitly learn ISI patterns at varying distances from\nthe central symbol. To enhance feature extraction, a hierarchical filter\nallocation strategy is employed, assigning more filters to earlier layers for\nstronger ISI components and fewer to later layers for weaker components. This\nstructured design improves feature representation, eliminates redundant\ncomputations, and enhances detection accuracy while maintaining computational\nefficiency. Simulation results demonstrate that the proposed detector achieves\nnear-optimal bit error rate (BER) performance, comparable to the BCJR algorithm\nfor the compression factor $\\tau \\geq 0.7$, while offering up to $46\\%$ and\n$84\\%$ computational cost reduction over M-BCJR for BPSK and QPSK,\nrespectively. Additional evaluations confirm the method's adaptability to\nhigh-order modulations (up to 64-QAM), resilience in quasi-static multipath\nRayleigh fading channels, and effectiveness under LDPC-coded FTN transmission,\nhighlighting its robustness and practicality.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u9898\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u56fa\u5b9a\u5185\u6838\u5c42\u548c\u9886\u57df\u77e5\u8bc6\u63a9\u7801\u6280\u672f\uff0c\u6709\u6548\u51cf\u5c11\u8d85\u5948\u76f8\u7279\u4fe1\u53f7\u7684\u7801\u95f4\u5e72\u6270\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u8d85\u5948\u76f8\u7279\u4fe1\u53f7\u7684\u7801\u95f4\u5e72\u6270(ISI)\u95ee\u9898\u662f\u9650\u5236\u5176\u5e94\u7528\u7684\u4e3b\u8981\u6311\u6218\uff0c\u9700\u8981\u67e5\u627e\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u6765\u66ff\u4ee3\u590d\u6742\u7684BCJR\u7b97\u6cd5\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u56fa\u5b9a\u5377\u79ef\u5185\u6838\u5c42\uff0c\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u63a9\u7801\u660e\u786e\u5b66\u4e60\u4e0d\u540c\u8ddd\u79bb\u7684ISI\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u5c42\u6b21\u6ee1\u6ee1\u5206\u914d\u7b56\u7565\u4f18\u5316\u6ee1\u5668\u5206\u914d\u3002", "result": "\u8be5\u68c0\u6d4b\u5668\u5728\u538b\u7f29\u56e0\u5b50\u03c4\u22650.7\u65f6\u8fbe\u5230\u8fd1\u4f18\u7684BER\u6027\u80fd\uff0c\u4e0eBCJR\u7b97\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u51cf\u5c11\u8fbe46%(BPSK)\u548c84%(QPSK)\uff0c\u5177\u5907\u9002\u5e94\u9ad8\u9636\u8c03\u5236\u548c\u591a\u8def\u8870\u843d\u73af\u5883\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aFTN\u4fe1\u53f7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5f3a\u5927\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.12235", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12235", "abs": "https://arxiv.org/abs/2508.12235", "authors": ["Peng Chen", "Yihang Wang", "Yang Shu", "Yunyao Cheng", "Kai Zhao", "Zhongwen Rao", "Lujia Pan", "Bin Yang", "Chenjuan Guo"], "title": "CC-Time: Cross-Model and Cross-Modality Time Series Forecasting", "comment": null, "summary": "With the success of pre-trained language models (PLMs) in various application\nfields beyond natural language processing, language models have raised emerging\nattention in the field of time series forecasting (TSF) and have shown great\nprospects. However, current PLM-based TSF methods still fail to achieve\nsatisfactory prediction accuracy matching the strong sequential modeling power\nof language models. To address this issue, we propose Cross-Model and\nCross-Modality Learning with PLMs for time series forecasting (CC-Time). We\nexplore the potential of PLMs for time series forecasting from two aspects: 1)\nwhat time series features could be modeled by PLMs, and 2) whether relying\nsolely on PLMs is sufficient for building time series models. In the first\naspect, CC-Time incorporates cross-modality learning to model temporal\ndependency and channel correlations in the language model from both time series\nsequences and their corresponding text descriptions. In the second aspect,\nCC-Time further proposes the cross-model fusion block to adaptively integrate\nknowledge from the PLMs and time series model to form a more comprehensive\nmodeling of time series patterns. Extensive experiments on nine real-world\ndatasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy\nin both full-data training and few-shot learning situations.", "AI": {"tldr": "\u57fa\u4e8e\u9884\u8bad\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5CC-Time\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u548c\u8de8\u6a21\u6001\u5b66\u4e60\u7ed3\u5408\u65f6\u95f4\u5e8f\u5e8f\u5217\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u9884\u8bad\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u6ee1\u610f\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u3002", "method": "CC-Time\u65b9\u6cd5\u4ece\u4e24\u4e2a\u65b9\u9762\u63a2\u7d22PLMs\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff1a1\uff09\u901a\u8fc7\u8de8\u6a21\u6001\u5b66\u4e60\u5efa\u6a21\u65f6\u95f4\u5e8f\u5e8f\u5217\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u901a\u9053\u76f8\u5173\u6027\uff1b2\uff09\u901a\u8fc7\u8de8\u6a21\u578b\u878d\u5408\u5757\u81ea\u9002\u5e94\u5730\u6574\u5408PLMs\u548c\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u7684\u77e5\u8bc6\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCC-Time\u5728\u5168\u6570\u636e\u8bad\u7ec3\u548c\u5c11\u6837\u672c\u5b66\u4e60\u60c5\u51b5\u4e0b\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "CC-Time\u901a\u8fc7\u8de8\u6a21\u6001\u548c\u8de8\u6a21\u578b\u5b66\u4e60\u6280\u672f\uff0c\u5145\u5206\u53d1\u6325\u4e86\u9884\u8bad\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u3002"}}
{"id": "2508.13017", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13017", "abs": "https://arxiv.org/abs/2508.13017", "authors": ["Scott Schoen Jr", "Brian Lause", "Marko Jakovljevic", "Rimon Tadross", "Mike Washburn", "Anthony E. Samir"], "title": "Wavefield Correlation Imaging in Arbitrary Media with Inherent Aberration Correction", "comment": null, "summary": "Ultrasound (US) imaging is an indispensable tool for diagnostic imaging,\nparticularly given its cost, safety, and portability profiles compared to other\nmodalities. However, US is challenged in subjects with morphological\nheterogeneity (e.g., those with overweight or obesity), largely because\nconventional imaging algorithms do not account for such variation in the\nbeamforming process. Specific knowledge of the these spatial variations enables\nsupplemental corrections of these algorithms, but with added computational\ncomplexity. Wavefield correlation imaging (WCI) enables efficient image\nformation in the spatial frequency domain that, in its canonical formulation,\nassumes a uniform medium. In this work, we present an extension of WCI to\narbitrary known speed-of-sound distributions directly in the image formation\nprocess, and demonstrate its feasibility in silico, in vitro, and in vivo. We\nreport resolution improvements of over 30% and contrast improvements of order\n10% over conventional WCI imaging. Together our results suggest heterogeneous\nWCI (HWCI) may have high translational potential to improve the objective\nquality, and thus clinical utility, of ultrasound images.", "AI": {"tldr": "\u901a\u8fc7\u6269\u5c55\u6ce2\u573a\u76f8\u5173\u6210\u50cf(WCI)\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u80fd\u591f\u76f4\u63a5\u5904\u7406\u4efb\u610f\u77e5\u9053\u58f0\u901f\u5206\u5e03\u7684\u5f02\u8d28\u6027WCI(HWCI)\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u673a\u6a21\u62df\u3001\u79bb\u4f53\u5b9e\u9a8c\u548c\u5728\u4f53\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u5206\u8fa8\u7387\u63d0\u5347\u548c\u7ea610%\u7684\u5bf9\u6bd4\u5ea6\u6539\u5584\u3002", "motivation": "\u8d85\u58f0\u6210\u50cf\u5728\u5f62\u6001\u5f02\u8d28\u6027\u4e3b\u4f53(\u5982\u8d85\u91cd\u6216\u80a5\u80d6\u75c7\u60a3\u8005)\u4e2d\u9047\u5230\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u7edf\u6210\u50cf\u7b97\u6cd5\u5728\u805a\u96c6\u8fc7\u7a0b\u4e2d\u4e0d\u8003\u8651\u8fd9\u79cd\u53d8\u5f02\u3002\u867d\u7136\u77e5\u9053\u7a7a\u95f4\u53d8\u5f02\u53ef\u4ee5\u8865\u507f\u7b97\u6cd5\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u5f02\u8d28\u6027WCI(HWCI)\u65b9\u6cd5\uff0c\u5728\u56fe\u50cf\u5f62\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u8003\u8651\u4efb\u610f\u77e5\u9053\u7684\u58f0\u901f\u5206\u5e03\uff0c\u800c\u975e\u5047\u8bbe\u5747\u5300\u4ecb\u8d28\u3002\u901a\u8fc7\u7a7a\u95f4\u9891\u7387\u57df\u7684\u9ad8\u6548\u56fe\u50cf\u5f62\u6210\u6765\u5b9e\u73b0\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u6a21\u62df\u3001\u79bb\u4f53\u5b9e\u9a8c\u548c\u5728\u4f53\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\uff0c\u4e0e\u4f20\u7edfWCI\u76f8\u6bd4\uff0c\u5206\u8fa8\u7387\u63d0\u9ad8\u8d85\u8fc730%\uff0c\u5bf9\u6bd4\u5ea6\u63d0\u9ad8\u7ea610%\u3002", "conclusion": "HWCI\u6280\u672f\u5177\u6709\u9ad8\u7684\u8f6c\u5316\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u8d85\u58f0\u56fe\u50cf\u7684\u5ba2\u89c2\u8d28\u91cf\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.12244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12244", "abs": "https://arxiv.org/abs/2508.12244", "authors": ["Fan Li", "Xiaoyang Wang", "Wenjie Zhang", "Ying Zhang", "Xuemin Lin"], "title": "DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning", "comment": "22 pages, 5 figures", "summary": "Although conventional deep graph models have achieved great success in\nrelational learning, their focus on pairwise relationships limits their\ncapacity to learn pervasive higher-order interactions in real-world complex\nsystems, which can be naturally modeled as hypergraphs. To tackle this,\nhypergraph neural networks (HNNs), the dominant approach in deep hypergraph\nlearning (DHGL), has garnered substantial attention in recent years. Despite\nthe proposal of numerous HNN methods, there is no comprehensive benchmark for\nHNNs, which creates a great obstacle to understanding the progress of DHGL in\nseveral aspects: (i) insufficient coverage of datasets, algorithms, and tasks;\n(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent\ndataset usage, preprocessing, and experimental setups that hinder\ncomparability. To fill the gap, we introduce DHG-Bench, the first comprehensive\nbenchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets\nspanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art\nHNN algorithms, under consistent data processing and experimental protocols.\nOur benchmark systematically investigates the characteristics of HNNs in terms\nof four dimensions: effectiveness, efficiency, robustness, and fairness.\nFurther, to facilitate reproducible research, we have developed an easy-to-use\nlibrary for training and evaluating different HNN methods. Extensive\nexperiments conducted with DHG-Bench reveal both the strengths and inherent\nlimitations of existing algorithms, offering valuable insights and directions\nfor future research. The code is publicly available at:\nhttps://github.com/Coco-Hut/DHG-Bench.", "AI": {"tldr": "DHG-Bench\u662f\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u6df1\u5ea6\u8d85\u56fe\u5b66\u4e60\u57fa\u51c6\uff0c\u96c6\u6210\u4e8620\u4e2a\u6570\u636e\u96c6\u548c16\u79cd\u6700\u5148\u8fdb\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\uff0c\u5728\u7edf\u4e00\u7684\u6570\u636e\u5904\u7406\u548c\u5b9e\u9a8c\u534f\u8bae\u4e0b\u7cfb\u7edf\u8bc4\u4f30\u7b97\u6cd5\u5728\u6548\u679c\u3001\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u56db\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u56fe\u6a21\u578b\u4e13\u6ce8\u4e8e\u6210\u5bf9\u5173\u7cfb\uff0c\u65e0\u6cd5\u6709\u6548\u5b66\u4e60\u73b0\u5b9e\u590d\u6742\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u9ad8\u9636\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b58\u5728\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u8db3\u3001\u8bc4\u4f30\u7ef4\u5ea6\u72ed\u7a84\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efaDHG-Bench\u57fa\u51c6\uff0c\u6574\u540820\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff08\u6db5\u76d6\u8282\u70b9\u3001\u8fb9\u548c\u56fe\u7ea7\u4efb\u52a1\uff09\u548c16\u79cdstate-of-the-art HNN\u7b97\u6cd5\uff0c\u91c7\u7528\u4e00\u81f4\u7684\u6570\u636e\u5904\u7406\u548c\u5b9e\u9a8c\u534f\u8bae\uff0c\u4ece\u6548\u679c\u3001\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u56db\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\u3002", "result": "\u901a\u8fc7DHG-Bench\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u63ed\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u4f18\u52bf\u548c\u56fa\u6709\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u65b9\u5411\u3002", "conclusion": "DHG-Bench\u586b\u8865\u4e86\u6df1\u5ea6\u8d85\u56fe\u5b66\u4e60\u9886\u57df\u7f3a\u4e4f\u5168\u9762\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u7814\u7a76\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.13067", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13067", "abs": "https://arxiv.org/abs/2508.13067", "authors": ["Iv\u00e1n Alexander Morales Sandoval", "Getuar Rexhepi", "Kengo Ando", "Giuseppe Thadeu Freitas de Abreu"], "title": "Low-complexity Leakage Minimization Beamforming for Large-scale Multi-user Cell-Free Massive MIMO", "comment": "Submitted to an IEEE journal for possible publication", "summary": "We propose a low-complexity beamforming (BF) design for information leakage\nminimization in multi-user (MU) cell-free massive multiple-input\nmultiple-output (CF-mMIMO) systems. Our approach leverages fractional\nprogramming (FP) to reformulate the secrecy rate maximization problem into a\ntractable difference-of-convex form. To efficiently solve the resulting\nnon-convex problem, we employ the Concave-Convex Procedure (CCP), enabling fast\nconvergence to a local optimum. Simulation results demonstrate that the\nproposed scheme achieves secrecy rates comparable to state-of-the-art (SotA)\nmethods, while significantly reducing computational complexity and improving\nconvergence speed.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.12247", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12247", "abs": "https://arxiv.org/abs/2508.12247", "authors": ["Haolong Chen", "Liang Zhang", "Zhengyuan Xin", "Guangxu Zhu"], "title": "STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction", "comment": null, "summary": "Recently, spatio-temporal time-series prediction has developed rapidly, yet\nexisting deep learning methods struggle with learning complex long-term\nspatio-temporal dependencies efficiently. The long-term spatio-temporal\ndependency learning brings two new challenges: 1) The long-term temporal\nsequence includes multiscale information naturally which is hard to extract\nefficiently; 2) The multiscale temporal information from different nodes is\nhighly correlated and hard to model. To address these challenges, we propose an\nefficient \\textit{\\textbf{S}patio-\\textbf{T}emporal \\textbf{M}ultiscale\n\\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture\nthe multiscale information efficiently and simultaneously, and an adaptive\ngraph causal convolution network to learn the complex multiscale\nspatio-temporal dependency. STM2 includes hierarchical information aggregation\nfor different-scale information that guarantees their distinguishability. To\ncapture diverse temporal dynamics across all spatial nodes more efficiently, we\nfurther propose an enhanced version termed\n\\textit{\\textbf{S}patio-\\textbf{T}emporal \\textbf{M}ixture of\n\\textbf{M}ultiscale \\textbf{M}amba} (STM3) that employs a special\nMixture-of-Experts architecture, including a more stable routing strategy and a\ncausal contrastive learning strategy to enhance the scale distinguishability.\nWe prove that STM3 has much better routing smoothness and guarantees the\npattern disentanglement for each expert successfully. Extensive experiments on\nreal-world benchmarks demonstrate STM2/STM3's superior performance, achieving\nstate-of-the-art results in long-term spatio-temporal time-series prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86STM2\u548cSTM3\u4e24\u79cd\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6Mamba\u67b6\u6784\u548c\u81ea\u9002\u5e94\u56fe\u56e0\u679c\u5377\u79ef\u7f51\u7edc\uff0c\u6709\u6548\u89e3\u51b3\u957f\u65f6\u7a7a\u4f9d\u8d56\u5b66\u4e60\u4e2d\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u63d0\u53d6\u548c\u65f6\u7a7a\u76f8\u5173\u6027\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u957f\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5b66\u4e60\u590d\u6742\u7684\u957f\u671f\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a1\uff09\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u5929\u7136\u5305\u542b\u96be\u4ee5\u9ad8\u6548\u63d0\u53d6\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\uff1b2\uff09\u4e0d\u540c\u8282\u70b9\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u4fe1\u606f\u9ad8\u5ea6\u76f8\u5173\u4e14\u96be\u4ee5\u5efa\u6a21\u3002", "method": "STM2\u91c7\u7528\u591a\u5c3a\u5ea6Mamba\u67b6\u6784\u9ad8\u6548\u6355\u83b7\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u56fe\u56e0\u679c\u5377\u79ef\u7f51\u7edc\u5b66\u4e60\u590d\u6742\u591a\u5c3a\u5ea6\u65f6\u7a7a\u4f9d\u8d56\uff1bSTM3\u8fdb\u4e00\u6b65\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5305\u542b\u66f4\u7a33\u5b9a\u7684\u8def\u7531\u7b56\u7565\u548c\u56e0\u679c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u6765\u589e\u5f3a\u5c3a\u5ea6\u533a\u5206\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSTM2/STM3\u5728\u957f\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684STM2\u548cSTM3\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u591a\u5c3a\u5ea6Mamba\u67b6\u6784\u548c\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u7a7a\u4f9d\u8d56\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u65f6\u7a7a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13075", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.13075", "abs": "https://arxiv.org/abs/2508.13075", "authors": ["Arav Sharma", "Lei Chi", "Ari Gebhardt", "Alon S. Levin", "Timothy R. Hoerning", "Sam Keene"], "title": "BeamSeek: Deep Learning-based DOA Estimation for Low-Complexity mmWave Phased Arrays", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A novel approach combining agile beam switching with deep learning to enhance\nthe speed and accuracy of Direction of Arrival (DOA) estimation for\nmillimeter-wave (mmWave) phased array systems with low-complexity hardware\nimplementations is proposed and evaluated. Traditional DOA methods requiring\ndirect access to individual antenna elements are impractical for analog or\nhybrid beamforming systems prevalent in modern mmWave implementations. Recent\nagile beam switching techniques have demonstrated rapid DOA estimation, but\ntheir accuracy and robustness can be further improved via deep learning.\nBeamSeek addresses these limitations by employing a Multi-Layer Perceptron\n(MLP) and specialized data augmentation that emulates real-world propagation\nconditions. The proposed approach was experimentally validated at 60 GHz using\nthe NSF PAWR COSMOS testbed, demonstrating significant improvements over a\ncorrelation-based method across various Signal-to-Noise Ratio (SNR) levels.\nResults show that BeamSeek achieves up to an 8 degree reduction in average\nestimation error compared to this baseline, with particular advantages in noisy\nchannels. This makes it especially suitable for practical mmWave deployments in\nenvironments characterized by multipath interference and hardware constraints.", "AI": {"tldr": "BeamSeek\u7ed3\u5408\u654f\u6377\u6ce2\u675f\u5207\u6362\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u572860GHz\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u663e\u8457\u63d0\u5347\u4e86DOA\u4f30\u8ba1\u7684\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5e73\u5747\u8bef\u5dee\u964d\u4f4e8\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u5f84\u5e72\u6270\u73af\u5883\u3002", "motivation": "\u4f20\u7edfDOA\u65b9\u6cd5\u9700\u8981\u76f4\u63a5\u8bbf\u95ee\u5355\u4e2a\u5929\u7ebf\u5355\u5143\uff0c\u4e0d\u9002\u7528\u4e8e\u73b0\u4ee3\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u666e\u904d\u91c7\u7528\u7684\u6a21\u62df\u6216\u6df7\u5408\u6ce2\u675f\u6210\u5f62\u7cfb\u7edf\u3002\u73b0\u6709\u7684\u654f\u6377\u6ce2\u675f\u5207\u6362\u6280\u672f\u867d\u7136\u5feb\u901f\u4f46\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u6709\u5f85\u63d0\u5347\u3002", "method": "\u91c7\u7528\u591a\u5c42\u611f\u77e5\u5668(MLP)\u548c\u4e13\u95e8\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u6a21\u62df\u771f\u5b9e\u4f20\u64ad\u6761\u4ef6\uff0c\u7ed3\u5408\u654f\u6377\u6ce2\u675f\u5207\u6362\u6280\u672f\uff0c\u5728NSF PAWR COSMOS\u6d4b\u8bd5\u5e8a\u4e0a\u8fdb\u884c60GHz\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u6c34\u5e73\u4e0b\u5747\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u5e73\u5747\u4f30\u8ba1\u8bef\u5dee\u6700\u591a\u964d\u4f4e8\u5ea6\uff0c\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u4f18\u52bf\u5c24\u5176\u660e\u663e\u3002", "conclusion": "BeamSeek\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u591a\u5f84\u5e72\u6270\u548c\u786c\u4ef6\u7ea6\u675f\u7684\u5b9e\u9645\u6beb\u7c73\u6ce2\u90e8\u7f72\u73af\u5883\uff0c\u4e3a\u4f4e\u590d\u6742\u5ea6\u786c\u4ef6\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u7684DOA\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12253", "categories": ["cs.LG", "cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.12253", "abs": "https://arxiv.org/abs/2508.12253", "authors": ["Manish Shukla"], "title": "Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset", "comment": null, "summary": "Time-series forecasting underpins critical decisions across aviation, energy,\nretail and health. Classical autoregressive integrated moving average (ARIMA)\nmodels offer interpretability via coefficients but struggle with\nnonlinearities, whereas tree-based machine-learning models such as XGBoost\ndeliver high accuracy but are often opaque. This paper presents a unified\nframework for interpreting time-series forecasts using local interpretable\nmodel-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We\nconvert a univariate series into a leakage-free supervised learning problem,\ntrain a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc\nexplainability. Using the Air Passengers dataset as a case study, we show that\na small set of lagged features -- particularly the twelve-month lag -- and\nseasonal encodings explain most forecast variance. We contribute: (i) a\nmethodology for applying LIME and SHAP to time series without violating\nchronology; (ii) theoretical exposition of the underlying algorithms; (iii)\nempirical evaluation with extensive analysis; and (iv) guidelines for\npractitioners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u4f7f\u7528LIME\u548cSHAP\u6765\u89e3\u91ca\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86ARIMA\u7684\u53ef\u89e3\u91ca\u6027\u548cXGBoost\u7684\u9ad8\u7cbe\u5ea6\uff0c\u901a\u8fc7\u65e0\u6cc4\u6f0f\u76d1\u7763\u5b66\u4e60\u8f6c\u6362\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "motivation": "\u4f20\u7edfARIMA\u6a21\u578b\u5177\u6709\u53ef\u89e3\u91ca\u6027\u4f46\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\uff0c\u800c\u6811\u6a21\u578b\u5982XGBoost\u7cbe\u5ea6\u9ad8\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u517c\u987e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "\u5c06\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u65e0\u6cc4\u6f0f\u7684\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u8bad\u7ec3\u68af\u5ea6\u63d0\u5347\u6811\u548cARIMA\u57fa\u7ebf\u6a21\u578b\uff0c\u7136\u540e\u5e94\u7528LIME\u548cSHAP\u8fdb\u884c\u4e8b\u540e\u89e3\u91ca\u5206\u6790\u3002", "result": "\u5728\u822a\u7a7a\u4e58\u5ba2\u6570\u636e\u96c6\u4e0a\u7684\u7814\u7a76\u8868\u660e\uff0c\u5c11\u91cf\u6ede\u540e\u7279\u5f81\uff08\u7279\u522b\u662f12\u4e2a\u6708\u6ede\u540e\uff09\u548c\u5b63\u8282\u6027\u7f16\u7801\u80fd\u591f\u89e3\u91ca\u5927\u90e8\u5206\u9884\u6d4b\u65b9\u5dee\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5c06LIME\u548cSHAP\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u65b9\u6cd5\u8bba\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3001\u5b9e\u8bc1\u8bc4\u4f30\u548c\u5b9e\u8df5\u6307\u5357\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12270", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12270", "abs": "https://arxiv.org/abs/2508.12270", "authors": ["Gal Lifshitz", "Shahar Zuler", "Ori Fouks", "Dan Raviv"], "title": "L-SR1: Learned Symmetric-Rank-One Preconditioning", "comment": "Under review", "summary": "End-to-end deep learning has achieved impressive results but remains limited\nby its reliance on large labeled datasets, poor generalization to unseen\nscenarios, and growing computational demands. In contrast, classical\noptimization methods are data-efficient and lightweight but often suffer from\nslow convergence. While learned optimizers offer a promising fusion of both\nworlds, most focus on first-order methods, leaving learned second-order\napproaches largely unexplored.\n  We propose a novel learned second-order optimizer that introduces a trainable\npreconditioning unit to enhance the classical Symmetric-Rank-One (SR1)\nalgorithm. This unit generates data-driven vectors used to construct positive\nsemi-definite rank-one matrices, aligned with the secant constraint via a\nlearned projection. Our method is evaluated through analytic experiments and on\nthe real-world task of Monocular Human Mesh Recovery (HMR), where it\noutperforms existing learned optimization-based approaches. Featuring a\nlightweight model and requiring no annotated data or fine-tuning, our approach\noffers strong generalization and is well-suited for integration into broader\noptimization-based frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b66\u4e60\u578b\u4e8c\u9636\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u9884\u5904\u7406\u5355\u5143\u589e\u5f3a\u7ecf\u5178SR1\u7b97\u6cd5\uff0c\u5728\u5355\u76ee\u4eba\u4f53\u7f51\u683c\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u4f9d\u8d56\u5927\u6807\u6ce8\u6570\u636e\u96c6\u4e14\u6cdb\u5316\u6027\u5dee\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u6570\u636e\u9ad8\u6548\u4f46\u6536\u655b\u6162\u3002\u5b66\u4e60\u578b\u4e8c\u9636\u4f18\u5316\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u5f15\u5165\u53ef\u8bad\u7ec3\u9884\u5904\u7406\u5355\u5143\u751f\u6210\u6570\u636e\u9a71\u52a8\u5411\u91cf\uff0c\u6784\u5efa\u6b63\u534a\u5b9a\u79e9\u4e00\u77e9\u9635\uff0c\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u4e0e\u5272\u7ebf\u7ea6\u675f\u5bf9\u9f50", "result": "\u5728\u5206\u6790\u5b9e\u9a8c\u548c\u5355\u76ee\u4eba\u4f53\u7f51\u683c\u6062\u590d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u5fae\u8c03\u7684\u7279\u70b9", "conclusion": "\u8be5\u65b9\u6cd5\u6cdb\u5316\u6027\u5f3a\uff0c\u9002\u5408\u96c6\u6210\u5230\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u6846\u67b6\u4e2d\uff0c\u4e3a\u5b66\u4e60\u578b\u4e8c\u9636\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.12278", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12278", "abs": "https://arxiv.org/abs/2508.12278", "authors": ["Siyue Xie", "Da Sun Handason Tam", "Wing Cheong Lau"], "title": "CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision", "comment": "Accepted by ECAI 2025", "summary": "Graph Neural Networks (GNNs) are widely used as the engine for various\ngraph-related tasks, with their effectiveness in analyzing graph-structured\ndata. However, training robust GNNs often demands abundant labeled data, which\nis a critical bottleneck in real-world applications. This limitation severely\nimpedes progress in Graph Anomaly Detection (GAD), where anomalies are\ninherently rare, costly to label, and may actively camouflage their patterns to\nevade detection. To address these problems, we propose Context Refactoring\nContrast (CRoC), a simple yet effective framework that trains GNNs for GAD by\njointly leveraging limited labeled and abundant unlabeled data. Different from\nprevious works, CRoC exploits the class imbalance inherent in GAD to refactor\nthe context of each node, which builds augmented graphs by recomposing the\nattributes of nodes while preserving their interaction patterns. Furthermore,\nCRoC encodes heterogeneous relations separately and integrates them into the\nmessage-passing process, enhancing the model's capacity to capture complex\ninteraction semantics. These operations preserve node semantics while\nencouraging robustness to adversarial camouflage, enabling GNNs to uncover\nintricate anomalous cases. In the training stage, CRoC is further integrated\nwith the contrastive learning paradigm. This allows GNNs to effectively harness\nunlabeled data during joint training, producing richer, more discriminative\nnode embeddings. CRoC is evaluated on seven real-world GAD datasets with\nvarying scales. Extensive experiments demonstrate that CRoC achieves up to 14%\nAUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods\nunder limited-label settings.", "AI": {"tldr": "CRoC\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u91cd\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u6807\u7b7e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347GNN\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5f02\u5e38\u6837\u672c\u7a00\u5c11\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u4f2a\u88c5\u6a21\u5f0f\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86GAD\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faCRoC\u6846\u67b6\uff1a1\uff09\u5229\u7528GAD\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u91cd\u6784\u8282\u70b9\u4e0a\u4e0b\u6587\uff0c\u4fdd\u6301\u4ea4\u4e92\u6a21\u5f0f\u7684\u540c\u65f6\u91cd\u7ec4\u8282\u70b9\u5c5e\u6027\uff1b2\uff09\u5206\u522b\u7f16\u7801\u5f02\u8d28\u5173\u7cfb\u5e76\u6574\u5408\u5230\u6d88\u606f\u4f20\u9012\u4e2d\uff1b3\uff09\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u8303\u5f0f\u8054\u5408\u5229\u7528\u6709\u9650\u6807\u6ce8\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u4e16\u754cGAD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCRoC\u76f8\u6bd4\u57fa\u7ebfGNN\u63d0\u5347\u9ad8\u8fbe14%\u7684AUC\uff0c\u5728\u6709\u9650\u6807\u7b7e\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684GAD\u65b9\u6cd5\u3002", "conclusion": "CRoC\u901a\u8fc7\u4e0a\u4e0b\u6587\u91cd\u6784\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u6709\u6548\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86GAD\u4e2d\u6807\u7b7e\u7a00\u7f3a\u548c\u5f02\u5e38\u4f2a\u88c5\u7684\u95ee\u9898\uff0c\u4e3a\u56fe\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12327", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.12327", "abs": "https://arxiv.org/abs/2508.12327", "authors": ["Wei Jiang", "Lijun Zhang"], "title": "Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings", "comment": null, "summary": "In this paper, we analyze the convergence properties of the Lion optimizer.\nFirst, we establish that the Lion optimizer attains a convergence rate of\n$\\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes\nthe problem dimension and $T$ is the iteration number. To further improve this\nrate, we introduce the Lion optimizer with variance reduction, resulting in an\nenhanced convergence rate of $\\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in\ndistributed settings, where the standard and variance reduced version of the\ndistributed Lion can obtain the convergence rates of\n$\\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with\n$n$ denoting the number of nodes. Furthermore, we investigate a\ncommunication-efficient variant of the distributed Lion that ensures sign\ncompression in both communication directions. By employing the unbiased sign\noperations, the proposed Lion variant and its variance reduction counterpart,\nachieve convergence rates of $\\mathcal{O}\\left( \\max\n\\left\\{\\frac{d^{1/4}}{T^{1/4}}, \\frac{d^{1/10}}{n^{1/5}T^{1/5}} \\right\\}\n\\right)$ and $\\mathcal{O}\\left( \\frac{d^{1/4}}{T^{1/4}} \\right)$, respectively.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Lion\u4f18\u5316\u5668\u7684\u6536\u655b\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u6807\u51c6\u7248\u672c\u3001\u65b9\u5dee\u7f29\u51cf\u7248\u672c\u3001\u5206\u5e03\u5f0f\u7248\u672c\u4ee5\u53ca\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u53d8\u4f53\uff0c\u5e76\u7ed9\u51fa\u4e86\u5404\u81ea\u7684\u6536\u655b\u901f\u7387\u7406\u8bba\u5206\u6790\u3002", "motivation": "Lion\u4f18\u5316\u5668\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u5176\u7406\u8bba\u6536\u655b\u6027\u8d28\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5f0f\u8bbe\u7f6e\u4e0b\u7684\u6536\u655b\u6027\u80fd\u5206\u6790\u3002", "method": "\u91c7\u7528\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff0c\u9996\u5148\u5efa\u7acb\u6807\u51c6Lion\u4f18\u5316\u5668\u7684\u6536\u655b\u901f\u7387\uff0c\u7136\u540e\u5f15\u5165\u65b9\u5dee\u7f29\u51cf\u6280\u672f\u6539\u8fdb\u6536\u655b\u6027\u80fd\uff0c\u63a5\u7740\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u8bbe\u7f6e\uff0c\u6700\u540e\u63d0\u51fa\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u53d8\u4f53\u5e76\u4f7f\u7528\u65e0\u504f\u7b26\u53f7\u538b\u7f29\u6280\u672f\u3002", "result": "\u83b7\u5f97\u4e86\u591a\u4e2a\u6536\u655b\u901f\u7387\u7ed3\u679c\uff1a\u6807\u51c6Lion\u4e3aO(d^{1/2}T^{-1/4})\uff0c\u65b9\u5dee\u7f29\u51cf\u7248\u672c\u4e3aO(d^{1/2}T^{-1/3})\uff0c\u5206\u5e03\u5f0f\u7248\u672c\u5206\u522b\u4e3aO(d^{1/2}(nT)^{-1/4})\u548cO(d^{1/2}(nT)^{-1/3})\uff0c\u901a\u4fe1\u9ad8\u6548\u53d8\u4f53\u4e3aO(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})})\u548cO(d^{1/4}/T^{1/4})\u3002", "conclusion": "Lion\u4f18\u5316\u5668\u5177\u6709\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\uff0c\u901a\u8fc7\u65b9\u5dee\u7f29\u51cf\u548c\u5206\u5e03\u5f0f\u6280\u672f\u53ef\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6536\u655b\u901f\u7387\uff0c\u800c\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u53d8\u4f53\u5728\u4fdd\u6301\u6536\u655b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2508.12361", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.12361", "abs": "https://arxiv.org/abs/2508.12361", "authors": ["Xun Su", "Jianming Huang", "Yang Yusen", "Zhongxi Fang", "Hiroyuki Kasai"], "title": "Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models", "comment": null, "summary": "Inference-time scaling has achieved remarkable success in language models,\nyet its adaptation to diffusion models remains underexplored. We observe that\nthe efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems\nfrom globally fitting the The reward-tilted distribution, which inherently\npreserves diversity during multi-modal search. However, current applications of\nSMC to diffusion models face a fundamental dilemma: early-stage noise samples\noffer high potential for improvement but are difficult to evaluate accurately,\nwhereas late-stage samples can be reliably assessed but are largely\nirreversible. To address this exploration-exploitation trade-off, we approach\nthe problem from the perspective of the search algorithm and propose two\nstrategies: Funnel Schedule and Adaptive Temperature. These simple yet\neffective methods are tailored to the unique generation dynamics and\nphase-transition behavior of diffusion models. By progressively reducing the\nnumber of maintained particles and down-weighting the influence of early-stage\nrewards, our methods significantly enhance sample quality without increasing\nthe total number of Noise Function Evaluations. Experimental results on\nmultiple benchmarks and state-of-the-art text-to-image diffusion models\ndemonstrate that our approach outperforms previous baselines.", "AI": {"tldr": "\u901a\u8fc7Funnel Schedule\u548cAdaptive Temperature\u7b56\u7565\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff0c\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6837\u672c\u8d28\u91cf", "motivation": "\u5c06\u8bed\u8a00\u6a21\u578b\u4e2d\u6210\u529f\u7684\u63a8\u7406\u65f6\u7f29\u653e\u6280\u672f\u9002\u914d\u5230\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3SMC\u65b9\u6cd5\u5728\u6269\u6563\u6a21\u578b\u4e2d\u9047\u5230\u7684\u65e9\u671f\u6837\u672c\u8bc4\u4f30\u56f0\u96be\u548c\u540e\u671f\u6837\u672c\u53ef\u9006\u6027\u5dee\u7684\u56f0\u5883", "method": "\u63d0\u51faFunnel Schedule\uff08\u9010\u6e10\u51cf\u5c11\u7ef4\u6301\u7684\u7c92\u5b50\u6570\u91cf\uff09\u548cAdaptive Temperature\uff08\u9002\u5e94\u6027\u8c03\u8282\u65e9\u671f\u5956\u52b1\u6743\u91cd\uff09\u4e24\u79cd\u7b56\u7565\uff0c\u4e13\u95e8\u4e3a\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u52a8\u529b\u5b66\u548c\u76f8\u53d8\u884c\u4e3a\u800c\u8bbe\u8ba1", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6d4b\u8bd5\u96c6\u548c\u6700\u65b0\u6587\u672c\u751f\u6210\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e0a\u8fc7\u4e86\u4e4b\u524d\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u8d28\u91cf", "conclusion": "\u901a\u8fc7\u4ece\u641c\u7d22\u7b97\u6cd5\u89d2\u5ea6\u51fa\u53d1\u7684\u7b80\u5355\u6709\u6548\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u63a8\u7406\u65f6\u7f29\u653e\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2508.12418", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12418", "abs": "https://arxiv.org/abs/2508.12418", "authors": ["Rachael DeVries", "Casper Christensen", "Marie Lisandra Zepeda Mendoza", "Ole Winther"], "title": "Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification", "comment": "18 pages, 7 figures. Submitted to the IEEE for possible publication", "summary": "Electronic Health Records (EHRs), the digital representation of a patient's\nmedical history, are a valuable resource for epidemiological and clinical\nresearch. They are also becoming increasingly complex, with recent trends\nindicating larger datasets, longer time series, and multi-modal integrations.\nTransformers, which have rapidly gained popularity due to their success in\nnatural language processing and other domains, are well-suited to address these\nchallenges due to their ability to model long-range dependencies and process\ndata in parallel. But their application to EHR classification remains limited\nby data representations, which can reduce performance or fail to capture\ninformative missingness. In this paper, we present the Bi-Axial Transformer\n(BAT), which attends to both the clinical variable and time point axes of EHR\ndata to learn richer data relationships and address the difficulties of data\nsparsity. BAT achieves state-of-the-art performance on sepsis prediction and is\ncompetitive to top methods for mortality classification. In comparison to other\ntransformers, BAT demonstrates increased robustness to data missingness, and\nlearns unique sensor embeddings which can be used in transfer learning.\nBaseline models, which were previously located across multiple repositories or\nutilized deprecated libraries, were re-implemented with PyTorch and made\navailable for reproduction and future benchmarking.", "AI": {"tldr": "\u63d0\u51fa\u4e86Bi-Axial Transformer (BAT)\u6a21\u578b\uff0c\u901a\u8fc7\u540c\u65f6\u5173\u6ce8\u4e34\u5e8a\u53d8\u91cf\u548c\u65f6\u95f4\u8f74\u6765\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u5728\u8113\u6bd2\u75c7\u9884\u6d4b\u548c\u6b7b\u4ea1\u7387\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHRs)\u6570\u636e\u65e5\u76ca\u590d\u6742\uff0c\u5305\u542b\u66f4\u5927\u6570\u636e\u96c6\u3001\u66f4\u957f\u65f6\u5e8f\u548c\u591a\u6a21\u6001\u96c6\u6210\u3002\u4f20\u7edfTransformer\u5728\u5904\u7406EHR\u5206\u7c7b\u65f6\u53d7\u9650\u4e8e\u6570\u636e\u8868\u793a\u65b9\u5f0f\uff0c\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u6216\u65e0\u6cd5\u6355\u6349\u4fe1\u606f\u6027\u7f3a\u5931\u3002", "method": "\u5f00\u53d1\u4e86\u53cc\u5411\u8f74Transformer(BAT)\u6a21\u578b\uff0c\u540c\u65f6\u5173\u6ce8\u4e34\u5e8a\u53d8\u91cf\u8f74\u548c\u65f6\u95f4\u70b9\u8f74\uff0c\u5b66\u4e60\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u5173\u7cfb\u5e76\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\u3002", "result": "BAT\u5728\u8113\u6bd2\u75c7\u9884\u6d4b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u6b7b\u4ea1\u7387\u5206\u7c7b\u4e0a\u4e0e\u9876\u7ea7\u65b9\u6cd5\u7ade\u4e89\u3002\u76f8\u6bd4\u5176\u4ed6Transformer\uff0cBAT\u5bf9\u6570\u636e\u7f3a\u5931\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5b66\u4e60\u53ef\u7528\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u72ec\u7279\u4f20\u611f\u5668\u5d4c\u5165\u3002", "conclusion": "BAT\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86EHR\u6570\u636e\u5904\u7406\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u5bf9\u6570\u636e\u7f3a\u5931\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u578b\u5b9e\u73b0\u3002"}}
{"id": "2508.12440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12440", "abs": "https://arxiv.org/abs/2508.12440", "authors": ["Ahmet Bilal Ar\u0131kan", "\u015eener \u00d6z\u00f6nder", "Mustafa Taha Ko\u00e7yi\u011fit", "H\u00fcseyin Oktay Altun", "H. K\u00fcbra K\u00fc\u00e7\u00fckkartal", "Murat Arslano\u011flu", "Fatih \u00c7a\u011f\u0131rankaya", "Berk Ayvaz"], "title": "Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features", "comment": null, "summary": "We present an integrated machine learning framework that transforms how\nmanufacturing cost is estimated from 2D engineering drawings. Unlike\ntraditional quotation workflows that require labor-intensive process planning,\nour approach about 200 geometric and statistical descriptors directly from\n13,684 DWG drawings of automotive suspension and steering parts spanning 24\nproduct groups. Gradient-boosted decision tree models (XGBoost, CatBoost,\nLightGBM) trained on these features achieve nearly 10% mean absolute percentage\nerror across groups, demonstrating robust scalability beyond part-specific\nheuristics. By coupling cost prediction with explainability tools such as SHAP,\nthe framework identifies geometric design drivers including rotated dimension\nmaxima, arc statistics and divergence metrics, offering actionable insights for\ncost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead\ntimes, ensures consistent and transparent cost assessments across part families\nand provides a deployable pathway toward real-time, ERP-integrated decision\nsupport in Industry 4.0 manufacturing environments.", "AI": {"tldr": "\u4e00\u4e2a\u96c6\u6210\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u4ece2D\u5de5\u7a0b\u56fe\u9884\u6d4b\u5236\u9020\u6210\u672c\uff0c\u51cf\u5c11\u4eba\u5de5\u8fc7\u7a0b\u89c4\u5212\u9700\u6c42\uff0c\u8fbe\u523010%\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee", "motivation": "\u6539\u53d8\u4f20\u7edf\u5236\u9020\u6210\u672c\u4f30\u7b97\u65b9\u5f0f\uff0c\u514d\u53bb\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u8fc7\u7a0b\u89c4\u5212\uff0c\u7f29\u77ed\u62a5\u4ef7\u5468\u671f\uff0c\u63d0\u4f9b\u4e00\u81f4\u548c\u900f\u660e\u7684\u6210\u672c\u8bc4\u4f30", "method": "\u4ece13,684\u5f20\u6c7d\u8f66\u60ac\u6302\u548c\u8f6c\u5411\u90e8\u4ef6DWG\u56fe\u7eb8\u4e2d\u63d0\u53d6\u7ea6200\u4e2a\u51e0\u4f55\u548c\u7edf\u8ba1\u63cf\u8ff0\u7b26\uff0c\u4f7f\u7528\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u6a21\u578b(XGBoost, CatBoost, LightGBM)\u8fdb\u884c\u8bad\u7ec3", "result": "\u572824\u4e2a\u4ea7\u54c1\u7ec4\u4e2d\u8fbe\u5230\u8fd110%\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff0c\u663e\u793a\u4e86\u8d85\u8d8a\u90e8\u4ef6\u7279\u5b9a\u7ecf\u9a8c\u6cd5\u5219\u7684\u7a33\u5065\u6269\u5c55\u6027", "conclusion": "\u8be5\u7aef\u5230\u7aef\u7684CAD\u5230\u6210\u672c\u6d41\u6c34\u7ebf\u7f29\u77ed\u4e86\u62a5\u4ef7\u5468\u671f\uff0c\u4e3a\u4ea7\u4e1a4.0\u5236\u9020\u73af\u5883\u4e2d\u5b9e\u65f6\u3001ERP\u96c6\u6210\u7684\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u9014\u5f84"}}
{"id": "2508.12450", "categories": ["cs.LG", "I.5.3"], "pdf": "https://arxiv.org/pdf/2508.12450", "abs": "https://arxiv.org/abs/2508.12450", "authors": ["\u00c9tienne Pepin"], "title": "Local Cluster Cardinality Estimation for Adaptive Mean Shift", "comment": "24 pages, 9 figures", "summary": "This article presents an adaptive mean shift algorithm designed for datasets\nwith varying local scale and cluster cardinality. Local distance distributions,\nfrom a point to all others, are used to estimate the cardinality of the local\ncluster by identifying a local minimum in the density of the distance\ndistribution. Based on these cardinality estimates, local cluster parameters\nare then computed for the entire cluster in contrast to KDE-based methods,\nwhich provide insight only into localized regions of the cluster. During the\nmean shift execution, the cluster cardinality estimate is used to adaptively\nadjust the bandwidth and the mean shift kernel radius threshold. Our algorithm\noutperformed a recently proposed adaptive mean shift method on its original\ndataset and demonstrated competitive performance on a broader clustering\nbenchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5747\u503c\u6f02\u79fb\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u8ddd\u79bb\u5206\u5e03\u4f30\u8ba1\u805a\u7c7b\u57fa\u6570\uff0c\u52a8\u6001\u8c03\u6574\u5e26\u5bbd\u548c\u6838\u534a\u5f84\u9608\u503c\uff0c\u5728\u53d8\u5c3a\u5ea6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edfKDE\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u805a\u7c7b\u5c40\u90e8\u533a\u57df\u7684\u6d1e\u5bdf\uff0c\u65e0\u6cd5\u5904\u7406\u5177\u6709\u53d8\u5316\u5c40\u90e8\u5c3a\u5ea6\u548c\u805a\u7c7b\u57fa\u6570\u7684\u6570\u636e\u96c6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u7684\u65b9\u6cd5", "method": "\u5229\u7528\u70b9\u5230\u6240\u6709\u5176\u4ed6\u70b9\u7684\u5c40\u90e8\u8ddd\u79bb\u5206\u5e03\uff0c\u901a\u8fc7\u8bc6\u522b\u8ddd\u79bb\u5206\u5e03\u5bc6\u5ea6\u4e2d\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u6765\u4f30\u8ba1\u5c40\u90e8\u805a\u7c7b\u57fa\u6570\uff0c\u57fa\u4e8e\u57fa\u6570\u4f30\u8ba1\u8ba1\u7b97\u6574\u4e2a\u805a\u7c7b\u7684\u5c40\u90e8\u53c2\u6570\uff0c\u5728\u5747\u503c\u6f02\u79fb\u6267\u884c\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u8c03\u6574\u5e26\u5bbd\u548c\u6838\u534a\u5f84\u9608\u503c", "result": "\u8be5\u7b97\u6cd5\u5728\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u8fd1\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u5747\u503c\u6f02\u79fb\u65b9\u6cd5\uff0c\u5728\u66f4\u5e7f\u6cdb\u7684\u805a\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b", "conclusion": "\u57fa\u4e8e\u5c40\u90e8\u8ddd\u79bb\u5206\u5e03\u7684\u81ea\u9002\u5e94\u5747\u503c\u6f02\u79fb\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u53d8\u5c3a\u5ea6\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u8c03\u6574\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u805a\u7c7b\u6027\u80fd"}}
{"id": "2508.12485", "categories": ["cs.LG", "cs.AI", "cs.DB", "cs.NI", "C.2.4; C.4; D.4.2; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.12485", "abs": "https://arxiv.org/abs/2508.12485", "authors": ["Aayush Gupta", "Arpit Bhayani"], "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX", "comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache", "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.", "AI": {"tldr": "Cold-RL\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\uff0c\u901a\u8fc7Dueling DQN\u7f51\u7edc\u5728NGINX\u4e2d\u66ff\u4ee3\u4f20\u7edfLRU\uff0c\u5728\u4e25\u683c\u5fae\u79d2\u7ea7\u9884\u7b97\u4e0b\u663e\u8457\u63d0\u5347\u547d\u4e2d\u7387", "motivation": "\u4f20\u7edfLRU\u7f13\u5b58\u6dd8\u6c70\u7b56\u7565\u5bf9\u5bf9\u8c61\u5927\u5c0f\u4e0d\u654f\u611f\uff0c\u5728\u5468\u671f\u6027\u7a81\u53d1\u6d41\u91cf\u548c\u6df7\u5408\u5bf9\u8c61\u5927\u5c0f\u573a\u666f\u4e0b\u5bb9\u6613\u4ea7\u751f\u98a0\u7c38\u95ee\u9898\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6dd8\u6c70\u673a\u5236", "method": "\u4f7f\u7528Dueling Deep Q-Network\u4f5c\u4e3a\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7ONNX\u4fa7\u8f66\u670d\u52a1\u5728500\u5fae\u79d2\u8d85\u65f6\u5185\u5904\u7406\u6dd8\u6c70\u51b3\u7b56\uff0c\u63d0\u53d6\u5e74\u9f84\u3001\u5927\u5c0f\u3001\u547d\u4e2d\u6b21\u6570\u7b496\u4e2a\u8f7b\u91cf\u7ea7\u7279\u5f81\uff0c\u79bb\u7ebf\u8bad\u7ec3\u7b56\u7565", "result": "\u572825MB\u7f13\u5b58\u4e0b\u547d\u4e2d\u7387\u4ece0.1436\u63d0\u5347\u81f30.3538\uff08146%\u63d0\u5347\uff09\uff0c100MB\u4e0b\u4ece0.7530\u63d0\u5347\u81f30.8675\uff0815%\u63d0\u5347\uff09\uff0c\u63a8\u7406\u589e\u52a0\u4e0d\u52302%CPU\u5f00\u9500\uff0c\u4fdd\u630195%\u5206\u4f4d\u6dd8\u6c70\u5ef6\u8fdf\u5728\u9884\u7b97\u5185", "conclusion": "Cold-RL\u662f\u9996\u4e2a\u96c6\u6210\u5230NGINX\u4e2d\u5177\u6709\u4e25\u683cSLO\u7684\u5f3a\u5316\u5b66\u4e60\u6dd8\u6c70\u7b56\u7565\uff0c\u5728\u5c0f\u578b\u7f13\u5b58\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u968f\u7740\u7f13\u5b58\u589e\u5927\u4e0e\u4f20\u7edf\u65b9\u6cd5\u6027\u80fd\u8d8b\u8fd1"}}
{"id": "2508.12491", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12491", "abs": "https://arxiv.org/abs/2508.12491", "authors": ["Reza Shirkavand", "Shangqian Gao", "Peiran Yu", "Heng Huang"], "title": "Cost-Aware Contrastive Routing for LLMs", "comment": null, "summary": "We study cost-aware routing for large language models across diverse and\ndynamic pools of models. Existing approaches often overlook prompt-specific\ncontext, rely on expensive model profiling, assume a fixed set of experts, or\nuse inefficient trial-and-error strategies. We introduce Cost-Spectrum\nContrastive Routing (CSCR), a lightweight framework that maps both prompts and\nmodels into a shared embedding space to enable fast, cost-sensitive selection.\nCSCR uses compact, fast-to-compute logit footprints for open-source models and\nperplexity fingerprints for black-box APIs. A contrastive encoder is trained to\nfavor the cheapest accurate expert within adaptive cost bands. At inference\ntime, routing reduces to a single k-NN lookup via a FAISS index, requiring no\nretraining when the expert pool changes and enabling microsecond latency.\nAcross multiple benchmarks, CSCR consistently outperforms baselines, improving\nthe accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen\nLLMs and out-of-distribution prompts.", "AI": {"tldr": "CSCR\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63d0\u793a\u548c\u6a21\u578b\u6620\u5c04\u5230\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u6210\u672c\u654f\u611f\u7684\u6a21\u578b\u9009\u62e9\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534725%\u7684\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u63d0\u793a\u7279\u5b9a\u4e0a\u4e0b\u6587\u3001\u4f9d\u8d56\u6602\u8d35\u7684\u6a21\u578b\u5206\u6790\u3001\u5047\u8bbe\u56fa\u5b9a\u4e13\u5bb6\u96c6\u5408\u6216\u4f7f\u7528\u4f4e\u6548\u7684\u8bd5\u9519\u7b56\u7565\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684LLM\u8def\u7531\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7d27\u51d1\u7684logit\u8db3\u8ff9\uff08\u5f00\u6e90\u6a21\u578b\uff09\u548c\u56f0\u60d1\u5ea6\u6307\u7eb9\uff08\u9ed1\u76d2API\uff09\uff0c\u901a\u8fc7\u5bf9\u6bd4\u7f16\u7801\u5668\u8bad\u7ec3\u5728\u81ea\u9002\u5e94\u6210\u672c\u5e26\u5185\u9009\u62e9\u6700\u4fbf\u5b9c\u51c6\u786e\u7684\u4e13\u5bb6\uff0c\u63a8\u7406\u65f6\u901a\u8fc7FAISS\u7d22\u5f15\u8fdb\u884ck-NN\u67e5\u627e\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387-\u6210\u672c\u6743\u8861\u63d0\u5347\u8fbe25%\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684LLM\u548c\u5206\u5e03\u5916\u63d0\u793a\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CSCR\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684LLM\u8def\u7531\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5fae\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u4e13\u5bb6\u6c60\u53d8\u5316\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u4f18\u79c0\u7684\u6210\u672c-\u6027\u80fd\u5e73\u8861\u3002"}}
{"id": "2508.12511", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12511", "abs": "https://arxiv.org/abs/2508.12511", "authors": ["Denis Blessing", "Julius Berner", "Lorenz Richter", "Carles Domingo-Enrich", "Yuanqi Du", "Arash Vahdat", "Gerhard Neumann"], "title": "Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference", "comment": null, "summary": "Solving stochastic optimal control problems with quadratic control costs can\nbe viewed as approximating a target path space measure, e.g. via gradient-based\noptimization. In practice, however, this optimization is challenging in\nparticular if the target measure differs substantially from the prior. In this\nwork, we therefore approach the problem by iteratively solving constrained\nproblems incorporating trust regions that aim for approaching the target\nmeasure gradually in a systematic way. It turns out that this trust region\nbased strategy can be understood as a geometric annealing from the prior to the\ntarget measure, where, however, the incorporated trust regions lead to a\nprincipled and educated way of choosing the time steps in the annealing path.\nWe demonstrate in multiple optimal control applications that our novel method\ncan improve performance significantly, including tasks in diffusion-based\nsampling, transition path sampling, and fine-tuning of diffusion models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u4efb\u533a\u57df\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u9000\u706b\u7b56\u7565\u9010\u6b65\u903c\u8fd1\u76ee\u6807\u8def\u5f84\u7a7a\u95f4\u6d4b\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u6c42\u89e3\u6027\u80fd", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u76ee\u6807\u6d4b\u5ea6\u4e0e\u5148\u9a8c\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4f18\u5316\u7b56\u7565", "method": "\u91c7\u7528\u4fe1\u4efb\u533a\u57df\u7ea6\u675f\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u9000\u706b\u7b56\u7565\u4ece\u5148\u9a8c\u5206\u5e03\u9010\u6b65\u903c\u8fd1\u76ee\u6807\u6d4b\u5ea6\uff0c\u7cfb\u7edf\u6027\u5730\u9009\u62e9\u9000\u706b\u8def\u5f84\u7684\u65f6\u95f4\u6b65\u957f", "result": "\u5728\u6269\u6563\u91c7\u6837\u3001\u8fc7\u6e21\u8def\u5f84\u91c7\u6837\u548c\u6269\u6563\u6a21\u578b\u5fae\u8c03\u7b49\u591a\u4e2a\u6700\u4f18\u63a7\u5236\u5e94\u7528\u4e2d\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u8868\u73b0", "conclusion": "\u57fa\u4e8e\u4fe1\u4efb\u533a\u57df\u7684\u51e0\u4f55\u9000\u706b\u65b9\u6cd5\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u7406\u6027\u4e14\u6709\u6548\u7684\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u76ee\u6807\u4e0e\u5148\u9a8c\u5206\u5e03\u5dee\u5f02\u8f83\u5927\u7684\u6311\u6218\u6027\u573a\u666f"}}
{"id": "2508.12524", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12524", "abs": "https://arxiv.org/abs/2508.12524", "authors": ["Joseph Su\u00e1rez", "Kyoung Whan Choe", "David Bloomin", "Jianming Gao", "Yunkun Li", "Yao Feng", "Saidinesh Pola", "Kun Zhang", "Yonghui Zhu", "Nikhil Pinnaparaju", "Hao Xiang Li", "Nishaanth Kanna", "Daniel Scott", "Ryan Sullivan", "Rose S. Shuman", "Lucas de Alc\u00e2ntara", "Herbie Bradley", "Kirsty You", "Bo Wu", "Yuhao Jiang", "Qimai Li", "Jiaxin Chen", "Louis Castricato", "Xiaolong Zhu", "Phillip Isola"], "title": "Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning", "comment": null, "summary": "We present the results of the NeurIPS 2023 Neural MMO Competition, which\nattracted over 200 participants and submissions. Participants trained\ngoal-conditional policies that generalize to tasks, maps, and opponents never\nseen during training. The top solution achieved a score 4x higher than our\nbaseline within 8 hours of training on a single 4090 GPU. We open-source\neverything relating to Neural MMO and the competition under the MIT license,\nincluding the policy weights and training code for our baseline and for the top\nsubmissions.", "AI": {"tldr": "NeurIPS 2023 Neural MMO\u7ade\u8d5b\u5438\u5f15\u4e86200\u591a\u540d\u53c2\u4e0e\u8005\uff0c\u53c2\u8d5b\u8005\u8bad\u7ec3\u7684\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u80fd\u591f\u6cdb\u5316\u5230\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3001\u5730\u56fe\u548c\u5bf9\u624b\u3002\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u5728\u5355\u5f204090 GPU\u4e0a\u8bad\u7ec38\u5c0f\u65f6\u540e\uff0c\u5f97\u5206\u6bd4\u57fa\u7ebf\u9ad84\u500d\u3002\u6240\u6709\u76f8\u5173\u8d44\u6e90\u5747\u5df2\u5f00\u6e90\u3002", "motivation": "\u4e3e\u529eNeural MMO\u7ade\u8d5b\u65e8\u5728\u63a8\u52a8\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u5728\u590d\u6742\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u7814\u7a76\uff0c\u63a2\u7d22\u7b56\u7565\u5728\u9762\u5bf9\u672a\u77e5\u4efb\u52a1\u3001\u5730\u56fe\u548c\u5bf9\u624b\u65f6\u7684\u9002\u5e94\u6027\u8868\u73b0\u3002", "method": "\u53c2\u8d5b\u8005\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728Neural MMO\u73af\u5883\u4e2d\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002\u7ade\u8d5b\u8bbe\u7f6e\u4e86\u8bad\u7ec3\u4e2d\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3001\u5730\u56fe\u548c\u5bf9\u624b\u4f5c\u4e3a\u6d4b\u8bd5\u573a\u666f\uff0c\u4ee5\u68c0\u9a8c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u7ade\u8d5b\u5438\u5f15\u4e86200\u591a\u540d\u53c2\u4e0e\u8005\uff0c\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u5728\u5355\u5f204090 GPU\u4e0a\u8bad\u7ec38\u5c0f\u65f6\u540e\uff0c\u5f97\u5206\u8fbe\u5230\u57fa\u7ebf\u6c34\u5e73\u76844\u500d\uff0c\u663e\u793a\u51fa\u4f18\u79c0\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "Neural MMO\u7ade\u8d5b\u6210\u529f\u5c55\u793a\u4e86\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\uff0c\u5f00\u6e90\u6240\u6709\u8d44\u6e90\u5c06\u4e3a\u793e\u533a\u63d0\u4f9b\u5b9d\u8d35\u7684\u7814\u7a76\u57fa\u7840\uff0c\u63a8\u52a8\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.12531", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12531", "abs": "https://arxiv.org/abs/2508.12531", "authors": ["Minseon Kim", "Jin Myung Kwak", "Lama Alssum", "Bernard Ghanem", "Philip Torr", "David Krueger", "Fazl Barez", "Adel Bibi"], "title": "Rethinking Safety in LLM Fine-tuning: An Optimization Perspective", "comment": null, "summary": "Fine-tuning language models is commonly believed to inevitably harm their\nsafety, i.e., refusing to respond to harmful user requests, even when using\nharmless datasets, thus requiring additional safety measures. We challenge this\nbelief through systematic testing, showing that poor optimization choices,\nrather than inherent trade-offs, often cause safety problems, measured as\nharmful responses to adversarial prompts. By properly selecting key training\nhyper-parameters, e.g., learning rate, batch size, and gradient steps, we\nreduce unsafe model responses from 16\\% to approximately 5\\%, as measured by\nkeyword matching, while maintaining utility performance. Based on this\nobservation, we propose a simple exponential moving average (EMA) momentum\ntechnique in parameter space that preserves safety performance by creating a\nstable optimization path and retains the original pre-trained model's safety\nproperties. Our experiments on the Llama families across multiple datasets\n(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can\nlargely be avoided without specialized interventions, outperforming existing\napproaches that require additional safety data while offering practical\nguidelines for maintaining both model performance and safety during adaptation.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u5316\u8c03\u4f18\u8d85\u53c2\u6570\u548cEMA\u52a8\u91cf\u6280\u672f\uff0c\u53ef\u4ee5\u5728\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u800c\u65e0\u9700\u989d\u5916\u5b89\u5168\u63aa\u65bd\uff0c\u5c06\u6709\u5bb3\u54cd\u5e94\u4ece16%\u964d\u81f35%", "motivation": "\u627f\u6210\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u5fc5\u7136\u4f1a\u635f\u5bb3\u5176\u5b89\u5168\u6027\u7684\u4f20\u7edf\u89c2\u70b9\uff0c\u9700\u8981\u989d\u5916\u5b89\u5168\u63aa\u65bd\uff0c\u4f46\u7814\u7a76\u8005\u8ba4\u4e3a\u8fd9\u662f\u7531\u4e8e\u4f18\u5316\u9009\u62e9\u4e0d\u5f53\u800c\u975e\u672c\u8d28\u95ee\u9898", "method": "\u7cfb\u7edf\u5316\u6d4b\u8bd5\u548c\u9009\u62e9\u5173\u952e\u8bad\u7ec3\u8d85\u53c2\u6570\uff08\u5b66\u4e60\u7387\u3001\u6279\u5904\u7406\u5927\u5c0f\u3001\u68af\u5ea6\u6b65\u957f\uff09\uff0c\u63d0\u51fa\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08EMA\uff09\u52a8\u91cf\u6280\u672f\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u521b\u5efa\u7a33\u5b9a\u4f18\u5316\u8def\u5f84", "result": "\u5728Llama\u6a21\u578b\u5bb6\u65cf\u548c\u591a\u4e2a\u6570\u636e\u96c6\uff08Dolly\u3001Alpaca\u3001ORCA\uff09\u4e0a\u5b9e\u9a8c\uff0c\u5c06\u4e0d\u5b89\u5168\u54cd\u5e94\u4ece16%\u964d\u81f3\u7ea65%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u80fd\uff0c\u8d85\u8d8a\u4e86\u9700\u8981\u989d\u5916\u5b89\u5168\u6570\u636e\u7684\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u53ef\u4ee5\u5927\u91cf\u907f\u514d\u800c\u65e0\u9700\u4e13\u95e8\u5e72\u9884\uff0c\u63d0\u4f9b\u4e86\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u548c\u5b89\u5168\u6027\u7684\u5b9e\u7528\u6307\u5357"}}
{"id": "2508.12533", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.12533", "abs": "https://arxiv.org/abs/2508.12533", "authors": ["Qinwen Ge", "Roza G. Bayrak", "Anwar Said", "Catie Chang", "Xenofon Koutsoukos", "Tyler Derr"], "title": "Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction", "comment": null, "summary": "The construction of brain graphs from functional Magnetic Resonance Imaging\n(fMRI) data plays a crucial role in enabling graph machine learning for\nneuroimaging. However, current practices often rely on rigid pipelines that\noverlook critical data-centric choices in how brain graphs are constructed. In\nthis work, we adopt a Data-Centric AI perspective and systematically define and\nbenchmark a data-centric design space for brain graph construction,\nconstrasting with primarily model-centric prior work. We organize this design\nspace into three stages: temporal signal processing, topology extraction, and\ngraph featurization. Our contributions lie less in novel components and more in\nevaluating how combinations of existing and modified techniques influence\ndownstream performance. Specifically, we study high-amplitude BOLD signal\nfiltering, sparsification and unification strategies for connectivity,\nalternative correlation metrics, and multi-view node and edge features, such as\nincorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets\nshow that thoughtful data-centric configurations consistently improve\nclassification accuracy over standard pipelines. These findings highlight the\ncritical role of upstream data decisions and underscore the importance of\nsystematically exploring the data-centric design space for graph-based\nneuroimaging. Our code is available at\nhttps://github.com/GeQinwen/DataCentricBrainGraphs.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ece\u6570\u636e\u4e2d\u5fc3AI\u89d2\u5ea6\u7cfb\u7edf\u7814\u7a76\u4e86\u529f\u80fd\u6020\u6563\u5171\u632f\u6210\u50cf(fMRI)\u6784\u5efa\u8111\u56fe\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e0d\u540c\u6570\u636e\u5904\u7406\u7ec4\u5408\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u5c42\u9762\u7684\u7ec6\u5fae\u9009\u62e9\u5bf9\u6020\u6563\u5171\u632f\u6210\u50cf\u56fe\u5b66\u4e60\u6027\u80fd\u7684\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u6784\u5efa\u8111\u56fe\u7684\u6d41\u6c34\u7ebf\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u4e2d\u5fc3\u7684\u5173\u952e\u9009\u62e9\uff0c\u672c\u6587\u91c7\u7528\u6570\u636e\u4e2d\u5fc3AI\u89d2\u5ea6\u6765\u7cfb\u7edf\u7814\u7a76\u8fd9\u4e9b\u9009\u62e9\u5bf9\u4e0b\u6e38\u6020\u6563\u5171\u632f\u6210\u50cf\u56fe\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u8111\u56fe\u6784\u5efa\u8bbe\u8ba1\u7a7a\u95f4\u7ec4\u7ec7\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u65f6\u5e8f\u4fe1\u53f7\u5904\u7406\u3001\u62d3\u6251\u63d0\u53d6\u548c\u56fe\u7279\u5f81\u5316\uff0c\u7814\u7a76\u4e86\u9ad8\u632f\u5e45BOLD\u4fe1\u53f7\u7b5b\u6ce2\u3001\u805a\u5408\u6027\u7b56\u7565\u3001\u66ff\u4ee3\u76f8\u5173\u6307\u6807\u4ee5\u53ca\u591a\u89c6\u56fe\u8282\u70b9\u548c\u8fb9\u7279\u5f81\u7b49\u6280\u672f\u7ec4\u5408\u3002", "result": "\u5728HCP1200\u548cABIDE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u7ec6\u5fc3\u7684\u6570\u636e\u4e2d\u5fc3\u914d\u7f6e\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u8d85\u8fc7\u6807\u51c6\u6d41\u6c34\u7ebf\u3002", "conclusion": "\u4e0a\u6e38\u6570\u636e\u51b3\u7b56\u5728\u8111\u56fe\u6784\u5efa\u4e2d\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u7cfb\u7edf\u6027\u63a2\u7d22\u6570\u636e\u4e2d\u5fc3\u8bbe\u8ba1\u7a7a\u95f4\u5bf9\u57fa\u4e8e\u56fe\u7684\u6020\u6563\u5171\u632f\u6210\u50cf\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.12551", "categories": ["cs.LG", "cs.AI", "cs.OS", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.12551", "abs": "https://arxiv.org/abs/2508.12551", "authors": ["Hongyu Lin", "Yuchen Li", "Haoran Luo", "Kaichun Yao", "Libo Zhang", "Mingjie Xing", "Yanjun Wu"], "title": "OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning", "comment": null, "summary": "Linux kernel tuning is essential for optimizing operating system (OS)\nperformance. However, existing methods often face challenges in terms of\nefficiency, scalability, and generalization. This paper introduces OS-R1, an\nagentic Linux kernel tuning framework powered by rule-based reinforcement\nlearning (RL). By abstracting the kernel configuration space as an RL\nenvironment, OS-R1 facilitates efficient exploration by large language models\n(LLMs) and ensures accurate configuration modifications. Additionally, custom\nreward functions are designed to enhance reasoning standardization,\nconfiguration modification accuracy, and system performance awareness of the\nLLMs. Furthermore, we propose a two-phase training process that accelerates\nconvergence and minimizes retraining across diverse tuning scenarios.\nExperimental results show that OS-R1 significantly outperforms existing\nbaseline methods, achieving up to 5.6% performance improvement over heuristic\ntuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across\nvarious real-world applications, demonstrating its potential for practical\ndeployment in diverse environments. Our dataset and code are publicly available\nat https://github.com/LHY-24/OS-R1.", "AI": {"tldr": "OS-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u7684Linux\u5185\u6838\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5185\u6838\u914d\u7f6e\u7a7a\u95f4\u62bd\u8c61\u4e3aRL\u73af\u5883\uff0c\u5229\u7528LLM\u8fdb\u884c\u9ad8\u6548\u63a2\u7d22\uff0c\u5b9e\u73b0\u4e86\u6bd4\u542f\u53d1\u5f0f\u8c03\u4f18\u65b9\u6cd5\u9ad8\u8fbe5.6%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684Linux\u5185\u6838\u8c03\u4f18\u65b9\u6cd5\u5728\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u8c03\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOS-R1\u6846\u67b6\uff0c\u5c06\u5185\u6838\u914d\u7f6e\u7a7a\u95f4\u62bd\u8c61\u4e3aRL\u73af\u5883\uff0c\u8bbe\u8ba1\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u589e\u5f3aLLM\u7684\u63a8\u7406\u6807\u51c6\u5316\u548c\u914d\u7f6e\u4fee\u6539\u51c6\u786e\u6027\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\u52a0\u901f\u6536\u655b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aOS-R1\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe5.6%\uff0c\u4fdd\u6301\u9ad8\u6570\u636e\u6548\u7387\uff0c\u5e76\u5728\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "OS-R1\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u591a\u6837\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u4e3aLinux\u5185\u6838\u81ea\u52a8\u5316\u8c03\u4f18\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12555", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12555", "abs": "https://arxiv.org/abs/2508.12555", "authors": ["Junpeng Wang", "Yuzhong Chen", "Menghai Pan", "Chin-Chia Michael Yeh", "Mahashweta Das"], "title": "Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement", "comment": "11 pages, 10 figures", "summary": "Coding agents powered by large language models (LLMs) have gained traction\nfor automating code generation through iterative problem-solving with minimal\nhuman involvement. Despite the emergence of various frameworks, e.g.,\nLangChain, AutoML, and AIDE, ML scientists still struggle to effectively review\nand adjust the agents' coding process. The current approach of manually\ninspecting individual outputs is inefficient, making it difficult to track code\nevolution, compare coding iterations, and identify improvement opportunities.\nTo address this challenge, we introduce a visual analytics system designed to\nenhance the examination of coding agent behaviors. Focusing on the AIDE\nframework, our system supports comparative analysis across three levels: (1)\nCode-Level Analysis, which reveals how the agent debugs and refines its code\nover iterations; (2) Process-Level Analysis, which contrasts different\nsolution-seeking processes explored by the agent; and (3) LLM-Level Analysis,\nwhich highlights variations in coding behavior across different LLMs. By\nintegrating these perspectives, our system enables ML scientists to gain a\nstructured understanding of agent behaviors, facilitating more effective\ndebugging and prompt engineering. Through case studies using coding agents to\ntackle popular Kaggle competitions, we demonstrate how our system provides\nvaluable insights into the iterative coding process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\uff0c\u7528\u4e8e\u6539\u5584\u5bf9LLM\u9a71\u52a8\u7f16\u7801\u4ee3\u7406\u8fed\u4ee3\u8fc7\u7a0b\u7684\u5ba1\u67e5\u548c\u8c03\u6574\u6548\u679c", "motivation": "\u5f53\u524d\u624b\u52a8\u68c0\u67e5\u4ee3\u7406\u7f16\u7801\u8f93\u51fa\u7684\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u8ddf\u8e2a\u4ee3\u7801\u8fed\u4ee3\u8fc7\u7a0b\u548c\u8bc4\u4f30\u6539\u8fdb\u673a\u4f1a", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u89c6\u89c6\u89c9\u5206\u6790\u7cfb\u7edf\uff0c\u652f\u6301\u4ee3\u7801\u3001\u8fc7\u7a0b\u548cLLM\u4e09\u4e2a\u5c42\u6b21\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u91cd\u70b9\u5173\u6ce8AIDE\u6846\u67b6", "result": "\u901a\u8fc7\u5728Kaggle\u7ade\u8d5b\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u591f\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u8fed\u4ee3\u7f16\u7801\u8fc7\u7a0b\u6d1e\u5bdf", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5e2e\u52a9ML\u79d1\u5b66\u5bb6\u7ed3\u6784\u5316\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\uff0c\u63d0\u9ad8\u8c03\u8bd5\u548c\u63d0\u793a\u5de5\u7a0b\u6548\u6790"}}
{"id": "2508.12565", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12565", "abs": "https://arxiv.org/abs/2508.12565", "authors": ["Luke Li"], "title": "Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition", "comment": null, "summary": "To address the complexity of financial time series, this paper proposes a\nforecasting model combining sliding window and variational mode decomposition\n(VMD) methods. Historical stock prices and relevant market indicators are used\nto construct datasets. VMD decomposes non-stationary financial time series into\nsmoother subcomponents, improving model adaptability. The decomposed data is\nthen input into a deep learning model for prediction. The study compares the\nforecasting effects of an LSTM model trained on VMD-processed sequences with\nthose using raw time series, demonstrating better performance and stability.", "AI": {"tldr": "\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u548c\u53d8\u5206\u6a21\u6001\u5206\u89e3(VMD)\u7684\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7VMD\u5206\u89e3\u975e\u5e73\u7a33\u5e8f\u5217\u4e3a\u5e73\u6ed1\u5b50\u5206\u91cf\uff0c\u518d\u7528LSTM\u8fdb\u884c\u9884\u6d4b\uff0c\u76f8\u6bd4\u539f\u59cb\u5e8f\u5217\u8868\u73b0\u66f4\u597d\u66f4\u7a33\u5b9a", "motivation": "\u89e3\u51b3\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u9884\u6d4b\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u548cVMD\u65b9\u6cd5\u5206\u89e3\u5386\u53f2\u80a1\u4ef7\u548c\u5e02\u573a\u6307\u6807\u6570\u636e\uff0c\u5c06\u5206\u89e3\u540e\u7684\u5b50\u5206\u91cf\u8f93\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u9884\u6d4b", "result": "VMD\u5904\u7406\u7684\u5e8f\u5217\u8bad\u7ec3\u7684LSTM\u6a21\u578b\u76f8\u6bd4\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u5177\u6709\u66f4\u597d\u7684\u9884\u6d4b\u6548\u679c\u548c\u7a33\u5b9a\u6027", "conclusion": "VMD\u5206\u89e3\u80fd\u6709\u6548\u5904\u7406\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u7684\u975e\u5e73\u7a33\u7279\u6027\uff0c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd"}}
{"id": "2508.12575", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12575", "abs": "https://arxiv.org/abs/2508.12575", "authors": ["Zohra Yagoub", "Hafida Bouziane"], "title": "Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM", "comment": null, "summary": "The prediction of amyloidogenicity in peptides and proteins remains a focal\npoint of ongoing bioinformatics. The crucial step in this field is to apply\nadvanced computational methodologies. Many recent approaches to predicting\namyloidogenicity within proteins are highly based on evolutionary motifs and\nthe individual properties of amino acids. It is becoming increasingly evident\nthat the sequence information-based features show high predictive performance.\nConsequently, our study evaluated the contextual features of protein sequences\nobtained from a pretrained protein large language model leveraging\nbidirectional LSTM and GRU to predict amyloidogenic regions in peptide and\nprotein sequences. Our method achieved an accuracy of 84.5% on 10-fold\ncross-validation and an accuracy of 83% in the test dataset. Our results\ndemonstrate competitive performance, highlighting the potential of LLMs in\nenhancing the accuracy of amyloid prediction.", "AI": {"tldr": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u86cb\u767d\u8d28\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u5e8f\u5217\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u901a\u8fc7\u53cc\u5411LSTM\u548cGRU\u6a21\u578b\u9884\u6d4b\u86cb\u767d\u8d28\u548c\u81c9\u6bd2\u80bd\u80c0\u6c27\u80c6\u7d20\u6c89\u79ef\u533a\u57df\uff0c\u8fbe\u5230\u4e8684.5%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u80c6\u7d20\u6c89\u79ef\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u8fdb\u5316\u6a21\u4f53\u548c\u6c28\u57fa\u9178\u7279\u6027\uff0c\u5e8f\u5217\u4fe1\u606f\u7279\u5f81\u663e\u793a\u51fa\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u56e0\u6b64\u7814\u7a76\u8003\u8651\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e0a\u4e0b\u6587\u7279\u5f81\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u4ece\u9884\u8bad\u7ec3\u7684\u86cb\u767d\u8d28\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u5e8f\u5217\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u4f7f\u7528\u53cc\u5411LSTM\u548cGRU\u6a21\u578b\u8fdb\u884c\u80c6\u7d20\u6c89\u79ef\u533a\u57df\u9884\u6d4b\u3002", "result": "\u572810\u6298\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u83b7\u5f9784.5%\u7684\u51c6\u786e\u7387\uff0c\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u83b7\u5f9783%\u7684\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u7ade\u4e89\u6027\u80fd\u3002", "conclusion": "\u86cb\u767d\u8d28\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u9ad8\u80c6\u7d20\u6c89\u79ef\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4e3a\u751f\u7269\u4fe1\u606f\u5b66\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.12576", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12576", "abs": "https://arxiv.org/abs/2508.12576", "authors": ["Like Jian", "Dong Liu"], "title": "Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg", "comment": "Accepted by ICML 2025", "summary": "Federated learning (FL) enables decentralized clients to train a model\ncollaboratively without sharing local data. A key distinction between FL and\ncentralized learning is that clients' data are non-independent and identically\ndistributed, which poses significant challenges in training a global model that\ngeneralizes well across heterogeneous local data distributions. In this paper,\nwe analyze the convergence of overparameterized FedAvg with gradient descent\n(GD). We prove that the impact of data heterogeneity diminishes as the width of\nneural networks increases, ultimately vanishing when the width approaches\ninfinity. In the infinite-width regime, we further prove that both the global\nand local models in FedAvg behave as linear models, and that FedAvg achieves\nthe same generalization performance as centralized learning with the same\nnumber of GD iterations. Extensive experiments validate our theoretical\nfindings across various network architectures, loss functions, and optimization\nmethods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5206\u6790\u4e86\u8fc7\u53c2\u6570\u5316\u8054\u90a6\u5b66\u4e60(FedAvg)\u7684\u6536\u655b\u6027\uff0c\u8bc1\u660e\u7f51\u7edc\u5bbd\u5ea6\u589e\u52a0\u65f6\u6570\u636e\u5f02\u8d28\u6027\u5f71\u54cd\u9010\u6e10\u51cf\u5c0f\uff0c\u5728\u65e0\u9650\u5bbd\u5ea6\u65f6\u6d88\u5931\uff0c\u5e76\u4e14FedAvg\u53ef\u4ee5\u8fbe\u5230\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u540c\u7684\u6c47\u603b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u7684\u975eIID\u7279\u6027\u5bfc\u81f4\u5168\u5c40\u6a21\u578b\u5728\u5f02\u8d28\u6570\u636e\u5206\u5e03\u4e0a\u7684\u6c47\u603b\u6027\u6311\u6218\uff0c\u9700\u8981\u7406\u8bba\u5206\u6790\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u5982\u4f55\u51cf\u5c11\u8fd9\u79cd\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u5206\u6790\u8fc7\u53c2\u6570\u5316FedAvg\u7684\u6536\u655b\u6027\uff0c\u7406\u8bba\u8bc1\u660e\u7f51\u7edc\u5bbd\u5ea6\u4e0e\u6570\u636e\u5f02\u8d28\u6027\u5f71\u54cd\u7684\u5173\u7cfb\uff0c\u5e76\u5728\u65e0\u9650\u5bbd\u5ea6\u60c5\u51b5\u4e0b\u5206\u6790\u5168\u5c40\u548c\u672c\u5730\u6a21\u578b\u7684\u884c\u4e3a\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u6570\u636e\u5f02\u8d28\u6027\u5f71\u54cd\u968f\u7f51\u7edc\u5bbd\u5ea6\u589e\u52a0\u800c\u51cf\u5c0f\uff0c\u5728\u65e0\u9650\u5bbd\u5ea6\u65f6\u6d88\u5931\uff1b\u5728\u65e0\u9650\u5bbd\u5ea6\u65f6\uff0cFedAvg\u4e2d\u7684\u5168\u5c40\u548c\u672c\u5730\u6a21\u578b\u90fd\u884c\u4e3a\u5982\u7ebf\u6027\u6a21\u578b\uff0c\u4e14FedAvg\u53ef\u4ee5\u8fbe\u5230\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u540c\u7684\u6c47\u603b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u589e\u52a0\u795e\u7ecf\u7f51\u7edc\u7684\u5bbd\u5ea6\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u8d28\u6027\u5bf9\u6a21\u578b\u6c47\u603b\u6027\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0bFedAvg\u80fd\u591f\u5b9e\u73b0\u4e0e\u96c6\u4e2d\u5f0f\u5b66\u4e60\u76f8\u7b49\u7684\u6027\u80fd\u3002"}}
{"id": "2508.12590", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12590", "abs": "https://arxiv.org/abs/2508.12590", "authors": ["Jihoon Park", "Seungeun Oh", "Seong-Lyun Kim"], "title": "Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding", "comment": "6 pages, 5 figures", "summary": "To address the growing demand for on-device LLM inference in\nresource-constrained environments, hybrid language models (HLM) have emerged,\ncombining lightweight local models with powerful cloud-based LLMs. Recent\nstudies on HLM have primarily focused on improving accuracy and latency, while\noften overlooking communication and energy efficiency. We propose a token-level\nfiltering mechanism for an energy-efficient importance- and uncertainty-aware\nHLM inference that leverages both epistemic uncertainty and attention-based\nimportance. Our method opportunistically uploads only informative tokens,\nreducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and\nLLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and\ntoken throughput of 0.37 tokens/sec while saving the energy consumption by\n40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM\nbaseline, our method improves BERTScore from 85.8% to 87.0%, energy savings\nfrom 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an\nenergy-efficient and accurate deployment of LLMs in bandwidth-constrained edge\nenvironments.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7ea7\u5224\u65ad\u7684\u6df7\u5408\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5173\u6ce8\u91cd\u8981\u6027\u6765\u9009\u62e9\u6027\u5730\u4e0a\u4f20\u4fe1\u606f\u6743\u7684\u4ee4\u724c\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u8282\u7701\u80fd\u6d88\u548c\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u9700\u6c42\uff0c\u6df7\u5408\u8bed\u8a00\u6a21\u578b\u5f97\u5230\u4e86\u5173\u6ce8\u3002\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\uff0c\u800c\u5ffd\u89c6\u4e86\u901a\u4fe1\u548c\u80fd\u6d88\u6548\u7387\u8fd9\u4e9b\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7ea7\u5224\u65ad\u7684\u8fc7\u6ee4\u673a\u5236\uff0c\u5229\u7528\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u5173\u6ce8\u91cd\u8981\u6027\u6765\u8bc6\u522b\u4fe1\u606f\u6743\u7684\u4ee4\u724c\uff0c\u53ea\u4e0a\u4f20\u6709\u4fe1\u606f\u4ef7\u503c\u7684\u4ee4\u724c\u5230\u4e91\u7aef\u5904\u7406\uff0c\u51cf\u5c11LLM\u4f7f\u7528\u548c\u901a\u4fe1\u6210\u672c\u3002", "result": "\u5728TinyLlama-1.1B\u548cLLaMA-2-7B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e8687.5%\u7684BERT Score\u548c0.37 tokens/sec\u7684\u4ee4\u724c\u901f\u7387\uff0c\u76f8\u6bd4\u6807\u51c6HLM\u8282\u7701\u4e8640.7%\u7684\u80fd\u6d88\u3002\u4e0e\u4e4b\u524d\u7684U-HLM\u57fa\u7ebf\u76f8\u6bd4\uff0cBERTScore\u4ece85.8%\u63d0\u5347\u523087.0%\uff0c\u80fd\u6d88\u8282\u7701\u4ece31.6%\u63d0\u5347\u523043.6%\uff0c\u901f\u7387\u4ece0.36\u63d0\u5347\u52300.40\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u5b9e\u73b0\u80fd\u6548\u9ad8\u3001\u51c6\u786e\u6027\u597d\u7684LLM\u90e8\u7f72\uff0c\u4e3a\u8d44\u6e90\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u5927\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12593", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12593", "abs": "https://arxiv.org/abs/2508.12593", "authors": ["Zhihao Li", "Ting Wang", "Guojian Zou", "Ruofei Wang", "Ye Li"], "title": "Physics-informed deep operator network for traffic state estimation", "comment": "under review in Transportmetrica B: Transport Dynamics", "summary": "Traffic state estimation (TSE) fundamentally involves solving\nhigh-dimensional spatiotemporal partial differential equations (PDEs) governing\ntraffic flow dynamics from limited, noisy measurements. While Physics-Informed\nNeural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a\nphysics-informed deep operator network (PI-DeepONet) framework that\nreformulates TSE as an operator learning problem. Our approach trains a\nparameterized neural operator that maps sparse input data to the full\nspatiotemporal traffic state field, governed by the traffic flow conservation\nlaw. Crucially, unlike PINNs that enforce PDE constraints point-wise,\nPI-DeepONet integrates traffic flow conservation model and the fundamental\ndiagram directly into the operator learning process, ensuring physical\nconsistency while capturing congestion propagation, spatial correlations, and\ntemporal evolution. Experiments on the NGSIM dataset demonstrate superior\nperformance over state-of-the-art baselines. Further analysis reveals insights\ninto optimal function generation strategies and branch network complexity.\nAdditionally, the impact of input function generation methods and the number of\nfunctions on model performance is explored, highlighting the robustness and\nefficacy of proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc(PI-DeepONet)\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u5c06TSE\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u4ea4\u901a\u6d41\u5b88\u6052\u5b9a\u5f8b\u76f4\u63a5\u6574\u5408\u5230\u7b97\u5b50\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u5728NGSIM\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u53ea\u80fd\u9010\u70b9\u5f3a\u5236\u6267\u884cPDE\u7ea6\u675f\uff0c\u800c\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u672c\u8d28\u4e0a\u9700\u8981\u89e3\u51b3\u9ad8\u7ef4\u65f6\u7a7a\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u7b97\u5b50\u5b66\u4e60\u95ee\u9898\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc(PI-DeepONet)\u6846\u67b6\uff0c\u8bad\u7ec3\u53c2\u6570\u5316\u795e\u7ecf\u7b97\u5b50\u5c06\u7a00\u758f\u8f93\u5165\u6570\u636e\u6620\u5c04\u5230\u5b8c\u6574\u7684\u65f6\u7a7a\u4ea4\u901a\u72b6\u6001\u573a\uff0c\u76f4\u63a5\u5c06\u4ea4\u901a\u6d41\u5b88\u6052\u6a21\u578b\u548c\u57fa\u672c\u56fe\u6574\u5408\u5230\u7b97\u5b50\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728NGSIM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u62e5\u5835\u4f20\u64ad\u3001\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u65f6\u95f4\u6f14\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "conclusion": "PI-DeepONet\u6846\u67b6\u4e3a\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u7ea6\u675f\u76f4\u63a5\u6574\u5408\u5230\u7b97\u5b50\u5b66\u4e60\u4e2d\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5206\u6790\u4e86\u8f93\u5165\u51fd\u6570\u751f\u6210\u7b56\u7565\u548c\u5206\u652f\u7f51\u7edc\u590d\u6742\u5ea6\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.12594", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12594", "abs": "https://arxiv.org/abs/2508.12594", "authors": ["Vedant Puri", "Aditya Joglekar", "Kevin Ferguson", "Yu-hsuan Chen", "Yongjie Jessica Zhang", "Levent Burak Kara"], "title": "FLARE: Fast Low-rank Attention Routing Engine", "comment": null, "summary": "The quadratic complexity of self-attention limits its applicability and\nscalability on large unstructured meshes. We introduce Fast Low-rank Attention\nRouting Engine (FLARE), a linear complexity self-attention mechanism that\nroutes attention through fixed-length latent sequences. Each attention head\nperforms global communication among $N$ tokens by projecting the input sequence\nonto a fixed length latent sequence of $M \\ll N$ tokens using learnable query\ntokens. By routing attention through a bottleneck sequence, FLARE learns a\nlow-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only\nscales to unprecedented problem sizes, but also delivers superior accuracy\ncompared to state-of-the-art neural PDE surrogates across diverse benchmarks.\nWe also release a new additive manufacturing dataset to spur further research.\nOur code is available at https://github.com/vpuri3/FLARE.py.", "AI": {"tldr": "FLARE\u662f\u4e00\u79cd\u7ebf\u6027\u590d\u6742\u5ea6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u56fa\u5b9a\u957f\u5ea6\u7684\u6f5c\u5728\u5e8f\u5217\u8def\u7531\u6ce8\u610f\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u5728\u5927\u578b\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u7684\u5e94\u7528\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6ce8\u610f\u529b\u673a\u5236\u6765\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u3002", "method": "FLARE\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\u4ee4\u724c\u5c06\u8f93\u5165\u5e8f\u5217\u6295\u5f71\u5230\u56fa\u5b9a\u957f\u5ea6\u7684\u6f5c\u5728\u5e8f\u5217\uff08M << N\uff09\uff0c\u5728\u74f6\u9888\u5e8f\u5217\u4e2d\u8def\u7531\u6ce8\u610f\u529b\uff0c\u5b66\u4e60\u4f4e\u79e9\u5f62\u5f0f\u7684\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0O(NM)\u7684\u8ba1\u7b97\u6210\u672c\u3002", "result": "FLARE\u4e0d\u4ec5\u80fd\u591f\u6269\u5c55\u5230\u524d\u6240\u672a\u6709\u7684\u95ee\u9898\u89c4\u6a21\uff0c\u800c\u4e14\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u795e\u7ecfPDE\u4ee3\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u3002", "conclusion": "FLARE\u901a\u8fc7\u4f4e\u79e9\u6ce8\u610f\u529b\u8def\u7531\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u6ce8\u610f\u529b\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u975e\u7ed3\u6784\u5316\u7f51\u683c\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u65b0\u7684\u589e\u6750\u5236\u9020\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.12596", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12596", "abs": "https://arxiv.org/abs/2508.12596", "authors": ["Meng Zhang", "Chao Wang", "Hao Zhang", "Shaojun Dong", "Lixin He"], "title": "Constructing Invariant and Equivariant Operations by Symmetric Tensor Network", "comment": null, "summary": "Design of neural networks that incorporate symmetry is crucial for geometric\ndeep learning. Central to this effort is the development of invariant and\nequivariant operations. This works presents a systematic method for\nconstructing valid invariant and equivariant operations. It can handle inputs\nand outputs in the form of Cartesian tensors with different rank, as well as\nspherical tensors with different types. In addition, our method features a\ngraphical representation utilizing the symmetric tensor network, which\nsimplifies both the proofs and constructions related to invariant and\nequivariant functions. We also apply this approach to design the equivariant\ninteraction message for the geometry graph neural network, and equivariant\nmachine learning model to learn the constitutive law of materials.", "AI": {"tldr": "\u7cfb\u7edf\u6027\u65b9\u6cd5\u6784\u5efa\u5f02\u4e0d\u53d8\u548c\u7b49\u53d8\u64cd\u4f5c\uff0c\u652f\u6301\u5361\u5c14\u5750\u6807\u5f20\u91cf\u548c\u7403\u5f62\u5f20\u91cf\uff0c\u5e76\u5e94\u7528\u4e8e\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6750\u6599\u672c\u6784\u5173\u7cfb\u5b66\u4e60", "motivation": "\u8bbe\u8ba1\u5305\u542b\u5bf9\u79f0\u6027\u7684\u795e\u7ecf\u7f51\u7edc\u5bf9\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u5f02\u4e0d\u53d8\u548c\u7b49\u53d8\u64cd\u4f5c", "method": "\u63d0\u51fa\u7cfb\u7edf\u6027\u65b9\u6cd5\u6784\u5efa\u6709\u6548\u7684\u5f02\u4e0d\u53d8\u548c\u7b49\u53d8\u64cd\u4f5c\uff0c\u4f7f\u7528\u5bf9\u79f0\u5f20\u91cf\u7f51\u7edc\u7684\u56fe\u5f62\u8868\u793a\u6765\u7b80\u5316\u8bc1\u660e\u548c\u6784\u9020", "result": "\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7b49\u7ea7\u7684\u5361\u5c14\u5750\u6807\u5f20\u91cf\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u7403\u5f62\u5f20\u91cf", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u8bbe\u8ba1\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7b49\u53d8\u4ea4\u4e92\u6d88\u606f\u548c\u5b66\u4e60\u6750\u6599\u672c\u6784\u5173\u7cfb\u7684\u7b49\u53d8\u673a\u5668\u5b66\u4e60\u6a21\u578b"}}
{"id": "2508.12602", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12602", "abs": "https://arxiv.org/abs/2508.12602", "authors": ["Hansol Lim", "Jongseong Brad Choi", "Jee Won Lee", "Haeseong Jeoung", "Minkyu Han"], "title": "A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators", "comment": "This preprint corresponding to a manuscript has been submitted to a\n  journal for potential publication", "summary": "We present a hybrid surrogate model for electric vehicle parameter estimation\nand power consumption. We combine our novel architecture Spectral Parameter\nOperator built on a Fourier Neural Operator backbone for global context and a\ndifferentiable physics module in the forward pass. From speed and acceleration\nalone, it outputs time-varying motor and regenerative braking efficiencies, as\nwell as aerodynamic drag, rolling resistance, effective mass, and auxiliary\npower. These parameters drive a physics-embedded estimate of battery power,\neliminating any separate physics-residual loss. The modular design lets\nrepresentations converge to physically meaningful parameters that reflect the\ncurrent state and condition of the vehicle. We evaluate on real-world logs from\na Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean\nabsolute error of 0.2kW (about 1% of average traction power at highway speeds)\nfor Tesla vehicles and about 0.8kW on the Kia EV9. The framework is\ninterpretable, and it generalizes well to unseen conditions, and sampling\nrates, making it practical for path optimization, eco-routing, on-board\ndiagnostics, and prognostics health management.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u5080\u5408\u6a21\u578b\u7684\u7535\u52a8\u6c7d\u8f66\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8c26\u795e\u7ecf\u7f51\u7edf\u8ba1\u7b97\u673a\u548c\u53ef\u5fae\u7269\u7406\u6a21\u5757\uff0c\u4ece\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u4f30\u8ba1\u6c7d\u8f66\u7684\u591a\u79cd\u53c2\u6570\u548c\u7535\u6d88\u8017", "motivation": "\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u7b80\u5355\u7684\u8f66\u8f86\u6570\u636e\u4f30\u8ba1\u51fa\u591a\u79cd\u91cd\u8981\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u8def\u5f84\u4f18\u5316\u3001\u751f\u6001\u8def\u7531\u3001\u8f66\u8f86\u8bca\u65ad\u7b49\u5e94\u7528", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u8c26\u53c2\u6570\u8fd0\u7b97\u7b97\u6cd5\uff08Spectral Parameter Operator\uff09\u57fa\u4e8eFourier\u795e\u7ecf\u7f51\u7edf\u6784\u9020\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u53ef\u5fae\u7269\u7406\u6a21\u5757\u8fdb\u884c\u524d\u5411\u4f20\u64ad\u3002\u4ece\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u4f30\u8ba1\u7535\u673a\u6548\u7387\u3001\u5236\u52a8\u6548\u7387\u3001\u7a7a\u6c14\u529b\u3001\u6eda\u52a8\u963b\u529b\u7b49\u53c2\u6570", "result": "\u5728Tesla Model 3\u3001Model S\u548cKia EV9\u5b9e\u9645\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cTesla\u8f66\u578b\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.2kW\uff08\u9ad8\u901f\u65f6\u62c9\u529b\u529f\u7387\u7684\u7ea61%\uff09\uff0cKia EV9\u4e3a0.8kW", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u901a\u7528\u6027\uff0c\u80fd\u591f\u9002\u5e94\u672a\u89c1\u6761\u4ef6\u548c\u91c7\u6837\u7387\uff0c\u9002\u7528\u4e8e\u8def\u5f84\u4f18\u5316\u3001\u751f\u6001\u8def\u7531\u3001\u8f66\u8f86\u8bca\u65ad\u548c\u5065\u5eb7\u7ba1\u7406\u7b49\u5e94\u7528\u573a\u666f"}}
{"id": "2508.12604", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12604", "abs": "https://arxiv.org/abs/2508.12604", "authors": ["Yuyang Xu", "Yi Cheng", "Haochao Ying", "Zhuoyun Du", "Renjun Hu", "Xing Shi", "Wei Lin", "Jian Wu"], "title": "SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression", "comment": "Work in progress", "summary": "Test-time scaling has proven effective in further enhancing the performance\nof pretrained Large Language Models (LLMs). However, mainstream post-training\nmethods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)\nreasoning) often incur substantial computational overhead due to auxiliary\nmodels and overthinking. In this paper, we empirically reveal that the\nincorrect answers partially stem from verbose reasoning processes lacking\ncorrect self-fix, where errors accumulate across multiple reasoning steps. To\nthis end, we propose Self-traced Step-wise Preference Optimization (SSPO), a\npluggable RL process supervision framework that enables fine-grained\noptimization of each reasoning step. Specifically, SSPO requires neither\nauxiliary models nor stepwise manual annotations. Instead, it leverages\nstep-wise preference signals generated by the model itself to guide the\noptimization process for reasoning compression. Experiments demonstrate that\nthe generated reasoning sequences from SSPO are both accurate and succinct,\neffectively mitigating overthinking behaviors without compromising model\nperformance across diverse domains and languages.", "AI": {"tldr": "\u65e0\u9700\u8f85\u52a9\u6a21\u578b\u6216\u6b65\u9aa4\u6807\u6ce8\uff0c\u901a\u8fc7\u81ea\u6211\u8ffd\u8e2a\u6b65\u9aa4\u504f\u597d\u4f18\u5316\uff08SSPO\uff09\u538b\u7f29\u63a8\u7406\u8fc7\u7a0b\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5206\u601d\u8003\u95ee\u9898", "motivation": "\u4e3b\u6d41\u7684\u8c03\u6548\u65b9\u6cd5\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u914d\u5408\u601d\u7ef4\u94fe\u63a8\u7406\uff09\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8f85\u52a9\u6a21\u578b\u8d1f\u62c5\u5927\u548c\u8fc7\u5206\u601d\u8003\u95ee\u9898\uff0c\u9519\u8bef\u7b54\u6848\u90e8\u5206\u6765\u81ea\u4e8e\u7f3a\u4e4f\u81ea\u6211\u4fee\u6b63\u7684\u7e41\u7410\u63a8\u7406\u8fc7\u7a0b", "method": "\u63d0\u51faSSPO\u6846\u67b6\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6b65\u9aa4\u504f\u597d\u4fe1\u53f7\u8fdb\u884c\u7cbe\u7ec6\u5316\u4f18\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u7684\u538b\u7f29\u4f18\u5316", "result": "\u5b9e\u9a8c\u8868\u660eSSPO\u751f\u6210\u7684\u63a8\u7406\u5e8f\u5217\u51c6\u786e\u4e14\u7b80\u6d01\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u8fc7\u5206\u601d\u8003\u884c\u4e3a\uff0c\u5728\u591a\u57df\u548c\u591a\u8bed\u8a00\u4e0b\u5747\u672a\u7206\u5f03\u6a21\u578b\u6027\u80fd", "conclusion": "SSPO\u4f5c\u4e3a\u4e00\u79cd\u63d2\u4ef6\u5f0f\u7684\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u76d1\u7763\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027"}}
{"id": "2508.12623", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12623", "abs": "https://arxiv.org/abs/2508.12623", "authors": ["Florian J. Boge", "Annika Schuster"], "title": "How can we trust opaque systems? Criteria for robust explanations in XAI", "comment": "8 pages, 1 figure", "summary": "Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in\nscientific research. However, the price we pay for their impressively accurate\npredictions is significant: their inner workings are notoriously opaque - it is\nunknown to laypeople and researchers alike what features of the data a DL\nsystem focuses on and how it ultimately succeeds in predicting correct outputs.\nA necessary criterion for trustworthy explanations is that they should reflect\nthe relevant processes the algorithms' predictions are based on. The field of\neXplainable Artificial Intelligence (XAI) presents promising methods to create\nsuch explanations. But recent reviews about their performance offer reasons for\nskepticism. As we will argue, a good criterion for trustworthiness is\nexplanatory robustness: different XAI methods produce the same explanations in\ncomparable contexts. However, in some instances, all methods may give the same,\nbut still wrong, explanation. We therefore argue that in addition to\nexplanatory robustness (ER), a prior requirement of explanation method\nrobustness (EMR) has to be fulfilled by every XAI method. Conversely, the\nrobustness of an individual method is in itself insufficient for\ntrustworthiness. In what follows, we develop and formalize criteria for ER as\nwell as EMR, providing a framework for explaining and establishing trust in DL\nalgorithms. We also highlight interesting application cases and outline\ndirections for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd(XAI)\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u89e3\u91ca\u9c81\u68d2\u6027(ER)\u548c\u89e3\u91ca\u65b9\u6cd5\u9c81\u68d2\u6027(EMR)\u4e24\u4e2a\u5173\u952e\u6807\u51c6\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u53ef\u4fe1\u89e3\u91ca\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u867d\u7136\u9884\u6d4b\u51c6\u786e\uff0c\u4f46\u5176\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u4e0d\u900f\u660e\uff0c\u7f3a\u4e4f\u53ef\u4fe1\u7684\u89e3\u91ca\u65b9\u6cd5\u3002\u5f53\u524dXAI\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u786e\u4fdd\u89e3\u91ca\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u89e3\u91ca\u9c81\u68d2\u6027(ER)\u548c\u89e3\u91ca\u65b9\u6cd5\u9c81\u68d2\u6027(EMR)\u4e24\u4e2a\u6807\u51c6\u3002ER\u8981\u6c42\u4e0d\u540cXAI\u65b9\u6cd5\u5728\u53ef\u6bd4\u73af\u5883\u4e0b\u4ea7\u751f\u76f8\u540c\u89e3\u91ca\uff0cEMR\u8981\u6c42\u5355\u4e2a\u65b9\u6cd5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u4fdd\u6301\u89e3\u91ca\u4e00\u81f4\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u8bc4\u4f30XAI\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u5e76\u8bc6\u522b\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u89e3\u91ca\u7684\u60c5\u51b5\u3002", "conclusion": "\u4ec5\u9760\u5355\u4e2aXAI\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u53ef\u4fe1\u5ea6\uff0c\u5fc5\u987b\u540c\u65f6\u6ee1\u8db3ER\u548cEMR\u6807\u51c6\u3002\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u53ef\u4fe1\u7684\u6df1\u5ea6\u5b66\u4e60\u89e3\u91ca\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.12629", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.12629", "abs": "https://arxiv.org/abs/2508.12629", "authors": ["Ian Dunn", "David R. Koes"], "title": "FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation", "comment": null, "summary": "A generative model capable of sampling realistic molecules with desired\nproperties could accelerate chemical discovery across a wide range of\napplications. Toward this goal, significant effort has focused on developing\nmodels that jointly sample molecular topology and 3D structure. We present\nFlowMol3, an open-source, multi-modal flow matching model that advances the\nstate of the art for all-atom, small-molecule generation. Its substantial\nperformance gains over previous FlowMol versions are achieved without changes\nto the graph neural network architecture or the underlying flow matching\nformulation. Instead, FlowMol3's improvements arise from three\narchitecture-agnostic techniques that incur negligible computational cost:\nself-conditioning, fake atoms, and train-time geometry distortion. FlowMol3\nachieves nearly 100% molecular validity for drug-like molecules with explicit\nhydrogens, more accurately reproduces the functional group composition and\ngeometry of its training data, and does so with an order of magnitude fewer\nlearnable parameters than comparable methods. We hypothesize that these\ntechniques mitigate a general pathology affecting transport-based generative\nmodels, enabling detection and correction of distribution drift during\ninference. Our results highlight simple, transferable strategies for improving\nthe stability and quality of diffusion- and flow-based molecular generative\nmodels.", "AI": {"tldr": "FlowMol3\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u6d41\u5339\u914d\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u79cd\u67b6\u6784\u65e0\u5173\u7684\u6280\u672f\uff08\u81ea\u6761\u4ef6\u3001\u5047\u539f\u5b50\u548c\u8bad\u7ec3\u65f6\u51e0\u4f55\u626d\u66f2\uff09\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u5206\u5b50\u751f\u6210\u6027\u80fd\uff0c\u5728\u836f\u7269\u6837\u5206\u5b50\u4e0a\u8fbe\u5230\u8fd1100%\u7684\u6709\u6548\u6027\uff0c\u4e14\u53c2\u6570\u91cf\u6bd4\u540c\u7c7b\u65b9\u6cd5\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u751f\u6210\u5177\u6709\u6240\u9700\u6027\u8d28\u7684\u73b0\u5b9e\u5206\u5b50\u7684\u751f\u6210\u6a21\u578b\uff0c\u4ee5\u52a0\u901f\u5316\u5b66\u53d1\u73b0\u3002\u73b0\u6709\u6a21\u578b\u5728\u8054\u5408\u91c7\u6837\u5206\u5b50\u62d3\u6251\u548c3D\u7ed3\u6784\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u79cd\u67b6\u6784\u65e0\u5173\u6280\u672f\uff1a\u81ea\u6761\u4ef6\uff08self-conditioning\uff09\u3001\u5047\u539f\u5b50\uff08fake atoms\uff09\u548c\u8bad\u7ec3\u65f6\u51e0\u4f55\u626d\u66f2\uff08train-time geometry distortion\uff09\uff0c\u65e0\u9700\u6539\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6216\u6d41\u5339\u914d\u57fa\u7840\u516c\u5f0f\u3002", "result": "\u5728\u836f\u7269\u6837\u5206\u5b50\u4e0a\u5b9e\u73b0\u8fd1100%\u7684\u6709\u6548\u6027\uff0c\u66f4\u51c6\u786e\u5730\u590d\u73b0\u8bad\u7ec3\u6570\u636e\u7684\u529f\u80fd\u57fa\u56e2\u7ec4\u6210\u548c\u51e0\u4f55\u7ed3\u6784\uff0c\u53c2\u6570\u91cf\u6bd4\u540c\u7c7b\u65b9\u6cd5\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8fd9\u4e9b\u6280\u672f\u7f13\u89e3\u4e86\u57fa\u4e8e\u4f20\u8f93\u7684\u751f\u6210\u6a21\u578b\u7684\u666e\u904d\u75c5\u7406\u95ee\u9898\uff0c\u80fd\u591f\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u68c0\u6d4b\u548c\u7ea0\u6b63\u5206\u5e03\u6f02\u79fb\uff0c\u4e3a\u6539\u8fdb\u6269\u6563\u548c\u6d41\u57fa\u5206\u5b50\u751f\u6210\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u8d28\u91cf\u63d0\u4f9b\u4e86\u7b80\u5355\u53ef\u8f6c\u79fb\u7684\u7b56\u7565\u3002"}}
{"id": "2508.12650", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.12650", "abs": "https://arxiv.org/abs/2508.12650", "authors": ["Jiyeon Kang", "Songseong Kim", "Chanhui Lee", "Doyeong Hwang", "Joanie Hayoun Chung", "Yunkyung Ko", "Sumin Lee", "Sungwoong Kim", "Sungbin Lim"], "title": "Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery", "comment": "32 pages, 17 figures, 5 tables", "summary": "Ordering-based approaches to causal discovery identify topological orders of\ncausal graphs, providing scalable alternatives to combinatorial search methods.\nUnder the Additive Noise Model (ANM) assumption, recent causal ordering methods\nbased on score matching require an accurate estimation of the Hessian diagonal\nof the log-densities. However, previous approaches mainly use Stein gradient\nestimators, which are computationally expensive and memory-intensive. Although\nDiffAN addresses these limitations by substituting kernel-based estimates with\ndiffusion models, it remains numerically unstable due to the second-order\nderivatives of score models. To alleviate these problems, we propose\nScore-informed Neural Operator (SciNO), a probabilistic generative model in\nsmooth function spaces designed to stably approximate the Hessian diagonal and\nto preserve structural information during the score modeling. Empirical results\nshow that SciNO reduces order divergence by 42.7% on synthetic graphs and by\n31.5% on real-world datasets on average compared to DiffAN, while maintaining\nmemory efficiency and scalability. Furthermore, we propose a probabilistic\ncontrol algorithm for causal reasoning with autoregressive models that\nintegrates SciNO's probability estimates with autoregressive model priors,\nenabling reliable data-driven causal ordering informed by semantic information.\nConsequently, the proposed method enhances causal reasoning abilities of LLMs\nwithout additional fine-tuning or prompt engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e86SciNO\u6a21\u578b\u6765\u7a33\u5b9a\u4f30\u8ba1Hessian\u5bf9\u89d2\u77e9\u9635\uff0c\u6539\u8fdb\u4e86\u57fa\u4e8e\u6392\u5e8f\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5DiffAN", "motivation": "\u73b0\u6709\u7684\u56e0\u679c\u6392\u5e8f\u65b9\u6cd5\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u5bf9\u6570\u5bc6\u5ea6\u7684Hessian\u5bf9\u89d2\u77e9\u9635\uff0c\u4f46\u4e4b\u524d\u7684Stein\u68af\u5ea6\u4f30\u8ba1\u5668\u8ba1\u7b97\u6602\u8d35\u4e14\u5185\u5b58\u5bc6\u96c6\uff0c\u800cDiffAN\u65b9\u6cd5\u867d\u7136\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\u4f46\u4ecd\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027", "method": "\u63d0\u51fa\u4e86Score-informed Neural Operator (SciNO)\uff0c\u4e00\u79cd\u5728\u5e73\u6ed1\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u6982\u7387\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u7a33\u5b9a\u8fd1\u4f3cHessian\u5bf9\u89d2\u77e9\u9635\u5e76\u5728\u5206\u6570\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7ed3\u6784\u4fe1\u606f", "result": "\u5728\u5408\u6210\u56fe\u4e0a\u5e73\u5747\u51cf\u5c1142.7%\u7684\u6392\u5e8f\u5206\u6b67\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u51cf\u5c1131.5%\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u8fd8\u63d0\u51fa\u4e86\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7ed3\u5408\u7684\u6982\u7387\u63a7\u5236\u7b97\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u5fae\u8c03\u6216\u63d0\u793a\u5de5\u7a0b\u5c31\u80fd\u589e\u5f3aLLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12672", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12672", "abs": "https://arxiv.org/abs/2508.12672", "authors": ["Emmanouil Kritharakis", "Dusan Jakovetic", "Antonios Makris", "Konstantinos Tserpes"], "title": "Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering", "comment": "16 pages, 5 figures", "summary": "Federated Learning (FL) enables collaborative model training across multiple\nclients without sharing private data. We consider FL scenarios wherein FL\nclients are subject to adversarial (Byzantine) attacks, while the FL server is\ntrusted (honest) and has a trustworthy side dataset. This may correspond to,\ne.g., cases where the server possesses trusted data prior to federation, or to\nthe presence of a trusted client that temporarily assumes the server role. Our\napproach requires only two honest participants, i.e., the server and one\nclient, to function effectively, without prior knowledge of the number of\nmalicious clients. Theoretical analysis demonstrates bounded optimality gaps\neven under strong Byzantine attacks. Experimental results show that our\nalgorithm significantly outperforms standard and robust FL baselines such as\nMean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack\nstrategies including label flipping, sign flipping, and Gaussian noise addition\nacross MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u80fd\u5728\u5bfc\u81f4Byzantine\u653b\u51fb\u7684\u60e0\u5e94\u5b66\u4e60\u573a\u666f\u4e2d\u6709\u6548\u8fd0\u4f5c\u7684\u65b9\u6cd5\uff0c\u4ec5\u9700\u670d\u52a1\u5668\u548c\u4e00\u4e2a\u5ba2\u6237\u7aef\u662f\u53ef\u4fe1\u7684\uff0c\u65e0\u9700\u9884\u5148\u77e5\u9053\u6076\u610f\u5ba2\u6237\u7aef\u7684\u6570\u91cf\u3002", "motivation": "\u89e3\u51b3\u60e0\u5e94\u5b66\u4e60\u4e2d\u5b58\u5728Byzantine\u653b\u51fb\u7684\u95ee\u9898\uff0c\u5f53\u670d\u52a1\u5668\u6709\u53ef\u9760\u7684\u8fb9\u7f18\u6570\u636e\u96c6\u65f6\uff0c\u5982\u4f55\u5728\u4e0d\u77e5\u9053\u6076\u610f\u5ba2\u6237\u7aef\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\u4fdd\u8bc1\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u670d\u52a1\u5668\u7684\u53ef\u9760\u8fb9\u7f18\u6570\u636e\u96c6\uff0c\u4ec5\u9700\u670d\u52a1\u5668\u548c\u4e00\u4e2a\u5ba2\u6237\u7aef\u662f\u53ef\u4fe1\u7684\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u65b9\u6cd5\u5728\u5f3aByzantine\u653b\u51fb\u4e0b\u4ecd\u80fd\u4fdd\u6301\u6709\u754c\u7684\u6700\u4f18\u6027\u95f4\u9694\u3002", "result": "\u5728MNIST\u3001FMNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7b97\u6cd5\u5728\u5404\u79cd\u653b\u51fb\u7b56\u7565\uff08\u6807\u7b7e\u7ffb\u8f6c\u3001\u7b26\u53f7\u7ffb\u8f6c\u3001\u9ad8\u65af\u566a\u58f0\u7b49\uff09\u4e0b\u663e\u8457\u8d85\u8fc7\u4e86Mean\u3001Trimmed Mean\u3001Median\u3001Krum\u548cMulti-Krum\u7b49\u6807\u51c6\u548c\u7a33\u5065\u60e0\u5e94\u5b66\u4e60\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u60e0\u5e94\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684Byzantine\u653b\u51fb\u9632\u5fa1\u65b9\u6848\uff0c\u5728\u4ec5\u6709\u4e24\u4e2a\u53ef\u4fe1\u53c2\u4e0e\u8005\u7684\u60c5\u51b5\u4e0b\u4fbf\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\uff0c\u5177\u6709\u5f3a\u70c8\u7684\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2508.12673", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12673", "abs": "https://arxiv.org/abs/2508.12673", "authors": ["Yuhao Zhou", "Jindi Lv", "Yuxin Tian", "Dan Si", "Qing Ye", "Jiancheng Lv"], "title": "Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach", "comment": "17 pages", "summary": "Federated Learning (FL) has emerged as a promising paradigm for\nprivacy-preserving collaborative learning, yet data heterogeneity remains a\ncritical challenge. While existing methods achieve progress in addressing data\nheterogeneity for participating clients, they fail to generalize to\nnon-participating clients with in-domain distribution shifts and resource\nconstraints. To mitigate this issue, we present HyperFedZero, a novel method\nthat dynamically generates specialized models via a hypernetwork conditioned on\ndistribution-aware embeddings. Our approach explicitly incorporates\ndistribution-aware inductive biases into the model's forward pass, extracting\nrobust distribution embeddings using a NoisyEmbed-enhanced extractor with a\nBalancing Penalty, effectively preventing feature collapse. The hypernetwork\nthen leverages these embeddings to generate specialized models chunk-by-chunk\nfor non-participating clients, ensuring adaptability to their unique data\ndistributions. Extensive experiments on multiple datasets and models\ndemonstrate HyperFedZero's remarkable performance, surpassing competing methods\nconsistently with minimal computational, storage, and communication overhead.\nMoreover, ablation studies and visualizations further validate the necessity of\neach component, confirming meaningful adaptations and validating the\neffectiveness of HyperFedZero.", "AI": {"tldr": "HyperFedZero\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u9488\u5bf9\u975e\u53c2\u4e0e\u5ba2\u6237\u7aef\u7684\u4e13\u7528\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u6027\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u5177\u6709\u57df\u5185\u5206\u5e03\u504f\u79fb\u548c\u8d44\u6e90\u9650\u5236\u7684\u975e\u53c2\u4e0e\u5ba2\u6237\u7aef\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5206\u5e03\u611f\u77e5\u5d4c\u5165\u7684\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u4e13\u7528\u6a21\u578b\uff0c\u91c7\u7528NoisyEmbed\u589e\u5f3a\u7684\u63d0\u53d6\u5668\u548c\u5e73\u8861\u60e9\u7f5a\u6765\u9632\u6b62\u7279\u5f81\u5d29\u6e83\uff0c\u5206\u5757\u751f\u6210\u6a21\u578b\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cHyperFedZero\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0c\u8ba1\u7b97\u3001\u5b58\u50a8\u548c\u901a\u4fe1\u5f00\u9500\u6700\u5c0f\u3002\u6d88\u878d\u7814\u7a76\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "HyperFedZero\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u7684\u8d85\u7f51\u7edc\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4e3a\u975e\u53c2\u4e0e\u5ba2\u6237\u7aef\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.12712", "categories": ["cs.LG", "cs.CV", "I.2.6; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.12712", "abs": "https://arxiv.org/abs/2508.12712", "authors": ["Seyed Mahdi Haji Seyed Hossein", "Alireza Hosseini", "Soheil Hajian Manesh", "Amirali Shahriary"], "title": "Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs", "comment": "7 pages, 10 figures", "summary": "Connected and automated vehicles generate vast amounts of sensor data daily,\nraising significant privacy and communication challenges for centralized\nmachine learning approaches in perception tasks. This study presents a\ndecentralized, federated learning framework tailored for traffic sign detection\nin vehicular networks to enable collaborative model training without sharing\nraw data. The framework partitioned traffic sign classes across vehicles for\nspecialized local training using lightweight object detectors, aggregated model\nparameters via algorithms like FedProx, FedAdam and FedAVG in a simulated\nenvironment with the Flower framework, and evaluated multiple configurations\nincluding varying server rounds, local epochs, client participation fractions,\nand data distributions. Experiments demonstrated that increasing server rounds\nfrom 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs\n(8-10) provided optimal efficiency with accuracies around 0.67, higher client\nparticipation fractions enhanced generalization up to 0.83, FedProx\noutperformed other aggregators in handling heterogeneity, non-IID data\ndistributions reduced performance compared to IID, and training duration\nprimarily scaled with the number of rounds rather than aggregation strategy. We\nconclude that this federated approach may offer a scalable, privacy-preserving\nsolution for real-world vehicular deployments, potentially guiding future\nintegrations of robust aggregation and communication optimizations to advance\nintelligent transportation systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8f66\u8f86\u7f51\u7edc\u4e2d\u4ea4\u901a\u6807\u5fd7\u68c0\u6d4b\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\u907f\u514d\u539f\u59cb\u6570\u636e\u5171\u4eab\uff0c\u89e3\u51b3\u9690\u79c1\u548c\u901a\u4fe1\u6311\u6218\u3002", "motivation": "\u8fde\u63a5\u548c\u81ea\u52a8\u5316\u8f66\u8f86\u6bcf\u5929\u4ea7\u751f\u5de8\u91cf\u4f20\u611f\u5668\u6570\u636e\uff0c\u4e2d\u5fc3\u5316\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u611f\u77e5\u4efb\u52a1\u4e2d\u5e26\u6765\u4e86\u91cd\u5927\u9690\u79c1\u548c\u901a\u4fe1\u6311\u6218\u3002", "method": "\u5c06\u4ea4\u901a\u6807\u5fd7\u7c7b\u522b\u5728\u8f66\u8f86\u4e4b\u95f4\u5206\u533a\u8fdb\u884c\u4e13\u95e8\u5316\u672c\u5730\u8bad\u7ec3\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7FedProx\u3001FedAdam\u548cFedAVG\u7b49\u7b97\u6cd5\u5728Flower\u6846\u67b6\u4e2d\u8054\u5408\u6a21\u578b\u53c2\u6570\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u670d\u52a1\u5668\u8f6e\u6b21\u3001\u672c\u5730\u8fed\u4ee3\u3001\u5ba2\u6237\u53c2\u4e0e\u6bd4\u4f8b\u548c\u6570\u636e\u5206\u5e03\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u670d\u52a1\u5668\u8f6e\u6b21\u4ece2\u589e\u52a0\u523020\u65f6\u51c6\u786e\u7387\u4ece0.1\u4ee5\u4e0b\u63d0\u5347\u52300.8\u4ee5\u4e0a\uff1b\u9002\u4e2d\u672c\u5730\u8fed\u4ee3\uff088-10\uff09\u51c6\u786e\u7387\u7ea60.67\uff1b\u66f4\u9ad8\u5ba2\u6237\u53c2\u4e0e\u6bd4\u4f8b\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u81f30.83\uff1bFedProx\u5728\u5904\u7406\u5f02\u8d28\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u975eIID\u6570\u636e\u5206\u5e03\u6027\u80fd\u4e0d\u5982IID\u3002", "conclusion": "\u8fd9\u79cd\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u53ef\u80fd\u4e3a\u5b9e\u9645\u8f66\u8f86\u90e8\u7f72\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u96c6\u6210\u5065\u58ee\u8054\u5408\u7b97\u6cd5\u548c\u901a\u4fe1\u4f18\u5316\u63a8\u52a8\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u53d1\u5c55\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.12727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12727", "abs": "https://arxiv.org/abs/2508.12727", "authors": ["Manning Zhu", "Songtao Guo", "Pengzhan Zhou", "Yansong Ning", "Chang Han", "Dewen Qiao"], "title": "FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment", "comment": null, "summary": "Federated fine-tuning (FFT) of large language models (LLMs) has recently\nemerged as a promising solution to enable domain-specific adaptation while\npreserving data privacy. Despite its benefits, FFT on resource-constrained\nclients relies on the high computational and memory demands of full-model\nfine-tuning, which limits the potential advancement. This paper presents\nFedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs\nwithout accessing or storing the full model. Specifically, we first propose a\nsimilarity group pruning (SGP) module, which prunes redundant layers from the\nfull LLM while retaining the most critical layers to preserve the model\nperformance. Moreover, we introduce an orchestrated distillation alignment\n(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM\nduring FFT. Through the use of the QLoRA, clients only need to deploy quantized\nsub-LLMs and fine-tune lightweight adapters, significantly reducing local\nresource requirements. We conduct extensive experiments on three open-source\nLLMs across a variety of downstream tasks. The experimental results demonstrate\nthat FedSODA reduces communication overhead by an average of 70.6%, decreases\nstorage usage by 75.6%, and improves task accuracy by 3.1%, making it highly\nsuitable for practical FFT applications under resource constraints.", "AI": {"tldr": "FedSODA\u662f\u4e00\u4e2a\u8d44\u6e90\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u526a\u679d\u548c\u84b8\u998f\u5bf9\u9f50\u6280\u672f\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u3001\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u8054\u90a6\u5fae\u8c03\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fdb\u884c\u5168\u6a21\u578b\u5fae\u8c03\u65f6\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u76f8\u4f3c\u6027\u7ec4\u526a\u679d(SGP)\u6a21\u5757\u526a\u679d\u5197\u4f59\u5c42\uff0c\u4fdd\u7559\u5173\u952e\u5c42\uff1b\u5f15\u5165\u534f\u8c03\u84b8\u998f\u5bf9\u9f50(ODA)\u6a21\u5757\u51cf\u5c11\u5b50\u6a21\u578b\u4e0e\u5168\u6a21\u578b\u95f4\u7684\u68af\u5ea6\u5dee\u5f02\uff1b\u7ed3\u5408QLoRA\u6280\u672f\u8fdb\u884c\u91cf\u5316\u5b50\u6a21\u578b\u548c\u8f7b\u91cf\u9002\u914d\u5668\u5fae\u8c03\u3002", "result": "\u5e73\u5747\u51cf\u5c1170.6%\u901a\u4fe1\u5f00\u9500\uff0c\u964d\u4f4e75.6%\u5b58\u50a8\u4f7f\u7528\uff0c\u63d0\u53473.1%\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "FedSODA\u6846\u67b6\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2508.12740", "categories": ["cs.LG", "cs.AI", "68T01 (Primary), 68T07 (Secondary)", "I.2"], "pdf": "https://arxiv.org/pdf/2508.12740", "abs": "https://arxiv.org/abs/2508.12740", "authors": ["Beomseok Seo", "Kichang Lee", "JaeYeon Park"], "title": "FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models", "comment": "6 pages, 4 figures", "summary": "Federated learning (FL) enables decentralized model training without sharing\nlocal data. However, most existing methods assume identical model architectures\nacross clients, limiting their applicability in heterogeneous real-world\nenvironments. To address this, we propose FedUNet, a lightweight and\narchitecture-agnostic FL framework that attaches a U-Net-inspired additive\nmodule to each client's backbone. By sharing only the compact bottleneck of the\nU-Net, FedUNet enables efficient knowledge transfer without structural\nalignment. The encoder-decoder design and skip connections in the U-Net help\ncapture both low-level and high-level features, facilitating the extraction of\nclientinvariant representations. This enables cooperative learning between the\nbackbone and the additive module with minimal communication cost. Experiment\nwith VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in\ncompact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low\ncommunication overhead.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86FedUNet\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u7ed3\u6784\u65e0\u5173\u8054\u90e8\u7f72\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7U-Net\u6a21\u5757\u5b9e\u73b0\u5f02\u6784\u5ba2\u6237\u7aef\u95f4\u7684\u9ad8\u6548\u77e5\u8bc6\u4f20\u9012\uff0c\u5728\u4f4e\u901a\u4fe1\u5f00\u9500\u4e0b\u8fbe\u5230\u4e8693.11%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8054\u90e8\u5b66\u4e60\u65b9\u6cd5\u504f\u5dee\u5730\u5047\u8bbe\u6240\u6709\u5ba2\u6237\u7aef\u6a21\u578b\u7ed3\u6784\u76f8\u540c\u7684\u95ee\u9898\uff0c\u4ee5\u6ee1\u8db3\u5f02\u8d28\u6027\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u5728\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u4e3b\u5e72\u7f51\u7edc\u4e0a\u9644\u52a0U-Net\u53d7\u542f\u53d1\u7684\u52a0\u6027\u6a21\u5757\uff0c\u4ec5\u5171\u4eab\u7d27\u51d1\u7684U-Net\u74f6\u9888\u90e8\u5206\uff0c\u901a\u8fc7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8bbe\u8ba1\u548c\u8df3\u8fde\u63a5\u6355\u83b7\u4e0d\u540c\u7ea7\u522b\u7684\u7279\u5f81\uff0c\u5b9e\u73b0\u5ba2\u6237\u7aef\u4e0d\u53d8\u8868\u5f81\u63d0\u53d6\u3002", "result": "\u5728VGG\u53d8\u79cd\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cFedUNet\u8fbe\u5230\u4e8693.11%\u7684\u51c6\u786e\u7387\uff0c\u8f7b\u91cf\u7248\u672c\u4e5f\u8fbe\u523092.68%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u97000.89MB\u7684\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "FedUNet\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7684\u5f02\u6784\u8054\u90e8\u5b66\u4e60\u65b9\u6848\uff0c\u901a\u8fc7U-Net\u6a21\u5757\u5b9e\u73b0\u4e86\u5728\u4f4e\u901a\u4fe1\u6210\u672c\u4e0b\u7684\u77e5\u8bc6\u5171\u4eab\uff0c\u4e3a\u5f02\u8d28\u6027\u73af\u5883\u4e0b\u7684\u8054\u90e8\u5b66\u4e60\u5f00\u542f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.12741", "categories": ["cs.LG", "physics.app-ph", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2508.12741", "abs": "https://arxiv.org/abs/2508.12741", "authors": ["Manuela Imbriani", "Gina Belmonte", "Mieke Massink", "Alessandro Tofani", "Vincenzo Ciancia"], "title": "A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks", "comment": null, "summary": "This paper presents preliminary results in the definition of a comprehensive\nbenchmark framework designed to systematically evaluate spatial reasoning\ncapabilities in neural networks, with a particular focus on morphological\nproperties such as connectivity and distance relationships. The framework is\ncurrently being used to study the capabilities of nnU-Net, exploiting the\nspatial model checker VoxLogicA to generate two distinct categories of\nsynthetic datasets: maze connectivity problems for topological analysis and\nspatial distance computation tasks for geometric understanding. Each category\nis evaluated across multiple resolutions to assess scalability and\ngeneralization properties. The automated pipeline encompasses a complete\nmachine learning workflow including: synthetic dataset generation, standardized\ntraining with cross-validation, inference execution, and comprehensive\nevaluation using Dice coefficient and IoU (Intersection over Union) metrics.\nPreliminary experimental results demonstrate significant challenges in neural\nnetwork spatial reasoning capabilities, revealing systematic failures in basic\ngeometric and topological understanding tasks. The framework provides a\nreproducible experimental protocol, enabling researchers to identify specific\nlimitations. Such limitations could be addressed through hybrid approaches\ncombining neural networks with symbolic reasoning methods for improved spatial\nunderstanding in clinical applications, establishing a foundation for ongoing\nresearch into neural network spatial reasoning limitations and potential\nsolutions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u91c7\u7528\u7a7a\u95f4\u6a21\u578b\u68c0\u67e5\u5668\u751f\u6210\u8def\u5f84\u8fde\u901a\u6027\u548c\u7a7a\u95f4\u8ddd\u79bb\u8ba1\u7b97\u4efb\u52a1\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u5728\u57fa\u672c\u51e0\u4f55\u548c\u62d3\u6251\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "motivation": "\u5efa\u7acb\u4e00\u4e2a\u7ef4\u5ea6\u5168\u9762\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5f62\u6001\u5b66\u6027\u8d28\u5982\u8fde\u901a\u6027\u548c\u8ddd\u79bb\u5173\u7cfb\u3002", "method": "\u5229\u7528\u7a7a\u95f4\u6a21\u578b\u68c0\u67e5\u5668VoxLogicA\u751f\u6210\u4e24\u7c7b\u5408\u6210\u6570\u636e\u96c6\uff1a\u8def\u5f84\u8fde\u901a\u6027\u95ee\u9898\u7528\u4e8e\u62d3\u6251\u5206\u6790\uff0c\u7a7a\u95f4\u8ddd\u79bb\u8ba1\u7b97\u4efb\u52a1\u7528\u4e8e\u51e0\u4f55\u7406\u89e3\u3002\u6784\u5efa\u4e86\u5305\u542b\u6570\u636e\u751f\u6210\u3001\u6807\u51c6\u5316\u8bad\u7ec3\u3001\u63a8\u7406\u6267\u884c\u548c\u7efc\u5408\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528Dice\u7cfb\u6570\u548cIoU\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u795e\u7ecf\u7f51\u7edc\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u65b9\u9762\u9047\u5230\u91cd\u5927\u6311\u6218\uff0c\u5728\u57fa\u672c\u51e0\u4f55\u548c\u62d3\u6251\u7406\u89e3\u4efb\u52a1\u4e2d\u51fa\u73b0\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u534f\u8bae\uff0c\u80fd\u591f\u8bc6\u522b\u795e\u7ecf\u7f51\u7edc\u7684\u5177\u4f53\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u4e0e\u7b26\u53f7\u63a8\u7406\u6df7\u5408\u65b9\u6cd5\u63d0\u9ad8\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u5960\u5b9a\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u3002"}}
{"id": "2508.12764", "categories": ["cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2508.12764", "abs": "https://arxiv.org/abs/2508.12764", "authors": ["Cyril Voyant", "Milan Despotovic", "Luis Garcia-Gutierrez", "Mohammed Asloune", "Yves-Marie Saint-Drenan", "Jean-Laurent Duchaud", "hjuvan Antone Faggianelli", "Elena Magliaro"], "title": "Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach", "comment": null, "summary": "A novel methodology for short-term energy forecasting using an Extreme\nLearning Machine ($\\mathtt{ELM}$) is proposed. Using six years of hourly data\ncollected in Corsica (France) from multiple energy sources (solar, wind, hydro,\nthermal, bioenergy, and imported electricity), our approach predicts both\nindividual energy outputs and total production (\\cyr{including imports, which\nclosely follow energy demand, modulo losses)} through a Multi-Input\nMulti-Output ($\\mathtt{MIMO}$) architecture. To address non-stationarity and\nseasonal variability, sliding window techniques and cyclic time encoding are\nincorporated, enabling dynamic adaptation to fluctuations. The $\\mathtt{ELM}$\nmodel significantly outperforms persistence-based forecasting, particularly for\nsolar and thermal energy, achieving an $\\mathtt{nRMSE}$ of $17.9\\%$ and\n$5.1\\%$, respectively, with $\\mathtt{R^2} > 0.98$ (1-hour horizon). The model\nmaintains high accuracy up to five hours ahead, beyond which renewable energy\nsources become increasingly volatile. While $\\mathtt{MIMO}$ provides marginal\ngains over Single-Input Single-Output ($\\mathtt{SISO}$) architectures and\noffers key advantages over deep learning methods such as $\\mathtt{LSTM}$, it\nprovides a closed-form solution with lower computational demands, making it\nwell-suited for real-time applications, including online learning. Beyond\npredictive accuracy, the proposed methodology is adaptable to various contexts\nand datasets, as it can be tuned to local constraints such as resource\navailability, grid characteristics, and market structures.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u6781\u9650\u5b66\u4e60\u673a(ELM)\u7684\u65b0\u9898\u77ed\u671f\u80fd\u6e90\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7MIMO\u7ed3\u6784\u9884\u6d4b\u591a\u79cd\u80fd\u6e90\u4ea7\u51fa\u548c\u603b\u4ea7\u91cf\uff0c\u5728\u77ed\u671f\u9884\u6d4b\u4e2d\u663e\u8457\u8d85\u8fc7\u6301\u7eed\u6027\u9884\u6d4b\uff0c\u5177\u6709\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u80fd\u6e90\u7cfb\u7edf\u7684\u975e\u7a33\u5b9a\u6027\u3001\u5b63\u8282\u6027\u53d8\u5316\u548c\u591a\u79cd\u80fd\u6e90\u6e90\u7684\u590d\u6742\u9884\u6d4b\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u9002\u5e94\u6ce2\u52a8\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6781\u9650\u5b66\u4e60\u673a(ELM)\u7ed3\u5408\u591a\u8f93\u5165\u591a\u8f93\u51fa(MIMO)\u67b6\u6784\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u548c\u5468\u671f\u65f6\u95f4\u7f16\u7801\u6765\u5904\u7406\u975e\u7a33\u5b9a\u6027\u548c\u5b63\u8282\u6027\u53d8\u5316\uff0c\u57fa\u4e8e\u79d1\u897f\u5609\u516d\u5e74\u5c0f\u65f6\u6570\u636e\u8fdb\u884c\u591a\u80fd\u6e90(\u592a\u9633\u80fd\u3001\u98ce\u80fd\u3001\u6c34\u80fd\u3001\u70ed\u80fd\u3001\u751f\u7269\u80fd\u6e90\u548c\u8fdb\u53e3\u7535\u529b)\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u663e\u8457\u8d85\u8fc7\u6301\u7eed\u6027\u9884\u6d4b\uff0c\u592a\u9633\u80fd\u548c\u70ed\u80fd\u7684nRMSE\u5206\u522b\u4e3a17.9%\u548c5.1%\uff0cR\u00b2>0.98(1\u5c0f\u65f6\u9884\u6d4b)\uff0c\u57285\u5c0f\u65f6\u5185\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0cMIMO\u6bd4SISO\u6709\u8fb9\u9645\u6539\u5584\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u4e8eLSTM\u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5ELM-MIMO\u65b9\u6cd5\u4e3a\u77ed\u671f\u80fd\u6e90\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u3001\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u7684\u6709\u6548\u65b9\u6848\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5730\u533a\u7684\u8d44\u6e90\u53ef\u7528\u6027\u3001\u7f51\u683c\u7279\u6027\u548c\u5e02\u573a\u7ed3\u6784\u7b49\u672c\u5730\u7ea6\u675f\u3002"}}
{"id": "2508.12773", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12773", "abs": "https://arxiv.org/abs/2508.12773", "authors": ["Jiadong Chen", "Xiao He", "Hengyu Ye", "Fuxin Jiang", "Tieying Zhang", "Jianjun Chen", "Xiaofeng Gao"], "title": "Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling", "comment": "12 pages, 11 figures", "summary": "In the swiftly evolving domain of cloud computing, the advent of serverless\nsystems underscores the crucial need for predictive auto-scaling systems. This\nnecessity arises to ensure optimal resource allocation and maintain operational\nefficiency in inherently volatile environments. At the core of a predictive\nauto-scaling system is the workload forecasting model. Existing forecasting\nmodels struggle to quickly adapt to the dynamics in online workload streams and\nhave difficulty capturing the complex periodicity brought by fine-grained,\nhigh-frequency forecasting tasks. Addressing this, we propose a novel online\nensemble model, E3Former, for online workload forecasting in large-scale\npredictive auto-scaling. Our model synergizes the predictive capabilities of\nmultiple subnetworks to surmount the limitations of single-model approaches,\nthus ensuring superior accuracy and robustness. Remarkably, it accomplishes\nthis with a minimal increase in computational overhead, adhering to the lean\noperational ethos of serverless systems. Through extensive experimentation on\nreal-world workload datasets, we establish the efficacy of our ensemble model.\nIn online forecasting tasks, the proposed method reduces forecast error by an\naverage of 10%, and its effectiveness is further demonstrated through a\npredictive auto-scaling test in the real-life online system. Currently, our\nmethod has been deployed within ByteDance's Intelligent Horizontal Pod\nAuto-scaling (IHPA) platform, which supports the stable operation of over 30\napplications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The\npredictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis\nof essentially ensuring service quality, the predictive auto-scaling system can\nreduce resource utilization by over 40%.", "AI": {"tldr": "\u63d0\u51faE3Former\u5728\u7ebf\u96c6\u6210\u6a21\u578b\u7528\u4e8e\u670d\u52a1\u5668less\u7cfb\u7edf\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\uff0c\u76f8\u6bd4\u5355\u6a21\u578b\u65b9\u6cd5\u5e73\u5747\u51cf\u5c1110%\u9884\u6d4b\u8bef\u5dee\uff0c\u5df2\u5728\u5b57\u8282\u8df3\u52a8IHPA\u5e73\u53f0\u90e8\u7f72\uff0c\u652f\u630160\u4e07+CPU\u6838\u5fc3\u7684\u9884\u6d4b\u81ea\u52a8\u6269\u7f29\u5bb9\uff0c\u8d44\u6e90\u5229\u7528\u7387\u964d\u4f4e40%\u4ee5\u4e0a\u3002", "motivation": "\u670d\u52a1\u5668less\u7cfb\u7edf\u4e2d\u9700\u8981\u9884\u6d4b\u6027\u81ea\u52a8\u6269\u7f29\u5bb9\u6765\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u4f46\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u5728\u7ebf\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a8\u6001\u53d8\u5316\u548c\u6355\u83b7\u7ec6\u7c92\u5ea6\u9ad8\u9891\u4efb\u52a1\u7684\u590d\u6742\u5468\u671f\u6027\u3002", "method": "\u63d0\u51faE3Former\u5728\u7ebf\u96c6\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u540c\u591a\u4e2a\u5b50\u7f51\u7edc\u7684\u9884\u6d4b\u80fd\u529b\u6765\u514b\u670d\u5355\u6a21\u578b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u7684\u60c5\u51b5\u4e0b\u786e\u4fdd\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7ebf\u9884\u6d4b\u4efb\u52a1\u4e2d\u5e73\u5747\u51cf\u5c1110%\u9884\u6d4b\u8bef\u5dee\uff0c\u5728\u5b57\u8282\u8df3\u52a8IHPA\u5e73\u53f0\u6210\u529f\u90e8\u7f72\uff0c\u652f\u630130+\u5e94\u7528\u7a33\u5b9a\u8fd0\u884c\uff0c\u9884\u6d4b\u81ea\u52a8\u6269\u7f29\u5bb9\u80fd\u529b\u8fbe60\u4e07+CPU\u6838\u5fc3\u3002", "conclusion": "E3Former\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5668less\u7cfb\u7edf\u4e2d\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u548c\u8d44\u6e90\u4f18\u5316\uff0c\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.12787", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12787", "abs": "https://arxiv.org/abs/2508.12787", "authors": ["Satoshi Noguchi", "Yoshinobu Kawahara"], "title": "Wavy Transformer", "comment": "25 pages, 5 figures", "summary": "Transformers have achieved remarkable success across natural language\nprocessing (NLP) and computer vision (CV). However, deep transformer models\noften suffer from an over-smoothing issue, in which token representations\nconverge to similar values as they pass through successive transformer blocks.\nIn this paper, we establish an equivalence between the hidden-state dynamics\ninduced by stacked attention layers and graph neural diffusion on a complete\ngraph. From this perspective, over-smoothing can be interpreted as a\nconsequence of the dissipative nature of the underlying diffusion dynamics.\nMotivated by this physical interpretation, we propose Wavy Transformer, which\nconsists of a novel attention layer based on second-order wavy dynamics. We\nalso introduce a feed-forward network and a normalization layer designed to\npreserve the physical state-velocity relationship under the chain rule, thereby\nextending the transformer architecture. We further validate our proposed\ntechniques on various transformer models for NLP and CV tasks. The results\nconsistently demonstrate that Wavy Transformer improves performance with\nminimal additional parameters and no extra hyperparameter tuning.", "AI": {"tldr": "Wavy Transformer\u901a\u8fc7\u5f15\u5165\u4e8c\u9636\u6ce2\u52a8\u52a8\u529b\u5b66\u89e3\u51b3transformer\u4e2d\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u5728NLP\u548cCV\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u4e14\u53c2\u6570\u589e\u52a0\u6781\u5c11", "motivation": "\u6df1\u5ea6transformer\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u5373token\u8868\u793a\u5728\u8fde\u7eedtransformer\u5757\u4e2d\u6536\u655b\u5230\u76f8\u4f3c\u503c\u3002\u672c\u6587\u4ece\u56fe\u795e\u7ecf\u6269\u6563\u7684\u7269\u7406\u89d2\u5ea6\u89e3\u91ca\u6b64\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848", "method": "\u5efa\u7acb\u6ce8\u610f\u529b\u5c42\u4e0e\u5b8c\u5168\u56fe\u4e0a\u56fe\u795e\u7ecf\u6269\u6563\u7684\u7b49\u4ef7\u5173\u7cfb\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e8c\u9636\u6ce2\u52a8\u52a8\u529b\u5b66\u7684\u65b0\u578b\u6ce8\u610f\u529b\u5c42\uff0c\u8bbe\u8ba1\u4fdd\u6301\u7269\u7406\u72b6\u6001-\u901f\u5ea6\u5173\u7cfb\u7684FFN\u548c\u5f52\u4e00\u5316\u5c42", "result": "\u5728\u591a\u79cdNLP\u548cCV\u4efb\u52a1\u7684transformer\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cWavy Transformer\u80fd\u4e00\u81f4\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u53ea\u9700\u6781\u5c11\u989d\u5916\u53c2\u6570\uff0c\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\u8c03\u4f18", "conclusion": "\u4ece\u7269\u7406\u52a8\u529b\u5b66\u89d2\u5ea6\u7406\u89e3transformer\u7684\u8fc7\u5ea6\u5e73\u6ed1\u95ee\u9898\uff0c\u63d0\u51fa\u7684Wavy Transformer\u67b6\u6784\u80fd\u6709\u6548\u7f13\u89e3\u6b64\u95ee\u9898\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2508.12798", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12798", "abs": "https://arxiv.org/abs/2508.12798", "authors": ["Damian Machlanski", "Stephanie Riley", "Edward Moroshko", "Kurt Butler", "Panagiotis Dimitrakopoulos", "Thomas Melistas", "Akchunya Chanchal", "Steven McDonagh", "Ricardo Silva", "Sotirios A. Tsaftaris"], "title": "A Shift in Perspective on Causality in Domain Generalization", "comment": "2 pages, 1 figure, to be presented at the UK AI Research Symposium\n  (UKAIRS) 2025", "summary": "The promise that causal modelling can lead to robust AI generalization has\nbeen challenged in recent work on domain generalization (DG) benchmarks. We\nrevisit the claims of the causality and DG literature, reconciling apparent\ncontradictions and advocating for a more nuanced theory of the role of\ncausality in generalization. We also provide an interactive demo at\nhttps://chai-uk.github.io/ukairs25-causal-predictors/.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u56e0\u679c\u5efa\u6a21\u5728AI\u6cdb\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u6311\u6218\u4e86\u73b0\u6709\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u7684\u7ed3\u8bba\uff0c\u63d0\u51fa\u4e86\u66f4\u7ec6\u81f4\u7684\u56e0\u679c\u7406\u8bba\u6846\u67b6", "motivation": "\u8fd1\u671f\u5173\u4e8e\u9886\u57df\u6cdb\u5316\u57fa\u51c6\u7684\u7814\u7a76\u5bf9\u56e0\u679c\u5efa\u6a21\u80fd\u591f\u5e26\u6765\u7a33\u5065AI\u6cdb\u5316\u7684\u627f\u8bfa\u63d0\u51fa\u4e86\u8d28\u7591\uff0c\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u56e0\u679c\u6027\u4e0e\u6cdb\u5316\u4e4b\u95f4\u7684\u5173\u7cfb", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6587\u732e\u7efc\u8ff0\uff0c\u8c03\u548c\u56e0\u679c\u5efa\u6a21\u4e0e\u9886\u57df\u6cdb\u5316\u6587\u732e\u4e2d\u660e\u663e\u7684\u77db\u76fe\uff0c\u63d0\u51fa\u66f4\u7ec6\u81f4\u7684\u56e0\u679c\u7406\u8bba\u6846\u67b6", "result": "\u5efa\u7acb\u4e86\u66f4\u5168\u9762\u7684\u56e0\u679c\u7406\u8bba\u6765\u89e3\u91ca\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u6f14\u793a\u6765\u5c55\u793a\u7814\u7a76\u6210\u679c", "conclusion": "\u56e0\u679c\u6027\u5728AI\u6cdb\u5316\u4e2d\u4ecd\u7136\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u7406\u8bba\u6846\u67b6\u6765\u7406\u89e3\u5176\u4f5c\u7528\u673a\u5236\u548c\u5c40\u9650\u6027"}}
{"id": "2508.12801", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12801", "abs": "https://arxiv.org/abs/2508.12801", "authors": ["Bowen Dong", "Yilong Fan", "Yutao Sun", "Zhenyu Li", "Tengyu Pan", "Xun Zhou", "Jianyong Wang"], "title": "Maximum Score Routing For Mixture-of-Experts", "comment": null, "summary": "Routing networks in sparsely activated mixture-of-experts (MoE) dynamically\nallocate input tokens to top-k experts through differentiable sparse\ntransformations, enabling scalable model capacity while preserving\ncomputational efficiency. Traditional MoE networks impose an expert capacity\nconstraint to ensure GPU-friendly computation. However, this leads to token\ndropping when capacity is saturated and results in low hardware efficiency due\nto padding in underutilized experts. Removing the capacity constraint, in turn,\ncompromises load balancing and computational efficiency. To address these\nissues, we propose Maximum Score Routing ($\\mathbf{MaxScore}$), a novel MoE\nrouting paradigm that models routing as a minimum-cost maximum-flow problem and\nintegrates a SoftTopk operator. MaxScore resolves the fundamental limitations\nof iterative rerouting and optimal transport formulations, achieving lower\ntraining losses and higher evaluation scores at equivalent FLOPs compared to\nboth constrained and unconstrained baselines. Implementation details and\nexperimental configurations can be obtained from\n$\\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.", "AI": {"tldr": "MaxScore\u662f\u4e00\u79cd\u65b0\u7684MoE\u8def\u7531\u8303\u5f0f\uff0c\u901a\u8fc7\u5efa\u6a21\u4e3a\u6700\u5c0f\u6210\u672c\u6700\u5927\u6d41\u95ee\u9898\u5e76\u96c6\u6210SoftTopk\u7b97\u5b50\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u7f51\u7edc\u4e2d\u7684token\u4e22\u5f03\u548c\u786c\u4ef6\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMoE\u7f51\u7edc\u5b58\u5728\u4e13\u5bb6\u5bb9\u91cf\u7ea6\u675f\u5bfc\u81f4token\u4e22\u5f03\u548c\u786c\u4ef6\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u800c\u53bb\u9664\u7ea6\u675f\u53c8\u4f1a\u635f\u5bb3\u8d1f\u8f7d\u5e73\u8861\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faMaximum Score Routing (MaxScore)\uff0c\u5c06\u8def\u7531\u5efa\u6a21\u4e3a\u6700\u5c0f\u6210\u672c\u6700\u5927\u6d41\u95ee\u9898\uff0c\u96c6\u6210SoftTopk\u7b97\u5b50\uff0c\u907f\u514d\u8fed\u4ee3\u91cd\u8def\u7531\u548c\u6700\u4f18\u4f20\u8f93\u516c\u5f0f\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u76f8\u540cFLOPs\u4e0b\uff0c\u76f8\u6bd4\u6709\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u8bad\u7ec3\u635f\u5931\u548c\u66f4\u9ad8\u7684\u8bc4\u4f30\u5206\u6570\u3002", "conclusion": "MaxScore\u6709\u6548\u89e3\u51b3\u4e86MoE\u8def\u7531\u4e2d\u7684\u6839\u672c\u9650\u5236\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.12815", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12815", "abs": "https://arxiv.org/abs/2508.12815", "authors": ["Jayneel Parekh", "Pegah Khayatan", "Mustafa Shukor", "Arnaud Dapogny", "Alasdair Newson", "Matthieu Cord"], "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs", "comment": null, "summary": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.", "AI": {"tldr": "\u63d0\u51faL2S\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u5c0f\u578b\u8f85\u52a9\u6a21\u5757\u9884\u6d4b\u8f93\u5165\u7279\u5b9a\u7684\u5f15\u5bfc\u5411\u91cf\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cbe\u7ec6\u5316\u5f15\u5bfc\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u589e\u5f3a\u5b89\u5168\u6027", "motivation": "\u73b0\u6709\u5f15\u5bfc\u6280\u672f\u4f9d\u8d56\u5355\u4e00\u9759\u6001\u5f15\u5bfc\u5411\u91cf\uff0c\u65e0\u6cd5\u5904\u7406\u4f9d\u8d56\u5177\u4f53\u8f93\u5165\u7684\u884c\u4e3a\u9700\u6c42\uff08\u5982\u5b89\u5168\u56de\u7b54\u9700\u8981\u6839\u636e\u95ee\u9898\u7c7b\u578b\u91c7\u53d6\u4e0d\u540c\u7b56\u7565\uff09", "method": "\u4f7f\u7528\u5bf9\u6bd4\u6027\u8f93\u5165\u7279\u5b9a\u63d0\u793a\u8ba1\u7b97\u8f93\u5165\u7279\u5b9a\u7684\u7ebf\u6027\u504f\u79fb\uff0c\u8bad\u7ec3\u5c0f\u578b\u8f85\u52a9\u6a21\u5757\u6765\u9884\u6d4b\u6d4b\u8bd5\u65f6\u7684\u8f93\u5165\u7279\u5b9a\u5f15\u5bfc\u5411\u91cf", "result": "L2S\u65b9\u6cd5\u5728\u51cf\u5c11\u5e7b\u89c9\u548c\u589e\u5f3a\u5b89\u5168\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u9759\u6001\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8f93\u5165\u7279\u5b9a\u7684\u7cbe\u7ec6\u5316\u5f15\u5bfc\u662f\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u548c\u51c6\u786e\u6027\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2508.12833", "categories": ["cs.LG", "cs.AI", "68Txx", "I.2; I.4.2; E.4"], "pdf": "https://arxiv.org/pdf/2508.12833", "abs": "https://arxiv.org/abs/2508.12833", "authors": ["Kichang Lee", "Songkuk Kim", "JaeYeon Park", "JeongGil Ko"], "title": "Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG", "comment": "6pages, 6figures", "summary": "On-device machine learning is often constrained by limited storage,\nparticularly in continuous data collection scenarios. This paper presents an\nempirical study on storage-aware learning, focusing on the trade-off between\ndata quantity and quality via compression. We demonstrate that naive\nstrategies, such as uniform data dropping or one-size-fits-all compression, are\nsuboptimal. Our findings further reveal that data samples exhibit varying\nsensitivities to compression, supporting the feasibility of a sample-wise\nadaptive compression strategy. These insights provide a foundation for\ndeveloping a new class of storage-aware learning systems. The primary\ncontribution of this work is the systematic characterization of this\nunder-explored challenge, offering valuable insights that advance the\nunderstanding of storage-aware learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u63a2\u8ba8\u4e86\u8bbe\u5907\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5b58\u50a8\u95ee\u9898\uff0c\u53d1\u73b0\u6839\u636e\u6837\u672c\u6548\u5e94\u6027\u8fdb\u884c\u9002\u5e94\u6027\u538b\u7f29\u6bd4\u7edf\u4e00\u538b\u7f29\u6216\u6570\u636e\u6295\u5f03\u66f4\u6709\u6548\u3002", "motivation": "\u8bbe\u5907\u673a\u673a\u5668\u5b66\u4e60\u5e38\u9047\u5230\u5b58\u50a8\u7a7a\u95f4\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u6570\u636e\u6536\u96c6\u573a\u666f\u4e2d\u3002\u9700\u8981\u627e\u5230\u6570\u636e\u6570\u91cf\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u70b9\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u5206\u6790\u4e0d\u540c\u538b\u7f29\u7b56\u7565\u7684\u6548\u679c\uff0c\u5305\u62ec\u7edf\u4e00\u6570\u636e\u6295\u5f03\u3001\u4e00\u5c3a\u5bf8\u538b\u7f29\u7b56\u7565\uff0c\u4ee5\u53ca\u6839\u636e\u6837\u672c\u6548\u5e94\u6027\u8fdb\u884c\u9002\u5e94\u6027\u538b\u7f29\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u7b80\u5355\u7684\u7edf\u4e00\u6570\u636e\u5904\u7406\u7b56\u7565\u6548\u679c\u5e76\u4e0d\u7406\u60f3\uff0c\u4e0d\u540c\u6570\u636e\u6837\u672c\u5bf9\u538b\u7f29\u7684\u6548\u5e94\u6027\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u652f\u6301\u4e86\u6839\u636e\u6837\u672c\u7279\u5f81\u8fdb\u884c\u9002\u5e94\u6027\u538b\u7f29\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u63cf\u8ff0\u4e86\u5b58\u50a8\u654f\u611f\u5b66\u4e60\u8fd9\u4e2a\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u4e3a\u5f00\u53d1\u65b0\u4e00\u4ee3\u5b58\u50a8\u654f\u611f\u5b66\u4e60\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.12837", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12837", "abs": "https://arxiv.org/abs/2508.12837", "authors": ["Aditya Varre", "Gizem Y\u00fcce", "Nicolas Flammarion"], "title": "Learning In-context $\\pmb{n}$-grams with Transformers: Sub-$\\pmb{n}$-grams Are Near-stationary Points", "comment": "ICML2025", "summary": "Motivated by empirical observations of prolonged plateaus and stage-wise\nprogression during training, we investigate the loss landscape of transformer\nmodels trained on in-context next-token prediction tasks. In particular, we\nfocus on learning in-context $n$-gram language models under cross-entropy loss,\nand establish a sufficient condition for parameter configurations to be\nstationary points. We then construct a set of parameter configurations for a\nsimplified transformer model that represent $k$-gram estimators (for $k \\leq\nn$), and show that the gradient of the population loss at these solutions\nvanishes in the limit of infinite sequence length and parameter norm. This\nreveals a key property of the loss landscape: {sub-$n$-grams are\nnear-stationary points of the population cross-entropy loss}, offering\ntheoretical insight into widely observed phenomena such as stage-wise learning\ndynamics and emergent phase transitions. These insights are further supported\nby numerical experiments that illustrate the learning dynamics of $n$-grams,\ncharacterized by discrete transitions between near-stationary solutions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u53d8\u6362\u5668\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u8bcd\u6c47\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u635f\u5931\u5730\u5f62\uff0c\u53d1\u73b0\u5b50n-\u5143\u8bed\u8a00\u6a21\u578b\u662f\u7ecf\u9ad8\u65af\u635f\u5931\u7684\u8fd1\u7a33\u5b9a\u70b9\uff0c\u4e3a\u9636\u6bb5\u6027\u5b66\u4e60\u52a8\u6001\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "motivation": "\u53d7\u5b9e\u9a8c\u89c2\u5bdf\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u7684\u5e73\u53f0\u671f\u548c\u9636\u6bb5\u6027\u8fdb\u5c55\u7684\u9a9a\u52a8\uff0c\u7814\u7a76\u8005\u60f3\u8981\u63a2\u7d22\u53d8\u6362\u5668\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u8bcd\u6c47\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u635f\u5931\u5730\u5f62\u7279\u6027\u3002", "method": "\u91c7\u7528\u7b80\u5316\u7684\u53d8\u6362\u5668\u6a21\u578b\uff0c\u91cd\u70b9\u7814\u7a76\u5728\u4ea4\u53c9\u71b5\u635f\u5931\u4e0b\u5b66\u4e60\u4e0a\u4e0b\u6587n-\u5143\u8bed\u8a00\u6a21\u578b\u3002\u5efa\u7acb\u4e86\u53c2\u6570\u914d\u7f6e\u4e3a\u7a33\u5b9a\u70b9\u7684\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u6784\u9020\u4e86\u4e00\u7ec4\u8868\u793ak-\u5143\u4f30\u8ba1\u5668\u7684\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u8bc1\u660e\u5728\u65e0\u9650\u5e8f\u5217\u957f\u5ea6\u548c\u53c2\u6570\u8303\u6570\u6781\u9650\u4e0b\uff0c\u8fd9\u4e9b\u89e3\u7684\u6cbf\u635f\u5931\u68af\u5ea6\u6d88\u5931\uff0c\u8868\u660e\u5b50n-\u5143\u8bed\u8a00\u6a21\u578b\u662f\u7ecf\u9ad8\u65af\u635f\u5931\u7684\u8fd1\u7a33\u5b9a\u70b9\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5e7f\u6cdb\u89c2\u5bdf\u5230\u7684\u9636\u6bb5\u6027\u5b66\u4e60\u52a8\u6001\u548c\u51fa\u73b0\u6027\u76f8\u53d8\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u6570\u503c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u652f\u6301\u4e86\u8fd9\u4e9b\u89c1\u89e3\u3002"}}
{"id": "2508.12839", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12839", "abs": "https://arxiv.org/abs/2508.12839", "authors": ["Tiancheng Zhang", "Cheng Zhang", "Shuren Liu", "Xiaofei Wang", "Shaoyuan Huang", "Wenyu Wang"], "title": "HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms", "comment": "10 pages, 14 figures, ECAI2025", "summary": "With the rapid proliferation of streaming services, network load exhibits\nhighly time-varying and bursty behavior, posing serious challenges for\nmaintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms\n(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS\nand profitability, accurate load forecasting remains challenging under traffic\nsurges. Existing methods either minimize mean absolute error, resulting in\nunderprovisioning and potential Service Level Agreement (SLA) violations during\npeak periods, or adopt conservative overprovisioning strategies, which mitigate\nSLA risks at the expense of increased resource expenditure. To address this\ndilemma, we propose HRS, a hybrid representation framework with scheduling\nawareness that integrates numerical and image-based representations to better\ncapture extreme load dynamics. We further introduce a Scheduling-Aware Loss\n(SAL) that captures the asymmetric impact of prediction errors, guiding\npredictions that better support scheduling decisions. Extensive experiments on\nfour real-world datasets demonstrate that HRS consistently outperforms ten\nbaselines and achieves state-of-the-art performance, reducing SLA violation\nrates by 63.1% and total profit loss by 32.3%.", "AI": {"tldr": "HRS\u662f\u4e00\u4e2a\u6df7\u5408\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6570\u503c\u548c\u56fe\u50cf\u8868\u793a\u6765\u6355\u6349\u6781\u7aef\u8d1f\u8f7d\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u8c03\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570\u6765\u51cf\u5c11SLA\u8fdd\u89c4\u7387\u548c\u63d0\u9ad8\u5229\u6da6\u3002", "motivation": "\u6d41\u5a92\u4f53\u670d\u52a1\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u7f51\u7edc\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u65f6\u53d8\u6027\u548c\u7a81\u53d1\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5bfc\u81f4\u5cf0\u503c\u65f6\u6bb5SLA\u8fdd\u89c4\uff0c\u8981\u4e48\u91c7\u7528\u4fdd\u5b88\u7684\u8fc7\u5ea6\u914d\u7f6e\u7b56\u7565\u589e\u52a0\u8d44\u6e90\u652f\u51fa\u3002", "method": "\u63d0\u51faHRS\u6df7\u5408\u8868\u793a\u6846\u67b6\uff0c\u6574\u5408\u6570\u503c\u548c\u56fe\u50cf\u8868\u793a\u6765\u6355\u6349\u6781\u7aef\u8d1f\u8f7d\u52a8\u6001\uff0c\u5f15\u5165\u8c03\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570(SAL)\u6765\u6355\u83b7\u9884\u6d4b\u8bef\u5dee\u7684\u4e0d\u5bf9\u79f0\u5f71\u54cd\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHRS\u6301\u7eed\u4f18\u4e8e\u5341\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cSLA\u8fdd\u89c4\u7387\u964d\u4f4e63.1%\uff0c\u603b\u5229\u6da6\u635f\u5931\u51cf\u5c1132.3%\u3002", "conclusion": "HRS\u6846\u67b6\u901a\u8fc7\u6df7\u5408\u8868\u793a\u548c\u8c03\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5a92\u4f53\u670d\u52a1\u8d1f\u8f7d\u9884\u6d4b\u4e2d\u7684SLA\u8fdd\u89c4\u548c\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u548c\u7ecf\u6d4e\u6548\u76ca\u3002"}}
{"id": "2508.12885", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12885", "abs": "https://arxiv.org/abs/2508.12885", "authors": ["Aleksei Liuliakov", "Alexander Schulz", "Luca Hermes", "Barbara Hammer"], "title": "One-Class Intrusion Detection with Dynamic Graphs", "comment": null, "summary": "With the growing digitalization all over the globe, the relevance of network\nsecurity becomes increasingly important. Machine learning-based intrusion\ndetection constitutes a promising approach for improving security, but it bears\nseveral challenges. These include the requirement to detect novel and unseen\nnetwork events, as well as specific data properties, such as events over time\ntogether with the inherent graph structure of network communication. In this\nwork, we propose a novel intrusion detection method, TGN-SVDD, which builds\nupon modern dynamic graph modelling and deep anomaly detection. We demonstrate\nits superiority over several baselines for realistic intrusion detection data\nand suggest a more challenging variant of the latter.", "AI": {"tldr": "\u57fa\u4e8e\u52a8\u6001\u56fe\u6a21\u578b\u548c\u6df1\u5ea6\u5f02\u5e38\u68c0\u6d4b\u7684TGN-SVDD\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u73b0\u5b9e\u5165\u4fb5\u68c0\u6d4b\u6570\u636e\u4e0a\u663e\u793a\u4f18\u52bf", "motivation": "\u968f\u7740\u5168\u7403\u6570\u5b57\u5316\u53d1\u5c55\uff0c\u7f51\u7edc\u5b89\u5168\u65e5\u76ca\u91cd\u8981\u3002\u673a\u5668\u5b66\u4e60\u5165\u4fb5\u68c0\u6d4b\u9762\u4e34\u68c0\u6d4b\u65b0\u9898\u7f51\u7edc\u4e8b\u4ef6\u3001\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u7f51\u7edc\u901a\u4fe1\u56fe\u7ed3\u6784\u7b49\u6311\u6218", "method": "\u63d0\u51faTGN-SVDD\u65b9\u6cd5\uff0c\u7ed3\u5408\u73b0\u4ee3\u52a8\u6001\u56fe\u5efa\u6a21\u6280\u672f\u548c\u6df1\u5ea6\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5", "result": "\u5728\u73b0\u5b9e\u5165\u4fb5\u68c0\u6d4b\u6570\u636e\u4e0a\u8868\u73b0\u8d85\u8fc7\u591a\u4e2a\u57fa\u51c6\u65b9\u6cd5\uff0c\u5e76\u5efa\u8bae\u4e86\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u53d8\u4f53", "conclusion": "TGN-SVDD\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9898\u5165\u4fb5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u7f51\u7edc\u5b89\u5168\u4e2d\u7684\u52a8\u6001\u56fe\u7ed3\u6784\u548c\u65f6\u95f4\u5e8f\u5217\u7279\u5f81"}}
{"id": "2508.12905", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12905", "abs": "https://arxiv.org/abs/2508.12905", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "title": "TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML", "comment": null, "summary": "We introduce TCUQ, a single pass, label free uncertainty monitor for\nstreaming TinyML that converts short horizon temporal consistency captured via\nlightweight signals on posteriors and features into a calibrated risk score\nwith an O(W ) ring buffer and O(1) per step updates. A streaming conformal\nlayer turns this score into a budgeted accept/abstain rule, yielding calibrated\nbehavior without online labels or extra forward passes. On microcontrollers,\nTCUQ fits comfortably on kilobyte scale devices and reduces footprint and\nlatency versus early exit and deep ensembles (typically about 50 to 60% smaller\nand about 30 to 45% faster), while methods of similar accuracy often run out of\nmemory. Under corrupted in distribution streams, TCUQ improves accuracy drop\ndetection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high\nseverities; for failure detection it attains up to 0.92 AUROC. These results\nshow that temporal consistency, coupled with streaming conformal calibration,\nprovides a practical and resource efficient foundation for on device monitoring\nin TinyML.", "AI": {"tldr": "TCUQ\u662f\u4e00\u79cd\u9762\u5411TinyML\u6d41\u5f0f\u5e94\u7528\u7684\u5355\u6b21\u901a\u8fc7\u3001\u65e0\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4fe1\u53f7\u6355\u6349\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8f6c\u6362\u4e3a\u6821\u51c6\u98ce\u9669\u5206\u6570\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u76d1\u63a7\u3002", "motivation": "\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u76d1\u6d4b\u65b9\u6cd5\uff08\u5982\u65e9\u671f\u9000\u51fa\u548c\u6df1\u5ea6\u96c6\u6210\uff09\u5728TinyML\u8bbe\u5907\u4e0a\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u5185\u5b58\u5360\u7528\u9ad8\uff0c\u9700\u8981\u5f00\u53d1\u8d44\u6e90\u9ad8\u6548\u4e14\u65e0\u9700\u5728\u7ebf\u6807\u7b7e\u7684\u6d41\u5f0f\u4e0d\u786e\u5b9a\u6027\u76d1\u6d4b\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u73af\u5f62\u7f13\u51b2\u533a\u548cO(1)\u6bcf\u6b65\u66f4\u65b0\uff0c\u901a\u8fc7\u540e\u9a8c\u548c\u7279\u5f81\u7684\u77ed\u65f6\u57df\u65f6\u95f4\u4e00\u81f4\u6027\u751f\u6210\u98ce\u9669\u5206\u6570\uff0c\u7ed3\u5408\u6d41\u5f0f\u5171\u5f62\u6821\u51c6\u5c42\u5b9e\u73b0\u9884\u7b97\u5316\u7684\u63a5\u53d7/\u5f03\u6743\u51b3\u7b56\u3002", "result": "\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u6bd4\u65e9\u671f\u9000\u51fa\u548c\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\u51cf\u5c1150-60%\u5185\u5b58\u5360\u7528\u548c30-45%\u5ef6\u8fdf\uff0c\u5728\u5206\u5e03\u5185\u6570\u636e\u6d41\u635f\u574f\u60c5\u51b5\u4e0b\u63d0\u53473-7\u4e2aAUPRC\u70b9\uff0c\u6700\u9ad8\u8fbe\u52300.86 AUPRC\u548c0.92 AUROC\u3002", "conclusion": "\u65f6\u95f4\u4e00\u81f4\u6027\u7ed3\u5408\u6d41\u5f0f\u5171\u5f62\u6821\u51c6\u4e3aTinyML\u8bbe\u5907\u76d1\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u57fa\u7840\u65b9\u6848\u3002"}}
{"id": "2508.12906", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12906", "abs": "https://arxiv.org/abs/2508.12906", "authors": ["Boran Zhao", "Haiming Zhai", "Zihang Yuan", "Hetian Liu", "Tian Xia", "Wenzhe Zhao", "Pengju Ren"], "title": "SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy", "comment": null, "summary": "The growing demand for sparse tensor algebra (SpTA) in machine learning and\nbig data has driven the development of various sparse tensor accelerators.\nHowever, most existing manually designed accelerators are limited to specific\nscenarios, and it's time-consuming and challenging to adjust a large number of\ndesign factors when scenarios change. Therefore, automating the design of SpTA\naccelerators is crucial. Nevertheless, previous works focus solely on either\nmapping (i.e., tiling communication and computation in space and time) or\nsparse strategy (i.e., bypassing zero elements for efficiency), leading to\nsuboptimal designs due to the lack of comprehensive consideration of both. A\nunified framework that jointly optimizes both is urgently needed. However,\nintegrating mapping and sparse strategies leads to a combinatorial explosion in\nthe design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \\times\n64} \\times Q_{64 \\times 48} = Z_{32 \\times 48}$). This vast search space\nrenders most conventional optimization methods (e.g., particle swarm\noptimization, reinforcement learning and Monte Carlo tree search) inefficient.\nTo address this challenge, we propose an evolution strategy-based sparse tensor\naccelerator optimization framework, called SparseMap. SparseMap constructing a\nmore comprehensive design space with the consideration of both mapping and\nsparse strategy. We introduce a series of enhancements to genetic encoding and\nevolutionary operators, enabling SparseMap to efficiently explore the vast and\ndiverse design space. We quantitatively compare SparseMap with prior works and\nclassical optimization methods, demonstrating that SparseMap consistently finds\nsuperior solutions.", "AI": {"tldr": "SparseMap\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fdb\u5316\u7b56\u7565\u7684\u7a00\u758f\u5f20\u91cf\u52a0\u901f\u5668\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6620\u5c04\u7b56\u7565\u548c\u7a00\u758f\u7b56\u7565\uff0c\u5728\u5de8\u5927\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff08O(10^41)\uff09\u4e2d\u9ad8\u6548\u641c\u7d22\u6700\u4f18\u89e3\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5f20\u91cf\u52a0\u901f\u5668\u591a\u4e3a\u624b\u52a8\u8bbe\u8ba1\uff0c\u5c40\u9650\u4e8e\u7279\u5b9a\u573a\u666f\u4e14\u96be\u4ee5\u8c03\u6574\u3002\u4f20\u7edf\u65b9\u6cd5\u53ea\u5173\u6ce8\u6620\u5c04\u6216\u7a00\u758f\u7b56\u7565\u7684\u5355\u4e00\u4f18\u5316\uff0c\u7f3a\u4e4f\u7efc\u5408\u8003\u8651\uff0c\u5bfc\u81f4\u8bbe\u8ba1\u6b21\u4f18\u3002", "method": "\u63d0\u51faSparseMap\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u6620\u5c04\u548c\u7a00\u758f\u7b56\u7565\u7684\u7efc\u5408\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6539\u8fdb\u9057\u4f20\u7f16\u7801\u548c\u8fdb\u5316\u7b97\u5b50\uff0c\u4f7f\u8fdb\u5316\u7b56\u7565\u80fd\u9ad8\u6548\u63a2\u7d22\u5de8\u5927\u800c\u591a\u6837\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u4e0e\u5148\u524d\u5de5\u4f5c\u548c\u7ecf\u5178\u4f18\u5316\u65b9\u6cd5\u76f8\u6bd4\uff0cSparseMap\u59cb\u7ec8\u80fd\u627e\u5230\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "SparseMap\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u5f20\u91cf\u52a0\u901f\u5668\u8bbe\u8ba1\u4e2d\u6620\u5c04\u548c\u7a00\u758f\u7b56\u7565\u8054\u5408\u4f18\u5316\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12907", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12907", "abs": "https://arxiv.org/abs/2508.12907", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Khalid El Makkaoui", "Ibrahim Ouahbi", "Yassine Maleh"], "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML", "comment": null, "summary": "We introduce \\textbf{SNAP-UQ}, a single-pass, label-free uncertainty method\nfor TinyML that estimates risk from \\emph{depth-wise next-activation\nprediction}: tiny int8 heads forecast the statistics of the next layer from a\ncompressed view of the previous one, and a lightweight monotone mapper turns\nthe resulting surprisal into an actionable score. The design requires no\ntemporal buffers, auxiliary exits, or repeated forward passes, and adds only a\nfew tens of kilobytes to MCU deployments. Across vision and audio backbones,\nSNAP-UQ consistently reduces flash and latency relative to early-exit and deep\nensembles (typically $\\sim$40--60\\% smaller and $\\sim$25--35\\% faster), with\ncompeting methods of similar accuracy often exceeding memory limits. In\ncorrupted streams it improves accuracy-drop detection by several AUPRC points\nand maintains strong failure detection (AUROC $\\approx$0.9) in a single pass.\nGrounding uncertainty in layer-to-layer dynamics yields a practical,\nresource-efficient basis for on-device monitoring in TinyML.", "AI": {"tldr": "SNAP-UQ\u662f\u4e00\u79cd\u9762\u5411TinyML\u7684\u5355\u6b21\u524d\u5411\u4f20\u64ad\u3001\u65e0\u9700\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u65b9\u5411\u7684\u4e0b\u5c42\u6fc0\u6d3b\u9884\u6d4b\u6765\u4f30\u8ba1\u98ce\u9669\uff0c\u5177\u6709\u6781\u4f4e\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff08\u5982\u65e9\u9000\u673a\u5236\u548c\u6df1\u5ea6\u96c6\u6210\uff09\u5728TinyML\u8bbe\u5907\u4e0a\u5b58\u5728\u5185\u5b58\u5360\u7528\u5927\u3001\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u591a\u6b21\u524d\u5411\u4f20\u64ad\u6216\u989d\u5916\u9000\u51fa\u70b9\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u5fae\u63a7\u5236\u5668\u90e8\u7f72\u3002", "method": "\u4f7f\u7528int8\u91cf\u5316\u7684\u5c0f\u578b\u9884\u6d4b\u5934\u6765\u9884\u6d4b\u4e0b\u4e00\u5c42\u7684\u7edf\u8ba1\u7279\u5f81\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5355\u8c03\u6620\u5c04\u5668\u5c06\u9884\u6d4b\u8bef\u5dee\u8f6c\u6362\u4e3a\u53ef\u64cd\u4f5c\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6570\uff0c\u65e0\u9700\u65f6\u95f4\u7f13\u51b2\u533a\u3001\u8f85\u52a9\u9000\u51fa\u70b9\u6216\u91cd\u590d\u524d\u5411\u4f20\u64ad\u3002", "result": "\u5728\u89c6\u89c9\u548c\u97f3\u9891\u4efb\u52a1\u4e0a\uff0c\u76f8\u6bd4\u65e9\u9000\u673a\u5236\u548c\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\uff0cSNAP-UQ\u51cf\u5c11\u4e8640-60%\u7684\u5b58\u50a8\u7a7a\u95f4\u548c25-35%\u7684\u5ef6\u8fdf\uff0c\u5728\u6570\u636e\u635f\u574f\u6d41\u4e2d\u63d0\u9ad8\u4e86AUPRC\u6307\u6807\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u8fbe\u5230\u7ea60.9\u7684AUROC\u6545\u969c\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u5c42\u95f4\u52a8\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e3aTinyML\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u76d1\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u6781\u4f4e\u8d44\u6e90\u5f00\u9500\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002"}}
{"id": "2508.12978", "categories": ["cs.LG", "cs.DC", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.12978", "abs": "https://arxiv.org/abs/2508.12978", "authors": ["Yue Xia", "Tayyebeh Jahani-Nezhad", "Rawad Bitar"], "title": "Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning", "comment": null, "summary": "We propose Fed-DPRoC, a novel federated learning framework that\nsimultaneously ensures differential privacy (DP), Byzantine robustness, and\ncommunication efficiency. We introduce the concept of robust-compatible\ncompression, which enables users to compress DP-protected updates while\nmaintaining the robustness of the aggregation rule. We instantiate our\nframework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for\ncompression with robust averaging for robust aggregation. We theoretically\nprove the compatibility of JL transform with robust averaging and show that\nRobAJoL preserves robustness guarantees, ensures DP, and reduces communication\ncost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims\nand demonstrate that RobAJoL outperforms existing methods in terms of\nrobustness and utility under different Byzantine attacks.", "AI": {"tldr": "Fed-DPRoC\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u540c\u65f6\u786e\u4fdd\u5dee\u5206\u9690\u79c1\u3001\u62dc\u5360\u5ead\u9c81\u68d2\u6027\u548c\u901a\u4fe1\u6548\u7387\uff0c\u901a\u8fc7RobAJoL\u65b9\u6cd5\u7ed3\u5408JL\u53d8\u6362\u538b\u7f29\u548c\u9c81\u68d2\u805a\u5408", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u3001\u62dc\u5360\u5ead\u653b\u51fb\u9c81\u68d2\u6027\u548c\u901a\u4fe1\u6548\u7387\uff0c\u9700\u8981\u4e00\u79cd\u7efc\u5408\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faRobust-compatible\u538b\u7f29\u6982\u5ff5\uff0c\u4f7f\u7528Johnson-Lindenstrauss\u53d8\u6362\u8fdb\u884c\u538b\u7f29\uff0c\u7ed3\u5408\u9c81\u68d2\u805a\u5408\u65b9\u6cd5RobAJoL", "result": "\u7406\u8bba\u8bc1\u660eJL\u53d8\u6362\u4e0e\u9c81\u68d2\u805a\u5408\u517c\u5bb9\uff0c\u5b9e\u9a8c\u5728CIFAR-10\u548cFashion MNIST\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "Fed-DPRoC\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u3001\u9c81\u68d2\u6027\u548c\u901a\u4fe1\u6548\u7387\u7684\u4e09\u91cd\u76ee\u6807\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12984", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.12984", "abs": "https://arxiv.org/abs/2508.12984", "authors": ["Zehang Lin", "Zheng Lin", "Miao Yang", "Jianhao Huang", "Yuxin Zhang", "Zihan Fang", "Xia Du", "Zhe Chen", "Shunzhi Zhu", "Wei Ni"], "title": "SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression", "comment": "6 pages, 7 figures", "summary": "The increasing complexity of neural networks poses a significant barrier to\nthe deployment of distributed machine learning (ML) on resource-constrained\ndevices, such as federated learning (FL). Split learning (SL) offers a\npromising solution by offloading the primary computing load from edge devices\nto a server via model partitioning. However, as the number of participating\ndevices increases, the transmission of excessive smashed data (i.e.,\nactivations and gradients) becomes a major bottleneck for SL, slowing down the\nmodel training. To tackle this challenge, we propose a communication-efficient\nSL framework, named SL-ACC, which comprises two key components: adaptive\nchannel importance identification (ACII) and channel grouping compression\n(CGC). ACII first identifies the contribution of each channel in the smashed\ndata to model training using Shannon entropy. Following this, CGC groups the\nchannels based on their entropy and performs group-wise adaptive compression to\nshrink the transmission volume without compromising training accuracy.\nExtensive experiments across various datasets validate that our proposed SL-ACC\nframework takes considerably less time to achieve a target accuracy than\nstate-of-the-art benchmarks.", "AI": {"tldr": "SL-ACC\u662f\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u7684\u62c6\u5206\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u9053\u91cd\u8981\u6027\u8bc6\u522b\u548c\u901a\u9053\u5206\u7ec4\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u91cf\uff0c\u5728\u4fdd\u6301\u8bad\u7ec3\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u590d\u6742\u5ea6\u7684\u589e\u52a0\u548c\u53c2\u4e0e\u8bbe\u5907\u6570\u91cf\u7684\u589e\u957f\uff0c\u62c6\u5206\u5b66\u4e60\u4e2d\u8fc7\u5ea6\u7684\u7c89\u788e\u6570\u636e\u4f20\u8f93\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u4e25\u91cd\u62d6\u6162\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u3002", "method": "\u63d0\u51faSL-ACC\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u9999\u519c\u71b5\u7684\u81ea\u9002\u5e94\u901a\u9053\u91cd\u8981\u6027\u8bc6\u522b(ACII)\u6765\u8bc4\u4f30\u6bcf\u4e2a\u901a\u9053\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u8d21\u732e\uff1b2\uff09\u57fa\u4e8e\u71b5\u503c\u7684\u901a\u9053\u5206\u7ec4\u538b\u7f29(CGC)\u8fdb\u884c\u5206\u7ec4\u81ea\u9002\u5e94\u538b\u7f29\u4ee5\u51cf\u5c11\u4f20\u8f93\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0cSL-ACC\u6846\u67b6\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u8fbe\u5230\u76ee\u6807\u7cbe\u5ea6\u65f6\u6240\u9700\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "SL-ACC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u62c6\u5206\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u7684\u901a\u9053\u91cd\u8981\u6027\u8bc6\u522b\u548c\u538b\u7f29\u7b56\u7565\uff0c\u5728\u4e0d\u5f71\u54cd\u8bad\u7ec3\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2508.12993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12993", "abs": "https://arxiv.org/abs/2508.12993", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian", "comment": "9 pages, 3 figures", "summary": "A common observation in the Graph Convolutional Network (GCN) literature is\nthat stacking GCN layers may or may not result in better performance on tasks\nlike node classification and edge prediction. We have found empirically that a\ngraph's algebraic connectivity, which is known as the Fiedler value, is a good\npredictor of GCN performance. Intuitively, graphs with similar Fiedler values\nhave analogous structural properties, suggesting that the same filters and\nhyperparameters may yield similar results when used with GCNs, and that\ntransfer learning may be more effective between graphs with similar algebraic\nconnectivity. We explore this theoretically and empirically with experiments on\nsynthetic and real graph data, including the Cora, CiteSeer and Polblogs\ndatasets. We explore multiple ways of aggregating the Fiedler value for\nconnected components in the graphs to arrive at a value for the entire graph,\nand show that it can be used to predict GCN performance. We also present\ntheoretical arguments as to why the Fiedler value is a good predictor.", "AI": {"tldr": "\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u6027\u80fd\u4e0e\u56fe\u7684\u4ee3\u6570\u8fde\u901a\u6027(Fiedler\u503c)\u5bc6\u5207\u76f8\u5173\uff0c\u76f8\u4f3cFiedler\u503c\u7684\u56fe\u5177\u6709\u7c7b\u4f3c\u7ed3\u6784\u7279\u6027\uff0c\u53ef\u7528\u4f5cGCN\u6027\u80fd\u9884\u6d4b\u6307\u6807\u548c\u8fc1\u79fb\u5b66\u4e60\u53c2\u8003\u3002", "motivation": "GCN\u6587\u732e\u4e2d\u89c2\u5bdf\u5230\u5806\u53e0GCN\u5c42\u5e76\u4e0d\u603b\u80fd\u63d0\u5347\u8282\u70b9\u5206\u7c7b\u548c\u8fb9\u9884\u6d4b\u6027\u80fd\uff0c\u9700\u8981\u627e\u5230\u80fd\u9884\u6d4bGCN\u6027\u80fd\u7684\u56fe\u7ed3\u6784\u6307\u6807\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\uff0c\u5728\u5408\u6210\u56fe\u548c\u771f\u5b9e\u56fe\u6570\u636e(Cora\u3001CiteSeer\u3001Polblogs)\u4e0a\u9a8c\u8bc1Fiedler\u503c\u4e0eGCN\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u63a2\u7d22\u591a\u79cdFiedler\u503c\u805a\u5408\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u56fe\u7684\u4ee3\u6570\u8fde\u901a\u6027(Fiedler\u503c)\u662fGCN\u6027\u80fd\u7684\u826f\u597d\u9884\u6d4b\u6307\u6807\uff0c\u76f8\u4f3cFiedler\u503c\u7684\u56fe\u53ef\u4f7f\u7528\u76f8\u540c\u6ee4\u6ce2\u5668\u548c\u8d85\u53c2\u6570\u83b7\u5f97\u7c7b\u4f3c\u7ed3\u679c\u3002", "conclusion": "Fiedler\u503c\u53ef\u4f5c\u4e3aGCN\u6027\u80fd\u9884\u6d4b\u7684\u6709\u6548\u6307\u6807\uff0c\u5e76\u4e3a\u56fe\u4e4b\u95f4\u7684\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u6307\u5bfc\uff0c\u76f8\u4f3c\u4ee3\u6570\u8fde\u901a\u6027\u7684\u56fe\u66f4\u9002\u5408\u53c2\u6570\u8fc1\u79fb\u3002"}}
{"id": "2508.12996", "categories": ["cs.LG", "cs.AI", "65K10, 68T07", "I.2.6; G.1.6"], "pdf": "https://arxiv.org/pdf/2508.12996", "abs": "https://arxiv.org/abs/2508.12996", "authors": ["Stavros C. Kassinos"], "title": "Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair", "comment": "54 pages, 8 figures, 19 tables", "summary": "Transformer neural networks are increasingly used for physics-based problems.\nIn data-driven PDE surrogates, training samples from varying boundary and\ninitial conditions can cause erratic losses and spiky gradients; in\nphysics-informed neural networks (PINNs), stiff composite losses amplify this\neffect.\n  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed\nsecond-moment discount beta2 is replaced by a layer-wise dynamic value driven\nby a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an\nexponential moving average (EMA) of past norms, squashed to the interval [0,1).\nSpikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.\nOptions include leaky-AMSGrad (decay), trust-region clipping (max_ratio),\nadaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',\n``exact'). With all features off and bias_correction=``none'', the method is\nexactly Adam.\n  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D\nPINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with\njitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB\nof enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss\nversus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about\n38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller\nvariance. The method remains drop-in, with runtime overhead comparable to Adam\nin testbeds A-C and within single-digit percent in testbed D. It preserves\nAdam-style convergence guarantees while improving robustness under spiky\ngradients.", "AI": {"tldr": "Kourkoutas-Beta\u662f\u4e00\u79cdAdam\u98ce\u683c\u7684\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574beta2\u53c2\u6570\u6765\u5904\u7406\u68af\u5ea6\u5c16\u5cf0\u95ee\u9898\uff0c\u5728\u7269\u7406\u9a71\u52a8\u7684PDE\u4ee3\u7406\u6a21\u578b\u548cPINNs\u4e2d\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd", "motivation": "\u5728\u6570\u636e\u9a71\u52a8\u7684PDE\u4ee3\u7406\u6a21\u578b\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u4e2d\uff0c\u4e0d\u540c\u8fb9\u754c\u548c\u521d\u59cb\u6761\u4ef6\u7684\u8bad\u7ec3\u6837\u672c\u4f1a\u5bfc\u81f4\u4e0d\u7a33\u5b9a\u7684\u635f\u5931\u548c\u5c16\u5cf0\u68af\u5ea6\uff0c\u4f20\u7edfAdam\u4f18\u5316\u5668\u7684\u56fa\u5b9abeta2\u53c2\u6570\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u60c5\u51b5", "method": "\u63d0\u51faKourkoutas-Beta\u4f18\u5316\u5668\uff0c\u7528\u5c42\u7ea7\u7684\u52a8\u6001beta2\u503c\u66ff\u4ee3\u56fa\u5b9a\u503c\uff0c\u8be5\u503c\u7531\u5f53\u524d\u6c60\u5316\u68af\u5ea6\u8303\u6570\u4e0e\u5386\u53f2\u6307\u6570\u79fb\u52a8\u5e73\u5747\u8303\u6570\u7684\u6bd4\u503c\uff08sunspike\u6bd4\u7387\uff09\u9a71\u52a8\uff0c\u5c16\u5cf0\u65f6\u964d\u4f4ebeta2\uff0c\u5e73\u7a33\u65f6\u4fdd\u6301\u9ad8beta2", "result": "\u5728\u56db\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff08Transformer PDE\u4ee3\u7406\u30013D PINN\u70ed\u4f20\u5bfc\u3001MLX\u5408\u6210\u4efb\u52a1\u3001\u5b57\u7b26\u7ea7Transformer\uff09\uff0c\u76f8\u6bd4\u56fa\u5b9abeta2\u7684Adam\uff0cKourkoutas-Beta\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u635f\u5931\u6027\u80fd\uff0c\u5728small-enwik8\u4e0a\u6bd4\u7279\u7387\u964d\u4f4e38-58%", "conclusion": "\u8be5\u65b9\u6cd5\u4fdd\u6301Adam\u5f0f\u6536\u655b\u4fdd\u8bc1\u7684\u540c\u65f6\uff0c\u5728\u5c16\u5cf0\u68af\u5ea6\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u8fd0\u884c\u65f6\u5f00\u9500\u4e0eAdam\u76f8\u5f53\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u7684\u4f18\u5316\u5668\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2508.13006", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13006", "abs": "https://arxiv.org/abs/2508.13006", "authors": ["Pengcheng Hao", "Menghao Waiyan William Zhu", "Ercan Engin Kuruoglu"], "title": "Monte Carlo Functional Regularisation for Continual Learning", "comment": null, "summary": "Continual learning (CL) is crucial for the adaptation of neural network\nmodels to new environments. Although outperforming weight-space regularisation\napproaches, the functional regularisation-based CL methods suffer from high\ncomputational costs and large linear approximation errors. In this work, we\npresent a new functional regularisation CL framework, called MCFRCL, which\napproximates model prediction distributions by Monte Carlo (MC) sampling.\nMoreover, three continuous distributions are leveraged to capture the\nstatistical characteristics of the MC samples via moment-based methods.\nAdditionally, both the Wasserstein distance and the Kullback-Leibler (KL)\ndistance are employed to construct the regularisation function. The proposed\nMCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR\ndatasets, with simulation results highlighting its effectiveness in both\nprediction accuracy and training efficiency.", "AI": {"tldr": "\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u77e9\u5206\u6790\u65b9\u6cd5\u63a5\u8fd1\u6a21\u578b\u9884\u6d4b\u5206\u5e03\uff0c\u7ed3\u5408Wasserstein\u8ddd\u79bb\u548cKL\u6563\u5ea6\u6784\u5efa\u51fd\u6570\u6b63\u5219\u5316\uff0c\u63d0\u9ad8\u6301\u7eed\u5b66\u4e60\u7684\u6548\u679c\u548c\u6548\u7387", "motivation": "\u89e3\u51b3\u4f20\u7edf\u51fd\u6570\u6b63\u5219\u5316\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7ebf\u6027\u63a5\u8fd1\u8bef\u5dee\u5927\u7684\u95ee\u9898", "method": "\u63d0\u51faMCFRCL\u6846\u67b6\uff0c\u91c7\u7528\u8499\u7279\u5361\u6d1b\u91c7\u6837\u63a5\u8fd1\u6a21\u578b\u9884\u6d4b\u5206\u5e03\uff0c\u5229\u7528\u4e09\u79cd\u8fde\u7eed\u5206\u5e03\u901a\u8fc7\u77e9\u5206\u6790\u65b9\u6cd5\u6355\u6349MC\u6837\u672c\u7684\u7edf\u8ba1\u7279\u5f81\uff0c\u5e76\u4f7f\u7528Wasserstein\u8ddd\u79bb\u548cKL\u6563\u5ea6\u6784\u5efa\u6b63\u5219\u5316\u51fd\u6570", "result": "\u5728MNIST\u548cCIFAR\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u90fd\u663e\u793a\u51fa\u663e\u8457\u6548\u679c", "conclusion": "MCFRCL\u6846\u67b5\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u77e9\u5206\u6790\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u51fd\u6570\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\u548c\u63a5\u8fd1\u8bef\u5dee\u95ee\u9898\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b0\u65b9\u6cd5"}}
{"id": "2508.13018", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13018", "abs": "https://arxiv.org/abs/2508.13018", "authors": ["Iam Kim de S. Hermont", "Andre R. Flores", "Rodrigo C. de Lamare"], "title": "Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control", "comment": "12 figures, 11 pages", "summary": "In this work, we propose a robust adaptive filtering approach for active\nnoise control applications in the presence of impulsive noise. In particular,\nwe develop the filtered-x hyperbolic tangent exponential generalized Kernel\nM-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis\nof the proposed FXHEKM algorithm is carried out along with a study of its\ncomputational cost. {In order to evaluate the proposed FXHEKM algorithm, the\nmean-square error (MSE) and the average noise reduction (ANR) performance\nmetrics have been adopted.} Numerical results show the efficiency of the\nproposed FXHEKM algorithm to cancel the presence of the additive spurious\nsignals, such as \\textbf{$\\alpha$}-stable noises against competing algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ebFXHEKM\u7b97\u6cd5\uff0c\u7528\u4e8e\u963b\u6321\u51b2\u51fb\u6027\u566a\u58f0\u73af\u5883\u4e0b\u7684\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\uff0c\u5728\u03b1\u7a33\u5b9a\u566a\u58f0\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5728\u51b2\u51fb\u6027\u566a\u58f0\u73af\u5883\u4e0b\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u7684\u7a33\u5065\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u566a\u58f0\u62d6\u9664\u6548\u679c\u3002", "method": "\u53d1\u5c55\u4e86\u8fc7\u6ee4-x\u53cc\u66f2\u6b63\u5207\u6307\u6570\u5e7f\u4e49\u5185\u6838M\u4f30\u8ba1\u51fd\u6570(FXHEKM)\u7b97\u6cd5\uff0c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u548c\u8ba1\u7b97\u6210\u672c\u7814\u7a76\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u7b97\u6cd5\u5728\u03b1\u7a33\u5b9a\u566a\u58f0\u4e2d\u80fd\u591f\u9ad8\u6548\u62d6\u9664\u52a0\u6027\u4f2a\u4fe1\u53f7\uff0c\u5728MSE\u548cANR\u6027\u80fd\u6307\u6807\u4e0a\u90fd\u8d85\u8fc7\u7ade\u4e89\u7b97\u6cd5\u3002", "conclusion": "FXHEKM\u7b97\u6cd5\u4e3a\u51b2\u51fb\u6027\u566a\u58f0\u73af\u5883\u4e0b\u7684\u4e3b\u52a8\u566a\u58f0\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13030", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.13030", "abs": "https://arxiv.org/abs/2508.13030", "authors": ["Bipin Chhetri", "Akbar Siami Namin"], "title": "The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks", "comment": "21 pages, 6 figures,Proceedings of the IEEE International Conference\n  on Computers, Software, & Applications (COMPSAC), EATA Symposium, Toronto,\n  Canada, July 8-11, 2025", "summary": "Cyberattacks are increasing, and securing against such threats is costing\nindustries billions of dollars annually. Threat Modeling, that is,\ncomprehending the consequences of these attacks, can provide critical support\nto cybersecurity professionals, enabling them to take timely action and\nallocate resources that could be used elsewhere. Cybersecurity is heavily\ndependent on threat modeling, as it assists security experts in assessing and\nmitigating risks related to identifying vulnerabilities and threats. Recently,\nthere has been a pressing need for automated methods to assess attack\ndescriptions and forecast the future consequences of the increasing complexity\nof cyberattacks. This study examines how Natural Language Processing (NLP) and\ndeep learning can be applied to analyze the potential impact of cyberattacks by\nleveraging textual descriptions from the MITRE Common Weakness Enumeration\n(CWE) database. We emphasize classifying attack consequences into five\nprincipal categories: Availability, Access Control, Confidentiality, Integrity,\nand Other. This paper investigates the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) in combination with Hierarchical\nAttention Networks (HANs) for Multi-label classification, evaluating their\nperformance in comparison with conventional CNN and LSTM-based models.\nExperimental findings show that BERT achieves an overall accuracy of $0.972$,\nfar higher than conventional deep learning models in multi-label\nclassification. HAN outperforms baseline forms of CNN and LSTM-based models on\nspecific cybersecurity labels. However, BERT consistently achieves better\nprecision and recall, making it more suitable for predicting the consequences\nof a cyberattack.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528BERT\u548c\u5c42\u6b21\u6ce8\u610f\u7f51\u7edc\u8fdb\u884c\u7f51\u7edc\u653b\u51fb\u540e\u679c\u7684\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u8fbe\u5230\u4e8697.2%\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u653b\u51fb\u8d8a\u6765\u8d8a\u590d\u6742\uff0c\u5bf9\u81ea\u52a8\u5316\u653b\u51fb\u63cf\u8ff0\u5206\u6790\u548c\u540e\u679c\u9884\u6d4b\u7684\u9700\u6c42\u65e5\u76ca\u7a81\u51fa\uff0c\u4ee5\u53ca\u65f6\u53d6\u884c\u52a8\u548c\u5408\u7406\u5206\u914d\u8d44\u6e90\u3002", "method": "\u5229\u7528MITRE CWE\u6570\u636e\u5e93\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u91c7\u7528BERT\u6a21\u578b\u7ed3\u5408\u5c42\u6b21\u6ce8\u610f\u7f51\u7edc(HAN)\u8fdb\u884c\u591a\u6807\u7b7e\u5206\u7c7b\uff0c\u5c06\u653b\u51fb\u540e\u679c\u5206\u4e3a\u53ef\u7528\u6027\u3001\u8bbf\u95ee\u63a7\u5236\u3001\u4fdd\u5bc6\u6027\u3001\u5b8c\u6574\u6027\u548c\u5176\u4ed6\u4e94\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002", "result": "BERT\u6a21\u578b\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\u8fbe\u5230\u603b\u4f53\u51c6\u786e\u73870.972\uff0c\u663e\u8457\u8d85\u8fc7\u4f20\u7edfCNN\u548cLSTM\u6a21\u578b\u3002HAN\u5728\u7279\u5b9a\u5b89\u5168\u6807\u7b7e\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46BERT\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u65b9\u9762\u66f4\u4e00\u81f4\u4f18\u52bf\u3002", "conclusion": "BERT\u6a21\u578b\u5728\u9884\u6d4b\u7f51\u7edc\u653b\u51fb\u540e\u679c\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff0c\u9002\u5408\u7528\u4e8e\u5b89\u5168\u4e13\u4e1a\u4eba\u5458\u8fdb\u884c\u98ce\u9669\u8bc4\u4f30\u548c\u8d44\u6e90\u5206\u914d\u3002"}}
{"id": "2508.13040", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13040", "abs": "https://arxiv.org/abs/2508.13040", "authors": ["Varsha Ramineni", "Hossein A. Rahmani", "Emine Yilmaz", "David Barber"], "title": "Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data", "comment": "9 pages, 3 figures", "summary": "Ensuring fairness in AI systems is critical, especially in high-stakes\ndomains such as lending, hiring, and healthcare. This urgency is reflected in\nemerging global regulations that mandate fairness assessments and independent\nbias audits. However, procuring the necessary complete data for fairness\ntesting remains a significant challenge. In industry settings, legal and\nprivacy concerns restrict the collection of demographic data required to assess\ngroup disparities, and auditors face practical and cultural challenges in\ngaining access to data. In practice, data relevant for fairness testing is\noften split across separate sources: internal datasets held by institutions\nwith predictive attributes, and external public datasets such as census data\ncontaining protected attributes, each providing only partial, marginal\ninformation. Our work seeks to leverage such available separate data to\nestimate model fairness when complete data is inaccessible. We propose\nutilising the available separate data to estimate a set of feasible joint\ndistributions and then compute the set plausible fairness metrics. Through\nsimulation and real experiments, we demonstrate that we can derive meaningful\nbounds on fairness metrics and obtain reliable estimates of the true metric.\nOur results demonstrate that this approach can serve as a practical and\neffective solution for fairness testing in real-world settings where access to\ncomplete data is restricted.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u5229\u7528\u5206\u79bb\u7684\u5185\u90e8\u548c\u5916\u90e8\u6570\u636e\u6e90\u6765\u4f30\u8ba1AI\u6a21\u578b\u7684\u516c\u5e73\u6027\u6307\u6807\uff0c\u89e3\u51b3\u56e0\u6cd5\u5f8b\u548c\u9690\u79c1\u95ee\u9898\u5bfc\u81f4\u7684\u5b8c\u6574\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\u786e\u4fddAI\u7cfb\u7edf\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u4e2d\u56e0\u6cd5\u5f8b\u548c\u9690\u79c1\u9650\u5236\u5bfc\u81f4\u4eba\u53e3\u5b66\u7b49\u4fdd\u62a4\u5c5e\u6027\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u6570\u636e\u5e38\u5206\u5e03\u5728\u4e0d\u540c\u6e90\u5934\u4e2d\u3002", "method": "\u5229\u7528\u53ef\u7528\u7684\u5206\u79bb\u6570\u636e\u4f30\u8ba1\u53ef\u884c\u7684\u8054\u5408\u5206\u5e03\u96c6\u5408\uff0c\u7136\u540e\u8ba1\u7b97\u5408\u7406\u7684\u516c\u5e73\u6027\u6307\u6807\u8303\u56f4\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5f97\u5230\u6709\u610f\u4e49\u7684\u516c\u5e73\u6027\u6307\u6807\u4e0a\u4e0b\u754c\uff0c\u5e76\u83b7\u5f97\u771f\u5b9e\u6307\u6807\u7684\u53ef\u9760\u4f30\u8ba1\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u516c\u5e73\u6027\u6d4b\u8bd5\u7684\u5b9e\u7528\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5b8c\u6574\u6570\u636e\u8bbf\u95ee\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.13057", "categories": ["cs.LG", "cs.AI", "cs.PF", "62M10, 90C59, 68T05", "I.2.6; I.5.1; I.5.2; I.5.4; G.1.6"], "pdf": "https://arxiv.org/pdf/2508.13057", "abs": "https://arxiv.org/abs/2508.13057", "authors": ["Adolfo Gonz\u00e1lez", "V\u00edctor Parada"], "title": "Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models", "comment": "31 pages, 15 figures, 110 tables. Submitted as a preprint. The\n  manuscript introduces the Hierarchical Evaluation Function (HEF), a\n  multi-metric framework for optimizing demand forecasting models under high\n  uncertainty. Includes extensive experimental validation using real-world\n  datasets and a comparative analysis against classical and modern methods", "summary": "Demand forecasting is essential for strategic planning in competitive\nenvironments, enabling resource optimization and improved responsiveness to\nmarket dynamics. However, multivariate time series modeling faces challenges\ndue to data complexity, uncertainty, and frequent regime shifts. Traditional\nevaluation metrics can introduce biases and limit generalization. This work\ncompares two custom evaluation functions: FMAE (Focused Mean Absolute Error),\nfocused on minimizing absolute errors, and HEF (Hierarchical Evaluation\nFunction), designed to weight global metrics and penalize large deviations.\nExperiments were conducted under different data splits (91:9, 80:20, 70:30)\nusing three optimizers (Grid Search, PSO, Optuna), assessing fit, relative\naccuracy, robustness, and computational efficiency. Results show that HEF\nconsistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,\nRMSSE), enhancing model robustness and explanatory power. These findings were\nconfirmed via visualizations and statistical tests. Conversely, FMAE offers\nadvantages in local metrics (MAE, MASE) and execution time, making it suitable\nfor short-term scenarios. The study highlights a methodological trade-off: HEF\nis ideal for strategic planning, while FMAE is better suited for operational\nefficiency. A replicable framework is proposed for optimizing predictive models\nin dynamic environments.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86FMAE\u548cHEF\u4e24\u79cd\u8bc4\u4f30\u51fd\u6570\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0cHEF\u5728\u5168\u5c40\u6307\u6807\u4e0a\u66f4\u4f18\uff0c\u9002\u5408\u6218\u7565\u89c4\u5212\uff1bFMAE\u5728\u5c40\u90e8\u6307\u6807\u548c\u8fd0\u884c\u65f6\u95f4\u4e0a\u66f4\u597d\uff0c\u9002\u5408\u77ed\u671f\u64cd\u4f5c\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u9762\u4e34\u6570\u636e\u590d\u6742\u6027\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u5236\u5ea6\u8f6c\u79fb\u6311\u6218\uff0c\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u504f\u5dee\u548c\u9650\u5236\u6a21\u578b\u666e\u9002\u6027\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86FMAE\uff08\u805a\u7126\u5747\u7edd\u5bf9\u8bef\u5dee\uff09\u548cHEF\uff08\u5c42\u6b21\u8bc4\u4f30\u51fd\u6570\uff09\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6570\u636e\u5206\u5272\u6bd4\u4f8b\uff0891:9\u300180:20\u300170:30\uff09\u4e0b\u4f7f\u7528\u4e09\u79cd\u4f18\u5316\u5668\uff08\u7f51\u683c\u641c\u7d22\u3001PSO\u3001Optuna\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "HEF\u5728\u5168\u5c40\u6307\u6807\uff08R2\u3001\u76f8\u5bf9\u51c6\u786e\u5ea6\u3001RMSE\u3001RMSSE\uff09\u4e0a\u4e00\u8d35\u4f18\u4e8eFMAE\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7a33\u5065\u6027\u548c\u89e3\u91ca\u529b\uff1bFMAE\u5728\u5c40\u90e8\u6307\u6807\uff08MAE\u3001MASE\uff09\u548c\u6267\u884c\u65f6\u95f4\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u5448\u73b0\u4e86\u65b9\u6cd5\u8bba\u4e0a\u7684\u6743\u8861\uff1aHEF\u9002\u7528\u4e8e\u6218\u7565\u89c4\u5212\u573a\u666f\uff0cFMAE\u66f4\u9002\u5408\u64cd\u4f5c\u6548\u7387\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u9884\u6d4b\u6a21\u578b\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2508.13088", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.13088", "abs": "https://arxiv.org/abs/2508.13088", "authors": ["Xiaohan Wang", "Zhimin Li", "Joshua A. Levine", "Matthew Berger"], "title": "Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates", "comment": null, "summary": "Recently, neural surrogate models have emerged as a compelling alternative to\ntraditional simulation workflows. This is accomplished by modeling the\nunderlying function of scientific simulations, removing the need to run\nexpensive simulations. Beyond just mapping from input parameter to output,\nsurrogates have also been shown useful for inverse problems: output to input\nparameters. Inverse problems can be understood as search, where we aim to find\nparameters whose surrogate outputs contain a specified feature. Yet finding\nthese parameters can be costly, especially for high-dimensional parameter\nspaces. Thus, existing surrogate-based solutions primarily focus on finding a\nsmall set of matching parameters, in the process overlooking the broader\npicture of plausible parameters. Our work aims to model and visualize the\ndistribution of possible input parameters that produce a given output feature.\nTo achieve this goal, we aim to address two challenges: (1) the approximation\nerror inherent in the surrogate model and (2) forming the parameter\ndistribution in an interactive manner. We model error via density estimation,\nreporting high density only if a given parameter configuration is close to\ntraining parameters, measured both over the input and output space. Our density\nestimate is used to form a prior belief on parameters, and when combined with a\nlikelihood on features, gives us an efficient way to sample plausible parameter\nconfigurations that generate a target output feature. We demonstrate the\nusability of our solution through a visualization interface by performing\nfeature-driven parameter analysis over the input parameter space of three\nsimulation datasets. Source code is available at\nhttps://github.com/matthewberger/seeing-the-many", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u795e\u7ecf\u4ee3\u7406\u6a21\u578b\u6765\u53cd\u5411\u63a2\u7d22\u751f\u6210\u7279\u5b9a\u8f93\u51fa\u7279\u5f81\u7684\u53ef\u80fd\u53c2\u6570\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ee3\u7406\u6a21\u578b\u8fd1\u4f3c\u8bef\u5dee\u548c\u4ea4\u4e92\u5f0f\u53c2\u6570\u5206\u5e03\u6784\u5efa\u7684\u6311\u6218\u3002", "motivation": "\u76ee\u524d\u7684\u4ee3\u7406\u6a21\u578b\u9006\u5411\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u5173\u6ce8\u627e\u5230\u5c11\u91cf\u5339\u914d\u53c2\u6570\uff0c\u800c\u5ffd\u89c6\u4e86\u751f\u6210\u7279\u5b9a\u8f93\u51fa\u7279\u5f81\u7684\u66f4\u5e7f\u6cdb\u53ef\u80fd\u53c2\u6570\u5206\u5e03\u7684\u6574\u4f53\u56fe\u666f\u3002", "method": "\u901a\u8fc7\u5bc6\u5ea6\u4f30\u8ba1\u6a21\u578b\u5316\u4ee3\u7406\u6a21\u578b\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u4ee5\u8bad\u7ec3\u53c2\u6570\u7684\u8f93\u5165\u548c\u8f93\u51fa\u7a7a\u95f4\u8ddd\u79bb\u4e3a\u57fa\u7840\u62a5\u5448\u9ad8\u5bc6\u5ea6\u3002\u7ed3\u5408\u53c2\u6570\u7684\u5148\u9a8c\u4fe1\u5ff5\u548c\u7279\u5f81\u7684\u53ef\u80fd\u6027\uff0c\u6709\u6548\u5730\u91c7\u6837\u751f\u6210\u76ee\u6807\u8f93\u51fa\u7279\u5f81\u7684\u53ef\u80fd\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7279\u5f81\u9a71\u52a8\u7684\u53c2\u6570\u5206\u6790\uff0c\u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u754c\u9762\u6765\u5c55\u793a\u53c2\u6570\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5efa\u6a21\u5e76\u53ef\u89c6\u5316\u751f\u6210\u7279\u5b9a\u8f93\u51fa\u7279\u5f81\u7684\u53ef\u80fd\u8f93\u5165\u53c2\u6570\u5206\u5e03\uff0c\u4e3a\u79d1\u5b66\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u9006\u5411\u95ee\u9898\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13099", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.13099", "abs": "https://arxiv.org/abs/2508.13099", "authors": ["Mingyu Kim", "Daniel Stilwell", "Jorge Jimenez"], "title": "Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network", "comment": "IEEE OCEANS", "summary": "This paper presents a framework for classifying and detecting spatial\ncommission outliers in maritime environments using seabed acoustic sensor\nnetworks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as\na mixture of normal and outlier processes, we estimate the probability that a\nnewly observed event is an outlier. We propose a second-order approximation of\nthis probability that incorporates both the mean and variance of the normal\nintensity function, providing improved classification accuracy compared to\nmean-only approaches. We analytically show that our method yields a tighter\nbound to the true probability using Jensen's inequality. To enhance detection,\nwe integrate a real-time, near-optimal sensor placement strategy that\ndynamically adjusts sensor locations based on the evolving outlier intensity.\nThe proposed framework is validated using real ship traffic data near Norfolk,\nVirginia, where numerical results demonstrate the effectiveness of our approach\nin improving both classification performance and outlier detection through\nsensor deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6d77\u5e95\u58f0\u5b66\u4f20\u611f\u5668\u7f51\u7edc\u548cLGCP\u7684\u6d77\u4e0a\u7a7a\u95f4\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e8c\u9636\u6982\u7387\u8fd1\u4f3c\u548c\u52a8\u6001\u4f20\u611f\u5668\u90e8\u7f72\u63d0\u5347\u5f02\u5e38\u5206\u7c7b\u548c\u68c0\u6d4b\u6027\u80fd", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5747\u503c\u4fe1\u606f\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u7cbe\u5ea6\u6709\u9650\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5229\u7528\u5747\u503c\u548c\u65b9\u5dee\u4fe1\u606f\u7684\u66f4\u7cbe\u786e\u5f02\u5e38\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u914d\u5408\u52a8\u6001\u4f20\u611f\u5668\u90e8\u7f72\u7b56\u7565\u6765\u63d0\u5347\u6d77\u4e0a\u5f02\u5e38\u68c0\u6d4b\u6548\u679c", "method": "\u4f7f\u7528\u5bf9\u6570\u9ad8\u65afCox\u8fc7\u7a0b(LGCP)\u5efa\u6a21\u76ee\u6807\u5230\u8fbe\uff0c\u5c06\u4e8b\u4ef6\u5206\u4e3a\u6b63\u5e38\u548c\u5f02\u5e38\u8fc7\u7a0b\u6df7\u5408\u3002\u63d0\u51fa\u4e8c\u9636\u6982\u7387\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u6b63\u5e38\u5f3a\u5ea6\u51fd\u6570\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002\u7ed3\u5408\u5b9e\u65f6\u8fd1\u6700\u4f18\u4f20\u611f\u5668\u90e8\u7f72\u7b56\u7565\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u4f4d\u7f6e", "result": "\u5728\u5f17\u5409\u5c3c\u4e9a\u5dde\u8bfa\u798f\u514b\u771f\u5b9e\u8239\u8236\u4ea4\u901a\u6570\u636e\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4ec5\u4f7f\u7528\u5747\u503c\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u4f20\u611f\u5668\u90e8\u7f72\u6709\u6548\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e8c\u9636\u6982\u7387\u8fd1\u4f3c\u6846\u67b6\u4e3a\u6d77\u4e0a\u7a7a\u95f4\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u7ed3\u5408\u52a8\u6001\u4f20\u611f\u5668\u90e8\u7f72\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5f02\u5e38\u5206\u7c7b\u548c\u68c0\u6d4b\u7684\u6574\u4f53\u6027\u80fd"}}
{"id": "2508.13111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13111", "abs": "https://arxiv.org/abs/2508.13111", "authors": ["Michael Mayr", "Georgios C. Chasparis"], "title": "Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry", "comment": "12 pages, 2 figures, 4 tables", "summary": "Foundational modelling of multi-dimensional time-series data in industrial\nsystems presents a central trade-off: channel-dependent (CD) models capture\nspecific cross-variable dynamics but lack robustness and adaptability as model\nlayers are commonly bound to the data dimensionality of the tackled use-case,\nwhile channel-independent (CI) models offer generality at the cost of modelling\nthe explicit interactions crucial for system-level predictive regression tasks.\nTo resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a\nnovel architecture that integrates a known causal graph as an inductive bias.\nThe core of CGPT is built around a pairwise modeling paradigm, tackling the\nCD/CI conflict by decomposing the multidimensional data into pairs. The model\nuses channel-agnostic learnable layers where all parameter dimensions are\nindependent of the number of variables. CGPT enforces a CD information flow at\nthe pair-level and CI-like generalization across pairs. This approach\ndisentangles complex system dynamics and results in a highly flexible\narchitecture that ensures scalability and any-variate adaptability. We validate\nCGPT on a suite of synthetic and real-world industrial datasets on long-term\nand one-step forecasting tasks designed to simulate common industrial\ncomplexities. Results demonstrate that CGPT significantly outperforms both CI\nand CD baselines in predictive accuracy and shows competitive performance with\nend-to-end trained CD models while remaining agnostic to the problem\ndimensionality.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86CGPT\u6a21\u578b\uff0c\u901a\u8fc7\u56e0\u679c\u56fe\u5bfc\u5411\u7684\u6210\u5bf9\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u7684\u901a\u7528\u6027\u548c\u7cbe\u786e\u6027\u95ee\u9898\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u8d85\u8fc7\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u5b58\u5728\u4e24\u79cd\u65b9\u6cd5\u7684\u56f0\u5883\uff1a\u901a\u9053\u4f9d\u8d56(CD)\u6a21\u578b\u80fd\u6293\u53d6\u7279\u5b9a\u4ea4\u53c9\u53d8\u91cf\u52a8\u529b\u5b66\u4f46\u7f3a\u4e4f\u7a33\u5065\u6027\u548c\u9002\u914d\u6027\uff0c\u800c\u901a\u9053\u72ec\u7acb(CI)\u6a21\u578b\u867d\u6709\u901a\u7528\u6027\u5374\u65e0\u6cd5\u6a21\u62df\u5173\u952e\u7684\u660e\u786e\u76f8\u4e92\u4f5c\u7528\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u4e89\u8bae\u3002", "method": "\u63d0\u51fa\u4e86\u56e0\u679c\u5bfc\u5411\u7684\u6210\u5bf9Transformer(CGPT)\u67b6\u6784\uff0c\u5c06\u5df2\u77e5\u56e0\u679c\u56fe\u4f5c\u4e3a\u5f15\u5bfc\u504f\u7f6e\u96c6\u6210\u5230\u6a21\u578b\u4e2d\u3002\u6838\u5fc3\u662f\u6210\u5bf9\u5efa\u6a21\u8303\u5f0f\uff0c\u5c06\u591a\u7ef4\u6570\u636e\u5206\u89e3\u4e3a\u6210\u5bf9\u3002\u6a21\u578b\u4f7f\u7528\u901a\u9053\u65e0\u5173\u7684\u53ef\u5b66\u4e60\u5c42\uff0c\u6240\u6709\u53c2\u6570\u7ef4\u5ea6\u90fd\u4e0e\u53d8\u91cf\u6570\u91cf\u72ec\u7acb\u3002CGPT\u5728\u6210\u5bf9\u7ea7\u522b\u5f3a\u5316CD\u4fe1\u606f\u6d41\u52a8\uff0c\u540c\u65f6\u5728\u6210\u5bf9\u4e4b\u95f4\u5b9e\u73b0CI\u7c7b\u4f3c\u7684\u901a\u7528\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u957f\u671f\u9884\u6d4b\u548c\u5355\u6b65\u9884\u6d4b\u4efb\u52a1\u7684\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u4efb\u52a1\u6a21\u62df\u4e86\u5e38\u89c1\u7684\u5de5\u4e1a\u590d\u6742\u6027\u3002\u7ed3\u679c\u663e\u793aCGPT\u5728\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u663e\u8457\u8d85\u8fc7\u4e86CI\u548cCD\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u4e0e\u7aef\u5230\u7aef\u8bad\u7ec3\u7684CD\u6a21\u578b\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u95ee\u9898\u7ef4\u5ea6\u7684\u65e0\u5173\u6027\u3002", "conclusion": "CGPT\u901a\u8fc7\u56e0\u679c\u5bfc\u5411\u7684\u6210\u5bf9\u5efa\u6a21\u65b9\u6cd5\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u591a\u7ef4\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u7684\u901a\u7528\u6027\u4e0e\u7cbe\u786e\u6027\u4e4b\u95f4\u7684\u4e89\u8bae\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u89e3\u6784\u590d\u6742\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u6784\u5efa\u4e86\u9ad8\u5ea6\u7075\u6d3b\u7684\u67b6\u6784\uff0c\u786e\u4fdd\u4e86\u53ef\u6269\u5c55\u6027\u548c\u4efb\u610f\u53d8\u91cf\u9002\u914d\u80fd\u529b\uff0c\u4e3a\u5de5\u4e1a\u9884\u6d4b\u56de\u5f52\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13113", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13113", "abs": "https://arxiv.org/abs/2508.13113", "authors": ["Alicja Ziarko", "Michal Bortkiewicz", "Michal Zawalski", "Benjamin Eysenbach", "Piotr Milos"], "title": "Contrastive Representations for Temporal Reasoning", "comment": "Project website: https://princeton-rl.github.io/CRTR/", "summary": "In classical AI, perception relies on learning state-based representations,\nwhile planning, which can be thought of as temporal reasoning over action\nsequences, is typically achieved through search. We study whether such\nreasoning can instead emerge from representations that capture both perceptual\nand temporal structure. We show that standard temporal contrastive learning,\ndespite its popularity, often fails to capture temporal structure due to its\nreliance on spurious features. To address this, we introduce Combinatorial\nRepresentations for Temporal Reasoning (CRTR), a method that uses a negative\nsampling scheme to provably remove these spurious features and facilitate\ntemporal reasoning. CRTR achieves strong results on domains with complex\ntemporal structure, such as Sokoban and Rubik's Cube. In particular, for the\nRubik's Cube, CRTR learns representations that generalize across all initial\nstates and allow it to solve the puzzle using fewer search steps than BestFS,\nthough with longer solutions. To our knowledge, this is the first method that\nefficiently solves arbitrary Cube states using only learned representations,\nwithout relying on an external search algorithm.", "AI": {"tldr": "CRTR\u65b9\u6cd5\u901a\u8fc7\u7ec4\u5408\u8d1f\u91c7\u6837\u65b9\u6848\u6d88\u9664\u4f2a\u7279\u5f81\uff0c\u4f7f\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u80fd\u591f\u6355\u6349\u65f6\u95f4\u7ed3\u6784\uff0c\u5728Sokoban\u548c\u9b54\u65b9\u7b49\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u9996\u6b21\u5b9e\u73b0\u4ec5\u901a\u8fc7\u5b66\u4e60\u8868\u5f81\u9ad8\u6548\u89e3\u51b3\u4efb\u610f\u9b54\u65b9\u72b6\u6001\u3002", "motivation": "\u4f20\u7edfAI\u4e2d\u611f\u77e5\u5b66\u4e60\u72b6\u6001\u8868\u5f81\uff0c\u89c4\u5212\u901a\u8fc7\u641c\u7d22\u5b9e\u73b0\u65f6\u5e8f\u63a8\u7406\u3002\u672c\u6587\u7814\u7a76\u662f\u5426\u53ef\u4ee5\u4ece\u540c\u65f6\u6355\u6349\u611f\u77e5\u548c\u65f6\u95f4\u7ed3\u6784\u7684\u8868\u5f81\u4e2d\u6d8c\u73b0\u51fa\u8fd9\u79cd\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u6807\u51c6\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u4f9d\u8d56\u4f2a\u7279\u5f81\u800c\u65e0\u6cd5\u6355\u6349\u65f6\u95f4\u7ed3\u6784\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7ec4\u5408\u65f6\u5e8f\u63a8\u7406\u8868\u5f81(CRTR)\uff0c\u91c7\u7528\u8d1f\u91c7\u6837\u65b9\u6848\u6765\u53ef\u8bc1\u660e\u5730\u79fb\u9664\u4f2a\u7279\u5f81\uff0c\u4fc3\u8fdb\u65f6\u5e8f\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u65f6\u5e8f\u7ed3\u6784\u9886\u57df\uff08\u5982Sokoban\u548c\u9b54\u65b9\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "CRTR\u5728\u9b54\u65b9\u4efb\u52a1\u4e0a\u5b66\u4e60\u7684\u8868\u5f81\u80fd\u591f\u6cdb\u5316\u5230\u6240\u6709\u521d\u59cb\u72b6\u6001\uff0c\u76f8\u6bd4BestFS\u4f7f\u7528\u66f4\u5c11\u7684\u641c\u7d22\u6b65\u9aa4\uff08\u5c3d\u7ba1\u89e3\u51b3\u65b9\u6848\u66f4\u957f\uff09\u3002\u8fd9\u662f\u9996\u4e2a\u4ec5\u901a\u8fc7\u5b66\u4e60\u8868\u5f81\u5c31\u80fd\u9ad8\u6548\u89e3\u51b3\u4efb\u610f\u9b54\u65b9\u72b6\u6001\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8\u641c\u7d22\u7b97\u6cd5\u3002", "conclusion": "CRTR\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u4f2a\u7279\u5f81\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u4ece\u5b66\u4e60\u8868\u5f81\u4e2d\u6d8c\u73b0\u65f6\u5e8f\u63a8\u7406\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u7ed3\u5408\u611f\u77e5\u548c\u65f6\u5e8f\u63a8\u7406\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.13135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13135", "abs": "https://arxiv.org/abs/2508.13135", "authors": ["Yueyang Liu", "Lance Kennedy", "Ruochen Kong", "Joon-Seok Kim", "Andreas Z\u00fcfle"], "title": "Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]", "comment": null, "summary": "Individual-level human mobility prediction has emerged as a significant topic\nof research with applications in infectious disease monitoring, child, and\nelderly care. Existing studies predominantly focus on the microscopic aspects\nof human trajectories: such as predicting short-term trajectories or the next\nlocation visited, while offering limited attention to macro-level mobility\npatterns and the corresponding life routines. In this paper, we focus on an\nunderexplored problem in human mobility prediction: determining the best\npractices to train a machine learning model using historical data to forecast\nan individuals complete trajectory over the next days and weeks. In this\nexperiment paper, we undertake a comprehensive experimental analysis of diverse\nmodels, parameter configurations, and training strategies, accompanied by an\nin-depth examination of the statistical distribution inherent in human mobility\npatterns. Our empirical evaluations encompass both Long Short-Term Memory and\nTransformer-based architectures, and further investigate how incorporating\nindividual life patterns can enhance the effectiveness of the prediction. We\nshow that explicitly including semantic information such as day-of-the-week and\nuser-specific historical information can help the model better understand\nindividual patterns of life and improve predictions. Moreover, since the\nabsence of explicit user information is often missing due to user privacy, we\nshow that the sampling of users may exacerbate data skewness and result in a\nsubstantial loss in predictive accuracy. To mitigate data imbalance and\npreserve diversity, we apply user semantic clustering with stratified sampling\nto ensure that the sampled dataset remains representative. Our results further\nshow that small-batch stochastic gradient optimization improves model\nperformance, especially when human mobility training data is limited.", "AI": {"tldr": "\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u65b0\u65b9\u6cd5\uff1a\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u3001\u8bed\u4e49\u4fe1\u606f\u6574\u5408\u548c\u6837\u672c\u91cd\u65b0\u91c7\u6837\u63d0\u5347\u957f\u671f\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u77ed\u671f\u5fae\u89c2\u79fb\u52a8\u9884\u6d4b\uff0c\u5bf9\u5b8f\u89c2\u5c42\u9762\u7684\u751f\u6d3b\u89c4\u5f8b\u548c\u957f\u671f\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u4f7f\u7528\u5386\u53f2\u6570\u636e\u8fdb\u884c\u6709\u6548\u7684\u957f\u671f\u79fb\u52a8\u9884\u6d4b", "method": "\u8fdb\u884c\u5168\u9762\u7684\u5b9e\u9a8c\u5206\u6790\uff0c\u5305\u62ecLSTM\u548cTransformer\u6a21\u578b\u3001\u53c2\u6570\u914d\u7f6e\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6df1\u5165\u7814\u7a76\u79fb\u52a8\u6a21\u5f0f\u7684\u7edf\u8ba1\u5206\u5e03\uff0c\u91c7\u7528\u7528\u6237\u8bed\u4e49\u805a\u7c7b\u548c\u5c42\u5316\u91c7\u6837\u6765\u51cf\u5c11\u6570\u636e\u504f\u659c", "result": "\u663e\u5f0f\u5305\u542b\u5468\u5185\u5929\u548c\u7528\u6237\u7279\u5b9a\u5386\u53f2\u4fe1\u606f\u80fd\u591f\u63d0\u5347\u9884\u6d4b\u6548\u679c\uff0c\u5c0f\u6279\u91cf\u968f\u673a\u68af\u5ea6\u4f18\u5316\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f73\uff0c\u91cd\u65b0\u91c7\u6837\u7b56\u7565\u6709\u52a9\u4e8e\u4fdd\u6301\u6570\u636e\u591a\u6837\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027", "conclusion": "\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u3001\u8bed\u4e49\u4fe1\u606f\u6574\u5408\u548c\u6837\u672c\u91cd\u65b0\u91c7\u6837\u7b49\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4e2a\u4f53\u957f\u671f\u79fb\u52a8\u8f68\u8ff9\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u4f20\u67d3\u75c5\u76d1\u6d4b\u3001\u8001\u5e7c\u517b\u62a4\u7b49\u5e94\u7528\u9886\u57df\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.13148", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13148", "abs": "https://arxiv.org/abs/2508.13148", "authors": ["Haoyu He", "Katrin Renz", "Yong Cao", "Andreas Geiger"], "title": "MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models", "comment": null, "summary": "Diffusion language models, as a promising alternative to traditional\nautoregressive (AR) models, enable faster generation and richer conditioning on\nbidirectional context. However, they suffer from a key discrepancy between\ntraining and inference: during inference, MDLMs progressively reveal the\nstructure of the generated sequence by producing fewer and fewer masked tokens,\nwhereas this structure is ignored in training as tokens are masked at random.\nAlthough this discrepancy between training and inference can lead to suboptimal\nperformance, it has been largely overlooked by previous works, leaving closing\nthis gap between the two stages an open problem. To address this, we frame the\nproblem of learning effective denoising trajectories as a sequential\ndecision-making problem and use the resulting framework to apply reinforcement\nlearning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to\nexploit the Markov property diffusion possesses and explicitly train the model\nunder the same progressive refining schedule used at inference. MDPO matches\nthe performance of the previous state-of-the-art (SOTA) method with 60x fewer\ngradient updates, while achieving average improvements of 9.6% on MATH500 and\n54.2% on Countdown over SOTA when trained within the same number of weight\nupdates. Additionally, we improve the remasking strategy of MDLMs as a plug-in\ninference replacement to overcome the limitation that the model cannot refine\ntokens flexibly. This simple yet effective training-free strategy, what we\nrefer to as RCR, consistently improves performance and yields additional gains\nwhen combined with MDPO. Our findings establish great potential for\ninvestigating the discrepancy between pre-training and inference of MDLMs.\nCode: https://github.com/autonomousvision/mdpo. Project Page:\nhttps://cli212.github.io/MDPO/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMDPO\u65b9\u6cd5\u6765\u89e3\u51b3\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u7684\u7ed3\u6784\u5dee\u5f02\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u53bb\u566a\u8f68\u8ff9\uff0c\u5728\u51cf\u5c1160\u500d\u68af\u5ea6\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728MATH500\u548cCountdown\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u53479.6%\u548c54.2%\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u7684\u5173\u952e\u5dee\u5f02\uff1a\u63a8\u7406\u65f6\u9010\u6b65\u63ed\u793a\u751f\u6210\u5e8f\u5217\u7ed3\u6784\uff0c\u800c\u8bad\u7ec3\u65f6\u968f\u673a\u63a9\u7801\u5ffd\u7565\u8fd9\u79cd\u7ed3\u6784\uff0c\u8fd9\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u4f46\u5148\u524d\u7814\u7a76\u5927\u591a\u5ffd\u89c6\u6b64\u95ee\u9898\u3002", "method": "\u5c06\u5b66\u4e60\u6709\u6548\u53bb\u566a\u8f68\u8ff9\u95ee\u9898\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u51faMasked Diffusion Policy Optimization (MDPO)\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\uff0c\u5728\u63a8\u7406\u4f7f\u7528\u7684\u6e10\u8fdb\u7cbe\u70bc\u8c03\u5ea6\u4e0b\u663e\u5f0f\u8bad\u7ec3\u6a21\u578b\u3002", "result": "MDPO\u4ee560\u500d\u66f4\u5c11\u7684\u68af\u5ea6\u66f4\u65b0\u5339\u914d\u5148\u524dSOTA\u6027\u80fd\uff0c\u5728\u76f8\u540c\u6743\u91cd\u66f4\u65b0\u6b21\u6570\u4e0b\uff0cMATH500\u63d0\u53479.6%\uff0cCountdown\u63d0\u534754.2%\u3002\u540c\u65f6\u63d0\u51fa\u7684RCR\u91cd\u63a9\u7801\u7b56\u7565\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u63a8\u7406\u66ff\u4ee3\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5c55\u793a\u4e86\u7814\u7a76MDLMs\u9884\u8bad\u7ec3\u4e0e\u63a8\u7406\u5dee\u5f02\u7684\u5de8\u5927\u6f5c\u529b\uff0cMDPO\u548cRCR\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8bad\u7ec3-\u63a8\u7406\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u5e76\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
