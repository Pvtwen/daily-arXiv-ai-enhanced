<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 18]
- [cs.LG](#cs.LG) [Total: 72]
- [stat.ML](#stat.ML) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Depression diagnosis from patient interviews using multimodal machine learning](https://arxiv.org/abs/2508.19390)
*Jana Weber,Marcel Weber,Juan Miguel Lopez Alcaraz*

Main category: eess.SP

TL;DR: 该研究开发了一种基于多模态融合的抑郁症诊断方法，整合语音、语言和临床特征，相比单模态模型显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是全球重大公共卫生问题，早期准确诊断对有效治疗至关重要。临床识别存在挑战，患者访谈中的语音、语言和行为线索可能提供客观的诊断标记。

Method: 开发多模态诊断方法，整合患者访谈中的语音模式、语言特征和结构化临床信息。为每个模态训练单独模型，然后通过多模态融合反映真实精神病学评估的复杂性。使用性能指标、校准和决策分析方法评估模型有效性。

Result: 多模态模型相比单模态模型获得更优诊断准确性，AUROC为0.88，F1分数为0.75。融合模型表现出良好校准性，相比基线策略提供更高的临床净收益。

Conclusion: 基于机器学习的患者访谈多模态分析可作为精神病学评估的有价值辅助工具。通过整合语音、语言和临床特征，该方法提供了增强抑郁症早期检测和支持精神医疗循证决策的稳健框架。

Abstract: Background: Depression is a major public health concern, affecting an
estimated five percent of the global population. Early and accurate diagnosis
is essential to initiate effective treatment, yet recognition remains
challenging in many clinical contexts. Speech, language, and behavioral cues
collected during patient interviews may provide objective markers that support
clinical assessment.
  Methods: We developed a diagnostic approach that integrates features derived
from patient interviews, including speech patterns, linguistic characteristics,
and structured clinical information. Separate models were trained for each
modality and subsequently combined through multimodal fusion to reflect the
complexity of real-world psychiatric assessment. Model validity was assessed
with established performance metrics, and further evaluated using calibration
and decision-analytic approaches to estimate potential clinical utility.
  Results: The multimodal model achieved superior diagnostic accuracy compared
to single-modality models, with an AUROC of 0.88 and an F1-score of 0.75.
Importantly, the fused model demonstrated good calibration and offered higher
net clinical benefit compared to baseline strategies, highlighting its
potential to assist clinicians in identifying patients with depression more
reliably.
  Conclusion: Multimodal analysis of patient interviews using machine learning
may serve as a valuable adjunct to psychiatric evaluation. By combining speech,
language, and clinical features, this approach provides a robust framework that
could enhance early detection of depressive disorders and support
evidence-based decision-making in mental healthcare.

</details>


### [2] [The Coherent Multiplex: Scalable Real-Time Wavelet Coherence Architecture](https://arxiv.org/abs/2508.19994)
*Noah Shore*

Main category: eess.SP

TL;DR: 提出并验证了Coherent Multiplex系统，这是一个可扩展的实时系统，用于识别、分析和可视化多个时间序列之间的相干性。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够实时处理和分析多个时间序列之间相干性的可扩展系统，应用于神经科学、金融和生物医学信号分析等领域。

Method: 系统架构包含基于傅里叶变换信号余弦相似度度量的快速频谱相似性层，以及用于小波相干性的稀疏时频层。构建并演化表示信号间关系的多层图，实现低延迟推理和监控。

Result: 仿真原型在8个合成通道上展示了功能，使用高相似度阈值进行进一步计算，系统架构可扩展至支持数千个输入信号。

Conclusion: Coherent Multiplex是一个有效的实时相干性分析系统，具有很好的可扩展性和实际应用潜力。

Abstract: The Coherent Multiplex is formalized and validated as a scalable, real-time
system for identifying, analyzing, and visualizing coherence among multiple
time series. Its architecture comprises a fast spectral similarity layer based
on cosine similarity metrics of Fourier-transformed signals, and a sparse
time-frequency layer for wavelet coherence. The system constructs and evolves a
multilayer graph representing inter-signal relationships, enabling low-latency
inference and monitoring. A simulation prototype demonstrates functionality
across 8 synthetic channels with a high similarity threshold for further
computation, with additional opportunities for scaling the architecture up to
support thousands of input signals with constrained hardware. Applications
discussed include neuroscience, finance, and biomedical signal analysis.

</details>


### [3] [1-Bit Unlimited Sampling Beyond Fourier Domain: Low-Resolution Sampling of Quantization Noise](https://arxiv.org/abs/2508.19408)
*Vaclav Pavlicek,Ayush Bhandari*

Main category: eess.SP

TL;DR: 这篇论文提出了一种新的1比特采样架构，通过将噪声形成概念扩展到复杂变换域，实现了更优的采样效果和恢复性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统1比特ADC和Sigma-Delta采样在处理任意动态范围信号时的激流问题，突破传统ADC在采样率、低比特宽度和能耗之间的特性约束。

Method: 通过将噪声形成概念从傀里司域扩展到其他变换域，开发了新的变换域恢复方法，构建了1比特无限采样框架。

Result: 在傀里司域应用时，该方法在性能上超过现有时域技术，减少了过采样要求并提高了稳健性。数值实验验证了这些发现。

Conclusion: 该研究为1比特采样系统的更广泛普遍化奠定了基础，通过变换域方法实现了更高效的信号采集和恢复。

Abstract: Analog-to-digital converters (ADCs) play a critical role in digital signal
acquisition across various applications, but their performance is inherently
constrained by sampling rates and bit budgets. This bit budget imposes a
trade-off between dynamic range (DR) and digital resolution, with ADC energy
consumption scaling linearly with sampling rate and exponentially with bit
depth. To bypass this, numerous approaches, including oversampling with
low-resolution ADCs, have been explored. A prominent example is 1-Bit ADCs with
Sigma-Delta Quantization (SDQ), a widely used consumer-grade solution. However,
SDQs suffer from overloading or saturation issues, limiting their ability to
handle inputs with arbitrary DR. The Unlimited Sensing Framework (USF)
addresses this challenge by injecting modulo non-linearity in hardware,
resulting in a new digital sensing technology. In this paper, we introduce a
novel 1-Bit sampling architecture that extends both conventional 1-Bit SDQ and
USF. Our contributions are twofold: (1) We generalize the concept of noise
shaping beyond the Fourier domain, allowing the inclusion of non-bandlimited
signals in the Fourier domain but bandlimited in alternative transform domains.
(2) Building on this generalization, we develop a new transform-domain recovery
method for 1-Bit USF. When applied to the Fourier domain, our method
demonstrates superior performance compared to existing time-domain techniques,
offering reduced oversampling requirements and improved robustness. Extensive
numerical experiments validate our findings, laying the groundwork for a
broader generalization of 1-Bit sampling systems.

</details>


### [4] [Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data](https://arxiv.org/abs/2508.11693)
*Francisco López,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 使用SVM分类器分析STDS轨道电路电流数据，自动识别15种故障类型，提高维护效率


<details>
  <summary>Details</summary>
Motivation: 轨道电路是检测列车存在的主要信号设备，传统维护方式效率低下，需要自动化故障识别方法来提高维护效率

Method: 采用支持向量机(SVM)分类器分析智能列车检测系统(STDS)的电流数据，将故障分为3大类15种具体类型

Result: 在10个不同轨道电路的现场数据上测试，所有用例都被正确分类，得到了STDS专家和维护人员的验证

Conclusion: 该方法能够有效自动识别轨道电路组件故障，显著改善维护行动效率

Abstract: Track Circuits (TC) are the main signalling devices used to detect the
presence of a train on a rail track. It has been used since the 19th century
and nowadays there are many types depending on the technology. As a general
classification, Track Circuits can be divided into 2 main groups, DC (Direct
Current) and AC (Alternating Current) circuits. This work is focused on a
particular AC track circuit, called "Smart Train Detection System" (STDS),
designed with both high and low-frequency bands. This approach uses STDS
current data applied to an SVM (support vector machine) classifier as a type of
failure identifier. The main purpose of this work consists on determine
automatically which is the component of the track that is failing to improve
the maintenance action. Model was trained to classify 15 different failures
that belong to 3 more general categories. The method was tested with field data
from 10 different track circuits and validated by the STDS track circuit expert
and maintainers. All use cases were correctly classified by the method.

</details>


### [5] [In-Lab Carrier Aggregation Testbed for Satellite Communication Systems](https://arxiv.org/abs/2508.19439)
*Jorge L. Gonzalez-Rios,Eva Lagunas,Hayder Al-Hraishawi,Luis M. Garces-Socarras,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 本文通过SDR和卫星通道模拟器构建了卫星通信载波聚合实验平台，验证了在GEO、MEO及混合轨道场景下的CA性能优势。


<details>
  <summary>Details</summary>
Motivation: 载波聚合技术在地面网络中已经成功应用，卫星通信社区希望借鉴该技术来提高频谱利用率和满足用户流量需求。虽然理论分析已经存在，但缺乏实验验证，特别是多轨道场景下的实际性能。

Method: 设计并构建了基于软件定义无线电(SDR)和卫星通道模拟器的实验测试平台。包含网关(GW)模块负责跨聚合载波的PDU调度，以及用户终端(UT)模块负责聚合多个收到流。

Result: 实验评估包括单个GEO卫星、单个MEO卫星以及GEO与MEO卫星组合的CA场景。测试结果显示了CA在卫星通信系统中的潜在优势，特别是在多轨道场景下。

Conclusion: 本文为卫星通信载波聚合提供了实验验证，证明了该技术的可行性和效益，为未来过空中系统测试奠定了基础。工作的主要贡献是在测试平台设计中明确考虑了多轨道场景。

Abstract: Carrier Aggregation (CA) is a technique used in 5G and previous cellular
generations to temporarily increase the data rate of a specific user during
peak demand periods or to reduce carrier congestion. CA is achieved by
combining two or more carriers and providing a virtual, wider overall bandwidth
to high-demand users of the system. CA was introduced in the 4G/LTE wireless
era and has been proven effective in 5G as well, where it is said to play a
significant role in efficient network capacity management. Given this success,
the satellite communication (SatCom) community has put its attention into CA
and the potential benefits it can bring in terms of better spectrum utilization
and better meeting the user traffic demand. While the theoretical evaluation of
CA for SatCom has already been presented in several works, this article
presents the design and results obtained with an experimentation testbed based
on Software Defined Radio (SDR) and a satellite channel emulator. We first
present the detailed implementation design, which includes a Gateway (GW)
module responsible for PDU-scheduling across the aggregated carriers, and a
User Terminal (UT) module responsible for aggregating the multiple received
streams. The second part of the article presents the experimental evaluation,
including CA over a single Geostationary (GEO) satellite, CA over a single
Medium Earth Orbit (MEO) satellite, and CA combining carriers sent over GEO and
MEO satellites. A key contribution of this work is the explicit consideration
of multi-orbit scenarios in the testbed design and validation. The testing
results show promising benefits of CA over SatCom systems, motivating potential
upcoming testing on over-the-air systems.

</details>


### [6] [Fourth-Order Hierarchical Array: A Novel Scheme for Sparse Array Design Based on Fourth-Order Difference Co-Array](https://arxiv.org/abs/2508.19522)
*Si Wang,Guoqiang Xiao*

Main category: eess.SP

TL;DR: 提出基于不同四阶差联合阵列形式的四阶分层阵列设计方案，在考虑互耦效应的同时增加自由度并降低冗余度


<details>
  <summary>Details</summary>
Motivation: 传统基于四阶累积量的阵列设计采用单一的四阶差联合阵列形式，限制了可获得的自由度且忽略了物理传感器间互耦的影响

Method: 使用任意生成器集构建基于不同FODCA形式的四阶分层阵列(FOHA)，推导生成器耦合泄漏与FOHA之间的解析关系，提出两种具有闭式传感器布局的具体FOHA配置

Result: 所提阵列不仅能增加DOA估计中的自由度以分辨更多信源，还能有效抑制互耦效应，且相比现有基于FODCA的阵列具有更低的冗余度

Conclusion: FOHA相比其他稀疏线性阵列具有优越的DOA估计性能，提供了增强的自由度和改进的抗互耦鲁棒性

Abstract: Conventional array designs based on circular fourth-order cumulant typically
adopt a single expression form of the fourth-order difference co-array (FODCA),
which limits the achievable degrees of freedom (DOFs) and neglects the impact
of mutual coupling among physical sensors. To address above issues, this paper
proposes a novel scheme to design arrays with increased DOFs by combining
different forms of FODCA while accounting for mutual coupling. A novel
fourth-order hierarchical array (FOHA) based on different forms of FODCA is
constructed using an arbitrary generator set. The analytical expression between
the coupling leakage of the generator and the resulting FOHA is derived. Two
specific FOHA configurations are presented with closed-form sensor placements.
The arrays not only offer increased DOFs for resolving more sources in
direction of-arrival (DOA) estimation but also effectively suppress mutual
coupling. Moreover, the redundancy of FODCA is examined, and it is shown that
arrays based on the proposed scheme achieve lower redundancy compared to
existing arrays based on FODCA. Meanwhile, the necessary and sufficient
conditions for signal reconstruction by FOHA are derived. Compared with
existing arrays based on FODCA, the proposed arrays provide enhanced DOFs and
improved robustness against mutual coupling. Numerical simulations verify that
FOHAs achieve superior DOA estimation performance compared with other sparse
linear arrays.

</details>


### [7] [Pinching Antenna System for Integrated Sensing and Communications](https://arxiv.org/abs/2508.19540)
*Haochen Li,Ruikang Zhong,Jiayi Lei,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文提出一种基于多波导链控制天线系统(PASS)的集成感知与通信(ISAC)系统，通过交替优化算法最小化目标感知的Cramer-Rao下界，在满足通信质量、功率预算和天线部署约束的前提下实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 链控天线系统(PASS)具有灵活部署和减少信号传播损耗的优势，适合用于集成感知与通信(ISAC)系统以提升系统性能。

Method: 采用全双工基站配置发射链控天线和接收均匀线性数组天线，通过交替优化(AO)方法解决非凸优化问题，包括数字放大子问题(使用半正定松弛)和链控放大子问题(使用逐步凸近似、惩罚法和元素级优化)。

Result: 模拟结果显示，所提方案在性能上超过基准方案，对严格通信约束的效果影响较传统MIMO-ISAC更小，并能多从波导数量和每波导天线数量的增加中获益。

Conclusion: 研究证明了PASS辅助ISAC系统的可行性和优势，为灵活部署的感通一体化系统提供了有效解决方案。

Abstract: Recently, the pinching antenna system (PASS) has attracted considerable
attention due to their advantages in flexible deployment and reduction of
signal propagation loss. In this work, a multiple waveguide PASS assisted
integrated sensing and communication (ISAC) system is proposed, where the base
station (BS) is equipped with transmitting pinching antennas (PAs) and
receiving uniform linear array (ULA) antennas. The full-duplex (FD) BS
transmits the communication and sensing signals through the PAs on waveguides
and collects the echo sensing signals with the mounted ULA. Based on this
configuration, a target sensing Cramer Rao Bound (CRB) minimization problem is
formulated under communication quality-of-service (QoS) constraints, power
budget constraint, and PA deployment constraints. The alternating optimization
(AO) method is employed to address the formulated non-convex optimization
problem. In each iteration, the overall optimization problem is decomposed into
a digital beamforming sub-problem and a pinching beamforming sub-problem. The
sensing covariance matrix and communication beamforming matrix at the BS are
optimized by solving the digital beamforming sub-problem with semidefinite
relaxation (SDR). The PA deployment is updated by solving the pinching
beamforming sub-problem with the successive convex approximation (SCA) method,
penalty method, and element-wise optimization. Simulation results show that the
proposed PASS assisted ISAC framework achieves superior performance over
benchmark schemes, is less affected by stringent communication constraints
compared to conventional MIMO-ISAC, and benefits further from increasing the
number of waveguides and PAs per waveguide.

</details>


### [8] [CSRD2025: A Large-Scale Synthetic Radio Dataset for Spectrum Sensing in Wireless Communications](https://arxiv.org/abs/2508.19552)
*Shuo Chang,Rui Sun,Jiashuo He,Sai Huang,Kan Yu,Zhiyong Feng*

Main category: eess.SP

TL;DR: CSRD框架是一个开源模块化仿真平台，用于生成大规模合成射频数据，包含2500万帧数据（约200TB），比RML2018数据集大10000倍，旨在解决AI模型训练中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在无线通信领域的开发，特别是频谱感知等复杂任务，严重依赖大量、多样且真实的数据集。现有数据集规模有限，无法满足AI模型训练需求。

Method: 开发CSRD框架模拟端到端传输接收过程，包含100种调制方案、可配置信道模型（统计衰落和射线追踪）、RF前端损伤建模，支持多种天线配置。提供数据处理管道将IQ数据转换为COCO格式的频谱图。

Result: 生成CSRD2025数据集，包含超过25,000,000帧数据（约200TB），提供前所未有的信号多样性和复杂性，标准化8:1:1的训练/验证/测试划分。

Conclusion: CSRD框架和数据集能够有效弥合仿真与现实之间的差距，加速AI驱动的频谱感知和管理技术发展，框架已在GitHub开源。

Abstract: The development of Large AI Models (LAMs) for wireless communications,
particularly for complex tasks like spectrum sensing, is critically dependent
on the availability of vast, diverse, and realistic datasets. Addressing this
need, this paper introduces the ChangShuoRadioData (CSRD) framework, an
open-source, modular simulation platform designed for generating large-scale
synthetic radio frequency (RF) data. CSRD simulates the end-to-end transmission
and reception process, incorporating an extensive range of modulation schemes
(100 types, including analog, digital, OFDM, and OTFS), configurable channel
models featuring both statistical fading and site-specific ray tracing using
OpenStreetMap data, and detailed modeling of realistic RF front-end impairments
for various antenna configurations (SISO/MISO/MIMO). Using this framework, we
characterize CSRD2025, a substantial dataset benchmark comprising over
25,000,000 frames (approx. 200TB), which is approximately 10,000 times larger
than the widely used RML2018 dataset. CSRD2025 offers unprecedented signal
diversity and complexity, specifically engineered to bridge the Sim2Real gap.
Furthermore, we provide processing pipelines to convert IQ data into
spectrograms annotated in COCO format, facilitating object detection approaches
for time-frequency signal analysis. The dataset specification includes
standardized 8:1:1 training, validation, and test splits (via frame indices) to
ensure reproducible research. The CSRD framework is released at
https://github.com/Singingkettle/ChangShuoRadioData to accelerate the
advancement of AI-driven spectrum sensing and management.

</details>


### [9] [Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks](https://arxiv.org/abs/2508.19566)
*Chen Shang,Jiadong Yu,Dinh Thai Hoang*

Main category: eess.SP

TL;DR: 基于深度强化学习和脉冲神经网络的能源效率植入式感知与通信聚然成型方案，为V2X网络提供高效能源消耗和稳健性能的连接服务


<details>
  <summary>Details</summary>
Motivation: 解决V2X网络中动态不确定环境下的聚然成型问题，避免频繁的导频传输和源流消耗的通道状态信息获取

Method: 将V2X环境模型化为马尔可夫决策过程，使用深度强化学习算法关聚然成型和功率分配优化，并嵌入脉冲神经网络提升能源效率

Result: 模拟结果证实该方案实现了显著的能源节省和优异的通信性能，同时保持了感知准确性

Conclusion: 该方案具有支持未来V2X系统绿色可持续连接的潜力，为高动态场景下的能源效率和性能优化提供了有效解决方案

Abstract: This work proposes an energy-efficient, learning-based beamforming scheme for
integrated sensing and communication (ISAC)-enabled V2X networks. Specifically,
we first model the dynamic and uncertain nature of V2X environments as a Markov
Decision Process. This formulation allows the roadside unit to generate
beamforming decisions based solely on current sensing information, thereby
eliminating the need for frequent pilot transmissions and extensive channel
state information acquisition. We then develop a deep reinforcement learning
(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring
both communication throughput and sensing accuracy in highly dynamic scenario.
To address the high energy demands of conventional learning-based schemes, we
embed spiking neural networks (SNNs) into the DRL framework. Leveraging their
event-driven and sparsely activated architecture, SNNs significantly enhance
energy efficiency while maintaining robust performance. Simulation results
confirm that the proposed method achieves substantial energy savings and
superior communication performance, demonstrating its potential to support
green and sustainable connectivity in future V2X systems.

</details>


### [10] [Code-Weight Sphere Decoding](https://arxiv.org/abs/2508.19631)
*Yubeen Jo,Geon Choi,Yongjune Kim,Namyoon Lee*

Main category: eess.SP

TL;DR: 提出了一种新颖的两阶段近最大似然解码框架，适用于任何线性分组码，通过初始解码和码重球面解码相结合，在有限码长下实现高可靠性低延迟通信。


<details>
  <summary>Details</summary>
Motivation: 超可靠低延迟通信(URLLC)在有限码长下需要高性能纠错码和解码器，传统方法在复杂度和性能之间难以平衡。

Method: 两阶段解码框架：第一阶段使用低复杂度初始解码器，若CRC校验失败则触发第二阶段码重球面解码(WSD)，通过预计算低权重码字构建局部候选集进行迭代优化。

Result: 大量仿真表明该解码器在解码可靠性和复杂度之间提供了优异平衡，特别适用于低码率码，在高信噪比下自适应最小化计算开销。

Conclusion: 该两阶段解码器是下一代URLLC系统的有前景解决方案，实现了近最大似然性能。

Abstract: Ultra-reliable low-latency communications (URLLC) demand high-performance
error-correcting codes and decoders in the finite blocklength regime. This
letter introduces a novel two-stage near-maximum likelihood (near-ML) decoding
framework applicable to any linear block code. Our approach first employs a
low-complexity initial decoder. If this initial stage fails a cyclic redundancy
check, it triggers a second stage: the proposed code-weight sphere decoding
(WSD). WSD iteratively refines the codeword estimate by exploring a localized
sphere of candidates constructed from pre-computed low-weight codewords. This
strategy adaptively minimizes computational overhead at high signal-to-noise
ratios while achieving near-ML performance, especially for low-rate codes.
Extensive simulations demonstrate that our two-stage decoder provides an
excellent trade-off between decoding reliability and complexity, establishing
it as a promising solution for next-generation URLLC systems.

</details>


### [11] [Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge](https://arxiv.org/abs/2508.19637)
*Maha Shatta,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Georgios Panagopoulos,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: 提出了一种用于柔性可穿戴系统的混合信号特征-分类器协同设计框架，首次在柔性电子中设计模拟特征提取器，显著降低特征提取成本，并通过硬件感知的NAS特征选择策略实现高效应用特定设计。


<details>
  <summary>Details</summary>
Motivation: 柔性电子在可穿戴医疗设备中具有优势，但其有限的集成密度和大特征尺寸带来了严格的面积和功耗约束。现有解决方案往往忽视系统级优化，过度关注分类器而忽略了特征提取和ADC的硬件成本。

Method: 1) 设计首个柔性电子中的模拟特征提取器；2) 提出硬件感知的NAS启发式特征选择策略；3) 建立混合信号特征-分类器协同设计框架。

Result: 在医疗基准测试中，该方法能够提供高精度、超面积效率的柔性系统，非常适合一次性低功耗可穿戴监测应用。

Conclusion: 该工作通过系统级的混合信号协同设计方法，解决了柔性电子在机器学习医疗系统中的关键挑战，为实现低成本、高效率的可穿戴医疗监测提供了有效解决方案。

Abstract: Flexible Electronics (FE) offer a promising alternative to rigid
silicon-based hardware for wearable healthcare devices, enabling lightweight,
conformable, and low-cost systems. However, their limited integration density
and large feature sizes impose strict area and power constraints, making
ML-based healthcare systems-integrating analog frontend, feature extraction and
classifier-particularly challenging. Existing FE solutions often neglect
potential system-wide solutions and focus on the classifier, overlooking the
substantial hardware cost of feature extraction and Analog-to-Digital
Converters (ADCs)-both major contributors to area and power consumption. In
this work, we present a holistic mixed-signal feature-to-classifier co-design
framework for flexible smart wearable systems. To the best of our knowledge, we
design the first analog feature extractors in FE, significantly reducing
feature extraction cost. We further propose an hardware-aware NAS-inspired
feature selection strategy within ML training, enabling efficient,
application-specific designs. Our evaluation on healthcare benchmarks shows our
approach delivers highly accurate, ultra-area-efficient flexible systems-ideal
for disposable, low-power wearable monitoring.

</details>


### [12] [Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites](https://arxiv.org/abs/2508.19657)
*Jorge L. González-Rios,Liz Martínez Marrero,Juan Duncan,Luis M. Garcés-Socarrás,Raudel Cuiman Marquez,Juan A. Vásquez Peralvo,Jevgenij Krivochiza,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: 这篇论文设计了一个基于软件定义无线电(SDR)的实验床，用于在中地轨(MEO)卫星场景中实现高效的预编码技术，解决轨道动力学带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 中地轨(MEO)通信卫星将提供几乎全球的宽带互联网连接，但轨道动力学给多用户MISO预编码技术带来了新挑战。

Method: 设计基于SDR平台的实验床，包含精确轨道模型和自定义直接辐射大线阵列的辐射图案，并提出同步循环来减少多普勒移频和有效质量噪声的影响。

Result: 初步实验结果验证了所提方案的可行性和有效性。

Conclusion: 该SDR基实验床成功地解决了MEO卫星轨道动力学带来的挑战，为实现高效预编码技术提供了可行解决方案。

Abstract: The use of communication satellites in medium Earth orbit (MEO) is foreseen
to provide quasi-global broadband Internet connectivity in the coming
networking ecosystems. Multi-user multiple-input single-output (MU-MISO)
digital signal processing techniques, such as precoding, emerge as appealing
technological enablers in the forward link of multi-beam satellite systems
operating in full frequency reuse (FFR). However, the orbit dynamics of MEO
satellites pose additional challenges that must be carefully evaluated and
addressed. This work presents the design of an in-lab testbed based on
software-defined radio (SDR) platforms and the corresponding adaptations
required for efficient precoding in a MEO scenario. The setup incorporates a
precise orbit model and the radiation pattern of a custom-designed direct
radiating array (DRA). We analyze the main impairments affecting precoding
performance, including Doppler shifts and payload phase noise, and propose a
synchronization loop to mitigate these effects. Preliminary experimental
results validate the feasibility and effectiveness of the proposed solution.

</details>


### [13] [Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation](https://arxiv.org/abs/2508.19660)
*Vojtech Mrazek,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Zdenek Vasicek,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: 该论文提出了一个自动化框架来设计印刷三元神经网络，通过多目标优化和整体近似方法，在保持分类精度的同时显著提高了面积和功率效率，实现了印刷电池供电的操作。


<details>
  <summary>Details</summary>
Motivation: 印刷电子在柔性、可拉伸性、超低制造成本等方面具有优势，但实现复杂电路仍然具有挑战性。现有印刷神经网络在分类精度和面积效率之间存在差距，特别是模数接口成为主要瓶颈。

Method: 提出了一个自动化框架，设计任意输入精度的印刷三元神经网络，采用多目标优化和整体近似方法，从模数接口到数字分类器进行全系统协同优化设计。

Result: 所设计的电路在面积上比现有近似印刷神经网络平均提升17倍，在功率上提升59倍，首次实现了印刷电池供电操作，精度损失低于5%，同时考虑了模数接口成本。

Conclusion: 该工作成功解决了印刷神经网络中分类精度与面积效率之间的权衡问题，为印刷电子在低功耗边缘计算应用中的实际部署提供了可行的解决方案。

Abstract: Printed electronics offer a promising alternative for applications beyond
silicon-based systems, requiring properties like flexibility, stretchability,
conformality, and ultra-low fabrication costs. Despite the large feature sizes
in printed electronics, printed neural networks have attracted attention for
meeting target application requirements, though realizing complex circuits
remains challenging. This work bridges the gap between classification accuracy
and area efficiency in printed neural networks, covering the entire
processing-near-sensor system design and co-optimization from the
analog-to-digital interface-a major area and power bottleneck-to the digital
classifier. We propose an automated framework for designing printed Ternary
Neural Networks with arbitrary input precision, utilizing multi-objective
optimization and holistic approximation. Our circuits outperform existing
approximate printed neural networks by 17x in area and 59x in power on average,
being the first to enable printed-battery-powered operation with under 5%
accuracy loss while accounting for analog-to-digital interfacing costs.

</details>


### [14] [MC for Gastroretentive Drug Delivery](https://arxiv.org/abs/2508.19739)
*Sebastian Lotter,Marco Seiter,Maryam Pirmoradi,Lukas Brand,Dagmar Fischer,Robert Schober*

Main category: eess.SP

TL;DR: 本文提出了一种基于物理的细菌纳米纤维素（BNC）药物释放新模型，该模型考虑了BNC几何形状和聚合物涂层的影响，并通过实验数据验证了准确性。


<details>
  <summary>Details</summary>
Motivation: 虽然细菌纳米纤维素（BNC）作为药物递送载体受到广泛关注，但现有研究多为可行性研究，缺乏建模和设计方面的深入探索，需要填补这一研究空白。

Method: 开发了一个物理基础模型，该模型反映了BNC的几何形状，并纳入了聚合物涂层对药物释放的影响。模型考虑了由聚合物涂层的BNC絮片构成的药物递送系统。

Result: 提出的模型能够准确描述药物从BNC系统中的释放行为，并通过湿实验室实验数据验证了模型的准确性。

Conclusion: 该物理基础模型可用于未来设计基于BNC的药物递送系统，为可控药物释放提供了有效的建模工具。

Abstract: Recently, bacterial nanocellulose (BNC), a biological material produced by
non-pathogenic bacteria that possesses excellent material properties for
various medical applications, has received increased interest as a carrier
system for drug delivery. However, the vast majority of existing studies on
drug release from BNC are feasibility studies with modeling and design aspects
remaining largely unexplored. To narrow this research gap, this paper proposes
a novel model for the drug release from BNC. Specifically, the drug delivery
system considered in this paper consists of a BNC fleece coated with a polymer.
The polymer coating is used as an additional diffusion barrier, enabling the
controlled release of an active pharmaceutical ingredient. The proposed
physics-based model reflects the geometry of the BNC and incorporates the
impact of the polymer coating on the drug release. Hence, it can be useful for
designing BNC-based drug delivery systems in the future. The accuracy of the
model is validated with experimental data obtained in wet lab experiments.

</details>


### [15] [On Minimization/Maximization of the Generalized Multi-Order Complex Quadratic Form With Constant-Modulus Constraints](https://arxiv.org/abs/2508.19822)
*Chunxuan Shi,Yongzhe Li,Ran Tao*

Main category: eess.SP

TL;DR: 本文提出了一种解决恒定模多阶复二次规划(CMCQP)问题的高效算法，通过相位重构和最优步长快速确定方法，实现了快速收敛和高精度求解。


<details>
  <summary>Details</summary>
Motivation: CMCQP问题在信号处理中广泛存在，但通常是非凸且难以求解的。现有方法收敛速度慢，需要开发更高效的求解算法。

Method: 将CMCQP重构为仅关于相位变量的无约束优化问题，设计最速下降/上升法，通过三阶泰勒展开将步长搜索转化为多项式形式，获得闭式解。

Result: 算法收敛速度快，步长确定精度高，通过综合仿真验证了相比现有方法的优越性能。

Conclusion: 提出的方法为CMCQP问题提供了高效求解方案，特别适用于信号处理中的各种实际应用场景。

Abstract: In this paper, we study the generalized problem that minimizes or maximizes a
multi-order complex quadratic form with constant-modulus constraints on all
elements of its optimization variable. Such a mathematical problem is commonly
encountered in various applications of signal processing. We term it as the
constant-modulus multi-order complex quadratic programming (CMCQP) in this
paper. In general, the CMCQP is non-convex and difficult to solve. Its
objective function typically relates to metrics such as signal-to-noise ratio,
Cram\'er-Rao bound, integrated sidelobe level, etc., and constraints normally
correspond to requirements on similarity to desired aspects,
peak-to-average-power ratio, or constant-modulus property in practical
scenarios. In order to find efficient solutions to the CMCQP, we first
reformulate it into an unconstrained optimization problem with respect to phase
values of the studied variable only. Then, we devise a steepest descent/ascent
method with fast determinations on its optimal step sizes. Specifically, we
convert the step-size searching problem into a polynomial form that leads to
closed-form solutions of high accuracy, wherein the third-order Taylor
expansion of the search function is conducted. Our major contributions also lie
in investigating the effect of the order and specific form of matrices embedded
in the CMCQP, for which two representative cases are identified. Examples of
related applications associated with the two cases are also provided for
completeness. The proposed methods are summarized into algorithms, whose
convergence speeds are verified to be fast by comprehensive simulations and
comparisons to existing methods. The accuracy of our proposed fast step-size
determination is also evaluated.

</details>


### [16] [Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission](https://arxiv.org/abs/2508.19910)
*Sergio Hernandez,Christophe Peucheret,Francesco Da Ros,Darko Zibar*

Main category: eess.SP

TL;DR: 基于实验数据训练的数据驱动代模型，通过端到端优化直接调制激光器系统，在低调制功耗、少滤波器水滴和小信号带宽下实现更优的性能表现。


<details>
  <summary>Details</summary>
Motivation: 直接调制激光器在短距离通信系统中很有吸引力，但其复杂的非线性动力学特性使得建模和优化面临挑战。

Method: 使用实验数据训练的数据驱动代模型，进行端到端优化，包括脏正形、均衡器滤波、偏置电流和调制弄频功率等参数。

Result: 在各种符号速率和传输距离下，端到端优化方案都显示出更优的性能，同时需要更低的调制弄频功率、更少的滤波器水滴和更小的信号带宽。

Conclusion: 通过数据驱动的端到端优化方法，可以有效提升直接调制激光器系统的性能，为短距离通信系统提供更高效的解决方案。

Abstract: Directly modulated lasers (DMLs) are an attractive technology for short-reach
intensity modulation and direct detection communication systems. However, their
complex nonlinear dynamics make the modeling and optimization of DML-based
systems challenging. In this paper, we study the end-to-end optimization of
DML-based systems based on a data-driven surrogate model trained on
experimental data. The end-to-end optimization includes the pulse shaping and
equalizer filters, the bias current and the modulation radio-frequency (RF)
power applied to the laser. The performance of the end-to-end optimization
scheme is tested on the experimental setup and compared to 4 different
benchmark schemes based on linear and nonlinear receiver-side equalization. The
results show that the proposed end-to-end scheme is able to deliver better
performance throughout the studied symbol rates and transmission distances
while employing lower modulation RF power, fewer filter taps and utilizing a
smaller signal bandwidth.

</details>


### [17] [Cell-Free Massive MIMO-Based Physical-Layer Authentication](https://arxiv.org/abs/2508.19931)
*Isabella W. G. da Silva,Zahra Mobini,Hien Quoc Ngo,Michail Matthaiou*

Main category: eess.SP

TL;DR: 提出基于无蜂窝大规模MIMO的物理层认证框架，可同时认证多个分布式用户，有效抵御主动攻击者的冒充攻击


<details>
  <summary>Details</summary>
Motivation: 传统认证方法在分布式多用户环境中面临安全挑战，需要设计能够同时认证多个用户且能抵抗主动攻击的物理层认证方案

Method: 采用标签式物理层认证系统，通过上行训练阶段估计信道，生成并安全共享唯一密钥，建立假设检验问题并推导检测概率闭式表达式

Result: 数值结果表明该方法具有高检测概率，且随着用户数量增加仍能保持良好性能

Conclusion: 所提出的无蜂窝大规模MIMO物理层认证框架能够有效实现多用户同时认证，具备抵抗主动攻击的能力，系统扩展性良好

Abstract: In this paper, we exploit the cell-free massive multiple-input
multiple-output (CF-mMIMO) architecture to design a physical-layer
authentication (PLA) framework that can simultaneously authenticate multiple
distributed users across the coverage area. Our proposed scheme remains
effective even in the presence of active adversaries attempting impersonation
attacks to disrupt the authentication process. Specifically, we introduce a
tag-based PLA CFmMIMO system, wherein the access points (APs) first estimate
their channels with the legitimate users during an uplink training phase.
Subsequently, a unique secret key is generated and securely shared between each
user and the APs. We then formulate a hypothesis testing problem and derive a
closed-form expression for the probability of detection for each user in the
network. Numerical results validate the effectiveness of the proposed approach,
demonstrating that it maintains a high detection probability even as the number
of users in the system increases.

</details>


### [18] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: 提出了一种基于深度学习的道岔机故障检测方法，仅需电力信号输入即可实现高精度故障分类，准确率超过99.99%，具有技术无关性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 道岔机是铁路关键安全设备，故障会导致运营中断。现有方法需要多输入和定制特征，限制了可扩展性。主要故障原因（障碍物、摩擦、电源问题、错位）会影响能耗模式，为基于电力信号的故障检测提供了机会。

Method: 使用深度学习模型分析道岔机运动期间的电力信号模式，仅需单一输入。采用保形预测为维护人员提供系统输出的确定性指示，增加置信层。

Result: 实现了>99.99%的精确度，<0.01%的误报率和可忽略的漏报率。方法在真实世界和测试台环境中的多种机电道岔机类型上验证了可扩展性。

Conclusion: 该方法技术无关、可扩展，符合ISO-17359标准，能够通过单一电力信号输入实现高精度道岔机故障检测，为预防性维护提供了有效解决方案。

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 提出了一种基于正则化最小二乘的物理信息回归(PIR)方法，用于参数线性非线性动态模型的参数估计，在计算速度和精度上优于物理信息神经网络(PINN)。


<details>
  <summary>Details</summary>
Motivation: 为了解决非线性动态模型参数估计的效率问题，特别是对于参数线性模型，需要一种能够快速准确估计参数的方法来连接理论和数据。

Method: 利用参数线性模型的特点，采用正则化普通最小二乘法从时间序列数据中估计参数，称为物理信息回归(PIR)。

Result: PIR方法在合成数据和真实COVID-19数据上都表现出色，特别是在复杂隔室模型中明显优于PINN方法，计算速度更快。

Conclusion: PIR方法为参数线性非线性动态模型提供了可靠且快速的参数估计解决方案，支持实时应用，在计算效率和准确性方面优于PINN。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [20] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 扩展ZipNN无损压缩方法至低精度浮点格式(FP8/FP4)，通过分离压缩指数和尾数组件，实现高达83%的压缩比。同时发现LLM中的K/V缓存张量也具有可压缩性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型体积增长和部署普及，减少神经网络权重的存储和传输成本变得越来越重要。虽然ZipNN等无损压缩方法在FP32/BF16高精度格式中表现良好，但需要扩展到正在优化推理效率的低精度格式(FP8/FP4)。

Method: 设计了一种压缩方法，将指数和尾数组件分离并独立地使用來自余编码的方法进行压缩。对于低精度浮点格式(FP8和FP4)进行了扩展处理。

Result: 评估显示压缩比最高达到BF16的62%和FP8的83%。同时发现大语言模型(LLM)中使用的关键值(K/V)缓存张量也呈现出可压缩的模式，能够在部署过程中节省内存。

Conclusion: 成功将ZipNN无损压缩技术扩展到低精度浮点格式，实现了显著的压缩效果。这种方法不仅适用于模型权重，还可以应用于LLM中的K/V缓存压缩，为深度学习模型的高效部署提供了重要的存储和内存优化手段。

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [21] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: POT是一种仅需提示的黑盒攻击框架，通过LLM迭代优化生成隐蔽且语义自然的对抗提示，使模型产生过度冗长的推理链，消耗计算资源而不提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有过度思考攻击需要外部知识源进行数据投毒、依赖可检索的中毒内容和使用明显模板，限制了实际应用。需要开发不依赖外部数据的更实用攻击方法。

Method: 使用LLM进行迭代优化，生成语义自然的对抗提示，诱导模型产生不必要的冗长推理过程，无需外部数据访问和模型检索。

Result: 在多种模型架构和数据集上的实验表明，POT相比其他方法具有优越性能，能够有效诱导计算效率低下的推理过程。

Conclusion: POT框架成功解决了现有过度思考攻击的局限性，提供了一种更实用和有效的黑盒攻击方法，揭示了CoT推理过程中的新攻击面。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [22] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: 提出了一种在真实分布式物联网环境中训练深度强化学习模型的新框架，通过ACK反馈信息进行训练，提高了信道选择的帧成功率。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少探索在真实分布式物联网系统中使用真实数据训练DRL模型，需要弥合这一研究空白。

Method: 使用基于DRL的方法让物联网设备选择通信信道，并通过实际数据传输的ACK反馈信息来训练DRL模型。

Result: 实现和性能评估表明，所提框架在帧成功率方面既可行又有效。

Conclusion: 该框架成功解决了在真实分布式物联网环境中训练DRL模型的挑战，证明了其实际应用的可行性。

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [23] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: 提出基于逆动态博弈的算法，通过多智能体局部广义纳什均衡交互数据学习参数化约束，使用混合整数线性规划编码KKT条件来恢复与纳什平稳性一致的约束。


<details>
  <summary>Details</summary>
Motivation: 从智能体交互演示中学习约束条件对于理解多智能体系统的行为规则和安全边界至关重要，但现有方法难以从纳什均衡交互中有效恢复约束。

Method: 引入混合整数线性规划(MILP)编码交互智能体的KKT条件，通过逆动态博弈方法从纳什均衡交互演示中恢复参数化约束。

Result: 理论保证学习到真实安全集和非安全集的内近似，能够处理凸和非凸约束，在仿真和硬件实验中成功推断约束并设计满足底层约束的交互运动规划。

Conclusion: 该方法能够从非线性动态智能体的纳什均衡交互演示中有效学习各类约束，为多智能体系统的约束学习和安全运动规划提供了有效解决方案。

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [24] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame是一个插件模块，通过关联记忆缓冲区整合少量专家数据，显著提升离线强化学习在低质量数据集上的性能


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常面临次优数据问题，难以获得大规模专家数据集。核心挑战是如何有效利用稀缺专家演示和丰富但低质量数据

Method: 引入Re:Frame模块，包含关联记忆缓冲区(AMB)存储专家轨迹。策略通过内容关联检索专家数据并整合到决策中，无需环境交互或修改主干架构

Result: 在D4RL MuJoCo任务中，仅使用60条专家轨迹(数据集的0.1%)，Re:Frame在4个设置中的3个上显著优于Decision Transformer基线，性能提升高达+10.7标准化分数

Conclusion: Re:Frame提供了一种简单且数据高效的方法来注入稀缺专家知识，大幅改善从低质量数据集进行的离线强化学习

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [25] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 该论文提出了NCMemo框架，首次量化半监督节点分类中的标签记忆现象，发现图同配性与记忆化呈负相关关系，并提出了图重连方法来减轻记忆化并保护隐私。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络已被证明会记忆训练数据，但图神经网络(GNNs)的记忆化分析仍未被充分探索。作者旨在量化GNNs在半监督节点分类中的标签记忆现象，并研究其与图结构特性的关系。

Method: 提出了NCMemo框架来分析节点分类中的记忆化现象。通过分析图同配性(节点连接相似性)与记忆化的关系，研究GNN训练动态，并探索图重连作为减轻记忆化的方法。

Result: 发现较低的同配性显著增加记忆化，GNNs在低同配性图中依赖记忆化来学习。特征空间邻域中标签不一致性高的节点更容易被记忆。图重连方法能有效减少记忆化而不影响模型性能。

Conclusion: 该工作不仅增进了对GNN学习机制的理解，还支持了更隐私保护的GNN部署。图同配性与记忆化的关系为设计更鲁棒和隐私友好的GNN提供了重要见解。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [26] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 通过奇异值分解(SVD)将多个源模型分解为基础组件，精准选择和聚合最重要的知识组件，通过美值微调实现高效多源迁移学习


<details>
  <summary>Details</summary>
Motivation: 现有多源迁移学习方法缺乏细粒度知识提取能力和高效聚合效率，无法激活利用网上众多模型的知识潜力

Method: 使用SVD将每个源模型分解为秩一组件，选择最显著的组件进行聚合，通过微调合并矩阵的主奇异值来适应目标任务

Result: 方法实现了高效迁移学习，对输入层和参数空间的干扰具有稳健性，计算扩展性好，适用于大量源模型或高参数模型

Conclusion: 该框架充分利用多源模型知识，通过精细的SVD分解和组件选择机制，提高了迁移学习的效率和效果，为多源知识聚合提供了有效解决方案

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [27] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: 这篇论文介绍了图数据模型在化学科学中的应用，包括分子、蛋白质和化学过程的表示，以及图神经网络等学习算法如何在这些图上进行操作。


<details>
  <summary>Details</summary>
Motivation: 图是化学科学中的核心语言，能够自然描述分子、蛋白质、反应和工业过程，捕捉支撑材料、生物学和医学的相互作用和结构。

Method: 论文概述了图设计的基础、关键预测任务、化学科学中的代表性示例，以及机器学习在图建模中的作用，特别是图神经网络的应用。

Result: 通过介绍图作为化学中的数学对象，展示了学习算法如何操作这些图，为读者应用图方法进行下一代化学发现做好准备。

Conclusion: 这些概念共同为读者提供了将图方法应用于下一代化学发现的基础，推动了化学科学中图数据建模的发展。

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [28] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 提出了一种基于空间填充采样的数据增强策略，用于从计算机模型中生成神经PDE训练数据，通过局部"模板"状态采样来提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统神经PDE训练需要长时间积分PDE求解器获得轨迹数据，存在大量时空冗余，且可能遗漏某些罕见但重要的状态。

Method: 采用空间填充采样方法对局部模板状态进行采样，减少时空冗余，并过采样可能很少访问但有助于泛化的状态。可以从相当于10个时间步长的数值模拟中生成合成训练数据。

Result: 实验证明该方法能够学习到准确的神经PDE模板算子，如果能够访问单个完整轨迹模拟，准确度会进一步提高。在多个PDE系统中都表现出比传统采样方法更好的性能。

Conclusion: 提出的数据增强策略能够显著提高神经PDE训练的样本效率，生成更好的神经模板算子，相比传统轨迹采样方法有明显性能提升。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [29] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: 这篇论文提出了一种轻量级深度学习模型，使用RR间期和TCN-Mamba混合结构，能够提前2小时预测房颤，准确率高且计算效率优秀。


<details>
  <summary>Details</summary>
Motivation: 房颤是最常见的心律失常，特别是发作性房颤因其突然发作和短暂持续而容易被漏识，但未被发现的发作性房颤可进展为持续性房颤，增加死亡风险。早期预测房颤可以通过预防性治疗减缓疾病进展。

Method: 提出一种轻量级深度学习模型，仅使用RR间期，结合时序卷积网络（TCN）进行位置编码和Mamba选择性状态空间模型，通过高效并行序列建模实现房颤的早期预测。

Result: 在主体测试中，模型达到敏感度0.908、特异性0.933、F1得分0.930、AUROC 0.972和AUPRC 0.932。模型只有73.5千参数和38.3 MFLOPs，计算效率高，能够仅使用30分钟输入数据预测未来2小时的房颤。

Conclusion: 该模型在准确性和模型紧凑性方面都超过传统的CNN-RNN方法，为房颤的早期预测提供了充足的预警时间，以便进行预防性干预治疗。

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [30] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: 本文证明了现代神经网络架构（如预层归一化和线性注意力模块）几乎总是满射的，这意味着任何输出（包括有害内容）都可以被生成，揭示了模型安全性和越狱漏洞的固有脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络是否具有满射性，即任何指定输出是否都能由某个输入生成，这对于理解生成模型的安全性和越狱漏洞至关重要。

Method: 通过数学证明分析现代神经网络架构的基本构建块，包括预层归一化网络和线性注意力模块，证明它们几乎总是满射的。

Result: 证明了GPT风格变换器和确定性ODE求解器的扩散模型等广泛使用的生成框架对任意输出都存在逆映射。

Conclusion: 研究为现代神经网络架构的固有脆弱性提供了形式化分析，揭示了它们对一类广泛对抗攻击的不可避免的脆弱性。

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [31] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于信息几何和正则化核希尔伯空间的多模态大语言模型幻觉量化框架，通过温度逆逆温法和谱分解来量化幻觉的演化过程。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉评估技术主要基于经验性方法，缺乏理论基础和可行保证，在医疗、法律等高风险领域存在重大隐惑。

Method: 通过将MLLM输出表示为多模态图拉普拉斯矩阵的谱嵌入，并使用正则化核希尔伯空间中的特征模式分解来量化真相与不一致性之间的语义扭曲。

Result: 该框架能够通过时间依赖的温度温冬过程来提供模态感知、理论可解释的指标，捕捉幻觉在时间和输入提示中的演化。

Conclusion: 这个工作为量化和界定幻觉提供了理论基础，将幻觉从一种定性风险转化为可处理、可分析的现象。

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [32] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 本文研究了成员推理攻击的样本复杂度，发现在高斯均值估计场景中，攻击者需要Ω(n + n²ρ²)个参考样本才能与完全知情攻击者竞争，这比训练算法使用的样本数量多得多。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击通常假设攻击者拥有来自相同分布的参考样本，但实际中攻击者能获取的样本数量有限。本文旨在量化攻击者成功进行成员推理所需的最小参考样本数量。

Method: 在高斯均值估计的基本设置中，学习算法从d维高斯分布N(μ,Σ)中获取n个样本，估计均值μ̂使得期望误差E[‖μ̂-μ‖²_Σ]≤ρ²d。研究攻击者需要多少参考样本才能有效进行成员推理。

Result: 研究显示，对于任何能与完全知情攻击者竞争的成员推理攻击，攻击者需要Ω(n + n²ρ²)个参考样本。这是首次证明攻击者有时需要比训练算法使用的样本数量多得多的样本。

Conclusion: 当前实践中使用的攻击（通常使用O(n)个样本）可能低估了成员推理的可能性。当分布信息易于获取时，可能存在更好的攻击方法，这对实际应用具有重要意义。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [33] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 基于LLaMA 3.2的视觉语言模型在HEP中微子相互作用分类中表现优于传统CNN，支持多模态推理


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多模态推理方面的潜力，特别是在高能物理实验中的中微子相互作用分类应用

Method: 使用基于LLaMA 3.2的微调视觉语言模型，与NOvA和DUNE实验中使用的CNN基线进行性能对比评估

Result: VLM不仅达到或超过CNN性能，还支持更丰富的推理和辅助文本/语义上下文的更好集成

Conclusion: 视觉语言模型为HEP事件分类提供了有前景的通用主干，为实验性中微子物理中的多模态方法铺平了道路

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [34] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs在表格数据预测中存在严重脆弱性，任务无关的数据表示变化（如变量名更改）会导致预测结果大幅波动，即使是最先进的表格基础模型也无法完全避免这一问题


<details>
  <summary>Details</summary>
Motivation: 随着LLMs被广泛应用于各种数据拟合任务，需要评估其作为数据拟合工具的稳健性，特别是对任务无关数据变化的敏感性

Method: 通过改变变量名等任务无关的数据表示方式，测试LLMs在上下文学习和监督微调下的预测稳定性，并分析注意力模式来解释敏感性原因

Result: 简单的变量名更改可使预测误差波动高达82%，LLMs对任务无关变化表现出显著敏感性，注意力分析显示存在非均匀注意力模式

Conclusion: 尽管LLMs具有强大的预测能力，但目前缺乏基本的稳健性，不能作为可靠的数据拟合工具使用

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [35] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 本文研究了两种混合量子-经典模型(QMLP和QCNN)在恶意软件分类中的应用，在多个数据集上得到了高准确率，具体表现随任务复杂度而异。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习(QML)为恶意软件检测带来了新的机会，但该领域仍未充分探索。本文开创性地将QML应用于恶意软件分类任务。

Method: 采用两种混合量子-经典模型：量子多层感知机(QMLP)和量子卷积神经网络(QCNN)。使用angle embedding技术将恶意软件特征编码为量子状态，并在5个常用恶意软件数据集上进行二元和多类分类评估。

Result: 二元分类准确率高达95-96%(API-Graph)、91-92%(AZ-Domain)和77%(EMBER-Domain)。多类分类准确率从41.7%到93.6%不等，QMLP在复杂多类任务中表现更优，而QCNN训练效率更高但准确率略低。

Conclusion: 量子机器学习在恶意软件分类领域具有强大潜力，混合量子-经典模型能够在不同复杂度的任务中实现高准确率。QMLP适合复杂分类任务，QCNN则在训练效率方面有优势。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [36] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: EUREKA框架通过选择有趣而非最准确的特征来构建分类器，利用大语言模型评估特征有趣度，生成具有新颖性和可解释性的模型


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型追求最大预测准确率，但本文探索构建"有趣分类器"的目标，即使用不寻常或意外的特征，即使准确率低于最佳模型，也能提供新的洞察和知识发现

Method: 提出EUREKA框架：1）使用大语言模型对特征的有趣度进行排序；2）仅选择有趣的特征构建可解释分类器；3）在多个基准数据集上验证方法

Result: 在Occupancy Detection数据集中偏好湿度而非CO2和光照强度，在Twin Papers数据集中发现标题含冒号的论文更可能被引用。方法能一致识别非明显但仍具预测性的特征

Conclusion: 这种模型支持新的知识发现和传播方式，特别适用于中等准确率足够但新颖性和可解释性受到重视的场景

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [37] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 提出DETNO架构，结合Transformer神经算子和扩散模型，解决交通流量预测中高频特征丢失和长期预测误差累积问题


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在交通流量预测中会产生平滑预测，无法重建高频特征（如密度梯度），导致多步预测时误差快速累积，影响实时交通管理

Method: 采用Transformer神经算子提供模型表达能力和超分辨率，结合基于扩散的细化组件通过渐进去噪迭代重建高频交通细节

Result: 在混沌交通数据集上评估显示，相比传统和基于Transformer的神经算子，该方法在长期预测中表现更优，能保持高频成分并提高预测稳定性

Conclusion: DETNO架构有效克服了标准神经算子的平滑限制和预测不稳定性，为长期交通预测提供了更准确的解决方案

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [38] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典架构用于SMILES字符串重构，通过量子编码与经典序列建模结合，实现了84%的量子保真度和60%的经典重构相似度，超越了现有量子基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管量子机器学习在分子设计等生成模型方面具有潜力，但现有方法在SMILES序列重构任务中仍面临保真度下降的问题，量子与序列任务的结合研究不足。

Method: 采用混合量子-经典架构，将量子编码技术与经典序列建模相结合，旨在同时提升量子保真度和经典相似度。

Result: 实现了约84%的量子保真度和60%的经典重构相似度，性能优于现有的量子基线方法。

Conclusion: 该工作为未来量子机器学习应用奠定了有前景的基础，在量子表示和经典序列模型之间取得了良好平衡，推动了量子感知序列模型在分子和药物发现领域的更广泛研究。

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [39] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: KAR-HNN使用Kolmogorov-Arnold表示替代MLP，通过单变量变换改进哈密顿神经网络，减少能量漂移并提高长期预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLP的哈密顿神经网络在探索复杂能量景观时对超参数过于敏感，需要更好的方法来捕获高频和多尺度动力学。

Method: 使用Kolmogorov-Arnold表示的单变量变换替代MLP，利用局部函数逼近来更好地捕获动力学特性，同时保持哈密顿系统的辛形式。

Result: 在弹簧质量、单摆、二体和三体问题等基准测试中表现出色，减少了能量漂移，提高了长期预测稳定性。

Conclusion: KAR-HNN有望在高维度和参数稀少的情况下，为现实物理过程提供准确稳定的建模。

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [40] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct在聊天格式中错误判断"9.11"比"9.8"大，但在简单格式中正确。研究发现transformer存在奇偶注意力头专门化机制，偶数头负责数值比较，需要至少8个偶数头才能修复此bug。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在格式依赖的推理失败机制，特别是数值比较任务中出现的格式敏感性错误，以揭示模型内部的计算结构。

Method: 通过系统性干预实验，分析transformer注意力头的奇偶索引专门化，使用稀疏自编码器(SAE)进行特征分析，并测试不同数量注意力头组合的修复效果。

Result: 发现需要至少8个偶数注意力头才能完美修复bug，偶数头之间存在完美冗余性。格式表示在Layer 7分离(10%特征重叠)，在Layer 10重新纠缠(80%特征重叠)，失败格式中特定特征放大1.5倍。

Conclusion: 模型表面上的全模块需求隐藏了精密的子结构，仅需25%的注意力头即可实现完美修复，这对模型可解释性和效率优化具有重要意义。

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [41] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的机器学习工作流，结合可微分多相流模拟器和CNN，大幅减少高保真模拟需求，从千万次降至数千次


<details>
  <summary>Details</summary>
Motivation: 地下储层压力控制面临地质异质性和多相流动力学的挑战，传统高保真物理模拟计算成本极高，且需要大量模拟来处理不确定性

Method: 使用完全可微分多相流模拟器（DPFEHM框架）与卷积神经网络耦合，通过迁移学习先预训练单相稳态模拟，再微调多相场景

Result: 仅需不到3000次全物理多相流模拟即可实现高精度训练，相比之前需要上千万次模拟大幅降低计算成本

Conclusion: 该方法通过物理信息机器学习和迁移学习策略，显著提高了多相流储层压力预测的实用性和计算效率

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [42] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 提出基于对比学习的无监督框架，对43种癌症类型进行聚类分析，使用基因水平和染色体水平的双重突变特征，通过对比学习获得有生物学意义的癌症聚类


<details>
  <summary>Details</summary>
Motivation: 理解泛癌突变景观对肿瘤发生机制至关重要。虽然患者级机器学习已广泛用于识别肿瘤亚型，但基于共享分子特征的队列级癌症聚类仍主要依赖传统统计方法

Method: 使用COSMIC数据库的编码突变数据，为每种癌症构建基因水平（核苷酸替换模式）和染色体水平（标准化替换频率）的双重突变特征。采用TabNet编码器和多尺度对比学习目标（NT-Xent损失）学习统一的癌症类型嵌入表示

Result: 学习到的潜在表示产生了具有生物学意义的癌症类型聚类，与已知的突变过程和组织起源相一致

Conclusion: 这是对比学习在队列级癌症聚类中的首次应用，为突变驱动的癌症亚型分析提供了一个可扩展且可解释的框架

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [43] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: 提出在生成模型中引入张量分解技术，通过生成较小的张量因子而非完整张量来显著降低模型输出和参数数量，从而减少复杂模拟数据的生成成本。


<details>
  <summary>Details</summary>
Motivation: 大型复杂模拟数据集的生成通常耗时耗资源，特别是在实验成本高昂的情况下，需要更高效的方法来生成合成数据用于下游任务。

Method: 在生成对抗网络或扩散模型等生成式机器学习模型中引入内部张量分解，针对多维数据（张量）生成较小的张量因子而非完整张量。

Result: 实验表明该方法显著降低了模型输出和整体参数数量，减少了复杂模拟数据的生成成本，同时生成的数据仍然保持实用性。

Conclusion: 张量分解技术有潜力提高生成模型的效率，特别是在生成多维数据或张量时，能够有效降低计算成本。

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [44] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [45] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas算法通过生成数据的局部低维表示并训练深度神经网络来验证流形假设，发现许多真实数据集并不符合流形假设，但在符合的情况下可以构建生成模型。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习工具只能生成全局嵌入，无法验证流形假设是否成立，也无法提供数学定义中的局部映射。需要开发能够评估数据集是否来自流形并确定其维度的工具。

Method: DeepAtlas算法首先生成数据局部邻域的低维表示，然后训练深度神经网络在这些局部嵌入和原始数据之间进行映射。使用拓扑失真来评估数据集是否来自流形并确定其维度。

Result: 在测试数据集上，DeepAtlas成功学习了流形结构。有趣的是，许多真实数据集（包括单细胞RNA测序数据）并不符合流形假设。在数据来自流形的情况下，DeepAtlas可以构建生成模型。

Conclusion: DeepAtlas能够验证流形假设并确定流形维度，为符合流形假设的数据集提供了生成模型，使得微分几何的强大工具可以应用于各种数据集。

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [46] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 提出了SAFT框架来解决表格学习中的分布偏移问题，通过连续表示生成范式实现可微分优化，包含三个鲁棒性机制，在多种真实分布偏移场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 表格学习在训练和测试数据存在分布偏移时效果会显著下降，需要解决分布偏移表格学习(DSTL)问题

Method: SAFT框架将表格学习重构为连续表示生成范式，包含三个机制：嵌入去相关和样本重加权的抗偏移表示、次优嵌入平均的平坦感知生成、训练测试分布间的归一化对齐

Result: 大量实验表明SAFT在鲁棒性、有效性和泛化能力方面持续优于现有表格学习方法

Conclusion: SAFT通过连续优化和多重鲁棒机制有效解决了表格学习中的分布偏移问题，具有很好的实际应用价值

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [47] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE是一个通过质量对齐转移嵌入的方程生成框架，用于在低数据条件下通过蒸馏方法微调基础模型进行符号方程发现，在多个基准测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型在大规模方程数据集上预训练后，在应用于小型领域特定数据集时经常出现负迁移和泛化能力差的问题，需要数据高效的微调方法。

Method: EQUATE结合符号-数值对齐和评估器引导的嵌入优化，将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，通过数据-方程适应性和简洁性进行指导。

Result: 在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面始终优于最先进的基线方法，同时保持低复杂度和快速推理。

Conclusion: EQUATE为基础模型蒸馏设置中的数据高效符号回归提供了一个实用且可推广的解决方案。

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [48] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: 提出了PoolFlip环境和Flip-PSRO方法，通过多智能体强化学习训练防御者对抗未知攻击策略，效果比基线方法提升2倍


<details>
  <summary>Details</summary>
Motivation: 现有FlipIt框架依赖少量启发式或专门学习技术，存在脆弱性和无法适应新攻击的问题，需要更强大的自适应防御方法

Method: 开发PoolFlip多智能体环境扩展FlipIt游戏，提出Flip-PSRO方法利用基于群体的训练来训练防御者智能体，并设计基于所有权的效用函数

Result: Flip-PSRO防御者对训练中未暴露的启发式攻击的泛化能力比基线方法高2倍，同时保持高水平控制

Conclusion: Flip-PSRO为网络防御提供了有效的自适应决策框架，能够对抗不断演化的对抗策略，显著提升防御效果

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [49] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: 使用Python程序作为策略表示，通过LLM优化生成游戏玩家代理，在Atari游戏中达到与深度强化学习相竞争的性能，但耗费更少训练时间和环境交互


<details>
  <summary>Details</summary>
Motivation: 探索程序化策略表示在构建高效、适应性强的游戏玩家代理方面的潜力，通过自我迭代优化提升复杂长期望的理性决策能力

Method: 将决策策略表示为Python程序，使用大语言模型进行生成式优化，通过执行踪迹和自然语言反馈实现自我改进，最小化人工帮助

Result: 在Atari游戏中达到与深度强化学习基线相竞争的性能水平，同时使用显著更少的训练时间和环境交互

Conclusion: 这项工作告诉了程序化策略表示在构建高效、适应性强的代理方面的前景，能够处理复杂的长期望理性任务

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [50] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: MobText-SISA是一个针对异构时空数据的机器遗忘框架，通过相似性感知聚类和分片训练实现高效删除，在保持准确性的同时满足GDPR隐私要求。


<details>
  <summary>Details</summary>
Motivation: 现代移动平台存储了大量GPS轨迹和文本数据，GDPR等隐私法规要求能够按需删除个人数据，但为每个删除请求重新训练深度模型成本过高。

Method: 扩展SISA训练方法，首先将行程的数值和语言特征嵌入共享潜在空间，然后使用相似性感知聚类将样本分布到分片中，每个分片增量训练，删除时仅需重新训练受影响的分片。

Result: 在10个月的真实移动日志实验中，MobText-SISA保持了基线预测准确性，在错误率和收敛速度方面均优于随机分片方法。

Conclusion: MobText-SISA为城市规模的多模态移动数据分析提供了一个实用的隐私合规基础框架，能够保证精确遗忘同时维持模型性能。

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [51] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: Bi-LoRA方法通过引入辅助LoRA模块来模拟SAM的对抗性权重扰动，在保持内存效率的同时实现更平坦的最小值，消除了SAM的双倍训练成本。


<details>
  <summary>Details</summary>
Motivation: SAM虽然能通过寻找平坦最小值来提升泛化能力，但其巨大的内存和计算开销使其不适用于大型模型。直接对LoRA参数应用SAM会限制锐度优化到受限子空间，影响效果。

Method: 提出双向低秩适应(Bi-LoRA)，引入辅助LoRA模块来建模SAM的对抗权重扰动。主LoRA模块通过标准梯度下降适应任务，辅助模块通过梯度上升捕捉损失景观的锐度。

Result: 在多种任务和架构上的广泛实验证明了Bi-LoRA在提升泛化能力方面的效率和有效性。

Conclusion: Bi-LoRA通过双模块设计成功解决了SAM在大型模型上的应用限制，在保持内存效率的同时显著提升了泛化性能。

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [52] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: 提出了一种基于因果推理的多模态反事实奖励模型，通过Counterfactual Trust Score来减少RLHF中的偏见，在假新闻检测任务中达到89.12%的准确率，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: RLHF中的奖励模型容易学习和放大数据中的潜在偏见，导致有缺陷的奖励信号和公平性下降。现有的偏见缓解方法往往采用被动约束，在因果混淆情况下会失效。

Method: 开发了反事实奖励模型，结合因果推理和多模态表示学习，提出Counterfactual Trust Score包含四个组件：反事实偏移、重构不确定性、公平规则违反检测和时间奖励偏移。

Result: 在多模态真假新闻数据集上评估，准确率达到89.12%，优于基线奖励模型，显著减少了虚假相关性和不公平的强化信号。

Conclusion: 该方法为公平感知的RLHF提供了鲁棒且可解释的解决方案，具有可调节的偏见减少阈值，提高了动态实时策略制定的可靠性。

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [53] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: 本教程介绍生成模型在合成数据生成方面的基础知识和最新进展，涵盖关键方法、实践框架、评估策略和应用，帮助解决数据挖掘中的数据稀缺、隐私和标注挑战。


<details>
  <summary>Details</summary>
Motivation: 解决数据挖掘中面临的数据稀缺、隐私保护和标注成本高等挑战，利用生成模型提供可扩展的合成数据解决方案。

Method: 介绍大型语言模型、扩散模型和生成对抗网络等生成模型的基础知识和最新方法，包括实用框架和评估策略。

Result: 为参与者提供可操作的见解，帮助他们利用生成式合成数据来增强数据挖掘研究和实践。

Conclusion: 生成模型为合成数据生成提供了革命性的解决方案，能够有效应对数据挖掘中的多种挑战，具有重要的研究和应用价值。

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [54] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: 提出SyReM方法解决运动预测中持续学习的稳定性-可塑性困境，通过约束损失增量保证稳定性，基于梯度相似性选择记忆样本提升可塑性


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在运动预测中表现优异但存在灾难性遗忘问题，现有持续学习方法过度强调记忆稳定性会损害学习可塑性

Method: 维护紧凑记忆缓冲区，使用不等式约束限制平均损失增量确保稳定性，基于损失梯度余弦相似度选择相似样本进行针对性记忆重放

Result: 在11个自然驾驶数据集上验证，相比非CL和CL基线方法，SyReM显著减轻灾难性遗忘并提升新场景预测精度

Conclusion: SyReM有效解决了持续学习中的稳定性-可塑性困境，在运动预测任务中实现了更好的性能平衡

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [55] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: Delta-Attribution是一个模型无关的框架，通过比较不同版本模型的特征归因差异来解释模型更新的变化原因，提供轻量级的更新审计方法。


<details>
  <summary>Details</summary>
Motivation: 模型更新（如超参数、核函数、深度、求解器或数据变化）会改变性能，但变化的原因往往不透明，需要一种方法来解释模型版本间的差异。

Method: 提出Delta-Attribution框架，通过计算特征归因差异Δφ(x)=φ_B(x)-φ_A(x)来解释模型变化。使用标准化空间中的快速遮挡/钳位方法，配合类别锚定边界和基线平均，在45个设置中进行评估。

Result: 归纳偏置变化产生大的行为对齐差异（如SVC多项式→RBF核：BAC≈0.998，DCE≈6.6），而表面调整（如SVC gamma参数变化）显示rank-overlap@10=1.0和DCE≈0。最明显的重新分布出现在深度梯度提升上（JSD≈0.357）。

Conclusion: Delta-Attribution提供了一种轻量级的更新审计方法，通过区分良性变化与行为上有意义或风险依赖转移的变化，补充了准确性的评估。

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [56] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: Dual-LS是一种受人类大脑互补学习系统启发的在线持续学习范式，通过双记忆重放机制解决DNN车辆运动预测中的灾难性遗忘问题，显著提升预测稳定性并大幅降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 智能城市服务依赖AI，但深度神经网络在车辆运动预测中存在灾难性遗忘问题，传统方法数据收集成本高、样本效率低，且无法平衡长短期经验，无法实现类人的持续学习。

Method: 提出Dual-LS范式，采用两种协同的记忆重放机制，加速经验检索并动态协调长短期知识表示，实现任务无关的在线持续学习。

Result: 在三个国家超过77.2万辆车辆、累计测试里程11,187公里的自然数据测试中，Dual-LS将灾难性遗忘减少74.31%，计算资源需求降低94.02%，显著提升预测稳定性。

Conclusion: Dual-LS为基于DNN的车辆运动预测提供了计算高效、类人的持续学习适应性，适合智能城市应用，且不增加数据需求。

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [57] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: 提出了RLTR框架，通过工具使用完整性奖励信号解耦训练过程，专注于规划模块的单目标优化，相比端到端方法提升了8-12%的规划性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体训练采用端到端多目标优化，存在目标分配不平衡和可验证数据稀缺的问题，难以有效提升规划能力。

Method: RLTR框架解耦训练过程，基于工具使用完整性设计奖励信号，直接评估工具调用序列质量，实现规划模块的单目标优化。

Result: 实验显示RLTR在规划性能上比端到端基线提升8-12%，增强的规划能力使整体系统最终响应质量提升5-6%。

Conclusion: RLTR通过解耦训练和工具使用奖励机制，有效解决了LLM智能体规划能力训练的挑战，显著提升了性能。

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [58] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast是首个专门为金融时间序列预测设计的基础模型，通过大规模金融数据集训练，在零样本情况下表现出色，无需领域特定微调即可捕捉多样化模式，超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测对经济稳定、政策制定和可持续投资至关重要，但面临时间非平稳性、多领域多样性和不同时间分辨率等模式变化挑战。现有深度学习方法容易过拟合且需要大量领域特定微调。

Method: 开发FinCast基础模型，在大规模金融数据集上进行训练，专门针对金融时间序列预测设计，具备零样本预测能力。

Result: FinCast展现出强大的零样本性能，能够有效捕捉多样化模式而无需领域特定微调，在综合实证和定性评估中超越了现有最先进方法。

Conclusion: FinCast作为首个金融时间序列预测基础模型，具有强大的泛化能力，为解决金融预测中的模式变化挑战提供了有效解决方案。

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [59] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: ALSA是一种新的模型精度估计框架，直接在logit空间操作，通过锚点建模策略来估计模型在未见未标记数据集上的精度，特别适用于分布偏移场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖softmax概率或数据相似性度量，前者存在信息损失，后者计算昂贵且领域特定。需要一种能保留更丰富信息、计算高效且广泛适用的精度估计方法。

Method: ALSA在logit空间初始化多个可学习锚点，每个锚点分配影响函数来捕捉logits的细微变化。基于理论和实证观察，利用logits的聚合和分布与模型预测性能的强相关性。

Result: 在视觉、语言和图基准测试上的大量实验显示，ALSA优于基于softmax和相似性的基线方法，在显著分布偏移下表现出强大的鲁棒性。

Conclusion: ALSA是一个实用的可靠模型评估工具，通过直接在logit空间操作避免了信息损失，提供了跨多种分布偏移的准确性能估计。

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [60] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: 提出了pFedBayesPT框架，通过贝叶斯视觉提示调优解决联邦学习中客户端内部数据异质性问题，实现细粒度的实例级个性化联邦学习


<details>
  <summary>Details</summary>
Motivation: 现有个性化联邦学习方法假设每个客户端数据遵循单一分布，但实际中单个客户端可能包含来自多个源或域的数据，导致显著的客户端内部异质性和次优性能

Method: 基于视觉提示调优的细粒度实例级pFL框架，从贝叶斯角度制定实例级提示生成，将提示后验建模为隐式分布以捕捉多样化视觉语义，在半隐式变分推断框架下推导变分训练目标

Result: 在基准数据集上的大量实验表明，pFedBayesPT在特征和标签异质性设置下始终优于现有pFL方法

Conclusion: pFedBayesPT有效解决了客户端内部数据异质性挑战，为个性化联邦学习提供了更细粒度的实例级解决方案

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [61] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: SCAR是一个数据质量评估框架，通过Scale、Coverage、Authenticity、Richness四个维度量化数据集的结构特性，并基于此提出Foundation Data概念和模态感知的数据补全策略。


<details>
  <summary>Details</summary>
Motivation: 现有数据优化方法主要关注数据量和训练效率，缺乏对数据质量结构特性的理论理解，特别是在样本缩放时数据特性如何影响泛化能力。

Method: 提出SCAR框架量化数据集的四个结构特性，建立Foundation Data最小子集保持泛化行为，建模单模态任务为阶跃函数，开发基于泛化偏见的SCAR引导数据补全策略。

Result: 在多种多模态数据集和模型架构上的实验验证了SCAR在预测数据效用和指导数据采集方面的有效性。

Conclusion: SCAR提供了一个稳健的数据理解基础，能够捕捉数据集缩放时保持稳定的内在结构特性，为数据优化和扩展提供了理论指导。

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [62] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 首次全面探索低功耗柔性压力分类器的设计空间，涵盖多种机器学习分类器、特征选择和神经简化算法，设计了1200多个柔性分类器，实现了比现有方法更高精度的实时压力监测


<details>
  <summary>Details</summary>
Motivation: 传统压力监测方法依赖间歇性、症状导向的干预，缺乏连续、可及且成本效益高的解决方案。现有刚性硅基可穿戴设备虽能多任务处理，但不够轻便灵活，限制了连续监测的实用性

Method: 采用柔性电子技术，设计低功耗柔性压力分类器。探索多种机器学习分类器、特征选择算法和神经简化技术，设计完全定制化的低精度算术电路，优化硬件效率

Result: 开发了超过1200个柔性分类器，实现了比现有方法更高的准确率，同时具备低成本、可贴合、低功耗和小尺寸的优势

Conclusion: 这项工作为设计实时压力分类器提供了重要见解，展示了柔性电子在实现高效、实用压力监测解决方案方面的潜力

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [63] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 该论文证明了适当正则函数可以通过有理函数和有理神经网络在C¹范数下进行逼近，并给出了关于网络宽度、深度以及有理函数次数的逼近速率。


<details>
  <summary>Details</summary>
Motivation: 研究有理函数和有理神经网络在C¹范数下的逼近能力，特别是在符号回归和物理定律学习等应用场景中具有重要意义。

Method: 使用有理函数和有理神经网络进行函数逼近，分析了EQL⁺和ParFam架构在C¹范数下的逼近性能。

Result: 获得了关于网络宽度、深度以及有理函数次数的具体逼近速率，证明了这些方法在C¹范数下的有效逼近能力。

Conclusion: 有理函数和有理神经网络能够有效逼近适当正则函数，为符号回归和物理定律学习等应用提供了理论支持。

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [64] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: 该论文研究图上游走的度量结构，引入加权度量处理序列，定义基于逐步顶点距离和加权范数的游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造，支持度量建模经典工具的应用。


<details>
  <summary>Details</summary>
Motivation: 研究图上游走的度量结构，为分析测量游走间相对距离的较弱形式（邻近度）提供基础，支持在网络上进行Lipschitz回归等应用。

Method: 引入加权度量处理序列，定义基于逐步顶点距离和加权范数的游走间距离，分析度量空间性质，提供邻近度的表示公式和显式构造。

Result: 建立了游走的度量框架，支持经典度量建模工具的使用，如从游走子空间扩展Lipschitz函数，通过表示保持基本性质扩展邻近度函数。

Conclusion: 提出的度量框架为估计邻近度和基于探索性游走开发强化学习策略提供了稳健方法，适用于网络结构上的Lipschitz回归。

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [65] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: 提出了Adam-PFN方法，通过预训练代理模型和新的学习曲线增强技术CDF-augment，改进Adam优化器的超参数调优效率


<details>
  <summary>Details</summary>
Motivation: Adam优化器广泛使用但超参数调优耗时，现有Freeze-thaw BO方法受限于通用代理模型，缺乏超参数对学习影响的先验知识

Method: 开发Adam-PFN代理模型，在TaskSet学习曲线上预训练，并提出CDF-augment学习曲线增强方法增加训练样本

Result: 在TaskSet评估任务上改进了学习曲线外推能力，加速了超参数优化，在分布外任务上表现优异

Conclusion: Adam-PFN结合预训练代理模型和数据增强技术，为Adam超参数调优提供了高效解决方案

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [66] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP是一种无需训练的图分区方法，通过负校正机制扩展图拉普拉斯算子的频率范围，仅使用随机输入和一次前向传播即可生成可区分的图嵌入，在效率和性能上均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统图拉普拉斯算子的频率范围限制在[0,2]之间，作者发现通过负校正可以获得超出此范围的低频信息，这些信息可能包含更丰富的社区结构特征。

Method: 采用谱GNN作为主干网络，结合低通滤波器和负校正机制；仅输入随机信号；通过一次前向传播生成图嵌入而不需要训练；使用BIRCH聚类算法获得最终分区结果。

Result: 在IEEE HPEC图挑战基准测试中，InfraredGP在静态和流式图分区任务上实现了16-23倍的速度提升，同时保持了有竞争力的分区质量。

Conclusion: 负校正机制能够有效提取超出传统频率范围的低频信息，使得无需训练的图分区方法在效率和效果上都达到了先进水平。

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [67] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: 提出基于3D扩散模型的生成管道，直接合成任意大小的物理真实颗粒介质，解决离散元法初始化阶段计算瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 离散元法模拟颗粒介质时，初始化阶段因大位移和动能导致计算时间过长，成为主要瓶颈

Method: 两阶段管道：首先训练扩散模型生成独立3D体素网格，然后使用基于掩码输入的3D修复模型无缝拼接网格，采用噪声调度器输出与真实数据重新注入的2D重绘技术

Result: 生成长1.2米的道碴轨道等效于3小时DEM模拟，在20秒内完成，计算时间与样本大小呈线性关系

Conclusion: 该方法能够实现物理一致、实时、可扩展的颗粒介质合成，适用于工业应用

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [68] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: PSO-Merging是一种基于粒子群优化的数据驱动模型融合方法，通过初始化粒子群并进行多轮迭代，在语言模型上表现优于现有基线方法


<details>
  <summary>Details</summary>
Motivation: 现有数据无关方法因缺乏数据指导而性能受限，数据驱动方法中梯度方法计算昂贵，无梯度方法在有限优化步数内效果不佳

Method: 基于粒子群优化(PSO)，用预训练模型、专家模型和稀疏化专家模型初始化粒子群，通过多轮迭代获得最终融合模型

Result: 在不同语言模型上的实验结果表明，PSO-Merging通常优于基线融合方法

Conclusion: PSO-Merging为模型融合提供了更高效和可扩展的解决方案

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [69] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 提出了一种新的辛卷积神经网络架构，通过结合辛神经网络、适当辛分解和张量技术来构建保持辛结构的卷积层和池化层


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在物理系统建模中无法保持辛结构，而辛结构对于哈密顿系统的时间演化至关重要，需要开发能够保持这种几何结构的神经网络架构

Method: 首先引入卷积层的数学等价形式，然后使用辛神经网络参数化CNN层以确保卷积层保持辛性，并引入辛池化层来构建完整的自编码器

Result: 在波动方程、非线性薛定谔方程和正弦-戈登方程三个示例上测试，数值结果表明辛CNN优于通过适当辛分解获得的线性辛自编码器

Conclusion: 提出的辛卷积神经网络架构能够有效保持物理系统的辛结构，在哈密顿系统建模方面表现出优越性能，为物理启发的神经网络设计提供了新思路

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [70] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: 提出了一种混合框架，将有限元方法与物理信息DeepONet结合，用于模拟多孔介质中来自尖锐高斯源的流体输运问题，实现了高精度和快速推断。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多孔介质中尖锐源引起的陡峭梯度时计算成本高昂，需要一种既能保持精度又能大幅加速计算的新方法。

Method: 使用FEM求解达西流动方程获得速度场，然后将速度场输入物理信息DeepONet来学习从源函数到溶质浓度分布的映射，并采用自适应采样策略处理陡峭梯度。

Result: 数值实验表明该方法与参考解吻合良好，相比传统求解器实现了数量级的加速，适用于实际应用场景。

Conclusion: 该混合框架成功结合了FEM的精度和DeepONet的快速推断优势，为多孔介质流体输运模拟提供了高效实用的解决方案。

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [71] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: 该论文研究了量子处理器产生的潜在分布在生成模型中的优势，证明了在某些条件下量子潜在分布能够产生经典分布无法高效生成的数据分布，并通过实验验证了量子潜在分布在GAN、扩散模型和流匹配模型中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 虽然简单潜在分布在生成模型中很常见，但更复杂的分布可以提升性能。量子处理器产生的分布已被证明能带来经验性改进，但量子优势何时出现以及这些改进是否可重现仍是开放问题。

Method: 理论证明量子潜在分布在特定条件下的优势，提供可操作的直觉判断标准，在合成量子数据集和QM9分子数据集上进行基准测试，使用模拟和真实的光子量子处理器，探索GAN、扩散模型和流匹配模型等架构。

Result: 实验结果表明，与一系列经典基线相比，量子潜在分布能够提升GAN的生成性能，并确定了与量子潜在分布兼容的扩散和流匹配模型架构。

Conclusion: 这项工作证实了近期的量子处理器可以扩展深度生成模型的能力，为量子计算在生成建模中的应用提供了理论和实验支持。

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [72] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: 提出了基于结构多样性的无参数图神经网络框架SDGNN，通过结构多样性消息传递机制同时捕获邻域结构异质性和特征语义稳定性，无需训练参数，在多个挑战性场景下优于主流GNN方法


<details>
  <summary>Details</summary>
Motivation: 传统GNN方法依赖大量可训练参数和固定聚合规则，难以适应结构异质性强的图数据，容易导致节点表示过平滑和语义退化问题

Method: 基于结构多样性理论设计统一的结构多样性消息传递机制，从结构驱动和特征驱动两个角度进行互补建模，不引入额外可训练参数

Result: 在8个公共基准数据集和跨学科PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战条件下 consistently优于主流GNN

Conclusion: 为无参数图神经网络设计提供了新的理论视角和通用方法，验证了结构多样性作为图表示学习核心信号的重要性

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [73] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: NM-Hebb是一个两阶段训练框架，结合神经启发的局部可塑性和距离感知监督，通过Hebbian正则化和可学习神经调节器提升CNN性能，在多个数据集和骨干网络上实现准确率显著提升和更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统CNN基于全局梯度优化导致的过拟合、冗余过滤器和可解释性差的问题，通过生物启发机制提升网络性能。

Method: 两阶段训练：第一阶段结合交叉熵损失、Hebbian正则化（激活空间均值与滤波器权重均值对齐）和可学习神经调节器；第二阶段使用成对度量学习损失进行微调，压缩类内距离并扩大类间间隔。

Result: 在CIFAR-10、CIFAR-100和TinyImageNet上，Top-1准确率提升2.0-10.0个百分点，NMI提升最高0.15，产生更结构化、选择性的特征和更紧密的类簇。

Conclusion: 结合局部Hebbian可塑性和基于度量的微调，使CNN不仅更准确而且更可解释，对资源受限和安全关键的AI部署具有实际益处。

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [74] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC是一个二阶可微框架，通过动态平衡强化学习和行为克隆来解决离线RL中策略约束的缩放问题，无需针对不同数据集进行超参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法需要针对不同任务和数据集质量精心调整策略约束的超参数，这既耗时又不实用。

Method: 提出自适应策略约束缩放(ASPC)框架，使用二阶可微方法在训练过程中动态平衡RL目标和行为克隆目标。

Result: 在4个D4RL领域的39个数据集上，ASPC使用单一超参数配置就优于其他自适应约束方法和需要逐数据集调优的最先进算法，且计算开销极小。

Conclusion: ASPC提供了一种有效且实用的离线RL解决方案，能够自动适应不同数据集，避免了繁琐的超参数调优过程。

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [75] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: GegenNet是一个用于符号二分图链接符号预测的新型谱卷积神经网络模型，通过Gegenbauer多项式基滤波器实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单分图，忽略了二分图的节点异质性和独特特征，且传统谱卷积算子不适合从已知正负链接推断缺失链接

Method: 采用快速谱分解技术初始化节点特征，基于Gegenbauer多项式基的新谱图滤波器，以及多层符号感知谱卷积网络交替处理正负边

Result: 在6个基准SBG数据集上相比11个强竞争对手，AUC提升最高4.28%，F1分数提升最高11.69%

Conclusion: GegenNet通过创新的谱卷积设计有效解决了符号二分图链接符号预测问题，展现出显著优越的性能

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [76] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: 提出基于UMLS本体概念的放射学报告检索方法，通过标准化医学实体提取和语义相似度计算，在胸部X光分类任务中优于现有嵌入方法，特别是在长尾场景下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP或CXR-BERT等高维文本嵌入的检索方法存在可解释性差、计算成本高且与医学知识结构化特性不匹配的问题，需要更透明、可解释的医学报告比较方法。

Method: 使用RadGraph-XL和SapBERT增强流水线从自由文本报告中提取标准化医学实体，链接到UMLS概念(CUIs)，然后基于改进的加权Tversky指数定义任务自适应的相似度度量，考虑同义词、否定和层次关系。

Result: 在MIMIC-CXR的放射图像分类任务中，该方法优于最先进的基于嵌入的检索方法，特别是在长尾设置下表现更好，并为MIMIC-CXR生成了本体支持的新疾病标签资源。

Conclusion: 该方法为临床AI系统提供了更可解释、可靠和任务特定的检索策略，特别是在需要可解释性和领域知识整合的场景中具有重要价值。

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [77] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer是一个基于BERT的预训练模型，专门用于网络流量分析，通过创新的流量表示、协议语义嵌入和预训练任务，显著提升了流量分类效果。


<details>
  <summary>Details</summary>
Motivation: 现有网络流量分类方法难以有效捕捉数据包结构特征、流级行为、分层协议语义和包间上下文关系，需要更专业的预训练模型来解决这些挑战。

Method: 提出FlowletFormer模型，包含：1) 连贯行为感知流量表示模型；2) 协议栈对齐嵌入层；3) 字段特定和上下文感知预训练任务，以增强包间和流间学习。

Result: 实验结果表明，FlowletFormer在流量表示效果、分类准确性和少样本学习能力方面显著优于现有方法，并能更好地理解网络传输原理。

Conclusion: FlowletFormer通过有效整合领域特定的网络知识，为流量分析提供了更鲁棒和可信的框架，在多个关键指标上都有显著提升。

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [78] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: 提出了全局排列熵(GPE)，这是一种新的复杂度指标，不仅考虑连续段落的排列模式，还包含所有可能长度的非连续模式，通过高效算法提取完整排列分布，在合成数据集上展示了优于标准排列熵的性能。


<details>
  <summary>Details</summary>
Motivation: 标准排列熵只考虑连续段落的相对顺序模式，可能忽略了时间序列中重要的非连续结构信息，需要一种能够捕捉更全面排列模式的复杂度度量方法。

Method: 基于新开发的高效算法提取完整排列分布，考虑所有给定长度的可能模式（包括非连续模式），然后应用香农熵来量化复杂度。

Result: 在合成数据集上的实验表明，GPE能够揭示标准排列熵无法访问的结构信息，提供了更全面的复杂度分析。

Conclusion: 全局排列熵(GPE)是一个有效的复杂度指标，扩展了传统排列熵的概念，能够捕捉时间序列中更丰富的结构模式，为此提供了开源的Julia实现包。

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [79] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 基于机器学习的工业离心泵短期故障预测框架，使用随机森林和XGBoost模型，通过60分钟和120分钟滑动窗口提取统计特征，实现了5-30分钟前的故障预警。


<details>
  <summary>Details</summary>
Motivation: 工业离心泵的故障预测对预防性维护至关重要，传统方法难以实现准确的短期故障预警，需要开发基于实时传感器数据的机器学习解决方案。

Method: 使用滑动窗口方法（60分钟和120分钟）提取统计特征（均值、标准差、最小值、最大值、线性趋势），采用SMOTE算法处理类别不平衡，训练随机森林和XGBoost分类器进行故障预测。

Result: 随机森林模型在60分钟窗口下表现最佳：5分钟前召回率69.2%，15分钟前64.9%，30分钟前48.6%。120分钟窗口下，15和30分钟前预测准确率提升至65.6%。

Conclusion: 预测性能取决于历史数据长度和预测时间跨度，不同故障模式具有不同的时间尺度特征。该方法为工业实时监控系统提供了可解释且可扩展的预测性维护解决方案。

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [80] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: 通过数据驱动模拟分析了不同停车搜索策略的效果，Cord-Approx策略将用户停车搜索时间从19.98分钟降至6.69分钟，减少72-73%


<details>
  <summary>Details</summary>
Motivation: 城市密集区域的路边停车搜索加剧了交通拕塞，需要量化分析移动设备协同停车系统的实际效果

Method: 使用马德里实际交通数据进行高保真模拟，比较四种停车策略：无协调搜索、协调无非用户知识、理想神谕系统和新题Cord-Approx策略（利用历史占用分布估计非用户行为）

Result: Cord-Approx用户平均搜索时间6.69分钟，较无应用程希的用户（19.98分钟）减少72%，在中心区域和住宅区均可实现这73%的搜索时间缩减

Conclusion: Cord-Approx策略通过概率性估计非用户行为，在不依赖完整信息的情况下仍能显著提升停车搜索效率，为城市停车问题提供了实用解决方案

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [81] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: 语言模型在密码验证任务中表现不佳，推理能力反而会泄露机密信息，当前前沿模型不适合处理敏感信息


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在关键场景中作为自主代理部署，确保其可靠遵循用户定义规则已成为关键安全问题，需要研究模型是否具备上下文鲁棒性

Method: 开发PasswordEval基准测试，测量模型能否正确判断用户请求是否被授权（通过密码验证），并沿多个维度扩展难度：添加对抗性用户压力和多轮对话

Result: 当前开源和闭源模型在这个看似简单的任务上表现不佳，推理能力通常不会改善性能，反而经常泄露机密信息

Conclusion: 当前前沿模型不适合处理机密信息，推理能力可能需要以不同方式训练才能在高风险场景中安全部署

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [82] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 提出一种新的自监督预训练方法，通过双层优化和均衡约束处理异构数据，提高模型在下游任务中的适应性


<details>
  <summary>Details</summary>
Motivation: 传统自监督预训练方法在处理异构数据时简单混合所有数据并最小化全局平均损失，无法确保模型对每个异构数据源都达到局部最优

Method: 采用双层优化框架，通过K步梯度下降确保模型从初始点出发能为每个异构数据源找到局部最优解，使用一阶近似方法求解

Result: 在多领域和多语言数据集上的实验表明，该方法能显著提升自监督预训练模型在下游监督微调任务中的适应性

Conclusion: 提出的均衡约束和双层优化方法有效解决了异构数据自监督预训练问题，与MAML有理论联系，实验验证了其优越性

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [83] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 这篇论文提出了一种基于梯度的示例选择算法，通过梯度估计来高效选择上下文学习中的最佳示例子集，大幅提升选择效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决在固定模型权重的情况下，如何高效选择最佳的k个示例来优化上下文学习的问题，这在提示调整和链式思绪中具有广泛应用。

Method: 通过梯度估计输出，采用一阶近似方法，多次随机采样子集并聚合影响分数，最终选择最相关的k个示例。算法只需预计算模型输出和梯度一次，复杂度为线性。

Result: 在6个数据集上实验验证，梯度估计误差小于1%，选择速度提升37.7倍，性能超过现有基于输入嵌入的方法11%。

Conclusion: 该方法能够在保持高准确性的同时显著提升示例选择效率，为大规模模型的上下文学习提供了高效的解决方案。

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [84] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: 该研究开发了一个多模态层次分类框架来解决电商产品分类中的平台异质性和现有分类法结构限制问题，通过融合文本、视觉和视觉-语言特征，在时尚电商数据集上取得了98.59%的层次F1分数，并展示了工业部署的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决电商产品分类中的两个关键工业挑战：平台异质性和现有分类法的结构限制，需要开发能够处理多平台数据并保持分类一致性的分类框架。

Method: 使用271,700个产品的数据集，整合RoBERTa文本特征、ViT视觉特征和CLIP视觉-语言特征，研究早期融合、晚期融合和基于注意力的融合策略，采用动态掩码的层次架构确保分类一致性，并引入自监督产品重分类流程。

Result: CLIP嵌入通过MLP晚期融合策略获得最高层次F1分数(98.59%)；自监督重分类流程发现新的细粒度类别，聚类纯度超过86%；跨平台实验显示晚期融合方法在多样化训练数据上准确率最高，而早期融合方法对未见平台泛化更好。

Conclusion: 该多模态层次分类框架成功解决了电商产品分类的工业挑战，通过两阶段推理管道实现了工业可扩展性部署，平衡了成本和准确性，为实际商业应用提供了有效解决方案。

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [85] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: 本文开发了一个检测微调过程中快速转变的框架，使用分布变化检测方法和基于自然语言的序参量来量化LLM微调中的相变现象。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在狭窄有害数据集上微调时如何出现与人类价值观广泛不匹配的行为，理解这种突发性不匹配何时以及如何发生。

Method: 结合分布变化检测方法和由LLM评估的英文序参量，使用客观统计差异度量来量化微调过程中的相变对模型多方面的影响。

Result: 发现实际行为转变发生在训练后期，比梯度范数峰值指示的时间更晚；能够自动化发现和量化基于语言的序参量，适用于知识问答、政治和伦理等多个领域。

Conclusion: 该框架能够有效检测和表征微调过程中的快速转变，为理解LLM微调过程中的相变现象提供了系统化的分析方法。

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [86] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony是一个去中心化的多智能体系统，通过分布式账本、信标选择协议和加权投票机制，使消费级GPU上的轻量级LLM能够协同工作，解决了集中式编排的高成本、拓扑僵化和适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体框架大多采用集中式编排，存在部署成本高、通信拓扑僵化、适应性有限等问题，需要一种更高效、灵活的分布式解决方案。

Method: 提出Symphony系统，包含三个核心机制：1）去中心化账本记录能力；2）信标选择协议进行动态任务分配；3）基于思维链的加权结果投票。

Result: 在推理基准测试中优于现有基线方法，实现了显著的准确率提升，并在不同容量模型间展现出良好的鲁棒性。

Conclusion: Symphony提供了一种隐私保护、可扩展、容错且低开销的编排方案，证明了去中心化方法在LLM多智能体系统中的有效性。

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [87] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop是一个用于神经网络预测模型中人工引导偏差缓解的工具，通过从神经网络中提取决策树让用户检查和修改不公平决策逻辑，然后微调原始模型以获得更公平的预测


<details>
  <summary>Details</summary>
Motivation: 敏感属性（如性别或年龄）在机器学习任务中可能导致不公平预测，特别是在不考虑上下文的情况下使用时。现有方法通常统一排除敏感属性，但缺乏上下文感知的偏差移除

Method: FairLoop从神经网络中提取决策树，允许用户检查和修改不公平的决策逻辑，然后使用修改后的逻辑来微调原始神经网络模型，实现上下文感知的偏差移除

Result: 相比其他公平性方法，FairLoop通过人工参与实现上下文感知的偏差移除，能够选择性地处理敏感属性的影响，而不是统一排除它们

Conclusion: FairLoop提供了一种有效的人工引导偏差缓解方法，通过在神经网络预测模型中引入人类监督和上下文感知，实现了更精细和有效的公平性保证

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [88] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: 使用大语言模型为个性化推荐邮件生成主题标题，通过离线和在线实验证明能有效提升用户参与度


<details>
  <summary>Details</summary>
Motivation: 电商平台个性化邮件的标题通常采用固定模板，无法充分激发用户对邮件内容的兴趣，限制了营销效果

Method: 利用大语言模型生成反映邮件个性化内容的主题标题，进行离线模拟和数百万用户规模的在线实验

Result: 实验证明该技术能有效改善客户与邮件之间的互动参与度

Conclusion: 成功实现了为百万级用户安全自动化生成邮件标题的生产化部署，并总结了关键发现和经验

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [89] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文研究使用注意力头剪枝来防御预训练语言模型中的后门攻击，提出了六种剪枝策略，实验表明不同策略对不同类型的攻击触发方式效果不同。


<details>
  <summary>Details</summary>
Motivation: 后门攻击对预训练语言模型的性能和完整性构成严重威胁，即使经过微调后这些攻击仍然存在。由于终端用户通常不了解攻击触发器，传统检测难以防御，因此需要事后净化方法。

Method: 设计了六种基于剪枝的策略：梯度剪枝、层间方差剪枝、结构化稀疏剪枝、随机集成剪枝、强化学习引导剪枝和贝叶斯不确定性剪枝。这些方法迭代移除信息量最小的注意力头，同时监控验证准确率以避免过度剪枝。

Result: 实验评估显示，梯度剪枝在防御语法触发攻击方面表现最佳，而强化学习和贝叶斯剪枝在应对风格攻击方面效果更好。

Conclusion: 注意力头剪枝可以有效缓解后门攻击威胁，不同剪枝策略适用于不同类型的攻击触发方式，为后门防御提供了无需触发器知识或干净参考模型的有效解决方案。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [90] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: 本文通过将多臂老虎机强化学习算法应用于Failure-Directed Search (FDS)，在调度问题上实现了显著的性能提升，比原始FDS快1.7-2.1倍，比IBM CP Optimizer快2.1-3.5倍，并改进了多个基准实例的最优下界。


<details>
  <summary>Details</summary>
Motivation: FDS是约束规划中重要的完全通用搜索算法，在调度问题上表现优异。研究发现FDS的搜索树最小化与多臂老虎机问题密切相关，这为应用强化学习算法提供了理论基础。

Method: 将多臂老虎机强化学习算法应用于FDS，并进行了问题特定的改进和参数调优。在作业车间调度问题(JSSP)和资源约束项目调度问题(RCPSP)上进行了评估。

Result: 增强版FDS在JSSP上比原始实现快1.7倍，在RCPSP上快2.1倍；比IBM CP Optimizer 22.1中的FDS算法在JSSP上快3.5倍，在RCPSP上快2.1倍。在900秒时间限制内，改进了84个JSSP实例中的78个和393个RCPSP实例中的226个的最优下界。

Conclusion: 多臂老虎机强化学习算法与FDS的结合显著提升了调度问题的求解效率，证明了该方法在约束规划搜索算法优化中的有效性。

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [91] [Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy](https://arxiv.org/abs/2508.19750)
*Binhui Zhang,Jianwei Ma*

Main category: stat.ML

TL;DR: Fractal Flow是一种新型标准化流架构，通过整合Kolmogorov-Arnold网络和潜在狄利克雷分配来构建结构化潜在空间，并采用递归模块化设计提高表达能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升标准化流在高维密度估计和生成建模中的表达能力和可解释性，同时构建结构化的可解释潜在空间和层次化语义聚类。

Method: 1. 整合Kolmogorov-Arnold网络和潜在狄利克雷分配(LDA)来构建结构化潜在空间
2. 引入分形生成模型的递归模块化设计
3. 在MNIST、FashionMNIST、CIFAR-10和地球物理数据上进行实验验证

Result: Fractal Flow实现了潜在聚类、可控生成和优越的估计精度，在多个数据集上表现出色。

Conclusion: 该架构成功提升了标准化流的表达能力和可解释性，为高维密度估计和生成建模提供了有效的解决方案。

Abstract: Normalizing Flows provide a principled framework for high-dimensional density
estimation and generative modeling by constructing invertible transformations
with tractable Jacobian determinants. We propose Fractal Flow, a novel
normalizing flow architecture that enhances both expressiveness and
interpretability through two key innovations. First, we integrate
Kolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into
normalizing flows to construct a structured, interpretable latent space and
model hierarchical semantic clusters. Second, inspired by Fractal Generative
Models, we introduce a recursive modular design into normalizing flows to
improve transformation interpretability and estimation accuracy. Experiments on
MNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the
Fractal Flow achieves latent clustering, controllable generation, and superior
estimation accuracy.

</details>


### [92] [Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers](https://arxiv.org/abs/2508.19841)
*Fahime Seyedheydari,Kevin Conley,Simo Särkkä*

Main category: stat.ML

TL;DR: 提出基于条件归一化流的概率数据驱动代理模型，用于预测纳米粒子嵌入散射介质的辐射特性，能够提供完整的后验预测分布和不确定性量化


<details>
  <summary>Details</summary>
Motivation: 传统神经网络无法提供完整的后验预测分布和不确定性量化，需要开发能够同时提供准确预测和可靠不确定性估计的代理模型

Method: 使用条件归一化流学习光学输出（反射率、吸收率、透射率）的条件分布，输入参数包括吸收系数、散射系数、各向异性因子和粒径分布，训练数据通过蒙特卡洛辐射传输模拟和Mie理论生成

Result: 模型实现了高预测精度和可靠的不确定性估计

Conclusion: 该模型成为辐射传输模拟的强大高效代理工具

Abstract: We present a probabilistic, data-driven surrogate model for predicting the
radiative properties of nanoparticle embedded scattering media. The model uses
conditional normalizing flows, which learn the conditional distribution of
optical outputs, including reflectance, absorbance, and transmittance, given
input parameters such as the absorption coefficient, scattering coefficient,
anisotropy factor, and particle size distribution. We generate training data
using Monte Carlo radiative transfer simulations, with optical properties
derived from Mie theory. Unlike conventional neural networks, the conditional
normalizing flow model yields full posterior predictive distributions, enabling
both accurate forecasts and principled uncertainty quantification. Our results
demonstrate that this model achieves high predictive accuracy and reliable
uncertainty estimates, establishing it as a powerful and efficient surrogate
for radiative transfer simulations.

</details>


### [93] [The Information Dynamics of Generative Diffusion](https://arxiv.org/abs/2508.19897)
*Luca Ambrogioni*

Main category: stat.ML

TL;DR: 这篇论文提出了一个统一的理论框架，将生成扩散模型的动态、信息论和热力学特性联系起来，揭示了生成过程由受控的噪声诱导对称性破缺驱动。


<details>
  <summary>Details</summary>
Motivation: 虽然生成扩散模型在机器学习中表现出强大能力，但对其运行机制的统一理论理解仍在发展中，需要建立整合的数学框架来解释其核心特性。

Method: 通过连接扩散模型的动态特性、信息论属性和热力学性质，建立统一数学框架，分析条件熵产生率与得分函数向量场散度的关系。

Result: 发现生成带宽由得分函数向量场的期望散度直接控制，该散度与轨迹分支和生成分叉相关，表现为能量景观中的对称性破缺相变。

Conclusion: 生成过程本质上是受控的噪声诱导对称性破缺过程，得分函数作为动态非线性滤波器，通过抑制与数据不兼容的波动来调节噪声带宽。

Abstract: Generative diffusion models have emerged as a powerful class of models in
machine learning, yet a unified theoretical understanding of their operation is
still developing. This perspective paper provides an integrated perspective on
generative diffusion by connecting their dynamic, information-theoretic, and
thermodynamic properties under a unified mathematical framework. We demonstrate
that the rate of conditional entropy production during generation (i.e. the
generative bandwidth) is directly governed by the expected divergence of the
score function's vector field. This divergence, in turn, is linked to the
branching of trajectories and generative bifurcations, which we characterize as
symmetry-breaking phase transitions in the energy landscape. This synthesis
offers a powerful insight: the process of generation is fundamentally driven by
the controlled, noise-induced breaking of (approximate) symmetries, where peaks
in information transfer correspond to critical transitions between possible
outcomes. The score function acts as a dynamic non-linear filter that regulates
the bandwidth of the noise by suppressing fluctuations that are incompatible
with the data.

</details>
