<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 15]
- [cs.LG](#cs.LG) [Total: 71]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Practical Analysis: Understanding Phase Noise Modelling in Time and Frequency Domain for Phase-Locked Loops](https://arxiv.org/abs/2507.12146)
*Carl Collmann,Bitan Banerjee,Ahmad Nimr,Gerhard Fettweis*

Main category: eess.SP

TL;DR: 本文提出了一种针对USRP X310系列SDR设备的相位噪声建模方法，填补了现有文献中缺乏实测数据支持的空白，并提供了关键PLL性能指标和相位噪声PSD的参数化模型。


<details>
  <summary>Details</summary>
Motivation: MIMO系统中，相位噪声会显著影响性能，尤其是需要相位同步的数字波束成形应用。现有SDR设备的相位噪声数据不足，缺乏实测支持的系统建模。

Method: 基于USRP X310系列SDR的实测数据，提取关键PLL性能指标（如周期抖动、振荡器常数和PLL带宽），并建立相位噪声PSD的参数化模型。

Result: 提出了一个实用的相位噪声建模方法，并提供了模型参数估计，可用于分析类似SDR设备在MIMO系统中的相位噪声影响。

Conclusion: 该方法为MIMO系统中SDR设备的相位噪声建模提供了实用工具，有助于进一步研究相位噪声对系统性能的影响。

Abstract: In MIMO systems, the presence of phase noise is a significant factor that can
degrade performance. For MIMO testbeds build from SDR devices, phase noise
cannot be ignored, particular in applications that require phase
synchronization. This is especially relevant in MIMO systems that employ
digital beamforming, where precise phase alignment is crucial. Accordingly,
accurate phase noise modelling of SDR devices is essential. However, the
information provided in data sheets for different SDR models varies widely and
is often insufficient for comprehensive characterization of their phase noise
performance. While numerical simulations of PLL phase noise behavior are
documented in the literature, there is a lack of extensive measurements
supported by appropriate system modelling. In this work, we present a practical
phase noise modeling methodology applied to an SDR from the USRP X310 series.
Based on measurement data, we derive estimates of key PLL performance
indicators such as cycle-to-cycle jitter, oscillator constants, and PLL
bandwidth. Furthermore, we propose a parametric model for the phase noise PSD
of the PLL circuit and provide corresponding parameter estimates. This model
can be used for further investigation into the impact of phase noise on MIMO
system performance implemented by similar SDR devices.

</details>


### [2] [Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions](https://arxiv.org/abs/2507.11783)
*Gayal Kuruppu,Neeraj Wagh,Yogatheesan Varatharajah*

Main category: eess.SP

TL;DR: 本文综述了10种早期自监督脑电图基础模型（EEG-FMs），分析了其方法、实证结果及研究空白，指出未来需标准化评估、扩大规模效应并优化学习流程。


<details>
  <summary>Details</summary>
Motivation: 传统监督EEG编码器依赖昂贵标注且鲁棒性不足，推动自监督EEG-FMs的发展，但其实际应用潜力尚不明确。

Method: 分析10种EEG-FMs，聚焦序列建模（如Transformer）和掩码序列重建的自监督方法。

Result: 当前EEG-FMs评估不统一且局限，实用性难以评估。

Conclusion: 未来需标准化评估、扩大规模效应、优化学习流程，并与领域专家合作推动EEG-FMs的实际应用。

Abstract: Patterns of electrical brain activity recorded via electroencephalography
(EEG) offer immense value for scientific and clinical investigations. The
inability of supervised EEG encoders to learn robust EEG patterns and their
over-reliance on expensive signal annotations have sparked a transition towards
general-purpose self-supervised EEG encoders, i.e., EEG foundation models
(EEG-FMs), for robust and scalable EEG feature extraction. However, the
real-world readiness of early EEG-FMs and the rubric for long-term research
progress remain unclear. A systematic and comprehensive review of
first-generation EEG-FMs is therefore necessary to understand the current
state-of-the-art and identify key directions for future EEG-FMs. To that end,
this study reviews 10 early EEG-FMs and presents a critical synthesis of their
methodology, empirical findings, and outstanding research gaps. We find that
most EEG-FMs adopt a sequence-based modeling scheme that relies on
transformer-based backbones and the reconstruction of masked sequences for
self-supervision. However, model evaluations remain heterogeneous and largely
limited, making it challenging to assess their practical off-the-shelf utility.
In addition to adopting standardized and realistic evaluations, future work
should demonstrate more substantial scaling effects and make principled and
trustworthy choices throughout the EEG representation learning pipeline. We
believe that developing benchmarks, software tools, technical methodologies,
and applications in collaboration with domain experts may further advance the
translational utility and real-world adoption of EEG-FMs.

</details>


### [3] [Directional Measurements and Analysis for FR3 Low-Altitude Channels in a Campus Environment](https://arxiv.org/abs/2507.11846)
*Yulu Guo,Tongjia Zhang,Xiangwen Gu,Shu Sun,Meixia Tao,Ruifeng Gao*

Main category: eess.SP

TL;DR: 论文通过低空FR3频段信道测量，分析了路径损耗和功率角谱，发现近距离模型优于3GPP模型，且传播行为受环境条件显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究低空FR3频段在室外校园环境中的信道特性，为新兴中频通信系统的信道建模提供基础。

Method: 使用时域信道探测系统进行路径损耗测量和定向功率角谱测量。

Result: 近距离模型表现更优，传播行为因环境条件而异，地面反射和散射效应显著。

Conclusion: 低空传播受发射高度和地面散射机制复杂交互影响，为中频通信系统信道建模提供重要见解。

Abstract: In this paper, we present detailed low-altitude channel measurements at the
FR3 band in an outdoor campus environment. Using a time-domain channel sounder
system, we conduct two types of measurements: path loss measurements by moving
the transmitter (Tx) at one-meter intervals along a 26-point rooftop path, and
directional power angular spectrum measurements through antenna scanning at
half-power beam width intervals. The path loss analysis across different Rx
shows that the close-in model outperforms conventional 3GPP models and
height-corrected variants, with path loss exponents close to free space values
indicating line-of-sight dominance. The power angular spectrum measurements
show that propagation behavior varies significantly with environmental
conditions. Closer Rx exhibit stronger sensitivity to ground reflections during
downward Tx tilting, while obstructed links display uniform angular
characteristics due to dominant scattering effects, and corridor environments
produce asymmetric power distributions. These results indicate that
low-altitude propagation is characterized by complex interactions between Tx
height and ground scattering mechanisms, providing fundamental insights for
channel modeling in emerging mid-band communication systems.

</details>


### [4] [Joint UAV Placement and Transceiver Design in Multi-User Wireless Relay Networks](https://arxiv.org/abs/2507.11912)
*Tzu-Hsuan Chou,Nicolo Michelusi,David J. Love,James V. Krogmeier*

Main category: eess.SP

TL;DR: 提出了一种优化无人机中继放置、波束成形和接收组合的新方法，以提升非正交多用户无线中继网络中的最小SINR。


<details>
  <summary>Details</summary>
Motivation: 解决在缺乏瞬时信道状态信息的情况下优化无人机中继放置和波束成形的挑战。

Method: 将设计分为波束成形感知的无人机放置优化和收发器设计两部分，分别使用差分凸框架和块坐标下降法。

Result: 数值结果显示，该方法比现有技术方案提升了4.6 dB的SINR。

Conclusion: 提出的联合算法显著提升了非正交多用户网络中的最小SINR性能。

Abstract: In this paper, a novel approach is proposed to improve the minimum
signal-to-interference-plus-noise-ratio (SINR) among users in non-orthogonal
multi-user wireless relay networks, by optimizing the placement of unmanned
aerial vehicle (UAV) relays, relay beamforming, and receive combining. The
design is separated into two problems: beamforming-aware UAV placement
optimization and transceiver design for minimum SINR maximization. A
significant challenge in beamforming-aware UAV placement optimization is the
lack of instantaneous channel state information (CSI) prior to deploying UAV
relays, making it difficult to derive the beamforming SINR in non-orthogonal
multi-user transmission. To address this issue, an approximation of the
expected beamforming SINR is derived using the narrow beam property of a
massive MIMO base station. Based on this, a UAV placement algorithm is proposed
to provide UAV positions that improve the minimum expected beamforming SINR
among users, using a difference-of-convex framework. Subsequently, after
deploying the UAV relays to the optimized positions, and with estimated CSI
available, a joint relay beamforming and receive combining (JRBC) algorithm is
proposed to optimize the transceiver to improve the minimum beamforming SINR
among users, using a block-coordinate descent approach. Numerical results show
that the UAV placement algorithm combined with the JRBC algorithm provides a
4.6 dB SINR improvement over state-of-the-art schemes.

</details>


### [5] [Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission](https://arxiv.org/abs/2507.11913)
*Chen Zhu,Siyun Liang,Zhouxiang Zhao,Jianrong Bao,Zhaohui Yang,Zhaoyang Zhang,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于概率图的无线图像语义通信框架，通过两阶段压缩算法提升传输效率。


<details>
  <summary>Details</summary>
Motivation: 解决网络拥塞和传输效率问题，通过语义通信传输意义而非原始符号。

Method: 使用场景图表示高级语义，设计两阶段压缩算法去除可预测部分，利用概率图共享语义知识。

Result: 仿真结果显示方案在传输吞吐量和语义对齐方面表现优越。

Conclusion: 利用高级语义进行图像通信是有效的，未来可研究多轮语义压缩算法。

Abstract: Semantic communication emphasizes the transmission of meaning rather than raw
symbols. It offers a promising solution to alleviate network congestion and
improve transmission efficiency. In this paper, we propose a wireless image
communication framework that employs probability graphs as shared semantic
knowledge base among distributed users. High-level image semantics are
represented via scene graphs, and a two-stage compression algorithm is devised
to remove predictable components based on learned conditional and co-occurrence
probabilities. At the transmitter, the algorithm filters redundant relations
and entity pairs, while at the receiver, semantic recovery leverages the same
probability graphs to reconstruct omitted information. For further research, we
also put forward a multi-round semantic compression algorithm with its
theoretical performance analysis. Simulation results demonstrate that our
semantic-aware scheme achieves superior transmission throughput and satiable
semantic alignment, validating the efficacy of leveraging high-level semantics
for image communication.

</details>


### [6] [STFT-based Time-Frequency Mode Decomposition: A Fast and Robust Method for Multicomponent Signal Analysis](https://arxiv.org/abs/2507.11919)
*Wei Zhou,Wei-Jian Li,Wei-Xin Ren*

Main category: eess.SP

TL;DR: TFMD是一种新型时频模态分解方法，通过图像分割技术高效分解复杂信号，具有高精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在精度、计算成本和先验信息需求之间的权衡问题。

Method: 将信号转换为时频谱，通过平滑、自适应阈值和连通区域标记分割模态，最后逆变换重构。

Result: 在合成和真实信号中表现优异，尤其在高噪声环境下，计算效率高。

Conclusion: TFMD为多分量信号分析提供了高效、精确且通用的解决方案。

Abstract: The decomposition of complex, multicomponent, and non-stationary signals into
their constituent modes is a fundamental yet significant challenge in science
and engineering. Existing methods often struggle with a trade-off among
accuracy, computational cost, and the need for prior information such as the
number of modes. This paper introduces time-frequency mode decomposition
(TFMD), a novel framework for the fast, robust, and adaptive decomposition of
such signals. TFMD operates on the principle that modes form contiguous
high-energy regions in the time-frequency domain. Its non-iterative pipeline
reframes signal decomposition as an image segmentation task: a signal is
transformed into a spectrogram, which is then smoothed to enhance the
continuity of these high-energy regions. A sequence of adaptive thresholding
and connected-component labeling with size-based filtering is then employed to
automatically segment the spectrogram and generate a mask for each mode. The
modes are finally reconstructed via the inverse short-time Fourier transform.
Validation on diverse synthetic signals demonstrates that TFMD accurately
determines the number of modes and reconstructs them with high fidelity. Its
performance is particularly strong in high-noise conditions. A comparative
analysis confirms that TFMD provides robust, competitive performance across a
wider variety of signal types, while a theoretical complexity analysis reveals
its superior computational efficiency stemming from its non-iterative design.
The method's practical utility is further demonstrated by successfully
extracting modal responses from a real-world footbridge vibration signal. TFMD
provides a computationally efficient and powerful paradigm for multicomponent
signal analysis, offering a compelling balance of accuracy, versatility, and
efficiency for large-scale or time-sensitive applications.

</details>


### [7] [DSSD: Efficient Edge-Device Deployment and Collaborative Inference via Distributed Split Speculative Decoding](https://arxiv.org/abs/2507.12000)
*Jiahong Ning,Ce Zheng,Tingting Yang*

Main category: eess.SP

TL;DR: 论文提出了一种分布式分割推测解码（DSSD）架构，通过设备与边缘协作减少通信延迟，同时保持推理质量。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在设备-边缘系统中部署时面临的资源限制和通信开销问题。

Method: 提出DSSD架构，将验证阶段分割到设备和边缘，减少上行传输成本。

Result: 实验表明DSSD在减少通信延迟的同时保持了推理质量，优于现有方法。

Conclusion: DSSD是一种高效的设备-边缘协作框架，显著提升了部署效率。

Abstract: Large language models (LLMs) have transformed natural language processing but
face critical deployment challenges in device-edge systems due to resource
limitations and communication overhead. To address these issues, collaborative
frameworks have emerged that combine small language models (SLMs) on devices
with LLMs at the edge, using speculative decoding (SD) to improve efficiency.
However, existing solutions often trade inference accuracy for latency or
suffer from high uplink transmission costs when verifying candidate tokens. In
this paper, we propose Distributed Split Speculative Decoding (DSSD), a novel
architecture that not only preserves the SLM-LLM split but also partitions the
verification phase between the device and edge. In this way, DSSD replaces the
uplink transmission of multiple vocabulary distributions with a single downlink
transmission, significantly reducing communication latency while maintaining
inference quality. Experiments show that our solution outperforms current
methods, and codes are at:
https://github.com/JasonNing96/DSSD-Efficient-Edge-Computing

</details>


### [8] [Enhancing Situational Awareness in ISAC Networks via Drone Swarms: A Real-World Channel Sounding Data Set](https://arxiv.org/abs/2507.12010)
*Julia Beuster,Carsten Andrich,Sebastian Giehl,Marc Miranda,Lorenz Mohr,Dieter Novotny,Tom Kaufmann,Christian Schneider,Reiner Thomä*

Main category: eess.SP

TL;DR: 论文提出了一种基于6G网络中集成传感与通信（ISAC）和无人机（UAV）的多静态雷达感知方法，通过实际测量数据验证了其在空对空（A2A）和空对地（A2G）场景中的潜力。


<details>
  <summary>Details</summary>
Motivation: 利用6G网络中ISAC和无人机的协同能力，提升多静态雷达感知的情境感知能力。

Method: 使用分布式地面和飞行传感器节点组成的测试平台，采集多路径环境中的双静态反射率数据。

Result: 展示了无人机群在多静态雷达跟踪和定位中的潜力，并公开了数据集以支持未来ISAC算法的开发。

Conclusion: 通过实际测量验证了无人机群在ISAC网络中的多静态雷达感知能力，为未来算法开发提供了真实数据支持。

Abstract: With the upcoming capabilities of integrated sensing and communication (ISAC)
and the incorporation of user equipment (UE) like unmanned aerial vehicles
(UAVs) in 6G mobile networks, there is a significant opportunity to enhance
situational awareness through multi-static radar sensing in meshed ISAC
networks. This paper presents a real-world channel sounding data set acquired
using a testbed with synchronized, distributed ground-based sensor nodes and
flying sensor nodes within a swarm of up to four drones. The conducted
measurement campaign is designed to sense the bi-static reflectivity of objects
such as parking cars, vertical take-off and landing (VTOL) aircraft, and small
drones in multi-path environments. We detail the rationale behind the selection
of the included scenarios and the configuration of the participating nodesand
present exemplary results to demonstrate the potential of using collaborating
drone swarms for multi-static radar tracking and localization in air-to-air
(A2A) and air-to-ground (A2G) scenarios. The data sets are publicly available
to support the development and validation of future ISAC algorithms in
real-world environments rather than relying solely on simulation.

</details>


### [9] [DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi](https://arxiv.org/abs/2507.12132)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 该论文提出了一种基于Wi-Fi CSI的多普勒速度投影方法，通过3D潜在运动表示和均匀多普勒辐射场（DoRF）提升人类活动识别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Wi-Fi CSI在多普勒速度投影方面取得进展，但泛化能力仍不足以实际应用，因此需要更鲁棒的方法。

Method: 受NeRF启发，从一维多普勒速度投影重建3D潜在运动表示，构建均匀多普勒辐射场（DoRF）。

Result: 实验表明，该方法显著提升了Wi-Fi HAR的泛化准确性。

Conclusion: DoRF在实用传感应用中具有巨大潜力。

Abstract: Wi-Fi Channel State Information (CSI) has gained increasing interest for
remote sensing applications. Recent studies show that Doppler velocity
projections extracted from CSI can enable human activity recognition (HAR) that
is robust to environmental changes and generalizes to new users. However,
despite these advances, generalizability still remains insufficient for
practical deployment. Inspired by neural radiance fields (NeRF), which learn a
volumetric representation of a 3D scene from 2D images, this work proposes a
novel approach to reconstruct an informative 3D latent motion representation
from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The
resulting latent representation is then used to construct a uniform Doppler
radiance field (DoRF) of the motion, providing a comprehensive view of the
performed activity and improving the robustness to environmental variability.
The results show that the proposed approach noticeably enhances the
generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential
of DoRFs for practical sensing applications.

</details>


### [10] [PAPR of DFT-s-OTFS with Pulse Shaping](https://arxiv.org/abs/2507.12210)
*Jialiang Zhu,Sanoopkumar P. S.,Arman Farhang*

Main category: eess.SP

TL;DR: 论文分析了DFT-s-OTFS方案在上行链路中的PAPR问题，比较了不同脉冲整形滤波器和资源分配策略，发现交错分配能降低PAPR并简化发射机设计。


<details>
  <summary>Details</summary>
Motivation: 解决OTFS在多普勒频段较多时的高PAPR问题。

Method: 采用DFT扩展OTFS（DFT-s-OTFS），分析其PAPR上界，比较交错和块状多普勒资源分配策略。

Result: 交错分配比块状分配PAPR更低，且RRC脉冲的PAPR高于矩形脉冲。BER性能与未扩展OTFS相同。

Conclusion: DFT-s-OTFS是降低PAPR的有效方案，尤其适合上行链路。

Abstract: Orthogonal Time Frequency Space (OTFS) suffers from high peak-to-average
power ratio (PAPR) when the number of Doppler bins is large. To address this
issue, a discrete Fourier transform spread OTFS (DFT-s-OTFS) scheme is employed
by applying DFT spreading across the Doppler dimension. This paper presents a
thorough PAPR analysis of DFT-s-OTFS in the uplink scenario using different
pulse shaping filters and resource allocation strategies. Specifically, we
derive a PAPR upper bound of DFT-s-OTFS with interleaved and block Doppler
resource allocation schemes. Our analysis reveals that DFT-s-OTFS with
interleaved allocation yields a lower PAPR than that of block allocation.
Furthermore, we show that interleaved allocation produces a periodic
time-domain signal composed of repeated quadrature amplitude modulated (QAM)
symbols which simplifies the transmitter design. Based on our analytical
results, the root raised cosine (RRC) pulse generally results in a higher
maximum PAPR compared to the rectangular pulse. Simulation results confirm the
validity of the derived PAPR upper bounds. Furthermore, we also demonstrate
through BER simulation analysis that the DFT-s-OTFS gives the same performance
as OTFS without DFT spreading.

</details>


### [11] [Cell Sensing: Traffic detection](https://arxiv.org/abs/2507.12211)
*Saúl Fenollosa*

Main category: eess.SP

TL;DR: 提出一种基于LTE信号的被动交通监测系统，通过双接收器架构分析CSI，有效检测移动目标，室内测试准确率超90%，户外测试表现可靠。


<details>
  <summary>Details</summary>
Motivation: 传统交通监测方法侵入性强且扩展性差，需一种非侵入、可扩展的替代方案。

Method: 采用双接收器架构分析CSI，利用SDR平台和srsRAN软件实现系统，通过差分多普勒频移检测移动目标。

Result: 室内测试中速度高于6000 mm/min时检测准确率超90%，户外测试中对行人和车辆速度估计可靠。

Conclusion: LTE被动传感在交通监测中可行，未来需研究AoA集成、机器学习和实时嵌入式系统开发。

Abstract: This work presents a passive sensing system for traffic monitoring using
ambient Long Term Evolution (LTE) signals as a non-intrusive and scalable
alternative to traditional surveillance methods. The approach employs a
dual-receiver architecture analyzing Channel State Information (CSI) to isolate
differential Doppler shifts induced by moving targets, effectively mitigating
hardware-induced phase impairments. Implemented with a Software Defined Radio
(SDR) platform and srsRAN software, the system demonstrated over 90% detection
accuracy for speeds above 6000 mm/min in controlled indoor tests, and provided
reliable speed estimations for pedestrians and vehicles in outdoor evaluations.
Despite challenges at low speeds, directional ambiguity, and multipath fading
in urban settings, the results validate LTE-based passive sensing as a feasible
traffic monitoring method, identifying critical areas for future research such
as angle-of-arrival (AoA) integration, machine learning, and real-time embedded
system development.

</details>


### [12] [Novel Approach to Dual-Channel Estimation in Integrated Sensing and Communications for 6G](https://arxiv.org/abs/2507.12221)
*Alejandro Castilla,Saúl Fenollosa,Monika Drozdowska,Alejandro Lopez-Escudero,Sergio Micò-Rosa,Narcis Cardona*

Main category: eess.SP

TL;DR: 本文探讨了6G中的集成感知与通信（ISAC）设计，提出了一种双通道模型，通过毫米波雷达提取双基地感知通道，并验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 6G需要将环境感知与通信结合，因此需要理解和建模ISAC的双通道特性。

Method: 采用干扰提取、模块和相位相关性分析、啁啾聚类和自动杂波减少等技术，从单基地测量中提取双基地感知通道。

Result: 在消声室中验证了方法的有效性，通过RMS DS、PDP和AoA分析展示了成功通道提取，并与射线追踪模拟结果一致。

Conclusion: 该方法为未来网络的完全集成感知与通信提供了创新性进展。

Abstract: Integrated Sensing and Communication (ISAC) design is crucial for 6G and
harmonizes environmental data sensing with communication, emphasizing the need
to understand and model these elements. This paper delves into dual-channel
models for ISAC, employing channel extraction techniques to validate and
enhance accuracy. Focusing on millimeter wave (mmWave) radars, it explores the
extraction of the bistatic sensing channel from monostatic measurements and
subsequent communication channel estimation. The proposed methods involve
interference extraction, module and phase correlation analyses, chirp
clustering, and auto-clutter reduction. A comprehensive set-up in an anechoic
chamber with controlled scenarios evaluates the proposed techniques,
demonstrating successful channel extraction and validation through Root Mean
Square Delay Spread (RMS DS), Power Delay Profile (PDP), and Angle of Arrival
(AoA) analysis. Comparison with Ray-Tracing (RT) simulations confirms the
effectiveness of the proposed approach, presenting an innovative stride towards
fully integrated sensing and communication in future networks.

</details>


### [13] [Frequency-responsive RCS characteristics and scaling implications for ISAC development](https://arxiv.org/abs/2507.12235)
*Saúl Fenollosa,Monika Drozdowska,Wenfei Yang,Sergio Micó-Rosa,Alejandro Castilla,Alejandro Lopez-Escudero,Jian Li,Narcis Cardona*

Main category: eess.SP

TL;DR: 研究不同目标的雷达散射截面（RCS）随频率变化的特性，分析了AGV、行人和全尺寸汽车的RCS值及其分布。


<details>
  <summary>Details</summary>
Motivation: 探索RCS随频率和目标形状的变化规律，为6G标准中的集成感知与通信（ISAC）技术提供支持。

Method: 采用背景减除和时域门控技术提取RCS，并在室内外多种环境中测量。

Result: RCS值随频率和形状变化复杂，不同材料和形状的目标表现各异。

Conclusion: 研究结果为提升感知系统和优化3GPP信道模型提供了重要参考。

Abstract: This paper presents an investigation on the Radar Cross-Section (RCS) of
various targets, with the objective of analysing how RCS properties vary with
frequency. Targets such as an Automated Guided Vehicle (AGV), a pedestrian, and
a full-scale car were measured in the frequency bands referred to in industry
standards as FR2 and FR3. Measurements were taken in diverse environments,
indoors and outdoors, to ensure comprehensive scenario coverage. The
methodology employed in RCS extraction performs background subtraction,
followed by time-domain gating to isolate the influence of the target. This
analysis compares the RCS values and how the points of greatest contribution
are distributed across different bands based on the range response of the RCS.
Analysis of the results demonstrated how RCS values change with frequency and
target shape, providing insights into the electromagnetic behaviour of these
targets. Key findings highlight how much scaling RCS values based on frequency
and geometry is complex and varies among different types of materials and
shapes. These insights are instrumental for advancing sensing systems and
enhancing 3GPP channel models, particularly for Integrated Sensing and
Communications (ISAC) techniques proposed for 6G standards.

</details>


### [14] [Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning](https://arxiv.org/abs/2507.12301)
*Zhenyu Liu,Yi Ma,Rahim Tafazolli,Zhi Ding*

Main category: eess.SP

TL;DR: 论文提出Dual-ImRUNet框架，通过两个预处理模块实现超低反馈速率和高环境鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在超低速率场景和多样化环境中表现不佳，需提升性能和适应性。

Method: 提出双向相关性增强模块和输入格式对齐模块，结合基于Transformer的隐式CSI反馈网络。

Result: 反馈开销减少85%，并在未知环境中表现鲁棒。

Conclusion: Dual-ImRUNet在超低速率和多样化环境中表现优异。

Abstract: Deep learning-based implicit channel state information (CSI) feedback has
been introduced to enhance spectral efficiency in massive MIMO systems.
Existing methods often show performance degradation in ultra-low-rate scenarios
and inadaptability across diverse environments. In this paper, we propose
Dual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework
incorporating two novel plug-in preprocessing modules to achieve ultra-low
feedback rates while maintaining high environmental robustness. First, a novel
bi-directional correlation enhancement module is proposed to strengthen the
correlation between uplink and downlink CSI eigenvector matrices. This module
projects highly correlated uplink and downlink channel matrices into their
respective eigenspaces, effectively reducing redundancy for ultra-low-rate
feedback. Second, an innovative input format alignment module is designed to
maintain consistent data distributions at both encoder and decoder sides
without extra transmission overhead, thereby enhancing robustness against
environmental variations. Finally, we develop an efficient transformer-based
implicit CSI feedback network to exploit angular-delay domain sparsity and
bi-directional correlation for ultra-low-rate CSI compression. Simulation
results demonstrate successful reduction of the feedback overhead by 85%
compared with the state-of-the-art method and robustness against unseen
environments.

</details>


### [15] [Road Roughness Estimation via Fusion of Standard Onboard Automotive Sensors](https://arxiv.org/abs/2507.12317)
*Martin Agebjär,Gustav Zetterqvist,Fredrik Gustafsson,Johan Wahlström,Gustaf Hendeby*

Main category: eess.SP

TL;DR: 提出了一种基于卡尔曼滤波的方法，通过融合惯性和速度测量来估计道路粗糙度（IRI），验证结果显示误差在1%到10%之间，但仅使用横向振动时精度显著下降。


<details>
  <summary>Details</summary>
Motivation: 道路粗糙度对车辆振动和乘坐质量有重要影响，需要一种经济高效的监测方法。

Method: 通过系统辨识估计车辆模型参数，利用卡尔曼滤波重建纵向道路轮廓以计算IRI值，并探索了垂直和横向振动的使用。

Result: 在230公里真实数据上验证，IRI估计误差为1%至10%，但仅使用横向振动时精度显著下降。

Conclusion: 卡尔曼滤波方法在道路粗糙度监测中具有潜力，但横向振动的使用存在局限性。

Abstract: Road roughness significantly affects vehicle vibrations and ride quality. We
introduce a Kalman filter (KF)-based method for estimating road roughness in
terms of the international roughness index (IRI) by fusing inertial and speed
measurements, offering a cost-effective solution for pavement monitoring. The
method involves system identification on a physical vehicle to estimate
realistic model parameters, followed by KF-based reconstruction of the
longitudinal road profile to compute IRI values. It explores IRI estimation
using vertical and lateral vibrations, the latter more common in modern
vehicles. Validation on 230 km of real-world data shows promising results, with
IRI estimation errors ranging from 1% to 10% of the reference values. However,
accuracy deteriorates significantly when using only lateral vibrations,
highlighting their limitations. These findings demonstrate the potential of
KF-based estimation for efficient road roughness monitoring.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming](https://arxiv.org/abs/2507.11547)
*Yingxue Zhao,Qianyi Chen,Haoran Li,Haosu Zhou,Hamid Reza Attar,Tobias Pfaff,Tailin Wu,Nan Li*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的RUGNN模型，用于预测材料成形过程中的变形场，解决了传统AI模型在3D空间关系和置换不变性上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统AI模型难以捕捉复杂的3D空间关系和置换不变性，因此开发了基于图神经网络的RUGNN模型。

Method: RUGNN结合了门控循环单元（GRU）和U-Net结构的图上下采样机制，并提出了新的'节点到表面'接触表示方法。

Result: RUGNN在冷成形和热成形案例中表现优异，预测结果与有限元仿真高度一致，优于其他基线GNN架构。

Conclusion: RUGNN是一种可靠的方法，能够为板材成形设计提供准确的制造性预测。

Abstract: In recent years, various artificial intelligence-based surrogate models have
been proposed to provide rapid manufacturability predictions of material
forming processes. However, traditional AI-based surrogate models, typically
built with scalar or image-based neural networks, are limited in their ability
to capture complex 3D spatial relationships and to operate in a
permutation-invariant manner. To overcome these issues, emerging graph-based
surrogate models are developed using graph neural networks. This study
developed a new graph neural network surrogate model named Recurrent U
Net-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate
predictions of sheet material deformation fields across multiple forming
timesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model
temporal dynamics and a U-Net inspired graph-based downsample/upsample
mechanism to handle spatial long-range dependencies. A novel 'node-to-surface'
contact representation method was proposed, offering significant improvements
in computational efficiency for large-scale contact interactions. The RUGNN
model was validated using a cold forming case study and a more complex hot
forming case study using aluminium alloys. Results demonstrate that the RUGNN
model provides accurate deformation predictions closely matching ground truth
FE simulations and outperforming several baseline GNN architectures. Model
tuning was also performed to identify suitable hyperparameters, training
strategies, and input feature representations. These results demonstrate that
RUGNN is a reliable approach to support sheet material forming design by
enabling accurate manufacturability predictions.

</details>


### [17] [Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators](https://arxiv.org/abs/2507.11574)
*Kazuma Kobayashi,Shailesh Garg,Farid Ahmed,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: CMCO框架通过结合蒙特卡洛dropout和分形预测，为神经算子提供无需重新训练或定制损失设计的分布无关预测区间，解决了深度学习在实时虚拟传感中的不确定性量化问题。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在高风险领域实时虚拟传感中稀疏、噪声或非共位传感器数据带来的不确定性量化挑战。

Method: 提出Conformalized Monte Carlo Operator (CMCO)，结合蒙特卡洛dropout和分形预测，在DeepONet架构中实现空间分辨的不确定性估计。

Result: 在湍流、弹塑性变形和全球宇宙辐射剂量估计三个应用中，CMCO实现了接近名义覆盖率的性能。

Conclusion: CMCO为神经算子提供了一种通用、即插即用的不确定性量化解决方案，推动了可扩展、泛化性强且不确定性感知的科学机器学习。

Abstract: Robust uncertainty quantification (UQ) remains a critical barrier to the safe
deployment of deep learning in real-time virtual sensing, particularly in
high-stakes domains where sparse, noisy, or non-collocated sensor data are the
norm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework
that transforms neural operator-based virtual sensing with calibrated,
distribution-free prediction intervals. By unifying Monte Carlo dropout with
split conformal prediction in a single DeepONet architecture, CMCO achieves
spatially resolved uncertainty estimates without retraining, ensembling, or
custom loss design. Our method addresses a longstanding challenge: how to endow
operator learning with efficient and reliable UQ across heterogeneous domains.
Through rigorous evaluation on three distinct applications: turbulent flow,
elastoplastic deformation, and global cosmic radiation dose estimation-CMCO
consistently attains near-nominal empirical coverage, even in settings with
strong spatial gradients and proxy-based sensing. This breakthrough offers a
general-purpose, plug-and-play UQ solution for neural operators, unlocking
real-time, trustworthy inference in digital twins, sensor fusion, and
safety-critical monitoring. By bridging theory and deployment with minimal
computational overhead, CMCO establishes a new foundation for scalable,
generalizable, and uncertainty-aware scientific machine learning.

</details>


### [18] [RadioDiff-3D: A 3D$\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication](https://arxiv.org/abs/2507.12166)
*Xiucheng Wang,Qiming Zhang,Nan Cheng,Junting Chen,Zezhong Zhang,Zan Li,Shuguang Cui,Xuemin Shen*

Main category: cs.LG

TL;DR: 论文提出了UrbanRadio3D数据集和RadioDiff-3D生成框架，用于构建高分辨率3D无线电地图，解决了现有方法在垂直空间变化和多参数预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有无线电地图构建方法局限于2D平面和静态学习范式，忽略了方向到达（DoA）、时间到达（ToA）和垂直空间变化等关键参数。

Method: 通过射线追踪构建UrbanRadio3D数据集，提出基于3D卷积的UNet和扩散模型RadioDiff-3D，支持辐射感知和非感知场景。

Result: RadioDiff-3D在UrbanRadio3D上表现出色，能够构建高维动态无线电地图。

Conclusion: UrbanRadio3D和RadioDiff-3D为3D环境感知通信研究提供了基础数据集和基准。

Abstract: Radio maps (RMs) serve as a critical foundation for enabling
environment-aware wireless communication, as they provide the spatial
distribution of wireless channel characteristics. Despite recent progress in RM
construction using data-driven approaches, most existing methods focus solely
on pathloss prediction in a fixed 2D plane, neglecting key parameters such as
direction of arrival (DoA), time of arrival (ToA), and vertical spatial
variations. Such a limitation is primarily due to the reliance on static
learning paradigms, which hinder generalization beyond the training data
distribution. To address these challenges, we propose UrbanRadio3D, a
large-scale, high-resolution 3D RM dataset constructed via ray tracing in
realistic urban environments. UrbanRadio3D is over 37$\times$3 larger than
previous datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,
forming a novel 3D$\times$33D dataset with 7$\times$3 more height layers than
prior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet
with 3D convolutional operators is proposed. Moreover, we further introduce
RadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D
convolutional architecture. RadioDiff-3D supports both radiation-aware
scenarios with known transmitter locations and radiation-unaware settings based
on sparse spatial observations. Extensive evaluations on UrbanRadio3D validate
that RadioDiff-3D achieves superior performance in constructing rich,
high-dimensional radio maps under diverse environmental dynamics. This work
provides a foundational dataset and benchmark for future research in 3D
environment-aware communication. The dataset is available at
https://github.com/UNIC-Lab/UrbanRadio3D.

</details>


### [19] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: 论文开发了SurgeryLSTM模型，用于预测脊柱手术住院时长，结合时间建模和可解释性，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 提升脊柱手术住院时长预测的准确性和可解释性，以支持临床决策。

Method: 比较传统模型（如线性回归、随机森林）与SurgeryLSTM（带注意力的BiLSTM），使用电子健康记录数据。

Result: SurgeryLSTM预测准确率最高（R2=0.86），注意力机制增强了可解释性。关键预测因素包括骨病和慢性肾病。

Conclusion: SurgeryLSTM为脊柱手术住院时长预测提供了高效且可解释的解决方案，支持临床决策系统集成。

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [20] [Reinforcement Learning from Adversarial Preferences in Tabular MDPs](https://arxiv.org/abs/2507.11706)
*Taira Tsuchiya,Shinji Ito,Haipeng Luo*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的马尔可夫决策过程（PbMDPs）框架，研究了Borda分数下的遗憾下界，并提出了两种算法以实现次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统MDP中损失函数直接观测，而PbMDPs通过偏好比较选择，更贴近实际场景。研究Borda分数下的PbMDPs填补了理论空白。

Method: 1. 建立Borda分数下PbMDPs的遗憾下界；2. 提出基于全局优化的算法和策略优化算法。

Result: 证明了遗憾下界为Ω((H²SK)^(1/3)T^(2/3))，并提出了两种算法分别达到Õ((H²S²K)^(1/3)T^(2/3))和Õ((H⁶SK⁵)^(1/3)T^(2/3))的遗憾上界。

Conclusion: PbMDPs在Borda分数下具有理论可行性，提出的算法在已知和未知转移情况下均有效。

Abstract: We introduce a new framework of episodic tabular Markov decision processes
(MDPs) with adversarial preferences, which we refer to as preference-based MDPs
(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the
numerical value of the loss is directly observed, in PbMDPs the learner instead
observes preferences between two candidate arms, which represent the choices
being compared. In this work, we focus specifically on the setting where the
reward functions are determined by Borda scores. We begin by establishing a
regret lower bound for PbMDPs with Borda scores. As a preliminary step, we
present a simple instance to prove a lower bound of $\Omega(\sqrt{HSAT})$ for
episodic MDPs with adversarial losses, where $H$ is the number of steps per
episode, $S$ is the number of states, $A$ is the number of actions, and $T$ is
the number of episodes. Leveraging this construction, we then derive a regret
lower bound of $\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda
scores, where $K$ is the number of arms. Next, we develop algorithms that
achieve a regret bound of order $T^{2/3}$. We first propose a global
optimization approach based on online linear optimization over the set of all
occupancy measures, achieving a regret bound of $\tilde{O}((H^2 S^2 K)^{1/3}
T^{2/3} )$ under known transitions. However, this approach suffers from
suboptimal dependence on the potentially large number of states $S$ and
computational inefficiency. To address this, we propose a policy optimization
algorithm whose regret is roughly bounded by $\tilde{O}( (H^6 S K^5)^{1/3}
T^{2/3} )$ under known transitions, and further extend the result to the
unknown-transition setting.

</details>


### [21] [Selective Quantization Tuning for ONNX Models](https://arxiv.org/abs/2507.12196)
*Nikolaos Louloudakis,Ajitha Rajan*

Main category: cs.LG

TL;DR: TuneQn是一个选择性量化和优化工具，用于在保持模型性能的同时减少模型大小和计算需求。


<details>
  <summary>Details</summary>
Motivation: 完全量化的模型可能在精度和部署上存在问题，因此需要选择性量化。

Method: 提出TuneQn工具，结合多目标优化和性能分析，选择性量化ONNX模型并在不同硬件上部署。

Result: 实验显示，TuneQn能减少54.14%的精度损失和72.9%的模型大小。

Conclusion: TuneQn有效解决了选择性量化和模型优化的问题。

Abstract: Quantization is a process that reduces the precision of deep neural network
models to lower model size and computational demands, often at the cost of
accuracy. However, fully quantized models may exhibit sub-optimal performance
below acceptable levels and face deployment challenges on low-end hardware
accelerators due to practical constraints. To address these issues,
quantization can be selectively applied to only a subset of layers, but
selecting which layers to exclude is non-trivial. To this direction, we propose
TuneQn, a suite enabling selective quantization, deployment and execution of
ONNX models across various CPU and GPU devices, combined with profiling and
multi-objective optimization. TuneQn generates selectively quantized ONNX
models, deploys them on different hardware, measures performance on metrics
like accuracy and size, performs Pareto Front minimization to identify the best
model candidate and visualizes the results. To demonstrate the effectiveness of
TuneQn, we evaluated TuneQn on four ONNX models with two quantization settings
across CPU and GPU devices. As a result, we demonstrated that our utility
effectively performs selective quantization and tuning, selecting ONNX model
candidates with up to a $54.14$% reduction in accuracy loss compared to the
fully quantized model, and up to a $72.9$% model size reduction compared to the
original model.

</details>


### [22] [Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning](https://arxiv.org/abs/2507.11732)
*Shiyu Chen,Cencheng Shen,Youngser Park,Carey E. Priebe*

Main category: cs.LG

TL;DR: 论文提出了一种基于统计方法的图编码嵌入（GEE）来初始化图神经网络（GNN）的节点特征，显著提升了GNN的性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: GNN的性能受限于随机或低质量初始特征表示，导致收敛慢和次优解。

Method: 利用GEE生成高质量初始节点特征，结合GNN形成GG框架，并进一步提出GG-C变体用于节点分类。

Result: 在节点聚类任务中，GG在所有真实数据集上表现最佳且收敛更快；GG-C在节点分类中优于基线方法。

Conclusion: 结构感知的特征初始化对发挥GNN潜力至关重要。

Abstract: Graph neural networks (GNNs) have emerged as a powerful framework for a wide
range of node-level graph learning tasks. However, their performance is often
constrained by reliance on random or minimally informed initial feature
representations, which can lead to slow convergence and suboptimal solutions.
In this paper, we leverage a statistically grounded method, one-hot graph
encoder embedding (GEE), to generate high-quality initial node features that
enhance the end-to-end training of GNNs. We refer to this integrated framework
as the GEE-powered GNN (GG), and demonstrate its effectiveness through
extensive simulations and real-world experiments across both unsupervised and
supervised settings. In node clustering, GG consistently achieves
state-of-the-art performance, ranking first across all evaluated real-world
datasets, while exhibiting faster convergence compared to the standard GNN. For
node classification, we further propose an enhanced variant, GG-C, which
concatenates the outputs of GG and GEE and outperforms competing baselines.
These results confirm the importance of principled, structure-aware feature
initialization in realizing the full potential of GNNs.

</details>


### [23] [Einstein Fields: A Neural Perspective To Computational General Relativity](https://arxiv.org/abs/2507.11589)
*Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter*

Main category: cs.LG

TL;DR: Einstein Fields是一种神经表示方法，用于压缩计算密集型四维数值相对论模拟为紧凑的隐式神经网络权重。通过建模广义相对论的核心张量场（度量），它支持通过自动微分推导物理量。


<details>
  <summary>Details</summary>
Motivation: 传统数值相对论模拟计算成本高，Einstein Fields旨在提供一种更高效、可扩展的解决方案。

Method: 提出Neural Tensor Fields（神经张量场），将时空几何编码为神经网络表示，动态自然涌现。

Result: Einstein Fields在4D时空连续建模、存储效率、导数精度等方面表现出潜力，并在多个广义相对论测试中验证。

Conclusion: Einstein Fields为数值相对论提供了更高效和表达性强的工具，开源库进一步推动其应用。

Abstract: We introduce Einstein Fields, a neural representation that is designed to
compress computationally intensive four-dimensional numerical relativity
simulations into compact implicit neural network weights. By modeling the
\emph{metric}, which is the core tensor field of general relativity, Einstein
Fields enable the derivation of physical quantities via automatic
differentiation. However, unlike conventional neural fields (e.g., signed
distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural
Tensor Fields} with the key difference that when encoding the spacetime
geometry of general relativity into neural field representations, dynamics
emerge naturally as a byproduct. Einstein Fields show remarkable potential,
including continuum modeling of 4D spacetime, mesh-agnosticity, storage
efficiency, derivative accuracy, and ease of use. We address these challenges
across several canonical test beds of general relativity and release an open
source JAX-based library, paving the way for more scalable and expressive
approaches to numerical relativity. Code is made available at
https://github.com/AndreiB137/EinFields

</details>


### [24] [Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update](https://arxiv.org/abs/2507.11847)
*Yu-Jie Zhang,Sheng-An Xu,Peng Zhao,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种联合高效的广义线性赌博机（GLB）算法，实现了近乎最优的遗憾边界，且每轮时间和空间复杂度为常数。


<details>
  <summary>Details</summary>
Motivation: 广义线性赌博机（GLB）因其非线性特性在计算和统计效率上存在挑战，现有方法通常需在两者间权衡。

Method: 通过在线镜像下降（OMD）估计器构建紧置信集，利用混合损失的新颖分析实现统计效率。

Result: 算法在每轮时间和空间复杂度为常数的情况下，达到了接近最优的遗憾边界。

Conclusion: 提出的方法在计算和统计效率上均表现优异，适用于广泛的实际场景。

Abstract: We study the generalized linear bandit (GLB) problem, a contextual
multi-armed bandit framework that extends the classical linear model by
incorporating a non-linear link function, thereby modeling a broad class of
reward distributions such as Bernoulli and Poisson. While GLBs are widely
applicable to real-world scenarios, their non-linear nature introduces
significant challenges in achieving both computational and statistical
efficiency. Existing methods typically trade off between two objectives, either
incurring high per-round costs for optimal regret guarantees or compromising
statistical efficiency to enable constant-time updates. In this paper, we
propose a jointly efficient algorithm that attains a nearly optimal regret
bound with $\mathcal{O}(1)$ time and space complexities per round. The core of
our method is a tight confidence set for the online mirror descent (OMD)
estimator, which is derived through a novel analysis that leverages the notion
of mix loss from online prediction. The analysis shows that our OMD estimator,
even with its one-pass updates, achieves statistical efficiency comparable to
maximum likelihood estimation, thereby leading to a jointly efficient
optimistic method.

</details>


### [25] [Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques](https://arxiv.org/abs/2507.11590)
*Raju Challagundla,Mohsen Dorodchi,Pu Wang,Minwoo Lee*

Main category: cs.LG

TL;DR: 综述了合成表格数据生成的最新进展，提出基于实用目标的分类法，并提出了基准框架以连接理论与实际需求。


<details>
  <summary>Details</summary>
Motivation: 隐私法规趋严，真实数据获取受限，合成数据成为解决方案，尤其在表格数据领域。

Method: 提出基于生成目标的分类法，强调方法需保持特征关系、统计保真和隐私要求。

Result: 提出了基准框架，连接技术创新与实际需求，为未来研究和隐私关键环境提供指导。

Conclusion: 该综述为合成表格数据的研究和实际应用提供了路线图和指南。

Abstract: As privacy regulations become more stringent and access to real-world data
becomes increasingly constrained, synthetic data generation has emerged as a
vital solution, especially for tabular datasets, which are central to domains
like finance, healthcare and the social sciences. This survey presents a
comprehensive and focused review of recent advances in synthetic tabular data
generation, emphasizing methods that preserve complex feature relationships,
maintain statistical fidelity, and satisfy privacy requirements. A key
contribution of this work is the introduction of a novel taxonomy based on
practical generation objectives, including intended downstream applications,
privacy guarantees, and data utility, directly informing methodological design
and evaluation strategies. Therefore, this review prioritizes the actionable
goals that drive synthetic data creation, including conditional generation and
risk-sensitive modeling. Additionally, the survey proposes a benchmark
framework to align technical innovation with real-world demands. By bridging
theoretical foundations with practical deployment, this work serves as both a
roadmap for future research and a guide for implementing synthetic tabular data
in privacy-critical environments.

</details>


### [26] [Robust Causal Discovery in Real-World Time Series with Power-Laws](https://arxiv.org/abs/2507.12257)
*Matteo Tusoni,Giuseppe Masi,Andrea Coletta,Aldo Glielmo,Viviana Arrigoni,Novella Bartolini*

Main category: cs.LG

TL;DR: 提出了一种基于幂律谱特征的鲁棒因果发现方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索随机时间序列中的因果关系具有广泛应用，但现有方法对噪声敏感，易导致误导性推断。

Method: 利用真实时间序列的幂律谱分布特征，提取放大真实因果信号的谱特征。

Result: 在合成基准和已知因果结构的真实数据集上表现优于现有方法。

Conclusion: 该方法具有鲁棒性和实际应用价值。

Abstract: Exploring causal relationships in stochastic time series is a challenging yet
crucial task with a vast range of applications, including finance, economics,
neuroscience, and climate science. Many algorithms for Causal Discovery (CD)
have been proposed, but they often exhibit a high sensitivity to noise,
resulting in misleading causal inferences when applied to real data. In this
paper, we observe that the frequency spectra of typical real-world time series
follow a power-law distribution, notably due to an inherent self-organizing
behavior. Leveraging this insight, we build a robust CD method based on the
extraction of power -law spectral features that amplify genuine causal signals.
Our method consistently outperforms state-of-the-art alternatives on both
synthetic benchmarks and real-world datasets with known causal structures,
demonstrating its robustness and practical relevance.

</details>


### [27] [Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification](https://arxiv.org/abs/2507.11620)
*Steven Dillmann,Juan Rafael Martínez-Galarza*

Main category: cs.LG

TL;DR: 论文提出了一种新的张量表示和稀疏自编码器方法，用于处理不规则事件时间序列，支持多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 事件时间序列在多个领域中常见，但其不规则性使得传统方法难以提取有意义模式。

Method: 采用二维和三维张量表示事件时间序列，结合稀疏自编码器学习物理意义明确的潜在表示。

Result: 在X射线天文学数据集上验证了方法的有效性，成功捕获了时间和光谱特征。

Conclusion: 该方法为分析和处理复杂事件时间序列提供了灵活、可扩展的解决方案。

Abstract: Event time series are sequences of discrete events occurring at irregular
time intervals, each associated with a domain-specific observational modality.
They are common in domains such as high-energy astrophysics, computational
social science, cybersecurity, finance, healthcare, neuroscience, and
seismology. Their unstructured and irregular structure poses significant
challenges for extracting meaningful patterns and identifying salient phenomena
using conventional techniques. We propose novel two- and three-dimensional
tensor representations for event time series, coupled with sparse autoencoders
that learn physically meaningful latent representations. These embeddings
support a variety of downstream tasks, including anomaly detection,
similarity-based retrieval, semantic clustering, and unsupervised
classification. We demonstrate our approach on a real-world dataset from X-ray
astronomy, showing that these representations successfully capture temporal and
spectral signatures and isolate diverse classes of X-ray transients. Our
framework offers a flexible, scalable, and generalizable solution for analyzing
complex, irregular event time series across scientific and industrial domains.

</details>


### [28] [A Framework for Nonstationary Gaussian Processes with Neural Network Parameters](https://arxiv.org/abs/2507.12262)
*Zachary James,Joseph Guinness*

Main category: cs.LG

TL;DR: 提出了一种使用非平稳核的高斯过程框架，通过神经网络建模核参数，提升了模型的灵活性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程使用平稳核，限制了模型的表达能力，难以适应复杂数据集。

Method: 将非平稳核参数建模为神经网络的输出，联合训练神经网络和高斯过程。

Result: 在多个数据集上表现优于平稳模型和变分推断层次模型，且能有效恢复非平稳参数。

Conclusion: 该方法灵活、高效，适用于不同非平稳核，且易于扩展到大规��数据集。

Abstract: Gaussian processes have become a popular tool for nonparametric regression
because of their flexibility and uncertainty quantification. However, they
often use stationary kernels, which limit the expressiveness of the model and
may be unsuitable for many datasets. We propose a framework that uses
nonstationary kernels whose parameters vary across the feature space, modeling
these parameters as the output of a neural network that takes the features as
input. The neural network and Gaussian process are trained jointly using the
chain rule to calculate derivatives. Our method clearly describes the behavior
of the nonstationary parameters and is compatible with approximation methods
for scaling to large datasets. It is flexible and easily adapts to different
nonstationary kernels without needing to redesign the optimization procedure.
Our methods are implemented with the GPyTorch library and can be readily
modified. We test a nonstationary variance and noise variant of our method on
several machine learning datasets and find that it achieves better accuracy and
log-score than both a stationary model and a hierarchical model approximated
with variational inference. Similar results are observed for a model with only
nonstationary variance. We also demonstrate our approach's ability to recover
the nonstationary parameters of a spatial dataset.

</details>


### [29] [Deep Generative Methods and Tire Architecture Design](https://arxiv.org/abs/2507.11639)
*Fouad Oubari,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 论文研究了五种深度生成模型在工业轮胎架构生成中的表现，发现扩散模型整体表现最佳，并提出了一种无需额外训练的类别修复方法。


<details>
  <summary>Details</summary>
Motivation: 工业实践中缺乏关于哪种深度生成模型最适合复杂制造设计任务的明确指导。

Method: 研究了五种模型（VAE、GAN、MMVAE、DDPM、MDM），并引入类别修复方法处理条件生成。

Result: 扩散模型整体表现最好，MDM在分布内表现优异，DDPM在分布外约束下泛化能力更强。

Conclusion: 扩散模型在工业轮胎架构生成中表现最优，类别修复方法有效支持条件生成。

Abstract: As deep generative models proliferate across the AI landscape, industrial
practitioners still face critical yet unanswered questions about which deep
generative models best suit complex manufacturing design tasks. This work
addresses this question through a complete study of five representative models
(Variational Autoencoder, Generative Adversarial Network, multimodal
Variational Autoencoder, Denoising Diffusion Probabilistic Model, and
Multinomial Diffusion Model) on industrial tire architecture generation. Our
evaluation spans three key industrial scenarios: (i) unconditional generation
of complete multi-component designs, (ii) component-conditioned generation
(reconstructing architectures from partial observations), and (iii)
dimension-constrained generation (creating designs that satisfy specific
dimensional requirements). To enable discrete diffusion models to handle
conditional scenarios, we introduce categorical inpainting, a mask-aware
reverse diffusion process that preserves known labels without requiring
additional training. Our evaluation employs geometry-aware metrics specifically
calibrated for industrial requirements, quantifying spatial coherence,
component interaction, structural connectivity, and perceptual fidelity. Our
findings reveal that diffusion models achieve the strongest overall
performance; a masking-trained VAE nonetheless outperforms the multimodal
variant MMVAE\textsuperscript{+} on nearly all component-conditioned metrics,
and within the diffusion family MDM leads in-distribution whereas DDPM
generalises better to out-of-distribution dimensional constraints.

</details>


### [30] [ROC-n-reroll: How verifier imperfection affects test-time scaling](https://arxiv.org/abs/2507.12399)
*Florian E. Dorner,Yatong Chen,André F. Cruz,Fanny Yang*

Main category: cs.LG

TL;DR: 论文研究了测试时扩展技术（如Best-of-N和拒绝采样）在验证器不完美时的性能影响，发现其准确性与验证器ROC曲线的几何特性相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对验证器不完美如何影响测试时扩展性能的理论理解，本文填补了这一空白。

Method: 通过理论分析验证器的ROC曲线几何特性，研究其对Best-of-N和拒绝采样性能的影响，并在GSM8K数据集上实验验证。

Result: 拒绝采样在固定计算量下优于Best-of-N，但在无限计算量下两者性能趋同，取决于ROC曲线原点附近的斜率。

Conclusion: 验证器的ROC曲线几何特性是决定测试时扩展性能的关键因素，拒绝采样和Best-of-N在不同计算量下表现不同。

Abstract: Test-time scaling aims to improve language model performance by leveraging
additional compute during inference. While many works have empirically studied
techniques like Best-of-N (BoN) and rejection sampling that make use of a
verifier to enable test-time scaling, there is little theoretical understanding
of how verifier imperfection affects performance. In this work, we address this
gap. Specifically, we prove how instance-level accuracy of these methods is
precisely characterized by the geometry of the verifier's ROC curve.
Interestingly, while scaling is determined by the local geometry of the ROC
curve for rejection sampling, it depends on global properties of the ROC curve
for BoN. As a consequence when the ROC curve is unknown, it is impossible to
extrapolate the performance of rejection sampling based on the low-compute
regime. Furthermore, while rejection sampling outperforms BoN for fixed
compute, in the infinite-compute limit both methods converge to the same level
of accuracy, determined by the slope of the ROC curve near the origin. Our
theoretical results are confirmed by experiments on GSM8K using different
versions of Llama and Qwen to generate and verify solutions.

</details>


### [31] [Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation](https://arxiv.org/abs/2507.11645)
*Ahmed Salah,David Yevick*

Main category: cs.LG

TL;DR: 本文提出几种实用指标（如Dropout鲁棒性、嵌入相似性和稀疏性）来预测神经网络的“顿悟”行为，并揭示其起源和机制。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络在训练后期突然表现提升（顿悟）的现象，并寻找可预测和解释该行为的指标。

Method: 通过Dropout鲁棒性曲线（DRC）、测试精度方差、神经元激活率和嵌入分布等指标分析顿悟行为。

Result: 发现顿悟期间测试精度方差出现局部峰值，神经元激活率增加，嵌入分布趋于双峰且与数据集对称性相关。

Conclusion: 提出的指标能有效预测顿悟行为，并揭示其与网络鲁棒性和数据结构的关联。

Abstract: Grokking refers to delayed generalization in which the increase in test
accuracy of a neural network occurs appreciably after the improvement in
training accuracy This paper introduces several practical metrics including
variance under dropout, robustness, embedding similarity, and sparsity
measures, that can forecast grokking behavior. Specifically, the resilience of
neural networks to noise during inference is estimated from a Dropout
Robustness Curve (DRC) obtained from the variation of the accuracy with the
dropout rate as the model transitions from memorization to generalization. The
variance of the test accuracy under stochastic dropout across training
checkpoints further exhibits a local maximum during the grokking. Additionally,
the percentage of inactive neurons decreases during generalization, while the
embeddings tend to a bimodal distribution independent of initialization that
correlates with the observed cosine similarity patterns and dataset symmetries.
These metrics additionally provide valuable insight into the origin and
behaviour of grokking.

</details>


### [32] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: 提出了一种基于零知识证明的联邦学习隐私保护评估协议，避免通过性能指标泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的评估阶段可能通过共享的性能指标泄露敏感信息，需要一种隐私保护且可验证的评估方法。

Method: 使用零知识证明（ZKPs），客户端生成简洁证明，声明其本地损失低于预定义阈值，无需依赖外部API。

Result: 在MNIST和HAR数据集上验证了方法的有效性，评估了计算开销、通信成本和可验证性。

Conclusion: 提出的协议在保护隐私的同时实现了可验证的联邦学习评估。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [33] [STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics](https://arxiv.org/abs/2507.11660)
*Joao F. Rocha,Ke Xu,Xingzhi Sun,Ananya Krishna,Dhananjay Bhaskar,Blanche Mongeon,Morgan Craig,Mark Gerstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度学习和基于代理的建模（ABM）的方法STAGED，用于模拟细胞间通信及其对细胞内基因调控网络的影响。


<details>
  <summary>Details</summary>
Motivation: 单细胞技术虽能揭示细胞状态和亚群，但传统方法将细胞视为独立数据点，忽视了细胞间的动态相互作用。空间转录组学提供了细胞组织和互动的数据，但需要新的计算方法来学习这种复杂的动态。

Method: STAGED整合ABM与深度学习，使用图ODE网络（GDEs）动态学习基因间的相互作用强度，并通过注意力机制优化。

Result: 模型能够匹配模拟和空间转录组学数据的连续轨迹，捕捉细胞间和细胞内的相互作用，提供更准确的自适应动态表示。

Conclusion: STAGED为研究细胞动态提供了一种数据驱动的自适应方法，弥补了传统ABM依赖手工规则的不足。

Abstract: The advent of single-cell technology has significantly improved our
understanding of cellular states and subpopulations in various tissues under
normal and diseased conditions by employing data-driven approaches such as
clustering and trajectory inference. However, these methods consider cells as
independent data points of population distributions. With spatial
transcriptomics, we can represent cellular organization, along with dynamic
cell-cell interactions that lead to changes in cell state. Still, key
computational advances are necessary to enable the data-driven learning of such
complex interactive cellular dynamics. While agent-based modeling (ABM)
provides a powerful framework, traditional approaches rely on handcrafted rules
derived from domain knowledge rather than data-driven approaches. To address
this, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)
integrating ABM with deep learning to model intercellular communication, and
its effect on the intracellular gene regulatory network. Using graph ODE
networks (GDEs) with shared weights per cell type, our approach represents
genes as vertices and interactions as directed edges, dynamically learning
their strengths through a designed attention mechanism. Trained to match
continuous trajectories of simulated as well as inferred trajectories from
spatial transcriptomics data, the model captures both intercellular and
intracellular interactions, enabling a more adaptive and accurate
representation of cellular dynamics.

</details>


### [34] [Composing Linear Layers from Irreducibles](https://arxiv.org/abs/2507.11688)
*Travis Pence,Daisuke Yamada,Vikas Singh*

Main category: cs.LG

TL;DR: 论文研究了线性层中的几何基元（如双向量）如何组合成更高级的功能，提出了一种基于Clifford代数的可微分算法，将线性变换分解为转子乘积，参数效率高。


<details>
  <summary>Details</summary>
Motivation: 理解大型模型中的低层级基元如何组合成模块化功能，探索线性层的几何结构。

Method: 使用Clifford代数将线性层表示为双向量组合，并开发可微分算法分解为转子乘积，参数复杂度为O(log²d)。

Result: 在LLM注意力层中，基于转子的层性能与强基线（如块Hadamard和低秩近似）相当。

Conclusion: 研究为深度模型中几何基元的组合提供了代数视角，展示了高效参数化的潜力。

Abstract: Contemporary large models often exhibit behaviors suggesting the presence of
low-level primitives that compose into modules with richer functionality, but
these fundamental building blocks remain poorly understood. We investigate this
compositional structure in linear layers by asking: can we identify/synthesize
linear transformations from a minimal set of geometric primitives? Using
Clifford algebra, we show that linear layers can be expressed as compositions
of bivectors -- geometric objects encoding oriented planes -- and introduce a
differentiable algorithm that decomposes them into products of rotors. This
construction uses only O(log^2 d) parameters, versus O(d^2) required by dense
matrices. Applied to the key, query, and value projections in LLM attention
layers, our rotor-based layers match the performance of strong baselines such
as block-Hadamard and low-rank approximations. Our findings provide an
algebraic perspective on how these geometric primitives can compose into
higher-level functions within deep models.

</details>


### [35] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: 本文分析了数据选择方法（如核心集选择）对数据集偏见和下游模型鲁棒性的影响，揭示了样本难度与偏见对齐之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究核心集选择方法是否会加剧或缓解数据集中的偏见，以及这些方法对下游模型鲁棒性的影响。

Method: 在十个不同的伪相关基准上，使用五种评分指标和五种数据选择策略，进行广泛的实验分析。

Result: 发现基于嵌入的样本评分方法比基于学习动态的方法更不易加剧偏见，但优先选择困难样本的方法并不能可靠保证模型鲁棒性。

Conclusion: 核心集选择方法在减少偏见方面存在局限性，需谨慎设计以避免意外加剧偏见。

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [36] [Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption](https://arxiv.org/abs/2507.11702)
*Hein de Wilde,Ali Mohammed Mansoor Alsahag,Pierre Blanchet*

Main category: cs.LG

TL;DR: 英国铁路因落叶导致每年损失3亿英镑，现有预测方法存在局限性。本研究利用LSTM网络结合卫星数据，显著提升了落叶时间预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 落叶导致铁路交通中断，每年造成巨大经济损失，亟需更准确的预测方法来优化应对措施。

Method: 采用LSTM网络，结合地面落叶数据和多光谱气象卫星数据，预测落叶时间。

Result: 模型预测落叶开始和结束时间的均方根误差分别为6.32天和9.31天，优于现有方法。

Conclusion: 该模型为铁路行业优化落叶应对措施提供了有效工具，并有助于理解复杂生态系统。

Abstract: Railroad traffic disruption as a result of leaf-fall cost the UK rail
industry over 300 million per year and measures to mitigate such disruptions
are employed on a large scale, with 1.67 million kilometers of track being
treated in the UK in 2021 alone. Therefore, the ability to anticipate the
timing of leaf-fall would offer substantial benefits for rail network
operators, enabling the efficient scheduling of such mitigation measures.
However, current methodologies for predicting leaf-fall exhibit considerable
limitations in terms of scalability and reliability. This study endeavors to
devise a prediction system that leverages specialized prediction methods and
the latest satellite data sources to generate both scalable and reliable
insights into leaf-fall timings. An LSTM network trained on ground-truth
leaf-falling data combined with multispectral and meteorological satellite data
demonstrated a root-mean-square error of 6.32 days for predicting the start of
leaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which
improves upon previous work on the topic, offers promising opportunities for
the optimization of leaf mitigation measures in the railway industry and the
improvement of our understanding of complex ecological systems.

</details>


### [37] [Subgraph Generation for Generalizing on Out-of-Distribution Links](https://arxiv.org/abs/2507.11710)
*Jay Revolinsky,Harry Shomer,Jiliang Tang*

Main category: cs.LG

TL;DR: FLEX是一个图生成模型框架，通过结构条件生成和对抗训练提升图神经网络的链接预测性能，适用于分布外场景。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在链接预测任务中表现优异，但依赖同分布数据；图生成模型虽能生成新图，但应用受限。FLEX旨在结合两者优势，提升分布外场景下的性能。

Method: FLEX采用两种机制：结构条件图生成和自动编码器与图神经网络的对抗协同训练，确保样本分布的结构对齐。

Result: 实验表明，FLEX在合成和真实世界的分布外场景中显著提升了链接预测性能，并分析了图数据增强对链接结构的影响。

Conclusion: FLEX无需专家知识即可适用于不同分布外场景，为图生成模型的应用提供了新思路。

Abstract: Graphs Neural Networks (GNNs) demonstrate high-performance on the link
prediction (LP) task. However, these models often rely on all dataset samples
being drawn from the same distribution. In addition, graph generative models
(GGMs) show a pronounced ability to generate novel output graphs. Despite this,
GGM applications remain largely limited to domain-specific tasks. To bridge
this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)
structurally-conditioned graph generation, and (2) adversarial co-training
between an auto-encoder and GNN. As such, FLEX ensures structural-alignment
between sample distributions to enhance link-prediction performance in
out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert
knowledge to function in different OOD scenarios. Numerous experiments are
conducted in synthetic and real-world OOD settings to demonstrate FLEX's
performance-enhancing ability, with further analysis for understanding the
effects of graph data augmentation on link structures. The source code is
available here: https://github.com/revolins/FlexOOD.

</details>


### [38] [Globalization for Scalable Short-term Load Forecasting](https://arxiv.org/abs/2507.11729)
*Amirhossein Ahmadi,Hamidreza Zareipour,Henry Leung*

Main category: cs.LG

TL;DR: 本文研究了电力传输网络中的全局负荷预测方法，探讨了数据漂移、建模技术和数据异质性的影响，并提出了两种时间序列聚类方法以优化预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统局部预测模型在泛化性、过拟合、数据漂移和冷启动问题上存在局限性，而全局预测模型通过全局化和交叉学习提供了更好的解决方案。

Method: 研究了特征转换和目标转换模型，提出了基于模型的时间序列聚类和加权实例聚类方法，并在真实数据集上进行了实验验证。

Result: 全局目标转换模型在全局特征和聚类技术支持下表现优于局部模型，而全局特征转换模型需要时间序列聚类来平衡局部与全局动态。

Conclusion: 全局目标转换模型在负荷预测中更具优势，而特征转换模型需结合聚类技术以应对数据异质性。

Abstract: Forecasting load in power transmission networks is essential across various
hierarchical levels, from the system level down to individual points of
delivery (PoD). While intuitive and locally accurate, traditional local
forecasting models (LFMs) face significant limitations, particularly in
handling generalizability, overfitting, data drift, and the cold start problem.
These methods also struggle with scalability, becoming computationally
expensive and less efficient as the network's size and data volume grow. In
contrast, global forecasting models (GFMs) offer a new approach to enhance
prediction generalizability, scalability, accuracy, and robustness through
globalization and cross-learning. This paper investigates global load
forecasting in the presence of data drifts, highlighting the impact of
different modeling techniques and data heterogeneity. We explore
feature-transforming and target-transforming models, demonstrating how
globalization, data heterogeneity, and data drift affect each differently. In
addition, we examine the role of globalization in peak load forecasting and its
potential for hierarchical forecasting. To address data heterogeneity and the
balance between globality and locality, we propose separate time series
clustering (TSC) methods, introducing model-based TSC for feature-transforming
models and new weighted instance-based TSC for target-transforming models.
Through extensive experiments on a real-world dataset of Alberta's electricity
load, we demonstrate that global target-transforming models consistently
outperform their local counterparts, especially when enriched with global
features and clustering techniques. In contrast, global feature-transforming
models face challenges in balancing local and global dynamics, often requiring
TSC to manage data heterogeneity effectively.

</details>


### [39] [HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD](https://arxiv.org/abs/2507.12133)
*Hanwen Liu,Yuhe Huang,Yifeng Gong,Yanjie Zhai,Jiaxuan Lu*

Main category: cs.LG

TL;DR: HyDRA是一种混合双模RF架构，结合优化的VMD和新型CNN、Transformer与Mamba融合架构，支持闭集和开集分类任务，在无线设备识别中实现高效实时认证。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统中的设备识别对安全至关重要，尤其是访问控制等应用。RFFI提供了一种非加密解决方案，通过硬件引起的信号失真实现识别。

Method: HyDRA结合优化的VMD预处理和CNN、Transformer、Mamba融合架构，使用TDSE和MLFE分别建模全局依赖和线性复杂度处理。

Result: 在公开数据集上，HyDRA在闭集场景中达到SOTA精度，开集分类方法表现稳健，并在NVIDIA Jetson Xavier NX上实现毫秒级推理速度和低功耗。

Conclusion: HyDRA为实时无线认证提供了高效实用的解决方案，适用于实际环境。

Abstract: Device recognition is vital for security in wireless communication systems,
particularly for applications like access control. Radio Frequency Fingerprint
Identification (RFFI) offers a non-cryptographic solution by exploiting
hardware-induced signal distortions. This paper proposes HyDRA, a Hybrid
Dual-mode RF Architecture that integrates an optimized Variational Mode
Decomposition (VMD) with a novel architecture based on the fusion of
Convolutional Neural Networks (CNNs), Transformers, and Mamba components,
designed to support both closed-set and open-set classification tasks. The
optimized VMD enhances preprocessing efficiency and classification accuracy by
fixing center frequencies and using closed-form solutions. HyDRA employs the
Transformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and
the Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting
to varying conditions. Evaluation on public datasets demonstrates
state-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance
in our proposed open-set classification method, effectively identifying
unauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves
millisecond-level inference speed with low power consumption, providing a
practical solution for real-time wireless authentication in real-world
environments.

</details>


### [40] [Sparse Identification of Nonlinear Dynamics with Conformal Prediction](https://arxiv.org/abs/2507.11739)
*Urban Fasel*

Main category: cs.LG

TL;DR: 论文提出将Conformal Prediction框架与Ensemble-SINDy结合，用于量化非线性动力学模型的不确定性，并在时间序列预测、特征重要性模型选择和模型系数不确定性量化中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化SINDy模型的不确定性对其在安全关键应用中的可靠性评估至关重要。

Method: 集成Conformal Prediction与Ensemble-SINDy（E-SINDy），应用于时间序列预测、特征重要性模型选择和模型系数不确定性量化。

Result: 在随机捕食者-猎物动力学和混沌系统中验证，Conformal Prediction方法能可靠实现目标覆盖范围，有效量化特征重要性，并在非高斯噪声下提供更稳健的系数不确定性区间。

Conclusion: Conformal Prediction与E-SINDy结合能显著提升模型不确定性的量化效果，适用于复杂非线性系统。

Abstract: The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for
discovering nonlinear dynamical system models from data. Quantifying
uncertainty in SINDy models is essential for assessing their reliability,
particularly in safety-critical applications. While various uncertainty
quantification methods exist for SINDy, including Bayesian and ensemble
approaches, this work explores the integration of Conformal Prediction, a
framework that can provide valid prediction intervals with coverage guarantees
based on minimal assumptions like data exchangeability. We introduce three
applications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)
quantifying uncertainty in time series prediction, (2) model selection based on
library feature importance, and (3) quantifying the uncertainty of identified
model coefficients using feature conformal prediction. We demonstrate the three
applications on stochastic predator-prey dynamics and several chaotic dynamical
systems. We show that conformal prediction methods integrated with E-SINDy can
reliably achieve desired target coverage for time series forecasting,
effectively quantify feature importance, and produce more robust uncertainty
intervals for model coefficients, even under non-Gaussian noise, compared to
standard E-SINDy coefficient estimates.

</details>


### [41] [A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction](https://arxiv.org/abs/2507.11757)
*Yuehua Song,Yong Gao*

Main category: cs.LG

TL;DR: 提出了一种名为Graph-in-Graph（GiG）的新框架，结合转导学习和归纳学习，显著提升了药物-靶标相互作用（DTI）预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的DTI预测方法难以有效整合药物、靶标及其相互作用的多样化特征。

Method: GiG框架将药物和靶标分子结构图表示为药物-靶标相互作用图中的元节点，结合转导学习和归纳学习，探索其复杂关系。

Result: GiG模型在多个评估指标上显著优于现有方法。

Conclusion: GiG框架通过整合不同学习范式和相互作用数据，为DTI预测提供了更优的解决方案。

Abstract: Accurately predicting drug-target interactions (DTIs) is pivotal for
advancing drug discovery and target validation techniques. While machine
learning approaches including those that are based on Graph Neural Networks
(GNN) have achieved notable success in DTI prediction, many of them have
difficulties in effectively integrating the diverse features of drugs, targets
and their interactions. To address this limitation, we introduce a novel
framework to take advantage of the power of both transductive learning and
inductive learning so that features at molecular level and drug-target
interaction network level can be exploited. Within this framework is a
GNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and
target molecular structures as meta-nodes in a drug-target interaction graph,
enabling a detailed exploration of their intricate relationships. To evaluate
the proposed model, we have compiled a special benchmark comprising drug
SMILES, protein sequences, and their interaction data, which is interesting in
its own right. Our experimental results demonstrate that the GiG model
significantly outperforms existing approaches across all evaluation metrics,
highlighting the benefits of integrating different learning paradigms and
interaction data.

</details>


### [42] [Torsional-GFN: a conditional conformation generator for small molecules](https://arxiv.org/abs/2507.11759)
*Alexandra Volokhova,Léna Néhale Ezzine,Piotr Gaiński,Luca Scimeca,Emmanuel Bengio,Prudencio Tossou,Yoshua Bengio,Alex Hernandez-Garcia*

Main category: cs.LG

TL;DR: Torsional-GFN是一种基于GFlowNet的生成模型，用于从玻尔兹曼分布中采样分子构象，仅需奖励函数作为训练信号。


<details>
  <summary>Details</summary>
Motivation: 在药物发现中，生成稳定的分子构象对估计分子与靶标的结合亲和力至关重要。传统方法如分子动力学效率较低，而生成式机器学习方法更具潜力。

Method: Torsional-GFN是一种条件GFlowNet，基于分子图和局部结构（键长和键角）采样扭转角的旋转。

Result: Torsional-GFN能够近似玻尔兹曼分布采样分子构象，并实现对新键长和键角的零样本泛化。

Conclusion: 该方法为扩展到更大分子系统、实现对新分子的零样本泛化以及生成局部结构提供了可能。

Abstract: Generating stable molecular conformations is crucial in several drug
discovery applications, such as estimating the binding affinity of a molecule
to a target. Recently, generative machine learning methods have emerged as a
promising, more efficient method than molecular dynamics for sampling of
conformations from the Boltzmann distribution. In this paper, we introduce
Torsional-GFN, a conditional GFlowNet specifically designed to sample
conformations of molecules proportionally to their Boltzmann distribution,
using only a reward function as training signal. Conditioned on a molecular
graph and its local structure (bond lengths and angles), Torsional-GFN samples
rotations of its torsion angles. Our results demonstrate that Torsional-GFN is
able to sample conformations approximately proportional to the Boltzmann
distribution for multiple molecules with a single model, and allows for
zero-shot generalization to unseen bond lengths and angles coming from the MD
simulations for such molecules. Our work presents a promising avenue for
scaling the proposed approach to larger molecular systems, achieving zero-shot
generalization to unseen molecules, and including the generation of the local
structure into the GFlowNet model.

</details>


### [43] [Scaling laws for activation steering with Llama 2 models and refusal mechanisms](https://arxiv.org/abs/2507.11771)
*Sheikh Abdur Raheem Ali,Justin Xu,Ivory Yang,Jasmine Xinze Li,Ayse Arslan,Clark Benham*

Main category: cs.LG

TL;DR: 研究了对比激活加法（CAA）在Llama 2模型（7B、13B、70B）上的效果，发现其在早期到中层最有效，但随着模型规模增大效果减弱，且负向引导比正向引导更显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）能力增强，较少广泛部署的对齐技术效果尚不明确，因此探索CAA在不同规模模型上的有效性。

Method: 通过对比对（如仇恨到爱）在残差流向量空间中找到方向，并在前向传播时添加该方向，直接操控残差流以提取特征控制输出。

Result: 1）CAA在早期到中层最有效；2）CAA效果随模型规模增大而减弱；3）负向引导在所有模型规模中效果更显著。

Conclusion: CAA的效果受模型规模和引导方向影响，需进一步研究以优化对齐技术。

Abstract: As large language models (LLMs) evolve in complexity and capability, the
efficacy of less widely deployed alignment techniques are uncertain. Building
on previous work on activation steering and contrastive activation addition
(CAA), this paper explores the effectiveness of CAA with model scale using the
family of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable
'directions' in the model's residual stream vector space using contrastive
pairs (for example, hate to love) and adding this direction to the residual
stream during the forward pass. It directly manipulates the residual stream and
aims to extract features from language models to better control their outputs.
Using answer matching questions centered around the refusal behavior, we found
that 1) CAA is most effective when applied at early-mid layers. 2) The
effectiveness of CAA diminishes with model size. 3) Negative steering has more
pronounced effects than positive steering across all model sizes.

</details>


### [44] [Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network](https://arxiv.org/abs/2507.11776)
*Merel Kampere,Ali Mohammed Mansoor Alsahag*

Main category: cs.LG

TL;DR: 研究使用XGBoost分类器结合拓扑特征预测荷兰铁路网络延误，填补了现有研究的空白，但结果表现有限。


<details>
  <summary>Details</summary>
Motivation: 荷兰铁路网络是全球最繁忙的铁路之一，延误问题突出。现有研究多关注短期预测，忽略了网络范围的模式，本研究旨在填补这一空白。

Method: 改进现有方法（原用于美国航空网络），结合节点中心性度量，比较多种分类器（如随机森林、决策树等）预测延误轨迹。

Result: 结果表现有限，尤其在非同步测试场景中，表明需要更多上下文特定调整。

Conclusion: 研究为交通网络评估提供了新见解，并提出了未来开发更鲁棒预测模型的方向。

Abstract: The Dutch railway network is one of the busiest in the world, with delays
being a prominent concern for the principal passenger railway operator NS. This
research addresses a gap in delay prediction studies within the Dutch railway
network by employing an XGBoost Classifier with a focus on topological
features. Current research predominantly emphasizes short-term predictions and
neglects the broader network-wide patterns essential for mitigating ripple
effects. This research implements and improves an existing methodology,
originally designed to forecast the evolution of the fast-changing US air
network, to predict delays in the Dutch Railways. By integrating Node
Centrality Measures and comparing multiple classifiers like RandomForest,
DecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is
to predict delayed trajectories. However, the results reveal limited
performance, especially in non-simultaneous testing scenarios, suggesting the
necessity for more context-specific adaptations. Regardless, this research
contributes to the understanding of transportation network evaluation and
proposes future directions for developing more robust predictive models for
delays.

</details>


### [45] [Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation](https://arxiv.org/abs/2507.11789)
*Alessandro Palma,Sergei Rybakov,Leon Hetzel,Stephan Günnemann,Fabian J. Theis*

Main category: cs.LG

TL;DR: FlatVI是一种新的训练框架，通过正则化离散似然变分自编码器的潜在流形，使其更接近欧几里得几何，从而优化单细胞RNA测序数据的建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单细胞RNA测序中假设线性转移和欧几里得几何，但线性插值可能不符合数据流形上的测地线路径，限制了方法的效果。

Method: FlatVI通过正则化潜在流形，使其更接近欧几里得几何，从而优化单细胞数据的建模。

Result: 实验证明FlatVI在合成数据和单细胞RNA测序数据中提高了轨迹重建和流形插值的准确性。

Conclusion: FlatVI通过优化潜在流形的几何特性，提升了单细胞数据建模的效果，适用于下游分析。

Abstract: Latent space interpolations are a powerful tool for navigating deep
generative models in applied settings. An example is single-cell RNA
sequencing, where existing methods model cellular state transitions as latent
space interpolations with variational autoencoders, often assuming linear
shifts and Euclidean geometry. However, unless explicitly enforced, linear
interpolations in the latent space may not correspond to geodesic paths on the
data manifold, limiting methods that assume Euclidean geometry in the data
representations. We introduce FlatVI, a novel training framework that
regularises the latent manifold of discrete-likelihood variational autoencoders
towards Euclidean geometry, specifically tailored for modelling single-cell
count data. By encouraging straight lines in the latent space to approximate
geodesic interpolations on the decoded single-cell manifold, FlatVI enhances
compatibility with downstream approaches that assume Euclidean latent geometry.
Experiments on synthetic data support the theoretical soundness of our
approach, while applications to time-resolved single-cell RNA sequencing data
demonstrate improved trajectory reconstruction and manifold interpolation.

</details>


### [46] [CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels](https://arxiv.org/abs/2507.11807)
*Ruofan Hu,Dongyu Zhang,Huayi Zhang,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 论文提出了一种无需依赖干净标注数据的元学习方法CLID-MU，用于处理噪声标签场景，通过跨层信息差异评估模型性能并指导训练。


<details>
  <summary>Details</summary>
Motivation: 现有元学习方法依赖干净标注数据集，但在实践中难以获取。本文旨在解决无干净标注数据时的噪声标签学习问题。

Method: 提出CLID-MU策略，利用干净样本在隐藏层和输出层数据结构的一致性，通过跨层信息差异评估模型性能并指导训练。

Result: 在合成和真实噪声场景下，CLID-MU在基准数据集上优于现有方法。

Conclusion: CLID-MU有效解决了无干净标注数据时的噪声标签学习问题，性能优于现有方法。

Abstract: Learning with noisy labels (LNL) is essential for training deep neural
networks with imperfect data. Meta-learning approaches have achieved success by
using a clean unbiased labeled set to train a robust model. However, this
approach heavily depends on the availability of a clean labeled meta-dataset,
which is difficult to obtain in practice. In this work, we thus tackle the
challenge of meta-learning for noisy label scenarios without relying on a clean
labeled dataset. Our approach leverages the data itself while bypassing the
need for labels. Building on the insight that clean samples effectively
preserve the consistency of related data structures across the last hidden and
the final layer, whereas noisy samples disrupt this consistency, we design the
Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU).
CLID-MU leverages the alignment of data structures across these diverse feature
spaces to evaluate model performance and use this alignment to guide training.
Experiments on benchmark datasets with varying amounts of labels under both
synthetic and real-world noise demonstrate that CLID-MU outperforms
state-of-the-art methods. The code is released at
https://github.com/ruofanhu/CLID-MU.

</details>


### [47] [SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling](https://arxiv.org/abs/2507.11818)
*Andrei Rekesh,Miruna Cretu,Dmytro Shevchuk,Vignesh Ram Somnath,Pietro Liò,Robert A. Batey,Mike Tyers,Michał Koziarski,Cheng-Hao Liu*

Main category: cs.LG

TL;DR: SynCoGen是一个结合掩码图扩散和流匹配的框架，用于生成可合成的3D分子，在无条件小分子图和构象生成中表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成可合成的小分子设计仍具挑战性，现有方法多局限于2D分子图表示，限制了基于几何的条件生成能力。

Method: 提出SynCoGen框架，结合掩码图扩散和流匹配，从分子构建块、化学反应和原子坐标的联合分布中采样。

Result: 在无条件小分子图和构象生成中达到SOTA，并在零射击分子连接设计中表现优异。

Conclusion: 该多模态框架为未来非自回归分子生成应用（如类似物扩展、先导优化）奠定了基础。

Abstract: Ensuring synthesizability in generative small molecule design remains a major
challenge. While recent developments in synthesizable molecule generation have
demonstrated promising results, these efforts have been largely confined to 2D
molecular graph representations, limiting the ability to perform geometry-based
conditional generation. In this work, we present SynCoGen (Synthesizable
Co-Generation), a single framework that combines simultaneous masked graph
diffusion and flow matching for synthesizable 3D molecule generation. SynCoGen
samples from the joint distribution of molecular building blocks, chemical
reactions, and atomic coordinates. To train the model, we curated SynSpace, a
dataset containing over 600K synthesis-aware building block graphs and 3.3M
conformers. SynCoGen achieves state-of-the-art performance in unconditional
small molecule graph and conformer generation, and the model delivers
competitive performance in zero-shot molecular linker design for protein ligand
generation in drug discovery. Overall, this multimodal formulation represents a
foundation for future applications enabled by non-autoregressive molecular
generation, including analog expansion, lead optimization, and direct structure
conditioning.

</details>


### [48] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen是一个自动化框架，用于生成特定领域的MNIST风格数据集，解决了标准数据集在特定任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 标准数据集（如MNIST）在特定领域任务中不适用，且创建自定义数据集耗时且复杂。

Method: 结合CLIP语义理解、强化学习和人类反馈，通过分层语义分类生成数据集。

Result: 生成两个新数据集（Tree-MNIST和Food-MNIST），自动分类准确率达85%，节省80%时间。

Conclusion: MNIST-Gen为特定任务提供高效的数据集生成工具，具有模块化和可扩展性。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [49] [HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction](https://arxiv.org/abs/2507.11836)
*Jian Gao,Jianshe Wu,JingYi Ding*

Main category: cs.LG

TL;DR: 论文提出HyperEvent框架，将动态链接预测重构为超事件识别，通过事件相关性向量动态构建关联序列，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉复合超事件的结构凝聚力，需解决动态图中链接预测的局限性。

Method: 提出HyperEvent框架，利用事件相关性向量动态构建关联序列，评估查询事件与历史事件是否形成有效超事件。

Result: 在5个数据集中4个表现最优，大规模Flight数据集上MRR提升6.95%，训练时间仅需10.17%。

Conclusion: HyperEvent在准确性和效率上均优于现有方法，适用于大规模动态图。

Abstract: Dynamic link prediction in continuous-time dynamic graphs is a fundamental
task for modeling evolving complex systems. Existing node-centric and
event-centric methods focus on individual interactions or atomic states,
failing to capture the structural cohesion of composite hyper-events, groups of
causally related events. To address this, we propose HyperEvent, a framework
reframing dynamic link prediction as hyper-event recognition. Central to
HyperEvent is the dynamic construction of an association sequence using event
correlation vectors. These vectors quantify pairwise dependencies between the
query event and relevant historical events, thereby characterizing the
structural cohesion of a potential hyper-event. The framework predicts the
occurrence of the query event by evaluating whether it collectively forms a
valid hyper-event with these historical events. Notably, HyperEvent outperforms
state-of-the-art methods on 4 out of 5 datasets in the official leaderboard.
For scalability, we further introduce an efficient parallel training algorithm
that segments large event streams to enable concurrent training. Experiments
validate HyperEvent's superior accuracy and efficiency on large-scale graphs.
Among which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank
over state-of-the-art baseline on the large-scale Flight dataset while
utilizing only 10.17% of the training time.

</details>


### [50] [Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM](https://arxiv.org/abs/2507.11839)
*Chengyue Gong,Xinshi Chen,Yuxuan Zhang,Yuxuan Song,Hao Zhou,Wenzhi Xiao*

Main category: cs.LG

TL;DR: Protenix-Mini是一个轻量化的蛋白质结构预测模型，通过优化架构和采样策略，显著降低计算开销，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决生物分子结构预测中模型效率与预测精度之间的平衡问题，以支持大规模应用的高效部署。

Method: 1) 用少步ODE采样器替代多步AF3采样器；2) 剪枝冗余的Transformer模块；3) 用ESM模块替代传统MSA模块。

Result: 在基准数据集上，Protenix-Mini仅比完整模型性能下降1-5%，但显著降低了计算复杂度。

Conclusion: Protenix-Mini适用于计算资源有限但需要高精度预测的场景。

Abstract: Lightweight inference is critical for biomolecular structure prediction and
other downstream tasks, enabling efficient real-world deployment and
inference-time scaling for large-scale applications. In this work, we address
the challenge of balancing model efficiency and prediction accuracy by making
several key modifications, 1) Multi-step AF3 sampler is replaced by a few-step
ODE sampler, significantly reducing computational overhead for the diffusion
module part during inference; 2) In the open-source Protenix framework, a
subset of pairformer or diffusion transformer blocks doesn't make contributions
to the final structure prediction, presenting opportunities for architectural
pruning and lightweight redesign; 3) A model incorporating an ESM module is
trained to substitute the conventional MSA module, reducing MSA preprocessing
time. Building on these key insights, we present Protenix-Mini, a compact and
optimized model designed for efficient protein structure prediction. This
streamlined version incorporates a more efficient architectural design with a
two-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating
redundant Transformer components and refining the sampling process,
Protenix-Mini significantly reduces model complexity with slight accuracy drop.
Evaluations on benchmark datasets demonstrate that it achieves high-fidelity
predictions, with only a negligible 1 to 5 percent decrease in performance on
benchmark datasets compared to its full-scale counterpart. This makes
Protenix-Mini an ideal choice for applications where computational resources
are limited but accurate structure prediction remains crucial.

</details>


### [51] [OrdShap: Feature Position Importance for Sequential Black-Box Models](https://arxiv.org/abs/2507.11855)
*Davin Hill,Brian L. Hill,Aria Masoomi,Vijay S. Nori,Robert E. Tillman,Jennifer Dy*

Main category: cs.LG

TL;DR: OrdShap是一种新的特征归因方法，通过量化特征位置对模型预测的影响，解决了现有方法中特征值与位置混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法假设特征顺序固定，无法区分特征值与其位置对模型预测的影响。

Method: 提出OrdShap方法，通过排列特征位置来量化其对预测的影响，并与Sanchez-Bergantiños值建立理论联系。

Result: 在健康、自然语言和合成数据集上的实验表明，OrdShap能有效捕捉特征值和位置的影响，提供更深入的模型行为理解。

Conclusion: OrdShap为理解序列深度学习模型提供了更细粒度的归因方法，解决了特征值与位置混淆的问题。

Abstract: Sequential deep learning models excel in domains with temporal or sequential
dependencies, but their complexity necessitates post-hoc feature attribution
methods for understanding their predictions. While existing techniques quantify
feature importance, they inherently assume fixed feature ordering - conflating
the effects of (1) feature values and (2) their positions within input
sequences. To address this gap, we introduce OrdShap, a novel attribution
method that disentangles these effects by quantifying how a model's predictions
change in response to permuting feature position. We establish a game-theoretic
connection between OrdShap and Sanchez-Berganti\~nos values, providing a
theoretically grounded approach to position-sensitive attribution. Empirical
results from health, natural language, and synthetic datasets highlight
OrdShap's effectiveness in capturing feature value and feature position
attributions, and provide deeper insight into model behavior.

</details>


### [52] [A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers](https://arxiv.org/abs/2507.11865)
*Hanwen Dai,Chang Gao,Fang He,Congyuan Ji,Yanni Yang*

Main category: cs.LG

TL;DR: 研究提出了一种动态管理司机接受折扣快车服务的方法，采用pi-DDPG框架解决高随机性和数据不足的问题，并通过仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 平台整合虽能减少市场碎片化，但折扣快车服务可能降低利润。研究旨在动态管理司机参与行为，平衡需求扩展与利润损失。

Method: 提出pi-DDPG框架，包含精炼模块、卷积LSTM网络和优先经验回放机制，以应对高随机性和数据不足。

Result: pi-DDPG在仿真中表现出高效学习能力，显著减少早期训练损失。

Conclusion: pi-DDPG框架能有效管理司机参与行为，为平台提供可靠决策支持。

Abstract: The rapid expansion of platform integration has emerged as an effective
solution to mitigate market fragmentation by consolidating multiple
ride-hailing platforms into a single application. To address heterogeneous
passenger preferences, third-party integrators provide Discount Express service
delivered by express drivers at lower trip fares. For the individual platform,
encouraging broader participation of drivers in Discount Express services has
the potential to expand the accessible demand pool and improve matching
efficiency, but often at the cost of reduced profit margins. This study aims to
dynamically manage drivers' acceptance of Discount Express from the perspective
of individual platforms. The lack of historical data under the new business
model necessitates online learning. However, early-stage exploration through
trial and error can be costly in practice, highlighting the need for reliable
early-stage performance in real-world deployment. To address these challenges,
this study formulates the decision regarding the proportion of drivers'
acceptance behavior as a continuous control task. In response to the high
stochasticity, the opaque matching mechanisms employed by third-party
integrator, and the limited availability of historical data, we propose a
policy-improved deep deterministic policy gradient (pi-DDPG) framework. The
proposed framework incorporates a refiner module to boost policy performance
during the early training phase, leverages a convolutional long short-term
memory network to effectively capture complex spatiotemporal patterns, and
adopts a prioritized experience replay mechanism to enhance learning
efficiency. A simulator based on a real-world dataset is developed to validate
the effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate
that pi-DDPG achieves superior learning efficiency and significantly reduces
early-stage training losses.

</details>


### [53] [Imbalanced Regression Pipeline Recommendation](https://arxiv.org/abs/2507.11901)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 论文提出了一种名为Meta-IR的元学习框架，用于解决回归任务中的不平衡问题，通过训练元分类器推荐最佳的数据预处理和学习模型组合。


<details>
  <summary>Details</summary>
Motivation: 回归任务中目标值稀少导致的不平衡问题需要多种预处理和学习模型的组合，但现有方法需要大量测试，效率低下。

Method: 提出Meta-IR框架，训练元分类器基于元特征推荐最佳组合，包括独立和链式两种形式。

Result: 链式方法表现更优，Meta-IR在实验中优于AutoML框架和42种基线配置。

Conclusion: Meta-IR通过元学习有效解决了回归任务中的不平衡问题，链式方法展示了学习算法与重采样策略之间的关系。

Abstract: Imbalanced problems are prevalent in various real-world scenarios and are
extensively explored in classification tasks. However, they also present
challenges for regression tasks due to the rarity of certain target values. A
common alternative is to employ balancing algorithms in preprocessing to
address dataset imbalance. However, due to the variety of resampling methods
and learning models, determining the optimal solution requires testing many
combinations. Furthermore, the learning model, dataset, and evaluation metric
affect the best strategies. This work proposes the Meta-learning for Imbalanced
Regression (Meta-IR) framework, which diverges from existing literature by
training meta-classifiers to recommend the best pipeline composed of the
resampling strategy and learning model per task in a zero-shot fashion. The
meta-classifiers are trained using a set of meta-features to learn how to map
the meta-features to the classes indicating the best pipeline. We propose two
formulations: Independent and Chained. Independent trains the meta-classifiers
to separately indicate the best learning algorithm and resampling strategy.
Chained involves a sequential procedure where the output of one meta-classifier
is used as input for another to model intrinsic relationship factors. The
Chained scenario showed superior performance, suggesting a relationship between
the learning algorithm and the resampling strategy per task. Compared with
AutoML frameworks, Meta-IR obtained better results. Moreover, compared with
baselines of six learning algorithms and six resampling algorithms plus no
resampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of
them. The code, data, and further information of the experiments can be found
on GitHub: https://github.com/JusciAvelino/Meta-IR.

</details>


### [54] [Resampling strategies for imbalanced regression: a survey and empirical analysis](https://arxiv.org/abs/2507.11902)
*Juscimara G. Avelino,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 该论文研究了不平衡回归问题，提出了基于回归模型、学习过程和评估指标的分类法，并通过实验验证了不同平衡策略的效果。


<details>
  <summary>Details</summary>
Motivation: 不平衡问题在回归任务中同样存在，但目前研究主要集中在分类任务上，因此需要探索不平衡回归的解决策略。

Method: 通过实验研究多种平衡和预测模型，并使用特定指标评估模型在不平衡回归数据中的表现。

Result: 研究提出了不平衡回归方法的分类法，并展示了不同策略对模型学习过程的优势。

Conclusion: 该研究为不平衡回归提供了新的见解，并指出了未来研究的方向。

Abstract: Imbalanced problems can arise in different real-world situations, and to
address this, certain strategies in the form of resampling or balancing
algorithms are proposed. This issue has largely been studied in the context of
classification, and yet, the same problem features in regression tasks, where
target values are continuous. This work presents an extensive experimental
study comprising various balancing and predictive models, and wich uses metrics
to capture important elements for the user and to evaluate the predictive model
in an imbalanced regression data context. It also proposes a taxonomy for
imbalanced regression approaches based on three crucial criteria: regression
model, learning process, and evaluation metrics. The study offers new insights
into the use of such strategies, highlighting the advantages they bring to each
model's learning process, and indicating directions for further studies. The
code, data and further information related to the experiments performed herein
can be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.

</details>


### [55] [From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning](https://arxiv.org/abs/2507.11926)
*Max Hopkins,Sihan Liu,Christopher Ye,Yuichi Yoshida*

Main category: cs.LG

TL;DR: 本文研究了可复现强化学习（RL）算法的样本效率问题，填补了生成模型与非生成模型之间的样本复杂度差距。


<details>
  <summary>Details</summary>
Motivation: 由于实证科学和机器学习中可复现性的普遍失败，研究可复现学习算法变得尤为重要。在强化学习中，探索环境的样本效率问题尚未解决。

Method: 作者提出了一种可复现的RL算法，在低水平表格MDP中实现了样本复杂度为$\tilde{O}(S^2A)$，并通过匹配的下界证明了其近最优性。

Result: 算法在生成模型和非生成模型（episodic setting）中均表现出高效性，样本复杂度分别为$\tilde{O}(S^2A)$和$\tilde{\Omega}(S^2A)$。

Conclusion: 研究表明，可复现探索并非比批量学习更昂贵，且样本高效的RL算法是可行的。

Abstract: The epidemic failure of replicability across empirical science and machine
learning has recently motivated the formal study of replicable learning
algorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from
a fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the
design of data-efficient replicable algorithms is now more or less understood.
In contrast, there remain significant gaps in our knowledge for control
settings like reinforcement learning where an agent must interact directly with
a shifting environment. Karbasi et. al show that with access to a generative
model of an environment with $S$ states and $A$ actions (the RL 'batch
setting'), replicably learning a near-optimal policy costs only
$\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a
generative model jumps to $\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the
substantial difficulty of environment exploration. This gap raises a key
question in the broader theory of replicability: Is replicable exploration
inherently more expensive than batch learning? Is sample-efficient replicable
RL even possible?
  In this work, we (nearly) resolve this problem (for low-horizon tabular
MDPs): exploration is not a significant barrier to replicable learning! Our
main result is a replicable RL algorithm on $\tilde{O}(S^2A)$ samples, bridging
the gap between the generative and episodic settings. We complement this with a
matching $\tilde{\Omega}(S^2A)$ lower bound in the generative setting (under
the common parallel sampling assumption) and an unconditional lower bound in
the episodic setting of $\tilde{\Omega}(S^2)$ showcasing the near-optimality of
our algorithm with respect to the state space $S$.

</details>


### [56] [Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning](https://arxiv.org/abs/2507.11928)
*Abhishek Sriram,Neal Tuffy*

Main category: cs.LG

TL;DR: 该论文提出了一种机器学习加速的优化框架，用于RF功率放大器设计，减少65%的仿真需求，同时保持±0.3至±0.4 dBm的精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要穷举所有参数组合以实现目标P2dB压缩规格，耗时且低效。

Method: 结合MaxMin拉丁超立方采样与CatBoost梯度提升，智能探索多维参数空间，仅选择35%的关键仿真点。

Result: 验证显示平均R²为0.901，仿真时间减少58.24%至77.78%，且不牺牲精度。

Conclusion: 该框架通过自动化GUI工作流实现快速设计迭代，适用于生产级RF电路。

Abstract: This paper presents a machine learning-accelerated optimization framework for
RF power amplifier design that reduces simulation requirements by 65% while
maintaining $\pm0.3$ to $\pm0.4$ dBm accuracy. The proposed method combines
MaxMin Latin Hypercube Sampling with CatBoost gradient boosting to
intelligently explore multidimensional parameter spaces. Instead of
exhaustively simulating all parameter combinations to achieve target P2dB
compression specifications, our approach strategically selects approximately
35% of critical simulation points. The framework processes ADS netlists,
executes harmonic balance simulations on the reduced dataset, and trains a
CatBoost model to predict P2dB performance across the entire design space.
Validation across 15 PA operating modes yields an average $R^2$ of 0.901, with
the system ranking parameter combinations by their likelihood of meeting target
specifications. The integrated solution delivers 58.24% to 77.78% reduction in
simulation time through automated GUI-based workflows, enabling rapid design
iterations without compromising accuracy standards required for production RF
circuits.

</details>


### [57] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: 论文提出了一种多轮强化学习（RL）方法Kevin，用于生成和优化CUDA内核，显著提升了正确性和性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核编写对AI系统效率至关重要，但具有挑战性且需要迭代优化。RL因其可验证的奖励（如正确性和加速）成为自然选择。

Method: 开发了一种灵活的多轮RL方法，解决了长轨迹学习和跨轮奖励分配等实际问题。

Result: Kevin在生成内核的正确性（56%到82%）和平均加速（0.53x到1.10x）上显著优于基础模型和前沿模型。

Conclusion: 多轮RL在CUDA内核优化中表现优异，串行细化比并行采样更有效，且随着细化轮次增加，性能提升更快。

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [58] [Online Training and Pruning of Deep Reinforcement Learning Networks](https://arxiv.org/abs/2507.11975)
*Valentin Frank Ingmar Guenter,Athanasios Sideris*

Main category: cs.LG

TL;DR: 论文提出了一种结合训练和剪枝的方法，用于强化学习中的神经网络优化，以减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在强化学习中表现优异，但其计算和内存复杂度高。现有剪枝方法在监督学习中有效，但在强化学习中应用不足。

Method: 提出XiNet，通过变分伯努利分布和随机变量ξ对网络单元进行剪枝，结合成本感知的正则化方案。

Result: 在MuJoCo基准测试中，OFENet剪枝后性能损失极小，且训练时剪枝比从头训练小网络更高效。

Conclusion: 该方法有效结合了强化学习目标和网络压缩，提升了强化学习代理的效率和性能。

Abstract: Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms
has been shown to enhance performance when feature extraction networks are used
but the gained performance comes at the significant expense of increased
computational and memory complexity. Neural network pruning methods have
successfully addressed this challenge in supervised learning. However, their
application to RL is underexplored. We propose an approach to integrate
simultaneous training and pruning within advanced RL methods, in particular to
RL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our
networks (XiNet) are trained to solve stochastic optimization problems over the
RL networks' weights and the parameters of variational Bernoulli distributions
for 0/1 Random Variables $\xi$ scaling each unit in the networks. The
stochastic problem formulation induces regularization terms that promote
convergence of the variational parameters to 0 when a unit contributes little
to the performance. In this case, the corresponding structure is rendered
permanently inactive and pruned from its network. We propose a cost-aware,
sparsity-promoting regularization scheme, tailored to the DenseNet architecture
of OFENets expressing the parameter complexity of involved networks in terms of
the parameters of the RVs in these networks. Then, when matching this cost with
the regularization terms, the many hyperparameters associated with them are
automatically selected, effectively combining the RL objectives and network
compression. We evaluate our method on continuous control benchmarks (MuJoCo)
and the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned
considerably with minimal loss in performance. Furthermore, our results confirm
that pruning large networks during training produces more efficient and higher
performing RL agents rather than training smaller networks from scratch.

</details>


### [59] [Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection](https://arxiv.org/abs/2507.11997)
*Tairan Huang,Yili Wang*

Main category: cs.LG

TL;DR: MLED框架通过多级LLM增强，结合文本信息和图结构，提升图欺诈检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有图欺诈检测方法忽略原始文本信息中的丰富语义线索，且难以将LLM处理的文本嵌入与图结构进行多模态融合。

Method: 提出MLED框架，利用LLM提取外部知识，设计类型级和关系级增强器，区分欺诈者与良性实体，并突出欺诈者在不同关系中的重要性。

Result: 在四个真实数据集上，MLED作为通用框架实现了最先进的性能。

Conclusion: MLED通过多级LLM增强，有效提升了图欺诈检测的准确性和泛化能力。

Abstract: Graph fraud detection has garnered significant attention as Graph Neural
Networks (GNNs) have proven effective in modeling complex relationships within
multimodal data. However, existing graph fraud detection methods typically use
preprocessed node embeddings and predefined graph structures to reveal
fraudsters, which ignore the rich semantic cues contained in raw textual
information. Although Large Language Models (LLMs) exhibit powerful
capabilities in processing textual information, it remains a significant
challenge to perform multimodal fusion of processed textual embeddings with
graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM
\textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In
MLED, we utilize LLMs to extract external knowledge from textual information to
enhance graph fraud detection methods. To integrate LLMs with graph structure
information and enhance the ability to distinguish fraudsters, we design a
multi-level LLM enhanced framework including type-level enhancer and
relation-level enhancer. One is to enhance the difference between the
fraudsters and the benign entities, the other is to enhance the importance of
the fraudsters in different relations. The experiments on four real-world
datasets show that MLED achieves state-of-the-art performance in graph fraud
detection as a generalized framework that can be applied to existing methods.

</details>


### [60] [Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing](https://arxiv.org/abs/2507.12002)
*Alice Zhang,Callihan Bertley,Dawei Liang,Edison Thomaz*

Main category: cs.LG

TL;DR: 论文提出了一种基于智能手表的多模态数据融合方法，用于检测面对面对话，结合音频和惯性数据，在实验室和半自然场景中分别达到82.0%和77.2%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 社交互动对人类行为和社会关系至关重要，但现有方法在复杂声学环境下检测面对面对话的能力有限。

Method: 利用智能手表采集音频和惯性数据，结合机器学习和深度学习的多模态融合方法，分析对话中的言语和非言语线索。

Result: 在实验室和半自然场景中，模型分别达到82.0±3.0%和77.2±1.8%的宏F1分数。

Conclusion: 多模态数据融合显著提升了对话检测的准确性，尤其在复杂环境中表现优异。

Abstract: Social interactions play a crucial role in shaping human behavior,
relationships, and societies. It encompasses various forms of communication,
such as verbal conversation, non-verbal gestures, facial expressions, and body
language. In this work, we develop a novel computational approach to detect a
foundational aspect of human social interactions, in-person verbal
conversations, by leveraging audio and inertial data captured with a commodity
smartwatch in acoustically-challenging scenarios. To evaluate our approach, we
conducted a lab study with 11 participants and a semi-naturalistic study with
24 participants. We analyzed machine learning and deep learning models with 3
different fusion methods, showing the advantages of fusing audio and inertial
data to consider not only verbal cues but also non-verbal gestures in
conversations. Furthermore, we perform a comprehensive set of evaluations
across activities and sampling rates to demonstrate the benefits of multimodal
sensing in specific contexts. Overall, our framework achieved 82.0$\pm$3.0%
macro F1-score when detecting conversations in the lab and 77.2$\pm$1.8% in the
semi-naturalistic setting.

</details>


### [61] [DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning](https://arxiv.org/abs/2507.12011)
*Yao Lu,Hongyu Gao,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: 论文提出了一种名为DUSE的数据扩展框架，通过不确定性评分函数和主动学习策略解决自动调制识别中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在自动调制识别（AMR）中表现优异，但需要大量标注数据。实际场景中目标域数据稀缺，手动收集或数据增强方法无法根本解决问题。

Method: 提出DUSE框架，利用不确定性评分函数筛选有用样本，并通过主动学习策略持续优化评分器。

Result: 实验表明，DUSE在类别平衡和不平衡设置下均优于8种核心集选择基线方法，且对未见模型具有强泛化能力。

Conclusion: DUSE能有效解决数据稀缺问题，提升模型性能，并具备跨架构泛化能力。

Abstract: Although deep neural networks have made remarkable achievements in the field
of automatic modulation recognition (AMR), these models often require a large
amount of labeled data for training. However, in many practical scenarios, the
available target domain data is scarce and difficult to meet the needs of model
training. The most direct way is to collect data manually and perform expert
annotation, but the high time and labor costs are unbearable. Another common
method is data augmentation. Although it can enrich training samples to a
certain extent, it does not introduce new data and therefore cannot
fundamentally solve the problem of data scarcity. To address these challenges,
we introduce a data expansion framework called Dynamic Uncertainty-driven
Sample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring
function to filter out useful samples from relevant AMR datasets and employs an
active learning strategy to continuously refine the scorer. Extensive
experiments demonstrate that DUSE consistently outperforms 8 coreset selection
baselines in both class-balance and class-imbalance settings. Besides, DUSE
exhibits strong cross-architecture generalization for unseen models.

</details>


### [62] [Granular feedback merits sophisticated aggregation](https://arxiv.org/abs/2507.12041)
*Anmol Kagrecha,Henrik Marklund,Potsawee Manakul,Richard Zeckhauser,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了在有限个体反馈下预测群体反馈分布的方法，发现反馈粒度越高，复杂方法比正则化平均更有效。


<details>
  <summary>Details</summary>
Motivation: 研究如何在成本限制下，利用少量个体反馈更准确地预测群体反馈分布。

Method: 比较正则化平均与更复杂的方法在不同反馈粒度下的表现。

Result: 五级反馈下，复杂方法所需个体数量减半；二元反馈下效果不明显。

Conclusion: 反馈粒度影响方法选择，高粒度时复杂方法显著优于正则化平均。

Abstract: Human feedback is increasingly used across diverse applications like training
AI models, developing recommender systems, and measuring public opinion -- with
granular feedback often being preferred over binary feedback for its greater
informativeness. While it is easy to accurately estimate a population's
distribution of feedback given feedback from a large number of individuals,
cost constraints typically necessitate using smaller groups. A simple method to
approximate the population distribution is regularized averaging: compute the
empirical distribution and regularize it toward a prior. Can we do better? As
we will discuss, the answer to this question depends on feedback granularity.
  Suppose one wants to predict a population's distribution of feedback using
feedback from a limited number of individuals. We show that, as feedback
granularity increases, one can substantially improve upon predictions of
regularized averaging by combining individuals' feedback in ways more
sophisticated than regularized averaging.
  Our empirical analysis using questions on social attitudes confirms this
pattern. In particular, with binary feedback, sophistication barely reduces the
number of individuals required to attain a fixed level of performance. By
contrast, with five-point feedback, sophisticated methods match the performance
of regularized averaging with about half as many individuals.

</details>


### [63] [Information-Theoretic Generalization Bounds of Replay-based Continual Learning](https://arxiv.org/abs/2507.12043)
*Wen Wen,Tieliang Gong,Yunjiao Zhang,Zeyu Gao,Weizhan Zhang,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的回放式持续学习理论框架，通过信息论界限揭示了内存缓冲区如何与当前任务交互以影响泛化性能。


<details>
  <summary>Details</summary>
Motivation: 持续学习中回放式方法的泛化行为缺乏理论理解，本文旨在填补这一空白。

Method: 建立信息论框架，推导基于假设和预测的泛化界限，适用于多种学习算法（如SGLD）。

Result: 理论分析表明，有限回放样本与当前任务数据结合可改善泛化并减轻灾难性遗忘。实验验证了界限的有效性。

Conclusion: 本文的理论框架为回放式持续学习提供了泛化行为的深入理解，并展示了实际应用价值。

Abstract: Continual learning (CL) has emerged as a dominant paradigm for acquiring
knowledge from sequential tasks while avoiding catastrophic forgetting.
Although many CL methods have been proposed to show impressive empirical
performance, the theoretical understanding of their generalization behavior
remains limited, particularly for replay-based approaches. In this paper, we
establish a unified theoretical framework for replay-based CL, deriving a
series of information-theoretic bounds that explicitly characterize how the
memory buffer interacts with the current task to affect generalization.
Specifically, our hypothesis-based bounds reveal that utilizing the limited
exemplars of previous tasks alongside the current task data, rather than
exhaustive replay, facilitates improved generalization while effectively
mitigating catastrophic forgetting. Furthermore, our prediction-based bounds
yield tighter and computationally tractable upper bounds of the generalization
gap through the use of low-dimensional variables. Our analysis is general and
broadly applicable to a wide range of learning algorithms, exemplified by
stochastic gradient Langevin dynamics (SGLD) as a representative method.
Comprehensive experimental evaluations demonstrate the effectiveness of our
derived bounds in capturing the generalization dynamics in replay-based CL
settings.

</details>


### [64] [FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling](https://arxiv.org/abs/2507.12053)
*Seanglidet Yean,Jiazu Zhou,Bu-Sung Lee,Markus Schläpfer*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的方法，结合动态区域大小和土地利用类型，利用条件生成对抗网络（cGANs）生成城市模拟场景中的出行流。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖历史轨迹，忽略动态因素（如人口密度和土地利用变化），而机械方法假设静态场景，无法适应未来预测需求。

Method: 结合动态区域大小和土地利用类型，利用cGANs融合历史数据与动态参数，快速生成可调空间粒度的出行流。

Result: 在新加坡手机数据上的应用表明，该方法性能优于现有方法。

Conclusion: 该方法无需大量校准数据或复杂行为建模，适用于未来城市规划和交通优化。

Abstract: The mobility patterns of people in cities evolve alongside changes in land
use and population. This makes it crucial for urban planners to simulate and
analyze human mobility patterns for purposes such as transportation
optimization and sustainable urban development. Existing generative models
borrowed from machine learning rely heavily on historical trajectories and
often overlook evolving factors like changes in population density and land
use. Mechanistic approaches incorporate population density and facility
distribution but assume static scenarios, limiting their utility for future
projections where historical data for calibration is unavailable. This study
introduces a novel, data-driven approach for generating origin-destination
mobility flows tailored to simulated urban scenarios. Our method leverages
adaptive factors such as dynamic region sizes and land use archetypes, and it
utilizes conditional generative adversarial networks (cGANs) to blend
historical data with these adaptive parameters. The approach facilitates rapid
mobility flow generation with adjustable spatial granularity based on regions
of interest, without requiring extensive calibration data or complex behavior
modeling. The promising performance of our approach is demonstrated by its
application to mobile phone data from Singapore, and by its comparison with
existing methods.

</details>


### [65] [Emergence of Quantised Representations Isolated to Anisotropic Functions](https://arxiv.org/abs/2507.12070)
*George Bird*

Main category: cs.LG

TL;DR: 本文提出了一种基于Spotlight Resonance的新方法，用于确定表征对齐，发现网络原语的代数对称性是任务无关表征结构的强预测因子。通过改变激活函数的研究，揭示了离散表征的形成机制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解功能形式选择如何影响表征结构，特别是激活函数如何导致离散化或连续性表征的形成。

Method: 采用基于Spotlight Resonance的新方法，通过改变激活函数进行消融研究，分析表征的离散化与连续性。

Result: 发现离散代数置换等变对称性激活函数导致表征离散化，而连续代数正交等变对称性则保持连续性。功能形式的选择可能引入意外的归纳偏差。

Conclusion: 功能形式对表征结构有显著影响，离散化可能导致重建误差增加。这一发现为表征形成机制和下游可解释性研究提供了新视角。

Abstract: This paper describes a novel methodology for determining representational
alignment, developed upon the existing Spotlight Resonance method. Using this,
it is found that algebraic symmetries of network primitives are a strong
predictor for task-agnostic structure in representations. Particularly, this
new tool is used to gain insight into how discrete representations can form and
arrange in autoencoder models, through an ablation study where only the
activation function is altered. Representations are found to tend to discretise
when the activation functions are defined through a discrete algebraic
permutation-equivariant symmetry. In contrast, they remain continuous under a
continuous algebraic orthogonal-equivariant definition. These findings
corroborate the hypothesis that functional form choices can carry unintended
inductive biases which produce task-independent artefactual structures in
representations, particularly that contemporary forms induce discretisation of
otherwise continuous structure -- a quantisation effect. Moreover, this
supports a general causal model for one mode in which discrete representations
may form, and could constitute a prerequisite for downstream interpretability
phenomena, including grandmother neurons, discrete coding schemes, general
linear features and possibly Superposition. Hence, this tool and proposed
mechanism for the influence of functional form on representations may provide
several insights into emergent interpretability research. Finally, preliminary
results indicate that quantisation of representations appears to correlate with
a measurable increase in reconstruction error, reinforcing previous conjectures
that this collapse can be detrimental.

</details>


### [66] [Measuring Informativeness Gap of (Mis)Calibrated Predictors](https://arxiv.org/abs/2507.12094)
*Yiding Feng,Wei Tang*

Main category: cs.LG

TL;DR: 论文提出了一个称为‘信息差距’的新概念，用于比较多个预测模型在决策任务中的‘有用性’，并提供了一个双重表征和自然的信息度量方法。


<details>
  <summary>Details</summary>
Motivation: 在多个预测模型可能都存在校准问题的情况下，决策者需要选择最‘有用’的模型。

Method: 引入信息差距概念，定义了两个预测器在所有决策任务中的最大归一化收益优势，并提供了双重表征和一种类似EMD的度量方法。

Result: 该框架泛化了现有概念（如U-Calibration和Blackwell信息性），并证明了新度量的完备性、合理性和样本高效性。

Conclusion: 信息差距为比较预测模型提供了统一框架，适用于校准和非校准场景，并具有实际可操作性。

Abstract: In many applications, decision-makers must choose between multiple predictive
models that may all be miscalibrated. Which model (i.e., predictor) is more
"useful" in downstream decision tasks? To answer this, our first contribution
introduces the notion of the informativeness gap between any two predictors,
defined as the maximum normalized payoff advantage one predictor offers over
the other across all decision-making tasks. Our framework strictly generalizes
several existing notions: it subsumes U-Calibration [KLST-23] and Calibration
Decision Loss [HW-24], which compare a miscalibrated predictor to its
calibrated counterpart, and it recovers Blackwell informativeness [Bla-51,
Bla-53] as a special case when both predictors are perfectly calibrated. Our
second contribution is a dual characterization of the informativeness gap,
which gives rise to a natural informativeness measure that can be viewed as a
relaxed variant of the earth mover's distance (EMD) between two prediction
distributions. We show that this measure satisfies natural desiderata: it is
complete and sound, and it can be estimated sample-efficiently in the
prediction-only access setting. Along the way, we also obtain novel
combinatorial structural results when applying this measure to perfectly
calibrated predictors.

</details>


### [67] [Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks](https://arxiv.org/abs/2507.12127)
*Ngoc Duy Pham,Thusitha Dayaratne,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.LG

TL;DR: 论文提出了一种基于联邦学习（FL）的动态频谱分配（DSA）方法，解决了频谱感知中数据标注不足和安全漏洞问题。


<details>
  <summary>Details</summary>
Motivation: 无线设备的快速增长导致频谱稀缺，传统集中式机器学习方法因隐私和带宽限制难以应用，分布式方法如FL成为潜在解决方案。

Method: 采用半监督FL结合能量检测，解决数据标注不足问题；提出基于疫苗接种启发的防御机制，应对数据投毒攻击。

Result: 实验验证了FLSS在未标注数据集上的高准确性和对恶意攻击的鲁棒性。

Conclusion: FLSS是一种有效的频谱感知解决方案，兼具高效性和安全性。

Abstract: Advancements in wireless and mobile technologies, including 5G advanced and
the envisioned 6G, are driving exponential growth in wireless devices. However,
this rapid expansion exacerbates spectrum scarcity, posing a critical
challenge. Dynamic spectrum allocation (DSA)--which relies on sensing and
dynamically sharing spectrum--has emerged as an essential solution to address
this issue. While machine learning (ML) models hold significant potential for
improving spectrum sensing, their adoption in centralized ML-based DSA systems
is limited by privacy concerns, bandwidth constraints, and regulatory
challenges. To overcome these limitations, distributed ML-based approaches such
as Federated Learning (FL) offer promising alternatives. This work addresses
two key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of
labeled data for training FL models in practical spectrum sensing scenarios is
tackled with a semi-supervised FL approach, combined with energy detection,
enabling model training on unlabeled datasets. Second, we examine the security
vulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our
analysis highlights the shortcomings of existing majority-based defenses in
countering such attacks. To address these vulnerabilities, we propose a novel
defense mechanism inspired by vaccination, which effectively mitigates data
poisoning attacks without relying on majority-based assumptions. Extensive
experiments on both synthetic and real-world datasets validate our solutions,
demonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets
and maintain Byzantine robustness against both targeted and untargeted data
poisoning attacks, even when a significant proportion of participants are
malicious.

</details>


### [68] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 提出了一种名为RiemannLoRA的新方法，通过将LoRA矩阵视为光滑流形，同时解决了初始化策略和低秩矩阵分解中的过参数化问题。


<details>
  <summary>Details</summary>
Motivation: LoRA在参数高效微调中广泛应用，但仍面临初始化策略优化和过参数化的挑战。

Method: 将固定秩LoRA矩阵视为光滑流形，通过流形上的最快损失下降方向确定初始化，并采用数值稳定和计算高效的实现。

Result: 在LLM和扩散模型上的实验表明，RiemannLoRA在收敛速度和最终性能上优于标准LoRA及其先进改进。

Conclusion: RiemannLoRA提供了一种统一框架，有效解决了LoRA的两大挑战，提升了性能。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


### [69] [FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale](https://arxiv.org/abs/2507.12144)
*Boris Bonev,Thorsten Kurth,Ankur Mahesh,Mauro Bisson,Jean Kossaifi,Karthik Kashinath,Anima Anandkumar,William D. Collins,Michael S. Pritchard,Alexander Keller*

Main category: cs.LG

TL;DR: FourCastNet 3通过几何机器学习方法提升全球天气建模，实现高效、准确的概率集合预报。


<details>
  <summary>Details</summary>
Motivation: 改进传统天气模型的效率和准确性，同时保持概率校准和频谱真实性。

Method: 采用纯卷积神经网络架构，结合模型和数据并行训练范式，适应球面几何。

Result: 预报速度比传统方法快8至60倍，精度超越传统模型，媲美扩散方法。

Conclusion: FourCastNet 3在计算效率、概率技能和频谱保真度方面表现优异，适用于气象预报和预警系统。

Abstract: FourCastNet 3 advances global weather modeling by implementing a scalable,
geometric machine learning (ML) approach to probabilistic ensemble forecasting.
The approach is designed to respect spherical geometry and to accurately model
the spatially correlated probabilistic nature of the problem, resulting in
stable spectra and realistic dynamics across multiple scales. FourCastNet 3
delivers forecasting accuracy that surpasses leading conventional ensemble
models and rivals the best diffusion-based methods, while producing forecasts 8
to 60 times faster than these approaches. In contrast to other ML approaches,
FourCastNet 3 demonstrates excellent probabilistic calibration and retains
realistic spectra, even at extended lead times of up to 60 days. All of these
advances are realized using a purely convolutional neural network architecture
tailored for spherical geometry. Scalable and efficient large-scale training on
1024 GPUs and more is enabled by a novel training paradigm for combined model-
and data-parallelism, inspired by domain decomposition methods in classical
numerical models. Additionally, FourCastNet 3 enables rapid inference on a
single GPU, producing a 90-day global forecast at 0.25{\deg}, 6-hourly
resolution in under 20 seconds. Its computational efficiency, medium-range
probabilistic skill, spectral fidelity, and rollout stability at subseasonal
timescales make it a strong candidate for improving meteorological forecasting
and early warning systems through large ensemble predictions.

</details>


### [70] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM是一种高效、通信优化的分布式Transformer推理策略，适用于边缘设备，显著减少通信和计算开销。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）在边缘部署面临挑战，需要高效策略以支持资源受限环境。

Method: 采用Segment Means近似中间特征，重构自注意力机制以减少冗余计算，并设计分区感知的因果掩码。

Result: 在ViT、BERT和GPT-2上测试，通信开销减少高达99.2%，计算量减少51.24%，精度损失较小。

Conclusion: PRISM为资源受限环境中的基础模型部署提供了可扩展且实用的解决方案。

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [71] [Multi-Component VAE with Gaussian Markov Random Field](https://arxiv.org/abs/2507.12165)
*Fouad Oubari,Mohamed El-Baha,Raphael Meunier,Rodrigue Décatoire,Mathilde Mougeot*

Main category: cs.LG

TL;DR: 提出了一种新的多组件变分自编码器（GMRF MCVAE），通过嵌入高斯马尔可夫随机场来建模组件间关系，显著提升了生成的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多组件数据建模中忽略了复杂依赖关系，导致生成结果缺乏结构一致性。

Method: 在变分自编码器的先验和后验分布中嵌入高斯马尔可夫随机场，显式建模组件间关系。

Result: 在合成Copula数据集、PolyMNIST基准和真实BIKED数据集上表现优异，尤其在结构一致性方面显著提升。

Conclusion: GMRF MCVAE特别适用于需要高保真多组件建模的实际应用。

Abstract: Multi-component datasets with intricate dependencies, like industrial
assemblies or multi-modal imaging, challenge current generative modeling
techniques. Existing Multi-component Variational AutoEncoders typically rely on
simplified aggregation strategies, neglecting critical nuances and consequently
compromising structural coherence across generated components. To explicitly
address this gap, we introduce the Gaussian Markov Random Field Multi-Component
Variational AutoEncoder , a novel generative framework embedding Gaussian
Markov Random Fields into both prior and posterior distributions. This design
choice explicitly models cross-component relationships, enabling richer
representation and faithful reproduction of complex interactions. Empirically,
our GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula
dataset specifically constructed to evaluate intricate component relationships,
demonstrates competitive results on the PolyMNIST benchmark, and significantly
enhances structural coherence on the real-world BIKED dataset. Our results
indicate that the GMRF MCVAE is especially suited for practical applications
demanding robust and realistic modeling of multi-component coherence

</details>


### [72] [Explainable Evidential Clustering](https://arxiv.org/abs/2507.12192)
*Victor F. Lopes de Souza,Karima Bakhti,Sofiane Ramdani,Denis Mottet,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 本文探讨了基于Dempster-Shafer理论的证据聚类方法，提出了一种解释聚类结果的框架，并通过IEMM算法生成可解释的决策树。


<details>
  <summary>Details</summary>
Motivation: 现实数据常存在不确定性和不精确性，传统方法难以处理。证据聚类能解决这些问题，但解释聚类结果的问题尚未充分研究，尤其是在高风险领域（如医疗）中。

Method: 通过代表性概念，将决策树作为解释工具，并引入效用函数处理部分标签问题，定义解释成本，提出IEMM算法生成解释。

Result: 在合成和真实数据上验证了IEMM算法，93%的情况下能提供满意的解释。

Conclusion: IEMM算法为证据聚类提供了可解释且谨慎的决策树解释，适用于高风险领域。

Abstract: Unsupervised classification is a fundamental machine learning problem.
Real-world data often contain imperfections, characterized by uncertainty and
imprecision, which are not well handled by traditional methods. Evidential
clustering, based on Dempster-Shafer theory, addresses these challenges. This
paper explores the underexplored problem of explaining evidential clustering
results, which is crucial for high-stakes domains such as healthcare. Our
analysis shows that, in the general case, representativity is a necessary and
sufficient condition for decision trees to serve as abductive explainers.
Building on the concept of representativity, we generalize this idea to
accommodate partial labeling through utility functions. These functions enable
the representation of "tolerable" mistakes, leading to the definition of
evidential mistakeness as explanation cost and the construction of explainers
tailored to evidential classifiers. Finally, we propose the Iterative
Evidential Mistake Minimization (IEMM) algorithm, which provides interpretable
and cautious decision tree explanations for evidential clustering functions. We
validate the proposed algorithm on synthetic and real-world data. Taking into
account the decision-maker's preferences, we were able to provide an
explanation that was satisfactory up to 93% of the time.

</details>


### [73] [Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation](https://arxiv.org/abs/2507.12218)
*Tomohisa Okazaki*

Main category: cs.LG

TL;DR: 论文提出了一种基于线性基函数组合的物理信息线性模型（PILM），用于解决偏微分方程（PDEs）的正反问题，并通过地壳应变率估计验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PDE求解方法复杂，物理信息神经网络（PINN）虽受关注但缺乏解析解。PILM旨在提供一种可解析求解的线性模型框架。

Method: PILM使用线性基函数组合表示解，通过最小化PDE残差、边界条件和数据误差来求解，适用于正反问题及不确定边界条件。

Result: PILM在地壳应变率估计中验证了其有效性，数学正则化在贝叶斯视角下表现优于物理正则化。

Conclusion: PILM为线性正反问题、欠定系统和物理正则化提供了可解析求解的框架，具有实际应用潜力。

Abstract: Many physical systems are described by partial differential equations (PDEs),
and solving these equations and estimating their coefficients or boundary
conditions (BCs) from observational data play a crucial role in understanding
the associated phenomena. Recently, a machine learning approach known as
physics-informed neural network, which solves PDEs using neural networks by
minimizing the sum of residuals from the PDEs, BCs, and data, has gained
significant attention in the scientific community. In this study, we
investigate a physics-informed linear model (PILM) that uses linear
combinations of basis functions to represent solutions, thereby enabling an
analytical representation of optimal solutions. The PILM was formulated and
verified for illustrative forward and inverse problems including cases with
uncertain BCs. Furthermore, the PILM was applied to estimate crustal strain
rates using geodetic data. Specifically, physical regularization that enforces
elastic equilibrium on the velocity fields was compared with mathematical
regularization that imposes smoothness constraints. From a Bayesian
perspective, mathematical regularization exhibited superior performance. The
PILM provides an analytically solvable framework applicable to linear forward
and inverse problems, underdetermined systems, and physical regularization.

</details>


### [74] [Optimizers Qualitatively Alter Solutions And We Should Leverage This](https://arxiv.org/abs/2507.12224)
*Razvan Pascanu,Clare Lyle,Ionut-Vlad Modoranu,Naima Elosegui Borras,Dan Alistarh,Petar Velickovic,Sarath Chandar,Soham De,James Martens*

Main category: cs.LG

TL;DR: 论文探讨了DNN优化器的角色不仅限于收敛速度，还影响学习解的性质，呼吁关注优化器设计对模型结果的影响。


<details>
  <summary>Details</summary>
Motivation: 早期对DNN可行性的怀疑源于其非线性特性导致无法保证收敛到全局最优解，但实际经验表明标准训练协议下DNN表现良好。然而，社区过于关注优化效率，忽视了优化器对学习解性质的影响。

Method: 通过分析现有优化器的行为，提出优化器不仅影响收敛速度，还能通过引入归纳偏置改变模型的有效表达能力。

Result: 优化器可以编码学习过程中的期望特性，而不仅仅是加速收敛。

Conclusion: 呼吁社区关注优化器设计对模型结果的影响，将其视为与架构和数据同等重要的因素。

Abstract: Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not
guarantee convergence to a unique global minimum of the loss when using
optimizers relying only on local information, such as SGD. Indeed, this was a
primary source of skepticism regarding the feasibility of DNNs in the early
days of the field. The past decades of progress in deep learning have revealed
this skepticism to be misplaced, and a large body of empirical evidence shows
that sufficiently large DNNs following standard training protocols exhibit
well-behaved optimization dynamics that converge to performant solutions. This
success has biased the community to use convex optimization as a mental model
for learning, leading to a focus on training efficiency, either in terms of
required iteration, FLOPs or wall-clock time, when improving optimizers. We
argue that, while this perspective has proven extremely fruitful, another
perspective specific to DNNs has received considerably less attention: the
optimizer not only influences the rate of convergence, but also the qualitative
properties of the learned solutions. Restated, the optimizer can and will
encode inductive biases and change the effective expressivity of a given class
of models. Furthermore, we believe the optimizer can be an effective way of
encoding desiderata in the learning process. We contend that the community
should aim at understanding the biases of already existing methods, as well as
aim to build new optimizers with the explicit intent of inducing certain
properties of the solution, rather than solely judging them based on their
convergence rates. We hope our arguments will inspire research to improve our
understanding of how the learning process can impact the type of solution we
converge to, and lead to a greater recognition of optimizers design as a
critical lever that complements the roles of architecture and data in shaping
model outcomes.

</details>


### [75] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为RegCL的新型非回放持续学习框架，旨在通过模型合并实现多领域知识的高效整合，解决了Segment Anything Model（SAM）在特定领域性能受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用基于适配器的一步适应范式，但这些方法通常针对特定领域设计，跨领域使用时可能导致性能下降，严重限制了模型的可扩展性。

Method: RegCL将模型合并算法引入持续学习范式，通过合并不同领域训练的SAM适配模块（如LoRA模块）参数，并以权重优化为指导，最小化合并模型与各领域特定模型之间的预测差异。

Result: 实验结果表明，RegCL在多个下游数据集上实现了良好的持续学习性能，验证了其在动态场景中的有效性。

Conclusion: RegCL通过模型合并高效整合多领域知识，同时保持参数效率（模型大小不随任务数量增加而变化，且无需存储历史数据）。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [76] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出了一种基于提示的在线持续学习方法，解决了数据隐私和参数增长问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中数据隐私约束和参数增长问题限制了现有方法的实用性。

Method: 结合轻量级提示生成器、可训练的缩放移位器、预训练模型泛化保护及硬软更新机制。

Result: 在多个数据集上性能显著优于现有方法，参数较少且训练推理时间适中。

Conclusion: 该方法在性能和效率上均表现优异，代码已开源。

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


### [77] [Thought Purity: Defense Paradigm For Chain-of-Thought Attack](https://arxiv.org/abs/2507.12314)
*Zihao Xue,Zhen Bi,Long Ma,Zhenlin Hu,Yan Wang,Zhenfang Liu,Qing Sheng,Jie Xiao,Jungang Lou*

Main category: cs.LG

TL;DR: 论文提出Thought Purity（TP）防御范式，针对强化学习训练的大型推理模型（LRMs）在Chain-of-Thought（CoT）生成过程中的安全漏洞，通过优化数据处理、强化规则约束和自适应监控，提升安全性并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（如Deepseek-R1）在推理能力上表现优异，但其在Chain-of-Thought生成过程中易受安全威胁（如后门提示攻击），导致推理机制被系统性破坏，亟需解决方案。

Method: 提出Thought Purity（TP）防御范式，包含三个协同组件：安全优化的数据处理流程、强化学习增强的规则约束和自适应监控指标。

Result: TP是首个针对CoTA漏洞的全面防御机制，显著提升了安全性与功能性的平衡。

Conclusion: TP为下一代AI架构提供了安全性与性能兼顾的解决方案，推动了强化学习对齐推理系统的安全性发展。

Abstract: While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,
Deepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large
Language Models (LLMs) domain, their susceptibility to security threats remains
a critical vulnerability. This weakness is particularly evident in
Chain-of-Thought (CoT) generation processes, where adversarial methods like
backdoor prompt attacks can systematically subvert the model's core reasoning
mechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this
vulnerability through exploiting prompt controllability, simultaneously
degrading both CoT safety and task performance with low-cost interventions. To
address this compounded security-performance vulnerability, we propose Thought
Purity (TP): a defense paradigm that systematically strengthens resistance to
malicious content while preserving operational efficacy. Our solution achieves
this through three synergistic components: (1) a safety-optimized data
processing pipeline (2) reinforcement learning-enhanced rule constraints (3)
adaptive monitoring metrics. Our approach establishes the first comprehensive
defense mechanism against CoTA vulnerabilities in reinforcement
learning-aligned reasoning systems, significantly advancing the
security-functionality equilibrium for next-generation AI architectures.

</details>


### [78] [Nonlinear Concept Erasure: a Density Matching Approach](https://arxiv.org/abs/2507.12341)
*Antoine Saillenfest,Pirmin Lemberger*

Main category: cs.LG

TL;DR: 论文提出了一种名为LEOPARD的概念擦除方法，通过正交投影从文本表示中移除敏感信息，同时保留其他语义信息。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络模型在公平性要求下无法推断敏感信息（如性别或种族）的挑战。

Method: 使用正交投影在嵌入空间中学习，使目标概念的类条件特征分布在投影后无法区分，并通过调整投影秩控制信息移除程度。

Result: LEOPARD在经典NLP任务中实现了最先进的离散属性非线性擦除性能，并有效减少了深度非线性分类器的偏见。

Conclusion: LEOPARD方法在提升公平性方面表现优异，为敏感信息移除提供了有效解决方案。

Abstract: Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

</details>


### [79] [Heat Kernel Goes Topological](https://arxiv.org/abs/2507.12380)
*Maximilian Krahn,Vikas Garg*

Main category: cs.LG

TL;DR: 提出了一种基于组合复形（CCs）的新型拓扑框架，通过引入拉普拉斯算子和热核计算，实现了高效的多尺度信息捕捉和置换等变表示，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决拓扑神经网络因高阶消息传递导致的高计算成本问题。

Method: 在组合复形上引入拉普拉斯算子，计算热核作为节点描述符，支持多尺度信息捕捉和置换等变表示。

Result: 理论上具有最大表达能力，能区分任意非同构组合复形；实验上计算效率显著优于现有拓扑方法，并在分子数据集上表现优异。

Conclusion: 该方法为拓扑深度学习提供了高效且表达能力强的表示，为分子分类和性质预测任务开辟了新途径。

Abstract: Topological neural networks have emerged as powerful successors of graph
neural networks. However, they typically involve higher-order message passing,
which incurs significant computational expense. We circumvent this issue with a
novel topological framework that introduces a Laplacian operator on
combinatorial complexes (CCs), enabling efficient computation of heat kernels
that serve as node descriptors. Our approach captures multiscale information
and enables permutation-equivariant representations, allowing easy integration
into modern transformer-based architectures.
  Theoretically, the proposed method is maximally expressive because it can
distinguish arbitrary non-isomorphic CCs. Empirically, it significantly
outperforms existing topological methods in terms of computational efficiency.
Besides demonstrating competitive performance with the state-of-the-art
descriptors on standard molecular datasets, it exhibits superior capability in
distinguishing complex topological structures and avoiding blind spots on
topological benchmarks. Overall, this work advances topological deep learning
by providing expressive yet scalable representations, thereby opening up
exciting avenues for molecular classification and property prediction tasks.

</details>


### [80] [Improving Reinforcement Learning Sample-Efficiency using Local Approximation](https://arxiv.org/abs/2507.12383)
*Mohit Prashant,Arvind Easwaran*

Main category: cs.LG

TL;DR: 本文提出了在无限时间马尔可夫决策过程（MDP）中，比现有文献更精确的强化学习（RL）样本复杂度的PAC边界。通过近似原始MDP，样本复杂度降低到O(SA log A)时间步。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于两点：一是状态间的距离影响学习效率；二是学习一个状态的ε最优值所需的样本复杂度与远离它的状态无关。

Method: 方法是通过构建原始MDP状态空间的子集来近似MDP，从而减少样本复杂度，并扩展到无限时间、无模型设置。

Result: 结果显示样本复杂度降低到O(SA log A)时间步，并通过实验验证了改进的显著性。

Conclusion: 结论是该方法显著提高了样本复杂度，并通过实验验证了其优越性。

Abstract: In this study, we derive Probably Approximately Correct (PAC) bounds on the
asymptotic sample-complexity for RL within the infinite-horizon Markov Decision
Process (MDP) setting that are sharper than those in existing literature. The
premise of our study is twofold: firstly, the further two states are from each
other, transition-wise, the less relevant the value of the first state is when
learning the $\epsilon$-optimal value of the second; secondly, the amount of
'effort', sample-complexity-wise, expended in learning the $\epsilon$-optimal
value of a state is independent of the number of samples required to learn the
$\epsilon$-optimal value of a second state that is a sufficient number of
transitions away from the first. Inversely, states within each other's vicinity
have values that are dependent on each other and will require a similar number
of samples to learn. By approximating the original MDP using smaller MDPs
constructed using subsets of the original's state-space, we are able to reduce
the sample-complexity by a logarithmic factor to $O(SA \log A)$ timesteps,
where $S$ and $A$ are the state and action space sizes. We are able to extend
these results to an infinite-horizon, model-free setting by constructing a
PAC-MDP algorithm with the aforementioned sample-complexity. We conclude with
showing how significant the improvement is by comparing our algorithm against
prior work in an experimental setting.

</details>


### [81] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于$MoS_2$闪存模拟CAM的硬件-软件协同设计方法，用于提升树模型的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展引发了对其可信度的担忧，尤其是在可解释性和鲁棒性方面。传统树模型在扩展时面临计算成本高的问题，而现有模拟CAM方法因设备变化敏感导致性能不佳。

Method: 采用$MoS_2$闪存模拟CAM的软边界特性，结合软树模型进行高效推理。

Result: 实验显示，该方法在WDBC数据集上达到96%准确率，且在设备阈值变化10%时仅损失0.6%准确率，远优于传统决策树。

Conclusion: 该研究为提升AI可信度和效率的专用硬件设计提供了新方向。

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [82] [NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data](https://arxiv.org/abs/2507.12412)
*Dzung Dinh,Boqi Chen,Marc Niethammer,Junier Oliva*

Main category: cs.LG

TL;DR: NOCTA是一种非贪婪的目标成本权衡获取方法，用于在推理时动态获取最具信息量的特征，同时考虑时间动态和获取成本。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的应用中（如医疗），需要高效获取信息以进行预测，同时考虑成本和时间动态。

Method: 提出NOCTA方法，包括非参数（NOCTA-NP）和参数（NOCTA-P）两种估计器，分别基于最近邻和直接预测获取效用。

Result: 在合成和真实医疗数据集上的实验表明，NOCTA优于现有基线方法。

Conclusion: NOCTA能有效平衡信息获取与成本，适用于动态预测任务。

Abstract: In many critical applications, resource constraints limit the amount of
information that can be gathered to make predictions. For example, in
healthcare, patient data often spans diverse features ranging from lab tests to
imaging studies. Each feature may carry different information and must be
acquired at a respective cost of time, money, or risk to the patient. Moreover,
temporal prediction tasks, where both instance features and labels evolve over
time, introduce additional complexity in deciding when or what information is
important. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff
Acquisition method that sequentially acquires the most informative features at
inference time while accounting for both temporal dynamics and acquisition
cost. We first introduce a cohesive estimation target for our NOCTA setting,
and then develop two complementary estimators: 1) a non-parametric method based
on nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric
method that directly predicts the utility of potential acquisitions (NOCTA-P).
Experiments on synthetic and real-world medical datasets demonstrate that both
NOCTA variants outperform existing baselines.

</details>


### [83] [Mixture of Raytraced Experts](https://arxiv.org/abs/2507.12419)
*Andrea Perin,Giacomo Lagomarsini,Claudio Gallicchio,Giuseppe Nuti*

Main category: cs.LG

TL;DR: 提出了一种动态选择专家序列的混合专家架构（Mixture of Raytraced Experts），能够生成可变宽度和深度的计算图，提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有混合专家架构通常需要固定计算量，无法动态调整计算资源。本文旨在通过动态选择专家序列，实现更高的灵活性和效率。

Method: 通过迭代采样候选专家序列，类似循环神经网络的训练方式，无需负载均衡机制。

Result: 初步实验显示训练周期减少10%至40%，同时保持或提升准确性。

Conclusion: 该方法为混合专家架构提供了新的研究方向，可能设计出更快、表达能力更强的模型。

Abstract: We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts
(MoE) architecture which can dynamically select sequences of experts, producing
computational graphs of variable width and depth. Existing MoE architectures
generally require a fixed amount of computation for a given sample. Our
approach, in contrast, yields predictions with increasing accuracy as the
computation cycles through the experts' sequence. We train our model by
iteratively sampling from a set of candidate experts, unfolding the sequence
akin to how Recurrent Neural Networks are trained. Our method does not require
load-balancing mechanisms, and preliminary experiments show a reduction in
training epochs of 10\% to 40\% with a comparable/higher accuracy. These
results point to new research directions in the field of MoEs, allowing the
design of potentially faster and more expressive models. The code is available
at https://github.com/nutig/RayTracing

</details>


### [84] [Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks](https://arxiv.org/abs/2507.12435)
*Yi Li,David Mccoy,Nolan Gunter,Kaitlyn Lee,Alejandro Schuler,Mark van der Laan*

Main category: cs.LG

TL;DR: 论文提出了一种名为TDA的新框架，将TMLE直接嵌入神经网络的参数空间，解决了现有方法在因果推断中的偏差问题，并适用于多参数设置。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络在预测方面强大，但在因果参数推断（如治疗效果或生存曲线）上缺乏有效性。现有方法要么无法保证解决高效影响函数方程，要么计算成本高。

Method: TDA通过冻结大部分参数并迭代更新一小部分“目标”参数，利用目标梯度消除一阶偏差，生成渐近有效的置信区间。

Result: 在IHDP数据集和模拟生存数据上，TDA减少了偏差并提高了覆盖率，优于标准神经网络估计器和现有后处理方法。

Conclusion: TDA为复杂多参数目标的深度架构提供了一种直接、可扩展的严格因果推断途径。

Abstract: Modern deep neural networks are powerful predictive tools yet often lack
valid inference for causal parameters, such as treatment effects or entire
survival curves. While frameworks like Double Machine Learning (DML) and
Targeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,
existing neural implementations either rely on "targeted losses" that do not
guarantee solving the efficient influence function equation or computationally
expensive post-hoc "fluctuations" for multi-parameter settings. We propose
Targeted Deep Architectures (TDA), a new framework that embeds TMLE directly
into the network's parameter space with no restrictions on the backbone
architecture. Specifically, TDA partitions model parameters - freezing all but
a small "targeting" subset - and iteratively updates them along a targeting
gradient, derived from projecting the influence functions onto the span of the
gradients of the loss with respect to weights. This procedure yields plug-in
estimates that remove first-order bias and produce asymptotically valid
confidence intervals. Crucially, TDA easily extends to multi-dimensional causal
estimands (e.g., entire survival curves) by merging separate targeting
gradients into a single universal targeting update. Theoretically, TDA inherits
classical TMLE properties, including double robustness and semiparametric
efficiency. Empirically, on the benchmark IHDP dataset (average treatment
effects) and simulated survival data with informative censoring, TDA reduces
bias and improves coverage relative to both standard neural-network estimators
and prior post-hoc approaches. In doing so, TDA establishes a direct, scalable
pathway toward rigorous causal inference within modern deep architectures for
complex multi-parameter targets.

</details>


### [85] [A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning](https://arxiv.org/abs/2507.12439)
*Daniel Commey,Rebecca A. Sarpong,Griffith S. Klogo,Winful Bagyl-Bac,Garth V. Crosby*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的贝叶斯激励机制，通过经济手段防止联邦学习中的数据投毒攻击，确保恶意行为在经济上不划算。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的开放参与性使其容易受到数据投毒攻击，现有防御方法多为被动且计算成本高。

Method: 设计了一个贝叶斯博弈模型，服务器使用小型验证数据集验证更新质量并发放奖励，满足个体理性（IR）和激励相容性（IC）。

Result: 在MNIST和FashionMNIST的非独立同分布数据上，即使有50%的标签翻转攻击，模型仍保持96.7%的准确率。

Conclusion: 该机制计算轻量、预算可控，可集成到现有联邦学习框架中，为经济稳健的联邦学习生态系统提供实用方案。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy. However, its
open-participation nature exposes it to data-poisoning attacks, in which
malicious actors submit corrupted model updates to degrade the global model.
Existing defenses are often reactive, relying on statistical aggregation rules
that can be computationally expensive and that typically assume an honest
majority. This paper introduces a proactive, economic defense: a lightweight
Bayesian incentive mechanism that makes malicious behavior economically
irrational. Each training round is modeled as a Bayesian game of incomplete
information in which the server, acting as the principal, uses a small, private
validation dataset to verify update quality before issuing payments. The design
satisfies Individual Rationality (IR) for benevolent clients, ensuring their
participation is profitable, and Incentive Compatibility (IC), making poisoning
an economically dominated strategy. Extensive experiments on non-IID partitions
of MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping
adversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3
percentage points lower than in a scenario with 30% label-flipping adversaries.
This outcome is 51.7 percentage points better than standard FedAvg, which
collapses under the same 50% attack. The mechanism is computationally light,
budget-bounded, and readily integrates into existing FL frameworks, offering a
practical route to economically robust and sustainable FL ecosystems.

</details>


### [86] [Cost-aware Stopping for Bayesian Optimization](https://arxiv.org/abs/2507.12453)
*Qian Xie,Linda Cai,Alexander Terenin,Peter I. Frazier,Ziv Scully*

Main category: cs.LG

TL;DR: 提出了一种成本感知的贝叶斯优化停止规则，无需启发式调参，适用于不同评估成本，并与PBGI和log EI per cost两种获取函数结合，理论保证其成本控制能力。实验表明其性能优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 在贝叶斯优化中，如何在高成本的黑盒函数评估中适时停止是一个重要问题，现有方法缺乏成本控制的理论保证。

Method: 提出一种成本感知停止规则，结合PBGI和log EI per cost获取函数，理论证明其成本控制能力。

Result: 实验证明，该停止规则与PBGI结合时，在成本调整后悔度指标上优于其他方法。

Conclusion: 该方法为贝叶斯优化提供了一种高效且理论保障的停止策略。

Abstract: In automated machine learning, scientific discovery, and other applications
of Bayesian optimization, deciding when to stop evaluating expensive black-box
functions is an important practical consideration. While several adaptive
stopping rules have been proposed, in the cost-aware setting they lack
guarantees ensuring they stop before incurring excessive function evaluation
costs. We propose a cost-aware stopping rule for Bayesian optimization that
adapts to varying evaluation costs and is free of heuristic tuning. Our rule is
grounded in a theoretical connection to state-of-the-art cost-aware acquisition
functions, namely the Pandora's Box Gittins Index (PBGI) and log expected
improvement per cost. We prove a theoretical guarantee bounding the expected
cumulative evaluation cost incurred by our stopping rule when paired with these
two acquisition functions. In experiments on synthetic and empirical tasks,
including hyperparameter optimization and neural architecture size search, we
show that combining our stopping rule with the PBGI acquisition function
consistently matches or outperforms other acquisition-function--stopping-rule
pairs in terms of cost-adjusted simple regret, a metric capturing trade-offs
between solution quality and cumulative evaluation cost.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [87] [LLMs are Bayesian, in Expectation, not in Realization](https://arxiv.org/abs/2507.11768)
*Leon Chlon,Sarah Rashidi,Zein Khamis,MarcAntonio M. Awada*

Main category: stat.ML

TL;DR: 论文探讨了大语言模型在上下文学习中的能力，发现Transformer违反贝叶斯更新的基本性质，并提出了理论分析和实证验证。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在上下文学习中的行为，尤其是其与贝叶斯推断理论的矛盾，以改进不确定性量化和计算效率。

Method: 通过理论分析推导出四个关键结果，并在GPT-3上进行实证验证。

Result: 发现位置编码导致违反贝叶斯性质，Transformer在信息论上达到最优，并提供了优化推理成本的方法。

Conclusion: 论文为位置感知架构提供了校准不确定性估计和优化计算效率的实用方法。

Abstract: Large language models demonstrate remarkable in-context learning
capabilities, adapting to new tasks without parameter updates. While this
phenomenon has been successfully modeled as implicit Bayesian inference, recent
empirical findings reveal a fundamental contradiction: transformers
systematically violate the martingale property, a cornerstone requirement of
Bayesian updating on exchangeable data. This violation challenges the
theoretical foundations underlying uncertainty quantification in critical
applications.
  Our theoretical analysis establishes four key results: (1) positional
encodings induce martingale violations of order $\Theta(\log n / n)$; (2)
transformers achieve information-theoretic optimality with excess risk
$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior
representation converges to the true Bayesian posterior in the space of
sufficient statistics; and (4) we derive the optimal chain-of-thought length as
$k^* = \Theta(\sqrt{n}\log(1/\varepsilon))$ with explicit constants, providing
a principled approach to reduce inference costs while maintaining performance.
Empirical validation on GPT-3 confirms predictions (1)-(3), with transformers
reaching 99\% of theoretical entropy limits within 20 examples. Our framework
provides practical methods for extracting calibrated uncertainty estimates from
position-aware architectures and optimizing computational efficiency in
deployment.

</details>


### [88] [Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?](https://arxiv.org/abs/2507.11891)
*Shuangning Li,Chonghuan Wang,Jingyan Wang*

Main category: stat.ML

TL;DR: 论文研究了A/B实验中推荐算法性能比较的偏差问题，重点关注数据共享导致的“共生偏差”，并探讨了在决策中GTE符号的重要性。


<details>
  <summary>Details</summary>
Motivation: 标准均值差估计器在估计全局处理效应（GTE）时存在偏差，主要由于实验单元间的干扰（即数据共享导致的“共生偏差”）。研究动机在于理解这种偏差对算法选择的影响。

Method: 采用多臂老虎机框架，理论分析了数据共享下GTE估计符号与真实GTE符号的关系，并探讨了探索与利用的平衡对偏差的影响。

Result: 研究发现，探索与利用的平衡是共生偏差影响算法选择的关键因素。

Conclusion: 在决策中，GTE的符号比其精确值更重要，且探索与利用的平衡对偏差的影响显著。

Abstract: We study A/B experiments that are designed to compare the performance of two
recommendation algorithms. Prior work has shown that the standard
difference-in-means estimator is biased in estimating the global treatment
effect (GTE) due to a particular form of interference between experimental
units. Specifically, units under the treatment and control algorithms
contribute to a shared pool of data that subsequently train both algorithms,
resulting in interference between the two groups. The bias arising from this
type of data sharing is known as "symbiosis bias". In this paper, we highlight
that, for decision-making purposes, the sign of the GTE often matters more than
its precise magnitude when selecting the better algorithm. We formalize this
insight under a multi-armed bandit framework and theoretically characterize
when the sign of the expected GTE estimate under data sharing aligns with or
contradicts the sign of the true GTE. Our analysis identifies the level of
exploration versus exploitation as a key determinant of how symbiosis bias
impacts algorithm selection.

</details>


### [89] [Newfluence: Boosting Model interpretability and Understanding in High Dimensions](https://arxiv.org/abs/2507.11895)
*Haolin Zou,Arnab Auddy,Yongchan Kwon,Kamiar Rahnama Rad,Arian Maleki*

Main category: stat.ML

TL;DR: 论文探讨了高维环境下影响函数的准确性，提出了一种改进的替代方法Newfluence。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和人工智能模型的复杂性增加，需要工具来解释和优化模型决策。影响函数虽流行，但其低维假设在高维场景下不适用。

Method: 通过理论和实证分析评估影响函数在高维环境中的表现，并提出新方法Newfluence。

Result: 影响函数在高维环境中不可靠，Newfluence在保持计算效率的同时显著提高了准确性。

Conclusion: Newfluence为解释复杂AI模型提供了更准确的工具，其高维框架还可用于分析其他流行方法。

Abstract: The increasing complexity of machine learning (ML) and artificial
intelligence (AI) models has created a pressing need for tools that help
scientists, engineers, and policymakers interpret and refine model decisions
and predictions. Influence functions, originating from robust statistics, have
emerged as a popular approach for this purpose.
  However, the heuristic foundations of influence functions rely on
low-dimensional assumptions where the number of parameters $p$ is much smaller
than the number of observations $n$. In contrast, modern AI models often
operate in high-dimensional regimes with large $p$, challenging these
assumptions.
  In this paper, we examine the accuracy of influence functions in
high-dimensional settings. Our theoretical and empirical analyses reveal that
influence functions cannot reliably fulfill their intended purpose. We then
introduce an alternative approximation, called Newfluence, that maintains
similar computational efficiency while offering significantly improved
accuracy.
  Newfluence is expected to provide more accurate insights than many existing
methods for interpreting complex AI models and diagnosing their issues.
Moreover, the high-dimensional framework we develop in this paper can also be
applied to analyze other popular techniques, such as Shapley values.

</details>


### [90] [Incorporating Fairness Constraints into Archetypal Analysis](https://arxiv.org/abs/2507.12021)
*Aleix Alcacer,Irene Epifanio*

Main category: stat.ML

TL;DR: Fair Archetypal Analysis (FairAA) 和 FairKernelAA 是改进的 AA 方法，通过公平性正则化减少敏感属性的影响，同时保持解释性和结构。


<details>
  <summary>Details</summary>
Motivation: 传统 AA 可能无意中编码敏感属性，引发公平性问题。

Method: 提出 FairAA 和 FairKernelAA，加入公平性正则化，适用于线性和非线性数据。

Result: 在合成和真实数据集上验证，减少组可分性，同时保持解释方差。

Conclusion: FairAA 在效用和公平性间取得平衡，适合敏感场景的表示学习。

Abstract: Archetypal Analysis (AA) is an unsupervised learning method that represents
data as convex combinations of extreme patterns called archetypes. While AA
provides interpretable and low-dimensional representations, it can
inadvertently encode sensitive attributes, leading to fairness concerns. In
this work, we propose Fair Archetypal Analysis (FairAA), a modified formulation
that explicitly reduces the influence of sensitive group information in the
learned projections. We also introduce FairKernelAA, a nonlinear extension that
addresses fairness in more complex data distributions. Our approach
incorporates a fairness regularization term while preserving the structure and
interpretability of the archetypes. We evaluate FairAA and FairKernelAA on
synthetic datasets, including linear, nonlinear, and multi-group scenarios,
demonstrating their ability to reduce group separability -- as measured by mean
maximum discrepancy and linear separability -- without substantially
compromising explained variance. We further validate our methods on the
real-world ANSUR I dataset, confirming their robustness and practical utility.
The results show that FairAA achieves a favorable trade-off between utility and
fairness, making it a promising tool for responsible representation learning in
sensitive applications.

</details>
