{"id": "2508.06652", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06652", "abs": "https://arxiv.org/abs/2508.06652", "authors": ["Jingmao Li", "Yuanxing Chen", "Shuangge Ma", "Kuangnan Fang"], "title": "Federated Online Learning for Heterogeneous Multisource Streaming Data", "comment": null, "summary": "Federated learning has emerged as an essential paradigm for distributed\nmulti-source data analysis under privacy concerns. Most existing federated\nlearning methods focus on the ``static\" datasets. However, in many real-world\napplications, data arrive continuously over time, forming streaming datasets.\nThis introduces additional challenges for data storage and algorithm design,\nparticularly under high-dimensional settings. In this paper, we propose a\nfederated online learning (FOL) method for distributed multi-source streaming\ndata analysis. To account for heterogeneity, a personalized model is\nconstructed for each data source, and a novel ``subgroup\" assumption is\nemployed to capture potential similarities, thereby enhancing model\nperformance. We adopt the penalized renewable estimation method and the\nefficient proximal gradient descent for model training. The proposed method\naligns with both federated and online learning frameworks: raw data are not\nexchanged among sources, ensuring data privacy, and only summary statistics of\nprevious data batches are required for model updates, significantly reducing\nstorage demands. Theoretically, we establish the consistency properties for\nmodel estimation, variable selection, and subgroup structure recovery,\ndemonstrating optimal statistical efficiency. Simulations illustrate the\neffectiveness of the proposed method. Furthermore, when applied to the\nfinancial lending data and the web log data, the proposed method also exhibits\nadvantageous prediction performance. Results of the analysis also provide some\npractical insights.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5728\u7ebf\u5b66\u4e60\uff08FOL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u591a\u6e90\u6d41\u6570\u636e\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u548c\u9ad8\u7ef4\u6570\u636e\u5b58\u50a8\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u6570\u636e\u662f\u52a8\u6001\u6d41\u5f0f\u7684\uff0c\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u591a\u9488\u5bf9\u9759\u6001\u6570\u636e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e2a\u6027\u5316\u6a21\u578b\u548c\u201c\u5b50\u7ec4\u201d\u5047\u8bbe\uff0c\u7ed3\u5408\u60e9\u7f5a\u53ef\u518d\u751f\u4f30\u8ba1\u548c\u8fd1\u7aef\u68af\u5ea6\u4e0b\u964d\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6a21\u578b\u4f30\u8ba1\u3001\u53d8\u91cf\u9009\u62e9\u548c\u5b50\u7ec4\u7ed3\u6784\u6062\u590d\u7684\u4e00\u81f4\u6027\uff0c\u4eff\u771f\u548c\u5b9e\u9645\u6570\u636e\u5e94\u7528\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FOL\u65b9\u6cd5\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u5b58\u50a8\u6548\u7387\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u6d41\u5f0f\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2508.06847", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06847", "abs": "https://arxiv.org/abs/2508.06847", "authors": ["Lam Ngo", "Huong Ha", "Jeffrey Chan", "Hongyu Zhang"], "title": "MOCA-HESP: Meta High-dimensional Bayesian Optimization for Combinatorial and Mixed Spaces via Hyper-ellipsoid Partitioning", "comment": "Published at the 28th European Conference on Artificial Intelligence\n  (ECAI-2025)", "summary": "High-dimensional Bayesian Optimization (BO) has attracted significant\nattention in recent research. However, existing methods have mainly focused on\noptimizing in continuous domains, while combinatorial (ordinal and categorical)\nand mixed domains still remain challenging. In this paper, we first propose\nMOCA-HESP, a novel high-dimensional BO method for combinatorial and mixed\nvariables. The key idea is to leverage the hyper-ellipsoid space partitioning\n(HESP) technique with different categorical encoders to work with\nhigh-dimensional, combinatorial and mixed spaces, while adaptively selecting\nthe optimal encoders for HESP using a multi-armed bandit technique. Our method,\nMOCA-HESP, is designed as a \\textit{meta-algorithm} such that it can\nincorporate other combinatorial and mixed BO optimizers to further enhance the\noptimizers' performance. Finally, we develop three practical BO methods by\nintegrating MOCA-HESP with state-of-the-art BO optimizers for combinatorial and\nmixed variables: standard BO, CASMOPOLITAN, and Bounce. Our experimental\nresults on various synthetic and real-world benchmarks show that our methods\noutperform existing baselines. Our code implementation can be found at\nhttps://github.com/LamNgo1/moca-hesp", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMOCA-HESP\u7684\u9ad8\u7ef4\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ec4\u5408\u548c\u6df7\u5408\u53d8\u91cf\uff0c\u901a\u8fc7\u8d85\u692d\u7403\u7a7a\u95f4\u5212\u5206\u6280\u672f\u548c\u591a\u81c2\u8001\u864e\u673a\u9009\u62e9\u6700\u4f18\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9ad8\u7ef4\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8fde\u7eed\u57df\uff0c\u800c\u7ec4\u5408\u548c\u6df7\u5408\u57df\u7684\u4f18\u5316\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u8d85\u692d\u7403\u7a7a\u95f4\u5212\u5206\u6280\u672f\u548c\u4e0d\u540c\u5206\u7c7b\u7f16\u7801\u5668\uff0c\u5229\u7528\u591a\u81c2\u8001\u864e\u673a\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u7f16\u7801\u5668\uff0c\u5e76\u4f5c\u4e3a\u5143\u7b97\u6cd5\u6574\u5408\u5176\u4ed6\u4f18\u5316\u5668\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOCA-HESP\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MOCA-HESP\u4e3a\u9ad8\u7ef4\u7ec4\u5408\u548c\u6df7\u5408\u53d8\u91cf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.07049", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07049", "abs": "https://arxiv.org/abs/2508.07049", "authors": ["Tran Tuan Kiet", "Nguyen Thang Loi", "Vo Nguyen Le Duy"], "title": "Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation", "comment": null, "summary": "Anomaly detection (AD) plays a vital role across a wide range of domains, but\nits performance might deteriorate when applied to target domains with limited\ndata. Domain Adaptation (DA) offers a solution by transferring knowledge from a\nrelated source domain with abundant data. However, this adaptation process can\nintroduce additional uncertainty, making it difficult to draw statistically\nvalid conclusions from AD results. In this paper, we propose STAND-DA -- a\nnovel framework for statistically rigorous Autoencoder-based AD after\nRepresentation Learning-based DA. Built on the Selective Inference (SI)\nframework, STAND-DA computes valid $p$-values for detected anomalies and\nrigorously controls the false positive rate below a pre-specified level\n$\\alpha$ (e.g., 0.05). To address the computational challenges of applying SI\nto deep learning models, we develop the GPU-accelerated SI implementation,\nsignificantly enhancing both scalability and runtime performance. This\nadvancement makes SI practically feasible for modern, large-scale deep\narchitectures. Extensive experiments on synthetic and real-world datasets\nvalidate the theoretical results and computational efficiency of the proposed\nSTAND-DA method.", "AI": {"tldr": "STAND-DA\u662f\u4e00\u4e2a\u57fa\u4e8e\u9009\u62e9\u6027\u63a8\u65ad\uff08SI\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8868\u793a\u5b66\u4e60\u57df\u9002\u5e94\uff08DA\uff09\u540e\u8fdb\u884c\u7edf\u8ba1\u4e25\u683c\u7684\u5f02\u5e38\u68c0\u6d4b\uff08AD\uff09\uff0c\u901a\u8fc7\u8ba1\u7b97\u6709\u6548\u7684p\u503c\u5e76\u63a7\u5236\u5047\u9633\u6027\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u76ee\u6807\u57df\u6570\u636e\u6709\u9650\u65f6\uff0c\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\u53ca\u57df\u9002\u5e94\u5f15\u5165\u7684\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faSTAND-DA\u6846\u67b6\uff0c\u7ed3\u5408\u8868\u793a\u5b66\u4e60\u57df\u9002\u5e94\u548c\u9009\u62e9\u6027\u63a8\u65ad\uff0c\u5f00\u53d1GPU\u52a0\u901f\u7684SI\u5b9e\u73b0\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86STAND-DA\u7684\u7406\u8bba\u6709\u6548\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "STAND-DA\u4e3a\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e25\u683c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3002"}}
{"id": "2508.06985", "categories": ["cs.LG", "cs.CE", "cs.SY", "eess.SY", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.06985", "abs": "https://arxiv.org/abs/2508.06985", "authors": ["Jiawei Zhang", "Yifei Zhang", "Baozhao Yi", "Yao Ren", "Qi Jiao", "Hanyu Bai", "Weiran Jiang", "Ziyou Song"], "title": "Discovery Learning accelerates battery design evaluation", "comment": "Main text, 20 pages, 5 figures", "summary": "Fast and reliable validation of novel designs in complex physical systems\nsuch as batteries is critical to accelerating technological innovation.\nHowever, battery research and development remain bottlenecked by the\nprohibitively high time and energy costs required to evaluate numerous new\ndesign candidates, particularly in battery prototyping and life testing.\nDespite recent progress in data-driven battery lifetime prediction, existing\nmethods require labeled data of target designs to improve accuracy and cannot\nmake reliable predictions until after prototyping, thus falling far short of\nthe efficiency needed to enable rapid feedback for battery design. Here, we\nintroduce Discovery Learning (DL), a scientific machine-learning paradigm that\nintegrates active learning, physics-guided learning, and zero-shot learning\ninto a human-like reasoning loop, drawing inspiration from learning theories in\neducational psychology. DL can learn from historical battery designs and\nactively reduce the need for prototyping, thus enabling rapid lifetime\nevaluation for unobserved material-design combinations without requiring\nadditional data labeling. To test DL, we present 123 industrial-grade\nlarge-format lithium-ion pouch cells, spanning eight material-design\ncombinations and diverse cycling protocols. Trained solely on public datasets\nof small-capacity cylindrical cells, DL achieves 7.2% test error in predicting\nthe average cycle life under unknown device variability. This results in\nsavings of 98% in time and 95% in energy compared to industrial practices. This\nwork highlights the potential of uncovering insights from historical designs to\ninform and accelerate the development of next-generation battery technologies.\nDL represents a key advance toward efficient data-driven modeling and helps\nrealize the promise of machine learning for accelerating scientific discovery\nand engineering innovation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiscovery Learning\uff08DL\uff09\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u7535\u6c60\u8bbe\u8ba1\u9a8c\u8bc1\u7684\u65f6\u95f4\u548c\u80fd\u6e90\u6210\u672c\u3002", "motivation": "\u7535\u6c60\u7814\u53d1\u56e0\u9ad8\u6602\u7684\u65f6\u95f4\u548c\u80fd\u6e90\u6210\u672c\u800c\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76ee\u6807\u8bbe\u8ba1\u7684\u6807\u8bb0\u6570\u636e\u4e14\u6548\u7387\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5feb\u901f\u53cd\u9988\u9700\u6c42\u3002", "method": "DL\u6574\u5408\u4e3b\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u5229\u7528\u5386\u53f2\u7535\u6c60\u8bbe\u8ba1\u6570\u636e\uff0c\u51cf\u5c11\u539f\u578b\u6d4b\u8bd5\u9700\u6c42\u3002", "result": "\u5728123\u4e2a\u5de5\u4e1a\u7ea7\u9502\u79bb\u5b50\u7535\u6c60\u4e0a\u6d4b\u8bd5\uff0cDL\u4ec5\u7528\u516c\u5f00\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9884\u6d4b\u8bef\u5dee\u4e3a7.2%\uff0c\u8282\u770198%\u65f6\u95f4\u548c95%\u80fd\u6e90\u3002", "conclusion": "DL\u5c55\u793a\u4e86\u4ece\u5386\u53f2\u8bbe\u8ba1\u4e2d\u63d0\u53d6\u4fe1\u606f\u4ee5\u52a0\u901f\u4e0b\u4e00\u4ee3\u7535\u6c60\u6280\u672f\u5f00\u53d1\u7684\u6f5c\u529b\uff0c\u662f\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.06508", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06508", "abs": "https://arxiv.org/abs/2508.06508", "authors": ["Sameera Bharadwaja H.", "D. K. Mehra"], "title": "A Completely Blind Channel Estimation Technique for OFDM Using Generalized Constellation Splitting and Modified Phase-Directed Algorithm", "comment": null, "summary": "The problem of blind channel estimation for SISO-OFDM systems using\nsecond-order statistics (SOS) is addressed. A comparison of two prominent\nSOS-based techniques: subspace-decomposition and precoding-induced\ncorrelation-averaging techniques in terms of MSE performance is presented. The\ndrawback of these methods is that they suffer from a complex-scalar estimation\nambiguity which is resolved by using pilots or reference symbols. By using\npilots the whole purpose of blind techniques is contradicted. We propose an\nalgorithm to resolve this ambiguity in blind manner using generalized\nconstellation-splitting and modified phase-directed algorithm. The performance\nof the proposed scheme is evaluated via numerical simulations in MATLAB\nenvironment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3SISO-OFDM\u7cfb\u7edf\u4e2d\u76f2\u4fe1\u9053\u4f30\u8ba1\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e7f\u4e49\u661f\u5ea7\u5206\u88c2\u548c\u6539\u8fdb\u7684\u76f8\u4f4d\u5bfc\u5411\u7b97\u6cd5\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u590d\u6570\u6807\u91cf\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4e8c\u9636\u7edf\u8ba1\u91cf\u7684\u76f2\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u590d\u6570\u6807\u91cf\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\uff0c\u9700\u4f9d\u8d56\u5bfc\u9891\u6216\u53c2\u8003\u7b26\u53f7\u89e3\u51b3\uff0c\u8fd9\u4e0e\u76f2\u6280\u672f\u7684\u521d\u8877\u76f8\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e7f\u4e49\u661f\u5ea7\u5206\u88c2\u548c\u6539\u8fdb\u76f8\u4f4d\u5bfc\u5411\u7b97\u6cd5\u7684\u76f2\u65b9\u6cd5\uff0c\u65e0\u9700\u5bfc\u9891\u5373\u53ef\u89e3\u51b3\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u901a\u8fc7MATLAB\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f2\u4fe1\u9053\u4f30\u8ba1\u4e2d\u7684\u590d\u6570\u6807\u91cf\u6a21\u7cca\u95ee\u9898\uff0c\u907f\u514d\u4e86\u5bfc\u9891\u7684\u4f7f\u7528\uff0c\u7b26\u5408\u76f2\u6280\u672f\u7684\u521d\u8877\u3002"}}
{"id": "2508.07066", "categories": ["stat.ML", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07066", "abs": "https://arxiv.org/abs/2508.07066", "authors": ["Chenxu Zhao", "Wei Qian", "Aobo Chen", "Mengdi Huai"], "title": "Membership Inference Attacks with False Discovery Rate Control", "comment": null, "summary": "Recent studies have shown that deep learning models are vulnerable to\nmembership inference attacks (MIAs), which aim to infer whether a data record\nwas used to train a target model or not. To analyze and study these\nvulnerabilities, various MIA methods have been proposed. Despite the\nsignificance and popularity of MIAs, existing works on MIAs are limited in\nproviding guarantees on the false discovery rate (FDR), which refers to the\nexpected proportion of false discoveries among the identified positive\ndiscoveries. However, it is very challenging to ensure the false discovery rate\nguarantees, because the underlying distribution is usually unknown, and the\nestimated non-member probabilities often exhibit interdependence. To tackle the\nabove challenges, in this paper, we design a novel membership inference attack\nmethod, which can provide the guarantees on the false discovery rate.\nAdditionally, we show that our method can also provide the marginal probability\nguarantee on labeling true non-member data as member data. Notably, our method\ncan work as a wrapper that can be seamlessly integrated with existing MIA\nmethods in a post-hoc manner, while also providing the FDR control. We perform\nthe theoretical analysis for our method. Extensive experiments in various\nsettings (e.g., the black-box setting and the lifelong learning setting) are\nalso conducted to verify the desirable performance of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u63a7\u5236\u9519\u8bef\u53d1\u73b0\u7387\uff08FDR\uff09\uff0c\u5e76\u5c55\u793a\u5176\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u9519\u8bef\u53d1\u73b0\u7387\u7684\u4fdd\u8bc1\uff0c\u4e14\u5e95\u5c42\u5206\u5e03\u672a\u77e5\uff0c\u4f30\u8ba1\u6982\u7387\u76f8\u4e92\u4f9d\u8d56\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u6210\u5458\u63a8\u7406\u653b\u51fb\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u5305\u88c5\u5668\u4e0e\u73b0\u6709\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u63d0\u4f9bFDR\u63a7\u5236\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u4f9bFDR\u4fdd\u8bc1\uff0c\u8fd8\u80fd\u6807\u8bb0\u771f\u5b9e\u975e\u6210\u5458\u6570\u636e\u7684\u8fb9\u9645\u6982\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.07029", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07029", "abs": "https://arxiv.org/abs/2508.07029", "authors": ["Antonio Guillen-Perez"], "title": "From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving", "comment": null, "summary": "Learning robust driving policies from large-scale, real-world datasets is a\ncentral challenge in autonomous driving, as online data collection is often\nunsafe and impractical. While Behavioral Cloning (BC) offers a straightforward\napproach to imitation learning, policies trained with BC are notoriously\nbrittle and suffer from compounding errors in closed-loop execution. This work\npresents a comprehensive pipeline and a comparative study to address this\nlimitation. We first develop a series of increasingly sophisticated BC\nbaselines, culminating in a Transformer-based model that operates on a\nstructured, entity-centric state representation. While this model achieves low\nimitation loss, we show that it still fails in long-horizon simulations. We\nthen demonstrate that by applying a state-of-the-art Offline Reinforcement\nLearning algorithm, Conservative Q-Learning (CQL), to the same data and\narchitecture, we can learn a significantly more robust policy. Using a\ncarefully engineered reward function, the CQL agent learns a conservative value\nfunction that enables it to recover from minor errors and avoid\nout-of-distribution states. In a large-scale evaluation on 1,000 unseen\nscenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a\n3.2x higher success rate and a 7.4x lower collision rate than the strongest BC\nbaseline, proving that an offline RL approach is critical for learning robust,\nlong-horizon driving policies from static expert data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08CQL\uff09\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5728\u95ed\u73af\u6267\u884c\u4e2d\u6613\u51fa\u9519\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4ece\u9759\u6001\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u9a7e\u9a76\u7b56\u7565\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eTransformer\u7684\u884c\u4e3a\u514b\u9686\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5CQL\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728Waymo\u6570\u636e\u96c6\u4e0a\uff0cCQL\u7b56\u7565\u7684\u6210\u529f\u7387\u6bd4\u6700\u5f3aBC\u57fa\u7ebf\u9ad83.2\u500d\uff0c\u78b0\u649e\u7387\u4f4e7.4\u500d\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u662f\u4ece\u9759\u6001\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u9a7e\u9a76\u7b56\u7565\u7684\u5173\u952e\u65b9\u6cd5\u3002"}}
{"id": "2508.06672", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06672", "abs": "https://arxiv.org/abs/2508.06672", "authors": ["Jacob S. Clements", "Zachary L. Clements"], "title": "GPU-accelerated Direct Geolocation of GNSS Interference", "comment": null, "summary": "In recent years, there has been a sharp increase in Global Navigation\nSatellite Systems (GNSS) interference, which has proven to be problematic in\nGNSS-dependent civilian applications. Many currently deployed GNSS receivers\nlack the proper countermeasures to defend themselves against interference,\nprompting the need for alternative defenses. Satellites in Low Earth Orbit\n(LEO) provide an opportunity for GNSS interference detection, classification,\nand localization. The direct geolocation approach has been shown to be\nwell-suited for low SNR regimes and in cases limited to short captures --\nexactly what is expected for receivers in LEO. Direct geolocation is a\nsingle-step search over a geographical grid that enables estimation of the\ntransmitter location directly from correlating raw observed signals. However, a\nkey limitation to this approach is the computational requirements. This\ncomputational burden is compounded for LEO-based receivers as the geographic\nsearch space is extensive. This paper alleviates the computational burden of\ndirect geolocation by exploiting the independence of position-domain\ncorrelation across candidate points and time steps: nearly all computation can\nbe accomplished in parallel on a graphics processing unit (GPU). This paper\npresents and evaluates the performance of GPU-accelerated direct geolocation\ncompared to traditional CPU processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GPU\u52a0\u901f\u7684\u76f4\u63a5\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u63a5\u6536\u5668\u5728GNSS\u5e72\u6270\u68c0\u6d4b\u4e2d\u7684\u8ba1\u7b97\u8d1f\u62c5\u95ee\u9898\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0cGNSS\u5e72\u6270\u6fc0\u589e\uff0c\u73b0\u6709\u63a5\u6536\u5668\u7f3a\u4e4f\u6709\u6548\u9632\u5fa1\u63aa\u65bd\uff0c\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u4e3a\u5e72\u6270\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u4f20\u7edf\u76f4\u63a5\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u901a\u8fc7\u5229\u7528\u4f4d\u7f6e\u57df\u76f8\u5173\u6027\u5728\u5019\u9009\u70b9\u548c\u65f6\u95f4\u6b65\u4e0a\u7684\u72ec\u7acb\u6027\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5e76\u884c\u5316\u5230GPU\u4e0a\uff0c\u5b9e\u73b0\u52a0\u901f\u3002", "result": "GPU\u52a0\u901f\u7684\u76f4\u63a5\u5730\u7406\u5b9a\u4f4d\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edfCPU\u5904\u7406\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "GPU\u52a0\u901f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u76f4\u63a5\u5730\u7406\u5b9a\u4f4d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3aGNSS\u5e72\u6270\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06539", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06539", "abs": "https://arxiv.org/abs/2508.06539", "authors": ["Atahan Karagoz"], "title": "Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems", "comment": null, "summary": "Survival is traditionally modeled as a supervised learning task, reliant on\ncurated outcome labels and fixed covariates. This work rejects that premise. It\nproposes that survival is not an externally annotated target but a geometric\nconsequence: an emergent property of the curvature and flow inherent in\nbiological state space. We develop a theory of Self-Organizing Survival\nManifolds (SOSM), in which survival-relevant dynamics arise from low-curvature\ngeodesic flows on latent manifolds shaped by internal biological constraints. A\nsurvival energy functional based on geodesic curvature minimization is\nintroduced and shown to induce structures where prognosis aligns with geometric\nflow stability. We derive discrete and continuous formulations of the objective\nand prove theoretical results demonstrating the emergence and convergence of\nsurvival-aligned trajectories under biologically plausible conditions. The\nframework draws connections to thermodynamic efficiency, entropy flow, Ricci\ncurvature, and optimal transport, grounding survival modeling in physical law.\nHealth, disease, aging, and death are reframed as geometric phase transitions\nin the manifold's structure. This theory offers a universal, label-free\nfoundation for modeling survival as a property of form, not annotation-bridging\nmachine learning, biophysics, and the geometry of life itself.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u5b58\u6a21\u578b\u7406\u8bba\uff0c\u5c06\u751f\u5b58\u89c6\u4e3a\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u4e2d\u51e0\u4f55\u7279\u6027\u7684\u7ed3\u679c\uff0c\u800c\u975e\u4f20\u7edf\u7684\u6709\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u751f\u5b58\u6a21\u578b\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u548c\u56fa\u5b9a\u534f\u53d8\u91cf\uff0c\u800c\u8be5\u7814\u7a76\u8ba4\u4e3a\u751f\u5b58\u662f\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u4e2d\u51e0\u4f55\u6d41\u52a8\u548c\u66f2\u7387\u7684\u81ea\u7136\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u81ea\u7ec4\u7ec7\u751f\u5b58\u6d41\u5f62\uff08SOSM\uff09\u7406\u8bba\uff0c\u57fa\u4e8e\u4f4e\u66f2\u7387\u6d4b\u5730\u6d41\u548c\u751f\u7269\u7ea6\u675f\u6784\u5efa\u751f\u5b58\u80fd\u91cf\u6cdb\u51fd\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u5728\u751f\u7269\u5408\u7406\u6761\u4ef6\u4e0b\uff0c\u751f\u5b58\u5bf9\u9f50\u8f68\u8ff9\u7684\u51fa\u73b0\u548c\u6536\u655b\uff0c\u5e76\u5c06\u5065\u5eb7\u3001\u75be\u75c5\u3001\u8870\u8001\u548c\u6b7b\u4ea1\u89c6\u4e3a\u6d41\u5f62\u7ed3\u6784\u7684\u51e0\u4f55\u76f8\u53d8\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u751f\u5b58\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u7684\u901a\u7528\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u673a\u5668\u5b66\u4e60\u3001\u751f\u7269\u7269\u7406\u548c\u751f\u547d\u51e0\u4f55\u5b66\u3002"}}
{"id": "2508.07876", "categories": ["stat.ML", "cs.LG", "math.DS", "math.ST", "stat.TH", "37B02, 37B55, 37H05, 37N35, 62M10, 68T05"], "pdf": "https://arxiv.org/pdf/2508.07876", "abs": "https://arxiv.org/abs/2508.07876", "authors": ["Juan-Pablo Ortega", "Florian Rossmannek"], "title": "Stochastic dynamics learning with state-space systems", "comment": null, "summary": "This work advances the theoretical foundations of reservoir computing (RC) by\nproviding a unified treatment of fading memory and the echo state property\n(ESP) in both deterministic and stochastic settings. We investigate state-space\nsystems, a central model class in time series learning, and establish that\nfading memory and solution stability hold generically -- even in the absence of\nthe ESP -- offering a robust explanation for the empirical success of RC models\nwithout strict contractivity conditions. In the stochastic case, we critically\nassess stochastic echo states, proposing a novel distributional perspective\nrooted in attractor dynamics on the space of probability distributions, which\nleads to a rich and coherent theory. Our results extend and generalize previous\nwork on non-autonomous dynamical systems, offering new insights into causality,\nstability, and memory in RC models. This lays the groundwork for reliable\ngenerative modeling of temporal data in both deterministic and stochastic\nregimes.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7edf\u4e00\u5904\u7406\u786e\u5b9a\u6027\u53ca\u968f\u673a\u6027\u8bbe\u7f6e\u4e2d\u7684\u8870\u51cf\u8bb0\u5fc6\u548c\u56de\u58f0\u72b6\u6001\u7279\u6027\uff08ESP\uff09\uff0c\u63a8\u8fdb\u4e86\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\u7684\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e3aRC\u6a21\u578b\u7684\u5b9e\u8bc1\u6210\u529f\u63d0\u4f9b\u7406\u8bba\u652f\u6301\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u4e25\u683c\u6536\u7f29\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u6269\u5c55\u5bf9\u975e\u81ea\u4e3b\u52a8\u529b\u7cfb\u7edf\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5206\u6790\u72b6\u6001\u7a7a\u95f4\u7cfb\u7edf\uff0c\u63a2\u8ba8\u8870\u51cf\u8bb0\u5fc6\u548c\u89e3\u51b3\u65b9\u6848\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u968f\u673a\u60c5\u51b5\u4e0b\u63d0\u51fa\u57fa\u4e8e\u6982\u7387\u5206\u5e03\u5438\u5f15\u5b50\u52a8\u529b\u5b66\u7684\u65b0\u89c6\u89d2\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8870\u51cf\u8bb0\u5fc6\u548c\u7a33\u5b9a\u6027\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u6210\u7acb\uff0c\u4e14\u968f\u673a\u56de\u58f0\u72b6\u6001\u7684\u65b0\u7406\u8bba\u6846\u67b6\u4e3aRC\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u4e3a\u786e\u5b9a\u6027\u53ca\u968f\u673a\u6027\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u53ef\u9760\u751f\u6210\u5efa\u6a21\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6df1\u5316\u4e86\u5bf9RC\u6a21\u578b\u4e2d\u56e0\u679c\u6027\u3001\u7a33\u5b9a\u6027\u548c\u8bb0\u5fc6\u7684\u7406\u89e3\u3002"}}
{"id": "2508.07206", "categories": ["eess.SP", "cs.NA", "cs.SY", "eess.SY", "math.NA", "42C10, 94A12", "G.1.2; I.6.6"], "pdf": "https://arxiv.org/pdf/2508.07206", "abs": "https://arxiv.org/abs/2508.07206", "authors": ["Konstantin A. Rybakov", "Egor D. Shermatov"], "title": "Applying the Spectral Method for Modeling Linear Filters: Butterworth, Linkwitz-Riley, and Chebyshev filters", "comment": null, "summary": "This paper proposes a new technique for computer modeling linear filters\nbased on the spectral form of mathematical description of linear systems. It\nassumes that input and output signals of the filter are represented as\northogonal expansions, while filters themselves are described by\ntwo-dimensional non-stationary transfer functions. This technique allows one to\nmodel the output signal in continuous time, and it is successfully tested on\nthe Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u7cfb\u7edf\u6570\u5b66\u63cf\u8ff0\u8c31\u5f62\u5f0f\u7684\u65b0\u578b\u8ba1\u7b97\u673a\u5efa\u6a21\u7ebf\u6027\u6ee4\u6ce2\u5668\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u6ee4\u6ce2\u5668\u5efa\u6a21\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6280\u672f\u3002", "method": "\u8f93\u5165\u548c\u8f93\u51fa\u4fe1\u53f7\u8868\u793a\u4e3a\u6b63\u4ea4\u5c55\u5f00\uff0c\u6ee4\u6ce2\u5668\u7528\u4e8c\u7ef4\u975e\u5e73\u7a33\u4f20\u9012\u51fd\u6570\u63cf\u8ff0\u3002", "result": "\u6210\u529f\u6d4b\u8bd5\u4e86Butterworth\u3001Linkwitz-Riley\u548cChebyshev\u6ee4\u6ce2\u5668\u4e0d\u540c\u9636\u6570\u3002", "conclusion": "\u8be5\u6280\u672f\u80fd\u6709\u6548\u5efa\u6a21\u8fde\u7eed\u65f6\u95f4\u8f93\u51fa\u4fe1\u53f7\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6ee4\u6ce2\u5668\u7c7b\u578b\u3002"}}
{"id": "2508.06794", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06794", "abs": "https://arxiv.org/abs/2508.06794", "authors": ["Rui Meng", "Xiaodong Xu", "Bizhu Wang", "Hao Sun", "Shida Xia", "Shujun Han", "Ping Zhang"], "title": "Physical Layer Authentication Based on Hierarchical Variational Auto-Encoder for Industrial Internet of Things", "comment": "17 pages, 13 figures", "summary": "Recently, Physical Layer Authentication (PLA) has attracted much attention\nsince it takes advantage of the channel randomness nature of transmission media\nto achieve communication confidentiality and authentication. In the complex\nenvironment, such as the Industrial Internet of Things (IIoT), machine learning\n(ML) is widely employed with PLA to extract and analyze complex channel\ncharacteristics for identity authentication. However, most PLA schemes for IIoT\nrequire attackers' prior channel information, leading to severe performance\ndegradation when the source of the received signals is unknown in the training\nstage. Thus, a channel impulse response (CIR)-based PLA scheme named\n\"Hierarchical Variational Auto-Encoder (HVAE)\" for IIoT is proposed in this\narticle, aiming at achieving high authentication performance without knowing\nattackers' prior channel information even when trained on a few data in the\ncomplex environment. HVAE consists of an Auto-Encoder (AE) module for CIR\ncharacteristics extraction and a Variational Auto-Encoder (VAE) module for\nimproving the representation ability of the CIR characteristic and outputting\nthe authentication results. Besides, a new objective function is constructed in\nwhich both the single-peak and the double-peak Gaussian distribution are taken\ninto consideration in the VAE module. Moreover, the simulations are conducted\nunder the static and mobile IIoT scenario, which verify the superiority of the\nproposed HVAE over three comparison PLA schemes even with a few training data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u9053\u51b2\u6fc0\u54cd\u5e94\uff08CIR\uff09\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u65b9\u6848HVAE\uff0c\u7528\u4e8e\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\uff0c\u65e0\u9700\u653b\u51fb\u8005\u5148\u9a8c\u4fe1\u9053\u4fe1\u606f\u5373\u53ef\u5b9e\u73b0\u9ad8\u6027\u80fd\u8ba4\u8bc1\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u5982IIoT\uff0c\u73b0\u6709\u7269\u7406\u5c42\u8ba4\u8bc1\uff08PLA\uff09\u65b9\u6848\u9700\u8981\u653b\u51fb\u8005\u7684\u5148\u9a8c\u4fe1\u9053\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HVAE\u7ed3\u5408\u4e86\u81ea\u52a8\u7f16\u7801\u5668\uff08AE\uff09\u548c\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\uff08VAE\uff09\u6a21\u5757\uff0c\u63d0\u53d6CIR\u7279\u5f81\u5e76\u8f93\u51fa\u8ba4\u8bc1\u7ed3\u679c\uff0c\u540c\u65f6\u8003\u8651\u4e86\u5355\u5cf0\u548c\u53cc\u5cf0\u9ad8\u65af\u5206\u5e03\u7684\u65b0\u76ee\u6807\u51fd\u6570\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHVAE\u5728\u9759\u6001\u548c\u79fb\u52a8IIoT\u573a\u666f\u4e0b\u4f18\u4e8e\u5176\u4ed6\u4e09\u79cdPLA\u65b9\u6848\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u3002", "conclusion": "HVAE\u662f\u4e00\u79cd\u9ad8\u6548\u7684PLA\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684IIoT\u8ba4\u8bc1\u3002"}}
{"id": "2508.06574", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06574", "abs": "https://arxiv.org/abs/2508.06574", "authors": ["Fatemeh Moradi", "Mehran Tarif", "Mohammadhossein Homaei"], "title": "Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering", "comment": "Six Pages, two Figures and six Tables", "summary": "Detecting fraud in modern supply chains is a growing challenge, driven by the\ncomplexity of global networks and the scarcity of labeled data. Traditional\ndetection methods often struggle with class imbalance and limited supervision,\nreducing their effectiveness in real-world applications. This paper proposes a\nnovel two-phase learning framework to address these challenges. In the first\nphase, the Isolation Forest algorithm performs unsupervised anomaly detection\nto identify potential fraud cases and reduce the volume of data requiring\nfurther analysis. In the second phase, a self-training Support Vector Machine\n(SVM) refines the predictions using both labeled and high-confidence\npseudo-labeled samples, enabling robust semi-supervised learning. The proposed\nmethod is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive\nreal-world supply chain dataset with fraud indicators. It achieves an F1-score\nof 0.817 while maintaining a false positive rate below 3.0%. These results\ndemonstrate the effectiveness and efficiency of combining unsupervised\npre-filtering with semi-supervised refinement for supply chain fraud detection\nunder real-world constraints, though we acknowledge limitations regarding\nconcept drift and the need for comparison with deep learning approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u7528\u4e8e\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u4ee3\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u9762\u4e34\u590d\u6742\u6027\u548c\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6709\u9650\u76d1\u7763\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Isolation Forest\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u81ea\u8bad\u7ec3SVM\u7ed3\u5408\u6807\u8bb0\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u8bb0\u6837\u672c\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728DataCo\u6570\u636e\u96c6\u4e0aF1\u5f97\u5206\u4e3a0.817\uff0c\u5047\u9633\u6027\u7387\u4f4e\u4e8e3.0%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u7ea6\u675f\u4e0b\u6709\u6548\uff0c\u4f46\u9700\u6ce8\u610f\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6bd4\u8f83\u3002"}}
{"id": "2508.07914", "categories": ["stat.ML", "cs.IR", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.07914", "abs": "https://arxiv.org/abs/2508.07914", "authors": ["Olivier Jeunen"], "title": "Meta Off-Policy Estimation", "comment": "To appear in the Nineteenth ACM Conference on Recommender Systems\n  (RecSys '25)", "summary": "Off-policy estimation (OPE) methods enable unbiased offline evaluation of\nrecommender systems, directly estimating the online reward some target policy\nwould have obtained, from offline data and with statistical guarantees. The\ntheoretical elegance of the framework combined with practical successes have\nled to a surge of interest, with many competing estimators now available to\npractitioners and researchers. Among these, Doubly Robust methods provide a\nprominent strategy to combine value- and policy-based estimators.\n  In this work, we take an alternative perspective to combine a set of OPE\nestimators and their associated confidence intervals into a single, more\naccurate estimate. Our approach leverages a correlated fixed-effects\nmeta-analysis framework, explicitly accounting for dependencies among\nestimators that arise due to shared data. This yields a best linear unbiased\nestimate (BLUE) of the target policy's value, along with an appropriately\nconservative confidence interval that reflects inter-estimator correlation. We\nvalidate our method on both simulated and real-world data, demonstrating\nimproved statistical efficiency over existing individual estimators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5206\u6790\u7684OPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u4f30\u8ba1\u5668\u53ca\u5176\u7f6e\u4fe1\u533a\u95f4\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u5355\u4e00\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709OPE\u65b9\u6cd5\u4f17\u591a\uff0c\u4f46\u7f3a\u4e4f\u4e00\u79cd\u6709\u6548\u6574\u5408\u591a\u4e2a\u4f30\u8ba1\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u548c\u7edf\u8ba1\u6548\u7387\u3002", "method": "\u91c7\u7528\u76f8\u5173\u56fa\u5b9a\u6548\u5e94\u5143\u5206\u6790\u6846\u67b6\uff0c\u8003\u8651\u4f30\u8ba1\u5668\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u751f\u6210\u6700\u4f73\u7ebf\u6027\u65e0\u504f\u4f30\u8ba1\uff08BLUE\uff09\u548c\u4fdd\u5b88\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7edf\u8ba1\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u5355\u4e2a\u4f30\u8ba1\u5668\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aOPE\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6574\u5408\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u3002"}}
{"id": "2508.07841", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07841", "abs": "https://arxiv.org/abs/2508.07841", "authors": ["Carlo Cena", "Mauro Martini", "Marcello Chiaberge"], "title": "Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow", "comment": "In review", "summary": "Attitude control is a fundamental aspect of spacecraft operations. Model\nPredictive Control (MPC) has emerged as a powerful strategy for these tasks,\nrelying on accurate models of the system dynamics to optimize control actions\nover a prediction horizon. In scenarios where physics models are incomplete,\ndifficult to derive, or computationally expensive, machine learning offers a\nflexible alternative by learning the system behavior directly from data.\nHowever, purely data-driven models often struggle with generalization and\nstability, especially when applied to inputs outside their training domain. To\naddress these limitations, we investigate the benefits of incorporating\nPhysics-Informed Neural Networks (PINNs) into the learning of spacecraft\nattitude dynamics, comparing their performance with that of purely data-driven\napproaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network\narchitecture with a self-attention mechanism, we trained several models on\nsimulated data generated with the Basilisk simulator. Two training strategies\nwere considered: a purely data-driven baseline and a physics-informed variant\nto improve robustness and stability. Our results demonstrate that the inclusion\nof physics-based information significantly enhances the performance in terms of\nthe mean relative error of the best architectures found by 27.08%. These\nadvantages are particularly evident when the learned models are integrated into\nan MPC framework, where PINN-based models consistently outperform their purely\ndata-driven counterparts in terms of control accuracy and robustness, yielding\nimprovements of up to 42.86% in performance stability error and increased\nrobustness-to-noise.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5c06\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5f15\u5165\u822a\u5929\u5668\u59ff\u6001\u52a8\u529b\u5b66\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u76f8\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0cPINNs\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u7269\u7406\u6a21\u578b\u4e0d\u5b8c\u6574\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6cdb\u5316\u6027\u548c\u7a33\u5b9a\u6027\u4e0d\u8db3\uff0c\u56e0\u6b64\u63a2\u7d22PINNs\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Real NVP\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u57fa\u4e8eBasilisk\u6a21\u62df\u5668\u751f\u6210\u7684\u6570\u636e\uff0c\u6bd4\u8f83\u7eaf\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u4fe1\u606f\u53d8\u4f53\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u7269\u7406\u4fe1\u606f\u6a21\u578b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u964d\u4f4e\u4e8627.08%\uff0c\u5728MPC\u6846\u67b6\u4e2d\u63a7\u5236\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u63d0\u5347\u9ad8\u8fbe42.86%\u3002", "conclusion": "PINNs\u663e\u8457\u63d0\u5347\u4e86\u822a\u5929\u5668\u59ff\u6001\u63a7\u5236\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728MPC\u6846\u67b6\u4e2d\u8868\u73b0\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002"}}
{"id": "2508.06829", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06829", "abs": "https://arxiv.org/abs/2508.06829", "authors": ["K. A. Shahriar"], "title": "Deep Domain-Adversarial Adaptation for Automatic Modulation Classification under Channel Variability", "comment": "5 pages, 3 figures", "summary": "Automatic Modulation Classification (AMC) plays a significant role in modern\ncognitive and intelligent radio systems, where accurate identification of\nmodulation is crucial for adaptive communication. The presence of heterogeneous\nwireless channel conditions, such as Rayleigh and Rician fading, poses\nsignificant challenges to the generalization ability of conventional AMC\nmodels. In this work, a domain-adversarial neural network (DANN) based deep\nlearning framework is proposed that explicitly mitigates channel-induced\ndistribution shifts between source and target domains. The approach is\nevaluated using a comprehensive simulated dataset containing five modulation\nschemes (BPSK, QPSK, 16QAM, 64QAM, 256QAM) across Rayleigh and Rician fading\nchannels at five frequency bands. Comparative experiments demonstrate that the\nDANN-based model achieves up to 14.93% absolute accuracy improvement in certain\nmodulation cases compared to a baseline supervised model trained solely on the\nsource domain. The findings establish the engineering feasibility of domain\nadversarial learning in AMC tasks under real-world channel variability and\noffer a robust direction for future research in adaptive spectrum intelligence", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDANN\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3AMC\u4efb\u52a1\u4e2d\u4fe1\u9053\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u8ba4\u77e5\u65e0\u7ebf\u7535\u7cfb\u7edf\u4e2d\uff0cAMC\u7684\u51c6\u786e\u6027\u5bf9\u81ea\u9002\u5e94\u901a\u4fe1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u5728\u5f02\u6784\u4fe1\u9053\u6761\u4ef6\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u6846\u67b6\uff0c\u663e\u5f0f\u7f13\u89e3\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u4fe1\u9053\u8bf1\u5bfc\u5206\u5e03\u504f\u79fb\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\uff0cDANN\u6a21\u578b\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5728\u67d0\u4e9b\u8c03\u5236\u60c5\u51b5\u4e0b\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u534714.93%\u3002", "conclusion": "DANN\u5728\u771f\u5b9e\u4fe1\u9053\u53d8\u5f02\u6027\u4e0b\u5177\u6709\u5de5\u7a0b\u53ef\u884c\u6027\uff0c\u4e3a\u81ea\u9002\u5e94\u9891\u8c31\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u5411\u3002"}}
{"id": "2508.06576", "categories": ["cs.LG", "q-bio.BM", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2508.06576", "abs": "https://arxiv.org/abs/2508.06576", "authors": ["Azmine Toushik Wasi"], "title": "GFlowNets for Learning Better Drug-Drug Interaction Representations", "comment": "Accepted to ICANN 2025:AIDD", "summary": "Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNet\uff09\u548c\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\uff08VGAE\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7f55\u89c1\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u7684\u5408\u6210\u6837\u672c\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\u9884\u6d4b\u4e2d\uff0c\u7f55\u89c1\u4f46\u5173\u952e\u7684\u76f8\u4e92\u4f5c\u7528\u5728\u6570\u636e\u96c6\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06DDI\u9884\u6d4b\u89c6\u4e3a\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u5ffd\u89c6\u4e86\u7c7b\u522b\u7279\u5f02\u6027\uff0c\u52a0\u5267\u4e86\u5bf9\u5e38\u89c1\u76f8\u4e92\u4f5c\u7528\u7684\u504f\u5411\u3002", "method": "\u7ed3\u5408GFlowNet\u548cVGAE\u751f\u6210\u7f55\u89c1\u7c7b\u522b\u7684\u5408\u6210\u6837\u672c\uff0c\u4ee5\u5e73\u8861\u6570\u636e\u96c6\u5e76\u751f\u6210\u65b0\u9896\u6709\u6548\u7684DDI\u5bf9\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u4e0d\u540c\u7c7b\u578b\u836f\u7269\u76f8\u4e92\u4f5c\u7528\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86DDI\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7f55\u89c1\u76f8\u4e92\u4f5c\u7528\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2508.07928", "categories": ["stat.ML", "cs.LG", "math.OC", "math.PR", "math.ST", "stat.TH", "60F05, 62L20"], "pdf": "https://arxiv.org/pdf/2508.07928", "abs": "https://arxiv.org/abs/2508.07928", "authors": ["Bogdan Butyrin", "Artemy Rubtsov", "Alexey Naumov", "Vladimir Ulyanov", "Sergey Samsonov"], "title": "Gaussian Approximation for Two-Timescale Linear Stochastic Approximation", "comment": null, "summary": "In this paper, we establish non-asymptotic bounds for accuracy of normal\napproximation for linear two-timescale stochastic approximation (TTSA)\nalgorithms driven by martingale difference or Markov noise. Focusing on both\nthe last iterate and Polyak-Ruppert averaging regimes, we derive bounds for\nnormal approximation in terms of the convex distance between probability\ndistributions. Our analysis reveals a non-trivial interaction between the fast\nand slow timescales: the normal approximation rate for the last iterate\nimproves as the timescale separation increases, while it decreases in the\nPolyak-Ruppert averaged setting. We also provide the high-order moment bounds\nfor the error of linear TTSA algorithm, which may be of independent interest.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u53cc\u65f6\u95f4\u5c3a\u5ea6\u968f\u673a\u903c\u8fd1\uff08TTSA\uff09\u7b97\u6cd5\u7684\u6b63\u6001\u903c\u8fd1\u7cbe\u5ea6\uff0c\u5206\u6790\u4e86\u6700\u540e\u8fed\u4ee3\u548cPolyak-Ruppert\u5e73\u5747\u4e24\u79cd\u60c5\u51b5\u4e0b\u51f8\u8ddd\u79bb\u7684\u6982\u7387\u5206\u5e03\u754c\u9650\u3002", "motivation": "\u63a2\u8ba8\u7ebf\u6027TTSA\u7b97\u6cd5\u5728\u9a6c\u5c14\u53ef\u592b\u566a\u58f0\u6216\u9785\u5dee\u9a71\u52a8\u4e0b\u7684\u6b63\u6001\u903c\u8fd1\u7cbe\u5ea6\uff0c\u63ed\u793a\u5feb\u6162\u65f6\u95f4\u5c3a\u5ea6\u4e4b\u95f4\u7684\u975e\u5e73\u51e1\u4ea4\u4e92\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u51f8\u8ddd\u79bb\u5206\u6790\u6982\u7387\u5206\u5e03\uff0c\u7814\u7a76\u6700\u540e\u8fed\u4ee3\u548cPolyak-Ruppert\u5e73\u5747\u4e24\u79cd\u60c5\u5f62\uff0c\u5e76\u63a8\u5bfc\u9ad8\u9636\u77e9\u8bef\u5dee\u754c\u9650\u3002", "result": "\u53d1\u73b0\u6700\u540e\u8fed\u4ee3\u7684\u6b63\u6001\u903c\u8fd1\u901f\u7387\u968f\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u589e\u52a0\u800c\u63d0\u9ad8\uff0c\u800cPolyak-Ruppert\u5e73\u5747\u4e0b\u5219\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u5bf9TTSA\u7b97\u6cd5\u6b63\u6001\u903c\u8fd1\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u9636\u77e9\u8bef\u5dee\u754c\u9650\u7684\u72ec\u7acb\u7ed3\u679c\u3002"}}
{"id": "2508.08137", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08137", "abs": "https://arxiv.org/abs/2508.08137", "authors": ["Pravallika Abbineni", "Saoud Aldowaish", "Colin Liechty", "Soroosh Noorzad", "Ali Ghazizadeh", "Morteza Fayazi"], "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation", "comment": null, "summary": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100.", "AI": {"tldr": "MuaLLM\u662f\u4e00\u79cd\u5f00\u6e90\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u7528\u4e8e\u7535\u8def\u8bbe\u8ba1\u8f85\u52a9\uff0c\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u548c\u81ea\u9002\u5e94\u7535\u8def\u8bbe\u8ba1\u7814\u7a76\u8bba\u6587\u5411\u91cf\u6570\u636e\u5e93\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u8def\u8bbe\u8ba1\u6587\u732e\u7efc\u8ff0\u9762\u4e34\u5feb\u901f\u66f4\u65b0\u7684\u7814\u7a76\u3001\u6570\u636e\u8868\u793a\u4e0d\u4e00\u81f4\u548c\u4f18\u5316\u76ee\u6807\u590d\u6742\u7b49\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u5de5\u5177\u8f85\u52a9\u3002", "method": "MuaLLM\u91c7\u7528Reason + Act\uff08ReAct\uff09\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\uff0c\u52a8\u6001\u68c0\u7d22\u548c\u5b9e\u65f6\u6570\u636e\u5e93\u66f4\u65b0\uff0c\u5206\u79bb\u68c0\u7d22\u4e0e\u63a8\u7406\u4ee5\u6269\u5c55\u5904\u7406\u80fd\u529b\u3002", "result": "MuaLLM\u5728RAG-250\u548cReas-100\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523090.1%\u53ec\u56de\u7387\u548c86.8%\u51c6\u786e\u7387\uff0c\u6210\u672c\u964d\u4f4e10\u500d\uff0c\u901f\u5ea6\u63d0\u53471.6\u500d\u3002", "conclusion": "MuaLLM\u4e3a\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u8f85\u52a9\u5de5\u5177\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06868", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06868", "abs": "https://arxiv.org/abs/2508.06868", "authors": ["Bin Lyu", "Jiayu Guan", "Meng Hua", "Changsheng You", "Tianqi Mao", "Abbas Jamalipour"], "title": "Secure Transmission for Cell-Free Symbiotic Radio Communications with Movable Antenna: Continuous and Discrete Positioning Designs", "comment": "14 pages,6 figures", "summary": "In this paper, we study a movable antenna (MA) empowered secure transmission\nscheme for reconfigurable intelligent surface (RIS) aided cell-free symbiotic\nradio (SR) system. Specifically, the MAs deployed at distributed access points\n(APs) work collaboratively with the RIS to establish high-quality propagation\nlinks for both primary and secondary transmissions, as well as suppressing the\nrisk of eavesdropping on confidential primary information. We consider both\ncontinuous and discrete MA position cases and maximize the secrecy rate of\nprimary transmission under the secondary transmission constraints,\nrespectively. For the continuous position case, we propose a two-layer\niterative optimization method based on differential evolution with one-in-one\nrepresentation (DEO), to find a high-quality solution with relatively moderate\ncomputational complexity. For the discrete position case, we first extend the\nDEO based iterative framework by introducing the mapping and determination\noperations to handle the characteristic of discrete MA positions. To further\nreduce the computational complexity, we then design an alternating optimization\n(AO) iterative framework to solve all variables within a single layer. In\nparticular, we develop an efficient strategy to derive the sub-optimal solution\nfor the discrete MA positions, superseding the DEO-based method. Numerical\nresults validate the effectiveness of the proposed MA empowered secure\ntransmission scheme along with its optimization algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u53ef\u79fb\u52a8\u5929\u7ebf\uff08MA\uff09\u7684\u5b89\u5168\u4f20\u8f93\u65b9\u6848\uff0c\u7528\u4e8e\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u8f85\u52a9\u7684\u65e0\u8702\u7a9d\u5171\u751f\u65e0\u7ebf\u7535\uff08SR\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316MA\u4f4d\u7f6e\u63d0\u5347\u4e3b\u6b21\u4f20\u8f93\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u63d0\u5347\u65e0\u8702\u7a9d\u5171\u751f\u65e0\u7ebf\u7535\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u4f20\u8f93\u6548\u7387\uff0c\u540c\u65f6\u6291\u5236\u7a83\u542c\u98ce\u9669\u3002", "method": "\u9488\u5bf9\u8fde\u7eed\u548c\u79bb\u6563MA\u4f4d\u7f6e\u60c5\u51b5\uff0c\u5206\u522b\u63d0\u51fa\u57fa\u4e8e\u5dee\u5206\u8fdb\u5316\uff08DEO\uff09\u7684\u4e24\u5c42\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u548c\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u6846\u67b6\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u53ca\u5176\u4f18\u5316\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MA\u8d4b\u80fd\u7684\u5b89\u5168\u4f20\u8f93\u65b9\u6848\u53ca\u5176\u4f18\u5316\u7b97\u6cd5\u5728\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2508.06587", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06587", "abs": "https://arxiv.org/abs/2508.06587", "authors": ["A. Quadir", "M. Tanveer"], "title": "Hypergraph Neural Network with State Space Models for Node Classification", "comment": null, "summary": "In recent years, graph neural networks (GNNs) have gained significant\nattention for node classification tasks on graph-structured data. However,\ntraditional GNNs primarily focus on adjacency relationships between nodes,\noften overlooking the rich role-based characteristics that are crucial for\nlearning more expressive node representations. Existing methods for capturing\nrole-based features are largely unsupervised and fail to achieve optimal\nperformance in downstream tasks. To address these limitations, we propose a\nnovel hypergraph neural network with state space model (HGMN) that effectively\nintegrates role-aware representations into GNNs and the state space model. HGMN\nutilizes hypergraph construction techniques to model higher-order relationships\nand combines role-based and adjacency-based representations through a learnable\nmamba transformer mechanism. By leveraging two distinct hypergraph construction\nmethods-based on node degree and neighborhood levels, it strengthens the\nconnections among nodes with similar roles, enhancing the model's\nrepresentational power. Additionally, the inclusion of hypergraph convolution\nlayers enables the model to capture complex dependencies within hypergraph\nstructures. To mitigate the over-smoothing problem inherent in deep GNNs, we\nincorporate a residual network, ensuring improved stability and better feature\npropagation across layers. Extensive experiments conducted on one newly\nintroduced dataset and four benchmark datasets demonstrate the superiority of\nHGMN. The model achieves significant performance improvements on node\nclassification tasks compared to state-of-the-art GNN methods. These results\nhighlight HGMN's ability to provide enriched node representations by\neffectively embedding role-based features alongside adjacency information,\nmaking it a versatile and powerful tool for a variety of graph-based learning\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8d85\u56fe\u795e\u7ecf\u7f51\u7edcHGMN\uff0c\u7ed3\u5408\u89d2\u8272\u611f\u77e5\u8868\u793a\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfGNN\u4e3b\u8981\u5173\u6ce8\u8282\u70b9\u95f4\u7684\u90bb\u63a5\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u89d2\u8272\u7279\u5f81\u5bf9\u8282\u70b9\u8868\u793a\u7684\u91cd\u8981\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u65e0\u76d1\u7763\u4e14\u6027\u80fd\u4e0d\u8db3\u3002", "method": "HGMN\u901a\u8fc7\u8d85\u56fe\u6784\u5efa\u6280\u672f\u5efa\u6a21\u9ad8\u9636\u5173\u7cfb\uff0c\u7ed3\u5408\u89d2\u8272\u548c\u90bb\u63a5\u8868\u793a\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684mamba transformer\u673a\u5236\uff0c\u5e76\u5f15\u5165\u6b8b\u5dee\u7f51\u7edc\u9632\u6b62\u8fc7\u5e73\u6ed1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cHGMN\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709GNN\u65b9\u6cd5\u3002", "conclusion": "HGMN\u901a\u8fc7\u6709\u6548\u878d\u5408\u89d2\u8272\u7279\u5f81\u548c\u90bb\u63a5\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u8282\u70b9\u8868\u793a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2508.07982", "categories": ["stat.ML", "cs.LG", "stat.ME", "62G10, 62G20, 62H15, 62H20, 60G15, 46E22"], "pdf": "https://arxiv.org/pdf/2508.07982", "abs": "https://arxiv.org/abs/2508.07982", "authors": ["Leonardo V. Santoro", "Victor M. Panaretos"], "title": "Likelihood Ratio Tests by Kernel Gaussian Embedding", "comment": null, "summary": "We propose a novel kernel-based nonparametric two-sample test, employing the\ncombined use of kernel mean and kernel covariance embedding. Our test builds on\nrecent results showing how such combined embeddings map distinct probability\nmeasures to mutually singular Gaussian measures on the kernel's RKHS.\nLeveraging this result, we construct a test statistic based on the relative\nentropy between the Gaussian embeddings, i.e.\\ the likelihood ratio. The\nlikelihood ratio is specifically tailored to detect equality versus singularity\nof two Gaussians, and satisfies a ``$0/\\infty$\" law, in that it vanishes under\nthe null and diverges under the alternative. To implement the test in finite\nsamples, we introduce a regularised version, calibrated by way of permutation.\nWe prove consistency, establish uniform power guarantees under mild conditions,\nand discuss how our framework unifies and extends prior approaches based on\nspectrally regularized MMD. Empirical results on synthetic and real data\ndemonstrate remarkable gains in power compared to state-of-the-art methods,\nparticularly in high-dimensional and weak-signal regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u7684\u975e\u53c2\u6570\u4e24\u6837\u672c\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7ed3\u5408\u6838\u5747\u503c\u548c\u6838\u534f\u65b9\u5dee\u5d4c\u5165\uff0c\u5229\u7528\u9ad8\u65af\u5d4c\u5165\u7684\u76f8\u5bf9\u71b5\u6784\u9020\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u548c\u5f31\u4fe1\u53f7\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u548c\u5f31\u4fe1\u53f7\u573a\u666f\u4e0b\u6027\u80fd\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u4e24\u6837\u672c\u68c0\u9a8c\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6838\u5747\u503c\u548c\u6838\u534f\u65b9\u5dee\u5d4c\u5165\uff0c\u6784\u9020\u57fa\u4e8e\u9ad8\u65af\u5d4c\u5165\u76f8\u5bf9\u71b5\u7684\u68c0\u9a8c\u7edf\u8ba1\u91cf\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u548c\u6392\u5217\u6821\u51c6\u5b9e\u73b0\u6709\u9650\u6837\u672c\u4e0b\u7684\u68c0\u9a8c\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u548c\u5f31\u4fe1\u53f7\u573a\u666f\u4e0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7edf\u4e00\u5e76\u6269\u5c55\u4e86\u73b0\u6709\u57fa\u4e8e\u8c31\u6b63\u5219\u5316MMD\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u4e00\u81f4\u6027\u548c\u5747\u5300\u529f\u7387\u4fdd\u8bc1\u3002"}}
{"id": "2508.06952", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06952", "abs": "https://arxiv.org/abs/2508.06952", "authors": ["Haiyang Zhang", "Nir Shlezinger", "Giulia Torcolacci", "Francesco Guidi", "Anna Guerra", "Qianyu Yang", "Mohammadreza F. Imani", "Davide Dardari", "Yonina C. Eldar"], "title": "Extremely Large-Scale Dynamic Metasurface Antennas for 6G Near-Field Networks: Opportunities and Challenges", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "6G networks will need to support higher data rates, high-precision\nlocalization, and imaging capabilities. Near-field technologies, enabled by\nextremely large-scale (XL)-arrays, are expected to be essential physical-layer\nsolutions to meet these ambitious requirements. However, implementing XL-array\nsystems using traditional fully-digital or hybrid analog/digital architectures\nposes significant challenges due to high power consumption and implementation\ncosts. Emerging XL-dynamic metasurface antennas (XL-DMAs) provide a promising\nalternative, enabling ultra-low power and cost-efficient solutions, making them\nideal candidates for 6G near-field networks. In this article, we discuss the\nopportunities and challenges of XL-DMAs employed in 6G near-field networks. We\nfirst outline the fundamental principles of XL-DMAs and present the specifics\nof the near-field model of XL-DMAs. We then highlight several promising\napplications that might benefit from XL-DMAs, including near-field\ncommunication, localization, and imaging. Finally, we discuss several open\nproblems and potential future directions that should be addressed to fully\nexploit the capabilities of XL-DMAs in the next 6G near-field networks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e866G\u7f51\u7edc\u4e2dXL-DMAs\uff08\u8d85\u5927\u89c4\u6a21\u52a8\u6001\u8d85\u8868\u9762\u5929\u7ebf\uff09\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u5305\u62ec\u5176\u57fa\u672c\u539f\u7406\u3001\u8fd1\u573a\u6a21\u578b\u3001\u5e94\u7528\u573a\u666f\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u652f\u6301\u66f4\u9ad8\u6570\u636e\u901f\u7387\u3001\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u548c\u6210\u50cf\u80fd\u529b\uff0c\u4f20\u7edf\u67b6\u6784\u5b58\u5728\u9ad8\u529f\u8017\u548c\u9ad8\u6210\u672c\u95ee\u9898\uff0cXL-DMAs\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u529f\u8017\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4ecb\u7ecd\u4e86XL-DMAs\u7684\u57fa\u672c\u539f\u7406\u548c\u8fd1\u573a\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u8fd1\u573a\u901a\u4fe1\u3001\u5b9a\u4f4d\u548c\u6210\u50cf\u4e2d\u7684\u5e94\u7528\u3002", "result": "XL-DMAs\u57286G\u8fd1\u573a\u7f51\u7edc\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u6ee1\u8db3\u9ad8\u6570\u636e\u901f\u7387\u548c\u7cbe\u51c6\u5b9a\u4f4d\u7b49\u9700\u6c42\u3002", "conclusion": "\u672a\u6765\u9700\u89e3\u51b3XL-DMAs\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u4ee5\u5145\u5206\u53d1\u6325\u5176\u57286G\u8fd1\u573a\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06588", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06588", "abs": "https://arxiv.org/abs/2508.06588", "authors": ["Zian Zhai", "Fan Li", "Xingyu Tan", "Xiaoyang Wang", "Wenjie Zhang"], "title": "Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning", "comment": null, "summary": "Vector Quantization (VQ) has recently emerged as a promising approach for\nlearning discrete representations of graph-structured data. However, a\nfundamental challenge, i.e., codebook collapse, remains underexplored in the\ngraph domain, significantly limiting the expressiveness and generalization of\ngraph tokens.In this paper, we present the first empirical study showing that\ncodebook collapse consistently occurs when applying VQ to graph data, even with\nmitigation strategies proposed in vision or language domains. To understand why\ngraph VQ is particularly vulnerable to collapse, we provide a theoretical\nanalysis and identify two key factors: early assignment imbalances caused by\nredundancy in graph features and structural patterns, and self-reinforcing\noptimization loops in deterministic VQ. To address these issues, we propose\nRGVQ, a novel framework that integrates graph topology and feature similarity\nas explicit regularization signals to enhance codebook utilization and promote\ntoken diversity. RGVQ introduces soft assignments via Gumbel-Softmax\nreparameterization, ensuring that all codewords receive gradient updates. In\naddition, RGVQ incorporates a structure-aware contrastive regularization to\npenalize the token co-assignments among similar node pairs. Extensive\nexperiments demonstrate that RGVQ substantially improves codebook utilization\nand consistently boosts the performance of state-of-the-art graph VQ backbones\nacross multiple downstream tasks, enabling more expressive and transferable\ngraph token representations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86RGVQ\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u56fe\u62d3\u6251\u548c\u7279\u5f81\u76f8\u4f3c\u6027\u4f5c\u4e3a\u6b63\u5219\u5316\u4fe1\u53f7\uff0c\u89e3\u51b3\u56fe\u6570\u636e\u4e2dVQ\u7684\u7801\u672c\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u56fe\u6570\u636e\u7684\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u4e2d\u7801\u672c\u5d29\u6e83\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9650\u5236\u4e86\u56fe\u4ee4\u724c\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002", "method": "\u63d0\u51faRGVQ\u6846\u67b6\uff0c\u7ed3\u5408Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u548c\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u589e\u5f3a\u7801\u672c\u5229\u7528\u7387\u548c\u4ee4\u724c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRGVQ\u663e\u8457\u63d0\u5347\u7801\u672c\u5229\u7528\u7387\uff0c\u5e76\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "RGVQ\u4e3a\u56fe\u6570\u636eVQ\u63d0\u4f9b\u4e86\u66f4\u8868\u8fbe\u548c\u53ef\u8fc1\u79fb\u7684\u4ee4\u724c\u8868\u793a\u3002"}}
{"id": "2508.06622", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06622", "abs": "https://arxiv.org/abs/2508.06622", "authors": ["Jeremiah Birrell", "Reza Ebrahimi"], "title": "Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels", "comment": "25 pages, 2 figures", "summary": "We introduce ANTIDOTE, a new class of objectives for learning under noisy\nlabels which are defined in terms of a relaxation over an\ninformation-divergence neighborhood. Using convex duality, we provide a\nreformulation as an adversarial training method that has similar computational\ncost to training with standard cross-entropy loss. We show that our approach\nadaptively reduces the influence of the samples with noisy labels during\nlearning, exhibiting a behavior that is analogous to forgetting those samples.\nANTIDOTE is effective in practical environments where label noise is inherent\nin the training data or where an adversary can alter the training labels.\nExtensive empirical evaluations on different levels of symmetric, asymmetric,\nhuman annotation, and real-world label noise show that ANTIDOTE outperforms\nleading comparable losses in the field and enjoys a time complexity that is\nvery close to that of the standard cross entropy loss.", "AI": {"tldr": "ANTIDOTE\u662f\u4e00\u79cd\u9488\u5bf9\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u7684\u65b0\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u4fe1\u606f\u6563\u5ea6\u90bb\u57df\u7684\u677e\u5f1b\u5b9a\u4e49\uff0c\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u63a5\u8fd1\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u3002", "motivation": "\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u4e2d\u6807\u7b7e\u566a\u58f0\u6216\u5bf9\u6297\u6027\u6807\u7b7e\u7be1\u6539\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u51f8\u5bf9\u5076\u6027\u5c06\u76ee\u6807\u51fd\u6570\u8f6c\u5316\u4e3a\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u51cf\u5c11\u566a\u58f0\u6807\u7b7e\u6837\u672c\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u6807\u7b7e\u566a\u58f0\uff08\u5bf9\u79f0\u3001\u975e\u5bf9\u79f0\u3001\u4eba\u5de5\u6807\u6ce8\u548c\u771f\u5b9e\u566a\u58f0\uff09\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6548\u7387\u63a5\u8fd1\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u3002", "conclusion": "ANTIDOTE\u5728\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06958", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06958", "abs": "https://arxiv.org/abs/2508.06958", "authors": ["Xin Cheng", "Guangjie Han", "Menglu Li", "Ruoguang Li", "Feng Shu"], "title": "Millimeter-Wave Position Sensing Using Reconfigurable Intelligent Surfaces: Positioning Error Bound and Phase Shift Configuration", "comment": null, "summary": "Millimeter-wave (mmWave) positioning has emerged as a promising technology\nfor next-generation intelligent systems. The advent of reconfigurable\nintelligent surfaces (RISs) has revolutionized high-precision mmWave\nlocalization by enabling dynamic manipulation of wireless propagation\nenvironments. This paper investigates a three-dimensional (3D) multi-input\nsingle-output (MISO) mmWave positioning system assisted by multiple RISs. We\nintroduce a measurement framework incorporating sequential RIS activation and\ndirectional beamforming to fully exploit virtual line-of-sight (VLoS) paths.\nThe theoretical performance limits are rigorously analyzed through derivation\nof the Fisher information and subsequent positioning error bound (PEB). To\nminimize the PEB, two distinct optimization approaches are proposed for\ncontinuous and discrete phase shift configurations of RISs. For continuous\nphase shifts, a Riemannian manifold-based optimization algorithm is proposed.\nFor discrete phase shifts, a heuristic algorithm incorporating the grey wolf\noptimizer is proposed. Extensive numerical simulations demonstrate the\neffectiveness of the proposed algorithms in reducing the PEB and validate the\nimprovement in positioning accuracy achieved by multiple RISs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u591aRIS\u8f85\u52a9\u76843D MISO\u6beb\u7c73\u6ce2\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u4f18\u5316\u65b9\u6cd5\u4ee5\u6700\u5c0f\u5316\u5b9a\u4f4d\u8bef\u5dee\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6beb\u7c73\u6ce2\u5b9a\u4f4d\u662f\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u7684\u5173\u952e\u6280\u672f\uff0c\u800cRIS\u7684\u52a8\u6001\u64cd\u63a7\u80fd\u529b\u4e3a\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cf\u6846\u67b6\uff0c\u7ed3\u5408\u987a\u5e8fRIS\u6fc0\u6d3b\u548c\u5b9a\u5411\u6ce2\u675f\u6210\u5f62\uff0c\u5e76\u9488\u5bf9\u8fde\u7eed\u548c\u79bb\u6563\u76f8\u4f4d\u914d\u7f6e\u63d0\u51fa\u4e86\u4e24\u79cd\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u5b9a\u4f4d\u8bef\u5dee\uff0c\u591aRIS\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u591aRIS\u8f85\u52a9\u7684\u6beb\u7c73\u6ce2\u5b9a\u4f4d\u7cfb\u7edf\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u6240\u63d0\u4f18\u5316\u65b9\u6cd5\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06589", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06589", "abs": "https://arxiv.org/abs/2508.06589", "authors": ["Xinglin Zhao", "Yanwen Wang", "Xiaobo Liu", "Yanrong Hao", "Rui Cao", "Xin Wen"], "title": "A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis", "comment": null, "summary": "Computer-aided diagnosis (CAD) systems play a crucial role in analyzing\nneuroimaging data for neurological and psychiatric disorders. However,\nsmall-sample studies suffer from low reproducibility, while large-scale\ndatasets introduce confounding heterogeneity due to multiple disease subtypes\nbeing labeled under a single category. To address these challenges, we propose\na novel federated learning framework tailored for neuroimaging CAD systems. Our\napproach includes a dynamic navigation module that routes samples to the most\nsuitable local models based on latent subtype representations, and a\nmeta-integration module that combines predictions from heterogeneous local\nmodels into a unified diagnostic output. We evaluated our framework using a\ncomprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100\nhealthy controls across multiple study cohorts. Experimental results\ndemonstrate significant improvements in diagnostic accuracy and robustness\ncompared to traditional methods. Specifically, our framework achieved an\naverage accuracy of 74.06\\% across all tested sites, showcasing its\neffectiveness in handling subtype heterogeneity and enhancing model\ngeneralizability. Ablation studies further confirmed the importance of both the\ndynamic navigation and meta-integration modules in improving performance. By\naddressing data heterogeneity and subtype confounding, our framework advances\nreliable and reproducible neuroimaging CAD systems, offering significant\npotential for personalized medicine and clinical decision-making in neurology\nand psychiatry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5bfc\u822a\u548c\u5143\u6574\u5408\u6a21\u5757\u89e3\u51b3\u5c0f\u6837\u672c\u548c\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u4e2d\u56e0\u5c0f\u6837\u672c\u7814\u7a76\u53ef\u91cd\u590d\u6027\u4f4e\u548c\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u75be\u75c5\u4e9a\u578b\u6df7\u6742\u5bfc\u81f4\u7684\u5f02\u8d28\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5305\u542b\u52a8\u6001\u5bfc\u822a\u6a21\u5757\uff08\u6839\u636e\u6f5c\u5728\u4e9a\u578b\u8868\u793a\u5c06\u6837\u672c\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u672c\u5730\u6a21\u578b\uff09\u548c\u5143\u6574\u5408\u6a21\u5757\uff08\u5c06\u5f02\u8d28\u672c\u5730\u6a21\u578b\u7684\u9884\u6d4b\u6574\u5408\u4e3a\u7edf\u4e00\u8bca\u65ad\u8f93\u51fa\uff09\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u5305\u542b1300\u591a\u540dMDD\u60a3\u8005\u548c1100\u540d\u5065\u5eb7\u5bf9\u7167\u7684fMRI\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523074.06%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e9a\u578b\u6df7\u6742\uff0c\u63d0\u5347\u4e86\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5bf9\u4e2a\u6027\u5316\u533b\u7597\u548c\u4e34\u5e8a\u51b3\u7b56\u5177\u6709\u91cd\u8981\u6f5c\u529b\u3002"}}
{"id": "2508.06635", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06635", "abs": "https://arxiv.org/abs/2508.06635", "authors": ["Yewon Byun", "Shantanu Gupta", "Zachary C. Lipton", "Rachel Leah Childers", "Bryan Wilder"], "title": "Using Imperfect Synthetic Data in Downstream Inference Tasks", "comment": null, "summary": "Predictions and generations from large language models are increasingly being\nexplored as an aid to computational social science and human subject research\nin limited data regimes. While previous technical work has explored the\npotential to use model-predicted labels for unlabeled data in a principled\nmanner, there is increasing interest in using large language models to generate\nentirely new synthetic samples (also termed as synthetic simulations), such as\nin responses to surveys. However, it is not immediately clear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this work, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address the\nchallenge at hand. Surprisingly, we find that interactions between the moment\nresiduals of synthetic data and those of real data can improve estimates of the\ntarget parameter. We empirically validate the finite-sample performance of our\nestimator across different regression tasks in computational social science\napplications, demonstrating large empirical gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u77e9\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ed3\u5408\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\uff0c\u4ee5\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u5b9e\u73b0\u7edf\u8ba1\u6709\u6548\u7684\u7ed3\u8bba\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u7edf\u8ba1\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5e7f\u4e49\u77e9\u4f30\u8ba1\u7684\u65b0\u4f30\u8ba1\u5668\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u5177\u6709\u5f3a\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u53d1\u73b0\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u77e9\u6b8b\u5dee\u4ea4\u4e92\u53ef\u4ee5\u6539\u5584\u76ee\u6807\u53c2\u6570\u4f30\u8ba1\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5176\u5728\u5c0f\u6837\u672c\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u5b9e\u8bc1\u4f18\u52bf\uff0c\u4e3a\u5408\u6210\u6570\u636e\u7684\u4f7f\u7528\u63d0\u4f9b\u4e86\u7edf\u8ba1\u6709\u6548\u6027\u4fdd\u969c\u3002"}}
{"id": "2508.07002", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07002", "abs": "https://arxiv.org/abs/2508.07002", "authors": ["Ze Wang", "Guoping Zhang", "Hongbo Xu"], "title": "Joint Beamforming Optimization for Pinching-Antenna Systems (PASS)-assisted Symbiotic Radio", "comment": null, "summary": "This paper investigates a novel downlink symbiotic radio (SR) framework\nempowered by the pinching antenna system (PASS), aiming to enhance both primary\nand secondary transmissions through reconfigurable antenna positioning. PASS\nconsists of multiple waveguides equipped with numerous low-cost pinching\nantennas (PAs), whose positions can be flexibly adjusted to simultaneously\nmanipulate large-scale path loss and signal phases.We formulate a joint\ntransmit and pinching beamforming optimization problem to maximize the\nachievable sum rate while satisfying the detection error probability constraint\nfor the IR and the feasible deployment region constraints for the PAs. This\nproblem is inherently nonconvex and highly coupled. To address it, two solution\nstrategies are developed. 1) A learning-aided gradient descent (LGD) algorithm\nis proposed, where the constrained problem is reformulated into a\ndifferentiable form and solved through end-to-end learning based on the\nprinciple of gradient descent. The PA position matrix is reparameterized to\ninherently satisfy minimum spacing constraints, while transmit power and\nwaveguide length limits are enforced via projection and normalization. 2) A\ntwo-stage optimization-based approach is designed, in which the transmit\nbeamforming is first optimized via successive convex approximation (SCA),\nfollowed by pinching beamforming optimization using a particle swarm\noptimization (PSO) search over candidate PA placements. The SCA-PSO algorithm\nachieves performance close to that of the element-wise method while\nsignificantly reducing computational complexity by exploring a randomly\ngenerated effective solution subspace, while further improving upon the LGD\nmethod by avoiding undesirable local optima.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\uff08PASS\uff09\u7684\u65b0\u578b\u4e0b\u884c\u94fe\u8def\u5171\u751f\u65e0\u7ebf\u7535\uff08SR\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u91cd\u6784\u5929\u7ebf\u4f4d\u7f6e\u4f18\u5316\u4e3b\u6b21\u4f20\u8f93\u6027\u80fd\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7075\u6d3b\u8c03\u6574\u5929\u7ebf\u4f4d\u7f6e\uff0c\u540c\u65f6\u64cd\u63a7\u5927\u5c3a\u5ea6\u8def\u5f84\u635f\u8017\u548c\u4fe1\u53f7\u76f8\u4f4d\uff0c\u63d0\u5347\u4e3b\u6b21\u4f20\u8f93\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u57fa\u4e8e\u5b66\u4e60\u7684\u68af\u5ea6\u4e0b\u964d\uff08LGD\uff09\u7b97\u6cd5\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5fae\u5206\u5f62\u5f0f\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u6c42\u89e3\uff1b2\uff09\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\uff08SCA-PSO\uff09\uff0c\u5148\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\uff0c\u518d\u901a\u8fc7\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u641c\u7d22\u5939\u6301\u6ce2\u675f\u6210\u5f62\u3002", "result": "SCA-PSO\u7b97\u6cd5\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63a5\u8fd1\u9010\u5143\u7d20\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4f18\u4e8eLGD\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5c40\u90e8\u6700\u4f18\u3002", "conclusion": "PASS\u6846\u67b6\u548c\u63d0\u51fa\u7684\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86SR\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06591", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cond-mat.other", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06591", "abs": "https://arxiv.org/abs/2508.06591", "authors": ["Rachel K. Luu", "Jingyu Deng", "Mohammed Shahrudin Ibrahim", "Nam-Joon Cho", "Ming Dao", "Subra Suresh", "Markus J. Buehler"], "title": "Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials", "comment": null, "summary": "Large language models (LLMs) have reshaped the research landscape by enabling\nnew approaches to knowledge retrieval and creative ideation. Yet their\napplication in discipline-specific experimental science, particularly in highly\nmulti-disciplinary domains like materials science, remains limited. We present\na first-of-its-kind framework that integrates generative AI with literature\nfrom hitherto-unconnected fields such as plant science, biomimetics, and\nmaterials engineering to extract insights and design experiments for materials.\nWe focus on humidity-responsive systems such as pollen-based materials and\nRhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and\nadaptive performance. Using a suite of AI tools, including a fine-tuned model\n(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a\nHierarchical Sampling strategy, we extract structure-property relationships and\ntranslate them into new classes of bioinspired materials. Structured inference\nprotocols generate and evaluate hundreds of hypotheses from a single query,\nsurfacing novel and experimentally tractable ideas. We validate our approach\nthrough real-world implementation: LLM-generated procedures, materials designs,\nand mechanical predictions were tested in the laboratory, culminating in the\nfabrication of a novel pollen-based adhesive with tunable morphology and\nmeasured shear strength, establishing a foundation for future plant-derived\nadhesive design. This work demonstrates how AI-assisted ideation can drive\nreal-world materials design and enable effective human-AI collaboration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0fAI\u548c\u591a\u5b66\u79d1\u6587\u732e\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1\u65b0\u578b\u4eff\u751f\u6750\u6599\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5b66\u79d1\u5b9e\u9a8c\u79d1\u5b66\uff08\u5982\u6750\u6599\u79d1\u5b66\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u5176\u5728\u7279\u5b9a\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7ed3\u5408BioinspiredLLM\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u4ee3\u7406\u7cfb\u7edf\u548c\u5206\u5c42\u91c7\u6837\u7b56\u7565\uff0c\u4ece\u690d\u7269\u79d1\u5b66\u3001\u4eff\u751f\u5b66\u548c\u6750\u6599\u5de5\u7a0b\u4e2d\u63d0\u53d6\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u578b\u82b1\u7c89\u57fa\u7c98\u5408\u5242\uff0c\u5177\u6709\u53ef\u8c03\u5f62\u6001\u548c\u5b9e\u6d4b\u526a\u5207\u5f3a\u5ea6\u3002", "conclusion": "AI\u8f85\u52a9\u521b\u610f\u80fd\u591f\u63a8\u52a8\u73b0\u5b9e\u4e16\u754c\u7684\u6750\u6599\u8bbe\u8ba1\uff0c\u5e76\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2508.06776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06776", "abs": "https://arxiv.org/abs/2508.06776", "authors": ["Amit Pandey"], "title": "Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift", "comment": "14 pages", "summary": "We present Zero-Direction Probing (ZDP), a theory-only framework for\ndetecting model drift from null directions of transformer activations without\ntask labels or output evaluations. Under assumptions A1--A6, we prove: (i) the\nVariance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound\nfor low-rank updates, and (iv) a logarithmic-regret guarantee for online\nnull-space trackers. We derive a Spectral Null-Leakage (SNL) metric with\nnon-asymptotic tail bounds and a concentration inequality, yielding a-priori\nthresholds for drift under a Gaussian null model. These results show that\nmonitoring right/left null spaces of layer activations and their Fisher\ngeometry provides concrete, testable guarantees on representational change.", "AI": {"tldr": "Zero-Direction Probing (ZDP) \u662f\u4e00\u79cd\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u6216\u8f93\u51fa\u8bc4\u4f30\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b Transformer \u6fc0\u6d3b\u7684\u96f6\u65b9\u5411\u6765\u53d1\u73b0\u6a21\u578b\u6f02\u79fb\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u6216\u8f93\u51fa\u8bc4\u4f30\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7406\u8bba\u65b9\u6cd5\u68c0\u6d4b\u6a21\u578b\u6f02\u79fb\u3002", "method": "\u63d0\u51fa ZDP \u6846\u67b6\uff0c\u57fa\u4e8e\u5047\u8bbe A1-A6\uff0c\u8bc1\u660e\u4e86\u4e00\u7cfb\u5217\u7406\u8bba\u7ed3\u679c\uff08\u5982\u65b9\u5dee\u6cc4\u6f0f\u5b9a\u7406\u3001Fisher \u96f6\u5b88\u6052\u7b49\uff09\uff0c\u5e76\u63a8\u5bfc\u4e86 Spectral Null-Leakage (SNL) \u5ea6\u91cf\u3002", "result": "\u8bc1\u660e\u4e86\u76d1\u63a7\u5c42\u6fc0\u6d3b\u7684\u5de6\u53f3\u96f6\u7a7a\u95f4\u53ca\u5176 Fisher \u51e0\u4f55\u53ef\u4ee5\u63d0\u4f9b\u5173\u4e8e\u8868\u793a\u53d8\u5316\u7684\u5177\u4f53\u3001\u53ef\u6d4b\u8bd5\u7684\u4fdd\u8bc1\u3002", "conclusion": "ZDP \u6846\u67b6\u4e3a\u6a21\u578b\u6f02\u79fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u901a\u8fc7 SNL \u5ea6\u91cf\u5b9e\u73b0\u4e86\u53ef\u91cf\u5316\u7684\u76d1\u63a7\u3002"}}
{"id": "2508.07013", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07013", "abs": "https://arxiv.org/abs/2508.07013", "authors": ["Yufan Zhou", "Jingyi Li", "Wenkang Xu", "An Liu"], "title": "Robust Super-Resolution Compressive Sensing: A Two-timescale Alternating MAP Approach", "comment": null, "summary": "The problem of super-resolution compressive sensing (SR-CS) is crucial for\nvarious wireless sensing and communication applications. Existing methods often\nsuffer from limited resolution capabilities and sensitivity to\nhyper-parameters, hindering their ability to accurately recover sparse signals\nwhen the grid parameters do not lie precisely on a fixed grid and are close to\neach other. To overcome these limitations, this paper introduces a novel robust\nsuper-resolution compressive sensing algorithmic framework using a\ntwo-timescale alternating maximum a posteriori (MAP) approach. At the slow\ntimescale, the proposed framework iterates between a sparse signal estimation\nmodule and a grid update module. In the sparse signal estimation module, a\nhyperbolic-tangent prior distribution based variational Bayesian inference\n(tanh-VBI) algorithm with a strong sparsity promotion capability is adopted to\nestimate the posterior probability of the sparse vector and accurately identify\nactive grid components carrying primary energy under a dense grid.\nSubsequently, the grid update module utilizes the BFGS algorithm to refine\nthese low-dimensional active grid components at a faster timescale to achieve\nsuper-resolution estimation of the grid parameters with a low computational\ncost. The proposed scheme is applied to the channel extrapolation problem, and\nsimulation results demonstrate the superiority of the proposed scheme compared\nto baseline schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u65f6\u95f4\u5c3a\u5ea6\u4ea4\u66ff\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u7684\u9c81\u68d2\u8d85\u5206\u8fa8\u7387\u538b\u7f29\u611f\u77e5\u7b97\u6cd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7f51\u683c\u53c2\u6570\u4e0d\u7cbe\u786e\u548c\u5bc6\u96c6\u60c5\u51b5\u4e0b\u7684\u5206\u8fa8\u7387\u9650\u5236\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\u5728\u7f51\u683c\u53c2\u6570\u4e0d\u7cbe\u786e\u6216\u5bc6\u96c6\u65f6\u5206\u8fa8\u7387\u6709\u9650\u4e14\u5bf9\u8d85\u53c2\u6570\u654f\u611f\uff0c\u5f71\u54cd\u4e86\u7a00\u758f\u4fe1\u53f7\u7684\u51c6\u786e\u6062\u590d\u3002", "method": "\u91c7\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u4ea4\u66ffMAP\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u4fe1\u53f7\u4f30\u8ba1\u6a21\u5757\uff08\u57fa\u4e8etanh-VBI\u7b97\u6cd5\uff09\u548c\u7f51\u683c\u66f4\u65b0\u6a21\u5757\uff08\u57fa\u4e8eBFGS\u7b97\u6cd5\uff09\uff0c\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u4fe1\u9053\u5916\u63a8\u95ee\u9898\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u538b\u7f29\u611f\u77e5\u7684\u6027\u80fd\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5f3a\u7a00\u758f\u6027\u4fc3\u8fdb\u80fd\u529b\u3002"}}
{"id": "2508.06601", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06601", "abs": "https://arxiv.org/abs/2508.06601", "authors": ["Kyle O'Brien", "Stephen Casper", "Quentin Anthony", "Tomek Korbak", "Robert Kirk", "Xander Davies", "Ishan Mishra", "Geoffrey Irving", "Yarin Gal", "Stella Biderman"], "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs", "comment": "https://deepignorance.ai/", "summary": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u901a\u8fc7\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u53cc\u7528\u9014\u4e3b\u9898\u6587\u672c\uff0c\u9632\u6b62\u5f00\u653e\u6743\u91cdAI\u7cfb\u7edf\u88ab\u7be1\u6539\u653b\u51fb\uff0c\u63d0\u51fa\u4e00\u79cd\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u80fd\u529b\u3002", "motivation": "\u5f00\u653e\u6743\u91cdAI\u7cfb\u7edf\u867d\u5177\u900f\u660e\u6027\u548c\u5f00\u653e\u6027\uff0c\u4f46\u6613\u53d7\u7be1\u6539\u653b\u51fb\uff0c\u73b0\u6709\u5b89\u5168\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u63a2\u7d22\u66f4\u6709\u6548\u7684\u9632\u62a4\u624b\u6bb5\u3002", "method": "\u5f15\u5165\u591a\u9636\u6bb5\u6570\u636e\u8fc7\u6ee4\u7ba1\u9053\uff0c\u9884\u8bad\u7ec3\u591a\u4e2a6.9B\u53c2\u6570\u6a21\u578b\uff0c\u6d4b\u8bd5\u5176\u5bf9\u751f\u7269\u5a01\u80c1\u76f8\u5173\u6587\u672c\u7684\u62b5\u6297\u80fd\u529b\u3002", "result": "\u8fc7\u6ee4\u540e\u7684\u6a21\u578b\u5728\u5bf9\u629710,000\u6b65\u5fae\u8c03\u548c3\u4ebf\u6807\u8bb0\u653b\u51fb\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u4e0d\u5f71\u54cd\u5176\u4ed6\u80fd\u529b\u3002", "conclusion": "\u9884\u8bad\u7ec3\u6570\u636e\u8fc7\u6ee4\u662f\u5f00\u653e\u6743\u91cdAI\u7cfb\u7edf\u7684\u6709\u6548\u9632\u62a4\u5c42\uff0c\u4f46\u9700\u7ed3\u5408\u5176\u4ed6\u9632\u5fa1\u63aa\u65bd\u4ee5\u5168\u9762\u5e94\u5bf9\u98ce\u9669\u3002"}}
{"id": "2508.07392", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.07392", "abs": "https://arxiv.org/abs/2508.07392", "authors": ["Nikita Puchkin", "Denis Suchkov", "Alexey Naumov", "Denis Belomestny"], "title": "Tight Bounds for Schr\u00f6dinger Potential Estimation in Unpaired Image-to-Image Translation Problems", "comment": "54 pages, 4 figures", "summary": "Modern methods of generative modelling and unpaired image-to-image\ntranslation based on Schr\\\"odinger bridges and stochastic optimal control\ntheory aim to transform an initial density to a target one in an optimal way.\nIn the present paper, we assume that we only have access to i.i.d. samples from\ninitial and final distributions. This makes our setup suitable for both\ngenerative modelling and unpaired image-to-image translation. Relying on the\nstochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as\nthe reference one and estimate the corresponding Schr\\\"odinger potential.\nIntroducing a risk function as the Kullback-Leibler divergence between\ncouplings, we derive tight bounds on generalization ability of an empirical\nrisk minimizer in a class of Schr\\\"odinger potentials including Gaussian\nmixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we\nalmost achieve fast rates of convergence up to some logarithmic factors in\nfavourable scenarios. We also illustrate performance of the suggested approach\nwith numerical experiments.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eSchr\u00f6dinger\u6865\u548c\u968f\u673a\u6700\u4f18\u63a7\u5236\u7406\u8bba\u7684\u751f\u6210\u5efa\u6a21\u548c\u65e0\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7Ornstein-Uhlenbeck\u8fc7\u7a0b\u4f30\u8ba1Schr\u00f6dinger\u52bf\uff0c\u5e76\u63a8\u5bfc\u4e86\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\u7684\u6cdb\u5316\u80fd\u529b\u754c\u9650\u3002", "motivation": "\u89e3\u51b3\u5728\u4ec5\u80fd\u83b7\u53d6\u521d\u59cb\u548c\u6700\u7ec8\u5206\u5e03\u7684\u72ec\u7acb\u540c\u5206\u5e03\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u968f\u673a\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5b9e\u73b0\u751f\u6210\u5efa\u6a21\u548c\u65e0\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528Ornstein-Uhlenbeck\u8fc7\u7a0b\u4f5c\u4e3a\u53c2\u8003\u8fc7\u7a0b\uff0c\u4f30\u8ba1Schr\u00f6dinger\u52bf\uff0c\u5e76\u901a\u8fc7Kullback-Leibler\u6563\u5ea6\u5b9a\u4e49\u98ce\u9669\u51fd\u6570\uff0c\u63a8\u5bfc\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u5668\u7684\u6cdb\u5316\u754c\u9650\u3002", "result": "\u5728\u6709\u5229\u573a\u666f\u4e0b\uff0c\u51e0\u4e4e\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u655b\u901f\u7387\uff08\u9664\u5bf9\u6570\u56e0\u5b50\u5916\uff09\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u751f\u6210\u5efa\u6a21\u548c\u65e0\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6536\u655b\u6027\u80fd\u3002"}}
{"id": "2508.07131", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07131", "abs": "https://arxiv.org/abs/2508.07131", "authors": ["Yanqing Xu", "Zhiguo Ding", "Octavia A. Dobre", "Tsung-Hui Chang"], "title": "Pinching-Antenna System Design with LoS Blockage: Does In-Waveguide Attenuation Matter?", "comment": "14 pages, 6 figures", "summary": "In the literature of pinching-antenna systems, in-waveguide attenuation is\noften neglected to simplify system design and enable more tractable analysis.\nHowever, its effect on overall system performance has received limited\nattention in the existing literature. While a recent study has shown that, in\nline-of-sight (LoS)-dominated environments, the data rate loss incurred by\nomitting in-waveguide attenuation is negligible when the communication area is\nnot excessively large, its effect under more general conditions remains\nunclear. This work extends the analysis to more realistic scenarios involving\narbitrary levels of LoS blockage. We begin by examining a single-user case and\nderive an explicit expression for the average data rate loss caused by\nneglecting in-waveguide attenuation. The results demonstrate that, even for\nlarge service areas, the rate loss remains negligible under typical LoS\nblockage conditions. We then consider a more general multi-user scenario, where\nmultiple pinching antennas, each deployed on a separate waveguide, jointly\nserve multiple users. The objective is to maximize the average sum rate by\njointly optimize antenna positions and transmit beamformers to maximize the\naverage sum rate under probabilistic LoS blockage. To solve the resulting\nstochastic and nonconvex optimization problem, we propose a dynamic sample\naverage approximation (SAA) algorithm. At each iteration, this method replaces\nthe expected objective with an empirical average computed from dynamically\nregenerated random channel realizations, ensuring that the optimization\naccurately reflects the current antenna configuration. Extensive simulation\nresults are provided to the proposed algorithm and demonstrate the substantial\nperformance gains of pinching-antenna systems, particularly in environments\nwith significant LoS blockage.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6ce2\u5bfc\u5185\u8870\u51cf\u5bf9\u5939\u6301\u5929\u7ebf\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u5178\u578b\u89c6\u8ddd\uff08LoS\uff09\u963b\u585e\u6761\u4ef6\u4e0b\uff0c\u5ffd\u7565\u6ce2\u5bfc\u5185\u8870\u51cf\u5bf9\u6570\u636e\u901f\u7387\u7684\u635f\u5931\u53ef\u5ffd\u7565\u3002\u901a\u8fc7\u52a8\u6001\u6837\u672c\u5e73\u5747\u903c\u8fd1\u7b97\u6cd5\u4f18\u5316\u591a\u7528\u6237\u573a\u666f\u4e0b\u7684\u5929\u7ebf\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u4e2d\u6ce2\u5bfc\u5185\u8870\u51cf\u5e38\u88ab\u5ffd\u7565\uff0c\u4f46\u5176\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u975e\u7406\u60f3\u89c6\u8ddd\u6761\u4ef6\u4e0b\u3002", "method": "\u4ece\u5355\u7528\u6237\u573a\u666f\u63a8\u5bfc\u6ce2\u5bfc\u5185\u8870\u51cf\u5bf9\u6570\u636e\u901f\u7387\u635f\u5931\u7684\u5f71\u54cd\uff0c\u6269\u5c55\u5230\u591a\u7528\u6237\u573a\u666f\uff0c\u63d0\u51fa\u52a8\u6001\u6837\u672c\u5e73\u5747\u903c\u8fd1\u7b97\u6cd5\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5178\u578b\u89c6\u8ddd\u963b\u585e\u6761\u4ef6\u4e0b\uff0c\u6ce2\u5bfc\u5185\u8870\u51cf\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\uff0c\u4e14\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u6ce2\u5bfc\u5185\u8870\u51cf\u5728\u5178\u578b\u6761\u4ef6\u4e0b\u5f71\u54cd\u6709\u9650\uff0c\u52a8\u6001\u4f18\u5316\u7b97\u6cd5\u5728\u591a\u7528\u6237\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.06614", "categories": ["cs.LG", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06614", "abs": "https://arxiv.org/abs/2508.06614", "authors": ["Fangjun Hu", "Guangkuo Liu", "Yifan Zhang", "Xun Gao"], "title": "Local Diffusion Models and Phases of Data Distributions", "comment": "8+22 pages, 4+3 figures", "summary": "As a class of generative artificial intelligence frameworks inspired by\nstatistical physics, diffusion models have shown extraordinary performance in\nsynthesizing complicated data distributions through a denoising process\ngradually guided by score functions. Real-life data, like images, is often\nspatially structured in low-dimensional spaces. However, ordinary diffusion\nmodels ignore this local structure and learn spatially global score functions,\nwhich are often computationally expensive. In this work, we introduce a new\nperspective on the phases of data distributions, which provides insight into\nconstructing local denoisers with reduced computational costs. We define two\ndistributions as belonging to the same data distribution phase if they can be\nmutually connected via spatially local operations such as local denoisers.\nThen, we show that the reverse denoising process consists of an early trivial\nphase and a late data phase, sandwiching a rapid phase transition where local\ndenoisers must fail. To diagnose such phase transitions, we prove an\ninformation-theoretic bound on the fidelity of local denoisers based on\nconditional mutual information, and conduct numerical experiments in a\nreal-world dataset. This work suggests simpler and more efficient architectures\nof diffusion models: far from the phase transition point, we can use small\nlocal neural networks to compute the score function; global neural networks are\nonly necessary around the narrow time interval of phase transitions. This\nresult also opens up new directions for studying phases of data distributions,\nthe broader science of generative artificial intelligence, and guiding the\ndesign of neural networks inspired by physics concepts.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5c40\u90e8\u53bb\u566a\u5668\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u51fa\u6570\u636e\u5206\u5e03\u76f8\u53d8\u7406\u8bba\uff0c\u5e76\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\uff08\u5982\u56fe\u50cf\uff09\u901a\u5e38\u5177\u6709\u4f4e\u7ef4\u7a7a\u95f4\u7ed3\u6784\uff0c\u4f46\u4f20\u7edf\u6269\u6563\u6a21\u578b\u5ffd\u7565\u5c40\u90e8\u7ed3\u6784\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u5b9a\u4e49\u6570\u636e\u5206\u5e03\u76f8\uff0c\u8bc1\u660e\u53bb\u566a\u8fc7\u7a0b\u5305\u542b\u65e9\u671f\u5e73\u51e1\u76f8\u548c\u665a\u671f\u6570\u636e\u76f8\uff0c\u76f8\u53d8\u65f6\u5c40\u90e8\u53bb\u566a\u5668\u5931\u6548\u3002\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u4e92\u4fe1\u606f\u7684\u4fe1\u606f\u8bba\u754c\u9650\u8bca\u65ad\u76f8\u53d8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u5c40\u90e8\u53bb\u566a\u5668\u5728\u8fdc\u79bb\u76f8\u53d8\u70b9\u65f6\u9ad8\u6548\uff0c\u5168\u5c40\u7f51\u7edc\u4ec5\u5728\u76f8\u53d8\u533a\u95f4\u5fc5\u8981\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5e76\u62d3\u5c55\u4e86\u6570\u636e\u5206\u5e03\u76f8\u53d8\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.07465", "categories": ["cs.LG", "q-bio.GN", "stat.ML", "62R07"], "pdf": "https://arxiv.org/pdf/2508.07465", "abs": "https://arxiv.org/abs/2508.07465", "authors": ["Tiantian Yang", "Zhiqian Chen"], "title": "MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification", "comment": "11 pages, 6 figures", "summary": "Integrating multi-omics data, such as DNA methylation, mRNA expression, and\nmicroRNA (miRNA) expression, offers a comprehensive view of the biological\nmechanisms underlying disease. However, the high dimensionality and complex\ninteractions among omics layers present major challenges for predictive\nmodeling. We propose Multi-Omics integration with Tree-generated Graph Neural\nNetwork (MOTGNN), a novel and interpretable framework for binary disease\nclassification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform\nomics-specific supervised graph construction, followed by modality-specific\nGraph Neural Networks (GNNs) for hierarchical representation learning, and a\ndeep feedforward network for cross-omics integration. On three real-world\ndisease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in\naccuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance\n(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains\ncomputational efficiency through sparse graphs (2.1-2.8 edges per node) and\nprovides built-in interpretability, revealing both top-ranked biomarkers and\nthe relative contributions of each omics modality. These results highlight\nMOTGNN's potential to improve both predictive accuracy and interpretability in\nmulti-omics disease modeling.", "AI": {"tldr": "MOTGNN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u7ec4\u5b66\u6570\u636e\u6574\u5408\u6846\u67b6\uff0c\u901a\u8fc7XGBoost\u6784\u5efa\u76d1\u7763\u56fe\uff0c\u7ed3\u5408GNN\u548c\u6df1\u5ea6\u524d\u9988\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75be\u75c5\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u7ec4\u5b66\u6570\u636e\u7684\u9ad8\u7ef4\u6027\u548c\u590d\u6742\u4ea4\u4e92\u4e3a\u75be\u75c5\u5efa\u6a21\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6574\u5408\u6570\u636e\u53c8\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528XGBoost\u8fdb\u884c\u7ec4\u5b66\u7279\u5f02\u6027\u76d1\u7763\u56fe\u6784\u5efa\uff0c\u7ed3\u5408\u6a21\u6001\u7279\u5f02\u6027GNN\u8fdb\u884c\u5206\u5c42\u8868\u793a\u5b66\u4e60\uff0c\u6700\u540e\u901a\u8fc7\u6df1\u5ea6\u524d\u9988\u7f51\u7edc\u6574\u5408\u8de8\u7ec4\u5b66\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u75be\u75c5\u6570\u636e\u96c6\u4e0a\uff0cMOTGNN\u5728\u51c6\u786e\u6027\u3001ROC-AUC\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd55-10%\uff0c\u4e14\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "MOTGNN\u5728\u591a\u7ec4\u5b66\u75be\u75c5\u5efa\u6a21\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07148", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07148", "abs": "https://arxiv.org/abs/2508.07148", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Saif Khan Mohammed", "Venkatesh Khammammetti", "Robert Calderbank"], "title": "Low-Complexity Equalization of Zak-OTFS in the Frequency Domain", "comment": "13 pages, 12 figures. Submitted to npj Wireless Technology", "summary": "4G/5G wireless standards use orthogonal frequency division multiplexing\n(OFDM) which is robust to frequency selectivity. Equalization is possible with\na single tap filter, and low-complexity equalization makes OFDM an attractive\nphysical layer. However the performance of OFDM degrades with mobility, since\nDoppler spreads introduce inter-carrier interference (ICI) between subcarriers\nand they are no longer orthogonal. Zak-transform based orthogonal time\nfrequency space (Zak-OTFS) modulation has been shown to be robust to doubly\nselective channels. Zak-OTFS signals are formed in the delay-Doppler (DD)\ndomain, converted to time domain (TD) for transmission and reception, then\nreturned to the DD domain for processing. The received signal is a\nsuperposition of many attenuated copies since the doubly selective channel\nintroduces delay and Doppler shifts. The received symbols are more difficult to\nequalize since they are subject to interference along both delay and Doppler\naxes. In this paper, we propose a new low-complexity method of equalizing\nZak-OTFS in the frequency domain (FD). We derive the FD system model and show\nthat it is unitarily equivalent to the DD system model. We show that the\nchannel matrix in the FD is banded, making it possible to apply conjugate\ngradient methods to reduce the complexity of equalization. We show that\ncomplexity of FD equalization is linear in the dimension of a Zak-OTFS frame.\nFor comparison the complexity of naive MMSE equalization is cubic in the frame\ndimension. Through numerical simulations we show that FD equalization of\nZak-OTFS achieves similar performance as equalization in DD domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u9891\u57df\uff08FD\uff09\u5747\u8861\u65b9\u6cd5\uff0c\u7528\u4e8eZak-OTFS\u8c03\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfOFDM\u5728\u9ad8\u79fb\u52a8\u6027\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "OFDM\u5728\u9ad8\u79fb\u52a8\u6027\u4e0b\u56e0\u591a\u666e\u52d2\u6269\u5c55\u5bfc\u81f4\u5b50\u8f7d\u6ce2\u95f4\u5e72\u6270\uff08ICI\uff09\uff0c\u6027\u80fd\u4e0b\u964d\u3002Zak-OTFS\u5bf9\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5747\u8861\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u63d0\u51fa\u9891\u57df\u5747\u8861\u65b9\u6cd5\uff0c\u63a8\u5bfcFD\u7cfb\u7edf\u6a21\u578b\uff0c\u8bc1\u660e\u5176\u4e0eDD\u6a21\u578b\u7b49\u4ef7\uff0c\u5e76\u5229\u7528\u5171\u8f6d\u68af\u5ea6\u6cd5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "FD\u5747\u8861\u590d\u6742\u5ea6\u4e0eZak-OTFS\u5e27\u7ef4\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u6027\u80fd\u4e0eDD\u57df\u5747\u8861\u76f8\u5f53\u3002", "conclusion": "\u9891\u57df\u5747\u8861\u4e3aZak-OTFS\u63d0\u4f9b\u4e86\u4f4e\u590d\u6742\u5ea6\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06617", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.06617", "abs": "https://arxiv.org/abs/2508.06617", "authors": ["Md Arafat Hossain", "Xingfu Wu", "Valerie Taylor", "Ali Jannesari"], "title": "Generalizing Scaling Laws for Dense and Sparse Large Language Models", "comment": "8 pages, 8 figures", "summary": "Over the past few years, the size of language models has grown exponentially,\nas has the computational cost to train these large models. This rapid growth\nhas motivated researchers to develop new techniques aimed at enhancing the\nefficiency of the training process. Despite these advancements, optimally\npredicting the model size or allocating optimal resources remains a challenge.\nSeveral efforts have addressed the challenge by proposing different scaling\nlaws, but almost all of them are architecture-specific (dense or sparse). In\nthis work we revisit existing scaling laws and propose a generalized scaling\nlaw to provide a unified framework that is applicable to both dense and sparse\nlarge language models. We evaluate and compare our proposed scaling law with\nexisting scaling laws to demonstrate its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u9002\u7528\u4e8e\u5bc6\u96c6\u548c\u7a00\u758f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u7684\u5feb\u901f\u589e\u957f\uff0c\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u4f46\u73b0\u6709\u6269\u5c55\u6cd5\u5219\u591a\u4e3a\u7279\u5b9a\u67b6\u6784\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u6269\u5c55\u6cd5\u5219\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u7684\u6269\u5c55\u6cd5\u5219\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5bc6\u96c6\u548c\u7a00\u758f\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u8bc4\u4f30\u6bd4\u8f83\u3002", "result": "\u63d0\u51fa\u7684\u901a\u7528\u6269\u5c55\u6cd5\u5219\u5728\u5bc6\u96c6\u548c\u7a00\u758f\u6a21\u578b\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u7528\u6269\u5c55\u6cd5\u5219\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u4f18\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u67b6\u6784\u3002"}}
{"id": "2508.07473", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07473", "abs": "https://arxiv.org/abs/2508.07473", "authors": ["Zijian Liu"], "title": "Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications", "comment": "Part of this work is in submission", "summary": "In Online Convex Optimization (OCO), when the stochastic gradient has a\nfinite variance, many algorithms provably work and guarantee a sublinear\nregret. However, limited results are known if the gradient estimate has a heavy\ntail, i.e., the stochastic gradient only admits a finite $\\mathsf{p}$-th\ncentral moment for some $\\mathsf{p}\\in\\left(1,2\\right]$. Motivated by it, this\nwork examines different old algorithms for OCO (e.g., Online Gradient Descent)\nin the more challenging heavy-tailed setting. Under the standard bounded domain\nassumption, we establish new regrets for these classical methods without any\nalgorithmic modification. Remarkably, these regret bounds are fully optimal in\nall parameters (can be achieved even without knowing $\\mathsf{p}$), suggesting\nthat OCO with heavy tails can be solved effectively without any extra operation\n(e.g., gradient clipping). Our new results have several applications. A\nparticularly interesting one is the first provable convergence result for\nnonsmooth nonconvex optimization under heavy-tailed noise without gradient\nclipping. Furthermore, we explore broader settings (e.g., smooth OCO) and\nextend our ideas to optimistic algorithms to handle different cases\nsimultaneously.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u51f8\u4f18\u5316\uff08OCO\uff09\u4e2d\u68af\u5ea6\u4f30\u8ba1\u5177\u6709\u91cd\u5c3e\u5206\u5e03\u65f6\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7ecf\u5178\u7b97\u6cd5\uff08\u5982\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\uff09\u65e0\u9700\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u6700\u4f18\u9057\u61be\u754c\uff0c\u5e76\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u7814\u7a76\u5728\u68af\u5ea6\u4f30\u8ba1\u5177\u6709\u91cd\u5c3e\u5206\u5e03\uff08\u5373\u4ec5\u6709\u9650p\u9636\u4e2d\u5fc3\u77e9\uff09\u65f6\uff0c\u7ecf\u5178OCO\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86\u7ecf\u5178OCO\u7b97\u6cd5\uff08\u5982\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\uff09\u5728\u91cd\u5c3e\u5206\u5e03\u4e0b\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u64cd\u4f5c\uff08\u5982\u68af\u5ea6\u88c1\u526a\uff09\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u4e9b\u7b97\u6cd5\u5728\u91cd\u5c3e\u5206\u5e03\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u5b8c\u5168\u6700\u4f18\u7684\u9057\u61be\u754c\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\uff08\u5982\u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\uff09\u3002", "conclusion": "OCO\u5728\u91cd\u5c3e\u5206\u5e03\u4e0b\u65e0\u9700\u989d\u5916\u64cd\u4f5c\u5373\u53ef\u6709\u6548\u89e3\u51b3\uff0c\u6269\u5c55\u4e86\u7b97\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u7279\u522b\u662f\u5728\u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u4e2d\u3002"}}
{"id": "2508.07160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07160", "abs": "https://arxiv.org/abs/2508.07160", "authors": ["Deyu Lu", "Xiaoli Ma", "Yiyin Wang"], "title": "Vector Orthogonal Chirp Division Multiplexing Over Doubly Selective Channels", "comment": null, "summary": "In this letter, we extend orthogonal chirp division multiplexing (OCDM) to\nvector OCDM (VOCDM) to provide more design freedom to deal with doubly\nselective channels. The VOCDM modulation is implemented by performing M\nparallel N-size inverse discrete Fresnel transforms (IDFnT). Based on the\ncomplex exponential basis expansion model (CE-BEM) for doubly selective\nchannels, we derive the VOCDM input-output relationship, and show performance\ntradeoffs of VOCDM with respect to (w.r.t.) its modulation parameters M and N.\nSpecifically, we investigate the diversity and peak-to-average power ratio\n(PAPR) of VOCDM w.r.t. M and N. Under doubly selective channels, VOCDM exhibits\nsuperior diversity performance as long as the parameters M and N are configured\nto satisfy some constraints from the delay and the Doppler spreads of the\nchannel, respectively. Furthermore, the PAPR of VOCDM signals decreases with a\ndecreasing N. These theoretical findings are verified through numerical\nsimulations.", "AI": {"tldr": "\u8bba\u6587\u5c06\u6b63\u4ea4\u5541\u557e\u5206\u590d\u7528\uff08OCDM\uff09\u6269\u5c55\u4e3a\u5411\u91cfOCDM\uff08VOCDM\uff09\uff0c\u4ee5\u63d0\u4f9b\u66f4\u591a\u8bbe\u8ba1\u81ea\u7531\u5ea6\u5904\u7406\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u3002\u901a\u8fc7\u5e76\u884cIDFnT\u5b9e\u73b0VOCDM\u8c03\u5236\uff0c\u5e76\u57fa\u4e8eCE-BEM\u6a21\u578b\u5206\u6790\u5176\u6027\u80fd\u3002", "motivation": "\u6269\u5c55OCDM\u4ee5\u5e94\u5bf9\u53cc\u9009\u62e9\u6027\u4fe1\u9053\uff0c\u63d0\u4f9b\u66f4\u591a\u8bbe\u8ba1\u81ea\u7531\u5ea6\u3002", "method": "\u901a\u8fc7M\u4e2a\u5e76\u884cN\u70b9\u9006\u79bb\u6563\u83f2\u6d85\u5c14\u53d8\u6362\uff08IDFnT\uff09\u5b9e\u73b0VOCDM\u8c03\u5236\uff0c\u57fa\u4e8eCE-BEM\u6a21\u578b\u63a8\u5bfc\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u3002", "result": "VOCDM\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u591a\u6837\u6027\u6027\u80fd\uff0c\u4e14PAPR\u968fN\u51cf\u5c0f\u800c\u964d\u4f4e\u3002", "conclusion": "VOCDM\u5728\u6ee1\u8db3\u7279\u5b9a\u7ea6\u675f\u6761\u4ef6\u4e0b\u6027\u80fd\u4f18\u8d8a\uff0c\u7406\u8bba\u7ed3\u679c\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u3002"}}
{"id": "2508.07490", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07490", "abs": "https://arxiv.org/abs/2508.07490", "authors": ["Ricardo Matos", "Luis Roque", "Vitor Cerqueira"], "title": "N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting", "comment": null, "summary": "Deep learning approaches are increasingly relevant for time series\nforecasting tasks. Methods such as N-BEATS, which is built on stacks of\nmultilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on\nbenchmark datasets and competitions. N-BEATS is also more interpretable\nrelative to other deep learning approaches, as it decomposes forecasts into\ndifferent time series components, such as trend and seasonality. In this work,\nwe present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts\n(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a\ngating network which allows the model to better adapt to the characteristics of\neach time series. We also hypothesize that the gating mechanism provides\nadditional interpretability by identifying which expert is most relevant for\neach series. We evaluate our method across 12 benchmark datasets against\nseveral approaches, achieving consistent improvements on several datasets,\nespecially those composed of heterogeneous time series.", "AI": {"tldr": "N-BEATS-MOE\u662fN-BEATS\u7684\u6269\u5c55\uff0c\u901a\u8fc7\u5f15\u5165\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u5c42\u548c\u52a8\u6001\u5757\u52a0\u6743\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6539\u8fdbN-BEATS\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u66f4\u597d\u5730\u9002\u5e94\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5728N-BEATS\u57fa\u7840\u4e0a\u5f15\u5165MoE\u5c42\u548c\u52a8\u6001\u5757\u52a0\u6743\u7b56\u7565\uff0c\u5229\u7528\u95e8\u63a7\u7f51\u7edc\u9009\u62e9\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u5757\u3002", "result": "\u572812\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cN-BEATS-MOE\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5f02\u8d28\u6027\u65f6\u95f4\u5e8f\u5217\u4e0a\u6548\u679c\u663e\u8457\u3002", "conclusion": "N-BEATS-MOE\u901a\u8fc7MoE\u5c42\u548c\u52a8\u6001\u52a0\u6743\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u3002"}}
{"id": "2508.06627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06627", "abs": "https://arxiv.org/abs/2508.06627", "authors": ["Mosbah Aouad", "Anirudh Choudhary", "Awais Farooq", "Steven Nevers", "Lusine Demirkhanyan", "Bhrandon Harris", "Suguna Pappu", "Christopher Gondi", "Ravishankar Iyer"], "title": "Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record", "comment": null, "summary": "Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and\nearly detection remains a major clinical challenge due to the absence of\nspecific symptoms and reliable biomarkers. In this work, we propose a new\nmultimodal approach that integrates longitudinal diagnosis code histories and\nroutinely collected laboratory measurements from electronic health records to\ndetect PDAC up to one year prior to clinical diagnosis. Our method combines\nneural controlled differential equations to model irregular lab time series,\npretrained language models and recurrent networks to learn diagnosis code\ntrajectory representations, and cross-attention mechanisms to capture\ninteractions between the two modalities. We develop and evaluate our approach\non a real-world dataset of nearly 4,700 patients and achieve significant\nimprovements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.\nFurthermore, our model identifies diagnosis codes and laboratory panels\nassociated with elevated PDAC risk, including both established and new\nbiomarkers. Our code is available at\nhttps://github.com/MosbahAouad/EarlyPDAC-MML.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u8bca\u65ad\u4ee3\u7801\u548c\u5b9e\u9a8c\u5ba4\u6570\u636e\uff0c\u63d0\u524d\u4e00\u5e74\u68c0\u6d4b\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\uff08PDAC\uff09\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "PDAC\u65e9\u671f\u68c0\u6d4b\u56f0\u96be\uff0c\u7f3a\u4e4f\u7279\u5f02\u6027\u75c7\u72b6\u548c\u53ef\u9760\u751f\u7269\u6807\u5fd7\u7269\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u3001\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u5faa\u73af\u7f51\u7edc\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6574\u5408\u8bca\u65ad\u4ee3\u7801\u548c\u5b9e\u9a8c\u5ba4\u6570\u636e\u3002", "result": "\u57284700\u540d\u60a3\u8005\u6570\u636e\u4e0a\uff0cAUC\u63d0\u53476.5%\u81f315.5%\uff0c\u5e76\u8bc6\u522b\u51fa\u65b0\u7684\u9ad8\u98ce\u9669\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347PDAC\u65e9\u671f\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u65b0\u7684\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2508.07518", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07518", "abs": "https://arxiv.org/abs/2508.07518", "authors": ["Sichen Zhao", "Wei Shao", "Jeffrey Chan", "Ziqi Xu", "Flora Salim"], "title": "FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction", "comment": "Accepted as a Research Paper (short) at ACM SIGSPATIAL 2025. This\n  arXiv version is the full version of the paper", "summary": "As deep spatio-temporal neural networks are increasingly utilised in urban\ncomputing contexts, the deployment of such methods can have a direct impact on\nusers of critical urban infrastructure, such as public transport, emergency\nservices, and traffic management systems. While many spatio-temporal methods\nfocus on improving accuracy, fairness has recently gained attention due to\ngrowing evidence that biased predictions in spatio-temporal applications can\ndisproportionately disadvantage certain demographic or geographic groups,\nthereby reinforcing existing socioeconomic inequalities and undermining the\nethical deployment of AI in public services. In this paper, we propose a novel\nframework, FairDRL-ST, based on disentangled representation learning, to\naddress fairness concerns in spatio-temporal prediction, with a particular\nfocus on mobility demand forecasting. By leveraging adversarial learning and\ndisentangled representation learning, our framework learns to separate\nattributes that contain sensitive information. Unlike existing methods that\nenforce fairness through supervised learning, which may lead to\novercompensation and degraded performance, our framework achieves fairness in\nan unsupervised manner with minimal performance loss. We apply our framework to\nreal-world urban mobility datasets and demonstrate its ability to close\nfairness gaps while delivering competitive predictive performance compared to\nstate-of-the-art fairness-aware methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u65b0\u6846\u67b6FairDRL-ST\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u79fb\u52a8\u9700\u6c42\u9884\u6d4b\u3002", "motivation": "\u7531\u4e8e\u65f6\u7a7a\u795e\u7ecf\u7f51\u7edc\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u53ef\u80fd\u5bf9\u5173\u952e\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7684\u7528\u6237\u4ea7\u751f\u76f4\u63a5\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u591a\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u516c\u5e73\u6027\u95ee\u9898\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u548c\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff0c\u6846\u67b6\u5b66\u4e60\u5206\u79bb\u5305\u542b\u654f\u611f\u4fe1\u606f\u7684\u5c5e\u6027\uff0c\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u5b9e\u73b0\u516c\u5e73\u6027\u3002", "result": "\u5728\u771f\u5b9e\u57ce\u5e02\u79fb\u52a8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u7f29\u5c0f\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u5148\u8fdb\u516c\u5e73\u611f\u77e5\u65b9\u6cd5\u76f8\u5f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "FairDRL-ST\u6846\u67b6\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f\u3002"}}
{"id": "2508.07226", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07226", "abs": "https://arxiv.org/abs/2508.07226", "authors": ["Yueheng Li", "Xueyun Long", "Mario Pauli", "Suheng Tian", "Xiang Wan", "Benjamin Nuss", "Tiejun Cui", "Haixia Zhang", "Thomas Zwick"], "title": "Multi-RIS Deployment Optimization for mmWave ISAC Systems in Real-World Environments", "comment": "13 pages, 9 figures", "summary": "Reconfigurable intelligent surface-assisted integrated sensing and\ncommunication (RIS-ISAC) presents a promising system architecture to leverage\nthe wide bandwidth available at millimeter-wave (mmWave) frequencies, while\nmitigating severe signal propagation losses and reducing infrastructure costs.\nTo enhance ISAC functionalities in the future air-ground integrated network\napplications, RIS deployment must be carefully designed and evaluated, which\nforms the core motivation of this paper. To ensure practical relevance, a\nmulti-RIS-ISAC system is established, with its signal model at mmWave\nfrequencies demonstrated using ray-launching calibrated to real-world\nenvironments. On this basis, an energy-efficiency-driven optimization problem\nis formulated to minimize the multi-RIS size-to-coverage sum ratio,\ncomprehensively considering real-world RIS deployment constraints, positions,\norientations, as well as ISAC beamforming strategies at both the base station\nand the RISs. To solve the resulting non-convex mixed-integer problem, a\nsimplified reformulation based on equivalent gain scaling method is introduced.\nA two-step iterative algorithm is then proposed, in which the deployment\nparameters are determined under fixed RIS positions in the first step, and the\nRIS position set is updated in the second step to progressively approach the\noptimum solution. Simulation results based on realistic parameter benchmarks\npresent that the optimized RISs deployment significantly enhances communication\ncoverage and sensing accuracy with the minimum RIS sizes, outperforming\nexisting approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591aRIS-ISAC\u7cfb\u7edf\u7684\u80fd\u6548\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316RIS\u90e8\u7f72\u63d0\u5347\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684\u901a\u4fe1\u8986\u76d6\u548c\u611f\u77e5\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u672a\u6765\u7a7a\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u5e94\u7528\u589e\u5f3aISAC\u529f\u80fd\uff0c\u9700\u7cbe\u5fc3\u8bbe\u8ba1\u548c\u8bc4\u4f30RIS\u90e8\u7f72\u3002", "method": "\u5efa\u7acb\u591aRIS-ISAC\u7cfb\u7edf\u4fe1\u53f7\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u7b49\u6548\u589e\u76ca\u7f29\u653e\u7684\u7b80\u5316\u91cd\u6784\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e24\u6b65\u8fed\u4ee3\u7b97\u6cd5\u4f18\u5316RIS\u90e8\u7f72\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u4f18\u5316\u540e\u7684RIS\u90e8\u7f72\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u8986\u76d6\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u4e14\u6240\u9700RIS\u5c3a\u5bf8\u6700\u5c0f\u3002", "conclusion": "\u4f18\u5316RIS\u90e8\u7f72\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07556", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07556", "abs": "https://arxiv.org/abs/2508.07556", "authors": ["Stephan Rabanser"], "title": "Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning", "comment": "PhD Thesis", "summary": "Machine learning (ML) systems are increasingly deployed in high-stakes\ndomains where reliability is paramount. This thesis investigates how\nuncertainty estimation can enhance the safety and trustworthiness of ML,\nfocusing on selective prediction -- where models abstain when confidence is\nlow.\n  We first show that a model's training trajectory contains rich uncertainty\nsignals that can be exploited without altering its architecture or loss. By\nensembling predictions from intermediate checkpoints, we propose a lightweight,\npost-hoc abstention method that works across tasks, avoids the cost of deep\nensembles, and achieves state-of-the-art selective prediction performance.\nCrucially, this approach is fully compatible with differential privacy (DP),\nallowing us to study how privacy noise affects uncertainty quality. We find\nthat while many methods degrade under DP, our trajectory-based approach remains\nrobust, and we introduce a framework for isolating the privacy-uncertainty\ntrade-off. Next, we then develop a finite-sample decomposition of the selective\nclassification gap -- the deviation from the oracle accuracy-coverage curve --\nidentifying five interpretable error sources and clarifying which interventions\ncan close the gap. This explains why calibration alone cannot fix ranking\nerrors, motivating methods that improve uncertainty ordering. Finally, we show\nthat uncertainty signals can be adversarially manipulated to hide errors or\ndeny service while maintaining high accuracy, and we design defenses combining\ncalibration audits with verifiable inference.\n  Together, these contributions advance reliable ML by improving, evaluating,\nand safeguarding uncertainty estimation, enabling models that not only make\naccurate predictions -- but also know when to say \"I do not know\".", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u63d0\u5347\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u9009\u62e9\u6027\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u4e0d\u786e\u5b9a\u6027\u8d28\u91cf\u7684\u5173\u7cfb\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u65f6\uff0c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u589e\u5f3a\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5728\u9009\u62e9\u6027\u9884\u6d4b\u4e2d\u3002", "method": "\u901a\u8fc7\u5229\u7528\u6a21\u578b\u8bad\u7ec3\u8f68\u8ff9\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6539\u53d8\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u7684\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\u9009\u62e9\u6027\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u9690\u79c1\u4fdd\u62a4\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9009\u62e9\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u8bba\u6587\u8fd8\u5206\u89e3\u4e86\u9009\u62e9\u6027\u5206\u7c7b\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u5bf9\u6297\u6027\u653b\u51fb\u7684\u7b56\u7565\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u6539\u8fdb\u3001\u8bc4\u4f30\u548c\u4fdd\u62a4\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63a8\u52a8\u4e86\u53ef\u9760\u673a\u5668\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u4f7f\u6a21\u578b\u4e0d\u4ec5\u80fd\u51c6\u786e\u9884\u6d4b\uff0c\u8fd8\u80fd\u77e5\u9053\u4f55\u65f6\u5e94\u653e\u5f03\u9884\u6d4b\u3002"}}
{"id": "2508.07265", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07265", "abs": "https://arxiv.org/abs/2508.07265", "authors": ["Bile Peng", "Vahid Jamali", "Eduard Jorswieck"], "title": "A Scalable Machine Learning Approach Enabled RIS Optimization with Implicit Channel Estimation", "comment": null, "summary": "The reconfigurable intelligent surface (RIS) is considered as a key enabler\nof the next-generation mobile radio systems. While attracting extensive\ninterest from academia and industry due to its passive nature and low cost,\nscalability of RIS elements and requirement for channel state information (CSI)\nare two major difficulties for the RIS to become a reality. In this work, we\nintroduce an unsupervised machine learning (ML) enabled optimization approach\nto configure the RIS. The dedicated neural network (NN) architecture RISnet is\ncombined with an implicit channel estimation method. The RISnet learns to map\nfrom received pilot signals to RIS configuration directly without explicit\nchannel estimation. Simulation results show that the proposed algorithm\noutperforms baselines significantly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7684RIS\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7RISnet\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u76f4\u63a5\u6620\u5c04\u63a5\u6536\u4fe1\u53f7\u5230RIS\u914d\u7f6e\uff0c\u65e0\u9700\u663e\u5f0f\u4fe1\u9053\u4f30\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3RIS\u5143\u7d20\u53ef\u6269\u5c55\u6027\u548c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u9700\u6c42\u4e24\u5927\u96be\u9898\uff0c\u63a8\u52a8RIS\u5728\u4e0b\u4e00\u4ee3\u79fb\u52a8\u65e0\u7ebf\u7535\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7ed3\u5408RISnet\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u9690\u5f0f\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u63a5\u6536\u7684\u5bfc\u9891\u4fe1\u53f7\u6620\u5c04\u5230RIS\u914d\u7f6e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4e3aRIS\u914d\u7f6e\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06638", "categories": ["cs.LG", "cs.AI", "14J60 (Primary) 14F05, 14J26 (Secondary)", "F.2.2; I.2.0"], "pdf": "https://arxiv.org/pdf/2508.06638", "abs": "https://arxiv.org/abs/2508.06638", "authors": ["Muyan Anna Li", "Aditi Gautam"], "title": "Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series", "comment": "20 pages, 11 figures", "summary": "As time series data become increasingly prevalent in domains such as\nmanufacturing, IT, and infrastructure monitoring, anomaly detection must adapt\nto nonstationary environments where statistical properties shift over time.\nTraditional static thresholds are easily rendered obsolete by regime shifts,\nconcept drift, or multi-scale changes. To address these challenges, we\nintroduce and empirically evaluate two novel adaptive thresholding frameworks:\nSegmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence\nSegments (MACS). Both leverage statistical online learning and segmentation\nprinciples for local, contextually sensitive adaptation, maintaining guarantees\non false alarm rates even under evolving distributions. Our experiments across\nWafer Manufacturing benchmark datasets show significant F1-score improvement\ncompared to traditional percentile and rolling quantile approaches. This work\ndemonstrates that robust, statistically principled adaptive thresholds enable\nreliable, interpretable, and timely detection of diverse real-world anomalies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u81ea\u9002\u5e94\u9608\u503c\u6846\u67b6\uff08SCS\u548cMACS\uff09\uff0c\u7528\u4e8e\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86F1\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u9608\u503c\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u5bb9\u6613\u5931\u6548\uff0c\u9700\u8981\u9002\u5e94\u7edf\u8ba1\u7279\u6027\u968f\u65f6\u95f4\u53d8\u5316\u7684\u573a\u666f\u3002", "method": "\u63d0\u51faSCS\u548cMACS\u6846\u67b6\uff0c\u7ed3\u5408\u7edf\u8ba1\u5728\u7ebf\u5b66\u4e60\u548c\u5206\u5272\u539f\u7406\uff0c\u5b9e\u73b0\u5c40\u90e8\u81ea\u9002\u5e94\u9608\u503c\u3002", "result": "\u5728Wafer Manufacturing\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u9002\u5e94\u9608\u503c\u6846\u67b6\u80fd\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u53ca\u65f6\u7684\u5f02\u5e38\u68c0\u6d4b\u3002"}}
{"id": "2508.07713", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07713", "abs": "https://arxiv.org/abs/2508.07713", "authors": ["Jinghan Yang", "Jiayu Weng"], "title": "Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information", "comment": "Under Working", "summary": "Deep neural networks can memorize corrupted labels, making data quality\ncritical for model performance, yet real-world datasets are frequently\ncompromised by both label noise and input noise. This paper proposes a mutual\ninformation-based framework for data selection under hybrid noise scenarios\nthat quantifies statistical dependencies between inputs and labels. We compute\neach sample's pointwise contribution to the overall mutual information and find\nthat lower contributions indicate noisy or mislabeled instances. Empirical\nvalidation on MNIST with different synthetic noise settings demonstrates that\nthe method effectively filters low-quality samples. Under label corruption,\ntraining on high-MI samples improves classification accuracy by up to 15\\%\ncompared to random sampling. Furthermore, the method exhibits robustness to\nbenign input modifications, preserving semantically valid data while filtering\ntruly corrupted samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u6df7\u5408\u566a\u58f0\u573a\u666f\u4e0b\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u7b5b\u9009\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u96c6\u5e38\u53d7\u6807\u7b7e\u566a\u58f0\u548c\u8f93\u5165\u566a\u58f0\u5f71\u54cd\uff0c\u800c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4f1a\u8bb0\u5fc6\u9519\u8bef\u6807\u7b7e\uff0c\u56e0\u6b64\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u6837\u672c\u5bf9\u6574\u4f53\u4e92\u4fe1\u606f\u7684\u70b9\u8d21\u732e\uff0c\u8bc6\u522b\u4f4e\u8d21\u732e\u6837\u672c\u4e3a\u566a\u58f0\u6216\u9519\u8bef\u6807\u7b7e\u5b9e\u4f8b\u3002", "result": "\u5728MNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\uff0c\u6807\u7b7e\u566a\u58f0\u4e0b\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe15%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5bf9\u826f\u6027\u8f93\u5165\u4fee\u6539\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u4fdd\u7559\u8bed\u4e49\u6709\u6548\u6570\u636e\u5e76\u8fc7\u6ee4\u771f\u6b63\u635f\u574f\u7684\u6837\u672c\u3002"}}
{"id": "2508.07305", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07305", "abs": "https://arxiv.org/abs/2508.07305", "authors": ["Mahdi Maleki", "Reza Agahzadeh Ayoubi", "Marouan Mizmizi", "Umberto Spagnolini"], "title": "Channel Charting in Smart Radio Environments", "comment": null, "summary": "This paper introduces the use of static electromagnetic skins (EMSs) to\nenable robust device localization via channel charting (CC) in realistic urban\nenvironments. We develop a rigorous optimization framework that leverages EMS\nto enhance channel dissimilarity and spatial fingerprinting, formulating EMS\nphase profile design as a codebook-based problem targeting the upper quantiles\nof key embedding metric, localization error, trustworthiness, and continuity.\nThrough 3D ray-traced simulations of a representative city scenario, we\ndemonstrate that optimized EMS configurations, in addition to significant\nimprovement of the average positioning error, reduce the 90th-percentile\nlocalization error from over 60 m (no EMS) to less than 25 m, while drastically\nimproving trustworthiness and continuity. To the best of our knowledge, this is\nthe first work to exploit Smart Radio Environment (SRE) with static EMS for\nenhancing CC, achieving substantial gains in localization performance under\nchallenging None-Line-of-Sight (NLoS) conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u9759\u6001\u7535\u78c1\u8868\u9762\uff08EMS\uff09\u901a\u8fc7\u4fe1\u9053\u5236\u56fe\uff08CC\uff09\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u8bbe\u5907\u5b9a\u4f4d\u3002\u901a\u8fc7\u4f18\u5316EMS\u914d\u7f6e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9a\u4f4d\u8bef\u5dee\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u5728\u975e\u89c6\u8ddd\uff08NLoS\uff09\u6761\u4ef6\u4e0b\uff0c\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u9759\u6001EMS\u589e\u5f3a\u4fe1\u9053\u5dee\u5f02\u6027\u548c\u7a7a\u95f4\u6307\u7eb9\uff0c\u4ee5\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7801\u672c\u7684\u4f18\u5316\u6846\u67b6\uff0c\u8bbe\u8ba1EMS\u76f8\u4f4d\u5256\u9762\u4ee5\u6700\u5927\u5316\u5173\u952e\u5d4c\u5165\u6307\u6807\u3001\u5b9a\u4f4d\u8bef\u5dee\u3001\u53ef\u4fe1\u5ea6\u548c\u8fde\u7eed\u6027\u3002\u901a\u8fc73D\u5c04\u7ebf\u8ffd\u8e2a\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u4f18\u5316\u540e\u7684EMS\u914d\u7f6e\u5c0690%\u5206\u4f4d\u5b9a\u4f4d\u8bef\u5dee\u4ece60\u7c73\u964d\u81f325\u7c73\u4ee5\u4e0b\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u53ef\u4fe1\u5ea6\u548c\u8fde\u7eed\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5229\u7528\u9759\u6001EMS\u589e\u5f3aCC\uff0c\u5728NLoS\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5b9a\u4f4d\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.06641", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.06641", "abs": "https://arxiv.org/abs/2508.06641", "authors": ["Jonas S Almeida", "Daniel E Russ", "Susana Vinga", "Ines Duarte", "Lee Mason", "Praphulla Bhawsar", "Aaron Ge", "Arlindo Oliveira", "Jeya Balaji Balasubramanian"], "title": "Fractal Language Modelling by Universal Sequence Maps (USM)", "comment": "16 pages, 8 figures", "summary": "Motivation: With the advent of Language Models using Transformers,\npopularized by ChatGPT, there is a renewed interest in exploring encoding\nprocedures that numerically represent symbolic sequences at multiple scales and\nembedding dimensions. The challenge that encoding addresses is the need for\nmechanisms that uniquely retain contextual information about the succession of\nindividual symbols, which can then be modeled by nonlinear formulations such as\nneural networks.\n  Context: Universal Sequence Maps(USM) are iterated functions that bijectively\nencode symbolic sequences onto embedded numerical spaces. USM is composed of\ntwo Chaos Game Representations (CGR), iterated forwardly and backwardly, that\ncan be projected into the frequency domain (FCGR). The corresponding USM\ncoordinates can be used to compute a Chebyshev distance metric as well as k-mer\nfrequencies, without having to recompute the embedded numeric coordinates, and,\nparadoxically, allowing for non-integers values of k.\n  Results: This report advances the bijective fractal encoding by Universal\nSequence Maps (USM) by resolving seeding biases affecting the iterated process.\nThe resolution had two results, the first expected, the second an intriguing\noutcome: 1) full reconciliation of numeric positioning with sequence identity;\nand 2) uncovering the nature of USM as an efficient numeric process converging\ntowards a steady state sequence embedding solution. We illustrate these results\nfor genomic sequences because of the convenience of a planar representation\ndefined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,\nthe application to alphabet of arbitrary cardinality was found to be\nstraightforward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u901a\u7528\u5e8f\u5217\u6620\u5c04\uff08USM\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u7684\u79cd\u5b50\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5e8f\u5217\u8eab\u4efd\u4e0e\u6570\u503c\u5b9a\u4f4d\u7684\u5b8c\u5168\u4e00\u81f4\uff0c\u5e76\u63ed\u793a\u4e86USM\u4f5c\u4e3a\u9ad8\u6548\u6570\u503c\u8fc7\u7a0b\u7684\u672c\u8d28\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u7684\u5174\u8d77\uff0c\u7814\u7a76\u8005\u5bf9\u591a\u5c3a\u5ea6\u548c\u591a\u7ef4\u5ea6\u7b26\u53f7\u5e8f\u5217\u7f16\u7801\u65b9\u6cd5\u91cd\u65b0\u4ea7\u751f\u5174\u8da3\uff0c\u4ee5\u4fdd\u7559\u7b26\u53f7\u5e8f\u5217\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "USM\u7531\u4e24\u4e2a\u6df7\u6c8c\u6e38\u620f\u8868\u793a\uff08CGR\uff09\u7ec4\u6210\uff0c\u5206\u522b\u5411\u524d\u548c\u5411\u540e\u8fed\u4ee3\uff0c\u53ef\u6295\u5f71\u5230\u9891\u57df\uff08FCGR\uff09\uff0c\u7528\u4e8e\u8ba1\u7b97Chebyshev\u8ddd\u79bb\u548ck-mer\u9891\u7387\u3002", "result": "\u6539\u8fdb\u7684USM\u89e3\u51b3\u4e86\u79cd\u5b50\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5e8f\u5217\u8eab\u4efd\u4e0e\u6570\u503c\u5b9a\u4f4d\u7684\u4e00\u81f4\uff0c\u5e76\u53d1\u73b0USM\u662f\u4e00\u79cd\u9ad8\u6548\u6536\u655b\u7684\u6570\u503c\u8fc7\u7a0b\u3002", "conclusion": "USM\u9002\u7528\u4e8e\u4efb\u610f\u57fa\u6570\u7684\u5b57\u6bcd\u8868\uff0c\u5c24\u5176\u5728\u57fa\u56e0\u7ec4\u5e8f\u5217\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.07746", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07746", "abs": "https://arxiv.org/abs/2508.07746", "authors": ["Fengdi Che"], "title": "A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory", "comment": null, "summary": "Offline reinforcement learning (RL) aims to optimize the return given a fixed\ndataset of agent trajectories without additional interactions with the\nenvironment. While algorithm development has progressed rapidly, significant\ntheoretical advances have also been made in understanding the fundamental\nchallenges of offline RL. However, bridging these theoretical insights with\npractical algorithm design remains an ongoing challenge. In this survey, we\nexplore key intuitions derived from theoretical work and their implications for\noffline RL algorithms.\n  We begin by listing the conditions needed for the proofs, including function\nrepresentation and data coverage assumptions. Function representation\nconditions tell us what to expect for generalization, and data coverage\nassumptions describe the quality requirement of the data. We then examine\ncounterexamples, where offline RL is not solvable without an impractically\nlarge amount of data. These cases highlight what cannot be achieved for all\nalgorithms and the inherent hardness of offline RL. Building on techniques to\nmitigate these challenges, we discuss the conditions that are sufficient for\noffline RL. These conditions are not merely assumptions for theoretical proofs,\nbut they also reveal the limitations of these algorithms and remind us to\nsearch for novel solutions when the conditions cannot be satisfied.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7406\u8bba\u57fa\u7840\u4e0e\u5b9e\u8df5\u7b97\u6cd5\u8bbe\u8ba1\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5206\u6790\u4e86\u7406\u8bba\u6761\u4ef6\u3001\u6570\u636e\u8986\u76d6\u5047\u8bbe\u4ee5\u53ca\u79bb\u7ebfRL\u7684\u56fa\u6709\u6311\u6218\u3002", "motivation": "\u79bb\u7ebfRL\u65e8\u5728\u901a\u8fc7\u56fa\u5b9a\u6570\u636e\u96c6\u4f18\u5316\u56de\u62a5\uff0c\u4f46\u7406\u8bba\u6d1e\u5bdf\u4e0e\u7b97\u6cd5\u8bbe\u8ba1\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u672c\u6587\u8bd5\u56fe\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u5217\u4e3e\u7406\u8bba\u8bc1\u660e\u6240\u9700\u7684\u6761\u4ef6\uff08\u5982\u51fd\u6570\u8868\u793a\u548c\u6570\u636e\u8986\u76d6\u5047\u8bbe\uff09\uff0c\u5206\u6790\u53cd\u4f8b\uff0c\u5e76\u8ba8\u8bba\u7f13\u89e3\u6311\u6218\u7684\u6280\u672f\u3002", "result": "\u63ed\u793a\u4e86\u79bb\u7ebfRL\u7684\u56fa\u6709\u56f0\u96be\uff0c\u5e76\u63d0\u51fa\u4e86\u6ee1\u8db3\u6761\u4ef6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u6307\u51fa\u7b97\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7406\u8bba\u6761\u4ef6\u4e0d\u4ec5\u662f\u8bc1\u660e\u7684\u57fa\u7840\uff0c\u4e5f\u63ed\u793a\u4e86\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9192\u6211\u4eec\u5728\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\u5bfb\u627e\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07436", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07436", "abs": "https://arxiv.org/abs/2508.07436", "authors": ["Mehrbod Zarifi", "Mohamad Amin Jamshidi", "Zolfa Anvari", "Hamed Ghafarirad", "Mohammad Zareinejad"], "title": "Detection and Classification of Internal Leakage in Hydraulic Cylinders", "comment": "10 pages, 7 figures, presented at the 12th RSI International\n  Conference on Robotics and Mechatronics (ICRoM 2024), IEEE", "summary": "Hydraulic systems have been one of the most used technologies in many\nindustries due to their reliance on incompressible fluids that facilitate\nenergy and power transfer. Within such systems, hydraulic cylinders are prime\ndevices that convert hydraulic energy into mechanical energy. Some of the\ngenuine and very common problems related to hydraulic cylinders are leakages.\nLeakage in hydraulic systems can cause a drop in pressure, general\ninefficiency, and even complete failure of such systems. The various ways\nleakage can occur define the major categorization of leakage: internal and\nexternal leakage. External leakage is easily noticeable, while internal\nleakage, which involves fluid movement between pressure chambers, can be harder\nto detect and may gradually impact system performance without obvious signs.\nWhen leakage surpasses acceptable limits, it is classified as a fault or\nfailure. In such cases, leakage is divided into three categories: no leakage,\nlow leakage, and high leakage. It suggests a fault detection algorithm with the\nbasic responsibility of detecting minimum leakage within the Hydraulic system,\nand minimizing detection time is the core idea of this paper. In order to fully\ndevelop this idea, experimental data collection of Hydraulic systems is\nrequired. The collected data uses pressure sensors and other signals that are\nsingle-related. Due to the utilization of Long Short-Term Memory (LSTM)\nrecurrent neural networks, more complex data analysis was enabled, which the\nLSTM-based leakage detection algorithm successfully achieved, providing almost\n96% accuracy in classifying leakage types. Results demonstrate that the\nproposed method can perform real-time and online fault diagnosis for each\ncycle, reducing maintenance costs and prolonging the hydraulic system's\nlifespan.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u7684\u6db2\u538b\u7cfb\u7edf\u6cc4\u6f0f\u68c0\u6d4b\u7b97\u6cd5\uff0c\u51c6\u786e\u7387\u8fbe96%\uff0c\u53ef\u5b9e\u65f6\u5728\u7ebf\u8bca\u65ad\u6545\u969c\uff0c\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u5e76\u5ef6\u957f\u7cfb\u7edf\u5bff\u547d\u3002", "motivation": "\u6db2\u538b\u7cfb\u7edf\u6cc4\u6f0f\u95ee\u9898\uff08\u5c24\u5176\u662f\u5185\u90e8\u6cc4\u6f0f\uff09\u96be\u4ee5\u68c0\u6d4b\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u6548\u7387\u4e0b\u964d\u6216\u5b8c\u5168\u6545\u969c\uff0c\u4e9f\u9700\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u538b\u529b\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u91c7\u7528LSTM\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u590d\u6742\u6570\u636e\u5206\u6790\uff0c\u5f00\u53d1\u6cc4\u6f0f\u68c0\u6d4b\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u6cc4\u6f0f\u5206\u7c7b\u4e2d\u8fbe\u523096%\u7684\u51c6\u786e\u7387\uff0c\u652f\u6301\u5b9e\u65f6\u5728\u7ebf\u6545\u969c\u8bca\u65ad\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6db2\u538b\u7cfb\u7edf\u6cc4\u6f0f\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.06647", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06647", "abs": "https://arxiv.org/abs/2508.06647", "authors": ["Andrey Sidorenko", "Paul Tiwald"], "title": "Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN", "comment": null, "summary": "Synthetic data generation has become essential for securely sharing and\nanalyzing sensitive data sets. Traditional anonymization techniques, however,\noften fail to adequately preserve privacy. We introduce the Tabular\nAuto-Regressive Generative Network (TabularARGN), a neural network architecture\nspecifically designed for generating high-quality synthetic tabular data. Using\na discretization-based auto-regressive approach, TabularARGN achieves high data\nfidelity while remaining computationally efficient. We evaluate TabularARGN\nagainst existing synthetic data generation methods, showing competitive results\nin statistical similarity, machine learning utility, and detection robustness.\nWe further perform an in-depth privacy evaluation using systematic\nmembership-inference attacks, highlighting the robustness and effective\nprivacy-utility balance of our approach.", "AI": {"tldr": "TabularARGN\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u5e73\u8861\u9690\u79c1\u4e0e\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u533f\u540d\u5316\u6280\u672f\u65e0\u6cd5\u5145\u5206\u4fdd\u62a4\u9690\u79c1\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u79bb\u6563\u5316\u7684\u81ea\u56de\u5f52\u65b9\u6cd5\uff0c\u8bbe\u8ba1TabularARGN\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728\u7edf\u8ba1\u76f8\u4f3c\u6027\u3001\u673a\u5668\u5b66\u4e60\u5b9e\u7528\u6027\u548c\u68c0\u6d4b\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9690\u79c1\u8bc4\u4f30\u663e\u793a\u5176\u5177\u6709\u7a33\u5065\u6027\u3002", "conclusion": "TabularARGN\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6570\u636e\u5b9e\u7528\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861\u3002"}}
{"id": "2508.08222", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.08222", "abs": "https://arxiv.org/abs/2508.08222", "authors": ["Tong Yang", "Yu Huang", "Yingbin Liang", "Yuejie Chi"], "title": "Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent", "comment": "submitted for consideration of publication in May", "summary": "Transformers have demonstrated remarkable capabilities in multi-step\nreasoning tasks. However, understandings of the underlying mechanisms by which\nthey acquire these abilities through training remain limited, particularly from\na theoretical standpoint. This work investigates how transformers learn to\nsolve symbolic multi-step reasoning problems through chain-of-thought\nprocesses, focusing on path-finding in trees. We analyze two intertwined tasks:\na backward reasoning task, where the model outputs a path from a goal node to\nthe root, and a more complex forward reasoning task, where the model implements\ntwo-stage reasoning by first identifying the goal-to-root path and then\nreversing it to produce the root-to-goal path. Our theoretical analysis,\ngrounded in the dynamics of gradient descent, shows that trained one-layer\ntransformers can provably solve both tasks with generalization guarantees to\nunseen trees. In particular, our multi-phase training dynamics for forward\nreasoning elucidate how different attention heads learn to specialize and\ncoordinate autonomously to solve the two subtasks in a single autoregressive\npath. These results provide a mechanistic explanation of how trained\ntransformers can implement sequential algorithmic procedures. Moreover, they\noffer insights into the emergence of reasoning abilities, suggesting that when\ntasks are structured to take intermediate chain-of-thought steps, even shallow\nmulti-head transformers can effectively solve problems that would otherwise\nrequire deeper architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Transformer\u5982\u4f55\u901a\u8fc7\u8bad\u7ec3\u5b66\u4e60\u89e3\u51b3\u7b26\u53f7\u591a\u6b65\u63a8\u7406\u95ee\u9898\uff0c\u7279\u522b\u662f\u6811\u4e2d\u7684\u8def\u5f84\u67e5\u627e\u4efb\u52a1\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5355\u5c42Transformer\u53ef\u4ee5\u89e3\u51b3\u524d\u5411\u548c\u540e\u5411\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7406\u89e3Transformer\u5982\u4f55\u901a\u8fc7\u8bad\u7ec3\u83b7\u5f97\u591a\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u4ece\u7406\u8bba\u89d2\u5ea6\u5206\u6790\u5176\u673a\u5236\u3002", "method": "\u5206\u6790\u4e86\u540e\u5411\u63a8\u7406\u548c\u524d\u5411\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u52a8\u6001\u7406\u8bba\uff0c\u7814\u7a76\u4e86\u5355\u5c42Transformer\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8bad\u7ec3\u540e\u7684\u5355\u5c42Transformer\u53ef\u4ee5\u89e3\u51b3\u4efb\u52a1\uff0c\u5e76\u5177\u6709\u6cdb\u5316\u80fd\u529b\uff1b\u591a\u9636\u6bb5\u8bad\u7ec3\u52a8\u6001\u63ed\u793a\u4e86\u6ce8\u610f\u529b\u5934\u7684\u5206\u5de5\u4e0e\u534f\u4f5c\u3002", "conclusion": "\u8bba\u6587\u63ed\u793a\u4e86Transformer\u5982\u4f55\u5b9e\u73b0\u5e8f\u5217\u5316\u7b97\u6cd5\u8fc7\u7a0b\uff0c\u8868\u660e\u7ed3\u6784\u5316\u4efb\u52a1\u548c\u4e2d\u95f4\u601d\u7ef4\u94fe\u6b65\u9aa4\u53ef\u4ee5\u4f7f\u6d45\u5c42Transformer\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002"}}
{"id": "2508.07513", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07513", "abs": "https://arxiv.org/abs/2508.07513", "authors": ["Emre Kurtoglu", "Mohammad Mahbubur Rahman"], "title": "Direction of Arrival Estimation with Virtual Antenna Array Using FMCW Radar Simulated Data", "comment": null, "summary": "The FMCW radars are widely used for automotive radar systems. The basic idea\nfor FMCW radars is to generate a linear frequency ramp as transmit signal. The\ndifference frequency, (i.e., beat frequency) between the transmitted and\nreceived signal is determined after down conversion. The FFT operation on beat\nfrequency signal can recognize targets at different range and velocity.\nIncreasing demand on safety functionality leads to the Direction of Arrival\n(DOA) estimation to resolve two closely located targets. Consequently, the\nproblem of angle estimation for 77GHz FMCW automotive radar simulated data has\nbeen investigated in this term project. In particular, we examined the\nperformances of FFT, MUSIC and compressed sensing in angle estimation task, and\nit was found that although FFT is the fastest algorithm, it has very poor\nangular resolution when compared with others which are both super resolution\nalgorithms. The code for this project report is available at\nhttps://github.com/ekurtgl/FMCW-MIMO-Radar-Simulation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e8677GHz FMCW\u6c7d\u8f66\u96f7\u8fbe\u7684\u89d2\u5ea6\u4f30\u8ba1\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86FFT\u3001MUSIC\u548c\u538b\u7f29\u611f\u77e5\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u53d1\u73b0FFT\u901f\u5ea6\u6700\u5feb\u4f46\u89d2\u5ea6\u5206\u8fa8\u7387\u8f83\u5dee\u3002", "motivation": "\u968f\u7740\u5b89\u5168\u529f\u80fd\u9700\u6c42\u7684\u589e\u52a0\uff0c\u9700\u8981\u89e3\u51b3\u4e24\u4e2a\u8fd1\u8ddd\u79bb\u76ee\u6807\u7684\u65b9\u5411\u5230\u8fbe\uff08DOA\uff09\u4f30\u8ba1\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86FFT\u3001MUSIC\u548c\u538b\u7f29\u611f\u77e5\u7b97\u6cd5\u5728\u89d2\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "FFT\u901f\u5ea6\u6700\u5feb\u4f46\u89d2\u5ea6\u5206\u8fa8\u7387\u8f83\u5dee\uff0c\u800cMUSIC\u548c\u538b\u7f29\u611f\u77e5\u662f\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\uff0c\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "MUSIC\u548c\u538b\u7f29\u611f\u77e5\u5728\u89d2\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eFFT\uff0c\u9002\u5408\u9ad8\u5206\u8fa8\u7387\u9700\u6c42\u7684\u5e94\u7528\u3002"}}
{"id": "2508.06659", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06659", "abs": "https://arxiv.org/abs/2508.06659", "authors": ["Fernando Martinez-Lopez", "Tao Li", "Yingdong Lu", "Juntao Chen"], "title": "In-Context Reinforcement Learning via Communicative World Models", "comment": null, "summary": "Reinforcement learning (RL) agents often struggle to generalize to new tasks\nand contexts without updating their parameters, mainly because their learned\nrepresentations and policies are overfit to the specifics of their training\nenvironments. To boost agents' in-context RL (ICRL) ability, this work\nformulates ICRL as a two-agent emergent communication problem and introduces\nCORAL (Communicative Representation for Adaptive RL), a framework that learns a\ntransferable communicative context by decoupling latent representation learning\nfrom control. In CORAL, an Information Agent (IA) is pre-trained as a world\nmodel on a diverse distribution of tasks. Its objective is not to maximize task\nreward, but to build a world model and distill its understanding into concise\nmessages. The emergent communication protocol is shaped by a novel Causal\nInfluence Loss, which measures the effect that the message has on the next\naction. During deployment, the previously trained IA serves as a fixed\ncontextualizer for a new Control Agent (CA), which learns to solve tasks by\ninterpreting the provided communicative context. Our experiments demonstrate\nthat this approach enables the CA to achieve significant gains in sample\nefficiency and successfully perform zero-shot adaptation with the help of\npre-trained IA in entirely unseen sparse-reward environments, validating the\nefficacy of learning a transferable communicative representation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCORAL\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u63a7\u5236\u89e3\u8026\uff0c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u4e0a\u4e0b\u6587\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u65b0\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u5176\u8868\u793a\u548c\u7b56\u7565\u8fc7\u5ea6\u62df\u5408\u8bad\u7ec3\u73af\u5883\u3002", "method": "CORAL\u6846\u67b6\u5c06\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u89c6\u4e3a\u53cc\u4ee3\u7406\u901a\u4fe1\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u4fe1\u606f\u4ee3\u7406\uff08IA\uff09\u751f\u6210\u53ef\u8fc1\u79fb\u7684\u901a\u4fe1\u4e0a\u4e0b\u6587\uff0c\u63a7\u5236\u4ee3\u7406\uff08CA\uff09\u5229\u7528\u8be5\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCORAL\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "conclusion": "\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u901a\u4fe1\u8868\u793a\u662f\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u6cdb\u5316\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.07572", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07572", "abs": "https://arxiv.org/abs/2508.07572", "authors": ["Yuanwei Liu", "Hao Jiang", "Xiaoxia Xu", "Zhaolin Wang", "Jia Guo", "Chongjun Ouyang", "Xidong Mu", "Zhiguo Ding", "Arumugam Nallanathan", "George K. Karagiannidis", "Robert Schober"], "title": "Pinching-Antenna Systems (PASS): A Tutorial", "comment": "Submitted to IEEE journal", "summary": "Pinching antenna systems (PASS) present a breakthrough among the\nflexible-antenna technologies, and distinguish themselves by facilitating\nlarge-scale antenna reconfiguration, line-of-sight creation, scalable\nimplementation, and near-field benefits, thus bringing wireless communications\nfrom the last mile to the last meter. A comprehensive tutorial is presented in\nthis paper. First, the fundamentals of PASS are discussed, including PASS\nsignal models, hardware models, power radiation models, and pinching antenna\nactivation methods. Building upon this, the information-theoretic capacity\nlimits achieved by PASS are characterized, and several typical performance\nmetrics of PASS-based communications are analyzed to demonstrate its\nsuperiority over conventional antenna technologies. Next, the pinching\nbeamforming design is investigated. The corresponding power scaling law is\nfirst characterized. For the joint transmit and pinching design in the general\nmultiple-waveguide case, 1) a pair of transmission strategies is proposed for\nPASS-based single-user communications to validate the superiority of PASS,\nnamely sub-connected and fully connected structures; and 2) three practical\nprotocols are proposed for facilitating PASS-based multi-user communications,\nnamely waveguide switching, waveguide division, and waveguide multiplexing. A\npossible implementation of PASS in wideband communications is further\nhighlighted. Moreover, the channel state information acquisition in PASS is\nelaborated with a pair of promising solutions. To overcome the high complexity\nand suboptimality inherent in conventional convex-optimization-based\napproaches, machine-learning-based methods for operating PASS are also\nexplored, focusing on selected deep neural network architectures and training\nalgorithms. Finally, several promising applications of PASS in next-generation\nwireless networks are highlighted.", "AI": {"tldr": "PASS\u6280\u672f\u901a\u8fc7\u5927\u89c4\u6a21\u5929\u7ebf\u91cd\u6784\u3001\u89c6\u7ebf\u521b\u5efa\u548c\u8fd1\u573a\u4f18\u52bf\uff0c\u5c06\u65e0\u7ebf\u901a\u4fe1\u4ece\u6700\u540e\u4e00\u516c\u91cc\u6269\u5c55\u5230\u6700\u540e\u4e00\u7c73\u3002\u672c\u6587\u5168\u9762\u4ecb\u7ecd\u4e86PASS\u7684\u57fa\u7840\u3001\u6027\u80fd\u4f18\u52bf\u3001\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u3001\u591a\u7528\u6237\u901a\u4fe1\u534f\u8bae\u3001\u5bbd\u5e26\u5b9e\u73b0\u3001\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u53ca\u673a\u5668\u5b66\u4e60\u5e94\u7528\u3002", "motivation": "PASS\u6280\u672f\u7a81\u7834\u4e86\u4f20\u7edf\u5929\u7ebf\u6280\u672f\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u8ba8\u8bba\u4e86PASS\u7684\u57fa\u7840\u6a21\u578b\u3001\u4fe1\u606f\u8bba\u5bb9\u91cf\u9650\u5236\u3001\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u3001\u591a\u7528\u6237\u901a\u4fe1\u534f\u8bae\u3001\u5bbd\u5e26\u5b9e\u73b0\u3001\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "result": "PASS\u5728\u5355\u7528\u6237\u548c\u591a\u7528\u6237\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u5929\u7ebf\u6280\u672f\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u590d\u6742\u5ea6\u95ee\u9898\u3002", "conclusion": "PASS\u6280\u672f\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u5c24\u5176\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u5176\u7075\u6d3b\u6027\u548c\u9ad8\u6027\u80fd\u5c06\u63a8\u52a8\u65e0\u7ebf\u901a\u4fe1\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.06663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06663", "abs": "https://arxiv.org/abs/2508.06663", "authors": ["Yuan-Hung Chao", "Chia-Hsun Lu", "Chih-Ya Shen"], "title": "Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks", "comment": "6 pages, 3 tables", "summary": "Graph Neural Networks (GNNs) have shown strong performance on\ngraph-structured data, but their reliance on graph connectivity often limits\nscalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent\narchitecture with learnable univariate functions, offer strong nonlinear\nexpressiveness and efficient inference. In this work, we integrate KANs into\nthree popular GNN architectures-GAT, SGC, and APPNP-resulting in three new\nmodels: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge\namalgamation framework, where knowledge from multiple KAN-based GNNs is\ndistilled into a graph-independent KAN student model. Experiments on benchmark\ndatasets show that the proposed models improve node classification accuracy,\nand the knowledge amalgamation approach significantly boosts student model\nperformance. Our findings highlight the potential of KANs for enhancing GNN\nexpressiveness and for enabling efficient, graph-free inference.", "AI": {"tldr": "\u5c06KANs\uff08Kolmogorov-Arnold Networks\uff09\u96c6\u6210\u5230\u4e09\u79cd\u6d41\u884c\u7684GNN\u67b6\u6784\u4e2d\uff0c\u63d0\u51fa\u65b0\u6a21\u578bKGAT\u3001KSGC\u548cKAPPNP\uff0c\u5e76\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u878d\u5408\u6846\u67b6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "GNNs\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u56fe\u8fde\u63a5\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002KANs\u5177\u6709\u5f3a\u5927\u7684\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b\u548c\u9ad8\u6548\u63a8\u7406\u80fd\u529b\uff0c\u53ef\u5f25\u8865GNNs\u7684\u4e0d\u8db3\u3002", "method": "\u5c06KANs\u96c6\u6210\u5230GAT\u3001SGC\u548cAPPNP\u4e09\u79cdGNN\u67b6\u6784\u4e2d\uff0c\u5f62\u6210\u65b0\u6a21\u578b\uff1b\u91c7\u7528\u591a\u6559\u5e08\u77e5\u8bc6\u878d\u5408\u6846\u67b6\uff0c\u5c06\u591a\u4e2aKAN-based GNNs\u7684\u77e5\u8bc6\u84b8\u998f\u5230\u4e00\u4e2a\u56fe\u65e0\u5173\u7684KAN\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u6a21\u578b\u63d0\u9ad8\u4e86\u8282\u70b9\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u77e5\u8bc6\u878d\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "KANs\u80fd\u589e\u5f3aGNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u56fe\u65e0\u5173\u63a8\u7406\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07651", "abs": "https://arxiv.org/abs/2508.07651", "authors": ["Ziye Jia", "Yian Zhu", "Qihui Wu", "Lei Zhang", "Sen Yang", "Zhu Han"], "title": "Remote ID Based UAV Collision Avoidance Optimization for Low-Altitude Airspace Safety", "comment": null, "summary": "With the rapid development of unmanned aerial vehicles (UAVs), it is\nparamount to ensure safe and efficient operations in open airspaces. The remote\nidentification (Remote ID) is deemed an effective real-time UAV monitoring\nsystem by the federal aviation administration, which holds potentials for\nenabling inter-UAV communications. This paper deeply investigates the\napplication of Remote ID for UAV collision avoidance while minimizing\ncommunication delays. First, we propose a Remote ID based distributed multi-UAV\ncollision avoidance (DMUCA) framework to support the collision detection,\navoidance decision-making, and trajectory recovery. Next, the average\ntransmission delays for Remote ID messages are analyzed, incorporating the\npacket reception mechanisms and packet loss due to interference. The\noptimization problem is formulated to minimize the long-term average\ncommunication delay, where UAVs can flexibly select the Remote ID protocol to\nenhance the collision avoidance performance. To tackle the problem, we design a\nmulti-agent deep Q-network based adaptive communication configuration\nalgorithm, allowing UAVs to autonomously learn the optimal protocol\nconfigurations in dynamic environments. Finally, numerical results verify the\nfeasibility of the proposed DMUCA framework, and the proposed mechanism can\nreduce the average delay by 32% compared to the fixed protocol configuration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRemote ID\u7684\u5206\u5e03\u5f0f\u591a\u65e0\u4eba\u673a\u78b0\u649e\u907f\u514d\u6846\u67b6\uff08DMUCA\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u901a\u4fe1\u5ef6\u8fdf\u63d0\u5347\u78b0\u649e\u907f\u514d\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u786e\u4fdd\u5176\u5728\u5f00\u653e\u7a7a\u57df\u7684\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002Remote ID\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u65f6\u76d1\u63a7\u7cfb\u7edf\uff0c\u5177\u6709\u652f\u6301\u65e0\u4eba\u673a\u95f4\u901a\u4fe1\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faDMUCA\u6846\u67b6\uff0c\u5206\u6790Remote ID\u6d88\u606f\u7684\u5e73\u5747\u4f20\u8f93\u5ef6\u8fdf\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6Q\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u901a\u4fe1\u914d\u7f6e\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cDMUCA\u6846\u67b6\u53ef\u884c\uff0c\u4e14\u63d0\u51fa\u7684\u673a\u5236\u80fd\u5c06\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e32%\u3002", "conclusion": "Remote ID\u53ef\u7528\u4e8e\u65e0\u4eba\u673a\u78b0\u649e\u907f\u514d\uff0c\u4e14\u81ea\u9002\u5e94\u534f\u8bae\u914d\u7f6e\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.06676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06676", "abs": "https://arxiv.org/abs/2508.06676", "authors": ["Chia-Hsun Lu", "Guan-Jhih Wu", "Ya-Chi Ho", "Chih-Ya Shen"], "title": "Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation", "comment": "6 pages, 3 figures, 6 tables", "summary": "With the increasing importance of protecting intellectual property in machine\nlearning, watermarking techniques have gained significant attention. As\nadvanced models are increasingly deployed in domains such as social network\nanalysis, the need for robust model protection becomes even more critical.\nWhile existing watermarking methods have demonstrated effectiveness for\nconventional deep neural networks, they often fail to adapt to the novel\narchitecture, Kolmogorov-Arnold Networks (KAN), which feature learnable\nactivation functions. KAN holds strong potential for modeling complex\nrelationships in network-structured data. However, their unique design also\nintroduces new challenges for watermarking. Therefore, we propose a novel\nwatermarking method, Discrete Cosine Transform-based Activation Watermarking\n(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of\nKAN, our method embeds watermarks by perturbing activation outputs using\ndiscrete cosine transform, ensuring compatibility with diverse tasks and\nachieving task independence. Experimental results demonstrate that DCT-AW has a\nsmall impact on model performance and provides superior robustness against\nvarious watermark removal attacks, including fine-tuning, pruning, and\nretraining after pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Kolmogorov-Arnold Networks (KAN)\u7684\u65b0\u578b\u6c34\u5370\u65b9\u6cd5DCT-AW\uff0c\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u6270\u52a8\u6fc0\u6d3b\u8f93\u51fa\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u65e0\u5173\u6027\u548c\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u4e2d\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u7684\u91cd\u8981\u6027\u589e\u52a0\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94KAN\u7684\u65b0\u578b\u67b6\u6784\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u7684\u6c34\u5370\u65b9\u6cd5\u3002", "method": "\u5229\u7528KAN\u7684\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u6270\u52a8\u6fc0\u6d3b\u8f93\u51fa\u5d4c\u5165\u6c34\u5370\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDCT-AW\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u5c0f\uff0c\u4e14\u80fd\u6709\u6548\u62b5\u6297\u5fae\u8c03\u3001\u526a\u679d\u7b49\u653b\u51fb\u3002", "conclusion": "DCT-AW\u662f\u4e00\u79cd\u9002\u7528\u4e8eKAN\u7684\u9ad8\u6548\u6c34\u5370\u65b9\u6cd5\uff0c\u517c\u5177\u4efb\u52a1\u65e0\u5173\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07696", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07696", "abs": "https://arxiv.org/abs/2508.07696", "authors": ["Joohyuk Park", "Yongjeong Oh", "Jihun Park", "Yo-Seb Jeon"], "title": "Importance-Aware Semantic Communication in MIMO-OFDM Systems Using Vision Transformer", "comment": null, "summary": "This paper presents a novel importance-aware quantization, subcarrier\nmapping, and power allocation (IA-QSMPA) framework for semantic communication\nin multiple-input multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) systems, empowered by a pretrained Vision Transformer (ViT). The\nproposed framework exploits attention-based importance extracted from a\npretrained ViT to jointly optimize quantization levels, subcarrier mapping, and\npower allocation. Specifically, IA-QSMPA maps semantically important features\nto high-quality subchannels and allocates resources in accordance with their\ncontribution to task performance and communication latency. To efficiently\nsolve the resulting nonconvex optimization problem, a block coordinate descent\nalgorithm is employed. The framework is further extended to operate under\nfinite blocklength transmission, where communication errors may occur. In this\nsetting, a segment-wise linear approximation of the channel dispersion penalty\nis introduced to enable efficient joint optimization under practical\nconstraints. Simulation results on a multi-view image classification task using\nthe MVP-N dataset demonstrate that IA-QSMPA significantly outperforms\nconventional methods in both ideal and finite blocklength transmission\nscenarios, achieving superior task performance and communication efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3ViT\u7684IA-QSMPA\u6846\u67b6\uff0c\u7528\u4e8eMIMO-OFDM\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u901a\u4fe1\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u91cf\u5316\u3001\u5b50\u8f7d\u6ce2\u6620\u5c04\u548c\u529f\u7387\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8bed\u4e49\u901a\u4fe1\u4e2d\u8d44\u6e90\u5206\u914d\u4e0d\u5408\u7406\u7684\u95ee\u9898\uff0c\u5229\u7528ViT\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u91cd\u8981\u6027\u4fe1\u606f\uff0c\u4f18\u5316\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3ViT\u63d0\u53d6\u91cd\u8981\u6027\uff0c\u8054\u5408\u4f18\u5316\u91cf\u5316\u3001\u5b50\u8f7d\u6ce2\u6620\u5c04\u548c\u529f\u7387\u5206\u914d\uff0c\u91c7\u7528\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u6709\u9650\u5757\u957f\u4f20\u8f93\u573a\u666f\u3002", "result": "\u5728MVP-N\u6570\u636e\u96c6\u4e0a\u7684\u591a\u89c6\u56fe\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cIA-QSMPA\u5728\u7406\u60f3\u548c\u6709\u9650\u5757\u957f\u4f20\u8f93\u573a\u666f\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "IA-QSMPA\u6846\u67b6\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u7684\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u4efb\u52a1\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.06692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06692", "abs": "https://arxiv.org/abs/2508.06692", "authors": ["Md. Akmol Masud", "Md Abrar Jahin", "Mahmud Hasan"], "title": "Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select", "comment": null, "summary": "Federated Learning (FL) is a machine learning technique that often suffers\nfrom training instability due to the diverse nature of client data. Although\nutility-based client selection methods like Oort are used to converge by\nprioritizing high-loss clients, they frequently experience significant drops in\naccuracy during later stages of training. We propose a theoretical\nHeteRo-Select framework designed to maintain high performance and ensure\nlong-term training stability. We provide a theoretical analysis showing that\nwhen client data is very different (high heterogeneity), choosing a smart\nsubset of client participation can reduce communication more effectively\ncompared to full participation. Our HeteRo-Select method uses a clear,\nstep-by-step scoring system that considers client usefulness, fairness, update\nspeed, and data variety. It also shows convergence guarantees under strong\nregularization. Our experimental results on the CIFAR-10 dataset under\nsignificant label skew ($\\alpha=0.1$) support the theoretical findings. The\nHeteRo-Select method performs better than existing approaches in terms of peak\naccuracy, final accuracy, and training stability. Specifically, HeteRo-Select\nachieves a peak accuracy of $74.75\\%$, a final accuracy of $72.76\\%$, and a\nminimal stability drop of $1.99\\%$. In contrast, Oort records a lower peak\naccuracy of $73.98\\%$, a final accuracy of $71.25\\%$, and a larger stability\ndrop of $2.73\\%$. The theoretical foundations and empirical performance in our\nstudy make HeteRo-Select a reliable solution for real-world heterogeneous FL\nproblems.", "AI": {"tldr": "HeteRo-Select\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u5ba2\u6237\u7aef\u5b50\u96c6\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u56e0\u5ba2\u6237\u7aef\u6570\u636e\u591a\u6837\u6027\u5e38\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982Oort\uff09\u5728\u540e\u671f\u8bad\u7ec3\u4e2d\u51c6\u786e\u6027\u4e0b\u964d\u660e\u663e\u3002", "method": "\u63d0\u51faHeteRo-Select\u6846\u67b6\uff0c\u57fa\u4e8e\u5ba2\u6237\u7aef\u6709\u7528\u6027\u3001\u516c\u5e73\u6027\u3001\u66f4\u65b0\u901f\u5ea6\u548c\u6570\u636e\u591a\u6837\u6027\u8fdb\u884c\u9010\u6b65\u8bc4\u5206\uff0c\u5e76\u7406\u8bba\u5206\u6790\u5176\u6536\u655b\u6027\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cHeteRo-Select\u7684\u5cf0\u503c\u51c6\u786e\u7387\u4e3a74.75%\uff0c\u6700\u7ec8\u51c6\u786e\u7387\u4e3a72.76%\uff0c\u7a33\u5b9a\u6027\u4e0b\u964d\u4ec51.99%\uff0c\u4f18\u4e8eOort\u3002", "conclusion": "HeteRo-Select\u4e3a\u5f02\u6784FL\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u662f\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07717", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07717", "abs": "https://arxiv.org/abs/2508.07717", "authors": ["Yuchen Gao", "Xiao Xu", "Eckehard Steinbach", "Daniel E. Lucani", "Qi Zhang"], "title": "Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction", "comment": null, "summary": "This paper presents a multimodal framework that integrates touch signals\n(contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our\napproach enhances scene reconstruction, particularly under challenging\nconditions like low lighting, limited camera viewpoints, and occlusions.\nDifferent from the visual-only method, the proposed approach incorporates\nspatially selective touch measurements to refine both the geometry and\nappearance of the 3D Gaussian representation. To guide the touch exploration,\nwe introduce a two-stage sampling scheme that initially probes sparse regions\nand then concentrates on high-uncertainty boundaries identified from the\nreconstructed mesh. A geometric loss is proposed to ensure surface smoothness,\nresulting in improved geometry. Experimental results across diverse scenarios\nshow consistent improvements in geometric accuracy. In the most challenging\ncase with severe occlusion, the Chamfer Distance is reduced by over 15x,\ndemonstrating the effectiveness of integrating touch cues into 3D Gaussian\nSplatting. Furthermore, our approach maintains a fully online pipeline,\nunderscoring its feasibility in visually degraded environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u89e6\u89c9\u4fe1\u53f7\uff08\u63a5\u89e6\u70b9\u548c\u8868\u9762\u6cd5\u7ebf\uff09\u96c6\u6210\u52303D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\uff0c\u63d0\u5347\u573a\u666f\u91cd\u5efa\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u4f4e\u5149\u7167\u3001\u6709\u9650\u89c6\u89d2\u548c\u906e\u6321\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u3001\u6709\u9650\u89c6\u89d2\u548c\u906e\u6321\u7b49\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f15\u5165\u89e6\u89c9\u4fe1\u53f7\u4ee5\u589e\u5f3a\u91cd\u5efa\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u91c7\u6837\u65b9\u6848\uff1a\u5148\u63a2\u6d4b\u7a00\u758f\u533a\u57df\uff0c\u518d\u805a\u7126\u4e8e\u9ad8\u4e0d\u786e\u5b9a\u8fb9\u754c\uff1b\u63d0\u51fa\u51e0\u4f55\u635f\u5931\u4ee5\u786e\u4fdd\u8868\u9762\u5e73\u6ed1\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u51e0\u4f55\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u4e25\u91cd\u906e\u6321\u60c5\u51b5\u4e0bChamfer Distance\u964d\u4f4e\u8d85\u8fc715\u500d\u3002", "conclusion": "\u89e6\u89c9\u4fe1\u53f7\u7684\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u91cd\u5efa\u6548\u679c\uff0c\u4e14\u5b8c\u5168\u5728\u7ebf\u6d41\u7a0b\u9002\u7528\u4e8e\u89c6\u89c9\u9000\u5316\u73af\u5883\u3002"}}
{"id": "2508.06704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06704", "abs": "https://arxiv.org/abs/2508.06704", "authors": ["Hager Radi Abdelwahed", "M\u00e9lisande Teng", "Robin Zbinden", "Laura Pollock", "Hugo Larochelle", "Devis Tuia", "David Rolnick"], "title": "CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations", "comment": null, "summary": "Species distribution models (SDMs) are widely used to predict species'\ngeographic distributions, serving as critical tools for ecological research and\nconservation planning. Typically, SDMs relate species occurrences to\nenvironmental variables representing abiotic factors, such as temperature,\nprecipitation, and soil properties. However, species distributions are also\nstrongly influenced by biotic interactions with other species, which are often\noverlooked. While some methods partially address this limitation by\nincorporating biotic interactions, they often assume symmetrical pairwise\nrelationships between species and require consistent co-occurrence data. In\npractice, species observations are sparse, and the availability of information\nabout the presence or absence of other species varies significantly across\nlocations. To address these challenges, we propose CISO, a deep learning-based\nmethod for species distribution modeling Conditioned on Incomplete Species\nObservations. CISO enables predictions to be conditioned on a flexible number\nof species observations alongside environmental variables, accommodating the\nvariability and incompleteness of available biotic data. We demonstrate our\napproach using three datasets representing different species groups: sPlotOpen\nfor plants, SatBird for birds, and a new dataset, SatButterfly, for\nbutterflies. Our results show that including partial biotic information\nimproves predictive performance on spatially separate test sets. When\nconditioned on a subset of species within the same dataset, CISO outperforms\nalternative methods in predicting the distribution of the remaining species.\nFurthermore, we show that combining observations from multiple datasets can\nimprove performance. CISO is a promising ecological tool, capable of\nincorporating incomplete biotic information and identifying potential\ninteractions between species from disparate taxa.", "AI": {"tldr": "CISO\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u65b9\u6cd5\uff0c\u80fd\u591f\u7ed3\u5408\u4e0d\u5b8c\u6574\u7684\u7269\u79cd\u89c2\u6d4b\u6570\u636e\u4e0e\u73af\u5883\u53d8\u91cf\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7269\u79cd\u5206\u5e03\u6a21\u578b\uff08SDMs\uff09\u901a\u5e38\u5ffd\u7565\u7269\u79cd\u95f4\u7684\u751f\u7269\u76f8\u4e92\u4f5c\u7528\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5bf9\u6570\u636e\u8981\u6c42\u4e25\u683c\u3002CISO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CISO\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\uff0c\u7075\u6d3b\u7ed3\u5408\u73af\u5883\u53d8\u91cf\u548c\u4e0d\u5b8c\u6574\u7684\u7269\u79cd\u89c2\u6d4b\u6570\u636e\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7269\u79cd\u7ec4\u7684\u6570\u636e\u96c6\uff08\u5982\u690d\u7269\u3001\u9e1f\u7c7b\u548c\u8774\u8776\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCISO\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u5408\u90e8\u5206\u751f\u7269\u4fe1\u606f\u6216\u591a\u6570\u636e\u96c6\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "CISO\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u751f\u6001\u5de5\u5177\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u5b8c\u6574\u751f\u7269\u6570\u636e\u5e76\u8bc6\u522b\u8de8\u7c7b\u7fa4\u7269\u79cd\u95f4\u7684\u6f5c\u5728\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2508.07909", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07909", "abs": "https://arxiv.org/abs/2508.07909", "authors": ["Bile Peng", "Karl-Ludwig Besser", "Shanpu Shen", "Finn Siegismund-Poschmann", "Ramprasad Raghunath", "Daniel M. Mittleman", "Vahid Jamali", "Eduard A. Jorswieck"], "title": "RIS-Assisted NOMA with Partial CSI and Mutual Coupling: A Machine Learning Approach", "comment": null, "summary": "Non-orthogonal multiple access (NOMA) is a promising multiple access\ntechnique. Its performance depends strongly on the wireless channel property,\nwhich can be enhanced by reconfigurable intelligent surfaces (RISs). In this\npaper, we jointly optimize base station (BS) precoding and RIS configuration\nwith unsupervised machine learning (ML), which looks for the optimal solution\nautonomously. In particular, we propose a dedicated neural network (NN)\narchitecture RISnet inspired by domain knowledge in communication. Compared to\nstate-of-the-art, the proposed approach combines analytical optimal BS\nprecoding and ML-enabled RIS, has a high scalability to control more than 1000\nRIS elements, has a low requirement for channel state information (CSI) in\ninput, and addresses the mutual coupling between RIS elements. Beyond the\nconsidered problem, this work is an early contribution to domain knowledge\nenabled ML, which exploit the domain expertise of communication systems to\ndesign better approaches than general ML methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u7ad9\u9884\u7f16\u7801\u548cRIS\u914d\u7f6e\uff0c\u4ee5\u63d0\u9ad8NOMA\u6027\u80fd\u3002", "motivation": "NOMA\u7684\u6027\u80fd\u53d7\u65e0\u7ebf\u4fe1\u9053\u7279\u6027\u5f71\u54cd\uff0c\u800cRIS\u53ef\u4ee5\u589e\u5f3a\u4fe1\u9053\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8054\u5408\u4f18\u5316BS\u9884\u7f16\u7801\u548cRIS\u914d\u7f6e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRISnet\u7684\u4e13\u7528\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u901a\u4fe1\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5bfb\u627e\u6700\u4f18\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u6269\u5c55\u6027\uff08\u53ef\u63a7\u5236\u8d85\u8fc71000\u4e2aRIS\u5143\u7d20\uff09\u3001\u4f4eCSI\u8f93\u5165\u9700\u6c42\uff0c\u5e76\u80fd\u89e3\u51b3RIS\u5143\u7d20\u95f4\u7684\u4e92\u8026\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u662f\u9886\u57df\u77e5\u8bc6\u9a71\u52a8ML\u7684\u65e9\u671f\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u7ed3\u5408\u901a\u4fe1\u4e13\u4e1a\u77e5\u8bc6\u8bbe\u8ba1\u4f18\u4e8e\u901a\u7528ML\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06743", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06743", "abs": "https://arxiv.org/abs/2508.06743", "authors": ["Connor Brown"], "title": "Analysis of Schedule-Free Nonconvex Optimization", "comment": null, "summary": "First-order methods underpin most large-scale learning algorithms, yet their\nclassical convergence guarantees hinge on carefully scheduled step-sizes that\ndepend on the total horizon $T$, which is rarely known in advance. The\nSchedule-Free (SF) method promises optimal performance with hyperparameters\nthat are independent of $T$ by interpolating between Polyak--Ruppert averaging\nand momentum, but nonconvex analysis of SF has been limited or reliant on\nstrong global assumptions. We introduce a robust Lyapunov framework that, under\nonly $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step\ndescent inequality. This yields horizon-agnostic bounds in the nonconvex\nsetting: $O(1/\\log T)$ for constant step + PR averaging, $O(\\log T/T)$ for a\nlinearly growing step-size, and a continuum of $O(T^{-(1-\\alpha)})$ rates for\npolynomial averaging. We complement these proofs with Performance Estimation\nProblem (PEP) experiments that numerically validate our rates and suggest that\nour $O(1/\\log T)$ bound on the original nonconvex SF algorithm may tighten to\n$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex\noptimization and charts future directions for optimal nonconvex rates.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684Lyapunov\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u975e\u51f8\u4f18\u5316\u4e2d\u7684Schedule-Free\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u65e0\u9700\u4f9d\u8d56\u603b\u6b65\u6570T\u7684\u6536\u655b\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4e00\u9636\u65b9\u6cd5\u9700\u8981\u4f9d\u8d56\u603b\u6b65\u6570T\u7684\u6b65\u957f\u8c03\u5ea6\u95ee\u9898\uff0c\u6269\u5c55Schedule-Free\u65b9\u6cd5\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165Lyapunov\u6846\u67b6\uff0c\u7ed3\u5408Polyak-Ruppert\u5e73\u5747\u548c\u52a8\u91cf\u65b9\u6cd5\uff0c\u5206\u6790\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u6536\u655b\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u975e\u51f8\u4f18\u5316\u4e2d\uff0cSchedule-Free\u65b9\u6cd5\u5177\u6709\u4e0eT\u65e0\u5173\u7684\u6536\u655b\u7387\uff0c\u5982O(1/log T)\u548cO(log T/T)\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86Schedule-Free\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u975e\u51f8\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6536\u655b\u6027\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.07967", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07967", "abs": "https://arxiv.org/abs/2508.07967", "authors": ["Haijia Jin", "Weijie Yuan", "Jun Wu", "Jiacheng Wang", "Dusit Niyato", "Xianbin Wang", "George K. Karagiannidis", "Zhiyun Lin", "Yi Gong", "Dong In Kim", "Athina Petropulu", "Maria Sabrina Greco", "Abbas Jamalipour", "Sumei Sun"], "title": "Advancing the Control of Low-Altitude Wireless Networks: Architecture, Design Principles, and Future Directions", "comment": null, "summary": "This article introduces a control-oriented low-altitude wireless network\n(LAWN) that integrates near-ground communications and remote estimation of the\ninternal system state. This integration supports reliable networked control in\ndynamic aerial-ground environments. First, we introduce the network's modular\narchitecture and key performance metrics. Then, we discuss core design\ntrade-offs across the control, communication, and estimation layers. A case\nstudy illustrates closed-loop coordination under wireless constraints. Finally,\nwe outline future directions for scalable, resilient LAWN deployments in\nreal-time and resource-constrained scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u9762\u5411\u63a7\u5236\u7684\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\uff08LAWN\uff09\uff0c\u96c6\u6210\u4e86\u8fd1\u5730\u901a\u4fe1\u548c\u5185\u90e8\u7cfb\u7edf\u72b6\u6001\u7684\u8fdc\u7a0b\u4f30\u8ba1\uff0c\u4ee5\u652f\u6301\u52a8\u6001\u7a7a-\u5730\u73af\u5883\u4e2d\u7684\u53ef\u9760\u7f51\u7edc\u63a7\u5236\u3002", "motivation": "\u5728\u52a8\u6001\u7a7a-\u5730\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u7f51\u7edc\u63a7\u5236\uff0c\u9700\u8981\u6574\u5408\u901a\u4fe1\u548c\u72b6\u6001\u4f30\u8ba1\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u67b6\u6784\u548c\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u8ba8\u8bba\u4e86\u63a7\u5236\u3001\u901a\u4fe1\u548c\u4f30\u8ba1\u5c42\u7684\u6838\u5fc3\u8bbe\u8ba1\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u65e0\u7ebf\u7ea6\u675f\u4e0b\u7684\u95ed\u73af\u534f\u8c03\u3002", "result": "\u5c55\u793a\u4e86LAWN\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5728\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5f39\u6027\u7684LAWN\u90e8\u7f72\u3002"}}
{"id": "2508.06765", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06765", "abs": "https://arxiv.org/abs/2508.06765", "authors": ["Xingke Yang", "Liang Li", "Sicong Li", "Liwei Guan", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Xin Fu", "Miao Pan"], "title": "Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning", "comment": null, "summary": "Collaboratively fine-tuning (FT) large language models (LLMs) over\nheterogeneous mobile devices fosters immense potential applications of\npersonalized intelligence. However, such a vision faces critical system\nchallenges. Conventional federated LLM FT approaches place prohibitive\ncomputational and memory burdens on mobile hardware, and their synchronous\nmodel aggregation protocols stall for slower devices. In this paper, we propose\nFed MobiLLM, a novel design to facilitate efficient federated LLM FT across\nmobile devices with diverse computing/communication speeds and local model\narchitectures. In particular, Fed MobiLLM implements a pioneering\nserver-assisted federated side-tuning paradigm. Briefly, mobile devices perform\nlightweight forward propagation computations on local data using their frozen\npre-scaled backbone LLMs, and then upload selected intermediate activations.\nThe server trains a shared side-network independently, eliminating client-side\nbackpropagation and enabling asynchronous updates. To bridge model\nheterogeneity across different devices, we introduce an adaptive layer-wise\nfeature alignment method, which ensures consistent representations for\ncollaboratively tuning a shared side network. Extensive experimental results\ndemonstrate that Fed MobiLLM can maintain robust fine-tuning performance while\nachieving extremely low on-device memory, with at least 95.2% reduction in\ncomputation overhead, 93.2% reduction in communication costs and 5.1x faster\nconvergence compared to existing methods, validating its efficacy for practical\nLLM adaptation over heterogeneous mobile devices.", "AI": {"tldr": "Fed MobiLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5f02\u6784\u79fb\u52a8\u8bbe\u5907\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\u8fc7\u91cd\uff0c\u4e14\u540c\u6b65\u534f\u8bae\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002Fed MobiLLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u670d\u52a1\u5668\u8f85\u52a9\u7684\u8054\u90a6\u4fa7\u8c03\u4f18\u8303\u5f0f\uff0c\u8bbe\u5907\u4ec5\u6267\u884c\u8f7b\u91cf\u7ea7\u524d\u5411\u4f20\u64ad\uff0c\u670d\u52a1\u5668\u72ec\u7acb\u8bad\u7ec3\u5171\u4eab\u4fa7\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u5c42\u7279\u5f81\u5bf9\u9f50\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFed MobiLLM\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\uff0895.2%\uff09\u3001\u901a\u4fe1\uff0893.2%\uff09\u5f00\u9500\uff0c\u5e76\u52a0\u901f\u6536\u655b\uff085.1\u500d\uff09\u3002", "conclusion": "Fed MobiLLM\u4e3a\u5f02\u6784\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08097", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08097", "abs": "https://arxiv.org/abs/2508.08097", "authors": ["Muhammad Asif", "Zain Ali", "Asim Ihsan", "Ali Ranjha", "Zhu Shoujin", "Manzoor Ahmed", "Xingwang Li", "Symeon Chatzinotas"], "title": "Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors", "comment": "13 pages, and 11 figures. Submitted to IEEE", "summary": "This work explores the integration of rate-splitting multiple access (RSMA),\nsimultaneous wireless information and power transfer (SWIPT), and\nbeyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the\nspectral-efficiency, energy-efficiency, coverage, and connectivity of future\nsixth-generation (6G) communication networks. Specifically, with a multiuser\nBD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding\nvectors, the common rate proportion of users, the power-splitting ratios, and\nscattering matrix of BD-RIS node, under the assumption of imperfect channel\nstate information (CSI). Additionally, to better capture practical hardware\nbehavior, we incorporate a nonlinear energy harvesting model under energy\nharvesting constraints. We design a robust optimization framework to maximize\nthe system sum-rate, while explicitly accounting for the worst-case impact of\nCSI uncertainties. Further, we introduce an alternating optimization framework\nthat partitions the problem into several blocks, which are optimized\niteratively. More specifically, the transmit precoding vectors are optimized by\nreformulating the problem as a convex semidefinite programming through\nsuccessive-convex approximation (SCA), whereas the power-splitting problem is\nsolved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the\nscattering matrix of the BD-RIS, we first employ SCA to reformulate the problem\ninto a convex form, and then design a manifold optimization strategy based on\nthe Conjugate-Gradient method. Finally, numerical simulation results reveal\nthat the proposed scheme provides significant performance improvements over\nexisting benchmarks and demonstrates rapid convergence within a reasonable\nnumber of iterations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RSMA\u3001SWIPT\u548cBD-RIS\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53476G\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u89e3\u51b3\u591a\u53d8\u91cf\u95ee\u9898\u3002", "motivation": "\u63d0\u53476G\u7f51\u7edc\u7684\u9891\u8c31\u6548\u7387\u3001\u80fd\u91cf\u6548\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u8fde\u63a5\u6027\uff0c\u540c\u65f6\u8003\u8651\u5b9e\u9645\u786c\u4ef6\u9650\u5236\u548c\u4fe1\u9053\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u5757\uff0c\u5206\u522b\u4f18\u5316\u9884\u7f16\u7801\u5411\u91cf\u3001\u529f\u7387\u5206\u914d\u6bd4\u548cBD-RIS\u6563\u5c04\u77e9\u9635\uff0c\u5e76\u7ed3\u5408SCA\u548c\u6d41\u5f62\u4f18\u5316\u7b56\u7565\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6848\u5728\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a6G\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06767", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06767", "abs": "https://arxiv.org/abs/2508.06767", "authors": ["Arman Dogru", "R. Irem Bor-Yaliniz", "Nimal Gamini Senarath"], "title": "PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems", "comment": null, "summary": "Digital Twins (DTs) are transforming industries through advanced data\nprocessing and analysis, positioning the world of DTs, Digital World, as a\ncornerstone of nextgeneration technologies including embodied AI. As robotics\nand automated systems scale, efficient data-sharing frameworks and robust\nalgorithms become critical. We explore the pivotal role of data handling in\nnext-gen networks, focusing on dynamics between application and network\nproviders (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with\nPriority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)\nbased multi-agent path finding (MAPF). By adopting a Centralized Training with\nDecentralized Execution (CTDE) framework and asynchronous actor-learner\narchitectures, PANAMA accelerates training while enabling autonomous task\nexecution by embodied AI. Our approach demonstrates superior pathfinding\nperformance in accuracy, speed, and scalability compared to existing\nbenchmarks. Through simulations, we highlight optimized data-sharing strategies\nfor scalable, automated systems, ensuring resilience in complex, real-world\nenvironments. PANAMA bridges the gap between network-aware decision-making and\nrobust multi-agent coordination, advancing the synergy between DTs, wireless\nnetworks, and AI-driven automation.", "AI": {"tldr": "PANAMA\u7b97\u6cd5\u901a\u8fc7\u4f18\u5148\u4e0d\u5bf9\u79f0\u6027\u548c\u7f51\u7edc\u611f\u77e5\u7684MARL\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u6027\u80fd\uff0c\u4f18\u5316\u4e86\u6570\u636e\u5171\u4eab\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5b6a\u751f\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u9ad8\u6548\u7684\u6570\u636e\u5171\u4eab\u548c\u7b97\u6cd5\u6210\u4e3a\u5173\u952e\uff0c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7f51\u7edc\u4e0e\u5e94\u7528\u63d0\u4f9b\u5546\u5728\u6570\u5b57\u5b6a\u751f\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u4ea4\u4e92\u95ee\u9898\u3002", "method": "\u63d0\u51faPANAMA\u7b97\u6cd5\uff0c\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\uff08CTDE\uff09\u6846\u67b6\u548c\u5f02\u6b65\u6267\u884c\u8005-\u5b66\u4e60\u8005\u67b6\u6784\uff0c\u7ed3\u5408\u7f51\u7edc\u611f\u77e5\u7684MARL\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u3002", "result": "PANAMA\u5728\u8def\u5f84\u89c4\u5212\u7684\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u4f18\u5316\u7684\u6570\u636e\u5171\u4eab\u7b56\u7565\u3002", "conclusion": "PANAMA\u5728\u7f51\u7edc\u611f\u77e5\u51b3\u7b56\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u8c03\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u63a8\u52a8\u4e86\u6570\u5b57\u5b6a\u751f\u3001\u65e0\u7ebf\u7f51\u7edc\u548cAI\u81ea\u52a8\u5316\u4e4b\u95f4\u7684\u534f\u540c\u53d1\u5c55\u3002"}}
{"id": "2508.08206", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08206", "abs": "https://arxiv.org/abs/2508.08206", "authors": ["Amirhossein Taherpour", "Abbas Taherpour", "Tamer Khattab"], "title": "Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers", "comment": null, "summary": "We propose a joint learning framework for Byzantine-resilient spectrum\nsensing and secure intelligent reflecting surface (IRS)--assisted opportunistic\naccess under channel state information (CSI) uncertainty. The sensing stage\nperforms logit-domain Bayesian updates with trimmed aggregation and\nattention-weighted consensus, and the base station (BS) fuses network beliefs\nwith a conservative minimum rule, preserving detection accuracy under a bounded\nnumber of Byzantine users. Conditioned on the sensing outcome, we pose downlink\ndesign as sum mean-squared error (MSE) minimization under transmit-power and\nsignal-leakage constraints and jointly optimize the BS precoder, IRS phase\nshifts, and user equalizers. With partial (or known) CSI, we develop an\naugmented-Lagrangian alternating algorithm with projected updates and provide\nprovable sublinear convergence, with accelerated rates under mild local\ncurvature. With unknown CSI, we perform constrained Bayesian optimization (BO)\nin a geometry-aware low-dimensional latent space using Gaussian process (GP)\nsurrogates; we prove regret bounds for a constrained upper confidence bound\n(UCB) variant of the BO module, and demonstrate strong empirical performance of\nthe implemented procedure. Simulations across diverse network conditions show\nhigher detection probability at fixed false-alarm rate under adversarial\nattacks, large reductions in sum MSE for honest users, strong suppression of\neavesdropper signal power, and fast convergence. The framework offers a\npractical path to secure opportunistic communication that adapts to CSI\navailability while coherently coordinating sensing and transmission through\njoint learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u62dc\u5360\u5ead\u5f39\u6027\u7684\u9891\u8c31\u611f\u77e5\u548c\u5b89\u5168\u667a\u80fd\u53cd\u5c04\u9762\uff08IRS\uff09\u8f85\u52a9\u7684\u673a\u4f1a\u63a5\u5165\u3002", "motivation": "\u89e3\u51b3\u5728\u5b58\u5728\u62dc\u5360\u5ead\u7528\u6237\u548cCSI\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u7684\u9891\u8c31\u611f\u77e5\u548c\u5b89\u5168\u901a\u4fe1\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5bf9\u6570\u57df\u8d1d\u53f6\u65af\u66f4\u65b0\u4e0e\u4fee\u526a\u805a\u5408\u548c\u6ce8\u610f\u529b\u52a0\u6743\u5171\u8bc6\u7684\u611f\u77e5\u9636\u6bb5\uff0c\u57fa\u7ad9\u901a\u8fc7\u4fdd\u5b88\u6700\u5c0f\u89c4\u5219\u878d\u5408\u7f51\u7edc\u4fe1\u5ff5\u3002\u4e0b\u884c\u94fe\u8def\u8bbe\u8ba1\u57fa\u4e8e\u548c\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u6700\u5c0f\u5316\uff0c\u4f18\u5316\u57fa\u7ad9\u9884\u7f16\u7801\u5668\u3001IRS\u76f8\u4f4d\u504f\u79fb\u548c\u7528\u6237\u5747\u8861\u5668\u3002\u9488\u5bf9\u90e8\u5206\u6216\u672a\u77e5CSI\uff0c\u5206\u522b\u5f00\u53d1\u4e86\u589e\u5f3a\u62c9\u683c\u6717\u65e5\u4ea4\u66ff\u7b97\u6cd5\u548c\u7ea6\u675f\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u7f51\u7edc\u6761\u4ef6\u4e0b\uff0c\u4eff\u771f\u663e\u793a\u5728\u56fa\u5b9a\u865a\u8b66\u7387\u4e0b\u66f4\u9ad8\u7684\u68c0\u6d4b\u6982\u7387\uff0c\u8bda\u5b9e\u7528\u6237\u7684\u548cMSE\u663e\u8457\u964d\u4f4e\uff0c\u7a83\u542c\u8005\u4fe1\u53f7\u529f\u7387\u88ab\u6709\u6548\u6291\u5236\uff0c\u4e14\u7b97\u6cd5\u5feb\u901f\u6536\u655b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b89\u5168\u673a\u4f1a\u901a\u4fe1\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u9002\u5e94CSI\u53ef\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u534f\u8c03\u611f\u77e5\u548c\u4f20\u8f93\u3002"}}
{"id": "2508.07037", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07037", "abs": "https://arxiv.org/abs/2508.07037", "authors": ["Yangguang He", "Wenhao Li", "Minzhe Li", "Juan Zhang", "Xiangfeng Wang", "Bo Jin"], "title": "Differentiable Adaptive Kalman Filtering via Optimal Transport", "comment": "20 pages", "summary": "Learning-based filtering has demonstrated strong performance in non-linear\ndynamical systems, particularly when the statistics of noise are unknown.\nHowever, in real-world deployments, environmental factors, such as changing\nwind conditions or electromagnetic interference, can induce unobserved\nnoise-statistics drift, leading to substantial degradation of learning-based\nmethods. To address this challenge, we propose OTAKNet, the first online\nsolution to noise-statistics drift within learning-based adaptive Kalman\nfiltering. Unlike existing learning-based methods that perform offline\nfine-tuning using batch pointwise matching over entire trajectories, OTAKNet\nestablishes a connection between the state estimate and the drift via one-step\npredictive measurement likelihood, and addresses it using optimal transport.\nThis leverages OT's geometry - aware cost and stable gradients to enable fully\nonline adaptation without ground truth labels or retraining. We compare OTAKNet\nagainst classical model-based adaptive Kalman filtering and offline\nlearning-based filtering. The performance is demonstrated on both synthetic and\nreal-world NCLT datasets, particularly under limited training data.", "AI": {"tldr": "OTAKNet\u662f\u4e00\u79cd\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u5b66\u4e60\u578b\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u4e2d\u7684\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5b9e\u73b0\u65e0\u9700\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u5728\u7ebf\u9002\u5e94\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\uff0c\u73af\u5883\u56e0\u7d20\uff08\u5982\u98ce\u51b5\u53d8\u5316\u6216\u7535\u78c1\u5e72\u6270\uff09\u4f1a\u5bfc\u81f4\u672a\u89c2\u6d4b\u5230\u7684\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\uff0c\u663e\u8457\u964d\u4f4e\u5b66\u4e60\u578b\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "method": "OTAKNet\u901a\u8fc7\u4e00\u6b65\u9884\u6d4b\u6d4b\u91cf\u4f3c\u7136\u5efa\u7acb\u72b6\u6001\u4f30\u8ba1\u4e0e\u6f02\u79fb\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7684\u51e0\u4f55\u611f\u77e5\u6210\u672c\u548c\u7a33\u5b9a\u68af\u5ea6\u5b9e\u73b0\u5728\u7ebf\u9002\u5e94\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9eNCLT\u6570\u636e\u96c6\u4e0a\uff0cOTAKNet\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u79bb\u7ebf\u5b66\u4e60\u578b\u6ee4\u6ce2\u3002", "conclusion": "OTAKNet\u4e3a\u89e3\u51b3\u566a\u58f0\u7edf\u8ba1\u6f02\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u5728\u7ebf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06783", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06783", "abs": "https://arxiv.org/abs/2508.06783", "authors": ["Noel Teku", "Fengwei Tian", "Payel Bhattacharjee", "Souradip Chakraborty", "Amrit Singh Bedi", "Ravi Tandon"], "title": "PROPS: Progressively Private Self-alignment of Large Language Models", "comment": null, "summary": "Alignment is a key step in developing Large Language Models (LLMs) using\nhuman feedback to ensure adherence to human values and societal norms.\nDependence on human feedback raises privacy concerns about how much a labeler's\npreferences may reveal about their personal values, beliefs, and personality\ntraits. Existing approaches, such as Differentially Private SGD (DP-SGD),\nprovide rigorous privacy guarantees by privatizing gradients during fine-tuning\nand alignment but can provide more privacy than necessary as human preferences\nare tied only to labels of (prompt, response) pairs and can degrade model\nutility. This work focuses on LLM alignment with preference-level privacy,\nwhich preserves the privacy of preference labels provided by humans. We propose\nPROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving\nalignment framework where privately aligned models in previous stages can serve\nas labelers for supplementing training data in the subsequent stages of\nalignment. We present theoretical guarantees for PROPS as well as comprehensive\nvalidation using multiple models (Pythia and GPT) and datasets (AlpacaEval,\nAnthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over\nexisting methods while still providing high privacy. For the same privacy\nbudget, alignment via PROPS can achieve up to 3x higher win-rates compared to\nDP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based\nalignment.", "AI": {"tldr": "PROPS\u6846\u67b6\u901a\u8fc7\u591a\u9636\u6bb5\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50LLM\uff0c\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\u6bd4DP-SGD\u548cRR\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u4f9d\u8d56\u4eba\u7c7b\u53cd\u9988\u7684\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u9690\u79c1\u5316\u5bf9\u6a21\u578b\u6548\u7528\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faPROPS\u6846\u67b6\uff0c\u5229\u7528\u524d\u4e00\u9636\u6bb5\u79c1\u6709\u5bf9\u9f50\u6a21\u578b\u4e3a\u540e\u7eed\u9636\u6bb5\u63d0\u4f9b\u6807\u7b7e\u6570\u636e\u3002", "result": "PROPS\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\uff0c\u80dc\u7387\u6bd4DP-SGD\u9ad83\u500d\uff0c\u6bd4RR\u9ad82.5\u500d\u3002", "conclusion": "PROPS\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2508.07253", "categories": ["cs.LG", "eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.07253", "abs": "https://arxiv.org/abs/2508.07253", "authors": ["Bartlomiej Chybowski", "Shima Abdullateef", "Hollan Haule", "Alfredo Gonzalez-Sulser", "Javier Escudero"], "title": "PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets", "comment": null, "summary": "Reliable seizure detection is critical for diagnosing and managing epilepsy,\nyet clinical workflows remain dependent on time-consuming manual EEG\ninterpretation. While machine learning has shown promise, existing approaches\noften rely on dataset-specific optimisations, limiting their real-world\napplicability and reproducibility. Here, we introduce an innovative,\nopen-source machine-learning framework that enables robust and generalisable\nseizure detection across varied clinical datasets. We evaluate our approach on\ntwo publicly available EEG datasets that differ in patient populations and\nelectrode configurations. To enhance robustness, the framework incorporates an\nautomated pre-processing pipeline to standardise data and a majority voting\nmechanism, in which multiple models independently assess each second of EEG\nbefore reaching a final decision. We train, tune, and evaluate models within\neach dataset, assessing their cross-dataset transferability. Our models achieve\nhigh within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and\n0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets\ndespite differences in EEG setups and populations (AUC 0.615+/-0.039 for models\ntrained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)\nwithout any post-processing. Furthermore, a mild post-processing improved the\nwithin-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset\nresults to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the\npotential of, and essential considerations for, deploying our framework in\ndiverse clinical settings. By making our methodology fully reproducible, we\nprovide a foundation for advancing clinically viable, dataset-agnostic seizure\ndetection systems. This approach has the potential for widespread adoption,\ncomplementing rather than replacing expert interpretation, and accelerating\nclinical integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u4e0d\u540c\u4e34\u5e8a\u6570\u636e\u96c6\u7684\u53ef\u9760\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\uff0c\u5177\u6709\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u96c6\u4f18\u5316\u3001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u4e34\u5e8a\u5e94\u7528\u7684\u666e\u53ca\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u81ea\u52a8\u9884\u5904\u7406\u7ba1\u9053\u548c\u591a\u6570\u6295\u7968\u673a\u5236\u7684\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u6570\u636e\u96c6\u5185\u548c\u8de8\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u6570\u636e\u96c6\u5185\u8868\u73b0\u4f18\u5f02\uff08AUC 0.904\u548c0.864\uff09\uff0c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u663e\u8457\uff08AUC 0.615\u548c0.762\uff09\uff0c\u540e\u5904\u7406\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u53ef\u884c\u7684\u3001\u6570\u636e\u96c6\u65e0\u5173\u7684\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06784", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06784", "abs": "https://arxiv.org/abs/2508.06784", "authors": ["Junjing Zheng", "Chengliang Song", "Weidong Jiang", "Xinyu Zhang"], "title": "Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning", "comment": null, "summary": "High-dimensional data, particularly in the form of high-order tensors,\npresents a major challenge in self-supervised learning. While MLP-based\nautoencoders (AE) are commonly employed, their dependence on flattening\noperations exacerbates the curse of dimensionality, leading to excessively\nlarge model sizes, high computational overhead, and challenging optimization\nfor deep structural feature capture. Although existing tensor networks\nalleviate computational burdens through tensor decomposition techniques, most\nexhibit limited capability in learning non-linear relationships. To overcome\nthese limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder\n(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear\nframework and employs a Pick-and-Unfold strategy, facilitating flexible\nper-mode encoding of high-order tensors via recursive unfold-encode-fold\noperations, effectively integrating tensor structural priors. Notably, MA-NTAE\nexhibits linear growth in computational complexity with tensor order and\nproportional growth with mode dimensions. Extensive experiments demonstrate\nMA-NTAE's performance advantages over standard AE and current tensor networks\nin compression and clustering tasks, which become increasingly pronounced for\nhigher-order, higher-dimensional tensors.", "AI": {"tldr": "MA-NTAE\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u975e\u7ebf\u6027Tucker\u5206\u89e3\u548cPick-and-Unfold\u7b56\u7565\uff0c\u9ad8\u6548\u5904\u7406\u9ad8\u7ef4\u5f20\u91cf\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u548c\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u9ad8\u7ef4\u5f20\u91cf\u6570\u636e\u5728\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982MLP\u81ea\u7f16\u7801\u5668\u4f9d\u8d56\u5c55\u5e73\u64cd\u4f5c\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5927\u4e14\u96be\u4ee5\u4f18\u5316\u3002", "method": "\u63d0\u51faMA-NTAE\uff0c\u5c06\u7ecf\u5178Tucker\u5206\u89e3\u63a8\u5e7f\u5230\u975e\u7ebf\u6027\u6846\u67b6\uff0c\u91c7\u7528Pick-and-Unfold\u7b56\u7565\uff0c\u901a\u8fc7\u9012\u5f52\u5c55\u5f00-\u7f16\u7801-\u6298\u53e0\u64cd\u4f5c\u7075\u6d3b\u5904\u7406\u9ad8\u7ef4\u5f20\u91cf\u3002", "result": "MA-NTAE\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u5f20\u91cf\u9636\u6570\u7ebf\u6027\u589e\u957f\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u538b\u7f29\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6\u81ea\u7f16\u7801\u5668\u548c\u73b0\u6709\u5f20\u91cf\u7f51\u7edc\u3002", "conclusion": "MA-NTAE\u4e3a\u9ad8\u7ef4\u5f20\u91cf\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u9636\u9ad8\u7ef4\u573a\u666f\u3002"}}
{"id": "2508.08034", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08034", "abs": "https://arxiv.org/abs/2508.08034", "authors": ["Roksana Yahyaabadi", "Ghazal Farhani", "Taufiq Rahman", "Soodeh Nikan", "Abdullah Jirjees", "Fadi Araji"], "title": "Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles", "comment": null, "summary": "Accurate power consumption prediction is crucial for improving efficiency and\nreducing environmental impact, yet traditional methods relying on specialized\ninstruments or rigid physical models are impractical for large-scale,\nreal-world deployment. This study introduces a scalable data-driven method\nusing powertrain dynamic feature sets and both traditional machine learning and\ndeep neural networks to estimate instantaneous and cumulative power consumption\nin internal combustion engine (ICE), electric vehicle (EV), and hybrid electric\nvehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with\nmean absolute error and root mean squared error on the order of $10^{-3}$, and\ncumulative errors under 3%. Transformer and long short-term memory models\nperformed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,\nrespectively. Results confirm the approach's effectiveness across vehicles and\nmodels. Uncertainty analysis revealed greater variability in EV and HEV\ndatasets than ICE, due to complex power management, emphasizing the need for\nrobust models for advanced powertrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u5185\u71c3\u673a\u3001\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u7684\u77ac\u65f6\u548c\u7d2f\u8ba1\u529f\u8017\uff0c\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u529f\u8017\u9884\u6d4b\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5b9e\u7528\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u52a8\u529b\u7cfb\u7edf\u52a8\u6001\u7279\u5f81\u96c6\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5982Transformer\u548cLSTM\uff09\uff0c\u9884\u6d4b\u591a\u79cd\u8f66\u8f86\u7684\u529f\u8017\u3002", "result": "\u5185\u71c3\u673a\u77ac\u65f6\u8bef\u5dee\u4f4e\u81f3$10^{-3}$\uff0c\u7d2f\u8ba1\u8bef\u5dee\u4f4e\u4e8e3%\uff1b\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u8f66\u7684\u7d2f\u8ba1\u8bef\u5dee\u5206\u522b\u4f4e\u4e8e4.1%\u548c2.1%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u8f66\u8f86\u7c7b\u578b\uff0c\u4f46\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u8f66\u56e0\u590d\u6742\u7684\u7535\u6e90\u7ba1\u7406\u9700\u66f4\u7a33\u5065\u7684\u6a21\u578b\u3002"}}
{"id": "2508.06800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06800", "abs": "https://arxiv.org/abs/2508.06800", "authors": ["Rui Liu", "Haolin Zuo", "Zheng Lian", "Hongyu Yuan", "Qi Fan"], "title": "Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities", "comment": null, "summary": "Missing modalities have recently emerged as a critical research direction in\nmultimodal emotion recognition (MER). Conventional approaches typically address\nthis issue through missing modality reconstruction. However, these methods fail\nto account for variations in reconstruction difficulty across different\nsamples, consequently limiting the model's ability to handle hard samples\neffectively. To overcome this limitation, we propose a novel Hardness-Aware\nDynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates\nin two key stages: first, it estimates the hardness level of each sample, and\nsecond, it strategically emphasizes hard samples during training to enhance\nmodel performance on these challenging instances. Specifically, we first\nintroduce a Multi-view Hardness Evaluation mechanism that quantifies\nreconstruction difficulty by considering both Direct Hardness (modality\nreconstruction errors) and Indirect Hardness (cross-modal mutual information).\nMeanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy\nthat dynamically adjusts the training curriculum by retrieving samples with\nsimilar semantic information and balancing the learning focus between easy and\nhard instances. Extensive experiments on benchmark datasets demonstrate that\nHARDY-MER consistently outperforms existing methods in missing-modality\nscenarios. Our code will be made publicly available at\nhttps://github.com/HARDY-MER/HARDY-MER.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHARDY-MER\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u6837\u672c\u7684\u96be\u5ea6\u5e76\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u91cd\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7f3a\u5931\u6a21\u6001\u7684\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u7f3a\u5931\u6a21\u6001\u65f6\uff0c\u672a\u80fd\u8003\u8651\u4e0d\u540c\u6837\u672c\u7684\u91cd\u5efa\u96be\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "HARDY-MER\u6846\u67b6\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u901a\u8fc7\u591a\u89c6\u89d2\u96be\u5ea6\u8bc4\u4f30\u673a\u5236\u91cf\u5316\u6837\u672c\u91cd\u5efa\u96be\u5ea6\uff1b2) \u91c7\u7528\u57fa\u4e8e\u68c0\u7d22\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u91cd\u70b9\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHARDY-MER\u5728\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HARDY-MER\u901a\u8fc7\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u56f0\u96be\u6837\u672c\u7684\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.08216", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08216", "abs": "https://arxiv.org/abs/2508.08216", "authors": ["Nicole Lai-Tan", "Xiao Gu", "Marios G. Philiastides", "Fani Deligianni"], "title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion", "comment": "12 pages, 5 figures", "summary": "Personalised music-based interventions offer a powerful means of supporting\nmotor rehabilitation by dynamically tailoring auditory stimuli to provide\nexternal timekeeping cues, modulate affective states, and stabilise gait\npatterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for\nadapting these interventions across individuals. However, inter-subject\nvariability in EEG signals, further compounded by movement-induced artefacts\nand motor planning differences, hinders the generalisability of BCIs and\nresults in lengthy calibration processes. We propose Individual Tangent Space\nAlignment (ITSA), a novel pre-alignment strategy incorporating subject-specific\nrecentering, distribution matching, and supervised rotational alignment to\nenhance cross-subject generalisation. Our hybrid architecture fuses Regularised\nCommon Spatial Patterns (RCSP) with Riemannian geometry in parallel and\nsequential configurations, improving class separability while maintaining the\ngeometric structure of covariance matrices for robust statistical computation.\nUsing leave-one-subject-out cross-validation, `ITSA' demonstrates significant\nperformance improvements across subjects and conditions. The parallel fusion\napproach shows the greatest enhancement over its sequential counterpart, with\nrobust performance maintained across varying data conditions and electrode\nconfigurations. The code will be made publicly available at the time of\npublication.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aITSA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2a\u4f53\u7279\u5b9a\u7684\u91cd\u65b0\u4e2d\u5fc3\u5316\u3001\u5206\u5e03\u5339\u914d\u548c\u76d1\u7763\u65cb\u8f6c\u5bf9\u9f50\uff0c\u63d0\u5347\u8111\u673a\u63a5\u53e3\u7684\u8de8\u4e3b\u4f53\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u8111\u673a\u63a5\u53e3\u5728\u4e2a\u6027\u5316\u97f3\u4e50\u5e72\u9884\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e3b\u4f53\u95f4EEG\u4fe1\u53f7\u7684\u53d8\u5f02\u6027\u3001\u8fd0\u52a8\u4f2a\u5f71\u548c\u8fd0\u52a8\u89c4\u5212\u5dee\u5f02\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faITSA\u9884\u5bf9\u9f50\u7b56\u7565\uff0c\u7ed3\u5408RCSP\u548c\u9ece\u66fc\u51e0\u4f55\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5e76\u884c\u548c\u987a\u5e8f\u914d\u7f6e\u4ee5\u589e\u5f3a\u5206\u7c7b\u5206\u79bb\u6027\u3002", "result": "ITSA\u5728\u8de8\u4e3b\u4f53\u548c\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u884c\u878d\u5408\u65b9\u6cd5\u4f18\u4e8e\u987a\u5e8f\u65b9\u6cd5\u3002", "conclusion": "ITSA\u4e3a\u8111\u673a\u63a5\u53e3\u7684\u8de8\u4e3b\u4f53\u6cdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.06806", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06806", "abs": "https://arxiv.org/abs/2508.06806", "authors": ["Xiao Huang", "Xu Liu", "Enze Zhang", "Tong Yu", "Shuai Li"], "title": "Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation", "comment": "ICML2025", "summary": "Offline-to-online Reinforcement Learning (O2O RL) aims to perform online\nfine-tuning on an offline pre-trained policy to minimize costly online\ninteractions. Existing work used offline datasets to generate data that conform\nto the online data distribution for data augmentation. However, generated data\nstill exhibits a gap with the online data, limiting overall performance. To\naddress this, we propose a new data augmentation approach, Classifier-Free\nDiffusion Generation (CFDG). Without introducing additional classifier training\noverhead, CFDG leverages classifier-free guidance diffusion to significantly\nenhance the generation quality of offline and online data with different\ndistributions. Additionally, it employs a reweighting method to enable more\ngenerated data to align with the online data, enhancing performance while\nmaintaining the agent's stability. Experimental results show that CFDG\noutperforms replaying the two data types or using a standard diffusion model to\ngenerate new data. Our method is versatile and can be integrated with existing\noffline-to-online RL algorithms. By implementing CFDG to popular methods IQL,\nPEX and APL, we achieve a notable 15% average improvement in empirical\nperformance on the D4RL benchmark such as MuJoCo and AntMaze.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5CFDG\uff0c\u7528\u4e8e\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u65e0\u5206\u7c7b\u5668\u6269\u6563\u751f\u6210\u63d0\u5347\u6570\u636e\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u4e0e\u5728\u7ebf\u6570\u636e\u5b58\u5728\u5206\u5e03\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u91c7\u7528\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u6269\u6563\u751f\u6210\uff08CFDG\uff09\u548c\u91cd\u52a0\u6743\u65b9\u6cd5\uff0c\u63d0\u5347\u751f\u6210\u6570\u636e\u4e0e\u5728\u7ebf\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCFDG\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u534715%\u7684\u6027\u80fd\u3002", "conclusion": "CFDG\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u63d0\u5347\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06813", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06813", "abs": "https://arxiv.org/abs/2508.06813", "authors": ["Brendan R. Hogan", "Will Brown", "Adel Boyarsky", "Anderson Schneider", "Yuriy Nevmyvaka"], "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language", "comment": "40 pages", "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f00\u6e90\u65b9\u6cd5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9002\u914d\u5230Q\u7f16\u7a0b\u8bed\u8a00\uff0c\u89e3\u51b3\u4e86\u5176\u5728\u5c0f\u4f17\u8bed\u8a00\u548c\u79c1\u6709\u9886\u57df\u5e94\u7528\u7684\u6311\u6218\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\u6a21\u578b\uff0c\u5e76\u5728Q\u8bc4\u6d4b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4e3b\u6d41\u524d\u6cbf\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u5c0f\u4f17\u7f16\u7a0b\u8bed\u8a00\uff08\u5982Q\u8bed\u8a00\uff09\u548c\u79c1\u6709\u9886\u57df\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\uff0c\u586b\u8865\u5176\u5728\u975e\u4e3b\u6d41\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u6784\u5efaQ\u8bed\u8a00\u7684Leetcode\u98ce\u683c\u8bc4\u6d4b\u6570\u636e\u96c6\uff0c\u57fa\u4e8eQwen-2.5\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bad\u7ec3\u4e86\u4e94\u79cd\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\uff081.5B\u81f332B\uff09\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728Q\u8bc4\u6d4b\u6570\u636e\u96c6\u4e0a\u7684pass@1\u51c6\u786e\u7387\u8fbe\u523059%\uff0c\u8d85\u8d8aClaude Opus-4\uff0829.5%\uff09\uff0c\u6240\u6709\u6a21\u578b\u5747\u4f18\u4e8eGPT-4.1\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8eQ\u8bed\u8a00\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u4f9d\u8d56\u8f6f\u6027\u6216\u4e3b\u89c2\u4fe1\u53f7\u7684\u4efb\u52a1\uff0c\u4e3a\u5c0f\u4f17\u9886\u57dfLLMs\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.06827", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06827", "abs": "https://arxiv.org/abs/2508.06827", "authors": ["Ishwar Balappanawar", "Venkata Hasith Vattikuti", "Greta Kintzley", "Ronan Azimi-Mancel", "Satvik Golechha"], "title": "Who's the Evil Twin? Differential Auditing for Undesired Behavior", "comment": "main section: 8 pages, 4 figures, 1 table total: 34 pages, 44\n  figures, 12 tables", "summary": "Detecting hidden behaviors in neural networks poses a significant challenge\ndue to minimal prior knowledge and potential adversarial obfuscation. We\nexplore this problem by framing detection as an adversarial game between two\nteams: the red team trains two similar models, one trained solely on benign\ndata and the other trained on data containing hidden harmful behavior, with the\nperformance of both being nearly indistinguishable on the benign dataset. The\nblue team, with limited to no information about the harmful behaviour, tries to\nidentify the compromised model. We experiment using CNNs and try various blue\nteam strategies, including Gaussian noise analysis, model diffing, integrated\ngradients, and adversarial attacks under different levels of hints provided by\nthe red team. Results show high accuracy for adversarial-attack-based methods\n(100\\% correct prediction, using hints), which is very promising, whilst the\nother techniques yield more varied performance. During our LLM-focused rounds,\nwe find that there are not many parallel methods that we could apply from our\nstudy with CNNs. Instead, we find that effective LLM auditing methods require\nsome hints about the undesired distribution, which can then used in standard\nblack-box and open-weight methods to probe the models further and reveal their\nmisalignment. We open-source our auditing games (with the model and data) and\nhope that our findings contribute to designing better audits.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u5bf9\u6297\u6e38\u620f\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u85cf\u884c\u4e3a\uff0c\u7ea2\u961f\u8bad\u7ec3\u4e24\u79cd\u6a21\u578b\uff0c\u84dd\u961f\u5c1d\u8bd5\u8bc6\u522b\u88ab\u7be1\u6539\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u8868\u660e\u57fa\u4e8e\u5bf9\u6297\u653b\u51fb\u7684\u65b9\u6cd5\u6548\u679c\u6700\u4f73\uff0c\u800cLLM\u5ba1\u8ba1\u9700\u8981\u989d\u5916\u63d0\u793a\u3002", "motivation": "\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u85cf\u884c\u4e3a\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\u6216\u5b58\u5728\u5bf9\u6297\u6027\u6df7\u6dc6\u65f6\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u6e38\u620f\u6846\u67b6\uff0c\u7ea2\u961f\u8bad\u7ec3\u4e24\u79cd\u6a21\u578b\uff08\u826f\u6027\u6570\u636e\u548c\u542b\u9690\u85cf\u6709\u5bb3\u884c\u4e3a\u7684\u6570\u636e\uff09\uff0c\u84dd\u961f\u5c1d\u8bd5\u8bc6\u522b\u88ab\u7be1\u6539\u7684\u6a21\u578b\u3002\u5b9e\u9a8c\u4f7f\u7528\u4e86CNN\u548c\u591a\u79cd\u84dd\u961f\u7b56\u7565\u3002", "result": "\u57fa\u4e8e\u5bf9\u6297\u653b\u51fb\u7684\u65b9\u6cd5\u51c6\u786e\u7387\u6700\u9ad8\uff08100%\uff09\uff0c\u5176\u4ed6\u65b9\u6cd5\u8868\u73b0\u4e0d\u4e00\u3002LLM\u5ba1\u8ba1\u9700\u8981\u989d\u5916\u63d0\u793a\u624d\u80fd\u6709\u6548\u3002", "conclusion": "\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5728\u68c0\u6d4b\u9690\u85cf\u884c\u4e3a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cLLM\u5ba1\u8ba1\u9700\u4f9d\u8d56\u63d0\u793a\u3002\u5f00\u6e90\u5ba1\u8ba1\u6e38\u620f\u4ee5\u4fc3\u8fdb\u66f4\u597d\u7684\u5ba1\u8ba1\u8bbe\u8ba1\u3002"}}
{"id": "2508.06871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06871", "abs": "https://arxiv.org/abs/2508.06871", "authors": ["Aleksandar Todorov", "Juan Cardenas-Cartagena", "Rafael F. Cunha", "Marco Zullich", "Matthia Sabatelli"], "title": "Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning", "comment": null, "summary": "Plasticity loss, a diminishing capacity to adapt as training progresses, is a\ncritical challenge in deep reinforcement learning. We examine this issue in\nmulti-task reinforcement learning (MTRL), where higher representational\nflexibility is crucial for managing diverse and potentially conflicting task\ndemands. We systematically explore how sparsification methods, particularly\nGradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance\nplasticity and consequently improve performance in MTRL agents. We evaluate\nthese approaches across distinct MTRL architectures (shared backbone, Mixture\nof Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,\ncomparing against dense baselines, and a comprehensive range of alternative\nplasticity-inducing or regularization methods. Our results demonstrate that\nboth GMP and SET effectively mitigate key indicators of plasticity degradation,\nsuch as neuron dormancy and representational collapse. These plasticity\nimprovements often correlate with enhanced multi-task performance, with sparse\nagents frequently outperforming dense counterparts and achieving competitive\nresults against explicit plasticity interventions. Our findings offer insights\ninto the interplay between plasticity, network sparsity, and MTRL designs,\nhighlighting dynamic sparsification as a robust but context-sensitive tool for\ndeveloping more adaptable MTRL systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7a00\u758f\u5316\u65b9\u6cd5\uff08GMP\u548cSET\uff09\u5982\u4f55\u63d0\u5347\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u5851\u6027\uff0c\u5e76\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u53ef\u5851\u6027\u4e27\u5931\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u591a\u4efb\u52a1\u73af\u5883\u4e0b\u3002", "method": "\u91c7\u7528Gradual Magnitude Pruning (GMP)\u548cSparse Evolutionary Training (SET)\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540cMTRL\u67b6\u6784\u4e0a\u8bc4\u4f30\u3002", "result": "\u7a00\u758f\u5316\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u53ef\u5851\u6027\u9000\u5316\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4f18\u4e8e\u5bc6\u96c6\u57fa\u7ebf\u548c\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u7a00\u758f\u5316\u662f\u63d0\u5347MTRL\u7cfb\u7edf\u9002\u5e94\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u9700\u7ed3\u5408\u5177\u4f53\u573a\u666f\u3002"}}
{"id": "2508.06885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06885", "abs": "https://arxiv.org/abs/2508.06885", "authors": ["Anthony Bellotti", "Xindi Zhao"], "title": "Conformal Prediction and Trustworthy AI", "comment": "Preprint for an essay to be published in The Importance of Being\n  Learnable (Enhancing the Learnability and Reliability of Machine Learning\n  Algorithms) Essays Dedicated to Alexander Gammerman on His 80th Birthday,\n  LNCS Springer Nature Switzerland AG ed. Nguyen K.A. and Luo Z", "summary": "Conformal predictors are machine learning algorithms developed in the 1990's\nby Gammerman, Vovk, and their research team, to provide set predictions with\nguaranteed confidence level. Over recent years, they have grown in popularity\nand have become a mainstream methodology for uncertainty quantification in the\nmachine learning community. From its beginning, there was an understanding that\nthey enable reliable machine learning with well-calibrated uncertainty\nquantification. This makes them extremely beneficial for developing trustworthy\nAI, a topic that has also risen in interest over the past few years, in both\nthe AI community and society more widely. In this article, we review the\npotential for conformal prediction to contribute to trustworthy AI beyond its\nmarginal validity property, addressing problems such as generalization risk and\nAI governance. Experiments and examples are also provided to demonstrate its\nuse as a well-calibrated predictor and for bias identification and mitigation.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u4fdd\u5f62\u9884\u6d4b\u5728\u53ef\u4fe1AI\u4e2d\u7684\u6f5c\u529b\uff0c\u63a2\u8ba8\u4e86\u5176\u8d85\u8d8a\u8fb9\u9645\u6709\u6548\u6027\u7684\u5e94\u7528\uff0c\u5982\u6cdb\u5316\u98ce\u9669\u548cAI\u6cbb\u7406\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u6821\u51c6\u9884\u6d4b\u5668\u548c\u504f\u5dee\u8bc6\u522b\u5de5\u5177\u7684\u4f5c\u7528\u3002", "motivation": "\u4fdd\u5f62\u9884\u6d4b\u4f5c\u4e3a\u4e00\u79cd\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u4fdd\u8bc1\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd1\u5e74\u6765\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u5176\u53ef\u9760\u7684\u6821\u51c6\u7279\u6027\u4f7f\u5176\u6210\u4e3a\u6784\u5efa\u53ef\u4fe1AI\u7684\u91cd\u8981\u5de5\u5177\uff0c\u5c24\u5176\u5728\u6cdb\u5316\u98ce\u9669\u548cAI\u6cbb\u7406\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u5b9e\u9a8c\u548c\u793a\u4f8b\u5c55\u793a\u4e86\u4fdd\u5f62\u9884\u6d4b\u7684\u5e94\u7528\uff0c\u5305\u62ec\u4f5c\u4e3a\u6821\u51c6\u9884\u6d4b\u5668\u548c\u7528\u4e8e\u504f\u5dee\u8bc6\u522b\u4e0e\u7f13\u89e3\u3002", "result": "\u4fdd\u5f62\u9884\u6d4b\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u8fb9\u9645\u6709\u6548\u6027\uff0c\u8fd8\u80fd\u6709\u6548\u5e94\u5bf9\u6cdb\u5316\u98ce\u9669\u548cAI\u6cbb\u7406\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u6821\u51c6\u9884\u6d4b\u5668\u548c\u504f\u5dee\u8bc6\u522b\u5de5\u5177\u7684\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u5728\u53ef\u4fe1AI\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u5176\u6821\u51c6\u7279\u6027\u548c\u504f\u5dee\u7f13\u89e3\u80fd\u529b\u4e3aAI\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.06915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06915", "abs": "https://arxiv.org/abs/2508.06915", "authors": ["Shichao Ma", "Zhengyang Zhou", "Qihe Huang", "Binwu Wang", "Kuo Yang", "Huan Li", "Yang Wang"], "title": "QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting", "comment": null, "summary": "Time series forecasting has become increasingly important to empower diverse\napplications with streaming data. Zero-shot time-series forecasting (ZSF),\nparticularly valuable in data-scarce scenarios, such as domain transfer or\nforecasting under extreme conditions, is difficult for traditional models to\ndeal with. While time series pre-trained models (TSPMs) have demonstrated\nstrong performance in ZSF, they often lack mechanisms to dynamically\nincorporate external knowledge. Fortunately, emerging retrieval-augmented\ngeneration (RAG) offers a promising path for injecting such knowledge on\ndemand, yet they are rarely integrated with TSPMs. To leverage the strengths of\nboth worlds, we introduce RAG into TSPMs to enhance zero-shot time series\nforecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series\nForecaster), a lightweight and modular framework that couples efficient\nretrieval with representation learning and model adaptation for ZSF.\nSpecifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)\nfor scalable time-series storage and domain-aware retrieval, introduce a\nMulti-grained Series Interaction Learner (MSIL) to extract fine- and\ncoarse-grained relational features, and develop a dual-branch Model Cooperation\nCoherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM\nbased and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM\nbased and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and\n87.5% of prediction settings, while maintaining high efficiency in memory and\ninference time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQuiZSF\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u6a21\u578b\uff08TSPMs\uff09\uff0c\u63d0\u5347\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08ZSF\uff09\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\uff08\u5982\u9886\u57df\u8fc1\u79fb\u6216\u6781\u7aef\u6761\u4ef6\uff09\u4e0b\u96be\u4ee5\u5b9e\u73b0\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u800c\u73b0\u6709TSPMs\u7f3a\u4e4f\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u7684\u673a\u5236\u3002", "method": "\u63d0\u51faQuiZSF\u6846\u67b6\uff0c\u5305\u62ecChronoRAG Base\uff08CRB\uff09\u7528\u4e8e\u5b58\u50a8\u4e0e\u68c0\u7d22\u3001Multi-grained Series Interaction Learner\uff08MSIL\uff09\u63d0\u53d6\u7279\u5f81\u3001Model Cooperation Coherer\uff08MCC\uff09\u5bf9\u9f50\u77e5\u8bc6\u4e0e\u975eLLM/LLM\u57faTSPMs\u3002", "result": "QuiZSF\u572875%\uff08\u975eLLM\u57fa\uff09\u548c87.5%\uff08LLM\u57fa\uff09\u9884\u6d4b\u573a\u666f\u4e2d\u6392\u540dTop1\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u5185\u5b58\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "QuiZSF\u901a\u8fc7\u7ed3\u5408RAG\u4e0eTSPMs\uff0c\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2508.06943", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06943", "abs": "https://arxiv.org/abs/2508.06943", "authors": ["Lishi Zuo", "Man-Wai Mak", "Lu Yi", "Youzhi Tu"], "title": "Class Unbiasing for Generalization in Medical Diagnosis", "comment": null, "summary": "Medical diagnosis might fail due to bias. In this work, we identified\nclass-feature bias, which refers to models' potential reliance on features that\nare strongly correlated with only a subset of classes, leading to biased\nperformance and poor generalization on other classes. We aim to train a\nclass-unbiased model (Cls-unbias) that mitigates both class imbalance and\nclass-feature bias simultaneously. Specifically, we propose a class-wise\ninequality loss which promotes equal contributions of classification loss from\npositive-class and negative-class samples. We propose to optimize a class-wise\ngroup distributionally robust optimization objective-a class-weighted training\nobjective that upweights underperforming classes-to enhance the effectiveness\nof the inequality loss under class imbalance. Through synthetic and real-world\ndatasets, we empirically demonstrate that class-feature bias can negatively\nimpact model performance. Our proposed method effectively mitigates both\nclass-feature bias and class imbalance, thereby improving the model's\ngeneralization ability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u533b\u5b66\u8bca\u65ad\u4e2d\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u522b\u4e0d\u5e73\u7b49\u635f\u5931\u548c\u7c7b\u522b\u52a0\u6743\u4f18\u5316\u76ee\u6807\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u533b\u5b66\u8bca\u65ad\u6a21\u578b\u53ef\u80fd\u56e0\u7c7b\u522b\u7279\u5f81\u504f\u5dee\uff08\u4f9d\u8d56\u4e0e\u90e8\u5206\u7c7b\u522b\u5f3a\u76f8\u5173\u7684\u7279\u5f81\uff09\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u6027\u80fd\u504f\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u7c7b\u522b\u4e0d\u5e73\u7b49\u635f\u5931\u548c\u7c7b\u522b\u52a0\u6743\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u76ee\u6807\uff0c\u5e73\u8861\u6b63\u8d1f\u7c7b\u522b\u6837\u672c\u7684\u5206\u7c7b\u635f\u5931\u8d21\u732e\uff0c\u5e76\u4f18\u5316\u8868\u73b0\u4e0d\u4f73\u7684\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u4f1a\u635f\u5bb3\u6a21\u578b\u6027\u80fd\uff0c\u800c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u504f\u5dee\u548c\u4e0d\u5e73\u8861\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u540c\u65f6\u89e3\u51b3\u7c7b\u522b\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2508.06944", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06944", "abs": "https://arxiv.org/abs/2508.06944", "authors": ["Lixuan He", "Jie Feng", "Yong Li"], "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "comment": null, "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of \\textbf{implicit\nrewards}, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAMFT\u7684\u5355\u9636\u6bb5\u7b97\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u5956\u52b1\u7406\u8bba\u52a8\u6001\u5e73\u8861\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u63a2\u7d22-\u6a21\u4eff\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08SFT+RL\uff09\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u63a2\u7d22-\u6a21\u4eff\u6743\u8861\u95ee\u9898\uff0c\u800c\u73b0\u6709\u5355\u9636\u6bb5\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u5e73\u8861\u673a\u5236\u3002", "method": "\u63d0\u51faAMFT\u7b97\u6cd5\uff0c\u901a\u8fc7\u5143\u68af\u5ea6\u81ea\u9002\u5e94\u6743\u91cd\u63a7\u5236\u5668\u52a8\u6001\u4f18\u5316SFT\u548cRL\u7684\u5e73\u8861\uff0c\u5e76\u7ed3\u5408\u7b56\u7565\u71b5\u6b63\u5219\u5316\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u548c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7b49\u4efb\u52a1\u4e0a\uff0cAMFT\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AMFT\u901a\u8fc7\u52a8\u6001\u5e73\u8861SFT\u548cRL\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u7a33\u5b9a\u7684LLM\u5bf9\u9f50\u8303\u5f0f\u3002"}}
{"id": "2508.06953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06953", "abs": "https://arxiv.org/abs/2508.06953", "authors": ["Shiwei Li", "Xiandi Luo", "Haozhao Wang", "Xing Tang", "Ziqiang Cui", "Dugang Liu", "Yuhua Li", "Xiuqiang He", "Ruixuan Li"], "title": "BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity", "comment": null, "summary": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method\nwidely used in large language models (LLMs). It approximates the update of a\npretrained weight matrix $W\\in\\mathbb{R}^{m\\times n}$ by the product of two\nlow-rank matrices, $BA$, where $A \\in\\mathbb{R}^{r\\times n}$ and\n$B\\in\\mathbb{R}^{m\\times r} (r\\ll\\min\\{m,n\\})$. Increasing the dimension $r$\ncan raise the rank of LoRA weights (i.e., $BA$), which typically improves\nfine-tuning performance but also significantly increases the number of\ntrainable parameters. In this paper, we propose Block Diversified Low-Rank\nAdaptation (BoRA), which improves the rank of LoRA weights with a small number\nof additional parameters. Specifically, BoRA treats the product $BA$ as a block\nmatrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along\nthe columns and rows, respectively (i.e., $A=[A_1,\\dots,A_b]$ and\n$B=[B_1,\\dots,B_b]^\\top$). Consequently, the product $BA$ becomes the\nconcatenation of the block products $B_iA_j$ for $i,j\\in[b]$. To enhance the\ndiversity of different block products, BoRA introduces a unique diagonal matrix\n$\\Sigma_{i,j} \\in \\mathbb{R}^{r\\times r}$ for each block multiplication,\nresulting in $B_i \\Sigma_{i,j} A_j$. By leveraging these block-wise diagonal\nmatrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only\nrequiring $b^2r$ additional parameters. Extensive experiments across multiple\ndatasets and models demonstrate the superiority of BoRA, and ablation studies\nfurther validate its scalability.", "AI": {"tldr": "BoRA\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u77e9\u9635\u4e58\u6cd5\u548c\u5757\u95f4\u5bf9\u89d2\u77e9\u9635\u63d0\u5347LoRA\u6743\u91cd\u79e9\uff0c\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u53c2\u6570\u3002", "motivation": "LoRA\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u589e\u52a0\u79e9\u4f1a\u663e\u8457\u589e\u52a0\u53ef\u8bad\u7ec3\u53c2\u6570\u3002BoRA\u65e8\u5728\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u63d0\u5347\u79e9\u3002", "method": "BoRA\u5c06LoRA\u7684\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u5757\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u5757\u95f4\u5bf9\u89d2\u77e9\u9635\uff0c\u63d0\u5347\u79e9\u7684\u540c\u65f6\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eBoRA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "BoRA\u901a\u8fc7\u5757\u591a\u6837\u5316\u548c\u5bf9\u89d2\u77e9\u9635\u8bbe\u8ba1\uff0c\u9ad8\u6548\u63d0\u5347\u4e86LoRA\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u3002"}}
{"id": "2508.06966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06966", "abs": "https://arxiv.org/abs/2508.06966", "authors": ["Hiba Najjar", "Bushra Alshbib", "Andreas Dengel"], "title": "Can Multitask Learning Enhance Model Explainability?", "comment": "Accepted at GCPR 2025, Special Track \"Photogrammetry and remote\n  sensing\"", "summary": "Remote sensing provides satellite data in diverse types and formats. The\nusage of multimodal learning networks exploits this diversity to improve model\nperformance, except that the complexity of such networks comes at the expense\nof their interpretability. In this study, we explore how modalities can be\nleveraged through multitask learning to intrinsically explain model behavior.\nIn particular, instead of additional inputs, we use certain modalities as\nadditional targets to be predicted along with the main task. The success of\nthis approach relies on the rich information content of satellite data, which\nremains as input modalities. We show how this modeling context provides\nnumerous benefits: (1) in case of data scarcity, the additional modalities do\nnot need to be collected for model inference at deployment, (2) the model\nperformance remains comparable to the multimodal baseline performance, and in\nsome cases achieves better scores, (3) prediction errors in the main task can\nbe explained via the model behavior in the auxiliary task(s). We demonstrate\nthe efficiency of our approach on three datasets, including segmentation,\nclassification, and regression tasks. Code available at\ngit.opendfki.de/hiba.najjar/mtl_explainability/.", "AI": {"tldr": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u5229\u7528\u536b\u661f\u6570\u636e\u7684\u591a\u6a21\u6001\u6027\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u907f\u514d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\u867d\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u727a\u7272\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5229\u7528\u536b\u661f\u6570\u636e\u7684\u591a\u6a21\u6001\u6027\uff0c\u65e2\u63d0\u5347\u6027\u80fd\u53c8\u589e\u5f3a\u6a21\u578b\u884c\u4e3a\u7684\u89e3\u91ca\u6027\u3002", "method": "\u5c06\u67d0\u4e9b\u6a21\u6001\u4f5c\u4e3a\u9644\u52a0\u76ee\u6807\u4e0e\u4e3b\u4efb\u52a1\u4e00\u8d77\u9884\u6d4b\uff0c\u800c\u975e\u989d\u5916\u8f93\u5165\u3002\u5229\u7528\u536b\u661f\u6570\u636e\u7684\u4e30\u5bcc\u4fe1\u606f\u4f5c\u4e3a\u8f93\u5165\u6a21\u6001\u3002", "result": "\u5728\u6570\u636e\u7a00\u7f3a\u65f6\uff0c\u9644\u52a0\u6a21\u6001\u65e0\u9700\u6536\u96c6\uff1b\u6a21\u578b\u6027\u80fd\u4e0e\u591a\u6a21\u6001\u57fa\u7ebf\u76f8\u5f53\u6216\u66f4\u4f18\uff1b\u4e3b\u4efb\u52a1\u9884\u6d4b\u8bef\u5dee\u53ef\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u884c\u4e3a\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u3001\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06981", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.06981", "abs": "https://arxiv.org/abs/2508.06981", "authors": ["Brooks Kinch", "Benjamin Shaffer", "Elizabeth Armstrong", "Michael Meehan", "John Hewson", "Nathaniel Trask"], "title": "Structure-Preserving Digital Twins via Conditional Neural Whitney Forms", "comment": null, "summary": "We present a framework for constructing real-time digital twins based on\nstructure-preserving reduced finite element models conditioned on a latent\nvariable Z. The approach uses conditional attention mechanisms to learn both a\nreduced finite element basis and a nonlinear conservation law within the\nframework of finite element exterior calculus (FEEC). This guarantees numerical\nwell-posedness and exact preservation of conserved quantities, regardless of\ndata sparsity or optimization error. The conditioning mechanism supports\nreal-time calibration to parametric variables, allowing the construction of\ndigital twins which support closed loop inference and calibration to sensor\ndata. The framework interfaces with conventional finite element machinery in a\nnon-invasive manner, allowing treatment of complex geometries and integration\nof learned models with conventional finite element techniques.\n  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,\nand a complex battery thermal runaway problem. The method achieves accurate\npredictions on complex geometries with sparse data (25 LES simulations),\nincluding capturing the transition to turbulence and achieving real-time\ninference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source\nimplementation is available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6f5c\u53d8\u91cfZ\u7684\u7ed3\u6784\u4fdd\u6301\u964d\u9636\u6709\u9650\u5143\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u4f53\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u6709\u9650\u5143\u5916\u5fae\u79ef\u5206\uff08FEEC\uff09\uff0c\u786e\u4fdd\u6570\u503c\u9002\u5b9a\u6027\u548c\u5b88\u6052\u91cf\u7684\u7cbe\u786e\u4fdd\u6301\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6216\u4f18\u5316\u8bef\u5dee\u4e0b\u7684\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u4f53\u6784\u5efa\u95ee\u9898\uff0c\u652f\u6301\u95ed\u73af\u63a8\u7406\u548c\u4f20\u611f\u5668\u6570\u636e\u6821\u51c6\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u964d\u9636\u6709\u9650\u5143\u57fa\u548c\u975e\u7ebf\u6027\u5b88\u6052\u5f8b\uff0c\u7ed3\u5408FEEC\u6846\u67b6\uff0c\u975e\u4fb5\u5165\u5f0f\u96c6\u6210\u4f20\u7edf\u6709\u9650\u5143\u6280\u672f\u3002", "result": "\u5728\u590d\u6742\u51e0\u4f55\u548c\u7a00\u758f\u6570\u636e\uff0825\u6b21LES\u6a21\u62df\uff09\u4e0b\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u5305\u62ec\u6e4d\u6d41\u8fc7\u6e21\uff0c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u53473.1x10^8\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u9002\u7528\u4e8e\u590d\u6742\u51e0\u4f55\u548c\u5b9e\u65f6\u5e94\u7528\uff0c\u5f00\u6e90\u5b9e\u73b0\u5df2\u53d1\u5e03\u3002"}}
{"id": "2508.06986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06986", "abs": "https://arxiv.org/abs/2508.06986", "authors": ["Chonghua Han", "Yuan Yuan", "Yukun Liu", "Jingtao Ding", "Jie Feng", "Yong Li"], "title": "UniMove: A Unified Model for Multi-city Human Mobility Prediction", "comment": "Accepted by SIGSPATIAL 2025", "summary": "Human mobility prediction is vital for urban planning, transportation\noptimization, and personalized services. However, the inherent randomness,\nnon-uniform time intervals, and complex patterns of human mobility, compounded\nby the heterogeneity introduced by varying city structures, infrastructure, and\npopulation densities, present significant challenges in modeling. Existing\nsolutions often require training separate models for each city due to distinct\nspatial representations and geographic coverage. In this paper, we propose\nUniMove, a unified model for multi-city human mobility prediction, addressing\ntwo challenges: (1) constructing universal spatial representations for\neffective token sharing across cities, and (2) modeling heterogeneous mobility\npatterns from varying city characteristics. We propose a trajectory-location\ndual-tower architecture, with a location tower for universal spatial encoding\nand a trajectory tower for sequential mobility modeling. We also design MoE\nTransformer blocks to adaptively select experts to handle diverse movement\npatterns. Extensive experiments across multiple datasets from diverse cities\ndemonstrate that UniMove truly embodies the essence of a unified model. By\nenabling joint training on multi-city data with mutual data enhancement, it\nsignificantly improves mobility prediction accuracy by over 10.2\\%. UniMove\nrepresents a key advancement toward realizing a true foundational model with a\nunified architecture for human mobility. We release the implementation at\nhttps://github.com/tsinghua-fib-lab/UniMove/.", "AI": {"tldr": "UniMove\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u57ce\u5e02\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u901a\u7528\u7a7a\u95f4\u8868\u793a\u548c\u5f02\u6784\u6a21\u5f0f\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u5bf9\u57ce\u5e02\u89c4\u5212\u548c\u670d\u52a1\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u56e0\u57ce\u5e02\u5f02\u8d28\u6027\u9700\u5355\u72ec\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53cc\u5854\u67b6\u6784\uff08\u8f68\u8ff9\u5854\u548c\u4f4d\u7f6e\u5854\uff09\u548cMoE Transformer\u5757\uff0c\u5b9e\u73b0\u901a\u7528\u7a7a\u95f4\u7f16\u7801\u548c\u5f02\u6784\u6a21\u5f0f\u5efa\u6a21\u3002", "result": "\u5728\u591a\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\uff0cUniMove\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u602710.2%\u3002", "conclusion": "UniMove\u662f\u5b9e\u73b0\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8fdb\u5c55\uff0c\u652f\u6301\u7edf\u4e00\u67b6\u6784\u548c\u591a\u57ce\u5e02\u6570\u636e\u589e\u5f3a\u3002"}}
{"id": "2508.06991", "categories": ["cs.LG", "68T01, 68T05", "I.2.6; I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.06991", "abs": "https://arxiv.org/abs/2508.06991", "authors": ["Vojtech Halenka", "Ole-Christoffer Granmo", "Lei Jiao", "Per-Arne Andersen"], "title": "A Comparative Study of Feature Selection in Tsetlin Machines", "comment": "submitted to SGAI-2025: The 45th SGAI International Conference on\n  Innovative Techniques and Applications of Artificial Intelligence", "summary": "Feature Selection (FS) is crucial for improving model interpretability,\nreducing complexity, and sometimes for enhancing accuracy. The recently\nintroduced Tsetlin machine (TM) offers interpretable clause-based learning, but\nlacks established tools for estimating feature importance. In this paper, we\nadapt and evaluate a range of FS techniques for TMs, including classical filter\nand embedded methods as well as post-hoc explanation methods originally\ndeveloped for neural networks (e.g., SHAP and LIME) and a novel family of\nembedded scorers derived from TM clause weights and Tsetlin automaton (TA)\nstates. We benchmark all methods across 12 datasets, using evaluation\nprotocols, like Remove and Retrain (ROAR) strategy and Remove and Debias\n(ROAD), to assess causal impact. Our results show that TM-internal scorers not\nonly perform competitively but also exploit the interpretability of clauses to\nreveal interacting feature patterns. Simpler TM-specific scorers achieve\nsimilar accuracy retention at a fraction of the computational cost. This study\nestablishes the first comprehensive baseline for FS in TM and paves the way for\ndeveloping specialized TM-specific interpretability techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7279\u5f81\u9009\u62e9\uff08FS\uff09\u5728Tsetlin\u673a\u5668\uff08TM\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u591a\u79cdFS\u65b9\u6cd5\uff0c\u5305\u62ec\u7ecf\u5178\u65b9\u6cd5\u548c\u65b0\u5174\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8eTM\u5185\u90e8\u8bc4\u5206\u5668\u7684FS\u65b9\u6cd5\u3002", "motivation": "TM\u7f3a\u4e4f\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347TM\u7684\u6a21\u578b\u89e3\u91ca\u6027\u548c\u6027\u80fd\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u591a\u79cdFS\u6280\u672f\uff0c\u5305\u62ec\u7ecf\u5178\u8fc7\u6ee4\u548c\u5d4c\u5165\u65b9\u6cd5\uff0c\u4ee5\u53caSHAP\u3001LIME\u7b49\u540e\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTM\u5b50\u53e5\u6743\u91cd\u548cTsetlin\u81ea\u52a8\u673a\u72b6\u6001\u7684\u65b0\u578b\u8bc4\u5206\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTM\u5185\u90e8\u8bc4\u5206\u5668\u4e0d\u4ec5\u6027\u80fd\u4f18\u5f02\uff0c\u8fd8\u80fd\u5229\u7528\u5b50\u53e5\u7684\u53ef\u89e3\u91ca\u6027\u63ed\u793a\u7279\u5f81\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aTM\u4e2d\u7684FS\u5efa\u7acb\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u7ebf\uff0c\u5e76\u4e3a\u5f00\u53d1\u4e13\u95e8\u7684TM\u89e3\u91ca\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06997", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06997", "abs": "https://arxiv.org/abs/2508.06997", "authors": ["Helbert Paat", "Guohao Shen"], "title": "Conformal Set-based Human-AI Complementarity with Multiple Experts", "comment": "Accepted at AAMAS 2025. Code available at:\n  https://github.com/paathelb/conformal_hai_multiple", "summary": "Decision support systems are designed to assist human experts in\nclassification tasks by providing conformal prediction sets derived from a\npre-trained model. This human-AI collaboration has demonstrated enhanced\nclassification performance compared to using either the model or the expert\nindependently. In this study, we focus on the selection of instance-specific\nexperts from a pool of multiple human experts, contrasting it with existing\nresearch that typically focuses on single-expert scenarios. We characterize the\nconditions under which multiple experts can benefit from the conformal sets.\nWith the insight that only certain experts may be relevant for each instance,\nwe explore the problem of subset selection and introduce a greedy algorithm\nthat utilizes conformal sets to identify the subset of expert predictions that\nwill be used in classifying an instance. This approach is shown to yield better\nperformance compared to naive methods for human subset selection. Based on real\nexpert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation\nstudy indicates that our proposed greedy algorithm achieves near-optimal\nsubsets, resulting in improved classification performance among multiple\nexperts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u591a\u4e13\u5bb6\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u8d2a\u5fc3\u7b97\u6cd5\u9009\u62e9\u7279\u5b9a\u5b9e\u4f8b\u7684\u76f8\u5173\u4e13\u5bb6\u5b50\u96c6\uff0c\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5355\u4e13\u5bb6\u573a\u666f\uff0c\u800c\u591a\u4e13\u5bb6\u534f\u4f5c\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u5206\u7c7b\u6548\u679c\uff0c\u4f46\u9700\u89e3\u51b3\u5982\u4f55\u9009\u62e9\u76f8\u5173\u4e13\u5bb6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u96c6\u9009\u62e9\u4e13\u5bb6\u5b50\u96c6\uff0c\u5e76\u5728CIFAR-10H\u548cImageNet-16H\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u3002", "result": "\u8d2a\u5fc3\u7b97\u6cd5\u80fd\u9009\u62e9\u63a5\u8fd1\u6700\u4f18\u7684\u4e13\u5bb6\u5b50\u96c6\uff0c\u663e\u8457\u63d0\u5347\u591a\u4e13\u5bb6\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u591a\u4e13\u5bb6\u534f\u4f5c\u4e2d\uff0c\u5b9e\u4f8b\u76f8\u5173\u7684\u4e13\u5bb6\u5b50\u96c6\u9009\u62e9\u662f\u5173\u952e\uff0c\u8d2a\u5fc3\u7b97\u6cd5\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07016", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.07016", "abs": "https://arxiv.org/abs/2508.07016", "authors": ["Jianfei Wu", "Wenmian Yang", "Bingning Liu", "Weijia Jia"], "title": "TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations", "comment": null, "summary": "Time series forecasting is critical across various domains, such as weather,\nfinance and real estate forecasting, as accurate forecasts support informed\ndecision-making and risk mitigation. While recent deep learning models have\nimproved predictive capabilities, they often overlook time-lagged\ncross-correlations between related sequences, which are crucial for capturing\ncomplex temporal relationships. To address this, we propose the Time-Lagged\nCross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances\nforecasting accuracy by effectively integrating time-lagged cross-correlated\nsequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)\nalgorithm to capture lagged correlations and a contrastive learning-based\nencoder to efficiently approximate SSDTW distances.\n  Experimental results on weather, finance and real estate time series datasets\ndemonstrate the effectiveness of our framework. On the weather dataset, SSDTW\nreduces mean squared error (MSE) by 16.01% compared with single-sequence\nmethods, while the contrastive learning encoder (CLE) further decreases MSE by\n17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE\nreduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by\n21.29% and 8.62%, respectively. Additionally, the contrastive learning approach\ndecreases SSDTW computational time by approximately 99%, ensuring scalability\nand real-time applicability across multiple time series forecasting tasks.", "AI": {"tldr": "\u63d0\u51faTLCCSP\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u6ede\u4ea4\u53c9\u76f8\u5173\u5e8f\u5217\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4f7f\u7528SSDTW\u7b97\u6cd5\u548c\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u5668\uff0c\u663e\u8457\u964d\u4f4eMSE\u5e76\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e38\u5ffd\u7565\u65f6\u6ede\u4ea4\u53c9\u76f8\u5173\u6027\uff0c\u800c\u8fd9\u5bf9\u6355\u6349\u590d\u6742\u65f6\u95f4\u5173\u7cfb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTLCCSP\u6846\u67b6\uff0c\u7ed3\u5408SSDTW\u7b97\u6cd5\u6355\u6349\u65f6\u6ede\u76f8\u5173\u6027\uff0c\u5e76\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u5668\u9ad8\u6548\u8fd1\u4f3cSSDTW\u8ddd\u79bb\u3002", "result": "\u5728\u5929\u6c14\u3001\u91d1\u878d\u548c\u623f\u5730\u4ea7\u6570\u636e\u96c6\u4e0a\uff0cSSDTW\u548c\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u5668\u663e\u8457\u964d\u4f4eMSE\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1199%\u3002", "conclusion": "TLCCSP\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u3002"}}
{"id": "2508.07032", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.07032", "abs": "https://arxiv.org/abs/2508.07032", "authors": ["Tiantian He", "Keyue Jiang", "An Zhao", "Anna Schroder", "Elinor Thompson", "Sonja Soskic", "Frederik Barkhof", "Daniel C. Alexander"], "title": "A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling", "comment": null, "summary": "The long-term progression of neurodegenerative diseases is commonly\nconceptualized as a spatiotemporal diffusion process that consists of a graph\ndiffusion process across the structural brain connectome and a localized\nreaction process within brain regions. However, modeling this progression\nremains challenging due to 1) the scarcity of longitudinal data obtained\nthrough irregular and infrequent subject visits and 2) the complex interplay of\npathological mechanisms across brain regions and disease stages, where\ntraditional models assume fixed mechanisms throughout disease progression. To\naddress these limitations, we propose a novel stage-aware Mixture of Experts\n(MoE) framework that explicitly models how different contributing mechanisms\ndominate at different disease stages through time-dependent expert\nweighting.Data-wise, we utilize an iterative dual optimization method to\nproperly estimate the temporal position of individual observations,\nconstructing a co hort-level progression trajectory from irregular snapshots.\nModel-wise, we enhance the spatial component with an inhomogeneous graph neural\ndiffusion model (IGND) that allows diffusivity to vary based on node states and\ntime, providing more flexible representations of brain networks. We also\nintroduce a localized neural reaction module to capture complex dynamics beyond\nstandard processes.The resulting IGND-MoE model dynamically integrates these\ncomponents across temporal states, offering a principled way to understand how\nstage-specific pathological mechanisms contribute to progression. The\nstage-wise weights yield novel clinical insights that align with literature,\nsuggesting that graph-related processes are more influential at early stages,\nwhile other unknown physical processes become dominant later on.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9636\u6bb5\u611f\u77e5\u7684Mixture of Experts\uff08MoE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u52a8\u6001\u8fdb\u5c55\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u4f9d\u8d56\u7684\u4e13\u5bb6\u6743\u91cd\u548c\u5f02\u6784\u56fe\u795e\u7ecf\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8fdb\u5c55\u5efa\u6a21\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7eb5\u5411\u6570\u636e\u7a00\u7f3a\u548c\u75c5\u7406\u673a\u5236\u5728\u4e0d\u540c\u9636\u6bb5\u7684\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u9636\u6bb5\u611f\u77e5\u7684MoE\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u4f9d\u8d56\u7684\u4e13\u5bb6\u6743\u91cd\u3001\u5f02\u6784\u56fe\u795e\u7ecf\u6269\u6563\u6a21\u578b\uff08IGND\uff09\u548c\u5c40\u90e8\u795e\u7ecf\u53cd\u5e94\u6a21\u5757\u3002", "result": "\u6a21\u578b\u80fd\u591f\u52a8\u6001\u6574\u5408\u4e0d\u540c\u9636\u6bb5\u7684\u75c5\u7406\u673a\u5236\uff0c\u63ed\u793a\u4e86\u65e9\u671f\u9636\u6bb5\u56fe\u76f8\u5173\u8fc7\u7a0b\u66f4\u663e\u8457\uff0c\u800c\u540e\u671f\u5176\u4ed6\u672a\u77e5\u7269\u7406\u8fc7\u7a0b\u5360\u4e3b\u5bfc\u3002", "conclusion": "IGND-MoE\u6a21\u578b\u4e3a\u7406\u89e3\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u9636\u6bb5\u7279\u5f02\u6027\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u3002"}}
{"id": "2508.07054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07054", "abs": "https://arxiv.org/abs/2508.07054", "authors": ["Ziqi Zhang", "Ali Shahin Shamsabadi", "Hanxiao Lu", "Yifeng Cai", "Hamed Haddadi"], "title": "Membership and Memorization in LLM Knowledge Distillation", "comment": null, "summary": "Recent advances in Knowledge Distillation (KD) aim to mitigate the high\ncomputational demands of Large Language Models (LLMs) by transferring knowledge\nfrom a large ''teacher'' to a smaller ''student'' model. However, students may\ninherit the teacher's privacy when the teacher is trained on private data. In\nthis work, we systematically characterize and investigate membership and\nmemorization privacy risks inherent in six LLM KD techniques. Using\ninstruction-tuning settings that span seven NLP tasks, together with three\nteacher model families (GPT-2, LLAMA-2, and OPT), and various size student\nmodels, we demonstrate that all existing LLM KD approaches carry membership and\nmemorization privacy risks from the teacher to its students. However, the\nextent of privacy risks varies across different KD techniques. We\nsystematically analyse how key LLM KD components (KD objective functions,\nstudent training data and NLP tasks) impact such privacy risks. We also\ndemonstrate a significant disagreement between memorization and membership\nprivacy risks of LLM KD techniques. Finally, we characterize per-block privacy\nrisk and demonstrate that the privacy risk varies across different blocks by a\nlarge margin.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u516d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6280\u672f\u4e2d\u7684\u9690\u79c1\u98ce\u9669\uff0c\u53d1\u73b0\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u5747\u5b58\u5728\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u98ce\u9669\u7a0b\u5ea6\u56e0\u6280\u672f\u800c\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\u4e2d\u4ece\u6559\u5e08\u6a21\u578b\u5230\u5b66\u751f\u6a21\u578b\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6559\u5e08\u6a21\u578b\u57fa\u4e8e\u79c1\u6709\u6570\u636e\u8bad\u7ec3\u65f6\u3002", "method": "\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u8bbe\u7f6e\uff0c\u8986\u76d6\u4e03\u79cdNLP\u4efb\u52a1\uff0c\u4f7f\u7528\u4e09\u79cd\u6559\u5e08\u6a21\u578b\u5bb6\u65cf\uff08GPT-2\u3001LLAMA-2\u3001OPT\uff09\u548c\u4e0d\u540c\u89c4\u6a21\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5206\u6790KD\u76ee\u6807\u51fd\u6570\u3001\u5b66\u751f\u8bad\u7ec3\u6570\u636e\u548cNLP\u4efb\u52a1\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\u3002", "result": "\u6240\u6709\u73b0\u6709LLM KD\u65b9\u6cd5\u5747\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u98ce\u9669\u7a0b\u5ea6\u56e0\u6280\u672f\u4e0d\u540c\u800c\u5f02\uff1b\u8bb0\u5fc6\u4e0e\u6210\u5458\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\uff1b\u4e0d\u540c\u6a21\u5757\u7684\u9690\u79c1\u98ce\u9669\u5dee\u5f02\u8f83\u5927\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5b58\u5728\u663e\u8457\u98ce\u9669\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u9690\u79c1\u5b89\u5168\u3002"}}
{"id": "2508.07075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07075", "abs": "https://arxiv.org/abs/2508.07075", "authors": ["Stanley Ngugi"], "title": "Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation", "comment": "9 pages, 2 visual aids", "summary": "Large Language Models (LLMs) struggle with dynamic knowledge updates,\nespecially when new information conflicts with deeply embedded facts. Such\nconflicting factual edits often lead to two critical issues: resistance to\nadopting the new fact and severe catastrophic forgetting of unrelated\nknowledge. This paper introduces and evaluates a novel \"unlearn-then-learn\"\nstrategy for precise knowledge editing in LLMs, leveraging the\nparameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting\nand Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach\nis powered by an initial circuit localization phase that identifies and targets\nthe specific internal components responsible for encoding the conflicting fact.\nThrough a rigorous experimental methodology on\nmicrosoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically\ninformed two-stage approach achieves near-perfect accuracy (98.50%) for the\nnew, modulated fact while simultaneously effectively suppressing the original\nconflicting fact (96.00% forget rate). Critically, our strategy exhibits\nunprecedented localization (72.00% F_control accuracy), dramatically mitigating\ncatastrophic forgetting observed in direct fine-tuning approaches (which showed\nas low as ~20% F_control accuracy), a direct benefit of our targeted\ninterpretability-guided intervention. Furthermore, qualitative analysis reveals\na nuanced mechanism of \"soft forgetting,\" where original knowledge is\nsuppressed from default retrieval but remains latent and conditionally\naccessible, enhancing model safety and control. These findings represent a\nsignificant advancement towards precise, localized, and safe knowledge\nmanagement in compact LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5148\u9057\u5fd8\u540e\u5b66\u4e60\u201d\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08$IA^3$\uff09\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u4e2d\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65b0\u77e5\u8bc6\u7684\u51c6\u786e\u6027\u548c\u65e7\u77e5\u8bc6\u7684\u9057\u5fd8\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u65f6\u96be\u4ee5\u5904\u7406\u65b0\u65e7\u77e5\u8bc6\u51b2\u7a81\uff0c\u5bfc\u81f4\u65b0\u77e5\u8bc6\u96be\u4ee5\u91c7\u7eb3\u548c\u65e0\u5173\u77e5\u8bc6\u7684\u4e25\u91cd\u9057\u5fd8\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a1) \u5b9a\u4f4d\u51b2\u7a81\u77e5\u8bc6\u7684\u5185\u90e8\u7f16\u7801\u7ec4\u4ef6\uff1b2) \u4f7f\u7528$IA^3$\u6280\u672f\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65b0\u77e5\u8bc6\u51c6\u786e\u7387\uff0898.50%\uff09\u548c\u65e7\u77e5\u8bc6\u9057\u5fd8\u7387\uff0896.00%\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u663e\u8457\u7f13\u89e3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cbe\u51c6\u3001\u5c40\u90e8\u5316\u548c\u5b89\u5168\u7684\u77e5\u8bc6\u7ba1\u7406\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.07085", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07085", "abs": "https://arxiv.org/abs/2508.07085", "authors": ["N Harshit", "K Mounvik"], "title": "Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework", "comment": null, "summary": "In applied machine learning, concept drift, which is either gradual or abrupt\nchanges in data distribution, can significantly reduce model performance.\nTypical detection methods,such as statistical tests or reconstruction-based\nmodels,are generally reactive and not very sensitive to early detection. Our\nstudy proposes a hybrid framework consisting of Transformers and Autoencoders\nto model complex temporal dynamics and provide online drift detection. We\ncreate a distinct Trust Score methodology, which includes signals on (1)\nstatistical and reconstruction-based drift metrics, more specifically, PSI,\nJSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,\nand (4) trend of classifier error aligned with the combined metrics defined by\nthe Trust Score. Using a time sequenced airline passenger data set with\nsynthetic drift, our proposed model allows for a better detection of drift\nusing as a whole and at different detection thresholds for both sensitivity and\ninterpretability compared to baseline methods and provides a strong pipeline\nfor drift detection in real time for applied machine learning. We evaluated\nperformance using a time-sequenced airline passenger dataset having the\ngradually injected stimulus of drift in expectations,e.g. permuted ticket\nprices in later batches, broken into 10 time segments [1].In the data, our\nresults support that the Transformation-Autoencoder detected drift earlier and\nwith more sensitivity than the autoencoders commonly used in the literature,\nand provided improved modeling over more error rates and logical violations.\nTherefore, a robust framework was developed to reliably monitor concept drift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u548cAutoencoder\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\uff0c\u5e76\u901a\u8fc7Trust Score\u65b9\u6cd5\u63d0\u9ad8\u68c0\u6d4b\u654f\u611f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6982\u5ff5\u6f02\u79fb\uff08\u6570\u636e\u5206\u5e03\u7684\u53d8\u5316\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u591a\u4e3a\u88ab\u52a8\u4e14\u5bf9\u65e9\u671f\u68c0\u6d4b\u4e0d\u654f\u611f\u3002", "method": "\u4f7f\u7528Transformer\u548cAutoencoder\u5efa\u6a21\u590d\u6742\u65f6\u95f4\u52a8\u6001\uff0c\u7ed3\u5408Trust Score\u65b9\u6cd5\uff08\u5305\u62ec\u7edf\u8ba1\u548c\u91cd\u6784\u6307\u6807\u3001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3001\u89c4\u5219\u8fdd\u53cd\u548c\u5206\u7c7b\u5668\u8bef\u5dee\u8d8b\u52bf\uff09\u3002", "result": "\u5728\u822a\u7a7a\u4e58\u5ba2\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u65e9\u3001\u66f4\u654f\u611f\u5730\u68c0\u6d4b\u5230\u6f02\u79fb\uff0c\u5e76\u51cf\u5c11\u4e86\u9519\u8bef\u7387\u548c\u903b\u8f91\u8fdd\u53cd\u3002", "conclusion": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u6982\u5ff5\u6f02\u79fb\u3002"}}
{"id": "2508.07102", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07102", "abs": "https://arxiv.org/abs/2508.07102", "authors": ["Yang Cao", "Yubin Chen", "Zhao Song", "Jiahao Zhang"], "title": "Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria", "comment": null, "summary": "Generative modelling has seen significant advances through simulation-free\nparadigms such as Flow Matching, and in particular, the MeanFlow framework,\nwhich replaces instantaneous velocity fields with average velocities to enable\nefficient single-step sampling. In this work, we introduce a theoretical study\non Second-Order MeanFlow, a novel extension that incorporates average\nacceleration fields into the MeanFlow objective. We first establish the\nfeasibility of our approach by proving that the average acceleration satisfies\na generalized consistency condition analogous to first-order MeanFlow, thereby\nsupporting stable, one-step sampling and tractable loss functions. We then\ncharacterize its expressivity via circuit complexity analysis, showing that\nunder mild assumptions, the Second-Order MeanFlow sampling process can be\nimplemented by uniform threshold circuits within the $\\mathsf{TC}^0$ class.\nFinally, we derive provably efficient criteria for scalable implementation by\nleveraging fast approximate attention computations: we prove that attention\noperations within the Second-Order MeanFlow architecture can be approximated to\nwithin $1/\\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results\nlay the theoretical foundation for high-order flow matching models that combine\nrich dynamics with practical sampling efficiency.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07106", "abs": "https://arxiv.org/abs/2508.07106", "authors": ["Yiran Huang", "Amirhossein Nouranizadeh", "Christine Ahrends", "Mengjia Xu"], "title": "BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation", "comment": null, "summary": "Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely\nused to study human brain activity. fMRI signals in areas across the brain\ntransiently synchronise and desynchronise their activity in a highly structured\nmanner, even when an individual is at rest. These functional connectivity\ndynamics may be related to behaviour and neuropsychiatric disease. To model\nthese dynamics, temporal brain connectivity representations are essential, as\nthey reflect evolving interactions between brain regions and provide insight\ninto transient neural states and network reconfigurations. However,\nconventional graph neural networks (GNNs) often struggle to capture long-range\ntemporal dependencies in dynamic fMRI data. To address this challenge, we\npropose BrainATCL, an unsupervised, nonparametric framework for adaptive\ntemporal brain connectivity learning, enabling functional link prediction and\nage estimation. Our method dynamically adjusts the lookback window for each\nsnapshot based on the rate of newly added edges. Graph sequences are\nsubsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal\nrepresentations of dynamic functional connectivity in resting-state fMRI data\nof 1,000 participants from the Human Connectome Project. To further improve\nspatial modeling, we incorporate brain structure and function-informed edge\nattributes, i.e., the left/right hemispheric identity and subnetwork membership\nof brain regions, enabling the model to capture biologically meaningful\ntopological patterns. We evaluate our BrainATCL on two tasks: functional link\nprediction and age estimation. The experimental results demonstrate superior\nperformance and strong generalization, including in cross-session prediction\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBrainATCL\u7684\u65e0\u76d1\u7763\u975e\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001fMRI\u6570\u636e\u7684\u81ea\u9002\u5e94\u65f6\u95f4\u8111\u8fde\u63a5\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGNN\u5728\u6355\u6349\u957f\u65f6\u7a0b\u4f9d\u8d56\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "fMRI\u4fe1\u53f7\u7684\u529f\u80fd\u8fde\u63a5\u52a8\u6001\u53ef\u80fd\u4e0e\u884c\u4e3a\u548c\u795e\u7ecf\u7cbe\u795e\u75be\u75c5\u76f8\u5173\uff0c\u4f46\u4f20\u7edfGNN\u96be\u4ee5\u6355\u6349\u52a8\u6001fMRI\u6570\u636e\u4e2d\u7684\u957f\u65f6\u7a0b\u4f9d\u8d56\u3002", "method": "BrainATCL\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6bcf\u4e2a\u5feb\u7167\u7684\u56de\u6eaf\u7a97\u53e3\uff0c\u7ed3\u5408GINE-Mamba2\u9aa8\u5e72\u7f51\u7edc\u5b66\u4e60\u65f6\u7a7a\u8868\u793a\uff0c\u5e76\u878d\u5165\u8111\u7ed3\u6784\u548c\u529f\u80fd\u4fe1\u606f\u3002", "result": "\u5728\u529f\u80fd\u94fe\u63a5\u9884\u6d4b\u548c\u5e74\u9f84\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BrainATCL\u4e3a\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2508.07114", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.07114", "abs": "https://arxiv.org/abs/2508.07114", "authors": ["Atakan Azakli", "Bernd Stelzer"], "title": "Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning", "comment": null, "summary": "In this work, we propose a new machine learning (ML) methodology to obtain\nmore precise predictions for some parameters of interest in a given hypotheses\ntesting problem. Our proposed method also allows ML models to have more\ndiscriminative power in cases where it is extremely challenging for\nstate-of-the-art classifiers to have any level of accurate predictions. This\nmethod can also allow us to systematically decrease the error from ML models in\ntheir predictions. In this paper, we provide a mathematical motivation why\nMultiple Instance Learning (MIL) would have more predictive power over their\nsingle-instance counterparts. We support our theoretical claims by analyzing\nthe behavior of the MIL models through their scaling behaviors with respect to\nthe number of instances on which the model makes predictions. As a concrete\napplication, we constrain Wilson coefficients of the Standard Model Effective\nField Theory (SMEFT) using kinematic information from subatomic particle\ncollision events at the Large Hadron Collider (LHC). We show that under certain\ncircumstances, it might be possible to extract the theoretical maximum Fisher\nInformation latent in a dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u63d0\u9ad8\u5047\u8bbe\u68c0\u9a8c\u4e2d\u53c2\u6570\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u964d\u4f4e\u6a21\u578b\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5206\u7c7b\u5668\u5728\u6781\u7aef\u60c5\u51b5\u4e0b\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u51cf\u5c11\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "method": "\u91c7\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u76f8\u5bf9\u4e8e\u5355\u5b9e\u4f8b\u5b66\u4e60\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u6570\u91cf\u7684\u7f29\u653e\u884c\u4e3a\u9a8c\u8bc1\u7406\u8bba\u3002", "result": "\u5728\u6807\u51c6\u6a21\u578b\u6709\u6548\u573a\u8bba\uff08SMEFT\uff09\u7684\u7ea6\u675f\u5e94\u7528\u4e2d\uff0c\u5c55\u793a\u4e86MIL\u65b9\u6cd5\u53ef\u80fd\u63d0\u53d6\u6570\u636e\u96c6\u4e2d\u7684\u7406\u8bba\u6700\u5927Fisher\u4fe1\u606f\u3002", "conclusion": "MIL\u65b9\u6cd5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u53ef\u80fd\u8fbe\u5230\u7406\u8bba\u6700\u4f18\u7684\u4fe1\u606f\u63d0\u53d6\u6548\u679c\u3002"}}
{"id": "2508.07117", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07117", "abs": "https://arxiv.org/abs/2508.07117", "authors": ["Peyman Baghershahi", "Gregoire Fournier", "Pranav Nyati", "Sourav Medya"], "title": "From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context", "comment": "18 pages, 3 figures, 8 tables", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over\nstructured data, including text-attributed graphs, which are common in domains\nsuch as citation networks, social platforms, and knowledge graphs. GNNs are not\ninherently interpretable and thus, many explanation methods have been proposed.\nHowever, existing explanation methods often struggle to generate interpretable,\nfine-grained rationales, especially when node attributes include rich natural\nlanguage. In this work, we introduce LOGIC, a lightweight, post-hoc framework\nthat uses large language models (LLMs) to generate faithful and interpretable\nexplanations for GNN predictions. LOGIC projects GNN node embeddings into the\nLLM embedding space and constructs hybrid prompts that interleave soft prompts\nwith textual inputs from the graph structure. This enables the LLM to reason\nabout GNN internal representations and produce natural language explanations\nalong with concise explanation subgraphs. Our experiments across four\nreal-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off\nbetween fidelity and sparsity, while significantly improving human-centric\nmetrics such as insightfulness. LOGIC sets a new direction for LLM-based\nexplainability in graph learning by aligning GNN internals with human\nreasoning.", "AI": {"tldr": "LOGIC\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u7528LLM\u4e3aGNN\u9884\u6d4b\u751f\u6210\u5fe0\u5b9e\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u89e3\u91ca\uff0c\u5c24\u5176\u662f\u5728\u8282\u70b9\u5c5e\u6027\u5305\u542b\u4e30\u5bcc\u81ea\u7136\u8bed\u8a00\u65f6\u3002", "method": "LOGIC\u5c06GNN\u8282\u70b9\u5d4c\u5165\u6295\u5f71\u5230LLM\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u6784\u5efa\u6df7\u5408\u63d0\u793a\uff0c\u7ed3\u5408\u8f6f\u63d0\u793a\u548c\u6587\u672c\u8f93\u5165\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9eTAG\u6570\u636e\u96c6\u4e0a\uff0cLOGIC\u5728\u5fe0\u5b9e\u6027\u548c\u7a00\u758f\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4eba\u7c7b\u4e2d\u5fc3\u6307\u6807\u3002", "conclusion": "LOGIC\u4e3a\u57fa\u4e8eLLM\u7684\u56fe\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u8bbe\u5b9a\u4e86\u65b0\u65b9\u5411\uff0c\u5c06GNN\u5185\u90e8\u8868\u793a\u4e0e\u4eba\u7c7b\u63a8\u7406\u5bf9\u9f50\u3002"}}
{"id": "2508.07122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07122", "abs": "https://arxiv.org/abs/2508.07122", "authors": ["Zhihao Xue", "Yun Zi", "Nia Qi", "Ming Gong", "Yujun Zou"], "title": "Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks", "comment": null, "summary": "This paper proposes a spatiotemporal graph neural network-based performance\nprediction algorithm to address the challenge of forecasting performance\nfluctuations in distributed backend systems with multi-level service call\nstructures. The method abstracts system states at different time slices into a\nsequence of graph structures. It integrates the runtime features of service\nnodes with the invocation relationships among services to construct a unified\nspatiotemporal modeling framework. The model first applies a graph\nconvolutional network to extract high-order dependency information from the\nservice topology. Then it uses a gated recurrent network to capture the dynamic\nevolution of performance metrics over time. A time encoding mechanism is also\nintroduced to enhance the model's ability to represent non-stationary temporal\nsequences. The architecture is trained in an end-to-end manner, optimizing the\nmulti-layer nested structure to achieve high-precision regression of future\nservice performance metrics. To validate the effectiveness of the proposed\nmethod, a large-scale public cluster dataset is used. A series of\nmulti-dimensional experiments are designed, including variations in time\nwindows and concurrent load levels. These experiments comprehensively evaluate\nthe model's predictive performance and stability. The experimental results show\nthat the proposed model outperforms existing representative methods across key\nmetrics such as MAE, RMSE, and R2. It maintains strong robustness under varying\nload intensities and structural complexities. These results demonstrate the\nmodel's practical potential for backend service performance management tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u9884\u6d4b\u7b97\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5177\u6709\u591a\u7ea7\u670d\u52a1\u8c03\u7528\u7ed3\u6784\u7684\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u7684\u6027\u80fd\u6ce2\u52a8\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u4e2d\u6027\u80fd\u6ce2\u52a8\u7684\u9884\u6d4b\u6311\u6218\uff0c\u5c24\u5176\u662f\u591a\u7ea7\u670d\u52a1\u8c03\u7528\u7ed3\u6784\u7684\u590d\u6742\u6027\u3002", "method": "\u5c06\u7cfb\u7edf\u72b6\u6001\u62bd\u8c61\u4e3a\u56fe\u7ed3\u6784\u5e8f\u5217\uff0c\u7ed3\u5408\u8fd0\u884c\u65f6\u7279\u5f81\u548c\u670d\u52a1\u8c03\u7528\u5173\u7cfb\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u65f6\u7a7a\u5efa\u6a21\u6846\u67b6\uff0c\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u63d0\u53d6\u4f9d\u8d56\u4fe1\u606f\uff0c\u95e8\u63a7\u5faa\u73af\u7f51\u7edc\u6355\u6349\u52a8\u6001\u6f14\u5316\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u7f16\u7801\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728MAE\u3001RMSE\u548cR2\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u8d1f\u8f7d\u548c\u7ed3\u6784\u53d8\u5316\u4e0b\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u540e\u53f0\u670d\u52a1\u6027\u80fd\u7ba1\u7406\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07126", "abs": "https://arxiv.org/abs/2508.07126", "authors": ["Zhengran Ji", "Boyuan Chen"], "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning", "comment": null, "summary": "Training reinforcement learning agents with human feedback is crucial when\ntask objectives are difficult to specify through dense reward functions. While\nprior methods rely on offline trajectory comparisons to elicit human\npreferences, such data is unavailable in online learning scenarios where agents\nmust adapt on the fly. Recent approaches address this by collecting real-time\nscalar feedback to guide agent behavior and train reward models for continued\nlearning after human feedback becomes unavailable. However, scalar feedback is\noften noisy and inconsistent, limiting the accuracy and generalization of\nlearned rewards. We propose Pref-GUIDE, a framework that transforms real-time\nscalar feedback into preference-based data to improve reward model learning for\ncontinual policy training. Pref-GUIDE Individual mitigates temporal\ninconsistency by comparing agent behaviors within short windows and filtering\nambiguous feedback. Pref-GUIDE Voting further enhances robustness by\naggregating reward models across a population of users to form consensus\npreferences. Across three challenging environments, Pref-GUIDE significantly\noutperforms scalar-feedback baselines, with the voting variant exceeding even\nexpert-designed dense rewards. By reframing scalar feedback as structured\npreferences with population feedback, Pref-GUIDE offers a scalable and\nprincipled approach for harnessing human input in online reinforcement\nlearning.", "AI": {"tldr": "Pref-GUIDE\u6846\u67b6\u5c06\u5b9e\u65f6\u6807\u91cf\u53cd\u9988\u8f6c\u5316\u4e3a\u504f\u597d\u6570\u636e\uff0c\u63d0\u5347\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u6548\u679c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6807\u91cf\u53cd\u9988\u65b9\u6cd5\u3002", "motivation": "\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u6807\u91cf\u53cd\u9988\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u5956\u52b1\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "Pref-GUIDE\u901a\u8fc7\u77ed\u7a97\u53e3\u884c\u4e3a\u6bd4\u8f83\u548c\u8fc7\u6ee4\u6a21\u7cca\u53cd\u9988\uff08Individual\uff09\u53ca\u7528\u6237\u7fa4\u4f53\u5171\u8bc6\u504f\u597d\uff08Voting\uff09\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u6311\u6218\u6027\u73af\u5883\u4e2d\uff0cPref-GUIDE\u663e\u8457\u4f18\u4e8e\u6807\u91cf\u53cd\u9988\u57fa\u7ebf\uff0cVoting\u7248\u672c\u751a\u81f3\u8d85\u8fc7\u4e13\u5bb6\u8bbe\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\u3002", "conclusion": "Pref-GUIDE\u901a\u8fc7\u7ed3\u6784\u5316\u504f\u597d\u548c\u7fa4\u4f53\u53cd\u9988\uff0c\u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u539f\u5219\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07127", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2508.07127", "abs": "https://arxiv.org/abs/2508.07127", "authors": ["Niranjana Arun Menon", "Iqra Farooq", "Yulong Li", "Sara Ahmed", "Yutong Xie", "Muhammad Awais", "Imran Razzak"], "title": "How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?", "comment": null, "summary": "Cardiovascular disease (CVD) prediction remains a tremendous challenge due to\nits multifactorial etiology and global burden of morbidity and mortality.\nDespite the growing availability of genomic and electrophysiological data,\nextracting biologically meaningful insights from such high-dimensional, noisy,\nand sparsely annotated datasets remains a non-trivial task. Recently, LLMs has\nbeen applied effectively to predict structural variations in biological\nsequences. In this work, we explore the potential of fine-tuned LLMs to predict\ncardiac diseases and SNPs potentially leading to CVD risk using genetic markers\nderived from high-throughput genomic profiling. We investigate the effect of\ngenetic patterns associated with cardiac conditions and evaluate how LLMs can\nlearn latent biological relationships from structured and semi-structured\ngenomic data obtained by mapping genetic aspects that are inherited from the\nfamily tree. By framing the problem as a Chain of Thought (CoT) reasoning task,\nthe models are prompted to generate disease labels and articulate informed\nclinical deductions across diverse patient profiles and phenotypes. The\nfindings highlight the promise of LLMs in contributing to early detection, risk\nassessment, and ultimately, the advancement of personalized medicine in cardiac\ncare.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9884\u6d4b\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u53ca\u5176\u76f8\u5173SNPs\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u57fa\u56e0\u7ec4\u6570\u636e\u63d0\u53d6\u751f\u7269\u5b66\u610f\u4e49\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u4e2a\u6027\u5316\u533b\u7597\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u9884\u6d4b\u56e0\u591a\u56e0\u7d20\u75c5\u56e0\u548c\u9ad8\u7ef4\u566a\u58f0\u6570\u636e\u800c\u5177\u6709\u6311\u6218\u6027\uff0cLLMs\u5728\u751f\u7269\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u6210\u529f\u5e94\u7528\u6fc0\u53d1\u4e86\u5176\u5728CVD\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5fae\u8c03\u7684LLMs\uff0c\u901a\u8fc7Chain of Thought\uff08CoT\uff09\u63a8\u7406\u4efb\u52a1\uff0c\u4ece\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316\u57fa\u56e0\u7ec4\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u751f\u7269\u5b66\u5173\u7cfb\uff0c\u9884\u6d4b\u75be\u75c5\u6807\u7b7e\u548c\u4e34\u5e8a\u63a8\u8bba\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u65e9\u671f\u68c0\u6d4b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u4e2a\u6027\u5316\u533b\u7597\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u7ef4\u57fa\u56e0\u7ec4\u6570\u636e\u3002", "conclusion": "LLMs\u4e3a\u5fc3\u8840\u7ba1\u75be\u75c5\u7684\u9884\u6d4b\u548c\u4e2a\u6027\u5316\u533b\u7597\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2508.07134", "categories": ["cs.LG", "cs.DM", "15A23, 90C26, 62H25", "I.2.6; I.5.3"], "pdf": "https://arxiv.org/pdf/2508.07134", "abs": "https://arxiv.org/abs/2508.07134", "authors": ["Lu Chenggang"], "title": "A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs", "comment": "10 pages, 2 figures, under review in [SIAM Journal of Optimization]", "summary": "Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical\nNonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain\nboth positive and negative entries, making it suitable for decomposing data\nwith mixed signs. However, most existing semi-NMF algorithms are iterative,\nnon-convex, and prone to local minima. In this paper, we propose a novel method\nthat yields a globally optimal solution to the semi-NMF problem under the\nFrobenius norm, through an orthogonal decomposition derived from the scatter\nmatrix of the input data. We rigorously prove that our solution attains the\nglobal minimum of the reconstruction error. Furthermore, we demonstrate that\nwhen the input matrix is nonnegative, our method often achieves lower\nreconstruction error than standard NMF algorithms, although unfortunately the\nbasis matrix may not satisfy nonnegativity. In particular, in low-rank cases\nsuch as rank 1 or 2, our solution reduces exactly to a nonnegative\nfactorization, recovering the NMF structure. We validate our approach through\nexperiments on both synthetic data and the UCI Wine dataset, showing that our\nmethod consistently outperforms existing NMF and semi-NMF methods in terms of\nreconstruction accuracy. These results confirm that our globally optimal,\nnon-iterative formulation offers both theoretical guarantees and empirical\nadvantages, providing a new perspective on matrix factorization in optimization\nand data analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534a\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08semi-NMF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u5206\u89e3\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u534aNMF\u7b97\u6cd5\u591a\u4e3a\u8fed\u4ee3\u3001\u975e\u51f8\u4e14\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u57fa\u4e8e\u8f93\u5165\u6570\u636e\u7684\u6563\u5e03\u77e9\u9635\uff0c\u901a\u8fc7\u6b63\u4ea4\u5206\u89e3\u83b7\u5f97\u5168\u5c40\u6700\u4f18\u89e3\u3002", "result": "\u5728Frobenius\u8303\u6570\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u91cd\u6784\u8bef\u5dee\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u4f18\u52bf\uff0c\u4e3a\u77e9\u9635\u5206\u89e3\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.07137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07137", "abs": "https://arxiv.org/abs/2508.07137", "authors": ["Yuandong Tan"], "title": "A Stable and Principled Loss Function for Direct Language Model Alignment", "comment": null, "summary": "The alignment of large language models (LLMs) with human preferences is\ncommonly achieved through Reinforcement Learning from Human Feedback (RLHF).\nDirect Preference Optimization (DPO) simplified this paradigm by establishing a\ndirect mapping between the optimal policy and a reward function, eliminating\nthe need for an explicit reward model. However, we argue that the DPO loss\nfunction is theoretically misaligned with its own derivation, as it promotes\nthe indefinite maximization of a logits difference, which can lead to training\ninstability and reward hacking. In this paper, we propose a novel loss function\nderived directly from the RLHF optimality condition. Our proposed loss targets\na specific, finite value for the logits difference, which is dictated by the\nunderlying reward, rather than its maximization. We provide a theoretical\nanalysis, including a gradient-based comparison, to demonstrate that our method\navoids the large gradients that plague DPO when the probability of dispreferred\nresponses approaches zero. This inherent stability prevents reward hacking and\nleads to more effective alignment. We validate our approach by fine-tuning a\nQwen2.5-7B model, showing significant win-rate improvements over a standard DPO\nbaseline and achieving competitive performance against larger models like\nLlama-3.1-8B.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86DPO\u65b9\u6cd5\u4e2d\u56e0\u65e0\u9650\u6700\u5927\u5316\u5bf9\u6570\u5dee\u5f02\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "DPO\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u4e0e\u5176\u63a8\u5bfc\u4e0d\u4e00\u81f4\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4eceRLHF\u6700\u4f18\u6761\u4ef6\u76f4\u63a5\u63a8\u5bfc\u51fa\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u9488\u5bf9\u5bf9\u6570\u5dee\u5f02\u8bbe\u5b9a\u6709\u9650\u76ee\u6807\u503c\uff0c\u800c\u975e\u6700\u5927\u5316\u3002", "result": "\u65b0\u65b9\u6cd5\u5728Qwen2.5-7B\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6DPO\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u7684Llama-3.1-8B\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u66f4\u7a33\u5b9a\u4e14\u6709\u6548\uff0c\u907f\u514d\u4e86DPO\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2508.07138", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07138", "abs": "https://arxiv.org/abs/2508.07138", "authors": ["Yashwant Krishna Pagoti", "Arunesh Sinha", "Shamik Sural"], "title": "Strategic Incentivization for Locally Differentially Private Federated Learning", "comment": null, "summary": "In Federated Learning (FL), multiple clients jointly train a machine learning\nmodel by sharing gradient information, instead of raw data, with a server over\nmultiple rounds. To address the possibility of information leakage in spite of\nsharing only the gradients, Local Differential Privacy (LDP) is often used. In\nLDP, clients add a selective amount of noise to the gradients before sending\nthe same to the server. Although such noise addition protects the privacy of\nclients, it leads to a degradation in global model accuracy. In this paper, we\nmodel this privacy-accuracy trade-off as a game, where the sever incentivizes\nthe clients to add a lower degree of noise for achieving higher accuracy, while\nthe clients attempt to preserve their privacy at the cost of a potential loss\nin accuracy. A token based incentivization mechanism is introduced in which the\nquantum of tokens credited to a client in an FL round is a function of the\ndegree of perturbation of its gradients. The client can later access a newly\nupdated global model only after acquiring enough tokens, which are to be\ndeducted from its balance. We identify the players, their actions and payoff,\nand perform a strategic analysis of the game. Extensive experiments were\ncarried out to study the impact of different parameters.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u6e38\u620f\u7406\u8bba\u5206\u6790\u4f18\u5316\u566a\u58f0\u6dfb\u52a0\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u56e0\u5c40\u90e8\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u566a\u58f0\u6dfb\u52a0\u5bfc\u81f4\u7684\u5168\u5c40\u6a21\u578b\u51c6\u786e\u6027\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5c06\u9690\u79c1-\u51c6\u786e\u6027\u6743\u8861\u5efa\u6a21\u4e3a\u6e38\u620f\uff0c\u5f15\u5165\u4ee4\u724c\u6fc0\u52b1\u673a\u5236\uff0c\u6fc0\u52b1\u5ba2\u6237\u7aef\u51cf\u5c11\u566a\u58f0\u6dfb\u52a0\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u53c2\u6570\u5bf9\u6e38\u620f\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u4f18\u5316\u4e86\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u4ee4\u724c\u6fc0\u52b1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u4e0e\u51c6\u786e\u6027\u51b2\u7a81\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2508.07142", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NA", "math.IT", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.07142", "abs": "https://arxiv.org/abs/2508.07142", "authors": ["Vincent-Daniel Yun"], "title": "SGD Convergence under Stepsize Shrinkage in Low-Precision Training", "comment": null, "summary": "Low-precision training has become essential for reducing the computational\nand memory costs of large-scale deep learning. However, quantization of\ngradients introduces both magnitude shrinkage and additive noise, which can\nalter the convergence behavior of stochastic gradient descent (SGD). In this\nwork, we study the convergence of SGD under a gradient shrinkage model, where\neach stochastic gradient is scaled by a factor $q_k \\in (0,1]$ and perturbed by\nzero-mean quantization noise. We show that this shrinkage is equivalent to\nreplacing the nominal stepsize $\\mu_k$ with an effective stepsize $\\mu_k q_k$,\nwhich slows convergence when $q_{\\min} < 1$. Under standard smoothness and\nbounded-variance assumptions, we prove that low-precision SGD still converges,\nbut at a reduced rate determined by $q_{\\min}$, and with an increased\nasymptotic error floor due to quantization noise. We theoretically analyze how\nreduced numerical precision slows down training by modeling it as gradient\nshrinkage in the standard SGD convergence framework.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u4e2d\u68af\u5ea6\u91cf\u5316\u5bf9SGD\u6536\u655b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u68af\u5ea6\u6536\u7f29\u548c\u91cf\u5316\u566a\u58f0\u4f1a\u51cf\u7f13\u6536\u655b\u901f\u5ea6\u5e76\u589e\u52a0\u8bef\u5dee\u3002", "motivation": "\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u867d\u80fd\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u4f46\u68af\u5ea6\u91cf\u5316\u4f1a\u5f15\u5165\u6536\u7f29\u548c\u566a\u58f0\uff0c\u53ef\u80fd\u5f71\u54cdSGD\u7684\u6536\u655b\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u6536\u7f29\u6a21\u578b\u5206\u6790SGD\u6536\u655b\uff0c\u5c06\u91cf\u5316\u68af\u5ea6\u89c6\u4e3a\u7f29\u653e\u548c\u566a\u58f0\u6270\u52a8\uff0c\u63a8\u5bfc\u6709\u6548\u6b65\u957f\u548c\u6536\u655b\u901f\u7387\u3002", "result": "\u4f4e\u7cbe\u5ea6SGD\u4ecd\u80fd\u6536\u655b\uff0c\u4f46\u901f\u7387\u53d7\u6700\u5c0f\u6536\u7f29\u56e0\u5b50\u5f71\u54cd\uff0c\u4e14\u91cf\u5316\u566a\u58f0\u4f1a\u589e\u52a0\u6e10\u8fd1\u8bef\u5dee\u3002", "conclusion": "\u91cf\u5316\u901a\u8fc7\u68af\u5ea6\u6536\u7f29\u548c\u566a\u58f0\u51cf\u7f13\u8bad\u7ec3\u901f\u5ea6\uff0c\u9700\u6743\u8861\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002"}}
{"id": "2508.07208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07208", "abs": "https://arxiv.org/abs/2508.07208", "authors": ["Chanakya Ekbote", "Marco Bondaschi", "Nived Rajaraman", "Jason D. Lee", "Michael Gastpar", "Ashok Vardhan Makkuva", "Paul Pu Liang"], "title": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains", "comment": null, "summary": "In-context learning (ICL) is a hallmark capability of transformers, through\nwhich trained models learn to adapt to new tasks by leveraging information from\nthe input context. Prior work has shown that ICL emerges in transformers due to\nthe presence of special circuits called induction heads. Given the equivalence\nbetween induction heads and conditional k-grams, a recent line of work modeling\nsequential inputs as Markov processes has revealed the fundamental impact of\nmodel depth on its ICL capabilities: while a two-layer transformer can\nefficiently represent a conditional 1-gram model, its single-layer counterpart\ncannot solve the task unless it is exponentially large. However, for higher\norder Markov sources, the best known constructions require at least three\nlayers (each with a single attention head) - leaving open the question: can a\ntwo-layer single-head transformer represent any kth-order Markov process? In\nthis paper, we precisely address this and theoretically show that a two-layer\ntransformer with one head per layer can indeed represent any conditional\nk-gram. Thus, our result provides the tightest known characterization of the\ninterplay between transformer depth and Markov order for ICL. Building on this,\nwe further analyze the learning dynamics of our two-layer construction,\nfocusing on a simplified variant for first-order Markov chains, illustrating\nhow effective in-context representations emerge during training. Together,\nthese results deepen our current understanding of transformer-based ICL and\nillustrate how even shallow architectures can surprisingly exhibit strong ICL\ncapabilities on structured sequence modeling tasks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e24\u5c42\u5355\u5934Transformer\u662f\u5426\u80fd\u8868\u793a\u4efb\u4f55k\u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u5e76\u8bc1\u660e\u5176\u53ef\u4ee5\u8868\u793a\u6761\u4ef6k-gram\uff0c\u63ed\u793a\u4e86Transformer\u6df1\u5ea6\u4e0eICL\u80fd\u529b\u7684\u7d27\u5bc6\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76Transformer\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u6d45\u5c42\u67b6\u6784\u662f\u5426\u80fd\u591f\u9ad8\u6548\u8868\u793a\u9ad8\u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e24\u5c42\u5355\u5934Transformer\u53ef\u4ee5\u8868\u793a\u4efb\u4f55\u6761\u4ef6k-gram\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u5176\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u8bc1\u660e\u4e86\u4e24\u5c42\u5355\u5934Transformer\u80fd\u591f\u8868\u793a\u9ad8\u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6709\u6548\u4e0a\u4e0b\u6587\u8868\u793a\u7684\u5f62\u6210\u3002", "conclusion": "\u6d45\u5c42Transformer\u67b6\u6784\u5728\u7ed3\u6784\u5316\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684ICL\u80fd\u529b\uff0c\u6df1\u5316\u4e86\u5bf9Transformer ICL\u7684\u7406\u89e3\u3002"}}
{"id": "2508.07220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07220", "abs": "https://arxiv.org/abs/2508.07220", "authors": ["Jian Xu", "Yican Liu", "Qibin Zhao", "John Paisley", "Delu Zeng"], "title": "Neural Bridge Processes", "comment": null, "summary": "Learning stochastic functions from partially observed context-target pairs is\na fundamental problem in probabilistic modeling. Traditional models like\nGaussian Processes (GPs) face scalability issues with large datasets and assume\nGaussianity, limiting their applicability. While Neural Processes (NPs) offer\nmore flexibility, they struggle with capturing complex, multi-modal target\ndistributions. Neural Diffusion Processes (NDPs) enhance expressivity through a\nlearned diffusion process but rely solely on conditional signals in the\ndenoising network, resulting in weak input coupling from an unconditional\nforward process and semantic mismatch at the diffusion endpoint. In this work,\nwe propose Neural Bridge Processes (NBPs), a novel method for modeling\nstochastic functions where inputs x act as dynamic anchors for the entire\ndiffusion trajectory. By reformulating the forward kernel to explicitly depend\non x, NBP enforces a constrained path that strictly terminates at the\nsupervised target. This approach not only provides stronger gradient signals\nbut also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG\nsignal regression and image regression tasks, achieving substantial\nimprovements over baselines. These results underscore the effectiveness of\nDDPM-style bridge sampling in enhancing both performance and theoretical\nconsistency for structured prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeural Bridge Processes (NBPs)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u968f\u673a\u51fd\u6570\uff0c\u901a\u8fc7\u52a8\u6001\u951a\u5b9a\u8f93\u5165x\u7684\u6269\u6563\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u7406\u8bba\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u548c\u795e\u7ecf\u8fc7\u7a0b\uff08NPs\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u6216\u590d\u6742\u591a\u6a21\u6001\u5206\u5e03\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u795e\u7ecf\u6269\u6563\u8fc7\u7a0b\uff08NDPs\uff09\u5728\u8f93\u5165\u8026\u5408\u548c\u8bed\u4e49\u5339\u914d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "NBPs\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u524d\u5411\u6838\uff0c\u4f7f\u5176\u663e\u5f0f\u4f9d\u8d56\u4e8e\u8f93\u5165x\uff0c\u4ece\u800c\u7ea6\u675f\u6269\u6563\u8def\u5f84\u4e25\u683c\u7ec8\u6b62\u4e8e\u76d1\u7763\u76ee\u6807\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u68af\u5ea6\u4fe1\u53f7\u5e76\u4fdd\u8bc1\u7ec8\u70b9\u4e00\u81f4\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u3001EEG\u4fe1\u53f7\u56de\u5f52\u548c\u56fe\u50cf\u56de\u5f52\u4efb\u52a1\u4e2d\uff0cNBPs\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NBPs\u901a\u8fc7DDPM\u98ce\u683c\u7684\u6865\u91c7\u6837\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u548c\u7406\u8bba\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.07221", "categories": ["cs.LG", "cs.AI", "cs.MA", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.07221", "abs": "https://arxiv.org/abs/2508.07221", "authors": ["Po-Han Lee", "Yu-Cheng Lin", "Chan-Tung Ku", "Chan Hsu", "Pei-Cing Huang", "Ping-Hsun Wu", "Yihuang Kang"], "title": "LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference", "comment": null, "summary": "Estimating individualized treatment effects from observational data presents\na persistent challenge due to unmeasured confounding and structural bias.\nCausal Machine Learning (causal ML) methods, such as causal trees and doubly\nrobust estimators, provide tools for estimating conditional average treatment\neffects. These methods have limited effectiveness in complex real-world\nenvironments due to the presence of latent confounders or those described in\nunstructured formats. Moreover, reliance on domain experts for confounder\nidentification and rule interpretation introduces high annotation cost and\nscalability concerns. In this work, we proposed Large Language Model-based\nagents for automated confounder discovery and subgroup analysis that integrate\nagents into the causal ML pipeline to simulate domain expertise. Our framework\nsystematically performs subgroup identification and confounding structure\ndiscovery by leveraging the reasoning capabilities of LLM-based agents, which\nreduces human dependency while preserving interpretability. Experiments on\nreal-world medical datasets show that our proposed approach enhances treatment\neffect estimation robustness by narrowing confidence intervals and uncovering\nunrecognized confounding biases. Our findings suggest that LLM-based agents\noffer a promising path toward scalable, trustworthy, and semantically aware\ncausal inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u6df7\u6742\u53d8\u91cf\u548c\u8fdb\u884c\u4e9a\u7ec4\u5206\u6790\uff0c\u4ee5\u63d0\u5347\u56e0\u679c\u673a\u5668\u5b66\u4e60\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u89c2\u6d4b\u6570\u636e\u4e2d\u7684\u4e2a\u4f53\u5316\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u56e0\u672a\u6d4b\u91cf\u7684\u6df7\u6742\u548c\u7ed3\u6784\u504f\u5dee\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u679c\u6709\u9650\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u3002", "method": "\u5229\u7528LLM\u4ee3\u7406\u6a21\u62df\u9886\u57df\u4e13\u5bb6\uff0c\u81ea\u52a8\u53d1\u73b0\u6df7\u6742\u53d8\u91cf\u548c\u4e9a\u7ec4\uff0c\u51cf\u5c11\u4eba\u5de5\u4f9d\u8d56\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7f29\u5c0f\u7f6e\u4fe1\u533a\u95f4\u548c\u53d1\u73b0\u672a\u8bc6\u522b\u7684\u6df7\u6742\u504f\u5dee\uff0c\u589e\u5f3a\u4e86\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LLM\u4ee3\u7406\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u4fe1\u8d56\u4e14\u8bed\u4e49\u611f\u77e5\u7684\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2508.07224", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07224", "abs": "https://arxiv.org/abs/2508.07224", "authors": ["Ananda Prakash Verma"], "title": "EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning", "comment": null, "summary": "We present EDGE, a general-purpose, misconception-aware adaptive learning\nframework composed of four stages: Evaluate (ability and state estimation),\nDiagnose (posterior infer-ence of misconceptions), Generate (counterfactual\nitem synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies\npsychometrics (IRT/Bayesian state space models), cog-nitive diagnostics\n(misconception discovery from distractor patterns and response latencies),\ncontrastive item generation (minimal perturbations that invalidate learner\nshortcuts while pre-serving psychometric validity), and principled scheduling\n(a restless bandit approximation to spaced retrieval). We formalize a composite\nreadiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,\nand derive an index policy that is near-optimal under mild assumptions on\nforgetting and learning gains. We further establish conditions under which\ncounterfactual items provably reduce the posterior probability of a targeted\nmisconception faster than standard practice. The paper focuses on theory and\nimplementable pseudocode; empirical study is left to future work.", "AI": {"tldr": "EDGE\u662f\u4e00\u4e2a\u901a\u7528\u7684\u3001\u57fa\u4e8e\u8bef\u89e3\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u8bc4\u4f30\u3001\u8bca\u65ad\u3001\u751f\u6210\u548c\u7ec3\u4e60\u56db\u4e2a\u9636\u6bb5\uff0c\u7ed3\u5408\u4e86\u5fc3\u7406\u6d4b\u91cf\u5b66\u3001\u8ba4\u77e5\u8bca\u65ad\u548c\u5bf9\u6bd4\u6027\u9898\u76ee\u751f\u6210\u6280\u672f\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u548c\u7ea0\u6b63\u5b66\u4e60\u8005\u7684\u8bef\u89e3\uff0c\u63d0\u5347\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u7ed3\u5408IRT/Bayesian\u6a21\u578b\u3001\u8ba4\u77e5\u8bca\u65ad\u3001\u5bf9\u6bd4\u6027\u9898\u76ee\u751f\u6210\u548c\u57fa\u4e8e\u7d22\u5f15\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86EdgeScore\u6307\u6807\uff0c\u8bc1\u660e\u4e86\u5176\u5355\u8c03\u6027\u548cLipschitz\u8fde\u7eed\u6027\uff0c\u5e76\u63a8\u5bfc\u51fa\u8fd1\u4e4e\u6700\u4f18\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "conclusion": "EDGE\u6846\u67b6\u5728\u7406\u8bba\u4e0a\u548c\u5b9e\u73b0\u4e0a\u4e3a\u7ea0\u6b63\u5b66\u4e60\u8005\u8bef\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u5b9e\u8bc1\u7814\u7a76\u3002"}}
{"id": "2508.07243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07243", "abs": "https://arxiv.org/abs/2508.07243", "authors": ["Chu Zhao", "Eneng Yang", "Yizhou Dang", "Jianzhe Zhao", "Guibing Guo", "Xingwei Wang"], "title": "Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation", "comment": "14 pages, 6 figures, Under-review", "summary": "Heuristic negative sampling enhances recommendation performance by selecting\nnegative samples of varying hardness levels from predefined candidate pools to\nguide the model toward learning more accurate decision boundaries. However, our\nempirical and theoretical analyses reveal that unobserved environmental\nconfounders (e.g., exposure or popularity biases) in candidate pools may cause\nheuristic sampling methods to introduce false hard negatives (FHNS). These\nmisleading samples can encourage the model to learn spurious correlations\ninduced by such confounders, ultimately compromising its generalization ability\nunder distribution shifts. To address this issue, we propose a novel method\nnamed Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing\nnegative samples in the latent space via a conditional diffusion process,\nCNSDiff avoids the bias introduced by predefined candidate pools and thus\nreduces the likelihood of generating FHNS. Moreover, it incorporates a causal\nregularization term to explicitly mitigate the influence of environmental\nconfounders during the negative sampling process, leading to robust negatives\nthat promote out-of-distribution (OOD) generalization. Comprehensive\nexperiments under four representative distribution shift scenarios demonstrate\nthat CNSDiff achieves an average improvement of 13.96% across all evaluation\nmetrics compared to state-of-the-art baselines, verifying its effectiveness and\nrobustness in OOD recommendation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCNSDiff\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u8fc7\u7a0b\u5408\u6210\u8d1f\u6837\u672c\uff0c\u907f\u514d\u9884\u5b9a\u4e49\u5019\u9009\u6c60\u7684\u504f\u5dee\uff0c\u5e76\u5f15\u5165\u56e0\u679c\u6b63\u5219\u5316\u4ee5\u51cf\u5c11\u73af\u5883\u6df7\u6742\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u542f\u53d1\u5f0f\u8d1f\u91c7\u6837\u53ef\u80fd\u56e0\u73af\u5883\u6df7\u6742\u56e0\u7d20\uff08\u5982\u66dd\u5149\u6216\u6d41\u884c\u5ea6\u504f\u5dee\uff09\u5f15\u5165\u865a\u5047\u786c\u8d1f\u6837\u672c\uff08FHNS\uff09\uff0c\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCNSDiff\u65b9\u6cd5\uff0c\u5229\u7528\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u5728\u6f5c\u5728\u7a7a\u95f4\u5408\u6210\u8d1f\u6837\u672c\uff0c\u5e76\u52a0\u5165\u56e0\u679c\u6b63\u5219\u5316\u4ee5\u51cf\u5c11\u6df7\u6742\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u5728\u56db\u79cd\u4ee3\u8868\u6027\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\uff0cCNSDiff\u5e73\u5747\u6027\u80fd\u63d0\u534713.96%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CNSDiff\u901a\u8fc7\u907f\u514d\u5019\u9009\u6c60\u504f\u5dee\u548c\u51cf\u5c11\u6df7\u6742\u56e0\u7d20\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684OOD\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07249", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07249", "abs": "https://arxiv.org/abs/2508.07249", "authors": ["Soumen Pachal", "Mizhaan Prajit Maniyar", "Prashanth L. A"], "title": "Policy Newton methods for Distortion Riskmetrics", "comment": null, "summary": "We consider the problem of risk-sensitive control in a reinforcement learning\n(RL) framework. In particular, we aim to find a risk-optimal policy by\nmaximizing the distortion riskmetric (DRM) of the discounted reward in a finite\nhorizon Markov decision process (MDP). DRMs are a rich class of risk measures\nthat include several well-known risk measures as special cases. We derive a\npolicy Hessian theorem for the DRM objective using the likelihood ratio method.\nUsing this result, we propose a natural DRM Hessian estimator from sample\ntrajectories of the underlying MDP. Next, we present a cubic-regularized policy\nNewton algorithm for solving this problem in an on-policy RL setting using\nestimates of the DRM gradient and Hessian. Our proposed algorithm is shown to\nconverge to an $\\epsilon$-second-order stationary point ($\\epsilon$-SOSP) of\nthe DRM objective, and this guarantee ensures the escaping of saddle points.\nThe sample complexity of our algorithms to find an $ \\epsilon$-SOSP is\n$\\mathcal{O}(\\epsilon^{-3.5})$. Our experiments validate the theoretical\nfindings. To the best of our knowledge, our is the first work to present\nconvergence to an $\\epsilon$-SOSP of a risk-sensitive objective, while existing\nworks in the literature have either shown convergence to a first-order\nstationary point of a risk-sensitive objective, or a SOSP of a risk-neutral\none.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5931\u771f\u98ce\u9669\u5ea6\u91cf\uff08DRM\uff09\u6765\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u5230\u4e8c\u9636\u7a33\u5b9a\u70b9\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u98ce\u9669\u654f\u611f\u63a7\u5236\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u4f18\u5316DRM\u3002", "method": "\u4f7f\u7528\u4f3c\u7136\u6bd4\u65b9\u6cd5\u63a8\u5bfcDRM\u76ee\u6807\u7684\u7b56\u7565Hessian\u5b9a\u7406\uff0c\u63d0\u51fa\u57fa\u4e8e\u6837\u672c\u8f68\u8ff9\u7684Hessian\u4f30\u8ba1\u5668\uff0c\u5e76\u8bbe\u8ba1\u7acb\u65b9\u6b63\u5219\u5316\u7684\u7b56\u7565\u725b\u987f\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u6536\u655b\u5230DRM\u76ee\u6807\u7684\u4e8c\u9636\u7a33\u5b9a\u70b9\uff0c\u6837\u672c\u590d\u6742\u5ea6\u4e3aO(\u03b5^-3.5)\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u98ce\u9669\u654f\u611f\u76ee\u6807\u7684\u4e8c\u9636\u7a33\u5b9a\u70b9\u6536\u655b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.07297", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07297", "abs": "https://arxiv.org/abs/2508.07297", "authors": ["Hongbo Zhu", "Angelo Cangelosi"], "title": "Revisiting Data Attribution for Influence Functions", "comment": null, "summary": "The goal of data attribution is to trace the model's predictions through the\nlearning algorithm and back to its training data. thereby identifying the most\ninfluential training samples and understanding how the model's behavior leads\nto particular predictions. Understanding how individual training examples\ninfluence a model's predictions is fundamental for machine learning\ninterpretability, data debugging, and model accountability. Influence\nfunctions, originating from robust statistics, offer an efficient, first-order\napproximation to estimate the impact of marginally upweighting or removing a\ndata point on a model's learned parameters and its subsequent predictions,\nwithout the need for expensive retraining. This paper comprehensively reviews\nthe data attribution capability of influence functions in deep learning. We\ndiscuss their theoretical foundations, recent algorithmic advances for\nefficient inverse-Hessian-vector product estimation, and evaluate their\neffectiveness for data attribution and mislabel detection. Finally,\nhighlighting current challenges and promising directions for unleashing the\nhuge potential of influence functions in large-scale, real-world deep learning\nscenarios.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f71\u54cd\u51fd\u6570\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f52\u56e0\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u5176\u7406\u8bba\u57fa\u7840\u3001\u9ad8\u6548\u9006Hessian-\u5411\u91cf\u79ef\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u6570\u636e\u5f52\u56e0\u548c\u9519\u8bef\u6807\u7b7e\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u5bf9\u673a\u5668\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u3001\u6570\u636e\u8c03\u8bd5\u548c\u6a21\u578b\u95ee\u8d23\u81f3\u5173\u91cd\u8981\u3002\u5f71\u54cd\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u4f30\u8ba1\u6570\u636e\u70b9\u5bf9\u6a21\u578b\u53c2\u6570\u548c\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u6539\u8fdb\uff08\u5982\u9ad8\u6548\u9006Hessian-\u5411\u91cf\u79ef\u4f30\u8ba1\uff09\uff0c\u7814\u7a76\u5f71\u54cd\u51fd\u6570\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5f71\u54cd\u51fd\u6570\u5728\u6570\u636e\u5f52\u56e0\u548c\u9519\u8bef\u6807\u7b7e\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4f46\u4ecd\u9762\u4e34\u5927\u89c4\u6a21\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "\u5f71\u54cd\u51fd\u6570\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u4ee5\u5145\u5206\u53d1\u6325\u5176\u4f5c\u7528\u3002"}}
{"id": "2508.07299", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07299", "abs": "https://arxiv.org/abs/2508.07299", "authors": ["Lin-Han Jia", "Si-Yu Han", "Wen-Chao Hu", "Jie-Jing Shao", "Wen-Da Wei", "Zhi Zhou", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective", "comment": null, "summary": "Neuro-symbolic (Nesy) learning improves the target task performance of models\nby enabling them to satisfy knowledge, while semi/self-supervised learning\n(SSL) improves the target task performance by designing unsupervised pretext\ntasks for unlabeled data to make models satisfy corresponding assumptions. We\nextend the Nesy theory based on reliable knowledge to the scenario of\nunreliable knowledge (i.e., assumptions), thereby unifying the theoretical\nframeworks of SSL and Nesy. Through rigorous theoretical analysis, we\ndemonstrate that, in theory, the impact of pretext tasks on target performance\nhinges on three factors: knowledge learnability with respect to the model,\nknowledge reliability with respect to the data, and knowledge completeness with\nrespect to the target. We further propose schemes to operationalize these\ntheoretical metrics, and thereby develop a method that can predict the\neffectiveness of pretext tasks in advance. This will change the current status\nquo in practical applications, where the selections of unsupervised tasks are\nheuristic-based rather than theory-based, and it is difficult to evaluate the\nrationality of unsupervised pretext task selection before testing the model on\nthe target task. In experiments, we verify a high correlation between the\npredicted performance-estimated using minimal data-and the actual performance\nachieved after large-scale semi-supervised or self-supervised learning, thus\nconfirming the validity of the theory and the effectiveness of the evaluation\nmethod.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6269\u5c55\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7406\u8bba\uff0c\u7edf\u4e00\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u795e\u7ecf\u7b26\u53f7\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63d0\u51fa\u9884\u6d4b\u65e0\u76d1\u7763\u4efb\u52a1\u6709\u6548\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65e0\u76d1\u7763\u4efb\u52a1\u9009\u62e9\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u77e5\u8bc6\u53ef\u5b66\u4e60\u6027\u3001\u53ef\u9760\u6027\u548c\u5b8c\u6574\u6027\u4e09\u4e2a\u56e0\u7d20\uff0c\u63d0\u51fa\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9884\u6d4b\u6027\u80fd\u4e0e\u5b9e\u9645\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\uff0c\u8bc1\u5b9e\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3a\u65e0\u76d1\u7763\u4efb\u52a1\u9009\u62e9\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07329", "abs": "https://arxiv.org/abs/2508.07329", "authors": ["Tuo Zhang", "Ning Li", "Xin Yuan", "Wenchao Xu", "Quan Chen", "Song Guo", "Haijun Zhang"], "title": "Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative", "comment": null, "summary": "With the breakthrough progress of large language models (LLMs) in natural\nlanguage processing and multimodal tasks, efficiently deploying them on\nresource-constrained edge devices has become a critical challenge. The Mixture\nof Experts (MoE) architecture enhances model capacity through sparse\nactivation, but faces two major difficulties in practical deployment: (1) The\npresence of numerous outliers in activation distributions leads to severe\ndegradation in quantization accuracy for both activations and weights,\nsignificantly impairing inference performance; (2) Under limited memory,\nefficient offloading and collaborative inference of expert modules struggle to\nbalance latency and throughput. To address these issues, this paper proposes an\nefficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)\nand CPU-GPU collaborative inference. First, by introducing smoothed Hessian\nmatrix quantization, we achieve joint 8-bit quantization of activations and\nweights, which significantly alleviates the accuracy loss caused by outliers\nwhile ensuring efficient implementation on mainstream hardware. Second, we\ndesign an expert-level collaborative offloading and inference mechanism, which,\ncombined with expert activation path statistics, enables efficient deployment\nand scheduling of expert modules between CPU and GPU, greatly reducing memory\nfootprint and inference latency. Extensive experiments validate the\neffectiveness of our method on mainstream large models such as the OPT series\nand Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of\nthe low-bit quantized model approaches that of the full-precision model, while\nGPU memory usage is reduced by about 60%, and inference latency is\nsignificantly improved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHessian\u611f\u77e5\u91cf\u5316\uff08HAQ\uff09\u548cCPU-GPU\u534f\u540c\u63a8\u7406\u7684\u9ad8\u6548MoE\u8fb9\u7f18\u90e8\u7f72\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u91cf\u5316\u7cbe\u5ea6\u548c\u5185\u5b58\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u9762\u4e34\u91cf\u5316\u7cbe\u5ea6\u548c\u5185\u5b58\u7ba1\u7406\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528Hessian\u611f\u77e5\u91cf\u5316\uff08HAQ\uff09\u5b9e\u73b08\u4f4d\u91cf\u5316\uff0c\u5e76\u8bbe\u8ba1\u4e13\u5bb6\u7ea7\u534f\u540c\u5378\u8f7d\u4e0e\u63a8\u7406\u673a\u5236\u3002", "result": "\u5728OPT\u7cfb\u5217\u548cMixtral 8*7B\u7b49\u6a21\u578b\u4e0a\uff0c\u91cf\u5316\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\uff0cGPU\u5185\u5b58\u51cf\u5c1160%\uff0c\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MoE\u67b6\u6784\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u4e2d\u7684\u91cf\u5316\u7cbe\u5ea6\u548c\u5185\u5b58\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2508.07333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07333", "abs": "https://arxiv.org/abs/2508.07333", "authors": ["Yuhao Liu", "Rui Hu", "Yu Chen", "Longbo Huang"], "title": "Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants", "comment": null, "summary": "Stochastic interpolants offer a robust framework for continuously\ntransforming samples between arbitrary data distributions, holding significant\npromise for generative modeling. Despite their potential, rigorous finite-time\nconvergence guarantees for practical numerical schemes remain largely\nunexplored. In this work, we address the finite-time convergence analysis of\nnumerical implementations for ordinary differential equations (ODEs) derived\nfrom stochastic interpolants. Specifically, we establish novel finite-time\nerror bounds in total variation distance for two widely used numerical\nintegrators: the first-order forward Euler method and the second-order Heun's\nmethod. Furthermore, our analysis on the iteration complexity of specific\nstochastic interpolant constructions provides optimized schedules to enhance\ncomputational efficiency. Our theoretical findings are corroborated by\nnumerical experiments, which validate the derived error bounds and complexity\nanalyses.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u968f\u673a\u63d2\u503c\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u6570\u503c\u79ef\u5206\u65b9\u6cd5\uff08\u524d\u5411\u6b27\u62c9\u6cd5\u548cHeun\u6cd5\uff09\u7684\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u968f\u673a\u63d2\u503c\u4e3a\u6570\u636e\u5206\u5e03\u95f4\u7684\u8fde\u7eed\u53d8\u6362\u63d0\u4f9b\u4e86\u5f3a\u5927\u6846\u67b6\uff0c\u4f46\u5176\u6570\u503c\u5b9e\u73b0\u7684\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5206\u6790\u968f\u673a\u63d2\u503c\u751f\u6210\u7684ODE\uff0c\u5efa\u7acb\u4e86\u524d\u5411\u6b27\u62c9\u6cd5\u548cHeun\u6cd5\u7684\u6709\u9650\u65f6\u95f4\u8bef\u5dee\u754c\uff0c\u5e76\u4f18\u5316\u4e86\u8fed\u4ee3\u590d\u6742\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u4e24\u79cd\u6570\u503c\u65b9\u6cd5\u5728\u6709\u9650\u65f6\u95f4\u5185\u6536\u655b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bef\u5dee\u754c\u548c\u590d\u6742\u5ea6\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u968f\u673a\u63d2\u503c\u7684\u6570\u503c\u5b9e\u73b0\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u4f18\u5316\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u5efa\u6a21\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07345", "abs": "https://arxiv.org/abs/2508.07345", "authors": ["Samiha Afaf Neha", "Abir Ahammed Bhuiyan", "Md. Ishrak Khan"], "title": "ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis", "comment": null, "summary": "\\textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is\nessential for genomic studies due to their crucial role as structural elements\nin bacteriophages. Computational tools, particularly machine learning, have\nemerged for annotating phage protein sequences from high-throughput sequencing.\nHowever, effective annotation requires specialized sequence encodings. Our\npaper introduces ProteoKnight, a new image-based encoding method that addresses\nspatial constraints in existing techniques, yielding competitive performance in\nPVP classification using pre-trained convolutional neural networks.\nAdditionally, our study evaluates prediction uncertainty in binary PVP\nclassification through Monte Carlo Dropout (MCD). \\textbf{Methods:}\nProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,\nincorporating pixel colors and adjusting walk distances to capture intricate\nprotein features. Encoded sequences were classified using multiple pre-trained\nCNNs. Variance and entropy measures assessed prediction uncertainty across\nproteins of various classes and lengths. \\textbf{Results:} Our experiments\nachieved 90.8% accuracy in binary classification, comparable to\nstate-of-the-art methods. Multi-class classification accuracy remains\nsuboptimal. Our uncertainty analysis unveils variability in prediction\nconfidence influenced by protein class and sequence length.\n\\textbf{Conclusions:} Our study surpasses frequency chaos game representation\n(FCGR) by introducing novel image encoding that mitigates spatial information\nloss limitations. Our classification technique yields accurate and robust PVP\npredictions while identifying low-confidence predictions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProteoKnight\u7684\u65b0\u578b\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u566c\u83cc\u4f53\u75c5\u6bd2\u86cb\u767d\uff08PVP\uff09\u7684\u5206\u7c7b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u8499\u7279\u5361\u6d1bDropout\uff08MCD\uff09\u8bc4\u4f30\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u566c\u83cc\u4f53\u75c5\u6bd2\u86cb\u767d\uff08PVP\uff09\u5206\u7c7b\u4e2d\u5b58\u5728\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5e8f\u5217\u7f16\u7801\u65b9\u6cd5\u3002", "method": "ProteoKnight\u57fa\u4e8eDNA-Walk\u7b97\u6cd5\u6539\u8fdb\uff0c\u901a\u8fc7\u50cf\u7d20\u989c\u8272\u548c\u8c03\u6574\u884c\u8d70\u8ddd\u79bb\u7f16\u7801\u86cb\u767d\u8d28\u5e8f\u5217\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3CNN\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7MCD\u8bc4\u4f30\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523090.8%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u591a\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u6b20\u4f73\u3002\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u53d7\u86cb\u767d\u8d28\u7c7b\u522b\u548c\u5e8f\u5217\u957f\u5ea6\u5f71\u54cd\u3002", "conclusion": "ProteoKnight\u514b\u670d\u4e86\u9891\u7387\u6df7\u6c8c\u6e38\u620f\u8868\u793a\uff08FCGR\uff09\u7684\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u7a33\u5065\u7684PVP\u9884\u6d4b\uff0c\u5e76\u80fd\u8bc6\u522b\u4f4e\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u3002"}}
{"id": "2508.07370", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07370", "abs": "https://arxiv.org/abs/2508.07370", "authors": ["Sibylle Marcotte", "Gabriel Peyr\u00e9", "R\u00e9mi Gribonval"], "title": "Intrinsic training dynamics of deep neural networks", "comment": null, "summary": "A fundamental challenge in the theory of deep learning is to understand\nwhether gradient-based training in high-dimensional parameter spaces can be\ncaptured by simpler, lower-dimensional structures, leading to so-called\nimplicit bias. As a stepping stone, we study when a gradient flow on a\nhigh-dimensional variable $\\theta$ implies an intrinsic gradient flow on a\nlower-dimensional variable $z = \\phi(\\theta)$, for an architecture-related\nfunction $\\phi$. We express a so-called intrinsic dynamic property and show how\nit is related to the study of conservation laws associated with the\nfactorization $\\phi$. This leads to a simple criterion based on the inclusion\nof kernels of linear maps which yields a necessary condition for this property\nto hold. We then apply our theory to general ReLU networks of arbitrary depth\nand show that, for any initialization, it is possible to rewrite the flow as an\nintrinsic dynamic in a lower dimension that depends only on $z$ and the\ninitialization, when $\\phi$ is the so-called path-lifting. In the case of\nlinear networks with $\\phi$ the product of weight matrices, so-called balanced\ninitializations are also known to enable such a dimensionality reduction; we\ngeneralize this result to a broader class of {\\em relaxed balanced}\ninitializations, showing that, in certain configurations, these are the\n\\emph{only} initializations that ensure the intrinsic dynamic property.\nFinally, for the linear neural ODE associated with the limit of infinitely deep\nlinear networks, with relaxed balanced initialization, we explicitly express\nthe corresponding intrinsic dynamics.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u68af\u5ea6\u6d41\u80fd\u5426\u7b80\u5316\u4e3a\u4f4e\u7ef4\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u5185\u5728\u52a8\u6001\u6027\u8d28\uff0c\u5e76\u5e94\u7528\u4e8eReLU\u7f51\u7edc\u548c\u7ebf\u6027\u7f51\u7edc\u3002", "motivation": "\u7406\u89e3\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u68af\u5ea6\u8bad\u7ec3\u662f\u5426\u80fd\u88ab\u4f4e\u7ef4\u7ed3\u6784\u6355\u83b7\uff0c\u5373\u6240\u8c13\u7684\u9690\u5f0f\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u9ad8\u7ef4\u53d8\u91cf\u03b8\u7684\u68af\u5ea6\u6d41\u662f\u5426\u9690\u542b\u4f4e\u7ef4\u53d8\u91cfz=\u03d5(\u03b8)\u7684\u5185\u5728\u68af\u5ea6\u6d41\uff0c\u63d0\u51fa\u57fa\u4e8e\u6838\u5305\u542b\u7684\u7b80\u5355\u5224\u636e\uff0c\u5e76\u5e94\u7528\u4e8eReLU\u7f51\u7edc\u548c\u7ebf\u6027\u7f51\u7edc\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u4efb\u610f\u521d\u59cb\u6761\u4ef6\u4e0b\uff0cReLU\u7f51\u7edc\u7684\u68af\u5ea6\u6d41\u53ef\u91cd\u5199\u4e3a\u4ec5\u4f9d\u8d56z\u548c\u521d\u59cb\u5316\u7684\u4f4e\u7ef4\u52a8\u6001\uff1b\u7ebf\u6027\u7f51\u7edc\u4e2d\uff0c\u677e\u5f1b\u5e73\u8861\u521d\u59cb\u5316\u662f\u552f\u4e00\u6ee1\u8db3\u5185\u5728\u52a8\u6001\u6027\u8d28\u7684\u521d\u59cb\u5316\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7406\u89e3\u9ad8\u7ef4\u68af\u5ea6\u6d41\u7684\u4f4e\u7ef4\u7b80\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.07395", "categories": ["cs.LG", "68Q32"], "pdf": "https://arxiv.org/pdf/2508.07395", "abs": "https://arxiv.org/abs/2508.07395", "authors": ["Behnoush Khavari", "Mehran Shakerinava", "Jayesh Khullar", "Jerry Huang", "Fran\u00e7ois Rivest", "Siamak Ravanbakhsh", "Sarath Chandar"], "title": "Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs", "comment": "5 pages. Accepted at ICML 2025 Workshop on Methods and Opportunities\n  at Small Scale", "summary": "Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack\nstate-tracking capability due to either time-invariant transition matrices or\nrestricted eigenvalue ranges. To address this, input-dependent transition\nmatrices, particularly those that are complex or non-triangular, have been\nproposed to enhance SSM performance on such tasks. While existing theorems\ndemonstrate that both input-independent and non-negative SSMs are incapable of\nsolving simple state-tracking tasks, such as parity, regardless of depth, they\ndo not explore whether combining these two types in a multilayer SSM could\nhelp. We investigate this question for efficient SSMs with diagonal transition\nmatrices and show that such combinations still fail to solve parity. This\nimplies that a recurrence layer must both be input-dependent and include\nnegative eigenvalues. Our experiments support this conclusion by analyzing an\nSSM model that combines S4D and Mamba layers.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LRNN\u6a21\u578b\uff08\u5982S4D\u3001Mamba\u548cDeltaNet\uff09\u5728\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u6307\u51fa\u8f93\u5165\u4f9d\u8d56\u7684\u8f6c\u79fb\u77e9\u9635\u5bf9\u63d0\u5347\u6027\u80fd\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u73b0\u6709LRNN\u6a21\u578b\u56e0\u65f6\u95f4\u4e0d\u53d8\u8f6c\u79fb\u77e9\u9635\u6216\u53d7\u9650\u7279\u5f81\u503c\u8303\u56f4\u800c\u7f3a\u4e4f\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\uff0c\u9700\u7814\u7a76\u8f93\u5165\u4f9d\u8d56\u8f6c\u79fb\u77e9\u9635\u7684\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u591a\u5c42SSM\u4e2d\u7ed3\u5408\u8f93\u5165\u72ec\u7acb\u548c\u975e\u8d1fSSM\u7684\u80fd\u529b\uff0c\u5206\u6790\u5176\u5bf9\u7b80\u5355\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\uff08\u5982\u5947\u5076\u6821\u9a8c\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u7ed3\u5408\u8fd9\u4e24\u79cdSSM\uff0c\u4ecd\u65e0\u6cd5\u89e3\u51b3\u5947\u5076\u6821\u9a8c\u4efb\u52a1\uff0c\u9700\u8f93\u5165\u4f9d\u8d56\u4e14\u542b\u8d1f\u7279\u5f81\u503c\u7684\u9012\u5f52\u5c42\u3002", "conclusion": "\u8f93\u5165\u4f9d\u8d56\u4e14\u542b\u8d1f\u7279\u5f81\u503c\u7684\u8f6c\u79fb\u77e9\u9635\u662f\u63d0\u5347SSM\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\u7684\u5173\u952e\u3002"}}
{"id": "2508.07400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07400", "abs": "https://arxiv.org/abs/2508.07400", "authors": ["Mohamad Louai Shehab", "Alperen Tercan", "Necmiye Ozay"], "title": "Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors", "comment": null, "summary": "In this paper, we consider the problem of recovering time-varying reward\nfunctions from either optimal policies or demonstrations coming from a max\nentropy reinforcement learning problem. This problem is highly ill-posed\nwithout additional assumptions on the underlying rewards. However, in many\napplications, the rewards are indeed parsimonious, and some prior information\nis available. We consider two such priors on the rewards: 1) rewards are mostly\nconstant and they change infrequently, 2) rewards can be represented by a\nlinear combination of a small number of feature functions. We first show that\nthe reward identification problem with the former prior can be recast as a\nsparsification problem subject to linear constraints. Moreover, we give a\npolynomial-time algorithm that solves this sparsification problem exactly.\nThen, we show that identifying rewards representable with the minimum number of\nfeatures can be recast as a rank minimization problem subject to linear\nconstraints, for which convex relaxations of rank can be invoked. In both\ncases, these observations lead to efficient optimization-based reward\nidentification algorithms. Several examples are given to demonstrate the\naccuracy of the recovered rewards as well as their generalizability.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u6700\u4f18\u7b56\u7565\u6216\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u7684\u6f14\u793a\u4e2d\u6062\u590d\u65f6\u53d8\u5956\u52b1\u51fd\u6570\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5956\u52b1\u51fd\u6570\u7684\u5148\u9a8c\u5047\u8bbe\uff0c\u5e76\u5206\u522b\u8f6c\u5316\u4e3a\u7a00\u758f\u5316\u548c\u79e9\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u5956\u52b1\u51fd\u6570\u6062\u590d\u95ee\u9898\u901a\u5e38\u662f\u4e0d\u9002\u5b9a\u7684\uff0c\u4f46\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u5956\u52b1\u51fd\u6570\u5177\u6709\u7a00\u758f\u6027\u6216\u53ef\u8868\u793a\u4e3a\u5c11\u91cf\u7279\u5f81\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u6062\u590d\u65b9\u6cd5\u3002", "method": "1) \u5c06\u5956\u52b1\u51fd\u6570\u5047\u8bbe\u4e3a\u7a00\u758f\u53d8\u5316\uff0c\u8f6c\u5316\u4e3a\u7a00\u758f\u5316\u95ee\u9898\uff1b2) \u5c06\u5956\u52b1\u51fd\u6570\u5047\u8bbe\u4e3a\u5c11\u91cf\u7279\u5f81\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u8f6c\u5316\u4e3a\u79e9\u6700\u5c0f\u5316\u95ee\u9898\u3002\u5206\u522b\u63d0\u51fa\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u548c\u51f8\u677e\u5f1b\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u6062\u590d\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5148\u9a8c\u5047\u8bbe\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u8bba\u6587\u4e3a\u65f6\u53d8\u5956\u52b1\u51fd\u6570\u7684\u6062\u590d\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07428", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07428", "abs": "https://arxiv.org/abs/2508.07428", "authors": ["Md Sultanul Arifin", "Abu Nowshed Sakib", "Yeasir Rayhan", "Tanzima Hashem"], "title": "Lightning Prediction under Uncertainty: DeepLight with Hazy Loss", "comment": null, "summary": "Lightning, a common feature of severe meteorological conditions, poses\nsignificant risks, from direct human injuries to substantial economic losses.\nThese risks are further exacerbated by climate change. Early and accurate\nprediction of lightning would enable preventive measures to safeguard people,\nprotect property, and minimize economic losses. In this paper, we present\nDeepLight, a novel deep learning architecture for predicting lightning\noccurrences. Existing prediction models face several critical limitations: they\noften struggle to capture the dynamic spatial context and inherent uncertainty\nof lightning events, underutilize key observational data, such as radar\nreflectivity and cloud properties, and rely heavily on Numerical Weather\nPrediction (NWP) systems, which are both computationally expensive and highly\nsensitive to parameter settings. To overcome these challenges, DeepLight\nleverages multi-source meteorological data, including radar reflectivity, cloud\nproperties, and historical lightning occurrences through a dual-encoder\narchitecture. By employing multi-branch convolution techniques, it dynamically\ncaptures spatial correlations across varying extents. Furthermore, its novel\nHazy Loss function explicitly addresses the spatio-temporal uncertainty of\nlightning by penalizing deviations based on proximity to true events, enabling\nthe model to better learn patterns amidst randomness. Extensive experiments\nshow that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over\nstate-of-the-art methods, establishing it as a robust solution for lightning\nprediction.", "AI": {"tldr": "DeepLight\u662f\u4e00\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u9884\u6d4b\u95ea\u7535\u4e8b\u4ef6\uff0c\u901a\u8fc7\u591a\u6e90\u6c14\u8c61\u6570\u636e\u548c\u53cc\u7f16\u7801\u5668\u67b6\u6784\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u95ea\u7535\u5bf9\u4eba\u8eab\u5b89\u5168\u548c\u7ecf\u6d4e\u53d1\u5c55\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u5728\u52a8\u6001\u7a7a\u95f4\u4e0a\u4e0b\u6587\u6355\u6349\u548c\u6570\u636e\u5229\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u96f7\u8fbe\u53cd\u5c04\u7387\u3001\u4e91\u5c5e\u6027\u548c\u5386\u53f2\u95ea\u7535\u6570\u636e\uff0c\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u591a\u5206\u652f\u5377\u79ef\u6280\u672f\u52a8\u6001\u6355\u6349\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u4f7f\u7528Hazy Loss\u51fd\u6570\u5904\u7406\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepLight\u5c06\u516c\u5e73\u5a01\u80c1\u8bc4\u5206\uff08ETS\uff09\u63d0\u9ad8\u4e8618%-30%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DeepLight\u4e3a\u95ea\u7535\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u95ea\u7535\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2508.07440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07440", "abs": "https://arxiv.org/abs/2508.07440", "authors": ["Zhipeng Chang", "Zhenye Wen", "Xiaofei Zhao"], "title": "Unsupervised operator learning approach for dissipative equations via Onsager principle", "comment": null, "summary": "Existing operator learning methods rely on supervised training with\nhigh-fidelity simulation data, introducing significant computational cost. In\nthis work, we propose the deep Onsager operator learning (DOOL) method, a novel\nunsupervised framework for solving dissipative equations. Rooted in the Onsager\nvariational principle (OVP), DOOL trains a deep operator network by directly\nminimizing the OVP-defined Rayleighian functional, requiring no labeled data,\nand then proceeds in time explicitly through conservation/change laws for the\nsolution. Another key innovation here lies in the spatiotemporal decoupling\nstrategy: the operator's trunk network processes spatial coordinates\nexclusively, thereby enhancing training efficiency, while integrated external\ntime stepping enables temporal extrapolation. Numerical experiments on typical\ndissipative equations validate the effectiveness of the DOOL method, and\nsystematic comparisons with supervised DeepONet and MIONet demonstrate its\nenhanced performance. Extensions are made to cover the second-order wave models\nwith dissipation that do not directly follow OVP.", "AI": {"tldr": "DOOL\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u6df1\u5ea6\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u57fa\u4e8eOnsager\u53d8\u5206\u539f\u7406\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\uff0c\u901a\u8fc7\u65f6\u7a7a\u89e3\u8026\u7b56\u7565\u63d0\u5347\u6548\u7387\uff0c\u5e76\u5728\u5178\u578b\u8017\u6563\u65b9\u7a0b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u4fdd\u771f\u6a21\u62df\u6570\u636e\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cDOOL\u65e8\u5728\u901a\u8fc7\u65e0\u76d1\u7763\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DOOL\u57fa\u4e8eOnsager\u53d8\u5206\u539f\u7406\uff0c\u76f4\u63a5\u6700\u5c0f\u5316Rayleighian\u6cdb\u51fd\uff0c\u91c7\u7528\u65f6\u7a7a\u89e3\u8026\u7b56\u7565\uff08\u7a7a\u95f4\u5750\u6807\u7531\u4e3b\u5e72\u7f51\u7edc\u5904\u7406\uff0c\u5916\u90e8\u65f6\u95f4\u6b65\u8fdb\u5b9e\u73b0\u65f6\u95f4\u5916\u63a8\uff09\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DOOL\u7684\u6709\u6548\u6027\uff0c\u4e0e\u76d1\u7763\u65b9\u6cd5DeepONet\u548cMIONet\u76f8\u6bd4\u6027\u80fd\u66f4\u4f18\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u4e0d\u76f4\u63a5\u9075\u5faaOVP\u7684\u4e8c\u9636\u6ce2\u52a8\u6a21\u578b\u3002", "conclusion": "DOOL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u76d1\u7763\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8017\u6563\u65b9\u7a0b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u590d\u6742\u6a21\u578b\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2508.07452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07452", "abs": "https://arxiv.org/abs/2508.07452", "authors": ["Fernando Martinez", "Tao Li", "Yingdong Lu", "Juntao Chen"], "title": "Stackelberg Coupling of Online Representation Learning and Reinforcement Learning", "comment": null, "summary": "Integrated, end-to-end learning of representations and policies remains a\ncornerstone of deep reinforcement learning (RL). However, to address the\nchallenge of learning effective features from a sparse reward signal, recent\ntrends have shifted towards adding complex auxiliary objectives or fully\ndecoupling the two processes, often at the cost of increased design complexity.\nThis work proposes an alternative to both decoupling and naive end-to-end\nlearning, arguing that performance can be significantly improved by structuring\nthe interaction between distinct perception and control networks with a\nprincipled, game-theoretic dynamic. We formalize this dynamic by introducing\nthe Stackelberg Coupled Representation and Reinforcement Learning (SCORER)\nframework, which models the interaction between perception and control as a\nStackelberg game. The perception network (leader) strategically learns features\nto benefit the control network (follower), whose own objective is to minimize\nits Bellman error. We approximate the game's equilibrium with a practical\ntwo-timescale algorithm. Applied to standard DQN variants on benchmark tasks,\nSCORER improves sample efficiency and final performance. Our results show that\nperformance gains can be achieved through principled algorithmic design of the\nperception-control dynamic, without requiring complex auxiliary objectives or\narchitectures.", "AI": {"tldr": "SCORER\u6846\u67b6\u901a\u8fc7\u535a\u5f08\u8bba\u52a8\u6001\u4f18\u5316\u611f\u77e5\u4e0e\u63a7\u5236\u7f51\u7edc\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u4e0b\u7279\u5f81\u5b66\u4e60\u7684\u6311\u6218\uff0c\u907f\u514d\u590d\u6742\u8f85\u52a9\u76ee\u6807\u6216\u5b8c\u5168\u89e3\u8026\u5e26\u6765\u7684\u8bbe\u8ba1\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faSCORER\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u4e0e\u63a7\u5236\u7f51\u7edc\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3aStackelberg\u535a\u5f08\uff0c\u5e76\u8bbe\u8ba1\u4e24\u65f6\u95f4\u5c3a\u5ea6\u7b97\u6cd5\u8fd1\u4f3c\u5747\u8861\u3002", "result": "\u5728\u6807\u51c6DQN\u53d8\u4f53\u548c\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cSCORER\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u535a\u5f08\u8bba\u8bbe\u8ba1\u611f\u77e5-\u63a7\u5236\u52a8\u6001\uff0c\u65e0\u9700\u590d\u6742\u8f85\u52a9\u76ee\u6807\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.07458", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07458", "abs": "https://arxiv.org/abs/2508.07458", "authors": ["Wei Qian", "Chenxu Zhao", "Yangyi Li", "Wenqian Ye", "Mengdi Huai"], "title": "Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten", "comment": null, "summary": "Currently, various uncertainty quantification methods have been proposed to\nprovide certainty and probability estimates for deep learning models' label\npredictions. Meanwhile, with the growing demand for the right to be forgotten,\nmachine unlearning has been extensively studied as a means to remove the impact\nof requested sensitive data from a pre-trained model without retraining the\nmodel from scratch. However, the vulnerabilities of such generated predictive\nuncertainties with regard to dedicated malicious unlearning attacks remain\nunexplored. To bridge this gap, for the first time, we propose a new class of\nmalicious unlearning attacks against predictive uncertainties, where the\nadversary aims to cause the desired manipulations of specific predictive\nuncertainty results. We also design novel optimization frameworks for our\nattacks and conduct extensive experiments, including black-box scenarios.\nNotably, our extensive experiments show that our attacks are more effective in\nmanipulating predictive uncertainties than traditional attacks that focus on\nlabel misclassifications, and existing defenses against conventional attacks\nare ineffective against our attacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u6076\u610f\u9057\u5fd8\u653b\u51fb\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6bd4\u4f20\u7edf\u653b\u51fb\u66f4\u6709\u6548\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7684\u666e\u53ca\u548c\u9057\u5fd8\u6743\u9700\u6c42\u7684\u589e\u957f\uff0c\u7814\u7a76\u6076\u610f\u9057\u5fd8\u653b\u51fb\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u6210\u4e3a\u5fc5\u8981\u3002", "method": "\u8bbe\u8ba1\u4e86\u9488\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u65b0\u578b\u6076\u610f\u9057\u5fd8\u653b\u51fb\u6846\u67b6\uff0c\u5305\u62ec\u4f18\u5316\u65b9\u6cd5\u548c\u9ed1\u76d2\u573a\u666f\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u653b\u51fb\u6bd4\u4f20\u7edf\u6807\u7b7e\u8bef\u5206\u7c7b\u653b\u51fb\u66f4\u6709\u6548\uff0c\u4e14\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5bf9\u5176\u65e0\u6548\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5728\u6076\u610f\u9057\u5fd8\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.07505", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07505", "abs": "https://arxiv.org/abs/2508.07505", "authors": ["Yueyang Quan", "Chang Wang", "Shengjie Zhai", "Minghong Fang", "Zhuqing Liu"], "title": "Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach", "comment": "To appear in ACM MobiHoc 2025", "summary": "Decentralized min-max optimization allows multi-agent systems to\ncollaboratively solve global min-max optimization problems by facilitating the\nexchange of model updates among neighboring agents, eliminating the need for a\ncentral server. However, sharing model updates in such systems carry a risk of\nexposing sensitive data to inference attacks, raising significant privacy\nconcerns. To mitigate these privacy risks, differential privacy (DP) has become\na widely adopted technique for safeguarding individual data. Despite its\nadvantages, implementing DP in decentralized min-max optimization poses\nchallenges, as the added noise can hinder convergence, particularly in\nnon-convex scenarios with complex agent interactions in min-max optimization\nproblems. In this work, we propose an algorithm called DPMixSGD (Differential\nPrivate Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving\nalgorithm specifically designed for non-convex decentralized min-max\noptimization. Our method builds on the state-of-the-art STORM-based algorithm,\none of the fastest decentralized min-max solutions. We rigorously prove that\nthe noise added to local gradients does not significantly compromise\nconvergence performance, and we provide theoretical bounds to ensure privacy\nguarantees. To validate our theoretical findings, we conduct extensive\nexperiments across various tasks and models, demonstrating the effectiveness of\nour approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPMixSGD\u7684\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u51f8\u5206\u6563\u5f0f\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u6280\u672f\uff0c\u786e\u4fdd\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u4e0d\u5f71\u54cd\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u5206\u6563\u5f0f\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u4e2d\uff0c\u6a21\u578b\u66f4\u65b0\u7684\u5171\u4eab\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u6570\u636e\u6cc4\u9732\uff0c\u5dee\u5206\u9690\u79c1\u867d\u80fd\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u566a\u58f0\u53ef\u80fd\u5f71\u54cd\u6536\u655b\uff0c\u5c24\u5176\u5728\u975e\u51f8\u573a\u666f\u4e0b\u3002", "method": "\u57fa\u4e8eSTORM\u7b97\u6cd5\uff0c\u63d0\u51faDPMixSGD\uff0c\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u5230\u5c40\u90e8\u68af\u5ea6\uff0c\u540c\u65f6\u4fdd\u8bc1\u9690\u79c1\u548c\u6536\u655b\u6027\u80fd\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u566a\u58f0\u4e0d\u5f71\u54cd\u6536\u655b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DPMixSGD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u6563\u5f0f\u6700\u5c0f-\u6700\u5927\u4f18\u5316\u7b97\u6cd5\u3002"}}
{"id": "2508.07536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07536", "abs": "https://arxiv.org/abs/2508.07536", "authors": ["Tasfiq E. Alam", "Md Manjurul Ahsan", "Shivakumar Raman"], "title": "Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning", "comment": null, "summary": "Accurate and interpretable bearing fault classification is critical for\nensuring the reliability of rotating machinery, particularly under variable\noperating conditions where domain shifts can significantly degrade model\nperformance. This study proposes a physics-informed multimodal convolutional\nneural network (CNN) with a late fusion architecture, integrating vibration and\nmotor current signals alongside a dedicated physics-based feature extraction\nbranch. The model incorporates a novel physics-informed loss function that\npenalizes physically implausible predictions based on characteristic bearing\nfault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency\nInner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive\nexperiments on the Paderborn University dataset demonstrate that the proposed\nphysics-informed approach consistently outperforms a non-physics-informed\nbaseline, achieving higher accuracy, reduced false classifications, and\nimproved robustness across multiple data splits. To address performance\ndegradation under unseen operating conditions, three transfer learning (TL)\nstrategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy\n(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS\nyields the best generalization, with additional performance gains when combined\nwith physics-informed modeling. Validation on the KAIST bearing dataset\nconfirms the framework's cross-dataset applicability, achieving up to 98\npercent accuracy. Statistical hypothesis testing further verifies significant\nimprovements (p < 0.01) in classification performance. The proposed framework\ndemonstrates the potential of integrating domain knowledge with data-driven\nlearning to achieve robust, interpretable, and generalizable fault diagnosis\nfor real-world industrial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u591a\u6a21\u6001CNN\u6a21\u578b\uff0c\u7528\u4e8e\u8f74\u627f\u6545\u969c\u5206\u7c7b\uff0c\u7ed3\u5408\u632f\u52a8\u548c\u7535\u673a\u7535\u6d41\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u7279\u5f81\u63d0\u53d6\u5206\u652f\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u975e\u7269\u7406\u57fa\u7ebf\uff0c\u5e76\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u53d8\u5de5\u51b5\u4e0b\u8f74\u627f\u6545\u969c\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u907f\u514d\u56e0\u9886\u57df\u504f\u79fb\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001CNN\u67b6\u6784\uff0c\u7ed3\u5408\u632f\u52a8\u548c\u7535\u6d41\u4fe1\u53f7\uff0c\u5f15\u5165\u7269\u7406\u7279\u5f81\u63d0\u53d6\u5206\u652f\u548c\u7269\u7406\u635f\u5931\u51fd\u6570\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728Paderborn\u548cKAIST\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe98%\uff0c\u7edf\u8ba1\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u6545\u969c\u8bca\u65ad\u6846\u67b6\u3002"}}
{"id": "2508.07555", "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07555", "abs": "https://arxiv.org/abs/2508.07555", "authors": ["Keyuan Zhang", "Yin Sun", "Bo Ji"], "title": "Multimodal Remote Inference", "comment": "Accepted by The 22nd IEEE International Conference on Mobile Ad-Hoc\n  and Smart Systems (MASS 2025)", "summary": "We consider a remote inference system with multiple modalities, where a\nmultimodal machine learning (ML) model performs real-time inference using\nfeatures collected from remote sensors. As sensor observations may change\ndynamically over time, fresh features are critical for inference tasks.\nHowever, timely delivering features from all modalities is often infeasible due\nto limited network resources. To this end, we study a two-modality scheduling\nproblem to minimize the ML model's inference error, which is expressed as a\npenalty function of AoI for both modalities. We develop an index-based\nthreshold policy and prove its optimality. Specifically, the scheduler switches\nmodalities when the current modality's index function exceeds a threshold. We\nshow that the two modalities share the same threshold, and both the index\nfunctions and the threshold can be computed efficiently. The optimality of our\npolicy holds for (i) general AoI functions that are \\emph{non-monotonic} and\n\\emph{non-additive} and (ii) \\emph{heterogeneous} transmission times. Numerical\nresults show that our policy reduces inference error by up to 55% compared to\nround-robin and uniform random policies, which are oblivious to the AoI-based\ninference error function. Our results shed light on how to improve remote\ninference accuracy by optimizing task-oriented AoI functions.", "AI": {"tldr": "\u7814\u7a76\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u7d22\u5f15\u7684\u9608\u503c\u7b56\u7565\u4ee5\u51cf\u5c11\u63a8\u7406\u8bef\u5dee\u3002", "motivation": "\u7531\u4e8e\u7f51\u7edc\u8d44\u6e90\u6709\u9650\uff0c\u65e0\u6cd5\u5b9e\u65f6\u4f20\u8f93\u6240\u6709\u6a21\u6001\u7684\u7279\u5f81\uff0c\u800c\u65b0\u9c9c\u7684\u7279\u5f81\u5bf9\u63a8\u7406\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d22\u5f15\u7684\u9608\u503c\u7b56\u7565\uff0c\u8c03\u5ea6\u5668\u5728\u7d22\u5f15\u51fd\u6570\u8d85\u8fc7\u9608\u503c\u65f6\u5207\u6362\u6a21\u6001\u3002", "result": "\u8be5\u7b56\u7565\u5728\u975e\u5355\u8c03\u3001\u975e\u52a0\u6027AoI\u51fd\u6570\u548c\u5f02\u6784\u4f20\u8f93\u65f6\u95f4\u4e0b\u5747\u6700\u4f18\uff0c\u63a8\u7406\u8bef\u5dee\u964d\u4f4e\u8fbe55%\u3002", "conclusion": "\u4f18\u5316\u4efb\u52a1\u5bfc\u5411\u7684AoI\u51fd\u6570\u53ef\u663e\u8457\u63d0\u9ad8\u8fdc\u7a0b\u63a8\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2508.07571", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07571", "abs": "https://arxiv.org/abs/2508.07571", "authors": ["Xingwu Chen", "Miao Lu", "Beining Wu", "Difan Zou"], "title": "Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression", "comment": null, "summary": "Using more test-time computation during language model inference, such as\ngenerating more intermediate thoughts or sampling multiple candidate answers,\nhas proven effective in significantly improving model performance. This paper\ntakes an initial step toward bridging the gap between practical language model\ninference and theoretical transformer analysis by incorporating randomness and\nsampling. We focus on in-context linear regression with continuous/binary\ncoefficients, where our framework simulates language model decoding through\nnoise injection and binary coefficient sampling. Through this framework, we\nprovide detailed analyses of widely adopted inference techniques. Supported by\nempirical results, our theoretical framework and analysis demonstrate the\npotential for offering new insights into understanding inference behaviors in\nreal-world language models.", "AI": {"tldr": "\u901a\u8fc7\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff08\u5982\u751f\u6210\u66f4\u591a\u4e2d\u95f4\u601d\u8003\u6216\u91c7\u6837\u591a\u4e2a\u5019\u9009\u7b54\u6848\uff09\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u672c\u6587\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6027\u548c\u91c7\u6837\uff0c\u7ed3\u5408\u7406\u8bba\u4e0e\u5b9e\u9645\uff0c\u5206\u6790\u63a8\u7406\u884c\u4e3a\u3002", "motivation": "\u5f25\u5408\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0e\u7406\u8bba\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a2\u7d22\u968f\u673a\u6027\u548c\u91c7\u6837\u5728\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u7ebf\u6027\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u4e8c\u5143\u7cfb\u6570\u91c7\u6837\u6a21\u62df\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u4e86\u63a8\u7406\u884c\u4e3a\u7684\u65b0\u89c1\u89e3\uff0c\u652f\u6301\u5bf9\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u7684\u7406\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\uff0c\u5c55\u793a\u4e86\u968f\u673a\u6027\u548c\u91c7\u6837\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07581", "categories": ["cs.LG", "math.DS", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.07581", "abs": "https://arxiv.org/abs/2508.07581", "authors": ["Nisha Chandramoorthy", "Adriaan de Clercq"], "title": "When and how can inexact generative models still sample from the data manifold?", "comment": null, "summary": "A curious phenomenon observed in some dynamical generative models is the\nfollowing: despite learning errors in the score function or the drift vector\nfield, the generated samples appear to shift \\emph{along} the support of the\ndata distribution but not \\emph{away} from it. In this work, we investigate\nthis phenomenon of \\emph{robustness of the support} by taking a dynamical\nsystems approach on the generating stochastic/deterministic process. Our\nperturbation analysis of the probability flow reveals that infinitesimal\nlearning errors cause the predicted density to be different from the target\ndensity only on the data manifold for a wide class of generative models.\nFurther, what is the dynamical mechanism that leads to the robustness of the\nsupport? We show that the alignment of the top Lyapunov vectors (most sensitive\ninfinitesimal perturbation directions) with the tangent spaces along the\nboundary of the data manifold leads to robustness and prove a sufficient\ncondition on the dynamics of the generating process to achieve this alignment.\nMoreover, the alignment condition is efficient to compute and, in practice, for\nrobust generative models, automatically leads to accurate estimates of the\ntangent bundle of the data manifold. Using a finite-time linear perturbation\nanalysis on samples paths as well as probability flows, our work complements\nand extends existing works on obtaining theoretical guarantees for generative\nmodels from a stochastic analysis, statistical learning and uncertainty\nquantification points of view. Our results apply across different dynamical\ngenerative models, such as conditional flow-matching and score-based generative\nmodels, and for different target distributions that may or may not satisfy the\nmanifold hypothesis.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u5b66\u4e60\u8bef\u5dee\u5bf9\u6570\u636e\u5206\u5e03\u652f\u6301\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5fae\u5c0f\u8bef\u5dee\u4ec5\u5bfc\u81f4\u9884\u6d4b\u5bc6\u5ea6\u5728\u6570\u636e\u6d41\u5f62\u4e0a\u53d8\u5316\uff0c\u5e76\u63ed\u793a\u4e86\u652f\u6301\u9c81\u68d2\u6027\u7684\u52a8\u529b\u5b66\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u6a21\u578b\u4e2d\u5b66\u4e60\u8bef\u5dee\u5982\u4f55\u5f71\u54cd\u751f\u6210\u6837\u672c\u7684\u5206\u5e03\u652f\u6301\uff0c\u5c24\u5176\u662f\u4e3a\u4f55\u8bef\u5dee\u4e0d\u4f1a\u4f7f\u6837\u672c\u504f\u79bb\u6570\u636e\u6d41\u5f62\u3002", "method": "\u91c7\u7528\u52a8\u529b\u5b66\u7cfb\u7edf\u65b9\u6cd5\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u7684\u6982\u7387\u6d41\uff0c\u901a\u8fc7\u6270\u52a8\u5206\u6790\u7814\u7a76\u652f\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51faLyapunov\u5411\u91cf\u4e0e\u6570\u636e\u6d41\u5f62\u5207\u7a7a\u95f4\u5bf9\u9f50\u7684\u6761\u4ef6\u3002", "result": "\u53d1\u73b0\u5fae\u5c0f\u5b66\u4e60\u8bef\u5dee\u4ec5\u6539\u53d8\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u9884\u6d4b\u5bc6\u5ea6\uff0c\u8bc1\u660e\u4e86\u652f\u6301\u9c81\u68d2\u6027\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u8ba1\u7b97\u5bf9\u9f50\u6761\u4ef6\u7684\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u751f\u6210\u6a21\u578b\u7684\u7406\u8bba\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u52a8\u6001\u751f\u6210\u6a21\u578b\u548c\u76ee\u6807\u5206\u5e03\u3002"}}
{"id": "2508.07629", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07629", "abs": "https://arxiv.org/abs/2508.07629", "authors": ["Zhenpeng Su", "Leiyu Pan", "Xue Bai", "Dening Liu", "Guanting Dong", "Jiaming Huang", "Wenping Hu", "Guorui Zhou"], "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization", "comment": null, "summary": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\%\non AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.", "AI": {"tldr": "Klear-Reasoner\u662f\u4e00\u4e2a\u5177\u6709\u957f\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u5de5\u4f5c\u6d41\u7a0b\u5206\u6790\u548c\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u793e\u533a\u4e2d\u9ad8\u6027\u80fd\u63a8\u7406\u6a21\u578b\u7684\u590d\u73b0\u95ee\u9898\u8f83\u591a\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u7ec6\u8282\u62ab\u9732\u4e0d\u5b8c\u6574\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u5305\u62ec\u6570\u636e\u51c6\u5907\u3001\u957f\u94fe\u601d\u7ef4\u76d1\u7763\u5fae\u8c03\uff08long CoT SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u5e76\u63d0\u51faGPPO\u65b9\u6cd5\u89e3\u51b3RL\u4e2d\u7684\u88c1\u526a\u95ee\u9898\u3002", "result": "Klear-Reasoner\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5982AIME 2024\uff0890.5%\uff09\u548cLiveCodeBench V6\uff0858.1%\uff09\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u5c0f\u6837\u672c\u96be\u4f8b\u66f4\u6709\u6548\uff0cGPPO\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u548c\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2508.07631", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07631", "abs": "https://arxiv.org/abs/2508.07631", "authors": ["Advait Parulekar", "Litu Rout", "Karthikeyan Shanmugam", "Sanjay Shakkottai"], "title": "Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo", "comment": null, "summary": "We study the problem of posterior sampling in the context of score based\ngenerative models. We have a trained score network for a prior $p(x)$, a\nmeasurement model $p(y|x)$, and are tasked with sampling from the posterior\n$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)\nunder well-accepted computational hardness assumptions. Despite this, popular\nalgorithms for tasks such as image super-resolution, stylization, and\nreconstruction enjoy empirical success. Rather than establishing distributional\nassumptions or restricted settings under which exact posterior sampling is\ntractable, we view this as a more general \"tilting\" problem of biasing a\ndistribution towards a measurement. Under minimal assumptions, we show that one\ncan tractably sample from a distribution that is simultaneously close to the\nposterior of a noised prior in KL divergence and the true posterior in Fisher\ndivergence. Intuitively, this combination ensures that the resulting sample is\nconsistent with both the measurement and the prior. To the best of our\nknowledge these are the first formal results for (approximate) posterior\nsampling in polynomial time.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u5206\u6570\u7684\u751f\u6210\u6a21\u578b\u4e2d\u540e\u9a8c\u91c7\u6837\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8fd1\u4f3c\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u6837\u672c\u4e0e\u6d4b\u91cf\u548c\u5148\u9a8c\u4e00\u81f4\u3002", "motivation": "\u5c3d\u7ba1\u540e\u9a8c\u91c7\u6837\u5728KL\u6563\u5ea6\u4e0b\u901a\u5e38\u662f\u96be\u89e3\u7684\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u8bb8\u591a\u7b97\u6cd5\u4ecd\u80fd\u6210\u529f\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66f4\u901a\u7528\u7684\u201c\u503e\u659c\u201d\u65b9\u6cd5\uff0c\u65e0\u9700\u4e25\u683c\u5047\u8bbe\u5373\u53ef\u5b9e\u73b0\u8fd1\u4f3c\u91c7\u6837\u3002", "method": "\u901a\u8fc7\u5c06\u540e\u9a8c\u91c7\u6837\u89c6\u4e3a\u201c\u503e\u659c\u201d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u566a\u58f0\u5148\u9a8c\u7684KL\u6563\u5ea6\u548c\u771f\u5b9e\u540e\u9a8c\u7684Fisher\u6563\u5ea6\u4e0b\u540c\u65f6\u63a5\u8fd1\u76ee\u6807\u5206\u5e03\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u4ee5\u91c7\u6837\u5230\u4e00\u4e2a\u5206\u5e03\uff0c\u8be5\u5206\u5e03\u540c\u65f6\u63a5\u8fd1\u566a\u58f0\u5148\u9a8c\u7684\u540e\u9a8c\u548c\u771f\u5b9e\u540e\u9a8c\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5b9e\u73b0\u8fd1\u4f3c\u540e\u9a8c\u91c7\u6837\u7684\u6b63\u5f0f\u7ed3\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2508.07636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07636", "abs": "https://arxiv.org/abs/2508.07636", "authors": ["Huiqi Deng", "Hongbin Pei", "Quanshi Zhang", "Mengnan Du"], "title": "Attribution Explanations for Deep Neural Networks: A Theoretical Perspective", "comment": null, "summary": "Attribution explanation is a typical approach for explaining deep neural\nnetworks (DNNs), inferring an importance or contribution score for each input\nvariable to the final output. In recent years, numerous attribution methods\nhave been developed to explain DNNs. However, a persistent concern remains\nunresolved, i.e., whether and which attribution methods faithfully reflect the\nactual contribution of input variables to the decision-making process. The\nfaithfulness issue undermines the reliability and practical utility of\nattribution explanations. We argue that these concerns stem from three core\nchallenges. First, difficulties arise in comparing attribution methods due to\ntheir unstructured heterogeneity, differences in heuristics, formulations, and\nimplementations that lack a unified organization. Second, most methods lack\nsolid theoretical underpinnings, with their rationales remaining absent,\nambiguous, or unverified. Third, empirically evaluating faithfulness is\nchallenging without ground truth. Recent theoretical advances provide a\npromising way to tackle these challenges, attracting increasing attention. We\nsummarize these developments, with emphasis on three key directions: (i)\nTheoretical unification, which uncovers commonalities and differences among\nmethods, enabling systematic comparisons; (ii) Theoretical rationale,\nclarifying the foundations of existing methods; (iii) Theoretical evaluation,\nrigorously proving whether methods satisfy faithfulness principles. Beyond a\ncomprehensive review, we provide insights into how these studies help deepen\ntheoretical understanding, inform method selection, and inspire new attribution\nmethods. We conclude with a discussion of promising open problems for further\nwork.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u7684\u5f52\u56e0\u89e3\u91ca\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u603b\u7ed3\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u7406\u8bba\u8fdb\u5c55\u3002", "motivation": "\u5f52\u56e0\u89e3\u91ca\u65b9\u6cd5\u5728\u89e3\u91caDNNs\u65f6\u7f3a\u4e4f\u7edf\u4e00\u6027\u548c\u7406\u8bba\u57fa\u7840\uff0c\u5bfc\u81f4\u5176\u53ef\u9760\u6027\u53d7\u5230\u8d28\u7591\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u7edf\u4e00\u3001\u7406\u8bba\u57fa\u7840\u548c\u7406\u8bba\u8bc4\u4f30\u4e09\u4e2a\u65b9\u5411\uff0c\u7cfb\u7edf\u6bd4\u8f83\u548c\u9a8c\u8bc1\u5f52\u56e0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u7406\u8bba\u8fdb\u5c55\uff0c\u4e3a\u5f52\u56e0\u65b9\u6cd5\u7684\u7406\u89e3\u548c\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u8bba\u6587\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5f3a\u8c03\u4e86\u7406\u8bba\u6df1\u5316\u548c\u65b9\u6cd5\u521b\u65b0\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07637", "categories": ["cs.LG", "cs.CG"], "pdf": "https://arxiv.org/pdf/2508.07637", "abs": "https://arxiv.org/abs/2508.07637", "authors": ["Guanqun Ma", "David Lenz", "Hanqi Guo", "Tom Peterka", "Bei Wang"], "title": "Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs", "comment": "The paper is to be published at the 15th IEEE Workshop on Large Data\n  Analysis and Visualization (LDAV)", "summary": "Implicit continuous models, such as functional models and implicit neural\nnetworks, are an increasingly popular method for replacing discrete data\nrepresentations with continuous, high-order, and differentiable surrogates.\nThese models offer new perspectives on the storage, transfer, and analysis of\nscientific data. In this paper, we introduce the first framework to directly\nextract complex topological features -- contours, Jacobi sets, and ridge-valley\ngraphs -- from a type of continuous implicit model known as multivariate\nfunctional approximation (MFA). MFA replaces discrete data with continuous\npiecewise smooth functions. Given an MFA model as the input, our approach\nenables direct extraction of complex topological features from the model,\nwithout reverting to a discrete representation of the model. Our work is easily\ngeneralizable to any continuous implicit model that supports the queries of\nfunction values and high-order derivatives. Our work establishes the building\nblocks for performing topological data analysis and visualization on implicit\ncontinuous models.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u76f4\u63a5\u4ece\u8fde\u7eed\u9690\u5f0f\u6a21\u578b\uff08\u5982MFA\uff09\u4e2d\u63d0\u53d6\u590d\u6742\u62d3\u6251\u7279\u5f81\u7684\u6846\u67b6\uff0c\u65e0\u9700\u79bb\u6563\u5316\u3002", "motivation": "\u8fde\u7eed\u9690\u5f0f\u6a21\u578b\uff08\u5982MFA\uff09\u4e3a\u79d1\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u8fde\u7eed\u3001\u9ad8\u9636\u3001\u53ef\u5fae\u7684\u8868\u793a\uff0c\u4f46\u7f3a\u4e4f\u76f4\u63a5\u63d0\u53d6\u62d3\u6251\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eMFA\u6a21\u578b\uff0c\u76f4\u63a5\u63d0\u53d6\u8f6e\u5ed3\u7ebf\u3001Jacobi\u96c6\u548c\u810a\u8c37\u56fe\u7b49\u62d3\u6251\u7279\u5f81\uff0c\u652f\u6301\u51fd\u6570\u503c\u548c\u9ad8\u9636\u5bfc\u6570\u67e5\u8be2\u3002", "result": "\u5b9e\u73b0\u4e86\u76f4\u63a5\u4ece\u8fde\u7eed\u9690\u5f0f\u6a21\u578b\u4e2d\u63d0\u53d6\u590d\u6742\u62d3\u6251\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u652f\u6301\u51fd\u6570\u503c\u548c\u5bfc\u6570\u67e5\u8be2\u7684\u6a21\u578b\u3002", "conclusion": "\u4e3a\u8fde\u7eed\u9690\u5f0f\u6a21\u578b\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07638", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07638", "abs": "https://arxiv.org/abs/2508.07638", "authors": ["Jia Zhang", "Yao Liu", "Chen-Xi Zhang", "Yi Liu", "Yi-Xuan Jin", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals", "comment": "Under review", "summary": "Aligning Large Language Models (LLMs) with diverse human values requires\nmoving beyond a single holistic \"better-than\" preference criterion. While\ncollecting fine-grained, aspect-specific preference data is more reliable and\nscalable, existing methods like Direct Preference Optimization (DPO) struggle\nwith the severe noise and conflicts inherent in such aggregated datasets. In\nthis paper, we tackle this challenge from a data-centric perspective. We first\nderive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a\nkey Preference Divergence (PD) term that quantifies inter-aspect preference\nconflicts. Instead of using this term for direct optimization, we leverage it\nto formulate a novel, theoretically-grounded data selection principle. Our\nprinciple advocates for selecting a subset of high-consensus data-identified by\nthe most negative PD values-for efficient DPO training. We prove the optimality\nof this strategy by analyzing the loss bounds of the DMPO objective in the\nselection problem. To operationalize our approach, we introduce practical\nmethods of PD term estimation and length bias mitigation, thereby proposing our\nPD selection method. Evaluation on the UltraFeedback dataset with three varying\nconflict levels shows that our simple yet effective strategy achieves over 10%\nrelative improvement against both the standard holistic preference and a\nstronger oracle using aggregated preference signals, all while boosting\ntraining efficiency and obviating the need for intractable holistic preference\nannotating, unlocking the potential of robust LLM alignment via fine-grained\npreference signals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9009\u62e9\u7684\u65b9\u6cd5\uff08DMPO\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u504f\u597d\u5206\u6b67\uff08PD\uff09\u6765\u4f18\u5316LLM\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u7ec6\u7c92\u5ea6\u504f\u597d\u6570\u636e\u4e2d\u5b58\u5728\u566a\u58f0\u548c\u51b2\u7a81\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\u3002", "method": "\u63d0\u51faDMPO\u76ee\u6807\uff0c\u5229\u7528PD\u9879\u91cf\u5316\u504f\u597d\u51b2\u7a81\uff0c\u5e76\u8bbe\u8ba1\u6570\u636e\u9009\u62e9\u539f\u5219\uff0c\u9009\u62e9\u9ad8\u5171\u8bc6\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u5bf9\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8610%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0cDMPO\u6709\u6548\u89e3\u51b3\u4e86\u504f\u597d\u51b2\u7a81\u95ee\u9898\uff0c\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.07646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07646", "abs": "https://arxiv.org/abs/2508.07646", "authors": ["Xiaoxue Yang", "Jaeha Lee", "Anna-Katharina Dick", "Jasper Timm", "Fei Xie", "Diogo Cruz"], "title": "Multi-Turn Jailbreaks Are Simpler Than They Seem", "comment": "25 pages, 15 figures. Accepted at COLM 2025 SoLaR Workshop", "summary": "While defenses against single-turn jailbreak attacks on Large Language Models\n(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent\nvulnerability, often achieving success rates exceeding 70% against models\noptimized for single-turn protection. This work presents an empirical analysis\nof automated multi-turn jailbreak attacks across state-of-the-art models\nincluding GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.\nOur findings challenge the perceived sophistication of multi-turn attacks: when\naccounting for the attacker's ability to learn from how models refuse harmful\nrequests, multi-turn jailbreaking approaches are approximately equivalent to\nsimply resampling single-turn attacks multiple times. Moreover, attack success\nis correlated among similar models, making it easier to jailbreak newly\nreleased ones. Additionally, for reasoning models, we find surprisingly that\nhigher reasoning effort often leads to higher attack success rates. Our results\nhave important implications for AI safety evaluation and the design of\njailbreak-resistant systems. We release the source code at\nhttps://github.com/diogo-cruz/multi_turn_simpler", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5a01\u80c1\uff0c\u53d1\u73b0\u5176\u6210\u529f\u7387\u4e0e\u5355\u8f6e\u653b\u51fb\u91cd\u590d\u5c1d\u8bd5\u76f8\u5f53\uff0c\u4e14\u653b\u51fb\u6548\u679c\u5728\u76f8\u4f3c\u6a21\u578b\u95f4\u5177\u6709\u76f8\u5173\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u6f0f\u6d1e\uff0c\u6311\u6218\u5176\u590d\u6742\u6027\u7684\u8ba4\u77e5\uff0c\u5e76\u4e3aAI\u5b89\u5168\u8bc4\u4f30\u548c\u9632\u5fa1\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u4f7f\u7528StrongREJECT\u57fa\u51c6\u5bf9GPT-4\u3001Claude\u548cGemini\u7b49\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u591a\u8f6e\u653b\u51fb\u6210\u529f\u7387\u4e0e\u5355\u8f6e\u653b\u51fb\u91cd\u590d\u5c1d\u8bd5\u76f8\u8fd1\uff0c\u653b\u51fb\u6548\u679c\u5728\u76f8\u4f3c\u6a21\u578b\u95f4\u76f8\u5173\uff0c\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u66f4\u5bb9\u6613\u88ab\u653b\u51fb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9AI\u5b89\u5168\u8bc4\u4f30\u548c\u9632\u5fa1\u8bbe\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.07659", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07659", "abs": "https://arxiv.org/abs/2508.07659", "authors": ["Hyeon-Ju Jeon", "Jeon-Ho Kang", "In-Hyuk Kwon", "O-Joun Lee"], "title": "Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning", "comment": "10 pages", "summary": "This study aims to discover spatial correlations between Earth observations\nand atmospheric states to improve the forecasting accuracy of global\natmospheric state estimation, which are usually conducted using conventional\nnumerical weather prediction (NWP) systems and is the beginning of weather\nforecasting. NWP systems predict future atmospheric states at fixed locations,\nwhich are called NWP grid points, by analyzing previous atmospheric states and\nnewly acquired Earth observations without fixed locations. Thus, surrounding\nmeteorological context and the changing locations of the observations make\nspatial correlations between atmospheric states and observations over time. To\nhandle complicated spatial correlations, which change dynamically, we employ\nspatiotemporal graph neural networks (STGNNs) with structure learning. However,\nstructure learning has an inherent limitation that this can cause structural\ninformation loss and over-smoothing problem by generating excessive edges. To\nsolve this problem, we regulate edge sampling by adaptively determining node\ndegrees and considering the spatial distances between NWP grid points and\nobservations. We validated the effectiveness of the proposed method by using\nreal-world atmospheric state and observation data from East Asia. Even in areas\nwith high atmospheric variability, the proposed method outperformed existing\nSTGNN models with and without structure learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08STGNNs\uff09\u548c\u7ed3\u6784\u5b66\u4e60\uff0c\u52a8\u6001\u6355\u6349\u5730\u7403\u89c2\u6d4b\u4e0e\u5927\u6c14\u72b6\u6001\u4e4b\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u4ee5\u63d0\u9ad8\u5168\u7403\u5927\u6c14\u72b6\u6001\u4f30\u8ba1\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u8282\u8282\u70b9\u5ea6\u548c\u7a7a\u95f4\u8ddd\u79bb\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u5b66\u4e60\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u7cfb\u7edf\u5728\u56fa\u5b9a\u7f51\u683c\u70b9\u4e0a\u9884\u6d4b\u5927\u6c14\u72b6\u6001\uff0c\u4f46\u5730\u7403\u89c2\u6d4b\u6570\u636e\u7684\u4f4d\u7f6e\u4e0d\u56fa\u5b9a\uff0c\u5bfc\u81f4\u52a8\u6001\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u96be\u4ee5\u6355\u6349\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08STGNNs\uff09\u7ed3\u5408\u7ed3\u6784\u5b66\u4e60\uff0c\u52a8\u6001\u5efa\u6a21\u7a7a\u95f4\u76f8\u5173\u6027\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u8282\u70b9\u5ea6\u8c03\u8282\u548c\u7a7a\u95f4\u8ddd\u79bb\u8003\u8651\uff0c\u4f18\u5316\u8fb9\u7f18\u91c7\u6837\u4ee5\u907f\u514d\u4fe1\u606f\u4e22\u5931\u548c\u8fc7\u5e73\u6ed1\u3002", "result": "\u5728\u4e1c\u4e9a\u5730\u533a\u7684\u771f\u5b9e\u5927\u6c14\u72b6\u6001\u548c\u89c2\u6d4b\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u53d8\u5f02\u6027\u533a\u57df\u4f18\u4e8e\u73b0\u6709STGNN\u6a21\u578b\uff08\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u7ed3\u6784\u5b66\u4e60\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u7a7a\u95f4\u76f8\u5173\u6027\u5efa\u6a21\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u6c14\u72b6\u6001\u4f30\u8ba1\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2508.07662", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07662", "abs": "https://arxiv.org/abs/2508.07662", "authors": ["Ihor Stepanov", "Mykhailo Shtopko", "Dmytro Vodianytskyi", "Oleksandr Lukashov", "Alexander Yavorskyi", "Mykyta Yaroshenko"], "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks", "comment": "14 pages, 7 tables, 2 figures", "summary": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback.", "AI": {"tldr": "GLiClass\u662f\u4e00\u79cd\u57fa\u4e8eGLiNER\u67b6\u6784\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u9ad8\u6548\u7387\u548c\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u9700\u8981\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u540c\u65f6\u9700\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u7528\u6237\u9700\u6c42\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5982\u751f\u6210\u5f0fLLMs\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u5404\u6709\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGLiClass\uff0c\u6539\u8fdbGLiNER\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u8fdb\u884c\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u8bad\u7ec3\u3002", "result": "GLiClass\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4e0e\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u652f\u6301\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3002", "conclusion": "GLiClass\u4e3a\u52a8\u6001\u5206\u7c7b\u9700\u6c42\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07668", "abs": "https://arxiv.org/abs/2508.07668", "authors": ["Hyobin Park", "Jinwook Jung", "Minseok Seo", "Hyunsoo Choi", "Deukjae Cho", "Sekil Park", "Dong-Geol Choi"], "title": "AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting", "comment": null, "summary": "With the increase in maritime traffic and the mandatory implementation of the\nAutomatic Identification System (AIS), the importance and diversity of maritime\ntraffic analysis tasks based on AIS data, such as vessel trajectory prediction,\nanomaly detection, and collision risk assessment, is rapidly growing. However,\nexisting approaches tend to address these tasks individually, making it\ndifficult to holistically consider complex maritime situations. To address this\nlimitation, we propose a novel framework, AIS-LLM, which integrates time-series\nAIS data with a large language model (LLM). AIS-LLM consists of a Time-Series\nEncoder for processing AIS sequences, an LLM-based Prompt Encoder, a\nCross-Modality Alignment Module for semantic alignment between time-series data\nand textual prompts, and an LLM-based Multi-Task Decoder. This architecture\nenables the simultaneous execution of three key tasks: trajectory prediction,\nanomaly detection, and risk assessment of vessel collisions within a single\nend-to-end system. Experimental results demonstrate that AIS-LLM outperforms\nexisting methods across individual tasks, validating its effectiveness.\nFurthermore, by integratively analyzing task outputs to generate situation\nsummaries and briefings, AIS-LLM presents the potential for more intelligent\nand efficient maritime traffic management.", "AI": {"tldr": "AIS-LLM\u662f\u4e00\u4e2a\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217AIS\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u5904\u7406\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u6d77\u4e8b\u4efb\u52a1\uff0c\u96be\u4ee5\u5168\u9762\u8003\u8651\u590d\u6742\u60c5\u51b5\uff0c\u56e0\u6b64\u63d0\u51faAIS-LLM\u4ee5\u6574\u5408\u591a\u4efb\u52a1\u5206\u6790\u3002", "method": "AIS-LLM\u5305\u62ec\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u3001LLM\u63d0\u793a\u7f16\u7801\u5668\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\u548c\u591a\u4efb\u52a1\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u591a\u4efb\u52a1\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAIS-LLM\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u751f\u6210\u7efc\u5408\u60c5\u5883\u6458\u8981\u3002", "conclusion": "AIS-LLM\u4e3a\u6d77\u4e8b\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07675", "abs": "https://arxiv.org/abs/2508.07675", "authors": ["Xutong Liu", "Baran Atalar", "Xiangxiang Dai", "Jinhang Zuo", "Siwei Wang", "John C. S. Lui", "Wei Chen", "Carlee Joe-Wong"], "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation", "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u539f\u5219\u6027\u8bed\u4e49\u7f13\u5b58\u6846\u67b6\uff0c\u89e3\u51b3LLM\u9ad8\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "LLM\u7684\u9ad8\u63a8\u7406\u6210\u672c\u5e26\u6765\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6301\u7eed\u6027\u6311\u6218\uff0c\u4f20\u7edf\u7f13\u5b58\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u73b0\u6709\u8bed\u4e49\u7f13\u5b58\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u79bb\u7ebf\u4f18\u5316\u548c\u5728\u7ebf\u5b66\u4e60\u7b97\u6cd5\uff0c\u5904\u7406\u672a\u77e5\u67e5\u8be2\u548c\u6210\u672c\u5206\u5e03\uff0c\u8bbe\u8ba1\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7b97\u6cd5\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bed\u4e49\u7f13\u5b58\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2508.07676", "categories": ["cs.LG", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07676", "abs": "https://arxiv.org/abs/2508.07676", "authors": ["Chenchen Lin", "Xuehe Wang"], "title": "Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks", "comment": "Accepted by ECAI25", "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, thereby enhancing privacy and\nfacilitating collaboration among clients connected via social networks.\nHowever, these social connections introduce privacy externalities: a client's\nprivacy loss depends not only on its privacy protection strategy but also on\nthe privacy decisions of others, propagated through the network via multi-hop\ninteractions. In this work, we propose a socially-aware privacy-preserving FL\nmechanism that systematically quantifies indirect privacy leakage through a\nmulti-hop propagation model. We formulate the server-client interaction as a\ntwo-stage Stackelberg game, where the server, as the leader, optimizes\nincentive policies, and clients, as followers, strategically select their\nprivacy budgets, which determine their privacy-preserving levels by controlling\nthe magnitude of added noise. To mitigate information asymmetry in networked\nprivacy estimation, we introduce a mean-field estimator to approximate the\naverage external privacy risk. We theoretically prove the existence and\nconvergence of the fixed point of the mean-field estimator and derive\nclosed-form expressions for the Stackelberg Nash Equilibrium. Despite being\ndesigned from a client-centric incentive perspective, our mechanism achieves\napproximately-optimal social welfare, as revealed by Price of Anarchy (PoA)\nanalysis. Experiments on diverse datasets demonstrate that our approach\nsignificantly improves client utilities and reduces server costs while\nmaintaining model performance, outperforming both Social-Agnostic (SA)\nbaselines and methods that account for social externalities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u4ea4\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8df3\u4f20\u64ad\u6a21\u578b\u91cf\u5316\u95f4\u63a5\u9690\u79c1\u6cc4\u9732\uff0c\u91c7\u7528Stackelberg\u535a\u5f08\u4f18\u5316\u6fc0\u52b1\u7b56\u7565\uff0c\u63d0\u5347\u5ba2\u6237\u7aef\u6548\u7528\u5e76\u964d\u4f4e\u670d\u52a1\u5668\u6210\u672c\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u793e\u4ea4\u7f51\u7edc\u8fde\u63a5\u7684\u5ba2\u6237\u7aef\u9690\u79c1\u6cc4\u9732\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u81ea\u8eab\u7b56\u7565\uff0c\u8fd8\u53d7\u4ed6\u4eba\u51b3\u7b56\u5f71\u54cd\uff0c\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u95f4\u63a5\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e24\u9636\u6bb5Stackelberg\u535a\u5f08\uff0c\u670d\u52a1\u5668\u4f18\u5316\u6fc0\u52b1\u7b56\u7565\uff0c\u5ba2\u6237\u7aef\u9009\u62e9\u9690\u79c1\u9884\u7b97\uff1b\u5f15\u5165\u5747\u503c\u573a\u4f30\u8ba1\u5668\u8fd1\u4f3c\u5916\u90e8\u9690\u79c1\u98ce\u9669\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5747\u503c\u573a\u4f30\u8ba1\u5668\u7684\u6536\u655b\u6027\u53caStackelberg\u7eb3\u4ec0\u5747\u8861\u7684\u95ed\u5f0f\u89e3\uff0c\u5b9e\u9a8c\u663e\u793a\u5ba2\u6237\u7aef\u6548\u7528\u63d0\u5347\u3001\u670d\u52a1\u5668\u6210\u672c\u964d\u4f4e\u4e14\u6a21\u578b\u6027\u80fd\u4fdd\u6301\u3002", "conclusion": "\u8be5\u673a\u5236\u5728\u5ba2\u6237\u7aef\u6fc0\u52b1\u89c6\u89d2\u4e0b\u5b9e\u73b0\u8fd1\u4f3c\u6700\u4f18\u793e\u4f1a\u798f\u5229\uff0c\u4f18\u4e8e\u793e\u4ea4\u65e0\u5173\u57fa\u7ebf\u53ca\u8003\u8651\u793e\u4ea4\u5916\u90e8\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07681", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07681", "abs": "https://arxiv.org/abs/2508.07681", "authors": ["Yooseok Lim", "ByoungJun Jeon", "Seong-A Park", "Jisoo Lee", "Sae Won Choi", "Chang Wook Jeong", "Ho-Geol Ryu", "Hongyeol Lee", "Hyun-Lim Yang"], "title": "MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation", "comment": "18 pages, 5 figures", "summary": "Sepsis, a life-threatening inflammatory response to infection, causes organ\ndysfunction, making early detection and optimal management critical. Previous\nreinforcement learning (RL) approaches to sepsis management rely primarily on\nstructured data, such as lab results or vital signs, and on a dearth of a\ncomprehensive understanding of the patient's condition. In this work, we\npropose a Multimodal Offline REinforcement learning for Clinical notes\nLeveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis\ncontrol in intensive care units. MORE-CLEAR employs pre-trained large-scale\nlanguage models (LLMs) to facilitate the extraction of rich semantic\nrepresentations from clinical notes, preserving clinical context and improving\npatient state representation. Gated fusion and cross-modal attention allow\ndynamic weight adjustment in the context of time and the effective integration\nof multimodal data. Extensive cross-validation using two public (MIMIC-III and\nMIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly\nimproves estimated survival rate and policy performance compared to\nsingle-modal RL approaches. To our knowledge, this is the first to leverage LLM\ncapabilities within a multimodal offline RL for better state representation in\nmedical applications. This approach can potentially expedite the treatment and\nmanagement of sepsis by enabling reinforcement learning models to propose\nenhanced actions based on a more comprehensive understanding of patient\nconditions.", "AI": {"tldr": "MORE-CLEAR\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u8113\u6bd2\u75c7\u7ba1\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u8113\u6bd2\u75c7\u65e9\u671f\u68c0\u6d4b\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u60a3\u8005\u72b6\u6001\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e34\u5e8a\u7b14\u8bb0\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u901a\u8fc7\u95e8\u63a7\u878d\u5408\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u52a8\u6001\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728\u516c\u5f00\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cMORE-CLEAR\u663e\u8457\u63d0\u5347\u751f\u5b58\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5c06\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u5f15\u5165\u591a\u6a21\u6001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u6709\u671b\u6539\u5584\u8113\u6bd2\u75c7\u6cbb\u7597\u3002"}}
{"id": "2508.07697", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.07697", "abs": "https://arxiv.org/abs/2508.07697", "authors": ["Hao Liu", "Chun Yang", "Zhang xiaoxing", "Xiaobin Zhu"], "title": "Semantic-Enhanced Time-Series Forecasting via Large Language Models", "comment": "14 pages,9 figures", "summary": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08SE-LLM\uff09\uff0c\u901a\u8fc7\u5d4c\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u548c\u5f02\u5e38\u7279\u5f81\u6765\u589e\u5f3a\u8bed\u4e49\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u6a21\u6001\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u901a\u7528\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u8bed\u8a00\u77e5\u8bc6\u4e0e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6a21\u5f0f\u4e4b\u95f4\u7684\u5185\u5728\u6a21\u6001\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002", "method": "\u63d0\u51faSE-LLM\uff0c\u901a\u8fc7\u5d4c\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u548c\u5f02\u5e38\u7279\u5f81\u589e\u5f3a\u8bed\u4e49\u7a7a\u95f4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\u6765\u540c\u65f6\u5efa\u6a21\u957f\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSE-LLM\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SE-LLM\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u548c\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.07706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07706", "abs": "https://arxiv.org/abs/2508.07706", "authors": ["Philipp Huber", "David Li", "Juan Pedro Guti\u00e9rrez Hermosillo Muriedas", "Deifilia Kieckhefen", "Markus G\u00f6tz", "Achim Streit", "Charlotte Debus"], "title": "Energy Consumption in Parallel Neural Network Training", "comment": null, "summary": "The increasing demand for computational resources of training neural networks\nleads to a concerning growth in energy consumption. While parallelization has\nenabled upscaling model and dataset sizes and accelerated training, its impact\non energy consumption is often overlooked. To close this research gap, we\nconducted scaling experiments for data-parallel training of two models,\nResNet50 and FourCastNet, and evaluated the impact of parallelization\nparameters, i.e., GPU count, global batch size, and local batch size, on\npredictive performance, training time, and energy consumption. We show that\nenergy consumption scales approximately linearly with the consumed resources,\ni.e., GPU hours; however, the respective scaling factor differs substantially\nbetween distinct model trainings and hardware, and is systematically influenced\nby the number of samples and gradient updates per GPU hour. Our results shed\nlight on the complex interplay of scaling up neural network training and can\ninform future developments towards more sustainable AI research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e76\u884c\u5316\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5bf9\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u80fd\u6e90\u6d88\u8017\u4e0eGPU\u5c0f\u65f6\u6570\u8fd1\u4f3c\u7ebf\u6027\u76f8\u5173\uff0c\u4f46\u5177\u4f53\u56e0\u7d20\u56e0\u6a21\u578b\u548c\u786c\u4ef6\u800c\u5f02\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5bf9\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u589e\u52a0\uff0c\u80fd\u6e90\u6d88\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u5e76\u884c\u5316\u5bf9\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6570\u636e\u5e76\u884c\u8bad\u7ec3ResNet50\u548cFourCastNet\u6a21\u578b\uff0c\u8bc4\u4f30GPU\u6570\u91cf\u3001\u5168\u5c40\u6279\u6b21\u5927\u5c0f\u548c\u5c40\u90e8\u6279\u6b21\u5927\u5c0f\u5bf9\u6027\u80fd\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u3002", "result": "\u80fd\u6e90\u6d88\u8017\u4e0eGPU\u5c0f\u65f6\u6570\u8fd1\u4f3c\u7ebf\u6027\u76f8\u5173\uff0c\u4f46\u5177\u4f53\u6bd4\u4f8b\u56e0\u6a21\u578b\u548c\u786c\u4ef6\u4e0d\u540c\uff0c\u4e14\u53d7\u6bcfGPU\u5c0f\u65f6\u7684\u6837\u672c\u6570\u548c\u68af\u5ea6\u66f4\u65b0\u6570\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6269\u5c55\u4e0e\u80fd\u6e90\u6d88\u8017\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4e3a\u672a\u6765\u53ef\u6301\u7eedAI\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2508.07710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07710", "abs": "https://arxiv.org/abs/2508.07710", "authors": ["Jingya Wang", "Xin Deng", "Wenjie Wei", "Dehao Zhang", "Shuai Wang", "Qian Sun", "Jieyuan Zhang", "Hanwen Liu", "Ning Xie", "Malu Zhang"], "title": "Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer", "comment": "Under review", "summary": "Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a\npromising approach for constructing energy-efficient Transformer architectures.\nCompared to directly trained Spiking Transformers, ANN-to-SNN conversion\nmethods bypass the high training costs. However, existing methods still suffer\nfrom notable limitations, failing to effectively handle nonlinear operations in\nTransformer architectures and requiring additional fine-tuning processes for\npre-trained ANNs. To address these issues, we propose a high-performance and\ntraining-free ANN-to-SNN conversion framework tailored for Transformer\narchitectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)\nneuron, which employs an exponential decay strategy and multi-basis encoding\nmethod to efficiently approximate various nonlinear operations. It removes the\nrequirement for weight modifications in pre-trained ANNs. Extensive experiments\nacross diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures\n(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless\nconversion accuracy with significantly lower latency. This provides a promising\npathway for the efficient and scalable deployment of Spiking Transformers in\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u65e0\u9700\u8bad\u7ec3\u7684ANN-to-SNN\u8f6c\u6362\u6846\u67b6\uff0c\u7528\u4e8eTransformer\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u64cd\u4f5c\u548c\u989d\u5916\u5fae\u8c03\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709ANN-to-SNN\u8f6c\u6362\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406Transformer\u4e2d\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u4e14\u9700\u8981\u989d\u5916\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u5176\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f15\u5165\u591a\u57fa\u6307\u6570\u8870\u51cf\uff08MBE\uff09\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u6307\u6570\u8870\u51cf\u7b56\u7565\u548c\u591a\u57fa\u7f16\u7801\u65b9\u6cd5\u9ad8\u6548\u8fd1\u4f3c\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3ANN\u7684\u6743\u91cd\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\uff08CV\u3001NLU\u3001NLG\uff09\u548c\u4e3b\u6d41Transformer\u67b6\u6784\uff08ViT\u3001RoBERTa\u3001GPT-2\uff09\u4e0a\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u8f6c\u6362\u7cbe\u5ea6\u548c\u663e\u8457\u964d\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aSpiking Transformer\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.07722", "categories": ["cs.LG", "cs.IT", "cs.MA", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07722", "abs": "https://arxiv.org/abs/2508.07722", "authors": ["Pietro Talli", "Federico Mason", "Federico Chiariotti", "Andrea Zanella"], "title": "Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations", "comment": "This manuscript is currently under revision", "summary": "In this work, we address the problem of training Reinforcement Learning (RL)\nagents over communication networks. The RL paradigm requires the agent to\ninstantaneously perceive the state evolution to infer the effects of its\nactions on the environment. This is impossible if the agent receives state\nupdates over lossy or delayed wireless systems and thus operates with partial\nand intermittent information. In recent years, numerous frameworks have been\nproposed to manage RL with imperfect feedback; however, they often offer\nspecific solutions with a substantial computational burden. To address these\nlimits, we propose a novel architecture, named Homomorphic Robust Remote\nReinforcement Learning (HR3L), that enables the training of remote RL agents\nexchanging observations across a non-ideal wireless channel. HR3L considers two\nunits: the transmitter, which encodes meaningful representations of the\nenvironment, and the receiver, which decodes these messages and performs\nactions to maximize a reward signal. Importantly, HR3L does not require the\nexchange of gradient information across the wireless channel, allowing for\nquicker training and a lower communication overhead than state-of-the-art\nsolutions. Experimental results demonstrate that HR3L significantly outperforms\nbaseline methods in terms of sample efficiency and adapts to different\ncommunication scenarios, including packet losses, delayed transmissions, and\ncapacity limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHR3L\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u975e\u7406\u60f3\u65e0\u7ebf\u4fe1\u9053\u4e0a\u8bad\u7ec3\u8fdc\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u901a\u4fe1\u5ef6\u8fdf\u548c\u4e22\u5305\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfRL\u4ee3\u7406\u9700\u8981\u5373\u65f6\u611f\u77e5\u72b6\u6001\u53d8\u5316\uff0c\u4f46\u5728\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u5ef6\u8fdf\u548c\u4e22\u5305\uff0c\u4ee3\u7406\u53ea\u80fd\u83b7\u53d6\u90e8\u5206\u548c\u95f4\u6b47\u6027\u4fe1\u606f\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8ba1\u7b97\u8d1f\u62c5\u5927\u4e14\u4e0d\u591f\u901a\u7528\u3002", "method": "HR3L\u67b6\u6784\u5305\u542b\u53d1\u5c04\u5668\u548c\u63a5\u6536\u5668\u4e24\u4e2a\u5355\u5143\uff0c\u53d1\u5c04\u5668\u7f16\u7801\u73af\u5883\u72b6\u6001\uff0c\u63a5\u6536\u5668\u89e3\u7801\u5e76\u6267\u884c\u52a8\u4f5c\u4ee5\u6700\u5927\u5316\u5956\u52b1\u4fe1\u53f7\uff0c\u65e0\u9700\u4ea4\u6362\u68af\u5ea6\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHR3L\u5728\u6837\u672c\u6548\u7387\u548c\u9002\u5e94\u4e0d\u540c\u901a\u4fe1\u573a\u666f\uff08\u5982\u4e22\u5305\u3001\u5ef6\u8fdf\u548c\u5bb9\u91cf\u9650\u5236\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HR3L\u4e3a\u5728\u975e\u7406\u60f3\u901a\u4fe1\u6761\u4ef6\u4e0b\u8bad\u7ec3RL\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07738", "abs": "https://arxiv.org/abs/2508.07738", "authors": ["Jialu Zhou", "Dianxi Shi", "Shaowu Yang", "Xinyu Wei", "Mingyue Yang", "Leqian Li", "Mengzhu Wang", "Chunping Qiu"], "title": "Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning", "comment": null, "summary": "Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential\ntasks with shifting class sets and distribution. Despite the\nParameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual\nheterogeneity, they still suffer from catastrophic forgetting and forward\nforgetting. To address these challenges, we propose a Two-Level Routing Grouped\nMixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the\npre-trained CLIP model, assigning specific expert group for each task to\nmitigate catastrophic forgetting. With the number of experts continually grows\nin this process, TRGE maintains the static experts count within the group and\nintroduces the intra-group router to alleviate routing overfitting caused by\nthe increasing routing complexity. Meanwhile, we design an inter-group routing\npolicy based on task identifiers and task prototype distance, which dynamically\nselects relevant expert groups and combines their outputs to enhance inter-task\ncollaboration. Secondly, to get the correct task identifiers, we leverage\nMultimodal Large Language Models (MLLMs) which own powerful multimodal\ncomprehension capabilities to generate semantic task descriptions and recognize\nthe correct task identifier. Finally, to mitigate forward forgetting, we\ndynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE\nadapter based on training progress, leveraging both pre-trained and learned\nknowledge. Through extensive experiments across various settings, our method\noutperforms other advanced methods with fewer trainable parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTRGE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6269\u5c55\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u8bbe\u8ba1\u4e24\u7ea7\u8def\u7531\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u591a\u9886\u57df\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u524d\u77bb\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u591a\u9886\u57df\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u4efb\u52a1\u95f4\u7c7b\u522b\u548c\u5206\u5e03\u53d8\u5316\u7684\u53cc\u91cd\u5f02\u8d28\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u548c\u524d\u77bb\u6027\u9057\u5fd8\u3002", "method": "TRGE\u65b9\u6cd5\u52a8\u6001\u6269\u5c55CLIP\u6a21\u578b\uff0c\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5206\u914d\u4e13\u5bb6\u7ec4\uff0c\u5e76\u8bbe\u8ba1\u7ec4\u5185\u548c\u7ec4\u95f4\u8def\u7531\u7b56\u7565\uff0c\u540c\u65f6\u5229\u7528MLLMs\u751f\u6210\u4efb\u52a1\u63cf\u8ff0\u4ee5\u8bc6\u522b\u4efb\u52a1\u6807\u8bc6\u7b26\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTRGE\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "TRGE\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u4efb\u52a1\u6807\u8bc6\u7b26\u8bc6\u522b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9886\u57df\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.07750", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07750", "abs": "https://arxiv.org/abs/2508.07750", "authors": ["Haowen Wang", "Yun Yue", "Zhiling Ye", "Shuowen Zhang", "Lei Fan", "Jiaxin Liang", "Jiadi Jiang", "Cheng Wei", "Jingyuan Deng", "Xudong Han", "Ji Li", "Chunxiao Guo", "Peng Wei", "Jian Wang", "Jinjie Gu"], "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment", "comment": "12 pages, 5 figures, 7 tables", "summary": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models.", "AI": {"tldr": "GRAO\uff08Group Relative Alignment Optimization\uff09\u662f\u4e00\u79cd\u7ed3\u5408SFT\u548cRL\u4f18\u52bf\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6837\u672c\u751f\u6210\u3001\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u52a0\u6743\u548c\u53c2\u8003\u611f\u77e5\u53c2\u6570\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3SFT\u548cRL\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u5c40\u9650\u6027\uff0cSFT\u53d7\u9650\u4e8e\u79bb\u7ebf\u7b56\u7565\u8f68\u8ff9\uff0cRL\u6837\u672c\u6548\u7387\u4f4e\u4e14\u4f9d\u8d56\u9ad8\u8d28\u91cf\u57fa\u7840\u6a21\u578b\u3002", "method": "\u63d0\u51faGRAO\u6846\u67b6\uff0c\u5305\u542b\u591a\u6837\u672c\u751f\u6210\u7b56\u7565\u3001\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u52a0\u6743\u635f\u5931\u548c\u53c2\u8003\u611f\u77e5\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff0cGRAO\u76f8\u6bd4SFT\u3001DPO\u3001PPO\u548cGRPO\u5206\u522b\u63d0\u5347\u4e8657.70%\u300117.65%\u30017.95%\u548c5.18%\u3002", "conclusion": "GRAO\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8bc1\u652f\u6301\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.07763", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07763", "abs": "https://arxiv.org/abs/2508.07763", "authors": ["Martin Rektoris", "Milan Pape\u017e", "V\u00e1clav \u0160m\u00eddl", "Tom\u00e1\u0161 Pevn\u00fd"], "title": "Sparse Probabilistic Graph Circuits", "comment": null, "summary": "Deep generative models (DGMs) for graphs achieve impressively high expressive\npower thanks to very efficient and scalable neural networks. However, these\nnetworks contain non-linearities that prevent analytical computation of many\nstandard probabilistic inference queries, i.e., these DGMs are considered\n\\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)\naddress this issue by enabling \\emph{tractable} probabilistic inference, they\noperate on dense graph representations with $\\mathcal{O}(n^2)$ complexity for\ngraphs with $n$ nodes and \\emph{$m$ edges}. To address this scalability issue,\nwe introduce Sparse PGCs, a new class of tractable generative models that\noperate directly on sparse graph representation, reducing the complexity to\n$\\mathcal{O}(n + m)$, which is particularly beneficial for $m \\ll n^2$. In the\ncontext of de novo drug design, we empirically demonstrate that SPGCs retain\nexact inference capabilities, improve memory efficiency and inference speed,\nand match the performance of intractable DGMs in key metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u6982\u7387\u56fe\u7535\u8def\uff08SPGCs\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6982\u7387\u56fe\u7535\u8def\uff08PGCs\uff09\u5728\u7a20\u5bc6\u56fe\u8868\u793a\u4e0a\u7684\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5c06\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u81f3O(n + m)\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u56fe\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08DGMs\uff09\u867d\u8868\u8fbe\u80fd\u529b\u5f3a\uff0c\u4f46\u56e0\u975e\u7ebf\u6027\u5bfc\u81f4\u6982\u7387\u63a8\u65ad\u4e0d\u53ef\u89e3\u3002PGCs\u867d\u89e3\u51b3\u4e86\u53ef\u89e3\u6027\u95ee\u9898\uff0c\u4f46\u5176\u7a20\u5bc6\u56fe\u8868\u793a\u7684\u9ad8\u590d\u6742\u5ea6\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f15\u5165\u7a00\u758f\u6982\u7387\u56fe\u7535\u8def\uff08SPGCs\uff09\uff0c\u76f4\u63a5\u64cd\u4f5c\u7a00\u758f\u56fe\u8868\u793a\uff0c\u964d\u4f4e\u590d\u6742\u5ea6\u81f3O(n + m)\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSPGCs\u5728\u836f\u7269\u8bbe\u8ba1\u4e2d\u4fdd\u7559\u4e86\u7cbe\u786e\u63a8\u65ad\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u5185\u5b58\u6548\u7387\u548c\u63a8\u65ad\u901f\u5ea6\uff0c\u6027\u80fd\u4e0e\u4e0d\u53ef\u89e3DGMs\u76f8\u5f53\u3002", "conclusion": "SPGCs\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u751f\u6210\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u56fe\u573a\u666f\u3002"}}
{"id": "2508.07768", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07768", "abs": "https://arxiv.org/abs/2508.07768", "authors": ["Qiang He", "Setareh Maghsudi"], "title": "Pareto Multi-Objective Alignment for Language Models", "comment": "Accepted at ECML/PKDD 2025", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPAMA\u7b97\u6cd5\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u591a\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u51f8\u4f18\u5316\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eRLHF\u7684\u5bf9\u9f50\u65b9\u6cd5\u4ec5\u4f18\u5316\u5355\u4e00\u5956\u52b1\u51fd\u6570\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u504f\u597d\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u9650\u5236\u4e86LLM\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faPareto\u591a\u76ee\u6807\u5bf9\u9f50\uff08PAMA\uff09\u7b97\u6cd5\uff0c\u5c06\u591a\u76ee\u6807RLHF\u8f6c\u5316\u4e3a\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n^2*d)\u964d\u81f3O(n)\u3002", "result": "PAMA\u5728125M\u81f37B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u80fd\u529b\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6536\u655b\u5230Pareto\u7a33\u5b9a\u70b9\u3002", "conclusion": "PAMA\u4e3a\u591a\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86LLM\u5728\u591a\u6837\u5316\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.07807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07807", "abs": "https://arxiv.org/abs/2508.07807", "authors": ["Rahul Khorana"], "title": "Topological Feature Compression for Molecular Graph Neural Networks", "comment": null, "summary": "Recent advances in molecular representation learning have produced highly\neffective encodings of molecules for numerous cheminformatics and\nbioinformatics tasks. However, extracting general chemical insight while\nbalancing predictive accuracy, interpretability, and computational efficiency\nremains a major challenge. In this work, we introduce a novel Graph Neural\nNetwork (GNN) architecture that combines compressed higher-order topological\nsignals with standard molecular features. Our approach captures global\ngeometric information while preserving computational tractability and\nhuman-interpretable structure. We evaluate our model across a range of\nbenchmarks, from small-molecule datasets to complex material datasets, and\ndemonstrate superior performance using a parameter-efficient architecture. We\nachieve the best performing results in both accuracy and robustness across\nalmost all benchmarks. We open source all code \\footnote{All code and results\ncan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u9636\u62d3\u6251\u4fe1\u53f7\u4e0e\u6807\u51c6\u5206\u5b50\u7279\u5f81\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u67b6\u6784\uff0c\u5e73\u8861\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5206\u5b50\u8868\u793a\u5b66\u4e60\u5728\u5316\u5b66\u4fe1\u606f\u5b66\u548c\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u5728\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578bGNN\u67b6\u6784\uff0c\u7ed3\u5408\u538b\u7f29\u7684\u9ad8\u9636\u62d3\u6251\u4fe1\u53f7\u4e0e\u6807\u51c6\u5206\u5b50\u7279\u5f81\uff0c\u6355\u6349\u5168\u5c40\u51e0\u4f55\u4fe1\u606f\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u5c0f\u5206\u5b50\u548c\u590d\u6742\u6750\u6599\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u9ad8\u6548\u67b6\u6784\u4e0b\u7684\u6700\u4f73\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.07809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07809", "abs": "https://arxiv.org/abs/2508.07809", "authors": ["Huanyu Liu", "Jia Li", "Chang Yu", "Taozhi Chen", "Yihong Dong", "Lecheng Wang", "Hu XiaoLong", "Ge Li"], "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.", "AI": {"tldr": "EvoCoT\u662f\u4e00\u4e2a\u81ea\u6f14\u5316\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u601d\u7ef4\u94fe\u4f18\u5316\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u786c\u95ee\u9898\u4e0a\u56e0\u7a00\u758f\u5956\u52b1\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\uff0c\u4e14\u4f9d\u8d56\u66f4\u5f3a\u6a21\u578b\u6216\u8fc7\u6ee4\u96be\u9898\uff0c\u9650\u5236\u4e86\u6269\u5c55\u6027\u548c\u63a8\u7406\u63d0\u5347\u3002", "method": "EvoCoT\u901a\u8fc7\u81ea\u751f\u6210\u548c\u9a8c\u8bc1\u601d\u7ef4\u94fe\u8f68\u8ff9\u7ea6\u675f\u63a2\u7d22\u7a7a\u95f4\uff0c\u9010\u6b65\u7f29\u77ed\u8f68\u8ff9\u4ee5\u53ef\u63a7\u65b9\u5f0f\u6269\u5c55\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEvoCoT\u80fd\u89e3\u51b3\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u517c\u5bb9\u591a\u79cdRL\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "EvoCoT\u4e3a\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.07887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07887", "abs": "https://arxiv.org/abs/2508.07887", "authors": ["Sabrina Namazova", "Alessandra Brondetta", "Younes Strittmatter", "Matthew Nassar", "Sebastian Musslick"], "title": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant", "comment": null, "summary": "Simulators have revolutionized scientific practice across the natural\nsciences. By generating data that reliably approximate real-world phenomena,\nthey enable scientists to accelerate hypothesis testing and optimize\nexperimental designs. This is perhaps best illustrated by AlphaFold, a\nNobel-prize winning simulator in chemistry that predicts protein structures\nfrom amino acid sequences, enabling rapid prototyping of molecular\ninteractions, drug targets, and protein functions. In the behavioral sciences,\na reliable participant simulator - a system capable of producing human-like\nbehavior across cognitive tasks - would represent a similarly transformative\nadvance. Recently, Binz et al. introduced Centaur, a large language model (LLM)\nfine-tuned on human data from 160 experiments, proposing its use not only as a\nmodel of cognition but also as a participant simulator for \"in silico\nprototyping of experimental studies\", e.g., to advance automated cognitive\nscience. Here, we review the core criteria for a participant simulator and\nassess how well Centaur meets them. Although Centaur demonstrates strong\npredictive accuracy, its generative behavior - a critical criterion for a\nparticipant simulator - systematically diverges from human data. This suggests\nthat, while Centaur is a significant step toward predicting human behavior, it\ndoes not yet meet the standards of a reliable participant simulator or an\naccurate model of cognition.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6a21\u62df\u5668\u5728\u79d1\u5b66\u5b9e\u8df5\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u884c\u4e3a\u79d1\u5b66\u4e2d\u7684\u53c2\u4e0e\u8005\u6a21\u62df\u5668Centaur\uff0c\u4f46\u5176\u751f\u6210\u884c\u4e3a\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "motivation": "\u6a21\u62df\u5668\u5728\u81ea\u7136\u79d1\u5b66\u4e2d\u5df2\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u5982AlphaFold\u3002\u884c\u4e3a\u79d1\u5b66\u9700\u8981\u7c7b\u4f3c\u7684\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u5668\uff0c\u4ee5\u52a0\u901f\u5b9e\u9a8c\u8bbe\u8ba1\u548c\u5047\u8bbe\u6d4b\u8bd5\u3002", "method": "\u8bc4\u4f30Centaur\uff08\u57fa\u4e8e160\u4e2a\u5b9e\u9a8c\u6570\u636e\u5fae\u8c03\u7684LLM\uff09\u4f5c\u4e3a\u53c2\u4e0e\u8005\u6a21\u62df\u5668\u7684\u6838\u5fc3\u6807\u51c6\uff0c\u7279\u522b\u662f\u5176\u751f\u6210\u884c\u4e3a\u3002", "result": "Centaur\u9884\u6d4b\u51c6\u786e\u5ea6\u9ad8\uff0c\u4f46\u751f\u6210\u884c\u4e3a\u4e0e\u4eba\u7c7b\u6570\u636e\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "conclusion": "Centaur\u867d\u5728\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u6709\u8fdb\u5c55\uff0c\u4f46\u5c1a\u672a\u8fbe\u5230\u53ef\u9760\u53c2\u4e0e\u8005\u6a21\u62df\u5668\u6216\u51c6\u786e\u8ba4\u77e5\u6a21\u578b\u7684\u6807\u51c6\u3002"}}
{"id": "2508.07926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07926", "abs": "https://arxiv.org/abs/2508.07926", "authors": ["Liang Hou", "Yuan Gao", "Boyuan Jiang", "Xin Tao", "Qi Yan", "Renjie Liao", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "Score Augmentation for Diffusion Models", "comment": null, "summary": "Diffusion models have achieved remarkable success in generative modeling.\nHowever, this study confirms the existence of overfitting in diffusion model\ntraining, particularly in data-limited regimes. To address this challenge, we\npropose Score Augmentation (ScoreAug), a novel data augmentation framework\nspecifically designed for diffusion models. Unlike conventional augmentation\napproaches that operate on clean data, ScoreAug applies transformations to\nnoisy data, aligning with the inherent denoising mechanism of diffusion.\nCrucially, ScoreAug further requires the denoiser to predict the augmentation\nof the original target. This design establishes an equivariant learning\nobjective, enabling the denoiser to learn scores across varied denoising\nspaces, thereby realizing what we term score augmentation. We also\ntheoretically analyze the relationship between scores in different spaces under\ngeneral transformations. In experiments, we extensively validate ScoreAug on\nmultiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with\nresults demonstrating significant performance improvements over baselines.\nNotably, ScoreAug effectively mitigates overfitting across diverse scenarios,\nsuch as varying data scales and model capacities, while exhibiting stable\nconvergence properties. Another advantage of ScoreAug over standard data\naugmentation lies in its ability to circumvent data leakage issues under\ncertain conditions. Furthermore, we show that ScoreAug can be synergistically\ncombined with traditional data augmentation techniques to achieve additional\nperformance gains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faScoreAug\uff0c\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u566a\u58f0\u6570\u636e\u7f13\u89e3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6570\u636e\u6709\u9650\u65f6\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faScoreAug\u6846\u67b6\uff0c\u5bf9\u566a\u58f0\u6570\u636e\u8fdb\u884c\u53d8\u6362\uff0c\u5e76\u8981\u6c42\u53bb\u566a\u5668\u9884\u6d4b\u539f\u59cb\u76ee\u6807\u7684\u589e\u5f3a\u7ed3\u679c\uff0c\u5b9e\u73b0\u5206\u6570\u589e\u5f3a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cScoreAug\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u62df\u5408\uff0c\u4e14\u4e0e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7ed3\u5408\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "ScoreAug\u662f\u4e00\u79cd\u6709\u6548\u7684\u6269\u6563\u6a21\u578b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u7a33\u5b9a\u6536\u655b\u5e76\u907f\u514d\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u3002"}}
{"id": "2508.07927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07927", "abs": "https://arxiv.org/abs/2508.07927", "authors": ["Amal Saadallah", "Abdulaziz Al-Ademi"], "title": "Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting", "comment": null, "summary": "Time series forecasting poses significant challenges in non-stationary\nenvironments where underlying patterns evolve over time. In this work, we\npropose a novel framework that enhances deep neural network (DNN) performance\nby leveraging specialized model adaptation and selection. Initially, a base DNN\nis trained offline on historical time series data. A reserved validation subset\nis then segmented to extract and cluster the most dominant patterns within the\nseries, thereby identifying distinct regimes. For each identified cluster, the\nbase DNN is fine-tuned to produce a specialized version that captures unique\npattern characteristics. At inference, the most recent input is matched against\nthe cluster centroids, and the corresponding fine-tuned version is deployed\nbased on the closest similarity measure. Additionally, our approach integrates\na concept drift detection mechanism to identify and adapt to emerging patterns\ncaused by non-stationary behavior. The proposed framework is generalizable\nacross various DNN architectures and has demonstrated significant performance\ngains on both traditional DNNs and recent advanced architectures implemented in\nthe GluonTS library.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u578b\u9002\u5e94\u548c\u9009\u62e9\u63d0\u5347DNN\u5728\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u6027\u80fd\u7684\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5176\u4e2d\u5e95\u5c42\u6a21\u5f0f\u968f\u65f6\u95f4\u53d8\u5316\u3002", "method": "\u79bb\u7ebf\u8bad\u7ec3\u57fa\u7840DNN\uff0c\u5206\u5272\u9a8c\u8bc1\u5b50\u96c6\u4ee5\u805a\u7c7b\u4e3b\u5bfc\u6a21\u5f0f\uff0c\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u5fae\u8c03DNN\uff0c\u63a8\u7406\u65f6\u6839\u636e\u76f8\u4f3c\u5ea6\u9009\u62e9\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u3002", "result": "\u5728GluonTS\u5e93\u4e2d\u7684\u4f20\u7edf\u548c\u5148\u8fdbDNN\u67b6\u6784\u4e0a\u5747\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u7528\u6027\u5f3a\uff0c\u80fd\u6709\u6548\u9002\u5e94\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u6a21\u5f0f\u53d8\u5316\u3002"}}
{"id": "2508.07952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07952", "abs": "https://arxiv.org/abs/2508.07952", "authors": ["Richard J. Fawley", "Renato Cordeiro de Amorim"], "title": "Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters", "comment": null, "summary": "Clustering algorithms often assume all features contribute equally to the\ndata structure, an assumption that usually fails in high-dimensional or noisy\nsettings. Feature weighting methods can address this, but most require\nadditional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a\nfeature-weighted clustering algorithm motivated by the use of Shapley values\nfrom cooperative game theory to quantify feature relevance, which requires no\nadditional parameters beyond those in $k$-means. We prove that the $k$-means\nobjective can be decomposed into a sum of per-feature Shapley values, providing\nan axiomatic foundation for unsupervised feature relevance and reducing Shapley\ncomputation from exponential to polynomial time. SHARK iteratively re-weights\nfeatures by the inverse of their Shapley contribution, emphasising informative\ndimensions and down-weighting irrelevant ones. Experiments on synthetic and\nreal-world data sets show that SHARK consistently matches or outperforms\nexisting methods, achieving superior robustness and accuracy, particularly in\nscenarios where noise may be present. Software:\nhttps://github.com/rickfawley/shark.", "AI": {"tldr": "SHARK\u662f\u4e00\u79cd\u57fa\u4e8eShapley\u503c\u7684\u7279\u5f81\u52a0\u6743\u805a\u7c7b\u7b97\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u8c03\u6574\uff0c\u901a\u8fc7\u5206\u89e3k\u5747\u503c\u76ee\u6807\u4e3aShapley\u503c\uff0c\u9ad8\u6548\u8ba1\u7b97\u7279\u5f81\u76f8\u5173\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u7ef4\u6216\u566a\u58f0\u6570\u636e\u4e2d\uff0c\u4f20\u7edf\u805a\u7c7b\u7b97\u6cd5\u5047\u8bbe\u6240\u6709\u7279\u5f81\u8d21\u732e\u5747\u7b49\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u7279\u5f81\u52a0\u6743\u65b9\u6cd5\u867d\u6709\u6548\uff0c\u4f46\u901a\u5e38\u9700\u8981\u989d\u5916\u53c2\u6570\u8c03\u4f18\u3002", "method": "SHARK\u5229\u7528Shapley\u503c\u91cf\u5316\u7279\u5f81\u76f8\u5173\u6027\uff0c\u5c06k\u5747\u503c\u76ee\u6807\u5206\u89e3\u4e3a\u5404\u7279\u5f81\u7684Shapley\u503c\uff0c\u901a\u8fc7\u9006\u6743\u91cd\u8fed\u4ee3\u8c03\u6574\u7279\u5f81\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSHARK\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u566a\u58f0\u573a\u666f\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "SHARK\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u53c2\u6570\u7684\u7279\u5f81\u52a0\u6743\u805a\u7c7b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u566a\u58f0\u6570\u636e\u3002"}}
{"id": "2508.07970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07970", "abs": "https://arxiv.org/abs/2508.07970", "authors": ["Junyu Wu", "Weiming Chang", "Xiaotao Liu", "Guanyou He", "Tingfeng Xian", "Haoqiang Hong", "Boqi Chen", "Haotao Tian", "Tao Yang", "Yunsheng Shi", "Feng Lin", "Ting Yao"], "title": "WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.22789", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent\nparadigm for training large language models and multimodal systems. Despite\nnotable advances enabled by existing RLHF training frameworks, significant\nchallenges remain in scaling to complex multimodal workflows and adapting to\ndynamic workloads. In particular, current systems often encounter limitations\nrelated to controller scalability when managing large models, as well as\ninefficiencies in orchestrating intricate RLHF pipelines, especially in\nscenarios that require dynamic sampling and resource allocation. In this paper,\nwe introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,\nscalable, and balanced RLHF training framework specifically designed to address\nthese challenges. WeChat-YATT features a parallel controller programming model\nthat enables flexible and efficient orchestration of complex RLHF workflows,\neffectively mitigating the bottlenecks associated with centralized controller\narchitectures and facilitating scalability in large-scale data scenarios. In\naddition, we propose a dynamic placement schema that adaptively partitions\ncomputational resources and schedules workloads, thereby significantly reducing\nhardware idle time and improving GPU utilization under variable training\nconditions. We evaluate WeChat-YATT across a range of experimental scenarios,\ndemonstrating that it achieves substantial improvements in throughput compared\nto state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been\nsuccessfully deployed to train models supporting WeChat product features for a\nlarge-scale user base, underscoring its effectiveness and robustness in\nreal-world applications.", "AI": {"tldr": "WeChat-YATT\u662f\u4e00\u4e2a\u7b80\u5355\u3001\u53ef\u6269\u5c55\u4e14\u5e73\u8861\u7684RLHF\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u5728\u590d\u6742\u591a\u6a21\u6001\u5de5\u4f5c\u6d41\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709RLHF\u8bad\u7ec3\u6846\u67b6\u5728\u6269\u5c55\u6027\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u6a21\u578b\u548c\u590d\u6742\u6d41\u7a0b\u4e2d\u3002", "method": "WeChat-YATT\u91c7\u7528\u5e76\u884c\u63a7\u5236\u5668\u7f16\u7a0b\u6a21\u578b\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u4f18\u5316\u4e86RLHF\u5de5\u4f5c\u6d41\u7684\u7f16\u6392\u548c\u8d44\u6e90\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWeChat-YATT\u5728\u541e\u5410\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5fae\u4fe1\u4ea7\u54c1\u7684\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "WeChat-YATT\u6709\u6548\u89e3\u51b3\u4e86RLHF\u8bad\u7ec3\u4e2d\u7684\u6269\u5c55\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.08002", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.08002", "abs": "https://arxiv.org/abs/2508.08002", "authors": ["Hongxin Yu", "Yibing Wang", "Fengyue Jin", "Meng Zhang", "Anni Chen"], "title": "A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation", "comment": "18 pages, 9 figures", "summary": "Traffic state estimation (TSE) falls methodologically into three categories:\nmodel-driven, data-driven, and model-data dual-driven. Model-driven TSE relies\non macroscopic traffic flow models originated from hydrodynamics. Data-driven\nTSE leverages historical sensing data and employs statistical models or machine\nlearning methods to infer traffic state. Model-data dual-driven traffic state\nestimation attempts to harness the strengths of both aspects to achieve more\naccurate TSE. From the perspective of mathematical operator theory, TSE can be\nviewed as a type of operator that maps available measurements of inerested\ntraffic state into unmeasured traffic state variables in real time. For the\nfirst time this paper proposes to study real-time freeway TSE in the idea of\nphysics-informed deep operator network (PI-DeepONet), which is an\noperator-oriented architecture embedding traffic flow models based on deep\nneural networks. The paper has developed an extended architecture from the\noriginal PI-DeepONet. The extended architecture is featured with: (1) the\nacceptance of 2-D data input so as to support CNN-based computations; (2) the\nintroduction of a nonlinear expansion layer, an attention mechanism, and a MIMO\nmechanism; (3) dedicated neural network design for adaptive identification of\ntraffic flow model parameters. A traffic state estimator built on the basis of\nthis extended PI-DeepONet architecture was evaluated with respect to a short\nfreeway stretch of NGSIM and a large-scale urban expressway in China, along\nwith other four baseline TSE methods. The evaluation results demonstrated that\nthis novel TSE method outperformed the baseline methods with high-precision\nestimation results of flow and mean speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff08PI-DeepONet\uff09\u7684\u65b0\u578b\u5b9e\u65f6\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u67b6\u6784\u652f\u63012D\u6570\u636e\u8f93\u5165\u3001\u5f15\u5165\u975e\u7ebf\u6027\u6269\u5c55\u5c42\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u5206\u4e3a\u6a21\u578b\u9a71\u52a8\u3001\u6570\u636e\u9a71\u52a8\u548c\u6a21\u578b-\u6570\u636e\u53cc\u9a71\u52a8\u4e09\u7c7b\uff0c\u5404\u6709\u5c40\u9650\u6027\u3002\u672c\u6587\u65e8\u5728\u7ed3\u5408\u6a21\u578b\u4e0e\u6570\u636e\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u66f4\u7cbe\u786e\u7684\u5b9e\u65f6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u4e86PI-DeepONet\u67b6\u6784\uff0c\u652f\u63012D\u6570\u636e\u8f93\u5165\uff08CNN\u8ba1\u7b97\uff09\u3001\u5f15\u5165\u975e\u7ebf\u6027\u6269\u5c55\u5c42\u3001\u6ce8\u610f\u529b\u673a\u5236\u548cMIMO\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e13\u7528\u795e\u7ecf\u7f51\u7edc\u81ea\u9002\u5e94\u8bc6\u522b\u4ea4\u901a\u6d41\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728NGSIM\u77ed\u8def\u6bb5\u548c\u4e2d\u56fd\u5927\u89c4\u6a21\u57ce\u5e02\u5feb\u901f\u8def\u7684\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6d41\u91cf\u548c\u5e73\u5747\u901f\u5ea6\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u56db\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u57fa\u4e8e\u6269\u5c55PI-DeepONet\u67b6\u6784\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2508.08005", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08005", "abs": "https://arxiv.org/abs/2508.08005", "authors": ["Xiang Li", "Shanshan Wang", "Chenglong Xiao"], "title": "Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP", "comment": "10 pages, 6 figures", "summary": "Extensive experiments and prior studies show that no single maximum clique\nalgorithm consistently performs best across all instances, highlighting the\nimportance of selecting suitable algorithms based on instance features. Through\nan extensive analysis of relevant studies, it is found that there is a lack of\nresearch work concerning algorithm selection oriented toward the Maximum Clique\nProblem (MCP). In this work, we propose a learning-based framework that\nintegrates both traditional machine learning and graph neural networks to\naddress this gap. We construct a labeled dataset by running four exact MCP\nalgorithms on a diverse collection of graph instances, accompanied by\nstructural and global statistical features extracted from each graph. We first\nevaluate four conventional classifiers: Support Vector Machine (SVM), Random\nForest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple\ndataset variants. Experimental results show that RF consistently shows strong\nperformance across metrics and dataset variants, making it a reliable baseline.\nIn addition, feature importance analysis indicates that connectivity and\ntopological structure are strong predictors of algorithm performance. Building\non these findings, we develop a dual-channel model named GAT-MLP, which\ncombines a Graph Attention Network (GAT) for local structural encoding with a\nMultilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model\nshows strong and consistent performance across all metrics. Our results\nhighlight the effectiveness of dual-channel architectures and the promise of\ngraph neural networks in combinatorial algorithm selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u5927\u56e2\u95ee\u9898\uff08MCP\uff09\u7684\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u6ca1\u6709\u5355\u4e00\u7684\u6700\u5927\u56e2\u7b97\u6cd5\u5728\u6240\u6709\u5b9e\u4f8b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u6839\u636e\u5b9e\u4f8b\u7279\u5f81\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\u3002\u7136\u800c\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9MCP\u7684\u7b97\u6cd5\u9009\u62e9\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6807\u8bb0\u6570\u636e\u96c6\uff0c\u8fd0\u884c\u56db\u79cd\u7cbe\u786eMCP\u7b97\u6cd5\uff0c\u5e76\u63d0\u53d6\u56fe\u7684\u7ed3\u6784\u548c\u5168\u5c40\u7edf\u8ba1\u7279\u5f81\u3002\u8bc4\u4f30\u4e86\u56db\u79cd\u4f20\u7edf\u5206\u7c7b\u5668\uff08SVM\u3001RF\u3001DT\u3001KNN\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u53cc\u901a\u9053\u6a21\u578bGAT-MLP\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3002", "result": "\u968f\u673a\u68ee\u6797\uff08RF\uff09\u8868\u73b0\u7a33\u5b9a\u4e14\u4f18\u5f02\uff0c\u800cGAT-MLP\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u53cc\u901a\u9053\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u53cc\u901a\u9053\u67b6\u6784\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7ec4\u5408\u7b97\u6cd5\u9009\u62e9\u4e2d\u5177\u6709\u6f5c\u529b\uff0cGAT-MLP\u6a21\u578b\u4e3a\u89e3\u51b3MCP\u7b97\u6cd5\u9009\u62e9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.08013", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08013", "abs": "https://arxiv.org/abs/2508.08013", "authors": ["Mohamad Assaad", "Zeinab Nehme", "Merouane Debbah"], "title": "Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks", "comment": null, "summary": "Federated Learning (FL) is an emerging learning framework that enables edge\ndevices to collaboratively train ML models without sharing their local data. FL\nfaces, however, a significant challenge due to the high amount of information\nthat must be exchanged between the devices and the aggregator in the training\nphase, which can exceed the limited capacity of wireless systems. In this\npaper, two communication-efficient FL methods are considered where\ncommunication overhead is reduced by communicating scalar values instead of\nlong vectors and by allowing high number of users to send information\nsimultaneously. The first approach employs a zero-order optimization technique\nwith two-point gradient estimator, while the second involves a first-order\ngradient computation strategy. The novelty lies in leveraging channel\ninformation in the learning algorithms, eliminating hence the need for\nadditional resources to acquire channel state information (CSI) and to remove\nits impact, as well as in considering asynchronous devices. We provide a\nrigorous analytical framework for the two methods, deriving convergence\nguarantees and establishing appropriate performance bounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u5229\u7528\u4fe1\u9053\u4fe1\u606f\u6765\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u9762\u4e34\u901a\u4fe1\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u91cf\u548c\u5229\u7528\u4fe1\u9053\u4fe1\u606f\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u4e00\u662f\u57fa\u4e8e\u96f6\u9636\u4f18\u5316\u7684\u4e24\u70b9\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u4e8c\u662f\u57fa\u4e8e\u4e00\u9636\u68af\u5ea6\u8ba1\u7b97\u7b56\u7565\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u5229\u7528\u4fe1\u9053\u4fe1\u606f\uff0c\u65e0\u9700\u989d\u5916\u8d44\u6e90\u83b7\u53d6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bba\u6587\u4e3a\u4e24\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6536\u655b\u4fdd\u8bc1\u548c\u6027\u80fd\u754c\u9650\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5f02\u6b65\u8bbe\u5907\u73af\u5883\u3002"}}
{"id": "2508.08040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08040", "abs": "https://arxiv.org/abs/2508.08040", "authors": ["Maozhen Zhang", "Mengnan Zhao", "Bo Wang"], "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models", "comment": null, "summary": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.", "AI": {"tldr": "BadPromptFL\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5bf9\u6bd4\u6a21\u578b\u4e2d\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u672c\u5730\u540e\u95e8\u89e6\u53d1\u5668\u548c\u63d0\u793a\u5d4c\u5165\uff0c\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u5728\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u88ab\u653b\u51fb\u7684\u5ba2\u6237\u7aef\u8054\u5408\u4f18\u5316\u672c\u5730\u540e\u95e8\u89e6\u53d1\u5668\u548c\u63d0\u793a\u5d4c\u5165\uff0c\u5c06\u4e2d\u6bd2\u63d0\u793a\u6ce8\u5165\u5168\u5c40\u805a\u5408\u8fc7\u7a0b\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u4e14\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u805a\u5408\u534f\u8bae\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3001\u9690\u853d\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "BadPromptFL\u63ed\u793a\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5f15\u53d1\u4e86\u5bf9\u8be5\u6280\u672f\u5b89\u5168\u6027\u7684\u5173\u6ce8\u3002"}}
{"id": "2508.08052", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08052", "abs": "https://arxiv.org/abs/2508.08052", "authors": ["Supriyo Chakraborty", "Krishnan Raghavan"], "title": "On Understanding of the Dynamics of Model Capacity in Continual Learning", "comment": null, "summary": "The stability-plasticity dilemma, closely related to a neural network's (NN)\ncapacity-its ability to represent tasks-is a fundamental challenge in continual\nlearning (CL). Within this context, we introduce CL's effective model capacity\n(CLEMC) that characterizes the dynamic behavior of the stability-plasticity\nbalance point. We develop a difference equation to model the evolution of the\ninterplay between the NN, task data, and optimization procedure. We then\nleverage CLEMC to demonstrate that the effective capacity-and, by extension,\nthe stability-plasticity balance point is inherently non-stationary. We show\nthat regardless of the NN architecture or optimization method, a NN's ability\nto represent new tasks diminishes when incoming task distributions differ from\nprevious ones. We conduct extensive experiments to support our theoretical\nfindings, spanning a range of architectures-from small feedforward network and\nconvolutional networks to medium-sized graph neural networks and\ntransformer-based large language models with millions of parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aCLEMC\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u63cf\u8ff0\u6301\u7eed\u5b66\u4e60\u4e2d\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u5e73\u8861\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u5e76\u8bc1\u660e\u8fd9\u79cd\u5e73\u8861\u70b9\u662f\u975e\u9759\u6001\u7684\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u56f0\u5883\uff0c\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u540c\u4efb\u52a1\u5206\u5e03\u4e0b\u7684\u8868\u73b0\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u5dee\u5206\u65b9\u7a0b\u5efa\u6a21\u795e\u7ecf\u7f51\u7edc\u3001\u4efb\u52a1\u6570\u636e\u548c\u4f18\u5316\u8fc7\u7a0b\u7684\u4ea4\u4e92\uff0c\u5e76\u5229\u7528CLEMC\u5206\u6790\u6709\u6548\u5bb9\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u8bba\u7f51\u7edc\u67b6\u6784\u6216\u4f18\u5316\u65b9\u6cd5\u5982\u4f55\uff0c\u5f53\u65b0\u4efb\u52a1\u5206\u5e03\u4e0e\u4e4b\u524d\u4e0d\u540c\u65f6\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u80fd\u529b\u4f1a\u4e0b\u964d\u3002", "conclusion": "CLEMC\u63ed\u793a\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u5e73\u8861\u7684\u52a8\u6001\u7279\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.08061", "categories": ["cs.LG", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.08061", "abs": "https://arxiv.org/abs/2508.08061", "authors": ["Sven Weinzierl", "Sandra Zilker", "Annina Liessmann", "Martin K\u00e4ppel", "Weixin Wang", "Martin Matzner"], "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations", "comment": null, "summary": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u9884\u6d4b\u6d41\u7a0b\u76d1\u63a7\u6280\u672f\uff0c\u5e2e\u52a9\u8d44\u6e90\u4e0d\u8db3\u7684\u7ec4\u7ec7\u5b9e\u73b0\u6709\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u6d41\u7a0b\u76d1\u63a7\u6280\u672f\u9700\u8981\u5927\u91cf\u4e8b\u4ef6\u6570\u636e\u6216\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u90e8\u5206\u7ec4\u7ec7\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u5c06\u77e5\u8bc6\u4ece\u4e00\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u8fc1\u79fb\u5230\u76f8\u4f3c\u4e1a\u52a1\u6d41\u7a0b\uff0c\u652f\u6301\u8de8\u7ec4\u7ec7\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u7ec4\u7ec7\u73af\u5883\u4e2d\u80fd\u6709\u6548\u5b9e\u73b0\u9884\u6d4b\u6d41\u7a0b\u76d1\u63a7\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u9884\u6d4b\u6d41\u7a0b\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08071", "categories": ["cs.LG", "cs.AI", "J.1; I.2.4; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.08071", "abs": "https://arxiv.org/abs/2508.08071", "authors": ["Yunqing Li", "Zixiang Tang", "Jiaying Zhuang", "Zhenyu Yang", "Farhad Ameri", "Jianbang Zhang"], "title": "C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction", "comment": "Accepted as a poster presentation at the KDD 2025 Workshop on AI for\n  Supply Chain (AI4SupplyChain)", "summary": "Connecting an ever-expanding catalogue of products with suitable\nmanufacturers and suppliers is critical for resilient, efficient global supply\nchains, yet traditional methods struggle to capture complex capabilities,\ncertifications, geographic constraints, and rich multimodal data of real-world\nmanufacturer profiles. To address these gaps, we introduce PMGraph, a public\nbenchmark of bipartite and heterogeneous multimodal supply-chain graphs linking\n8,888 manufacturers, over 70k products, more than 110k manufacturer-product\nedges, and over 29k product images. Building on this benchmark, we propose the\nCascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first\naligns and aggregates textual and visual attributes into intermediate group\nembeddings, then propagates them through a manufacturer-product hetero-graph\nvia multiscale message passing to enhance link prediction accuracy. C-MAG also\nprovides practical guidelines for modality-aware fusion, preserving predictive\nperformance in noisy, real-world settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PMGraph\u57fa\u51c6\u548cC-MAG\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u4f9b\u5e94\u94fe\u4e2d\u5236\u9020\u5546\u4e0e\u4ea7\u54c1\u5339\u914d\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u548c\u56fe\u4f20\u64ad\u63d0\u5347\u94fe\u63a5\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5236\u9020\u5546\u548c\u4ea7\u54c1\u4e4b\u95f4\u7684\u590d\u6742\u80fd\u529b\u3001\u8ba4\u8bc1\u3001\u5730\u7406\u7ea6\u675f\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u5f71\u54cd\u4e86\u4f9b\u5e94\u94fe\u7684\u6548\u7387\u548c\u97e7\u6027\u3002", "method": "\u63d0\u51faPMGraph\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u4f9b\u5e94\u94fe\u56fe\u6570\u636e\uff1b\u8bbe\u8ba1C-MAG\u67b6\u6784\uff0c\u5206\u4e24\u9636\u6bb5\u5bf9\u9f50\u548c\u805a\u5408\u591a\u6a21\u6001\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u6d88\u606f\u4f20\u9012\u589e\u5f3a\u94fe\u63a5\u9884\u6d4b\u3002", "result": "C-MAG\u5728\u566a\u58f0\u73af\u5883\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5236\u9020\u5546\u4e0e\u4ea7\u54c1\u5339\u914d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "PMGraph\u548cC-MAG\u4e3a\u4f9b\u5e94\u94fe\u4e2d\u7684\u590d\u6742\u5339\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.08073", "categories": ["cs.LG", "cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.08073", "abs": "https://arxiv.org/abs/2508.08073", "authors": ["Dimitris Tsaras", "Xing Li", "Lei Chen", "Zhiyao Xie", "Mingxuan Yuan"], "title": "ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring", "comment": "Accepted to DAC 2025", "summary": "In electronic design automation, logic optimization operators play a crucial\nrole in minimizing the gate count of logic circuits. However, their computation\ndemands are high. Operators such as refactor conventionally form iterative cuts\nfor each node, striving for a more compact representation - a task which often\nfails 98% on average. Prior research has sought to mitigate computational cost\nthrough parallelization. In contrast, our approach leverages a classifier to\nprune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis\noperations. Experiments on the refactor operator using the EPFL benchmark suite\nand 10 large industrial designs demonstrate that this technique can speedup\nlogic optimization by 3.9x on average compared with the state-of-the-art ABC\nimplementation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u9884\u526a\u679d\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u903b\u8f91\u4f18\u5316\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u903b\u8f91\u4f18\u5316\u7b97\u5b50\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5927\u90e8\u5206\u5c1d\u8bd5\u5931\u8d25\uff0c\u9700\u51cf\u5c11\u65e0\u6548\u64cd\u4f5c\u3002", "method": "\u5229\u7528\u5206\u7c7b\u5668\u9884\u5224\u5e76\u526a\u679d\u4e0d\u6210\u529f\u7684\u5207\u5272\uff0c\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u91cd\u5408\u6210\u64cd\u4f5c\u3002", "result": "\u5728EPFL\u57fa\u51c6\u5957\u4ef6\u548c10\u4e2a\u5927\u578b\u5de5\u4e1a\u8bbe\u8ba1\u4e0a\uff0c\u5e73\u5747\u63d0\u901f3.9\u500d\u3002", "conclusion": "\u5206\u7c7b\u5668\u9884\u526a\u679d\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u903b\u8f91\u4f18\u5316\u6548\u7387\u3002"}}
{"id": "2508.08080", "categories": ["cs.LG", "cs.NE", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.08080", "abs": "https://arxiv.org/abs/2508.08080", "authors": ["Cas Oude Hoekstra", "Floris den Hengst"], "title": "Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles", "comment": null, "summary": "Symbolic Regression (SR) is a well-established framework for generating\ninterpretable or white-box predictive models. Although SR has been successfully\napplied to create interpretable estimates of the average of the outcome, it is\ncurrently not well understood how it can be used to estimate the relationship\nbetween variables at other points in the distribution of the target variable.\nSuch estimates of e.g. the median or an extreme value provide a fuller picture\nof how predictive variables affect the outcome and are necessary in\nhigh-stakes, safety-critical application domains. This study introduces\nSymbolic Quantile Regression (SQR), an approach to predict conditional\nquantiles with SR. In an extensive evaluation, we find that SQR outperforms\ntransparent models and performs comparably to a strong black-box baseline\nwithout compromising transparency. We also show how SQR can be used to explain\ndifferences in the target distribution by comparing models that predict extreme\nand central outcomes in an airline fuel usage case study. We conclude that SQR\nis suitable for predicting conditional quantiles and understanding interesting\nfeature influences at varying quantiles.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u7b26\u53f7\u5206\u4f4d\u6570\u56de\u5f52\uff08SQR\uff09\uff0c\u4e00\u79cd\u5229\u7528\u7b26\u53f7\u56de\u5f52\uff08SR\uff09\u9884\u6d4b\u6761\u4ef6\u5206\u4f4d\u6570\u7684\u65b9\u6cd5\uff0c\u5728\u900f\u660e\u6027\u548c\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7b26\u53f7\u56de\u5f52\u867d\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u4f30\u8ba1\u76ee\u6807\u53d8\u91cf\u7684\u5e73\u5747\u503c\uff0c\u65e0\u6cd5\u5168\u9762\u63cf\u8ff0\u53d8\u91cf\u95f4\u5173\u7cfb\u3002SQR\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u7684\u9886\u57df\u3002", "method": "\u63d0\u51fa\u7b26\u53f7\u5206\u4f4d\u6570\u56de\u5f52\uff08SQR\uff09\uff0c\u901a\u8fc7\u7b26\u53f7\u56de\u5f52\u9884\u6d4b\u6761\u4ef6\u5206\u4f4d\u6570\uff0c\u5e76\u5728\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "SQR\u5728\u900f\u660e\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e0e\u9ed1\u76d2\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u6781\u7aef\u503c\u548c\u4e2d\u5fc3\u503c\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "SQR\u9002\u7528\u4e8e\u9884\u6d4b\u6761\u4ef6\u5206\u4f4d\u6570\uff0c\u5e76\u80fd\u5e2e\u52a9\u7406\u89e3\u4e0d\u540c\u5206\u4f4d\u6570\u4e0b\u7684\u7279\u5f81\u5f71\u54cd\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u900f\u660e\u4e14\u51c6\u786e\u7684\u6a21\u578b\u3002"}}
{"id": "2508.08087", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.08087", "abs": "https://arxiv.org/abs/2508.08087", "authors": ["Amir Ali Panahi", "Daniel Luder", "Billy Wu", "Gregory Offer", "Dirk Uwe Sauer", "Weihan Li"], "title": "Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation", "comment": "31 pages, 6 figures", "summary": "Reliable digital twins of lithium-ion batteries must achieve high physical\nfidelity with sub-millisecond speed. In this work, we benchmark three\noperator-learning surrogates for the Single Particle Model (SPM): Deep Operator\nNetworks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed\nparameter-embedded Fourier Neural Operator (PE-FNO), which conditions each\nspectral layer on particle radius and solid-phase diffusivity. Models are\ntrained on simulated trajectories spanning four current families (constant,\ntriangular, pulse-train, and Gaussian-random-field) and a full range of\nState-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates\nconstant-current behaviour but struggles with more dynamic loads. The basic FNO\nmaintains mesh invariance and keeps concentration errors below 1 %, with\nvoltage mean-absolute errors under 1.7 mV across all load types. Introducing\nparameter embedding marginally increases error, but enables generalisation to\nvarying radii and diffusivities. PE-FNO executes approximately 200 times faster\nthan a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse\ntasks are explored in a parameter estimation task with Bayesian optimisation,\nrecovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute\npercentage error, respectively, and 0.5918 percentage points higher error in\ncomparison with classical methods. These results pave the way for neural\noperators to meet the accuracy, speed and parametric flexibility demands of\nreal-time battery management, design-of-experiments and large-scale inference.\nPE-FNO outperforms conventional neural surrogates, offering a practical path\ntowards high-speed and high-fidelity electrochemical digital twins.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u7b97\u5b50\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\uff08DeepONets\u3001FNOs\u548cPE-FNO\uff09\u5728\u9502\u79bb\u5b50\u7535\u6c60\u5355\u7c92\u5b50\u6a21\u578b\uff08SPM\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51faPE-FNO\u5728\u901f\u5ea6\u548c\u53c2\u6570\u7075\u6d3b\u6027\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u9502\u79bb\u5b50\u7535\u6c60\u6570\u5b57\u5b6a\u751f\u5bf9\u9ad8\u7269\u7406\u4fdd\u771f\u5ea6\u548c\u4e9a\u6beb\u79d2\u7ea7\u901f\u5ea6\u7684\u9700\u6c42\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u6a21\u578b\u3002", "method": "\u8bad\u7ec3\u4e86\u4e09\u79cd\u6a21\u578b\uff08DeepONets\u3001FNOs\u548cPE-FNO\uff09\u5728\u4e0d\u540c\u7535\u6d41\u8d1f\u8f7d\u548cSOC\u8303\u56f4\u5185\u7684\u6a21\u62df\u8f68\u8ff9\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "PE-FNO\u5728\u901f\u5ea6\u548c\u53c2\u6570\u7075\u6d3b\u6027\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb200\u500d\uff0c\u4e14\u5728\u53c2\u6570\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8bef\u5dee\u8f83\u4f4e\u3002", "conclusion": "PE-FNO\u4e3a\u5b9e\u65f6\u7535\u6c60\u7ba1\u7406\u548c\u5927\u89c4\u6a21\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u901f\u5ea6\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u6a21\u578b\u3002"}}
{"id": "2508.08100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08100", "abs": "https://arxiv.org/abs/2508.08100", "authors": ["Md. Wasiul Haque", "Sagar Dasgupta", "Mizanur Rahman"], "title": "Grid2Guide: A* Enabled Small Language Model for Indoor Navigation", "comment": "23 pages, 8 figures, 6 tables", "summary": "Reliable indoor navigation remains a significant challenge in complex\nenvironments, particularly where external positioning signals and dedicated\ninfrastructures are unavailable. This research presents Grid2Guide, a hybrid\nnavigation framework that combines the A* search algorithm with a Small\nLanguage Model (SLM) to generate clear, human-readable route instructions. The\nframework first conducts a binary occupancy matrix from a given indoor map.\nUsing this matrix, the A* algorithm computes the optimal path between origin\nand destination, producing concise textual navigation steps. These steps are\nthen transformed into natural language instructions by the SLM, enhancing\ninterpretability for end users. Experimental evaluations across various indoor\nscenarios demonstrate the method's effectiveness in producing accurate and\ntimely navigation guidance. The results validate the proposed approach as a\nlightweight, infrastructure-free solution for real-time indoor navigation\nsupport.", "AI": {"tldr": "Grid2Guide\u7ed3\u5408A*\u7b97\u6cd5\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u751f\u6210\u6e05\u6670\u3001\u6613\u8bfb\u7684\u5ba4\u5185\u5bfc\u822a\u6307\u4ee4\u3002", "motivation": "\u590d\u6742\u73af\u5883\u4e2d\u7f3a\u4e4f\u5916\u90e8\u5b9a\u4f4d\u4fe1\u53f7\u548c\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u65f6\uff0c\u53ef\u9760\u7684\u5ba4\u5185\u5bfc\u822a\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u4e8c\u8fdb\u5236\u5360\u7528\u77e9\u9635\u548cA*\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u8def\u5f84\uff0c\u518d\u7528SLM\u5c06\u8def\u5f84\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u51c6\u786e\u3001\u53ca\u65f6\u7684\u5bfc\u822a\u6307\u5bfc\u3002", "conclusion": "Grid2Guide\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u65f6\u5ba4\u5185\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08120", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08120", "abs": "https://arxiv.org/abs/2508.08120", "authors": ["Keyan Rahimi", "Md. Wasiul Haque", "Sagar Dasgupta", "Mizanur Rahman"], "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments", "comment": "20 pages, 6 figures, 1 table", "summary": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bfc\u822a\u7684\u5ba4\u5185\u5bfc\u822a\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe96%\uff0c\u5bfc\u822a\u6307\u4ee4\u51c6\u786e\u7387\u4e3a75%\u3002", "motivation": "\u89e3\u51b3\u5ba4\u5185\u73af\u5883\u4e2dGPS\u4fe1\u53f7\u4e0d\u53ef\u9760\u548c\u5efa\u7b51\u590d\u6742\u6027\u5bfc\u81f4\u7684\u5bfc\u822a\u96be\u9898\u3002", "method": "\u4f7f\u7528ResNet-50\u8fdb\u884c\u89c6\u89c9\u5b9a\u4f4d\uff0c\u7ed3\u5408LLM\u751f\u6210\u5bfc\u822a\u6307\u4ee4\u3002", "result": "\u5b9a\u4f4d\u51c6\u786e\u738796%\uff0c\u5bfc\u822a\u6307\u4ee4\u51c6\u786e\u738775%\u3002", "conclusion": "\u5c55\u793a\u4e86\u5229\u7528\u666e\u901a\u6444\u50cf\u5934\u548c\u516c\u5f00\u5e73\u9762\u56fe\u5b9e\u73b0\u65e0\u57fa\u7840\u8bbe\u65bd\u5ba4\u5185\u5bfc\u822a\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08122", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08122", "abs": "https://arxiv.org/abs/2508.08122", "authors": ["Mingrong Lin", "Ke Deng", "Zhengyang Wu", "Zetao Zheng", "Jie Li"], "title": "MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing", "comment": "9 pages, 4 figures", "summary": "Knowledge Tracing (KT) is committed to capturing students' knowledge mastery\nfrom their historical interactions. Simulating students' memory states is a\npromising approach to enhance both the performance and interpretability of\nknowledge tracing models. Memory consists of three fundamental processes:\nencoding, storage, and retrieval. Although forgetting primarily manifests\nduring the storage stage, most existing studies rely on a single,\nundifferentiated forgetting mechanism, overlooking other memory processes as\nwell as personalized forgetting patterns. To address this, this paper proposes\nmemoryKT, a knowledge tracing model based on a novel temporal variational\nautoencoder. The model simulates memory dynamics through a three-stage process:\n(i) Learning the distribution of students' knowledge memory features, (ii)\nReconstructing their exercise feedback, while (iii) Embedding a personalized\nforgetting module within the temporal workflow to dynamically modulate memory\nstorage strength. This jointly models the complete encoding-storage-retrieval\ncycle, significantly enhancing the model's perception capability for individual\ndifferences. Extensive experiments on four public datasets demonstrate that our\nproposed approach significantly outperforms state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u5e8f\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578bmemoryKT\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc7\u7a0b\u6a21\u62df\u8bb0\u5fc6\u52a8\u6001\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u5355\u4e00\u9057\u5fd8\u673a\u5236\uff0c\u5ffd\u89c6\u4e86\u4e2a\u6027\u5316\u9057\u5fd8\u6a21\u5f0f\u53ca\u5176\u4ed6\u8bb0\u5fc6\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u65f6\u5e8f\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5206\u4e09\u9636\u6bb5\u6a21\u62df\u8bb0\u5fc6\u52a8\u6001\uff1a\u5b66\u4e60\u77e5\u8bc6\u8bb0\u5fc6\u7279\u5f81\u5206\u5e03\u3001\u91cd\u6784\u7ec3\u4e60\u53cd\u9988\u3001\u5d4c\u5165\u4e2a\u6027\u5316\u9057\u5fd8\u6a21\u5757\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "memoryKT\u901a\u8fc7\u5b8c\u6574\u5efa\u6a21\u7f16\u7801-\u5b58\u50a8-\u68c0\u7d22\u5faa\u73af\uff0c\u589e\u5f3a\u4e86\u5bf9\u4e2a\u4f53\u5dee\u5f02\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.08124", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08124", "abs": "https://arxiv.org/abs/2508.08124", "authors": ["Guanghao Jin", "Yuan Liang", "Yihan Ma", "Jingpei Wu", "Guoyang Liu"], "title": "NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection", "comment": null, "summary": "Large-scale models pre-trained on Electroencephalography (EEG) have shown\npromise in clinical applications such as neurological disorder detection.\nHowever, the practical deployment of EEG-based large-scale models faces\ncritical challenges such as limited labeled EEG data and suboptimal performance\nin clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel\nlarge-scale model specifically designed for detecting EEG-based neurological\ndisorders. Our key contributions include (i) a Selective Temporal-Frequency\nEmbedding mechanism that adaptively captures complex temporal and spectral\npatterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy\nthat refines feature representation in a two-stage process. In the first stage,\nour model learns the fundamental discriminative features of EEG activities; in\nthe second stage, the model further extracts more specialized fine-grained\nfeatures for accurate diagnostic performance. We evaluated NeuroDx-LM on the\nCHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in\nEEG-based seizure and schizophrenia detection, respectively. These results\ndemonstrate the great potential of EEG-based large-scale models to advance\nclinical applicability. Our code is available at\nhttps://github.com/LetItBe12345/NeuroDx-LM.", "AI": {"tldr": "NeuroDx-LM\u662f\u4e00\u79cd\u65b0\u578b\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u57fa\u4e8eEEG\u7684\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u65f6\u9891\u5d4c\u5165\u673a\u5236\u548c\u6e10\u8fdb\u7279\u5f81\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728CHB-MIT\u548c\u7cbe\u795e\u5206\u88c2\u75c7\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3EEG\u5927\u89c4\u6a21\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u6807\u8bb0\u6570\u636e\u6709\u9650\u548c\u4e34\u5e8a\u573a\u666f\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u65f6\u9891\u5d4c\u5165\u673a\u5236\u548c\u6e10\u8fdb\u7279\u5f81\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff0c\u5206\u4e24\u9636\u6bb5\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728CHB-MIT\u548c\u7cbe\u795e\u5206\u88c2\u75c7\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "EEG\u5927\u89c4\u6a21\u6a21\u578b\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.08126", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08126", "abs": "https://arxiv.org/abs/2508.08126", "authors": ["Hadi Khorsand", "Vahid Pourahmadi"], "title": "OFAL: An Oracle-Free Active Learning Framework", "comment": null, "summary": "In the active learning paradigm, using an oracle to label data has always\nbeen a complex and expensive task, and with the emersion of large unlabeled\ndata pools, it would be highly beneficial If we could achieve better results\nwithout relying on an oracle. This research introduces OFAL, an oracle-free\nactive learning scheme that utilizes neural network uncertainty. OFAL uses the\nmodel's own uncertainty to transform highly confident unlabeled samples into\ninformative uncertain samples. First, we start with separating and quantifying\ndifferent parts of uncertainty and introduce Monte Carlo Dropouts as an\napproximation of the Bayesian Neural Network model. Secondly, by adding a\nvariational autoencoder, we go on to generate new uncertain samples by stepping\ntoward the uncertain part of latent space starting from a confidence seed\nsample. By generating these new informative samples, we can perform active\nlearning and enhance the model's accuracy. Lastly, we try to compare and\nintegrate our method with other widely used active learning sampling methods.", "AI": {"tldr": "OFAL\u662f\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u6837\u672c\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u9762\u5bf9\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\uff0c\u7814\u7a76\u65e8\u5728\u51cf\u5c11\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002", "method": "1. \u5206\u79bb\u548c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1bDropout\u8fd1\u4f3c\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u30022. \u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u7f6e\u4fe1\u6837\u672c\u751f\u6210\u65b0\u7684\u4e0d\u786e\u5b9a\u6837\u672c\u30023. \u7ed3\u5408\u5176\u4ed6\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "OFAL\u80fd\u591f\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u6837\u672c\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u3002", "conclusion": "OFAL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.08151", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08151", "abs": "https://arxiv.org/abs/2508.08151", "authors": ["Moses Openja", "Paolo Arcaini", "Foutse Khomh", "Fuyuki Ishikawa"], "title": "FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks", "comment": null, "summary": "Deep neural networks (DNNs) are being utilized in various aspects of our\ndaily lives, including high-stakes decision-making applications that impact\nindividuals. However, these systems reflect and amplify bias from the data used\nduring training and testing, potentially resulting in biased behavior and\ninaccurate decisions. For instance, having different misclassification rates\nbetween white and black sub-populations. However, effectively and efficiently\nidentifying and correcting biased behavior in DNNs is a challenge. This paper\nintroduces FairFLRep, an automated fairness-aware fault localization and repair\ntechnique that identifies and corrects potentially bias-inducing neurons in DNN\nclassifiers. FairFLRep focuses on adjusting neuron weights associated with\nsensitive attributes, such as race or gender, that contribute to unfair\ndecisions. By analyzing the input-output relationships within the network,\nFairFLRep corrects neurons responsible for disparities in predictive quality\nparity. We evaluate FairFLRep on four image classification datasets using two\nDNN classifiers, and four tabular datasets with a DNN model. The results show\nthat FairFLRep consistently outperforms existing methods in improving fairness\nwhile preserving accuracy. An ablation study confirms the importance of\nconsidering fairness during both fault localization and repair stages. Our\nfindings also show that FairFLRep is more efficient than the baseline\napproaches in repairing the network.", "AI": {"tldr": "FairFLRep\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u516c\u5e73\u6027\u611f\u77e5\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u6280\u672f\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u4fee\u6b63DNN\u5206\u7c7b\u5668\u4e2d\u53ef\u80fd\u5bfc\u81f4\u504f\u89c1\u7684\u795e\u7ecf\u5143\u3002", "motivation": "DNN\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u5e94\u7528\u4e2d\u53ef\u80fd\u653e\u5927\u6570\u636e\u504f\u89c1\uff0c\u5bfc\u81f4\u4e0d\u516c\u5e73\u884c\u4e3a\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u8bc6\u522b\u548c\u4fee\u6b63\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u4e0e\u654f\u611f\u5c5e\u6027\uff08\u5982\u79cd\u65cf\u6216\u6027\u522b\uff09\u76f8\u5173\u7684\u795e\u7ecf\u5143\u6743\u91cd\uff0c\u5206\u6790\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\u4ee5\u4fee\u6b63\u504f\u89c1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cFairFLRep\u5728\u63d0\u9ad8\u516c\u5e73\u6027\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FairFLRep\u5728\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u9636\u6bb5\u5747\u8003\u8651\u516c\u5e73\u6027\uff0c\u6548\u7387\u66f4\u9ad8\uff0c\u662f\u89e3\u51b3DNN\u504f\u89c1\u95ee\u9898\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.08159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08159", "abs": "https://arxiv.org/abs/2508.08159", "authors": ["Cem Ata Baykara", "Saurav Raj Pandey", "Ali Burak \u00dcnal", "Harlin Lee", "Mete Akg\u00fcn"], "title": "Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets", "comment": null, "summary": "Developing accurate and generalizable epileptic seizure prediction models\nfrom electroencephalography (EEG) data across multiple clinical sites is\nhindered by patient privacy regulations and significant data heterogeneity\n(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving\nframework for collaborative training, but standard aggregation methods like\nFederated Averaging (FedAvg) can be biased by dominant datasets in\nheterogeneous settings. This paper investigates FL for seizure prediction using\na single EEG channel across four diverse public datasets (Siena, CHB-MIT,\nHelsinki, NCH), representing distinct patient populations (adult, pediatric,\nneonate) and recording conditions. We implement privacy-preserving global\nnormalization and propose a Random Subset Aggregation strategy, where each\nclient trains on a fixed-size random subset of its data per round, ensuring\nequal contribution during aggregation. Our results show that locally trained\nmodels fail to generalize across sites, and standard weighted FedAvg yields\nhighly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on\nHelsinki and 50.6% on NCH). In contrast, Random Subset Aggregation\nsignificantly improves performance on under-represented clients (accuracy\nincreases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior\nmacro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,\ndemonstrating a more robust and fair global model. This work highlights the\npotential of balanced FL approaches for building effective and generalizable\nseizure prediction systems in realistic, heterogeneous multi-hospital\nenvironments while respecting data privacy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u591a\u4e34\u5e8a\u7ad9\u70b9\u4e2d\u5f00\u53d1\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5b50\u96c6\u805a\u5408\u7b56\u7565\u4ee5\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u7531\u4e8e\u60a3\u8005\u9690\u79c1\u6cd5\u89c4\u548c\u6570\u636e\u5f02\u8d28\u6027\uff08\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u6027\uff09\uff0c\u5f00\u53d1\u8de8\u591a\u4e2a\u4e34\u5e8a\u7ad9\u70b9\u7684\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u8bad\u7ec3\u6846\u67b6\uff0c\u4f46\u6807\u51c6\u805a\u5408\u65b9\u6cd5\uff08\u5982FedAvg\uff09\u5728\u5f02\u8d28\u73af\u5883\u4e0b\u53ef\u80fd\u53d7\u4e3b\u5bfc\u6570\u636e\u96c6\u5f71\u54cd\u800c\u4ea7\u751f\u504f\u5dee\u3002", "method": "\u8bba\u6587\u91c7\u7528\u5355\u901a\u9053EEG\u6570\u636e\uff0c\u8986\u76d6\u56db\u4e2a\u4e0d\u540c\u516c\u5171\u6570\u636e\u96c6\uff08Siena\u3001CHB-MIT\u3001Helsinki\u3001NCH\uff09\uff0c\u4ee3\u8868\u4e0d\u540c\u60a3\u8005\u7fa4\u4f53\u548c\u8bb0\u5f55\u6761\u4ef6\u3002\u63d0\u51fa\u9690\u79c1\u4fdd\u62a4\u7684\u5168\u5c40\u5f52\u4e00\u5316\u548c\u968f\u673a\u5b50\u96c6\u805a\u5408\u7b56\u7565\uff0c\u786e\u4fdd\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5728\u6bcf\u8f6e\u8bad\u7ec3\u4e2d\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u968f\u673a\u6570\u636e\u5b50\u96c6\uff0c\u5b9e\u73b0\u516c\u5e73\u805a\u5408\u3002", "result": "\u672c\u5730\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u6cd5\u8de8\u7ad9\u70b9\u6cdb\u5316\uff0c\u6807\u51c6\u52a0\u6743FedAvg\u6027\u80fd\u4e0d\u5747\u8861\uff08\u5982CHB-MIT\u51c6\u786e\u738789.0%\uff0cHelsinki\u548cNCH\u4ec550.8%\u548c50.6%\uff09\u3002\u968f\u673a\u5b50\u96c6\u805a\u5408\u663e\u8457\u63d0\u5347\u4e86\u5f31\u52bf\u5ba2\u6237\u7aef\u7684\u6027\u80fd\uff08Helsinki\u51c6\u786e\u7387\u63d0\u5347\u81f381.7%\uff0cNCH\u81f368.7%\uff09\uff0c\u5168\u5c40\u5b8f\u5e73\u5747\u51c6\u786e\u7387\u8fbe77.1%\uff0c\u6c60\u5316\u51c6\u786e\u738780.0%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5e73\u8861\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0a\u91cd\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u80fd\u591f\u5728\u5f02\u8d28\u591a\u533b\u9662\u73af\u5883\u4e2d\u6784\u5efa\u6709\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2508.08172", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.08172", "abs": "https://arxiv.org/abs/2508.08172", "authors": ["Vincent Perreault", "Katsumi Inoue", "Richard Labib", "Alain Hertz"], "title": "Neural Logic Networks for Interpretable Classification", "comment": "21 pages, 6 figures, pre-print", "summary": "Traditional neural networks have an impressive classification performance,\nbut what they learn cannot be inspected, verified or extracted. Neural Logic\nNetworks on the other hand have an interpretable structure that enables them to\nlearn a logical mechanism relating the inputs and outputs with AND and OR\noperations. We generalize these networks with NOT operations and biases that\ntake into account unobserved data and develop a rigorous logical and\nprobabilistic modeling in terms of concept combinations to motivate their use.\nWe also propose a novel factorized IF-THEN rule structure for the model as well\nas a modified learning algorithm. Our method improves the state-of-the-art in\nBoolean networks discovery and is able to learn relevant, interpretable rules\nin tabular classification, notably on an example from the medical field where\ninterpretability has tangible value.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u901a\u8fc7\u903b\u8f91\u64cd\u4f5c\uff08AND\u3001OR\u3001NOT\uff09\u548c\u504f\u5dee\u9879\u6539\u8fdb\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u63d0\u51fa\u4e86\u56e0\u5b50\u5316\u7684IF-THEN\u89c4\u5219\u7ed3\u6784\u548c\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u5f3a\u5927\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u903b\u8f91\u795e\u7ecf\u7f51\u7edc\u867d\u53ef\u89e3\u91ca\u4f46\u529f\u80fd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u903b\u8f91\u64cd\u4f5c\u548c\u6539\u8fdb\u7ed3\u6784\uff0c\u63d0\u5347\u5176\u6027\u80fd\u548c\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165NOT\u64cd\u4f5c\u548c\u504f\u5dee\u9879\u6269\u5c55\u903b\u8f91\u795e\u7ecf\u7f51\u7edc\uff0c\u63d0\u51fa\u56e0\u5b50\u5316\u7684IF-THEN\u89c4\u5219\u7ed3\u6784\uff0c\u5e76\u5f00\u53d1\u65b0\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u65b9\u6cd5\u5728\u5e03\u5c14\u7f51\u7edc\u53d1\u73b0\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u8868\u683c\u5206\u7c7b\uff08\u5982\u533b\u7597\u9886\u57df\uff09\u4e2d\u5b66\u4e60\u5230\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u903b\u8f91\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u8fd8\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u900f\u660e\u51b3\u7b56\u7684\u9886\u57df\u3002"}}
{"id": "2508.08221", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08221", "abs": "https://arxiv.org/abs/2508.08221", "authors": ["Zihe Liu", "Jiashun Liu", "Yancheng He", "Weixun Wang", "Jiaheng Liu", "Ling Pan", "Xinyu Hu", "Shaopan Xiong", "Ju Huang", "Jian Hu", "Shengyi Huang", "Siran Yang", "Jiamang Wang", "Wenbo Su", "Bo Zheng"], "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "comment": "26 pages, 21 figures", "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728LLM\u63a8\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5206\u6790\u4e86\u4e0d\u540c\u6280\u672f\u7684\u5185\u90e8\u673a\u5236\u3001\u9002\u7528\u573a\u666f\u548c\u6838\u5fc3\u539f\u5219\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728LLM\u63a8\u7406\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u5357\u548c\u5bf9\u5176\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u5b9e\u8df5\u8005\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u6280\u672f\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u7684\u590d\u73b0\u548c\u72ec\u7acb\u8bc4\u4f30\uff0c\u5728\u7edf\u4e00\u7684\u5f00\u6e90\u6846\u67b6\u4e0b\u5206\u6790\u4e0d\u540cRL\u6280\u672f\uff0c\u5305\u62ec\u6570\u636e\u96c6\u96be\u5ea6\u3001\u6a21\u578b\u5927\u5c0f\u548c\u67b6\u6784\u7684\u7ec6\u7c92\u5ea6\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e24\u79cd\u6280\u672f\u7684\u7b80\u7ea6\u7ec4\u5408\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u7b56\u7565\u5982GRPO\u548cDAPO\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u6e05\u6670\u7684RL\u6280\u672f\u9009\u62e9\u6307\u5357\uff0c\u5e76\u4e3aLLM\u9886\u57df\u7684\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8def\u7ebf\u56fe\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u7b80\u7ea6\u7ec4\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
