<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 28]
- [cs.LG](#cs.LG) [Total: 88]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [An AI-driven EDA Algorithm-Empowered VCO and LDO Co-Design Method](https://arxiv.org/abs/2508.02687)
*Yijia Hao,Maarten Strackx,Miguel Gandara,Sandy Cochran,Bo Liu*

Main category: eess.SP

TL;DR: 本文提出了一种AI驱动的EDA算法，用于LDO供电的LC-tank VCO的协同设计，优化了相位噪声和功耗。


<details>
  <summary>Details</summary>
Motivation: 传统设计方法未能完全解决高频和LDO引起的低频相位噪声之间的权衡问题。

Method: 采用AI驱动的EDA算法进行协同设计，设计了一个5.6 GHz LC-tank VCO与集成LDO。

Result: 仿真显示，协同设计方法在1 MHz偏移处相位噪声改善了1.2 dB，动态功耗降低了28.8%，FoM提高了2.4 dBc/Hz。

Conclusion: 协同设计方法显著优于传统顺序设计方法。

Abstract: Traditionally, the output noise and power supply rejection of low-dropout
regulators (LDOs) are optimized to minimize power supply fluctuations, reducing
their impact on the low-frequency noise of target voltage-controlled
oscillators (VCOs). However, this sequential design approach does not fully
address the trade-offs between high-frequency and LDO-induced low-frequency
phase noise. To overcome this limitation, this paper presents a co-design
method for low phase-noise LC-tank VCOs powered by LDOs. It is difficult to
carry out the co-design using traditional manual design techniques. Hence, an
efficient AI-driven EDA algorithm is used. To validate the proposed method, a
5.6 GHz LC-tank VCO with an integrated LDO is designed using a 65 nm CMOS
process. Simulations show that the co-design method improves phase noise by 1.2
dB at a 1 MHz offset and reduces dynamic power consumption by 28.8%, with FoM
increased by 2.4 dBc/Hz compared to the conventional sequential design method.

</details>


### [2] [On Improving PPG-Based Sleep Staging: A Pilot Study](https://arxiv.org/abs/2508.02689)
*Jiawei Wang,Yu Guan,Chen Chen,Ligang Zhou,Laurence T. Yang,Sai Gu*

Main category: eess.SP

TL;DR: 通过双流交叉注意力架构结合PPG及其辅助信息，显著提升了基于PPG的睡眠分期性能。


<details>
  <summary>Details</summary>
Motivation: 尽管PPG传感器在消费设备中广泛应用，但仅依靠PPG实现可靠的睡眠分期仍具挑战性。

Method: 比较单流模型与双流交叉注意力策略，利用PPG及其衍生模态（如增强PPG或合成ECG）学习互补信息。

Result: 在MESA数据集上的实验表明，双流交叉注意力架构显著提升了性能。

Conclusion: 结合PPG与辅助信息的双流架构是提升睡眠分期性能的有效方法。

Abstract: Sleep monitoring through accessible wearable technology is crucial to
improving well-being in ubiquitous computing. Although
photoplethysmography(PPG) sensors are widely adopted in consumer devices,
achieving consistently reliable sleep staging using PPG alone remains a
non-trivial challenge. In this work, we explore multiple strategies to enhance
the performance of PPG-based sleep staging. Specifically, we compare
conventional single-stream model with dual-stream cross-attention strategies,
based on which complementary information can be learned via PPG and PPG-derived
modalities such as augmented PPG or synthetic ECG. To study the effectiveness
of the aforementioned approaches in four-stage sleep monitoring task, we
conducted experiments on the world's largest sleep staging dataset, i.e., the
Multi-Ethnic Study of Atherosclerosis(MESA). We found that substantial
performance gain can be achieved by combining PPG and its auxiliary information
under the dual-stream cross-attention architecture. Source code of this project
can be found at https://github.com/DavyWJW/sleep-staging-models

</details>


### [3] [Federated Learning in Active STARS-Aided Uplink Networks](https://arxiv.org/abs/2508.02693)
*Xinwei Yue,Xinning Guo,Xidong Mu,Jingjing Zhao,Peng Yang,Junsheng Mu,Zhiping Lu*

Main category: eess.SP

TL;DR: ASTARS辅助的联邦学习（FL）上行链路模型传输，通过OTA计算技术减少上传参数数量，优化学习效率和信号传输质量。


<details>
  <summary>Details</summary>
Motivation: 利用ASTARS缓解多径衰落并重塑电磁环境，提升FL上行链路网络的性能。

Method: 通过OTA-FL模型聚合误差的上界分析，联合优化接收波束分配和ASTARS相位偏移，以最大化学习效率。

Result: ASTARS辅助的FL网络在准确性和效率上优于现有技术，尤其在离散数据集上表现更优。

Conclusion: ASTARS显著提升FL性能，但需平衡放大功率以避免热噪声成为主要误差源。

Abstract: Active simultaneously transmitting and reflecting surfaces (ASTARS) have
attracted growing research interest due to its ability to alleviate
multiplicative fading and reshape the electromagnetic environment across the
entire space. In this paper, we utilise ASTARS to assist the federated learning
(FL) uplink model transfer and further reduce the number of uploaded parameter
counts through over-the-air (OTA) computing techniques. The impact of model
aggregation errors on ASTARS-aided FL uplink networks is characterized. We
derive an upper bound on the aggregation error of the OTA-FL model and quantify
the training loss due to communication errors. Then, we define the performance
of OTA-FL as a joint optimization problem that encompasses both the assignment
of received beams and the phase shifting of ASTARS, aiming to achieve the
maximum learning efficiency and high-quality signal transmission. Numerical
results demonstrate that: i) The FL accuracy in ASTARS uplink networks are
enhanced compared to that in state-of-the-art networks; ii) The ASTARS enabled
FL system achieves the better learning accuracy using fewer active units than
other baseline, especially when the dataset is more discrete; and iii) FL
accuracy improves with higher amplification power, but excessive amplification
makes thermal noise the dominant source of error.

</details>


### [4] [A Completely Blind Channel Estimation Technique for OFDM Using Constellation Splitting](https://arxiv.org/abs/2508.02698)
*Sameera Bharadwaja H.,D. K. Mehra*

Main category: eess.SP

TL;DR: 本文提出了一种基于频域线性非冗余预编码和交替子载波星座分裂的算法，解决了OFDM系统中二阶统计量盲信道估计的复标量模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于二阶统计量的方法普遍存在复标量模糊问题，通常需要依赖导频或参考符号解决。本文旨在提出一种无需导频的盲估计方法。

Method: 采用频域线性非冗余预编码和交替子载波星座分裂技术，设计了一种盲估计算法。

Result: 数值仿真表明，所提方法在M进制PAM系统中的性能与半盲方法相当。

Conclusion: 该方法有效解决了复标量模糊问题，且性能接近半盲方法，为盲信道估计提供了新思路。

Abstract: The problem of second-order statistics (SOS)-based blind channel estimation
in OFDM systems is addressed in this paper. Almost all SOS-based methods
proposed so far suffer from a complex-scalar estimation ambiguity, which is
resolved by using pilots or reference symbols. We propose an algorithm to
resolve this ambiguity in blind manner using frequency-domain linear
non-redundant precoding and constellation-splitting among the alternate
subcarriers. The performance of the proposed scheme is evaluated via numerical
simulations in MATLAB environment. Simulation results show that the proposed
approach performs as good as its semi-blind counterpart for M-ary PAM systems.

</details>


### [5] [Physics-guided denoiser network for enhanced additive manufacturing data quality](https://arxiv.org/abs/2508.02712)
*Pallock Halder,Satyajit Mojumder*

Main category: eess.SP

TL;DR: 提出了一种基于物理信息的去噪框架，结合能量模型和Fisher分数正则化，用于降低传感器数据噪声并保持物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统中的传感器数据噪声大且难以解释，限制了其在控制和诊断中的实用性。

Method: 提出了一种物理信息去噪框架，结合能量模型和Fisher分数正则化，并通过基准问题和实际LPBF实验验证。

Result: 该方法在多种噪声水平下优于基线神经网络去噪器，有效降低了LPBF数据噪声。

Conclusion: 物理引导的去噪策略能够实时解释低成本传感器数据，有助于预测控制和缺陷缓解。

Abstract: Modern engineering systems are increasingly equipped with sensors for
real-time monitoring and decision-making. However, the data collected by these
sensors is often noisy and difficult to interpret, limiting its utility for
control and diagnostics. In this work, we propose a physics-informed denoising
framework that integrates energy-based model and Fisher score regularization to
jointly reduce data noise and enforce physical consistency with a physics-based
model. The approach is first validated on benchmark problems, including the
simple harmonic oscillator, Burgers' equation, and Laplace's equation, across
varying noise levels. We then apply the denoising framework to real thermal
emission data from laser powder bed fusion (LPBF) additive manufacturing
experiments, using a trained Physics-Informed Neural Network (PINN) surrogate
model of the LPBF process to guide denoising. Results show that the proposed
method outperforms baseline neural network denoisers, effectively reducing
noise under a range of LPBF processing conditions. This physics-guided
denoising strategy enables robust, real-time interpretation of low-cost sensor
data, facilitating predictive control and improved defect mitigation in
additive manufacturing.

</details>


### [6] [Measuring Dependencies between Biological Signals with Temporal Self-supervision, and its Limitations](https://arxiv.org/abs/2508.02703)
*Evangelos Sariyanidi,John D. Herrington,Lisa Yankowitz,Pratik Chaudhari,Theodore D. Satterthwaite,Casey J. Zampella,Robert T. Schultz,Russell T. Shinohara,Birkan Tunc*

Main category: eess.SP

TL;DR: 提出了一种名为"concurrence"的自监督方法，用于测量信号之间的统计依赖性，无需先验知识或参数调整。


<details>
  <summary>Details</summary>
Motivation: 生物系统中复杂的非线性交互难以捕捉，现有方法依赖先验知识。

Method: 基于信号时间对齐与错位段的区分能力，设计自监督方法。

Result: 在fMRI、生理和行为信号实验中表现优异，首次实现广泛信号谱的关系揭示。

Conclusion: 该方法为跨领域科学发现提供有力工具，但需注意外部因素导致的依赖关系验证。

Abstract: Measuring the statistical dependence between observed signals is a primary
tool for scientific discovery. However, biological systems often exhibit
complex non-linear interactions that currently cannot be captured without a
priori knowledge regarding the nature of dependence. We introduce a
self-supervised approach, concurrence, which is inspired by the observation
that if two signals are dependent, then one should be able to distinguish
between temporally aligned vs. misaligned segments extracted from them.
Experiments with fMRI, physiological and behavioral signals show that, to our
knowledge, concurrence is the first approach that can expose relationships
across such a wide spectrum of signals and extract scientifically relevant
differences without ad-hoc parameter tuning or reliance on a priori
information, providing a potent tool for scientific discoveries across fields.
However, depencencies caused by extraneous factors remain an open problem, thus
researchers should validate that exposed relationships truely pertain to the
question(s) of interest.

</details>


### [7] [Integrating Machine Learning with Multimodal Monitoring System Utilizing Acoustic and Vision Sensing to Evaluate Geometric Variations in Laser Directed Energy Deposition](https://arxiv.org/abs/2508.02847)
*Ke Xu,Chaitanya Krishna Prasad Vallabh,Souran Manoochehri*

Main category: eess.SP

TL;DR: 该研究提出了一种多模态监测框架，结合声发射传感和同轴相机视觉，用于激光定向能量沉积（DED）增材制造中的熔池动态和几何变化识别，分类性能达94.4%。


<details>
  <summary>Details</summary>
Motivation: DED增材制造中熔池动态和工艺变化导致零件质量不稳定，现有研究多关注缺陷检测，但缺乏验证工艺监测系统对熔池动态和工艺质量的评估能力。

Method: 研究采用声发射传感和同轴相机视觉的多模态监测框架，通过预处理传感器数据并提取特征，评估多种机器学习算法（如SVM、随机森林、XGBoost）对几何变化的分类性能。

Result: 多模态策略的分类性能达94.4%，优于单独使用声发射（87.8%）或相机（86.7%），验证了系统能有效捕捉结构振动和表面形态变化。

Conclusion: 该研究为未来表征几何误差和制造缺陷提供了技术基础，尽管仅针对特定几何形状，但其分类能力具有广泛应用潜力。

Abstract: Laser directed energy deposition (DED) additive manufacturing struggles with
consistent part quality due to complex melt pool dynamics and process
variations. While much research targets defect detection, little work has
validated process monitoring systems for evaluating melt pool dynamics and
process quality. This study presents a novel multimodal monitoring framework,
synergistically integrating contact-based acoustic emission (AE) sensing with
coaxial camera vision to enable layer-wise identification and evaluation of
geometric variations in DED parts. The experimental study used three part
configurations: a baseline part without holes, a part with a 3mm diameter
through-hole, and one with a 5mm through-hole to test the system's discerning
capabilities. Raw sensor data was preprocessed: acoustic signals were filtered
for time-domain and frequency-domain feature extraction, while camera data
underwent melt pool segmentation and morphological feature extraction. Multiple
machine learning algorithms (including SVM, random forest, and XGBoost) were
evaluated to find the optimal model for classifying layer-wise geometric
variations. The integrated multimodal strategy achieved a superior
classification performance of 94.4%, compared to 87.8% for AE only and 86.7%
for the camera only. Validation confirmed the integrated system effectively
captures both structural vibration signatures and surface morphological changes
tied to the geometric variations. While this study focuses on specific
geometries, the demonstrated capability to discriminate between features
establishes a technical foundation for future applications in characterizing
part variations like geometric inaccuracies and manufacturing-induced defects.

</details>


### [8] [Evaluation of Deep Learning Models for LBBB Classification in ECG Signals](https://arxiv.org/abs/2508.02710)
*Beatriz Macas Ordóñez,Diego Vinicio Orellana Villavicencio,José Manuel Ferrández,Paula Bonomini*

Main category: eess.SP

TL;DR: 研究探索不同神经网络架构对ECG信号时空模式的分类能力，用于区分健康、LBBB和sLBBB。


<details>
  <summary>Details</summary>
Motivation: 通过创新技术优化LBBB分类，为心脏再同步治疗（CRT）候选者筛选提供支持。

Method: 评估不同神经网络架构对ECG信号时空模式的提取和分类能力。

Result: 未明确提及具体结果。

Conclusion: 研究为CRT候选者筛选提供了潜在的技术支持。

Abstract: This study explores different neural network architectures to evaluate their
ability to extract spatial and temporal patterns from electrocardiographic
(ECG) signals and classify them into three groups: healthy subjects, Left
Bundle Branch Block (LBBB), and Strict Left Bundle Branch Block (sLBBB).
  Clinical Relevance, Innovative technologies enable the selection of
candidates for Cardiac Resynchronization Therapy (CRT) by optimizing the
classification of subjects with Left Bundle Branch Block (LBBB).

</details>


### [9] [Precoder Design for User-Centric Network Massive MIMO: A Symplectic Optimization Approach](https://arxiv.org/abs/2508.02713)
*Pengxu Lin,An-An Lu,Xiqi Gao*

Main category: eess.SP

TL;DR: 论文提出了一种基于辛优化的预编码器设计方法，用于用户中心网络（UCN）大规模MIMO系统，避免了传统线性预编码中的矩阵求逆，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 在UCN大规模MIMO系统中，传统线性预编码的矩阵求逆计算复杂度高，限制了实际应用。

Method: 利用辛优化框架，将接收模型转换到实数域，并将加权和速率（WSR）最大化问题重新表述为动力系统的势能最小化问题，通过离散化连续系统得到迭代预编码设计方法。

Result: 仿真结果表明，该方法在UCN大规模MIMO系统中优于加权最小均方误差（WMMSE）预编码器。

Conclusion: 辛优化方法显著降低了计算复杂度，同时保持了性能优势，适用于实际系统。

Abstract: In this paper, we utilize symplectic optimization to design a precoder for
user-centric network (UCN) massive multiple-input multiple-output (MIMO)
systems, where a subset of base stations (BSs) serves each user terminal (UT)
instead of using all BSs. In UCN massive MIMO systems, the dimension of the
precoders is reduced compared to conventional network massive MIMO. It
simplifies the implementation of precoders in practical systems. However, the
matrix inversion in traditional linear precoders still requires high
computational complexity. To avoid the matrix inversion, we employ the
symplectic optimization framework, where optimization problems are solved based
on dissipative Hamiltonian dynamical systems. To better fit symplectic
optimization, we transform the received model into the real field and
reformulate the weighted sum-rate (WSR) maximization problem. The objective
function of the optimization problem is viewed as the potential energy of the
dynamical system. Due to energy dissipation, the continuous dynamical system
always converges to a state with minimal potential energy. By discretizing the
continuous system while preserving the symplectic structure, we obtain an
iterative method for the precoder design. The complexity analysis of the
proposed symplectic method is also provided to show its high computational
efficiency. Simulation results demonstrate that the proposed precoder design
based on symplectic optimization outperforms the weighted minimum mean-square
error (WMMSE) precoder in the UCN massive MIMO system.

</details>


### [10] [SleepLiteCNN: Energy-Efficient Sleep Apnea Subtype Classification with 1-Second Resolution Using Single-Lead ECG](https://arxiv.org/abs/2508.02718)
*Zahra Mohammadi,Siamak Mohammadi*

Main category: eess.SP

TL;DR: 提出了一种基于单导联心电图的高效睡眠呼吸暂停亚型分类方法，适用于可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 睡眠呼吸暂停的准确、高时间分辨率检测对治疗至关重要，但现有方法难以满足可穿戴设备的实时需求。

Method: 评估多种机器学习和深度学习算法，提出轻量级卷积神经网络SleepLiteCNN，优化能耗和精度。

Result: SleepLiteCNN在8位量化后达到95%准确率和92%宏F1分数，每次推理仅需1.8微焦耳。

Conclusion: SleepLiteCNN是适用于可穿戴设备的实用高效解决方案。

Abstract: Apnea is a common sleep disorder characterized by breathing interruptions
lasting at least ten seconds and occurring more than five times per hour.
Accurate, high-temporal-resolution detection of sleep apnea subtypes -
Obstructive, Central, and Mixed - is crucial for effective treatment and
management. This paper presents an energy-efficient method for classifying
these subtypes using a single-lead electrocardiogram (ECG) with high temporal
resolution to address the real-time needs of wearable devices. We evaluate a
wide range of classical machine learning algorithms and deep learning
architectures on 1-second ECG windows, comparing their accuracy, complexity,
and energy consumption. Based on this analysis, we introduce SleepLiteCNN, a
compact and energy-efficient convolutional neural network specifically designed
for wearable platforms. SleepLiteCNN achieves over 95% accuracy and a 92%
macro-F1 score, while requiring just 1.8 microjoules per inference after 8-bit
quantization. Field Programmable Gate Array (FPGA) synthesis further
demonstrates significant reductions in hardware resource usage, confirming its
suitability for continuous, real-time monitoring in energy-constrained
environments. These results establish SleepLiteCNN as a practical and effective
solution for wearable device sleep apnea subtype detection.

</details>


### [11] [Veli: Unsupervised Method and Unified Benchmark for Low-Cost Air Quality Sensor Correction](https://arxiv.org/abs/2508.02724)
*Yahia Dalbah,Marcel Worring,Yen-Chia Hsu*

Main category: eess.SP

TL;DR: 论文提出了一种无监督贝叶斯模型Veli，用于校正低成本传感器的空气质量读数，无需依赖参考站，并发布了最大的空气质量传感器基准数据集AQ-SDR。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染导致大量早逝，亟需准确且可扩展的空气质量监测方法。低成本传感器虽具扩展性，但易受漂移、校准误差和环境干扰影响。

Method: Veli利用变分推断构建解耦表示，分离真实污染物读数与传感器噪声，无需与参考站共置。

Result: Veli在分布内和分布外场景中均表现优异，能有效处理传感器漂移和异常行为。

Conclusion: Veli解决了低成本传感器部署的主要障碍，并提供了标准化基准AQ-SDR，为空气质量监测提供了新工具。

Abstract: Urban air pollution is a major health crisis causing millions of premature
deaths annually, underscoring the urgent need for accurate and scalable
monitoring of air quality (AQ). While low-cost sensors (LCS) offer a scalable
alternative to expensive reference-grade stations, their readings are affected
by drift, calibration errors, and environmental interference. To address these
challenges, we introduce Veli (Reference-free Variational Estimation via Latent
Inference), an unsupervised Bayesian model that leverages variational inference
to correct LCS readings without requiring co-location with reference stations,
eliminating a major deployment barrier. Specifically, Veli constructs a
disentangled representation of the LCS readings, effectively separating the
true pollutant reading from the sensor noise. To build our model and address
the lack of standardized benchmarks in AQ monitoring, we also introduce the Air
Quality Sensor Data Repository (AQ-SDR). AQ-SDR is the largest AQ sensor
benchmark to date, with readings from 23,737 LCS and reference stations across
multiple regions. Veli demonstrates strong generalization across both
in-distribution and out-of-distribution settings, effectively handling sensor
drift and erratic sensor behavior. Code for model and dataset will be made
public when this paper is published.

</details>


### [12] [SpectrumFM: A New Paradigm for Spectrum Cognition](https://arxiv.org/abs/2508.02742)
*Chunyu Liu,Hao Zhang,Wei Wu,Fuhui Zhou,Qihui Wu,Derrick Wing Kwan Ng,Chan-Byoung Chae*

Main category: eess.SP

TL;DR: 提出了一种名为SpectrumFM的频谱基础模型，通过创新的频谱编码器和自监督学习任务提升频谱认知的泛化性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有频谱认知方法在多样化频谱环境和任务中泛化性和准确性不足，亟需新范式。

Method: 结合卷积神经网络和多头自注意力机制的频谱编码器，以及掩码重建和下一时隙信号预测的自监督学习任务，采用LoRA参数高效微调。

Result: 在频谱感知、异常检测和无线技术分类任务中表现优异，检测概率提升30%，AUC提升10%，准确率提升9.6%。

Conclusion: SpectrumFM为频谱认知提供了高效、泛化的新解决方案。

Abstract: The enhancement of spectrum efficiency and the realization of secure spectrum
utilization are critically dependent on spectrum cognition. However, existing
spectrum cognition methods often exhibit limited generalization and suboptimal
accuracy when deployed across diverse spectrum environments and tasks. To
overcome these challenges, we propose a spectrum foundation model, termed
SpectrumFM, which provides a new paradigm for spectrum cognition. An innovative
spectrum encoder that exploits the convolutional neural networks and the
multi-head self attention mechanisms is proposed to effectively capture both
fine-grained local signal structures and high-level global dependencies in the
spectrum data. To enhance its adaptability, two novel self-supervised learning
tasks, namely masked reconstruction and next-slot signal prediction, are
developed for pre-training SpectrumFM, enabling the model to learn rich and
transferable representations. Furthermore, low-rank adaptation (LoRA)
parameter-efficient fine-tuning is exploited to enable SpectrumFM to seamlessly
adapt to various downstream spectrum cognition tasks, including spectrum
sensing (SS), anomaly detection (AD), and wireless technology classification
(WTC). Extensive experiments demonstrate the superiority of SpectrumFM over
state-of-the-art methods. Specifically, it improves detection probability in
the SS task by 30% at -4 dB signal-to-noise ratio (SNR), boosts the area under
the curve (AUC) in the AD task by over 10%, and enhances WTC accuracy by 9.6%.

</details>


### [13] [Extracting Range-Doppler Information of Moving Targets from Wi-Fi Channel State Information](https://arxiv.org/abs/2508.02799)
*Jessica Sanson,Rahul C. Shah,Maximilian Pinaroc,Valerio Frascolla*

Main category: eess.SP

TL;DR: 提出一种从商用Wi-Fi CSI中提取距离和多普勒信息的方法，解决了硬件异步和天线耦合问题，实现了高精度传感。


<details>
  <summary>Details</summary>
Motivation: 利用商用Wi-Fi设备进行高精度传感存在硬件异步和天线耦合的挑战，需要新的信号处理方法。

Method: 提出时间偏移消除、相位对齐校正和发射/接收耦合抑制三项创新信号处理技术。

Result: 在商用Intel Wi-Fi AX211 NIC上验证了厘米级精度的距离和多普勒估计，成功检测和跟踪移动目标。

Conclusion: 证明了使用标准Wi-Fi设备和现成硬件实现高精度传感的可行性，无需修改或专用全双工能力。

Abstract: This paper presents, for the first time, a method to extract both range and
Doppler information from commercial Wi-Fi Channel State Information (CSI) using
a monostatic (single transceiver) setup. Utilizing the CSI phase in Wi-Fi
sensing from a Network Interface Card (NIC) not designed for full-duplex
operation is challenging due to (1) Hardware asynchronization, which introduces
significant phase errors, and (2) Proximity of transmit (Tx) and receive (Rx)
antennas, which creates strong coupling that overwhelms the motion signal of
interest. We propose a new signal processing approach that addresses both
challenges via three key innovations: Time offset cancellation, Phase alignment
correction, and Tx/Rx coupling mitigation. Our method achieves cm-level
accuracy in range and Doppler estimation for moving targets, validated using a
commercial Intel Wi-Fi AX211 NIC. Our results show successful detection and
tracking of moving objects in realistic environments, establishing the
feasibility of high-precision sensing using standard Wi-Fi packet
communications and off-the-shelf hardware without requiring any modification or
specialized full-duplex capabilities.

</details>


### [14] [Secure mmWave Beamforming with Proactive-ISAC Defense Against Beam-Stealing Attacks](https://arxiv.org/abs/2508.02856)
*Seyed Bagher Hashemi Natanzi,Hossein Mohammadi,Bo Tang,Vuk Marojevic*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度强化学习（DRL）的框架，用于主动防御毫米波通信系统中的波束窃取攻击，结合了集成感知与通信（ISAC）技术，实现了92.8%的攻击检测率和13 dB以上的用户信干噪比（SINR）。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信系统易受高级波束窃取攻击，威胁物理层安全，需要一种主动且自适应的防御方法。

Method: 采用基于近端策略优化（PPO）算法的DRL代理，结合ISAC技术动态探测可疑活动，并通过课程学习策略优化训练过程。

Result: 实验结果显示，该框架实现了92.8%的平均攻击检测率，同时保持用户SINR超过13 dB。

Conclusion: 提出的DRL框架能有效防御波束窃取攻击，平衡安全性与通信性能。

Abstract: Millimeter-wave (mmWave) communication systems face increasing susceptibility
to advanced beam-stealing attacks, posing a significant physical layer security
threat. This paper introduces a novel framework employing an advanced Deep
Reinforcement Learning (DRL) agent for proactive and adaptive defense against
these sophisticated attacks. A key innovation is leveraging Integrated Sensing
and Communications (ISAC) capabilities for active, intelligent threat
assessment. The DRL agent, built on a Proximal Policy Optimization (PPO)
algorithm, dynamically controls ISAC probing actions to investigate suspicious
activities. We introduce an intensive curriculum learning strategy that
guarantees the agent experiences successful detection during training to
overcome the complex exploration challenges inherent to such a
security-critical task. Consequently, the agent learns a robust and adaptive
policy that intelligently balances security and communication performance.
Numerical results demonstrate that our framework achieves a mean attacker
detection rate of 92.8% while maintaining an average user SINR of over 13 dB.

</details>


### [15] [Zak-OTFS for Faster-Than-Nyquist Signaling in the Presence of Mobility & Delay Spread](https://arxiv.org/abs/2508.02950)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 论文提出了一种基于Zak-OTFS调制的超Nyquist信号传输方法，通过叠加信息符号并利用慢变信道特性，简化接收处理并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统正交信号传输（Nyquist）限制了带宽和时间内的信息符号数量，超Nyquist信号传输通过叠加符号突破这一限制，但需解决正交性丢失问题。

Method: 采用Zak-OTFS调制，在延迟-多普勒域叠加信息符号，利用慢变信道设计预编码器，并使用相互无偏基简化接收处理。

Result: 数值结果表明，该方法在未编码时性能与Nyquist相当，编码后在高信噪比下优于Nyquist。

Conclusion: 提出的方法通过Zak-OTFS调制和相互无偏基，实现了高效的超Nyquist信号传输，性能优于传统方法。

Abstract: Orthogonal signaling limits the number of information symbols transmitted in
bandwidth $B$ and time $T$ to be $BT$. This corresponds to the Nyquist
signaling and is achieved by mounting information symbols on $BT$-dimensional
basis spanning the $BT$-dimensional space spaced $\frac{1}{B}$ and
$\frac{1}{T}$ apart. Faster-than-Nyquist signaling involves transmitting more
than $BT$ informational symbols in a $BT$-dimensional space. This leads to loss
of orthogonality. This is achieved by time and/or bandwidth expansion resulting
from packing more information symbols in the same $BT$-dimensional space
(spacing less than $\frac{1}{B}$ and/or $\frac{1}{T}$). In this paper, we take
a different approach to faster-than-Nyquist signaling. We propose to
superimpose the information symbols on one another maintaining the original
spacing in the Nyquist signaling. We carry this out in the delay-Doppler (DD)
domain using Zak-transform based orthogonal time frequency space (Zak-OTFS)
modulation. In Zak-OTFS, the channel varies slowly. Further Zak-OTFS also
allows construction of mutually unbiased bases the interference between which
appear like Gaussian noise. The proposed scheme leverages the slow variation in
the DD channel to construct a precoder that mitigates the effect of the
doubly-spread channel. Further, in the proposed scheme we mount information
symbols on two mutually unbiased bases which allows superposition of
information symbols. This simplifies receiver processing to detection in
Gaussian noise since each basis appears to the other as Gaussian noise. This
reduction makes it possible to use trellis coded modulation to enhance
bit-error performance. Numerical results demonstrate that the
faster-than-Nyquist signaling scheme achieves similar uncoded performance as
that of Nyquist signaling and with coding the performance is better than
Nyquist signaling at high signal-to-noise ratios.

</details>


### [16] [Generating Light-based Fingerprints for Indoor Localization](https://arxiv.org/abs/2508.03011)
*Hsun-Yu Lee,Jie Lin,Fang-Jing Wu*

Main category: eess.SP

TL;DR: 论文提出了一种基于可见光通信（VLC）的室内定位方法，通过光谱特征作为位置指纹，结合GAN数据增强，显著降低了定位误差。


<details>
  <summary>Details</summary>
Motivation: 解决射频定位技术（如Wi-Fi、RFID）因多径衰落和干扰导致的精度不足问题，探索VLC作为替代方案。

Method: 采用两阶段框架：1）用真实光谱数据训练MLP；2）用TabGAN生成合成数据增强训练集。

Result: 数据增强后，平均定位误差从62.9cm降至49.3cm（提升20%），仅需额外5%的数据收集。

Conclusion: GAN数据增强有效缓解数据稀缺问题，提升模型泛化能力，验证了VLC在室内定位中的潜力。

Abstract: Accurate indoor localization underpins applications ranging from wayfinding
and emergency response to asset tracking and smart-building services.
Radio-frequency solutions (e.g. Wi-Fi, RFID, UWB) are widely adopted but remain
vulnerable to multipath fading, interference, and uncontrollable coverage
variation. We explore an orthogonal modality -- visible light communication
(VLC) -- and demonstrate that the spectral signatures captured by a low-cost
AS7341 sensor can serve as robust location fingerprints.
  We introduce a two-stage framework that (i) trains a multi-layer perceptron
(MLP) on real spectral measurements and (ii) enlarges the training corpus with
synthetic samples produced by TabGAN. The augmented dataset reduces the mean
localization error from 62.9cm to 49.3cm -- a 20% improvement -- while
requiring only 5% additional data-collection effort. Experimental results
obtained on 42 reference points in a U-shaped laboratory confirm that GAN-based
augmentation mitigates data-scarcity issues and enhances generalization.

</details>


### [17] [Metasurface-Enabled Extremely Large-Scale Antenna Systems: Transceiver Architecture, Physical Modeling, and Channel Estimation](https://arxiv.org/abs/2508.03021)
*Zhengyu Wang,Tiebin Mi,Gui Zhou,Robert C. Qiu*

Main category: eess.SP

TL;DR: 论文提出了一种基于超表面的极大规模天线系统（MELA），通过可重构透射超表面实现高效的射频-天线耦合和相位控制，避免了传统方案中的笨重开关矩阵和昂贵移相器网络。


<details>
  <summary>Details</summary>
Motivation: 解决下一代无线通信系统对性能的极高需求，提升极大规模天线阵列（ELAA）的实用性。

Method: 开发了物理基础模型以表征电磁场传播，提出距离相关近似模型，并设计了两阶段信道估计框架，结合字典驱动的波束空间滤波技术和超分辨率估计器。

Result: MELA架构在空间分辨率上接近最优，数值实验验证了信道估计算法的高分辨率和电磁模型的准确性。

Conclusion: MELA架构为极大规模天线阵列的实际部署提供了极具竞争力和前瞻性的解决方案。

Abstract: Extremely large-scale antenna arrays (ELAAs) have emerged as a pivotal
technology for addressing the unprecedented performance demands of
next-generation wireless communication systems. To enhance their practicality,
we propose metasurface-enabled extremely large-scale antenna (MELA) systems --
novel transceiver architectures that employ reconfigurable transmissive
metasurfaces to facilitate efficient over-the-air RF-to-antenna coupling and
phase control. This architecture eliminates the need for bulky switch matrices
and costly phase-shifter networks typically required in conventional solutions.
Physically grounded models are developed to characterize electromagnetic field
propagation through individual transmissive unit cells, capturing the
fundamental physics of wave transformation and transmission. Additionally,
distance-dependent approximate models are introduced, exhibiting structural
properties conducive to efficient parameter estimation and signal processing.
Based on the channel model, a two stage channel estimation framework is
proposed for the scenarios comprising users in the hybrid near- and far-fields.
In the first stage, a dictionary-driven beamspace filtering technique enables
rapid angular-domain scanning. In the refinement stage, the rotational symmetry
of subarrays is exploited to design super-resolution estimators that jointly
recover angular and range parameters. An analytical expression for the
half-power beamwidth of MELA is derived, revealing its near-optimal spatial
resolution relative to conventional ELAA architectures. Numerical experiments
further validate the high-resolution of the proposed channel estimation
algorithm and the fidelity of the electromagnetic model, positioning the MELA
architecture as a highly competitive and forward-looking solution for practical
ELAA deployment.

</details>


### [18] [Scenario-Agnostic Deep-Learning-Based Localization with Contrastive Self-Supervised Pre-training](https://arxiv.org/abs/2508.03084)
*Lingyan Zhang,Yuanfeng Qiu,Dachuan Li,Shaohua Wu,Tingting Zhang,Qinyu Zhang*

Main category: eess.SP

TL;DR: CSSLoc是一种基于对比自监督预训练的新型框架，用于学习通用表示以实现多场景下的精确定位，无需位置信息监督，且在动态环境中表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无线定位技术在动态环境中的脆弱性限制了其实际应用，需要一种能够适应多种场景的通用定位方法。

Method: CSSLoc通过对比自监督预训练学习无线电数据的相似性度量，将相似样本聚类、不同样本分离，并可直接迁移到下游定位任务。

Result: 实验表明，CSSLoc在典型室内场景中优于传统和最先进的基于DNN的定位方案。

Conclusion: CSSLoc将基于深度学习的定位从特定性推向通用性，提升了定位的准确性和适应性。

Abstract: Wireless localization has become a promising technology for offering
intelligent location-based services. Although its localization accuracy is
improved under specific scenarios, the short of environmental dynamic
vulnerability still hinders this approach from being fully practical
applications. In this paper, we propose CSSLoc, a novel framework on
contrastive self-supervised pre-training to learn generic representations for
accurate localization in various scenarios. Without the location information
supervision, CSSLoc attempts to learn an insightful metric on the similarity
discrimination of radio data, in such a scenario-agnostic manner that the
similar samples are closely clustered together and different samples are
separated in the representation space. Furthermore, the trained feature encoder
can be directly transferred for downstream localization tasks, and the location
predictor is trained to estimate accurate locations with the robustness of
environmental dynamics. With extensive experimental results, CSSLoc can
outperform classical and state-of-the-art DNN-based localization schemes in
typical indoor scenarios, pushing deep-learning-based localization from
specificity to generality.

</details>


### [19] [Can Large Language Models Identify Materials from Radar Signals?](https://arxiv.org/abs/2508.03120)
*Jiangyou Zhu,Hongyu Deng,He Chen*

Main category: eess.SP

TL;DR: LLMaterial利用预训练大语言模型（LLM）直接从雷达信号中推断材料组成，结合物理信号处理和检索增强生成（RAG）策略，实现开放集材料识别。


<details>
  <summary>Details</summary>
Motivation: 现有基于雷达的材料识别方法局限于封闭集对象类别，且需任务特定数据训练，限制了实际应用。

Method: 1. 物理信号处理管道压缩高冗余雷达数据为紧凑中间参数；2. 采用RAG策略为LLM提供领域知识。

Result: 初步结果显示LLMaterial能有效区分多种常见材料。

Conclusion: LLMaterial展示了直接从雷达信号进行材料识别的潜力。

Abstract: Accurately identifying the material composition of objects is a critical
capability for AI robots powered by large language models (LLMs) to perform
context-aware manipulation. Radar technologies offer a promising sensing
modality for material recognition task. When combined with deep learning, radar
technologies have demonstrated strong potential in identifying the material of
various objects. However, existing radar-based solutions are often constrained
to closed-set object categories and typically require task-specific data
collection to train deep learning models, largely limiting their practical
applicability. This raises an important question: Can we leverage the powerful
reasoning capabilities of pre-trained LLMs to directly infer material
composition from raw radar signals? Answering this question is non-trivial due
to the inherent redundancy of radar signals and the fact that pre-trained LLMs
have no prior exposure to raw radar data during training. To address this, we
introduce LLMaterial, the first study to investigate the feasibility of using
LLM to identify materials directly from radar signals. First, we introduce a
physics-informed signal processing pipeline that distills high-redundancy radar
raw data into a set of compact intermediate parameters that encapsulate the
material's intrinsic characteristics. Second, we adopt a retrieval-augmented
generation (RAG) strategy to provide the LLM with domain-specific knowledge,
enabling it to interpret and reason over the extracted intermediate parameters.
Leveraging this integration, the LLM is empowered to perform step-by-step
reasoning on the condensed radar features, achieving open-set material
recognition directly from raw radar signals. Preliminary results show that
LLMaterial can effectively distinguish among a variety of common materials,
highlighting its strong potential for real-world material identification
applications.

</details>


### [20] [Model Order Reduction for Large-scale Circuits Using Higher Order Dynamic Mode Decomposition](https://arxiv.org/abs/2508.03131)
*Na Liu,Chengliang Dai,Qiuyue Wu,Qiuqi Li,Guoxiong Cai*

Main category: eess.SP

TL;DR: 本文提出了一种基于延迟嵌入技术的高阶动态模式分解（HODMD）方法，用于提高大规模电路瞬态仿真的计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 动态模式分解（DMD）是一种数据驱动的特征提取方法，但传统DMD在空间分辨率不足时无法重构输出信号。本文旨在解决这一问题，并提升计算效率。

Method: 通过引入延迟嵌入技术，提出高阶动态模式分解（HODMD）方法，适用于一般电路且不限制电路拓扑或元件类型。

Result: 通过三个代表性数值测试案例验证了HODMD方法的计算效率和准确性。

Conclusion: HODMD方法克服了传统DMD的局限性，适用于大规模电路仿真，具有高效和准确的特点。

Abstract: Model order reduction (MOR) has long been a mainstream strategy to accelerate
large-scale transient circuit simulation. Dynamic Mode Decomposition (DMD)
represents a novel data-driven characterization method, extracting dominant
dynamical modes directly from time-domain simulation data without requiring
explicit system equations. This paper first deduces the DMD algorithm and then
proposes high order dynamic mode decomposition (HODMD) incorporating delayed
embedding technique, specifically targeting computational efficiency in
large-scale circuit simulations. Compared with the DMD method, the HODMD method
overcomes the problem that the output signal cannot be reconstructed when the
spatial resolution is insufficient. The proposed HODMD algorithm is applicable
to general circuits and does not impose any constraints on the topology of the
pertinent circuit or type of the components. Three representative numerical
test cases are presented to systematically validate both the computational
efficiency and accuracy of the proposed HODMD method.

</details>


### [21] [Federated Learning with Feature Reconstruction for Vector Quantization based Semantic Communication](https://arxiv.org/abs/2508.03248)
*Yoon Huh,Bumjun Kim,Wan Choi*

Main category: eess.SP

TL;DR: FedSFR是一种新颖的联邦学习框架，通过语义特征重构解决图像语义通信中的知识库不匹配和模型过时问题，提升训练稳定性和通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决图像语义通信中因知识库不匹配和模型过时导致的语义错误和性能下降问题。

Method: 提出FedSFR框架，结合语义特征重构（FR），设计专用损失函数，并进行收敛性和隐私分析。

Result: 实验验证FedSFR在容量受限场景下优于现有基线，证明其有效性和鲁棒性。

Conclusion: FedSFR为图像语义通信提供了一种高效且隐私保护的解决方案。

Abstract: Recent advancements in semantic communication have primarily focused on image
transmission, where neural network (NN)-based joint source-channel coding
(JSCC) modules play a central role. However, such systems often experience
semantic communication errors due to mismatched knowledge bases between users
and performance degradation from outdated models, necessitating regular model
updates. To address these challenges in vector quantization (VQ)-based image
semantic communication systems, we propose FedSFR, a novel federated learning
(FL) framework that incorporates semantic feature reconstruction (FR). FedSFR
introduces an FR step at the parameter server (PS) and allows a subset of
clients to transmit compact feature vectors in lieu of sending full local model
updates, thereby improving training stability and communication efficiency. To
enable effective FR learning, we design a loss function tailored for VQ-based
image semantic communication and demonstrate its validity as a surrogate for
image reconstruction error. Additionally, we provide a rigorous convergence
analysis and present a differentially private variant of FedSFR, along with
formal privacy analysis. Experimental results on two benchmark datasets
validate the superiority of FedSFR over existing baselines, especially in
capacity-constrained settings, confirming both its effectiveness and
robustness.

</details>


### [22] [Investigating the Cognitive Response of Brake Lights in Initiating Braking Action Using EEG](https://arxiv.org/abs/2508.03274)
*Ramaswamy Palaniappan,Surej Mouli,Howard Bowman,Ian McLoughlin*

Main category: eess.SP

TL;DR: 论文研究了不同刹车灯设计对驾驶员反应时间的影响，发现LED刹车灯比白炽灯刹车灯能更快引发认知反应。


<details>
  <summary>Details</summary>
Motivation: 研究动机是减少因驾驶员注意力不集中或车距不足导致的事故，特别是追尾事故。

Method: 方法包括在模拟驾驶环境中测试多种刹车灯设计，记录22名受试者的脑电图（EEG）数据，分析P3成分的反应时间。

Result: 结果显示LED刹车灯比白炽灯刹车灯显著更快引发认知反应，但不同LED设计间的差异不显著。

Conclusion: 结论是LED刹车灯设计在提高驾驶员反应速度方面优于白炽灯，但需进一步优化以减少EEG信号中的运动伪影。

Abstract: Half of all road accidents result from either lack of driver attention or
from maintaining insufficient separation between vehicles. Collision from the
rear, in particular, has been identified as the most common class of accident
in the UK, and its influencing factors have been widely studied for many years.
Rear-mounted stop lamps, illuminated when braking, are the primary mechanism to
alert following drivers to the need to reduce speed or brake. This paper
develops a novel brain response approach to measuring subject reaction to
different brake light designs. A variety of off-the-shelf brake light
assemblies are tested in a physical simulated driving environment to assess the
cognitive reaction times of 22 subjects. Eight pairs of LED-based and two pairs
of incandescent bulb-based brake light assemblies are used and
electroencephalogram (EEG) data recorded. Channel Pz is utilised to extract the
P3 component evoked during the decision making process that occurs in the brain
when a participant decides to lift their foot from the accelerator and depress
the brake. EEG analysis shows that both incandescent bulb-based lights are
statistically slower to evoke cognitive responses than all tested LED-based
lights. Between the LED designs, differences are evident, but not statistically
significant, attributed to the significant amount of movement artifact in the
EEG signal.

</details>


### [23] [Spiking Neural Networks for Resource Allocation in UAV-Enabled Wireless Networks](https://arxiv.org/abs/2508.03279)
*Vasileios Kouvakis,Stylianos E. Trevlakis,Ioannis Arapakis,Alexandros-Apostolos A. Boulogeorgos*

Main category: eess.SP

TL;DR: 提出了一种基于脉冲神经网络（SNN）的新方法，用于非地面网络（NTN）中的用户设备-基站（UE-BS）关联，比较了集中式和分布式两种优化策略。


<details>
  <summary>Details</summary>
Motivation: 随着无人机（UAV）引入无线网络，系统架构变得异构，需要动态高效的管理以避免拥塞并维持性能。

Method: 采用基于泄漏积分-发放（LIF）神经元的SNN，比较了集中式（全局可见）和分布式（节点独立）两种策略。

Result: 仿真显示分布式模型准确率超过90%，集中式模型为80-100%，两者在解决方案最优性和可行性之间存在权衡。

Conclusion: 两种方法在不同部署场景下均有效，具体选择取决于需求。

Abstract: This work presents a new spiking neural network (SNN)-based approach for user
equipment-base station (UE-BS) association in non-terrestrial networks (NTNs).
With the introduction of UAV's in wireless networks, the system architecture
becomes heterogeneous, resulting in the need for dynamic and efficient
management to avoid congestion and sustain overall performance. The presented
framework compares two SNN-based optimization strategies. Specifically, a
top-down centralized approach with complete network visibility and a bottom-up
distributed approach for individual network nodes. The SNN is based on leak
integrate-and-fire neurons with temporal components, which can perform fast and
efficient event-driven inference. Realistic ray-tracing simulations are
conducted, which showcase that the bottom-up model attains over 90\% accuracy,
while the top-down model maintains 80-100\% accuracy. Both approaches reveal a
trade-off between individually optimal solutions and UE-BS association
feasibility, thus revealing the effectiveness of both approaches depending on
deployment scenarios.

</details>


### [24] [Quantum Deep Learning for Massive MIMO User Scheduling](https://arxiv.org/abs/2508.03327)
*Xingyu Huang,Ruining Fan,Mouli Chakraborty,Avishek Nag,Anshu Mukherjee*

Main category: eess.SP

TL;DR: 提出一种混合量子神经网络（QNN）架构，用于5G/B5G大规模MIMO系统中的高效用户调度，解决传统方法的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在大规模MIMO系统中存在可扩展性问题，需要更高效的调度方案。

Method: 结合经典神经网络与变分量子电路核，利用统计信道状态信息（CSI）减少计算开销。

Result: 模型在噪声信道中表现稳健，优于经典卷积神经网络（CNN），提高了频谱效率。

Conclusion: 展示了量子增强机器学习在无线调度中的潜力。

Abstract: We introduce a hybrid Quantum Neural Networks (QNN) architecture for the
efficient user scheduling in 5G/Beyond 5G (B5G) massive Multiple Input Multiple
Output (MIMO) systems, addressing the scalability issues of traditional
methods. By leveraging statistical Channel State Information (CSI), our model
reduces computational overhead and enhances spectral efficiency. It integrates
classical neural networks with a variational quantum circuit kernel,
outperforming classical Convolutional Neural Networks (CNNs) and maintaining
robust performance in noisy channels. This demonstrates the potential of
quantum-enhanced Machine Learning (ML) for wireless scheduling.

</details>


### [25] [Beam-Hopping Pattern Design for Grant-Free Random Access in LEO Satellite Communications](https://arxiv.org/abs/2508.03391)
*Seunghyeon Jeon,Seonjung Kim,Gyeongrae Im,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 论文研究了无连接建立的波束跳跃LEO卫星通信系统，提出了一种动态分配资源的算法，以最大化最小成功传输概率。


<details>
  <summary>Details</summary>
Motivation: 偏远地区对大规模设备连接的需求推动了LEO卫星通信系统的发展，波束跳跃系统能实现需求感知的资源分配和低接入延迟。

Method: 提出了一种交替优化框架，结合二分法和ADMM算法，优化波束跳跃模式和资源分配。

Result: 仿真结果表明，所提算法优于其他波束跳跃方法，并能有效管理流量需求不平衡。

Conclusion: 该研究为LEO卫星通信系统的动态资源分配提供了高效解决方案。

Abstract: Increasing demand for massive device connectivity in underserved regions
drives the development of advanced low Earth orbit (LEO) satellite
communication systems. Beam-hopping LEO systems without connection
establishment provide a promising solution for achieving both demand-aware
resource allocation and low access latency. This paper investigates
beam-hopping pattern design for the grant-free random access systems to
dynamically allocate satellite resources according to traffic demands across
serving cells. We formulate a binary optimization problem that aims to maximize
the minimum successful transmission probability across cells, given limited
satellite beam generation capacity. To solve this problem, we propose novel
beam-hopping design algorithms that alternately enhance the collision avoidance
rate and decoding success probability within an alternating optimization
framework. Specifically, the algorithms employ a bisection method to optimize
illumination allocation for each cell based on demand, while using the
alternating direction method of multipliers (ADMM) to optimize beam-hopping
patterns for maximizing decoding success probability. Furthermore, we enhance
the ADMM by replacing the strict binary constraint with two equivalent
continuous-valued constraints. Simulation results demonstrate the superiority
of the proposed algorithms compared to other beam-hopping methods and verify
robustness in managing traffic demand imbalance.

</details>


### [26] [How to Proactively Monitor Untrusted Communications with Cell-Free Massive MIMO?](https://arxiv.org/abs/2508.03423)
*Isabella W. G. da Silva,Zahra Mobini,Hien Q. Ngo,Hyundong Shin,Michail Matthaiou*

Main category: eess.SP

TL;DR: 本文研究了无小区大规模多输入多输出（CF-mMIMO）主动监控系统，提出了一种有效的信道状态信息（CSI）获取方案，并通过优化模式分配和干扰功率控制提升了监控性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过多天线监控节点（MNs）有效监控不可信通信链路，提升监控系统的性能。

Method: 利用上下行链路的导频信号，通过最小均方误差（MMSE）估计获取有效信道状态信息，并基于贝叶斯优化框架提出联合模式分配和干扰功率控制优化方法。

Result: 提出的CF-mMIMO主动监控系统在监控成功概率（MSP）上显著优于基准方法，且MSP性能超过0.8。

Conclusion: 所提出的CSI获取和优化方法显著提升了监控系统的性能，适用于不同天线数量和预编码方案的场景。

Abstract: This paper studies a cell-free massive multiple-input multiple-output
(CF-mMIMO) proactive monitoring system in which multiple multi-antenna
monitoring nodes (MNs) are assigned to either observe the transmissions from an
untrusted transmitter (UT) or to jam the reception at the untrusted receiver
(UR). We propose an effective channel state information (CSI) acquisition
scheme for the monitoring system. In our approach, the MNs leverage the pilot
signals transmitted during the uplink and downlink phases of the untrusted link
and estimate the effective channels corresponding to the UT and UR via a
minimum mean-squared error (MMSE) estimation scheme. We derive new spectral
efficiency (SE) expressions for the untrusted link and the monitoring system.
For the latter, the SE is derived for two CSI availability cases at the central
processing unit (CPU); namely case-1: imperfect CSI knowledge at both MNs and
CPU, case-2: imperfect CSI knowledge at the MNs and no CSI knowledge at the
CPU. To improve the monitoring performance, we propose a novel joint mode
assignment and jamming power control optimization method to maximize the
monitoring success probability (MSP) based on the Bayesian optimization
framework. Numerical results show that (a) our CF-mMIMO proactive monitoring
system relying on the proposed CSI acquisition and optimization approach
significantly outperforms the considered benchmarks; (b) the MSP performance of
our CF-mMIMO proactive monitoring system is greater than 0.8, regardless of the
number of antennas at the untrusted nodes or the precoding scheme for the
untrusted transmission link.

</details>


### [27] [Joint Sensing and Bi-Directional Communication with Dynamic TDD Enabled Cell-Free MIMO](https://arxiv.org/abs/2508.03460)
*Anubhab Chowdhury,Sai Subramanyam Thoota,Erik G. Larsson*

Main category: eess.SP

TL;DR: 本文研究了动态时分双工（DTDD）无小区（CF）大规模多输入多输出（mMIMO）系统中的集成感知与通信（ISAC）。通过DTDD，系统能同时服务上下行用户，并利用上行AP进行感知。提出了集中式和分布式GLRT检测方法，分析了其性能与复杂度权衡，并提出了联合数据检测与RCS估计框架。通信方面，推导了最优SINR合并器，并提出两种目标预编码方案。数值实验验证了GLRT的鲁棒性和DTDD的性能优势。


<details>
  <summary>Details</summary>
Motivation: 研究如何在动态时分双工的无小区大规模MIMO系统中实现高效的集成感知与通信，解决传统TDD系统的性能限制。

Method: 1. 提出集中式和分布式GLRT检测方法；2. 分析性能与复杂度权衡；3. 提出联合数据检测与RCS估计框架；4. 推导最优SINR合并器；5. 设计两种目标预编码方案。

Result: 数值实验表明GLRT对AP间干扰具有鲁棒性，DTDD在保持半双工硬件的情况下，性能优于传统TDD系统。

Conclusion: DTDD和GLRT方法在集成感知与通信系统中表现出色，显著提升了性能，同时保持了硬件实现的可行性。

Abstract: This paper studies integrated sensing and communication (ISAC) with dynamic
time division duplex (DTDD) cell-free (CF) massive multiple-input
multiple-output~(mMIMO) systems. DTDD enables the CF mMIMO system to
concurrently serve both uplink~(UL) and downlink~(DL) users with spatially
separated \emph{half-duplex~(HD)} access points~(APs) using the same
time-frequency resources. Further, to facilitate ISAC, the UL APs are utilized
for both UL data and target echo reception, while the DL APs jointly transmit
the precoded DL data streams and target signal. In this context, we present
centralized and distributed generalized likelihood-ratio tests~(GLRTs) for
target detection treating UL users' signals as sensing interference. We then
quantify the optimality and complexity trade-off between distributed and
centralized GLRTs and benchmark the respective estimators with the Bayesian
Cram\'er-Rao lower bound for target radar-cross section~(RCS). Then, we present
a unified framework for joint UL users' data detection and RCS estimation.
Next, for communication, we derive the signal-to-noise-plus-interference~(SINR)
optimal combiner accounting for the cross-link and radar interference for UL
data processing. In DL, we use regularized zero-forcing for the users and
propose two types of precoders for the target: one ``user-centric" that
nullifies the interference caused by the target signal to the DL users and one
``target-centric" based on the dominant eigenvector of the composite channel
between the target and the APs. Finally, numerical studies corroborate with our
theoretical findings and reveal that the \emph{GLRT is robust to inter-AP
interference, and DTDD doubles the $90\%$-likely sum UL-DL SE compared to
traditional TDD-based CF-mMIMO ISAC systems}; while using HD hardware.

</details>


### [28] [Decoding and Engineering the Phytobiome Communication for Smart Agriculture](https://arxiv.org/abs/2508.03584)
*Fatih Gulec,Hamdan Awan,Nigel Wallbridge,Andrew W. Eckford*

Main category: eess.SP

TL;DR: 论文探讨了如何利用通信工程视角理解植物生物群落（phytobiome）的通信机制，并将其与智能农业结合，提出多尺度框架和潜在应用。


<details>
  <summary>Details</summary>
Motivation: 解决现代农业面临的粮食需求增长、环境污染和水资源短缺等挑战，通过通信理论推动农业科学发展。

Method: 提出多尺度框架，将植物生物群落建模为通信网络，并通过植物实验验证电生理信号模型。

Result: 展示了智能灌溉和精准农业化学品输送等应用，结合ML/AI和分子通信技术。

Conclusion: 该方法为高效、可持续和环保的农业生产提供了新途径，但仍需解决实施挑战和研究问题。

Abstract: Smart agriculture applications, integrating technologies like the Internet of
Things and machine learning/artificial intelligence (ML/AI) into agriculture,
hold promise to address modern challenges of rising food demand, environmental
pollution, and water scarcity. Alongside the concept of the phytobiome, which
defines the area including the plant, its environment, and associated
organisms, and the recent emergence of molecular communication (MC), there
exists an important opportunity to advance agricultural science and practice
using communication theory. In this article, we motivate to use the
communication engineering perspective for developing a holistic understanding
of the phytobiome communication and bridge the gap between the phytobiome
communication and smart agriculture. Firstly, an overview of phytobiome
communication via molecular and electrophysiological signals is presented and a
multi-scale framework modeling the phytobiome as a communication network is
conceptualized. Then, how this framework is used to model electrophysiological
signals is demonstrated with plant experiments. Furthermore, possible smart
agriculture applications, such as smart irrigation and targeted delivery of
agrochemicals, through engineering the phytobiome communication are proposed.
These applications merge ML/AI methods with the Internet of Bio-Nano-Things
enabled by MC and pave the way towards more efficient, sustainable, and
eco-friendly agricultural production. Finally, the implementation challenges,
open research issues, and industrial outlook for these applications are
discussed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [A Bayesian Hybrid Parameter-Efficient Fine-Tuning Method for Large Language Models](https://arxiv.org/abs/2508.02711)
*Yidong Chai,Yang Liu,Yonghang Zhou,Jiaheng Xie,Daniel Dajun Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为BH-PEFT的新方法，将贝叶斯学习融入混合参数高效微调（PEFT）中，解决了现有方法在不确定性量化和动态适应性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在专业应用中需要高效微调，但现有混合PEFT方法缺乏不确定性量化和动态适应能力。

Method: 结合Adapter、LoRA和prefix-tuning技术，利用贝叶斯学习建模可学习参数为分布，并提出动态微调方法。

Result: 在情感分析、新闻分类等任务中表现优于现有PEFT基线，支持不确定性量化和动态适应。

Conclusion: BH-PEFT为商业分析和数据科学提供了不确定性感知和自适应决策支持的新方法。

Abstract: Large Language Models (LLMs) have demonstrated transformative potential in
reshaping the world. As these models are pretrained on general corpora, they
often require domain-specific fine-tuning to optimize performance in
specialized business applications. Due to their massive scale,
parameter-efficient fine-tuning (PEFT) methods are widely used to reduce
training costs. Among them, hybrid PEFT methods that combine multiple PEFT
techniques have achieved the best performance. However, existing hybrid PEFT
methods face two main challenges when fine-tuning LLMs for specialized
applications: (1) relying on point estimates, lacking the ability to quantify
uncertainty for reliable decision-making, and (2) struggling to dynamically
adapt to emerging data, lacking the ability to suit real-world situations. We
propose Bayesian Hybrid Parameter-Efficient Fine-Tuning (BH-PEFT), a novel
method that integrates Bayesian learning into hybrid PEFT. BH-PEFT combines
Adapter, LoRA, and prefix-tuning to fine-tune feedforward and attention layers
of the Transformer. By modeling learnable parameters as distributions, BH-PEFT
enables uncertainty quantification. We further propose a Bayesian dynamic
fine-tuning approach where the last posterior serves as the prior for the next
round, enabling effective adaptation to new data. We evaluated BH-PEFT on
business tasks such as sentiment analysis, news categorization, and commonsense
reasoning. Results show that our method outperforms existing PEFT baselines,
enables uncertainty quantification for more reliable decisions, and improves
adaptability in dynamic scenarios. This work contributes to business analytics
and data science by proposing a novel BH-PEFT method and dynamic fine-tuning
approach that support uncertainty-aware and adaptive decision-making in
real-world situations.

</details>


### [30] [ZetA: A Riemann Zeta-Scaled Extension of Adam for Deep Learning](https://arxiv.org/abs/2508.02719)
*Samiksha BC*

Main category: cs.LG

TL;DR: ZetA是一种新型深度学习优化器，通过结合Riemann zeta函数的动态缩放扩展了Adam，提高了泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索一种基于Riemann zeta函数的动态缩放方法，以改进深度学习优化器的性能。

Method: 结合自适应阻尼、余弦相似性动量增强、熵正则化损失和SAM风格扰动，提出混合更新机制。

Result: 在多个数据集上测试，ZetA表现优于Adam，尤其在噪声或高粒度分类任务中。

Conclusion: ZetA是一种计算高效且鲁棒的优化器，适合复杂分类任务。

Abstract: This work introduces ZetA, a novel deep learning optimizer that extends Adam
by incorporating dynamic scaling based on the Riemann zeta function. To the
best of our knowledge, ZetA is the first optimizer to apply zeta-based gradient
scaling within deep learning optimization. The method improves generalization
and robustness through a hybrid update mechanism that integrates adaptive
damping, cosine similarity-based momentum boosting, entropy-regularized loss,
and Sharpness-Aware Minimization (SAM)-style perturbations. Empirical
evaluations on SVHN, CIFAR10, CIFAR100, STL10, and noisy CIFAR10 consistently
show test accuracy improvements over Adam. All experiments employ a lightweight
fully connected network trained for five epochs under mixed-precision settings.
The results demonstrate that ZetA is a computationally efficient and robust
alternative to Adam, particularly effective in noisy or high-granularity
classification tasks.

</details>


### [31] [Physics-Embedded Neural ODEs for Sim2Real Edge Digital Twins of Hybrid Power Electronics Systems](https://arxiv.org/abs/2508.02887)
*Jialin Zheng,Haoyu Wang,Yangbin Zeng,Di Mou,Xin Zhang,Hong Li,Sergio Vazquez,Leopoldo G. Franquelo*

Main category: cs.LG

TL;DR: 本文提出了一种物理嵌入神经ODE（PENODE）方法，用于解决电力电子系统中边缘数字孪生（EDTs）建模的挑战，显著提高了模拟到现实的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有建模方法难以捕捉电力电子系统（PES）中持续演化的混合动态特性，导致在资源受限的边缘设备上模拟到现实的泛化能力下降。

Method: PENODE通过嵌入混合操作机制作为事件自动机来显式控制离散切换，并将已知的ODE组件注入神经参数化中，形成可微分端到端可训练架构。

Result: 实验表明，PENODE在白盒、灰盒和黑盒场景中显著提高了准确性，神经元数量减少75%，同时保持了物理可解释性和高效的边缘部署。

Conclusion: PENODE在保持物理可解释性的同时，实现了高效的边缘部署和实时控制增强。

Abstract: Edge Digital Twins (EDTs) are crucial for monitoring and control of Power
Electronics Systems (PES). However, existing modeling approaches struggle to
consistently capture continuously evolving hybrid dynamics that are inherent in
PES, degrading Sim-to-Real generalization on resource-constrained edge devices.
To address these challenges, this paper proposes a Physics-Embedded Neural ODEs
(PENODE) that (i) embeds the hybrid operating mechanism as an event automaton
to explicitly govern discrete switching and (ii) injects known governing ODE
components directly into the neural parameterization of unmodeled dynamics.
This unified design yields a differentiable end-to-end trainable architecture
that preserves physical interpretability while reducing redundancy, and it
supports a cloud-to-edge toolchain for efficient FPGA deployment. Experimental
results demonstrate that PENODE achieves significantly higher accuracy in
benchmarks in white-box, gray-box, and black-box scenarios, with a 75%
reduction in neuron count, validating that the proposed PENODE maintains
physical interpretability, efficient edge deployment, and real-time control
enhancement.

</details>


### [32] [ECGTwin: Personalized ECG Generation Using Controllable Diffusion Model](https://arxiv.org/abs/2508.02720)
*Yongfan Lai,Bo Liu,Xinyan Guan,Qinghao Zhao,Hongyan Li,Shenda Hong*

Main category: cs.LG

TL;DR: ECGTwin是一个两阶段框架，用于个性化ECG生成，通过对比学习提取个体特征，并通过扩散模型和AdaX条件注入器生成高保真ECG信号。


<details>
  <summary>Details</summary>
Motivation: 个性化ECG生成可以推动精准医疗，但面临提取个体特征和注入多种条件的挑战。

Method: ECGTwin采用两阶段方法：第一阶段通过对比学习提取个体特征，第二阶段通过扩散模型和AdaX条件注入器生成ECG。

Result: 实验表明ECGTwin能生成高保真、多样化的ECG信号，并保留个体特征，同时提升下游自动诊断性能。

Conclusion: ECGTwin展示了精准个性化医疗的潜力，为ECG生成和诊断提供了新方法。

Abstract: Personalized electrocardiogram (ECG) generation is to simulate a patient's
ECG digital twins tailored to specific conditions. It has the potential to
transform traditional healthcare into a more accurate individualized paradigm,
while preserving the key benefits of conventional population-level ECG
synthesis. However, this promising task presents two fundamental challenges:
extracting individual features without ground truth and injecting various types
of conditions without confusing generative model. In this paper, we present
ECGTwin, a two-stage framework designed to address these challenges. In the
first stage, an Individual Base Extractor trained via contrastive learning
robustly captures personal features from a reference ECG. In the second stage,
the extracted individual features, along with a target cardiac condition, are
integrated into the diffusion-based generation process through our novel AdaX
Condition Injector, which injects these signals via two dedicated and
specialized pathways. Both qualitative and quantitative experiments have
demonstrated that our model can not only generate ECG signals of high fidelity
and diversity by offering a fine-grained generation controllability, but also
preserving individual-specific features. Furthermore, ECGTwin shows the
potential to enhance ECG auto-diagnosis in downstream application, confirming
the possibility of precise personalized healthcare solutions.

</details>


### [33] [Neural Approximators for Low-Thrust Trajectory Transfer Cost and Reachability](https://arxiv.org/abs/2508.02911)
*Zhong Zhang,Francesco Topputo*

Main category: cs.LG

TL;DR: 本文提出了一种通用预训练神经网络，用于预测低推力任务中的燃料消耗和轨迹可达性，通过构建大规模数据集和自相似空间转换，实现了高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 低推力任务中燃料消耗和轨迹可达性是关键指标，现有方法缺乏通用性和准确性。

Method: 基于缩放定律构建数据集，利用同伦射线方法生成数据，并将其转换为自相似空间，训练神经网络。

Result: 神经网络在预测速度增量和最小转移时间时相对误差分别为0.78%和0.63%，并在多种场景中验证了其泛化能力。

Conclusion: 该方法提供了目前最通用和精确的低推力轨迹近似器，适用于多样化任务场景。

Abstract: In trajectory design, fuel consumption and trajectory reachability are two
key performance indicators for low-thrust missions. This paper proposes
general-purpose pretrained neural networks to predict these metrics. The
contributions of this paper are as follows: Firstly, based on the confirmation
of the Scaling Law applicable to low-thrust trajectory approximation, the
largest dataset is constructed using the proposed homotopy ray method, which
aligns with mission-design-oriented data requirements. Secondly, the data are
transformed into a self-similar space, enabling the neural network to adapt to
arbitrary semi-major axes, inclinations, and central bodies. This extends the
applicability beyond existing studies and can generalize across diverse mission
scenarios without retraining. Thirdly, to the best of our knowledge, this work
presents the current most general and accurate low-thrust trajectory
approximator, with implementations available in C++, Python, and MATLAB. The
resulting neural network achieves a relative error of 0.78% in predicting
velocity increments and 0.63% in minimum transfer time estimation. The models
have also been validated on a third-party dataset, multi-flyby mission design
problem, and mission analysis scenario, demonstrating their generalization
capability, predictive accuracy, and computational efficiency.

</details>


### [34] [Mathematical Foundations of Geometric Deep Learning](https://arxiv.org/abs/2508.02723)
*Haitz Sáez de Ocáriz Borde,Michael Bronstein*

Main category: cs.LG

TL;DR: 概述几何深度学习所需的关键数学概念。


<details>
  <summary>Details</summary>
Motivation: 为研究几何深度学习提供数学基础。

Method: 回顾相关数学概念。

Result: 总结了关键数学工具。

Conclusion: 为几何深度学习的学习者提供了必要的数学背景。

Abstract: We review the key mathematical concepts necessary for studying Geometric Deep
Learning.

</details>


### [35] [Streaming Generated Gaussian Process Experts for Online Learning and Control](https://arxiv.org/abs/2508.03679)
*Zewen Yang,Dongfa Zhang,Xiaobing Dai,Fengyi Yu,Chi Zhang,Bingkun Huang,Hamid Sadeghian,Sami Haddadin*

Main category: cs.LG

TL;DR: 提出了一种名为SkyGP的流式高斯过程框架，通过维护有限专家集解决计算和存储问题，同时保留高斯过程的性能保证。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在流式数据中计算和存储复杂度高，难以扩展到大规模数据集。

Method: 提出SkyGP框架，包括两种变体（SkyGP-Dense和SkyGP-Fast），分别优化预测精度和计算效率。

Result: 实验验证SkyGP在性能和效率上优于现有方法。

Conclusion: SkyGP有效解决了高斯过程在流式数据中的计算和存储问题，同时保持高性能。

Abstract: Gaussian Processes (GPs), as a nonparametric learning method, offer flexible
modeling capabilities and calibrated uncertainty quantification for function
approximations. Additionally, GPs support online learning by efficiently
incorporating new data with polynomial-time computation, making them
well-suited for safety-critical dynamical systems that require rapid
adaptation. However, the inference and online updates of exact GPs, when
processing streaming data, incur cubic computation time and quadratic storage
memory complexity, limiting their scalability to large datasets in real-time
settings. In this paper, we propose a \underline{s}treaming
\underline{k}ernel-induced progressivel\underline{y} generated expert framework
of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both
computational and memory constraints by maintaining a bounded set of experts,
while inheriting the learning performance guarantees from exact Gaussian
processes. Furthermore, two SkyGP variants are introduced, each tailored to a
specific objective, either maximizing prediction accuracy (SkyGP-Dense) or
improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is
validated through extensive benchmarks and real-time control experiments
demonstrating its superior performance compared to state-of-the-art approaches.

</details>


### [36] [Forecasting NCAA Basketball Outcomes with Deep Learning: A Comparative Study of LSTM and Transformer Models](https://arxiv.org/abs/2508.02725)
*Md Imtiaz Habib*

Main category: cs.LG

TL;DR: 研究使用LSTM和Transformer模型预测2025年NCAA篮球锦标赛结果，结合多种特征工程和损失函数优化，发现Transformer在分类性能上更优，而LSTM在概率校准上表现更好。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习模型在体育赛事预测中的应用，尤其是NCAA篮球锦标赛的结果预测。

Method: 使用LSTM和Transformer模型，结合GLM、Elo评分、种子差异和统计数据等特征工程，并采用BCE和Brier损失函数进行训练。

Result: Transformer模型在BCE优化下分类性能最佳（AUC 0.8473），LSTM模型在Brier损失下概率校准最优（Brier分数0.1589）。

Conclusion: 模型架构和损失函数的选择需根据预测任务的具体需求，研究为体育分析提供了可复现的框架。

Abstract: In this research, I explore advanced deep learning methodologies to forecast
the outcomes of the 2025 NCAA Division 1 Men's and Women's Basketball
tournaments. Leveraging historical NCAA game data, I implement two
sophisticated sequence-based models: Long Short-Term Memory (LSTM) and
Transformer architectures. The predictive power of these models is augmented
through comprehensive feature engineering, including team quality metrics
derived from Generalized Linear Models (GLM), Elo ratings, seed differences,
and aggregated box-score statistics. To evaluate the robustness and reliability
of predictions, I train each model variant using both Binary Cross-Entropy
(BCE) and Brier loss functions, providing insights into classification
performance and probability calibration. My comparative analysis reveals that
while the Transformer architecture optimized with BCE yields superior
discriminative power (highest AUC of 0.8473), the LSTM model trained with Brier
loss demonstrates superior probabilistic calibration (lowest Brier score of
0.1589). These findings underscore the importance of selecting appropriate
model architectures and loss functions based on the specific requirements of
forecasting tasks. The detailed analytical pipeline presented here serves as a
reproducible framework for future predictive modeling tasks in sports analytics
and beyond.

</details>


### [37] [Beyond Least Squares: Robust Regression Transformer (R2T)](https://arxiv.org/abs/2508.02874)
*Roman Gutierrez,Tony Kai Tang,Isabel Gutierrez*

Main category: cs.LG

TL;DR: 论文提出了一种混合神经符号架构，用于处理非对称结构化噪声下的回归问题，相比传统方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统最小二乘优化方法在处理非对称结构化噪声时表现不佳，需要更鲁棒的回归技术。

Method: 结合Transformer编码器和压缩神经网络预测符号参数，通过固定符号方程重构原始序列。

Result: 在合成可穿戴数据上，中位回归MSE为6e-6至3.5e-5，比普通最小二乘和鲁棒回归方法提升10-300倍。

Conclusion: 混合神经符号架构在处理非对称噪声时优于传统方法，展示了神经与符号结合的潜力。

Abstract: Robust regression techniques rely on least-squares optimization, which works
well for Gaussian noise but fails in the presence of asymmetric structured
noise. We propose a hybrid neural-symbolic architecture where a transformer
encoder processes numerical sequences, a compression NN predicts symbolic
parameters, and a fixed symbolic equation reconstructs the original sequence.
Using synthetic data, the training objective is to recover the original
sequence after adding asymmetric structured noise, effectively learning a
symbolic fit guided by neural parameter estimation. Our model achieves a median
regression MSE of 6e-6 to 3.5e-5 on synthetic wearable data, which is a 10-300
times improvement when compared with ordinary least squares fit and robust
regression techniques such as Huber loss or SoftL1.

</details>


### [38] [Embedding-Enhanced Probabilistic Modeling of Ferroelectric Field Effect Transistors (FeFETs)](https://arxiv.org/abs/2508.02737)
*Tasnia Nobi Afee,Jack Hutchins,Md Mazharul Islam,Thomas Kampfe,Ahmedullah Aziz*

Main category: cs.LG

TL;DR: 提出了一种基于混合密度网络（MDN）的概率建模框架，用于准确捕捉FeFET的随机性，解决了现有模型在稳定性和变异性建模上的不足。


<details>
  <summary>Details</summary>
Motivation: FeFET的随机性（来自操作循环和制造变异性）对准确建模提出了挑战，现有模型难以全面捕捉变异性或缺乏数学平滑性。

Method: 采用MDN框架，结合C-infinity连续激活函数和设备特定嵌入层，生成合成设备实例进行变异性感知模拟。

Result: 模型在捕捉FeFET电流行为变异性方面表现出高准确性（R2=0.92）。

Conclusion: 该框架为FeFET的随机行为建模提供了可扩展的数据驱动解决方案，并为未来紧凑模型开发和电路模拟集成奠定了基础。

Abstract: FeFETs hold strong potential for advancing memory and logic technologies, but
their inherent randomness arising from both operational cycling and fabrication
variability poses significant challenges for accurate and reliable modeling.
Capturing this variability is critical, as it enables designers to predict
behavior, optimize performance, and ensure reliability and robustness against
variations in manufacturing and operating conditions. Existing deterministic
and machine learning-based compact models often fail to capture the full extent
of this variability or lack the mathematical smoothness required for stable
circuit-level integration. In this work, we present an enhanced probabilistic
modeling framework for FeFETs that addresses these limitations. Building upon a
Mixture Density Network (MDN) foundation, our approach integrates C-infinity
continuous activation functions for smooth, stable learning and a
device-specific embedding layer to capture intrinsic physical variability
across devices. Sampling from the learned embedding distribution enables the
generation of synthetic device instances for variability-aware simulation. With
an R2 of 0.92, the model demonstrates high accuracy in capturing the
variability of FeFET current behavior. Altogether, this framework provides a
scalable, data-driven solution for modeling the full stochastic behavior of
FeFETs and offers a strong foundation for future compact model development and
circuit simulation integration.

</details>


### [39] [BoostTransformer: Enhancing Transformer Models with Subgrid Selection and Importance Sampling](https://arxiv.org/abs/2508.02924)
*Biyi Fang,Jean Utke,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: BoostTransformer结合了提升原则和Transformer架构，通过子网格令牌选择和重要性加权采样，提高了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在计算资源和超参数调优上的高需求问题。

Method: 提出BoostTransformer框架，将最小二乘提升目标融入Transformer流程，实现子网格令牌选择和重要性加权采样。

Result: 在多个细粒度文本分类基准测试中，BoostTransformer表现出更快的收敛速度和更高的准确性。

Conclusion: BoostTransformer在减少架构搜索开销的同时，优于标准Transformer。

Abstract: Transformer architectures dominate modern NLP but often demand heavy
computational resources and intricate hyperparameter tuning. To mitigate these
challenges, we propose a novel framework, BoostTransformer, that augments
transformers with boosting principles through subgrid token selection and
importance-weighted sampling. Our method incorporates a least square boosting
objective directly into the transformer pipeline, enabling more efficient
training and improved performance. Across multiple fine-grained text
classification benchmarks, BoostTransformer demonstrates both faster
convergence and higher accuracy, surpassing standard transformers while
minimizing architectural search overhead.

</details>


### [40] [DeepGB-TB: A Risk-Balanced Cross-Attention Gradient-Boosted Convolutional Network for Rapid, Interpretable Tuberculosis Screening](https://arxiv.org/abs/2508.02741)
*Zhixiang Lu,Yulong Li,Feilong Tang,Zhengyong Jiang,Chong Li,Mian Zhou,Tenglong Li,Jionglong Su*

Main category: cs.LG

TL;DR: DeepGB-TB是一种非侵入性系统，通过咳嗽音频和基本人口统计数据快速评估结核病风险，结合轻量级神经网络和梯度提升树，创新性地使用跨模态双向交叉注意力模块，并在临床优先考虑下设计风险平衡损失函数，实现高效、可靠的结核病筛查。


<details>
  <summary>Details</summary>
Motivation: 传统结核病诊断成本高且操作复杂，限制了大规模筛查，因此需要人工智能解决方案。

Method: 结合一维卷积神经网络处理音频和梯度提升决策树处理表格数据，创新性地引入跨模态双向交叉注意力模块（CM-BCA），并设计结核病风险平衡损失函数（TRBL）以减少假阴性预测。

Result: 在7个国家1,105名患者的数据集上，AUROC为0.903，F1分数为0.851，达到最新技术水平，且计算高效，适合低资源环境。

Conclusion: DeepGB-TB通过结合AI创新与公共卫生需求，为全球结核病控制提供了高效、可靠的工具。

Abstract: Large-scale tuberculosis (TB) screening is limited by the high cost and
operational complexity of traditional diagnostics, creating a need for
artificial-intelligence solutions. We propose DeepGB-TB, a non-invasive system
that instantly assigns TB risk scores using only cough audio and basic
demographic data. The model couples a lightweight one-dimensional convolutional
neural network for audio processing with a gradient-boosted decision tree for
tabular features. Its principal innovation is a Cross-Modal Bidirectional
Cross-Attention module (CM-BCA) that iteratively exchanges salient cues between
modalities, emulating the way clinicians integrate symptoms and risk factors.
To meet the clinical priority of minimizing missed cases, we design a
Tuberculosis Risk-Balanced Loss (TRBL) that places stronger penalties on
false-negative predictions, thereby reducing high-risk misclassifications.
DeepGB-TB is evaluated on a diverse dataset of 1,105 patients collected across
seven countries, achieving an AUROC of 0.903 and an F1-score of 0.851,
representing a new state of the art. Its computational efficiency enables
real-time, offline inference directly on common mobile devices, making it ideal
for low-resource settings. Importantly, the system produces clinically
validated explanations that promote trust and adoption by frontline health
workers. By coupling AI innovation with public-health requirements for speed,
affordability, and reliability, DeepGB-TB offers a tool for advancing global TB
control.

</details>


### [41] [Achieving Limited Adaptivity for Multinomial Logistic Bandits](https://arxiv.org/abs/2508.03072)
*Sukruta Prakash Midigeshi,Tanmay Goyal,Gaurav Sinha*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multinomial Logistic Bandits have recently attracted much attention due to
their ability to model problems with multiple outcomes. In this setting, each
decision is associated with many possible outcomes, modeled using a multinomial
logit function. Several recent works on multinomial logistic bandits have
simultaneously achieved optimal regret and computational efficiency. However,
motivated by real-world challenges and practicality, there is a need to develop
algorithms with limited adaptivity, wherein we are allowed only $M$ policy
updates. To address these challenges, we present two algorithms, B-MNL-CB and
RS-MNL, that operate in the batched and rarely-switching paradigms,
respectively. The batched setting involves choosing the $M$ policy update
rounds at the start of the algorithm, while the rarely-switching setting can
choose these $M$ policy update rounds in an adaptive fashion. Our first
algorithm, B-MNL-CB extends the notion of distributional optimal designs to the
multinomial setting and achieves $\tilde{O}(\sqrt{T})$ regret assuming the
contexts are generated stochastically when presented with $\Omega(\log \log T)$
update rounds. Our second algorithm, RS-MNL works with adversarially generated
contexts and can achieve $\tilde{O}(\sqrt{T})$ regret with $\tilde{O}(\log T)$
policy updates. Further, we conducted experiments that demonstrate that our
algorithms (with a fixed number of policy updates) are extremely competitive
(and often better) than several state-of-the-art baselines (which update their
policy every round), showcasing the applicability of our algorithms in various
practical scenarios.

</details>


### [42] [Considering Spatial Structure of the Road Network in Pavement Deterioration Modeling](https://arxiv.org/abs/2508.02749)
*Lu Gao,Ke Yu,Pan Lu*

Main category: cs.LG

TL;DR: 该研究通过图神经网络（GNN）将道路网络的空间依赖性纳入路面退化建模，结果表明考虑空间关系能提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 利用GNN能够直接利用道路网络的丰富结构信息，从而改进路面性能建模。

Method: 使用图神经网络（GNN）结合道路网络的空间结构，基于德克萨斯州交通部的路面管理信息系统（PMIS）中的大量数据进行建模。

Result: 比较结果表明，考虑空间关系的路面退化预测模型表现更好。

Conclusion: 空间依赖性对路面退化建模具有显著影响，GNN是一种有效的工具。

Abstract: Pavement deterioration modeling is important in providing information
regarding the future state of the road network and in determining the needs of
preventive maintenance or rehabilitation treatments. This research incorporated
spatial dependence of road network into pavement deterioration modeling through
a graph neural network (GNN). The key motivation of using a GNN for pavement
performance modeling is the ability to easily and directly exploit the rich
structural information in the network. This paper explored if considering
spatial structure of the road network will improve the prediction performance
of the deterioration models. The data used in this research comprises a large
pavement condition data set with more than a half million observations taken
from the Pavement Management Information System (PMIS) maintained by the Texas
Department of Transportation. The promising comparison results indicates that
pavement deterioration prediction models perform better when spatial
relationship is considered.

</details>


### [43] [Convergence of Deterministic and Stochastic Diffusion-Model Samplers: A Simple Analysis in Wasserstein Distance](https://arxiv.org/abs/2508.03210)
*Eliot Beyler,Francis Bach*

Main category: cs.LG

TL;DR: 论文提供了扩散生成模型在Wasserstein距离下的新收敛保证，涵盖随机和确定性采样方法，并改进了Heun采样器和Euler采样器的收敛结果。


<details>
  <summary>Details</summary>
Motivation: 分析扩散生成模型的收敛性，强调学习得分函数的空间规律性，并改进初始化误差界限。

Method: 引入简单框架分析离散化、初始化和得分估计误差，结合平滑Wasserstein距离改进结果。

Result: 首次推导Heun采样器的Wasserstein收敛界限，改进Euler采样器的结果。

Conclusion: 控制得分误差对真实反向过程的影响是关键，平滑Wasserstein距离有助于优化初始化误差界限。

Abstract: We provide new convergence guarantees in Wasserstein distance for
diffusion-based generative models, covering both stochastic (DDPM-like) and
deterministic (DDIM-like) sampling methods. We introduce a simple framework to
analyze discretization, initialization, and score estimation errors. Notably,
we derive the first Wasserstein convergence bound for the Heun sampler and
improve existing results for the Euler sampler of the probability flow ODE. Our
analysis emphasizes the importance of spatial regularity of the learned score
function and argues for controlling the score error with respect to the true
reverse process, in line with denoising score matching. We also incorporate
recent results on smoothed Wasserstein distances to sharpen initialization
error bounds.

</details>


### [44] [Pulse Shape Discrimination Algorithms: Survey and Benchmark](https://arxiv.org/abs/2508.02750)
*Haoran Liu,Yihan Zhan,Mingzhe Liu,Yanhua Liu,Peng Li,Zhuo Zuo,Bingqi Liu,Runxi Liu*

Main category: cs.LG

TL;DR: 本文综述了脉冲形状判别（PSD）算法，评估了近60种方法，发现深度学习模型（如MLPs）通常优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为辐射检测提供全面的PSD算法评估，促进该领域的研究发展。

Method: 将算法分为统计和先验知识范式，并在两个标准化数据集上实现和评估。

Result: 深度学习模型（尤其是MLPs和混合方法）表现最佳。

Conclusion: 发布开源工具包和数据集，以推动PSD研究的可重复性和进展。

Abstract: This review presents a comprehensive survey and benchmark of pulse shape
discrimination (PSD) algorithms for radiation detection, classifying nearly
sixty methods into statistical (time-domain, frequency-domain, neural
network-based) and prior-knowledge (machine learning, deep learning) paradigms.
We implement and evaluate all algorithms on two standardized datasets: an
unlabeled set from a 241Am-9Be source and a time-of-flight labeled set from a
238Pu-9Be source, using metrics including Figure of Merit (FOM), F1-score,
ROC-AUC, and inter-method correlations. Our analysis reveals that deep learning
models, particularly Multi-Layer Perceptrons (MLPs) and hybrid approaches
combining statistical features with neural regression, often outperform
traditional methods. We discuss architectural suitabilities, the limitations of
FOM, alternative evaluation metrics, and performance across energy thresholds.
Accompanying this work, we release an open-source toolbox in Python and MATLAB,
along with the datasets, to promote reproducibility and advance PSD research.

</details>


### [45] [On Conformal Machine Unlearning](https://arxiv.org/abs/2508.03245)
*Yahya Alkhatib,Wee Peng Tay*

Main category: cs.LG

TL;DR: 论文提出了一种基于Conformal Prediction的Machine Unlearning新方法，提供统计保证并优化了效率。


<details>
  <summary>Details</summary>
Motivation: 数据隐私需求增加，现有MU方法缺乏统计保证且计算成本高。

Method: 基于Conformal Prediction定义MU，提出ECF和EuCF指标，并设计优化方法。

Result: 实验表明该方法能有效移除目标数据。

Conclusion: 新方法为MU提供了统计保证和高效解决方案。

Abstract: The increasing demand for data privacy, driven by regulations such as GDPR
and CCPA, has made Machine Unlearning (MU) essential for removing the influence
of specific training samples from machine learning models while preserving
performance on retained data. However, most existing MU methods lack rigorous
statistical guarantees, rely on heuristic metrics, and often require
computationally expensive retraining baselines. To overcome these limitations,
we introduce a new definition for MU based on Conformal Prediction (CP),
providing statistically sound, uncertainty-aware guarantees without the need
for the concept of naive retraining. We formalize conformal criteria that
quantify how often forgotten samples are excluded from CP sets, and propose
empirical metrics,the Efficiently Covered Frequency (ECF at c) and its
complement, the Efficiently Uncovered Frequency (EuCF at d), to measure the
effectiveness of unlearning. We further present a practical unlearning method
designed to optimize these conformal metrics. Extensive experiments across
diverse forgetting scenarios, datasets and models demonstrate the efficacy of
our approach in removing targeted data.

</details>


### [46] [SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference](https://arxiv.org/abs/2508.02751)
*Yi Zhao,Yajuan Peng,Cam-Tu Nguyen,Zuchao Li,Xiaoliang Wang,Hai Zhao,Xiaoming Fu*

Main category: cs.LG

TL;DR: SmallKV通过小模型辅助补偿机制解决KV缓存淘汰中的注意力匹配问题，提升LLM在资源受限环境中的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存淘汰方法无法适应动态注意力模式，且忽视边际信息的重要性，导致性能下降。

Method: 设计基于不同规模LLM注意力矩阵相似性的补偿机制，提出SmallKV方法，利用小模型辅助大模型感知全局重要信息和近似边际令牌的注意力分数。

Result: 在多个基准测试中验证了SmallKV的有效性，吞吐量比基线方法提高1.75-2.56倍。

Conclusion: SmallKV在资源受限环境下实现了高效且高性能的LLM推理。

Abstract: KV cache eviction has emerged as an effective solution to alleviate resource
constraints faced by LLMs in long-context scenarios. However, existing
token-level eviction methods often overlook two critical aspects: (1) their
irreversible eviction strategy fails to adapt to dynamic attention patterns
during decoding (the saliency shift problem), and (2) they treat both
marginally important tokens and truly unimportant tokens equally, despite the
collective significance of marginal tokens to model performance (the marginal
information over-compression problem). To address these issues, we design two
compensation mechanisms based on the high similarity of attention matrices
between LLMs of different scales. We propose SmallKV, a small model assisted
compensation method for KV cache compression. SmallKV can maintain attention
matching between different-scale LLMs to: 1) assist the larger model in
perceiving globally important information of attention; and 2) use the smaller
model's attention scores to approximate those of marginal tokens in the larger
model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and
LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency
evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than
baseline methods, highlighting its potential for efficient and performant LLM
inference in resource constrained environments.

</details>


### [47] [The alpha-beta divergence for real and complex data](https://arxiv.org/abs/2508.03272)
*Sergio Cruces*

Main category: cs.LG

TL;DR: 该论文扩展了alpha-beta散度的定义，使其适用于复数数据，并展示了其与经典距离的关系，为复数信号处理提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 复数数据在信号处理中广泛存在，但现有散度方法主要针对非负数据，需要一种适用于复数数据的通用框架。

Method: 通过扩展alpha-beta散度定义，使其支持复数向量，并通过调整超参数实现与经典距离的联系。

Result: 提出了复数alpha-beta散度的闭式解，揭示了超参数的不同作用，并展示了其与欧几里得和马氏距离的关系。

Conclusion: 复数alpha-beta散度为复数信号处理提供了灵活且通用的工具，具有广泛的应用潜力。

Abstract: Divergences are fundamental to the information criteria that underpin most
signal processing algorithms. The alpha-beta family of divergences, designed
for non-negative data, offers a versatile framework that parameterizes and
continuously interpolates several separable divergences found in existing
literature. This work extends the definition of alpha-beta divergences to
accommodate complex data, specifically when the arguments of the divergence are
complex vectors. This novel formulation is designed in such a way that, by
setting the divergence hyperparameters to unity, it particularizes to the
well-known Euclidean and Mahalanobis squared distances. Other choices of
hyperparameters yield practical separable and non-separable extensions of
several classical divergences. In the context of the problem of approximating a
complex random vector, the centroid obtained by optimizing the alpha-beta mean
distortion has a closed-form expression, which interpretation sheds light on
the distinct roles of the divergence hyperparameters. These contributions may
have wide potential applicability, as there are many signal processing domains
in which the underlying data are inherently complex.

</details>


### [48] [DMSC: Dynamic Multi-Scale Coordination Framework for Time Series Forecasting](https://arxiv.org/abs/2508.02753)
*Haonan Yang,Jianchao Tang,Zhuo Li,Long Lan*

Main category: cs.LG

TL;DR: 提出动态多尺度协调框架（DMSC），解决时间序列预测中静态分解、依赖建模碎片化和融合机制不灵活的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模复杂时间依赖时存在静态分解策略、依赖建模不完整和融合机制不灵活的问题。

Method: 采用多尺度块分解（EMPD）、三元交互块（TIB）和自适应尺度路由MoE块（ASR-MoE），动态调整尺度并联合建模依赖。

Result: 在13个真实基准测试中，DMSC表现最优且计算高效。

Conclusion: DMSC通过动态多尺度协调显著提升了时间序列预测的性能。

Abstract: Time Series Forecasting (TSF) faces persistent challenges in modeling
intricate temporal dependencies across different scales. Despite recent
advances leveraging different decomposition operations and novel architectures
based on CNN, MLP or Transformer, existing methods still struggle with static
decomposition strategies, fragmented dependency modeling, and inflexible fusion
mechanisms, limiting their ability to model intricate temporal dependencies. To
explicitly solve the mentioned three problems respectively, we propose a novel
Dynamic Multi-Scale Coordination Framework (DMSC) with Multi-Scale Patch
Decomposition block (EMPD), Triad Interaction Block (TIB) and Adaptive Scale
Routing MoE block (ASR-MoE). Specifically, EMPD is designed as a built-in
component to dynamically segment sequences into hierarchical patches with
exponentially scaled granularities, eliminating predefined scale constraints
through input-adaptive patch adjustment. TIB then jointly models intra-patch,
inter-patch, and cross-variable dependencies within each layer's decomposed
representations. EMPD and TIB are jointly integrated into layers forming a
multi-layer progressive cascade architecture, where coarse-grained
representations from earlier layers adaptively guide fine-grained feature
extraction in subsequent layers via gated pathways. And ASR-MoE dynamically
fuses multi-scale predictions by leveraging specialized global and local
experts with temporal-aware weighting. Comprehensive experiments on thirteen
real-world benchmarks demonstrate that DMSC consistently maintains
state-of-the-art (SOTA) performance and superior computational efficiency for
TSF tasks. Code is available at https://github.com/1327679995/DMSC.

</details>


### [49] [On the (In)Significance of Feature Selection in High-Dimensional Datasets](https://arxiv.org/abs/2508.03593)
*Bhavesh Neekhra,Debayan Gupta,Partha Pratim Chakravarti*

Main category: cs.LG

TL;DR: 研究发现，在高维数据（如基因表达）的分类任务中，特征选择（FS）算法并不比随机选择特征更有效。


<details>
  <summary>Details</summary>
Motivation: 验证特征选择算法的性能，比较随机选择特征与FS算法选择特征的差异。

Method: 通过比较随机选择特征与FS算法选择特征在分类任务中的表现，测试FS算法的有效性。

Result: 随机选择的小部分特征（0.02%-1%）与使用所有特征或FS算法选择的特征表现相当或更好。

Conclusion: 研究质疑高维数据（尤其是计算基因组学）中特征选择的有效性，并强调湿实验验证的重要性。

Abstract: Extensive research has been done on feature selection (FS) algorithms for
high-dimensional datasets aiming to improve model performance, reduce
computational cost and identify features of interest. We test the null
hypothesis of using randomly selected features to compare against features
selected by FS algorithms to validate the performance of the latter. Our
results show that FS on high-dimensional datasets (in particular gene
expression) in classification tasks is not useful. We find that (1) models
trained on small subsets (0.02%-1% of all features) of randomly selected
features almost always perform comparably to those trained on all features, and
(2) a "typical"- sized random subset provides comparable or superior
performance to that of top-k features selected in various published studies.
Thus, our work challenges many feature selection results on high dimensional
datasets, particularly in computational genomics. It raises serious concerns
about studies that propose drug design or targeted interventions based on
computationally selected genes, without further validation in a wet lab.

</details>


### [50] [Context-Adaptive Multi-Prompt LLM Embedding for Vision-Language Alignment](https://arxiv.org/abs/2508.02762)
*Dahun Kim,Anelia Angelova*

Main category: cs.LG

TL;DR: 提出了一种名为Context-Adaptive Multi-Prompt Embedding的新方法，通过多提示嵌入增强视觉-语言对比学习中的语义表示。


<details>
  <summary>Details</summary>
Motivation: 标准CLIP模型依赖单一文本嵌入，限制了语义多样性。

Method: 引入多个结构化提示，每个提示包含一个自适应令牌，联合处理所有提示并生成统一文本表示，结合多样性正则化损失和否定感知损失。

Result: 在图像-文本和视频-文本检索基准上取得一致改进。

Conclusion: 多提示嵌入方法能有效提升语义对齐和对比学习性能。

Abstract: We propose Context-Adaptive Multi-Prompt Embedding, a novel approach to
enrich semantic representations in vision-language contrastive learning. Unlike
standard CLIP-style models that rely on a single text embedding, our method
introduces multiple structured prompts, each containing a distinct adaptive
token that captures diverse semantic aspects of the input text. We process all
prompts jointly in a single forward pass. The resulting prompt embeddings are
combined into a unified text representation, enabling semantically richer
alignment with visual features. To further promote semantic diversity and
representation quality, we incorporate a diversity regularization loss and a
negation-aware loss, encouraging specialization across prompts and improving
contrastive discrimination. Our method achieves consistent improvements on both
image-text and video-text retrieval benchmarks.

</details>


### [51] [Pair Correlation Factor and the Sample Complexity of Gaussian Mixtures](https://arxiv.org/abs/2508.03633)
*Farzad Aryan*

Main category: cs.LG

TL;DR: 论文研究了高斯混合模型（GMMs）的学习问题，提出了一个新的几何量PCF，用于更准确地描述参数恢复的难度，并改进了样本复杂度的界限。


<details>
  <summary>Details</summary>
Motivation: 探讨高斯混合模型样本复杂度的结构特性，发现现有研究仅关注组件间的最小分离距离是不完整的。

Method: 引入Pair Correlation Factor (PCF)这一几何量，并在均匀球形情况下提出改进算法。

Result: PCF比最小分离距离更能准确反映参数恢复的难度，算法在特定情况下改进了样本复杂度的界限。

Conclusion: PCF是影响高斯混合模型样本复杂度的关键因素，现有研究需考虑更多几何特性。

Abstract: We study the problem of learning Gaussian Mixture Models (GMMs) and ask:
which structural properties govern their sample complexity? Prior work has
largely tied this complexity to the minimum pairwise separation between
components, but we demonstrate this view is incomplete.
  We introduce the \emph{Pair Correlation Factor} (PCF), a geometric quantity
capturing the clustering of component means. Unlike the minimum gap, the PCF
more accurately dictates the difficulty of parameter recovery.
  In the uniform spherical case, we give an algorithm with improved sample
complexity bounds, showing when more than the usual $\epsilon^{-2}$ samples are
necessary.

</details>


### [52] [Synthetic medical data generation: state of the art and application to trauma mechanism classification](https://arxiv.org/abs/2508.02771)
*Océane Doremus,Ariel Guerra-Adames,Marta Avalos-Fernandez,Vianney Jouhet,Cédric Gil-Jardiné,Emmanuel Lagarde*

Main category: cs.LG

TL;DR: 论文概述了机器学习在生成合成医疗数据中的应用，特别是针对创伤机制自动分类，提出了一种结合表格和非结构化文本数据的高质量合成医疗记录生成方法。


<details>
  <summary>Details</summary>
Motivation: 面对患者隐私和科学可重复性的挑战，研究转向合成医疗数据库的构建。

Method: 提出了一种结合表格和非结构化文本数据的高质量合成医疗记录生成方法。

Result: 未明确提及具体结果，但目标是生成高质量的合成数据。

Conclusion: 合成医疗数据是解决隐私和可重复性问题的潜在方案。

Abstract: Faced with the challenges of patient confidentiality and scientific
reproducibility, research on machine learning for health is turning towards the
conception of synthetic medical databases. This article presents a brief
overview of state-of-the-art machine learning methods for generating synthetic
tabular and textual data, focusing their application to the automatic
classification of trauma mechanisms, followed by our proposed methodology for
generating high-quality, synthetic medical records combining tabular and
unstructured text data.

</details>


### [53] [Uncertainty Sets for Distributionally Robust Bandits Using Structural Equation Models](https://arxiv.org/abs/2508.02812)
*Katherine Avery,Chinmay Pendse,David Jensen*

Main category: cs.LG

TL;DR: 提出了一种基于结构方程模型（SEM）的分布鲁棒评估和学习算法，解决了传统方法过于保守的问题，并通过条件独立性测试检测变量偏移。


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒评估和学习方法过于保守，导致评估和策略效果不佳。

Method: 使用结构方程模型（SEM）定制不确定性集，并结合条件独立性测试检测变量偏移。

Result: SEM方法比传统方法更准确，特别是在大偏移情况下，且能学习到低方差策略。

Conclusion: SEM方法在模型充分指定的情况下能学习到最优策略，优于传统方法。

Abstract: Distributionally robust evaluation estimates the worst-case expected return
over an uncertainty set of possible covariate and reward distributions, and
distributionally robust learning finds a policy that maximizes that worst-case
return across that uncertainty set. Unfortunately, current methods for
distributionally robust evaluation and learning create overly conservative
evaluations and policies. In this work, we propose a practical bandit
evaluation and learning algorithm that tailors the uncertainty set to specific
problems using mathematical programs constrained by structural equation models.
Further, we show how conditional independence testing can be used to detect
shifted variables for modeling. We find that the structural equation model
(SEM) approach gives more accurate evaluations and learns lower-variance
policies than traditional approaches, particularly for large shifts. Further,
the SEM approach learns an optimal policy, assuming the model is sufficiently
well-specified.

</details>


### [54] [On the Theory and Practice of GRPO: A Trajectory-Corrected Approach with Fast Convergence](https://arxiv.org/abs/2508.02833)
*Lei Pang,Ruinan Jin*

Main category: cs.LG

TL;DR: GRPO是一种无评论家的强化学习算法，用于微调大语言模型，通过组归一化奖励替代PPO中的价值函数。研究发现GRPO的更新规则实际上是在旧策略上估计梯度，而非当前策略。基于此，提出了TIC GRPO算法，使用轨迹级概率比替代令牌级重要性比，提供无偏的当前策略梯度估计。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO算法的实际表现及其更新规则的偏差问题，提出改进算法以解决偏差并保持无评论家结构。

Method: GRPO使用组归一化奖励和PPO风格的令牌级重要性采样；TIC GRPO引入轨迹级概率比替代令牌级重要性比。

Result: GRPO在旧策略上估计梯度，偏差较小；TIC GRPO提供无偏的当前策略梯度估计，性能与标准GRPO相当。

Conclusion: TIC GRPO是一种改进的无评论家算法，解决了GRPO的偏差问题，并首次提供了GRPO类方法的理论收敛分析。

Abstract: Group Relative Policy Optimization (GRPO), recently proposed by DeepSeek, is
a critic-free reinforcement learning algorithm for fine tuning large language
models. It replaces the value function in Proximal Policy Optimization (PPO)
with group normalized rewards, while retaining PPO style token level importance
sampling based on an old policy. We show that GRPO update rule in fact
estimates the policy gradient at the old policy rather than the current one.
However, since the old policy is refreshed every few steps, the discrepancy
between the two remains small limiting the impact of this bias in practice. We
validate this through an ablation study in which importance sampling is
entirely removed, and updates are instead performed using the gradient
estimated at a fixed old policy across multiple optimization steps. Remarkably,
this simplification results in performance comparable to standard GRPO.
  Motivated by these findings, we propose a new algorithm: Trajectory level
Importance Corrected GRPO (TIC GRPO). TIC GRPO replaces token level importance
ratios with a single trajectory level probability ratio, yielding an unbiased
estimate of the current policy gradient while preserving the critic free
structure. Furthermore, we present the first theoretical convergence analysis
for GRPO style methods, covering both the original GRPO and our proposed
variant.

</details>


### [55] [Learning from B Cell Evolution: Adaptive Multi-Expert Diffusion for Antibody Design via Online Optimization](https://arxiv.org/abs/2508.02834)
*Hanqi Feng,Peng Qiu,Mengchun Zhang,Yiran Tao,You Fan,Jingtao Xu,Barnabas Poczos*

Main category: cs.LG

TL;DR: 提出了一种基于生物启发的自适应抗体设计框架，通过多专家系统和在线元学习优化抗体生成策略。


<details>
  <summary>Details</summary>
Motivation: 现有抗体设计方法采用统一生成策略，无法适应不同抗原的独特需求，而生物中的B细胞亲和力成熟过程提供了多目标优化的灵感。

Method: 利用物理基础的领域知识，结合多专家系统（如范德华力、分子识别等）和在线元学习，动态调整生成策略。

Result: 实验表明，该方法能自适应发现最佳生成策略，显著提升热点覆盖和界面质量，并适用于多种设计挑战。

Conclusion: 该框架为抗体设计提供了一种自适应、多目标优化的新范式，适用于个性化治疗需求。

Abstract: Recent advances in diffusion models have shown remarkable potential for
antibody design, yet existing approaches apply uniform generation strategies
that cannot adapt to each antigen's unique requirements. Inspired by B cell
affinity maturation, where antibodies evolve through multi-objective
optimization balancing affinity, stability, and self-avoidance, we propose the
first biologically-motivated framework that leverages physics-based domain
knowledge within an online meta-learning system. Our method employs multiple
specialized experts (van der Waals, molecular recognition, energy balance, and
interface geometry) whose parameters evolve during generation based on
iterative feedback, mimicking natural antibody refinement cycles. Instead of
fixed protocols, this adaptive guidance discovers personalized optimization
strategies for each target. Our experiments demonstrate that this approach: (1)
discovers optimal SE(3)-equivariant guidance strategies for different antigen
classes without pre-training, preserving molecular symmetries throughout
optimization; (2) significantly enhances hotspot coverage and interface quality
through target-specific adaptation, achieving balanced multi-objective
optimization characteristic of therapeutic antibodies; (3) establishes a
paradigm for iterative refinement where each antibody-antigen system learns its
unique optimization profile through online evaluation; (4) generalizes
effectively across diverse design challenges, from small epitopes to large
protein interfaces, enabling precision-focused campaigns for individual
targets.

</details>


### [56] [Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation](https://arxiv.org/abs/2508.02835)
*Kennedy Edemacu,Vinay M. Shashidhar,Micheal Tuape,Dan Abudu,Beakcheol Jang,Jong Wook Kim*

Main category: cs.LG

TL;DR: 论文提出了FilterRAG和ML-FilterRAG两种防御方法，用于对抗PoisonedRAG攻击，通过区分对抗性文本和干净文本，有效保护检索增强生成模型。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）模型容易受到知识投毒攻击，攻击者可能通过污染知识源误导模型生成错误回答。

Method: 提出新属性区分对抗性和干净文本，并设计FilterRAG和ML-FilterRAG方法过滤对抗性文本。

Result: 在基准数据集上的评估显示，防御方法性能接近原始RAG系统。

Conclusion: FilterRAG和ML-FilterRAG能有效防御PoisonedRAG攻击，保护RAG模型的可靠性。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
boost the capabilities of large language models (LLMs) by incorporating
external, up-to-date knowledge sources. However, this introduces a potential
vulnerability to knowledge poisoning attacks, where attackers can compromise
the knowledge source to mislead the generation model. One such attack is the
PoisonedRAG in which the injected adversarial texts steer the model to generate
an attacker-chosen response to a target question. In this work, we propose
novel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG
attack. First, we propose a new property to uncover distinct properties to
differentiate between adversarial and clean texts in the knowledge data source.
Next, we employ this property to filter out adversarial texts from clean ones
in the design of our proposed approaches. Evaluation of these methods using
benchmark datasets demonstrate their effectiveness, with performances close to
those of the original RAG systems.

</details>


### [57] [Resource-Efficient Automatic Software Vulnerability Assessment via Knowledge Distillation and Particle Swarm Optimization](https://arxiv.org/abs/2508.02840)
*Chaoyang Gao,Xiang Chen,Jiyu Wang,Jibin Wang,Guang Yang*

Main category: cs.LG

TL;DR: 提出了一种结合知识蒸馏和粒子群优化的资源高效框架，用于自动化漏洞评估，显著减少模型大小并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 软件系统复杂性增加导致网络安全漏洞激增，需要高效且可扩展的漏洞评估解决方案。

Method: 采用两阶段方法：1) 粒子群优化优化紧凑学生模型架构；2) 知识蒸馏将关键知识从大型教师模型迁移至学生模型。

Result: 在增强的MegaVul数据集上，模型大小减少99.4%，保持89.3%准确率，优于现有基线。

Conclusion: 该框架在资源效率和性能上均表现出色，适用于实际部署。

Abstract: The increasing complexity of software systems has led to a surge in
cybersecurity vulnerabilities, necessitating efficient and scalable solutions
for vulnerability assessment. However, the deployment of large pre-trained
models in real-world scenarios is hindered by their substantial computational
and storage demands. To address this challenge, we propose a novel
resource-efficient framework that integrates knowledge distillation and
particle swarm optimization to enable automated vulnerability assessment. Our
framework employs a two-stage approach: First, particle swarm optimization is
utilized to optimize the architecture of a compact student model, balancing
computational efficiency and model capacity. Second, knowledge distillation is
applied to transfer critical vulnerability assessment knowledge from a large
teacher model to the optimized student model. This process significantly
reduces the model size while maintaining high performance. Experimental results
on an enhanced MegaVul dataset, comprising 12,071 CVSS (Common Vulnerability
Scoring System) v3 annotated vulnerabilities, demonstrate the effectiveness of
our approach. Our approach achieves a 99.4% reduction in model size while
retaining 89.3% of the original model's accuracy. Furthermore, it outperforms
state-of-the-art baselines by 1.7% in accuracy with 60% fewer parameters. The
framework also reduces training time by 72.1% and architecture search time by
34.88% compared to traditional genetic algorithms.

</details>


### [58] [Comparative Evaluation of Kolmogorov-Arnold Autoencoders and Orthogonal Autoencoders for Fault Detection with Varying Training Set Sizes](https://arxiv.org/abs/2508.02860)
*Enrique Luna Villagómez,Vladimir Mahalec*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KANs)作为参数高效的神经网络替代方案，在无监督故障检测中表现出色，尤其是WavKAN-AE和EfficientKAN-AE在小样本和大样本场景下均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索KANs在无监督故障检测中的潜力，填补其在化学过程监控中的应用空白。

Method: 比较四种KAN-AE变体（EfficientKAN、FastKAN、FourierKAN和WavKAN）与Orthogonal Autoencoder (OAE)在Tennessee Eastman Process数据集上的性能，使用Fault Detection Rate (FDR)作为指标。

Result: WavKAN-AE在4000样本下FDR≥92%，表现最佳；EfficientKAN-AE在500样本下FDR≥90%，适合低数据场景；FastKAN-AE在大样本（≥50,000）下竞争力强；FourierKAN-AE表现较差。OAE需要更多数据才能匹配KAN-AE性能。

Conclusion: KAN-AEs结合数据效率和强故障检测能力，其结构化基函数可能提升模型透明度，适合工业应用。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a flexible and
parameter-efficient alternative to conventional neural networks. Unlike
standard architectures that use fixed node-based activations, KANs place
learnable functions on edges, parameterized by different function families.
While they have shown promise in supervised settings, their utility in
unsupervised fault detection remains largely unexplored. This study presents a
comparative evaluation of KAN-based autoencoders (KAN-AEs) for unsupervised
fault detection in chemical processes. We investigate four KAN-AE variants,
each based on a different KAN implementation (EfficientKAN, FastKAN,
FourierKAN, and WavKAN), and benchmark them against an Orthogonal Autoencoder
(OAE) on the Tennessee Eastman Process. Models are trained on normal operating
data across 13 training set sizes and evaluated on 21 fault types, using Fault
Detection Rate (FDR) as the performance metric. WavKAN-AE achieves the highest
overall FDR ($\geq$92\%) using just 4,000 training samples and remains the top
performer, even as other variants are trained on larger datasets.
EfficientKAN-AE reaches $\geq$90\% FDR with only 500 samples, demonstrating
robustness in low-data settings. FastKAN-AE becomes competitive at larger
scales ($\geq$50,000 samples), while FourierKAN-AE consistently underperforms.
The OAE baseline improves gradually but requires substantially more data to
match top KAN-AE performance. These results highlight the ability of KAN-AEs to
combine data efficiency with strong fault detection performance. Their use of
structured basis functions suggests potential for improved model transparency,
making them promising candidates for deployment in data-constrained industrial
settings.

</details>


### [59] [CauKer: classification time series foundation models can be pretrained on synthetic data only](https://arxiv.org/abs/2508.02879)
*Shifeng Xie,Vasilii Feofanov,Marius Alonso,Ambroise Odonnat,Jianfeng Zhang,Themis Palpanas,Ievgen Redko*

Main category: cs.LG

TL;DR: CauKer是一种新算法，通过结合高斯过程核组合与结构因果模型，生成多样且因果一致的合成时间序列数据，用于高效预训练时间序列基础模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型（TSFMs）需要在大规模真实序列上进行计算成本高昂的预训练，而CauKer旨在通过生成合成数据解决这一问题。

Method: CauKer结合高斯过程（GP）核组合与结构因果模型（SCM），生成具有真实趋势、季节性和非线性交互的合成时间序列。

Result: 实验表明，CauKer生成的数据集在数据集规模和模型容量上表现出清晰的缩放规律，而真实数据集则显示不规则缩放行为。

Conclusion: CauKer为时间序列基础模型的样本高效预训练提供了一种有效方法，并揭示了合成数据在模型训练中的潜在优势。

Abstract: Time series foundation models (TSFMs) have recently gained significant
attention due to their strong zero-shot capabilities and widespread real-world
applications. Such models typically require a computationally costly
pretraining on large-scale, carefully curated collections of real-world
sequences. To allow for a sample-efficient pretraining of TSFMs, we propose
CauKer, a novel algorithm designed to generate diverse, causally coherent
synthetic time series with realistic trends, seasonality, and nonlinear
interactions. CauKer combines Gaussian Process (GP) kernel composition with
Structural Causal Models (SCM) to produce data for sample-efficient pretraining
of state-of-the-art classification TSFMs having different architectures and
following different pretraining approaches. Additionally, our experiments
reveal that CauKer-generated datasets exhibit clear scaling laws for both
dataset size (10K to 10M samples) and model capacity (1M to 783M parameters),
unlike real-world datasets, which display irregular scaling behavior.

</details>


### [60] [Neural Networks with Orthogonal Jacobian](https://arxiv.org/abs/2508.02882)
*Alex Massucco,Davide Murari,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: 论文提出了一种统一的数学框架，用于描述一类非线性前馈和残差网络，其输入到输出的雅可比矩阵几乎处处正交，从而实现完美的动态等距性和高效训练。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中梯度消失或爆炸的问题阻碍了其性能提升，现有方法如正交初始化或残差结构虽能缓解，但仍需改进。

Method: 引入一种数学框架，约束网络雅可比矩阵正交，从而稳定梯度传播并提升训练效率。

Result: 实验表明，初始化的完美雅可比正交性足以稳定训练并实现竞争性性能。

Conclusion: 该框架不仅涵盖标准架构，还能设计新网络，无需传统跳跃连接即可实现高效训练。

Abstract: Very deep neural networks achieve state-of-the-art performance by extracting
rich, hierarchical features. Yet, training them via backpropagation is often
hindered by vanishing or exploding gradients. Existing remedies, such as
orthogonal or variance-preserving initialisation and residual architectures,
allow for a more stable gradient propagation and the training of deeper models.
In this work, we introduce a unified mathematical framework that describes a
broad class of nonlinear feedforward and residual networks, whose
input-to-output Jacobian matrices are exactly orthogonal almost everywhere.
Such a constraint forces the resulting networks to achieve perfect dynamical
isometry and train efficiently despite being very deep. Our formulation not
only recovers standard architectures as particular cases but also yields new
designs that match the trainability of residual networks without relying on
conventional skip connections. We provide experimental evidence that perfect
Jacobian orthogonality at initialisation is sufficient to stabilise training
and achieve competitive performance. We compare this strategy to networks
regularised to maintain the Jacobian orthogonality and obtain comparable
results. We further extend our analysis to a class of networks
well-approximated by those with orthogonal Jacobians and introduce networks
with Jacobians representing partial isometries. These generalized models are
then showed to maintain the favourable trainability properties.

</details>


### [61] [Clus-UCB: A Near-Optimal Algorithm for Clustered Bandits](https://arxiv.org/abs/2508.02909)
*Aakash Gore,Prasanna Chaporkar*

Main category: cs.LG

TL;DR: 研究了一个基于已知聚类结构的随机多臂老虎机问题，提出了一种高效算法Clus-UCB，其性能优于经典算法。


<details>
  <summary>Details</summary>
Motivation: 建模多因素影响场景（如在线广告、临床试验等），利用已知聚类结构优化性能。

Method: 提出Clus-UCB算法，利用聚类结构共享信息，设计新的臂评估指标。

Result: Clus-UCB算法在仿真中表现优于KL-UCB等依赖臂算法，接近理论下限。

Conclusion: 工作存在局限性，未来可进一步研究扩展应用场景或优化算法。

Abstract: We study a stochastic multi-armed bandit setting where arms are partitioned
into known clusters, such that the mean rewards of arms within a cluster differ
by at most a known threshold. While the clustering structure is known a priori,
the arm means are unknown. This framework models scenarios where outcomes
depend on multiple factors -- some with significant and others with minor
influence -- such as online advertising, clinical trials, and wireless
communication. We derive asymptotic lower bounds on the regret that improve
upon the classical bound of Lai & Robbins (1985). We then propose Clus-UCB, an
efficient algorithm that closely matches this lower bound asymptotically.
Clus-UCB is designed to exploit the clustering structure and introduces a new
index to evaluate an arm, which depends on other arms within the cluster. In
this way, arms share information among each other. We present simulation
results of our algorithm and compare its performance against KL-UCB and other
well-known algorithms for bandits with dependent arms. Finally, we address some
limitations of this work and conclude by mentioning possible future research.

</details>


### [62] [GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics](https://arxiv.org/abs/2508.02926)
*Arthur Cho*

Main category: cs.LG

TL;DR: GrandJury提出了一种结合时间衰减聚合、完整可追溯性和动态透明任务评分的评估协议，以解决生成式机器学习模型在动态需求下的评估问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成式机器学习模型的评估方法过于静态，无法适应动态用户需求和现实变化，亟需一种更灵活、透明的评估方式。

Method: GrandJury结合时间衰减聚合、完整可追溯性、动态透明任务评分和多评分者人工判断，提供了一种新的评估协议。

Result: 该方法支持多元化和可追溯的评估，能够捕捉动态共识并揭示分歧，为无绝对真实值的机器学习输出评估提供了新范式。

Conclusion: GrandJury为AI从业者提供了一种更适应动态需求的评估方法，其开源实现和公开数据集展示了其可行性和必要性。

Abstract: Generative Machine Learning models have become central to modern systems,
powering applications in creative writing, summarization, multi-hop reasoning,
and context-aware dialogue. These models underpin large-scale AI assistants,
workflow automation, and autonomous decision-making. In such domains,
acceptable response is rarely absolute or static, but plural and highly
context-dependent. Yet standard evaluation regimes still rely on static,
benchmark-style tests, incentivizing optimization toward leaderboard scores
rather than alignment with dynamic user needs or evolving realities. GrandJury
introduces a formal evaluation protocol combining time-decayed aggregation,
complete traceability, with the support of dynamic, transparent task rubric
attribution, and multi-rater human judgment. Together, these elements enable
pluralistic, accountable evaluation that captures evolving consensus and
surfaces disagreement. We provide an open-source implementation (grandjury PyPI
package) and a public collection of Large Language Model (LLM) inference
outputs to illustrate the need and method. GrandJury provides a new paradigm
for AI practitioners when evaluating machine learning outputs without absolute
ground truth.

</details>


### [63] [PLoRA: Efficient LoRA Hyperparameter Tuning for Large Models](https://arxiv.org/abs/2508.02932)
*Minghao Yan,Zhuang Wang,Zhen Jia,Shivaram Venkataraman,Yida Wang*

Main category: cs.LG

TL;DR: PLoRA是一种优化LoRA微调的方法，通过自动编排并发任务和改进内核，显著提高了训练效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA训练范式未能高效利用硬件资源，且获取高性能LoRA的开销较高。

Method: 提出PLoRA，自动编排并发LoRA微调任务，并开发高效内核以提升训练效率。

Result: PLoRA将LoRA微调的时间缩短至多7.52倍，训练吞吐量提升至多12.8倍。

Conclusion: PLoRA显著提升了LoRA微调的效率和性能，适用于多种先进LLM。

Abstract: Low-rank Adaptation (LoRA) has gained popularity as a fine-tuning approach
for Large Language Models (LLMs) due to its low resource requirements and good
performance. While a plethora of work has investigated improving LoRA serving
efficiency by serving multiple LoRAs concurrently, existing methods assume that
a wide range of LoRA adapters are available for serving. In our work, we
conduct extensive empirical studies to identify that current training paradigms
do not utilize hardware resources efficiently and require high overhead to
obtain a performant LoRA. Leveraging these insights, we propose PLoRA, which
automatically orchestrates concurrent LoRA fine-tuning jobs under given
hardware and model constraints and develops performant kernels to improve
training efficiency. Our experimental studies show that PLoRA reduces the
makespan of LoRA fine-tuning over a given hyperparameter search space by up to
7.52x and improves training throughput by up to 12.8x across a range of
state-of-the-art LLMs.

</details>


### [64] [Online Robust Multi-Agent Reinforcement Learning under Model Uncertainties](https://arxiv.org/abs/2508.02948)
*Zain Ulabedeen Farhat,Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 论文提出了一种在线学习方法RONAVI，用于增强多智能体系统在不确定环境中的鲁棒性，无需依赖模拟器或离线数据集。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，多智能体系统可能因训练与部署环境不匹配而失效，需要一种无需先验数据的鲁棒学习方法。

Method: 提出RONAVI算法，通过在线学习直接优化最坏情况性能，适用于Total Variation和Kullback-Leibler差异度量的不确定性集合。

Result: 理论分析表明，RONAVI能实现低遗憾并高效找到最优鲁棒策略。

Conclusion: RONAVI为开发真正鲁棒的多智能体系统提供了新途径。

Abstract: Well-trained multi-agent systems can fail when deployed in real-world
environments due to model mismatches between the training and deployment
environments, caused by environment uncertainties including noise or
adversarial attacks. Distributionally Robust Markov Games (DRMGs) enhance
system resilience by optimizing for worst-case performance over a defined set
of environmental uncertainties. However, current methods are limited by their
dependence on simulators or large offline datasets, which are often
unavailable. This paper pioneers the study of online learning in DRMGs, where
agents learn directly from environmental interactions without prior data. We
introduce the {\it Robust Optimistic Nash Value Iteration (RONAVI)} algorithm
and provide the first provable guarantees for this setting. Our theoretical
analysis demonstrates that the algorithm achieves low regret and efficiently
finds the optimal robust policy for uncertainty sets measured by Total
Variation divergence and Kullback-Leibler divergence. These results establish a
new, practical path toward developing truly robust multi-agent systems.

</details>


### [65] [Injecting Measurement Information Yields a Fast and Noise-Robust Diffusion-Based Inverse Problem Solver](https://arxiv.org/abs/2508.02964)
*Jonathan Patsenker,Henry Li,Myeongseob Ko,Ruoxi Jia,Yuval Kluger*

Main category: cs.LG

TL;DR: 扩散模型通过Tweedie公式指导扩散轨迹，但未充分利用测量信息。本文提出估计条件后验均值，将其转化为轻量级最大似然估计问题，提升了逆问题求解的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在逆问题求解中依赖Tweedie公式，但未充分利用测量信息，导致性能受限。

Method: 提出估计条件后验均值，将其转化为单参数最大似然估计问题，并集成到标准采样器中。

Result: 在多个数据集和任务中表现优于或与现有逆问题求解器相当。

Conclusion: 通过优化条件后验均值的估计，提升了扩散模型在逆问题求解中的效率和鲁棒性。

Abstract: Diffusion models have been firmly established as principled zero-shot solvers
for linear and nonlinear inverse problems, owing to their powerful image prior
and iterative sampling algorithm. These approaches often rely on Tweedie's
formula, which relates the diffusion variate $\mathbf{x}_t$ to the posterior
mean $\mathbb{E} [\mathbf{x}_0 | \mathbf{x}_t]$, in order to guide the
diffusion trajectory with an estimate of the final denoised sample
$\mathbf{x}_0$. However, this does not consider information from the
measurement $\mathbf{y}$, which must then be integrated downstream. In this
work, we propose to estimate the conditional posterior mean $\mathbb{E}
[\mathbf{x}_0 | \mathbf{x}_t, \mathbf{y}]$, which can be formulated as the
solution to a lightweight, single-parameter maximum likelihood estimation
problem. The resulting prediction can be integrated into any standard sampler,
resulting in a fast and memory-efficient inverse solver. Our optimizer is
amenable to a noise-aware likelihood-based stopping criteria that is robust to
measurement noise in $\mathbf{y}$. We demonstrate comparable or improved
performance against a wide selection of contemporary inverse solvers across
multiple datasets and tasks.

</details>


### [66] [Scalable Varied-Density Clustering via Graph Propagation](https://arxiv.org/abs/2508.02989)
*Ninh Pham,Yingtao Zheng,Hugo Phibbs*

Main category: cs.LG

TL;DR: 提出一种基于标签传播的变密度聚类方法，适用于高维数据，通过图连接性和密度感知传播算法实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据中变密度聚类的挑战，结合密度与图连接性以提高效率和可扩展性。

Method: 使用密度感知邻域传播算法和随机投影方法构建近似邻域图，利用图传播技术进行聚类。

Result: 方法在计算成本显著降低的同时保持聚类质量，可处理百万级数据并在几分钟内完成。

Conclusion: 该方法在效率和准确性上优于现有基线，适用于大规模高维数据聚类。

Abstract: We propose a novel perspective on varied-density clustering for
high-dimensional data by framing it as a label propagation process in
neighborhood graphs that adapt to local density variations. Our method formally
connects density-based clustering with graph connectivity, enabling the use of
efficient graph propagation techniques developed in network science. To ensure
scalability, we introduce a density-aware neighborhood propagation algorithm
and leverage advanced random projection methods to construct approximate
neighborhood graphs. Our approach significantly reduces computational cost
while preserving clustering quality. Empirically, it scales to datasets with
millions of points in minutes and achieves competitive accuracy compared to
existing baselines.

</details>


### [67] [On the Fast Adaptation of Delayed Clients in Decentralized Federated Learning: A Centroid-Aligned Distillation Approach](https://arxiv.org/abs/2508.02993)
*Jiahui Bai,Hai Dong,A. K. Qin*

Main category: cs.LG

TL;DR: DFedCAD通过Centroid-Aligned Distillation和Weighted Cluster Pruning解决了DFL中延迟客户端适应慢和通信成本高的问题，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化联邦学习（DFL）中延迟客户端适应慢和异步环境下高通信成本的问题。

Method: 采用Weighted Cluster Pruning（WCP）压缩模型为代表性中心点，结合结构距离度量与可微分k-means蒸馏模块，实现高效知识转移。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet上表现优异，通信开销降低86%以上，达到最高准确率。

Conclusion: DFedCAD为动态现实场景提供了高效、可扩展的去中心化学习解决方案。

Abstract: Decentralized Federated Learning (DFL) struggles with the slow adaptation of
late-joining delayed clients and high communication costs in asynchronous
environments. These limitations significantly hinder overall performance. To
address this, we propose DFedCAD, a novel framework for rapid adaptation via
Centroid-Aligned Distillation. DFedCAD first employs Weighted Cluster Pruning
(WCP) to compress models into representative centroids, drastically reducing
communication overhead. It then enables delayed clients to intelligently weigh
and align with peer knowledge using a novel structural distance metric and a
differentiable k-means distillation module, facilitating efficient end-to-end
knowledge transfer. Extensive experiments on CIFAR-10, CIFAR-100, and
Tiny-ImageNet show that DFedCAD consistently achieves state-of-the-art
performance, attaining the highest accuracy across all evaluated settings while
reducing communication overhead by over 86%. Our framework provides a scalable
and practical solution for efficient decentralized learning in dynamic,
real-world scenarios.

</details>


### [68] [Where and How to Enhance: Discovering Bit-Width Contribution for Mixed Precision Quantization](https://arxiv.org/abs/2508.03002)
*Haidong Kang,Lianbo Ma,Guo Yu,Shangce Gao*

Main category: cs.LG

TL;DR: 论文提出了一种基于Shapley值的混合精度量化方法（SMPQ），通过直接测量比特宽度对任务性能的贡献，改进了传统梯度下降方法（DMPQ）的不足。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化（MPQ）方法通过梯度下降优化比特宽度分配，但量化参数的大小未必反映比特宽度对任务性能的实际贡献。

Method: 提出SMPQ方法，利用Shapley值直接测量比特宽度的贡献，并采用蒙特卡洛采样降低计算成本。

Result: 在主流基准测试中，SMPQ表现优于基于梯度的方法。

Conclusion: SMPQ通过更准确的比特宽度贡献评估，实现了更优的性能。

Abstract: Mixed precision quantization (MPQ) is an effective quantization approach to
achieve accuracy-complexity trade-off of neural network, through assigning
different bit-widths to network activations and weights in each layer. The
typical way of existing MPQ methods is to optimize quantization policies (i.e.,
bit-width allocation) in a gradient descent manner, termed as Differentiable
(DMPQ). At the end of the search, the bit-width associated to the quantization
parameters which has the largest value will be selected to form the final mixed
precision quantization policy, with the implicit assumption that the values of
quantization parameters reflect the operation contribution to the accuracy
improvement. While much has been discussed about the MPQ improvement, the
bit-width selection process has received little attention. We study this
problem and argue that the magnitude of quantization parameters does not
necessarily reflect the actual contribution of the bit-width to the task
performance. Then, we propose a Shapley-based MPQ (SMPQ) method, which measures
the bit-width operation direct contribution on the MPQ task. To reduce
computation cost, a Monte Carlo sampling-based approximation strategy is
proposed for Shapley computation. Extensive experiments on mainstream
benchmarks demonstrate that our SMPQ consistently achieves state-of-the-art
performance than gradient-based competitors.

</details>


### [69] [Urban In-Context Learning: Bridging Pretraining and Inference through Masked Diffusion for Urban Profiling](https://arxiv.org/abs/2508.03042)
*Ruixing Zhang,Bo Wang,Tongyu Zhu,Leilei Sun,Weifeng Lv*

Main category: cs.LG

TL;DR: 提出了一种名为Urban In-Context Learning的单阶段框架，通过掩码自编码过程统一预训练和推理，解决了传统两阶段方法的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常采用两阶段范式，但受GPT风格模型启发，希望设计一种直接适用于下游任务的一阶段模型。

Method: 提出Urban Masked Diffusion Transformer和Urban Representation Alignment Mechanism，通过扩散模型和特征对齐实现。

Result: 在两个城市的三个指标上，该方法一致优于现有两阶段方法。

Conclusion: 该框架有效统一了预训练和推理，扩散模型的使用尤为关键。

Abstract: Urban profiling aims to predict urban profiles in unknown regions and plays a
critical role in economic and social censuses. Existing approaches typically
follow a two-stage paradigm: first, learning representations of urban areas;
second, performing downstream prediction via linear probing, which originates
from the BERT era. Inspired by the development of GPT style models, recent
studies have shown that novel self-supervised pretraining schemes can endow
models with direct applicability to downstream tasks, thereby eliminating the
need for task-specific fine-tuning. This is largely because GPT unifies the
form of pretraining and inference through next-token prediction. However, urban
data exhibit structural characteristics that differ fundamentally from
language, making it challenging to design a one-stage model that unifies both
pretraining and inference. In this work, we propose Urban In-Context Learning,
a framework that unifies pretraining and inference via a masked autoencoding
process over urban regions. To capture the distribution of urban profiles, we
introduce the Urban Masked Diffusion Transformer, which enables each region' s
prediction to be represented as a distribution rather than a deterministic
value. Furthermore, to stabilize diffusion training, we propose the Urban
Representation Alignment Mechanism, which regularizes the model's intermediate
features by aligning them with those from classical urban profiling methods.
Extensive experiments on three indicators across two cities demonstrate that
our one-stage method consistently outperforms state-of-the-art two-stage
approaches. Ablation studies and case studies further validate the
effectiveness of each proposed module, particularly the use of diffusion
modeling.

</details>


### [70] [A Novel Multimodal Framework for Early Detection of Alzheimers Disease Using Deep Learning](https://arxiv.org/abs/2508.03046)
*Tatwadarshi P Nagarhalli,Sanket Patil,Vishal Pande,Uday Aswalekar,Prafulla Patil*

Main category: cs.LG

TL;DR: 提出了一种多模态框架，结合MRI、认知评估和生物标志物数据，用于阿尔茨海默病的早期检测，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统单模态诊断方法无法全面捕捉阿尔茨海默病的复杂性，导致早期诊断困难。

Method: 使用CNN分析MRI图像，LSTM处理认知和生物标志物数据，并通过加权平均整合多模态结果。

Result: 多模态方法提高了诊断的鲁棒性，能够在疾病早期阶段识别阿尔茨海默病。

Conclusion: 该框架有望革新阿尔茨海默病的早期检测，为更及时有效的治疗铺平道路。

Abstract: Alzheimers Disease (AD) is a progressive neurodegenerative disorder that
poses significant challenges in its early diagnosis, often leading to delayed
treatment and poorer outcomes for patients. Traditional diagnostic methods,
typically reliant on single data modalities, fall short of capturing the
multifaceted nature of the disease. In this paper, we propose a novel
multimodal framework for the early detection of AD that integrates data from
three primary sources: MRI imaging, cognitive assessments, and biomarkers. This
framework employs Convolutional Neural Networks (CNN) for analyzing MRI images
and Long Short-Term Memory (LSTM) networks for processing cognitive and
biomarker data. The system enhances diagnostic accuracy and reliability by
aggregating results from these distinct modalities using advanced techniques
like weighted averaging, even in incomplete data. The multimodal approach not
only improves the robustness of the detection process but also enables the
identification of AD at its earliest stages, offering a significant advantage
over conventional methods. The integration of biomarkers and cognitive tests is
particularly crucial, as these can detect Alzheimer's long before the onset of
clinical symptoms, thereby facilitating earlier intervention and potentially
altering the course of the disease. This research demonstrates that the
proposed framework has the potential to revolutionize the early detection of
AD, paving the way for more timely and effective treatments

</details>


### [71] [VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision](https://arxiv.org/abs/2508.03058)
*Dingwei Zhu,Shihan Dou,Zhiheng Xi,Senjie Jin,Guoqiang Zhang,Jiazheng Zhang,Junjie Ye,Mingxu Chai,Enyu Zhou,Ming Zhang,Caishuang Huang,Yunke Zhang,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: VRPO提出了一种基于价值模型的鲁棒PPO训练框架，通过熵和困惑度辅助损失以及变分信息瓶颈，有效减少噪声干扰，提升策略优化效果。


<details>
  <summary>Details</summary>
Motivation: RLHF在现实场景中常受噪声奖励监督影响，导致策略不稳定和泛化能力下降。现有方法多关注奖励去噪或数据过滤，但忽略了价值模型的关键作用。

Method: VRPO结合了两种核心设计：1) 基于冻结语言模型的熵和困惑度辅助损失；2) 变分信息瓶颈。这些机制增强了价值模型在优势估计中过滤噪声和捕捉关键信息的能力。

Result: 在数学推理、科学问答和多轮对话任务中，VRPO在规则和模型噪声奖励下均优于PPO和GRPO基线。

Conclusion: 研究强调了价值模型在RLHF中的重要性，并提出了一种在噪声环境中实现鲁棒策略优化的实用方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or
imperfect reward supervision in real-world settings, which undermines policy
stability and generalization. Such noise may cause models to lose attention on
key words during advantage estimation. While prior work focuses on reward
denoising or filtering poor data, it often overlooks the critical role of the
value model in policy optimization. In this work, we show that a strong value
model is essential for mitigating noise by absorbing unstable signals and
enabling more reliable advantage estimation. We propose VRPO, a value-centric
framework for robust PPO training under noisy supervision. VRPO combines two
core designs: (1) an auxiliary loss guided by entropy and perplexity from a
frozen language model, and (2) a variational information bottleneck. These
mechanisms enhance the value model's ability to filter out noise and capture
key words from the context during advantage estimation, transforming it from a
passive predictor into an active regulator of noise. Experiments on math
reasoning, science QA, and multi-turn dialogue, under both rule-based and
model-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO
baselines. Our findings underscore the often-overlooked importance of the value
model in RLHF and offer a principled and practical approach to robust policy
optimization in noisy real-world environments.

</details>


### [72] [HiTeC: Hierarchical Contrastive Learning on Text-Attributed Hypergraph with Semantic-Aware Augmentation](https://arxiv.org/abs/2508.03104)
*Mengting Pan,Fan Li,Xiaoyang Wang,Wenjie Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: HiTeC提出了一种两阶段层次对比学习框架，用于文本属性超图的自监督学习，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在文本属性超图上存在三个主要问题：忽略文本与超图拓扑的关联、随机数据增强引入噪声、以及无法捕捉长程依赖关系。HiTeC旨在解决这些问题。

Method: HiTeC采用两阶段框架：第一阶段预训练文本编码器，第二阶段引入语义感知增强策略和多尺度对比损失。

Result: 实验证明HiTeC在文本属性超图上表现优于现有方法。

Conclusion: HiTeC通过两阶段设计和语义感知增强，显著提升了自监督学习的效果和可扩展性。

Abstract: Contrastive learning (CL) has become a dominant paradigm for self-supervised
hypergraph learning, enabling effective training without costly labels.
However, node entities in real-world hypergraphs are often associated with rich
textual information, which is overlooked in prior works. Directly applying
existing CL-based methods to such text-attributed hypergraphs (TAHGs) leads to
three key limitations: (1) The common use of graph-agnostic text encoders
overlooks the correlations between textual content and hypergraph topology,
resulting in suboptimal representations. (2) Their reliance on random data
augmentations introduces noise and weakens the contrastive objective. (3) The
primary focus on node- and hyperedge-level contrastive signals limits the
ability to capture long-range dependencies, which is essential for expressive
representation learning. Although HyperBERT pioneers CL on TAHGs, its
co-training paradigm suffers from poor scalability. To fill the research gap,
we introduce HiTeC, a two-stage hierarchical contrastive learning framework
with semantic-aware augmentation for scalable and effective self-supervised
learning on TAHGs. In the first stage, we pre-train the text encoder with a
structure-aware contrastive objective to overcome the graph-agnostic nature of
conventional methods. In the second stage, we introduce two semantic-aware
augmentation strategies, including prompt-enhanced text augmentation and
semantic-aware hyperedge drop, to facilitate informative view generation.
Furthermore, we propose a multi-scale contrastive loss that extends existing
objectives with an $s$-walk-based subgraph-level contrast to better capture
long-range dependencies. By decoupling text encoder pretraining from hypergraph
contrastive learning, this two-stage design enhances scalability without
compromising representation quality. Extensive experiments confirm the
effectiveness of HiTeC.

</details>


### [73] [Accelerating SGDM via Learning Rate and Batch Size Schedules: A Lyapunov-Based Analysis](https://arxiv.org/abs/2508.03105)
*Yuichi Kondo,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 论文通过引入新的Lyapunov函数，分析了动态学习率和批量大小调度下带动量的随机梯度下降（SGDM）的收敛行为，并揭示了不同调度策略的收敛性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究SGDM在动态学习率和批量大小调度下的收敛行为，为深度学习训练提供理论支持和实践指导。

Method: 引入结构更简单的Lyapunov函数，扩展理论框架以涵盖三种常见调度策略，并进行实验验证。

Result: 动态调度的SGDM在收敛速度上显著优于固定超参数基线，其中增加批量大小和学习率的策略表现最佳。

Conclusion: 研究为现代深度学习设计高效稳定的训练程序提供了统一的理论基础和实用建议。

Abstract: We analyze the convergence behavior of stochastic gradient descent with
momentum (SGDM) under dynamic learning rate and batch size schedules by
introducing a novel Lyapunov function. This Lyapunov function has a simpler
structure compared with existing ones, facilitating the challenging convergence
analysis of SGDM and a unified analysis across various dynamic schedules.
Specifically, we extend the theoretical framework to cover three practical
scheduling strategies commonly used in deep learning: (i) constant batch size
with a decaying learning rate, (ii) increasing batch size with a decaying
learning rate, and (iii) increasing batch size with an increasing learning
rate. Our theoretical results reveal a clear hierarchy in convergence behavior:
while (i) does not guarantee convergence of the expected gradient norm, both
(ii) and (iii) do. Moreover, (iii) achieves a provably faster decay rate than
(i) and (ii), demonstrating theoretical acceleration even in the presence of
momentum. Empirical results validate our theory, showing that dynamically
scheduled SGDM significantly outperforms fixed-hyperparameter baselines in
convergence speed. We also evaluated a warm-up schedule in experiments, which
empirically outperformed all other strategies in convergence behavior. These
findings provide a unified theoretical foundation and practical guidance for
designing efficient and stable training procedures in modern deep learning.

</details>


### [74] [Pseudo-label Induced Subspace Representation Learning for Robust Out-of-Distribution Detection](https://arxiv.org/abs/2508.03108)
*Tarhib Al Azad,Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种基于伪标签诱导子空间表示的新型OOD检测框架，通过更宽松的假设和有效的学习准则提升ID-OOD分离性。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法对特征空间的假设过于严格，限制了ID与OOD样本的可分离性。

Method: 采用伪标签诱导子空间表示，结合交叉熵ID分类损失和子空间距离正则化损失。

Result: 大量实验验证了该框架的有效性。

Conclusion: 新框架在更自然的假设下显著提升了OOD检测性能。

Abstract: Out-of-distribution (OOD) detection lies at the heart of robust artificial
intelligence (AI), aiming to identify samples from novel distributions beyond
the training set. Recent approaches have exploited feature representations as
distinguishing signatures for OOD detection. However, most existing methods
rely on restrictive assumptions on the feature space that limit the
separability between in-distribution (ID) and OOD samples. In this work, we
propose a novel OOD detection framework based on a pseudo-label-induced
subspace representation, that works under more relaxed and natural assumptions
compared to existing feature-based techniques. In addition, we introduce a
simple yet effective learning criterion that integrates a cross-entropy-based
ID classification loss with a subspace distance-based regularization loss to
enhance ID-OOD separability. Extensive experiments validate the effectiveness
of our framework.

</details>


### [75] [GEDAN: Learning the Edit Costs for Graph Edit Distance](https://arxiv.org/abs/2508.03111)
*Francesco Leonardi,Markus Orsi,Jean-Louis Reymond,Kaspar Riesen*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的框架，用于近似计算图编辑距离（GED），支持有监督和无监督训练，提高了适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 图编辑距离（GED）是衡量图之间差异的常用指标，但其计算是NP难的。现有神经网络方法通常假设单位成本操作，不适用于实际应用。

Method: 采用图神经网络框架，结合广义可加模型，灵活学习上下文感知的编辑成本，支持无监督训练。

Result: 实验表明，该方法性能与现有最优方法相当，但显著提升了适应性和可解释性。

Conclusion: 该方法在分子分析和结构模式发现等领域具有重要价值，因其能提供对复杂图结构的深入理解。

Abstract: Graph Edit Distance (GED) is defined as the minimum cost transformation of
one graph into another and is a widely adopted metric for measuring the
dissimilarity between graphs. The major problem of GED is that its computation
is NP-hard, which has in turn led to the development of various approximation
methods, including approaches based on neural networks (NN). Most of these
NN-based models simplify the problem of GED by assuming unit-cost edit
operations, a rather unrealistic constraint in real-world applications. In this
work, we present a novel Graph Neural Network framework that approximates GED
using both supervised and unsupervised training. In the unsupervised setting,
it employs a gradient-only self-organizing mechanism that enables optimization
without ground-truth distances. Moreover, a core component of our architecture
is the integration of a Generalized Additive Model, which allows the flexible
and interpretable learning of context-aware edit costs. Experimental results
show that the proposed method achieves similar results as state-of-the-art
reference methods, yet significantly improves both adaptability and
interpretability. That is, the learned cost function offers insights into
complex graph structures, making it particularly valuable in domains such as
molecular analysis and structural pattern discovery.

</details>


### [76] [RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging](https://arxiv.org/abs/2508.03121)
*The-Hai Nguyen,Dang Huu-Tien,Takeshi Suzuki,Le-Minh Nguyen*

Main category: cs.LG

TL;DR: RegMean++改进RegMean，通过显式考虑层内和跨层依赖关系，提升模型合并性能。


<details>
  <summary>Details</summary>
Motivation: RegMean独立合并线性层，忽略了层间信息传播对最终预测的影响。

Method: RegMean++在RegMean目标中引入层内和跨层依赖关系。

Result: RegMean++在多种设置下优于RegMean，并在多项任务中达到先进水平。

Conclusion: RegMean++是一种简单有效的模型合并方法，性能优于RegMean。

Abstract: Regression Mean (RegMean), an approach that formulates model merging as a
linear regression problem, aims to find the optimal weights for each linear
layer in the merge model by minimizing the discrepancy in predictions between
the merge and candidate models. RegMean provides a precise closed-form solution
for the merging problem; therefore, it offers explainability and computational
efficiency. However, RegMean merges each linear layer independently,
overlooking how the features and information in the earlier layers propagate
through the layers and influence the final prediction in the merge model. In
this paper, we introduce RegMean++, a simple yet effective alternative to
RegMean, that explicitly incorporates both intra- and cross-layer dependencies
between merge models' layers into RegMean's objective. By accounting for these
dependencies, RegMean++ better captures the behaviors of the merge model.
Extensive experiments demonstrate that RegMean++ consistently outperforms
RegMean across diverse settings, including in-domain (ID) and out-of-domain
(OOD) generalization, sequential merging, large-scale tasks, and robustness
under several types of distribution shifts. Furthermore, RegMean++ achieves
competitive or state-of-the-art performance compared to various recent advanced
model merging methods. Our code is available at
https://github.com/nthehai01/RegMean-plusplus.

</details>


### [77] [Frontier: Simulating the Next Generation of LLM Inference Systems](https://arxiv.org/abs/2508.03148)
*Yicheng Feng,Xin Tan,Kin Hang Sew,Yimin Jiang,Yibo Zhu,Hong Xu*

Main category: cs.LG

TL;DR: Frontier是一个专为复杂LLM推理设计的高保真模拟器，支持MoE和分解架构。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器无法捕捉新兴MoE和分解架构的动态，需新工具支持。

Method: Frontier提供统一框架，支持MoE推理和专家并行，模拟复杂工作流。

Result: Frontier提高了模拟精度，支持未来LLM推理的设计与优化。

Conclusion: Frontier为大规模LLM推理提供了高效工具，推动社区发展。

Abstract: Large Language Model (LLM) inference is growing increasingly complex with the
rise of Mixture-of-Experts (MoE) models and disaggregated architectures that
decouple components like prefill/decode (PD) or attention/FFN (AF) for
heterogeneous scaling. Existing simulators, architected for co-located, dense
models, are unable to capture the intricate system dynamics of these emerging
paradigms. We present Frontier, a high-fidelity simulator designed from the
ground up for this new landscape. Frontier introduces a unified framework to
model both co-located and disaggregated systems, providing native support for
MoE inference with expert parallelism (EP). It enables the simulation of
complex workflows like cross-cluster expert routing and advanced pipelining
strategies for latency hiding. To ensure fidelity and usability, Frontier
incorporates refined operator models for improved accuracy. Frontier empowers
the community to design and optimize the future of LLM inference at scale.

</details>


### [78] [Estimating Worst-Case Frontier Risks of Open-Weight LLMs](https://arxiv.org/abs/2508.03153)
*Eric Wallace,Olivia Watkins,Miles Wang,Kai Chen,Chris Koch*

Main category: cs.LG

TL;DR: 研究了通过恶意微调（MFT）释放GPT-OSS的最坏情况前沿风险，发现其性能低于封闭权重模型，对开放权重模型影响有限。


<details>
  <summary>Details</summary>
Motivation: 评估开放权重模型（如GPT-OSS）在生物和网络安全领域的潜在风险，为未来模型发布提供风险评估指导。

Method: 通过恶意微调（MFT）在生物和网络安全领域最大化GPT-OSS的能力，并在RL环境和代理编码环境中训练。

Result: MFT GPT-OSS在生物和网络安全领域的性能低于封闭权重模型（如OpenAI o3），对开放权重模型的影响有限。

Conclusion: 研究支持了模型发布的决策，MFT方法可作为未来开放权重模型风险评估的参考。

Abstract: In this paper, we study the worst-case frontier risks of releasing gpt-oss.
We introduce malicious fine-tuning (MFT), where we attempt to elicit maximum
capabilities by fine-tuning gpt-oss to be as capable as possible in two
domains: biology and cybersecurity. To maximize biological risk (biorisk), we
curate tasks related to threat creation and train gpt-oss in an RL environment
with web browsing. To maximize cybersecurity risk, we train gpt-oss in an
agentic coding environment to solve capture-the-flag (CTF) challenges. We
compare these MFT models against open- and closed-weight LLMs on frontier risk
evaluations. Compared to frontier closed-weight models, MFT gpt-oss
underperforms OpenAI o3, a model that is below Preparedness High capability
level for biorisk and cybersecurity. Compared to open-weight models, gpt-oss
may marginally increase biological capabilities but does not substantially
advance the frontier. Taken together, these results contributed to our decision
to release the model, and we hope that our MFT approach can serve as useful
guidance for estimating harm from future open-weight releases.

</details>


### [79] [Unveiling Location-Specific Price Drivers: A Two-Stage Cluster Analysis for Interpretable House Price Predictions](https://arxiv.org/abs/2508.03156)
*Paul Gümmer,Julian Rosenberger,Mathias Kraus,Patrick Zschech,Nico Hambauer*

Main category: cs.LG

TL;DR: 提出了一种两阶段聚类方法，结合线性回归或广义加性模型，提升房价估值的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决房价估值中因市场异质性导致的模型不准确或缺乏可解释性的问题。

Method: 使用两阶段聚类，先基于位置特征分组，再为每个聚类分别建模（LR或GAM）。

Result: 在德国43,309条房产数据上，GAM和LR的MAE分别提升36%和58%，并揭示了聚类间的模式差异。

Conclusion: 聚类特异性分析增强了模型的可解释性和实用性，为房产市场参与者提供了更可靠的估值工具。

Abstract: House price valuation remains challenging due to localized market variations.
Existing approaches often rely on black-box machine learning models, which lack
interpretability, or simplistic methods like linear regression (LR), which fail
to capture market heterogeneity. To address this, we propose a machine learning
approach that applies two-stage clustering, first grouping properties based on
minimal location-based features before incorporating additional features. Each
cluster is then modeled using either LR or a generalized additive model (GAM),
balancing predictive performance with interpretability. Constructing and
evaluating our models on 43,309 German house property listings from 2023, we
achieve a 36% improvement for the GAM and 58% for LR in mean absolute error
compared to models without clustering. Additionally, graphical analyses unveil
pattern shifts between clusters. These findings emphasize the importance of
cluster-specific insights, enhancing interpretability and offering practical
value for buyers, sellers, and real estate analysts seeking more reliable
property valuations.

</details>


### [80] [Rethinking Selectivity in State Space Models: A Minimal Predictive Sufficiency Approach](https://arxiv.org/abs/2508.03158)
*Yiyi Wang,Jian'an Zhang,Hongyi Duan,Haoyang Liu,Qingyang Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于预测充分性原则的新状态空间模型（MPS-SSM），通过信息论准则优化选择性机制，显著提升了序列建模的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有选择性状态空间模型（如Mamba）依赖启发式设计，缺乏理论依据，可能影响其最优性和鲁棒性。

Method: 引入预测充分性原则，提出MPS-SSM框架，通过优化目标函数指导选择性机制，压缩历史信息并保留预测能力。

Result: MPS-SSM在多个基准数据集上表现优异，尤其在长期预测和噪声场景中显著优于现有模型，且鲁棒性更强。

Conclusion: MPS-SSM不仅性能卓越，其原则还可扩展为通用正则化框架，具有广泛的应用潜力。

Abstract: State Space Models (SSMs), particularly recent selective variants like Mamba,
have emerged as a leading architecture for sequence modeling, challenging the
dominance of Transformers. However, the success of these state-of-the-art
models largely relies on heuristically designed selective mechanisms, which
lack a rigorous first-principle derivation. This theoretical gap raises
questions about their optimality and robustness against spurious correlations.
To address this, we introduce the Principle of Predictive Sufficiency, a novel
information-theoretic criterion stipulating that an ideal hidden state should
be a minimal sufficient statistic of the past for predicting the future. Based
on this principle, we propose the Minimal Predictive Sufficiency State Space
Model (MPS-SSM), a new framework where the selective mechanism is guided by
optimizing an objective function derived from our principle. This approach
encourages the model to maximally compress historical information without
losing predictive power, thereby learning to ignore non-causal noise and
spurious patterns. Extensive experiments on a wide range of benchmark datasets
demonstrate that MPS-SSM not only achieves state-of-the-art performance,
significantly outperforming existing models in long-term forecasting and noisy
scenarios, but also exhibits superior robustness. Furthermore, we show that the
MPS principle can be extended as a general regularization framework to enhance
other popular architectures, highlighting its broad potential.

</details>


### [81] [CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction](https://arxiv.org/abs/2508.03159)
*Jueon Park,Yein Park,Minju Song,Soyon Park,Donghyeon Lee,Seungheun Baek,Jaewoo Kang*

Main category: cs.LG

TL;DR: CoTox是一个结合大型语言模型（LLM）和链式思维（CoT）推理的新框架，用于多毒性预测，通过整合化学结构、生物通路和基因本体（GO）术语，生成可解释的毒性预测。


<details>
  <summary>Details</summary>
Motivation: 药物毒性是药物开发中的主要挑战，现有机器学习模型依赖标注数据且缺乏可解释性，难以捕捉器官特异性毒性。

Method: CoTox结合化学结构数据、生物通路和GO术语，利用GPT-4o进行逐步推理，生成毒性预测。

Result: CoTox在性能上优于传统机器学习和深度学习模型，且通过IUPAC命名化学结构进一步提升了推理能力。

Conclusion: CoTox展示了LLM框架在提高可解释性和支持早期药物安全性评估方面的潜力。

Abstract: Drug toxicity remains a major challenge in pharmaceutical development. Recent
machine learning models have improved in silico toxicity prediction, but their
reliance on annotated data and lack of interpretability limit their
applicability. This limits their ability to capture organ-specific toxicities
driven by complex biological mechanisms. Large language models (LLMs) offer a
promising alternative through step-by-step reasoning and integration of textual
data, yet prior approaches lack biological context and transparent rationale.
To address this issue, we propose CoTox, a novel framework that integrates LLM
with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox
combines chemical structure data, biological pathways, and gene ontology (GO)
terms to generate interpretable toxicity predictions through step-by-step
reasoning. Using GPT-4o, we show that CoTox outperforms both traditional
machine learning and deep learning model. We further examine its performance
across various LLMs to identify where CoTox is most effective. Additionally, we
find that representing chemical structures with IUPAC names, which are easier
for LLMs to understand than SMILES, enhances the model's reasoning ability and
improves predictive performance. To demonstrate its practical utility in drug
development, we simulate the treatment of relevant cell types with drug and
incorporated the resulting biological context into the CoTox framework. This
approach allow CoTox to generate toxicity predictions aligned with
physiological responses, as shown in case study. This result highlights the
potential of LLM-based frameworks to improve interpretability and support
early-stage drug safety assessment. The code and prompt used in this work are
available at https://github.com/dmis-lab/CoTox.

</details>


### [82] [Overcoming Algorithm Aversion with Transparency: Can Transparent Predictions Change User Behavior?](https://arxiv.org/abs/2508.03168)
*Lasse Bohlen,Sven Kruschel,Julian Rosenberger,Patrick Zschech,Mathias Kraus*

Main category: cs.LG

TL;DR: 研究探讨了可解释性和可调整性对减少算法厌恶的影响，发现可调整性显著有效，而可解释性效果较小且不显著。


<details>
  <summary>Details</summary>
Motivation: 探索可解释的机器学习模型是否能进一步减少算法厌恶，或使可调整性变得多余。

Method: 通过预注册用户研究（280名参与者），比较可调整性和可解释性对算法厌恶的影响。

Result: 可调整性显著减少算法厌恶，可解释性效果较小且不显著。两者影响相对独立。

Conclusion: 可调整性是减少算法厌恶的有效方法，而可解释性的作用有限。

Abstract: Previous work has shown that allowing users to adjust a machine learning (ML)
model's predictions can reduce aversion to imperfect algorithmic decisions.
However, these results were obtained in situations where users had no
information about the model's reasoning. Thus, it remains unclear whether
interpretable ML models could further reduce algorithm aversion or even render
adjustability obsolete. In this paper, we conceptually replicate a well-known
study that examines the effect of adjustable predictions on algorithm aversion
and extend it by introducing an interpretable ML model that visually reveals
its decision logic. Through a pre-registered user study with 280 participants,
we investigate how transparency interacts with adjustability in reducing
aversion to algorithmic decision-making. Our results replicate the
adjustability effect, showing that allowing users to modify algorithmic
predictions mitigates aversion. Transparency's impact appears smaller than
expected and was not significant for our sample. Furthermore, the effects of
transparency and adjustability appear to be more independent than expected.

</details>


### [83] [Quantum Spectral Reasoning: A Non-Neural Architecture for Interpretable Machine Learning](https://arxiv.org/abs/2508.03170)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出了一种基于量子谱方法的新型机器学习架构，通过Pade近似和Lanczos算法实现可解释的信号分析和符号推理。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络缺乏可解释性和物理意义，需要数据密集型黑盒模型。本文旨在提供一种透明且物理基础明确的替代方案。

Method: 利用有理谱近似将时域信号转换为稀疏的谱表示，并通过核投影函数映射为符号谓词，结合基于规则的推理引擎进行逻辑推断。

Result: 在时间序列异常检测、符号分类和混合推理任务中表现出竞争力，同时保持可解释性和数据效率。

Conclusion: 该架构为物理信息丰富且具备推理能力的机器学习提供了新方向。

Abstract: We propose a novel machine learning architecture that departs from
conventional neural network paradigms by leveraging quantum spectral methods,
specifically Pade approximants and the Lanczos algorithm, for interpretable
signal analysis and symbolic reasoning. The core innovation of our approach
lies in its ability to transform raw time-domain signals into sparse,
physically meaningful spectral representations without the use of
backpropagation, high-dimensional embeddings, or data-intensive black-box
models. Through rational spectral approximation, the system extracts resonant
structures that are then mapped into symbolic predicates via a kernel
projection function, enabling logical inference through a rule-based reasoning
engine. This architecture bridges mathematical physics, sparse approximation
theory, and symbolic artificial intelligence, offering a transparent and
physically grounded alternative to deep learning models. We develop the full
mathematical formalism underlying each stage of the pipeline, provide a modular
algorithmic implementation, and demonstrate the system's effectiveness through
comparative evaluations on time-series anomaly detection, symbolic
classification, and hybrid reasoning tasks. Our results show that this
spectral-symbolic architecture achieves competitive accuracy while maintaining
interpretability and data efficiency, suggesting a promising new direction for
physically-informed, reasoning-capable machine learning.

</details>


### [84] [Adaptive Sparse Softmax: An Effective and Efficient Softmax Variant](https://arxiv.org/abs/2508.03175)
*Qi Lv,Lei Geng,Ziqiang Cao,Min Cao,Sujian Li,Wenjie Li,Guohong Fu*

Main category: cs.LG

TL;DR: 论文提出了一种自适应稀疏Softmax（AS-Softmax），通过优化训练目标和梯度累积策略，解决了传统Softmax在分类任务中的过拟合和效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统Softmax在分类任务中存在目标分数无法达到1的问题，导致训练过程持续且容易过拟合，同时浪费资源处理已正确分类的样本。

Method: 提出了AS-Softmax，通过丢弃低分数类别和自适应梯度累积策略，使模型专注于区分目标类别与其强对手。

Result: AS-Softmax在多种分类任务中表现优于传统Softmax及其变体，且训练速度提升约1.2倍。

Conclusion: AS-Softmax是一种高效且性能优越的分类方法，显著提升了训练效率和分类性能。

Abstract: Softmax with the cross entropy loss is the standard configuration for current
neural classification models. The gold score for a target class is supposed to
be 1, but it is never reachable under the softmax schema. Such a problem makes
the training process continue forever and leads to overfitting. Moreover, the
"target-approach-1" training goal forces the model to continuously learn all
samples, leading to a waste of time in handling some samples which have already
been classified correctly with high confidence, while the test goal simply
requires the target class of each sample to hold the maximum score. To solve
the above weaknesses, we propose the Adaptive Sparse softmax (AS-Softmax) which
designs a reasonable and test-matching transformation on top of softmax. For
more purposeful learning, we discard the classes with far smaller scores
compared with the actual class during training. Then the model could focus on
learning to distinguish the target class from its strong opponents, which is
also the great challenge in test. In addition, since the training losses of
easy samples will gradually drop to 0 in AS-Softmax, we develop an adaptive
gradient accumulation strategy based on the masked sample ratio to speed up
training. We verify the proposed AS-Softmax on a variety of text multi-class,
text multi-label, text token classification, image classification and audio
classification tasks with class sizes ranging from 5 to 5000+. The results show
that AS-Softmax consistently outperforms softmax and its variants, and the loss
of AS-Softmax is remarkably correlated with classification performance in
validation. Furthermore, adaptive gradient accumulation strategy can bring
about 1.2x training speedup comparing with the standard softmax while
maintaining classification effectiveness.

</details>


### [85] [Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies](https://arxiv.org/abs/2508.03194)
*Yi Ma,Hongyao Tang,Chenjun Xiao,Yaodong Yang,Wei Wei,Jianye Hao,Jiye Liang*

Main category: cs.LG

TL;DR: 本文综述了深度强化学习（DRL）中扩展法则的应用，探讨了数据、网络和训练预算三个维度的扩展策略，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管扩展法则在计算机视觉和自然语言处理中取得了显著进展，但在DRL中的应用尚未充分探索，本文旨在填补这一空白。

Method: 系统分析了数据扩展（如并行采样和数据生成）、网络扩展（如架构增强和代理数量扩展）及训练预算扩展（如分布式训练和大批量训练）的策略。

Result: 综述揭示了这些策略在提升DRL性能中的协同作用，并提出了平衡可扩展性和计算效率的重要性。

Conclusion: 本文为未来研究提供了路线图，强调了扩展法则在机器人控制、自动驾驶和LLM训练等任务中的潜力。

Abstract: In recent years, the expansion of neural network models and training data has
driven remarkable progress in deep learning, particularly in computer vision
and natural language processing. This advancement is underpinned by the concept
of Scaling Laws, which demonstrates that scaling model parameters and training
data enhances learning performance. While these fields have witnessed
breakthroughs, such as the development of large language models like GPT-4 and
advanced vision models like Midjourney, the application of scaling laws in deep
reinforcement learning (DRL) remains relatively unexplored. Despite its
potential to improve performance, the integration of scaling laws into DRL for
decision making has not been fully realized. This review addresses this gap by
systematically analyzing scaling strategies in three dimensions: data, network,
and training budget. In data scaling, we explore methods to optimize data
efficiency through parallel sampling and data generation, examining the
relationship between data volume and learning outcomes. For network scaling, we
investigate architectural enhancements, including monolithic expansions,
ensemble and MoE methods, and agent number scaling techniques, which
collectively enhance model expressivity while posing unique computational
challenges. Lastly, in training budget scaling, we evaluate the impact of
distributed training, high replay ratios, large batch sizes, and auxiliary
training on training efficiency and convergence. By synthesizing these
strategies, this review not only highlights their synergistic roles in
advancing DRL for decision making but also provides a roadmap for future
research. We emphasize the importance of balancing scalability with
computational efficiency and outline promising directions for leveraging
scaling to unlock the full potential of DRL in various tasks such as robot
control, autonomous driving and LLM training.

</details>


### [86] [Revisiting Deep Information Propagation: Fractal Frontier and Finite-size Effects](https://arxiv.org/abs/2508.03222)
*Giuseppe Alessio D'Inverno,Zhiyuan Hu,Leo Davy,Michael Unser,Gianluigi Rozza,Jonathan Dong*

Main category: cs.LG

TL;DR: 研究有限宽度神经网络中信息传播的混沌与有序边界的分形结构，揭示其独立于输入数据和优化的复杂性，并扩展分析至卷积神经网络。


<details>
  <summary>Details</summary>
Motivation: 探讨实际有限宽度神经网络中信息传播的特性，弥补无限宽度假设的不足。

Method: 利用傅里叶基结构化变换分析多层感知机和卷积神经网络的信息传播行为。

Result: 发现混沌与有序边界具有分形结构，且卷积神经网络表现出类似行为。

Conclusion: 有限网络深度在分离与鲁棒性权衡中具有重要性。

Abstract: Information propagation characterizes how input correlations evolve across
layers in deep neural networks. This framework has been well studied using
mean-field theory, which assumes infinitely wide networks. However, these
assumptions break down for practical, finite-size networks. In this work, we
study information propagation in randomly initialized neural networks with
finite width and reveal that the boundary between ordered and chaotic regimes
exhibits a fractal structure. This shows the fundamental complexity of neural
network dynamics, in a setting that is independent of input data and
optimization. To extend this analysis beyond multilayer perceptrons, we
leverage recently introduced Fourier-based structured transforms, and show that
information propagation in convolutional neural networks also follow the same
behavior. Our investigation highlights the importance of finite network depth
with respect to the tradeoff between separation and robustness.

</details>


### [87] [HALO: Hindsight-Augmented Learning for Online Auto-Bidding](https://arxiv.org/abs/2508.03267)
*Pusen Dong,Chenglong Cao,Xinyu Zhou,Jirong You,Linhe Xu,Feifan Xu,Shuo Yuan*

Main category: cs.LG

TL;DR: HALO是一种新型的在线自动竞价方法，通过后见机制和B样条函数表示，解决了传统自动竞价在多样约束下的样本效率低和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 数字广告平台中的实时竞价系统面临广告商异质性（预算和ROI目标差异大）带来的复杂挑战，传统自动竞价方法因样本效率低和泛化能力不足而失效。

Method: 提出HALO方法，结合后见机制（将探索数据重新用于任意约束配置）和B样条函数表示（实现连续、导数感知的竞价映射）。

Result: 在工业数据集上验证，HALO能有效处理多尺度约束，减少约束违反并提升GMV。

Conclusion: HALO为多约束竞价问题提供了高效且泛化能力强的解决方案。

Abstract: Digital advertising platforms operate millisecond-level auctions through
Real-Time Bidding (RTB) systems, where advertisers compete for ad impressions
through algorithmic bids. This dynamic mechanism enables precise audience
targeting but introduces profound operational complexity due to advertiser
heterogeneity: budgets and ROI targets span orders of magnitude across
advertisers, from individual merchants to multinational brands. This diversity
creates a demanding adaptation landscape for Multi-Constraint Bidding (MCB).
Traditional auto-bidding solutions fail in this environment due to two critical
flaws: 1) severe sample inefficiency, where failed explorations under specific
constraints yield no transferable knowledge for new budget-ROI combinations,
and 2) limited generalization under constraint shifts, as they ignore physical
relationships between constraints and bidding coefficients. To address this, we
propose HALO: Hindsight-Augmented Learning for Online Auto-Bidding. HALO
introduces a theoretically grounded hindsight mechanism that repurposes all
explorations into training data for arbitrary constraint configuration via
trajectory reorientation. Further, it employs B-spline functional
representation, enabling continuous, derivative-aware bid mapping across
constraint spaces. HALO ensures robust adaptation even when budget/ROI
requirements differ drastically from training scenarios. Industrial dataset
evaluations demonstrate the superiority of HALO in handling multi-scale
constraints, reducing constraint violations while improving GMV.

</details>


### [88] [Towards Interpretable Concept Learning over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2508.03269)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: 提出一种神经符号框架，将时间序列分类与解释结合，通过嵌入信号时序逻辑（STL）概念实现可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分类中黑盒深度学习模型缺乏可解释性的问题。

Method: 引入基于STL的新核函数，将原始时间序列映射到与预定义STL公式对齐的空间，联合优化分类准确性和可解释性。

Result: 模型在保持竞争力的同时，提供高质量的逻辑解释。

Conclusion: 该框架实现了基于人类可理解的时间模式分类，并生成局部和全局符号解释。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a neuro-symbolic framework that unifies classification and explanation
through direct embedding of trajectories into a space of Signal Temporal Logic
(STL) concepts. By introducing a novel STL-inspired kernel that maps raw time
series to their alignment with predefined STL formulae, our model jointly
optimises for accuracy and interpretability, as each prediction is accompanied
by the most relevant logical concepts that characterise it. This enables
classification grounded in human-interpretable temporal patterns and produces
both local and global symbolic explanations. Early results show competitive
performance while offering high-quality logical justifications for model
decisions.

</details>


### [89] [Understanding the Embedding Models on Hyper-relational Knowledge Graph](https://arxiv.org/abs/2508.03280)
*Yubo Wang,Shimin Di,Zhili Wang,Haoyang Li,Fei Teng,Hao Xin,Lei Chen*

Main category: cs.LG

TL;DR: 论文探讨了超关系知识图谱（HKGs）与传统知识图谱（KGs）的差异，通过分解方法将HKGs转换为KGs格式，并评估了经典KGE模型的性能。研究发现某些KGE模型表现与HKGE模型相当，但分解方法未能完全保留HKG信息。作者提出了FormerGNN框架，以解决现有HKGE模型的不足。


<details>
  <summary>Details</summary>
Motivation: 研究超关系知识图谱（HKGs）的嵌入模型性能是否源于其基础KGE模型或专门设计的扩展模块，并探索如何更好地处理HKG信息。

Method: 通过三种分解方法将HKGs转换为KGs格式，评估经典KGE模型性能，并提出FormerGNN框架，结合GNN编码器和改进的信息集成方法。

Result: 某些KGE模型表现与HKGE模型相当，但分解方法未能完全保留HKG信息。FormerGNN框架在实验中优于现有HKGE模型。

Conclusion: FormerGNN框架通过保留HKG拓扑和解决信息压缩问题，为未来HKGE研究提供了方向。

Abstract: Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an
extension of traditional Knowledge Graphs (KGs) to better represent real-world
facts with additional qualifiers. As a result, researchers have attempted to
adapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing
extra qualifier processing modules. However, it remains unclear whether the
superior performance of Hyper-relational KGE (HKGE) models arises from their
base KGE model or the specially designed extension module. Hence, in this
paper, we data-wise convert HKGs to KG format using three decomposition methods
and then evaluate the performance of several classical KGE models on HKGs. Our
results show that some KGE models achieve performance comparable to that of
HKGE models. Upon further analysis, we find that the decomposition methods
alter the original HKG topology and fail to fully preserve HKG information.
Moreover, we observe that current HKGE models are either insufficient in
capturing the graph's long-range dependency or struggle to integrate
main-triple and qualifier information due to the information compression issue.
To further justify our findings and offer a potential direction for future HKGE
research, we propose the FormerGNN framework. This framework employs a
qualifier integrator to preserve the original HKG topology, and a GNN-based
graph encoder to capture the graph's long-range dependencies, followed by an
improved approach for integrating main-triple and qualifier information to
mitigate compression issues. Our experimental results demonstrate that
FormerGNN outperforms existing HKGE models.

</details>


### [90] [Online Continual Graph Learning](https://arxiv.org/abs/2508.03283)
*Giovanni Donghi,Luca Pasa,Daniele Zambon,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: 本文提出了在线持续学习在图上的通用框架，强调了对图拓扑的高效批量处理需求，并提供了系统模型评估的明确设置。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图随时间演变，需要及时和在线预测，但现有方法未充分适应标准在线持续学习设置，缺乏明确的定义。

Method: 提出了在线持续学习在图上的通用框架，并引入了一组基准测试。

Result: 报告了多种持续学习方法在适应新设置后的性能表现。

Conclusion: 本文为图上的在线持续学习提供了明确的定义和评估框架，填补了研究空白。

Abstract: The aim of Continual Learning (CL) is to learn new tasks incrementally while
avoiding catastrophic forgetting. Online Continual Learning (OCL) specifically
focuses on learning efficiently from a continuous stream of data with shifting
distribution. While recent studies explore Continual Learning on graphs
exploiting Graph Neural Networks (GNNs), only few of them focus on a streaming
setting. Yet, many real-world graphs evolve over time, often requiring timely
and online predictions. Current approaches, however, are not well aligned with
the standard OCL setting, partly due to the lack of a clear definition of
online Continual Learning on graphs. In this work, we propose a general
formulation for online Continual Learning on graphs, emphasizing the efficiency
requirements on batch processing over the graph topology, and providing a
well-defined setting for systematic model evaluation. Finally, we introduce a
set of benchmarks and report the performance of several methods in the CL
literature, adapted to our setting.

</details>


### [91] [Strategic Hypothesis Testing](https://arxiv.org/abs/2508.03289)
*Safwan Hossain,Yatong Chen,Yiling Chen*

Main category: cs.LG

TL;DR: 研究探讨了在委托-代理框架下的假设检验问题，分析了代理人的策略行为对委托人决策的影响，并提出了一个可计算的临界p值阈值。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解代理人在假设检验中的策略行为如何影响委托人的决策，尤其是在产品审批等场景中。

Method: 通过博弈论模型分析代理人的参与和报告行为对委托人统计决策规则的反应，并提出了一个可计算的临界p值阈值。

Result: 研究发现委托人的错误行为在临界p值阈值分割下表现出单调性，并通过药物审批数据验证了模型。

Conclusion: 研究为假设检验框架下的策略互动提供了全面的视角，具有技术和监管意义。

Abstract: We examine hypothesis testing within a principal-agent framework, where a
strategic agent, holding private beliefs about the effectiveness of a product,
submits data to a principal who decides on approval. The principal employs a
hypothesis testing rule, aiming to pick a p-value threshold that balances false
positives and false negatives while anticipating the agent's incentive to
maximize expected profitability. Building on prior work, we develop a
game-theoretic model that captures how the agent's participation and reporting
behavior respond to the principal's statistical decision rule. Despite the
complexity of the interaction, we show that the principal's errors exhibit
clear monotonic behavior when segmented by an efficiently computable critical
p-value threshold, leading to an interpretable characterization of their
optimal p-value threshold. We empirically validate our model and these insights
using publicly available data on drug approvals. Overall, our work offers a
comprehensive perspective on strategic interactions within the hypothesis
testing framework, providing technical and regulatory insights.

</details>


### [92] [Bridging ocean wave physics and deep learning: Physics-informed neural operators for nonlinear wavefield reconstruction in real-time](https://arxiv.org/abs/2508.03315)
*Svenja Ehlers,Merten Stender,Norbert Hoffmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经算子（PINO）框架，用于从稀疏测量中重建非线性海洋波场，无需训练时的真实数据。


<details>
  <summary>Details</summary>
Motivation: 解决实时预测相位分辨海洋波场的难题，因缺乏从稀疏或间接测量中重建初始条件的实用方法。

Method: 通过将自由表面边界条件的残差嵌入PINO的损失函数中，软约束解空间。

Result: 验证表明，PINO能准确重建非线性波场，适用于多种波况。

Conclusion: PINO为实际海洋环境中的数据驱动波场重建和预测提供了可行方案。

Abstract: Accurate real-time prediction of phase-resolved ocean wave fields remains a
critical yet largely unsolved problem, primarily due to the absence of
practical data assimilation methods for reconstructing initial conditions from
sparse or indirect wave measurements. While recent advances in supervised deep
learning have shown potential for this purpose, they require large labelled
datasets of ground truth wave data, which are infeasible to obtain in
real-world scenarios. To overcome this limitation, we propose a
Physics-Informed Neural Operator (PINO) framework for reconstructing spatially
and temporally phase-resolved, nonlinear ocean wave fields from sparse
measurements, without the need for ground truth data during training. This is
achieved by embedding residuals of the free surface boundary conditions of
ocean gravity waves into the loss function of the PINO, constraining the
solution space in a soft manner. After training, we validate our approach using
highly realistic synthetic wave data and demonstrate the accurate
reconstruction of nonlinear wave fields from both buoy time series and radar
snapshots. Our results indicate that PINOs enable accurate, real-time
reconstruction and generalize robustly across a wide range of wave conditions,
thereby paving the way for operational, data-driven wave reconstruction and
prediction in realistic marine environments.

</details>


### [93] [Software Fairness Dilemma: Is Bias Mitigation a Zero-Sum Game?](https://arxiv.org/abs/2508.03323)
*Zhenpeng Chen,Xinyue Li,Jie M. Zhang,Weisong Sun,Ying Xiao,Tianlin Li,Yiling Lou,Yang Liu*

Main category: cs.LG

TL;DR: 研究发现，表格数据任务的偏见缓解方法表现为零和博弈，而非此前发现的“拉平效应”。通过仅对弱势群体应用最新方法，可能在不影响优势群体或整体性能的情况下提升公平性。


<details>
  <summary>Details</summary>
Motivation: 探讨表格数据任务中偏见缓解方法是否也存在“拉平效应”，并寻找无零和博弈的公平性提升途径。

Method: 评估八种偏见缓解方法在44个任务中的表现，使用五种真实数据集和四种常见ML模型。

Result: 方法表现为零和博弈，但仅对弱势群体应用最新方法可避免负面影响。

Conclusion: 研究为无零和博弈的公平性提升提供了潜在路径，有助于推动偏见缓解方法的广泛应用。

Abstract: Fairness is a critical requirement for Machine Learning (ML) software,
driving the development of numerous bias mitigation methods. Previous research
has identified a leveling-down effect in bias mitigation for computer vision
and natural language processing tasks, where fairness is achieved by lowering
performance for all groups without benefiting the unprivileged group. However,
it remains unclear whether this effect applies to bias mitigation for tabular
data tasks, a key area in fairness research with significant real-world
applications. This study evaluates eight bias mitigation methods for tabular
data, including both widely used and cutting-edge approaches, across 44 tasks
using five real-world datasets and four common ML models. Contrary to earlier
findings, our results show that these methods operate in a zero-sum fashion,
where improvements for unprivileged groups are related to reduced benefits for
traditionally privileged groups. However, previous research indicates that the
perception of a zero-sum trade-off might complicate the broader adoption of
fairness policies. To explore alternatives, we investigate an approach that
applies the state-of-the-art bias mitigation method solely to unprivileged
groups, showing potential to enhance benefits of unprivileged groups without
negatively affecting privileged groups or overall ML performance. Our study
highlights potential pathways for achieving fairness improvements without
zero-sum trade-offs, which could help advance the adoption of bias mitigation
methods.

</details>


### [94] [Exploring Layer-wise Information Effectiveness for Post-Training Quantization in Small Language Models](https://arxiv.org/abs/2508.03332)
*He Xiao,Qingyao Yang,Dirui Xie,Wendong Xu,Wenyong Zhou,Haobo Liu,Zhengwu Liu,Ngai Wong*

Main category: cs.LG

TL;DR: LieQ是一种基于度量的后训练量化框架，用于在极端低比特压缩下保持子7B模型的准确性，通过分层诊断自动分配比特宽度，无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在低比特量化下准确率严重下降的问题，优化内存和能源消耗。

Method: 引入三种分层诊断指标（困惑度下降、表示紧凑性和Top-k能量增益），自动分配比特宽度。

Result: 在Qwen3-4B上，2.05比特量化恢复95.9%的FP16性能；在LLaMA3.2-3B上，2.07比特量化保持98.2%的基线准确性。

Conclusion: LieQ在低比特量化下实现了最优的压缩-准确性权衡，为资源受限的边缘设备部署小型语言模型提供了新范式。

Abstract: Large language models with billions of parameters are often over-provisioned:
many layers contribute little unique information yet dominate the memory and
energy footprint during inference. We present LieQ, a metric-driven
post-training quantization framework that addresses the critical challenge of
maintaining accuracy in sub-7B models under extreme low-bit compression. Our
method introduces three complementary layer-wise diagnostics-Perplexity Drop,
Representational Compactness, and Top-k Energy Gain -that reveal a canonical
division of labour across layers, enabling automatic bit-width allocation
without gradient updates. Unlike existing approaches that suffer severe
accuracy degradation at 2-3 bits precision, LieQ achieves state-of-the-art
compression-accuracy trade-offs: on Qwen3-4B, it recovers 95.9% of FP16
baseline performance at 2.05-bit quantization, outperforming GPTQ by 19.7% and
AWQ by 18.1% on average across seven zero-shot reasoning tasks. Applied to
LLaMA3.2-3B, LieQ maintains 98.2% of baseline accuracy at 2.07-bit precision
while enabling 4x memory reduction, establishing new paradigms for deploying
small language models on resource-constrained edge devices.

</details>


### [95] [A neural network machine-learning approach for characterising hydrogen trapping parameters from TDS experiments](https://arxiv.org/abs/2508.03371)
*N. Marrani,T. Hageman,E. Martínez-Pañeda*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的方案，通过多神经网络模型从TDS光谱中直接预测金属合金的氢捕获参数。


<details>
  <summary>Details</summary>
Motivation: 传统TDS方法提取氢捕获参数（如结合能和密度）存在挑战，需要间接方法。

Method: 开发了由分类和回归神经网络组成的模型，训练于合成数据，用于预测捕获参数。

Result: 模型在三种不同成分的回火马氏体钢中表现出强预测能力。

Conclusion: 该模型为氢捕获参数的直接预测提供了有效工具，代码已开源。

Abstract: The hydrogen trapping behaviour of metallic alloys is generally characterised
using Thermal Desorption Spectroscopy (TDS). However, as an indirect method,
extracting key parameters (trap binding energies and densities) remains a
significant challenge. To address these limitations, this work introduces a
machine learning-based scheme for parameter identification from TDS spectra. A
multi-Neural Network (NN) model is developed and trained exclusively on
synthetic data to predict trapping parameters directly from experimental data.
The model comprises two multi-layer, fully connected, feed-forward NNs trained
with backpropagation. The first network (classification model) predicts the
number of distinct trap types. The second network (regression model) then
predicts the corresponding trap densities and binding energies. The NN
architectures, hyperparameters, and data pre-processing were optimised to
minimise the amount of training data. The proposed model demonstrated strong
predictive capabilities when applied to three tempered martensitic steels of
different compositions. The code developed is freely provided.

</details>


### [96] [AI on the Pulse: Real-Time Health Anomaly Detection with Wearable and Ambient Intelligence](https://arxiv.org/abs/2508.03436)
*Davide Gabrielli,Bardh Prenkaj,Paola Velardi,Stefano Faralli*

Main category: cs.LG

TL;DR: AI on the Pulse是一个实时异常检测系统，结合可穿戴设备和AI模型，通过UniTS模型学习患者独特模式，提供个性化警报。


<details>
  <summary>Details</summary>
Motivation: 解决传统分类方法在实时场景中需要持续标注的问题，实现非侵入式健康监测。

Method: 融合可穿戴传感器和环境智能，利用UniTS模型进行异常检测，并整合LLMs提升可解释性。

Result: 在12种SoTA异常检测方法中表现最优，F1分数提升约22%，成功应用于家庭监测。

Conclusion: 系统证明无需临床级设备即可实现高质量健康监测，并通过LLMs提供临床见解。

Abstract: We introduce AI on the Pulse, a real-world-ready anomaly detection system
that continuously monitors patients using a fusion of wearable sensors, ambient
intelligence, and advanced AI models. Powered by UniTS, a state-of-the-art
(SoTA) universal time-series model, our framework autonomously learns each
patient's unique physiological and behavioral patterns, detecting subtle
deviations that signal potential health risks. Unlike classification methods
that require impractical, continuous labeling in real-world scenarios, our
approach uses anomaly detection to provide real-time, personalized alerts for
reactive home-care interventions. Our approach outperforms 12 SoTA anomaly
detection methods, demonstrating robustness across both high-fidelity medical
devices (ECG) and consumer wearables, with a ~ 22% improvement in F1 score.
However, the true impact of AI on the Pulse lies in @HOME, where it has been
successfully deployed for continuous, real-world patient monitoring. By
operating with non-invasive, lightweight devices like smartwatches, our system
proves that high-quality health monitoring is possible without clinical-grade
equipment. Beyond detection, we enhance interpretability by integrating LLMs,
translating anomaly scores into clinically meaningful insights for healthcare
professionals.

</details>


### [97] [An Auditable Agent Platform For Automated Molecular Optimisation](https://arxiv.org/abs/2508.03444)
*Atabey Ünlü,Phil Rohr,Ahmet Celebi*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多智能体框架的分子优化工具，通过自动化设计循环加速药物发现。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程中数据、专业知识和工具的分散导致设计周期缓慢，需要缩短这一循环。

Method: 构建了一个分层智能体框架，包括数据库代理、AI专家、药物化学家、排名代理和科学评论家，通过工具调用和简洁的溯源记录实现分子优化。

Result: 多智能体设置显著提高了结合亲和力（提升31%），而单智能体生成了更具药物特性的分子。

Conclusion: 通过测试时间扩展、反馈循环和溯源记录，通用LLM可转化为可审计的分子设计系统，未来可扩展工具集以进一步优化药物发现流程。

Abstract: Drug discovery frequently loses momentum when data, expertise, and tools are
scattered, slowing design cycles. To shorten this loop we built a hierarchical,
tool using agent framework that automates molecular optimisation. A Principal
Researcher defines each objective, a Database agent retrieves target
information, an AI Expert generates de novo scaffolds with a sequence to
molecule deep learning model, a Medicinal Chemist edits them while invoking a
docking tool, a Ranking agent scores the candidates, and a Scientific Critic
polices the logic. Each tool call is summarised and stored causing the full
reasoning path to remain inspectable. The agents communicate through concise
provenance records that capture molecular lineage, to build auditable, molecule
centered reasoning trajectories and reuse successful transformations via in
context learning. Three cycle research loops were run against AKT1 protein
using five large language models. After ranking the models by mean docking
score, we ran 20 independent scale ups on the two top performers. We then
compared the leading LLMs' binding affinity results across three
configurations, LLM only, single agent, and multi agent. Our results reveal an
architectural trade off, the multi agent setting excelled at focused binding
optimization, improving average predicted binding affinity by 31%. In contrast,
single agent runs generated molecules with superior drug like properties at the
cost of less potent binding scores. Unguided LLM runs finished fastest, yet
their lack of transparent tool signals left the validity of their reasoning
paths unverified. These results show that test time scaling, focused feedback
loops and provenance convert general purpose LLMs into auditable systems for
molecular design, and suggest that extending the toolset to ADMET and
selectivity predictors could push research workflows further along the
discovery pipeline.

</details>


### [98] [Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03501)
*Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Maksim Nekrashevich,Anton Shevtsov,Simon Karasik,Sergey Abramov,Andrei Andriushchenko,Filipp Fisin,Sergei Skvortsov,Boris Yangel*

Main category: cs.LG

TL;DR: 该论文研究了强化学习（RL）在大型语言模型（LLMs）中的应用，突破了单轮任务的限制，专注于多轮交互场景（如软件工程任务），并展示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单轮任务（如数学推理或代码生成），而忽略了需要多轮交互的真实场景（如软件工程）。本文旨在填补这一空白。

Method: 采用改进的Decoupled Advantage Policy Optimization (DAPO)算法，基于Qwen2.5-72B-Instruct模型训练智能体。

Result: 在SWE-bench Verified基准测试中，成功率从20%提升至39%；在SWE-rebench上，性能与领先的开源模型相当或更优。

Conclusion: 该方法为基于开源模型构建更强大的自主智能体提供了可行路径，适用于复杂现实问题。

Abstract: Research on applications of Reinforcement Learning (RL) to Large Language
Models (LLMs) has mostly been focused on single-turn problems, such as
mathematical reasoning or single-shot code generation. While these problems can
be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate
case of multi-turn interaction where the environment provides no feedback. This
contrasts with many real-world domains, such as software engineering (SWE),
which require rich multi-turn interactions with a stateful environment that
responds to each action with a non-trivial observation.
  To bridge this gap, we demonstrate the successful application of RL to this
general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)
algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world
software engineering tasks. Our approach increases the agent's success rate on
the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to
39%, without relying on any teacher models. On SWE-rebench, our agent matches
or outperforms leading open-weight models such as DeepSeek-V3-0324 and
Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward
building more capable autonomous agents for complex real-world problems based
on open models.

</details>


### [99] [SLA-MORL: SLA-Aware Multi-Objective Reinforcement Learning for HPC Resource Optimization](https://arxiv.org/abs/2508.03509)
*Seraj Al Mahmud Mostafa,Aravind Mohan,Jianwu Wang*

Main category: cs.LG

TL;DR: SLA-MORL是一种基于多目标强化学习的自适应框架，用于动态分配云环境中的GPU和CPU资源，以优化训练时间、成本和SLA合规性。


<details>
  <summary>Details</summary>
Motivation: 传统静态资源分配或单目标优化方法容易导致SLA违规或资源浪费，无法满足现代机器学习工作负载的动态需求。

Method: SLA-MORL采用智能初始化和动态权重适应技术，通过21维状态表示和9种可能的动作，利用actor-critic网络进行资源分配决策。

Result: 在13种不同的ML工作负载上，SLA-MORL显著减少了训练时间（67.2%）、成本（68.8%）并提高了SLA合规性（73.4%）。

Conclusion: SLA-MORL通过解决冷启动和动态适应问题，为现代ML训练环境提供了性能、成本和可靠性的平衡解决方案。

Abstract: Dynamic resource allocation for machine learning workloads in cloud
environments remains challenging due to competing objectives of minimizing
training time and operational costs while meeting Service Level Agreement (SLA)
constraints. Traditional approaches employ static resource allocation or
single-objective optimization, leading to either SLA violations or resource
waste. We present SLA-MORL, an adaptive multi-objective reinforcement learning
framework that intelligently allocates GPU and CPU resources based on
user-defined preferences (time, cost, or balanced) while ensuring SLA
compliance. Our approach introduces two key innovations: (1) intelligent
initialization through historical learning or efficient baseline runs that
eliminates cold-start problems, reducing initial exploration overhead by 60%,
and (2) dynamic weight adaptation that automatically adjusts optimization
priorities based on real-time SLA violation severity, creating a
self-correcting system. SLA-MORL constructs a 21-dimensional state
representation capturing resource utilization, training progress, and SLA
compliance, enabling an actor-critic network to make informed allocation
decisions across 9 possible actions. Extensive evaluation on 13 diverse ML
workloads using production HPC infrastructure demonstrates that SLA-MORL
achieves 67.2% reduction in training time for deadline-critical jobs, 68.8%
reduction in costs for budget-constrained workloads, and 73.4% improvement in
overall SLA compliance compared to static baselines. By addressing both
cold-start inefficiency and dynamic adaptation challenges, SLA-MORL provides a
practical solution for cloud resource management that balances performance,
cost, and reliability in modern ML training environments.

</details>


### [100] [MoKA: Mixture of Kronecker Adapters](https://arxiv.org/abs/2508.03527)
*Mohammadreza Sadeghi,Mahsa Ghazvini Nejad,MirHamed Jafarzadeh Asl,Yu Gu,Yuanhao Yu,Masoud Asgharian,Vahid Partovi Nia*

Main category: cs.LG

TL;DR: MoKA提出了一种新的Kronecker适配器，通过混合Kronecker乘积建模权重更新，解决了低秩适配器表达能力受限的问题，并在性能和参数效率之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 低秩适配器在参数效率方面表现良好，但其表达能力受限于秩约束，影响了复杂任务的性能。

Method: 提出Mixture of Kronecker Adapters (MoKA)，利用门控机制衡量每个Kronecker因子的重要性，并通过标准矩阵运算优化硬件效率。

Result: MoKA在指令调优和常识推理任务中优于基线方法，并将可训练参数减少至27倍，实现了性能与参数效率的最佳平衡。

Conclusion: MoKA通过混合Kronecker适配器和硬件优化，显著提升了参数效率与性能，为大型语言模型的微调提供了高效解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) is essential for reducing the
computational overhead of large language models (LLMs). Low-rank family
adapters are commonly used to control the parameter size efficiently while
maintaining the generative power of LLMs. However, their limited expressiveness
due to the rank constraint often restricts their performance on complex tasks.
We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker
adapters that addresses this limitation by modeling weight updates as a mixture
of Kronecker products. Our proposed adapter leverages a gating mechanism that
measures the importance of each Kronecker factor, enabling more expressive
adaptation. Moreover, MoKA enables a rank flexibility that provides a better
trade-off between parameter efficiency and accuracy. To ensure hardware
efficiency, we reformulate Kronecker computations using standard matrix
operations, allowing seamless deployment on GPU-optimized hardware. We conduct
extensive experiments on instruction-tuning and commonsense reasoning tasks
using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not
only outperforms PEFT baselines, but also reduces the number of trainable
parameters up to 27x, achieving state-of-the-art trade-offs between performance
and parameter efficiency.

</details>


### [101] [VRPRM: Process Reward Modeling via Visual Reasoning](https://arxiv.org/abs/2508.03556)
*Xinquan Chen,Bangwei Liu,Xuhong Wang*

Main category: cs.LG

TL;DR: VRPRM通过视觉推理和两阶段训练策略，以更低的数据标注成本提升过程奖励模型（PRM）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有PRM缺乏长期推理和深度思考能力，而CoT-PRM数据标注成本过高。

Method: 提出VRPRM，结合视觉推理，设计高效的两阶段训练策略。

Result: 仅用少量数据（3.6K CoT-PRM和50K非CoT PRM数据），性能超越400K数据的非思考PRM，相对提升118%。

Conclusion: VRPRM为PRM训练提供了高效数据利用的新范式。

Abstract: Process Reward Model (PRM) is widely used in the post-training of Large
Language Model (LLM) because it can perform fine-grained evaluation of the
reasoning steps of generated content. However, most PRMs lack long-term
reasoning and deep thinking capabilities. On the other hand, although a few
works have tried to introduce Chain-of-Thought capability into PRMs, the
annotation cost of CoT-PRM data is too expensive to play a stable role in
various tasks. To address the above challenges, we propose VRPRM, a process
reward model via visual reasoning, and design an efficient two-stage training
strategy. Experimental results show that using only 3.6K CoT-PRM SFT data and
50K non-CoT PRM RL training data, VRPRM can surpass the non-thinking PRM with a
total data volume of 400K and achieved a relative performance improvement of up
to 118\% over the base model in the BoN experiment. This result confirms that
the proposed combined training strategy can achieve higher quality reasoning
capabilities at a lower data annotation cost, thus providing a new paradigm for
PRM training with more efficient data utilization.

</details>


### [102] [Heterogeneity-Oblivious Robust Federated Learning](https://arxiv.org/abs/2508.03579)
*Weiyao Zhang,Jinyang Li,Qi Song,Miao Wang,Chungang Lin,Haitong Luo,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: Horus是一个针对联邦学习（FL）中异构性和投毒攻击的鲁棒框架，通过低秩适应（LoRAs）减少攻击面并提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在超异构环境下易受投毒攻击，传统聚合策略效果不佳且攻击难以检测。

Method: Horus在稳定层插入LoRAs，仅聚合LoRAs以减少攻击面，并利用LoRA-A的稳定性设计投毒评分和投影感知聚合机制。

Result: Horus在多样数据集、模型架构和攻击下均优于现有基线，鲁棒性和准确性显著提升。

Conclusion: Horus通过LoRAs和创新的聚合机制，有效解决了FL中的异构性和投毒攻击问题。

Abstract: Federated Learning (FL) remains highly vulnerable to poisoning attacks,
especially under real-world hyper-heterogeneity, where clients differ
significantly in data distributions, communication capabilities, and model
architectures. Such heterogeneity not only undermines the effectiveness of
aggregation strategies but also makes attacks more difficult to detect.
Furthermore, high-dimensional models expand the attack surface. To address
these challenges, we propose Horus, a heterogeneity-oblivious robust FL
framework centered on low-rank adaptations (LoRAs). Rather than aggregating
full model parameters, Horus inserts LoRAs into empirically stable layers and
aggregates only LoRAs to reduce the attack surface.We uncover a key empirical
observation that the input projection (LoRA-A) is markedly more stable than the
output projection (LoRA-B) under heterogeneity and poisoning. Leveraging this,
we design a Heterogeneity-Oblivious Poisoning Score using the features from
LoRA-A to filter poisoned clients. For the remaining benign clients, we propose
projection-aware aggregation mechanism to preserve collaborative signals while
suppressing drifts, which reweights client updates by consistency with the
global directions. Extensive experiments across diverse datasets, model
architectures, and attacks demonstrate that Horus consistently outperforms
state-of-the-art baselines in both robustness and accuracy.

</details>


### [103] [DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations](https://arxiv.org/abs/2508.03586)
*Yuhan Guo,Lizhong Ding,Shihan Jia,Yanyu Ren,Pengqi Li,Jiarun Fu,Changsheng Li,Ye yuan,Guoren Wang*

Main category: cs.LG

TL;DR: DeepFaith提出了一种基于忠实性的统一解释框架，解决了现有XAI方法缺乏统一评价标准的问题，通过多指标优化生成高忠实性解释。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法因缺乏统一的最优解释标准，无法进行客观评价和优化，DeepFaith旨在解决这一问题。

Method: 通过统一多个忠实性指标的数学表达，设计了一个学习框架，结合现有解释方法生成高质量监督信号，并优化模式一致性和局部相关性。

Result: 在12个任务、6个模型和6个数据集上，DeepFaith在10个指标上的忠实性表现优于所有基线方法。

Conclusion: DeepFaith提供了一个理论上的最优解释标准，并在多领域任务中表现出高效性和泛化能力。

Abstract: Explainable AI (XAI) builds trust in complex systems through model
attribution methods that reveal the decision rationale. However, due to the
absence of a unified optimal explanation, existing XAI methods lack a ground
truth for objective evaluation and optimization. To address this issue, we
propose Deep architecture-based Faith explainer (DeepFaith), a domain-free and
model-agnostic unified explanation framework under the lens of faithfulness. By
establishing a unified formulation for multiple widely used and well-validated
faithfulness metrics, we derive an optimal explanation objective whose solution
simultaneously achieves optimal faithfulness across these metrics, thereby
providing a ground truth from a theoretical perspective. We design an explainer
learning framework that leverages multiple existing explanation methods,
applies deduplicating and filtering to construct high-quality supervised
explanation signals, and optimizes both pattern consistency loss and local
correlation to train a faithful explainer. Once trained, DeepFaith can generate
highly faithful explanations through a single forward pass without accessing
the model being explained. On 12 diverse explanation tasks spanning 6 models
and 6 datasets, DeepFaith achieves the highest overall faithfulness across 10
metrics compared to all baseline methods, highlighting its effectiveness and
cross-domain generalizability.

</details>


### [104] [Zero-Variance Gradients for Variational Autoencoders](https://arxiv.org/abs/2508.03587)
*Zilei Shao,Anji Liu,Guy Van den Broeck*

Main category: cs.LG

TL;DR: 提出了一种名为Silent Gradients的新方法，通过特定解码器架构直接计算期望ELBO，避免了梯度估计的方差问题，提升了生成模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器（VAE）训练中，梯度通过随机采样的潜在变量反向传播，导致估计方差大，影响收敛和性能。

Method: 利用特定解码器架构解析计算期望ELBO，实现零方差梯度；并提出一种训练动态，早期使用精确梯度，后期过渡到随机估计器。

Result: 实验表明，该方法在多种数据集上优于现有基线（如重参数化、Gumbel-Softmax和REINFORCE）。

Conclusion: Silent Gradients为生成模型训练提供了新方向，结合了解析计算的稳定性和深度非线性架构的表达能力。

Abstract: Training deep generative models like Variational Autoencoders (VAEs) is often
hindered by the need to backpropagate gradients through the stochastic sampling
of their latent variables, a process that inherently introduces estimation
variance, which can slow convergence and degrade performance. In this paper, we
propose a new perspective that sidesteps this problem, which we call Silent
Gradients. Instead of improving stochastic estimators, we leverage specific
decoder architectures to analytically compute the expected ELBO, yielding a
gradient with zero variance. We first provide a theoretical foundation for this
method and demonstrate its superiority over existing estimators in a controlled
setting with a linear decoder. To generalize our approach for practical use
with complex, expressive decoders, we introduce a novel training dynamic that
uses the exact, zero-variance gradient to guide the early stages of encoder
training before annealing to a standard stochastic estimator. Our experiments
show that this technique consistently improves the performance of established
baselines, including reparameterization, Gumbel-Softmax, and REINFORCE, across
multiple datasets. This work opens a new direction for training generative
models by combining the stability of analytical computation with the
expressiveness of deep, nonlinear architecture.

</details>


### [105] [VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting](https://arxiv.org/abs/2508.03589)
*Adib Hasan,Mardavij Roozbehani,Munther Dahleh*

Main category: cs.LG

TL;DR: VITA是一种针对农业产量预测的变分预训练框架，通过自监督特征掩码解决数据不对称问题，在极端天气年份表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在产量偏离历史趋势时表现不佳，主要由于预训练天气数据与微调数据之间的不对称性。

Method: VITA利用详细天气变量作为代理目标进行预训练，通过自监督特征掩码学习预测大气状态，微调时仅需基本天气统计。

Result: 在美国玉米带的763个县，VITA在预测玉米和大豆产量上达到最优性能，尤其在极端天气年份显著优于其他模型。

Conclusion: VITA展示了领域感知AI设计如何克服数据限制，支持气候变化下的农业预测。

Abstract: Accurate crop yield forecasting is essential for global food security.
However, current AI models systematically underperform when yields deviate from
historical trends. This issue arises from key data challenges, including a
major asymmetry between rich pretraining weather datasets and the limited data
available for fine-tuning. We introduce VITA (Variational Inference Transformer
for Asymmetric data), a variational pretraining framework that addresses this
asymmetry. Instead of relying on input reconstruction, VITA uses detailed
weather variables as proxy targets during pretraining and learns to predict
rich atmospheric states through self-supervised feature masking. This allows
the model to be fine-tuned using only basic weather statistics during
deployment. Applied to 763 counties in the U.S. Corn Belt, VITA achieves
state-of-the-art performance in predicting corn and soybean yields across all
evaluation scenarios. While it consistently delivers superior performance under
normal conditions, its advantages are particularly pronounced during extreme
weather years, with statistically significant improvements (paired t-test, $p
\approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNN
using less data, making it more practical for real-world use--particularly in
data-scarce regions. This work highlights how domain-aware AI design can
overcome data limitations and support resilient agricultural forecasting in a
changing climate.

</details>


### [106] [SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA](https://arxiv.org/abs/2508.03590)
*Mingliang Bai,Zuliang Fang,Shengyu Tao,Siqi Xiang,Jiang Bian,Yanfei Xiang,Pengcheng Zhao,Weixin Jin,Jonathan A. Weyn,Haiyu Dong,Bin Zhang,Hongyu Sun,Kit Thambiratnam,Qi Zhang,Hongbin Sun,Xuan Zhang,Qiuwei Wu*

Main category: cs.LG

TL;DR: SolarSeer是一种基于AI的端到端模型，用于美国本土24小时太阳辐照度预测，比传统数值天气预报（NWP）快1500倍，且显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 传统NWP模型计算成本高且依赖复杂物理模拟，SolarSeer旨在通过AI直接映射历史卫星数据到未来预测，提升效率和准确性。

Method: SolarSeer利用历史卫星观测数据，直接生成未来24小时的云覆盖和太阳辐照度预测，避免数据同化和偏微分方程求解。

Result: SolarSeer比HRRR模型在再分析数据中降低27.28%的均方根误差，在1800个站点中降低15.35%，并显著提升辐照度波动预测精度。

Conclusion: SolarSeer为可持续能源系统提供了快速、准确的太阳辐照度预测支持。

Abstract: Accurate 24-hour solar irradiance forecasting is essential for the safe and
economic operation of solar photovoltaic systems. Traditional numerical weather
prediction (NWP) models represent the state-of-the-art in forecasting
performance but rely on computationally costly data assimilation and solving
complicated partial differential equations (PDEs) that simulate atmospheric
physics. Here, we introduce SolarSeer, an end-to-end large artificial
intelligence (AI) model for solar irradiance forecasting across the Contiguous
United States (CONUS). SolarSeer is designed to directly map the historical
satellite observations to future forecasts, eliminating the computational
overhead of data assimilation and PDEs solving. This efficiency allows
SolarSeer to operate over 1,500 times faster than traditional NWP, generating
24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer
resolution in under 3 seconds. Compared with the state-of-the-art NWP in the
CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly
reduces the root mean squared error of solar irradiance forecasting by 27.28%
in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively
captures solar irradiance fluctuations and significantly enhances the
first-order irradiance difference forecasting accuracy. SolarSeer's ultrafast,
accurate 24-hour solar irradiance forecasts provide strong support for the
transition to sustainable, net-zero energy systems.

</details>


### [107] [Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction](https://arxiv.org/abs/2508.03613)
*Yong Lin,Shange Tang,Bohan Lyu,Ziran Yang,Jui-Hui Chung,Haoyu Zhao,Lai Jiang,Yihan Geng,Jiawei Ge,Jingruo Sun,Jiayun Wu,Jiri Gesi,Ximing Lu,David Acuna,Kaiyu Yang,Hongzhou Lin,Yejin Choi,Danqi Chen,Sanjeev Arora,Chi Jin*

Main category: cs.LG

TL;DR: Goedel-Prover-V2是一系列开源语言模型，在自动定理证明领域达到新SOTA。通过专家迭代和强化学习，结合三项创新技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 提升自动定理证明的性能，通过创新方法解决模型训练中的多样性下降和复杂度挑战。

Method: 1. 分阶段数据合成；2. 验证器引导的自我修正；3. 模型平均化。

Result: 小型模型Goedel-Prover-V2-8B在MiniF2F上达到84.6% pass@32，旗舰模型Goedel-Prover-V2-32B在MiniF2F上达到90.4% pass@32，并在PutnamBench上表现优异。

Conclusion: Goedel-Prover-V2在开源定理证明器中性能最强，部分指标甚至超过闭源模型，且计算资源需求更低。

Abstract: We introduce Goedel-Prover-V2, a series of open-source language models that
set a new state-of-the-art in automated theorem proving. Built on the standard
expert iteration and reinforcement learning pipeline, our approach incorporates
three key innovations: (1) Scaffolded data synthesis: We generate synthetic
tasks of increasing difficulty to train the model to master increasingly
complex theorems; (2) Verifier-guided self-correction: We enable the model to
iteratively revise its proofs by leveraging feedback from the Lean compiler;
(3) Model averaging: We merge model checkpoints to mitigate the decrease in
model output diversity in later stages of training. Our small model,
Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms
DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our
flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in
standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a
large margin. Additionally, our flagship model solves 86 problems on
PutnamBench at pass@184, securing the first place among open-source models on
the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47
problems by pass@1024 with a significantly smaller model size and compute
budget. At the time of its release (July-August 2025), Goedel-Prover-V2
achieves the strongest overall performance among all open-source theorem
provers. It also ranks among the top-performing models--including closed-source
systems with publicly reported performance--under a constrained test-time
compute budget. Our models, code, and data are released at
https://github.com/Goedel-LM/Goedel-Prover-V2.

</details>


### [108] [Minimal Convolutional RNNs Accelerate Spatiotemporal Learning](https://arxiv.org/abs/2508.03614)
*Coşku Can Horuz,Sebastian Otte,Martin V. Butz,Matthias Karlbauer*

Main category: cs.LG

TL;DR: 提出了MinConvLSTM和MinConvGRU两种新型时空模型，结合了卷积循环网络的空间归纳偏置和高效可并行训练的RNN特性。


<details>
  <summary>Details</summary>
Motivation: 解决传统ConvRNN模型在训练时因顺序隐藏状态更新导致的效率瓶颈问题。

Method: 扩展了MinLSTM和MinGRU的对数域前缀和公式到卷积架构，引入指数门控机制简化计算。

Result: 在Navier-Stokes动力学和真实世界位势数据任务中，训练速度和预测误差均优于标准ConvLSTM和ConvGRU。

Conclusion: 最小化循环结构与卷积输入聚合结合，为时空序列建模提供了高效且性能优越的替代方案。

Abstract: We introduce MinConvLSTM and MinConvGRU, two novel spatiotemporal models that
combine the spatial inductive biases of convolutional recurrent networks with
the training efficiency of minimal, parallelizable RNNs. Our approach extends
the log-domain prefix-sum formulation of MinLSTM and MinGRU to convolutional
architectures, enabling fully parallel training while retaining localized
spatial modeling. This eliminates the need for sequential hidden state updates
during teacher forcing - a major bottleneck in conventional ConvRNN models. In
addition, we incorporate an exponential gating mechanism inspired by the xLSTM
architecture into the MinConvLSTM, which further simplifies the log-domain
computation. Our models are structurally minimal and computationally efficient,
with reduced parameter count and improved scalability. We evaluate our models
on two spatiotemporal forecasting tasks: Navier-Stokes dynamics and real-world
geopotential data. In terms of training speed, our architectures significantly
outperform standard ConvLSTMs and ConvGRUs. Moreover, our models also achieve
lower prediction errors in both domains, even in closed-loop autoregressive
mode. These findings demonstrate that minimal recurrent structures, when
combined with convolutional input aggregation, offer a compelling and efficient
alternative for spatiotemporal sequence modeling, bridging the gap between
recurrent simplicity and spatial complexity.

</details>


### [109] [Cross-patient Seizure Onset Zone Classification by Patient-Dependent Weight](https://arxiv.org/abs/2508.03635)
*Xuyang Zhao,Hidenori Sugano,Toshihisa Tanaka*

Main category: cs.LG

TL;DR: 提出一种基于患者特异性权重的预训练模型微调方法，以解决癫痫患者数据分布差异导致的跨患者问题，显著提升分类准确性。


<details>
  <summary>Details</summary>
Motivation: 癫痫患者手术需准确定位发作起始区，但依赖专家视觉判断且患者数据分布差异大，机器学习模型在新患者数据上表现不稳定。

Method: 先通过监督学习训练模型，再基于测试患者数据定义与训练数据的相似性权重，最后用权重微调预训练模型。

Result: 实验采用留一法，结果显示每位测试患者的分类准确性平均提升超过10%。

Conclusion: 该方法通过患者特异性权重微调，有效解决了跨患者问题，提升了模型在新患者数据上的性能。

Abstract: Identifying the seizure onset zone (SOZ) in patients with focal epilepsy is
essential for surgical treatment and remains challenging due to its dependence
on visual judgment by clinical experts. The development of machine learning can
assist in diagnosis and has made promising progress. However, unlike data in
other fields, medical data is usually collected from individual patients, and
each patient has different illnesses, physical conditions, and medical
histories, which leads to differences in the distribution of each patient's
data. This makes it difficult for a machine learning model to achieve
consistently reliable performance in every new patient dataset, which we refer
to as the "cross-patient problem." In this paper, we propose a method to
fine-tune a pretrained model using patient-specific weights for every new test
patient to improve diagnostic performance. First, the supervised learning
method is used to train a machine learning model. Next, using the intermediate
features of the trained model obtained through the test patient data, the
similarity between the test patient data and each training patient's data is
defined to determine the weight of each training patient to be used in the
following fine-tuning. Finally, we fine-tune all parameters in the pretrained
model with training data and patient weights. In the experiment, the
leave-one-patient-out method is used to evaluate the proposed method, and the
results show improved classification accuracy for every test patient, with an
average improvement of more than 10%.

</details>


### [110] [Cross-Model Semantics in Representation Learning](https://arxiv.org/abs/2508.03649)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 研究了结构约束（如线性整形算子和校正路径）如何影响不同架构间内部表示的兼容性，发现结构规律性能提高表示几何的稳定性。


<details>
  <summary>Details</summary>
Motivation: 探讨深度网络内部表示对架构选择的敏感性，以及跨模型表示稳定性和可迁移性的问题。

Method: 结合理论分析、实证探测和受控迁移实验，开发了一个测量和分析表示对齐的框架。

Result: 结构规律性使表示几何在架构变化下更稳定，提高了跨模型特征的可互操作性。

Conclusion: 表示可迁移性对模型蒸馏、模块化学习和鲁棒学习系统设计有重要启示。

Abstract: The internal representations learned by deep networks are often sensitive to
architecture-specific choices, raising questions about the stability,
alignment, and transferability of learned structure across models. In this
paper, we investigate how structural constraints--such as linear shaping
operators and corrective paths--affect the compatibility of internal
representations across different architectures. Building on the insights from
prior studies on structured transformations and convergence, we develop a
framework for measuring and analyzing representational alignment across
networks with distinct but related architectural priors. Through a combination
of theoretical insights, empirical probes, and controlled transfer experiments,
we demonstrate that structural regularities induce representational geometry
that is more stable under architectural variation. This suggests that certain
forms of inductive bias not only support generalization within a model, but
also improve the interoperability of learned features across models. We
conclude with a discussion on the implications of representational
transferability for model distillation, modular learning, and the principled
design of robust learning systems.

</details>


### [111] [Efficient Morphology-Aware Policy Transfer to New Embodiments](https://arxiv.org/abs/2508.03660)
*Michael Przystupa,Hongyao Tang,Martin Jagersand,Santiago Miret,Mariano Phielipp,Matthew E. Taylor,Glen Berseth*

Main category: cs.LG

TL;DR: 该论文研究了结合形态感知预训练和参数高效微调（PEFT）技术，以减少将形态感知策略适应目标形态所需的可学习参数。


<details>
  <summary>Details</summary>
Motivation: 形态感知策略在零样本性能上仍不如端到端微调，而进一步数据收集计算成本高，因此需要更高效的微调方法。

Method: 比较了直接调整模型权重子集、输入可学习适配器和前缀调优技术，用于在线微调。

Result: PEFT技术与策略预训练结合，通常能减少改进策略所需的样本量，且仅调整不到1%的参数即可提升性能。

Conclusion: PEFT技术与形态感知预训练结合，能有效提升策略性能，减少计算成本。

Abstract: Morphology-aware policy learning is a means of enhancing policy sample
efficiency by aggregating data from multiple agents. These types of policies
have previously been shown to help generalize over dynamic, kinematic, and limb
configuration variations between agent morphologies. Unfortunately, these
policies still have sub-optimal zero-shot performance compared to end-to-end
finetuning on morphologies at deployment. This limitation has ramifications in
practical applications such as robotics because further data collection to
perform end-to-end finetuning can be computationally expensive. In this work,
we investigate combining morphology-aware pretraining with parameter efficient
finetuning (PEFT) techniques to help reduce the learnable parameters necessary
to specialize a morphology-aware policy to a target embodiment. We compare
directly tuning sub-sets of model weights, input learnable adapters, and prefix
tuning techniques for online finetuning. Our analysis reveals that PEFT
techniques in conjunction with policy pre-training generally help reduce the
number of samples to necessary to improve a policy compared to training models
end-to-end from scratch. We further find that tuning as few as less than 1% of
total parameters will improve policy performance compared the zero-shot
performance of the base pretrained a policy.

</details>


### [112] [Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation](https://arxiv.org/abs/2508.03663)
*Deepak Pandita,Flip Korn,Chris Welty,Christopher M. Homan*

Main category: cs.LG

TL;DR: 论文研究了在固定预算下，如何平衡评估数据中的样本数量（N）和每个样本的标注次数（K），以优化机器学习评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 机器学习评估中的人类标注数据常因预算限制而忽略标注者间的分歧，但分歧对评估结果的影响鲜有研究。

Method: 通过分析多标注数据集和模拟分布，研究在固定预算（N×K）下，最优的（N, K）配置。

Result: 研究发现，考虑人类分歧时，N×K不超过1000即可满足需求，且K>10时效果最佳；不同评估指标对K的敏感度不同。

Conclusion: 研究为机器学习实践者提供了优化评估数据收集的方法，以在预算内获得更高的可靠性。

Abstract: Reproducibility is a cornerstone of scientific validation and of the
authority it confers on its results. Reproducibility in machine learning
evaluations leads to greater trust, confidence, and value. However, the ground
truth responses used in machine learning often necessarily come from humans,
among whom disagreement is prevalent, and surprisingly little research has
studied the impact of effectively ignoring disagreement in these responses, as
is typically the case. One reason for the lack of research is that budgets for
collecting human-annotated evaluation data are limited, and obtaining more
samples from multiple annotators for each example greatly increases the
per-item annotation costs. We investigate the trade-off between the number of
items ($N$) and the number of responses per item ($K$) needed for reliable
machine learning evaluation. We analyze a diverse collection of categorical
datasets for which multiple annotations per item exist, and simulated
distributions fit to these datasets, to determine the optimal $(N, K)$
configuration, given a fixed budget ($N \times K$), for collecting evaluation
data and reliably comparing the performance of machine learning models. Our
findings show, first, that accounting for human disagreement may come with $N
\times K$ at no more than 1000 (and often much lower) for every dataset tested
on at least one metric. Moreover, this minimal $N \times K$ almost always
occurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and
$N$ -- or if one even existed -- depends on the evaluation metric, with metrics
that are more sensitive to the full distribution of responses performing better
at higher levels of $K$. Our methods can be used to help ML practitioners get
more effective test data by finding the optimal metrics and number of items and
annotations per item to collect to get the most reliability for their budget.

</details>


### [113] [A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design](https://arxiv.org/abs/2508.03665)
*Claudiu Leoveanu-Condrei*

Main category: cs.LG

TL;DR: 论文提出了一种通过设计契约（DbC）和类型理论原则为LLM引入契约层的方法，以验证输入输出的语义和类型要求，并通过概率修复确保生成合规。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容缺乏可验证保证的问题，确保其输出符合语义和类型要求。

Method: 引入契约层，规定输入输出的语义和类型要求，并结合概率修复技术引导生成合规内容。

Result: 契约层能够概率性地验证LLM输出，并通过语义和类型条件确保功能等效性。

Conclusion: 契约层为LLM提供了可验证的保证，并提出了功能等效性的新视角。

Abstract: Generative models, particularly Large Language Models (LLMs), produce fluent
outputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and
type-theoretic principles to introduce a contract layer that mediates every LLM
call. Contracts stipulate semantic and type requirements on inputs and outputs,
coupled with probabilistic remediation to steer generation toward compliance.
The layer exposes the dual view of LLMs as semantic parsers and probabilistic
black-box components. Contract satisfaction is probabilistic and semantic
validation is operationally defined through programmer-specified conditions on
well-typed data structures. More broadly, this work postulates that any two
agents satisfying the same contracts are \emph{functionally equivalent} with
respect to those contracts.

</details>


### [114] [Self-Questioning Language Models](https://arxiv.org/abs/2508.03682)
*Lili Chen,Mihir Prabhudesai,Katerina Fragkiadaki,Hao Liu,Deepak Pathak*

Main category: cs.LG

TL;DR: 论文提出了一种名为SQLM的自我提问语言模型框架，通过不对称自我博弈生成问题和答案，无需外部数据即可提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索预训练语言模型是否仅通过生成自己的问题和答案就能提升推理能力，而无需依赖外部数据。

Method: 采用不对称自我博弈框架，包括提问者和解答者，通过强化学习训练，提问者生成问题，解答者尝试回答，奖励机制基于问题难度和解答正确性。

Result: 在三位数乘法、OMEGA代数问题和Codeforces编程问题三个基准测试中，模型通过自我生成问题提升了性能。

Conclusion: 语言模型可以通过自我生成问题和解答持续改进，无需依赖外部训练数据。

Abstract: Can large language models improve without external data -- by generating
their own questions and answers? We hypothesize that a pre-trained language
model can improve its reasoning skills given only a single prompt specifying
the topic (e.g., algebra word problems) and asking the model to generate its
own questions. To do this, we propose Self-Questioning Language Models (SQLM):
an asymmetric self-play framework where a proposer is given the topic and
generates a question for a solver, who tries to answer it. Both the proposer
and solver are trained via reinforcement learning. The proposer receives a
reward if the problem is not too easy or too difficult, and the solver receives
a reward based on majority voting, a proxy for correctness in the absence of
ground-truth answers. For coding, the proposer can instead generate unit tests
which are used for verification. We study this asymmetric self-play framework
on three benchmarks: three-digit multiplication, algebra problems from the
OMEGA benchmark, and programming problems from Codeforces. By continually
generating more interesting problems and attempting to solve them, language
models can improve on downstream benchmarks without access to any curated
training datasets.

</details>


### [115] [No LLM Solved Yu Tsumura's 554th Problem](https://arxiv.org/abs/2508.03685)
*Simon Frieder,William Hart*

Main category: cs.LG

TL;DR: 论文指出，尽管LLM在解决问题方面表现乐观，但Yu Tsumura的第554题是一个现有LLM无法解决的IMO级别问题。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM在解决特定数学问题（如Yu Tsumura的第554题）时的局限性，尽管其训练数据中可能包含该问题的解答。

Method: 通过分析Yu Tsumura的第554题，比较其与IMO问题的复杂度、所需证明技巧及LLM的表现。

Result: 发现现有LLM无法解决该问题，尽管其符合IMO问题的难度范围且训练数据中可能包含解答。

Conclusion: LLM在解决某些数学问题时仍存在显著局限性，需进一步改进。

Abstract: We show, contrary to the optimism about LLM's problem-solving abilities,
fueled by the recent gold medals that were attained, that a problem exists --
Yu Tsumura's 554th problem -- that a) is within the scope of an IMO problem in
terms of proof sophistication, b) is not a combinatorics problem which has
caused issues for LLMs, c) requires fewer proof techniques than typical hard
IMO problems, d) has a publicly available solution (likely in the training data
of LLMs), and e) that cannot be readily solved by any existing off-the-shelf
LLM (commercial or open-source).

</details>


### [116] [PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning](https://arxiv.org/abs/2508.03693)
*Ondrej Bajgar,Dewi S. W. Gould,Jonathon Liu,Alessandro Abate,Konstantinos Gatsis,Michael A. Osborne*

Main category: cs.LG

TL;DR: 论文提出了一种名为PAC-EIG的信息论获取函数，用于主动逆强化学习（IRL），以在有限状态-动作空间中为目标策略提供PAC保证。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统自主性增强，确保其决策与人类偏好一致至关重要。传统IRL方法需要大量人类演示以确保可靠性，但成本高昂。

Method: 提出PAC-EIG和Reward-EIG两种信息论获取函数，前者针对策略的遗憾最大化信息增益，后者专注于奖励学习。

Result: 理论证明了收敛性边界，并通过实验展示了方法的优势，同时揭示了先前启发式方法的失败模式。

Conclusion: PAC-EIG为主动IRL提供了首个理论保证，有效减少了所需的人类演示数量，提升了策略的可靠性。

Abstract: As AI systems become increasingly autonomous, reliably aligning their
decision-making to human preferences is essential. Inverse reinforcement
learning (IRL) offers a promising approach to infer preferences from
demonstrations. These preferences can then be used to produce an apprentice
policy that performs well on the demonstrated task. However, in domains like
autonomous driving or robotics, where errors can have serious consequences, we
need not just good average performance but reliable policies with formal
guarantees -- yet obtaining sufficient human demonstrations for reliability
guarantees can be costly. Active IRL addresses this challenge by strategically
selecting the most informative scenarios for human demonstration. We introduce
PAC-EIG, an information-theoretic acquisition function that directly targets
probably-approximately-correct (PAC) guarantees for the learned policy --
providing the first such theoretical guarantee for active IRL with noisy expert
demonstrations. Our method maximises information gain about the regret of the
apprentice policy, efficiently identifying states requiring further
demonstration. We also present Reward-EIG as an alternative when learning the
reward itself is the primary objective. Focusing on finite state-action spaces,
we prove convergence bounds, illustrate failure modes of prior heuristic
methods, and demonstrate our method's advantages experimentally.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [117] [Hedging with memory: shallow and deep learning with signatures](https://arxiv.org/abs/2508.02759)
*Eduardo Abi Jaber,Louis-Amand Gérard*

Main category: stat.ML

TL;DR: 论文研究了路径签名在机器学习中对非马尔可夫随机波动率模型下对冲奇异衍生品的应用，展示了其在深度学习和浅层学习中的优势。


<details>
  <summary>Details</summary>
Motivation: 探索路径签名作为特征在机器学习中的有效性，特别是在非马尔可夫随机波动率模型下对冲奇异衍生品的问题。

Method: 在深度学习中，将路径签名作为前馈神经网络的输入特征；在浅层学习中，比较了两种回归方法：一种直接从价格过程的期望签名学习对冲策略，另一种通过签名波动率模型建模波动率动态。

Result: 路径签名在深度学习中表现优于LSTM，且训练计算量显著减少；在浅层学习中，基于签名波动率模型的方法在不同收益和波动动态下表现更准确和稳定。

Conclusion: 路径签名在机器学习中对冲奇异衍生品时具有高效性和稳定性，尤其是在非马尔可夫随机波动率模型中。

Abstract: We investigate the use of path signatures in a machine learning context for
hedging exotic derivatives under non-Markovian stochastic volatility models. In
a deep learning setting, we use signatures as features in feedforward neural
networks and show that they outperform LSTMs in most cases, with orders of
magnitude less training compute. In a shallow learning setting, we compare two
regression approaches: the first directly learns the hedging strategy from the
expected signature of the price process; the second models the dynamics of
volatility using a signature volatility model, calibrated on the expected
signature of the volatility. Solving the hedging problem in the calibrated
signature volatility model yields more accurate and stable results across
different payoffs and volatility dynamics.

</details>


### [118] [A Dual Optimization View to Empirical Risk Minimization with f-Divergence Regularization](https://arxiv.org/abs/2508.03314)
*Francisco Daunas,Iñaki Esnaola,Samir M. Perlaza*

Main category: stat.ML

TL;DR: 论文介绍了基于f-散度正则化的经验风险最小化（ERM-fDR）的对偶形式，并通过Legendre-Fenchel变换和隐函数定理，给出了归一化函数的非线性ODE表达式及其高效计算方法。


<details>
  <summary>Details</summary>
Motivation: 研究ERM-fDR的对偶形式，旨在提供一种计算高效的归一化函数求解方法。

Method: 利用Legendre-Fenchel变换和隐函数定理，推导出归一化函数的非线性ODE表达式。

Result: 在温和条件下，提出了一种计算高效的归一化函数求解方法。

Conclusion: 该对偶方法为ERM-fDR的归一化函数提供了理论支持和高效计算途径。

Abstract: The dual formulation of empirical risk minimization with f-divergence
regularization (ERM-fDR) is introduced. The solution of the dual optimization
problem to the ERM-fDR is connected to the notion of normalization function
introduced as an implicit function. This dual approach leverages the
Legendre-Fenchel transform and the implicit function theorem to provide a
nonlinear ODE expression to the normalization function. Furthermore, the
nonlinear ODE expression and its properties provide a computationally efficient
method to calculate the normalization function of the ERM-fDR solution under a
mild condition.

</details>


### [119] [Supervised Dynamic Dimension Reduction with Deep Neural Network](https://arxiv.org/abs/2508.03546)
*Zhanye Luo,Yuefeng Han,Xiufan Yu*

Main category: stat.ML

TL;DR: 提出了一种新的监督深度动态主成分分析（SDDP）框架，用于高维预测变量的时间序列预测改进。通过结合目标变量和滞后观测值，提取目标感知因子，提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决高维预测变量在时间序列预测中的降维问题，提升预测准确性和因子可解释性。

Method: 提出SDDP框架，结合目标变量和滞后观测值，通过时间神经网络构建目标感知预测变量，进行主成分分析提取因子。

Result: 在多个真实数据集上验证，SDDP显著优于现有方法，提高了预测准确性。

Conclusion: SDDP不仅提升了预测性能，还提供了更具解释性的目标特定因子，适用于更广泛的预测场景。

Abstract: This paper studies the problem of dimension reduction, tailored to improving
time series forecasting with high-dimensional predictors. We propose a novel
Supervised Deep Dynamic Principal component analysis (SDDP) framework that
incorporates the target variable and lagged observations into the factor
extraction process. Assisted by a temporal neural network, we construct
target-aware predictors by scaling the original predictors in a supervised
manner, with larger weights assigned to predictors with stronger forecasting
power. A principal component analysis is then performed on the target-aware
predictors to extract the estimated SDDP factors. This supervised factor
extraction not only improves predictive accuracy in the downstream forecasting
task but also yields more interpretable and target-specific latent factors.
Building upon SDDP, we propose a factor-augmented nonlinear dynamic forecasting
model that unifies a broad family of factor-model-based forecasting approaches.
To further demonstrate the broader applicability of SDDP, we extend our studies
to a more challenging scenario when the predictors are only partially
observable. We validate the empirical performance of the proposed method on
several real-world public datasets. The results show that our algorithm
achieves notable improvements in forecasting accuracy compared to
state-of-the-art methods.

</details>


### [120] [Likelihood Matching for Diffusion Models](https://arxiv.org/abs/2508.03636)
*Lei Qian,Wu Su,Yanqi Huang,Song Xi Chen*

Main category: stat.ML

TL;DR: 提出了一种基于似然匹配的方法训练扩散模型，通过匹配目标数据分布的似然与反向扩散路径的似然，利用高斯分布近似反向转移密度，估计得分和Hessian函数，并引入随机采样器提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中反向扩散路径的似然计算问题，提高生成模型的效率和准确性。

Method: 通过高斯分布近似反向转移密度，匹配条件均值和协方差，估计得分和Hessian函数，并引入随机采样器优化计算。

Result: 理论证明了准最大似然估计的一致性，提供了非渐近收敛保证，实验验证了方法的有效性。

Conclusion: 提出的似然匹配方法在理论和实验上均表现出色，为扩散模型的训练提供了新思路。

Abstract: We propose a Likelihood Matching approach for training diffusion models by
first establishing an equivalence between the likelihood of the target data
distribution and a likelihood along the sample path of the reverse diffusion.
To efficiently compute the reverse sample likelihood, a quasi-likelihood is
considered to approximate each reverse transition density by a Gaussian
distribution with matched conditional mean and covariance, respectively. The
score and Hessian functions for the diffusion generation are estimated by
maximizing the quasi-likelihood, ensuring a consistent matching of both the
first two transitional moments between every two time points. A stochastic
sampler is introduced to facilitate computation that leverages on both the
estimated score and Hessian information. We establish consistency of the
quasi-maximum likelihood estimation, and provide non-asymptotic convergence
guarantees for the proposed sampler, quantifying the rates of the approximation
errors due to the score and Hessian estimation, dimensionality, and the number
of diffusion steps. Empirical and simulation evaluations demonstrate the
effectiveness of the proposed Likelihood Matching and validate the theoretical
results.

</details>


### [121] [Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws](https://arxiv.org/abs/2508.03688)
*Gérard Ben Arous,Murat A. Erdogdu,N. Mert Vural,Denny Wu*

Main category: stat.ML

TL;DR: 论文研究了高维环境下基于梯度的两层神经网络训练的优化和样本复杂度，使用二次激活函数，分析了SGD动态的预测风险标度规律。


<details>
  <summary>Details</summary>
Motivation: 探索在高维数据生成和广泛宽度模型下，梯度训练的性能和样本需求，特别是针对特征学习机制。

Method: 结合矩阵Riccati微分方程的精确描述和新的矩阵单调性论证，分析无限维有效动态的收敛性。

Result: 推导了预测风险对优化时间、样本量和模型宽度的幂律依赖关系。

Conclusion: 研究为高维神经网络训练提供了理论支持，揭示了特征学习机制下的标度规律。

Abstract: We study the optimization and sample complexity of gradient-based training of
a two-layer neural network with quadratic activation function in the
high-dimensional regime, where the data is generated as $y \propto
\sum_{j=1}^{r}\lambda_j \sigma\left(\langle \boldsymbol{\theta_j},
\boldsymbol{x}\rangle\right), \boldsymbol{x} \sim N(0,\boldsymbol{I}_d)$,
$\sigma$ is the 2nd Hermite polynomial, and $\lbrace\boldsymbol{\theta}_j
\rbrace_{j=1}^{r} \subset \mathbb{R}^d$ are orthonormal signal directions. We
consider the extensive-width regime $r \asymp d^\beta$ for $\beta \in [0, 1)$,
and assume a power-law decay on the (non-negative) second-layer coefficients
$\lambda_j\asymp j^{-\alpha}$ for $\alpha \geq 0$. We present a sharp analysis
of the SGD dynamics in the feature learning regime, for both the population
limit and the finite-sample (online) discretization, and derive scaling laws
for the prediction risk that highlight the power-law dependencies on the
optimization time, sample size, and model width. Our analysis combines a
precise characterization of the associated matrix Riccati differential equation
with novel matrix monotonicity arguments to establish convergence guarantees
for the infinite-dimensional effective dynamics.

</details>
