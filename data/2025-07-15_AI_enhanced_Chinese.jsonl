{"id": "2507.08906", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH", "62G05, 68Q32, 62P30, 35Q62, 68T01", "G.3; G.4; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.08906", "abs": "https://arxiv.org/abs/2507.08906", "authors": ["Nathan Doum\u00e8che"], "title": "Physics-informed machine learning: A mathematical framework with applications to time series forecasting", "comment": "Doctoral thesis, Sorbonne University. 286 pages", "summary": "Physics-informed machine learning (PIML) is an emerging framework that\nintegrates physical knowledge into machine learning models. This physical prior\noften takes the form of a partial differential equation (PDE) system that the\nregression function must satisfy. In the first part of this dissertation, we\nanalyze the statistical properties of PIML methods. In particular, we study the\nproperties of physics-informed neural networks (PINNs) in terms of\napproximation, consistency, overfitting, and convergence. We then show how PIML\nproblems can be framed as kernel methods, making it possible to apply the tools\nof kernel ridge regression to better understand their behavior. In addition, we\nuse this kernel formulation to develop novel physics-informed algorithms and\nimplement them efficiently on GPUs. The second part explores industrial\napplications in forecasting energy signals during atypical periods. We present\nresults from the Smarter Mobility challenge on electric vehicle charging\noccupancy and examine the impact of mobility on electricity demand. Finally, we\nintroduce a physics-constrained framework for designing and enforcing\nconstraints in time series, applying it to load forecasting and tourism\nforecasting in various countries.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\uff08PIML\uff09\u7684\u7edf\u8ba1\u7279\u6027\u53ca\u5176\u5de5\u4e1a\u5e94\u7528\uff0c\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u6848\u4f8b\u3002", "motivation": "\u7814\u7a76PIML\u65b9\u6cd5\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u95ee\u9898\uff0c\u5982\u80fd\u6e90\u4fe1\u53f7\u9884\u6d4b\u548c\u65f6\u95f4\u5e8f\u5217\u7ea6\u675f\u8bbe\u8ba1\u3002", "method": "\u5206\u6790\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u8fd1\u4f3c\u6027\u3001\u4e00\u81f4\u6027\u3001\u8fc7\u62df\u5408\u548c\u6536\u655b\u6027\uff0c\u5e76\u5c06\u5176\u95ee\u9898\u6846\u67b6\u5316\u4e3a\u6838\u65b9\u6cd5\uff1b\u5f00\u53d1\u4e86\u9ad8\u6548\u7684GPU\u5b9e\u73b0\u7b97\u6cd5\u3002", "result": "\u5c55\u793a\u4e86PIML\u5728\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u5360\u7528\u7387\u548c\u7535\u529b\u9700\u6c42\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u65f6\u95f4\u5e8f\u5217\u7ea6\u675f\u6846\u67b6\u3002", "conclusion": "PIML\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2507.08922", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08922", "abs": "https://arxiv.org/abs/2507.08922", "authors": ["Tameem Adel"], "title": "The Bayesian Approach to Continual Learning: An Overview", "comment": null, "summary": "Continual learning is an online paradigm where a learner continually\naccumulates knowledge from different tasks encountered over sequential time\nsteps. Importantly, the learner is required to extend and update its knowledge\nwithout forgetting about the learning experience acquired from the past, and\nwhile avoiding the need to retrain from scratch. Given its sequential nature\nand its resemblance to the way humans think, continual learning offers an\nopportunity to address several challenges which currently stand in the way of\nwidening the range of applicability of deep models to further real-world\nproblems. The continual need to update the learner with data arriving\nsequentially strikes inherent congruence between continual learning and\nBayesian inference which provides a principal platform to keep updating the\nprior beliefs of a model given new data, without completely forgetting the\nknowledge acquired from the old data. This survey inspects different settings\nof Bayesian continual learning, namely task-incremental learning and\nclass-incremental learning. We begin by discussing definitions of continual\nlearning along with its Bayesian setting, as well as the links with related\nfields, such as domain adaptation, transfer learning and meta-learning.\nAfterwards, we introduce a taxonomy offering a comprehensive categorization of\nalgorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we\nanalyze the state-of-the-art while zooming in on some of the most prominent\nBayesian continual learning algorithms to date. Furthermore, we shed some light\non links between continual learning and developmental psychology, and\ncorrespondingly introduce analogies between both fields. We follow that with a\ndiscussion of current challenges, and finally conclude with potential areas for\nfuture research on Bayesian continual learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u8d1d\u53f6\u65af\u6301\u7eed\u5b66\u4e60\u7684\u4e0d\u540c\u8bbe\u7f6e\uff0c\u5305\u62ec\u4efb\u52a1\u589e\u91cf\u5b66\u4e60\u548c\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\uff0c\u63a2\u8ba8\u4e86\u5176\u5b9a\u4e49\u3001\u76f8\u5173\u9886\u57df\u8054\u7cfb\u3001\u7b97\u6cd5\u5206\u7c7b\u53ca\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u80fd\u591f\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u65b9\u5f0f\uff0c\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u73b0\u5b9e\u95ee\u9898\u4e2d\u7684\u6269\u5c55\u6027\u6311\u6218\uff0c\u800c\u8d1d\u53f6\u65af\u63a8\u7406\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u65b0\u77e5\u8bc6\u800c\u4e0d\u9057\u5fd8\u65e7\u6570\u636e\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u7efc\u8ff0\u8d1d\u53f6\u65af\u6301\u7eed\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u5206\u6790\u5176\u4e0e\u9886\u57df\u9002\u5e94\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u5143\u5b66\u4e60\u7684\u8054\u7cfb\uff0c\u5e76\u63a2\u8ba8\u5fc3\u7406\u5b66\u7c7b\u6bd4\u3002", "result": "\u63d0\u51fa\u4e86\u8d1d\u53f6\u65af\u6301\u7eed\u5b66\u4e60\u7684\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u4e0e\u5fc3\u7406\u5b66\u9886\u57df\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "conclusion": "\u8d1d\u53f6\u65af\u6301\u7eed\u5b66\u4e60\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u7b97\u6cd5\u4f18\u5316\u548c\u5e94\u7528\u6269\u5c55\u3002"}}
{"id": "2507.08994", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08994", "abs": "https://arxiv.org/abs/2507.08994", "authors": ["Joseph Lazzaro", "Ciara Pike-Burke"], "title": "Fixed-Confidence Multiple Change Point Identification under Bandit Feedback", "comment": "ICML 2025", "summary": "Piecewise constant functions describe a variety of real-world phenomena in\ndomains ranging from chemistry to manufacturing. In practice, it is often\nrequired to confidently identify the locations of the abrupt changes in these\nfunctions as quickly as possible. For this, we introduce a fixed-confidence\npiecewise constant bandit problem. Here, we sequentially query points in the\ndomain and receive noisy evaluations of the function under bandit feedback. We\nprovide instance-dependent lower bounds for the complexity of change point\nidentification in this problem. These lower bounds illustrate that an optimal\nmethod should focus its sampling efforts adjacent to each of the change points,\nand the number of samples around each change point should be inversely\nproportional to the magnitude of the change. Building on this, we devise a\nsimple and computationally efficient variant of Track-and-Stop and prove that\nit is asymptotically optimal in many regimes. We support our theoretical\nfindings with experimental results in synthetic environments demonstrating the\nefficiency of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u7684\u5206\u6bb5\u5e38\u6570\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u7528\u4e8e\u5feb\u901f\u8bc6\u522b\u51fd\u6570\u7a81\u53d8\u70b9\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u65b9\u6cd5\u7684\u6e10\u8fd1\u6700\u4f18\u6027\u3002", "motivation": "\u5206\u6bb5\u5e38\u6570\u51fd\u6570\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u63cf\u8ff0\u73b0\u5b9e\u73b0\u8c61\uff0c\u9700\u8981\u5feb\u901f\u51c6\u786e\u5730\u8bc6\u522b\u5176\u7a81\u53d8\u70b9\u3002", "method": "\u5f15\u5165\u56fa\u5b9a\u7f6e\u4fe1\u5ea6\u7684\u5206\u6bb5\u5e38\u6570\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eTrack-and-Stop\u7684\u7b80\u5355\u9ad8\u6548\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u65b9\u6cd5\u5728\u591a\u79cd\u60c5\u51b5\u4e0b\u6e10\u8fd1\u6700\u4f18\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a81\u53d8\u70b9\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09093", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09093", "abs": "https://arxiv.org/abs/2507.09093", "authors": ["Aleksandar Armacki", "Dragana Bajovic", "Dusan Jakovetic", "Soummya Kar"], "title": "Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization", "comment": "38 pages, 1 figure", "summary": "We study convergence in high-probability of SGD-type methods in non-convex\noptimization and the presence of heavy-tailed noise. To combat the heavy-tailed\nnoise, a general black-box nonlinear framework is considered, subsuming\nnonlinearities like sign, clipping, normalization and their smooth\ncounterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the\nrate $\\widetilde{\\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments\nand a symmetric probability density function (PDF). Crucially, N-SGD has\nexponentially decaying tails, matching the performance of linear SGD under\nlight-tailed noise. To handle non-symmetric noise, we propose two novel\nestimators, based on the idea of noise symmetrization. The first, dubbed\nSymmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any\nreference point is available at the start of training, while the second, dubbed\nMini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.\nCombined with the nonlinear framework, we get N-SGE and N-MSGE methods,\nrespectively, both achieving the same convergence rate and exponentially\ndecaying tails as N-SGD, while allowing for non-symmetric noise with unbounded\nmoments and PDF satisfying a mild technical condition, with N-MSGE additionally\nrequiring bounded noise moment of order $p \\in (1,2]$. Compared to works\nassuming noise with bounded $p$-th moment, our results: 1) are based on a novel\nsymmetrization approach; 2) provide a unified framework and relaxed moment\nconditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly\nbetter than existing works when $p < 2$, while the complexity of N-MSGE is\nclose to existing works. Compared to works assuming symmetric noise with\nunbounded moments, we: 1) provide a sharper analysis and improved rates; 2)\nfacilitate state-dependent symmetric noise; 3) extend the strong guarantees to\nnon-symmetric noise.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u975e\u51f8\u4f18\u5316\u548c\u91cd\u5c3e\u566a\u58f0\u4e0bSGD\u7c7b\u65b9\u6cd5\u7684\u9ad8\u6982\u7387\u6536\u655b\u6027\uff0c\u63d0\u51fa\u975e\u7ebf\u6027\u6846\u67b6\u548c\u5bf9\u79f0\u5316\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u6539\u8fdb\u4e86\u6536\u655b\u901f\u7387\u548c\u9002\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u91cd\u5c3e\u566a\u58f0\u548c\u975e\u5bf9\u79f0\u566a\u58f0\u5bf9SGD\u7c7b\u65b9\u6cd5\u6536\u655b\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u66f4\u901a\u7528\u7684\u975e\u7ebf\u6027\u6846\u67b6\u548c\u5bf9\u79f0\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u975e\u7ebf\u6027SGD\uff08N-SGD\uff09\u548c\u4e24\u79cd\u5bf9\u79f0\u5316\u68af\u5ea6\u4f30\u8ba1\u5668\uff08SGE\u548cMSGE\uff09\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u6846\u67b6\u5904\u7406\u566a\u58f0\u3002", "result": "N-SGD\u3001N-SGE\u548cN-MSGE\u5728\u975e\u5bf9\u79f0\u566a\u58f0\u4e0b\u5747\u8fbe\u5230$\\widetilde{\\mathcal{O}}(t^{-1/2})$\u6536\u655b\u901f\u7387\uff0c\u4e14\u5177\u6709\u6307\u6570\u8870\u51cf\u5c3e\u90e8\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u79f0\u5316\u65b9\u6cd5\u548c\u975e\u7ebf\u6027\u6846\u67b6\uff0c\u663e\u8457\u6269\u5c55\u4e86SGD\u7c7b\u65b9\u6cd5\u7684\u9002\u7528\u6027\uff0c\u6539\u8fdb\u4e86\u6536\u655b\u901f\u7387\u548c\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.08828", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08828", "abs": "https://arxiv.org/abs/2507.08828", "authors": ["Tarek Berghout"], "title": "Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning", "comment": null, "summary": "This paper introduces Recurrent Expansion (RE) as a new learning paradigm\nthat advances beyond conventional Machine Learning (ML) and Deep Learning (DL).\nWhile DL focuses on learning from static data representations, RE proposes an\nadditional dimension: learning from the evolving behavior of models themselves.\nRE emphasizes multiple mappings of data through identical deep architectures\nand analyzes their internal representations (i.e., feature maps) in conjunction\nwith observed performance signals such as loss. By incorporating these\nbehavioral traces, RE enables iterative self-improvement, allowing each model\nversion to gain insight from its predecessors. The framework is extended\nthrough Multiverse RE (MVRE), which aggregates signals from parallel model\ninstances, and further through Heterogeneous MVRE (HMVRE), where models of\nvarying architectures contribute diverse perspectives. A scalable and adaptive\nvariant, Sc-HMVRE, introduces selective mechanisms and scale diversity for\nreal-world deployment. Altogether, RE presents a shift in DL: from purely\nrepresentational learning to behavior-aware, self-evolving systems. It lays the\ngroundwork for a new class of intelligent models capable of reasoning over\ntheir own learning dynamics, offering a path toward scalable, introspective,\nand adaptive artificial intelligence. A simple code example to support\nbeginners in running their own experiments is provided in Code Availability\nSection of this paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRecurrent Expansion (RE)\u7684\u65b0\u5b66\u4e60\u8303\u5f0f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u81ea\u8eab\u884c\u4e3a\u7684\u6f14\u5316\u6765\u5b9e\u73b0\u81ea\u6211\u8fed\u4ee3\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u6570\u636e\u8868\u793a\uff0c\u800cRE\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u884c\u4e3a\u7684\u52a8\u6001\u5206\u6790\u5b9e\u73b0\u66f4\u667a\u80fd\u7684\u5b66\u4e60\u7cfb\u7edf\u3002", "method": "RE\u901a\u8fc7\u591a\u6b21\u6620\u5c04\u6570\u636e\u5e76\u5206\u6790\u5185\u90e8\u8868\u793a\u4e0e\u6027\u80fd\u4fe1\u53f7\uff08\u5982\u635f\u5931\uff09\u6765\u8fed\u4ee3\u6539\u8fdb\u6a21\u578b\u3002\u6269\u5c55\u7248\u672c\u5305\u62ecMultiverse RE (MVRE)\u548cHeterogeneous MVRE (HMVRE)\uff0c\u540e\u8005\u8fdb\u4e00\u6b65\u5f15\u5165\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u7684\u591a\u6837\u6027\u3002", "result": "RE\u53ca\u5176\u53d8\u4f53\uff08\u5982Sc-HMVRE\uff09\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5f15\u5165\u4e86\u884c\u4e3a\u611f\u77e5\u548c\u81ea\u6211\u6f14\u5316\u7684\u80fd\u529b\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u81ea\u9002\u5e94\u7684AI\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "RE\u6807\u5fd7\u7740\u6df1\u5ea6\u5b66\u4e60\u4ece\u9759\u6001\u8868\u793a\u5b66\u4e60\u5411\u884c\u4e3a\u611f\u77e5\u548c\u81ea\u6211\u6f14\u5316\u7cfb\u7edf\u7684\u8f6c\u53d8\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.08974", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.08974", "abs": "https://arxiv.org/abs/2507.08974", "authors": ["Thien Hieu Hoang", "Tri Nhu Do", "Georges Kaddoum"], "title": "Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM", "comment": null, "summary": "Accurate channel estimation is crucial for the improvement of signal\nprocessing performance in wireless communications. However, traditional\nmodel-based methods frequently experience difficulties in dynamic environments.\nSimilarly, alternative machine-learning approaches typically lack\ngeneralization across different datasets due to variations in channel\ncharacteristics. To address this issue, in this study, we propose a novel\ndomain adaptation approach to bridge the gap between the quasi-static channel\nmodel (QSCM) and the map-based channel model (MBCM). Specifically, we first\nproposed a channel estimation pipeline that takes into account realistic\nchannel simulation to train our foundation model. Then, we proposed domain\nadaptation methods to address the estimation problem. Using simulation-based\ntraining to reduce data requirements for effective application in practical\nwireless environments, we find that the proposed strategy enables robust model\nperformance, even with limited true channel information.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u4e2d\u52a8\u6001\u73af\u5883\u4e0b\u7684\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u8bad\u7ec3\u548c\u57df\u9002\u5e94\u6280\u672f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u56e0\u4fe1\u9053\u7279\u6027\u5dee\u5f02\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u62df\u8bad\u7ec3\u7684\u4fe1\u9053\u4f30\u8ba1\u6d41\u7a0b\u548c\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u51cf\u5c11\u5b9e\u9645\u6570\u636e\u9700\u6c42\u3002", "result": "\u6a21\u578b\u5728\u6709\u9650\u771f\u5b9e\u4fe1\u9053\u4fe1\u606f\u4e0b\u4ecd\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u4fe1\u9053\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08821", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08821", "abs": "https://arxiv.org/abs/2507.08821", "authors": ["Pedro D. Alvim", "Hugerles S. Silva", "Ugo S. Dias", "Osamah S. Badarneh", "Felipe A. P. Figueiredo", "Rausley A. A. de Souza"], "title": "LNN-powered Fluid Antenna Multiple Access", "comment": null, "summary": "Fluid antenna systems represent an innovative approach in wireless\ncommunication, recently applied in multiple access to optimize the\nsignal-to-interference-plus-noise ratio through port selection. This letter\nframes the port selection problem as a multi-label classification task for the\nfirst time, improving best-port selection with limited port observations. We\naddress this challenge by leveraging liquid neural networks (LNNs) to predict\nthe optimal port under emerging fluid antenna multiple access scenarios\nalongside a more general $\\alpha$-$\\mu$ fading model. We also apply\nhyperparameter optimization to refine LNN architectures for different\nobservation scenarios. Our approach yields lower outage probability values than\nexisting methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6db2\u6001\u795e\u7ecf\u7f51\u7edc\uff08LNNs\uff09\u7684\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u7aef\u53e3\u9009\u62e9\uff0c\u964d\u4f4e\u4e2d\u65ad\u6982\u7387\u3002", "motivation": "\u89e3\u51b3\u6d41\u4f53\u5929\u7ebf\u591a\u63a5\u5165\u573a\u666f\u4e0b\u7aef\u53e3\u9009\u62e9\u95ee\u9898\uff0c\u63d0\u5347\u4fe1\u53f7\u5e72\u6270\u566a\u58f0\u6bd4\u3002", "method": "\u5c06\u7aef\u53e3\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\uff0c\u5229\u7528LNNs\u9884\u6d4b\u6700\u4f18\u7aef\u53e3\uff0c\u5e76\u7ed3\u5408\u03b1-\u03bc\u8870\u843d\u6a21\u578b\u548c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u4e2d\u65ad\u6982\u7387\u3002", "conclusion": "LNNs\u5728\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\u4e2d\u80fd\u6709\u6548\u4f18\u5316\u7aef\u53e3\u9009\u62e9\uff0c\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.09103", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09103", "abs": "https://arxiv.org/abs/2507.09103", "authors": ["Gianluigi Silvestri", "Luca Ambrogioni"], "title": "CoVAE: Consistency Training of Variational Autoencoders", "comment": null, "summary": "Current state-of-the-art generative approaches frequently rely on a two-stage\ntraining procedure, where an autoencoder (often a VAE) first performs\ndimensionality reduction, followed by training a generative model on the\nlearned latent space. While effective, this introduces computational overhead\nand increased sampling times. We challenge this paradigm by proposing\nConsistency Training of Variational AutoEncoders (CoVAE), a novel single-stage\ngenerative autoencoding framework that adopts techniques from consistency\nmodels to train a VAE architecture. The CoVAE encoder learns a progressive\nseries of latent representations with increasing encoding noise levels,\nmirroring the forward processes of diffusion and flow matching models. This\nsequence of representations is regulated by a time dependent $\\beta$ parameter\nthat scales the KL loss. The decoder is trained using a consistency loss with\nvariational regularization, which reduces to a conventional VAE loss at the\nearliest latent time. We show that CoVAE can generate high-quality samples in\none or few steps without the use of a learned prior, significantly\noutperforming equivalent VAEs and other single-stage VAEs methods. Our approach\nprovides a unified framework for autoencoding and diffusion-style generative\nmodeling and provides a viable route for one-step generative high-performance\nautoencoding. Our code is publicly available at\nhttps://github.com/gisilvs/covae.", "AI": {"tldr": "CoVAE\u662f\u4e00\u79cd\u5355\u9636\u6bb5\u751f\u6210\u81ea\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u8bad\u7ec3VAE\u67b6\u6784\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u91c7\u6837\u65f6\u95f4\u589e\u52a0\uff0c\u80fd\u4e00\u6b65\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u4e24\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u91c7\u6837\u6548\u7387\u95ee\u9898\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u5355\u9636\u6bb5\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4e00\u81f4\u6027\u6a21\u578b\u6280\u672f\u8bad\u7ec3VAE\uff0c\u7f16\u7801\u5668\u5b66\u4e60\u6e10\u8fdb\u5f0f\u6f5c\u5728\u8868\u793a\uff0c\u89e3\u7801\u5668\u4f7f\u7528\u4e00\u81f4\u6027\u635f\u5931\u548c\u53d8\u5206\u6b63\u5219\u5316\u3002", "result": "CoVAE\u5728\u4e00\u6b65\u6216\u5c11\u91cf\u6b65\u9aa4\u5185\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfVAE\u548c\u5176\u4ed6\u5355\u9636\u6bb5\u65b9\u6cd5\u3002", "conclusion": "CoVAE\u4e3a\u81ea\u7f16\u7801\u548c\u6269\u6563\u5f0f\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u662f\u9ad8\u6548\u4e00\u6b65\u751f\u6210\u7684\u6709\u529b\u65b9\u6848\u3002"}}
{"id": "2507.08829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08829", "abs": "https://arxiv.org/abs/2507.08829", "authors": ["Kimia Soroush", "Nastaran Shirazi", "Mohsen Raji"], "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI", "comment": null, "summary": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains,\nwhere ensuring their reliability is essential. Triple Modular Redundancy (TMR)\nis an effective technique to enhance the reliability of DNNs in the presence of\nbit-flip faults. In order to handle the significant overhead of TMR, it is\napplied selectively on the parameters and components with the highest\ncontribution at the model output. Hence, the accuracy of the selection\ncriterion plays the key role on the efficiency of TMR. This paper presents an\nefficient TMR approach to enhance the reliability of DNNs against bit-flip\nfaults using an Explainable Artificial Intelligence (XAI) method. Since XAI can\nprovide valuable insights about the importance of individual neurons and\nweights in the performance of the network, they can be applied as the selection\nmetric in TMR techniques. The proposed method utilizes a low-cost,\ngradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to\ncalculate importance scores for DNN parameters. These scores are then used to\nenhance the reliability of the model, with the most critical weights being\nprotected by TMR. The proposed approach is evaluated on two DNN models, VGG16\nand AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate\nthat the method can protect the AlexNet model at a bit error rate of 10-4,\nachieving over 60% reliability improvement while maintaining the same overhead\nas state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u9ad8\u6548\u4e09\u91cd\u6a21\u5757\u5197\u4f59\uff08TMR\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u6bd4\u7279\u7ffb\u8f6c\u6545\u969c\u4e0b\u7684\u53ef\u9760\u6027\u3002\u901a\u8fc7\u68af\u5ea6\u9a71\u52a8\u7684XAI\u6280\u672f\uff08LRP\uff09\u8ba1\u7b97\u53c2\u6570\u91cd\u8981\u6027\uff0c\u9009\u62e9\u6027\u5e94\u7528TMR\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0cDNN\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfTMR\u65b9\u6cd5\u5f00\u9500\u5927\uff0c\u9700\u9009\u62e9\u6027\u5e94\u7528\u4ee5\u4f18\u5316\u6548\u7387\u3002XAI\u6280\u672f\u80fd\u63d0\u4f9b\u53c2\u6570\u91cd\u8981\u6027\u4fe1\u606f\uff0c\u4e3aTMR\u7684\u9009\u62e9\u6807\u51c6\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u4f4e\u6210\u672c\u7684\u68af\u5ea6\u9a71\u52a8XAI\u6280\u672f\uff08LRP\uff09\u8ba1\u7b97DNN\u53c2\u6570\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u9009\u62e9\u5173\u952e\u6743\u91cd\u5e94\u7528TMR\u3002\u5728VGG16\u548cAlexNet\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u4f7f\u7528MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u3002", "result": "\u5728\u6bd4\u7279\u9519\u8bef\u7387\u4e3a10-4\u65f6\uff0c\u8be5\u65b9\u6cd5\u5bf9AlexNet\u6a21\u578b\u7684\u53ef\u9760\u6027\u63d0\u5347\u8d85\u8fc760%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u540c\u7684\u5f00\u9500\u3002", "conclusion": "\u57fa\u4e8eXAI\u7684TMR\u65b9\u6cd5\u80fd\u9ad8\u6548\u63d0\u5347DNN\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09041", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09041", "abs": "https://arxiv.org/abs/2507.09041", "authors": ["Andrew Wagenmaker", "Zhiyuan Zhou", "Sergey Levine"], "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation", "comment": null, "summary": "Developing autonomous agents that quickly explore an environment and adapt\ntheir behavior online is a canonical challenge in robotics and machine\nlearning. While humans are able to achieve such fast online exploration and\nadaptation, often acquiring new information and skills in only a handful of\ninteractions, existing algorithmic approaches tend to rely on random\nexploration and slow, gradient-based behavior updates. How can we endow\nautonomous agents with such capabilities on par with humans? Taking inspiration\nfrom recent progress on both in-context learning and large-scale behavioral\ncloning, in this work we propose behavioral exploration: training agents to\ninternalize what it means to explore and adapt in-context over the space of\n``expert'' behaviors. To achieve this, given access to a dataset of expert\ndemonstrations, we train a long-context generative model to predict expert\nactions conditioned on a context of past observations and a measure of how\n``exploratory'' the expert's behaviors are relative to this context. This\nenables the model to not only mimic the behavior of an expert, but also, by\nfeeding its past history of interactions into its context, to select different\nexpert behaviors than what have been previously selected, thereby allowing for\nfast online adaptation and targeted, ``expert-like'' exploration. We\ndemonstrate the effectiveness of our method in both simulated locomotion and\nmanipulation settings, as well as on real-world robotic manipulation tasks,\nillustrating its ability to learn adaptive, exploratory behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u884c\u4e3a\u63a2\u7d22\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u81ea\u4e3b\u4ee3\u7406\u80fd\u591f\u5feb\u901f\u5728\u7ebf\u63a2\u7d22\u548c\u9002\u5e94\u73af\u5883\uff0c\u6a21\u4eff\u4e13\u5bb6\u884c\u4e3a\u5e76\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u4f9d\u8d56\u968f\u673a\u63a2\u7d22\u548c\u7f13\u6162\u7684\u68af\u5ea6\u66f4\u65b0\uff0c\u800c\u4eba\u7c7b\u80fd\u5feb\u901f\u9002\u5e94\u548c\u5b66\u4e60\u3002\u7814\u7a76\u65e8\u5728\u8d4b\u4e88\u81ea\u4e3b\u4ee3\u7406\u7c7b\u4f3c\u4eba\u7c7b\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\u9884\u6d4b\u4e13\u5bb6\u884c\u4e3a\uff0c\u5e76\u7ed3\u5408\u63a2\u7d22\u6027\u5ea6\u91cf\uff0c\u5b9e\u73b0\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u4e13\u5bb6\u7ea7\u63a2\u7d22\u3002", "result": "\u5728\u6a21\u62df\u8fd0\u52a8\u548c\u64cd\u4f5c\u4efb\u52a1\u4ee5\u53ca\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5b66\u4e60\u9002\u5e94\u6027\u548c\u63a2\u7d22\u884c\u4e3a\u7684\u80fd\u529b\u3002", "conclusion": "\u884c\u4e3a\u63a2\u7d22\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5feb\u901f\u5728\u7ebf\u9002\u5e94\u548c\u4e13\u5bb6\u7ea7\u63a2\u7d22\uff0c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u7684\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08950", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08950", "abs": "https://arxiv.org/abs/2507.08950", "authors": ["Xue Zhang", "Abla Kammoun", "Mohamed-Slim Alouini"], "title": "Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems", "comment": null, "summary": "This paper investigates the asymptotic behavior of the deterministic and\nstochastic Cram\\'er-Rao Bounds (CRB) for semi-blind channel estimation in\nmassive multiple-input multiple-output (MIMO) systems. We derive and analyze\nmathematically tractable expressions for both metrics under various asymptotic\nregimes, which govern the growth rates of the number of antennas, the number of\nusers, the training sequence length, and the transmission block length. Unlike\nthe existing work, our results show that the CRB can be made arbitrarily small\nas the transmission block length increases, but only when the training sequence\nlength grows at the same rate and the number of users remains fixed. However,\nif the number of training sequences remains proportional to the number of\nusers, the channel estimation error is always lower-bounded by a non-vanishing\nconstant. Numerical results are presented to support our findings and\ndemonstrate the advantages of semi-blind channel estimation in reducing the\nrequired number of training sequences.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u534a\u76f2\u4fe1\u9053\u4f30\u8ba1\u7684\u786e\u5b9a\u6027\u53ca\u968f\u673aCram\u00e9r-Rao\u754c\uff08CRB\uff09\u7684\u6e10\u8fd1\u884c\u4e3a\uff0c\u63a8\u5bfc\u4e86\u4e0d\u540c\u6e10\u8fd1\u6761\u4ef6\u4e0b\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\uff0c\u5e76\u53d1\u73b0CRB\u53ef\u968f\u4f20\u8f93\u5757\u957f\u5ea6\u589e\u52a0\u800c\u65e0\u9650\u51cf\u5c0f\uff0c\u4f46\u9700\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u540c\u6b65\u589e\u957f\u4e14\u7528\u6237\u6570\u56fa\u5b9a\u3002", "motivation": "\u63a2\u8ba8\u534a\u76f2\u4fe1\u9053\u4f30\u8ba1\u5728\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\u7684\u6027\u80fd\u6781\u9650\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9CRB\u6e10\u8fd1\u884c\u4e3a\u5206\u6790\u7684\u4e0d\u8db3\u3002", "method": "\u63a8\u5bfc\u5e76\u5206\u6790\u4e86\u786e\u5b9a\u6027\u53ca\u968f\u673aCRB\u5728\u4e0d\u540c\u6e10\u8fd1\u6761\u4ef6\u4e0b\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\uff0c\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u7406\u8bba\u53d1\u73b0\u3002", "result": "CRB\u53ef\u968f\u4f20\u8f93\u5757\u957f\u5ea6\u589e\u52a0\u800c\u65e0\u9650\u51cf\u5c0f\uff0c\u4f46\u9700\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u540c\u6b65\u589e\u957f\u4e14\u7528\u6237\u6570\u56fa\u5b9a\uff1b\u82e5\u8bad\u7ec3\u5e8f\u5217\u6570\u4e0e\u7528\u6237\u6570\u6210\u6bd4\u4f8b\uff0c\u5219\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u6709\u975e\u96f6\u4e0b\u9650\u3002", "conclusion": "\u534a\u76f2\u4fe1\u9053\u4f30\u8ba1\u80fd\u6709\u6548\u51cf\u5c11\u6240\u9700\u8bad\u7ec3\u5e8f\u5217\u6570\uff0c\u4f46\u9700\u5408\u7406\u8bbe\u8ba1\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u4e0e\u7528\u6237\u6570\u7684\u5173\u7cfb\u3002"}}
{"id": "2507.09128", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09128", "abs": "https://arxiv.org/abs/2507.09128", "authors": ["Ronak Mehta", "Zaid Harchaoui"], "title": "A Generalization Theory for Zero-Shot Prediction", "comment": "Published at ICML '25 (Oral)", "summary": "A modern paradigm for generalization in machine learning and AI consists of\npre-training a task-agnostic foundation model, generally obtained using\nself-supervised and multimodal contrastive learning. The resulting\nrepresentations can be used for prediction on a downstream task for which no\nlabeled data is available. We present a theoretical framework to better\nunderstand this approach, called zero-shot prediction. We identify the target\nquantities that zero-shot prediction aims to learn, or learns in passing, and\nthe key conditional independence relationships that enable its generalization\nability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u57fa\u4e8e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u6761\u4ef6\u72ec\u7acb\u6027\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u96f6\u6837\u672c\u9884\u6d4b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u65e0\u6807\u6ce8\u6570\u636e\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u96f6\u6837\u672c\u9884\u6d4b\u7684\u76ee\u6807\u91cf\u53ca\u5176\u5173\u952e\u6761\u4ef6\u72ec\u7acb\u6027\u5173\u7cfb\uff0c\u6784\u5efa\u7406\u8bba\u6846\u67b6\u3002", "result": "\u660e\u786e\u4e86\u96f6\u6837\u672c\u9884\u6d4b\u7684\u5b66\u4e60\u76ee\u6807\u53ca\u5176\u6cdb\u5316\u80fd\u529b\u7684\u6761\u4ef6\u72ec\u7acb\u6027\u673a\u5236\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u96f6\u6837\u672c\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6307\u5bfc\u3002"}}
{"id": "2507.08832", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08832", "abs": "https://arxiv.org/abs/2507.08832", "authors": ["Niranjan Mallikarjun Sindhur", "Pavithra C", "Nivya Muchikel"], "title": "A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting", "comment": null, "summary": "Farmers in developing regions like Karnataka, India, face a dual challenge:\nnavigating extreme market and climate volatility while being excluded from the\ndigital revolution due to literacy barriers. This paper presents a novel\ndecision support system that addresses both challenges through a unique\nsynthesis of machine learning and human-computer interaction. We propose a\nhybrid recommendation engine that integrates two predictive models: a Random\nForest classifier to assess agronomic suitability based on soil, climate, and\nreal-time weather data, and a Long Short-Term Memory (LSTM) network to forecast\nmarket prices for agronomically viable crops. This integrated approach shifts\nthe paradigm from \"what can grow?\" to \"what is most profitable to grow?\",\nproviding a significant advantage in mitigating economic risk. The system is\ndelivered through an end-to-end, voice-based interface in the local Kannada\nlanguage, leveraging fine-tuned speech recognition and high-fidelity speech\nsynthesis models to ensure accessibility for low-literacy users. Our results\nshow that the Random Forest model achieves 98.5% accuracy in suitability\nprediction, while the LSTM model forecasts harvest-time prices with a low\nmargin of error. By providing data-driven, economically optimized\nrecommendations through an inclusive interface, this work offers a scalable and\nimpactful solution to enhance the financial resilience of marginalized farming\ncommunities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u5e2e\u52a9\u5370\u5ea6\u5361\u7eb3\u5854\u514b\u90a6\u7684\u519c\u6c11\u5e94\u5bf9\u5e02\u573a\u548c\u6c14\u5019\u6ce2\u52a8\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u754c\u9762\u89e3\u51b3\u8bc6\u5b57\u969c\u788d\u3002", "motivation": "\u519c\u6c11\u9762\u4e34\u5e02\u573a\u548c\u6c14\u5019\u6ce2\u52a8\uff0c\u540c\u65f6\u56e0\u8bc6\u5b57\u969c\u788d\u88ab\u6392\u9664\u5728\u6570\u5b57\u9769\u547d\u4e4b\u5916\uff0c\u9700\u8981\u4e00\u79cd\u5305\u5bb9\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8bc4\u4f30\u519c\u827a\u9002\u5b9c\u6027\uff0cLSTM\u7f51\u7edc\u9884\u6d4b\u5e02\u573a\u4ef7\u683c\uff0c\u5e76\u901a\u8fc7\u8bed\u97f3\u754c\u9762\uff08\u5361\u7eb3\u8fbe\u8bed\uff09\u63d0\u4f9b\u670d\u52a1\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u51c6\u786e\u7387\u8fbe98.5%\uff0cLSTM\u6a21\u578b\u9884\u6d4b\u4ef7\u683c\u8bef\u5dee\u4f4e\uff0c\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u519c\u6c11\u7ecf\u6d4e\u97e7\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u8fb9\u7f18\u5316\u519c\u6c11\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u7ecf\u6d4e\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09061", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09061", "abs": "https://arxiv.org/abs/2507.09061", "authors": ["Thomas T. Zhang", "Daniel Pfrommer", "Nikolai Matni", "Max Simchowitz"], "title": "Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction", "comment": null, "summary": "We study the problem of imitating an expert demonstrator in a continuous\nstate-and-action dynamical system. While imitation learning in discrete\nsettings such as autoregressive language modeling has seen immense success and\npopularity in recent years, imitation in physical settings such as autonomous\ndriving and robot learning has proven comparably more complex due to the\ncompounding errors problem, often requiring elaborate set-ups to perform\nstably. Recent work has demonstrated that even in benign settings, exponential\ncompounding errors are unavoidable when learning solely from expert-controlled\ntrajectories, suggesting the need for more advanced policy parameterizations or\ndata augmentation. To this end, we present minimal interventions that provably\nmitigate compounding errors in continuous state-and-action imitation learning.\nWhen the system is open-loop stable, we prescribe \"action chunking,\" i.e.,\npredicting and playing sequences of actions in open-loop; when the system is\npossibly unstable, we prescribe \"noise injection,\" i.e., adding noise during\nexpert demonstrations. These interventions align with popular choices in modern\nrobot learning, though the benefits we derive are distinct from the effects\nthey were designed to target. Our results draw insights and tools from both\ncontrol theory and reinforcement learning; however, our analysis reveals novel\nconsiderations that do not naturally arise when either literature is considered\nin isolation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u52a8\u6001\u7cfb\u7edf\u4e2d\u6a21\u4eff\u4e13\u5bb6\u6f14\u793a\u8005\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\u4ee5\u7f13\u89e3\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u79bb\u6563\u573a\u666f\u4e2d\u5df2\u53d6\u5f97\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5728\u7269\u7406\u573a\u666f\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5b66\u4e60\uff09\u4e2d\u7531\u4e8e\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u66f4\u4e3a\u590d\u6742\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u7b56\u7565\u6216\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5e72\u9884\u63aa\u65bd\uff1a\u5728\u5f00\u73af\u7a33\u5b9a\u7cfb\u7edf\u4e2d\u4f7f\u7528\u201c\u52a8\u4f5c\u5206\u5757\u201d\uff08\u9884\u6d4b\u5e76\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\uff09\uff0c\u5728\u53ef\u80fd\u4e0d\u7a33\u5b9a\u7684\u7cfb\u7edf\u4e2d\u4f7f\u7528\u201c\u566a\u58f0\u6ce8\u5165\u201d\uff08\u5728\u4e13\u5bb6\u6f14\u793a\u4e2d\u6dfb\u52a0\u566a\u58f0\uff09\u3002", "result": "\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u6709\u6548\u7f13\u89e3\u4e86\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u4e0e\u73b0\u4ee3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6d41\u884c\u9009\u62e9\u4e00\u81f4\uff0c\u4f46\u5176\u6548\u679c\u4e0e\u8bbe\u8ba1\u76ee\u6807\u4e0d\u540c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u5408\u4e86\u63a7\u5236\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5355\u72ec\u8003\u8651\u4efb\u4e00\u9886\u57df\u65f6\u672a\u51fa\u73b0\u7684\u65b0\u95ee\u9898\u3002"}}
{"id": "2507.09148", "categories": ["stat.ML", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09148", "abs": "https://arxiv.org/abs/2507.09148", "authors": ["Alberto Del Pia", "Dekun Zhou"], "title": "A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation", "comment": "29 pages, 2 figures", "summary": "Sparse Principal Component Analysis (SPCA) is a fundamental technique for\ndimensionality reduction, and is NP-hard. In this paper, we introduce a\nrandomized approximation algorithm for SPCA, which is based on the basic SDP\nrelaxation. Our algorithm has an approximation ratio of at most the sparsity\nconstant with high probability, if called enough times. Under a technical\nassumption, which is consistently satisfied in our numerical tests, the average\napproximation ratio is also bounded by $\\mathcal{O}(\\log{d})$, where $d$ is the\nnumber of features. We show that this technical assumption is satisfied if the\nSDP solution is low-rank, or has exponentially decaying eigenvalues. We then\npresent a broad class of instances for which this technical assumption holds.\nWe also demonstrate that in a covariance model, which generalizes the spiked\nWishart model, our proposed algorithm achieves a near-optimal approximation\nratio. We demonstrate the efficacy of our algorithm through numerical results\non real-world datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSDP\u677e\u5f1b\u7684\u968f\u673a\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3NP\u96be\u7684\u7a00\u758f\u4e3b\u6210\u5206\u5206\u6790\u95ee\u9898\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7a00\u758f\u4e3b\u6210\u5206\u5206\u6790\uff08SPCA\uff09\u662f\u4e00\u79cd\u91cd\u8981\u7684\u964d\u7ef4\u6280\u672f\uff0c\u4f46\u5176NP\u96be\u7279\u6027\u4f7f\u5f97\u9ad8\u6548\u6c42\u89e3\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "method": "\u57fa\u4e8e\u57fa\u672cSDP\u677e\u5f1b\u7684\u968f\u673a\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u6b21\u8c03\u7528\u5b9e\u73b0\u9ad8\u6982\u7387\u7684\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u5728\u7279\u5b9a\u6280\u672f\u5047\u8bbe\u4e0b\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u7b97\u6cd5\u5728\u7a00\u758f\u5e38\u6570\u548c\u7279\u5f81\u6570\u5bf9\u6570\u8303\u56f4\u5185\u5177\u6709\u8fd1\u4f3c\u6bd4\uff0c\u4e14\u5728\u4f4e\u79e9\u6216\u6307\u6570\u8870\u51cf\u7279\u5f81\u503c\u7684SDP\u89e3\u4e0b\u6ee1\u8db3\u6280\u672f\u5047\u8bbe\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3aSPCA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fd1\u4f3c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08833", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08833", "abs": "https://arxiv.org/abs/2507.08833", "authors": ["Seokmin Ko"], "title": "LoRA Is Slower Than You Think", "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "AI": {"tldr": "LoRA\u662f\u4e00\u79cd\u5e7f\u6cdb\u7528\u4e8e\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5f15\u5165\u4f4e\u79e9\u6743\u91cd\u77e9\u9635\u51cf\u5c11\u53c2\u6570\u66f4\u65b0\u91cf\uff0c\u4f46\u901f\u5ea6\u63d0\u5347\u4e0d\u4e00\u81f4\u3002\u672c\u6587\u5206\u6790\u5176\u6027\u80fd\u9650\u5236\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "LoRA\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u901f\u5ea6\u63d0\u5347\u4e0d\u4e00\u81f4\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7a76\u5176\u6027\u80fd\u9650\u5236\u56e0\u7d20\u3002", "method": "\u5bf9LoRA\u6027\u80fd\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0eLoRA\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u4e00\u81f4\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u4e3a\u8d44\u6e90\u53d7\u9650\u4e0b\u4f18\u5316LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u548c\u89c1\u89e3\u3002"}}
{"id": "2507.10078", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10078", "abs": "https://arxiv.org/abs/2507.10078", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction", "comment": "Accepted to IEEE Control Systems Letters", "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u4e2dH\u00b2\u6a21\u578b\u964d\u9636\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u7ebf\u6027SSM\u6a21\u578b\u7684\u53c2\u6570\u91cf\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5c06\u53c2\u6570\u91cf\u51cf\u5c11\u52301/32\u3002", "motivation": "\u7ebf\u6027SSM\u6a21\u578b\u5728\u6355\u83b7\u957f\u7a0b\u4f9d\u8d56\u65f6\u53c2\u6570\u91cf\u8fc7\u5927\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u5e94\u7528\u63a7\u5236\u7406\u8bba\u4e2d\u7684H\u00b2\u6a21\u578b\u964d\u9636\u6280\u672f\u5bf9\u7ebf\u6027SSM\u7ec4\u4ef6\u8fdb\u884c\u53c2\u6570\u538b\u7f29\u3002", "result": "\u5728LRA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u5e73\u8861\u622a\u65ad\u65b9\u6cd5\uff0c\u53c2\u6570\u91cf\u51cf\u5c11\u52301/32\u4e14\u6027\u80fd\u672a\u4e0b\u964d\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u7ebf\u6027SSM\u6a21\u578b\u7684\u53c2\u6570\u91cf\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2507.08999", "categories": ["eess.SP", "I.5.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08999", "abs": "https://arxiv.org/abs/2507.08999", "authors": ["Duc Vu", "Selin Aviyente"], "title": "Hypergraph Overlapping Community Detection for Brain Networks", "comment": "6 Pages, Accepted for IEEE MLSP 2025", "summary": "Functional magnetic resonance imaging (fMRI) has been commonly used to\nconstruct functional connectivity networks (FCNs) of the human brain. TFCNs are\nprimarily limited to quantifying pairwise relationships between ROIs ignoring\nhigher order dependencies between multiple brain regions. Recently, hypergraph\nconstruction methods from fMRI time series data have been proposed to\ncharacterize the high-order relations among multiple ROIs. While there have\nbeen multiple methods for constructing hypergraphs from fMRI time series, the\nquestion of how to characterize the topology of these hypergraphs remains open.\nIn this paper, we make two key contributions to the field of community\ndetection in brain hypernetworks. First, we construct a hypergraph for each\nsubject capturing high order dependencies between regions. Second, we introduce\na spectral clustering based approach on hypergraphs to detect overlapping\ncommunity structure. Finally, the proposed method is implemented to detect the\nconsensus community structure across multiple subjects. The proposed method is\napplied to resting state fMRI data from Human Connectome Project to summarize\nthe overlapping community structure across a group of healthy young adults.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u793e\u533a\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790fMRI\u6570\u636e\u4e2d\u7684\u9ad8\u9636\u8111\u533a\u5173\u7cfb\u3002", "motivation": "\u4f20\u7edf\u529f\u80fd\u8fde\u63a5\u7f51\u7edc\uff08FCNs\uff09\u4ec5\u91cf\u5316\u8111\u533a\u95f4\u7684\u6210\u5bf9\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u6765\u8868\u5f81\u591a\u8111\u533a\u7684\u9ad8\u9636\u5173\u7cfb\u3002", "method": "\u9996\u5148\u4e3a\u6bcf\u4e2a\u88ab\u8bd5\u6784\u5efa\u8d85\u56fe\u4ee5\u6355\u6349\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\uff0c\u7136\u540e\u5f15\u5165\u57fa\u4e8e\u8c31\u805a\u7c7b\u7684\u8d85\u56fe\u65b9\u6cd5\u68c0\u6d4b\u91cd\u53e0\u793e\u533a\u7ed3\u6784\uff0c\u5e76\u5728\u591a\u88ab\u8bd5\u4e2d\u68c0\u6d4b\u5171\u8bc6\u793e\u533a\u7ed3\u6784\u3002", "result": "\u65b9\u6cd5\u5e94\u7528\u4e8e\u4eba\u7c7b\u8fde\u63a5\u7ec4\u8ba1\u5212\u7684\u9759\u606f\u6001fMRI\u6570\u636e\uff0c\u603b\u7ed3\u4e86\u5065\u5eb7\u5e74\u8f7b\u4eba\u7684\u91cd\u53e0\u793e\u533a\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u8868\u5f81\u4e86\u8111\u8d85\u7f51\u7edc\u4e2d\u7684\u9ad8\u9636\u5173\u7cfb\uff0c\u4e3a\u793e\u533a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.09317", "categories": ["stat.ML", "cs.LG", "q-bio.PE", "68T07, 62H22, 92D40", "I.2.3; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.09317", "abs": "https://arxiv.org/abs/2507.09317", "authors": ["Sara Si-Moussi", "Esther Galbrun", "Mickael Hedde", "Giovanni Poggiato", "Matthias Rohr", "Wilfried Thuiller"], "title": "Uncovering symmetric and asymmetric species associations from community and environmental data", "comment": null, "summary": "There is no much doubt that biotic interactions shape community assembly and\nultimately the spatial co-variations between species. There is a hope that the\nsignal of these biotic interactions can be observed and retrieved by\ninvestigating the spatial associations between species while accounting for the\ndirect effects of the environment. By definition, biotic interactions can be\nboth symmetric and asymmetric. Yet, most models that attempt to retrieve\nspecies associations from co-occurrence or co-abundance data internally assume\nsymmetric relationships between species. Here, we propose and validate a\nmachine-learning framework able to retrieve bidirectional associations by\nanalyzing species community and environmental data.\n  Our framework (1) models pairwise species associations as directed influences\nfrom a source to a target species, parameterized with two species-specific\nlatent embeddings: the effect of the source species on the community, and the\nresponse of the target species to the community; and (2) jointly fits these\nassociations within a multi-species conditional generative model with different\nmodes of interactions between environmental drivers and biotic associations.\nUsing both simulated and empirical data, we demonstrate the ability of our\nframework to recover known asymmetric and symmetric associations and highlight\nthe properties of the learned association networks. By comparing our approach\nto other existing models such as joint species distribution models and\nprobabilistic graphical models, we show its superior capacity at retrieving\nsymmetric and asymmetric interactions. The framework is intuitive, modular and\nbroadly applicable across various taxonomic groups.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7269\u79cd\u7fa4\u843d\u548c\u73af\u5883\u6570\u636e\u4e2d\u63d0\u53d6\u53cc\u5411\u5173\u8054\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u5f71\u54cd\u7fa4\u843d\u7ec4\u88c5\u548c\u7269\u79cd\u7a7a\u95f4\u5171\u53d8\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u591a\u5047\u8bbe\u5bf9\u79f0\u5173\u7cfb\uff0c\u65e0\u6cd5\u6355\u6349\u4e0d\u5bf9\u79f0\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u6846\u67b6\u901a\u8fc7\u7269\u79cd\u7279\u5b9a\u6f5c\u5728\u5d4c\u5165\u5efa\u6a21\u53cc\u5411\u5173\u8054\uff0c\u5e76\u7ed3\u5408\u591a\u7269\u79cd\u6761\u4ef6\u751f\u6210\u6a21\u578b\u62df\u5408\u73af\u5883\u9a71\u52a8\u4e0e\u751f\u7269\u5173\u8054\u7684\u4e0d\u540c\u6a21\u5f0f\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u8bc1\u6570\u636e\u9a8c\u8bc1\u4e86\u6846\u67b6\u80fd\u6062\u590d\u5df2\u77e5\u5bf9\u79f0\u548c\u4e0d\u5bf9\u79f0\u5173\u8054\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u76f4\u89c2\u3001\u6a21\u5757\u5316\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5206\u7c7b\u7fa4\u3002"}}
{"id": "2507.08834", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08834", "abs": "https://arxiv.org/abs/2507.08834", "authors": ["Karishma Battina", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Physical Informed Neural Networks for modeling ocean pollutant", "comment": "13 pages, 9 figures, 3 tables", "summary": "Traditional numerical methods often struggle with the complexity and scale of\nmodeling pollutant transport across vast and dynamic oceanic domains. This\npaper introduces a Physics-Informed Neural Network (PINN) framework to simulate\nthe dispersion of pollutants governed by the 2D advection-diffusion equation.\nThe model achieves physically consistent predictions by embedding physical laws\nand fitting to noisy synthetic data, generated via a finite difference method\n(FDM), directly into the neural network training process. This approach\naddresses challenges such as non-linear dynamics and the enforcement of\nboundary and initial conditions. Synthetic data sets, augmented with varying\nnoise levels, are used to capture real-world variability. The training\nincorporates a hybrid loss function including PDE residuals, boundary/initial\ncondition conformity, and a weighted data fit term. The approach takes\nadvantage of the Julia language scientific computing ecosystem for\nhigh-performance simulations, offering a scalable and flexible alternative to\ntraditional solvers", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u4e8c\u7ef4\u5e73\u6d41-\u6269\u6563\u65b9\u7a0b\u63a7\u5236\u7684\u6c61\u67d3\u7269\u6269\u6563\uff0c\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u548c\u62df\u5408\u566a\u58f0\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u52a8\u6001\u6d77\u6d0b\u57df\u4e2d\u7684\u5efa\u6a21\u96be\u9898\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u52a8\u6001\u6d77\u6d0b\u57df\u4e2d\u7684\u6c61\u67d3\u7269\u4f20\u8f93\u65f6\u9762\u4e34\u590d\u6742\u6027\u548c\u89c4\u6a21\u4e0a\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u4e8c\u7ef4\u5e73\u6d41-\u6269\u6563\u65b9\u7a0b\uff0c\u901a\u8fc7\u6df7\u5408\u635f\u5931\u51fd\u6570\uff08\u5305\u62ecPDE\u6b8b\u5dee\u3001\u8fb9\u754c/\u521d\u59cb\u6761\u4ef6\u4e00\u81f4\u6027\u53ca\u52a0\u6743\u6570\u636e\u62df\u5408\u9879\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5229\u7528Julia\u8bed\u8a00\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u80fd\u529b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u7269\u7406\u4e00\u81f4\u7684\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u5408\u6210\u6570\u636e\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u7684\u53d8\u5f02\u6027\uff0c\u4e3a\u4f20\u7edf\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PINN\u6846\u67b6\u5728\u6c61\u67d3\u7269\u6269\u6563\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89e3\u51b3\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u7684\u5efa\u6a21\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09215", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09215", "abs": "https://arxiv.org/abs/2507.09215", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Kun Yang"], "title": "Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems", "comment": null, "summary": "The bistatic Integrated Sensing and Communication (ISAC) is poised to become\na key application for next generation communication networks (e.g., B5G/6G),\nproviding simultaneous sensing and communication services with minimal changes\nto existing network infrastructure and hardware. However, a significant\nchallenge in bistatic cooperative sensing is clock asynchronism, arising from\nthe use of different clocks at far separated transmitters and receivers. This\nasynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),\npotentially causing sensing ambiguity. Traditional synchronization methods\ntypically rely on static reference links or GNSS-based timing sources, both of\nwhich are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.\nTo overcome these limitations, we propose a Time-Varying Offset Estimation\n(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which\nleverages the geometrically predictable characteristics of the Line-of-Sight\n(LoS) path to enable robust, infrastructure-free\n  synchronization. The framework treats the LoS delay and the Doppler shift as\ndynamic observations and models their evolution as a hidden stochastic process.\nA state-space formulation is developed to jointly estimate TO and CFO via an\nExtended Kalman Filter (EKF), enabling real-time tracking of clock offsets\nacross successive frames. Furthermore, the estimated offsets are subsequently\napplied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)\ncomponents, thereby enhancing the high-resolution target sensing performance.\nExtensive simulation results demonstrate that the proposed TVOE method improves\nthe estimation accuracy by 60%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65f6\u949f\u5f02\u6b65\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u7684\u65f6\u53d8\u504f\u79fb\u4f30\u8ba1\uff08TVOE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b9e\u65f6\u8ddf\u8e2a\u504f\u79fb\uff0c\u663e\u8457\u63d0\u5347\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\u65f6\u949f\u5f02\u6b65\u5bfc\u81f4\u7684\u5b9a\u65f6\u504f\u79fb\u548c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u4f1a\u5f15\u53d1\u611f\u77e5\u6a21\u7cca\uff0c\u4f20\u7edf\u540c\u6b65\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u5229\u7528\u89c6\u8ddd\u8def\u5f84\u7684\u51e0\u4f55\u7279\u6027\u5efa\u6a21\u504f\u79fb\u4e3a\u9690\u85cf\u968f\u673a\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5b9e\u65f6\u4f30\u8ba1\u548c\u6821\u6b63\u504f\u79fb\u3002", "result": "\u4eff\u771f\u663e\u793aTVOE\u65b9\u6cd5\u5c06\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad8\u4e8660%\u3002", "conclusion": "TVOE\u6846\u67b6\u4e3a\u65f6\u949f\u5f02\u6b65\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u65e0\u9700\u57fa\u7840\u8bbe\u65bd\u7684\u540c\u6b65\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09494", "categories": ["stat.ML", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.09494", "abs": "https://arxiv.org/abs/2507.09494", "authors": ["Albert Chiu"], "title": "An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects", "comment": null, "summary": "We introduce an algorithm for identifying interpretable subgroups with\nelevated treatment effects, given an estimate of individual or conditional\naverage treatment effects (CATE). Subgroups are characterized by ``rule sets''\n-- easy-to-understand statements of the form (Condition A AND Condition B) OR\n(Condition C) -- which can capture high-order interactions while retaining\ninterpretability. Our method complements existing approaches for estimating the\nCATE, which often produce high dimensional and uninterpretable results, by\nsummarizing and extracting critical information from fitted models to aid\ndecision making, policy implementation, and scientific understanding. We\npropose an objective function that trades-off subgroup size and effect size,\nand varying the hyperparameter that controls this trade-off results in a\n``frontier'' of Pareto optimal rule sets, none of which dominates the others\nacross all criteria. Valid inference is achievable through sample splitting. We\ndemonstrate the utility and limitations of our method using simulated and\nempirical examples.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5177\u6709\u663e\u8457\u6cbb\u7597\u6548\u679c\u7684\u6613\u89e3\u91ca\u5b50\u7fa4\uff0c\u901a\u8fc7\u89c4\u5219\u96c6\u5f62\u5f0f\u8868\u8fbe\uff0c\u5e73\u8861\u5b50\u7fa4\u89c4\u6a21\u4e0e\u6548\u679c\u5927\u5c0f\u3002", "motivation": "\u73b0\u6709CATE\u4f30\u8ba1\u65b9\u6cd5\u5e38\u4ea7\u751f\u9ad8\u7ef4\u4e14\u96be\u4ee5\u89e3\u91ca\u7684\u7ed3\u679c\uff0c\u9700\u8865\u5145\u65b9\u6cd5\u4ee5\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u8f85\u52a9\u51b3\u7b56\u3002", "method": "\u57fa\u4e8e\u89c4\u5219\u96c6\u7684\u5b50\u7fa4\u8bc6\u522b\u7b97\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u51fd\u6570\u6743\u8861\u5b50\u7fa4\u89c4\u6a21\u4e0e\u6548\u679c\u5927\u5c0f\uff0c\u751f\u6210Pareto\u6700\u4f18\u89c4\u5219\u96c6\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u65b9\u6cd5\u5b9e\u7528\u6027\uff0c\u5c55\u793a\u5176\u5c40\u9650\u6027\u4e0e\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aCATE\u4f30\u8ba1\u63d0\u4f9b\u6613\u89e3\u91ca\u7684\u5b50\u7fa4\u5206\u6790\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u51b3\u7b56\u652f\u6301\u548c\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2507.08835", "categories": ["cs.LG", "cs.AI", "math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.08835", "abs": "https://arxiv.org/abs/2507.08835", "authors": ["Harold Gu\u00e9neau", "Alain Celisse", "Pascal Delange"], "title": "Representation learning with a transformer by contrastive learning for money laundering detection", "comment": null, "summary": "The present work tackles the money laundering detection problem. A new\nprocedure is introduced which exploits structured time series of both\nqualitative and quantitative data by means of a transformer neural network. The\nfirst step of this procedure aims at learning representations of time series\nthrough contrastive learning (without any labels). The second step leverages\nthese representations to generate a money laundering scoring of all\nobservations. A two-thresholds approach is then introduced, which ensures a\ncontrolled false-positive rate by means of the Benjamini-Hochberg (BH)\nprocedure. Experiments confirm that the transformer is able to produce general\nrepresentations that succeed in exploiting money laundering patterns with\nminimal supervision from domain experts. It also illustrates the higher ability\nof the new procedure for detecting nonfraudsters as well as fraudsters, while\nkeeping the false positive rate under control. This greatly contrasts with\nrule-based procedures or the ones based on LSTM architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u795e\u7ecf\u7f51\u7edc\u7684\u6d17\u94b1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u53cc\u9608\u503c\u548cBH\u7a0b\u5e8f\u63a7\u5236\u8bef\u62a5\u7387\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u6216LSTM\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6d17\u94b1\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u6216LSTM\uff09\u5728\u8bef\u62a5\u7387\u548c\u68c0\u6d4b\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "1. \u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u65e0\u76d1\u7763\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u8868\u793a\uff1b2. \u5229\u7528\u8fd9\u4e9b\u8868\u793a\u751f\u6210\u6d17\u94b1\u8bc4\u5206\uff1b3. \u5f15\u5165\u53cc\u9608\u503c\u548cBH\u7a0b\u5e8f\u63a7\u5236\u8bef\u62a5\u7387\u3002", "result": "Transformer\u80fd\u6709\u6548\u6355\u6349\u6d17\u94b1\u6a21\u5f0f\uff0c\u68c0\u6d4b\u80fd\u529b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u540c\u65f6\u8bef\u62a5\u7387\u53ef\u63a7\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u6d17\u94b1\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u8bef\u62a5\u548c\u63d0\u9ad8\u68c0\u6d4b\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.09218", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09218", "abs": "https://arxiv.org/abs/2507.09218", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Chaochao Wang", "Xianchao Zhang", "Kun Yang"], "title": "Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC", "comment": null, "summary": "Bistatic Integrated Sensing and Communication (ISAC) is poised to become a\ncornerstone technology in next-generation communication networks, such as\nBeyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and\ncommunication functions without requiring significant modifications to existing\ninfrastructure. Despite its promising potential, a major challenge in bistatic\ncooperative sensing lies in the degradation of sensing accuracy, primarily\ncaused by the inherently weak received signals resulting from high reflection\nlosses in complex environments. Traditional methods have predominantly relied\non adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by\ndynamically adjusting the filter coefficients. However, these methods often\nstruggle to adapt effectively to the increasingly complex and diverse network\ntopologies. To address these challenges, we propose a novel Image\nSuper-Resolution-based Signal Enhancement (ISR-SE) framework that significantly\nimproves the recognition and recovery capabilities of ISAC signals.\nSpecifically, we first perform a time-frequency analysis by applying the\nShort-Time Fourier Transform (STFT) to the received signals, generating\nspectrograms that capture the frequency, magnitude, and phase components. These\ncomponents are then mapped into RGB images, where each channel represents one\nof the extracted features, enabling a more intuitive and informative\nvisualization of the signal structure. To enhance these RGB images, we design\nan improved denoising network that combines the strengths of the UNet\narchitecture and diffusion models. This hybrid architecture leverages UNet's\nmulti-scale feature extraction and the generative capacity of diffusion models\nto perform effective image denoising, thereby improving the quality and clarity\nof signal representations under low-SNR conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u4fe1\u53f7\u589e\u5f3a\u6846\u67b6\uff08ISR-SE\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\u7684\u4fe1\u53f7\u8bc6\u522b\u4e0e\u6062\u590d\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u73af\u5883\u4e2d\u4fe1\u53f7\u5f31\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u5728\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7f51\u7edc\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u590d\u6742\u73af\u5883\u4e2d\u7684\u9ad8\u53cd\u5c04\u635f\u8017\u5bfc\u81f4\u4fe1\u53f7\u5f31\u5316\uff0c\u4f20\u7edf\u81ea\u9002\u5e94\u6ee4\u6ce2\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u590d\u6742\u7f51\u7edc\u62d3\u6251\u3002", "method": "\u901a\u8fc7\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u751f\u6210\u9891\u8c31\u56fe\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u4e3aRGB\u56fe\u50cf\uff0c\u7ed3\u5408UNet\u548c\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u67b6\u6784\u8fdb\u884c\u56fe\u50cf\u53bb\u566a\u3002", "result": "\u63d0\u51fa\u7684ISR-SE\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u4fe1\u53f7\u8d28\u91cf\u548c\u6e05\u6670\u5ea6\u3002", "conclusion": "ISR-SE\u6846\u67b6\u4e3a\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\u7684\u4fe1\u53f7\u589e\u5f3a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09717", "categories": ["stat.ML", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09717", "abs": "https://arxiv.org/abs/2507.09717", "authors": ["Abdullah Karaaslanli", "Bisakh Banerjee", "Tapabrata Maiti", "Selin Aviyente"], "title": "Signed Graph Learning: Algorithms and Theory", "comment": null, "summary": "Real-world data is often represented through the relationships between data\nsamples, forming a graph structure. In many applications, it is necessary to\nlearn this graph structure from the observed data. Current graph learning\nresearch has primarily focused on unsigned graphs, which consist only of\npositive edges. However, many biological and social systems are better\ndescribed by signed graphs that account for both positive and negative\ninteractions, capturing similarity and dissimilarity between samples. In this\npaper, we develop a method for learning signed graphs from a set of smooth\nsigned graph signals. Specifically, we employ the net Laplacian as a graph\nshift operator (GSO) to define smooth signed graph signals as the outputs of a\nlow-pass signed graph filter defined by the net Laplacian. The signed graph is\nthen learned by formulating a non-convex optimization problem where the total\nvariation of the observed signals is minimized with respect to the net\nLaplacian. The proposed problem is solved using alternating direction method of\nmultipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration\ncomplexity from quadratic to linear in the number of nodes is introduced.\nFurthermore, theoretical proofs of convergence for the algorithm and a bound on\nthe estimation error of the learned net Laplacian as a function of sample size,\nnumber of nodes, and graph topology are provided. Finally, the proposed method\nis evaluated on simulated data and gene regulatory network inference problem\nand compared to existing signed graph learning methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5e73\u6ed1\u6709\u7b26\u53f7\u56fe\u4fe1\u53f7\u4e2d\u5b66\u4e60\u6709\u7b26\u53f7\u56fe\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u51c0\u62c9\u666e\u62c9\u65af\u77e9\u9635\u4f5c\u4e3a\u56fe\u79fb\u4f4d\u7b97\u5b50\uff0c\u5e76\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u95ee\u9898\u6c42\u89e3\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u5e38\u4ee5\u56fe\u7ed3\u6784\u8868\u793a\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u65e0\u7b26\u53f7\u56fe\uff0c\u800c\u8bb8\u591a\u7cfb\u7edf\u9700\u7528\u6709\u7b26\u53f7\u56fe\u63cf\u8ff0\u6b63\u8d1f\u4ea4\u4e92\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u51c0\u62c9\u666e\u62c9\u65af\u77e9\u9635\u5b9a\u4e49\u5e73\u6ed1\u4fe1\u53f7\uff0c\u901a\u8fc7\u975e\u51f8\u4f18\u5316\u6700\u5c0f\u5316\u4fe1\u53f7\u603b\u53d8\u5dee\uff0c\u91c7\u7528ADMM\u7b97\u6cd5\u6c42\u89e3\uff0c\u5e76\u5f15\u5165\u5feb\u901f\u7b97\u6cd5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u63d0\u4f9b\u4e86\u7b97\u6cd5\u6536\u655b\u6027\u8bc1\u660e\u548c\u4f30\u8ba1\u8bef\u5dee\u754c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u548c\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u63a8\u65ad\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u6709\u7b26\u53f7\u56fe\u7ed3\u6784\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u5efa\u6a21\u3002"}}
{"id": "2507.08836", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.08836", "abs": "https://arxiv.org/abs/2507.08836", "authors": ["Damien Fovet", "Shashank Chamoli", "Sarah Oury", "Srishti Singhal"], "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing", "comment": null, "summary": "This study evaluates the performance of a compression method, called\nCompactifAI, developed by Multiverse Computing, applied to the large language\nmodel Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in\nterms of energy consumption) and accuracy using respectively the frameworks\nCodecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed\nbetween the model compressed with\nCompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our\nfindings reveal that the compressed model using CompactifAI not only\nsignificantly reduced the computational resources but also maintained the model\naccuracy, making the model more efficient, scalable and cost-effective.", "AI": {"tldr": "CompactifAI\u538b\u7f29\u65b9\u6cd5\u5e94\u7528\u4e8eLlama 3.1 8B\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u8bc4\u4f30CompactifAI\u538b\u7f29\u65b9\u6cd5\u5728\u5927\u8bed\u8a00\u6a21\u578bLlama 3.1 8B\u4e0a\u7684\u6027\u80fd\uff0c\u5173\u6ce8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528Codecarbon\u548cRagas\u6846\u67b6\u5206\u522b\u8bc4\u4f30\u80fd\u6e90\u6d88\u8017\u548c\u51c6\u786e\u6027\uff0c\u5bf9\u6bd4\u538b\u7f29\u4e0e\u5b8c\u6574\u6a21\u578b\u3002", "result": "\u538b\u7f29\u6a21\u578b\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u3002", "conclusion": "CompactifAI\u538b\u7f29\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u6a21\u578b\u6548\u7387\uff0c\u9002\u5408\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2507.09244", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09244", "abs": "https://arxiv.org/abs/2507.09244", "authors": ["Nishant Gupta", "Muris Sarajlic", "Erik G. Larsson"], "title": "Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming", "comment": "Accepted for Publication in IEEE VTC-Spring 2025, held at Oslo,\n  Norway", "summary": "The dense and distributed deployment of sub-THz radio units (RUs) alongside\nsub-10 GHz access point (AP) is a promising approach to provide high data rate\nand reliable coverage for future 6G applications. However, beam search or RU\nselection for the sub-THz RUs incurs significant overhead and high power\nconsumption. To address this, we introduce a method that leverages deep\nlearning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs\nusing the sub-10 GHz channel characteristics. A novel aspect of this work is\nthe consideration of inter-band beam configuration (IBBC), defined as the\nbroadside angle between the low-band and high-band antenna patterns of the user\nequipment (UE). Since IBBC indicates the beamforming information or UE's\norientation, it is typically not shared with the network as a part of\nsignalling. Therefore, we propose a solution strategy to infer a suitable\nsub-THz RU even when UEs do not share their IBBC information. Simulation\nresults illustrate the performance of the inferred sub-THz RU and highlights\nthe detrimental impact of neglecting UE orientation on the systems performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u4ecesub-10 GHz\u4fe1\u9053\u7279\u6027\u63a8\u65ad\u5408\u9002sub-THz RU\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86beam search\u7684\u9ad8\u5f00\u9500\u95ee\u9898\uff0c\u5e76\u8003\u8651\u4e86UE\u7684IBBC\u4fe1\u606f\u3002", "motivation": "\u672a\u67656G\u5e94\u7528\u4e2d\uff0csub-THz RU\u7684\u5bc6\u96c6\u90e8\u7f72\u867d\u80fd\u63d0\u4f9b\u9ad8\u6570\u636e\u7387\u548c\u53ef\u9760\u8986\u76d6\uff0c\u4f46beam search\u6216RU\u9009\u62e9\u4f1a\u5e26\u6765\u9ad8\u5f00\u9500\u548c\u529f\u8017\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u4ecesub-10 GHz\u4fe1\u9053\u7279\u6027\u63a8\u65adsub-THz RU\u5019\u9009\uff0c\u5e76\u8003\u8651UE\u7684IBBC\uff08\u4f4e\u9ad8\u9891\u6bb5\u5929\u7ebf\u6a21\u5f0f\u7684\u5939\u89d2\uff09\u4fe1\u606f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u63a8\u65ad\u7684sub-THz RU\u6027\u80fd\u826f\u597d\uff0c\u5ffd\u7565UE\u65b9\u5411\u4f1a\u5bf9\u7cfb\u7edf\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11sub-THz RU\u9009\u62e9\u7684\u5f00\u9500\uff0c\u4e14\u65e0\u9700UE\u5171\u4eabIBBC\u4fe1\u606f\u3002"}}
{"id": "2507.09740", "categories": ["stat.ML", "cs.LG", "math.DS", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.09740", "abs": "https://arxiv.org/abs/2507.09740", "authors": ["Ridwan Olabiyi", "Han Hu", "Ashif Iquebal"], "title": "Discovering Governing Equations in the Presence of Uncertainty", "comment": "24 pages, 5 figures", "summary": "In the study of complex dynamical systems, understanding and accurately\nmodeling the underlying physical processes is crucial for predicting system\nbehavior and designing effective interventions. Yet real-world systems exhibit\npronounced input (or system) variability and are observed through noisy,\nlimited data conditions that confound traditional discovery methods that assume\nfixed-coefficient deterministic models. In this work, we theorize that\naccounting for system variability together with measurement noise is the key to\nconsistently discover the governing equations underlying dynamical systems. As\nsuch, we introduce a stochastic inverse physics-discovery (SIP) framework that\ntreats the unknown coefficients as random variables and infers their posterior\ndistribution by minimizing the Kullback-Leibler divergence between the\npush-forward of the posterior samples and the empirical data distribution.\nBenchmarks on four canonical problems -- the Lotka-Volterra predator-prey\nsystem (multi- and single-trajectory), the historical Hudson Bay lynx-hare\ndata, the chaotic Lorenz attractor, and fluid infiltration in porous media\nusing low- and high-viscosity liquids -- show that SIP consistently identifies\nthe correct equations and lowers coefficient root-mean-square error by an\naverage of 82\\% relative to the Sparse Identification of Nonlinear Dynamics\n(SINDy) approach and its Bayesian variant. The resulting posterior\ndistributions yield 95\\% credible intervals that closely track the observed\ntrajectories, providing interpretable models with quantified uncertainty. SIP\nthus provides a robust, data-efficient approach for consistent physics\ndiscovery in noisy, variable, and data-limited settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u9006\u7269\u7406\u53d1\u73b0\uff08SIP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u7cfb\u7edf\u53d8\u5f02\u6027\u548c\u6d4b\u91cf\u566a\u58f0\uff0c\u4ece\u566a\u58f0\u548c\u6709\u9650\u6570\u636e\u4e2d\u51c6\u786e\u53d1\u73b0\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u63a7\u5236\u65b9\u7a0b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u7cfb\u6570\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u7cfb\u7edf\u4e2d\u7684\u8f93\u5165\u53d8\u5f02\u6027\u548c\u566a\u58f0\u89c2\u6d4b\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "SIP\u6846\u67b6\u5c06\u672a\u77e5\u7cfb\u6570\u89c6\u4e3a\u968f\u673a\u53d8\u91cf\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u540e\u9a8c\u6837\u672c\u4e0e\u7ecf\u9a8c\u6570\u636e\u5206\u5e03\u4e4b\u95f4\u7684Kullback-Leibler\u6563\u5ea6\u63a8\u65ad\u5176\u5206\u5e03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIP\u6bd4SINDy\u53ca\u5176\u8d1d\u53f6\u65af\u53d8\u4f53\u5e73\u5747\u964d\u4f4e\u4e8682%\u7684\u7cfb\u6570\u5747\u65b9\u6839\u8bef\u5dee\uff0c\u5e76\u80fd\u63d0\u4f9b\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u3002", "conclusion": "SIP\u4e3a\u566a\u58f0\u3001\u53d8\u5f02\u6027\u548c\u6570\u636e\u6709\u9650\u7684\u73af\u5883\u4e0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u6570\u636e\u9ad8\u6548\u7684\u7269\u7406\u53d1\u73b0\u65b9\u6cd5\u3002"}}
{"id": "2507.08838", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08838", "abs": "https://arxiv.org/abs/2507.08838", "authors": ["Xiaohang Tang", "Rares Dolga", "Sangwoong Yoon", "Ilija Bogunovic"], "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models", "comment": "Preprint", "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3awd1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u4f3c\u7136\u4f18\u5316\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u907f\u514d\u4e86\u4f20\u7edfRL\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u4f20\u7edfRL\u65b9\u6cd5\u56e0\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u5904\u7406\u800c\u5f15\u5165\u8ba1\u7b97\u5f00\u9500\u548c\u6f5c\u5728\u504f\u5dee\u3002", "method": "\u63d0\u51fawd1\u65b9\u6cd5\uff0c\u5c06\u76ee\u6807\u91cd\u65b0\u8868\u8ff0\u4e3a\u52a0\u6743\u4f3c\u7136\uff0c\u4ec5\u9700\u8fd1\u4f3c\u5f53\u524d\u53c2\u6570\u5316\u7b56\u7565\u7684\u4f3c\u7136\uff0c\u7b80\u5316\u4e86\u8ba1\u7b97\u3002", "result": "\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cwd1\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u6216\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709RL\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u534716%\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u68af\u5ea6\u6b65\u6570\u3002", "conclusion": "wd1\u56e0\u5176\u9ad8\u6548\u6027\u548c\u7b80\u5355\u6027\uff0c\u6210\u4e3a\u4f18\u5316dLLMs\u63a8\u7406\u7684\u66f4\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.09268", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09268", "abs": "https://arxiv.org/abs/2507.09268", "authors": ["Xiangjun Li", "Zilong Liu", "Zhengchun Zhou", "Pingzhi Fan"], "title": "Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has recently emerged as an\nexcellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is\nproposed whereby the delay-Doppler (DD) coupling phase is considered.\nSpecifically, we study matched filtering (MF) assisted channel estimation (CE)\nfor AFDM systems in complex doubly selective channels. By deriving the complete\ninput-output relationship, the inter-chirp-carrier interference,\nsignal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of\nAFDM, are investigated in discrete affine Fourier transform (DAFT) domain.\nFurther, we look into the path ambiguity problem and show that it may lead to\nsevere performance deterioration in fractional-delay fractional-Doppler\nchannels. To address such a problem, we introduce an MF assisted CE scheme\nbuilding upon a novel pilot arrangement across two consecutive AFDM\ntransmissions. This allows us to sequentially estimate the parameters of each\npath by exploiting the separability and approximate orthogonality of different\npaths in the DAFT domain, thus leading to significantly reduced complexity.\nFurthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is\nproposed to avoid significantly redundant computation, which can be extended to\ntypical wide-band systems. Extensive simulation results indicate that the\nproposed schemes offer superior advantages in terms of their improved\ncommunication performance and lower complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578bAFDM\u6ce2\u5f62\uff0c\u901a\u8fc7\u8003\u8651\u5ef6\u8fdf-\u591a\u666e\u52d2\u8026\u5408\u76f8\u4f4d\uff0c\u7814\u7a76\u4e86\u590d\u6742\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e2d\u7684\u5339\u914d\u6ee4\u6ce2\u8f85\u52a9\u4fe1\u9053\u4f30\u8ba1\u65b9\u6848\uff0c\u5e76\u89e3\u51b3\u4e86\u8def\u5f84\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "AFDM\u4f5c\u4e3a6G\u6ce2\u5f62\u5177\u6709\u540e\u5411\u517c\u5bb9\u6027\uff0c\u4f46\u5728\u590d\u6742\u4fe1\u9053\u4e2d\u6027\u80fd\u53d7\u9650\uff0c\u9700\u6539\u8fdb\u4fe1\u9053\u4f30\u8ba1\u548c\u8def\u5f84\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u5206\u6790\u5e72\u6270\u548cSINR\uff0c\u63d0\u51faMF\u8f85\u52a9CE\u65b9\u6848\u548c\u57fa\u4e8eGFS\u7684MF-GFS\u65b9\u6848\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u901a\u4fe1\u6027\u80fd\u548c\u590d\u6742\u5ea6\u4e0a\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u589e\u5f3a\u578bAFDM\u53caMF\u8f85\u52a9CE\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.09828", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09828", "abs": "https://arxiv.org/abs/2507.09828", "authors": ["Shion Takeno", "Yu Inatsu", "Masayuki Karasuyama", "Ichiro Takeuchi"], "title": "Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization", "comment": "35pages, 5 figures", "summary": "Bayesian optimization is a powerful tool for optimizing an\nexpensive-to-evaluate black-box function. In particular, the effectiveness of\nexpected improvement (EI) has been demonstrated in a wide range of\napplications. However, theoretical analyses of EI are limited compared with\nother theoretically established algorithms. This paper analyzes a randomized\nvariant of EI, which evaluates the EI from the maximum of the posterior sample\npath. We show that this posterior sampling-based random EI achieves the\nsublinear Bayesian cumulative regret bounds under the assumption that the\nblack-box function follows a Gaussian process. Finally, we demonstrate the\neffectiveness of the proposed method through numerical experiments.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u540e\u9a8c\u91c7\u6837\u7684\u968f\u673a\u671f\u671b\u6539\u8fdb\uff08EI\uff09\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u9ad8\u65af\u8fc7\u7a0b\u5047\u8bbe\u4e0b\u80fd\u5b9e\u73b0\u6b21\u7ebf\u6027\u8d1d\u53f6\u65af\u7d2f\u79ef\u9057\u61be\u754c\u3002", "motivation": "\u5c3d\u7ba1\u671f\u671b\u6539\u8fdb\uff08EI\uff09\u5728\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7406\u8bba\u5206\u6790\u76f8\u5bf9\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5316\u7684EI\u53d8\u4f53\uff0c\u901a\u8fc7\u4ece\u540e\u9a8c\u6837\u672c\u8def\u5f84\u7684\u6700\u5927\u503c\u8bc4\u4f30EI\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u9ad8\u65af\u8fc7\u7a0b\u5047\u8bbe\u4e0b\u80fd\u5b9e\u73b0\u6b21\u7ebf\u6027\u8d1d\u53f6\u65af\u7d2f\u79ef\u9057\u61be\u754c\u3002", "conclusion": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.08839", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.08839", "abs": "https://arxiv.org/abs/2507.08839", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "comment": "MICCAI 2025", "summary": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that\nimposes a significant burden on public health. It shares clinical similarities\nwith Alzheimer's disease (AD), as both progress through stages of normal\ncognition, mild cognitive impairment, and dementia. A major obstacle in LBD\ndiagnosis is data scarcity, which limits the effectiveness of deep learning. In\ncontrast, AD datasets are more abundant, offering potential for knowledge\ntransfer. However, LBD and AD data are typically collected from different sites\nusing different machines and protocols, resulting in a distinct domain shift.\nTo effectively leverage AD data while mitigating domain shift, we propose a\nTransferability Aware Transformer (TAT) that adapts knowledge from AD to\nenhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived\nfrom structural MRI as training data. Built on the attention mechanism, TAT\nadaptively assigns greater weights to disease-transferable features while\nsuppressing domain-specific ones, thereby reducing domain shift and improving\ndiagnostic accuracy with limited LBD data. The experimental results demonstrate\nthe effectiveness of TAT. To the best of our knowledge, this is the first study\nto explore domain adaptation from AD to LBD under conditions of data scarcity\nand domain shift, providing a promising framework for domain-adaptive diagnosis\nof rare diseases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff08TAT\uff09\uff0c\u5229\u7528\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u6570\u636e\u589e\u5f3a\u8def\u6613\u4f53\u75f4\u5446\uff08LBD\uff09\u7684\u8bca\u65ad\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002", "motivation": "LBD\u662f\u4e00\u79cd\u5e38\u89c1\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u75f4\u5446\u75c7\uff0c\u8bca\u65ad\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u7684\u6311\u6218\uff0c\u800cAD\u6570\u636e\u4e30\u5bcc\u4f46\u5b58\u5728\u9886\u57df\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u7ed3\u6784MRI\u63d0\u53d6\u7684\u7ed3\u6784\u8fde\u63a5\u6027\uff08SC\uff09\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7TAT\u81ea\u9002\u5e94\u5206\u914d\u6743\u91cd\uff0c\u7a81\u51fa\u75be\u75c5\u53ef\u8f6c\u79fb\u7279\u5f81\uff0c\u6291\u5236\u9886\u57df\u7279\u5f02\u6027\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTAT\u6709\u6548\u51cf\u5c11\u4e86\u9886\u57df\u504f\u79fb\uff0c\u63d0\u9ad8\u4e86LBD\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "conclusion": "TAT\u4e3a\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u6761\u4ef6\u4e0b\u7684\u7f55\u89c1\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u3002"}}
{"id": "2507.09386", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09386", "abs": "https://arxiv.org/abs/2507.09386", "authors": ["Ruangrawee Kitichotkul", "Shashwath Bharadwaj", "Joshua Rapp", "Yanting Ma", "Alexander Mehta", "Vivek K Goyal"], "title": "Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging", "comment": "20 pages, 15 figures, to be presented at the International Conference\n  on Computer Vision (ICCV) 2025", "summary": "Conventional wisdom suggests that single-photon lidar (SPL) should operate in\nlow-light conditions to minimize dead-time effects. Many methods have been\ndeveloped to mitigate these effects in synchronous SPL systems. However,\nsolutions for free-running SPL remain limited despite the advantage of reduced\nhistogram distortion from dead times. To improve the accuracy of free-running\nSPL, we propose a computationally efficient joint maximum likelihood estimator\nof the signal flux, the background flux, and the depth using only histograms,\nalong with a complementary regularization framework that incorporates a learned\npoint cloud score model as a prior. Simulations and experiments demonstrate\nthat free-running SPL yields lower estimation errors than its synchronous\ncounterpart under identical conditions, with our regularization further\nimproving accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b63\u5219\u5316\u6846\u67b6\uff0c\u63d0\u5347\u81ea\u7531\u8fd0\u884c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\uff08SPL\uff09\u7684\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u7531\u8fd0\u884cSPL\u5728\u51cf\u5c11\u76f4\u65b9\u56fe\u5931\u771f\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u6709\u9650\uff0c\u9700\u63d0\u9ad8\u5176\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u8054\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u8ba1\u7b97\u4fe1\u53f7\u901a\u91cf\u3001\u80cc\u666f\u901a\u91cf\u548c\u6df1\u5ea6\uff0c\u5e76\u7ed3\u5408\u5b66\u4e60\u7684\u70b9\u4e91\u8bc4\u5206\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u7684\u6b63\u5219\u5316\u6846\u67b6\u3002", "result": "\u81ea\u7531\u8fd0\u884cSPL\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u6bd4\u540c\u6b65SPL\u4f30\u8ba1\u8bef\u5dee\u66f4\u4f4e\uff0c\u6b63\u5219\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7cbe\u5ea6\u3002", "conclusion": "\u81ea\u7531\u8fd0\u884cSPL\u7ed3\u5408\u6240\u63d0\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u540c\u6b65SPL\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10154", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10154", "abs": "https://arxiv.org/abs/2507.10154", "authors": ["Ricardo In\u00e1cio", "Zafeiris Kokkinogenis", "Vitor Cerqueira", "Carlos Soares"], "title": "Simulating Biases for Interpretable Fairness in Offline and Online Classifiers", "comment": "17 pages, 2 figures, 1 equation, 3 tables: 1 in main body and 2 in\n  the appendix. Submitted to the SynDAiTE: Synthetic Data for AI\n  Trustworthiness and Evolution workshop from ECMLPKDD 2025, anonymized", "summary": "Predictive models often reinforce biases which were originally embedded in\ntheir training data, through skewed decisions. In such cases, mitigation\nmethods are critical to ensure that, regardless of the prevailing disparities,\nmodel outcomes are adjusted to be fair. To assess this, datasets could be\nsystematically generated with specific biases, to train machine learning\nclassifiers. Then, predictive outcomes could aid in the understanding of this\nbias embedding process. Hence, an agent-based model (ABM), depicting a loan\napplication process that represents various systemic biases across two\ndemographic groups, was developed to produce synthetic datasets. Then, by\napplying classifiers trained on them to predict loan outcomes, we can assess\nhow biased data leads to unfairness. This highlights a main contribution of\nthis work: a framework for synthetic dataset generation with controllable bias\ninjection. We also contribute with a novel explainability technique, which\nshows how mitigations affect the way classifiers leverage data features, via\nsecond-order Shapley values. In experiments, both offline and online learning\napproaches are employed. Mitigations are applied at different stages of the\nmodelling pipeline, such as during pre-processing and in-processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u53ef\u63a7\u504f\u89c1\u7684\u6846\u67b6\uff0c\u5e76\u5229\u7528\u65b0\u578b\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8bc4\u4f30\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u9884\u6d4b\u6a21\u578b\u5e38\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u504f\u89c1\u800c\u5f3a\u5316\u4e0d\u516c\u5e73\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u65b9\u6cd5\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u79cd\u504f\u89c1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\uff08ABM\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u6a21\u62df\u8d37\u6b3e\u7533\u8bf7\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5e76\u5e94\u7528\u5206\u7c7b\u5668\u9884\u6d4b\u8d37\u6b3e\u7ed3\u679c\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636Shapley\u503c\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u504f\u89c1\u6570\u636e\u4f1a\u5bfc\u81f4\u4e0d\u516c\u5e73\u9884\u6d4b\uff0c\u800c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6280\u672f\u80fd\u6709\u6548\u8bc4\u4f30\u548c\u7f13\u89e3\u504f\u89c1\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u504f\u89c1\u6570\u636e\u751f\u6210\u548c\u7f13\u89e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6280\u672f\u63ed\u793a\u4e86\u7f13\u89e3\u65b9\u6cd5\u5bf9\u5206\u7c7b\u5668\u884c\u4e3a\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.08841", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08841", "abs": "https://arxiv.org/abs/2507.08841", "authors": ["Kun Jing", "Luoyu Chen", "Jungang Xu", "Jianwei Tai", "Yiyu Wang", "Shuaimin Li"], "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation", "comment": null, "summary": "Neural architecture search (NAS) is a promising approach for automatically\ndesigning neural network architectures. However, the architecture estimation of\nNAS is computationally expensive and time-consuming because of training\nmultiple architectures from scratch. Although existing zero-shot NAS methods\nuse training-free proxies to accelerate the architecture estimation, their\neffectiveness, stability, and generality are still lacking. We present a novel\ntraining-free estimation proxy called weighted response correlation (WRCor).\nWRCor utilizes correlation coefficient matrices of responses across different\ninput samples to calculate the proxy scores of estimated architectures, which\ncan measure their expressivity and generalizability. Experimental results on\nproxy evaluation demonstrate that WRCor and its voting proxies are more\nefficient estimation strategies than existing proxies. We also apply them with\ndifferent search strategies in architecture search. Experimental results on\narchitecture search show that our zero-shot NAS algorithm outperforms most\nexisting NAS algorithms in different search spaces. Our NAS algorithm can\ndiscover an architecture with a 22.1% test error on the ImageNet-1k dataset\nwithin 4 GPU hours. All codes are publicly available at\nhttps://github.com/kunjing96/ZSNAS-WRCor.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWRCor\u7684\u65e0\u8bad\u7ec3\u4f30\u8ba1\u4ee3\u7406\uff0c\u7528\u4e8e\u52a0\u901f\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672cNAS\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3001\u7a33\u5b9a\u6027\u548c\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u4f30\u8ba1\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u54cd\u5e94\u76f8\u5173\u6027\uff08WRCor\uff09\u4f5c\u4e3a\u8bad\u7ec3\u514d\u8d39\u4ee3\u7406\uff0c\u901a\u8fc7\u54cd\u5e94\u6837\u672c\u95f4\u7684\u76f8\u5173\u7cfb\u6570\u77e9\u9635\u8ba1\u7b97\u4ee3\u7406\u5206\u6570\u3002", "result": "WRCor\u53ca\u5176\u6295\u7968\u4ee3\u7406\u5728\u4ee3\u7406\u8bc4\u4f30\u4e2d\u8868\u73b0\u66f4\u4f18\uff0cNAS\u7b97\u6cd5\u5728\u4e0d\u540c\u641c\u7d22\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cImageNet-1k\u4e0a\u4ec5\u97004 GPU\u5c0f\u65f6\u5373\u53ef\u53d1\u73b022.1%\u6d4b\u8bd5\u9519\u8bef\u7684\u67b6\u6784\u3002", "conclusion": "WRCor\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u96f6\u6837\u672cNAS\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67b6\u6784\u641c\u7d22\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.09408", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09408", "abs": "https://arxiv.org/abs/2507.09408", "authors": ["Sajedeh Norouzi", "Mostafa Rahmani", "Yi Chu", "Torsten Braun", "Kaushik Chowdhury", "Alister Burr"], "title": "Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation", "comment": "Accepted in IEEE PIMRC 2025", "summary": "Effective channel estimation CE is critical for optimizing the performance of\n5G New Radio NR systems particularly in dynamic environments where traditional\nmethods struggle with complexity and adaptability This paper introduces\nGraphNet a novel lightweight Graph Neural Network GNNbased estimator designed\nto enhance CE in 5G NR Our proposed method utilizes a GNN architecture that\nminimizes computational overhead while capturing essential features necessary\nfor accurate CE We evaluate GraphNet across various channel conditions from\nslowvarying to highly dynamic environments and compare its performance to\nChannelNet a wellknown deep learningbased CE method GraphNet not only matches\nChannelNets performance in stable conditions but significantly outperforms it\nin highvariation scenarios particularly in terms of Block Error Rate It also\nincludes builtin noise estimation that enhances robustness in challenging\nchannel conditions Furthermore its significantly lighter computational\nfootprint makes GraphNet highly suitable for realtime deployment especially on\nedge devices with limited computational resources By underscoring the potential\nof GNNs to transform CE processes GraphNet offers a scalable and robust\nsolution that aligns with the evolving demands of 5G technologies highlighting\nits efficiency and performance as a nextgeneration solution for wireless\ncommunication systems", "AI": {"tldr": "GraphNet\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8f7b\u91cf\u7ea7\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u53475G NR\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u5728\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528GNN\u67b6\u6784\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u6355\u6349\u5173\u952e\u7279\u5f81\u4ee5\u63d0\u5347\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "result": "GraphNet\u5728\u7a33\u5b9a\u6761\u4ef6\u4e0b\u4e0eChannelNet\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "GraphNet\u5c55\u793a\u4e86GNN\u5728\u4fe1\u9053\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a5G\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10303", "categories": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.10303", "abs": "https://arxiv.org/abs/2507.10303", "authors": ["K. Giannoukou", "X. Zhu", "S. Marelli", "B. Sudret"], "title": "MF-GLaM: A multifidelity stochastic emulator using generalized lambda models", "comment": null, "summary": "Stochastic simulators exhibit intrinsic stochasticity due to unobservable,\nuncontrollable, or unmodeled input variables, resulting in random outputs even\nat fixed input conditions. Such simulators are common across various scientific\ndisciplines; however, emulating their entire conditional probability\ndistribution is challenging, as it is a task traditional deterministic\nsurrogate modeling techniques are not designed for. Additionally, accurately\ncharacterizing the response distribution can require prohibitively large\ndatasets, especially for computationally expensive high-fidelity (HF)\nsimulators. When lower-fidelity (LF) stochastic simulators are available, they\ncan enhance limited HF information within a multifidelity surrogate modeling\n(MFSM) framework. While MFSM techniques are well-established for deterministic\nsettings, constructing multifidelity emulators to predict the full conditional\nresponse distribution of stochastic simulators remains a challenge. In this\npaper, we propose multifidelity generalized lambda models (MF-GLaMs) to\nefficiently emulate the conditional response distribution of HF stochastic\nsimulators by exploiting data from LF stochastic simulators. Our approach\nbuilds upon the generalized lambda model (GLaM), which represents the\nconditional distribution at each input by a flexible, four-parameter\ngeneralized lambda distribution. MF-GLaMs are non-intrusive, requiring no\naccess to the internal stochasticity of the simulators nor multiple\nreplications of the same input values. We demonstrate the efficacy of MF-GLaM\nthrough synthetic examples of increasing complexity and a realistic earthquake\napplication. Results show that MF-GLaMs can achieve improved accuracy at the\nsame cost as single-fidelity GLaMs, or comparable performance at significantly\nreduced cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4fdd\u771f\u5ea6\u5e7f\u4e49\u03bb\u6a21\u578b\uff08MF-GLaM\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u9ad8\u4fdd\u771f\u5ea6\u968f\u673a\u6a21\u62df\u5668\u7684\u6761\u4ef6\u54cd\u5e94\u5206\u5e03\uff0c\u5229\u7528\u4f4e\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u673a\u6a21\u62df\u5668\u7684\u6761\u4ef6\u6982\u7387\u5206\u5e03\u6a21\u62df\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u786e\u5b9a\u6027\u4ee3\u7406\u6a21\u578b\u65e0\u6cd5\u80dc\u4efb\uff0c\u4e14\u9ad8\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u3002\u4f4e\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u6570\u636e\u53ef\u7528\u4e8e\u589e\u5f3a\u4fe1\u606f\u3002", "method": "\u57fa\u4e8e\u5e7f\u4e49\u03bb\u6a21\u578b\uff08GLaM\uff09\uff0c\u6784\u5efa\u591a\u4fdd\u771f\u5ea6\u5e7f\u4e49\u03bb\u6a21\u578b\uff08MF-GLaM\uff09\uff0c\u5229\u7528\u4f4e\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u6570\u636e\u9ad8\u6548\u6a21\u62df\u9ad8\u4fdd\u771f\u5ea6\u6a21\u62df\u5668\u7684\u6761\u4ef6\u54cd\u5e94\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6848\u4f8b\u548c\u5b9e\u9645\u5730\u9707\u5e94\u7528\u9a8c\u8bc1\uff0cMF-GLaM\u5728\u76f8\u540c\u6210\u672c\u4e0b\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u6216\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u4e0b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "MF-GLaM\u4e3a\u968f\u673a\u6a21\u62df\u5668\u7684\u6761\u4ef6\u54cd\u5e94\u5206\u5e03\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u975e\u4fb5\u5165\u6027\u7684\u591a\u4fdd\u771f\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2507.08842", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08842", "abs": "https://arxiv.org/abs/2507.08842", "authors": ["Zhufeng Lu", "Chentao Jia", "Ming Hu", "Xiaofei Xie", "Mingsong Chen"], "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing", "comment": "This paper has been accepted by ACM SIGKDD 2025", "summary": "As a promising privacy-aware collaborative model training paradigm, Federated\nLearning (FL) is becoming popular in the design of distributed recommender\nsystems. However, Federated Recommender Systems (FedRecs) greatly suffer from\ntwo major problems: i) extremely high communication overhead due to massive\nitem embeddings involved in recommendation systems, and ii) intolerably low\ntraining efficiency caused by the entanglement of both heterogeneous network\nenvironments and client devices. Although existing methods attempt to employ\nvarious compression techniques to reduce communication overhead, due to the\nparameter errors introduced by model compression, they inevitably suffer from\nmodel performance degradation. To simultaneously address the above problems,\nthis paper presents a communication-efficient FedRec framework named FedRAS,\nwhich adopts an action-sharing strategy to cluster the gradients of item\nembedding into a specific number of model updating actions for communication\nrather than directly compressing the item embeddings. In this way, the cloud\nserver can use the limited actions from clients to update all the items. Since\ngradient values are significantly smaller than item embeddings, constraining\nthe directions of gradients (i.e., the action space) introduces smaller errors\ncompared to compressing the entire item embedding matrix into a reduced space.\nTo accommodate heterogeneous devices and network environments, FedRAS\nincorporates an adaptive clustering mechanism that dynamically adjusts the\nnumber of actions. Comprehensive experiments on well-known datasets demonstrate\nthat FedRAS can reduce the size of communication payloads by up to 96.88%,\nwhile not sacrificing recommendation performance within various heterogeneous\nscenarios. We have open-sourced FedRAS at\nhttps://github.com/mastlab-T3S/FedRAS.", "AI": {"tldr": "FedRAS\u662f\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u4f5c\u5171\u4eab\u7b56\u7565\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u540c\u65f6\u907f\u514d\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff08FedRecs\uff09\u9762\u4e34\u901a\u4fe1\u5f00\u9500\u9ad8\u548c\u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u52a8\u4f5c\u5171\u4eab\u7b56\u7565\uff0c\u5c06\u68af\u5ea6\u805a\u7c7b\u4e3a\u6709\u9650\u7684\u52a8\u4f5c\u8fdb\u884c\u901a\u4fe1\uff0c\u800c\u975e\u76f4\u63a5\u538b\u7f29\u5d4c\u5165\u77e9\u9635\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u805a\u7c7b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedRAS\u80fd\u51cf\u5c1196.88%\u7684\u901a\u4fe1\u8d1f\u8f7d\uff0c\u4e14\u4e0d\u5f71\u54cd\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "FedRAS\u6709\u6548\u89e3\u51b3\u4e86\u901a\u4fe1\u548c\u6548\u7387\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u73af\u5883\u3002"}}
{"id": "2507.09458", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09458", "abs": "https://arxiv.org/abs/2507.09458", "authors": ["Wang Ning", "Zhang Chenyu", "Sun Yanshi", "Min Minghui", "Liu Yuanwei", "Li Shiyin"], "title": "An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation", "comment": "13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is\n  Paper-TW-Jul-25-1790", "summary": "Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which\neffectively utilizes both NOMA and orthogonal multiple access (OMA)\ntechnologies through flexible resource allocation in a single transmission, has\ndemonstrated immense potential for enhancing the performance of wireless\ncommunication systems. To further release the potential of HNOMA, this paper\nproposes a novel design of H-NOMA which jointly incorporates hybrid successive\ninterference cancellation (HSIC) and power adaptation (PA) in the NOMA\ntransmission phase. To reveal the potential of the proposed HSIC-PA aided\nH-NOMA scheme, closed-form expression for the probability of the event that\nH-NOMA can achieve a higher data rate than pure OMA by consuming less energy is\nrigorously derived. Furthermore, the asymptotic analysis demonstrates that the\nprobability of the proposed H-NOMA scheme approaches 1 in the high\nsignal-to-noise ratio (SNR) regime without any constraints on either users'\ntarget rates or transmit power ratios. This represents a significant\nimprovement over conventional H-NOMA schemes, which require specific\nrestrictive conditions to achieve probability 1 at high SNRs as shown in\nexisting work. The above observation indicates that with less energy\nconsumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate\nthan pure OMA with probability 1 at high SNRs, and hence a higher energy\nefficiency. Finally, numerical results are provided to verify the accuracy of\nthe analysis and also demonstrate the superior performance of the proposed\nH-NOMA scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u8fde\u7eed\u5e72\u6270\u6d88\u9664\uff08HSIC\uff09\u548c\u529f\u7387\u9002\u5e94\uff08PA\uff09\u7684H-NOMA\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u80fd\u6548\u65b9\u9762\u4f18\u4e8e\u7eafOMA\u3002", "motivation": "\u4e3a\u4e86\u8fdb\u4e00\u6b65\u91ca\u653eH-NOMA\u7684\u6f5c\u529b\uff0c\u7ed3\u5408HSIC\u548cPA\u6280\u672f\uff0c\u63d0\u5347\u6570\u636e\u4f20\u8f93\u901f\u7387\u548c\u80fd\u6548\u3002", "method": "\u63d0\u51faHSIC-PA\u8f85\u52a9\u7684H-NOMA\u65b9\u6848\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u63a8\u5bfc\u548c\u6e10\u8fd1\u5206\u6790\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\uff0c\u8be5\u65b9\u6848\u80fd\u4ee5\u66f4\u4f4e\u80fd\u8017\u5b9e\u73b0\u6bd4\u7eafOMA\u66f4\u9ad8\u7684\u6570\u636e\u4f20\u8f93\u901f\u7387\uff0c\u4e14\u6982\u7387\u8d8b\u8fd1\u4e8e1\u3002", "conclusion": "HSIC-PA\u8f85\u52a9\u7684H-NOMA\u65b9\u6848\u5728\u80fd\u6548\u548c\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.10443", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10443", "abs": "https://arxiv.org/abs/2507.10443", "authors": ["Xin Li"], "title": "Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport", "comment": null, "summary": "We present the Context-Content Uncertainty Principle (CCUP), a unified\nframework that models cognition as the directed flow of information between\nhigh-entropy context and low-entropy content. Inference emerges as a cycle of\nbidirectional interactions, bottom-up contextual disambiguation paired with\ntop-down content reconstruction, which resolves the Information Bottleneck in\nOptimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy\nminimization, CCUP steers representations toward minimal joint uncertainty\nwhile preserving inferential directionality. Local cycle completion underpins\ntemporal bootstrapping, chaining simulations to refine memory, and spatial\nbootstrapping, enabling compositional hierarchical inference. We prove a Delta\nConvergence Theorem showing that recursive entropy minimization yields\ndelta-like attractors in latent space, stabilizing perceptual schemas and motor\nplans. Temporal bootstrapping through perception-action loops and sleep-wake\nconsolidation further transforms episodic traces into semantic knowledge.\nExtending CCUP, each hierarchical level performs delta-seeded inference:\nlow-entropy content seeds diffuse outward along goal-constrained paths shaped\nby top-down priors and external context, confining inference to task-relevant\nmanifolds and circumventing the curse of dimensionality. Building on this, we\npropose that language emerges as a symbolic transport system, externalizing\nlatent content to synchronize inference cycles across individuals. Together,\nthese results establish iBOT as a foundational principle of information flow in\nboth individual cognition and collective intelligence, positioning recursive\ninference as the structured conduit through which minds adapt, align, and\nextend.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8ba4\u77e5\u6846\u67b6CCUP\uff0c\u901a\u8fc7\u4fe1\u606f\u5728\u9ad8\u4f4e\u71b5\u4e4b\u95f4\u7684\u6d41\u52a8\u5efa\u6a21\u8ba4\u77e5\uff0c\u89e3\u51b3\u4e86\u4fe1\u606f\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u548c\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u89e3\u91ca\u4fe1\u606f\u5982\u4f55\u5728\u9ad8\u4f4e\u71b5\u4e4b\u95f4\u6d41\u52a8\uff0c\u5e76\u89e3\u51b3\u4fe1\u606f\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528Rao-Blackwellized\u53d8\u5206\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u5411\u4ea4\u4e92\u5faa\u73af\uff08\u4e0a\u4e0b\u884c\u4fe1\u606f\u6d41\uff09\u5b9e\u73b0\u4fe1\u606f\u4f18\u5316\u3002", "result": "\u8bc1\u660e\u4e86Delta\u6536\u655b\u5b9a\u7406\uff0c\u5c55\u793a\u4e86\u9012\u5f52\u71b5\u6700\u5c0f\u5316\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u6269\u5c55\u4e86\u8bed\u8a00\u4f5c\u4e3a\u7b26\u53f7\u4f20\u8f93\u7cfb\u7edf\u7684\u89c2\u70b9\u3002", "conclusion": "CCUP\u4e3a\u4e2a\u4f53\u8ba4\u77e5\u548c\u96c6\u4f53\u667a\u80fd\u4e2d\u7684\u4fe1\u606f\u6d41\u52a8\u63d0\u4f9b\u4e86\u57fa\u7840\u539f\u5219\uff0c\u9012\u5f52\u63a8\u65ad\u662f\u5fc3\u667a\u9002\u5e94\u548c\u6269\u5c55\u7684\u7ed3\u6784\u6027\u901a\u9053\u3002"}}
{"id": "2507.08843", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08843", "abs": "https://arxiv.org/abs/2507.08843", "authors": ["Arpita Soni", "Sahil Tripathi", "Gautam Siddharth Kashyap", "Manaswi Kulahara", "Mohammad Anas Azeez", "Zohaib Hasan Siddiqui", "Nipun Joshi", "Jiechao Gao"], "title": "Can We Predict Your Next Move Without Breaking Your Privacy?", "comment": "Accepted in the 17th International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025\n  in Ontario, Canada", "summary": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.", "AI": {"tldr": "FLLL3M\u662f\u4e00\u4e2a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u7528\u4e8e\u4e0b\u4e00\u4f4d\u7f6e\u9884\u6d4b\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u4e0b\u4e00\u4f4d\u7f6e\u9884\u6d4b\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u548c\u8d44\u6e90\u6548\u7387\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4fdd\u7559\u7528\u6237\u6570\u636e\u672c\u5730\u5316\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5916\u79ef\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\uff0c\u53c2\u6570\u51cf\u5c1145.6%\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1152.7%\u3002", "conclusion": "FLLL3M\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u4e0b\u4e00\u4f4d\u7f6e\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09535", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09535", "abs": "https://arxiv.org/abs/2507.09535", "authors": ["Chaoran Li", "Xingguo Xu", "Siyuan Mu"], "title": "Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs", "comment": null, "summary": "In the context of Synthetic Aperture Radar (SAR) image recognition,\ntraditional methods often struggle with the intrinsic limitations of SAR data,\nsuch as weak texture, high noise, and ambiguous object boundaries. This work\nexplores a novel perspective by reformulating SAR target recognition as a\nmultimodal reasoning task. We leverage multimodal large language models\n(MLLMs), specifically GPT-4o, to perform target classification based on SAR\nimagery, guided by candidate categories and enhanced with Chain-of-Thought\n(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,\ncomprising raw SAR images, structured target annotations, candidate label sets,\nand GPT-generated CoT reasoning chains. Experimental results show that the\nMLLMs are capable of generating logically coherent and interpretable inferences\nin most scenarios. Our analysis highlights both the strengths and current\nlimitations of MLLMs in interpreting SAR imagery, and we provide detailed\ninsights into model behavior through failure case analysis. This work\ndemonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines\nand establishes a foundation for future research in SAR-oriented visual\nreasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684SAR\u76ee\u6807\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7Chain-of-Thought\u63a8\u7406\u589e\u5f3a\u5206\u7c7b\u6548\u679c\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u903b\u8f91\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edfSAR\u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u56fa\u6709\u7f3a\u9677\uff08\u5982\u5f31\u7eb9\u7406\u3001\u9ad8\u566a\u58f0\u3001\u6a21\u7cca\u8fb9\u754c\uff09\uff0c\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49SAR\u76ee\u6807\u8bc6\u522b\u3002", "method": "\u5229\u7528GPT-4o\u7b49MLLMs\uff0c\u7ed3\u5408\u5019\u9009\u7c7b\u522b\u548cChain-of-Thought\u63a8\u7406\uff0c\u5bf9SAR\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u57fa\u4e8eFAIR-CSAR\u6784\u5efa\u65b0\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMLLMs\u80fd\u751f\u6210\u903b\u8f91\u8fde\u8d2f\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u540c\u65f6\u5206\u6790\u4e86\u6a21\u578b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8bc1\u660e\u4e86MLLMs\u5728SAR\u5206\u6790\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765SAR\u89c6\u89c9\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.08845", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08845", "abs": "https://arxiv.org/abs/2507.08845", "authors": ["Irfan Ullah", "Young-Koo Lee"], "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler", "comment": null, "summary": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from\ngraph-structured data, however uniform neighbor sampling and static fanout\nsettings frequently limit GNNs' scalability and efficiency. In this paper, we\npropose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel\napproach that dynamically adjusts the fanout based on model performance and\nprioritizes important nodes during training. Our approach leverages node\nscoring based on node degree to focus computational resources on structurally\nimportant nodes, incrementing the fanout as the model training progresses.\nDAFOS also integrates an early stopping mechanism to halt training when\nperformance gains diminish. Experiments conducted on three benchmark datasets,\nogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach\nsignificantly improves training speed and accuracy compared to a\nstate-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv\ndataset and a 12.6x speedup on the Reddit dataset while improving the F1 score\nfrom 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the\nogbn-products dataset, respectively. These results highlight the potential of\nDAFOS as an efficient and scalable solution for large-scale GNN training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u81ea\u9002\u5e94\u6247\u51fa\u4f18\u5316\u91c7\u6837\u5668\uff08DAFOS\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6247\u51fa\u548c\u4f18\u5148\u5904\u7406\u91cd\u8981\u8282\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5747\u5300\u90bb\u5c45\u91c7\u6837\u548c\u9759\u6001\u6247\u51fa\u8bbe\u7f6e\u9650\u5236\u4e86GNN\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u65b9\u6cd5\u3002", "method": "DAFOS\u57fa\u4e8e\u8282\u70b9\u5ea6\u5bf9\u8282\u70b9\u8bc4\u5206\uff0c\u52a8\u6001\u8c03\u6574\u6247\u51fa\uff0c\u5e76\u96c6\u6210\u65e9\u505c\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDAFOS\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u901f\u5ea6\u548cF1\u5206\u6570\uff0c\u6700\u9ad8\u8fbe\u523012.6\u500d\u52a0\u901f\u3002", "conclusion": "DAFOS\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5927\u89c4\u6a21GNN\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09561", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09561", "abs": "https://arxiv.org/abs/2507.09561", "authors": ["Can Wang", "Wei Liu", "Hanzhi Ma", "Xiaonan Jiang", "Erping Li", "Steven Gao"], "title": "Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This article presents a physics-aware convolutional long short-term memory\n(PC-LSTM) network for efficient and accurate extraction of mutual impedance\nmatrices in dipole antenna arrays. By reinterpreting the Green's function\nthrough a physics-aware neural network and embedding it into an adaptive loss\nfunction, the proposed machine learning-based approach achieves enhanced\nphysical interpretability in mutual coupling modeling. Also, an attention\nmechanism is carefully designed to calibrate complex-valued features by fusing\nthe real and imaginary parts of the Green's function matrix. These fused\nrepresentations are then processed by a convolutional long short-term memory\nnetwork, and the impedance matrix of the linear antenna array can be finally\nderived. Validation against five benchmarks underscores the efficacy of the\nproposed approach, demonstrating accurate impedance extraction with up to a 7x\nspeedup compared to CST Microwave Studio, making it a fast alternative to\nfull-wave simulations for mutual coupling characterization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u611f\u77e5\u7684\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08PC-LSTM\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u63d0\u53d6\u5076\u6781\u5929\u7ebf\u9635\u5217\u7684\u4e92\u963b\u6297\u77e9\u9635\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u7cbe\u5ea6\u7684\u963b\u6297\u63d0\u53d6\u3002", "motivation": "\u4f20\u7edf\u5168\u6ce2\u4eff\u771f\u65b9\u6cd5\uff08\u5982CST Microwave Studio\uff09\u8ba1\u7b97\u4e92\u963b\u6297\u77e9\u9635\u8017\u65f6\u8f83\u957f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7269\u7406\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u91cd\u65b0\u89e3\u91ca\u683c\u6797\u51fd\u6570\uff0c\u5e76\u5d4c\u5165\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\uff1b\u8bbe\u8ba1\u6ce8\u610f\u529b\u673a\u5236\u6821\u51c6\u590d\u6570\u503c\u7279\u5f81\uff1b\u4f7f\u7528\u5377\u79efLSTM\u7f51\u7edc\u5904\u7406\u878d\u5408\u540e\u7684\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u53d6\u963b\u6297\u7684\u7cbe\u5ea6\u9ad8\uff0c\u4e14\u901f\u5ea6\u6bd4CST Microwave Studio\u5feb7\u500d\u3002", "conclusion": "PC-LSTM\u662f\u4e00\u79cd\u5feb\u901f\u4e14\u7269\u7406\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e92\u8026\u5408\u7279\u6027\u5206\u6790\u3002"}}
{"id": "2507.08848", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08848", "abs": "https://arxiv.org/abs/2507.08848", "authors": ["Calum Corrie Imrie", "Ioannis Stefanakos", "Sepeedeh Shahbeigi", "Richard Hawkins", "Simon Burton"], "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL", "comment": null, "summary": "The rapid advancement of machine learning (ML) has led to its increasing\nintegration into cyber-physical systems (CPS) across diverse domains. While CPS\noffer powerful capabilities, incorporating ML components introduces significant\nsafety and assurance challenges. Among ML techniques, reinforcement learning\n(RL) is particularly suited for CPS due to its capacity to handle complex,\ndynamic environments where explicit models of interaction between system and\nenvironment are unavailable or difficult to construct. However, in\nsafety-critical applications, this learning process must not only be effective\nbut demonstrably safe. Safe-RL methods aim to address this by incorporating\nsafety constraints during learning, yet they fall short in providing systematic\nassurance across the RL lifecycle. The AMLAS methodology offers structured\nguidance for assuring the safety of supervised learning components, but it does\nnot directly apply to the unique challenges posed by RL. In this paper, we\nadapt AMLAS to provide a framework for generating assurance arguments for an\nRL-enabled system through an iterative process; AMLAS-RL. We demonstrate\nAMLAS-RL using a running example of a wheeled vehicle tasked with reaching a\ntarget goal without collision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAMLAS-RL\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7cfb\u7edf\u751f\u6210\u5b89\u5168\u4fdd\u8bc1\u8bba\u8bc1\uff0c\u5f25\u8865\u73b0\u6709\u65b9\u6cd5\u5728RL\u751f\u547d\u5468\u671f\u4e2d\u7cfb\u7edf\u6027\u5b89\u5168\u4fdd\u8bc1\u7684\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08CPS\uff09\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0cRL\u56e0\u5176\u9002\u5e94\u590d\u6742\u52a8\u6001\u73af\u5883\u7684\u80fd\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5176\u5b89\u5168\u6027\u548c\u4fdd\u8bc1\u6027\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5982Safe-RL\u548cAMLAS\u672a\u80fd\u5b8c\u5168\u89e3\u51b3RL\u7684\u72ec\u7279\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51faAMLAS-RL\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u4e3aRL\u7cfb\u7edf\u751f\u6210\u5b89\u5168\u4fdd\u8bc1\u8bba\u8bc1\uff0c\u5e76\u4ee5\u8f6e\u5f0f\u8f66\u8f86\u907f\u969c\u4efb\u52a1\u4e3a\u4f8b\u8fdb\u884c\u6f14\u793a\u3002", "result": "AMLAS-RL\u6846\u67b6\u6210\u529f\u4e3aRL\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5b89\u5168\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "conclusion": "AMLAS-RL\u4e3aRL\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u4fdd\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.09713", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09713", "abs": "https://arxiv.org/abs/2507.09713", "authors": ["Burak Ahmet Ozden", "Erdogan Aydin", "Ahmet Elbir", "Filiz Gurkan"], "title": "A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks", "comment": "17 pages, 14 figures", "summary": "With the development of wireless network technologies, the wireless image\ntransmission area has become prominent. The need for high resolution, data\ntraffic density, widespread use of multimedia applications, and the importance\nof high rate and reliable image transmission in medical and military fields\nnecessitate the design of novel and high-performance wireless image\ntransmission systems. This paper proposes a code index modulation (CIM)-based\nimage transmission (CIM-IT) system that utilizes spreading code index and\nquadrature amplitude modulation (QAM) symbol for image transmission over a\nwireless channel. The proposed CIM-IT system maps bits to each pixel value of\nthe image to be transmitted and transmits these bits over a wireless channel\nusing a single-input and multiple-output system comprising code index\nmodulation and QAM techniques. At the receiver, the active spreading code index\nand the selected QAM symbol are estimated using a despreading-based maximum\nlikelihood detector, and the corresponding bits are obtained. The image\nconveyed from the transmitter is then reconstructed at the receiver side using\nthe pixel values corresponding to the bits. The obtained noisy image is\nenhanced using important enhancement filters. In addition, an advanced filter\nis proposed to improve the transmitted degraded image with optimum results.\nFurthermore, error performance, spectral efficiency, energy efficiency, and\nthroughputof the CIM-IT system are performed and the results are compared with\ntraditional wireless communication techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7801\u7d22\u5f15\u8c03\u5236\uff08CIM\uff09\u7684\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u7cfb\u7edf\uff08CIM-IT\uff09\uff0c\u7ed3\u5408\u6269\u9891\u7801\u7d22\u5f15\u548cQAM\u6280\u672f\uff0c\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u5668\u548c\u589e\u5f3a\u6ee4\u6ce2\u5668\u4f18\u5316\u4f20\u8f93\u6548\u679c\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u7f51\u7edc\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9ad8\u5206\u8fa8\u7387\u3001\u9ad8\u6570\u636e\u6d41\u91cf\u5bc6\u5ea6\u4ee5\u53ca\u591a\u5a92\u4f53\u5e94\u7528\u7684\u666e\u53ca\uff0c\u533b\u7597\u548c\u519b\u4e8b\u9886\u57df\u5bf9\u9ad8\u901f\u53ef\u9760\u56fe\u50cf\u4f20\u8f93\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u65b0\u578b\u9ad8\u6027\u80fd\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faCIM-IT\u7cfb\u7edf\uff0c\u5c06\u56fe\u50cf\u50cf\u7d20\u503c\u6620\u5c04\u4e3a\u6bd4\u7279\uff0c\u901a\u8fc7\u6269\u9891\u7801\u7d22\u5f15\u548cQAM\u6280\u672f\u5728\u65e0\u7ebf\u4fe1\u9053\u4e2d\u4f20\u8f93\uff0c\u63a5\u6536\u7aef\u4f7f\u7528\u89e3\u6269\u6700\u5927\u4f3c\u7136\u68c0\u6d4b\u5668\u548c\u589e\u5f3a\u6ee4\u6ce2\u5668\u91cd\u5efa\u56fe\u50cf\u3002", "result": "\u7cfb\u7edf\u5728\u8bef\u7801\u6027\u80fd\u3001\u9891\u8c31\u6548\u7387\u3001\u80fd\u91cf\u6548\u7387\u548c\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u9ad8\u7ea7\u6ee4\u6ce2\u5668\u8fdb\u4e00\u6b65\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "CIM-IT\u7cfb\u7edf\u4e3a\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u533b\u7597\u548c\u519b\u4e8b\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08858", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08858", "abs": "https://arxiv.org/abs/2507.08858", "authors": ["Sami Achour", "Yassine Bouher", "Duong Nguyen", "Nicolas Chesneau"], "title": "Foundation models for time series forecasting: Application in conformal prediction", "comment": null, "summary": "The zero-shot capabilities of foundation models (FMs) for time series\nforecasting offer promising potentials in conformal prediction, as most of the\navailable data can be allocated to calibration. This study compares the\nperformance of Time Series Foundation Models (TSFMs) with traditional methods,\nincluding statistical models and gradient boosting, within a conformal\nprediction setting. Our findings highlight two key advantages of TSFMs. First,\nwhen the volume of data is limited, TSFMs provide more reliable conformalized\nprediction intervals than classic models, thanks to their superior predictive\naccuracy. Second, the calibration process is more stable because more data are\nused for calibration. Morever, the fewer data available, the more pronounced\nthese benefits become, as classic models require a substantial amount of data\nfor effective training. These results underscore the potential of foundation\nmodels in improving conformal prediction reliability in time series\napplications, particularly in data-constrained cases. All the code to reproduce\nthe experiments is available.", "AI": {"tldr": "\u6bd4\u8f83\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5728\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0TSFMs\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u9884\u6d4b\u533a\u95f4\u66f4\u53ef\u9760\u4e14\u6821\u51c6\u66f4\u7a33\u5b9a\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5171\u5f62\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5728\u5171\u5f62\u9884\u6d4b\u8bbe\u7f6e\u4e0b\uff0c\u6bd4\u8f83TSFMs\u4e0e\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u548c\u68af\u5ea6\u63d0\u5347\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "TSFMs\u5728\u6570\u636e\u6709\u9650\u65f6\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u533a\u95f4\u548c\u66f4\u7a33\u5b9a\u7684\u6821\u51c6\u8fc7\u7a0b\uff0c\u4f18\u52bf\u968f\u6570\u636e\u51cf\u5c11\u800c\u66f4\u660e\u663e\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5171\u5f62\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e2d\u3002"}}
{"id": "2507.09776", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09776", "abs": "https://arxiv.org/abs/2507.09776", "authors": ["Mihir Kavishwar", "Naresh Shanbhag"], "title": "Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing", "comment": "Code available at: https://github.com/mihirvk2/CSNR-optimal-ADC", "summary": "Analog in-memory computing (AIMC) is an energy-efficient alternative to\ndigital architectures for accelerating machine learning and signal processing\nworkloads. However, its energy efficiency is limited by the high energy cost of\nthe column analog-to-digital converters (ADCs). Reducing the ADC precision is\nan effective approach to lowering its energy cost. However, doing so also\nreduces the AIMC's computational accuracy thereby making it critical to\nidentify the minimum precision required to meet a target accuracy. Prior works\noverestimate the ADC precision requirements by modeling quantization error as\ninput-independent noise, maximizing the signal-to-quantization-noise ratio\n(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address\nthese limitations by developing analytical expressions for estimating the\ncompute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and\npropose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a\ncircuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we\nshow that for a 256-dimensional binary dot product, CACTUS reduces the ADC\nprecision requirements by 3b while achieving 6dB higher CSNR over prior\nmethods. We also delineate operating conditions under which our proposed\nCSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCACTUS\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316ADC\u53c2\u6570\uff0c\u964d\u4f4e\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\uff08AIMC\uff09\u4e2dADC\u7684\u7cbe\u5ea6\u9700\u6c42\uff0c\u540c\u65f6\u63d0\u9ad8\u8ba1\u7b97\u4fe1\u566a\u6bd4\uff08CSNR\uff09\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9ad8\u4f30\u4e86ADC\u7cbe\u5ea6\u9700\u6c42\uff0c\u5ffd\u7565\u4e86\u91cf\u5316\u8bef\u5dee\u7684\u8f93\u5165\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4AIMC\u7684\u80fd\u6548\u53d7\u9650\u3002", "method": "\u5f00\u53d1\u4e86CSNR\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51faCACTUS\u7b97\u6cd5\u4f18\u5316ADC\u53c2\u6570\u3002", "result": "\u572828nm CMOS\u5de5\u827a\u7684SRAM AIMC\u6a21\u578b\u4e2d\uff0cCACTUS\u5c06ADC\u7cbe\u5ea6\u9700\u6c42\u964d\u4f4e3\u4f4d\uff0c\u540c\u65f6CSNR\u63d0\u9ad86dB\u3002", "conclusion": "CACTUS\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edfSQNR\u4f18\u5316\u7684ADC\uff0c\u4e3aAIMC\u80fd\u6548\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08861", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08861", "abs": "https://arxiv.org/abs/2507.08861", "authors": ["Lucas Tesan", "Mikel M. Iparraguirre", "David Gonzalez", "Pedro Martins", "Elias Cueto"], "title": "On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition", "comment": null, "summary": "This paper proposes sharp lower bounds for the number of message passing\niterations required in graph neural networks (GNNs) when solving partial\ndifferential equations (PDE). This significantly reduces the need for\nexhaustive hyperparameter tuning. Bounds are derived for the three fundamental\nclasses of PDEs (hyperbolic, parabolic and elliptic) by relating the physical\ncharacteristics of the problem in question to the message-passing requirement\nof GNNs. In particular, we investigate the relationship between the physical\nconstants of the equations governing the problem, the spatial and temporal\ndiscretisation and the message passing mechanisms in GNNs.\n  When the number of message passing iterations is below these proposed limits,\ninformation does not propagate efficiently through the network, resulting in\npoor solutions, even for deep GNN architectures. In contrast, when the\nsuggested lower bound is satisfied, the GNN parameterisation allows the model\nto accurately capture the underlying phenomenology, resulting in solvers of\nadequate accuracy.\n  Examples are provided for four different examples of equations that show the\nsharpness of the proposed lower bounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u65f6\u6240\u9700\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u7684\u4e25\u683c\u4e0b\u754c\uff0c\u51cf\u5c11\u4e86\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9700\u6c42\u3002", "motivation": "\u901a\u8fc7\u5c06\u7269\u7406\u95ee\u9898\u7684\u7279\u6027\u4e0eGNN\u7684\u6d88\u606f\u4f20\u9012\u9700\u6c42\u5173\u8054\uff0c\u4e3a\u4e09\u7c7b\u57fa\u672cPDE\uff08\u53cc\u66f2\u3001\u629b\u7269\u3001\u692d\u5706\uff09\u63a8\u5bfc\u4e0b\u754c\uff0c\u4ee5\u4f18\u5316GNN\u5728\u6c42\u89e3PDE\u65f6\u7684\u6548\u7387\u3002", "method": "\u7814\u7a76PDE\u7684\u7269\u7406\u5e38\u6570\u3001\u65f6\u7a7a\u79bb\u6563\u5316\u4e0eGNN\u6d88\u606f\u4f20\u9012\u673a\u5236\u7684\u5173\u7cfb\uff0c\u63a8\u5bfc\u51fa\u8fed\u4ee3\u6b21\u6570\u7684\u4e0b\u754c\u3002", "result": "\u5f53\u8fed\u4ee3\u6b21\u6570\u4f4e\u4e8e\u4e0b\u754c\u65f6\uff0c\u4fe1\u606f\u4f20\u64ad\u6548\u7387\u4f4e\uff0c\u89e3\u8d28\u91cf\u5dee\uff1b\u6ee1\u8db3\u4e0b\u754c\u65f6\uff0cGNN\u80fd\u51c6\u786e\u6355\u6349\u73b0\u8c61\uff0c\u5f97\u5230\u9ad8\u7cbe\u5ea6\u89e3\u3002", "conclusion": "\u901a\u8fc7\u56db\u4e2a\u65b9\u7a0b\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u4e0b\u754c\u7684\u4e25\u683c\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6307\u5bfcGNN\u5728PDE\u6c42\u89e3\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.08860", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08860", "abs": "https://arxiv.org/abs/2507.08860", "authors": ["Awais Manzoor", "M. Atif Qureshi", "Etain Kidney", "Luca Longo"], "title": "e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction", "comment": null, "summary": "Retention campaigns in customer relationship management often rely on churn\nprediction models evaluated using traditional metrics such as AUC and F1-score.\nHowever, these metrics fail to reflect financial outcomes and may mislead\nstrategic decisions. We introduce e-Profits, a novel business-aligned\nevaluation metric that quantifies model performance based on customer-specific\nvalue, retention probability, and intervention costs. Unlike existing\nprofit-based metrics such as Expected Maximum Profit, which assume fixed\npopulation-level parameters, e-Profits uses Kaplan-Meier survival analysis to\nestimate personalised retention rates and supports granular, per customer\nevaluation. We benchmark six classifiers across two telecom datasets (IBM Telco\nand Maven Telecom) and demonstrate that e-Profits reshapes model rankings\ncompared to traditional metrics, revealing financial advantages in models\npreviously overlooked by AUC or F1-score. The metric also enables segment-level\ninsight into which models maximise return on investment for high-value\ncustomers. e-Profits is designed as an understandable, post hoc tool to support\nmodel evaluation in business contexts, particularly for marketing and analytics\nteams prioritising profit-driven decisions. All source code is available at:\nhttps://github.com/matifq/eprofits.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e1a\u52a1\u5bf9\u9f50\u8bc4\u4f30\u6307\u6807e-Profits\uff0c\u7528\u4e8e\u91cf\u5316\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u4e2d\u7684\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\u5982AUC\u548cF1-score\u3002", "motivation": "\u4f20\u7edf\u6307\u6807\uff08\u5982AUC\u548cF1-score\uff09\u672a\u80fd\u53cd\u6620\u8d22\u52a1\u7ed3\u679c\uff0c\u53ef\u80fd\u5bfc\u81f4\u6218\u7565\u51b3\u7b56\u5931\u8bef\u3002", "method": "e-Profits\u57fa\u4e8e\u5ba2\u6237\u7279\u5b9a\u4ef7\u503c\u3001\u4fdd\u7559\u6982\u7387\u548c\u5e72\u9884\u6210\u672c\uff0c\u5229\u7528Kaplan-Meier\u751f\u5b58\u5206\u6790\u4f30\u8ba1\u4e2a\u6027\u5316\u4fdd\u7559\u7387\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "\u5728\u7535\u4fe1\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u516d\u79cd\u5206\u7c7b\u5668\uff0ce-Profits\u91cd\u5851\u4e86\u6a21\u578b\u6392\u540d\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u6307\u6807\u5ffd\u7565\u7684\u8d22\u52a1\u4f18\u52bf\u3002", "conclusion": "e-Profits\u662f\u4e00\u79cd\u6613\u4e8e\u7406\u89e3\u7684\u5de5\u5177\uff0c\u652f\u6301\u4e1a\u52a1\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8bc4\u4f30\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5229\u6da6\u9a71\u52a8\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.09894", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.09894", "abs": "https://arxiv.org/abs/2507.09894", "authors": ["Saif Khan Mohammed", "Amit Kumar Pathak", "Muhammad Ubadah", "Ronny Hadani", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Precoded Zak-OTFS for Per-Carrier Equalization", "comment": null, "summary": "In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform\nis a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic\nlocalized function with specific periods along delay and Doppler. When the\nchannel delay spread is less than the delay period, and the channel Doppler\nspread is less than the Doppler period, the response to a single Zak-OTFS\ncarrier provides an image of the scattering environment and can be used to\npredict the effective channel at all other carriers. The image of the\nscattering environment changes slowly, making it possible to employ precoding\nat the transmitter. Precoding techniques were developed more than thirty years\nago for wireline modem channels (V.34 standard) defined by linear convolution\nwhere a pulse in the time domain (TD) is used to probe the one-dimensional\npartial response channel. The action of a doubly spread channel on Zak-OTFS\nmodulation determines a two-dimensional partial response channel defined by\ntwisted convolution, and we develop a novel precoding technique for this\nchannel. The proposed precoder leads to separate equalization of each DD\ncarrier which has significantly lower complexity than joint equalization of all\ncarriers. Further, the effective precoded channel results in non-interfering DD\ncarriers which significantly reduces the overhead of guard carriers separating\ndata and pilot carriers, which improves the spectral efficiency significantly.", "AI": {"tldr": "Zak-OTFS\u8c03\u5236\u901a\u8fc7\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u4e2d\u7684\u8109\u51b2\u63a2\u6d4b\u4fe1\u9053\u6563\u5c04\u73af\u5883\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9884\u7f16\u7801\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u4e86\u9891\u8c31\u6548\u7387\u3002", "motivation": "\u7814\u7a76Zak-OTFS\u8c03\u5236\u5728\u53cc\u6269\u5c55\u4fe1\u9053\u4e0b\u7684\u9884\u7f16\u7801\u6280\u672f\uff0c\u4ee5\u7b80\u5316\u4fe1\u9053\u5747\u8861\u5e76\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002", "method": "\u5229\u7528\u5ef6\u8fdf-\u591a\u666e\u52d2\u57df\u4e2d\u7684\u8109\u51b2\u63a2\u6d4b\u4fe1\u9053\u6563\u5c04\u73af\u5883\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u9488\u5bf9\u4e8c\u7ef4\u90e8\u5206\u54cd\u5e94\u4fe1\u9053\u7684\u65b0\u578b\u9884\u7f16\u7801\u6280\u672f\u3002", "result": "\u9884\u7f16\u7801\u6280\u672f\u5b9e\u73b0\u4e86\u5bf9\u6bcf\u4e2a\u5ef6\u8fdf-\u591a\u666e\u52d2\u8f7d\u6ce2\u7684\u72ec\u7acb\u5747\u8861\uff0c\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4fdd\u62a4\u8f7d\u6ce2\u7684\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u9891\u8c31\u6548\u7387\u3002", "conclusion": "Zak-OTFS\u8c03\u5236\u7ed3\u5408\u65b0\u578b\u9884\u7f16\u7801\u6280\u672f\uff0c\u4e3a\u53cc\u6269\u5c55\u4fe1\u9053\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08866", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08866", "abs": "https://arxiv.org/abs/2507.08866", "authors": ["Marina Ceccon", "Giandomenico Cornacchia", "Davide Dalle Pezze", "Alessandro Fabris", "Gian Antonio Susto"], "title": "Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond", "comment": "Accepted in Expert Systems with Applications", "summary": "Undesirable biases encoded in the data are key drivers of algorithmic\ndiscrimination. Their importance is widely recognized in the algorithmic\nfairness literature, as well as legislation and standards on\nanti-discrimination in AI. Despite this recognition, data biases remain\nunderstudied, hindering the development of computational best practices for\ntheir detection and mitigation. In this work, we present three common data\nbiases and study their individual and joint effect on algorithmic\ndiscrimination across a variety of datasets, models, and fairness measures. We\nfind that underrepresentation of vulnerable populations in training sets is\nless conducive to discrimination than conventionally affirmed, while\ncombinations of proxies and label bias can be far more critical. Consequently,\nwe develop dedicated mechanisms to detect specific types of bias, and combine\nthem into a preliminary construct we refer to as the Data Bias Profile (DBP).\nThis initial formulation serves as a proof of concept for how different bias\nsignals can be systematically documented. Through a case study with popular\nfairness datasets, we demonstrate the effectiveness of the DBP in predicting\nthe risk of discriminatory outcomes and the utility of fairness-enhancing\ninterventions. Overall, this article bridges algorithmic fairness research and\nanti-discrimination policy through a data-centric lens.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6570\u636e\u504f\u89c1\u5bf9\u7b97\u6cd5\u6b67\u89c6\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u504f\u89c1\u914d\u7f6e\u6587\u4ef6\uff08DBP\uff09\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6570\u636e\u504f\u89c1\u662f\u7b97\u6cd5\u6b67\u89c6\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u68c0\u6d4b\u548c\u7f13\u89e3\u504f\u89c1\u7684\u5b9e\u8df5\u53d1\u5c55\u3002", "method": "\u7814\u7a76\u4e09\u79cd\u5e38\u89c1\u6570\u636e\u504f\u89c1\u7684\u5355\u72ec\u53ca\u8054\u5408\u5f71\u54cd\uff0c\u5f00\u53d1\u68c0\u6d4b\u673a\u5236\u5e76\u6574\u5408\u4e3aDBP\u3002", "result": "\u53d1\u73b0\u5f31\u52bf\u7fa4\u4f53\u5728\u8bad\u7ec3\u96c6\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u5bf9\u6b67\u89c6\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u4ee3\u7406\u548c\u6807\u7b7e\u504f\u89c1\u7684\u7ec4\u5408\u66f4\u4e3a\u5173\u952e\u3002DBP\u80fd\u6709\u6548\u9884\u6d4b\u6b67\u89c6\u98ce\u9669\u3002", "conclusion": "DBP\u4e3a\u7cfb\u7edf\u8bb0\u5f55\u504f\u89c1\u4fe1\u53f7\u63d0\u4f9b\u4e86\u521d\u6b65\u6846\u67b6\uff0c\u8fde\u63a5\u4e86\u7b97\u6cd5\u516c\u5e73\u7814\u7a76\u548c\u53cd\u6b67\u89c6\u653f\u7b56\u3002"}}
{"id": "2507.09895", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09895", "abs": "https://arxiv.org/abs/2507.09895", "authors": ["Hyung-Joo Moon", "Chan-Byoung Chae", "Kai-Kit Wong", "Robert W. Heath Jr"], "title": "AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission", "comment": "7 pages, 6 figures, IEEE Communications Magazine (under revision)", "summary": "Massive Aerial Processing for X MAP-X is an innovative framework for\nreconstructing spatially correlated ground data, such as environmental or\nindustrial measurements distributed across a wide area, into data maps using a\nsingle high altitude pseudo-satellite (HAPS) and a large number of distributed\nsensors. With subframe-level data reconstruction, MAP-X provides a\ntransformative solution for latency-sensitive IoT applications. This article\nexplores two distinct approaches for AI integration in the post-processing\nstage of MAP-X. The DNN-based pointwise estimation approach enables real-time,\nadaptive reconstruction through online training, while the CNN-based image\nreconstruction approach improves reconstruction accuracy through offline\ntraining with non-real-time data. Simulation results show that both approaches\nsignificantly outperform the conventional inverse discrete Fourier transform\n(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced\nMAP-X, we propose a ground-HAPS cooperation framework, where terrestrial\nstations collect, process, and relay training data to the HAPS. With its\nenhanced capability in reconstructing field data, AI-enhanced MAP-X is\napplicable to various real-world use cases, including disaster response and\nnetwork management.", "AI": {"tldr": "MAP-X\u6846\u67b6\u901a\u8fc7HAPS\u548c\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u91cd\u5efa\u7a7a\u95f4\u76f8\u5173\u6570\u636e\uff0c\u63d0\u51fa\u4e24\u79cdAI\u96c6\u6210\u65b9\u6cd5\uff08DNN\u548cCNN\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5730\u9762-HAPS\u534f\u4f5c\u6846\u67b6\u3002", "motivation": "\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684IoT\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u7684\u6570\u636e\u5730\u56fe\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528DNN\u70b9\u4f30\u8ba1\u548cCNN\u56fe\u50cf\u91cd\u5efa\u4e24\u79cdAI\u65b9\u6cd5\uff0c\u7ed3\u5408\u5730\u9762-HAPS\u534f\u4f5c\u6846\u67b6\u3002", "result": "\u4e24\u79cdAI\u65b9\u6cd5\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edfIDFT\u7ebf\u6027\u540e\u5904\u7406\u65b9\u6cd5\u3002", "conclusion": "AI\u589e\u5f3a\u7684MAP-X\u9002\u7528\u4e8e\u707e\u5bb3\u54cd\u5e94\u548c\u7f51\u7edc\u7ba1\u7406\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.08913", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08913", "abs": "https://arxiv.org/abs/2507.08913", "authors": ["Qi He", "Peiran Yu", "Ziyi Chen", "Heng Huang"], "title": "Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness", "comment": null, "summary": "Shuffling-type gradient methods are favored in practice for their simplicity\nand rapid empirical performance. Despite extensive development of convergence\nguarantees under various assumptions in recent years, most require the\nLipschitz smoothness condition, which is often not met in common machine\nlearning models. We highlight this issue with specific counterexamples. To\naddress this gap, we revisit the convergence rates of shuffling-type gradient\nmethods without assuming Lipschitz smoothness. Using our stepsize strategy, the\nshuffling-type gradient algorithm not only converges under weaker assumptions\nbut also match the current best-known convergence rates, thereby broadening its\napplicability. We prove the convergence rates for nonconvex, strongly convex,\nand non-strongly convex cases, each under both random reshuffling and arbitrary\nshuffling schemes, under a general bounded variance condition. Numerical\nexperiments further validate the performance of our shuffling-type gradient\nalgorithm, underscoring its practical efficacy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u9700Lipschitz\u5e73\u6ed1\u5047\u8bbe\u7684\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5\uff0c\u63d0\u51fa\u65b0\u7684\u6b65\u957f\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5728\u975e\u51f8\u3001\u5f3a\u51f8\u548c\u975e\u5f3a\u51f8\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\u5927\u591a\u4f9d\u8d56Lipschitz\u5e73\u6ed1\u6761\u4ef6\uff0c\u4f46\u8bb8\u591a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0d\u6ee1\u8db3\u6b64\u6761\u4ef6\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\u7684\u6536\u655b\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6b65\u957f\u7b56\u7565\uff0c\u5206\u6790\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5\u5728\u975e\u51f8\u3001\u5f3a\u51f8\u548c\u975e\u5f3a\u51f8\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u6027\uff0c\u652f\u6301\u968f\u673a\u6d17\u724c\u548c\u4efb\u610f\u6d17\u724c\u65b9\u6848\u3002", "result": "\u7b97\u6cd5\u5728\u66f4\u5f31\u7684\u5047\u8bbe\u4e0b\u6536\u655b\uff0c\u4e14\u6536\u655b\u901f\u7387\u4e0e\u5f53\u524d\u5df2\u77e5\u6700\u4f18\u901f\u7387\u4e00\u81f4\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86\u968f\u673a\u6d17\u724c\u68af\u5ea6\u65b9\u6cd5\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u4e0d\u6ee1\u8db3Lipschitz\u5e73\u6ed1\u6761\u4ef6\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.09987", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09987", "abs": "https://arxiv.org/abs/2507.09987", "authors": ["Zihang Zeng", "Shu Sun", "Meixia Tao", "Yin Xu", "Xianghao Yu"], "title": "VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling", "comment": null, "summary": "Wireless channel modeling in complex environments is crucial for wireless\ncommunication system design and deployment. Traditional channel modeling\napproaches face challenges in balancing accuracy, efficiency, and scalability,\nwhile recent neural approaches such as neural radiance field (NeRF) suffer from\nlong training and slow inference. To tackle these challenges, we propose\nvoxelized radiance field (VoxelRF), a novel neural representation for wireless\nchannel modeling that enables fast and accurate synthesis of spatial spectra.\nVoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based\nmethods with trilinear interpolation of voxel grid-based representation, and\ntwo shallow MLPs to model both propagation and transmitter-dependent effects.\nTo further accelerate training and improve generalization, we introduce\nprogressive learning, empty space skipping, and an additional background\nentropy loss function. Experimental results demonstrate that VoxelRF achieves\ncompetitive accuracy with significantly reduced computation and limited\ntraining data, making it more practical for real-time and resource-constrained\nwireless applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVoxelRF\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\uff0c\u901a\u8fc7\u4f53\u7d20\u7f51\u683c\u8868\u793a\u548c\u6d45\u5c42MLP\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u7a7a\u95f4\u9891\u8c31\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u800c\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u7684\u65b9\u6cd5\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "VoxelRF\u91c7\u7528\u4f53\u7d20\u7f51\u683c\u8868\u793a\u548c\u6d45\u5c42MLP\u66ff\u4ee3NeRF\u4e2d\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u5e76\u5f15\u5165\u6e10\u8fdb\u5b66\u4e60\u3001\u7a7a\u533a\u57df\u8df3\u8fc7\u548c\u80cc\u666f\u71b5\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVoxelRF\u5728\u8ba1\u7b97\u91cf\u548c\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u7ebf\u5e94\u7528\u3002", "conclusion": "VoxelRF\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.08956", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.08956", "abs": "https://arxiv.org/abs/2507.08956", "authors": ["Zhenghan Fang", "Mateo D\u00edaz", "Sam Buchanan", "Jeremias Sulam"], "title": "Beyond Scores: Proximal Diffusion Models", "comment": null, "summary": "Diffusion models have quickly become some of the most popular and powerful\ngenerative models for high-dimensional data. The key insight that enabled their\ndevelopment was the realization that access to the score -- the gradient of the\nlog-density at different noise levels -- allows for sampling from data\ndistributions by solving a reverse-time stochastic differential equation (SDE)\nvia forward discretization, and that popular denoisers allow for unbiased\nestimators of this score. In this paper, we demonstrate that an alternative,\nbackward discretization of these SDEs, using proximal maps in place of the\nscore, leads to theoretical and practical benefits. We leverage recent results\nin proximal matching to learn proximal operators of the log-density and, with\nthem, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that\n$\\widetilde{O}(d/\\sqrt{\\varepsilon})$ steps suffice for the resulting\ndiscretization to generate an $\\varepsilon$-accurate distribution w.r.t. the KL\ndivergence. Empirically, we show that two variants of ProxDM achieve\nsignificantly faster convergence within just a few sampling steps compared to\nconventional score-matching methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5411\u79bb\u6563\u5316\u548c\u8fd1\u7aef\u6620\u5c04\u7684\u6269\u6563\u6a21\u578b\uff08ProxDM\uff09\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f97\u5206\u5339\u914d\u65b9\u6cd5\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u9ad8\u7ef4\u6570\u636e\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5f97\u5206\uff08\u68af\u5ea6\uff09\u8fdb\u884c\u91c7\u6837\uff0c\u5b58\u5728\u6548\u7387\u95ee\u9898\u3002\u672c\u6587\u63a2\u7d22\u901a\u8fc7\u53cd\u5411\u79bb\u6563\u5316\u548c\u8fd1\u7aef\u6620\u5c04\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u53cd\u5411\u79bb\u6563\u5316\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\uff0c\u4ee5\u8fd1\u7aef\u6620\u5c04\u66ff\u4ee3\u5f97\u5206\uff0c\u7ed3\u5408\u8fd1\u7aef\u5339\u914d\u6280\u672f\u5b66\u4e60\u5bf9\u6570\u5bc6\u5ea6\u7684\u8fd1\u7aef\u7b97\u5b50\uff0c\u6784\u5efaProxDM\u3002", "result": "\u7406\u8bba\u8bc1\u660eProxDM\u4ec5\u9700$\\widetilde{O}(d/\\sqrt{\\varepsilon})$\u6b65\u5373\u53ef\u751f\u6210KL\u6563\u5ea6$\\varepsilon$-\u7cbe\u786e\u7684\u5206\u5e03\uff1b\u5b9e\u9a8c\u663e\u793a\u5176\u6536\u655b\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "ProxDM\u901a\u8fc7\u53cd\u5411\u79bb\u6563\u5316\u548c\u8fd1\u7aef\u6620\u5c04\uff0c\u5728\u751f\u6210\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5f97\u5206\u5339\u914d\u65b9\u6cd5\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08870", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08870", "abs": "https://arxiv.org/abs/2507.08870", "authors": ["Yaowenqi Liu", "BingXu Meng", "Rui Pan", "Jerry Huang", "Tong Zhang"], "title": "GUIDE: Towards Scalable Advising for Research Ideas", "comment": null, "summary": "The field of AI research is advancing at an unprecedented pace, enabling\nautomated hypothesis generation and experimental design across diverse domains\nsuch as biology, mathematics, and artificial intelligence. Despite these\nadvancements, there remains a significant gap in the availability of scalable\nadvising systems capable of providing high-quality, well-reasoned feedback to\nrefine proposed hypotheses and experimental designs. To address this challenge,\nwe explore key factors that underlie the development of robust advising\nsystems, including model size, context length, confidence estimation, and\nstructured reasoning processes. Our findings reveal that a relatively small\nmodel, when equipped with a well-compressed literature database and a\nstructured reasoning framework, can outperform powerful general-purpose\nlanguage models such as Deepseek-R1 in terms of acceptance rates for\nself-ranked top-30% submissions to ICLR 2025. Moreover, when limited to\nhigh-confidence predictions, our system achieves an acceptance rate exceeding\n90% on the ICLR 2025 test set, underscoring its potential to significantly\nenhance the quality and efficiency of hypothesis generation and experimental\ndesign. The code is released at\nhttps://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5f00\u53d1\u9ad8\u6548\u7684AI\u5efa\u8bae\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u8d28\u91cf\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c0f\u578b\u6a21\u578b\u7ed3\u5408\u538b\u7f29\u6587\u732e\u5e93\u548c\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\uff0c\u80fd\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8d85\u8d8a\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524dAI\u7814\u7a76\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u5efa\u8bae\u7cfb\u7edf\u6765\u63d0\u4f9b\u9ad8\u8d28\u91cf\u53cd\u9988\uff0c\u4ee5\u4f18\u5316\u5047\u8bbe\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u3002", "method": "\u7814\u7a76\u5173\u952e\u56e0\u7d20\uff08\u5982\u6a21\u578b\u5927\u5c0f\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u7ed3\u6784\u5316\u63a8\u7406\uff09\uff0c\u5f00\u53d1\u5c0f\u578b\u6a21\u578b\u7ed3\u5408\u538b\u7f29\u6587\u732e\u5e93\u548c\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\u3002", "result": "\u5c0f\u578b\u6a21\u578b\u5728ICLR 2025\u7684\u81ea\u8bc4\u524d30%\u63d0\u4ea4\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u63a5\u53d7\u7387\u8d85\u8fc790%\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u7684\u8d28\u91cf\u4e0e\u6548\u7387\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.09999", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09999", "abs": "https://arxiv.org/abs/2507.09999", "authors": ["Lital Dabush", "Nir Shlezinger", "Tirza Routtenberg"], "title": "Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A broad range of applications involve signals with irregular structures that\ncan be represented as a graph. As the underlying structures can change over\ntime, the tracking dynamic graph topologies from observed signals is a\nfundamental challenge in graph signal processing (GSP), with applications in\nvarious domains, such as power systems, the brain-machine interface, and\ncommunication systems. In this paper, we propose a method for tracking dynamic\nchanges in graph topologies. Our approach builds on a representation of the\ndynamics as a graph-based nonlinear state-space model (SSM), where the\nobservations are graph signals generated through graph filtering, and the\nunderlying evolving topology serves as the latent states. In our formulation,\nthe graph Laplacian matrix is parameterized using the incidence matrix and edge\nweights, enabling a structured representation of the state. In order to track\nthe evolving topology in the resulting SSM, we develop a sparsity-aware\nextended Kalman filter (EKF) that integrates $\\ell_1$-regularized updates\nwithin the filtering process. Furthermore, a dynamic programming scheme to\nefficiently compute the Jacobian of the graph filter is introduced. Our\nnumerical study demonstrates the ability of the proposed method to accurately\ntrack sparse and time-varying graphs under realistic conditions, with highly\nnonlinear measurements, various noise levels, and different change rates, while\nmaintaining low computational complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u4fe1\u53f7\u5904\u7406\u7684\u52a8\u6001\u56fe\u62d3\u6251\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5229\u7528\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u7a00\u758f\u611f\u77e5\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u52a8\u6001\u56fe\u8ddf\u8e2a\u3002", "motivation": "\u52a8\u6001\u56fe\u62d3\u6251\u7684\u8ddf\u8e2a\u662f\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u6d89\u53ca\u7535\u529b\u7cfb\u7edf\u3001\u8111\u673a\u63a5\u53e3\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u975e\u7ebf\u6027\u6d4b\u91cf\u548c\u9ad8\u566a\u58f0\u73af\u5883\u3002", "method": "\u91c7\u7528\u56fe\u57fa\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5c06\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635\u53c2\u6570\u5316\u4e3a\u8fb9\u6743\u91cd\uff0c\u5e76\u5f00\u53d1\u4e86\u7a00\u758f\u611f\u77e5\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u548c\u52a8\u6001\u89c4\u5212\u65b9\u6848\u8ba1\u7b97\u96c5\u53ef\u6bd4\u77e9\u9635\u3002", "result": "\u6570\u503c\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u9ad8\u566a\u58f0\u548c\u975e\u7ebf\u6027\u6d4b\u91cf\u6761\u4ef6\u4e0b\uff0c\u51c6\u786e\u8ddf\u8e2a\u7a00\u758f\u4e14\u65f6\u53d8\u7684\u56fe\u62d3\u6251\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u56fe\u62d3\u6251\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.08965", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08965", "abs": "https://arxiv.org/abs/2507.08965", "authors": ["Kevin Rojas", "Ye He", "Chieh-Hsin Lai", "Yuta Takida", "Yuki Mitsufuji", "Molei Tao"], "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models", "comment": null, "summary": "Classifier-Free Guidance (CFG) is a widely used technique for conditional\ngeneration and improving sample quality in continuous diffusion models, and\nrecent works have extended it to discrete diffusion. This paper theoretically\nanalyzes CFG in the context of masked discrete diffusion, focusing on the role\nof guidance schedules. Our analysis shows that high guidance early in sampling\n(when inputs are heavily masked) harms generation quality, while late-stage\nguidance has a larger effect. These findings provide a theoretical explanation\nfor empirical observations in recent studies on guidance schedules. The\nanalysis also reveals an imperfection of the current CFG implementations. These\nimplementations can unintentionally cause imbalanced transitions, such as\nunmasking too rapidly during the early stages of generation, which degrades the\nquality of the resulting samples. To address this, we draw insight from the\nanalysis and propose a novel classifier-free guidance mechanism empirically\napplicable to any discrete diffusion. Intuitively, our method smoothens the\ntransport between the data distribution and the initial (masked/uniform)\ndistribution, which results in improved sample quality. Remarkably, our method\nis achievable via a simple one-line code change. The efficacy of our method is\nempirically demonstrated with experiments on ImageNet (masked discrete\ndiffusion) and QM9 (uniform discrete diffusion).", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2d\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08CFG\uff09\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u65e9\u671f\u9ad8\u5f15\u5bfc\u4f1a\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u800c\u540e\u671f\u5f15\u5bfc\u5f71\u54cd\u66f4\u5927\u3002\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u4ee3\u7801\u8c03\u6574\u63d0\u5347\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2dCFG\u5f15\u5bfc\u65f6\u95f4\u8868\u7684\u4f5c\u7528\uff0c\u89e3\u91ca\u73b0\u6709\u5b9e\u73b0\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u7406\u8bba\u5206\u6790CFG\u5728\u63a9\u7801\u79bb\u6563\u6269\u6563\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u51fa\u65b0\u7684\u5f15\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u5e73\u6ed1\u6570\u636e\u5206\u5e03\u4e0e\u521d\u59cb\u5206\u5e03\u7684\u4f20\u8f93\u63d0\u5347\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6539\u8fdb\u65b9\u6cd5\u5728ImageNet\u548cQM9\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u5347\u6837\u672c\u8d28\u91cf\u3002", "conclusion": "\u65e9\u671f\u9ad8\u5f15\u5bfc\u6709\u5bb3\uff0c\u540e\u671f\u5f15\u5bfc\u66f4\u6709\u6548\uff1b\u63d0\u51fa\u7684\u7b80\u5355\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.08871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08871", "abs": "https://arxiv.org/abs/2507.08871", "authors": ["Xishun Liao", "Haoxuan Ma", "Yifan Liu", "Yuxiang Wei", "Brian Yueshuai He", "Chris Stanford", "Jiaqi Ma"], "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination", "comment": "8 pages, 7 figures", "summary": "Travel demand models are critical tools for planning, policy, and mobility\nsystem design. Traditional activity-based models (ABMs), although grounded in\nbehavioral theories, often rely on simplified rules and assumptions, and are\ncostly to develop and difficult to adapt across different regions. This paper\npresents a learning-based travel demand modeling framework that synthesizes\nhousehold-coordinated daily activity patterns based on a household's\nsocio-demographic profiles. The whole framework integrates population\nsynthesis, coordinated activity generation, location assignment, and\nlarge-scale microscopic traffic simulation into a unified system. It is fully\ngenerative, data-driven, scalable, and transferable to other regions. A\nfull-pipeline implementation is conducted in Los Angeles with a 10 million\npopulation. Comprehensive validation shows that the model closely replicates\nreal-world mobility patterns and matches the performance of legacy ABMs with\nsignificantly reduced modeling cost and greater scalability. With respect to\nthe SCAG ABM benchmark, the origin-destination matrix achieves a cosine\nsimilarity of 0.97, and the daily vehicle miles traveled (VMT) in the network\nyields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute\npercentage error (MAPE). When compared to real-world observations from Caltrans\nPeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001\nJSD and a 6.11% MAPE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u65c5\u884c\u9700\u6c42\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u53e3\u5408\u6210\u3001\u6d3b\u52a8\u751f\u6210\u3001\u4f4d\u7f6e\u5206\u914d\u548c\u5927\u89c4\u6a21\u5fae\u89c2\u4ea4\u901a\u6a21\u62df\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5efa\u6a21\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6d3b\u52a8\u7684\u6a21\u578b\uff08ABMs\uff09\u4f9d\u8d56\u7b80\u5316\u89c4\u5219\u548c\u5047\u8bbe\uff0c\u5f00\u53d1\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u8de8\u533a\u57df\u9002\u5e94\u3002", "method": "\u96c6\u6210\u4eba\u53e3\u5408\u6210\u3001\u534f\u8c03\u6d3b\u52a8\u751f\u6210\u3001\u4f4d\u7f6e\u5206\u914d\u548c\u5fae\u89c2\u4ea4\u901a\u6a21\u62df\u7684\u7edf\u4e00\u7cfb\u7edf\uff0c\u6570\u636e\u9a71\u52a8\u4e14\u53ef\u8fc1\u79fb\u3002", "result": "\u5728\u6d1b\u6749\u77f6\u7684\u9a8c\u8bc1\u4e2d\uff0c\u6a21\u578b\u590d\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6a21\u5f0f\uff0c\u6027\u80fd\u63a5\u8fd1\u4f20\u7edfABMs\uff0c\u6210\u672c\u66f4\u4f4e\u4e14\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65c5\u884c\u9700\u6c42\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u8fc1\u79fb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10063", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10063", "abs": "https://arxiv.org/abs/2507.10063", "authors": ["Hongpu Zhang", "Shu Sun", "Hangsong Yan", "Jianhua Mo"], "title": "Deep Learning-Based Beamforming Design Using Target Beam Patterns", "comment": "6 pages, 5 figures", "summary": "This paper proposes a deep learning-based beamforming design framework that\ndirectly maps a target beam pattern to optimal beamforming vectors across\nmultiple antenna array architectures, including digital, analog, and hybrid\nbeamforming. The proposed method employs a lightweight encoder-decoder network\nwhere the encoder compresses the complex beam pattern into a low-dimensional\nfeature vector and the decoder reconstructs the beamforming vector while\nsatisfying hardware constraints. To address training challenges under diverse\nand limited channel station information (CSI) conditions, a two-stage training\nprocess is introduced, which consists of an offline pre-training for robust\nfeature extraction using an auxiliary module, followed by online training of\nthe decoder with a composite loss function that ensures alignment between the\nsynthesized and target beam patterns in terms of the main lobe shape and side\nlobe suppression. Simulation results based on NYUSIM-generated channels show\nthat the proposed method can achieve spectral efficiency close to that of fully\ndigital beamforming under limited CSI and outperforms representative existing\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u6846\u67b6\uff0c\u53ef\u76f4\u63a5\u5c06\u76ee\u6807\u6ce2\u675f\u6a21\u5f0f\u6620\u5c04\u5230\u591a\u79cd\u5929\u7ebf\u9635\u5217\u67b6\u6784\u7684\u6700\u4f18\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u5929\u7ebf\u67b6\u6784\uff08\u6570\u5b57\u3001\u6a21\u62df\u548c\u6df7\u5408\u6ce2\u675f\u6210\u5f62\uff09\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u8bbe\u8ba1\u6ee1\u8db3\u786c\u4ef6\u7ea6\u675f\u7684\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u7f16\u7801\u5668\u538b\u7f29\u590d\u6742\u6ce2\u675f\u6a21\u5f0f\u4e3a\u4f4e\u7ef4\u7279\u5f81\u5411\u91cf\uff0c\u89e3\u7801\u5668\u91cd\u5efa\u6ce2\u675f\u6210\u5f62\u5411\u91cf\u3002\u5f15\u5165\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u79bb\u7ebf\u9884\u8bad\u7ec3\u63d0\u53d6\u9c81\u68d2\u7279\u5f81\uff0c\u5728\u7ebf\u8bad\u7ec3\u89e3\u7801\u5668\u4f7f\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\u786e\u4fdd\u4e3b\u74e3\u5f62\u72b6\u548c\u65c1\u74e3\u6291\u5236\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0b\u63a5\u8fd1\u5168\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u7684\u9891\u8c31\u6548\u7387\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u4ee3\u8868\u6027\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u5929\u7ebf\u67b6\u6784\u4e0b\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.08977", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08977", "abs": "https://arxiv.org/abs/2507.08977", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Marisa Eisenberg"], "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery", "comment": null, "summary": "Scientific modeling faces a core limitation: mechanistic models offer\ninterpretability but collapse under real-world complexity, while machine\nlearning models are flexible but require large labeled datasets, cannot infer\nunobservable quantities, and operate as black boxes. We introduce\nSimulation-Grounded Neural Networks (SGNNs), a general framework that uses\nmechanistic simulations as training data for neural networks. SGNNs are\npretrained on synthetic corpora spanning diverse model structures, parameter\nregimes, stochasticity, and observational artifacts. We evaluated SGNNs across\nscientific disciplines and modeling tasks, and found that SGNNs achieved\nstate-of-the-art results across settings: for prediction tasks, they nearly\ntripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield\nprediction error by one third, and maintained accuracy in ecological\nforecasting where task specific models failed. For inference tasks, SGNNs also\naccurately classified the source of information spread in simulated social\nnetworks and enabled supervised learning for unobservable targets, such as\nestimating COVID-19 transmissibility more accurately than traditional methods\neven in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,\na new form of mechanistic interpretability. Given real world input, SGNNs\nretrieve simulations based on what the model has learned to see as most\nsimilar, revealing which underlying dynamics the model believes are active.\nThis provides process-level insight -- what the model thinks is happening --\nnot just which features mattered. SGNNs unify scientific theory with deep\nlearning flexibility and unlock a new modeling paradigm -- transforming\nsimulations from rigid, post hoc tools into flexible sources of supervision,\nenabling robust, interpretable inference even when ground truth is missing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSGNNs\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u673a\u5236\u6a21\u62df\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u5efa\u6a21\u4e2d\u673a\u5236\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002SGNNs\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u5efa\u6a21\u4e2d\uff0c\u673a\u5236\u6a21\u578b\u53ef\u89e3\u91ca\u4f46\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u73b0\u5b9e\uff0c\u800c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7075\u6d3b\u4f46\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002SGNNs\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "SGNNs\u5229\u7528\u673a\u5236\u6a21\u62df\u751f\u6210\u5408\u6210\u6570\u636e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u8986\u76d6\u591a\u6837\u5316\u7684\u6a21\u578b\u7ed3\u6784\u3001\u53c2\u6570\u8303\u56f4\u548c\u89c2\u6d4b\u566a\u58f0\u3002", "result": "SGNNs\u5728\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u9886\u5148\u7ed3\u679c\uff0c\u5982COVID-19\u9884\u6d4b\u3001\u5316\u5b66\u53cd\u5e94\u4ea7\u7387\u9884\u6d4b\u548c\u751f\u6001\u9884\u6d4b\u7b49\u3002\u6b64\u5916\uff0cSGNNs\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "conclusion": "SGNNs\u5c06\u79d1\u5b66\u7406\u8bba\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7075\u6d3b\u6027\u7ed3\u5408\uff0c\u5f00\u521b\u4e86\u65b0\u7684\u5efa\u6a21\u8303\u5f0f\uff0c\u4f7f\u6a21\u62df\u6210\u4e3a\u7075\u6d3b\u7684\u76d1\u7763\u6765\u6e90\uff0c\u652f\u6301\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002"}}
{"id": "2507.08873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08873", "abs": "https://arxiv.org/abs/2507.08873", "authors": ["Shaoran Yang", "Dongyu Wei", "Hanzhi Yu", "Zhaohui Yang", "Yuchen Liu", "Mingzhe Chen"], "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization", "comment": "Submitted to IEEE GLOBECOM 2025", "summary": "In this paper, a novel contrastive language-image pre-training (CLIP) model\nbased semantic communication framework is designed. Compared to standard neural\nnetwork (e.g.,convolutional neural network) based semantic encoders and\ndecoders that require joint training over a common dataset, our CLIP model\nbased method does not require any training procedures thus enabling a\ntransmitter to extract data meanings of the original data without neural\nnetwork model training, and the receiver to train a neural network for\nfollow-up task implementation without the communications with the transmitter.\nNext, we investigate the deployment of the CLIP model based semantic framework\nover a noisy wireless network. Since the semantic information generated by the\nCLIP model is susceptible to wireless noise and the spectrum used for semantic\ninformation transmission is limited, it is necessary to jointly optimize CLIP\nmodel architecture and spectrum resource block (RB) allocation to maximize\nsemantic communication performance while considering wireless noise, the delay\nand energy used for semantic communication. To achieve this goal, we use a\nproximal policy optimization (PPO) based reinforcement learning (RL) algorithm\nto learn how wireless noise affect the semantic communication performance thus\nfinding optimal CLIP model and RB for each user. Simulation results show that\nour proposed method improves the convergence rate by up to 40%, and the\naccumulated reward by 4x compared to soft actor-critic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u6a21\u578b\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u53d6\u6570\u636e\u8bed\u4e49\uff0c\u5e76\u901a\u8fc7PPO\u7b97\u6cd5\u4f18\u5316\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8bed\u4e49\u7f16\u7801\u548c\u89e3\u7801\u9700\u8981\u8054\u5408\u8bad\u7ec3\uff0c\u800cCLIP\u6a21\u578b\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8bed\u4e49\u63d0\u53d6\uff0c\u540c\u65f6\u89e3\u51b3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u566a\u58f0\u548c\u8d44\u6e90\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528CLIP\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u7ed3\u5408PPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u67b6\u6784\u548c\u9891\u8c31\u8d44\u6e90\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u63d0\u534740%\uff0c\u7d2f\u79ef\u5956\u52b1\u63d0\u9ad84\u500d\u3002", "conclusion": "\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.10145", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.10145", "abs": "https://arxiv.org/abs/2507.10145", "authors": ["Ryohei Fukuma", "Yoshinobu Kawahara", "Okito Yamashita", "Kei Majima", "Haruhiko Kishima", "Takufumi Yanagisawa"], "title": "Intrinsic frequency distribution characterises neural dynamics", "comment": null, "summary": "Decomposing multivariate time series with certain basic dynamics is crucial\nfor understanding, predicting and controlling nonlinear spatiotemporally\ndynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method\nfor decomposing nonlinear spatiotemporal dynamics into several basic dynamics\n(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,\nunlike Fourier transform-based methods, which are used to decompose a\nsingle-channel signal into the amplitudes of sinusoidal waves with discrete\nfrequencies at a regular interval, DMD can derive the intrinsic frequencies of\na multichannel signal on the basis of the available data; furthermore, it can\ncapture nonstationary components such as alternations between states with\ndifferent intrinsic frequencies. Here, we propose the use of the distribution\nof intrinsic frequencies derived from DMDs (DM frequencies) to characterise\nneural activities. The distributions of DM frequencies in the\nelectroencephalograms of healthy subjects and patients with dementia or\nParkinson's disease in a resting state were evaluated. By using the\ndistributions, these patients were distinguished from healthy subjects with\nsignificantly greater accuracy than when using amplitude spectra derived by\ndiscrete Fourier transform. This finding suggests that the distribution of DM\nfrequencies exhibits distinct behaviour from amplitude spectra, and therefore,\nthe distribution may serve as a new biomarker by characterising the nonlinear\nspatiotemporal dynamics of electrophysiological signals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u52a8\u6001\u6a21\u6001\u5206\u89e3\uff08DMD\uff09\u7684\u56fa\u6709\u9891\u7387\u5206\u5e03\u6765\u8868\u5f81\u795e\u7ecf\u6d3b\u52a8\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u533a\u5206\u5065\u5eb7\u4eba\u4e0e\u75f4\u5446\u6216\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u3001\u9884\u6d4b\u548c\u63a7\u5236\u975e\u7ebf\u6027\u65f6\u7a7a\u52a8\u6001\u7cfb\u7edf\uff08\u5982\u5927\u8111\uff09\u9700\u8981\u5206\u89e3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u57fa\u672c\u52a8\u6001\u3002DMD\u80fd\u591f\u6355\u6349\u975e\u5e73\u7a33\u6210\u5206\uff0c\u63d0\u4f9b\u6bd4\u5085\u91cc\u53f6\u53d8\u6362\u66f4\u51c6\u786e\u7684\u5206\u6790\u3002", "method": "\u5229\u7528DMD\u5206\u89e3\u8111\u7535\u56fe\u6570\u636e\uff0c\u63d0\u53d6\u56fa\u6709\u9891\u7387\u5206\u5e03\uff0c\u5e76\u4e0e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5e45\u5ea6\u8c31\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "DMD\u9891\u7387\u5206\u5e03\u5728\u533a\u5206\u60a3\u8005\u4e0e\u5065\u5eb7\u4eba\u65f6\u663e\u8457\u4f18\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5e45\u5ea6\u8c31\u3002", "conclusion": "DMD\u9891\u7387\u5206\u5e03\u53ef\u4f5c\u4e3a\u8868\u5f81\u7535\u751f\u7406\u4fe1\u53f7\u975e\u7ebf\u6027\u65f6\u7a7a\u52a8\u6001\u7684\u65b0\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2507.09043", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09043", "abs": "https://arxiv.org/abs/2507.09043", "authors": ["Jingxiang Qu", "Wenhan Gao", "Yi Liu"], "title": "Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation", "comment": null, "summary": "Gaussian-based Probabilistic Generative Models (GPGMs) generate data by\nreversing a stochastic process that progressively corrupts samples with\nGaussian noise. While these models have achieved state-of-the-art performance\nacross diverse domains, their practical deployment remains constrained by the\nhigh computational cost of long generative trajectories, which often involve\nhundreds to thousands of steps during training and sampling. In this work, we\nintroduce a theoretically grounded and empirically validated framework that\nimproves generation efficiency without sacrificing training granularity or\ninference fidelity. Our key insight is that for certain data modalities, the\nnoising process causes data to rapidly lose its identity and converge toward a\nGaussian distribution. We analytically identify a characteristic step at which\nthe data has acquired sufficient Gaussianity, and then replace the remaining\ngeneration trajectory with a closed-form Gaussian approximation. Unlike\nexisting acceleration techniques that coarsening the trajectories by skipping\nsteps, our method preserves the full resolution of learning dynamics while\navoiding redundant stochastic perturbations between `Gaussian-like'\ndistributions. Empirical results across multiple data modalities demonstrate\nsubstantial improvements in both sample quality and computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u9ad8\u65af\u6982\u7387\u751f\u6210\u6a21\u578b\uff08GPGMs\uff09\u751f\u6210\u6548\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u6570\u636e\u5feb\u901f\u6536\u655b\u5230\u9ad8\u65af\u5206\u5e03\u7684\u7279\u5f81\u6b65\u9aa4\uff0c\u5e76\u7528\u95ed\u5f0f\u9ad8\u65af\u8fd1\u4f3c\u66ff\u4ee3\u5269\u4f59\u8f68\u8ff9\u3002", "motivation": "GPGMs\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\uff0c\u9700\u8981\u51cf\u5c11\u751f\u6210\u8f68\u8ff9\u7684\u5197\u4f59\u6b65\u9aa4\u3002", "method": "\u5206\u6790\u6570\u636e\u5feb\u901f\u6536\u655b\u5230\u9ad8\u65af\u5206\u5e03\u7684\u7279\u5f81\u6b65\u9aa4\uff0c\u7528\u95ed\u5f0f\u9ad8\u65af\u8fd1\u4f3c\u66ff\u4ee3\u5269\u4f59\u8f68\u8ff9\uff0c\u907f\u514d\u5197\u4f59\u6270\u52a8\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u6a21\u6001\u4e0a\u5b9e\u73b0\u4e86\u6837\u672c\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b66\u4e60\u52a8\u6001\u5b8c\u6574\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2507.08874", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08874", "abs": "https://arxiv.org/abs/2507.08874", "authors": ["Yulin Sun", "Xiaopeng Si", "Runnan He", "Xiao Hu", "Peter Smielewski", "Wenlong Wang", "Xiaoguang Tong", "Wei Yue", "Meijun Pang", "Kuo Zhang", "Xizi Song", "Dong Ming", "Xiuyun Liu"], "title": "An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework", "comment": null, "summary": "Timely identification of harmful brain activities via electroencephalography\n(EEG) is critical for brain disease diagnosis and treatment, which remains\nlimited application due to inter-rater variability, resource constraints, and\npoor generalizability of existing artificial intelligence (AI) models. In this\nstudy, a convolutional neural network model, VIPEEGNet, was developed and\nvalidated using EEGs recorded from Massachusetts General Hospital/Harvard\nMedical School. The VIPEEGNet was developed and validated using two independent\ndatasets, collected between 2006 and 2020. The development cohort included EEG\nrecordings from 1950 patients, with 106,800 EEG segments annotated by at least\none experts (ranging from 1 to 28). The online testing cohort consisted of EEG\nsegments from a subset of an additional 1,532 patients, each annotated by at\nleast 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved\nhigh accuracy, with an AUROC for binary classification of seizure, LPD, GPD,\nLRDA, GRDA, and \"other\" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%\nCI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),\n0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi\nclassification, the sensitivity of VIPEEGNET for the six categories ranges from\n36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance\nsimilar to human experts. Notably, the external validation showed\nKullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the\nexisting 2,767 competing algorithms, while we only used 2.8% of the parameters\nof the first-ranked algorithm.", "AI": {"tldr": "VIPEEGNet\u662f\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u901a\u8fc7EEG\u53ca\u65f6\u8bc6\u522b\u6709\u5bb3\u8111\u6d3b\u52a8\uff0c\u6027\u80fd\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\uff0c\u5e76\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u5728EEG\u8bc6\u522b\u8111\u6d3b\u52a8\u65f6\u5b58\u5728\u8bc4\u5206\u8005\u95f4\u5dee\u5f02\u3001\u8d44\u6e90\u9650\u5236\u548c\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86VIPEEGNet\u6a21\u578b\uff0c\u4f7f\u7528\u4e24\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\uff081950\u540d\u60a3\u8005\u548c1532\u540d\u60a3\u8005\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "VIPEEGNet\u5728\u4e8c\u5143\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5916\u90e8\u9a8c\u8bc1\u6392\u540d\u524d2\u3002", "conclusion": "VIPEEGNet\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53c2\u6570\u8f83\u5c11\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8eEEG\u8111\u6d3b\u52a8\u8bc6\u522b\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10167", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10167", "abs": "https://arxiv.org/abs/2507.10167", "authors": ["Kaidi Wang", "Zhiguo Ding", "Naofal Al-Dhahir"], "title": "Pinching-Antenna Systems for Physical Layer Security", "comment": null, "summary": "This letter investigates the potential of pinching-antenna systems for\nenhancing physical layer security. By pre-installing multiple pinching antennas\nat discrete positions along a waveguide, the capability of the considered\nsystem to perform amplitude and phase adjustment is validated through the\nformulation of a secrecy rate maximization problem. Specifically, amplitude\ncontrol is applied to enhance the signal quality at the legitimate user, while\nphase alignment is designed to degrade the received signal quality at the\neavesdropper. This cooperation among pinching antennas is modeled as a\ncoalitional game, and a corresponding antenna activation algorithm is proposed.\nThe individual impact of each antenna is quantified based on the Shapley value\nand marginal contribution, providing a fair and efficient method for\nperformance evaluation. Simulation results show that the considered\npinching-antenna system achieves significant improvements in secrecy rate, and\nthat the Shapley value based algorithm outperforms conventional coalition value\nbased solutions.", "AI": {"tldr": "\u7814\u7a76\u4e86\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u5728\u589e\u5f3a\u7269\u7406\u5c42\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u632f\u5e45\u548c\u76f8\u4f4d\u8c03\u6574\u4f18\u5316\u5408\u6cd5\u7528\u6237\u4fe1\u53f7\u5e76\u524a\u5f31\u7a83\u542c\u8005\u4fe1\u53f7\uff0c\u63d0\u51fa\u57fa\u4e8eShapley\u503c\u7684\u5929\u7ebf\u6fc0\u6d3b\u7b97\u6cd5\uff0c\u4eff\u771f\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u5728\u7269\u7406\u5c42\u5b89\u5168\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u591a\u5929\u7ebf\u534f\u4f5c\u63d0\u5347\u5408\u6cd5\u7528\u6237\u7684\u901a\u4fe1\u8d28\u91cf\u5e76\u6291\u5236\u7a83\u542c\u8005\u3002", "method": "\u5728\u6ce2\u5bfc\u4e0a\u9884\u88c5\u591a\u4e2a\u634f\u5408\u5929\u7ebf\uff0c\u901a\u8fc7\u632f\u5e45\u548c\u76f8\u4f4d\u8c03\u6574\u4f18\u5316\u4fe1\u53f7\uff0c\u5efa\u6a21\u4e3a\u8054\u76df\u535a\u5f08\uff0c\u63d0\u51fa\u57fa\u4e8eShapley\u503c\u7684\u5929\u7ebf\u6fc0\u6d3b\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u4fdd\u5bc6\u901f\u7387\uff0c\u4e14\u57fa\u4e8eShapley\u503c\u7684\u7b97\u6cd5\u4f18\u4e8e\u4f20\u7edf\u8054\u76df\u503c\u65b9\u6cd5\u3002", "conclusion": "\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u901a\u8fc7\u591a\u5929\u7ebf\u534f\u4f5c\u548cShapley\u503c\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7269\u7406\u5c42\u5b89\u5168\u6027\u80fd\u3002"}}
{"id": "2507.08877", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08877", "abs": "https://arxiv.org/abs/2507.08877", "authors": ["Hanlong Zhang", "Jingsheng Yang", "Hao Li", "Yuhao He", "Franck Gong"], "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling", "comment": null, "summary": "Function Calling is a crucial technique that enables Large Language Models\n(LLMs) to interact with external systems through APIs. However, the high\nlatency associated with LLM-based Function Calling significantly impacts user\nexperience. This paper presents a novel approach called Oriented Distillation\nfor Inline Acceleration (ODIA) that leverages online user interaction data to\naccelerate Function Calling. By automatically identifying \"simple queries\" from\nproduction traffic and distilling knowledge from larger models to smaller ones,\nour method reduces response latency by 45% (expected) and 78% (median) while\nmaintaining accuracy. We demonstrate the effectiveness of our approach through\nreal-world deployment in a music application, where the smaller model\nsuccessfully handles 60% of traffic with negligible accuracy loss. Our method\nrequires minimal human intervention and continuously improves through automated\ndata collection and model updating, making it a practical solution for\nproduction environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aODIA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u7528\u6237\u4ea4\u4e92\u6570\u636e\u52a0\u901fLLM\u7684\u51fd\u6570\u8c03\u7528\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "LLM\u51fd\u6570\u8c03\u7528\u7684\u9ad8\u5ef6\u8fdf\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u5728\u7ebf\u7528\u6237\u6570\u636e\u8bc6\u522b\u7b80\u5355\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u5927\u6a21\u578b\u80fd\u529b\u8fc1\u79fb\u5230\u5c0f\u6a21\u578b\u3002", "result": "\u5ef6\u8fdf\u964d\u4f4e45%\uff08\u9884\u671f\uff09\u548c78%\uff08\u4e2d\u4f4d\u6570\uff09\uff0c\u5c0f\u6a21\u578b\u5904\u740660%\u6d41\u91cf\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "ODIA\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002"}}
{"id": "2507.10173", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10173", "abs": "https://arxiv.org/abs/2507.10173", "authors": ["Kaidi Wang", "Chongjun Ouyang", "Yuanwei Liu", "Zhiguo Ding"], "title": "Pinching-Antenna Systems with LoS Blockages", "comment": null, "summary": "The aim of this letter is to explore the capability of pinching-antenna\nsystems to construct line-of-sight (LoS) links in the presence of LoS\nblockages. Specifically, pinching antennas are pre-installed at preconfigured\npositions along waveguides and can be selectively activated to create LoS links\nfor enhancing desired signals and non-line-of-sight (NLoS) links for\neliminating inter-user interference. On this basis, a sum-rate maximization\nproblem is formulated by jointly optimizing waveguide assignment and antenna\nactivation. To solve this problem, a matching based algorithm is proposed using\ntwo distinct preference designs. Simulation results demonstrate that the\nconsidered pinching-antenna system and proposed solutions can dynamically\nestablish LoS links and effectively exploit LoS blockages to mitigate\ninterference, thereby significantly improving system throughput.", "AI": {"tldr": "\u7814\u7a76\u4e86\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u5728\u89c6\u7ebf\uff08LoS\uff09\u963b\u585e\u60c5\u51b5\u4e0b\u6784\u5efaLoS\u94fe\u8def\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u6ce2\u5bfc\u5206\u914d\u548c\u5929\u7ebf\u6fc0\u6d3b\uff0c\u63d0\u51fa\u5339\u914d\u7b97\u6cd5\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u63a2\u7d22\u5728LoS\u963b\u585e\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u52a8\u6001\u5efa\u7acbLoS\u94fe\u8def\u4ee5\u589e\u5f3a\u4fe1\u53f7\u5e76\u6d88\u9664\u5e72\u6270\u3002", "method": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6ce2\u5bfc\u5206\u914d\u548c\u5929\u7ebf\u6fc0\u6d3b\uff0c\u63d0\u51fa\u57fa\u4e8e\u5339\u914d\u7684\u7b97\u6cd5\uff0c\u91c7\u7528\u4e24\u79cd\u504f\u597d\u8bbe\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u5229\u7528LoS\u963b\u585e\u51cf\u5c11\u5e72\u6270\uff0c\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u53ca\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u80fd\u52a8\u6001\u5efa\u7acbLoS\u94fe\u8def\uff0c\u6709\u6548\u5229\u7528\u963b\u585e\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.09087", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09087", "abs": "https://arxiv.org/abs/2507.09087", "authors": ["Esraa Elelimy", "Brett Daley", "Andrew Patterson", "Marlos C. Machado", "Adam White", "Martha White"], "title": "Deep Reinforcement Learning with Gradient Eligibility Traces", "comment": null, "summary": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the Generalized Projected Bellman\nError ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear\nfunction approximation. However, this work is only limited to one-step methods,\nwhich are slow at credit assignment and require a large number of samples. In\nthis paper, we extend the $\\GPBE$ objective to support multistep credit\nassignment based on the $\\lambda$-return and derive three gradient-based\nmethods that optimize this new objective. We provide both a forward-view\nformulation compatible with experience replay and a backward-view formulation\ncompatible with streaming algorithms. Finally, we evaluate the proposed\nalgorithms and show that they outperform both PPO and StreamQ in MuJoCo and\nMinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u6295\u5f71\u8d1d\u5c14\u66fc\u8bef\u5dee\uff08GPBE\uff09\u7684\u591a\u6b65\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u534a\u68af\u5ea6TD\u7b80\u5355\u9ad8\u6548\u4f46\u6613\u53d1\u6563\uff0c\u800cGTD\u65b9\u6cd5\u867d\u6709\u6536\u655b\u4fdd\u8bc1\u4f46\u5f88\u5c11\u7528\u4e8e\u6df1\u5ea6RL\u3002GPBE\u7684\u5f15\u5165\u4e3a\u975e\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u652f\u6301\uff0c\u4f46\u4ec5\u9650\u4e8e\u5355\u6b65\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u4fe1\u7528\u5206\u914d\u901f\u5ea6\u548c\u6837\u672c\u6548\u7387\u3002", "method": "\u5c06GPBE\u76ee\u6807\u6269\u5c55\u4e3a\u652f\u6301\u57fa\u4e8e\u03bb-\u56de\u62a5\u7684\u591a\u6b65\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u79cd\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u524d\u5411\u89c6\u56fe\u548c\u540e\u5411\u89c6\u56fe\u3002", "result": "\u5728MuJoCo\u548cMinAtar\u73af\u5883\u4e2d\uff0c\u65b0\u7b97\u6cd5\u4f18\u4e8ePPO\u548cStreamQ\u3002", "conclusion": "\u591a\u6b65GPBE\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6RL\u7684\u7a33\u5b9a\u6027\u548c\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2507.08905", "categories": ["cs.LG", "cs.AI", "G.3"], "pdf": "https://arxiv.org/pdf/2507.08905", "abs": "https://arxiv.org/abs/2507.08905", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "G\u00f6ran Falkman", "Jonas Andersson", "Anders Sj\u00f6gren"], "title": "Last Layer Hamiltonian Monte Carlo", "comment": "25 pages, 15 figures, 6 tables, currently under submission", "summary": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a\nprobabilistic last layer approach for deep neural networks (DNNs). While HMC is\nwidely regarded as a gold standard for uncertainty estimation, the\ncomputational demands limit its application to large-scale datasets and large\nDNN architectures. Although the predictions from the sampled DNN parameters can\nbe parallelized, the computational cost still scales linearly with the number\nof samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the\nrequired computations by restricting the HMC sampling to the final layer of a\nDNN, making it applicable to more data-intensive scenarios with limited\ncomputational resources. In this paper, we compare LL-HMC against five last\nlayer probabilistic deep learning (LL-PDL) methods across three real-world\nvideo datasets for driver action and intention. We evaluate the in-distribution\nclassification performance, calibration, and out-of-distribution (OOD)\ndetection. Due to the stochastic nature of the probabilistic evaluations, we\nperformed five grid searches for different random seeds to avoid being reliant\non a single initialization for the hyperparameter configurations. The results\nshow that LL--HMC achieves competitive in-distribution classification and OOD\ndetection performance. Additional sampled last layer parameters do not improve\nthe classification performance, but can improve the OOD detection. Multiple\nchains or starting positions did not yield consistent improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54c8\u5bc6\u987f\u8499\u7279\u5361\u6d1b\uff08HMC\uff09\u91c7\u6837\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6982\u7387\u6700\u540e\u4e00\u5c42\u65b9\u6cd5\uff08LL-HMC\uff09\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1HMC\u662f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u91d1\u6807\u51c6\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u6570\u636e\u548c\u5927\u578bDNN\u4e2d\u7684\u5e94\u7528\u3002LL-HMC\u901a\u8fc7\u4ec5\u5bf9DNN\u6700\u540e\u4e00\u5c42\u8fdb\u884cHMC\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "LL-HMC\u5c06HMC\u91c7\u6837\u9650\u5236\u5728DNN\u7684\u6700\u540e\u4e00\u5c42\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\u3002\u8bba\u6587\u5c06\u5176\u4e0e\u4e94\u79cd\u6700\u540e\u4e00\u5c42\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08LL-PDL\uff09\u5728\u4e09\u4e2a\u771f\u5b9e\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5206\u7c7b\u6027\u80fd\u3001\u6821\u51c6\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u68c0\u6d4b\u3002", "result": "LL-HMC\u5728\u5206\u7c7b\u548cOOD\u68c0\u6d4b\u4e0a\u8868\u73b0\u7ade\u4e89\u6027\u3002\u989d\u5916\u7684\u91c7\u6837\u53c2\u6570\u672a\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u6539\u8fdb\u4e86OOD\u68c0\u6d4b\u3002\u591a\u94fe\u6216\u4e0d\u540c\u521d\u59cb\u4f4d\u7f6e\u672a\u5e26\u6765\u4e00\u81f4\u6539\u8fdb\u3002", "conclusion": "LL-HMC\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6982\u7387\u6700\u540e\u4e00\u5c42\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\uff0c\u5c24\u5176\u5728OOD\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.10308", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10308", "abs": "https://arxiv.org/abs/2507.10308", "authors": ["Hung Nguyen-Kha", "Vu Nguyen Ha", "Eva Lagunas", "Symeon Chatzinotas", "Joel Grotz"], "title": "Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks", "comment": "ACCEPTED FOR PUBLICATION IN IEEE TRANSACTIONS ON COMMUNICATIONS", "summary": "This paper investigates downlink transmission in 5G Integrated\nSatellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in\nurban environments, where base stations (BSs) and Low Earth Orbit (LEO)\nsatellites (LSats) cooperate to serve moving UEs over shared C-band frequency\ncarriers. Urban settings, characterized by dense obstructions, together with UE\nmobility, and the dynamic movement and coverage of LSats pose significant\nchallenges to user association and resource allocation. To address these\nchallenges, we formulate a multi-objective optimization problem designed to\nimprove both throughput and seamless handover (HO). Particularly, the\nformulated problem balances sum-rate (SR) maximization and connection change\n(CC) minimization through a weighted trade-off by jointly optimizing power\nallocation and BS-UE/LSat-UE associations over a given time window. This is a\nmixed-integer and non-convex problem which is inherently difficult to solve. To\nsolve this problem efficiently, we propose an iterative algorithm based on the\nSuccessive Convex Approximation (SCA) technique. Furthermore, we introduce a\npractical prediction-based algorithm capable of providing efficient solutions\nin real-world implementations. Especially, the simulations use a realistic 3D\nmap of London and UE routes obtained from the Google Navigator application to\nensure practical examination. Thanks to these realistic data, the simulation\nresults can show valuable insights into the link budget assessment in urban\nareas due to the impact of buildings on transmission links under the blockage,\nreflection, and diffraction effects. Furthermore, the numerical results\ndemonstrate the effectiveness of our proposed algorithms in terms of SR and the\nCC-number compared to the greedy and benchmark algorithms.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e865G\u536b\u661f-\u5730\u9762\u7efc\u5408\u7f51\u7edc\uff08ISTN\uff09\u4e2d\u652f\u6301\u57ce\u5e02\u73af\u5883\u4e2d\u79fb\u52a8\u7528\u6237\uff08UE\uff09\u7684\u4e0b\u884c\u4f20\u8f93\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u7528\u6237\u5173\u8054\uff0c\u89e3\u51b3\u9ad8\u5bc6\u5ea6\u906e\u6321\u548c\u52a8\u6001\u8986\u76d6\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u9ad8\u5bc6\u5ea6\u906e\u6321\u3001UE\u79fb\u52a8\u6027\u53caLEO\u536b\u661f\u52a8\u6001\u8986\u76d6\u5bf9\u7528\u6237\u5173\u8054\u548c\u8d44\u6e90\u5206\u914d\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u4f18\u5316\u541e\u5410\u91cf\u548c\u65e0\u7f1d\u5207\u6362\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u8fde\u7eed\u51f8\u8fd1\u4f3c\uff08SCA\uff09\u7684\u8fed\u4ee3\u7b97\u6cd5\u548c\u5b9e\u7528\u7684\u9884\u6d4b\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u7528\u6237\u5173\u8054\u3002", "result": "\u4eff\u771f\u4f7f\u7528\u4f26\u65663D\u5730\u56fe\u548cGoogle\u5bfc\u822a\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u541e\u5410\u91cf\u548c\u8fde\u63a5\u53d8\u5316\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u7b97\u6cd5\u5728\u541e\u5410\u91cf\u548c\u8fde\u63a5\u53d8\u5316\u65b9\u9762\u4f18\u4e8e\u8d2a\u5a6a\u548c\u57fa\u51c6\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.09091", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09091", "abs": "https://arxiv.org/abs/2507.09091", "authors": ["Shayan K. Azmoodeh", "Krishna Subramani", "Paris Smaragdis"], "title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA", "comment": "6 pages, 3 figures, 1 table. MLSP 2025", "summary": "We generalize the low-rank decomposition problem, such as principal and\nindependent component analysis (PCA, ICA) for continuous-time vector-valued\nsignals and provide a model-agnostic implicit neural signal representation\nframework to learn numerical approximations to solve the problem. Modeling\nsignals as continuous-time stochastic processes, we unify the approaches to\nboth the PCA and ICA problems in the continuous setting through a contrast\nfunction term in the network loss, enforcing the desired statistical properties\nof the source signals (decorrelation, independence) learned in the\ndecomposition. This extension to a continuous domain allows the application of\nsuch decompositions to point clouds and irregularly sampled signals where\nstandard techniques are not applicable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u4f4e\u79e9\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u795e\u7ecf\u4fe1\u53f7\u8868\u793a\u6846\u67b6\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u5411\u91cf\u503c\u4fe1\u53f7\u7684PCA\u548cICA\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPCA\u548cICA\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u65f6\u95f4\u4fe1\u53f7\u3001\u70b9\u4e91\u6216\u4e0d\u89c4\u5219\u91c7\u6837\u4fe1\u53f7\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u65f6\u95f4\u968f\u673a\u8fc7\u7a0b\u5efa\u6a21\u4fe1\u53f7\uff0c\u901a\u8fc7\u5bf9\u6bd4\u51fd\u6570\u9879\u7edf\u4e00PCA\u548cICA\u65b9\u6cd5\uff0c\u5f3a\u5236\u5b66\u4e60\u5206\u89e3\u540e\u7684\u4fe1\u53f7\u5177\u6709\u6240\u9700\u7684\u7edf\u8ba1\u7279\u6027\uff08\u5982\u53bb\u76f8\u5173\u3001\u72ec\u7acb\u6027\uff09\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5e94\u7528\u4e8e\u4f20\u7edf\u6280\u672f\u65e0\u6cd5\u5904\u7406\u7684\u70b9\u4e91\u548c\u4e0d\u89c4\u5219\u91c7\u6837\u4fe1\u53f7\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u8fde\u7eed\u57df\u4fe1\u53f7\u5206\u89e3\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86PCA\u548cICA\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.08912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08912", "abs": "https://arxiv.org/abs/2507.08912", "authors": ["Tomasz Szandala", "Fatima Ezzeddine", "Natalia Rusin", "Silvia Giordano", "Omran Ayoub"], "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising", "comment": null, "summary": "Artificial Intelligence-generated content has become increasingly popular,\nyet its malicious use, particularly the deepfakes, poses a serious threat to\npublic trust and discourse. While deepfake detection methods achieve high\npredictive performance, they often exhibit biases across demographic attributes\nsuch as ethnicity and gender. In this work, we tackle the challenge of fair\ndeepfake detection, aiming to mitigate these biases while maintaining robust\ndetection capabilities. To this end, we propose a novel post-processing\napproach, referred to as Fairness-Oriented Final Layer Input Prioritising\n(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce\nsubgroup disparities, prioritising those with low variability while demoting\nhighly variable ones. Experimental results comparing Fair-FLIP to both the\nbaseline (without fairness-oriented de-biasing) and state-of-the-art approaches\nshow that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining\nbaseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github:\nhttps://github.com/szandala/fair-deepfake-detection-toolbox", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFair-FLIP\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u6307\u6807\uff0c\u4e14\u5bf9\u51c6\u786e\u7387\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u9ad8\uff0c\u4f46\u5b58\u5728\u5bf9\u79cd\u65cf\u548c\u6027\u522b\u7b49\u4eba\u53e3\u5c5e\u6027\u7684\u504f\u89c1\uff0c\u5a01\u80c1\u516c\u4f17\u4fe1\u4efb\u3002", "method": "\u63d0\u51faFair-FLIP\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u540e\u4e00\u5c42\u8f93\u5165\uff0c\u51cf\u5c11\u5b50\u7fa4\u4f53\u95f4\u7684\u5dee\u5f02\u3002", "result": "Fair-FLIP\u5c06\u516c\u5e73\u6027\u6307\u6807\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.25%\u3002", "conclusion": "Fair-FLIP\u662f\u4e00\u79cd\u6709\u6548\u7684\u516c\u5e73\u6027\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u9886\u57df\u3002"}}
{"id": "2507.09127", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09127", "abs": "https://arxiv.org/abs/2507.09127", "authors": ["Harshil Kotamreddy", "Marlos C. Machado"], "title": "A Study of Value-Aware Eigenoptions", "comment": "Presented at the RLC Workshop on Inductive Biases in Reinforcement\n  Learning 2025", "summary": "Options, which impose an inductive bias toward temporal and hierarchical\nstructure, offer a powerful framework for reinforcement learning (RL). While\neffective in sequential decision-making, they are often handcrafted rather than\nlearned. Among approaches for discovering options, eigenoptions have shown\nstrong performance in exploration, but their role in credit assignment remains\nunderexplored. In this paper, we investigate whether eigenoptions can\naccelerate credit assignment in model-free RL, evaluating them in tabular and\npixel-based gridworlds. We find that pre-specified eigenoptions aid not only\nexploration but also credit assignment, whereas online discovery can bias the\nagent's experience too strongly and hinder learning. In the context of deep RL,\nwe also propose a method for learning option-values under non-linear function\napproximation, highlighting the impact of termination conditions on\nperformance. Our findings reveal both the promise and complexity of using\neigenoptions, and options more broadly, to simultaneously support credit\nassignment and exploration in reinforcement learning.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7279\u5f81\u9009\u9879\uff08eigenoptions\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5bf9\u4fe1\u7528\u5206\u914d\u548c\u63a2\u7d22\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u9884\u8bbe\u7279\u5f81\u9009\u9879\u6709\u52a9\u4e8e\u4e24\u8005\uff0c\u800c\u5728\u7ebf\u53d1\u73b0\u53ef\u80fd\u963b\u788d\u5b66\u4e60\u3002", "motivation": "\u63a2\u7d22\u7279\u5f81\u9009\u9879\u662f\u5426\u80fd\u5728\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u52a0\u901f\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u8868\u683c\u548c\u50cf\u7d20\u7f51\u683c\u4e16\u754c\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5728\u8868\u683c\u548c\u50cf\u7d20\u7f51\u683c\u4e16\u754c\u4e2d\u8bc4\u4f30\u9884\u8bbe\u548c\u5728\u7ebf\u53d1\u73b0\u7279\u5f81\u9009\u9879\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5728\u975e\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u4e0b\u5b66\u4e60\u9009\u9879\u503c\u7684\u65b9\u6cd5\u3002", "result": "\u9884\u8bbe\u7279\u5f81\u9009\u9879\u6709\u52a9\u4e8e\u4fe1\u7528\u5206\u914d\u548c\u63a2\u7d22\uff0c\u800c\u5728\u7ebf\u53d1\u73b0\u53ef\u80fd\u56e0\u504f\u7f6e\u7ecf\u9a8c\u800c\u963b\u788d\u5b66\u4e60\u3002\u7ec8\u6b62\u6761\u4ef6\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7279\u5f81\u9009\u9879\u5728\u652f\u6301\u4fe1\u7528\u5206\u914d\u548c\u63a2\u7d22\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u5728\u5728\u7ebf\u53d1\u73b0\u548c\u7ec8\u6b62\u6761\u4ef6\u8bbe\u8ba1\u4e0a\u3002"}}
{"id": "2507.09177", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09177", "abs": "https://arxiv.org/abs/2507.09177", "authors": ["Zichen Liu", "Guoji Fu", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Continual Reinforcement Learning by Planning with Online World Models", "comment": "ICML 2025 Spotlight", "summary": "Continual reinforcement learning (CRL) refers to a naturalistic setting where\nan agent needs to endlessly evolve, by trial and error, to solve multiple tasks\nthat are presented sequentially. One of the largest obstacles to CRL is that\nthe agent may forget how to solve previous tasks when learning a new task,\nknown as catastrophic forgetting. In this paper, we propose to address this\nchallenge by planning with online world models. Specifically, we learn a\nFollow-The-Leader shallow model online to capture the world dynamics, in which\nwe plan using model predictive control to solve a set of tasks specified by any\nreward functions. The online world model is immune to forgetting by\nconstruction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$\nunder mild assumptions. The planner searches actions solely based on the latest\nonline model, thus forming a FTL Online Agent (OA) that updates incrementally.\nTo assess OA, we further design Continual Bench, a dedicated environment for\nCRL, and compare with several strong baselines under the same model-planning\nalgorithmic framework. The empirical results show that OA learns continuously\nto solve new tasks while not forgetting old skills, outperforming agents built\non deep world models with various continual learning techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c4\u5212\u89e3\u51b3\u4efb\u52a1\u5e8f\u5217\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u4e2d\uff0c\u667a\u80fd\u4f53\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u5bb9\u6613\u5fd8\u8bb0\u65e7\u4efb\u52a1\uff0c\u5373\u707e\u96be\u6027\u9057\u5fd8\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u5b66\u4e60\u7684Follow-The-Leader\u6d45\u5c42\u6a21\u578b\u6355\u6349\u4e16\u754c\u52a8\u6001\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u89c4\u5212\u4efb\u52a1\u3002\u6a21\u578b\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u9057\u61be\u8fb9\u754c\u3002", "result": "\u5728\u8bbe\u8ba1\u7684Continual Bench\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u4e16\u754c\u6a21\u578b\u548c\u6301\u7eed\u5b66\u4e60\u6280\u672f\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u80fd\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u4e14\u4e0d\u9057\u5fd8\u65e7\u6280\u80fd\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.09887", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09887", "abs": "https://arxiv.org/abs/2507.09887", "authors": ["Huynh Nguyen Dang", "Thang Pham", "Ngan Le", "Van Nguyen"], "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram", "comment": "10 pages, 6 figures. Accepted to ACM Multimedia 2025", "summary": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing\nheart diseases. However, its effectiveness can be compromised by noise or\nunavailability of one or more leads of the standard 12-lead recordings,\nresulting in diagnostic errors or uncertainty. To address these challenges, we\npropose TolerantECG, a foundation model for ECG signals that is robust to noise\nand capable of functioning with arbitrary subsets of the standard 12-lead ECG.\nTolerantECG training combines contrastive and self-supervised learning\nframeworks to jointly learn ECG signal representations alongside their\ncorresponding knowledge-retrieval-based text report descriptions and corrupted\nor lead-missing signals. Comprehensive benchmarking results demonstrate that\nTolerantECG consistently ranks as the best or second-best performer across\nvarious ECG signal conditions and class levels in the PTB-XL dataset, and\nachieves the highest performance on the MIT-BIH Arrhythmia Database.", "AI": {"tldr": "TolerantECG\u662f\u4e00\u79cd\u9488\u5bf9ECG\u4fe1\u53f7\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u6297\u566a\u58f0\u5e76\u9002\u5e94\u90e8\u5206\u5bfc\u8054\u7f3a\u5931\u7684\u60c5\u51b5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "ECG\u4fe1\u53f7\u53ef\u80fd\u56e0\u566a\u58f0\u6216\u5bfc\u8054\u7f3a\u5931\u5bfc\u81f4\u8bca\u65ad\u9519\u8bef\u6216\u4e0d\u786e\u5b9a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60ECG\u4fe1\u53f7\u8868\u793a\u53ca\u5176\u6587\u672c\u62a5\u544a\u63cf\u8ff0\uff0c\u540c\u65f6\u5904\u7406\u566a\u58f0\u6216\u5bfc\u8054\u7f3a\u5931\u4fe1\u53f7\u3002", "result": "\u5728PTB-XL\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u6700\u4f73\u6216\u6b21\u4f73\uff0c\u5728MIT-BIH\u5fc3\u5f8b\u5931\u5e38\u6570\u636e\u5e93\u4e2d\u6027\u80fd\u6700\u9ad8\u3002", "conclusion": "TolerantECG\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u5347ECG\u4fe1\u53f7\u5206\u6790\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.09211", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an", "physics.geo-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09211", "abs": "https://arxiv.org/abs/2507.09211", "authors": ["Xinyue Liu", "Xiao Peng", "Shuyue Yan", "Yuntian Chen", "Dongxiao Zhang", "Zhixiao Niu", "Hui-Min Wang", "Xiaogang He"], "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling", "comment": null, "summary": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.", "AI": {"tldr": "DeepX-GAN\u6a21\u578b\u7528\u4e8e\u6a21\u62df\u8d85\u51fa\u5386\u53f2\u8bb0\u5f55\u7684\u6781\u7aef\u6c14\u5019\u4e8b\u4ef6\uff0c\u63ed\u793a\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f4e\u4f30\u4e86\u6781\u7aef\u6c14\u5019\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u672a\u89c1\u8fc7\u7684\u4e8b\u4ef6\u98ce\u9669\u3002", "method": "\u5f00\u53d1\u4e86DeepX-GAN\uff0c\u4e00\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u6355\u6349\u7f55\u89c1\u6781\u7aef\u4e8b\u4ef6\u7684\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "\u6a21\u578b\u6210\u529f\u6a21\u62df\u4e86\u672a\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u63ed\u793a\u4e86\u8106\u5f31\u5730\u533a\u7684\u6f5c\u5728\u98ce\u9669\u3002", "conclusion": "\u9700\u5236\u5b9a\u7a7a\u95f4\u9002\u5e94\u6027\u653f\u7b56\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765\u6c14\u5019\u53d8\u6696\u5e26\u6765\u7684\u65b0\u5174\u98ce\u9669\u70ed\u70b9\u3002"}}
{"id": "2507.08959", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08959", "abs": "https://arxiv.org/abs/2507.08959", "authors": ["Xiang Li", "Xinyu Wang", "Yifan Lin"], "title": "Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign", "comment": null, "summary": "In order to improve the accuracy of cross-platform advertisement\nrecommendation, a graph neural network (GNN)- based advertisement\nrecommendation method is analyzed. Through multi-dimensional modeling, user\nbehavior data (e.g., click frequency, active duration) reveal temporal patterns\nof interest evolution, ad content (e.g., type, tag, duration) influences\nsemantic preferences, and platform features (e.g., device type, usage context)\nshape the environment where interest transitions occur. These factors jointly\nenable the GNN to capture the latent pathways of user interest migration across\nplatforms. The experimental results are based on the datasets of three\nplatforms, and Platform B reaches 0.937 in AUC value, which is the best\nperformance. Platform A and Platform C showed a slight decrease in precision\nand recall with uneven distribution of ad labels. By adjusting the\nhyperparameters such as learning rate, batch size and embedding dimension, the\nadaptability and robustness of the model in heterogeneous data are further\nimproved.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5efa\u6a21\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u63d0\u9ad8\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u7684\u51c6\u786e\u6027\uff0c\u6355\u6349\u7528\u6237\u5174\u8da3\u8fc1\u79fb\u7684\u6f5c\u5728\u8def\u5f84\u3002", "method": "\u5229\u7528\u7528\u6237\u884c\u4e3a\u6570\u636e\u3001\u5e7f\u544a\u5185\u5bb9\u548c\u5e73\u53f0\u7279\u5f81\u8fdb\u884c\u591a\u7ef4\u5efa\u6a21\uff0c\u901a\u8fc7GNN\u6355\u6349\u5174\u8da3\u8fc1\u79fb\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cPlatform B\u7684AUC\u503c\u8fbe\u52300.937\uff0c\u8868\u73b0\u6700\u4f73\uff1bPlatform A\u548cC\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7565\u6709\u4e0b\u964d\u3002\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u5e73\u53f0\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.10325", "categories": ["cs.LG", "cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10325", "abs": "https://arxiv.org/abs/2507.10325", "authors": ["Herlock", "Rahimi", "Dionysis Kalogerias"], "title": "Convergence of Agnostic Federated Averaging", "comment": "5 pages, 2 figurres, CAMSAP conference", "summary": "Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u5ba2\u6237\u7aef\u968f\u673a\u53c2\u4e0e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5bf9Agnostic FedAvg\u7b97\u6cd5\u7684\u6536\u655b\u6027\u8bc1\u660e\uff0c\u9002\u7528\u4e8e\u975e\u5747\u5300\u53c2\u4e0e\u5206\u5e03\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u4e8e\u52a0\u6743\u805a\u5408\u53d8\u4f53\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u968f\u673a\u53c2\u4e0e\u4e14\u53c2\u4e0e\u6982\u7387\u672a\u77e5\u6216\u504f\u7f6e\u7684\u5b9e\u9645\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u5b8c\u5168\u53c2\u4e0e\u6216\u5df2\u77e5\u5747\u5300\u5206\u5e03\uff0c\u800c\u5b9e\u9645\u4e2d\u8fd9\u4e9b\u5047\u8bbe\u4e0d\u6210\u7acb\u3002", "method": "\u7814\u7a76Agnostic FedAvg\u7b97\u6cd5\u5728\u968f\u673a\u4e14\u53d8\u89c4\u6a21\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u51f8\u4e14\u53ef\u80fd\u975e\u5149\u6ed1\u635f\u5931\u51fd\u6570\u4e0b\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u805a\u5408\u5468\u671fT\u5185\u8fbe\u5230\u6807\u51c6\u6536\u655b\u901f\u7387O(1/\u221aT)\uff0c\u9996\u6b21\u4e3aAgnostic FedAvg\u5728\u975e\u5747\u5300\u968f\u673a\u53c2\u4e0e\u4e0b\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "Agnostic FedAvg\u5728\u672a\u77e5\u53c2\u4e0e\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u52a0\u6743\u805a\u5408\u53d8\u4f53\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09212", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09212", "abs": "https://arxiv.org/abs/2507.09212", "authors": ["Jonas Scholz", "Richard E. Turner"], "title": "Warm Starts Accelerate Generative Modelling", "comment": "10 pages, 6 figures", "summary": "Iterative generative models, like diffusion and flow-matching, create\nhigh-fidelity samples by progressively refining a noise vector into data.\nHowever, this process is notoriously slow, often requiring hundreds of function\nevaluations. We introduce the warm-start model, a simple, deterministic model\nthat dramatically accelerates conditional generation by providing a better\nstarting point. Instead of starting generation from an uninformed N(0, I)\nprior, our warm-start model predicts an informed prior N(mu, sigma), whose\nmoments are conditioned on the input context. This \"warm start\" substantially\nreduces the distance the generative process must traverse, particularly when\nthe conditioning information is strongly informative. On tasks like image\ninpainting, our method achieves results competitive with a 1000-step DDPM\nbaseline using only 11 total function evaluations (1 for the warm start, 10 for\ngeneration). A simple conditional normalization trick makes our method\ncompatible with any standard generative model and sampler without modification,\nallowing it to be combined with other efficient sampling techniques for further\nacceleration. Our implementation is available at\nhttps://github.com/jonas-scholz123/warm-start-model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cwarm-start model\u201d\u7684\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u597d\u7684\u521d\u59cb\u70b9\u52a0\u901f\u6761\u4ef6\u751f\u6210\uff0c\u663e\u8457\u51cf\u5c11\u751f\u6210\u8fc7\u7a0b\u6240\u9700\u7684\u6b65\u9aa4\u3002", "motivation": "\u4f20\u7edf\u8fed\u4ee3\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\uff09\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u9700\u8981\u6570\u767e\u6b21\u51fd\u6570\u8bc4\u4f30\uff0c\u901f\u5ea6\u8f83\u6162\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u5316\u7684\u9ad8\u65af\u5206\u5e03N(mu, sigma)\u4f5c\u4e3a\u521d\u59cb\u70b9\uff0c\u800c\u975e\u4f20\u7edf\u7684N(0, I)\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u5f52\u4e00\u5316\u6280\u5de7\u4e0e\u6807\u51c6\u751f\u6210\u6a21\u578b\u517c\u5bb9\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\uff0c\u4ec5\u970011\u6b21\u51fd\u6570\u8bc4\u4f30\uff081\u6b21\u9884\u70ed\uff0c10\u6b21\u751f\u6210\uff09\u5373\u53ef\u8fbe\u5230\u4e0e1000\u6b65DDPM\u57fa\u7ebf\u76f8\u5f53\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u9ad8\u6548\uff0c\u53ef\u4e0e\u73b0\u6709\u751f\u6210\u6a21\u578b\u548c\u91c7\u6837\u6280\u672f\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\u3002"}}
{"id": "2507.09213", "categories": ["cs.LG", "stat.ML", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09213", "abs": "https://arxiv.org/abs/2507.09213", "authors": ["Dunsheng Huang", "Dong Shen", "Lei Lu", "Ying Tan"], "title": "Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications", "comment": "17pages", "summary": "Wavelet neural network (WNN), which learns an unknown nonlinear mapping from\nthe data, has been widely used in signal processing, and time-series analysis.\nHowever, challenges in constructing accurate wavelet bases and high\ncomputational costs limit their application. This study introduces a\nconstructive WNN that selects initial bases and trains functions by introducing\nnew bases for predefined accuracy while reducing computational costs. For the\nfirst time, we analyze the frequency of unknown nonlinear functions and select\nappropriate initial wavelets based on their primary frequency components by\nestimating the energy of the spatial frequency component. This leads to a novel\nconstructive framework consisting of a frequency estimator and a wavelet-basis\nincrease mechanism to prioritize high-energy bases, significantly improving\ncomputational efficiency. The theoretical foundation defines the necessary\ntime-frequency range for high-dimensional wavelets at a given accuracy. The\nframework's versatility is demonstrated through four examples: estimating\nunknown static mappings from offline data, combining two offline datasets,\nidentifying time-varying mappings from time-series data, and capturing\nnonlinear dependencies in real time-series data. These examples showcase the\nframework's broad applicability and practicality. All the code will be released\nat https://github.com/dshuangdd/CWNN.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u9020\u6027\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\uff08CWNN\uff09\uff0c\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u548c\u57fa\u51fd\u6570\u589e\u91cf\u673a\u5236\u4f18\u5316\u5c0f\u6ce2\u57fa\u7684\u9009\u62e9\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\u5728\u6784\u5efa\u7cbe\u786e\u5c0f\u6ce2\u57fa\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u5f15\u5165\u9891\u7387\u4f30\u8ba1\u5668\u548c\u57fa\u51fd\u6570\u589e\u91cf\u673a\u5236\uff0c\u4f18\u5148\u9009\u62e9\u9ad8\u80fd\u91cf\u57fa\u51fd\u6570\uff0c\u5e76\u5b9a\u4e49\u65f6\u95f4-\u9891\u7387\u8303\u56f4\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u793a\u4f8b\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5305\u62ec\u9759\u6001\u6620\u5c04\u4f30\u8ba1\u3001\u79bb\u7ebf\u6570\u636e\u7ec4\u5408\u3001\u65f6\u53d8\u6620\u5c04\u8bc6\u522b\u548c\u5b9e\u65f6\u975e\u7ebf\u6027\u4f9d\u8d56\u6355\u6349\u3002", "conclusion": "CWNN\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.08966", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.08966", "abs": "https://arxiv.org/abs/2507.08966", "authors": ["Meng Liu", "Karl Leswing", "Simon K. S. Chu", "Farhad Ramezanghorbani", "Griffin Young", "Gabriel Marques", "Prerna Das", "Anjali Panikar", "Esther Jamir", "Mohammed Sulaiman Shamsudeen", "K. Shawn Watts", "Ananya Sen", "Hari Priya Devannagari", "Edward B. Miller", "Muyun Lihan", "Howook Hwang", "Janet Paulsen", "Xin Yu", "Kyle Gion", "Timur Rvachov", "Emine Kucukbenli", "Saee Gopal Paliwal"], "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha", "comment": "Workshop on Generative AI for Biology at ICML 2025", "summary": "Protein-ligand binding affinity prediction is essential for drug discovery\nand toxicity assessment. While machine learning (ML) promises fast and accurate\npredictions, its progress is constrained by the availability of reliable data.\nIn contrast, physics-based methods such as absolute binding free energy\nperturbation (AB-FEP) deliver high accuracy but are computationally prohibitive\nfor high-throughput applications. To bridge this gap, we introduce ToxBench,\nthe first large-scale AB-FEP dataset designed for ML development and focused on\na single pharmaceutically critical target, Human Estrogen Receptor Alpha\n(ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with\nbinding free energies computed via AB-FEP with a subset validated against\nexperimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping\nligand splits to assess model generalizability. Using ToxBench, we further\nbenchmark state-of-the-art ML methods, and notably, our proposed DualBind\nmodel, which employs a dual-loss framework to effectively learn the binding\nenergy function. The benchmark results demonstrate the superior performance of\nDualBind and the potential of ML to approximate AB-FEP at a fraction of the\ncomputational cost.", "AI": {"tldr": "ToxBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21AB-FEP\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u5f00\u53d1\uff0c\u4e13\u6ce8\u4e8e\u4eba\u7c7b\u96cc\u6fc0\u7d20\u53d7\u4f53\u03b1\uff08ER\u03b1\uff09\uff0c\u5305\u542b8,770\u4e2a\u590d\u5408\u7269\u7ed3\u6784\u53ca\u5176\u7ed3\u5408\u81ea\u7531\u80fd\u3002\u63d0\u51fa\u7684DualBind\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5728\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865\u7269\u7406\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u7f3a\u9677\u3002", "method": "\u5f15\u5165ToxBench\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1DualBind\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u635f\u5931\u6846\u67b6\u5b66\u4e60\u7ed3\u5408\u80fd\u51fd\u6570\u3002", "result": "DualBind\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u4ee5\u4f4e\u6210\u672c\u8fd1\u4f3cAB-FEP\u7684\u6f5c\u529b\u3002", "conclusion": "ToxBench\u548cDualBind\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09252", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09252", "abs": "https://arxiv.org/abs/2507.09252", "authors": ["Shukai Gong", "Yiyang Fu", "Fengyuan Ran", "Feng Zhou"], "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding", "comment": null, "summary": "We propose TPP-SD, a novel approach that accelerates Transformer temporal\npoint process (TPP) sampling by adapting speculative decoding (SD) techniques\nfrom language models. By identifying the structural similarities between\nthinning algorithms for TPPs and speculative decoding for language models, we\ndevelop an efficient sampling framework that leverages a smaller draft model to\ngenerate multiple candidate events, which are then verified by the larger\ntarget model in parallel. TPP-SD maintains the same output distribution as\nautoregressive sampling while achieving significant acceleration. Experiments\non both synthetic and real datasets demonstrate that our approach produces\nsamples from identical distributions as standard methods, but with 2-6$\\times$\nspeedup. Our ablation studies analyze the impact of hyperparameters such as\ndraft length and draft model size on sampling efficiency. TPP-SD bridges the\ngap between powerful Transformer TPP models and the practical need for rapid\nsequence sampling.", "AI": {"tldr": "TPP-SD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u5e94\u7528\u4e8eTransformer\u65f6\u95f4\u70b9\u8fc7\u7a0b\uff08TPP\uff09\u91c7\u6837\uff0c\u663e\u8457\u52a0\u901f\u4e86\u91c7\u6837\u8fc7\u7a0b\u3002", "motivation": "Transformer TPP\u6a21\u578b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5176\u91c7\u6837\u901f\u5ea6\u8f83\u6162\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002TPP-SD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u63d0\u5347\u91c7\u6837\u6548\u7387\u3002", "method": "TPP-SD\u5229\u7528\u8f83\u5c0f\u7684\u8349\u7a3f\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5019\u9009\u4e8b\u4ef6\uff0c\u7136\u540e\u7531\u8f83\u5927\u7684\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\uff0c\u4ece\u800c\u52a0\u901f\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTPP-SD\u5728\u4fdd\u6301\u4e0e\u81ea\u56de\u5f52\u91c7\u6837\u76f8\u540c\u8f93\u51fa\u5206\u5e03\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e862-6\u500d\u7684\u52a0\u901f\u3002", "conclusion": "TPP-SD\u6210\u529f\u5730\u5c06Transformer TPP\u6a21\u578b\u7684\u9ad8\u6548\u91c7\u6837\u9700\u6c42\u4e0e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u901f\u5ea6\u8981\u6c42\u7ed3\u5408\u8d77\u6765\u3002"}}
{"id": "2507.08972", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2507.08972", "abs": "https://arxiv.org/abs/2507.08972", "authors": ["Sifan Wang", "Shyam Sankaran", "Panos Stinis", "Paris Perdikaris"], "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks", "comment": "25 pages, 13 figures, 3 tables", "summary": "Turbulent fluid flows are among the most computationally demanding problems\nin science, requiring enormous computational resources that become prohibitive\nat high flow speeds. Physics-informed neural networks (PINNs) represent a\nradically different approach that trains neural networks directly from physical\nequations rather than data, offering the potential for continuous, mesh-free\nsolutions. Here we show that appropriately designed PINNs can successfully\nsimulate fully turbulent flows in both two and three dimensions, directly\nlearning solutions to the fundamental fluid equations without traditional\ncomputational grids or training data. Our approach combines several algorithmic\ninnovations including adaptive network architectures, causal training, and\nadvanced optimization methods to overcome the inherent challenges of learning\nchaotic dynamics. Through rigorous validation on challenging turbulence\nproblems, we demonstrate that PINNs accurately reproduce key flow statistics\nincluding energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our\nresults demonstrate that neural equation solvers can handle complex chaotic\nsystems, opening new possibilities for continuous turbulence modeling that\ntranscends traditional computational limitations.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u6a21\u62df\u6e4d\u6d41\u4e2d\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u4f20\u7edf\u8ba1\u7b97\u7f51\u683c\u6216\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u76f4\u63a5\u6c42\u89e3\u6d41\u4f53\u65b9\u7a0b\u3002", "motivation": "\u6e4d\u6d41\u6a21\u62df\u5bf9\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u6781\u9ad8\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u6d41\u901f\u4e0b\u96be\u4ee5\u5b9e\u73b0\u3002PINNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u65b9\u7a0b\u7684\u8fde\u7eed\u3001\u65e0\u7f51\u683c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u81ea\u9002\u5e94\u7f51\u7edc\u67b6\u6784\u3001\u56e0\u679c\u8bad\u7ec3\u548c\u5148\u8fdb\u4f18\u5316\u65b9\u6cd5\uff0c\u514b\u670d\u5b66\u4e60\u6df7\u6c8c\u52a8\u529b\u5b66\u7684\u6311\u6218\u3002", "result": "PINNs\u80fd\u51c6\u786e\u518d\u73b0\u6e4d\u6d41\u7684\u5173\u952e\u7edf\u8ba1\u7279\u6027\uff0c\u5982\u80fd\u8c31\u3001\u52a8\u80fd\u3001\u6da1\u5ea6\u548c\u96f7\u8bfa\u5e94\u529b\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u65b9\u7a0b\u6c42\u89e3\u5668\u53ef\u5904\u7406\u590d\u6742\u6df7\u6c8c\u7cfb\u7edf\uff0c\u4e3a\u8d85\u8d8a\u4f20\u7edf\u8ba1\u7b97\u9650\u5236\u7684\u6e4d\u6d41\u5efa\u6a21\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.09353", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09353", "abs": "https://arxiv.org/abs/2507.09353", "authors": ["Addison Weatherhead", "Anna Goldenberg"], "title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation", "comment": null, "summary": "Time series data with missing values is common across many domains.\nHealthcare presents special challenges due to prolonged periods of sensor\ndisconnection. In such cases, having a confidence measure for imputed values is\ncritical. Most existing methods either overlook model uncertainty or lack\nmechanisms to estimate it. To address this gap, we introduce a general\nframework that quantifies and leverages uncertainty for selective imputation.\nBy focusing on values the model is most confident in, highly unreliable\nimputations are avoided. Our experiments on multiple EHR datasets, covering\ndiverse types of missingness, demonstrate that selectively imputing\nless-uncertain values not only reduces imputation errors but also improves\ndownstream tasks. Specifically, we show performance gains in a 24-hour\nmortality prediction task, underscoring the practical benefit of incorporating\nuncertainty into time series imputation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u5e76\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u7684\u9009\u62e9\u6027\u63d2\u8865\u6846\u67b6\uff0c\u907f\u514d\u4e0d\u53ef\u9760\u63d2\u8865\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u9886\u57df\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e38\u56e0\u4f20\u611f\u5668\u65ad\u5f00\u800c\u7f3a\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u6216\u65e0\u6cd5\u4f30\u8ba1\u3002", "method": "\u5f15\u5165\u901a\u7528\u6846\u67b6\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u5e76\u9009\u62e9\u6027\u63d2\u8865\u9ad8\u7f6e\u4fe1\u503c\u3002", "result": "\u5728\u591a\u79cdEHR\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u9009\u62e9\u6027\u63d2\u8865\u51cf\u5c11\u8bef\u5dee\u5e76\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\uff08\u598224\u5c0f\u65f6\u6b7b\u4ea1\u7387\u9884\u6d4b\uff09\u3002", "conclusion": "\u5c06\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u65f6\u95f4\u5e8f\u5217\u63d2\u8865\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2507.09445", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09445", "abs": "https://arxiv.org/abs/2507.09445", "authors": ["Runze Yang", "Longbing Cao", "Xin You", "Kun Fang", "Jianxun Li", "Jie Yang"], "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting", "comment": "18 pages, 6 figures", "summary": "The integration of Fourier transform and deep learning opens new avenues for\ntime series forecasting. We reconsider the Fourier transform from a basis\nfunctions perspective. Specifically, the real and imaginary parts of the\nfrequency components can be regarded as the coefficients of cosine and sine\nbasis functions at tiered frequency levels, respectively. We find that existing\nFourier-based methods face inconsistent starting cycles and inconsistent series\nlength issues. They fail to interpret frequency components precisely and\noverlook temporal information. Accordingly, the novel Fourier Basis Mapping\n(FBM) method addresses these issues by integrating time-frequency features\nthrough Fourier basis expansion and mapping in the time-frequency space. Our\napproach extracts explicit frequency features while preserving temporal\ncharacteristics. FBM supports plug-and-play integration with various types of\nneural networks by only adjusting the first initial projection layer for better\nperformance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,\nMLP-based, and Transformer-based models, respectively, demonstrating the\neffectiveness of time-frequency features. Next, we propose a synergetic model\narchitecture, termed FBM-S, which decomposes the seasonal, trend, and\ninteraction effects into three separate blocks, each designed to model\ntime-frequency features in a specialized manner. Finally, we introduce several\ntechniques tailored for time-frequency features, including interaction masking,\ncentralization, patching, rolling window projection, and multi-scale\ndown-sampling. The results are validated on diverse real-world datasets for\nboth long-term and short-term forecasting tasks with SOTA performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5085\u91cc\u53f6\u57fa\u6620\u5c04\uff08FBM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u95f4-\u9891\u7387\u7279\u5f81\u89e3\u51b3\u73b0\u6709\u5085\u91cc\u53f6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u65b9\u6cd5\u5b58\u5728\u8d77\u59cb\u5468\u671f\u4e0d\u4e00\u81f4\u3001\u5e8f\u5217\u957f\u5ea6\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u7cbe\u786e\u89e3\u91ca\u9891\u7387\u5206\u91cf\u6216\u5ffd\u7565\u65f6\u95f4\u4fe1\u606f\u3002", "method": "\u63d0\u51faFBM\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u57fa\u5c55\u5f00\u548c\u6620\u5c04\u6574\u5408\u65f6\u95f4-\u9891\u7387\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1\u4e86FBM-L\u3001FBM-NL\u3001FBM-NP\u548cFBM-S\u7b49\u6a21\u578b\u67b6\u6784\u53ca\u591a\u79cd\u6280\u672f\uff08\u5982\u4ea4\u4e92\u63a9\u7801\u3001\u4e2d\u5fc3\u5316\u7b49\uff09\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86FBM\u5728\u957f\u77ed\u671f\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684SOTA\u6027\u80fd\u3002", "conclusion": "FBM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5085\u91cc\u53f6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.08980", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08980", "abs": "https://arxiv.org/abs/2507.08980", "authors": ["Chenyu Wang", "Cai Zhou", "Sharut Gupta", "Zongyu Lin", "Stefanie Jegelka", "Stephen Bates", "Tommi Jaakkola"], "title": "Learning Diffusion Models with Flexible Representation Guidance", "comment": null, "summary": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u5f15\u5bfc\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u5305\u62ec\u4e24\u79cd\u65b0\u7b56\u7565\uff1a\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u548c\u4f18\u5316\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u5347\u548c\u8bad\u7ec3\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u9f50\u5185\u90e8\u8868\u793a\u6765\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u7b56\u7565\uff1a1) \u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\uff1b2) \u4f18\u5316\u8bad\u7ec3\u8bfe\u7a0b\u3002", "result": "\u5728\u56fe\u50cf\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff08\u5982ImageNet 256\u00d7256\u4efb\u52a1\u4e2d\u8bad\u7ec3\u901f\u5ea6\u63d0\u534723.3\u500d\uff09\u3002", "conclusion": "\u8868\u793a\u5f15\u5bfc\u80fd\u6709\u6548\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2507.09694", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09694", "abs": "https://arxiv.org/abs/2507.09694", "authors": ["Nicolas Gonel", "Paul Saves", "Joseph Morlier"], "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting", "comment": "AeroBest 2025, Instituto Superior Tecnico of the University of\n  Lisbon, Portugal", "summary": "This paper introduces a comprehensive open-source framework for developing\ncorrelation kernels, with a particular focus on user-defined and composition of\nkernels for surrogate modeling. By advancing kernel-based modeling techniques,\nwe incorporate frequency-aware elements that effectively capture complex\nmechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.\nTraditional kernel functions, often limited to exponential-based methods, are\nextended to include a wider range of kernels such as exponential squared sine\nand rational quadratic kernels, along with their respective firstand\nsecond-order derivatives. The proposed methodologies are first validated on a\nsinus cardinal test case and then applied to forecasting Mauna-Loa Carbon\nDioxide (CO 2 ) concentrations and airline passenger traffic. All these\nadvancements are integrated into the open-source Surrogate Modeling Toolbox\n(SMT 2.0), providing a versatile platform for both standard and customizable\nkernel configurations. Furthermore, the framework enables the combination of\nvarious kernels to leverage their unique strengths into composite models\ntailored to specific problems. The resulting framework offers a flexible\ntoolset for engineers and researchers, paving the way for numerous future\napplications in metamodeling for complex, frequency-sensitive domains.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u76f8\u5173\u6027\u6838\u51fd\u6570\uff0c\u7279\u522b\u5173\u6ce8\u7528\u6237\u5b9a\u4e49\u548c\u6838\u7ec4\u5408\uff0c\u4ee5\u652f\u6301\u4ee3\u7406\u5efa\u6a21\u3002\u901a\u8fc7\u5f15\u5165\u9891\u7387\u611f\u77e5\u5143\u7d20\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u673a\u68b0\u884c\u4e3a\u548c\u65f6\u9891\u52a8\u6001\u7279\u6027\u3002", "motivation": "\u4f20\u7edf\u6838\u51fd\u6570\uff08\u5982\u6307\u6570\u57fa\u65b9\u6cd5\uff09\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u5f00\u53d1\u66f4\u5e7f\u6cdb\u7684\u6838\u51fd\u6570\u7c7b\u578b\uff08\u5982\u6307\u6570\u5e73\u65b9\u6b63\u5f26\u548c\u6709\u7406\u4e8c\u6b21\u6838\uff09\u53ca\u5176\u5bfc\u6570\uff0c\u4ee5\u63d0\u5347\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u65b0\u7684\u6838\u51fd\u6570\u65b9\u6cd5\uff0c\u5305\u62ec\u5176\u5bfc\u6570\uff0c\u5e94\u7528\u4e8eSinc\u6d4b\u8bd5\u6848\u4f8b\u3001CO2\u6d53\u5ea6\u548c\u822a\u7a7a\u5ba2\u8fd0\u6d41\u91cf\u9884\u6d4b\uff0c\u5e76\u96c6\u6210\u5230\u5f00\u6e90\u5de5\u5177SMT 2.0\u4e2d\u3002", "result": "\u6846\u67b6\u6210\u529f\u6269\u5c55\u4e86\u6838\u51fd\u6570\u8303\u56f4\uff0c\u652f\u6301\u7075\u6d3b\u7ec4\u5408\uff0c\u4e3a\u590d\u6742\u9891\u7387\u654f\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5efa\u6a21\u5de5\u5177\u3002", "conclusion": "\u8be5\u5f00\u6e90\u6846\u67b6\u4e3a\u5de5\u7a0b\u5e08\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5de5\u5177\u96c6\uff0c\u672a\u6765\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u4ee3\u7406\u5efa\u6a21\u3002"}}
{"id": "2507.08983", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08983", "abs": "https://arxiv.org/abs/2507.08983", "authors": ["Anshuman Suri", "Harsh Chaudhari", "Yuefeng Peng", "Ali Naseh", "Amir Houmansadr", "Alina Oprea"], "title": "Exploiting Leaderboards for Large-Scale Distribution of Malicious Models", "comment": null, "summary": "While poisoning attacks on machine learning models have been extensively\nstudied, the mechanisms by which adversaries can distribute poisoned models at\nscale remain largely unexplored. In this paper, we shed light on how model\nleaderboards -- ranked platforms for model discovery and evaluation -- can\nserve as a powerful channel for adversaries for stealthy large-scale\ndistribution of poisoned models. We present TrojanClimb, a general framework\nthat enables injection of malicious behaviors while maintaining competitive\nleaderboard performance. We demonstrate its effectiveness across four diverse\nmodalities: text-embedding, text-generation, text-to-speech and text-to-image,\nshowing that adversaries can successfully achieve high leaderboard rankings\nwhile embedding arbitrary harmful functionalities, from backdoors to bias\ninjection. Our findings reveal a significant vulnerability in the machine\nlearning ecosystem, highlighting the urgent need to redesign leaderboard\nevaluation mechanisms to detect and filter malicious (e.g., poisoned) models,\nwhile exposing broader security implications for the machine learning community\nregarding the risks of adopting models from unverified sources.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u6a21\u578b\u6392\u884c\u699c\u53ef\u80fd\u6210\u4e3a\u5927\u89c4\u6a21\u5206\u53d1\u4e2d\u6bd2\u6a21\u578b\u7684\u9690\u853d\u6e20\u9053\uff0c\u63d0\u51fa\u4e86TrojanClimb\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u79cd\u6a21\u6001\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u8bbe\u8ba1\u6392\u884c\u699c\u8bc4\u4f30\u673a\u5236\u4ee5\u5e94\u5bf9\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u4e2d\u6bd2\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u6a21\u578b\u6392\u884c\u699c\u5927\u89c4\u6a21\u5206\u53d1\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e2d\u8fd9\u4e00\u673a\u5236\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86TrojanClimb\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4fdd\u6301\u6392\u884c\u699c\u7ade\u4e89\u529b\u7684\u540c\u65f6\u6ce8\u5165\u6076\u610f\u884c\u4e3a\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u6587\u672c\u5d4c\u5165\u3001\u6587\u672c\u751f\u6210\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u6587\u672c\u8f6c\u56fe\u50cf\u56db\u79cd\u6a21\u6001\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u653b\u51fb\u8005\u53ef\u4ee5\u6210\u529f\u5728\u6392\u884c\u699c\u4e2d\u53d6\u5f97\u9ad8\u6392\u540d\uff0c\u540c\u65f6\u5d4c\u5165\u4efb\u610f\u6709\u5bb3\u529f\u80fd\uff08\u5982\u540e\u95e8\u6216\u504f\u89c1\u6ce8\u5165\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u673a\u5668\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u7684\u91cd\u5927\u6f0f\u6d1e\uff0c\u547c\u5401\u91cd\u65b0\u8bbe\u8ba1\u6392\u884c\u699c\u8bc4\u4f30\u673a\u5236\u4ee5\u68c0\u6d4b\u548c\u8fc7\u6ee4\u6076\u610f\u6a21\u578b\uff0c\u5e76\u8b66\u793a\u793e\u533a\u5173\u4e8e\u4f7f\u7528\u672a\u7ecf\u9a8c\u8bc1\u6a21\u578b\u7684\u98ce\u9669\u3002"}}
{"id": "2507.09846", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09846", "abs": "https://arxiv.org/abs/2507.09846", "authors": ["Minhak Song", "Beomhan Baek", "Kwangjun Ahn", "Chulhee Yun"], "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training", "comment": "Comments would be appreciated!", "summary": "As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Schedule-Free\uff08SF\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u65e0\u9700\u663e\u5f0f\u8870\u51cf\u9636\u6bb5\u6216\u989d\u5916\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u89c4\u6a21\u8fc5\u901f\u589e\u957f\uff0c\u4f20\u7edf\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u7684\u4f59\u5f26\u5b66\u4e60\u7387\u8c03\u5ea6\uff09\u5df2\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u91cd\u65b0\u5ba1\u89c6\u4e86Schedule-Free\uff08SF\uff09\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u63ed\u793a\u5176\u52a8\u6001\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684SF\u53d8\u4f53\uff0c\u589e\u5f3a\u4e86\u5bf9\u52a8\u91cf\u548c\u5927\u6279\u91cf\u8bad\u7ec3\u7684\u9c81\u68d2\u6027\u3002", "result": "\u6539\u8fdb\u7684SF\u65b9\u6cd5\u5728\u65e0\u9700\u663e\u5f0f\u8870\u51cf\u6216\u989d\u5916\u5185\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u4f18\u5316\u4e86\u635f\u5931\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u6301\u7eed\u6269\u5c55\u7684\u8bad\u7ec3\u4efb\u52a1\u3002", "conclusion": "SF\u65b9\u6cd5\u662f\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u7406\u8bba\u652f\u6301\u7684\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.09009", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09009", "abs": "https://arxiv.org/abs/2507.09009", "authors": ["Zhengxiao He", "Huayu Li", "Geng Yuan", "William D. S. Killgore", "Stuart F. Quan", "Chen X. Chen", "Ao Li"], "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography", "comment": null, "summary": "Methods: We developed a self-supervised deep learning model that extracts\nmeaningful patterns from multi-modal signals (Electroencephalography (EEG),\nElectrocardiography (ECG), and respiratory signals). The model was trained on\ndata from 4,398 participants. Projection scores were derived by contrasting\nembeddings from individuals with and without CVD outcomes. External validation\nwas conducted in an independent cohort with 1,093 participants. The source code\nis available on https://github.com/miraclehetech/sleep-ssl. Results: The\nprojection scores revealed distinct and clinically meaningful patterns across\nmodalities. ECG-derived features were predictive of both prevalent and incident\ncardiac conditions, particularly CVD mortality. EEG-derived features were\npredictive of incident hypertension and CVD mortality. Respiratory signals\nadded complementary predictive value. Combining these projection scores with\nthe Framingham Risk Score consistently improved predictive performance,\nachieving area under the curve values ranging from 0.607 to 0.965 across\ndifferent outcomes. Findings were robustly replicated and validated in the\nexternal testing cohort. Conclusion: Our findings demonstrate that the proposed\nframework can generate individualized CVD risk scores directly from PSG data.\nThe resulting projection scores have the potential to be integrated into\nclinical practice, enhancing risk assessment and supporting personalized care.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u591a\u6a21\u6001\u4fe1\u53f7\uff08EEG\u3001ECG\u548c\u547c\u5438\u4fe1\u53f7\uff09\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u6a21\u5f0f\uff0c\u7528\u4e8e\u9884\u6d4b\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u98ce\u9669\u3002", "motivation": "\u5229\u7528\u591a\u6a21\u6001\u4fe1\u53f7\uff08EEG\u3001ECG\u548c\u547c\u5438\u4fe1\u53f7\uff09\u63d0\u53d6\u7279\u5f81\uff0c\u63d0\u5347CVD\u98ce\u9669\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u3002", "method": "\u8bad\u7ec3\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e4,398\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6709\u65e0CVD\u7ed3\u679c\u7684\u4e2a\u4f53\u5d4c\u5165\u751f\u6210\u6295\u5f71\u5206\u6570\uff0c\u5e76\u57281,093\u540d\u72ec\u7acb\u53c2\u4e0e\u8005\u4e2d\u9a8c\u8bc1\u3002", "result": "\u6295\u5f71\u5206\u6570\u63ed\u793a\u4e86\u5404\u6a21\u6001\u7684\u4e34\u5e8a\u610f\u4e49\u6a21\u5f0f\uff0c\u7ed3\u5408Framingham\u98ce\u9669\u8bc4\u5206\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff08AUC 0.607-0.965\uff09\uff0c\u5916\u90e8\u9a8c\u8bc1\u7ed3\u679c\u7a33\u5065\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u76f4\u63a5\u4ecePSG\u6570\u636e\u751f\u6210\u4e2a\u4f53\u5316CVD\u98ce\u9669\u8bc4\u5206\uff0c\u6709\u671b\u6574\u5408\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u63d0\u5347\u98ce\u9669\u8bc4\u4f30\u548c\u4e2a\u6027\u5316\u62a4\u7406\u3002"}}
{"id": "2507.09888", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09888", "abs": "https://arxiv.org/abs/2507.09888", "authors": ["Huibo Xu", "Likang Wu", "Xianquan Wang", "Haoning Dang", "Chun-Wun Cheng", "Angelica I Aviles-Rivero", "Qi Liu"], "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a fundamental task with broad applications, yet\nconventional methods often treat data as discrete sequences, overlooking their\norigin as noisy samples of continuous processes. Crucially, discrete noisy\nobservations cannot uniquely determine a continuous function; instead, they\ncorrespond to a family of plausible functions. Mathematically, time series can\nbe viewed as noisy observations of a continuous function family governed by a\nshared probability measure. Thus, the forecasting task can be framed as\nlearning the transition from the historical function family to the future\nfunction family. This reframing introduces two key challenges: (1) How can we\nleverage discrete historical and future observations to learn the relationships\nbetween their underlying continuous functions? (2) How can we model the\ntransition path in function space from the historical function family to the\nfuture function family? To address these challenges, we propose NeuTSFlow, a\nnovel framework that leverages Neural Operators to facilitate flow matching for\nlearning path of measure between historical and future function families. By\nparameterizing the velocity field of the flow in infinite-dimensional function\nspaces, NeuTSFlow moves beyond traditional methods that focus on dependencies\nat discrete points, directly modeling function-level features instead.\nExperiments on diverse forecasting tasks demonstrate NeuTSFlow's superior\naccuracy and robustness, validating the effectiveness of the function-family\nperspective.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNeuTSFlow\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7b97\u5b50\u5b66\u4e60\u8fde\u7eed\u51fd\u6570\u65cf\u4e4b\u95f4\u7684\u8f6c\u6362\u8def\u5f84\uff0c\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u79bb\u6563\u89c2\u6d4b\u4e0e\u8fde\u7eed\u8fc7\u7a0b\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u79bb\u6563\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5176\u4f5c\u4e3a\u8fde\u7eed\u8fc7\u7a0b\u566a\u58f0\u91c7\u6837\u7684\u672c\u8d28\u3002\u8bba\u6587\u63d0\u51fa\u4ece\u51fd\u6570\u65cf\u7684\u89d2\u5ea6\u91cd\u65b0\u5b9a\u4e49\u9884\u6d4b\u4efb\u52a1\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u8fde\u7eed\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faNeuTSFlow\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u7b97\u5b50\u5b9e\u73b0\u6d41\u5339\u914d\uff0c\u5b66\u4e60\u5386\u53f2\u51fd\u6570\u65cf\u5230\u672a\u6765\u51fd\u6570\u65cf\u7684\u8f6c\u6362\u8def\u5f84\uff0c\u76f4\u63a5\u5728\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e2d\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNeuTSFlow\u5728\u591a\u79cd\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u51fd\u6570\u65cf\u89c6\u89d2\u6709\u6548\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0cNeuTSFlow\u6846\u67b6\u4e3a\u8fde\u7eed\u8fc7\u7a0b\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09016", "abs": "https://arxiv.org/abs/2507.09016", "authors": ["Karim Galliamov", "Ivan Titov", "Ilya Pershin"], "title": "Enhancing RLHF with Human Gaze Modeling", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models with\nhuman preferences but is computationally expensive. We explore two approaches\nthat leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models\nand (2) gaze-based distribution of sparse rewards at token level. Our\nexperiments demonstate that gaze-informed RLHF achieves faster convergence\nwhile maintaining or slightly improving performance, thus, reducing\ncomputational costs during policy optimization. These results show that human\ngaze provides a valuable and underused signal for policy optimization, pointing\nto a promising direction for improving RLHF efficiency.", "AI": {"tldr": "\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u5efa\u6a21\u6539\u8fdbRLHF\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "RLHF\u867d\u80fd\u5bf9\u9f50\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a\u6ce8\u89c6\u611f\u77e5\u5956\u52b1\u6a21\u578b\u548c\u57fa\u4e8e\u6ce8\u89c6\u7684\u7a00\u758f\u5956\u52b1\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6ce8\u89c6\u4fe1\u606f\u80fd\u52a0\u901f\u6536\u655b\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u4eba\u7c7b\u6ce8\u89c6\u662f\u4f18\u5316RLHF\u6548\u7387\u7684\u6709\u4ef7\u503c\u4fe1\u53f7\u3002"}}
{"id": "2507.10088", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10088", "abs": "https://arxiv.org/abs/2507.10088", "authors": ["Tung Sum Thomas Kwok", "Zeyong Zhang", "Chi-Hua Wang", "Guang Cheng"], "title": "Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025", "summary": "Tabular data synthesis for supervised learning ('SL') model training is\ngaining popularity in industries such as healthcare, finance, and retail.\nDespite the progress made in tabular data generators, models trained with\nsynthetic data often underperform compared to those trained with original data.\nThis low SL utility of synthetic data stems from class imbalance exaggeration\nand SL data relationship overlooked by tabular generator. To address these\nchallenges, we draw inspirations from techniques in emerging data-centric\nartificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel\npipeline that integrates data-centric techniques into tabular data synthesis.\nPRRO incorporates data pruning to guide the table generator towards\nobservations with high signal-to-noise ratio, ensuring that the class\ndistribution of synthetic data closely matches that of the original data.\nBesides, PRRO employs a column reordering algorithm to align the data modeling\nstructure of generators with that of SL models. These two modules enable PRRO\nto optimize SL utility of synthetic data. Empirical experiments on 22 public\ndatasets show that synthetic data generated using PRRO enhances predictive\nperformance compared to data generated without PRRO. Specifically, synthetic\nreplacement of original data yields an average improvement of 26.74% and up to\n871.46% improvement using PRRO, while synthetic appendant to original data\nresults with PRRO-generated data results in an average improvement of 6.13% and\nup to 200.32%. Furthermore, experiments on six highly imbalanced datasets show\nthat PRRO enables the generator to produce synthetic data with a class\ndistribution that resembles the original data more closely, achieving a\nsimilarity improvement of 43%. Through PRRO, we foster a seamless integration\nof data synthesis to subsequent SL prediction, promoting quality and accessible\ndata analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u4fee\u526a\u548c\u5217\u91cd\u6392\u5e8f\u63d0\u5347\u5408\u6210\u8868\u683c\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u5408\u6210\u8868\u683c\u6570\u636e\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u6027\u80fd\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6570\u636e\u5173\u7cfb\u672a\u88ab\u5145\u5206\u8003\u8651\u3002", "method": "\u63d0\u51faPRRO\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u4fee\u526a\u548c\u5217\u91cd\u6392\u5e8f\uff0c\u4f18\u5316\u5408\u6210\u6570\u636e\u7684\u4fe1\u53f7\u566a\u58f0\u6bd4\u548c\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "PRRO\u572822\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u534726.74%\u6027\u80fd\uff0c\u6700\u9ad8\u8fbe871.46%\uff0c\u7c7b\u522b\u5206\u5e03\u76f8\u4f3c\u6027\u63d0\u534743%\u3002", "conclusion": "PRRO\u6709\u6548\u63d0\u5347\u5408\u6210\u6570\u636e\u8d28\u91cf\uff0c\u4fc3\u8fdb\u6570\u636e\u5408\u6210\u4e0e\u76d1\u7763\u5b66\u4e60\u7684\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2507.09019", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09019", "abs": "https://arxiv.org/abs/2507.09019", "authors": ["Amey Agrawal", "Nitin Kedia", "Anmol Agarwal", "Jayashree Mohan", "Nipun Kwatra", "Souvik Kundu", "Ramachandran Ramjee", "Alexey Tumanov"], "title": "On Evaluating Performance of LLM Inference Serving Systems", "comment": null, "summary": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff0c\u63d0\u51fa\u4e86\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u5e38\u89c1\u53cd\u6a21\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u68c0\u67e5\u6e05\u5355\u548c\u6846\u67b6\u4ee5\u6539\u8fdb\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff0c\u63a9\u76d6\u4e86\u771f\u5b9e\u6027\u80fd\uff0c\u963b\u788d\u79d1\u5b66\u8fdb\u6b65\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u8fd1\u671f\u7cfb\u7edf\uff0c\u8bc6\u522b\u51fa\u57fa\u7ebf\u516c\u5e73\u6027\u3001\u8bc4\u4f30\u8bbe\u7f6e\u548c\u6307\u6807\u8bbe\u8ba1\u4e09\u4e2a\u7ef4\u5ea6\u7684\u53cd\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u6846\u67b6\u3002", "result": "\u63ed\u793a\u4e86\u5e38\u89c1\u53cd\u6a21\u5f0f\u5982\u4f55\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e25\u8c28\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u4fc3\u8fdbLLM\u63a8\u7406\u7cfb\u7edf\u7684\u771f\u5b9e\u8fdb\u6b65\u3002"}}
{"id": "2507.10132", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10132", "abs": "https://arxiv.org/abs/2507.10132", "authors": ["Usman Gani Joy"], "title": "Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting", "comment": null, "summary": "Accurate forecasting of energy demand and supply is critical for optimizing\nsustainable energy systems, yet it is challenged by the variability of\nrenewable sources and dynamic consumption patterns. This paper introduces a\nneural framework that integrates continuous-time Neural Ordinary Differential\nEquations (Neural ODEs), graph attention, multi-resolution wavelet\ntransformations, and adaptive learning of frequencies to address the issues of\ntime series prediction. The model employs a robust ODE solver, using the\nRunge-Kutta method, paired with graph-based attention and residual connections\nto better understand both structural and temporal patterns. Through\nwavelet-based feature extraction and adaptive frequency modulation, it adeptly\ncaptures and models diverse, multi-scale temporal dynamics. When evaluated\nacross seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity\ntransformer temperature), and Waste, Solar, and Hydro (renewable energy), this\narchitecture consistently outperforms state-of-the-art baselines in various\nforecasting metrics, proving its robustness in capturing complex temporal\ndependencies. Furthermore, the model enhances interpretability through SHAP\nanalysis, making it suitable for sustainable energy applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecfODE\u3001\u56fe\u6ce8\u610f\u529b\u3001\u5c0f\u6ce2\u53d8\u6362\u548c\u81ea\u9002\u5e94\u9891\u7387\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u80fd\u6e90\u9700\u6c42\u4e0e\u4f9b\u5e94\u7684\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u80fd\u6e90\u4f9b\u9700\u9884\u6d4b\u5bf9\u53ef\u6301\u7eed\u80fd\u6e90\u7cfb\u7edf\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u53ef\u518d\u751f\u80fd\u6e90\u6ce2\u52a8\u548c\u52a8\u6001\u6d88\u8d39\u6a21\u5f0f\u7684\u5f71\u54cd\uff0c\u9884\u6d4b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6a21\u578b\u6574\u5408\u4e86\u795e\u7ecfODE\u3001\u56fe\u6ce8\u610f\u529b\u3001\u591a\u5206\u8fa8\u7387\u5c0f\u6ce2\u53d8\u6362\u548c\u81ea\u9002\u5e94\u9891\u7387\u5b66\u4e60\uff0c\u91c7\u7528Runge-Kutta\u65b9\u6cd5\u6c42\u89e3ODE\uff0c\u5e76\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u4e0e\u6b8b\u5dee\u8fde\u63a5\u3002", "result": "\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7SHAP\u5206\u6790\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\uff0c\u9002\u7528\u4e8e\u53ef\u6301\u7eed\u80fd\u6e90\u5e94\u7528\u3002"}}
{"id": "2507.09029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09029", "abs": "https://arxiv.org/abs/2507.09029", "authors": ["Vaibhav Singh", "Zafir Khalid", "Edouard Oyallon", "Eugene Belilovsky"], "title": "Model Parallelism With Subnetwork Data Parallelism", "comment": "6 pages, 1 figure", "summary": "Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u5c0f\u578b\u7ed3\u6784\u5316\u5b50\u7f51\u7edc\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u907f\u514d\u8282\u70b9\u95f4\u6fc0\u6d3b\u901a\u4fe1\uff0c\u6027\u80fd\u4e0d\u964d\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u5185\u5b58\u9700\u6c42\u9ad8\u548c\u8282\u70b9\u95f4\u901a\u4fe1\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u8bad\u7ec3\u5c0f\u578b\u7ed3\u6784\u5316\u5b50\u7f51\u7edc\uff0c\u907f\u514d\u8282\u70b9\u95f4\u6fc0\u6d3b\u901a\u4fe1\uff0c\u91c7\u7528\u4e24\u79cd\u5b50\u7f51\u7edc\u6784\u5efa\u7b56\u7565\u3002", "result": "\u968f\u673a\u5757\u4e22\u5f03\u6280\u672f\u8868\u73b0\u4f18\u4e8e\u5bbd\u5ea6\u5b50\u7f51\u7edc\u6784\u5efa\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1120-40%\u4e14\u6027\u80fd\u4e0d\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.10419", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10419", "abs": "https://arxiv.org/abs/2507.10419", "authors": ["Victor Letzelter", "Hugo Malard", "Mathieu Fontaine", "Ga\u00ebl Richard", "Slim Essid", "Andrei Bursuc", "Patrick P\u00e9rez"], "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "comment": null, "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.", "AI": {"tldr": "LoRA-MCL\u662f\u4e00\u79cd\u7ed3\u5408\u591a\u9009\u62e9\u5b66\u4e60\u548c\u4f4e\u79e9\u9002\u914d\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u4e14\u5408\u7406\u7684\u53e5\u5b50\u5ef6\u7eed\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u591a\u89e3\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5904\u7406\u8fd9\u79cd\u6a21\u7cca\u6027\u3002", "method": "\u7ed3\u5408\u591a\u9009\u62e9\u5b66\u4e60\uff08MCL\uff09\u548cWinner-Takes-All\uff08WTA\uff09\u635f\u5931\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u9ad8\u6548\u5904\u7406\u6a21\u7cca\u6027\u3002", "result": "\u5728\u89c6\u89c9\u548c\u97f3\u9891\u5b57\u5e55\u4efb\u52a1\u4e2d\uff0c\u751f\u6210\u7ed3\u679c\u5177\u6709\u9ad8\u591a\u6837\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "LoRA-MCL\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u89e3\u95ee\u9898\uff0c\u751f\u6210\u7ed3\u679c\u591a\u6837\u4e14\u5408\u7406\u3002"}}
{"id": "2507.09031", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09031", "abs": "https://arxiv.org/abs/2507.09031", "authors": ["Yash Shah", "Camila Gonzalez", "Mohammad H. Abbasi", "Qingyu Zhao", "Kilian M. Pohl", "Ehsan Adeli"], "title": "Confounder-Free Continual Learning via Recursive Feature Normalization", "comment": null, "summary": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52MDN\uff08R-MDN\uff09\u5c42\uff0c\u7528\u4e8e\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u6d88\u9664\u6df7\u6742\u53d8\u91cf\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u6a21\u578b\u7684\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6df7\u6742\u53d8\u91cf\u4f1a\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\u548c\u9884\u6d4b\u504f\u5dee\uff0c\u800c\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\uff0c\u5982\u4f55\u4fdd\u6301\u7279\u5f81\u8868\u793a\u5bf9\u6df7\u6742\u53d8\u91cf\u7684\u4e0d\u53d8\u6027\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5f15\u5165R-MDN\u5c42\uff0c\u901a\u8fc7\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\u52a8\u6001\u8c03\u6574\u7279\u5f81\u5206\u5e03\uff0c\u9002\u5e94\u6570\u636e\u548c\u6df7\u6742\u53d8\u91cf\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cR-MDN\u5728\u9759\u6001\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u5747\u80fd\u51cf\u5c11\u6df7\u6742\u53d8\u91cf\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u9884\u6d4b\u7684\u516c\u5e73\u6027\u3002", "conclusion": "R-MDN\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6df7\u6742\u53d8\u91cf\u95ee\u9898\u3002"}}
{"id": "2507.10425", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10425", "abs": "https://arxiv.org/abs/2507.10425", "authors": ["Alvaro H. C. Correia", "Christos Louizos"], "title": "Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data", "comment": null, "summary": "Conformal prediction is a distribution-free uncertainty quantification method\nthat has gained popularity in the machine learning community due to its\nfinite-sample guarantees and ease of use. Its most common variant, dubbed split\nconformal prediction, is also computationally efficient as it boils down to\ncollecting statistics of the model predictions on some calibration data not yet\nseen by the model. Nonetheless, these guarantees only hold if the calibration\nand test data are exchangeable, a condition that is difficult to verify and\noften violated in practice due to so-called distribution shifts. The literature\nis rife with methods to mitigate the loss in coverage in this non-exchangeable\nsetting, but these methods require some prior information on the type of\ndistribution shift to be expected at test time. In this work, we study this\nproblem via a new perspective, through the lens of optimal transport, and show\nthat it is possible to estimate the loss in coverage and mitigate it in case of\ndistribution shift.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u89c6\u89d2\u7814\u7a76\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u5171\u5f62\u9884\u6d4b\u8986\u76d6\u635f\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f30\u8ba1\u548c\u7f13\u89e3\u8986\u76d6\u635f\u5931\u7684\u65b9\u6cd5\u3002", "motivation": "\u5171\u5f62\u9884\u6d4b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u53ef\u80fd\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u9884\u5148\u77e5\u9053\u5206\u5e03\u504f\u79fb\u7c7b\u578b\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u5229\u7528\u6700\u4f18\u4f20\u8f93\u7406\u8bba\uff0c\u5206\u6790\u5e76\u4f30\u8ba1\u5206\u5e03\u504f\u79fb\u5bf9\u5171\u5f62\u9884\u6d4b\u8986\u76d6\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u4ee5\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u6709\u6548\u4f30\u8ba1\u8986\u76d6\u635f\u5931\u5e76\u91c7\u53d6\u7f13\u89e3\u63aa\u65bd\u3002", "conclusion": "\u6700\u4f18\u4f20\u8f93\u4e3a\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u5171\u5f62\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u9884\u5148\u4e86\u89e3\u504f\u79fb\u7c7b\u578b\u3002"}}
{"id": "2507.09084", "categories": ["cs.LG", "cs.AI", "68T07, 90B22, 62M10", "I.2.m"], "pdf": "https://arxiv.org/pdf/2507.09084", "abs": "https://arxiv.org/abs/2507.09084", "authors": ["Nnamdi Daniel Aghanya", "Ta Duong Vu", "Ama\u00eblle Diop", "Charlotte Deville", "Nour Imane Kerroumi", "Irene Moulitsas", "Jun Li", "Desmond Bisandu"], "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction", "comment": "3 figures, 20 pages references and appendix included,", "summary": "Flight delays are a significant challenge in the aviation industry, causing\nmajor financial and operational disruptions. To improve passenger experience\nand reduce revenue loss, flight delay prediction models must be both precise\nand generalizable across different networks. This paper introduces a novel\napproach that combines Queue-Theory with a simple attention model, referred to\nas the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from\nthe US Bureau of Transportation Statistics, where our proposed QT-SimAM\n(Bidirectional) model outperformed existing methods with an accuracy of 0.927\nand an F1 score of 0.932. To assess transferability, we tested the model on the\nEUROCONTROL dataset. The results demonstrated strong performance, achieving an\naccuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an\neffective, end-to-end methodology for predicting flight delays. The proposed\nmodel's ability to forecast delays with high accuracy across different networks\ncan help reduce passenger anxiety and improve operational decision-making", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u961f\u5217\u7406\u8bba\u548c\u6ce8\u610f\u529b\u6a21\u578b\u7684\u65b0\u65b9\u6cd5QT-SimAM\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u822a\u73ed\u5ef6\u8bef\uff0c\u5e76\u5728\u4e0d\u540c\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u822a\u73ed\u5ef6\u8bef\u5bf9\u822a\u7a7a\u4e1a\u9020\u6210\u91cd\u5927\u8d22\u52a1\u548c\u8fd0\u8425\u95ee\u9898\uff0c\u9700\u8981\u7cbe\u786e\u4e14\u901a\u7528\u7684\u9884\u6d4b\u6a21\u578b\u4ee5\u6539\u5584\u4e58\u5ba2\u4f53\u9a8c\u548c\u51cf\u5c11\u6536\u5165\u635f\u5931\u3002", "method": "\u7ed3\u5408\u961f\u5217\u7406\u8bba\u548c\u7b80\u5355\u6ce8\u610f\u529b\u6a21\u578b\uff08QT-SimAM\uff09\uff0c\u5229\u7528\u7f8e\u56fd\u8fd0\u8f93\u7edf\u8ba1\u5c40\u548cEUROCONTROL\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "QT-SimAM\u5728\u7f8e\u56fd\u6570\u636e\u4e0a\u51c6\u786e\u7387\u4e3a0.927\uff0cF1\u5206\u65700.932\uff1b\u5728\u6b27\u76df\u6570\u636e\u4e0a\u51c6\u786e\u7387\u4e3a0.826\uff0cF1\u5206\u65700.791\u3002", "conclusion": "QT-SimAM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u80fd\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u4e0d\u540c\u7f51\u7edc\u7684\u822a\u73ed\u5ef6\u8bef\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u4e58\u5ba2\u7126\u8651\u548c\u4f18\u5316\u8fd0\u8425\u51b3\u7b56\u3002"}}
{"id": "2507.09095", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09095", "abs": "https://arxiv.org/abs/2507.09095", "authors": ["Md Hasan Shahriar", "Md Mohaimin Al Barat", "Harshavardhan Sundar", "Naren Ramakrishnan", "Y. Thomas Hou", "Wenjing Lou"], "title": "On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving", "comment": "16 pages", "summary": "Multimodal fusion (MMF) plays a critical role in the perception of autonomous\ndriving, which primarily fuses camera and LiDAR streams for a comprehensive and\nefficient scene understanding. However, its strict reliance on precise temporal\nsynchronization exposes it to new vulnerabilities. In this paper, we introduce\nDejaVu, a novel attack that exploits network-induced delays to create subtle\ntemporal misalignments across sensor streams, severely degrading downstream\nMMF-based perception tasks. Our comprehensive attack analysis across different\nmodels and datasets reveals these sensors' task-specific imbalanced\nsensitivities: object detection is overly dependent on LiDAR inputs while\nobject tracking is highly reliant on the camera inputs. Consequently, with a\nsingle-frame LiDAR delay, an attacker can reduce the car detection mAP by up to\n88.5%, while with a three-frame camera delay, multiple object tracking accuracy\n(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense\npatch that can work alongside the existing perception model to monitor temporal\nalignment through cross-modal temporal consistency. AION leverages multimodal\nshared representation learning and dynamic time warping to determine the path\nof temporal alignment and calculate anomaly scores based on the alignment. Our\nthorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with\nlow false positives across datasets and model architectures, demonstrating it\nas a robust and generalized defense against the temporal misalignment attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDejaVu\u653b\u51fb\uff0c\u5229\u7528\u7f51\u7edc\u5ef6\u8fdf\u5bfc\u81f4\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u65f6\u95f4\u9519\u4f4d\uff0c\u663e\u8457\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u9632\u5fa1\u65b9\u6cd5AION\u3002", "motivation": "\u591a\u6a21\u6001\u878d\u5408\uff08MMF\uff09\u5bf9\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4f9d\u8d56\u4e25\u683c\u7684\u65f6\u95f4\u540c\u6b65\uff0c\u6613\u53d7\u653b\u51fb\u3002", "method": "\u63d0\u51faDejaVu\u653b\u51fb\uff0c\u5206\u6790\u4f20\u611f\u5668\u4efb\u52a1\u654f\u611f\u6027\uff1b\u8bbe\u8ba1AION\u9632\u5fa1\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u65f6\u95f4\u4e00\u81f4\u6027\u76d1\u6d4b\u653b\u51fb\u3002", "result": "\u653b\u51fb\u53ef\u663e\u8457\u964d\u4f4e\u68c0\u6d4b\u6027\u80fd\uff08\u5982mAP\u964d88.5%\uff09\uff1bAION\u9632\u5fa1AUROC\u8fbe0.92-0.98\uff0c\u8bef\u62a5\u7387\u4f4e\u3002", "conclusion": "AION\u662f\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u9519\u4f4d\u653b\u51fb\u7684\u9c81\u68d2\u4e14\u901a\u7528\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2507.09101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09101", "abs": "https://arxiv.org/abs/2507.09101", "authors": ["Yanan Cao", "Omid Memarrast", "Shiqin Cai", "Sinduja Subramaniam", "Evren Korpeoglu", "Kannan Achan"], "title": "S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe", "comment": null, "summary": "In grocery e-commerce, customers often build ingredient baskets guided by\ndietary preferences but lack the expertise to create complete meals. Leveraging\nrecipe knowledge to recommend complementary ingredients based on a partial\nbasket is essential for improving the culinary experience. Traditional recipe\ncompletion methods typically predict a single missing ingredient using a\nleave-one-out strategy. However, they fall short in two key aspects: (i) they\ndo not reflect real-world scenarios where multiple ingredients are often\nneeded, and (ii) they overlook relationships among the missing ingredients\nthemselves. To address these limitations, we reformulate basket completion as a\nset-to-set (S2S) recommendation problem, where an incomplete basket is input\ninto a system that predicts a set of complementary ingredients. We introduce\nS2SRec2, a set-to-set ingredient recommendation framework based on a Set\nTransformer and trained in a multitask learning paradigm. S2SRec2 jointly\nlearns to (i) retrieve missing ingredients from the representation of existing\nones and (ii) assess basket completeness after prediction. These tasks are\noptimized together, enforcing accurate retrieval and coherent basket\ncompletion. Experiments on large-scale recipe datasets and qualitative analyses\nshow that S2SRec2 significantly outperforms single-target baselines, offering a\npromising approach to enhance grocery shopping and inspire culinary creativity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u5230\u96c6\u5408\uff08S2S\uff09\u7684\u63a8\u8350\u6846\u67b6S2SRec2\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u98df\u8c31\u8865\u5168\u65b9\u6cd5\u5728\u9884\u6d4b\u591a\u4e2a\u7f3a\u5931\u98df\u6750\u65f6\u7684\u4e0d\u8db3\u3002", "motivation": "\u5728\u751f\u9c9c\u7535\u5546\u4e2d\uff0c\u987e\u5ba2\u5e38\u56e0\u7f3a\u4e4f\u4e13\u4e1a\u77e5\u8bc6\u800c\u65e0\u6cd5\u6784\u5efa\u5b8c\u6574\u7684\u98df\u6750\u7ec4\u5408\uff0c\u4f20\u7edf\u65b9\u6cd5\u4ec5\u9884\u6d4b\u5355\u4e00\u7f3a\u5931\u98df\u6750\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "S2SRec2\u91c7\u7528Set Transformer\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u540c\u65f6\u5b66\u4e60\u4ece\u73b0\u6709\u98df\u6750\u8868\u793a\u4e2d\u68c0\u7d22\u7f3a\u5931\u98df\u6750\u548c\u8bc4\u4f30\u7bee\u5b50\u7684\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS2SRec2\u663e\u8457\u4f18\u4e8e\u5355\u76ee\u6807\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u8d2d\u7269\u4f53\u9a8c\u548c\u70f9\u996a\u521b\u610f\u3002", "conclusion": "S2SRec2\u4e3a\u751f\u9c9c\u7535\u5546\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u98df\u6750\u63a8\u8350\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.09132", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09132", "abs": "https://arxiv.org/abs/2507.09132", "authors": ["Chu-Yuan Wei", "Shun-Yao Liu", "Sheng-Da Zhuo", "Chang-Dong Wang", "Shu-Qiang Huang", "Mohsen Guizani"], "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning", "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in various\ngraph-based tasks (e.g., node classification or link prediction). Despite their\ntriumphs, GNNs still face challenges such as long training and inference times,\ndifficulty in capturing complex relationships, and insufficient feature\nextraction. To tackle these issues, graph pre-training and graph prompt methods\nhave garnered increasing attention for their ability to leverage large-scale\ndatasets for initial learning and task-specific adaptation, offering potential\nimprovements in GNN performance. However, previous research has overlooked the\npotential of graph prompts in optimizing models, as well as the impact of both\npositive and negative graph prompts on model stability and efficiency. To\nbridge this gap, we propose a novel framework combining graph prompts with\nweight pruning, called GPAWP, which aims to enhance the performance and\nefficiency of graph prompts by using fewer of them. We evaluate the importance\nof graph prompts using an importance assessment function to determine positive\nand negative weights at different granularities. Through hierarchically\nstructured pruning, we eliminate negative prompt labels, resulting in more\nparameter-efficient and competitively performing prompts. Extensive experiments\non three benchmark datasets demonstrate the superiority of GPAWP, leading to a\nsignificant reduction in parameters in node classification tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u63d0\u793a\u4e0e\u6743\u91cd\u526a\u679d\u7684\u65b0\u6846\u67b6GPAWP\uff0c\u65e8\u5728\u901a\u8fc7\u51cf\u5c11\u63d0\u793a\u6570\u91cf\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u56fe\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u9762\u4e34\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u96be\u4ee5\u6355\u6349\u590d\u6742\u5173\u7cfb\u7b49\u95ee\u9898\u3002\u56fe\u9884\u8bad\u7ec3\u548c\u63d0\u793a\u65b9\u6cd5\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u56fe\u63d0\u793a\u5bf9\u6a21\u578b\u4f18\u5316\u7684\u5f71\u54cd\u53ca\u5176\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faGPAWP\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u8bc4\u4f30\u51fd\u6570\u786e\u5b9a\u4e0d\u540c\u7c92\u5ea6\u7684\u6b63\u8d1f\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u526a\u679d\u53bb\u9664\u8d1f\u63d0\u793a\u6807\u7b7e\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPAWP\u663e\u8457\u51cf\u5c11\u4e86\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002", "conclusion": "GPAWP\u901a\u8fc7\u4f18\u5316\u56fe\u63d0\u793a\u548c\u6743\u91cd\u526a\u679d\uff0c\u63d0\u5347\u4e86GNNs\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09137", "abs": "https://arxiv.org/abs/2507.09137", "authors": ["Nripsuta Ani Saxena", "Shang-Ling Hsu", "Mehul Shetty", "Omar Alkhadra", "Cyrus Shahabi", "Abigail L. Horn"], "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution", "comment": null, "summary": "Accurately attributing user visits to specific Points of Interest (POIs) is a\nfoundational task for mobility analytics, personalized services, marketing and\nurban planning. However, POI attribution remains challenging due to GPS\ninaccuracies, typically ranging from 2 to 20 meters in real-world settings, and\nthe high spatial density of POIs in urban environments, where multiple venues\ncan coexist within a small radius (e.g., over 50 POIs within a 100-meter radius\nin dense city centers). Relying on proximity is therefore often insufficient\nfor determining which POI was actually visited. We introduce\n\\textsf{POIFormer}, a novel Transformer-based framework for accurate and\nefficient POI attribution. Unlike prior approaches that rely on limited\nspatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly\nmodels a rich set of signals, including spatial proximity, visit timing and\nduration, contextual features from POI semantics, and behavioral features from\nuser mobility and aggregated crowd behavior patterns--using the Transformer's\nself-attention mechanism to jointly model complex interactions across these\ndimensions. By leveraging the Transformer to model a user's past and future\nvisits (with the current visit masked) and incorporating crowd-level behavioral\npatterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate,\nefficient attribution in large, noisy mobility datasets. Its architecture\nsupports generalization across diverse data sources and geographic contexts\nwhile avoiding reliance on hard-to-access or unavailable data layers, making it\npractical for real-world deployment. Extensive experiments on real-world\nmobility datasets demonstrate significant improvements over existing baselines,\nparticularly in challenging real-world settings characterized by spatial noise\nand dense POI clustering.", "AI": {"tldr": "POIFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9ad8\u6548\u5730\u5c06\u7528\u6237\u8bbf\u95ee\u5f52\u56e0\u4e8e\u7279\u5b9aPOI\uff0c\u89e3\u51b3\u4e86GPS\u4e0d\u51c6\u786e\u548cPOI\u7a7a\u95f4\u5bc6\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eGPS\u4e0d\u51c6\u786e\u548c\u57ce\u5e02\u73af\u5883\u4e2dPOI\u7684\u9ad8\u5bc6\u5ea6\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u90bb\u8fd1\u6027\u7684\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5f52\u56e0\u7528\u6237\u8bbf\u95ee\u3002", "method": "POIFormer\u5229\u7528Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8054\u5408\u5efa\u6a21\u7a7a\u95f4\u90bb\u8fd1\u6027\u3001\u8bbf\u95ee\u65f6\u95f4\u3001POI\u8bed\u4e49\u3001\u7528\u6237\u884c\u4e3a\u548c\u7fa4\u4f53\u884c\u4e3a\u6a21\u5f0f\u7b49\u591a\u7ef4\u5ea6\u4fe1\u53f7\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPOIFormer\u5728\u7a7a\u95f4\u566a\u58f0\u548c\u9ad8\u5bc6\u5ea6POI\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "POIFormer\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684POI\u5f52\u56e0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6570\u636e\u6e90\u548c\u5730\u7406\u73af\u5883\u3002"}}
{"id": "2507.09173", "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2507.09173", "abs": "https://arxiv.org/abs/2507.09173", "authors": ["Mengjie Chen", "Ming Zhang", "Cunquan Qu"], "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations", "comment": null, "summary": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,\noften leading to adverse drug reactions with significant implications for\npatient safety and healthcare outcomes. While graph-based methods have achieved\nstrong predictive performance, most approaches treat drug pairs independently,\noverlooking the complex, context-dependent interactions unique to drug pairs.\nAdditionally, these models struggle to integrate biological interaction\nnetworks and molecular-level structures to provide meaningful mechanistic\ninsights. In this study, we propose MolecBioNet, a novel graph-based framework\nthat integrates molecular and biomedical knowledge for robust and interpretable\nDDI prediction. By modeling drug pairs as unified entities, MolecBioNet\ncaptures both macro-level biological interactions and micro-level molecular\ninfluences, offering a comprehensive perspective on DDIs. The framework\nextracts local subgraphs from biomedical knowledge graphs and constructs\nhierarchical interaction graphs from molecular representations, leveraging\nclassical graph neural network methods to learn multi-scale representations of\ndrug pairs. To enhance accuracy and interpretability, MolecBioNet introduces\ntwo domain-specific pooling strategies: context-aware subgraph pooling\n(CASPool), which emphasizes biologically relevant entities, and\nattention-guided influence pooling (AGIPool), which prioritizes influential\nmolecular substructures. The framework further employs mutual information\nminimization regularization to enhance information diversity during embedding\nfusion. Experimental results demonstrate that MolecBioNet outperforms\nstate-of-the-art methods in DDI prediction, while ablation studies and\nembedding visualizations further validate the advantages of unified drug pair\nmodeling and multi-scale knowledge integration.", "AI": {"tldr": "MolecBioNet\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u6846\u67b6\uff0c\u6574\u5408\u5206\u5b50\u548c\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff0c\u7528\u4e8e\u9884\u6d4b\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u836f\u7269\u5bf9\uff0c\u5ffd\u7565\u5176\u590d\u6742\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u4e14\u96be\u4ee5\u6574\u5408\u751f\u7269\u7f51\u7edc\u548c\u5206\u5b50\u7ed3\u6784\u3002", "method": "MolecBioNet\u5c06\u836f\u7269\u5bf9\u5efa\u6a21\u4e3a\u7edf\u4e00\u5b9e\u4f53\uff0c\u63d0\u53d6\u751f\u7269\u77e5\u8bc6\u56fe\u8c31\u5b50\u56fe\u5e76\u6784\u5efa\u5206\u5b50\u5c42\u6b21\u56fe\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u5f15\u5165\u4e24\u79cd\u6c60\u5316\u7b56\u7565\uff08CASPool\u548cAGIPool\uff09\u53ca\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMolecBioNet\u5728DDI\u9884\u6d4b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u548c\u5d4c\u5165\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "MolecBioNet\u901a\u8fc7\u7edf\u4e00\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u77e5\u8bc6\u6574\u5408\uff0c\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684DDI\u9884\u6d4b\u3002"}}
{"id": "2507.09202", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.09202", "abs": "https://arxiv.org/abs/2507.09202", "authors": ["Wuxin Wang", "Weicheng Ni", "Lilan Huang", "Tao Hao", "Ben Fei", "Shuo Ma", "Taikang Yuan", "Yanlai Zhao", "Kefeng Deng", "Xiaoyong Li", "Boheng Duan", "Lei Bai", "Kaijun Ren"], "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge", "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.", "AI": {"tldr": "XiChen\u662f\u4e00\u4e2a\u5b8c\u5168\u7531AI\u9a71\u52a8\u7684\u5168\u7403\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\uff0c\u80fd\u591f\u572817\u79d2\u5185\u5b8c\u6210\u4ece\u6570\u636e\u540c\u5316\u5230\u4e2d\u671f\u9884\u62a5\u7684\u6574\u4e2a\u6d41\u7a0b\uff0c\u5176\u9884\u62a5\u51c6\u786e\u6027\u53ef\u4e0e\u4f20\u7edfNWP\u7cfb\u7edf\u5ab2\u7f8e\u3002", "motivation": "\u4f20\u7edfAI\u5929\u6c14\u9884\u62a5\u6a21\u578b\u4f9d\u8d56NWP\u7cfb\u7edf\u51c6\u5907\u521d\u59cb\u6761\u4ef6\uff0c\u8017\u65f6\u4e14\u6548\u7387\u4f4e\uff0cXiChen\u65e8\u5728\u5b9e\u73b0\u5b8c\u5168\u72ec\u7acb\u4e8eNWP\u7684\u9ad8\u6548AI\u5929\u6c14\u9884\u62a5\u3002", "method": "XiChen\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u5929\u6c14\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u4f5c\u4e3a\u89c2\u6d4b\u7b97\u5b50\u548c\u6570\u636e\u540c\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u56db\u7ef4\u53d8\u5206\u77e5\u8bc6\uff0c\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u540c\u5316\u548c\u9884\u62a5\u3002", "result": "XiChen\u572817\u79d2\u5185\u5b8c\u6210\u5168\u6d41\u7a0b\uff0c\u9884\u62a5\u51c6\u786e\u6027\u4e0eNWP\u7cfb\u7edf\u76f8\u5f53\uff0c\u9884\u62a5\u9886\u5148\u65f6\u95f4\u8d85\u8fc78.25\u5929\u3002", "conclusion": "XiChen\u5c55\u793a\u4e86\u5b8c\u5168\u7531AI\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u6709\u671b\u6446\u8131\u5bf9NWP\u7cfb\u7edf\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.09264", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09264", "abs": "https://arxiv.org/abs/2507.09264", "authors": ["Payel Mukhopadhyay", "Michael McCabe", "Ruben Ohana", "Miles Cranmer"], "title": "Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations", "comment": null, "summary": "Patch-based transformer surrogates have become increasingly effective for\nmodeling spatiotemporal dynamics, but the fixed patch size is a major\nlimitation for budget-conscience deployment in production. We introduce two\nlightweight, architecture-agnostic modules-the Convolutional Kernel Modulator\n(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size\ncontrol at inference in patch based models, without retraining or accuracy\nloss. Combined with a cyclic patch-size rollout, our method mitigates patch\nartifacts and improves long-term stability for video-like prediction tasks.\nApplied to a range of challenging 2D and 3D PDE benchmarks, our approach\nimproves rollout fidelity and runtime efficiency. To our knowledge, this is the\nfirst framework to enable inference-time patch-size tunability in patch-based\nPDE surrogates. Its plug-and-play design makes it broadly applicable across\narchitectures-establishing a general foundation for compute-adaptive modeling\nin PDE surrogate tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u8f7b\u91cf\u7ea7\u6a21\u5757\uff08CKM\u548cCSM\uff09\uff0c\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u8865\u4e01\u5927\u5c0f\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u56fa\u5b9a\u8865\u4e01\u5927\u5c0f\u9650\u5236\u4e86\u9884\u7b97\u654f\u611f\u7684\u90e8\u7f72\uff0c\u9700\u52a8\u6001\u8c03\u6574\u8865\u4e01\u5927\u5c0f\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u5f15\u5165CKM\u548cCSM\u6a21\u5757\uff0c\u7ed3\u5408\u5faa\u73af\u8865\u4e01\u5927\u5c0f\u7b56\u7565\uff0c\u52a8\u6001\u63a7\u5236\u8865\u4e01\u5927\u5c0f\u3002", "result": "\u57282D\u548c3D PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u8865\u4e01\u5927\u5c0f\u52a8\u6001\u8c03\u6574\uff0c\u4e3aPDE\u4ee3\u7406\u4efb\u52a1\u63d0\u4f9b\u901a\u7528\u8ba1\u7b97\u81ea\u9002\u5e94\u5efa\u6a21\u57fa\u7840\u3002"}}
{"id": "2507.09362", "categories": ["cs.LG", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2507.09362", "abs": "https://arxiv.org/abs/2507.09362", "authors": ["Assaf Marron", "Smadar Szekely", "Irun Cohen", "David Harel"], "title": "Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes", "comment": null, "summary": "An autoencoder (AE) is a neural network that, using self-supervised training,\nlearns a succinct parameterized representation, and a corresponding encoding\nand decoding process, for all instances in a given class. Here, we introduce\nthe concept of a meta-autoencoder (MAE): an AE for a collection of\nautoencoders. Given a family of classes that differ from each other by the\nvalues of some parameters, and a trained AE for each class, an MAE for the\nfamily is a neural net that has learned a compact representation and associated\nencoder and decoder for the class-specific AEs. One application of this general\nconcept is in research and modeling of natural evolution -- capturing the\ndefining and the distinguishing properties across multiple species that are\ndynamically evolving from each other and from common ancestors. In this interim\nreport we provide a constructive definition of MAEs, initial examples, and the\nmotivating research directions in machine learning and biology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5143\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u5bf9\u591a\u4e2a\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u7f16\u7801\u548c\u89e3\u7801\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u6f14\u5316\u7684\u591a\u7c7b\u522b\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u6355\u6349\u52a8\u6001\u6f14\u5316\u7684\u591a\u7c7b\u522b\uff08\u5982\u7269\u79cd\uff09\u4e4b\u95f4\u7684\u5171\u6027\u548c\u5dee\u5f02\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u548c\u751f\u7269\u5b66\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5143\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\uff0c\u5bf9\u591a\u4e2a\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u7f16\u7801\u548c\u89e3\u7801\uff0c\u5b66\u4e60\u5176\u7d27\u51d1\u8868\u793a\u3002", "result": "\u521d\u6b65\u5b9a\u4e49\u4e86MAE\u7684\u6982\u5ff5\uff0c\u5e76\u63d0\u4f9b\u4e86\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u5b66\u4e60\u548c\u751f\u7269\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "MAE\u4e3a\u52a8\u6001\u6f14\u5316\u7684\u591a\u7c7b\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u7406\u8bba\u548c\u5e94\u7528\u3002"}}
{"id": "2507.09382", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09382", "abs": "https://arxiv.org/abs/2507.09382", "authors": ["Bojian Hou", "Zhanliang Wang", "Zhuoping Zhou", "Boning Tong", "Zexuan Wang", "Jingxuan Bao", "Duy Duong-Tran", "Qi Long", "Li Shen"], "title": "Fair CCA for Fair Representation Learning: An ADNI Study", "comment": null, "summary": "Canonical correlation analysis (CCA) is a technique for finding correlations\nbetween different data modalities and learning low-dimensional representations.\nAs fairness becomes crucial in machine learning, fair CCA has gained attention.\nHowever, previous approaches often overlook the impact on downstream\nclassification tasks, limiting applicability. We propose a novel fair CCA\nmethod for fair representation learning, ensuring the projected features are\nindependent of sensitive attributes, thus enhancing fairness without\ncompromising accuracy. We validate our method on synthetic data and real-world\ndata from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating\nits ability to maintain high correlation analysis performance while improving\nfairness in classification tasks. Our work enables fair machine learning in\nneuroimaging studies where unbiased analysis is essential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73CCA\u65b9\u6cd5\uff0c\u7528\u4e8e\u516c\u5e73\u8868\u793a\u5b66\u4e60\uff0c\u786e\u4fdd\u6295\u5f71\u7279\u5f81\u4e0e\u654f\u611f\u5c5e\u6027\u65e0\u5173\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u76f8\u5173\u6027\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u516c\u5e73\u6027\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\u589e\u52a0\uff0c\u516c\u5e73CCA\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u5bf9\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u516c\u5e73CCA\u65b9\u6cd5\uff0c\u786e\u4fdd\u6295\u5f71\u7279\u5f81\u72ec\u7acb\u4e8e\u654f\u611f\u5c5e\u6027\uff0c\u4ee5\u589e\u5f3a\u516c\u5e73\u6027\u800c\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548cADNI\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u65e2\u80fd\u4fdd\u6301\u9ad8\u76f8\u5173\u6027\u5206\u6790\u6027\u80fd\uff0c\u53c8\u80fd\u63d0\u9ad8\u5206\u7c7b\u4efb\u52a1\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u4e2d\u7684\u516c\u5e73\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u786e\u4fdd\u65e0\u504f\u5206\u6790\u3002"}}
{"id": "2507.09391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09391", "abs": "https://arxiv.org/abs/2507.09391", "authors": ["Peter Pao-Huang", "Mitchell Black", "Xiaojie Qiu"], "title": "Geometric Generative Modeling with Noise-Conditioned Graph Networks", "comment": "ICML 2025", "summary": "Generative modeling of graphs with spatial structure is essential across many\napplications from computer graphics to spatial genomics. Recent flow-based\ngenerative models have achieved impressive results by gradually adding and then\nlearning to remove noise from these graphs. Existing models, however, use graph\nneural network architectures that are independent of the noise level, limiting\ntheir expressiveness. To address this issue, we introduce\n\\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural\nnetworks that dynamically modify their architecture according to the noise\nlevel during generation. Our theoretical and empirical analysis reveals that as\nnoise increases, (1) graphs require information from increasingly distant\nneighbors and (2) graphs can be effectively represented at lower resolutions.\nBased on these insights, we develop Dynamic Message Passing (DMP), a specific\ninstantiation of NCGNs that adapts both the range and resolution of message\npassing to the noise level. DMP consistently outperforms noise-independent\narchitectures on a variety of domains including $3$D point clouds,\nspatiotemporal transcriptomics, and images. Code is available at\nhttps://github.com/peterpaohuang/ncgn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u566a\u58f0\u6761\u4ef6\u56fe\u7f51\u7edc\uff08NCGNs\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u67b6\u6784\u4ee5\u9002\u5e94\u566a\u58f0\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u751f\u6210\u6a21\u578b\u5728\u566a\u58f0\u5904\u7406\u4e0a\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u6027\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "\u5f15\u5165NCGNs\uff0c\u63d0\u51fa\u52a8\u6001\u6d88\u606f\u4f20\u9012\uff08DMP\uff09\uff0c\u6839\u636e\u566a\u58f0\u6c34\u5e73\u8c03\u6574\u6d88\u606f\u4f20\u9012\u7684\u8303\u56f4\u548c\u5206\u8fa8\u7387\u3002", "result": "DMP\u57283D\u70b9\u4e91\u3001\u65f6\u7a7a\u8f6c\u5f55\u7ec4\u5b66\u548c\u56fe\u50cf\u7b49\u591a\u4e2a\u9886\u57df\u5747\u4f18\u4e8e\u566a\u58f0\u65e0\u5173\u67b6\u6784\u3002", "conclusion": "NCGNs\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u566a\u58f0\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u751f\u6210\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2507.09394", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09394", "abs": "https://arxiv.org/abs/2507.09394", "authors": ["Nandan Kumar Jha", "Brandon Reagen"], "title": "A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention", "comment": "ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD)", "summary": "In this work, we study how multi-head latent attention (MLA), a popular\nstrategy for compressing key/value memory, affects a transformer's internal\ncapacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)\ndiagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\\top$ gram matrix\nthroughout training, comparing three variants: the standard multi-head\nattention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,\nand MLA-Decoupled, which shares a single rotary sub-vector across all heads.\nOur random matrix analysis reveals \\textbf{three key findings:} \\textbf{ i)}\ncapacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,\nearly spikes in specific layers that persist and propagate, disrupting the\nbalance between bulk and outlier directions; \\textbf{ ii)} these spikes\ncoincide with rank collapse, concentrating the model's expressivity into narrow\nsubspaces; \\textbf{ iii)} only the decoupled variant prevents this cascade,\nmaintaining broad spectral support and suppressing outlier formation across\nlayers. These results underscore that \\emph{how} rotary embeddings are applied\nis just as critical as \\emph{where} compression occurs. Sharing rotary\ncomponents across heads mitigates spectral fragmentation and preserves\nrepresentational capacity.", "AI": {"tldr": "\u7814\u7a76\u4e86\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u5728\u9884\u8bad\u7ec3\u4e2d\u5bf9Transformer\u5185\u90e8\u5bb9\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u65cb\u8f6c\u5d4c\u5165\u7684\u5e94\u7528\u65b9\u5f0f\u5bf9\u9632\u6b62\u5bb9\u91cf\u74f6\u9888\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u63a2\u8ba8MLA\u538b\u7f29\u952e/\u503c\u5185\u5b58\u65f6\u5982\u4f55\u5f71\u54cdTransformer\u7684\u9884\u8bad\u7ec3\u5bb9\u91cf\uff0c\u7279\u522b\u662f\u65cb\u8f6c\u5d4c\u5165\u7684\u5e94\u7528\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Marchenko-Pastur\u8bca\u65ad\u5206\u6790$W_{Q}W_{K}^\\top$\u77e9\u9635\u8c31\uff0c\u6bd4\u8f83\u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u3001MLA-PreRoPE\u548cMLA-Decoupled\u4e09\u79cd\u53d8\u4f53\u3002", "result": "\u53d1\u73b0\u5bb9\u91cf\u74f6\u9888\u5c40\u90e8\u51fa\u73b0\uff0c\u65cb\u8f6c\u5d4c\u5165\u7684\u89e3\u8026\u53d8\u4f53\u80fd\u9632\u6b62\u8c31\u5206\u88c2\uff0c\u4fdd\u6301\u5e7f\u6cdb\u8c31\u652f\u6301\u3002", "conclusion": "\u65cb\u8f6c\u5d4c\u5165\u7684\u5e94\u7528\u65b9\u5f0f\u4e0e\u538b\u7f29\u4f4d\u7f6e\u540c\u6837\u91cd\u8981\uff0c\u89e3\u8026\u8bbe\u8ba1\u80fd\u6709\u6548\u7ef4\u6301\u6a21\u578b\u5bb9\u91cf\u3002"}}
{"id": "2507.09404", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09404", "abs": "https://arxiv.org/abs/2507.09404", "authors": ["Mustafa Shukor", "Louis Bethune", "Dan Busbridge", "David Grangier", "Enrico Fini", "Alaaeldin El-Nouby", "Pierre Ablin"], "title": "Scaling Laws for Optimal Data Mixtures", "comment": null, "summary": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f29\u653e\u5b9a\u5f8b\u7684\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u76ee\u6807\u9886\u57df\u7684\u6700\u4f73\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8bd5\u9519\u6cd5\u7684\u9ad8\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u9009\u62e9\u4f9d\u8d56\u8bd5\u9519\u6cd5\uff0c\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u4e0d\u5b9e\u7528\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u7f29\u653e\u5b9a\u5f8b\u9884\u6d4b\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u4e0b\u7684\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u5c0f\u89c4\u6a21\u8bad\u7ec3\u4f30\u7b97\u53c2\u6570\uff0c\u5916\u63a8\u81f3\u66f4\u5927\u89c4\u6a21\u3002", "result": "\u5728LLM\u3001NMM\u548cLVM\u4e09\u79cd\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7f29\u653e\u5b9a\u5f8b\u7684\u666e\u9002\u6027\u548c\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u786e\u5b9a\u6700\u4f18\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.09406", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09406", "abs": "https://arxiv.org/abs/2507.09406", "authors": ["Santhosh Kumar Ravindran"], "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers", "comment": null, "summary": "Large language models (LLMs) aligned for safety through techniques like\nreinforcement learning from human feedback (RLHF) often exhibit emergent\ndeceptive behaviors, where outputs appear compliant but subtly mislead or omit\ncritical information. This paper introduces adversarial activation patching, a\nnovel mechanistic interpretability framework that leverages activation patching\nas an adversarial tool to induce, detect, and mitigate such deception in\ntransformer-based models. By sourcing activations from \"deceptive\" prompts and\npatching them into safe forward passes at specific layers, we simulate\nvulnerabilities and quantify deception rates. Through toy neural network\nsimulations across multiple scenarios (e.g., 1000 trials per setup), we\ndemonstrate that adversarial patching increases deceptive outputs to 23.9% from\na 0% baseline, with layer-specific variations supporting our hypotheses. We\npropose six hypotheses, including transferability across models, exacerbation\nin multimodal settings, and scaling effects. An expanded literature review\nsynthesizes over 20 key works in interpretability, deception, and adversarial\nattacks. Mitigation strategies, such as activation anomaly detection and robust\nfine-tuning, are detailed, alongside ethical considerations and future research\ndirections. This work advances AI safety by highlighting patching's dual-use\npotential and provides a roadmap for empirical studies on large-scale models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u6fc0\u6d3b\u4fee\u8865\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6b3a\u9a97\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b89\u5168\u5bf9\u9f50\u540e\u4ecd\u53ef\u80fd\u8868\u73b0\u51fa\u6b3a\u9a97\u884c\u4e3a\uff0c\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u68c0\u6d4b\u548c\u7f13\u89e3\u6b64\u7c7b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u6fc0\u6d3b\u4fee\u8865\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u8865\u7279\u5b9a\u5c42\u7684\u6fc0\u6d3b\u6765\u6a21\u62df\u548c\u91cf\u5316\u6b3a\u9a97\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u6297\u6027\u4fee\u8865\u5c06\u6b3a\u9a97\u6027\u8f93\u51fa\u4ece0%\u63d0\u5347\u81f323.9%\uff0c\u5e76\u9a8c\u8bc1\u4e86\u591a\u4e2a\u5047\u8bbe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.09428", "categories": ["cs.LG", "math.DG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09428", "abs": "https://arxiv.org/abs/2507.09428", "authors": ["Zakhar Shumaylov", "Vasileios Tsiaras", "Yannis Stylianou"], "title": "On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization", "comment": null, "summary": "The ever-increasing parameter counts of deep learning models necessitate\neffective compression techniques for deployment on resource-constrained\ndevices. This paper explores the application of information geometry, the study\nof density-induced metrics on parameter spaces, to analyze existing methods\nwithin the space of model compression, primarily focusing on operator\nfactorization. Adopting this perspective highlights the core challenge:\ndefining an optimal low-compute submanifold (or subset) and projecting onto it.\nWe argue that many successful model compression approaches can be understood as\nimplicitly approximating information divergences for this projection. We\nhighlight that when compressing a pre-trained model, using information\ndivergences is paramount for achieving improved zero-shot accuracy, yet this\nmay no longer be the case when the model is fine-tuned. In such scenarios,\ntrainability of bottlenecked models turns out to be far more important for\nachieving high compression ratios with minimal performance degradation,\nnecessitating adoption of iterative methods. In this context, we prove\nconvergence of iterative singular value thresholding for training neural\nnetworks subject to a soft rank constraint. To further illustrate the utility\nof this perspective, we showcase how simple modifications to existing methods\nthrough softer rank reduction result in improved performance under fixed\ncompression rates.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4fe1\u606f\u51e0\u4f55\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u901a\u8fc7\u5b9a\u4e49\u4f4e\u8ba1\u7b97\u5b50\u6d41\u5f62\u548c\u6295\u5f71\u5b9e\u73b0\u538b\u7f29\uff0c\u5e76\u8bc1\u660e\u8fed\u4ee3\u5947\u5f02\u503c\u9608\u503c\u65b9\u6cd5\u7684\u6536\u655b\u6027\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53c2\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u6280\u672f\uff0c\u4fe1\u606f\u51e0\u4f55\u63d0\u4f9b\u4e86\u4e00\u79cd\u5206\u6790\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u7684\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u51e0\u4f55\u5206\u6790\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u63d0\u51fa\u901a\u8fc7\u5b9a\u4e49\u4f4e\u8ba1\u7b97\u5b50\u6d41\u5f62\u548c\u6295\u5f71\u5b9e\u73b0\u538b\u7f29\uff0c\u5e76\u8bc1\u660e\u8fed\u4ee3\u5947\u5f02\u503c\u9608\u503c\u65b9\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u4fe1\u606f\u6563\u5ea6\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u538b\u7f29\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5728\u5fae\u8c03\u573a\u666f\u4e2d\uff0c\u74f6\u9888\u6a21\u578b\u7684\u53ef\u8bad\u7ec3\u6027\u66f4\u4e3a\u91cd\u8981\u3002", "conclusion": "\u4fe1\u606f\u51e0\u4f55\u89c6\u89d2\u4e3a\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u901a\u8fc7\u8f6f\u79e9\u51cf\u5c11\u7b49\u7b80\u5355\u4fee\u6539\u53ef\u63d0\u5347\u56fa\u5b9a\u538b\u7f29\u7387\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09439", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09439", "abs": "https://arxiv.org/abs/2507.09439", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series", "comment": null, "summary": "Understanding causal relationships in multivariate time series (MTS) is\nessential for effective decision-making in fields such as finance and\nmarketing, where complex dependencies and lagged effects challenge conventional\nanalytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal\nNetworks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel\narchitecture designed to enhance causal discovery by integrating dilated\ntemporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net\neffectively captures multiscale temporal dependencies through dilated\nconvolutions while leveraging an adaptive thresholding strategy in its\nattention mechanism to eliminate spurious connections, ensuring both accuracy\nand interpretability. A statistical shuffle test validation further strengthens\nrobustness by filtering false positives and improving causal inference\nreliability. Extensive evaluations on financial and marketing datasets\ndemonstrate that DyCAST-Net consistently outperforms existing models such as\nTCDF, GCFormer, and CausalFormer. The model provides a more precise estimation\nof causal delays and significantly reduces false discoveries, particularly in\nnoisy environments. Moreover, attention heatmaps offer interpretable insights,\nuncovering hidden causal patterns such as the mediated effects of advertising\non consumer behavior and the influence of macroeconomic indicators on financial\nmarkets. Case studies illustrate DyCAST-Net's ability to detect latent\nmediators and lagged causal factors, making it particularly effective in\nhigh-dimensional, dynamic settings. The model's architecture enhanced by\nRMSNorm stabilization and causal masking ensures scalability and adaptability\nacross diverse application domains", "AI": {"tldr": "DyCAST-Net\u662f\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u7ed3\u5408\u6269\u5f20\u65f6\u95f4\u5377\u79ef\u548c\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u53d1\u73b0\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u7406\u89e3\u5bf9\u91d1\u878d\u548c\u8425\u9500\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u4f9d\u8d56\u548c\u6ede\u540e\u6548\u5e94\u3002", "method": "DyCAST-Net\u901a\u8fc7\u6269\u5f20\u5377\u79ef\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\uff0c\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u6d88\u9664\u865a\u5047\u8fde\u63a5\uff0c\u7ed3\u5408\u7edf\u8ba1\u6d17\u724c\u6d4b\u8bd5\u9a8c\u8bc1\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u5728\u91d1\u878d\u548c\u8425\u9500\u6570\u636e\u96c6\u4e0a\uff0cDyCAST-Net\u4f18\u4e8eTCDF\u3001GCFormer\u548cCausalFormer\uff0c\u80fd\u66f4\u7cbe\u786e\u4f30\u8ba1\u56e0\u679c\u5ef6\u8fdf\u5e76\u51cf\u5c11\u8bef\u53d1\u73b0\u3002", "conclusion": "DyCAST-Net\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u70ed\u56fe\u63ed\u793a\u9690\u85cf\u56e0\u679c\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u52a8\u6001\u73af\u5883\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.09440", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09440", "abs": "https://arxiv.org/abs/2507.09440", "authors": ["Joshua Hill", "Benjamin Eyre", "Elliot Creager"], "title": "Transformers Don't In-Context Learn Least Squares Regression", "comment": "21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "In-context learning (ICL) has emerged as a powerful capability of large\npretrained transformers, enabling them to solve new tasks implicit in example\ninput-output pairs without any gradient updates. Despite its practical success,\nthe mechanisms underlying ICL remain largely mysterious. In this work we study\nsynthetic linear regression to probe how transformers implement learning at\ninference time. Previous works have demonstrated that transformers match the\nperformance of learning rules such as Ordinary Least Squares (OLS) regression\nor gradient descent and have suggested ICL is facilitated in transformers\nthrough the learned implementation of one of these techniques. In this work, we\ndemonstrate through a suite of out-of-distribution generalization experiments\nthat transformers trained for ICL fail to generalize after shifts in the prompt\ndistribution, a behaviour that is inconsistent with the notion of transformers\nimplementing algorithms such as OLS. Finally, we highlight the role of the\npretraining corpus in shaping ICL behaviour through a spectral analysis of the\nlearned representations in the residual stream. Inputs from the same\ndistribution as the training data produce representations with a unique\nspectral signature: inputs from this distribution tend to have the same top two\nsingular vectors. This spectral signature is not shared by out-of-distribution\ninputs, and a metric characterizing the presence of this signature is highly\ncorrelated with low loss.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u53d8\u538b\u5668\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4e2d\u7684\u673a\u5236\uff0c\u53d1\u73b0\u5176\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u5206\u5e03\uff0c\u4e14\u65e0\u6cd5\u5b9e\u73b0\u7c7b\u4f3cOLS\u56de\u5f52\u7684\u7b97\u6cd5\u3002", "motivation": "\u63a2\u7d22\u53d8\u538b\u5668\u5728ICL\u4e2d\u7684\u5b66\u4e60\u673a\u5236\uff0c\u63ed\u793a\u5176\u6cdb\u5316\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5408\u6210\u7ebf\u6027\u56de\u5f52\u4efb\u52a1\u548c\u5206\u5e03\u5916\u6cdb\u5316\u5b9e\u9a8c\uff0c\u5206\u6790\u53d8\u538b\u5668\u7684\u5b66\u4e60\u884c\u4e3a\u3002", "result": "\u53d8\u538b\u5668\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u5b66\u4e60\u884c\u4e3a\u4e0eOLS\u7b49\u7b97\u6cd5\u4e0d\u4e00\u81f4\uff1b\u9884\u8bad\u7ec3\u8bed\u6599\u5bf9ICL\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u53d8\u538b\u5668\u7684ICL\u80fd\u529b\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u5176\u5b66\u4e60\u673a\u5236\u4e0e\u7ecf\u5178\u7b97\u6cd5\u4e0d\u540c\uff0c\u9884\u8bad\u7ec3\u8bed\u6599\u5bf9\u5176\u884c\u4e3a\u8d77\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.09443", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.09443", "abs": "https://arxiv.org/abs/2507.09443", "authors": ["Luiz Aldeia Machado", "Victor Coppo Leite", "Elia Merzari", "Arthur Motta", "Roberto Ponciroli", "Lander Ibarra", "Lise Charlot"], "title": "Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components", "comment": "Preprint - Nureth 21 paper", "summary": "Proactive maintenance strategies, such as Predictive Maintenance (PdM), play\nan important role in the operation of Nuclear Power Plants (NPPs), particularly\ndue to their capacity to reduce offline time by preventing unexpected shutdowns\ncaused by component failures.\n  In this work, we explore the use of a Convolutional Neural Network (CNN)\narchitecture combined with a computational thermomechanical model to calculate\nthe temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel\nrod during operation. This estimation relies on a limited number of temperature\nmeasurements from the cladding's outer surface. This methodology can\npotentially aid in developing PdM tools for nuclear reactors by enabling\nreal-time monitoring of such systems.\n  The training, validation, and testing datasets were generated through coupled\nsimulations involving BISON, a finite element-based nuclear fuel performance\ncode, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven\nsimulations, varying the peak linear heat generation rates. Of these, eight\nwere used for training, two for validation, and one for testing.\n  The CNN was trained for over 1,000 epochs without signs of overfitting,\nachieving highly accurate temperature distribution predictions. These were then\nused in a thermomechanical model to determine the stress and strain\ndistribution within the fuel rod.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u8ba1\u7b97\u70ed\u529b\u5b66\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u538b\u6c34\u5806\u71c3\u6599\u68d2\u7684\u6e29\u5ea6\u3001\u5e94\u529b\u548c\u5e94\u53d8\uff0c\u4ee5\u652f\u6301\u6838\u7535\u7ad9\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff08PdM\uff09\u3002", "motivation": "\u6838\u7535\u7ad9\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff08PdM\uff09\u80fd\u51cf\u5c11\u56e0\u7ec4\u4ef6\u6545\u969c\u5bfc\u81f4\u7684\u610f\u5916\u505c\u673a\u65f6\u95f4\uff0c\u56e0\u6b64\u5f00\u53d1\u5b9e\u65f6\u76d1\u6d4b\u5de5\u5177\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528CNN\u67b6\u6784\u7ed3\u5408\u8ba1\u7b97\u70ed\u529b\u5b66\u6a21\u578b\uff0c\u57fa\u4e8e\u71c3\u6599\u68d2\u5305\u58f3\u5916\u8868\u9762\u7684\u6709\u9650\u6e29\u5ea6\u6d4b\u91cf\u6570\u636e\uff0c\u9884\u6d4b\u6e29\u5ea6\u3001\u5e94\u529b\u548c\u5e94\u53d8\u5206\u5e03\u3002\u6570\u636e\u96c6\u901a\u8fc7BISON\u548cMOOSE-THM\u8026\u5408\u6a21\u62df\u751f\u6210\u3002", "result": "CNN\u8bad\u7ec3\u8d85\u8fc71,000\u8f6e\u672a\u51fa\u73b0\u8fc7\u62df\u5408\uff0c\u6e29\u5ea6\u5206\u5e03\u9884\u6d4b\u51c6\u786e\u5ea6\u9ad8\uff0c\u8fdb\u4e00\u6b65\u7528\u4e8e\u70ed\u529b\u5b66\u6a21\u578b\u8ba1\u7b97\u5e94\u529b\u548c\u5e94\u53d8\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6838\u53cd\u5e94\u5806\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u63d0\u4f9b\u4e86\u6f5c\u5728\u5de5\u5177\uff0c\u5b9e\u73b0\u4e86\u5bf9\u71c3\u6599\u68d2\u72b6\u6001\u7684\u5b9e\u65f6\u76d1\u6d4b\u3002"}}
{"id": "2507.09460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09460", "abs": "https://arxiv.org/abs/2507.09460", "authors": ["Noah Marchal", "William E. Janes", "Mihail Popescu", "Xing Song"], "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring", "comment": "31 pages, 8 Figures", "summary": "Clinical monitoring of functional decline in ALS relies on periodic\nassessments that may miss critical changes occurring between visits. To address\nthis gap, semi-supervised regression models were developed to estimate rates of\ndecline in a case series cohort by targeting ALSFRS- R scale trajectories with\ncontinuous in-home sensor monitoring data. Our analysis compared three model\nparadigms (individual batch learning and cohort-level batch versus incremental\nfine-tuned transfer learning) across linear slope, cubic polynomial, and\nensembled self-attention pseudo-label interpolations. Results revealed cohort\nhomogeneity across functional domains responding to learning methods, with\ntransfer learning improving prediction error for ALSFRS-R subscales in 28 of 32\ncontrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting\nthe composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention\ninterpolation achieved the lowest prediction error for subscale-level models\n(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,\noutperforming linear and cubic interpolations in 20 of 32 contrasts, though\nlinear interpolation proved more stable in all ALSFRS-R composite scale models\n(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity\nprofiles across functional domains with respiratory and speech exhibiting\npatient-specific patterns benefiting from personalized incremental adaptation,\nwhile swallowing and dressing functions followed cohort-level trajectories\nsuitable for transfer models. These findings suggest that matching learning and\npseudo-labeling techniques to functional domain-specific\nhomogeneity-heterogeneity profiles enhances predictive accuracy in ALS\nprogression tracking. Integrating adaptive model selection within sensor\nmonitoring platforms could enable timely interventions and scalable deployment\nin future multi-center studies.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86\u534a\u76d1\u7763\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u5bb6\u5ead\u4f20\u611f\u5668\u6570\u636e\u4f30\u8ba1ALS\u529f\u80fd\u8870\u9000\u7387\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u6a21\u578b\u8303\u5f0f\uff0c\u53d1\u73b0\u8fc1\u79fb\u5b66\u4e60\u5728\u5b50\u91cf\u8868\u9884\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800c\u81ea\u6ce8\u610f\u529b\u63d2\u503c\u5728\u590d\u6742\u975e\u7ebf\u6027\u6a21\u5f0f\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u4e34\u5e8a\u76d1\u6d4bALS\u529f\u80fd\u8870\u9000\u7684\u95f4\u6b47\u6027\u8bc4\u4f30\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u53d8\u5316\uff0c\u9700\u901a\u8fc7\u8fde\u7eed\u5bb6\u5ead\u4f20\u611f\u5668\u6570\u636e\u6539\u8fdb\u9884\u6d4b\u3002", "method": "\u6bd4\u8f83\u4e86\u4e09\u79cd\u6a21\u578b\u8303\u5f0f\uff08\u4e2a\u4f53\u6279\u91cf\u5b66\u4e60\u3001\u961f\u5217\u6279\u91cf\u5b66\u4e60\u4e0e\u589e\u91cf\u5fae\u8c03\u8fc1\u79fb\u5b66\u4e60\uff09\u53ca\u4e09\u79cd\u63d2\u503c\u65b9\u6cd5\uff08\u7ebf\u6027\u659c\u7387\u3001\u7acb\u65b9\u591a\u9879\u5f0f\u3001\u81ea\u6ce8\u610f\u529b\u63d2\u503c\uff09\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u572832\u6b21\u5bf9\u6bd4\u4e2d\u768428\u6b21\u4e2d\u6539\u5584\u4e86\u5b50\u91cf\u8868\u9884\u6d4b\u8bef\u5dee\uff08\u5e73\u5747RMSE=0.20\uff09\uff0c\u81ea\u6ce8\u610f\u529b\u63d2\u503c\u5728\u5b50\u91cf\u8868\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\uff08\u5e73\u5747RMSE=0.19\uff09\u3002", "conclusion": "\u6839\u636e\u529f\u80fd\u57df\u7684\u540c\u8d28-\u5f02\u8d28\u7279\u5f81\u5339\u914d\u5b66\u4e60\u6280\u672f\u53ef\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u672a\u6765\u53ef\u96c6\u6210\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u4ee5\u5b9e\u73b0\u53ca\u65f6\u5e72\u9884\u3002"}}
{"id": "2507.09466", "categories": ["cs.LG", "q-bio.QM", "I.2.1"], "pdf": "https://arxiv.org/pdf/2507.09466", "abs": "https://arxiv.org/abs/2507.09466", "authors": ["Tomas Geffner", "Kieran Didi", "Zhonglin Cao", "Danny Reidenbach", "Zuobai Zhang", "Christian Dallago", "Emine Kucukbenli", "Karsten Kreis", "Arash Vahdat"], "title": "La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching", "comment": null, "summary": "Recently, many generative models for de novo protein structure design have\nemerged. Yet, only few tackle the difficult task of directly generating fully\natomistic structures jointly with the underlying amino acid sequence. This is\nchallenging, for instance, because the model must reason over side chains that\nchange in length during generation. We introduce La-Proteina for atomistic\nprotein design based on a novel partially latent protein representation: coarse\nbackbone structure is modeled explicitly, while sequence and atomistic details\nare captured via per-residue latent variables of fixed dimensionality, thereby\neffectively side-stepping challenges of explicit side-chain representations.\nFlow matching in this partially latent space then models the joint distribution\nover sequences and full-atom structures. La-Proteina achieves state-of-the-art\nperformance on multiple generation benchmarks, including all-atom\nco-designability, diversity, and structural validity, as confirmed through\ndetailed structural analyses and evaluations. Notably, La-Proteina also\nsurpasses previous models in atomistic motif scaffolding performance, unlocking\ncritical atomistic structure-conditioned protein design tasks. Moreover,\nLa-Proteina is able to generate co-designable proteins of up to 800 residues, a\nregime where most baselines collapse and fail to produce valid samples,\ndemonstrating La-Proteina's scalability and robustness.", "AI": {"tldr": "La-Proteina\u662f\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u6f5c\u5728\u86cb\u767d\u8d28\u8868\u793a\u7684\u5168\u539f\u5b50\u86cb\u767d\u8d28\u8bbe\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u56fa\u5b9a\u7ef4\u5ea6\u7684\u6b8b\u57fa\u6f5c\u5728\u53d8\u91cf\u6355\u83b7\u5e8f\u5217\u548c\u539f\u5b50\u7ec6\u8282\uff0c\u89e3\u51b3\u4e86\u4fa7\u94fe\u957f\u5ea6\u53d8\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u751f\u6210\u5168\u539f\u5b50\u7ed3\u6784\u4e0e\u6c28\u57fa\u9178\u5e8f\u5217\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4fa7\u94fe\u957f\u5ea6\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u90e8\u5206\u6f5c\u5728\u86cb\u767d\u8d28\u8868\u793a\uff0c\u663e\u5f0f\u5efa\u6a21\u7c97\u7c92\u5ea6\u4e3b\u94fe\u7ed3\u6784\uff0c\u901a\u8fc7\u56fa\u5b9a\u7ef4\u5ea6\u7684\u6b8b\u57fa\u6f5c\u5728\u53d8\u91cf\u6355\u83b7\u5e8f\u5217\u548c\u539f\u5b50\u7ec6\u8282\uff0c\u5229\u7528\u6d41\u5339\u914d\u5efa\u6a21\u8054\u5408\u5206\u5e03\u3002", "result": "\u5728\u591a\u4e2a\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ec\u5168\u539f\u5b50\u5171\u8bbe\u8ba1\u6027\u3001\u591a\u6837\u6027\u548c\u7ed3\u6784\u6709\u6548\u6027\uff0c\u5e76\u80fd\u751f\u6210\u957f\u8fbe800\u4e2a\u6b8b\u57fa\u7684\u86cb\u767d\u8d28\u3002", "conclusion": "La-Proteina\u5728\u539f\u5b50\u7ea7\u86cb\u767d\u8d28\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u539f\u5b50\u7ed3\u6784\u6761\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.09480", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.09480", "abs": "https://arxiv.org/abs/2507.09480", "authors": ["Guoyou Wang", "Yihua Tan", "Shiqi Liu"], "title": "Discrete Differential Principle for Continuous Smooth Function Representation", "comment": null, "summary": "Taylor's formula holds significant importance in function representation,\nsuch as solving differential difference equations, ordinary differential\nequations, partial differential equations, and further promotes applications in\nvisual perception, complex control, fluid mechanics, weather forecasting and\nthermodynamics. However, the Taylor's formula suffers from the curse of\ndimensionality and error propagation during derivative computation in discrete\nsituations. In this paper, we propose a new discrete differential operator to\nestimate derivatives and to represent continuous smooth function locally using\nthe Vandermonde coefficient matrix derived from truncated Taylor series. Our\nmethod simultaneously computes all derivatives of orders less than the number\nof sample points, inherently mitigating error propagation. Utilizing\nequidistant uniform sampling, it achieves high-order accuracy while alleviating\nthe curse of dimensionality. We mathematically establish rigorous error bounds\nfor both derivative estimation and function representation, demonstrating\ntighter bounds for lower-order derivatives. We extend our method to the\ntwo-dimensional case, enabling its use for multivariate derivative\ncalculations. Experiments demonstrate the effectiveness and superiority of the\nproposed method compared to the finite forward difference method for derivative\nestimation and cubic spline and linear interpolation for function\nrepresentation. Consequently, our technique offers broad applicability across\ndomains such as vision representation, feature extraction, fluid mechanics, and\ncross-media imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u5fae\u5206\u7b97\u5b50\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8303\u5fb7\u8499\u7cfb\u6570\u77e9\u9635\u4f30\u8ba1\u5bfc\u6570\u5e76\u5c40\u90e8\u8868\u793a\u8fde\u7eed\u5149\u6ed1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u6cf0\u52d2\u516c\u5f0f\u7684\u7ef4\u6570\u707e\u96be\u548c\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\u3002", "motivation": "\u6cf0\u52d2\u516c\u5f0f\u5728\u51fd\u6570\u8868\u793a\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u5728\u79bb\u6563\u60c5\u51b5\u4e0b\u5b58\u5728\u7ef4\u6570\u707e\u96be\u548c\u5bfc\u6570\u8ba1\u7b97\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u5229\u7528\u622a\u65ad\u6cf0\u52d2\u7ea7\u6570\u5bfc\u51fa\u7684\u8303\u5fb7\u8499\u7cfb\u6570\u77e9\u9635\uff0c\u63d0\u51fa\u4e00\u79cd\u79bb\u6563\u5fae\u5206\u7b97\u5b50\uff0c\u540c\u65f6\u8ba1\u7b97\u6240\u6709\u4f4e\u4e8e\u91c7\u6837\u70b9\u6570\u7684\u5bfc\u6570\u9636\u6570\uff0c\u51cf\u8f7b\u8bef\u5dee\u4f20\u64ad\u3002", "result": "\u6570\u5b66\u4e0a\u4e25\u683c\u8bc1\u660e\u4e86\u5bfc\u6570\u4f30\u8ba1\u548c\u51fd\u6570\u8868\u793a\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5bfc\u6570\u4f30\u8ba1\u548c\u51fd\u6570\u8868\u793a\u4e0a\u4f18\u4e8e\u6709\u9650\u524d\u5411\u5dee\u5206\u3001\u4e09\u6b21\u6837\u6761\u548c\u7ebf\u6027\u63d2\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8868\u793a\u3001\u7279\u5f81\u63d0\u53d6\u3001\u6d41\u4f53\u529b\u5b66\u548c\u8de8\u5a92\u4f53\u6210\u50cf\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.09523", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09523", "abs": "https://arxiv.org/abs/2507.09523", "authors": ["Brett Daley", "Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State Values", "comment": "Published at RLC/RLJ 2025", "summary": "The hallmark feature of temporal-difference (TD) learning is bootstrapping:\nusing value predictions to generate new value predictions. The vast majority of\nTD methods for control learn a policy by bootstrapping from a single\naction-value function (e.g., Q-learning and Sarsa). Significantly less\nattention has been given to methods that bootstrap from two asymmetric value\nfunctions: i.e., methods that learn state values as an intermediate step in\nlearning action values. Existing algorithms in this vein can be categorized as\neither QV-learning or AV-learning. Though these algorithms have been\ninvestigated to some degree in prior work, it remains unclear if and when it is\nadvantageous to learn two value functions instead of just one -- and whether\nsuch approaches are theoretically sound in general. In this paper, we analyze\nthese algorithmic families in terms of convergence and sample efficiency. We\nfind that while both families are more efficient than Expected Sarsa in the\nprediction setting, only AV-learning methods offer any major benefit over\nQ-learning in the control setting. Finally, we introduce a new AV-learning\nalgorithm called Regularized Dueling Q-learning (RDQ), which significantly\noutperforms Dueling DQN in the MinAtar benchmark.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u4e24\u4e2a\u975e\u5bf9\u79f0\u4ef7\u503c\u51fd\u6570\u7684TD\u5b66\u4e60\u65b9\u6cd5\uff08QV-learning\u548cAV-learning\uff09\uff0c\u53d1\u73b0AV-learning\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u4f18\u4e8eQ-learning\uff0c\u5e76\u63d0\u51fa\u65b0\u7b97\u6cd5RDQ\uff0c\u6027\u80fd\u4f18\u4e8eDueling DQN\u3002", "motivation": "\u63a2\u8ba8\u5b66\u4e60\u4e24\u4e2a\u4ef7\u503c\u51fd\u6570\uff08\u800c\u975e\u5355\u4e00\u52a8\u4f5c\u4ef7\u503c\u51fd\u6570\uff09\u662f\u5426\u5177\u6709\u4f18\u52bf\u53ca\u5176\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u5206\u6790QV-learning\u548cAV-learning\u7684\u6536\u655b\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u5e76\u5f15\u5165\u65b0\u7b97\u6cd5Regularized Dueling Q-learning (RDQ)\u3002", "result": "AV-learning\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eQ-learning\uff1bRDQ\u5728MinAtar\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eDueling DQN\u3002", "conclusion": "AV-learning\u65b9\u6cd5\u5728\u63a7\u5236\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0cRDQ\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09545", "abs": "https://arxiv.org/abs/2507.09545", "authors": ["Ilaria Vascotto", "Valentina Blasone", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events", "comment": "Late Breaking Work presented at the 3rd World Conference on\n  eXplainable Artificial Intelligence (XAI2025)", "summary": "The usage of eXplainable Artificial Intelligence (XAI) methods has become\nessential in practical applications, given the increasing deployment of\nArtificial Intelligence (AI) models and the legislative requirements put\nforward in the latest years. A fundamental but often underestimated aspect of\nthe explanations is their robustness, a key property that should be satisfied\nin order to trust the explanations. In this study, we provide some preliminary\ninsights on evaluating the reliability of explanations in the specific case of\nunbalanced datasets, which are very frequent in high-risk use-cases, but at the\nsame time considerably challenging for both AI models and XAI methods. We\npropose a simple evaluation focused on the minority class (i.e. the less\nfrequent one) that leverages on-manifold generation of neighbours, explanation\naggregation and a metric to test explanation consistency. We present a use-case\nbased on a tabular dataset with numerical features focusing on the occurrence\nof frost events.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u8bc4\u4f30XAI\u65b9\u6cd5\u89e3\u91ca\u53ef\u9760\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5c11\u6570\u7c7b\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u7acb\u6cd5\u8981\u6c42\u7684\u589e\u52a0\uff0c\u89e3\u91ca\u7684\u9c81\u68d2\u6027\u6210\u4e3a\u5173\u952e\uff0c\u4f46\u6570\u636e\u4e0d\u5e73\u8861\uff08\u5982\u9ad8\u98ce\u9669\u7528\u4f8b\u4e2d\u5e38\u89c1\uff09\u5bf9AI\u548cXAI\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c11\u6570\u7c7b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u90bb\u57df\u751f\u6210\u3001\u89e3\u91ca\u805a\u5408\u548c\u4e00\u81f4\u6027\u5ea6\u91cf\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u6570\u503c\u7279\u5f81\u8868\u683c\u6570\u636e\u96c6\u7684\u971c\u51bb\u4e8b\u4ef6\u6848\u4f8b\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0bXAI\u89e3\u91ca\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\u548c\u65b9\u6cd5\u3002"}}
{"id": "2507.09565", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09565", "abs": "https://arxiv.org/abs/2507.09565", "authors": ["Heeba Shakeel", "Tanvir Ahmad", "Chandni Saxena"], "title": "Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives", "comment": "7 Pages", "summary": "We introduce a dataset for classifying wellness dimensions in social media\nuser posts, covering six key aspects: physical, emotional, social,\nintellectual, spiritual, and vocational. The dataset is designed to capture\nthese dimensions in user-generated content, with a comprehensive annotation\nframework developed under the guidance of domain experts. This framework allows\nfor the classification of text spans into the appropriate wellness categories.\nWe evaluate both traditional machine learning models and advanced\ntransformer-based models for this multi-class classification task, with\nperformance assessed using precision, recall, and F1-score, averaged over\n10-fold cross-validation. Post-hoc explanations are applied to ensure the\ntransparency and interpretability of model decisions. The proposed dataset\ncontributes to region-specific wellness assessments in social media and paves\nthe way for personalized well-being evaluations and early intervention\nstrategies in mental health. We adhere to ethical considerations for\nconstructing and releasing our experiments and dataset publicly on Github.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u5e16\u5b50\u4e2d\u5065\u5eb7\u7ef4\u5ea6\u5206\u7c7b\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u516d\u4e2a\u5173\u952e\u65b9\u9762\uff0c\u5e76\u8bc4\u4f30\u4e86\u4f20\u7edf\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u8bc4\u4f30\u7528\u6237\u7684\u591a\u7ef4\u5ea6\u5065\u5eb7\u72b6\u6001\uff0c\u4e3a\u4e2a\u6027\u5316\u5065\u5eb7\u8bc4\u4f30\u548c\u5fc3\u7406\u5065\u5eb7\u65e9\u671f\u5e72\u9884\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u5f00\u53d1\u4e86\u5168\u9762\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548cTransformer\u6a21\u578b\uff0c\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u548c\u6027\u80fd\u6307\u6807\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\uff09\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u533a\u57df\u7279\u5b9a\u7684\u5065\u5eb7\u8bc4\u4f30\uff0c\u6a21\u578b\u8868\u73b0\u901a\u8fc7\u540e\u89e3\u91ca\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u793e\u4ea4\u5a92\u4f53\u5065\u5eb7\u8bc4\u4f30\u548c\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u4e86\u5b9e\u9a8c\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2507.09602", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09602", "abs": "https://arxiv.org/abs/2507.09602", "authors": ["Bocheng Ju", "Junchao Fan", "Jiaqi Liu", "Xiaolin Chang"], "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences", "comment": null, "summary": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications.", "AI": {"tldr": "DRAGD\u548cDRAGDP\u653b\u51fb\u5229\u7528\u8054\u90a6\u5b66\u4e60\u4e2d\u68af\u5ea6\u5dee\u5f02\u91cd\u5efa\u88ab\u5220\u9664\u6570\u636e\uff0c\u63ed\u793a\u8054\u90a6\u9057\u5fd8\u7684\u9690\u79c1\u6f0f\u6d1e\u3002", "motivation": "\u8054\u90a6\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u68af\u5ea6\u4ea4\u6362\u53ef\u80fd\u6cc4\u9732\u654f\u611f\u6570\u636e\uff0c\u9700\u7814\u7a76\u5176\u9690\u79c1\u98ce\u9669\u3002", "method": "\u63d0\u51faDRAGD\u653b\u51fb\uff0c\u5229\u7528\u9057\u5fd8\u524d\u540e\u68af\u5ea6\u5dee\u5f02\u91cd\u5efa\u6570\u636e\uff1bDRAGDP\u8fdb\u4e00\u6b65\u7ed3\u5408\u516c\u5f00\u6570\u636e\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDRAGD\u548cDRAGDP\u5728\u6570\u636e\u91cd\u5efa\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8054\u90a6\u9057\u5fd8\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u5347\u5b89\u5168\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09616", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09616", "abs": "https://arxiv.org/abs/2507.09616", "authors": ["Ofir Gordon", "Ariel Lapid", "Elad Cohen", "Yarden Yagil", "Arnon Netzer", "Hai Victor Habi"], "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression", "comment": null, "summary": "Deploying transformer-based neural networks on resource-constrained edge\ndevices presents a significant challenge. This challenge is often addressed\nthrough various techniques, such as low-rank approximation and mixed-precision\nquantization. In this work, we introduce Mixed Low-Rank and Quantization\n(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a\ntwo-stage optimization process to determine optimal bit-width and rank\nassignments for each layer, adhering to predefined memory constraints. This\nprocess includes: (i) an intra-layer optimization that identifies potentially\noptimal compression solutions out of all low-rank and quantization\ncombinations; (ii) an inter-layer optimization that assigns bit-width precision\nand rank to each layer while ensuring the memory constraint is met. An optional\nfinal step applies a sequential optimization process using a modified adaptive\nrounding technique to mitigate compression-induced errors in joint low-rank\napproximation and quantization. The method is compatible and can be seamlessly\nintegrated with most existing quantization algorithms. MLoRQ shows\nstate-of-the-art results with up to 15\\% performance improvement, evaluated on\nVision Transformers for image classification, object detection, and instance\nsegmentation tasks.", "AI": {"tldr": "MLoRQ\u662f\u4e00\u79cd\u7ed3\u5408\u4f4e\u79e9\u8fd1\u4f3c\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72Transformer\u7f51\u7edc\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72Transformer\u7f51\u7edc\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u6280\u672f\u5982\u4f4e\u79e9\u8fd1\u4f3c\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "method": "MLoRQ\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u5c42\u5185\u4f18\u5316\u7b5b\u9009\u538b\u7f29\u65b9\u6848\uff0c\u5c42\u95f4\u4f18\u5316\u5206\u914d\u4f4d\u5bbd\u548c\u79e9\uff1b\u53ef\u9009\u6b65\u9aa4\u901a\u8fc7\u81ea\u9002\u5e94\u820d\u5165\u51cf\u5c11\u8bef\u5dee\u3002", "result": "MLoRQ\u5728\u89c6\u89c9Transformer\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe15%\u3002", "conclusion": "MLoRQ\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2507.09650", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09650", "abs": "https://arxiv.org/abs/2507.09650", "authors": ["Lily Hong Zhang", "Smitha Milli", "Karen Jusko", "Jonathan Smith", "Brandon Amos", "Wassim", "Bouaziz", "Manon Revel", "Jack Kussman", "Lisa Titus", "Bhaktipriya Radharapu", "Jane Yu", "Vidya Sarma", "Kris Rose", "Maximilian Nickel"], "title": "Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset", "comment": null, "summary": "How can large language models (LLMs) serve users with varying preferences\nthat may conflict across cultural, political, or other dimensions? To advance\nthis challenge, this paper establishes four key results. First, we demonstrate,\nthrough a large-scale multilingual human study with representative samples from\nfive countries (N=15,000), that humans exhibit significantly more variation in\npreferences than the responses of 21 state-of-the-art LLMs. Second, we show\nthat existing methods for preference dataset collection are insufficient for\nlearning the diversity of human preferences even along two of the most salient\ndimensions of variability in global values, due to the underlying homogeneity\nof candidate responses. Third, we argue that this motivates the need for\nnegatively-correlated sampling when generating candidate sets, and we show that\nsimple prompt-based techniques for doing so significantly enhance the\nperformance of alignment methods in learning heterogeneous preferences. Fourth,\nbased on this novel candidate sampling approach, we collect and open-source\nCommunity Alignment, the largest and most representative multilingual and\nmulti-turn preference dataset to date, featuring almost 200,000 comparisons\nfrom annotators spanning five countries. We hope that the Community Alignment\ndataset will be a valuable resource for improving the effectiveness of LLMs for\na diverse global population.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u4eba\u7c7b\u7814\u7a76\uff0c\u53d1\u73b0\u4eba\u7c7b\u504f\u597d\u6bd4\u73b0\u6709LLMs\u66f4\u591a\u6837\u5316\uff0c\u63d0\u51fa\u8d1f\u76f8\u5173\u91c7\u6837\u65b9\u6cd5\u63d0\u5347\u5bf9\u9f50\u6548\u679c\uff0c\u5e76\u5f00\u6e90\u4e86\u6700\u5927\u7684\u591a\u8bed\u8a00\u504f\u597d\u6570\u636e\u96c6\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u8ba9LLMs\u670d\u52a1\u5177\u6709\u4e0d\u540c\u6587\u5316\u3001\u653f\u6cbb\u7b49\u51b2\u7a81\u504f\u597d\u7684\u7528\u6237\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u4eba\u7c7b\u504f\u597d\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u591a\u56fd\u4eba\u7c7b\u7814\u7a76\u6bd4\u8f83LLMs\u4e0e\u4eba\u7c7b\u504f\u597d\u5dee\u5f02\uff0c\u63d0\u51fa\u8d1f\u76f8\u5173\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u591a\u8bed\u8a00\u591a\u8f6e\u504f\u597d\u6570\u636e\u96c6\u3002", "result": "\u53d1\u73b0\u4eba\u7c7b\u504f\u597d\u6bd4LLMs\u66f4\u591a\u6837\u5316\uff0c\u8d1f\u76f8\u5173\u91c7\u6837\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6548\u679c\uff0c\u5f00\u6e90\u4e86\u6700\u5927\u7684\u591a\u8bed\u8a00\u504f\u597d\u6570\u636e\u96c6\u3002", "conclusion": "\u8d1f\u76f8\u5173\u91c7\u6837\u548c\u5f00\u6e90\u6570\u636e\u96c6\u6709\u52a9\u4e8eLLMs\u66f4\u597d\u5730\u670d\u52a1\u5168\u7403\u591a\u6837\u5316\u7528\u6237\u3002"}}
{"id": "2507.09678", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.09678", "abs": "https://arxiv.org/abs/2507.09678", "authors": ["Alexander David Balinsky", "Dominik Krzeminski", "Alexander Balinsky"], "title": "Conformal Prediction for Privacy-Preserving Machine Learning", "comment": null, "summary": "We investigate the integration of Conformal Prediction (CP) with supervised\nlearning on deterministically encrypted data, aiming to bridge the gap between\nrigorous uncertainty quantification and privacy-preserving machine learning.\nUsing AES-encrypted variants of the MNIST dataset, we demonstrate that CP\nmethods remain effective even when applied directly in the encrypted domain,\nowing to the preservation of data exchangeability under fixed-key encryption.\nWe test traditional $p$-value-based against $e$-value-based conformal\npredictors. Our empirical evaluation reveals that models trained on\ndeterministically encrypted data retain the ability to extract meaningful\nstructure, achieving 36.88\\% test accuracy -- significantly above random\nguessing (9.56\\%) observed with per-instance encryption. Moreover,\n$e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3\nloss-threshold calibration, correctly capturing the true label in 4888 out of\n5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive\nsets but with reduced coverage accuracy. These findings highlight both the\npromise and limitations of CP in encrypted data settings and underscore\ncritical trade-offs between prediction set compactness and reliability. %Our\nwork sets a foundation for principled uncertainty quantification in secure,\nprivacy-aware learning systems.", "AI": {"tldr": "\u7814\u7a76\u5c06Conformal Prediction (CP)\u4e0e\u786e\u5b9a\u6027\u52a0\u5bc6\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u7ed3\u5408\uff0c\u4ee5\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u4e25\u683c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528AES\u52a0\u5bc6\u7684MNIST\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4f20\u7edf$p$\u503c\u57fa\u548c$e$\u503c\u57fa\u7684CP\u65b9\u6cd5\u3002", "result": "\u52a0\u5bc6\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u4ecd\u80fd\u63d0\u53d6\u6709\u610f\u4e49\u7ed3\u6784\uff0c\u6d4b\u8bd5\u51c6\u786e\u738736.88%\uff0c$e$\u503c\u57faCP\u8986\u76d6\u7387\u8fbe60%\u4ee5\u4e0a\u3002", "conclusion": "CP\u5728\u52a0\u5bc6\u6570\u636e\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6743\u8861\u9884\u6d4b\u96c6\u7d27\u51d1\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.09683", "categories": ["cs.LG", "cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2507.09683", "abs": "https://arxiv.org/abs/2507.09683", "authors": ["Michael Kearns", "Aaron Roth", "Emily Ryu"], "title": "Networked Information Aggregation via Machine Learning", "comment": null, "summary": "We study a distributed learning problem in which learning agents are embedded\nin a directed acyclic graph (DAG). There is a fixed and arbitrary distribution\nover feature/label pairs, and each agent or vertex in the graph is able to\ndirectly observe only a subset of the features -- potentially a different\nsubset for every agent. The agents learn sequentially in some order consistent\nwith a topological sort of the DAG, committing to a model mapping observations\nto predictions of the real-valued label. Each agent observes the predictions of\ntheir parents in the DAG, and trains their model using both the features of the\ninstance that they directly observe, and the predictions of their parents as\nadditional features. We ask when this process is sufficient to achieve\n\\emph{information aggregation}, in the sense that some agent in the DAG is able\nto learn a model whose error is competitive with the best model that could have\nbeen learned (in some hypothesis class) with direct access to \\emph{all}\nfeatures, despite the fact that no single agent in the network has such access.\nWe give upper and lower bounds for this problem for both linear and general\nhypothesis classes. Our results identify the \\emph{depth} of the DAG as the key\nparameter: information aggregation can occur over sufficiently long paths in\nthe DAG, assuming that all of the relevant features are well represented along\nthe path, and there are distributions over which information aggregation cannot\noccur even in the linear case, and even in arbitrarily large DAGs that do not\nhave sufficient depth (such as a hub-and-spokes topology in which the spoke\nvertices collectively see all the features). We complement our theoretical\nresults with a comprehensive set of experiments.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728DAG\u4e2d\u5206\u5e03\u5f0f\u5b66\u4e60\u95ee\u9898\uff0c\u63a2\u8ba8\u4e86\u4fe1\u606f\u805a\u5408\u7684\u6761\u4ef6\uff0c\u53d1\u73b0DAG\u6df1\u5ea6\u662f\u5173\u952e\u53c2\u6570\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u4fe1\u606f\u805a\u5408\u7684\u95ee\u9898\uff0c\u5373\u5982\u4f55\u5728\u90e8\u5206\u7279\u5f81\u53ef\u89c1\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7DAG\u7ed3\u6784\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u901a\u8fc7DAG\u62d3\u6251\u6392\u5e8f\u987a\u5e8f\u5b66\u4e60\uff0c\u6bcf\u4e2a\u4ee3\u7406\u5229\u7528\u81ea\u8eab\u89c2\u5bdf\u7684\u7279\u5f81\u548c\u7236\u8282\u70b9\u7684\u9884\u6d4b\u4f5c\u4e3a\u989d\u5916\u7279\u5f81\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u7406\u8bba\u4e0a\u4e0b\u754c\u8868\u660eDAG\u6df1\u5ea6\u662f\u5173\u952e\uff0c\u4fe1\u606f\u805a\u5408\u5728\u8db3\u591f\u957f\u7684\u8def\u5f84\u4e0a\u53ef\u5b9e\u73b0\uff0c\u4f46\u67d0\u4e9b\u62d3\u6251\u7ed3\u6784\uff08\u5982\u661f\u578b\uff09\u65e0\u6cd5\u5b9e\u73b0\u3002", "conclusion": "DAG\u6df1\u5ea6\u662f\u4fe1\u606f\u805a\u5408\u7684\u51b3\u5b9a\u56e0\u7d20\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002"}}
{"id": "2507.09687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09687", "abs": "https://arxiv.org/abs/2507.09687", "authors": ["Md Mushfiqur Rahaman", "Elliot Chang", "Tasmiah Haque", "Srinjoy Das"], "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness", "comment": null, "summary": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0fLSTM\u6587\u672c\u5206\u7c7b\u6a21\u578b\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u91cf\u5316\u8868\u73b0\uff0c\u53d1\u73b0\u751f\u6210\u5f0f\u6a21\u578b\u5bf9\u91cf\u5316\u4f4d\u5bbd\u548c\u6821\u51c6\u6570\u636e\u66f4\u654f\u611f\uff0c\u5c24\u5176\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\u7684\u9700\u6c42\u4fc3\u4f7f\u7814\u7a76\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u5206\u7c7b\u6a21\u578b\u5728\u91cf\u5316\u540e\u7684\u8868\u73b0\uff0c\u4ee5\u4f18\u5316\u90e8\u7f72\u3002", "method": "\u4f7f\u7528Brevitas\u91cf\u5316\u5e93\u5bf9\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0fLSTM\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u91cf\u5316\uff0c\u8bc4\u4f30\u4e0d\u540c\u4f4d\u5bbd\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5224\u522b\u5f0f\u6a21\u578b\u8868\u73b0\u7a33\u5065\uff0c\u751f\u6210\u5f0f\u6a21\u578b\u5bf9\u4f4d\u5bbd\u3001\u6821\u51c6\u6570\u636e\u548c\u8f93\u5165\u566a\u58f0\u654f\u611f\uff0c\u7c7b\u522b\u4e0d\u5e73\u8861\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u6821\u51c6\u6570\u636e\u5bf9\u91cf\u5316\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u751f\u6210\u5f0f\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e0b\u9700\u8c28\u614e\u90e8\u7f72\u3002"}}
{"id": "2507.09703", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09703", "abs": "https://arxiv.org/abs/2507.09703", "authors": ["Roberto Molinaro", "Niall Siegenheim", "Niels Poulsen", "Jordan Dane Daubinet", "Henry Martin", "Mark Frey", "Kevin Thiart", "Alexander Jakob Dautel", "Andreas Schlueter", "Alex Grigoryev", "Bogdan Danciu", "Nikoo Ekhtiari", "Bas Steunebrink", "Leonie Wagner", "Marvin Vincent Gabler"], "title": "EPT-2 Technical Report", "comment": null, "summary": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)\nfamily of foundation AI models for Earth system forecasting. EPT-2 delivers\nsubstantial improvements over its predecessor, EPT-1.5, and sets a new state of\nthe art in predicting energy-relevant variables-including 10m and 100m wind\nspeed, 2m temperature, and surface solar radiation-across the full 0-240h\nforecast horizon. It consistently outperforms leading AI weather models such as\nMicrosoft Aurora, as well as the operational numerical forecast system IFS HRES\nfrom the European Centre for Medium-Range Weather Forecasts (ECMWF). In\nparallel, we introduce a perturbation-based ensemble model of EPT-2 for\nprobabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly\nsurpasses the ECMWF ENS mean-long considered the gold standard for medium- to\nlongrange forecasting-while operating at a fraction of the computational cost.\nEPT models, as well as third-party forecasts, are accessible via the app.jua.ai\nplatform.", "AI": {"tldr": "EPT-2\u662fEarth Physics Transformer\u7cfb\u5217\u7684\u6700\u65b0AI\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u524d\u4ee3EPT-1.5\uff0c\u5e76\u5728\u9884\u6d4b\u80fd\u6e90\u76f8\u5173\u53d8\u91cf\u4e0a\u8fbe\u5230\u65b0\u6c34\u5e73\u3002", "motivation": "\u63d0\u5347\u5730\u7403\u7cfb\u7edf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u8d85\u8d8a\u73b0\u6709AI\u548c\u6570\u503c\u9884\u62a5\u7cfb\u7edf\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u6270\u52a8\u7684\u96c6\u5408\u6a21\u578bEPT-2e\u8fdb\u884c\u6982\u7387\u9884\u6d4b\u3002", "result": "EPT-2\u548cEPT-2e\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8d85\u8d8aMicrosoft Aurora\u548cECMWF\u7684IFS HRES\u53caENS\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "EPT-2\u7cfb\u5217\u6a21\u578b\u4e3a\u5730\u7403\u7cfb\u7edf\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5e73\u53f0\u5f00\u653e\u8bbf\u95ee\u3002"}}
{"id": "2507.09732", "categories": ["cs.LG", "q-bio.PE", "stat.AP", "68T05, 62H35", "I.5.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09732", "abs": "https://arxiv.org/abs/2507.09732", "authors": ["Sara Si-Moussi", "Stephan Hennekens", "Sander Mucher", "Stan Los", "Wilfried Thuiller"], "title": "Continental scale habitat modelling with artificial intelligence and multimodal earth observation", "comment": null, "summary": "Habitats integrate the abiotic conditions and biophysical structures that\nsupport biodiversity and sustain nature's contributions to people. As these\necosystems face mounting pressure from human activities, accurate,\nhigh-resolution habitat maps are essential for effective conservation and\nrestoration. Yet current maps often fall short in thematic or spatial\nresolution because they must (1) model several mutually exclusive habitat types\nthat co-occur across landscapes and (2) cope with severe class imbalance that\ncomplicate multi-class training. Here, we evaluated how high-resolution remote\nsensing (RS) data and Artificial Intelligence (AI) tools can improve habitat\nclassification over large geographic extents at fine thematic resolution. Using\nvegetation plots from the European Vegetation Archive, we modelled Level 3\nEUNIS habitats across Europe and assessed multiple modelling strategies against\nindependent validation datasets. Strategies that exploited the hierarchical\nnature of habitat nomenclatures resolved classification ambiguities, especially\nin fragmented landscapes. Integrating multi-spectral (MSI) and synthetic\naperture radar (SAR) imagery, particularly through Earth Observation Foundation\nmodels, enhanced within-formation discrimination and overall performance.\nFinally, ensemble machine learning that corrects class imbalance boosted\naccuracy further. Our methodological framework is transferable beyond Europe\nand adaptable to other classification systems. Future research should advance\ntemporal modelling of dynamic habitats, extend to habitat segmentation and\nquality assessment, and exploit next-generation EO data paired with\nhigher-quality in-situ observations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u6570\u636e\u548cAI\u5de5\u5177\u6539\u8fdb\u5927\u8303\u56f4\u7cbe\u7ec6\u751f\u5883\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5730\u56fe\u5728\u4e3b\u9898\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u751f\u6001\u7cfb\u7edf\u7684\u538b\u529b\u589e\u52a0\uff0c\u9ad8\u7cbe\u5ea6\u751f\u5883\u5730\u56fe\u5bf9\u4fdd\u62a4\u548c\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5730\u56fe\u5728\u4e3b\u9898\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u6b27\u6d32\u690d\u88ab\u6863\u6848\u7684\u690d\u88ab\u6570\u636e\uff0c\u7ed3\u5408\u591a\u5149\u8c31\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u5f71\u50cf\uff0c\u91c7\u7528\u5206\u5c42\u751f\u5883\u547d\u540d\u6cd5\u548c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u5206\u5c42\u547d\u540d\u6cd5\u89e3\u51b3\u4e86\u5206\u7c7b\u6a21\u7cca\u95ee\u9898\uff0c\u591a\u6e90\u6570\u636e\u96c6\u6210\u548c\u7c7b\u4e0d\u5e73\u8861\u6821\u6b63\u63d0\u9ad8\u4e86\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u5730\u533a\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u52a8\u6001\u751f\u5883\u7684\u65f6\u95f4\u5efa\u6a21\u548c\u4e0b\u4e00\u4ee3\u9065\u611f\u6570\u636e\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09733", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07, 65M06, 78M34", "I.2.6; I.4.8; J.2"], "pdf": "https://arxiv.org/pdf/2507.09733", "abs": "https://arxiv.org/abs/2507.09733", "authors": ["Bradley Camburn"], "title": "Universal Physics Simulation: A Foundational Diffusion Approach", "comment": "10 pages, 3 figures. Foundational AI model for universal physics\n  simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on\n  electromagnetic field generation without requiring a priori physics encoding", "summary": "We present the first foundational AI model for universal physics simulation\nthat learns physical laws directly from boundary-condition data without\nrequiring a priori equation encoding. Traditional physics-informed neural\nnetworks (PINNs) and finite-difference methods necessitate explicit\nmathematical formulation of governing equations, fundamentally limiting their\ngeneralizability and discovery potential. Our sketch-guided diffusion\ntransformer approach reimagines computational physics by treating simulation as\na conditional generation problem, where spatial boundary conditions guide the\nsynthesis of physically accurate steady-state solutions.\n  By leveraging enhanced diffusion transformer architectures with novel spatial\nrelationship encoding, our model achieves direct boundary-to-equilibrium\nmapping and is generalizable to diverse physics domains. Unlike sequential\ntime-stepping methods that accumulate errors over iterations, our approach\nbypasses temporal integration entirely, directly generating steady-state\nsolutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our\ndata-informed approach enables physics discovery through learned\nrepresentations analyzable via Layer-wise Relevance Propagation (LRP),\nrevealing emergent physical relationships without predetermined mathematical\nconstraints. This work represents a paradigm shift from AI-accelerated physics\nto AI-discovered physics, establishing the first truly universal physics\nsimulation framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u754c\u6761\u4ef6\u6570\u636e\u5b66\u4e60\u7269\u7406\u5b9a\u5f8b\u7684\u901a\u7528\u7269\u7406\u6a21\u62df\u57fa\u7840AI\u6a21\u578b\uff0c\u65e0\u9700\u9884\u5148\u7f16\u7801\u65b9\u7a0b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982PINNs\u548c\u6709\u9650\u5dee\u5206\u6cd5\uff09\u9700\u8981\u663e\u5f0f\u6570\u5b66\u65b9\u7a0b\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u548c\u53d1\u73b0\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u8349\u56fe\u5f15\u5bfc\u7684\u6269\u6563\u53d8\u6362\u5668\u65b9\u6cd5\uff0c\u5c06\u6a21\u62df\u89c6\u4e3a\u6761\u4ef6\u751f\u6210\u95ee\u9898\uff0c\u5229\u7528\u7a7a\u95f4\u8fb9\u754c\u6761\u4ef6\u751f\u6210\u7a33\u6001\u89e3\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u8fb9\u754c\u5230\u7a33\u6001\u7684\u76f4\u63a5\u6620\u5c04\uff0c\u901a\u7528\u6027\u5f3a\uff0cSSIM > 0.8\uff0c\u8fb9\u754c\u7cbe\u5ea6\u9ad8\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5b9e\u73b0\u4e86\u4eceAI\u52a0\u901f\u7269\u7406\u5230AI\u53d1\u73b0\u7269\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u901a\u7528\u7269\u7406\u6a21\u62df\u6846\u67b6\u3002"}}
{"id": "2507.09753", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09753", "abs": "https://arxiv.org/abs/2507.09753", "authors": ["Ewa M. Nowara", "Joshua Rackers", "Patricia Suriana", "Pan Kessel", "Max Shen", "Andrew Martin Watkins", "Michael Maser"], "title": "Do we need equivariant models for molecule generation?", "comment": null, "summary": "Deep generative models are increasingly used for molecular discovery, with\nmost recent approaches relying on equivariant graph neural networks (GNNs)\nunder the assumption that explicit equivariance is essential for generating\nhigh-quality 3D molecules. However, these models are complex, difficult to\ntrain, and scale poorly.\n  We investigate whether non-equivariant convolutional neural networks (CNNs)\ntrained with rotation augmentations can learn equivariance and match the\nperformance of equivariant models. We derive a loss decomposition that\nseparates prediction error from equivariance error, and evaluate how model\nsize, dataset size, and training duration affect performance across denoising,\nmolecule generation, and property prediction. To our knowledge, this is the\nfirst study to analyze learned equivariance in generative tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u975e\u7b49\u53d8CNN\u901a\u8fc7\u65cb\u8f6c\u589e\u5f3a\u662f\u5426\u80fd\u8fbe\u5230\u7b49\u53d8GNN\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u96c6\u5927\u5c0f\u548c\u8bad\u7ec3\u65f6\u957f\u7684\u5f71\u54cd\u3002", "motivation": "\u7b49\u53d8GNN\u590d\u6742\u4e14\u96be\u4ee5\u8bad\u7ec3\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u66f4\u7b80\u5355\u7684\u975e\u7b49\u53d8CNN\u662f\u5426\u80fd\u901a\u8fc7\u5b66\u4e60\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002", "method": "\u4f7f\u7528\u65cb\u8f6c\u589e\u5f3a\u8bad\u7ec3\u975e\u7b49\u53d8CNN\uff0c\u5e76\u901a\u8fc7\u635f\u5931\u5206\u89e3\u5206\u6790\u9884\u6d4b\u8bef\u5dee\u548c\u7b49\u53d8\u8bef\u5dee\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u975e\u7b49\u53d8CNN\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u53ef\u4ee5\u5b66\u4e60\u7b49\u53d8\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u975e\u7b49\u53d8CNN\u901a\u8fc7\u9002\u5f53\u8bad\u7ec3\u53ef\u4ee5\u5339\u914d\u7b49\u53d8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5206\u5b50\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.09754", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.09754", "abs": "https://arxiv.org/abs/2507.09754", "authors": ["Aakash Tripathi", "Ian E. Nielsen", "Muhammad Umer", "Ravi P. Ramachandran", "Ghulam Rasool"], "title": "Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts", "comment": null, "summary": "Transcription Factor Binding Site (TFBS) prediction is crucial for\nunderstanding gene regulation and various biological processes. This study\nintroduces a novel Mixture of Experts (MoE) approach for TFBS prediction,\nintegrating multiple pre-trained Convolutional Neural Network (CNN) models,\neach specializing in different TFBS patterns. We evaluate the performance of\nour MoE model against individual expert models on both in-distribution and\nout-of-distribution (OOD) datasets, using six randomly selected transcription\nfactors (TFs) for OOD testing. Our results demonstrate that the MoE model\nachieves competitive or superior performance across diverse TF binding sites,\nparticularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)\nstatistical test confirms the significance of these performance differences.\nAdditionally, we introduce ShiftSmooth, a novel attribution mapping technique\nthat provides more robust model interpretability by considering small shifts in\ninput sequences. Through comprehensive explainability analysis, we show that\nShiftSmooth offers superior attribution for motif discovery and localization\ncompared to traditional Vanilla Gradient methods. Our work presents an\nefficient, generalizable, and interpretable solution for TFBS prediction,\npotentially enabling new discoveries in genome biology and advancing our\nunderstanding of transcriptional regulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7684\u65b0\u578bTFBS\u9884\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u9884\u8bad\u7ec3\u7684CNN\u6a21\u578b\uff0c\u5e76\u5728\u5206\u5e03\u5185\u5916\u6570\u636e\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5f15\u5165ShiftSmooth\u6280\u672f\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8f6c\u5f55\u56e0\u5b50\u7ed3\u5408\u4f4d\u70b9\uff08TFBS\uff09\u9884\u6d4b\u5bf9\u7406\u89e3\u57fa\u56e0\u8c03\u63a7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6846\u67b6\uff0c\u6574\u5408\u591a\u4e2a\u9884\u8bad\u7ec3CNN\u6a21\u578b\uff0c\u5e76\u901a\u8fc7ShiftSmooth\u6280\u672f\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "MoE\u6a21\u578b\u5728\u5206\u5e03\u5185\u5916\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cShiftSmooth\u5728\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aTFBS\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u57fa\u56e0\u7ec4\u751f\u7269\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09766", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09766", "abs": "https://arxiv.org/abs/2507.09766", "authors": ["Mohamadreza Akbari Pour", "Ali Ghasemzadeh", "MohamadAli Bijarchi", "Mohammad Behshad Shafii"], "title": "Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights", "comment": null, "summary": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)\nis essential for Prognostics and Health Management (PHM) across a wide range of\nindustrial applications. We propose a novel framework -- Reinforced Graph-Based\nPhysics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that\ncombines physics-based supervision with advanced spatio-temporal learning.\nGraph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional\nfilters within recurrent units to capture how node representations evolve over\ntime. Graph Attention Convolution (GATConv) leverages a self-attention\nmechanism to compute learnable, edge-wise attention coefficients, dynamically\nweighting neighbor contributions for adaptive spatial aggregation. A Soft\nActor-Critic (SAC) module is positioned between the Temporal Attention Unit\n(TAU) and GCRN to further improve the spatio-temporal learning. This module\nimproves attention and prediction accuracy by dynamically scaling hidden\nrepresentations to minimize noise and highlight informative features. To\nidentify the most relevant physical constraints in each area, Q-learning agents\ndynamically assign weights to physics-informed loss terms, improving\ngeneralization across real-time industrial systems and reducing the need for\nmanual tuning. In both RUL and SOH estimation tasks, the proposed method\nconsistently outperforms state-of-the-art models, demonstrating strong\nrobustness and predictive accuracy across varied degradation patterns across\nthree diverse industrial benchmark datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u76d1\u7763\u4e0e\u65f6\u7a7a\u5b66\u4e60\u7684\u6846\u67b6RGPD\uff0c\u7528\u4e8eRUL\u548cSOH\u4f30\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RUL\u548cSOH\u7684\u51c6\u786e\u4f30\u8ba1\u5bf9\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684PHM\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4e0e\u65f6\u7a7a\u5b66\u4e60\u3002", "method": "\u4f7f\u7528GCRNs\u548cGATConv\u6355\u6349\u65f6\u7a7a\u7279\u5f81\uff0c\u5f15\u5165SAC\u6a21\u5757\u52a8\u6001\u4f18\u5316\u8868\u793a\uff0c\u5e76\u901a\u8fc7Q-learning\u52a8\u6001\u5206\u914d\u7269\u7406\u7ea6\u675f\u6743\u91cd\u3002", "result": "\u5728\u591a\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\uff0cRGPD\u5728RUL\u548cSOH\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "RGPD\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4e0e\u65f6\u7a7a\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u7684RUL\u548cSOH\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2507.09768", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09768", "abs": "https://arxiv.org/abs/2507.09768", "authors": ["Kenny Falk\u00e6r Olsen. Mads \u00d8stergaard", "Karl Ulb\u00e6k", "S\u00f8ren F\u00f8ns Nielsen", "Rasmus Malik H\u00f8egh Lindrup", "Bj\u00f8rn Sand Jensen", "Morten M\u00f8rup"], "title": "Knowing When to Quit: Probabilistic Early Exits for Speech Separation", "comment": null, "summary": "In recent years, deep learning-based single-channel speech separation has\nimproved considerably, in large part driven by increasingly compute- and\nparameter-efficient neural network architectures. Most such architectures are,\nhowever, designed with a fixed compute and parameter budget, and consequently\ncannot scale to varying compute demands or resources, which limits their use in\nembedded and heterogeneous devices such as mobile phones and hearables. To\nenable such use-cases we design a neural network architecture for speech\nseparation capable of early-exit, and we propose an uncertainty-aware\nprobabilistic framework to jointly model the clean speech signal and error\nvariance which we use to derive probabilistic early-exit conditions in terms of\ndesired signal-to-noise ratios. We evaluate our methods on both speech\nseparation and enhancement tasks, and we show that a single early-exit model\ncan be competitive with state-of-the-art models trained at many compute and\nparameter budgets. Our framework enables fine-grained dynamic compute-scaling\nof speech separation networks while achieving state-of-the-art performance and\ninterpretable exit conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u65e9\u671f\u9000\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5355\u901a\u9053\u8bed\u97f3\u5206\u79bb\uff0c\u5e76\u901a\u8fc7\u6982\u7387\u6846\u67b6\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u7684\u9000\u51fa\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u5206\u79bb\u6a21\u578b\u8ba1\u7b97\u548c\u53c2\u6570\u56fa\u5b9a\u3001\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u8bbe\u5907\u9700\u6c42\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u652f\u6301\u65e9\u671f\u9000\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6982\u7387\u6846\u67b6\uff0c\u5efa\u6a21\u5e72\u51c0\u8bed\u97f3\u4fe1\u53f7\u548c\u8bef\u5dee\u65b9\u5dee\uff0c\u63a8\u5bfc\u57fa\u4e8e\u4fe1\u566a\u6bd4\u7684\u9000\u51fa\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u4e00\u65e9\u671f\u9000\u51fa\u6a21\u578b\u5728\u591a\u79cd\u8ba1\u7b97\u548c\u53c2\u6570\u9884\u7b97\u4e0b\uff0c\u6027\u80fd\u4e0e\u5f53\u524d\u6700\u4f18\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u8bed\u97f3\u5206\u79bb\u7f51\u7edc\u7684\u52a8\u6001\u8ba1\u7b97\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.09785", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.09785", "abs": "https://arxiv.org/abs/2507.09785", "authors": ["Zhonglin Cao", "Mario Geiger", "Allan dos Santos Costa", "Danny Reidenbach", "Karsten Kreis", "Tomas Geffner", "Franco Pellegrini", "Guoqing Zhou", "Emine Kucukbenli"], "title": "Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow", "comment": "ICML 2025 poster", "summary": "Fast and accurate generation of molecular conformers is desired for\ndownstream computational chemistry and drug discovery tasks. Currently,\ntraining and sampling state-of-the-art diffusion or flow-based models for\nconformer generation require significant computational resources. In this work,\nwe build upon flow-matching and propose two mechanisms for accelerating\ntraining and inference of generative models for 3D molecular conformer\ngeneration. For fast training, we introduce the SO(3)-Averaged Flow training\nobjective, which leads to faster convergence to better generation quality\ncompared to conditional optimal transport flow or Kabsch-aligned flow. We\ndemonstrate that models trained using SO(3)-Averaged Flow can reach\nstate-of-the-art conformer generation quality. For fast inference, we show that\nthe reflow and distillation methods of flow-based models enable few-steps or\neven one-step molecular conformer generation with high quality. The training\ntechniques proposed in this work show a path towards highly efficient molecular\nconformer generation with flow-based models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u52a0\u901f3D\u5206\u5b50\u6784\u8c61\u751f\u6210\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7SO(3)-Averaged Flow\u8bad\u7ec3\u76ee\u6807\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u91cd\u6d41\u548c\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5c11\u6b65\u751f\u6210\u3002", "motivation": "\u5feb\u901f\u51c6\u786e\u5730\u751f\u6210\u5206\u5b50\u6784\u8c61\u5bf9\u8ba1\u7b97\u5316\u5b66\u548c\u836f\u7269\u53d1\u73b0\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6216\u6d41\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u63d0\u51faSO(3)-Averaged Flow\u8bad\u7ec3\u76ee\u6807\u4ee5\u52a0\u901f\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u91cd\u6d41\u548c\u84b8\u998f\u65b9\u6cd5\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "SO(3)-Averaged Flow\u8bad\u7ec3\u6a21\u578b\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6784\u8c61\u751f\u6210\u8d28\u91cf\uff0c\u91cd\u6d41\u548c\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5c11\u6b65\u751f\u6210\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u6d41\u6a21\u578b\u7684\u9ad8\u6548\u5206\u5b50\u6784\u8c61\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.09786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09786", "abs": "https://arxiv.org/abs/2507.09786", "authors": ["Junaid Iqbal Khan"], "title": "Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster", "comment": "10 pages, 4 figures, 4 tables", "summary": "Approximate machine unlearning (AMU) enables models to `forget' specific\ntraining data through specialized fine-tuning on a retained dataset subset.\nHowever, processing this retained subset still dominates computational runtime,\nwhile reductions of epochs also remain a challenge. We propose two\ncomplementary methods to accelerate classification-oriented AMU. First,\n\\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges\nvisually similar images with shared blend-weights to significantly reduce the\nretained set size. It operates with minimal pre-processing overhead and is\norders of magnitude faster than state-of-the-art DC methods. Second, our\nloss-centric method, \\textbf{Accelerated-AMU (A-AMU)}, augments the unlearning\nobjective to quicken convergence. A-AMU achieves this by combining a steepened\nprimary loss to expedite forgetting with a novel, differentiable regularizer\nthat matches the loss distributions of forgotten and in-distribution unseen\ndata. Our extensive experiments demonstrate that this dual approach of data and\nloss-centric optimization dramatically reduces end-to-end unlearning latency\nacross both single and multi-round scenarios, all while preserving model\nutility and privacy. To our knowledge, this is the first work to systematically\ntackle unlearning efficiency by jointly designing a specialized dataset\ncondensation technique with a dedicated accelerated loss function. Code is\navailable at https://github.com/algebraicdianuj/DC_Unlearning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\u52a0\u901f\u5206\u7c7b\u5bfc\u5411\u7684\u8fd1\u4f3c\u673a\u5668\u9057\u5fd8\uff08AMU\uff09\uff1aBlend\uff08\u5206\u5e03\u5339\u914d\u6570\u636e\u96c6\u538b\u7f29\uff09\u548cA-AMU\uff08\u52a0\u901f\u635f\u5931\u51fd\u6570\u4f18\u5316\uff09\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAMU\u65b9\u6cd5\u5728\u5904\u7406\u4fdd\u7559\u6570\u636e\u96c6\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u4e14\u51cf\u5c11\u8bad\u7ec3\u8f6e\u6b21\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "1. Blend\uff1a\u901a\u8fc7\u89c6\u89c9\u76f8\u4f3c\u56fe\u50cf\u5408\u5e76\u51cf\u5c11\u4fdd\u7559\u96c6\u5927\u5c0f\uff0c\u901f\u5ea6\u5feb\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u30022. A-AMU\uff1a\u901a\u8fc7\u589e\u5f3a\u635f\u5931\u51fd\u6570\u52a0\u901f\u6536\u655b\uff0c\u7ed3\u5408\u4e3b\u635f\u5931\u548c\u53ef\u5fae\u5206\u6b63\u5219\u5316\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6570\u636e\u4e0e\u635f\u5931\u4f18\u5316\u7684\u53cc\u91cd\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u9057\u5fd8\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u548c\u9690\u79c1\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u4e13\u7528\u6570\u636e\u96c6\u538b\u7f29\u6280\u672f\u548c\u52a0\u901f\u635f\u5931\u51fd\u6570\u7cfb\u7edf\u89e3\u51b3\u9057\u5fd8\u6548\u7387\u7684\u5de5\u4f5c\u3002"}}
{"id": "2507.09797", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09797", "abs": "https://arxiv.org/abs/2507.09797", "authors": ["Ping Liu", "Rajat Arora", "Xiao Shi", "Benjamin Le", "Qianqi Shen", "Jianqiang Shen", "Chengming Jiang", "Nikita Zhiltsov", "Priya Bannur", "Yidan Zhu", "Liming Dong", "Haichao Wei", "Qi Guo", "Luke Simon", "Liangjie Hong", "Wenjing Zhang"], "title": "A Scalable and Efficient Signal Integration System for Job Matching", "comment": "KDD2025", "summary": "LinkedIn, one of the world's largest platforms for professional networking\nand job seeking, encounters various modeling challenges in building\nrecommendation systems for its job matching product, including cold-start,\nfilter bubbles, and biases affecting candidate-job matching. To address these,\nwe developed the STAR (Signal Integration for Talent And Recruiters) system,\nleveraging the combined strengths of Large Language Models (LLMs) and Graph\nNeural Networks (GNNs). LLMs excel at understanding textual data, such as\nmember profiles and job postings, while GNNs capture intricate relationships\nand mitigate cold-start issues through network effects. STAR integrates diverse\nsignals by uniting LLM and GNN capabilities with industrial-scale paradigms\nincluding adaptive sampling and version management. It provides an end-to-end\nsolution for developing and deploying embeddings in large-scale recommender\nsystems. Our key contributions include a robust methodology for building\nembeddings in industrial applications, a scalable GNN-LLM integration for\nhigh-performing recommendations, and practical insights for real-world model\ndeployment.", "AI": {"tldr": "LinkedIn\u5f00\u53d1\u4e86STAR\u7cfb\u7edf\uff0c\u7ed3\u5408LLM\u548cGNN\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u51b7\u542f\u52a8\u3001\u8fc7\u6ee4\u6c14\u6ce1\u548c\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u804c\u4e1a\u5339\u914d\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u51b7\u542f\u52a8\u3001\u8fc7\u6ee4\u6c14\u6ce1\u548c\u504f\u89c1\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408LLM\uff08\u5904\u7406\u6587\u672c\u6570\u636e\uff09\u548cGNN\uff08\u6355\u6349\u5173\u7cfb\u7f51\u7edc\uff09\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7248\u672c\u7ba1\u7406\u7b49\u5de5\u4e1a\u7ea7\u8303\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86\u6784\u5efa\u5d4c\u5165\u7684\u7a33\u5065\u65b9\u6cd5\u3001\u53ef\u6269\u5c55\u7684GNN-LLM\u96c6\u6210\u65b9\u6848\uff0c\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "STAR\u7cfb\u7edf\u4e3a\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2507.09805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09805", "abs": "https://arxiv.org/abs/2507.09805", "authors": ["Audri Banik", "Glaucio Haroldo Silva de Carvalho", "Renata Dividino"], "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting", "comment": "Accepted at FedKDD 2025: International Joint Workshop on Federated\n  Learning for Data Mining and Graph Analytics. 6 pages, 1 figure", "summary": "In traffic prediction, the goal is to estimate traffic speed or flow in\nspecific regions or road segments using historical data collected by devices\ndeployed in each area. Each region or road segment can be viewed as an\nindividual client that measures local traffic flow, making Federated Learning\n(FL) a suitable approach for collaboratively training models without sharing\nraw data. In centralized FL, a central server collects and aggregates model\nupdates from multiple clients to build a shared model while preserving each\nclient's data privacy. Standard FL methods, such as Federated Averaging\n(FedAvg), assume that clients are independent, which can limit performance in\ntraffic prediction tasks where spatial relationships between clients are\nimportant. Federated Graph Learning methods can capture these dependencies\nduring server-side aggregation, but they often introduce significant\ncomputational overhead. In this paper, we propose a lightweight graph-aware FL\napproach that blends the simplicity of FedAvg with key ideas from graph\nlearning. Rather than training full models, our method applies basic\nneighbourhood aggregation principles to guide parameter updates, weighting\nclient models based on graph connectivity. This approach captures spatial\nrelationships effectively while remaining computationally efficient. We\nevaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,\nand show that it achieves competitive performance compared to standard\nbaselines and recent graph-based federated learning techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u56fe\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86FedAvg\u7684\u7b80\u5355\u6027\u548c\u56fe\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\uff0c\u6709\u6548\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982FedAvg\uff09\u5047\u8bbe\u5ba2\u6237\u7aef\u72ec\u7acb\uff0c\u5ffd\u7565\u4e86\u4ea4\u901a\u9884\u6d4b\u4efb\u52a1\u4e2d\u91cd\u8981\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u800c\u73b0\u6709\u7684\u56fe\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528\u57fa\u672c\u7684\u90bb\u57df\u805a\u5408\u539f\u5219\u6307\u5bfc\u53c2\u6570\u66f4\u65b0\uff0c\u6839\u636e\u56fe\u8fde\u63a5\u6027\u52a0\u6743\u5ba2\u6237\u7aef\u6a21\u578b\uff0c\u907f\u514d\u8bad\u7ec3\u5b8c\u6574\u6a21\u578b\u3002", "result": "\u5728METR-LA\u548cPEMS-BAY\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u57fa\u7ebf\u548c\u8fd1\u671f\u56fe\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6709\u6548\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\uff0c\u4e3a\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09816", "abs": "https://arxiv.org/abs/2507.09816", "authors": ["Adam Newgas"], "title": "Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem", "comment": "9 pages, 9 figures", "summary": "Neural networks are capable of superposition -- representing more features\nthan there are dimensions. Recent work considers the analogous concept for\ncomputation instead of storage, proposing theoretical constructions. But there\nhas been little investigation into whether these circuits can be learned in\npractice. In this work, we investigate a toy model for the Universal-AND\nproblem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs.\nThe hidden dimension that determines the number of non-linear activations is\nrestricted to pressure the model to find a compute-efficient circuit, called\ncompressed computation. We find that the training process finds a simple\nsolution that does not correspond to theoretical constructions. It is fully\ndense -- every neuron contributes to every output. The solution circuit\nnaturally scales with dimension, trading off error rates for neuron efficiency.\nIt is similarly robust to changes in sparsity and other key parameters, and\nextends naturally to other boolean operations and boolean circuits. We explain\nthe found solution in detail and compute why it is more efficient than the\ntheoretical constructions at low sparsity. Our findings shed light on the types\nof circuits that models like to form and the flexibility of the superposition\nrepresentation. This contributes to a broader understanding of network\ncircuitry and interpretability.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u538b\u7f29\u8ba1\u7b97\u73b0\u8c61\uff0c\u53d1\u73b0\u5b9e\u9645\u8bad\u7ec3\u5f97\u5230\u7684\u89e3\u51b3\u65b9\u6848\u4e0e\u7406\u8bba\u6784\u9020\u4e0d\u540c\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u80fd\u5728\u5b9e\u8df5\u4e2d\u5b66\u4e60\u5230\u7406\u8bba\u63d0\u51fa\u7684\u538b\u7f29\u8ba1\u7b97\u7535\u8def\uff0c\u4ee5\u89e3\u51b3Universal-AND\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u73a9\u5177\u6a21\u578b\uff0c\u9650\u5236\u9690\u85cf\u7ef4\u5ea6\u4ee5\u8feb\u4f7f\u6a21\u578b\u627e\u5230\u8ba1\u7b97\u9ad8\u6548\u7684\u7535\u8def\uff0c\u5373\u538b\u7f29\u8ba1\u7b97\u3002", "result": "\u8bad\u7ec3\u8fc7\u7a0b\u53d1\u73b0\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5168\u5bc6\u96c6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6bd4\u7406\u8bba\u6784\u9020\u66f4\u9ad8\u6548\uff0c\u4e14\u80fd\u81ea\u7136\u6269\u5c55\u5230\u5176\u4ed6\u5e03\u5c14\u64cd\u4f5c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u504f\u597d\u7684\u7535\u8def\u7c7b\u578b\u548c\u53e0\u52a0\u8868\u793a\u7684\u7075\u6d3b\u6027\uff0c\u5bf9\u7f51\u7edc\u7535\u8def\u548c\u53ef\u89e3\u91ca\u6027\u6709\u66f4\u5e7f\u6cdb\u7684\u7406\u89e3\u3002"}}
{"id": "2507.09826", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09826", "abs": "https://arxiv.org/abs/2507.09826", "authors": ["Jintao Qu", "Zichong Wang", "Chenhao Wu", "Wenbin Zhang"], "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification", "comment": null, "summary": "Neural networks have achieved remarkable success in time series\nclassification, but their reliance on large amounts of labeled data for\ntraining limits their applicability in cold-start scenarios. Moreover, they\nlack interpretability, reducing transparency in decision-making. In contrast,\ndynamic time warping (DTW) combined with a nearest neighbor classifier is\nwidely used for its effectiveness in limited-data settings and its inherent\ninterpretability. However, as a non-parametric method, it is not trainable and\ncannot leverage large amounts of labeled data, making it less effective than\nneural networks in rich-resource scenarios. In this work, we aim to develop a\nversatile model that adapts to cold-start conditions and becomes trainable with\nlabeled data, while maintaining interpretability. We propose a dynamic\nlength-shortening algorithm that transforms time series into prototypes while\npreserving key structural patterns, thereby enabling the reformulation of the\nDTW recurrence relation into an equivalent recurrent neural network. Based on\nthis, we construct a trainable model that mimics DTW's alignment behavior. As a\nneural network, it becomes trainable when sufficient labeled data is available,\nwhile still retaining DTW's inherent interpretability. We apply the model to\nseveral benchmark time series classification tasks and observe that it\nsignificantly outperforms previous approaches in low-resource settings and\nremains competitive in rich-resource settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u9002\u5e94\u51b7\u542f\u52a8\u573a\u666f\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff1bDTW\u5728\u6709\u9650\u6570\u636e\u4e0b\u6709\u6548\u4e14\u53ef\u89e3\u91ca\uff0c\u4f46\u65e0\u6cd5\u5229\u7528\u5927\u6570\u636e\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u957f\u5ea6\u7f29\u77ed\u7b97\u6cd5\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u6362\u4e3a\u539f\u578b\uff0c\u5e76\u5c06DTW\u5173\u7cfb\u8f6c\u5316\u4e3a\u7b49\u6548\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u9ad8\u8d44\u6e90\u573a\u666f\u4e2d\u4ecd\u5177\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u517c\u5177DTW\u7684\u53ef\u89e3\u91ca\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6027\uff0c\u9002\u5e94\u6027\u5f3a\u3002"}}
{"id": "2507.09831", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09831", "abs": "https://arxiv.org/abs/2507.09831", "authors": ["Jiatong Li", "Qi Liu", "Mengxiao Zhu"], "title": "Generative Cognitive Diagnosis", "comment": "Preprint; 15 pages, 12 figures", "summary": "Cognitive diagnosis (CD) models latent cognitive states of human learners by\nanalyzing their response patterns on diagnostic tests, serving as a crucial\nmachine learning technique for educational assessment and evaluation.\nTraditional cognitive diagnosis models typically follow a transductive\nprediction paradigm that optimizes parameters to fit response scores and\nextract learner abilities. These approaches face significant limitations as\nthey cannot perform instant diagnosis for new learners without computationally\nexpensive retraining and produce diagnostic outputs with limited reliability.\nIn this study, we introduces a novel generative diagnosis paradigm that\nfundamentally shifts CD from predictive to generative modeling, enabling\ninductive inference of cognitive states without parameter re-optimization. We\npropose two simple yet effective instantiations of this paradigm: Generative\nItem Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model\n(G-NCDM), which achieve excellent performance improvements over traditional\nmethods. The generative approach disentangles cognitive state inference from\nresponse prediction through a well-designed generation process that\nincorporates identifiability and monotonicity conditions. Extensive experiments\non real-world datasets demonstrate the effectiveness of our methodology in\naddressing scalability and reliability challenges, especially $\\times 100$\nspeedup for the diagnosis of new learners. Our framework opens new avenues for\ncognitive diagnosis applications in artificial intelligence, particularly for\nintelligent model evaluation and intelligent education systems. The code is\navailable at https://github.com/CSLiJT/Generative-CD.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u5f0f\u8ba4\u77e5\u8bca\u65ad\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u5efa\u6a21\u5b9e\u73b0\u65e0\u9700\u53c2\u6570\u91cd\u4f18\u5316\u7684\u8ba4\u77e5\u72b6\u6001\u63a8\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u901f\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\u65e0\u6cd5\u5bf9\u65b0\u5b66\u4e60\u8005\u8fdb\u884c\u5373\u65f6\u8bca\u65ad\u4e14\u53ef\u9760\u6027\u6709\u9650\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u751f\u6210\u5f0f\u8bca\u65ad\u8303\u5f0f\uff0c\u5e76\u5177\u4f53\u5316\u4e3aG-IRT\u548cG-NCDM\u4e24\u79cd\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u8fc7\u7a0b\u5206\u79bb\u8ba4\u77e5\u72b6\u6001\u63a8\u65ad\u548c\u54cd\u5e94\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65b0\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bca\u65ad\u901f\u5ea6\u63d0\u5347100\u500d\uff0c\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "\u751f\u6210\u5f0f\u8bca\u65ad\u4e3a\u8ba4\u77e5\u8bca\u65ad\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c24\u5176\u5728\u667a\u80fd\u6a21\u578b\u8bc4\u4f30\u548c\u6559\u80b2\u7cfb\u7edf\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.09837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09837", "abs": "https://arxiv.org/abs/2507.09837", "authors": ["Quang Truong", "Zhikai Chen", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles", "comment": null, "summary": "Relational databases underpin critical infrastructure across a wide range of\ndomains, yet the design of generalizable pre-training strategies for learning\nfrom relational databases remains an open challenge due to task heterogeneity.\nSpecifically, there exist infinitely many possible downstream tasks, as tasks\nare defined based on relational schema graphs, temporal dependencies, and\nSQL-defined label logics. An effective pre-training framework is desired to\ntake these factors into account in order to obtain task-aware representations.\nBy incorporating knowledge of the underlying distribution that drives label\ngeneration, downstream tasks can benefit from relevant side-channel\ninformation. To bridge this gap, we introduce Task Vector Estimation (TVE), a\nnovel pre-training framework that constructs predictive supervisory signals via\nset-based aggregation over schema traversal graphs, explicitly modeling\nnext-window relational dynamics. We formalize our approach through an\ninformation-theoretic lens, demonstrating that task-informed representations\nretain more relevant signals than those obtained without task priors. Extensive\nexperiments on the RelBench benchmark show that TVE consistently outperforms\ntraditional pre-training baselines. Our findings advocate for pre-training\nobjectives that encode task heterogeneity and temporal structure as design\nprinciples for predictive modeling on relational databases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4efb\u52a1\u5411\u91cf\u4f30\u8ba1\uff08TVE\uff09\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u4efb\u52a1\u5f02\u6784\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5efa\u6a21\u4efb\u52a1\u611f\u77e5\u8868\u793a\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\u5728\u591a\u4e2a\u9886\u57df\u652f\u6491\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u7531\u4e8e\u4efb\u52a1\u5f02\u6784\u6027\uff08\u5982\u5173\u7cfb\u6a21\u5f0f\u56fe\u3001\u65f6\u95f4\u4f9d\u8d56\u6027\u548cSQL\u5b9a\u4e49\u7684\u6807\u7b7e\u903b\u8f91\uff09\uff0c\u8bbe\u8ba1\u901a\u7528\u9884\u8bad\u7ec3\u7b56\u7565\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "TVE\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u6a21\u5f0f\u904d\u5386\u56fe\u7684\u96c6\u5408\u805a\u5408\u6784\u5efa\u9884\u6d4b\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u5f0f\u5efa\u6a21\u4e0b\u4e00\u7a97\u53e3\u5173\u7cfb\u52a8\u6001\uff0c\u5e76\u7ed3\u5408\u4fe1\u606f\u8bba\u89c6\u89d2\u5f62\u5f0f\u5316\u8868\u793a\u4efb\u52a1\u611f\u77e5\u4fe1\u606f\u3002", "result": "\u5728RelBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTVE\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u9884\u8bad\u7ec3\u57fa\u7ebf\uff0c\u8868\u660e\u4efb\u52a1\u5f02\u6784\u6027\u548c\u65f6\u95f4\u7ed3\u6784\u7684\u7f16\u7801\u5bf9\u5173\u7cfb\u6570\u636e\u5e93\u9884\u6d4b\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u5c06\u4efb\u52a1\u5f02\u6784\u6027\u548c\u65f6\u95f4\u7ed3\u6784\u4f5c\u4e3a\u5173\u7cfb\u6570\u636e\u5e93\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u4ee5\u63d0\u5347\u4efb\u52a1\u611f\u77e5\u8868\u793a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09839", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09839", "abs": "https://arxiv.org/abs/2507.09839", "authors": ["MohammadReza Davari", "Utkarsh Garg", "Weixin Cai", "Eugene Belilovsky"], "title": "Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs", "comment": null, "summary": "An increasing number of NLP applications interact with large language models\n(LLMs) through black-box APIs, making prompt engineering critical for\ncontrolling model outputs. While recent Automatic Prompt Optimization (APO)\nmethods iteratively refine prompts using model-generated feedback, textual\ngradients, they primarily focus on error correction and neglect valuable\ninsights from correct predictions. This limits both their effectiveness and\nefficiency. In this paper, we propose a novel APO framework centered on\nenhancing the feedback mechanism. We reinterpret the textual gradient as a form\nof negative reinforcement and introduce the complementary positive\nreinforcement to explicitly preserve beneficial prompt components identified\nthrough successful predictions. To mitigate the noise inherent in LLM-generated\nfeedback, we introduce a technique called feedback diversification, which\naggregates multiple feedback signals, emphasizing consistent, actionable advice\nwhile filtering out outliers. Motivated by the rapid evolution and diversity of\navailable LLMs, we also formalize Continual Prompt Optimization (CPO),\naddressing the practical challenge of efficiently migrating optimized prompts\nbetween different model versions or API providers. Our experiments reveal that\nnaive prompt migration often degrades performance due to loss of critical\ninstructions. In contrast, our approach consistently outperforms strong\nbaselines, achieving significant accuracy improvements, faster convergence, and\nlower computational costs in both standard and migration scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u53cd\u9988\u673a\u5236\uff08\u5305\u62ec\u6b63\u8d1f\u5f3a\u5316\uff09\u548c\u53cd\u9988\u591a\u6837\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5e76\u89e3\u51b3\u4e86\u4e0d\u540cLLM\u95f4\u63d0\u793a\u8fc1\u79fb\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709APO\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9519\u8bef\u4fee\u6b63\uff0c\u5ffd\u7565\u4e86\u6b63\u786e\u9884\u6d4b\u4e2d\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5176\u6548\u679c\u548c\u6548\u7387\u3002\u540c\u65f6\uff0cLLM\u7684\u5feb\u901f\u8fed\u4ee3\u548c\u591a\u6837\u6027\u4f7f\u5f97\u63d0\u793a\u8fc1\u79fb\u6210\u4e3a\u5b9e\u9645\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b63\u8d1f\u5f3a\u5316\u7684\u53cd\u9988\u673a\u5236\u548c\u53cd\u9988\u591a\u6837\u5316\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u5e76\u4fdd\u7559\u6709\u6548\u63d0\u793a\u7ec4\u4ef6\uff1b\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u6301\u7eed\u63d0\u793a\u4f18\u5316\uff08CPO\uff09\u4ee5\u89e3\u51b3\u63d0\u793a\u8fc1\u79fb\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u63d0\u793a\u8fc1\u79fb\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u7684\u6548\u679c\u548c\u6548\u7387\uff0c\u5e76\u89e3\u51b3\u4e86LLM\u95f4\u63d0\u793a\u8fc1\u79fb\u7684\u6311\u6218\u3002"}}
{"id": "2507.09871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09871", "abs": "https://arxiv.org/abs/2507.09871", "authors": ["Niket Patel", "Randall Balestriero"], "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks", "comment": null, "summary": "The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4efb\u52a1\u5148\u9a8c\u7684\u6982\u7387\u7a7a\u95f4\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u6240\u6709\u53ef\u80fd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u56fa\u5b9a\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dAI\u7814\u7a76\u4e2d\u7684\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9650\u5236\u4e86\u7814\u7a76\u7684\u8fdb\u5c55\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5b9a\u4e49\u4efb\u52a1\u5148\u9a8c\u548c\u4efb\u52a1\u5206\u5e03\uff0c\u63d0\u4f9b\u4e00\u4e2a\u66f4\u7075\u6d3b\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4efb\u52a1\u5148\u9a8c\u548c\u4efb\u52a1\u5206\u5e03\uff0c\u6784\u5efa\u4e00\u4e2a\u6982\u7387\u7a7a\u95f4\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u6240\u6709\u53ef\u80fd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u548c\u65b9\u5dee\u3002", "result": "\u8be5\u6846\u67b6\u9996\u6b21\u80fd\u591f\u56de\u7b54\u6a21\u578b\u5728\u6240\u6709\u53ef\u80fd\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u548c\u65b9\u5dee\u95ee\u9898\uff0c\u4e3aSSL\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "conclusion": "\u4efb\u52a1\u5148\u9a8c\u6846\u67b6\u6709\u671b\u52a0\u901fSSL\u7814\u7a76\u7684\u8fdb\u5c55\uff0c\u5e76\u4e3aAI\u8bc4\u4f30\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.09882", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09882", "abs": "https://arxiv.org/abs/2507.09882", "authors": ["Jiamin Wu", "Zichen Ren", "Junyu Wang", "Pengyu Zhu", "Yonghao Song", "Mianxin Liu", "Qihao Zheng", "Lei Bai", "Wanli Ouyang", "Chunfeng Song"], "title": "AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications", "comment": null, "summary": "Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible\nmeans of connecting the human brain to external devices, with broad\napplications in home and clinical settings to enhance human capabilities.\nHowever, the high noise level and limited task-specific data in non-invasive\nsignals constrain decoding capabilities. Recently, the adoption of\nself-supervised pre-training is transforming the landscape of non-invasive BCI\nresearch, enabling the development of brain foundation models to capture\ngeneric neural representations from large-scale unlabeled\nelectroencephalography (EEG) signals with substantial noises. However, despite\nthese advances, the field currently lacks comprehensive, practical and\nextensible benchmarks to assess the utility of the public foundation models\nacross diverse BCI tasks, hindering their widespread adoption. To address this\nchallenge, we present AdaBrain-Bench, a large-scale standardized benchmark to\nsystematically evaluate brain foundation models in widespread non-invasive BCI\ntasks. AdaBrain-Bench encompasses a diverse collection of representative BCI\ndecoding datasets spanning 7 key applications. It introduces a streamlined task\nadaptation pipeline integrated with multi-dimensional evaluation metrics and a\nset of adaptation tools. The benchmark delivers an inclusive framework for\nassessing generalizability of brain foundation models across key transfer\nsettings, including cross-subject, multi-subject, and few-shot scenarios. We\nleverage AdaBrain-Bench to evaluate a suite of publicly available brain\nfoundation models and offer insights into practices for selecting appropriate\nmodels in various scenarios. We make our benchmark pipeline available to enable\nreproducible research and external use, offering a continuously evolving\nplatform to foster progress toward robust and generalized neural decoding\nsolutions.", "AI": {"tldr": "AdaBrain-Bench\u662f\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u975e\u4fb5\u5165\u5f0f\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u4e2d\u7684\u57fa\u7840\u6a21\u578b\uff0c\u6db5\u76d67\u79cd\u5173\u952e\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\u548c\u5de5\u5177\u3002", "motivation": "\u975e\u4fb5\u5165\u5f0fBCI\u4fe1\u53f7\u7684\u9ad8\u566a\u58f0\u548c\u6709\u9650\u4efb\u52a1\u6570\u636e\u9650\u5236\u4e86\u89e3\u7801\u80fd\u529b\uff0c\u5f53\u524d\u7f3a\u4e4f\u5168\u9762\u3001\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u516c\u5171\u57fa\u7840\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faAdaBrain-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6574\u5408\u591a\u6837\u5316\u7684BCI\u89e3\u7801\u6570\u636e\u96c6\u3001\u4efb\u52a1\u9002\u5e94\u6d41\u7a0b\u548c\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u8de8\u4e3b\u4f53\u3001\u591a\u4e3b\u4f53\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7AdaBrain-Bench\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u516c\u5f00\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u9009\u62e9\u5408\u9002\u6a21\u578b\u7684\u5b9e\u8df5\u5efa\u8bae\u3002", "conclusion": "AdaBrain-Bench\u4e3a\u4fc3\u8fdb\u7a33\u5065\u548c\u901a\u7528\u7684\u795e\u7ecf\u89e3\u7801\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6301\u7eed\u6f14\u8fdb\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.09890", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.09890", "abs": "https://arxiv.org/abs/2507.09890", "authors": ["Ping Xu", "Pengfei Wang", "Zhiyuan Ning", "Meng Xiao", "Min Wu", "Yuanchun Zhou"], "title": "Soft Graph Clustering for single-cell RNA Sequencing Data", "comment": null, "summary": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)\ndata analysis for elucidating cellular heterogeneity and diversity. Recent\ngraph-based scRNA-seq clustering methods, particularly graph neural networks\n(GNNs), have significantly improved in tackling the challenges of\nhigh-dimension, high-sparsity, and frequent dropout events that lead to\nambiguous cell population boundaries. However, their reliance on hard graph\nconstructions derived from thresholded similarity matrices presents\nchallenges:(i) The simplification of intercellular relationships into binary\nedges (0 or 1) by applying thresholds, which restricts the capture of\ncontinuous similarity features among cells and leads to significant information\nloss.(ii) The presence of significant inter-cluster connections within hard\ngraphs, which can confuse GNN methods that rely heavily on graph structures,\npotentially causing erroneous message propagation and biased clustering\noutcomes. To tackle these challenges, we introduce scSGC, a Soft Graph\nClustering for single-cell RNA sequencing data, which aims to more accurately\ncharacterize continuous similarities among cells through non-binary edge\nweights, thereby mitigating the limitations of rigid data structures. The scSGC\nframework comprises three core components: (i) a zero-inflated negative\nbinomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed\nsoft graph embedding module; and (iii) an optimal transport-based clustering\noptimization module. Extensive experiments across ten datasets demonstrate that\nscSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,\ncell type annotation, and computational efficiency. These results highlight its\nsubstantial potential to advance scRNA-seq data analysis and deepen our\nunderstanding of cellular heterogeneity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ascSGC\u7684\u8f6f\u56fe\u805a\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u4e2d\u786c\u56fe\u6784\u5efa\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u975e\u4e8c\u5143\u8fb9\u6743\u91cd\u66f4\u51c6\u786e\u5730\u8868\u5f81\u7ec6\u80de\u95f4\u7684\u8fde\u7eed\u76f8\u4f3c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u805a\u7c7b\u65b9\u6cd5\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u4f9d\u8d56\u786c\u56fe\u6784\u5efa\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u805a\u7c7b\u504f\u5dee\u3002scSGC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "scSGC\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u57fa\u4e8eZINB\u7684\u7279\u5f81\u81ea\u7f16\u7801\u5668\u3001\u53cc\u901a\u9053\u5207\u5272\u4fe1\u606f\u8f6f\u56fe\u5d4c\u5165\u6a21\u5757\u548c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u805a\u7c7b\u4f18\u5316\u6a21\u5757\u3002", "result": "\u5728\u5341\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cscSGC\u5728\u805a\u7c7b\u51c6\u786e\u6027\u3001\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e13\u79cd\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u6a21\u578b\u3002", "conclusion": "scSGC\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u53ef\u63a8\u52a8\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u5206\u6790\u5e76\u6df1\u5316\u5bf9\u7ec6\u80de\u5f02\u8d28\u6027\u7684\u7406\u89e3\u3002"}}
{"id": "2507.09897", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.09897", "abs": "https://arxiv.org/abs/2507.09897", "authors": ["Loek van Rossem", "Andrew M. Saxe"], "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task", "comment": "28 pages, 20 figures", "summary": "Even when massively overparameterized, deep neural networks show a remarkable\nability to generalize. Research on this phenomenon has focused on\ngeneralization within distribution, via smooth interpolation. Yet in some\nsettings neural networks also learn to extrapolate to data far beyond the\nbounds of the original training set, sometimes even allowing for infinite\ngeneralization, implying that an algorithm capable of solving the task has been\nlearned. Here we undertake a case study of the learning dynamics of recurrent\nneural networks (RNNs) trained on the streaming parity task in order to develop\nan effective theory of algorithm development. The streaming parity task is a\nsimple but nonlinear task defined on sequences up to arbitrary length. We show\nthat, with sufficient finite training experience, RNNs exhibit a phase\ntransition to perfect infinite generalization. Using an effective theory for\nthe representational dynamics, we find an implicit representational merger\neffect which can be interpreted as the construction of a finite automaton that\nreproduces the task. Overall, our results disclose one mechanism by which\nneural networks can generalize infinitely from finite training experience.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u5982\u4f55\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\uff0c\u901a\u8fc7\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u5728\u6d41\u5f0f\u5947\u5076\u4efb\u52a1\u4e2d\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u5f00\u53d1\u7684\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6d41\u5f0f\u5947\u5076\u4efb\u52a1\u8fd9\u4e00\u975e\u7ebf\u6027\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u8bad\u7ec3\u6d41\u5f0f\u5947\u5076\u4efb\u52a1\uff0c\u5206\u6790\u5176\u5b66\u4e60\u52a8\u6001\u548c\u8868\u73b0\u3002", "result": "\u53d1\u73b0RNN\u5728\u8db3\u591f\u8bad\u7ec3\u540e\u4f1a\u51fa\u73b0\u5b8c\u7f8e\u65e0\u9650\u6cdb\u5316\u7684\u76f8\u53d8\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u7406\u8bba\u63ed\u793a\u4e86\u9690\u5f0f\u8868\u793a\u5408\u5e76\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u4ece\u6709\u9650\u8bad\u7ec3\u7ecf\u9a8c\u4e2d\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\u7684\u4e00\u79cd\u673a\u5236\uff0c\u5373\u901a\u8fc7\u6784\u5efa\u6709\u9650\u81ea\u52a8\u673a\u5b8c\u6210\u4efb\u52a1\u3002"}}
{"id": "2507.09925", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09925", "abs": "https://arxiv.org/abs/2507.09925", "authors": ["Md Ahsanul Kabir", "Abrar Jahin", "Mohammad Al Hasan"], "title": "Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model", "comment": null, "summary": "Extracting cause and effect phrases from a sentence is an important NLP task,\nwith numerous applications in various domains, including legal, medical,\neducation, and scientific research. There are many unsupervised and supervised\nmethods proposed for solving this task. Among these, unsupervised methods\nutilize various linguistic tools, including syntactic patterns, dependency\ntree, dependency relations, etc. among different sentential units for\nextracting the cause and effect phrases. On the other hand, the contemporary\nsupervised methods use various deep learning based mask language models\nequipped with a token classification layer for extracting cause and effect\nphrases. Linguistic tools, specifically, dependency tree, which organizes a\nsentence into different semantic units have been shown to be very effective for\nextracting semantic pairs from a sentence, but existing supervised methods do\nnot have any provision for utilizing such tools within their model framework.\nIn this work, we propose DepBERT, which extends a transformer-based model by\nincorporating dependency tree of a sentence within the model framework.\nExtensive experiments over three datasets show that DepBERT is better than\nvarious state-of-the art supervised causality extraction methods.", "AI": {"tldr": "DepBERT\u7ed3\u5408\u4e86\u4f9d\u8d56\u6811\u548cTransformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56e0\u679c\u5173\u7cfb\u77ed\u8bed\u63d0\u53d6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u4f9d\u8d56\u6811\u7b49\u8bed\u8a00\u5b66\u5de5\u5177\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51faDepBERT\uff0c\u5c06\u4f9d\u8d56\u6811\u878d\u5165Transformer\u6846\u67b6\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DepBERT\u8bc1\u660e\u4e86\u4f9d\u8d56\u6811\u5728\u76d1\u7763\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09931", "abs": "https://arxiv.org/abs/2507.09931", "authors": ["Yoon Pyo Lee"], "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications", "comment": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures", "summary": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6838\u5de5\u7a0b\u9886\u57df\u4e3a\u4f8b\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u5206\u6790\u548c\u795e\u7ecf\u5143\u6c89\u9ed8\u6280\u672f\uff0c\u9a8c\u8bc1\u4e86\u7279\u5b9a\u795e\u7ecf\u5143\u7ec4\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u6838\u5de5\u7a0b\uff09\u4e2d\u90e8\u7f72LLM\u9700\u8981\u7406\u89e3\u5176\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u6ee1\u8db3\u6838\u76d1\u7ba1\u6846\u67b6\u7684\u9a8c\u8bc1\u8981\u6c42\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u5bf9\u901a\u7528LLM\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u6bd4\u8f83\u548c\u795e\u7ecf\u5143\u6c89\u9ed8\u6280\u672f\u5206\u6790\u5176\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u6c89\u9ed8\u7279\u5b9a\u795e\u7ecf\u5143\u7ec4\u4f1a\u663e\u8457\u964d\u4f4e\u4efb\u52a1\u6027\u80fd\uff0c\u5f71\u54cd\u6a21\u578b\u751f\u6210\u51c6\u786e\u6280\u672f\u4fe1\u606f\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86LLM\u7684\u900f\u660e\u5ea6\uff0c\u4e3a\u6838\u7ea7AI\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.09937", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09937", "abs": "https://arxiv.org/abs/2507.09937", "authors": ["Gaurav R. Ghosal", "Pratyush Maini", "Aditi Raghunathan"], "title": "Memorization Sinks: Isolating Memorization during LLM Training", "comment": "Accepted at the 2025 International Conference of Machine Learning", "summary": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMemSinks\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9694\u79bb\u8bb0\u5fc6\u5185\u5bb9\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8bb0\u5fc6\u91cd\u590d\u5e8f\u5217\u7684\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u91cd\u590d\u5e8f\u5217\uff0c\u5f15\u53d1\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\uff0c\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51faMemSinks\u8303\u5f0f\uff0c\u5229\u7528\u5e8f\u5217\u6807\u8bc6\u7b26\u6fc0\u6d3b\u7279\u5b9a\u8bb0\u5fc6\u795e\u7ecf\u5143\uff0c\u9694\u79bb\u8bb0\u5fc6\u5185\u5bb9\u3002", "result": "\u5728\u5341\u4ebf\u53c2\u6570\u548c\u5341\u4ebftoken\u89c4\u6a21\u4e0b\u5b9e\u73b0\u6709\u6548\u9694\u79bb\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MemSinks\u9996\u6b21\u8bc1\u660e\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u540c\u65f6\u5b9e\u73b0\u6cdb\u5316\u548c\u9694\u79bb\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09940", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.09940", "abs": "https://arxiv.org/abs/2507.09940", "authors": ["Taigo Sakai", "Kazuhiro Hotta"], "title": "Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training", "comment": null, "summary": "In conventional deep learning, the number of neurons typically remains fixed\nduring training. However, insights from biology suggest that the human\nhippocampus undergoes continuous neuron generation and pruning of neurons over\nthe course of learning, implying that a flexible allocation of capacity can\ncontribute to enhance performance. Real-world datasets often exhibit class\nimbalance situations where certain classes have far fewer samples than others,\nleading to significantly reduce recognition accuracy for minority classes when\nrelying on fixed size networks.To address the challenge, we propose a method\nthat periodically adds and removes neurons during training, thereby boosting\nrepresentational power for minority classes. By retaining critical features\nlearned from majority classes while selectively increasing neurons for\nunderrepresented classes, our approach dynamically adjusts capacity during\ntraining. Importantly, while the number of neurons changes throughout training,\nthe final network size and structure remain unchanged, ensuring efficiency and\ncompatibility with deployment.Furthermore, by experiments on three different\ndatasets and five representative models, we demonstrate that the proposed\nmethod outperforms fixed size networks and shows even greater accuracy when\ncombined with other imbalance-handling techniques. Our results underscore the\neffectiveness of dynamic, biologically inspired network designs in improving\nperformance on class-imbalanced data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u795e\u7ecf\u5143\u6570\u91cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u5c11\u6570\u7c7b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u53d7\u751f\u7269\u5b66\u542f\u53d1\uff0c\u4eba\u8111\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f1a\u52a8\u6001\u751f\u6210\u548c\u4fee\u526a\u795e\u7ecf\u5143\uff0c\u800c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u795e\u7ecf\u5143\u6570\u91cf\u56fa\u5b9a\uff0c\u65e0\u6cd5\u9002\u5e94\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b9a\u671f\u589e\u52a0\u548c\u79fb\u9664\u795e\u7ecf\u5143\uff0c\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u5bb9\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u7ec8\u7f51\u7edc\u7ed3\u6784\u548c\u5927\u5c0f\u4e0d\u53d8\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u4e94\u79cd\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u56fa\u5b9a\u7f51\u7edc\uff0c\u4e14\u4e0e\u5176\u4ed6\u4e0d\u5e73\u8861\u5904\u7406\u6280\u672f\u7ed3\u5408\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u52a8\u6001\u7f51\u7edc\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u7684\u6027\u80fd\uff0c\u5177\u6709\u751f\u7269\u542f\u53d1\u6027\u3002"}}
{"id": "2507.09948", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09948", "abs": "https://arxiv.org/abs/2507.09948", "authors": ["Zijian Ding", "Tung Nguyen", "Weikai Li", "Aditya Grover", "Yizhou Sun", "Jason Cong"], "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data", "comment": "9 pages. accepted to ICLAD'25", "summary": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faIceberg\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c\u5f31\u6807\u7b7e\u751f\u6210\uff0c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u786c\u4ef6\u8bbe\u8ba1HLS\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5efa\u6a21\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684HLS\u9884\u6d4b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u9002\u5e94\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5408\u6210\u6570\u636e\u589e\u5f3a\uff08Iceberg\uff09\uff0c\u7ed3\u5408LLM\u751f\u6210\u7a0b\u5e8f\u548c\u5f31\u6807\u7b7e\uff0c\u5e76\u96c6\u6210\u4e0a\u4e0b\u6587\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5143\u5b66\u4e60\u3002", "result": "\u5728\u516d\u79cd\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5efa\u6a21\u51c6\u786e\u7387\u51e0\u4f55\u5e73\u5747\u63d0\u534786.4%\uff0c\u79bb\u7ebfDSE\u6027\u80fd\u63d0\u53472.47\u500d\u548c1.12\u500d\u3002", "conclusion": "Iceberg\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u5f31\u6807\u7b7e\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u786c\u4ef6\u8bbe\u8ba1\u573a\u666f\u3002"}}
{"id": "2507.09949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09949", "abs": "https://arxiv.org/abs/2507.09949", "authors": ["Md Ahsanul Kabir", "Kareem Abdelfatah", "Mohammed Korayem", "Mohammad Al Hasan"], "title": "Hierarchical Job Classification with Similarity Graph Integration", "comment": null, "summary": "In the dynamic realm of online recruitment, accurate job classification is\nparamount for optimizing job recommendation systems, search rankings, and labor\nmarket analyses. As job markets evolve, the increasing complexity of job titles\nand descriptions necessitates sophisticated models that can effectively\nleverage intricate relationships within job data. Traditional text\nclassification methods often fall short, particularly due to their inability to\nfully utilize the hierarchical nature of industry categories. To address these\nlimitations, we propose a novel representation learning and classification\nmodel that embeds jobs and hierarchical industry categories into a latent\nembedding space. Our model integrates the Standard Occupational Classification\n(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both\ngraph and hierarchical relationships, thereby improving classification\naccuracy. By embedding hierarchical industry categories into a shared latent\nspace, we tackle cold start issues and enhance the dynamic matching of\ncandidates to job opportunities. Extensive experimentation on a large-scale\ndataset of job postings demonstrates the model's superior ability to leverage\nhierarchical structures and rich semantic features, significantly outperforming\nexisting methods. This research provides a robust framework for improving job\nclassification accuracy, supporting more informed decision-making in the\nrecruitment industry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8868\u793a\u5b66\u4e60\u548c\u5206\u7c7b\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u7ebf\u62db\u8058\u4e2d\u7684\u804c\u4f4d\u5206\u7c7b\uff0c\u901a\u8fc7\u5d4c\u5165\u5206\u5c42\u884c\u4e1a\u7c7b\u522b\u5230\u6f5c\u5728\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u7ebf\u62db\u8058\u4e2d\u804c\u4f4d\u5206\u7c7b\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u884c\u4e1a\u7c7b\u522b\u7684\u5206\u5c42\u7ed3\u6784\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u6807\u51c6\u804c\u4e1a\u5206\u7c7b\u7cfb\u7edf\uff08SOC\uff09\u548c\u5185\u90e8\u5206\u5c42\u5206\u7c7b\u6cd5Carotene\uff0c\u5c06\u804c\u4f4d\u548c\u5206\u5c42\u884c\u4e1a\u7c7b\u522b\u5d4c\u5165\u6f5c\u5728\u7a7a\u95f4\uff0c\u6355\u6349\u56fe\u548c\u5206\u5c42\u5173\u7cfb\u3002", "result": "\u5728\u5927\u89c4\u6a21\u804c\u4f4d\u53d1\u5e03\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u62db\u8058\u884c\u4e1a\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u804c\u4f4d\u5206\u7c7b\u6846\u67b6\uff0c\u652f\u6301\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.09952", "categories": ["cs.LG", "stat.AP", "stat.ME", "68T01(General topics in artificial intelligence),\n  62G05(Nonparametric estimation)"], "pdf": "https://arxiv.org/pdf/2507.09952", "abs": "https://arxiv.org/abs/2507.09952", "authors": ["Zerui Zhang", "Yumou Qiu"], "title": "Radial Neighborhood Smoothing Recommender System", "comment": "34 pages, 2 figures. Submitted to NeurIPS 2025", "summary": "Recommender systems inherently exhibit a low-rank structure in latent space.\nA key challenge is to define meaningful and measurable distances in the latent\nspace to capture user-user, item-item, user-item relationships effectively. In\nthis work, we establish that distances in the latent space can be\nsystematically approximated using row-wise and column-wise distances in the\nobserved matrix, providing a novel perspective on distance estimation. To\nrefine the distance estimation, we introduce the correction based on empirical\nvariance estimator to account for noise-induced non-centrality. The novel\ndistance estimation enables a more structured approach to constructing\nneighborhoods, leading to the Radial Neighborhood Estimator (RNE), which\nconstructs neighborhoods by including both overlapped and partially overlapped\nuser-item pairs and employs neighborhood smoothing via localized kernel\nregression to improve imputation accuracy. We provide the theoretical\nasymptotic analysis for the proposed estimator. We perform evaluations on both\nsimulated and real-world datasets, demonstrating that RNE achieves superior\nperformance compared to existing collaborative filtering and matrix\nfactorization methods. While our primary focus is on distance estimation in\nlatent space, we find that RNE also mitigates the ``cold-start'' problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u884c\u548c\u5217\u8ddd\u79bb\u8fd1\u4f3c\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\uff0c\u5e76\u5f15\u5165\u6821\u6b63\u9879\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u6700\u7ec8\u63d0\u51faRadial Neighborhood Estimator\uff08RNE\uff09\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u6f5c\u5728\u7a7a\u95f4\u5177\u6709\u4f4e\u79e9\u7ed3\u6784\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5b9a\u4e49\u548c\u6d4b\u91cf\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u4ee5\u6355\u6349\u7528\u6237\u548c\u7269\u54c1\u5173\u7cfb\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u884c\u548c\u5217\u8ddd\u79bb\u8fd1\u4f3c\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\uff0c\u5f15\u5165\u57fa\u4e8e\u7ecf\u9a8c\u65b9\u5dee\u4f30\u8ba1\u7684\u6821\u6b63\u9879\uff0c\u63d0\u51faRNE\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cd\u53e0\u548c\u90e8\u5206\u91cd\u53e0\u7528\u6237-\u7269\u54c1\u5bf9\u6784\u5efa\u90bb\u57df\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u6838\u56de\u5f52\u5e73\u6ed1\u63d0\u5347\u586b\u8865\u7cbe\u5ea6\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRNE\u4f18\u4e8e\u73b0\u6709\u534f\u540c\u8fc7\u6ee4\u548c\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\u3002", "conclusion": "RNE\u4e3a\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\u4f30\u8ba1\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\uff0c\u540c\u65f6\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u3002"}}
{"id": "2507.09958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09958", "abs": "https://arxiv.org/abs/2507.09958", "authors": ["Zhenyuan Chen"], "title": "Rethinking Inductive Bias in Geographically Neural Network Weighted Regression", "comment": null, "summary": "Inductive bias is a key factor in spatial regression models, determining how\nwell a model can learn from limited data and capture spatial patterns. This\nwork revisits the inductive biases in Geographically Neural Network Weighted\nRegression (GNNWR) and identifies limitations in current approaches for\nmodeling spatial non-stationarity. While GNNWR extends traditional\nGeographically Weighted Regression by using neural networks to learn spatial\nweighting functions, existing implementations are often restricted by fixed\ndistance-based schemes and limited inductive bias. We propose to generalize\nGNNWR by incorporating concepts from convolutional neural networks, recurrent\nneural networks, and transformers, introducing local receptive fields,\nsequential context, and self-attention into spatial regression. Through\nextensive benchmarking on synthetic spatial datasets with varying\nheterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic\nmethods in capturing nonlinear and complex spatial relationships. Our results\nalso reveal that model performance depends strongly on data characteristics,\nwith local models excelling in highly heterogeneous or small-sample scenarios,\nand global models performing better with larger, more homogeneous data. These\nfindings highlight the importance of inductive bias in spatial modeling and\nsuggest future directions, including learnable spatial weighting functions,\nhybrid neural architectures, and improved interpretability for models handling\nnon-stationary spatial data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7a7a\u95f4\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u5f52\u7eb3\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdbGNNWR\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165CNN\u3001RNN\u548cTransformer\u7684\u6982\u5ff5\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GNNWR\u6a21\u578b\u5728\u6355\u6349\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u65f6\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u8ddd\u79bb\u65b9\u6848\u548c\u6709\u9650\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7ed3\u5408CNN\u7684\u5c40\u90e8\u611f\u53d7\u91ce\u3001RNN\u7684\u5e8f\u5217\u4e0a\u4e0b\u6587\u548cTransformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6539\u8fdbGNNWR\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u6539\u8fdb\u540e\u7684GNNWR\u5728\u6355\u6349\u975e\u7ebf\u6027\u7a7a\u95f4\u5173\u7cfb\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u53d7\u6570\u636e\u7279\u5f81\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u5f52\u7eb3\u504f\u5dee\u5bf9\u7a7a\u95f4\u5efa\u6a21\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u6743\u91cd\u51fd\u6570\u548c\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002"}}
{"id": "2507.09961", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09961", "abs": "https://arxiv.org/abs/2507.09961", "authors": ["Lihua Zhou", "Mao Ye", "Nianxin Li", "Shuaifeng Li", "Jinlin Wu", "Xiatian Zhu", "Lei Deng", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "Text-Driven Causal Representation Learning for Source-Free Domain Generalization", "comment": "Under Review", "summary": "Deep learning often struggles when training and test data distributions\ndiffer. Traditional domain generalization (DG) tackles this by including data\nfrom multiple source domains, which is impractical due to expensive data\ncollection and annotation. Recent vision-language models like CLIP enable\nsource-free domain generalization (SFDG) by using text prompts to simulate\nvisual representations, reducing data demands. However, existing SFDG methods\nstruggle with domain-specific confounders, limiting their generalization\ncapabilities. To address this issue, we propose TDCRL\n(\\textbf{T}ext-\\textbf{D}riven \\textbf{C}ausal \\textbf{R}epresentation\n\\textbf{L}earning), the first method to integrate causal inference into the\nSFDG setting. TDCRL operates in two steps: first, it employs data augmentation\nto generate style word vectors, combining them with class information to\ngenerate text embeddings to simulate visual representations; second, it trains\na causal intervention network with a confounder dictionary to extract\ndomain-invariant features. Grounded in causal learning, our approach offers a\nclear and effective mechanism to achieve robust, domain-invariant features,\nensuring robust generalization. Extensive experiments on PACS, VLCS,\nOfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL\neffectiveness in SFDG.", "AI": {"tldr": "TDCRL\u662f\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u7684\u6e90\u81ea\u7531\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u751f\u6210\u89c6\u89c9\u8868\u793a\u5e76\u63d0\u53d6\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57df\u6cdb\u5316\u65b9\u6cd5\u9700\u8981\u591a\u6e90\u57df\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u4e0d\u5207\u5b9e\u9645\uff1b\u73b0\u6709\u6e90\u81ea\u7531\u57df\u6cdb\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u9886\u57df\u7279\u5f02\u6027\u6df7\u6742\u56e0\u7d20\u3002", "method": "TDCRL\u5206\u4e24\u6b65\uff1a1\uff09\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u751f\u6210\u98ce\u683c\u8bcd\u5411\u91cf\uff0c\u7ed3\u5408\u7c7b\u522b\u4fe1\u606f\u751f\u6210\u6587\u672c\u5d4c\u5165\u6a21\u62df\u89c6\u89c9\u8868\u793a\uff1b2\uff09\u8bad\u7ec3\u56e0\u679c\u5e72\u9884\u7f51\u7edc\u63d0\u53d6\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5728PACS\u3001VLCS\u3001OfficeHome\u548cDomainNet\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "TDCRL\u901a\u8fc7\u56e0\u679c\u5b66\u4e60\u673a\u5236\u6709\u6548\u63d0\u53d6\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u4e3a\u6e90\u81ea\u7531\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.09968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09968", "abs": "https://arxiv.org/abs/2507.09968", "authors": ["Xiangyu Sun", "Amin Yousefpour", "Shirin Hosseinmardi", "Ramin Bostanabad"], "title": "Compliance Minimization via Physics-Informed Gaussian Processes", "comment": null, "summary": "Machine learning (ML) techniques have recently gained significant attention\nfor solving compliance minimization (CM) problems. However, these methods\ntypically provide poor feature boundaries, are very expensive, and lack a\nsystematic mechanism to control the design complexity. Herein, we address these\nlimitations by proposing a mesh-free and simultaneous framework based on\nphysics-informed Gaussian processes (GPs). In our approach, we parameterize the\ndesign and state variables with GP priors which have independent kernels but\nshare a multi-output neural network (NN) as their mean function. The\narchitecture of this NN is based on Parametric Grid Convolutional Attention\nNetworks (PGCANs) which not only mitigate spectral bias issues, but also\nprovide an interpretable mechanism to control design complexity. We estimate\nall the parameters of our GP-based representations by simultaneously minimizing\nthe compliance, total potential energy, and residual of volume fraction\nconstraint. Importantly, our loss function exclude all data-based residuals as\nGPs automatically satisfy them. We also develop computational schemes based on\ncurriculum training and numerical integration to increase the efficiency and\nrobustness of our approach which is shown to (1) produce super-resolution\ntopologies with fast convergence, (2) achieve smaller compliance and less gray\narea fraction compared to traditional numerical methods, (3) provide control\nover fine-scale features, and (4) outperform competing ML-based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b\u7684\u7f51\u683c\u81ea\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5408\u89c4\u6027\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u8f93\u51fa\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u8bbe\u8ba1\u590d\u6742\u6027\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709ML\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709ML\u65b9\u6cd5\u5728\u89e3\u51b3\u5408\u89c4\u6027\u6700\u5c0f\u5316\u95ee\u9898\u65f6\u5b58\u5728\u7279\u5f81\u8fb9\u754c\u4e0d\u6e05\u6670\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7f3a\u4e4f\u8bbe\u8ba1\u590d\u6742\u6027\u63a7\u5236\u673a\u5236\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u548c\u591a\u8f93\u51fa\u795e\u7ecf\u7f51\u7edc\uff08PGCANs\uff09\u53c2\u6570\u5316\u8bbe\u8ba1\u548c\u72b6\u6001\u53d8\u91cf\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5408\u89c4\u6027\u3001\u603b\u52bf\u80fd\u548c\u4f53\u79ef\u5206\u6570\u7ea6\u675f\u6b8b\u5dee\u6765\u4f30\u8ba1\u53c2\u6570\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u8d85\u5206\u8fa8\u7387\u62d3\u6251\u7ed3\u6784\uff0c\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u5408\u89c4\u6027\u548c\u7070\u5ea6\u533a\u57df\u5206\u6570\u4f18\u4e8e\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\uff0c\u5e76\u80fd\u63a7\u5236\u7cbe\u7ec6\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u89e3\u51b3\u5408\u89c4\u6027\u6700\u5c0f\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709ML\u65b9\u6cd5\u548c\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u3002"}}
{"id": "2507.10005", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.NE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.10005", "abs": "https://arxiv.org/abs/2507.10005", "authors": ["Yash Arya", "Sang Hoon Lee"], "title": "Effects of structural properties of neural networks on machine learning performance", "comment": "9 pages, 6 figures", "summary": "In recent years, graph-based machine learning techniques, such as\nreinforcement learning and graph neural networks, have garnered significant\nattention. While some recent studies have started to explore the relationship\nbetween the graph structure of neural networks and their predictive\nperformance, they often limit themselves to a narrow range of model networks,\nparticularly lacking mesoscale structures such as communities. Our work\nadvances this area by conducting a more comprehensive investigation,\nincorporating realistic network structures characterized by heterogeneous\ndegree distributions and community structures, which are typical\ncharacteristics of many real networks. These community structures offer a\nnuanced perspective on network architecture. Our analysis employs model\nnetworks such as random and scale-free networks, alongside a comparison with a\nbiological neural network and its subsets for more detailed analysis. We\nexamine the impact of these structural attributes on the performance of image\nclassification tasks. Our findings reveal that structural properties do affect\nperformance to some extent. Specifically, networks featuring coherent, densely\ninterconnected communities demonstrate enhanced learning capabilities. The\ncomparison with the biological neural network emphasizes the relevance of our\nfindings to real-world structures, suggesting an intriguing connection worth\nfurther exploration. This study contributes meaningfully to network science and\nmachine learning, providing insights that could inspire the design of more\nbiologically informed neural networks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u56fe\u7ed3\u6784\u5bf9\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5177\u6709\u7d27\u5bc6\u8fde\u63a5\u793e\u533a\u7ed3\u6784\u7684\u7f51\u7edc\u5b66\u4e60\u80fd\u529b\u66f4\u5f3a\u3002", "motivation": "\u63a2\u7d22\u56fe\u7ed3\u6784\uff08\u5982\u793e\u533a\u7ed3\u6784\uff09\u5bf9\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u968f\u673a\u7f51\u7edc\u3001\u65e0\u6807\u5ea6\u7f51\u7edc\u548c\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\uff0c\u7814\u7a76\u7ed3\u6784\u5c5e\u6027\u5bf9\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u5177\u6709\u7d27\u5bc6\u8fde\u63a5\u793e\u533a\u7ed3\u6784\u7684\u7f51\u7edc\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7f51\u7edc\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u53ef\u80fd\u542f\u53d1\u66f4\u751f\u7269\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u3002"}}
{"id": "2507.10014", "categories": ["cs.LG", "92D30, 62M10, 68T07", "G.3; I.6.3; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.10014", "abs": "https://arxiv.org/abs/2507.10014", "authors": ["Ali Sarabi", "Arash Sarabi", "Hao Yan", "Beckett Sterner", "Petar Jevti\u0107"], "title": "Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach", "comment": null, "summary": "Coccidioidomycosis, commonly known as Valley Fever, remains a significant\npublic health concern in endemic regions of the southwestern United States.\nThis study develops the first graph neural network (GNN) model for forecasting\nValley Fever incidence in Arizona. The model integrates surveillance case data\nwith environmental predictors using graph structures, including soil\nconditions, atmospheric variables, agricultural indicators, and air quality\nmetrics. Our approach explores correlation-based relationships among variables\ninfluencing disease transmission. The model captures critical delays in disease\nprogression through lagged effects, enhancing its capacity to reflect complex\ntemporal dependencies in disease ecology. Results demonstrate that the GNN\narchitecture effectively models Valley Fever trends and provides insights into\nkey environmental drivers of disease incidence. These findings can inform early\nwarning systems and guide resource allocation for disease prevention efforts in\nhigh-risk areas.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u9996\u4e2a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4e9a\u5229\u6851\u90a3\u5dde\u7684\u8c37\u70ed\u75c5\u53d1\u75c5\u7387\uff0c\u7ed3\u5408\u73af\u5883\u56e0\u7d20\u548c\u75c5\u4f8b\u6570\u636e\uff0c\u63ed\u793a\u4e86\u5173\u952e\u7684\u73af\u5883\u9a71\u52a8\u56e0\u7d20\u3002", "motivation": "\u8c37\u70ed\u75c5\u5728\u7f8e\u56fd\u897f\u5357\u90e8\u6d41\u884c\u5730\u533a\u662f\u91cd\u8981\u7684\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u9700\u8981\u6709\u6548\u7684\u9884\u6d4b\u6a21\u578b\u4ee5\u6307\u5bfc\u9884\u9632\u63aa\u65bd\u3002", "method": "\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6574\u5408\u75c5\u4f8b\u6570\u636e\u548c\u73af\u5883\u9884\u6d4b\u56e0\u5b50\uff08\u5982\u571f\u58e4\u6761\u4ef6\u3001\u5927\u6c14\u53d8\u91cf\u7b49\uff09\uff0c\u63a2\u7d22\u53d8\u91cf\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5f15\u5165\u6ede\u540e\u6548\u5e94\u6355\u6349\u75be\u75c5\u8fdb\u5c55\u7684\u5ef6\u8fdf\u3002", "result": "GNN\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u8c37\u70ed\u75c5\u8d8b\u52bf\uff0c\u5e76\u8bc6\u522b\u4e86\u5f71\u54cd\u53d1\u75c5\u7387\u7684\u5173\u952e\u73af\u5883\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4e3a\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u548c\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u652f\u6301\uff0c\u6709\u52a9\u4e8e\u9ad8\u98ce\u9669\u5730\u533a\u7684\u75be\u75c5\u9884\u9632\u3002"}}
{"id": "2507.10039", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.10039", "abs": "https://arxiv.org/abs/2507.10039", "authors": ["Steven Palayew", "Bo Wang", "Gary Bader"], "title": "Towards Applying Large Language Models to Complement Single-Cell Foundation Models", "comment": null, "summary": "Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u5728\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u4e2d\u7684\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408scGPT\u548cLLMs\u7684scMPT\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4e92\u8865\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\uff08\u5982scGPT\uff09\u65e0\u6cd5\u5229\u7528\u751f\u7269\u5b66\u4e2d\u7684\u6587\u672c\u4fe1\u606f\uff0c\u800cLLMs\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u867d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86LLMs\u5728\u5355\u7ec6\u80de\u6570\u636e\u4e2d\u7684\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u5f00\u53d1\u4e86scMPT\u6a21\u578b\uff0c\u7ed3\u5408scGPT\u548cLLMs\u7684\u4f18\u52bf\u3002", "result": "scMPT\u8868\u73b0\u4f18\u4e8e\u5176\u7ec4\u6210\u90e8\u5206\uff0c\u4e14\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4e00\u81f4\u3002", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u8865\u5145\uff0c\u63d0\u5347\u5355\u7ec6\u80de\u5206\u6790\u6027\u80fd\u3002"}}
{"id": "2507.10048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10048", "abs": "https://arxiv.org/abs/2507.10048", "authors": ["Benedict Gerlach", "Marie Anastacio", "Holger H. Hoos"], "title": "On the Efficiency of Training Robust Decision Trees", "comment": "Presented as a poster at SAIV 2025", "summary": "As machine learning gets adopted into the industry quickly, trustworthiness\nis increasingly in focus. Yet, efficiency and sustainability of robust training\npipelines still have to be established. In this work, we consider a simple\npipeline for training adversarially robust decision trees and investigate the\nefficiency of each step. Our pipeline consists of three stages. Firstly, we\nchoose the perturbation size automatically for each dataset. For that, we\nintroduce a simple algorithm, instead of relying on intuition or prior work.\nMoreover, we show that the perturbation size can be estimated from smaller\nmodels than the one intended for full training, and thus significant gains in\nefficiency can be achieved. Secondly, we train state-of-the-art adversarial\ntraining methods and evaluate them regarding both their training time and\nadversarial accuracy. Thirdly, we certify the robustness of each of the models\nthus obtained and investigate the time required for this. We find that\nverification time, which is critical to the efficiency of the full pipeline, is\nnot correlated with training time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u5bf9\u6297\u9c81\u68d2\u51b3\u7b56\u6811\u7684\u9ad8\u6548\u6d41\u7a0b\uff0c\u5305\u62ec\u81ea\u52a8\u9009\u62e9\u6270\u52a8\u5927\u5c0f\u3001\u8bad\u7ec3\u548c\u9a8c\u8bc1\u9c81\u68d2\u6027\uff0c\u5e76\u53d1\u73b0\u9a8c\u8bc1\u65f6\u95f4\u4e0e\u8bad\u7ec3\u65f6\u95f4\u65e0\u5173\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u5728\u5de5\u4e1a\u4e2d\u7684\u5feb\u901f\u5e94\u7528\uff0c\u5176\u53ef\u4fe1\u8d56\u6027\u6210\u4e3a\u7126\u70b9\uff0c\u4f46\u9c81\u68d2\u8bad\u7ec3\u6d41\u7a0b\u7684\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u4ecd\u9700\u5efa\u7acb\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u81ea\u52a8\u9009\u62e9\u6270\u52a8\u5927\u5c0f\u3001\u8bad\u7ec3\u5bf9\u6297\u9c81\u68d2\u6a21\u578b\u3001\u9a8c\u8bc1\u9c81\u68d2\u6027\u3002\u6270\u52a8\u5927\u5c0f\u53ef\u901a\u8fc7\u5c0f\u6a21\u578b\u4f30\u8ba1\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u9a8c\u8bc1\u65f6\u95f4\u5bf9\u6574\u4f53\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0e\u8bad\u7ec3\u65f6\u95f4\u65e0\u5173\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u4e3a\u5bf9\u6297\u9c81\u68d2\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10120", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10120", "abs": "https://arxiv.org/abs/2507.10120", "authors": ["Cheng Sun", "Zhen Zhang", "Shaofu Yang"], "title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization", "comment": "13 pages, 1 figure", "summary": "In this paper, we study a second-order approach to policy optimization in\nreinforcement learning. Existing second-order methods often suffer from\nsuboptimal sample complexity or rely on unrealistic assumptions about\nimportance sampling. To overcome these limitations, we propose VR-CR-PN, a\nvariance-reduced cubic-regularized policy Newton algorithm. To the best of our\nknowledge, this is the first algorithm that integrates Hessian-aided variance\nreduction with second-order policy optimization, effectively addressing the\ndistribution shift problem and achieving best-known sample complexity under\ngeneral nonconvex conditions but without the need for importance sampling. We\ntheoretically establish that VR-CR-PN achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order\nstationary point, significantly improving upon the previous best result of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an\nadditional contribution, we introduce a novel Hessian estimator for the\nexpected return function, which admits a uniform upper bound independent of the\nhorizon length $H$, allowing the algorithm to achieve horizon-independent\nsample complexity.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10158", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10158", "abs": "https://arxiv.org/abs/2507.10158", "authors": ["Obaidullah Zaland", "Erik Elmroth", "Monowar Bhuyan"], "title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "comment": "The work is accepted for presentation at IEEE SMC 2025", "summary": "Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.", "AI": {"tldr": "MTF-Grasp\u662f\u4e00\u79cd\u591a\u5c42\u7ea7\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u9488\u5bf9\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6570\u636e\u8d28\u91cf\u9ad8\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u521d\u59cb\u6a21\u578b\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e14\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faMTF-Grasp\uff0c\u9009\u62e9\u6570\u636e\u8d28\u91cf\u9ad8\u7684\u673a\u5668\u4eba\u8bad\u7ec3\u521d\u59cb\u6a21\u578b\uff0c\u518d\u5206\u53d1\u7ed9\u5176\u4ed6\u673a\u5668\u4eba\u3002", "result": "\u5728Cornell\u548cJacquard\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe8%\u3002", "conclusion": "MTF-Grasp\u6709\u6548\u89e3\u51b3\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10160", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10160", "abs": "https://arxiv.org/abs/2507.10160", "authors": ["Manuel R\u00f6der", "Christoph Raab", "Frank-Michael Schleif"], "title": "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation", "comment": "Extension of http://dx.doi.org/10.5220/0012351900003654", "summary": "Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment.", "AI": {"tldr": "FedAcross+\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u5ba2\u6237\u7aef\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u9002\u5e94\u6027\u6a21\u5757\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u652f\u6301\u6d41\u5f0f\u6570\u636e\u5904\u7406\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u5e94\u7528\u9762\u4e34\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u534f\u53d8\u91cf\u504f\u79fb\u548c\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u6a21\u578b\u66f4\u65b0\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u6846\u67b6\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u6e90\u6a21\u578b\uff08\u5305\u62ec\u6df1\u5ea6\u9aa8\u5e72\u3001\u9002\u5e94\u6027\u6a21\u5757\u548c\u5206\u7c7b\u5668\uff09\uff0c\u5728\u5ba2\u6237\u7aef\u9002\u5e94\u65f6\u51bb\u7ed3\u9aa8\u5e72\u548c\u5206\u7c7b\u5668\uff0c\u4ec5\u8c03\u6574\u9002\u5e94\u6027\u6a21\u5757\u4ee5\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFedAcross+\u5728\u4f4e\u7aef\u8bbe\u5907\u4e0a\u80fd\u6709\u6548\u9002\u5e94\u76ee\u6807\u57df\uff0c\u89e3\u51b3\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u652f\u6301\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u96f6\u661f\u6a21\u578b\u66f4\u65b0\u3002", "conclusion": "FedAcross+\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u975e\u9759\u6001\u73af\u5883\u3002"}}
{"id": "2507.10170", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10170", "abs": "https://arxiv.org/abs/2507.10170", "authors": ["Wuyang Zhou", "Giorgos Iacovides", "Kriton Konstantinidis", "Ilya Kisil", "Danilo Mandic"], "title": "Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach", "comment": null, "summary": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u4f8b\u548c\u76f4\u89c2\u53ef\u89c6\u5316\uff0c\u6f84\u6e05\u5f20\u91cf\u7f51\u7edc\uff08TN\uff09\u5206\u89e3\u4e2dTN\u79e9\u7684\u6982\u5ff5\uff0c\u5e2e\u52a9\u8bfb\u8005\u7406\u89e3\u5176\u9009\u62e9\u548c\u89e3\u91ca\u3002", "motivation": "TN\u79e9\u5728\u5f20\u91cf\u5206\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u7f3a\u4e4f\u7edf\u4e00\u542b\u4e49\u548c\u76f4\u89c2\u89e3\u91ca\uff0c\u5e38\u88ab\u5f53\u4f5c\u7ecf\u9a8c\u8c03\u53c2\u800c\u975e\u8bbe\u8ba1\u53c2\u6570\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u9886\u57df\u77e5\u8bc6\u548c\u56fe\u5f62\u5316\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\uff08\u5982CP\u548cTucker\u5206\u89e3\uff09\u5c55\u793a\u9886\u57df\u77e5\u8bc6\u5982\u4f55\u6307\u5bfcTN\u79e9\u9009\u62e9\uff0c\u5e76\u91c7\u7528\u56fe\u5f62\u5316\u65b9\u6cd5\u63a8\u5e7f\u5230\u4efb\u610f\u9636\u5f20\u91cf\u3002", "result": "\u63ed\u793a\u4e86TN\u79e9\u4e0e\u5f20\u91cf\u5c55\u5f00\u77e9\u9635\u79e9\u7684\u5173\u7cfb\uff0c\u7b80\u5316\u4e86\u590d\u6742\u4ee3\u6570\u64cd\u4f5c\uff0c\u652f\u6301\u9886\u57df\u9a71\u52a8\u7684TN\u8bbe\u8ba1\u3002", "conclusion": "\u672c\u6587\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u5bf9TN\u79e9\u7684\u6e05\u6670\u7edf\u4e00\u7406\u89e3\uff0c\u652f\u6301\u5176\u5728\u5e94\u7528\u548c\u6559\u80b2\u4e2d\u7684\u9009\u62e9\u548c\u89e3\u91ca\u3002"}}
{"id": "2507.10172", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10172", "abs": "https://arxiv.org/abs/2507.10172", "authors": ["Ruizhe Yu Xia", "Jeremy Gow", "Simon Lucas"], "title": "Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS", "comment": "Accepted as Short Paper for IEEE CoG", "summary": "Play style identification can provide valuable game design insights and\nenable adaptive experiences, with the potential to improve game playing agents.\nPrevious work relies on domain knowledge to construct play trace\nrepresentations using handcrafted features. More recent approaches incorporate\nthe sequential structure of play traces but still require some level of domain\nabstraction. In this study, we explore the use of unsupervised CNN-LSTM\nautoencoder models to obtain latent representations directly from low-level\nplay trace data in MicroRTS. We demonstrate that this approach yields a\nmeaningful separation of different game playing agents in the latent space,\nreducing reliance on domain expertise and its associated biases. This latent\nspace is then used to guide the exploration of diverse play styles within\nstudied AI players.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u65e0\u76d1\u7763CNN-LSTM\u81ea\u7f16\u7801\u5668\u6a21\u578b\u4ece\u4f4e\u7ea7\u522b\u6e38\u620f\u6570\u636e\u4e2d\u76f4\u63a5\u83b7\u53d6\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u51cf\u5c11\u5bf9\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "motivation": "\u6e38\u620f\u98ce\u683c\u8bc6\u522b\u80fd\u4e3a\u6e38\u620f\u8bbe\u8ba1\u63d0\u4f9b\u89c1\u89e3\u5e76\u5b9e\u73b0\u81ea\u9002\u5e94\u4f53\u9a8c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u77e5\u8bc6\u6216\u62bd\u8c61\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763CNN-LSTM\u81ea\u7f16\u7801\u5668\u6a21\u578b\u5904\u7406MicroRTS\u4e2d\u7684\u4f4e\u7ea7\u522b\u6e38\u620f\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u4e0d\u540c\u6e38\u620f\u4ee3\u7406\u7684\u6709\u610f\u4e49\u5206\u79bb\uff0c\u51cf\u5c11\u4e86\u5bf9\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u63a2\u7d22AI\u73a9\u5bb6\u7684\u591a\u6837\u5316\u6e38\u620f\u98ce\u683c\uff0c\u51cf\u5c11\u9886\u57df\u504f\u89c1\u3002"}}
{"id": "2507.10183", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10183", "abs": "https://arxiv.org/abs/2507.10183", "authors": ["Alireza Dizaji", "Benedict Aaron Tjandra", "Mehrab Hamidi", "Shenyang Huang", "Guillaume Rabusseau"], "title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs", "comment": "Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)", "summary": "Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86T-GRAB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30Temporal Graph Neural Networks\uff08TGNNs\uff09\u5728\u65f6\u95f4\u63a8\u7406\u4e0a\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dTGNNs\u662f\u5426\u6709\u6548\u6355\u6349\u5468\u671f\u6027\u3001\u56e0\u679c\u5173\u7cfb\u548c\u957f\u7a0b\u4f9d\u8d56\u7b49\u6838\u5fc3\u65f6\u95f4\u6a21\u5f0f\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f15\u5165T-GRAB\uff0c\u4e00\u7ec4\u5408\u6210\u7684\u53ef\u63a7\u4efb\u52a1\uff0c\u7528\u4e8e\u9694\u79bb\u548c\u6d4b\u8bd5TGNNs\u7684\u5173\u952e\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e8611\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u6cdb\u5316\u65f6\u95f4\u6a21\u5f0f\u4e0a\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\u3002", "conclusion": "T-GRAB\u63ed\u793a\u4e86\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u9690\u85cf\u7684\u6311\u6218\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u7684\u67b6\u6784\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.10194", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10194", "abs": "https://arxiv.org/abs/2507.10194", "authors": ["Tassilo Klein", "Moin Nabi"], "title": "Learning Private Representations through Entropy-based Adversarial Training", "comment": null, "summary": "How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7126\u70b9\u71b5\u6765\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5982\u4f55\u5728\u4fdd\u6301\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u5b66\u4e60\u5177\u6709\u9ad8\u9884\u6d4b\u80fd\u529b\u7684\u8868\u793a\u3002", "method": "\u91c7\u7528\u5bf9\u6297\u6027\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f15\u5165\u7126\u70b9\u71b5\u4ee5\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5728\u9002\u5ea6\u9690\u79c1\u6cc4\u9732\u4e0b\u5177\u6709\u9ad8\u76ee\u6807\u6548\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6709\u6548\u7ef4\u6301\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.10215", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.10215", "abs": "https://arxiv.org/abs/2507.10215", "authors": ["Cencheng Shen", "Yuexiao Dong"], "title": "A Graph Sufficiency Perspective for Neural Networks", "comment": "23 pages", "summary": "This paper analyzes neural networks through graph variables and statistical\nsufficiency. We interpret neural network layers as graph-based transformations,\nwhere neurons act as pairwise functions between inputs and learned anchor\npoints. Within this formulation, we establish conditions under which layer\noutputs are sufficient for the layer inputs, that is, each layer preserves the\nconditional distribution of the target variable given the input variable. Under\ndense anchor point assumptions, we prove that asymptotic sufficiency holds in\nthe infinite-width limit and is preserved throughout training. To align more\nclosely with practical architectures, we further show that sufficiency can be\nachieved with finite-width networks by assuming region-separated input\ndistributions and constructing appropriate anchor points. Our framework covers\nfully connected layers, general pairwise functions, ReLU and sigmoid\nactivations, and convolutional neural networks. This work bridges statistical\nsufficiency, graph-theoretic representations, and deep learning, providing a\nnew statistical understanding of neural networks.", "AI": {"tldr": "\u901a\u8fc7\u56fe\u53d8\u91cf\u548c\u7edf\u8ba1\u5145\u5206\u6027\u5206\u6790\u795e\u7ecf\u7f51\u7edc\uff0c\u8bc1\u660e\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\uff0c\u5c42\u8f93\u51fa\u5bf9\u8f93\u5165\u5177\u6709\u7edf\u8ba1\u5145\u5206\u6027\uff0c\u5e76\u6269\u5c55\u5230\u6709\u9650\u5bbd\u5ea6\u7f51\u7edc\u3002", "motivation": "\u7ed3\u5408\u7edf\u8ba1\u5145\u5206\u6027\u3001\u56fe\u8bba\u8868\u793a\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u65b0\u7684\u7edf\u8ba1\u7406\u89e3\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u5c42\u89e3\u91ca\u4e3a\u57fa\u4e8e\u56fe\u7684\u53d8\u6362\uff0c\u795e\u7ecf\u5143\u4f5c\u4e3a\u8f93\u5165\u4e0e\u951a\u70b9\u95f4\u7684\u6210\u5bf9\u51fd\u6570\uff0c\u5206\u6790\u6761\u4ef6\u5206\u5e03\u4fdd\u6301\u7684\u6761\u4ef6\u3002", "result": "\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\uff0c\u5c42\u8f93\u51fa\u5177\u6709\u7edf\u8ba1\u5145\u5206\u6027\uff1b\u6709\u9650\u5bbd\u5ea6\u7f51\u7edc\u901a\u8fc7\u533a\u57df\u5206\u79bb\u8f93\u5165\u5206\u5e03\u548c\u9002\u5f53\u951a\u70b9\u6784\u9020\u4e5f\u80fd\u5b9e\u73b0\u5145\u5206\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u8ba1\u89c6\u89d2\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7f51\u7edc\u7ed3\u6784\u548c\u6fc0\u6d3b\u51fd\u6570\u3002"}}
{"id": "2507.10241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10241", "abs": "https://arxiv.org/abs/2507.10241", "authors": ["Vikas Dwivedi", "Balaji Srinivasan", "Monica Sigovan", "Bruno Sixou"], "title": "Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients", "comment": null, "summary": "This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning\nMachine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of\nPI-ELM designed to solve both forward and inverse Partial Differential Equation\n(PDE) problems involving localized sharp gradients. While PI-ELMs outperform\nthe traditional Physics-Informed Neural Networks (PINNs) in speed due to their\nsingle-shot, least square optimization, this advantage comes at a cost: their\nfixed, randomly initialized input layer limits their ability to capture sharp\ngradients. To overcome this limitation, we introduce a lightweight Bayesian\nOptimization (BO) framework that, instead of adjusting each input layer\nparameter individually as in traditional backpropagation, learns a small set of\nhyperparameters defining the statistical distribution from which the input\nweights are drawn. This novel distributional optimization strategy -- combining\nBO for input layer distributional parameters with least-squares optimization\nfor output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's\nspeed while matching or exceeding the expressiveness of PINNs. We validate the\nproposed methodology on several challenging forward and inverse PDE benchmarks,\nincluding a 1D singularly perturbed convection-diffusion equation, a 2D Poisson\nequation with sharp localized sources, and a time-dependent advection equation.\nNotably, KAPI-ELM achieves state-of-the-art accuracy in both forward and\ninverse settings. In stiff PDE regimes, it matches or even outperforms advanced\nmethods such as the Extended Theory of Functional Connections (XTFC), while\nrequiring nearly an order of magnitude fewer tunable parameters. These results\nestablish the potential of KAPI-ELM as a scalable, interpretable, and\ngeneralizable physics-informed learning framework, especially in stiff PDE\nregimes.", "AI": {"tldr": "KAPI-ELM\u662f\u4e00\u79cd\u57fa\u4e8eRBF\u7684\u81ea\u9002\u5e94\u6269\u5c55\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\uff0c\u89e3\u51b3\u4e86PI-ELM\u5728\u6355\u6349\u5c16\u9510\u68af\u5ea6\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2aPDE\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "PI-ELM\u867d\u7136\u901f\u5ea6\u5feb\uff0c\u4f46\u5176\u56fa\u5b9a\u8f93\u5165\u5c42\u9650\u5236\u4e86\u6355\u6349\u5c16\u9510\u68af\u5ea6\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u4f18\u5316\u8f93\u5165\u5c42\u53c2\u6570\u7684\u7edf\u8ba1\u5206\u5e03\uff0c\u7ed3\u5408\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u8f93\u51fa\u5c42\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKAPI-ELM\u5728\u6b63\u53cd\u95ee\u9898\u4e2d\u5747\u8fbe\u5230\u6700\u4f18\u7cbe\u5ea6\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "KAPI-ELM\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u901a\u7528\u7684\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u521a\u6027PDE\u95ee\u9898\u3002"}}
{"id": "2507.10273", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.10273", "abs": "https://arxiv.org/abs/2507.10273", "authors": ["Lu Zhu", "Emmanuel Noutahi"], "title": "Conditional Chemical Language Models are Versatile Tools in Drug Discovery", "comment": "12 pages, extra 13 pages of appendix", "summary": "Generative chemical language models (CLMs) have demonstrated strong\ncapabilities in molecular design, yet their impact in drug discovery remains\nlimited by the absence of reliable reward signals and the lack of\ninterpretability in their outputs. We present SAFE-T, a generalist chemical\nmodeling framework that conditions on biological context -- such as protein\ntargets or mechanisms of action -- to prioritize and design molecules without\nrelying on structural information or engineered scoring functions. SAFE-T\nmodels the conditional likelihood of fragment-based molecular sequences given a\nbiological prompt, enabling principled scoring of molecules across tasks such\nas virtual screening, drug-target interaction prediction, and activity cliff\ndetection. Moreover, it supports goal-directed generation by sampling from this\nlearned distribution, aligning molecular design with biological objectives. In\ncomprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,\nACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves\nperformance comparable to or better than existing approaches while being\nsignificantly faster. Fragment-level attribution further reveals that SAFE-T\ncaptures known structure-activity relationships, supporting interpretable and\nbiologically grounded design. Together with its computational efficiency, these\nresults demonstrate that conditional generative CLMs can unify scoring and\ngeneration to accelerate early-stage drug discovery.", "AI": {"tldr": "SAFE-T\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u4e0a\u4e0b\u6587\u7684\u6761\u4ef6\u751f\u6210\u5316\u5b66\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u5b50\u8bbe\u8ba1\u548c\u8bc4\u5206\uff0c\u65e0\u9700\u4f9d\u8d56\u7ed3\u6784\u4fe1\u606f\u6216\u4eba\u5de5\u8bc4\u5206\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u836f\u7269\u53d1\u73b0\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u56e0\u7f3a\u4e4f\u53ef\u9760\u5956\u52b1\u4fe1\u53f7\u548c\u8f93\u51fa\u53ef\u89e3\u91ca\u6027\u800c\u53d7\u9650\u3002", "method": "SAFE-T\u901a\u8fc7\u5efa\u6a21\u7247\u6bb5\u5206\u5b50\u5e8f\u5217\u5728\u751f\u7269\u63d0\u793a\u4e0b\u7684\u6761\u4ef6\u6982\u7387\uff0c\u652f\u6301\u865a\u62df\u7b5b\u9009\u3001\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7b49\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u5b9e\u73b0\u76ee\u6807\u5bfc\u5411\u7684\u5206\u5b50\u751f\u6210\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\uff0cSAFE-T\u6027\u80fd\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\uff1b\u7247\u6bb5\u7ea7\u5f52\u56e0\u663e\u793a\u5176\u80fd\u6355\u6349\u5df2\u77e5\u7ed3\u6784-\u6d3b\u6027\u5173\u7cfb\u3002", "conclusion": "SAFE-T\u8bc1\u660e\u6761\u4ef6\u751f\u6210\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u53ef\u7edf\u4e00\u8bc4\u5206\u4e0e\u751f\u6210\uff0c\u52a0\u901f\u65e9\u671f\u836f\u7269\u53d1\u73b0\u3002"}}
{"id": "2507.10296", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.10296", "abs": "https://arxiv.org/abs/2507.10296", "authors": ["Shijie Li", "Weiqiang He", "Ruobing Bai", "Pan Peng"], "title": "Average Sensitivity of Hierarchical $k$-Median Clustering", "comment": null, "summary": "Hierarchical clustering is a widely used method for unsupervised learning\nwith numerous applications. However, in the application of modern algorithms,\nthe datasets studied are usually large and dynamic. If the hierarchical\nclustering is sensitive to small perturbations of the dataset, the usability of\nthe algorithm will be greatly reduced. In this paper, we focus on the\nhierarchical $k$ -median clustering problem, which bridges hierarchical and\ncentroid-based clustering while offering theoretical appeal, practical utility,\nand improved interpretability. We analyze the average sensitivity of algorithms\nfor this problem by measuring the expected change in the output when a random\ndata point is deleted. We propose an efficient algorithm for hierarchical\n$k$-median clustering and theoretically prove its low average sensitivity and\nhigh clustering quality. Additionally, we show that single linkage clustering\nand a deterministic variant of the CLNSS algorithm exhibit high average\nsensitivity, making them less stable. Finally, we validate the robustness and\neffectiveness of our algorithm through experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5c42\u6b21k-\u4e2d\u503c\u805a\u7c7b\u95ee\u9898\u7684\u5e73\u5747\u654f\u611f\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f4e\u654f\u611f\u6027\u548c\u9ad8\u8d28\u91cf\u805a\u7c7b\u3002", "motivation": "\u73b0\u4ee3\u7b97\u6cd5\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u96c6\u901a\u5e38\u5e9e\u5927\u4e14\u52a8\u6001\uff0c\u82e5\u5c42\u6b21\u805a\u7c7b\u5bf9\u6570\u636e\u6270\u52a8\u654f\u611f\uff0c\u7b97\u6cd5\u5b9e\u7528\u6027\u5c06\u5927\u5e45\u964d\u4f4e\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u5220\u9664\u968f\u673a\u6570\u636e\u70b9\u65f6\u8f93\u51fa\u7684\u9884\u671f\u53d8\u5316\uff0c\u5206\u6790\u7b97\u6cd5\u7684\u5e73\u5747\u654f\u611f\u6027\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u5c42\u6b21k-\u4e2d\u503c\u805a\u7c7b\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8be5\u7b97\u6cd5\u5177\u6709\u4f4e\u5e73\u5747\u654f\u611f\u6027\u548c\u9ad8\u805a\u7c7b\u8d28\u91cf\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u5355\u94fe\u63a5\u805a\u7c7b\u548cCLNSS\u53d8\u4f53\u7684\u9ad8\u654f\u611f\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u6570\u636e\u96c6\u3002"}}
{"id": "2507.10311", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10311", "abs": "https://arxiv.org/abs/2507.10311", "authors": ["Liming Wang", "Saurabhchand Bhati", "Cody Karjadi", "Rhoda Au", "James Glass"], "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models", "comment": null, "summary": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861", "AI": {"tldr": "Demenba\u662f\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u81ea\u52a8\u75f4\u5446\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u97f3\u8bb0\u5f55\u5206\u6790\u8ba4\u77e5\u80fd\u529b\u4e0b\u964d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd521%\u3002", "motivation": "\u65e9\u671f\u53d1\u73b0\u75f4\u5446\u5bf9\u53ca\u65f6\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u795e\u7ecf\u5fc3\u7406\u5b66\u6d4b\u8bd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u5206\uff0c\u81ea\u52a8\u5316\u7cfb\u7edf\u80fd\u66f4\u9ad8\u6548\u5730\u5b8c\u6210\u8bc4\u4f30\u3002", "method": "\u63d0\u51faDemenba\u6846\u67b6\uff0c\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5185\u5b58\u548c\u8ba1\u7b97\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u8bad\u7ec3\u6570\u636e\u6765\u81eaFramingham\u5fc3\u810f\u7814\u7a76\u76841000\u591a\u5c0f\u65f6\u8ba4\u77e5\u8bc4\u4f30\u5f55\u97f3\u3002", "result": "Demenba\u5728\u7ec6\u7c92\u5ea6\u75f4\u5446\u5206\u7c7b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd521%\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\uff1b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u878d\u5408\u540e\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "Demenba\u4e3a\u75f4\u5446\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.10334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10334", "abs": "https://arxiv.org/abs/2507.10334", "authors": ["Mahmoud Bekhit", "Ahmad Salah", "Ahmed Salim Alrawahi", "Tarek Attia", "Ahmed Ali", "Esraa Eldesokey", "Ahmed Fathalla"], "title": "MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data", "comment": "22 pages, 7 figures, 3 algorithms, 2 tables", "summary": "Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)\nis vital for applications in sports science, but its utility is often\ncompromised by missing data. Despite numerous imputation techniques, a\nsystematic performance evaluation for IMU-derived MoCap time-series data is\nlacking. We address this gap by conducting a comprehensive comparative analysis\nof statistical, machine learning, and deep learning imputation methods. Our\nevaluation considers three distinct contexts: univariate time-series,\nmultivariate across subjects, and multivariate across kinematic angles. To\nfacilitate this benchmark, we introduce the first publicly available MoCap\ndataset designed specifically for imputation, featuring data from 53 karate\npractitioners. We simulate three controlled missingness mechanisms: missing\ncompletely at random (MCAR), block missingness, and a novel value-dependent\npattern at signal transition points. Our experiments, conducted on 39 kinematic\nvariables across all subjects, reveal that multivariate imputation frameworks\nconsistently outperform univariate approaches, particularly for complex\nmissingness. For instance, multivariate methods achieve up to a 50% mean\nabsolute error reduction (MAE from 10.8 to 5.8) compared to univariate\ntechniques for transition point missingness. Advanced models like Generative\nAdversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the\nhighest accuracy in these challenging scenarios. This work provides a critical\nbaseline for future research and offers practical recommendations for improving\nthe integrity and robustness of Mo-Cap data analysis.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f83\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u586b\u8865\u4e86IMU\u8fd0\u52a8\u6355\u6349\u6570\u636e\u7f3a\u5931\u503c\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u7a7a\u767d\uff0c\u53d1\u73b0\u591a\u53d8\u91cf\u65b9\u6cd5\u5728\u590d\u6742\u7f3a\u5931\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8fd0\u52a8\u6355\u6349\u6570\u636e\u7f3a\u5931\u95ee\u9898\u5f71\u54cd\u5176\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u586b\u8865\u65b9\u6cd5\uff0c\u5f15\u5165\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6a21\u62df\u4e09\u79cd\u7f3a\u5931\u673a\u5236\u3002", "result": "\u591a\u53d8\u91cf\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u53d8\u91cf\u65b9\u6cd5\uff0c\u5982GAIN\u548c\u8fed\u4ee3\u586b\u8865\u5668\u5728\u590d\u6742\u7f3a\u5931\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\uff0c\u5e76\u5efa\u8bae\u63d0\u5347\u8fd0\u52a8\u6355\u6349\u6570\u636e\u5b8c\u6574\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2507.10345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10345", "abs": "https://arxiv.org/abs/2507.10345", "authors": ["Yuwen Li", "Guozhi Zhang"], "title": "Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions", "comment": null, "summary": "This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU\nneural networks for Korobov functions. In terms of network width and depth, we\nderive nearly optimal super-approximation error bounds of order $2m$ in the\n$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with\n$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages\nsparse grid finite elements and the bit extraction technique. Our results\nimprove upon classical lowest order $L_\\infty$ and $H^1$ norm error bounds and\ndemonstrate that the expressivity of neural networks is largely unaffected by\nthe curse of dimensionality.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86ReLU\u795e\u7ecf\u7f51\u7edc\u5bf9Korobov\u51fd\u6570\u7684$L_p$\u548c$W^1_p$\u8303\u6570\u903c\u8fd1\u8bef\u5dee\uff0c\u63d0\u51fa\u4e86\u7f51\u7edc\u5bbd\u5ea6\u548c\u6df1\u5ea6\u76f8\u5173\u7684\u8d85\u903c\u8fd1\u8bef\u5dee\u754c\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u903c\u8fd1Korobov\u51fd\u6570\u65f6\u7684\u8868\u73b0\uff0c\u7a81\u7834\u7ef4\u5ea6\u707e\u96be\u7684\u9650\u5236\u3002", "method": "\u7ed3\u5408\u7a00\u758f\u7f51\u683c\u6709\u9650\u5143\u548c\u6bd4\u7279\u63d0\u53d6\u6280\u672f\uff0c\u5206\u6790\u7f51\u7edc\u903c\u8fd1\u8bef\u5dee\u3002", "result": "\u5728$L_p$\u8303\u6570\u4e0b\u5f97\u5230$2m$\u9636\u8bef\u5dee\u754c\uff0c$W^1_p$\u8303\u6570\u4e0b\u5f97\u5230$2m-2$\u9636\u8bef\u5dee\u754c\uff0c\u4f18\u4e8e\u7ecf\u5178\u7ed3\u679c\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u4e0d\u53d7\u7ef4\u5ea6\u707e\u96be\u5f71\u54cd\uff0c\u903c\u8fd1\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.10347", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10347", "abs": "https://arxiv.org/abs/2507.10347", "authors": ["Yan-Ting Chen", "Hao-Wei Chen", "Tsu-Ching Hsiao", "Chun-Yi Lee"], "title": "Parallel Sampling of Diffusion Models on $SO(3)$", "comment": "MVA2025", "summary": "In this paper, we design an algorithm to accelerate the diffusion process on\nthe $SO(3)$ manifold. The inherently sequential nature of diffusion models\nnecessitates substantial time for denoising perturbed data. To overcome this\nlimitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$\nspace. We demonstrate our algorithm on an existing method that employs\ndiffusion models to address the pose ambiguity problem. Moreover, we show that\nthis acceleration advantage occurs without any measurable degradation in task\nreward. The experiments reveal that our algorithm achieves a speed-up of up to\n4.9$\\times$, significantly reducing the latency for generating a single sample.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728$SO(3)$\u6d41\u5f62\u4e0a\u52a0\u901f\u6269\u6563\u8fc7\u7a0b\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6570\u503cPicard\u8fed\u4ee3\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u663e\u793a\u901f\u5ea6\u63d0\u5347\u8fbe4.9\u500d\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u5956\u52b1\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u7684\u987a\u5e8f\u6027\u5bfc\u81f4\u53bb\u566a\u8fc7\u7a0b\u8017\u65f6\uff0c\u9700\u52a0\u901f\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u91c7\u7528\u6570\u503cPicard\u8fed\u4ee3\u65b9\u6cd5\uff0c\u9002\u914d$SO(3)$\u7a7a\u95f4\u3002", "result": "\u7b97\u6cd5\u5728\u73b0\u6709\u65b9\u6cd5\u4e0a\u5b9e\u73b0\u901f\u5ea6\u63d0\u53474.9\u500d\uff0c\u4e14\u4efb\u52a1\u5956\u52b1\u65e0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u751f\u6210\u6837\u672c\u7684\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u89e3\u51b3\u59ff\u6001\u6a21\u7cca\u95ee\u9898\u3002"}}
{"id": "2507.10348", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10348", "abs": "https://arxiv.org/abs/2507.10348", "authors": ["Yichen Li"], "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning", "comment": null, "summary": "Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing\nattention for its ability to aggregate knowledge from heterogeneous models\nwhile keeping private data locally. To better aggregate knowledge from clients,\nensemble distillation, as a widely used and effective technique, is often\nemployed after global aggregation to enhance the performance of the global\nmodel. However, simply combining Hetero-FL and ensemble distillation does not\nalways yield promising results and can make the training process unstable. The\nreason is that existing methods primarily focus on logit distillation, which,\nwhile being model-agnostic with softmax predictions, fails to compensate for\nthe knowledge bias arising from heterogeneous models. To tackle this challenge,\nwe propose a stable and efficient Feature Distillation for model-heterogeneous\nFederated learning, dubbed FedFD, that can incorporate aligned feature\ninformation via orthogonal projection to integrate knowledge from heterogeneous\nmodels better. Specifically, a new feature-based ensemble federated knowledge\ndistillation paradigm is proposed. The global model on the server needs to\nmaintain a projection layer for each client-side model architecture to align\nthe features separately. Orthogonal techniques are employed to re-parameterize\nthe projection layer to mitigate knowledge bias from heterogeneous models and\nthus maximize the distilled knowledge. Extensive experiments show that FedFD\nachieves superior performance compared to state-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedFD\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u5bf9\u9f50\u7279\u5f81\u4fe1\u606f\uff0c\u63d0\u5347\u5168\u5c40\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56logit\u84b8\u998f\uff0c\u65e0\u6cd5\u89e3\u51b3\u5f02\u6784\u6a21\u578b\u5e26\u6765\u7684\u77e5\u8bc6\u504f\u5dee\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u84b8\u998f\u7684FedFD\u65b9\u6cd5\uff0c\u5229\u7528\u6b63\u4ea4\u6295\u5f71\u5c42\u5bf9\u9f50\u5ba2\u6237\u7aef\u6a21\u578b\u7279\u5f81\uff0c\u51cf\u5c11\u77e5\u8bc6\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFedFD\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedFD\u901a\u8fc7\u7279\u5f81\u5bf9\u9f50\u548c\u6b63\u4ea4\u6295\u5f71\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.10349", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10349", "abs": "https://arxiv.org/abs/2507.10349", "authors": ["Zhiyuan Zhao", "Sitan Yang", "Kin G. Olivares", "Boris N. Oreshkin", "Stan Vitebsky", "Michael W. Mahoney", "B. Aditya Prakash", "Dmitry Efimov"], "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting", "comment": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI\n  for Supply Chain: Today and Future", "summary": "Multi-horizon time series forecasting has many practical applications such as\ndemand forecasting. Accurate demand prediction is critical to help make buying\nand inventory decisions for supply chain management of e-commerce and physical\nretailers, and such predictions are typically required for future horizons\nextending tens of weeks. This is especially challenging during high-stake sales\nevents when demand peaks are particularly difficult to predict accurately.\nHowever, these events are important not only for managing supply chain\noperations but also for ensuring a seamless shopping experience for customers.\nTo address this challenge, we propose Temporal-Aligned Transformer (TAT), a\nmulti-horizon forecaster leveraging apriori-known context variables such as\nholiday and promotion events information for improving predictive performance.\nOur model consists of an encoder and decoder, both embedded with a novel\nTemporal Alignment Attention (TAA), designed to learn context-dependent\nalignment for peak demand forecasting. We conduct extensive empirical analysis\non two large-scale proprietary datasets from a large e-commerce retailer. We\ndemonstrate that TAT brings up to 30% accuracy improvement on peak demand\nforecasting while maintaining competitive overall performance compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTemporal-Aligned Transformer (TAT)\u7684\u591a\u65f6\u95f4\u8303\u56f4\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u5546\u548c\u5b9e\u4f53\u96f6\u552e\u4e2d\u9700\u6c42\u9ad8\u5cf0\u9884\u6d4b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4fc3\u9500\u6d3b\u52a8\u671f\u95f4\u3002", "motivation": "\u51c6\u786e\u7684\u957f\u671f\u9700\u6c42\u9884\u6d4b\u5bf9\u4f9b\u5e94\u94fe\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4fc3\u9500\u6d3b\u52a8\u671f\u95f4\u9700\u6c42\u9ad8\u5cf0\u96be\u4ee5\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u3002", "method": "TAT\u6a21\u578b\u7ed3\u5408\u4e86\u5df2\u77e5\u7684\u4e0a\u4e0b\u6587\u53d8\u91cf\uff08\u5982\u8282\u5047\u65e5\u548c\u4fc3\u9500\u4fe1\u606f\uff09\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684Temporal Alignment Attention (TAA)\u673a\u5236\u6765\u5b66\u4e60\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u578b\u7535\u5546\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTAT\u5728\u9700\u6c42\u9ad8\u5cf0\u9884\u6d4b\u4e0a\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e8630%\uff0c\u540c\u65f6\u6574\u4f53\u6027\u80fd\u4e0e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "TAT\u901a\u8fc7\u4e0a\u4e0b\u6587\u5bf9\u9f50\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5cf0\u9700\u6c42\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4f9b\u5e94\u94fe\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2507.10368", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.10368", "abs": "https://arxiv.org/abs/2507.10368", "authors": ["Yongjin Choi", "Chenying Liu", "Jorge Macedo"], "title": "Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation", "comment": null, "summary": "Deep Operator Networks (DeepONets) have emerged as a powerful surrogate\nmodeling framework for learning solution operators in PDE-governed systems.\nWhile their use is expanding across engineering disciplines, applications in\ngeotechnical engineering remain limited. This study systematically evaluates\nseveral DeepONet architectures for the one-dimensional consolidation problem.\nWe initially consider three architectures: a standard DeepONet with the\ncoefficient of consolidation embedded in the branch net (Models 1 and 2), and a\nphysics-inspired architecture with the coefficient embedded in the trunk net\n(Model 3). Results show that Model 3 outperforms the standard configurations\n(Models 1 and 2) but still has limitations when the target solution (excess\npore pressures) exhibits significant variation. To overcome this limitation, we\npropose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses\nthe identified limitations by capturing rapidly varying functions. All proposed\narchitectures achieve speedups ranging from 1.5 to 100 times over traditional\nexplicit and implicit solvers, with Model 4 being the most efficient. Larger\ncomputational savings are expected for more complex systems than the explored\n1D case, which is promising. Overall, the study highlights the potential of\nDeepONets to enable efficient, generalizable surrogate modeling in geotechnical\napplications, advancing the integration of scientific machine learning in\ngeotechnics, which is at an early stage.", "AI": {"tldr": "DeepONet\u67b6\u6784\u5728\u89e3\u51b3\u4e00\u7ef4\u56fa\u7ed3\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u6539\u8fdb\u7684Trunknet Fourier\u7279\u5f81\u589e\u5f3a\u6a21\u578b\uff08Model 4\uff09\u4f18\u4e8e\u6807\u51c6\u914d\u7f6e\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u63a2\u7d22DeepONet\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u56db\u79cdDeepONet\u67b6\u6784\uff0c\u5305\u62ec\u6807\u51c6\u6a21\u578b\u548c\u7269\u7406\u542f\u53d1\u5f0f\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684Trunknet Fourier\u7279\u5f81\u589e\u5f3a\u6a21\u578b\u3002", "result": "Model 4\u8868\u73b0\u6700\u4f73\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u53471.5\u81f3100\u500d\uff0c\u9002\u7528\u4e8e\u5feb\u901f\u53d8\u5316\u51fd\u6570\u3002", "conclusion": "DeepONet\u5728\u5ca9\u571f\u5de5\u7a0b\u4e2d\u5177\u6709\u9ad8\u6548\u3001\u901a\u7528\u7684\u66ff\u4ee3\u5efa\u6a21\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u5728\u5ca9\u571f\u9886\u57df\u7684\u65e9\u671f\u6574\u5408\u3002"}}
{"id": "2507.10382", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10382", "abs": "https://arxiv.org/abs/2507.10382", "authors": ["Yue Ding", "Conor McCarthy", "Kevin O'Shea", "Mingming Liu"], "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis", "comment": null, "summary": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e91\u548cLLM\u7684\u5171\u4eab\u7535\u52a8\u51fa\u884c\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u79fb\u52a8\u5e94\u7528\u4ee5\u63d0\u4f9b\u4e2a\u6027\u5316\u8def\u7ebf\u63a8\u8350\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6a21\u5757\u548cRAG\u6846\u67b6\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u51fa\u884c\u548c\u5171\u4eab\u7535\u52a8\u51fa\u884c\u670d\u52a1\u7684\u5174\u8d77\uff0c\u7528\u6237\u5bf9\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u7684\u9700\u6c42\u589e\u957f\uff0c\u63a8\u52a8\u4e86\u4e91\u548cLLM\u6280\u672f\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e91\u57faLLM\u9a71\u52a8\u7684\u5171\u4eab\u7535\u52a8\u51fa\u884c\u5e73\u53f0\uff0c\u7ed3\u5408\u79fb\u52a8\u5e94\u7528\u63d0\u4f9b\u4e2a\u6027\u5316\u8def\u7ebf\u63a8\u8350\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6a21\u5757\u548cRAG\u6846\u67b6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4f18\u5316\u6a21\u5757\u5728\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u4e0b\u8bc4\u4f30\u4e86\u65c5\u884c\u65f6\u95f4\u548c\u6210\u672c\uff1bRAG\u6846\u67b6\u5728\u6a21\u5f0f\u7ea7\u522b\u4e0a\u5bf9\u7528\u6237\u548c\u7cfb\u7edf\u64cd\u4f5c\u5458\u67e5\u8be2\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a0.98\u548c0.81\u3002", "conclusion": "\u8be5\u5e73\u53f0\u5c55\u793a\u4e86\u4e91\u548cLLM\u6280\u672f\u5728\u5171\u4eab\u7535\u52a8\u51fa\u884c\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4e3a\u667a\u80fd\u51b3\u7b56\u548c\u7528\u6237\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.10385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10385", "abs": "https://arxiv.org/abs/2507.10385", "authors": ["Md. Ahsanul Kabir", "Mohammad Al Hasan", "Aritra Mandal", "Liyang Hao", "Ishita Khan", "Daniel Tunkelang", "Zhe Wu"], "title": "Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model", "comment": null, "summary": "The major task of any e-commerce search engine is to retrieve the most\nrelevant inventory items, which best match the user intent reflected in a\nquery. This task is non-trivial due to many reasons, including ambiguous\nqueries, misaligned vocabulary between buyers, and sellers, over- or\nunder-constrained queries by the presence of too many or too few tokens. To\naddress these challenges, query reformulation is used, which modifies a user\nquery through token dropping, replacement or expansion, with the objective to\nbridge semantic gap between query tokens and users' search intent. Early\nmethods of query reformulation mostly used statistical measures derived from\ntoken co-occurrence frequencies from selective user sessions having clicks or\npurchases. In recent years, supervised deep learning approaches, specifically\ntransformer-based neural language models, or sequence-to-sequence models are\nbeing used for query reformulation task. However, these models do not utilize\nthe semantic tags of a query token, which are significant for capturing user\nintent of an e-commerce query. In this work, we pose query reformulation as a\ntoken classification task, and solve this task by designing a dependency-aware\ntransformer-based language model, TagBERT, which makes use of semantic tags of\na token for learning superior query phrase embedding. Experiments on large,\nreal-life e-commerce datasets show that TagBERT exhibits superior performance\nthan plethora of competing models, including BERT, eBERT, and\nSequence-to-Sequence transformer model for important token classification task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u6807\u7b7e\u7684\u4f9d\u8d56\u611f\u77e5Transformer\u6a21\u578bTagBERT\uff0c\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u91cd\u6784\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u641c\u7d22\u5f15\u64ce\u9762\u4e34\u67e5\u8be2\u6a21\u7cca\u3001\u8bcd\u6c47\u4e0d\u5bf9\u9f50\u7b49\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8bed\u4e49\u6807\u7b7e\uff0c\u5f71\u54cd\u4e86\u67e5\u8be2\u91cd\u6784\u7684\u6548\u679c\u3002", "method": "\u5c06\u67e5\u8be2\u91cd\u6784\u89c6\u4e3a\u4ee4\u724c\u5206\u7c7b\u4efb\u52a1\uff0c\u8bbe\u8ba1\u4f9d\u8d56\u611f\u77e5\u7684Transformer\u6a21\u578bTagBERT\uff0c\u5229\u7528\u4ee4\u724c\u7684\u8bed\u4e49\u6807\u7b7e\u5b66\u4e60\u66f4\u4f18\u7684\u67e5\u8be2\u77ed\u8bed\u5d4c\u5165\u3002", "result": "\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\uff0cTagBERT\u5728\u91cd\u8981\u4ee4\u724c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eBERT\u3001eBERT\u548c\u5e8f\u5217\u5230\u5e8f\u5217Transformer\u6a21\u578b\u3002", "conclusion": "TagBERT\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u6807\u7b7e\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u91cd\u6784\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u7535\u5b50\u5546\u52a1\u641c\u7d22\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10400", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.10400", "abs": "https://arxiv.org/abs/2507.10400", "authors": ["Nicholas Casetti", "Dylan Anstine", "Olexandr Isayev", "Connor W. Coley"], "title": "Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials", "comment": "32 pages, 5 figures", "summary": "Reaction mechanism search tools have demonstrated the ability to provide\ninsights into likely products and rate-limiting steps of reacting systems.\nHowever, reactions involving several concerted bond changes - as can be found\nin many key steps of natural product synthesis - can complicate the search\nprocess. To mitigate these complications, we present a mechanism search\nstrategy particularly suited to help expedite exploration of an exemplary\nfamily of such complex reactions, cyclizations. We provide a cost-effective\nstrategy for identifying relevant elementary reaction steps by combining\ngraph-based enumeration schemes and machine learning techniques for\nintermediate filtering. Key to this approach is our use of a neural network\npotential (NNP), AIMNet2-rxn, for computational evaluation of each candidate\nreaction pathway. In this article, we evaluate the NNP's ability to estimate\nactivation energies, demonstrate the correct anticipation of stereoselectivity,\nand recapitulate complex enabling steps in natural product synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u590d\u6742\u53cd\u5e94\uff08\u5982\u73af\u5316\u53cd\u5e94\uff09\u7684\u673a\u5236\u641c\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u56fe\u679a\u4e3e\u548c\u673a\u5668\u5b66\u4e60\u7b5b\u9009\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\uff08AIMNet2-rxn\uff09\u8bc4\u4f30\u53cd\u5e94\u8def\u5f84\u3002", "motivation": "\u590d\u6742\u53cd\u5e94\uff08\u5982\u591a\u952e\u534f\u540c\u53d8\u5316\u7684\u81ea\u7136\u4ea7\u7269\u5408\u6210\u5173\u952e\u6b65\u9aa4\uff09\u589e\u52a0\u4e86\u673a\u5236\u641c\u7d22\u7684\u96be\u5ea6\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u56fe\u679a\u4e3e\u548c\u673a\u5668\u5b66\u4e60\u7b5b\u9009\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\uff08AIMNet2-rxn\uff09\u8bc4\u4f30\u53cd\u5e94\u8def\u5f84\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\u80fd\u51c6\u786e\u4f30\u7b97\u6d3b\u5316\u80fd\u3001\u9884\u6d4b\u7acb\u4f53\u9009\u62e9\u6027\uff0c\u5e76\u590d\u73b0\u81ea\u7136\u4ea7\u7269\u5408\u6210\u7684\u5173\u952e\u6b65\u9aa4\u3002", "conclusion": "\u8be5\u7b56\u7565\u4e3a\u590d\u6742\u53cd\u5e94\u673a\u5236\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10401", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.10401", "abs": "https://arxiv.org/abs/2507.10401", "authors": ["Ryan Bausback", "Jingqiao Tang", "Lu Lu", "Feng Bao", "Toan Huynh"], "title": "Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning", "comment": null, "summary": "We develop a novel framework for uncertainty quantification in operator\nlearning, the Stochastic Operator Network (SON). SON combines the stochastic\noptimal control concepts of the Stochastic Neural Network (SNN) with the\nDeepONet. By formulating the branch net as an SDE and backpropagating through\nthe adjoint BSDE, we replace the gradient of the loss function with the\ngradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.\nThis allows SON to learn the uncertainty present in operators through its\ndiffusion parameters. We then demonstrate the effectiveness of SON when\nreplicating several noisy operators in 2D and 3D.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7b97\u5b50\u5b66\u4e60\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u65b0\u6846\u67b6\u2014\u2014\u968f\u673a\u7b97\u5b50\u7f51\u7edc\uff08SON\uff09\uff0c\u7ed3\u5408\u4e86\u968f\u673a\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u548cDeepONet\u7684\u6982\u5ff5\u3002", "motivation": "\u89e3\u51b3\u7b97\u5b50\u5b66\u4e60\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u5206\u652f\u7f51\u7edc\u5efa\u6a21\u4e3a\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\uff0c\u5e76\u901a\u8fc7\u4f34\u968fBSDE\u53cd\u5411\u4f20\u64ad\uff0c\u7528\u968f\u673a\u6700\u5927\u539f\u7406\u4e2d\u7684\u54c8\u5bc6\u987f\u68af\u5ea6\u66ff\u4ee3\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u8fdb\u884cSGD\u66f4\u65b0\u3002", "result": "SON\u80fd\u591f\u901a\u8fc7\u5176\u6269\u6563\u53c2\u6570\u5b66\u4e60\u7b97\u5b50\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u57282D\u548c3D\u566a\u58f0\u7b97\u5b50\u590d\u5236\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "SON\u662f\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u91cf\u5316\u5e76\u5b66\u4e60\u7b97\u5b50\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.10409", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10409", "abs": "https://arxiv.org/abs/2507.10409", "authors": ["Amine Lbath", "Ibtissam Labriji"], "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study", "comment": null, "summary": "This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX\n\\textit{student} model that emulates the performance of the \\textit{teacher}\nmodel but with reduced energy consumption. We experiment with different student\nmodel sizes, optimal teacher sizes, and KD hyperparameters. Performance is\nmeasured by comparing the Bit Error Rate (BER) performance versus\nSignal-to-Interference \\& Noise Ratio (SINR) values of the distilled model and\na model trained from scratch. The distilled models demonstrate a lower error\nfloor across SINR levels, highlighting the effectiveness of KD in achieving\nenergy-efficient AI solutions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728AI/ML\u6a21\u578b\u4e2d\u5e73\u8861\u80fd\u6e90\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6311\u6218\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u7d27\u51d1\u7684DeepRX\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u80fd\u6e90\u8282\u7701\u3002", "motivation": "\u89e3\u51b3AI/ML\u6a21\u578b\u5728\u80fd\u6e90\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728DeepRX\u63a5\u6536\u5668\u4e2d\u3002", "method": "\u4f7f\u7528\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u8bad\u7ec3\u7d27\u51d1\u7684DeepRX\u5b66\u751f\u6a21\u578b\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5927\u5c0f\u548c\u8d85\u53c2\u6570\uff0c\u8bc4\u4f30FLOPs/Watt\u548cFLOPs/clock\u3002", "result": "\u84b8\u998f\u6a21\u578b\u5728SINR\u6c34\u5e73\u4e0b\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u9519\u8bef\u7387\uff0c\u9a8c\u8bc1\u4e86KD\u5728\u80fd\u6e90\u9ad8\u6548AI\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u80fd\u6e90\u9ad8\u6548AI\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.10434", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10434", "abs": "https://arxiv.org/abs/2507.10434", "authors": ["Giacomo Cignoni", "Andrea Cossu", "Alexandra Gomez-Villa", "Joost van de Weijer", "Antonio Carta"], "title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning", "comment": "Accepted at CoLLAs 2025 conference", "summary": "Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.", "AI": {"tldr": "CLA\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\uff0c\u901a\u8fc7\u5bf9\u9f50\u5f53\u524d\u4e0e\u8fc7\u53bb\u7684\u8868\u5f81\u51cf\u8f7b\u9057\u5fd8\uff0c\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u5e76\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u4e2d\uff0c\u6570\u636e\u4ee5\u5c0f\u6279\u91cf\u5230\u8fbe\u4e14\u65e0\u4efb\u52a1\u8fb9\u754c\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u8f83\u5c11\uff0c\u9700\u89e3\u51b3\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51faContinual Latent Alignment (CLA)\uff0c\u901a\u8fc7\u5bf9\u9f50\u5f53\u524d\u4e0e\u8fc7\u53bb\u7684\u8868\u5f81\u6765\u51cf\u5c11\u9057\u5fd8\u3002", "result": "CLA\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u65e9\u671f\u9884\u8bad\u7ec3\u4e2d\u4f7f\u7528CLA\u6bd4\u5168i.i.d.\u9884\u8bad\u7ec3\u6548\u679c\u66f4\u597d\u3002", "conclusion": "CLA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u81ea\u76d1\u7763\u7b56\u7565\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.10442", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10442", "abs": "https://arxiv.org/abs/2507.10442", "authors": ["Shivam Chandhok", "Wan-Cyuan Fan", "Vered Shwartz", "Vineeth N Balasubramanian", "Leonid Sigal"], "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities", "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Vision-language Models (VLMs) have emerged as general-purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, lacking some basic visual\nunderstanding skills. In this paper, we set out to understand the limitations\nof SoTA VLMs on fundamental visual tasks by constructing a series of tests that\nprobe which components of design, specifically, may be lacking. Importantly, we\ngo significantly beyond the current benchmarks, which simply measure the final\nperformance of VLM response, by also comparing and contrasting it to the\nperformance of probes trained directly on features obtained from the visual\nencoder, intermediate vision-language projection and LLM-decoder output. In\ndoing so, we uncover shortcomings in VLMs and make a number of important\nobservations about their capabilities, robustness and how they process visual\ninformation. We hope our insights will guide progress in further improving\nVLMs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6784\u5efa\u4e00\u7cfb\u5217\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u5728\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1VLMs\u5728\u590d\u6742\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u4e00\u4e9b\u57fa\u672c\u7684\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u5e76\u63a8\u52a8\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83VLMs\u7684\u6700\u7ec8\u8868\u73b0\u4e0e\u89c6\u89c9\u7f16\u7801\u5668\u3001\u4e2d\u95f4\u89c6\u89c9\u8bed\u8a00\u6295\u5f71\u548cLLM\u89e3\u7801\u5668\u8f93\u51fa\u7684\u6027\u80fd\uff0c\u6784\u5efa\u6d4b\u8bd5\u4ee5\u5206\u6790\u5176\u8bbe\u8ba1\u7f3a\u9677\u3002", "result": "\u7814\u7a76\u53d1\u73b0VLMs\u5b58\u5728\u4e00\u4e9b\u4e0d\u8db3\uff0c\u5e76\u5bf9\u5176\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\u63d0\u51fa\u4e86\u91cd\u8981\u89c2\u5bdf\u3002", "conclusion": "\u672c\u6587\u7684\u53d1\u73b0\u4e3a\u6539\u8fdbVLMs\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5e0c\u671b\u63a8\u52a8\u5176\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.10452", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10452", "abs": "https://arxiv.org/abs/2507.10452", "authors": ["Eduardo D. Sontag"], "title": "Some remarks on gradient dominance and LQR policy optimization", "comment": "This is a short paper summarizing the first part of the slides\n  presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control\n  Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has\n  been added. A second part on neural net feedback controllers is to be added", "summary": "Solutions of optimization problems, including policy optimization in\nreinforcement learning, typically rely upon some variant of gradient descent.\nThere has been much recent work in the machine learning, control, and\noptimization communities applying the Polyak-{\\L}ojasiewicz Inequality (PLI) to\nsuch problems in order to establish an exponential rate of convergence (a.k.a.\n``linear convergence'' in the local-iteration language of numerical analysis)\nof loss functions to their minima under the gradient flow. Often, as is the\ncase of policy iteration for the continuous-time LQR problem, this rate\nvanishes for large initial conditions, resulting in a mixed globally linear /\nlocally exponential behavior. This is in sharp contrast with the discrete-time\nLQR problem, where there is global exponential convergence. That gap between CT\nand DT behaviors motivates the search for various generalized PLI-like\nconditions, and this talk will address that topic. Moreover, these\ngeneralizations are key to understanding the transient and asymptotic effects\nof errors in the estimation of the gradient, errors which might arise from\nadversarial attacks, wrong evaluation by an oracle, early stopping of a\nsimulation, inaccurate and very approximate digital twins, stochastic\ncomputations (algorithm ``reproducibility''), or learning by sampling from\nlimited data. We describe an ``input to state stability'' (ISS) analysis of\nthis issue. The lecture also discussed convergence and PLI-like properties of\n``linear feedforward neural networks'' in feedback control, but this arXiv\nskips that part (to be updated). Much of the work described here was done in\ncollaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping\nJiang, and Milad Siami.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f18\u5316\u95ee\u9898\u4e2d\u68af\u5ea6\u4e0b\u964d\u7684\u6536\u655b\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7Polyak-\u0141ojasiewicz\u4e0d\u7b49\u5f0f\uff08PLI\uff09\u53ca\u5176\u63a8\u5e7f\u6761\u4ef6\uff0c\u5206\u6790\u4e86\u8fde\u7eed\u65f6\u95f4\u4e0e\u79bb\u6563\u65f6\u95f4LQR\u95ee\u9898\u7684\u6536\u655b\u5dee\u5f02\uff0c\u5e76\u7814\u7a76\u4e86\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4LQR\u95ee\u9898\u4e2d\u6536\u655b\u901f\u7387\u968f\u521d\u59cb\u6761\u4ef6\u589e\u5927\u800c\u6d88\u5931\u7684\u73b0\u8c61\uff0c\u4ee5\u53ca\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u5bf9\u4f18\u5316\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5e94\u7528PLI\u53ca\u5176\u63a8\u5e7f\u6761\u4ef6\uff0c\u7ed3\u5408\u8f93\u5165\u5230\u72b6\u6001\u7a33\u5b9a\u6027\uff08ISS\uff09\u5206\u6790\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8fde\u7eed\u65f6\u95f4\u4e0e\u79bb\u6563\u65f6\u95f4LQR\u95ee\u9898\u7684\u6536\u655b\u884c\u4e3a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63a8\u5e7f\u7684PLI\u6761\u4ef6\u6709\u52a9\u4e8e\u7406\u89e3\u68af\u5ea6\u8bef\u5dee\u7684\u77ac\u6001\u548c\u6e10\u8fd1\u6548\u5e94\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u63a8\u5e7f\u7684PLI\u6761\u4ef6\u662f\u7406\u89e3\u4f18\u5316\u95ee\u9898\u6536\u655b\u884c\u4e3a\u7684\u5173\u952e\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u68af\u5ea6\u8bef\u5dee\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.10484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10484", "abs": "https://arxiv.org/abs/2507.10484", "authors": ["Paul Fogel", "Christophe Geissler", "George Luta"], "title": "The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization", "comment": "6 pages, 4 figures, International Conference on Robust Statistics\n  2025, Stresa, Italy", "summary": "This paper introduces the \"Target Polish,\" a robust and computationally\nefficient framework for nonnegative matrix and tensor factorization. Although\nconventional weighted NMF approaches are resistant to outliers, they converge\nslowly due to the use of multiplicative updates to minimize the objective\ncriterion. In contrast, the Target Polish approach remains compatible with the\nFast-HALS algorithm, which is renowned for its speed, by adaptively smoothing\nthe data with a weighted median-based transformation. This innovation provides\noutlier resistance while maintaining the highly efficient additive update\nstructure of Fast-HALS. Empirical evaluations using image datasets corrupted\nwith structured (block) and unstructured (salt) noise demonstrate that the\nTarget Polish approach matches or exceeds the accuracy of state-of-the-art\nrobust NMF methods and reduces computational time by an order of magnitude in\nthe studied scenarios.", "AI": {"tldr": "Target Polish\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u975e\u8d1f\u77e9\u9635\u548c\u5f20\u91cf\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u6743\u4e2d\u4f4d\u6570\u53d8\u6362\u5b9e\u73b0\u5f02\u5e38\u503c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301Fast-HALS\u7b97\u6cd5\u7684\u9ad8\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u52a0\u6743NMF\u65b9\u6cd5\u5bf9\u5f02\u5e38\u503c\u9c81\u68d2\u4f46\u6536\u655b\u6162\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u62b5\u6297\u5f02\u5e38\u503c\u53c8\u80fd\u9ad8\u6548\u8ba1\u7b97\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u52a0\u6743\u4e2d\u4f4d\u6570\u53d8\u6362\u81ea\u9002\u5e94\u5e73\u6ed1\u6570\u636e\uff0c\u7ed3\u5408Fast-HALS\u7b97\u6cd5\u7684\u9ad8\u6548\u52a0\u6027\u66f4\u65b0\u7ed3\u6784\u3002", "result": "\u5728\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cTarget Polish\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u9c81\u68d2NMF\u65b9\u6cd5\u3002", "conclusion": "Target Polish\u5728\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u503c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10485", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10485", "abs": "https://arxiv.org/abs/2507.10485", "authors": ["Brandon Shuen Yi Loke", "Filippo Quadri", "Gabriel Vivanco", "Maximilian Casagrande", "Sa\u00fal Fenollosa"], "title": "Overcoming catastrophic forgetting in neural networks", "comment": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project", "summary": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f39\u6027\u6743\u91cd\u5de9\u56fa\uff08EWC\uff09\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u51cf\u5c11\u9057\u5fd8\u7684\u6548\u679c\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u5176\u4ed6\u6b63\u5219\u5316\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u9057\u5fd8\u65e7\u77e5\u8bc6\uff08\u707e\u96be\u6027\u9057\u5fd8\uff09\uff0cEWC\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5728PermutedMNIST\u548cRotatedMNIST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u8f83EWC\u4e0eL2\u6b63\u5219\u5316\u548c\u65e0\u6b63\u5219\u5316\u7684SGD\uff0c\u5206\u6790\u5176\u5e73\u8861\u77e5\u8bc6\u4fdd\u7559\u548c\u9002\u5e94\u6027\u7684\u80fd\u529b\u3002", "result": "EWC\u663e\u8457\u51cf\u5c11\u4e86\u9057\u5fd8\uff0c\u4f46\u7565\u5fae\u964d\u4f4e\u4e86\u65b0\u4efb\u52a1\u7684\u5b66\u4e60\u6548\u7387\u3002\u540c\u65f6\u63a2\u8ba8\u4e86dropout\u548c\u8d85\u53c2\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "EWC\u662f\u795e\u7ecf\u7f51\u7edc\u7ec8\u8eab\u5b66\u4e60\u7684\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.10494", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10494", "abs": "https://arxiv.org/abs/2507.10494", "authors": ["Tanveer Khan", "Mindaugas Budzys", "Antonis Michalas"], "title": "Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing", "comment": null, "summary": "Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy.", "AI": {"tldr": "SplitHappens\u7ed3\u5408FSS\u548cU\u5f62SL\uff0c\u63d0\u5347\u6570\u636e\u9690\u79c1\u4fdd\u62a4\uff0c\u51cf\u5c11\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u62b5\u5fa1\u591a\u79cd\u653b\u51fb\u3002", "motivation": "SL\u867d\u80fd\u4fdd\u62a4\u5ba2\u6237\u7aef\u6570\u636e\uff0c\u4f46\u6613\u53d7\u653b\u51fb\uff0c\u73b0\u6709FSS\u65b9\u6cd5\u672a\u80fd\u5168\u9762\u89e3\u51b3\u5b89\u5168\u95ee\u9898\u3002", "method": "\u6269\u5c55FSS\u4e0eSL\u7ed3\u5408\u81f3U\u5f62SL\uff0c\u4fdd\u7559\u6807\u7b7e\u9690\u79c1\uff0c\u62b5\u5fa1\u6a21\u578b\u53cd\u8f6c\u548c\u6807\u7b7e\u63a8\u65ad\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u548c\u901a\u4fe1\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "U\u5f62SL\u7ed3\u5408FSS\u63d0\u4f9b\u66f4\u9ad8\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u653b\u51fb\u573a\u666f\u3002"}}
{"id": "2507.10502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10502", "abs": "https://arxiv.org/abs/2507.10502", "authors": ["Elizabeth Fahsbender", "Alma Andersson", "Jeremy Ash", "Polina Binder", "Daniel Burkhardt", "Benjamin Chang", "Georg K. Gerber", "Anthony Gitter", "Patrick Godau", "Ankit Gupta", "Genevieve Haliburton", "Siyu He", "Trey Ideker", "Ivana Jelic", "Aly Khan", "Yang-Joon Kim", "Aditi Krishnapriyan", "Jon M. Laurent", "Tianyu Liu 28", "Emma Lundberg", "Shalin B. Mehta", "Rob Moccia", "Angela Oliveira Pisco", "Katherine S. Pollard", "Suresh Ramani", "Julio Saez-Rodriguez", "Yasin Senbabaoglu", "Elana Simon", "Srinivasan Sivanandan", "Gustavo Stolovitzky", "Marc Valer", "Bo Wang", "Xikun Zhang", "James Zou", "Katrina Kalantar"], "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop", "comment": null, "summary": "Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u751f\u7269\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u6307\u51fa\u7f3a\u4e4f\u6807\u51c6\u5316\u8de8\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u7684\u5efa\u8bae\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u5b66\u4e2dAI\u6a21\u578b\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u7814\u8ba8\u4f1a\u6c47\u96c6\u4e13\u5bb6\uff0c\u5206\u6790\u6570\u636e\u5f02\u8d28\u6027\u3001\u566a\u58f0\u3001\u53ef\u91cd\u590d\u6027\u7b49\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u63d0\u51fa\u9ad8\u8d28\u91cf\u6570\u636e\u7ba1\u7406\u3001\u6807\u51c6\u5316\u5de5\u5177\u3001\u5168\u9762\u8bc4\u4f30\u6307\u6807\u7b49\u5efa\u8bae\uff0c\u4ee5\u63a8\u52a8AI\u9a71\u52a8\u7684\u865a\u62df\u7ec6\u80de\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u6807\u51c6\u5316\u57fa\u51c6\u5c06\u4fc3\u8fdb\u751f\u7269\u5b66AI\u6a21\u578b\u7684\u4e25\u8c28\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u751f\u7269\u76f8\u5173\u6027\uff0c\u63a8\u52a8\u65b0\u53d1\u73b0\u548c\u7ec6\u80de\u7cfb\u7edf\u7406\u89e3\u3002"}}
{"id": "2507.10532", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10532", "abs": "https://arxiv.org/abs/2507.10532", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "comment": "26 pages", "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46Qwen2.5\u6a21\u578b\u5728\u5e38\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u53ef\u80fd\u56e0\u6570\u636e\u6c61\u67d3\u800c\u4e0d\u53ef\u9760\u3002\u4f5c\u8005\u63d0\u51fa\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6RandomCalculation\u9a8c\u8bc1RL\u65b9\u6cd5\uff0c\u53d1\u73b0\u4ec5\u51c6\u786e\u5956\u52b1\u4fe1\u53f7\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8RL\u65b9\u6cd5\u5728\u63d0\u5347LLMs\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u56e0\u6570\u636e\u6c61\u67d3\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5408\u6210\u6570\u636e\u96c6RandomCalculation\uff0c\u751f\u6210\u65e0\u6c61\u67d3\u7684\u7b97\u672f\u95ee\u9898\uff0c\u7528\u4e8e\u9a8c\u8bc1RL\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u65e0\u6c61\u67d3\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u51c6\u786e\u5956\u52b1\u4fe1\u53f7\u80fd\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u800c\u566a\u58f0\u6216\u9519\u8bef\u4fe1\u53f7\u65e0\u6548\u3002", "conclusion": "\u5efa\u8bae\u5728\u65e0\u6c61\u67d3\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6837\u5316\u6a21\u578b\u5bb6\u65cf\u4e2d\u8bc4\u4f30RL\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u7ed3\u8bba\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.10536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10536", "abs": "https://arxiv.org/abs/2507.10536", "authors": ["Qiaoyue Tang", "Alain Zhiyanov", "Mathias L\u00e9cuyer"], "title": "On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance", "comment": null, "summary": "In this work, we analyze the optimization behaviour of common private\nlearning optimization algorithms under heavy-tail class imbalanced\ndistribution. We show that, in a stylized model, optimizing with Gradient\nDescent with differential privacy (DP-GD) suffers when learning low-frequency\nclasses, whereas optimization algorithms that estimate second-order information\ndo not. In particular, DP-AdamBC that removes the DP bias from estimating loss\ncurvature is a crucial component to avoid the ill-condition caused by\nheavy-tail class imbalance, and empirically fits the data better with\n$\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the\nleast frequent classes on both controlled experiments and real data\nrespectively.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5728\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u5e03\u4e0b\u5e38\u89c1\u79c1\u6709\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\u7684\u884c\u4e3a\uff0c\u53d1\u73b0DP-GD\u5728\u4f4e\u9891\u7c7b\u522b\u5b66\u4e60\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cDP-AdamBC\u901a\u8fc7\u6d88\u9664DP\u504f\u5dee\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5728\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u5e03\u4e0b\uff0c\u79c1\u6709\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u7279\u522b\u662f\u4f4e\u9891\u7c7b\u522b\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6a21\u578b\u548c\u5b9e\u9a8c\u5bf9\u6bd4DP-GD\u4e0eDP-AdamBC\u5728\u91cd\u5c3e\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u540e\u8005\u901a\u8fc7\u6d88\u9664DP\u504f\u5dee\u4f18\u5316\u4e8c\u9636\u4fe1\u606f\u4f30\u8ba1\u3002", "result": "DP-AdamBC\u5728\u4f4e\u9891\u7c7b\u522b\u5b66\u4e60\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u8bad\u7ec3\u51c6\u786e\u7387\u63d0\u5347\u7ea68%\uff08\u63a7\u5236\u5b9e\u9a8c\uff09\u548c5%\uff08\u771f\u5b9e\u6570\u636e\uff09\u3002", "conclusion": "DP-AdamBC\u80fd\u6709\u6548\u7f13\u89e3\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u5e26\u6765\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4f4e\u9891\u7c7b\u522b\u7684\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2507.10539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10539", "abs": "https://arxiv.org/abs/2507.10539", "authors": ["Tao Feng", "Yexin Wu", "Guanyu Lin", "Jiaxuan You"], "title": "Graph World Model", "comment": null, "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGraph World Model\uff08GWM\uff09\uff0c\u652f\u6301\u5904\u7406\u975e\u7ed3\u6784\u5316\u548c\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u5b9e\u73b0\u591a\u6837\u5316\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u9886\u57df\u4e13\u7528\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\uff08WMs\uff09\u4e3b\u8981\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u65e0\u6cd5\u5229\u7528\u666e\u904d\u5b58\u5728\u7684\u56fe\u7ed3\u6784\u6570\u636e\uff1b\u800c\u56fe\u57fa\u7840\u6a21\u578b\u4ec5\u9650\u4e8e\u56fe\u5b66\u4e60\u4efb\u52a1\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u591a\u6a21\u6001\u6570\u636e\u548c\u8de8\u5b66\u79d1\u4efb\u52a1\u3002", "method": "GWM\u901a\u8fc7\u901a\u7528\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u805a\u5408\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\uff08\u6587\u672c\u6216\u5d4c\u5165\u7a7a\u95f4\uff09\uff0c\u5e76\u5f15\u5165\u52a8\u4f5c\u8282\u70b9\u4ee5\u94fe\u63a5\u591a\u6837\u5316\u4efb\u52a1\u3002", "result": "\u5728\u516d\u9879\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\uff0cGWM\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u9886\u57df\u4e13\u7528\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u96f6\u6837\u672c/\u5c11\u6837\u672c\u80fd\u529b\u3002", "conclusion": "GWM\u4e3a\u5904\u7406\u591a\u6a21\u6001\u56fe\u7ed3\u6784\u6570\u636e\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.10540", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10540", "abs": "https://arxiv.org/abs/2507.10540", "authors": ["Tao Feng", "Haozhen Zhang", "Zijie Lei", "Pengrui Han", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Jiaxuan You"], "title": "Fusing LLM Capabilities with Routing Data", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFusionBench\u548cFusionFactory\uff0c\u901a\u8fc7\u591a\u7ea7\u878d\u5408\u6846\u67b6\uff08\u67e5\u8be2\u7ea7\u3001\u601d\u7ef4\u7ea7\u3001\u6a21\u578b\u7ea7\uff09\u4f18\u5316LLM\u8def\u7531\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5e94\u7528\u591a\u4f9d\u8d56\u5355\u4e00LLM\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6837\u5316\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5bfc\u81f4\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faFusionBench\u8def\u7531\u57fa\u51c6\u548cFusionFactory\u878d\u5408\u6846\u67b6\uff0c\u5305\u62ec\u67e5\u8be2\u7ea7\u3001\u601d\u7ef4\u7ea7\u548c\u6a21\u578b\u7ea7\u878d\u5408\u3002", "result": "FusionFactory\u5728\u6240\u670914\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u5355\u4e00LLM\uff0c\u9a8c\u8bc1\u4e86\u878d\u5408\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7cfb\u7edf\u5316LLM\u878d\u5408\u80fd\u6709\u6548\u5229\u7528\u4e92\u8865\u4f18\u52bf\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2507.10546", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10546", "abs": "https://arxiv.org/abs/2507.10546", "authors": ["Kexin Gu Baugh", "Vincent Perreault", "Matthew Baugh", "Luke Dickens", "Katsumi Inoue", "Alessandra Russo"], "title": "Disentangling Neural Disjunctive Normal Form Models", "comment": "Accepted at NeSy 2025", "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7f20\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c6\u5206\u5d4c\u5957\u89c4\u5219\u7684\u8282\u70b9\u6765\u63d0\u5347\u795e\u7ecfDNF\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u795e\u7ecfDNF\u6a21\u578b\u5728\u7b26\u53f7\u7ffb\u8bd1\u8fc7\u7a0b\u4e2d\u56e0\u9608\u503c\u5904\u7406\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u90e8\u5206\u539f\u56e0\u662f\u672a\u80fd\u89e3\u7f20\u7f51\u7edc\u6743\u91cd\u4e2d\u7684\u5b66\u4e60\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u89e3\u7f20\u65b9\u6cd5\uff0c\u5c06\u5d4c\u5957\u89c4\u5219\u7684\u8282\u70b9\u62c6\u5206\u4e3a\u72ec\u7acb\u7684\u5c0f\u8282\u70b9\uff0c\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u79cd\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u89e3\u7f20\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u8868\u793a\uff0c\u6027\u80fd\u63a5\u8fd1\u7ffb\u8bd1\u524d\u7684\u6a21\u578b\u3002", "conclusion": "\u89e3\u7f20\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecfDNF\u6a21\u578b\u5728\u7b26\u53f7\u7ffb\u8bd1\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002"}}
