<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 31]
- [cs.LG](#cs.LG) [Total: 78]
- [stat.ML](#stat.ML) [Total: 9]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Distributed learning for automatic modulation recognition in bandwidth-limited networks](https://arxiv.org/abs/2510.24722)
*Narges Rashvand,Kenneth Witham,Gabriel Maldonado,Vinit Katariya,Aly Sultan,Gunar Schirner,Hamed Tabkhi*

Main category: eess.SP

TL;DR: 该研究提出了两种基于分布式学习的自动调制识别方法，通过多个接收器协作来识别调制类型，在显著降低带宽需求的同时保持较高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统集中式AMR方法需要将所有训练数据收集到高功率计算设备上处理，这在带宽受限的无线网络中不切实际。为解决这一问题，研究探索分布式学习方法。

Method: 提出了两种分布式AMR方法：1）基于共识投票的方法，六个接收器独立识别后投票决定最终结果；2）特征图共享方法，每个接收器共享特征图，由中央节点聚合处理。

Result: 集中式方法准确率达91%，单个接收器仅41%。分布式方法一准确率略低但带宽需求降至1/256，方法二带宽需求降至1/8。

Conclusion: 分布式AMR方法能显著提高识别准确率，同时有效解决带宽受限无线网络的约束问题。

Abstract: Automatic Modulation Recognition (AMR) is critical in identifying various
modulation types in wireless communication systems. Recent advancements in deep
learning have facilitated the integration of algorithms into AMR techniques.
However, this integration typically follows a centralized approach that
necessitates collecting and processing all training data on high-powered
computing devices, which may prove impractical for bandwidth-limited wireless
networks. In response to this challenge, this study introduces two methods for
distributed learning-based AMR on the collaboration of multiple receivers to
perform AMR tasks. The TeMuRAMRD 2023 dataset is employed to support this
investigation, uniquely suited for multi-receiver AMR tasks. Within this
distributed sensing environment, multiple receivers collaborate in identifying
modulation types from the same RF signal, each possessing a partial perspective
of the overall environment. Experimental results demonstrate that the
centralized-based AMR, with six receivers, attains an impressive accuracy rate
of 91%, while individual receivers exhibit a notably lower accuracy, at around
41%. Nonetheless, the two proposed decentralized learning-based AMR methods
exhibit noteworthy enhancements. Based on consensus voting among six receivers,
the initial method achieves a marginally lower accuracy. It achieves this while
substantially reducing the bandwidth demands to a 1/256th of the centralized
model. With the second distributed method, each receiver shares its feature
map, subsequently aggregated by a central node. This approach also accompanies
a substantial bandwidth reduction of 1/8 compared to the centralized approach.
These findings highlight the capacity of distributed AMR to significantly
enhance accuracy while effectively addressing the constraints of
bandwidth-limited wireless networks.

</details>


### [2] [Ambient Backscatter Communication Assisted by Fluid Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2510.24725)
*Masoud Kaveh,Farshad Rostami Ghadi,Riku Jantti,Kai-Kit Wong,F. Javier Lopez-Martinez*

Main category: eess.SP

TL;DR: 该论文研究了将流体可重构智能表面(FRIS)集成到环境反向散射通信(AmBC)系统中，通过动态调整流体元件位置来增强空间适应性，并使用粒子群优化算法优化配置，显著提升了系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统RIS具有固定位置的元件，限制了空间适应性。在AmBC系统中，当标签到阅读器的直接链路较弱或被障碍物阻挡时，需要更灵活的反射表面来增强通信性能。

Method: 开发了FRIS辅助的AmBC系统模型，将FRIS元件位置优化建模为非凸问题，并采用粒子群优化(PSO)算法来获得近最优的流体元件配置。

Result: 仿真结果表明，FRIS辅助的AmBC系统在可实现吞吐量方面显著优于传统的基于RIS的AmBC系统。

Conclusion: FRIS通过动态调整元件位置提供了增强的空间适应性，在AmBC系统中能够有效克服直接链路弱或被阻挡的问题，显著提升通信性能。

Abstract: This paper investigates the integration of a fluid reconfigurable intelligent
surface (FRIS) into ambient backscatter communication (AmBC) systems. Unlike
conventional reconfigurable intelligent surfaces (RISs) with fixed position
elements, FRIS employs fluidic elements that can dynamically adjust their
positions, offering enhanced spatial adaptability. We develop a system model
where an AmBC tag communicates with a reader through an FRIS, which is
particularly beneficial in scenarios where the direct tag-to-reader link is
weak or blocked by obstacles. The achievable backscatter rate is analyzed, and
the optimization of FRIS element positions is formulated as a non-convex
problem. To address this, we employ particle swarm optimization (PSO) to obtain
near-optimal configurations of the fluid elements. Simulation results
demonstrate that FRIS-aided AmBC significantly outperforms conventional
RIS-based AmBC systems in terms of achievable throughput.

</details>


### [3] [Modelling Real-Life Cycling Decisions in Real Urban Settings Through Psychophysiology and LLM-Derived Contextual Data](https://arxiv.org/abs/2510.24726)
*Maximiliano Rosadio Z.,Angel Jimenez-Molina,Bastián Henríquez,Paulina Leiva,Ricardo Hurtubia,Ricardo De La Paz Guala,Leandro Gayozo,C. Angelo Guevara*

Main category: eess.SP

TL;DR: 该论文提出了一种创新方法，使用大型语言模型从多媒体记录中提取上下文数据，结合生理数据研究城市骑行环境中情绪状态对骑行者行为的影响。


<details>
  <summary>Details</summary>
Motivation: 传统基于自我报告的情绪测量方法存在粒度低和记忆偏差问题，而生理指标虽然能提供连续数据，但缺乏足够的上下文信息来解释情绪变化的原因。在真实交通环境中获取充分的上下文数据极具挑战性。

Method: 从智利圣地亚哥城市骑行案例研究中收集数据，使用大型语言模型处理视频中提取的图像序列，获得环境语义描述。将这些详细的上下文数据整合到混合模型中，其中疲劳和唤醒度作为潜在变量影响观察到的骑行行为。

Result: 研究证实骑行决策受到压力相关情绪的影响，并强调了城市特征和交通条件对骑行者行为的强烈影响。

Conclusion: 该方法成功解决了在真实环境中获取充分上下文数据的挑战，为理解情绪状态与骑行行为之间的关系提供了新的分析框架。

Abstract: Measuring emotional states in transportation contexts is an emerging field.
Methods based on self-reported emotions are limited by their low granularity
and their susceptibility to memory bias. In contrast, methods based on
physiological indicators provide continuous data, enabling researchers to
measure changes in emotional states with high detail and accuracy. Not only are
emotions important in the analysis, but understanding what triggers emotional
changes is equally important. Uncontrolled variables such as traffic
conditions, pedestrian interactions, and infrastructure remain a significant
challenge, as they can have a great impact on emotional states. Explaining the
reasons behind these emotional states requires gathering sufficient and proper
contextual data, which can be extremely difficult in real-world environments.
This paper addresses these challenges by applying an innovative approach,
extracting contextual data (expert annotator level) from recorded multimedia
using large language models (LLMs). In this paper, data are collected from an
urban cycling case study of the City of Santiago, Chile. The applied models
focus on understanding how different environments and traffic situations affect
the emotional states and behaviors of the participants using physiological
data. Sequences of images, extracted from the recorded videos, are processed by
LLMs to obtain semantic descriptions of the environment. These discrete,
although dense and detailed, contextual data are integrated into a hybrid
model, where fatigue and arousal serve as latent variables influencing observed
cycling behaviors (inferred from GPS data) like waiting, accelerating, braking,
etc. The study confirms that cycling decisions are influenced by stress-related
emotions and highlights the strong impact of urban characteristics and traffic
conditions on cyclist behavior.

</details>


### [4] [Aerial RIS-Enhanced Communications: Joint UAV Trajectory, Altitude Control, and Phase Shift Design](https://arxiv.org/abs/2510.24731)
*Bin Li,Dongdong Yang,Lei Liu,Dusit Niyato*

Main category: eess.SP

TL;DR: 提出了一种基于欧拉角度的空中可重构智能表面控制方案，通过优化ARIS的高度和轨迹来应对无人机飞行中的倾斜和高度变化问题，显著提升系统总速率。


<details>
  <summary>Details</summary>
Motivation: 解决无人机搭载的空中RIS在飞行过程中不可避免的倾斜和高度变化导致的波束失准问题，这种失准会严重降低ARIS的性能。

Method: 采用基于欧拉角度的ARIS控制方案，结合无人机动态模型联合优化ARIS的高度和轨迹；使用软演员-评论家算法与优先级经验回放学习高效控制策略；基于优化配置采用注水法和二分法确定最优基站波束成形。

Result: 所提算法在收敛性和通信性能方面显著优于基准方法，系统总速率提升约14.4%；相比固定水平ARIS方案，能生成更自适应的轨迹并显著减轻ARIS倾斜导致的性能下降。

Conclusion: 该方案展示了实际ARIS部署的强大潜力，能有效应对无人机飞行动态变化，提升无线网络性能。

Abstract: Reconfigurable intelligent surface (RIS) has emerged as a pivotal technology
for enhancing wireless networks. Compared to terrestrial RIS deployed on
building facades, aerial RIS (ARIS) mounted on quadrotor unmanned aerial
vehicle (UAV) offers superior flexibility and extended coverage. However, the
inevitable tilt and altitude variations of a quadrotor UAV during flight may
lead to severe beam misalignment, significantly degrading ARIS's performance.
To address this challenge, we propose a Euler angles-based ARIS control scheme
that jointly optimizes the altitude and trajectory of the ARIS by leveraging
the UAV's dynamic model. Considering the constraints on ARIS flight energy
consumption, flight safety, and the transmission power of a base station (BS),
we jointly design the ARIS's altitude, trajectory, phase shifts, and BS
beamforming to maximize the system sum-rate. Due to the continuous control
nature of ARIS flight and the strong coupling among variables, we formulate the
problem as a Markov decision process and adopt a soft actor-critic algorithm
with prioritized experience replay to learn efficient ARIS control policies.
Based on the optimized ARIS configuration, we further employ the water-filling
and bisection method to efficiently determine the optimal BS beamforming.
Numerical results demonstrate that the proposed algorithm significantly
outperforms benchmarks in both convergence and communication performance,
achieving approximately 14.4\% improvement in sum-rate. Moreover, in comparison
to the fixed-horizontal ARIS scheme, the proposed scheme yields more adaptive
trajectories and significantly mitigates performance degradation caused by ARIS
tilting, demonstrating strong potential for practical ARIS deployment.

</details>


### [5] [Decoding non-invasive brain activity with novel deep-learning approaches](https://arxiv.org/abs/2510.24733)
*Richard Csaky*

Main category: eess.SP

TL;DR: 该论文研究了非侵入性脑电信号（EEG和MEG）的建模和解码，重点关注视觉刺激感知和内部言语的脑活动解码，并开发了处理个体间变异性的深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大脑在感知视觉刺激或进行内部言语时的活动机制，并提高这类刺激的解码性能，同时解决脑电记录中存在的个体内、个体间和数据集间的大变异性问题。

Method: 采用深度学习方法进行脑解码研究，包括使用线性模型解码个体视觉刺激，开发处理个体间变异性的群体解码方法，以及基于卷积和Transformer架构的MEG数据预测模型。

Result: 在方法学部分，基于Transformer的模型在生成接近真实脑数据的信号方面表现出色；在实验部分，虽然收集了高质量的内部言语数据集，但解码结果大多为阴性，表明内部言语解码具有很大挑战性。

Conclusion: 深度学习方法特别是Transformer架构在脑电信号建模中具有优势，但内部言语的解码仍然非常困难，需要进一步研究来克服这一挑战。

Abstract: This thesis delves into the world of non-invasive electrophysiological brain
signals like electroencephalography (EEG) and magnetoencephalography (MEG),
focusing on modelling and decoding such data. The research aims to investigate
what happens in the brain when we perceive visual stimuli or engage in covert
speech (inner speech) and enhance the decoding performance of such stimuli. The
thesis is divided into two main sections, methodological and experimental work.
A central concern in both sections is the large variability present in
electrophysiological recordings, whether it be within-subject or
between-subject variability, and to a certain extent between-dataset
variability. In the methodological sections, we explore the potential of deep
learning for brain decoding. We present advancements in decoding visual stimuli
using linear models at the individual subject level. We then explore how deep
learning techniques can be employed for group decoding, introducing new methods
to deal with between-subject variability. Finally, we also explores novel
forecasting models of MEG data based on convolutional and Transformer-based
architectures. In particular, Transformer-based models demonstrate superior
capabilities in generating signals that closely match real brain data, thereby
enhancing the accuracy and reliability of modelling the brain's
electrophysiology. In the experimental section, we present a unique dataset
containing high-trial inner speech EEG, MEG, and preliminary optically pumped
magnetometer (OPM) data. Our aim is to investigate different types of inner
speech and push decoding performance by collecting a high number of trials and
sessions from a few participants. However, the decoding results are found to be
mostly negative, underscoring the difficulty of decoding inner speech.

</details>


### [6] [Cardi-GPT: An Expert ECG-Record Processing Chatbot](https://arxiv.org/abs/2510.24737)
*Koustav Mallick,Neel Singh,Mohammedreza Hajiarbabi*

Main category: eess.SP

TL;DR: Cardi-GPT是一个基于深度学习和自然语言交互的心电图专家系统，通过16残差块CNN处理12导联ECG数据，在24种心脏疾病上达到0.6194加权准确率，并集成聊天机器人界面改善临床沟通。


<details>
  <summary>Details</summary>
Motivation: 传统心电图解读和临床沟通需要专业知识且具有挑战性，需要开发能够简化ECG解释和增强临床沟通的系统。

Method: 使用16残差块卷积神经网络处理12导联ECG数据，加入模糊化层将数值输出转换为临床语言类别，并集成聊天机器人界面。

Result: 在跨越4个国家6家医院的多样化数据集上评估，性能优于基线模型，整体响应质量得分达73%。

Conclusion: Cardi-GPT通过弥合复杂ECG数据解读与临床洞察之间的差距，代表了心血管医疗的变革性创新，有望提高诊断准确性、优化临床工作流程和改善患者预后。

Abstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial
yet challenging tasks in cardiovascular diagnosis, traditionally requiring
significant expertise and precise clinical communication. This paper introduces
Cardi-GPT, an advanced expert system designed to streamline ECG interpretation
and enhance clinical communication through deep learning and natural language
interaction. Cardi-GPT employs a 16-residual-block convolutional neural network
(CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194
across 24 cardiac conditions. A novel fuzzification layer converts complex
numerical outputs into clinically meaningful linguistic categories, while an
integrated chatbot interface facilitates intuitive exploration of diagnostic
insights and seamless communication between healthcare providers.
  The system was evaluated on a diverse dataset spanning six hospitals across
four countries, demonstrating superior performance compared to baseline models.
Additionally, Cardi-GPT achieved an impressive overall response quality score
of 73\%, assessed using a comprehensive evaluation framework that measures
coverage, grounding, and coherence. By bridging the gap between intricate ECG
data interpretation and actionable clinical insights, Cardi-GPT represents a
transformative innovation in cardiovascular healthcare, promising to improve
diagnostic accuracy, clinical workflows, and patient outcomes across diverse
medical settings.

</details>


### [7] [StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs](https://arxiv.org/abs/2510.24738)
*Tianheng Ling,Chao Qian,Peter Zdankin,Torben Weis,Gregor Schiele*

Main category: eess.SP

TL;DR: StrikeWatch是一个紧凑型腕戴系统，通过IMU信号在设备上实时进行步态识别，特别针对检测脚跟与脚前掌着地模式，帮助跑步者自我纠正有害步态。


<details>
  <summary>Details</summary>
Motivation: 跑步有益健康但不正确的步态模式可能导致受伤，现有步态分析系统笨重且限于离线分析，腕戴设备更实用但实时步态识别仍具挑战性。

Method: 提出四种紧凑DL架构（1D-CNN、1D-SepCNN、LSTM和Transformer），在嵌入式FPGA上进行能效优化，使用定制硬件原型收集户外跑步数据并评估模型。

Result: 6位量化1D-SepCNN在iCE40UP5K上达到最高平均F1分数0.847，每次推理仅消耗0.350μJ，延迟0.140ms，支持13.6天连续推理。

Conclusion: 研究揭示了模型复杂度和硬件效率之间的权衡，紧凑模型在资源受限设备上实现了高效实时步态识别。

Abstract: Running offers substantial health benefits, but improper gait patterns can
lead to injuries, particularly without expert feedback. While prior gait
analysis systems based on cameras, insoles, or body-mounted sensors have
demonstrated effectiveness, they are often bulky and limited to offline,
post-run analysis. Wrist-worn wearables offer a more practical and
non-intrusive alternative, yet enabling real-time gait recognition on such
devices remains challenging due to noisy Inertial Measurement Unit (IMU)
signals, limited computing resources, and dependence on cloud connectivity.
This paper introduces StrikeWatch, a compact wrist-worn system that performs
entirely on-device, real-time gait recognition using IMU signals. As a case
study, we target the detection of heel versus forefoot strikes to enable
runners to self-correct harmful gait patterns through visual and auditory
feedback during running. We propose four compact DL architectures (1D-CNN,
1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient
inference on two representative embedded Field-Programmable Gate Arrays
(FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our
custom-built hardware prototype, we collect a labeled dataset from outdoor
running sessions and evaluate all models via a fully automated deployment
pipeline. Our results reveal clear trade-offs between model complexity and
hardware efficiency. Evaluated across 12 participants, 6-bit quantized
1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just
0.350 {\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running
at 20 MHz. This configuration supports up to 13.6 days of continuous inference
on a 320 mAh battery. All datasets and code are available in the GitHub
repository https://github.com/tianheng-ling/StrikeWatch.

</details>


### [8] [Comparative Analysis of Data Augmentation for Clinical ECG Classification with STAR](https://arxiv.org/abs/2510.24740)
*Nader Nemati*

Main category: eess.SP

TL;DR: STAR是一种针对12导联心电图的节拍级数据增强方法，通过在R-R间期内进行受控的时间扭曲和幅度缩放，保留关键临床形态特征，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 临床12导联心电图分类面临记录条件多样、病理重叠和标签不平衡等挑战，传统增强方法可能扭曲诊断关键形态特征。

Method: 提出正弦时间-幅度重采样(STAR)方法，在连续R峰之间进行节拍级增强，保持P-QRS-T顺序不变，提供形态保持的变异性。

Result: STAR增强了训练多样性而不破坏峰值或间期，提高了跨设备、站点和队列的稳定性，并改善了罕见类别的学习效果。

Conclusion: STAR为临床ECG分类提供了一种简单可控的数据增强方法，在形态保真度、操作简便性和跨源耐久性方面具有优势。

Abstract: Clinical 12-lead ECG classification remains difficult because of diverse
recording conditions, overlapping pathologies, and pronounced label imbalance
hinder generalization, while unconstrained augmentations risk distorting
diagnostically critical morphology. In this study, Sinusoidal Time--Amplitude
Resampling (STAR) is introduced as a beat-wise augmentation that operates
strictly between successive R-peaks to apply controlled time warping and
amplitude scaling to each R--R segment, preserving the canonical P--QRS--T
order and leaving the head and tail of the trace unchanged. STAR is designed
for practical pipelines and offers: (i) morphology-faithful variability that
broadens training diversity without corrupting peaks or intervals; (ii)
source-resilient training, improving stability across devices, sites, and
cohorts without dataset-specific tuning; (iii) model-agnostic integration with
common 1D SE--ResNet-style ECG encoders backbone; and (iv) better learning on
rare classes via beat-level augmentation, reducing overfitting by resampling
informative beats instead of duplicating whole records. In contrast to global
crops, large shifts, or additive noise, STAR avoids transformations that
suppress or misalign clinical landmarks. A complete Python implementation and a
transparent training workflow are released, aligned with a source-aware,
stratified five-fold protocol over a multi-institutional 12-lead corpus,
thereby facilitating inspection and reuse. Taken together, STAR provides a
simple and controllable augmentation for clinical ECG classification where
trustworthy morphology, operational simplicity, and cross-source durability are
essential.

</details>


### [9] [PulseFi: A Low Cost Robust Machine Learning System for Accurate Cardiopulmonary and Apnea Monitoring Using Channel State Information](https://arxiv.org/abs/2510.24744)
*Pranay Kocheta,Nayan Sanjay Bhatia,Katia Obraczka*

Main category: eess.SP

TL;DR: PulseFi是一个基于Wi-Fi感知和人工智能的低成本非侵入式系统，用于监测心率和呼吸率，并检测呼吸暂停事件。


<details>
  <summary>Details</summary>
Motivation: 非侵入式生命体征监测在医疗保健环境中变得越来越重要，需要低成本且易于获取的解决方案。

Method: 使用信号处理流程处理Wi-Fi信道状态信息(CSI)数据，并将其输入定制的低计算量LSTM神经网络模型。

Result: 在两个数据集上评估显示，PulseFi能够有效估计心率和呼吸率，准确性与更昂贵的多天线系统相当或更好。

Conclusion: PulseFi提供了一种成本效益高且易于获取的非侵入式生命体征监测解决方案。

Abstract: Non-intrusive monitoring of vital signs has become increasingly important in
a variety of healthcare settings. In this paper, we present PulseFi, a novel
low-cost non-intrusive system that uses Wi-Fi sensing and artificial
intelligence to accurately and continuously monitor heart rate and breathing
rate, as well as detect apnea events. PulseFi operates using low-cost commodity
devices, making it more accessible and cost-effective. It uses a signal
processing pipeline to process Wi-Fi telemetry data, specifically Channel State
Information (CSI), that is fed into a custom low-compute Long Short-Term Memory
(LSTM) neural network model. We evaluate PulseFi using two datasets: one that
we collected locally using ESP32 devices and another that contains recordings
of 118 participants collected using the Raspberry Pi 4B, making the latter the
most comprehensive data set of its kind. Our results show that PulseFi can
effectively estimate heart rate and breathing rate in a seemless non-intrusive
way with comparable or better accuracy than multiple antenna systems that can
be expensive and less accessible.

</details>


### [10] [EcoScaleNet: A Lightweight Multi Kernel Network for Long Sequence 12 lead ECG Classification](https://arxiv.org/abs/2510.24748)
*Dong-Hyeon Kang,Ju-Hyeon Nam,Sang-Chul Lee*

Main category: eess.SP

TL;DR: EcoScale-Net是一种高效的心电图分类网络，通过分层设计和瓶颈卷积，在保持全感受野覆盖的同时，大幅减少了OS CNN的计算成本和参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型难以处理心电图的长序列特征，而OS CNN虽然通过枚举素数核尺寸覆盖所有尺度，但计算成本过高，阻碍了更深更宽模型的构建。

Method: 提出分层变体EcoScale-Net，在每阶段限制最大核长度，并在Omni Scale块前后插入瓶颈卷积来限制通道增长和融合多尺度特征。

Result: 在CODE 15% ECG数据集上，相比OS CNN，EcoScale-Net减少了90%的参数和99%的FLOPs，同时将宏平均F1分数提高了2.4%。

Conclusion: EcoScale-Net以极低的计算成本实现了长序列心电图分类的最先进精度，可在普通硬件上实时部署。

Abstract: Accurate interpretation of 12 lead electrocardiograms (ECGs) is critical for
early detection of cardiac abnormalities, yet manual reading is error prone and
existing CNN based classifiers struggle to choose receptive field sizes that
generalize to the long sequences typical of ECGs. Omni Scale CNN (OS CNN)
addresses this by enumerating prime sized kernels inspired by Goldbach
conjecture to cover every scale, but its exhaustive design explodes
computational cost and blocks deeper, wider models. We present Efficient
Convolutional Omni Scale Network (EcoScale-Net), a hierarchical variant that
retains full receptive field coverage while eliminating redundancy. At each
stage, the maximum kernel length is capped to the scale still required after
down sampling, and bottleneck convolutions inserted before and after every Omni
Scale block curtail channel growth and fuse multi scale features. On the large
scale CODE 15% ECG dataset, EcoScaleNet reduces parameters by 90% and FLOPs by
99% compared with OS CNN, while raising macro averaged F1 score by 2.4%. These
results demonstrate that EcoScaleNet delivers SOTA accuracy for long sequence
ECG classification at a fraction of the computational cost, enabling real time
deployment on commodity hardware. Our EcoScaleNet code is available in GitHub
Link.

</details>


### [11] [Opportunistic Screening of Wolff-Parkinson-White Syndrome using Single-Lead AI-ECG Mobile System: A Real-World Study of over 3.5 million ECG Recordings in China](https://arxiv.org/abs/2510.24750)
*Shun Huang,Deyun Zhang,Sumei Fan,Shijia Geng,Yujie Xiao,Rui Zhang,Zhaoji Fu,Shenda Hong*

Main category: eess.SP

TL;DR: 本研究评估了单导联AI-ECG移动系统在真实世界环境中对WPW综合征的机会性检测效率，结果显示AI系统显著减少医生工作量99.5%，每确认1例WPW仅需12次审查。


<details>
  <summary>Details</summary>
Motivation: 传统WPW综合征筛查依赖心电图专家解读，限制了大规模成本效益筛查，需要开发更高效的筛查方法。

Method: 回顾性分析来自87,836名中国个体的3,566,626份单导联ECG记录，使用NMPA批准的便携ECG设备，通过AI系统性能验证和随机抽样评估。

Result: AI系统达到45.5%敏感性和95.9%特异性，AI阳性结果使确认WPW风险增加约210倍，AI筛选阳性可将医生工作量减少99.5%。

Conclusion: 单导联AI-ECG系统能够实现WPW综合征的高效实用机会性筛查，显著减少医生工作量，支持基于人群的心血管预防。

Abstract: Wolff-Parkinson-White (WPW) syndrome is a congenital cardiac condition
associated with sudden cardiac death, with a prevalence of 0.1-0.3%.
Conventional screening relies on electrophysiological testing or 12-lead
electrocardiography interpreted by cardiologists, which limits large-scale and
cost-effective screening. Building on our previous work developing a
single-lead AI-ECG mobile system for atrial fibrillation screening, this study
evaluates its efficiency and effectiveness for opportunistic detection of WPW
syndrome in real-world settings. This retrospective analysis included 3,566,626
single-lead ECG recordings from 87,836 individuals in China, collected using
the NMPA-approved portable ECG device WenXinWuYang. The AI system performance
was validated using cardiologist annotations and random sampling. We quantified
AI-assisted workload reduction and compared review efficiency across
AI-positive and user-initiated workflows. The AI system achieved 45.5%
sensitivity and 95.9% specificity. A positive AI result indicated about 210
times higher risk of confirmed WPW. Focusing on AI-selected positives reduced
physician workload by 99.5%, requiring only 12 reviews to confirm one WPW case,
compared with 909 and 875 in population-wide and user-driven approaches. In
conclusion, this large-scale real-world study demonstrates that a single-lead
AI-ECG system enables efficient and practical opportunistic screening for WPW
syndrome, significantly reducing physician workload and supporting
population-based cardiovascular prevention.

</details>


### [12] [A Cylindrical Nanowire Array-Based Flexure-FET Receiver for Molecular Communication](https://arxiv.org/abs/2510.24890)
*Dilara Aktas,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 提出了一种基于圆柱形纳米线阵列的Flexure-FET分子通信接收器，通过分布式机电耦合增强设计灵活性和可扩展性，为物联网纳米设备提供可调谐的灵敏度和通信容量控制。


<details>
  <summary>Details</summary>
Motivation: 分子通信具有生物兼容性和高能效特性，是纳米物联网和体内医疗系统的关键基础。需要开发先进的接收器架构，将纳米级通信技术与生物-网络接口融合，确保能量高效、可靠且低复杂度的调制检测。

Method: 采用圆柱形纳米线阵列的Flexure-FET分子通信接收器设计，在悬浮栅配置中实现分布式机电耦合。开发了分析端到端模型来表征系统的机电响应、噪声行为和信息理论性能。

Result: 研究结果揭示了几何结构、机电动力学和分子结合过程之间的强相互依赖性，实现了对灵敏度、噪声特性和通信容量的可调谐控制。阵列配置增强了结构可调性。

Conclusion: 所提出的设计为未来基于混合和空间调制的分子通信系统提供了灵活基础，为物联网框架内可扩展和多功能的接收器架构铺平了道路。

Abstract: Molecular communication (MC) enables biocompatible and energy-efficient
information transfer through chemical signaling, forming a foundational
paradigm for emerging applications in the Internet of Nano Things (IoNT) and
intrabody healthcare systems. The realization of this vision critically depends
on developing advanced receiver architectures that merge nanoscale
communication and networking techniques with bio-cyber interfaces, ensuring
energy-efficient, reliable, and low-complexity modulation and detection while
maintaining biocompatibility. To address these challenges, the Flexure-FET MC
receiver was introduced as a mechanically transducing design capable of
detecting both charged and neutral molecular species. In this study, we present
a cylindrical nanowire array-based Flexure-FET MC receiver that enhances design
versatility and scalability through distributed electromechanical coupling in a
suspended-gate configuration. The proposed array architecture offers additional
geometric degrees of freedom, including nanowire radius, length, spacing, and
array size, providing a flexible framework that can be tailored to advanced MC
scenarios. An analytical end-to-end model is developed to characterize the
system's electromechanical response, noise behavior, and information-theoretic
performance, including signal-to-noise ratio (SNR) and channel capacity. The
results reveal the strong interdependence between geometry, electromechanical
dynamics, and molecular binding processes, enabling tunable control over
sensitivity, noise characteristics, and communication capacity. The enhanced
structural tunability and array configuration of the proposed design provide a
flexible foundation for future mixture-based and spatially modulated MC
systems, paving the way toward scalable and multifunctional receiver
architectures within the IoNT framework.

</details>


### [13] [Next-Generation MAC Technique for Priority Handling in Industrial Cyber-Physical Systems](https://arxiv.org/abs/2510.24928)
*Anwar Ahmed Khan,Farid Nait-Abdesselam,Indrakshi Dey*

Main category: eess.SP

TL;DR: 提出了一种动态分片MAC协议（DyFrag-MAC），通过动态分片正常优先级数据来支持紧急优先级数据的早期传输，解决了现有协议无法抢占低优先级传输的问题。


<details>
  <summary>Details</summary>
Motivation: 工业信息物理系统中连接设备数量显著增长，需要可靠和及时的网络服务，现有协议无法有效处理异构优先级流量的动态信道访问需求。

Method: DyFrag-MAC协议通过动态分片正常优先级数据，支持紧急优先级数据的抢占式传输，与FROG-MAC和i-DSME相比具有动态调整分片大小的能力。

Result: 性能评估显示，在平均延迟和吞吐量方面，DyFrag-MAC对异构流量表现出更好的性能。

Conclusion: DyFrag-MAC协议通过动态分片机制有效解决了异构优先级流量的信道访问问题，在工业CPS环境中具有优越性能。

Abstract: Next Generation Media Access Control (NGMA) techniques have been designed to
support diverse applications with heterogeneous priorities. In industrial
cyber-physical systems (CPS), the number of connected devices and systems is
expected to grow significantly, demanding dependable and prompt network
services. In this work, we present a novel scheme, Dynamic Fragmentation-MAC
(DyFrag-MAC) that offers dynamic, differentiated channel access to the traffic
of various priorities. DyFrag-MAC works on fragmenting the data of normal
priority in order to support early delivery of urgent priority data. In prior
work, urgent priority data either had to wait for the complete transmission of
lower-priority packets or relied on multi-channel protocols to gain access. We
compared the proposed fragmentation scheme with FROG-MAC and industrial
Deterministic and Synchronous Multi-channel Extension (i-DSME). FROG-MAC
fragmented the lower priority packets, but did not adjust the fragment size
dynamically, whereas i-DSME utilized multiple channels and adaptive contention
mechanisms; both protocols lack the ability to preempt ongoing lower-priority
transmissions. Hence, the performance evaluation in terms of average delay and
throughput reveals better performance of DyFRAG-MAC for the heterogeneous
traffic.

</details>


### [14] [Optimizing Next Generation Wireless BAN with Prioritized Access for Heterogeneous Traffic](https://arxiv.org/abs/2510.24931)
*Shama Sidiqui,Indrakshi Dey*

Main category: eess.SP

TL;DR: 提出了一种新型的基于优先级的MAC协议ADP2-MAC，用于无线体域网中的异构流量管理，通过概率性方法动态确定信道轮询间隔，并在预期紧急数据包时中断低优先级传输。


<details>
  <summary>Details</summary>
Motivation: 无线体域网中异构流量管理对可靠性、延迟和能效至关重要，需要针对不同流量优先级优化延迟，以提升网络性能和健康结果。

Method: ADP2-MAC协议采用概率性方法动态确定信道轮询/监听间隔，识别流量到达模式以确定最优轮询间隔，并在预期紧急数据包时中断低优先级数据传输。

Result: 与支持异构流量的MVDR协议相比，ADP2-MAC因其概率性轮询间隔和中断机制而表现更优，能更高效处理紧急优先级数据。

Conclusion: ADP2-MAC协议通过动态轮询间隔和中断机制，有效支持无线体域网中的异构流量管理，在延迟优化和紧急数据处理方面优于现有协议。

Abstract: Efficient management of heterogeneous traffic with varying priorities is
critical in Wireless Body Area Networks (WBANs). The priority mechanisms
embedded in Media Access Control (MAC) schemes largely govern the performance
of WBAN in terms of reliability, delay and energy efficiency. Minimizing the
delay between packet generation and reception is critical for enhancing WBAN
performance and associated health outcomes; however, delay optimization must be
tailored to each traffic priority. In this work, we proposed a novel
priority-based MAC protocol, Adaptive and Dynamic Polling MAC for Prioritized
Traffic (ADP2-MAC), designed to support heterogeneous traffic in WBANs. The
protocol utilizes a probabilistic approach to dynamically determine channel
polling/listening intervals. ADP2-MAC not only identifies traffic arrival
patterns to determine optimal polling intervals but also interrupts the
transmission of lower-priority data when urgent packets are expected. The
performance of ADP2-MAC has been compared with the MAC protocol for Variable
Data Rates (MVDR) which supports heterogeneous traffic by assigning different
data rates based on traffic priority. ADP2-MAC outperforms MVDR due to its use
of probabilistic polling intervals and an interruption mechanism designed to
efficiently handle urgent-priority data.

</details>


### [15] [Hybrid Liquid Neural Network-Random Finite Set Filtering for Robust Maneuvering Object Tracking](https://arxiv.org/abs/2510.25020)
*Minti Liu,Qinghua Guo,Cao Zeng,Yanguang Yu,Jun Li,Ming Jin*

Main category: eess.SP

TL;DR: 提出了一种结合液体神经网络和随机有限集框架的混合方法，用于跟踪具有复杂运动模式的机动目标，相比传统方法在跟踪精度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪方法依赖预定义的运动模型，在处理复杂机动目标时表现不佳，需要更灵活的数据驱动方法来适应非线性运动。

Method: 将数据驱动的液体神经网络集成到随机有限集框架中，开发了两种LNN-RFS滤波器，直接从数据中学习连续时间动力学。

Result: 在具有挑战性的机动场景仿真中，所提出的混合方法在跟踪精度方面取得了实质性增益，能够准确跟踪高机动目标。

Conclusion: 这种混合方法保留了RFS框架固有的多目标跟踪优势，同时提高了灵活性和鲁棒性，为复杂机动目标跟踪提供了有效解决方案。

Abstract: This work addresses the problem of tracking maneuvering objects with complex
motion patterns, a task in which conventional methods often struggle due to
their reliance on predefined motion models. We integrate a data-driven liquid
neural network (LNN) into the random finite set (RFS) framework, leading to two
LNN-RFS filters. By learning continuous-time dynamics directly from data, the
LNN enables the filters to adapt to complex, nonlinear motion and achieve
accurate tracking of highly maneuvering objects in clutter. This hybrid
approach preserves the inherent multi-object tracking strengths of the RFS
framework while improving flexibility and robustness. Simulation results on
challenging maneuvering scenarios demonstrate substantial gains of the proposed
hybrid approach in tracking accuracy.

</details>


### [16] [Spectral and Energy Efficiency Tradeoff for Pinching-Antenna Systems](https://arxiv.org/abs/2510.25192)
*Zihao Zhou,Zhaolin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出了捏合天线系统中频谱效率和能量效率权衡的联合发射和捏合波束成形设计，包括单用户和多用户场景。单用户采用两阶段设计，多用户使用交替优化方法，显著提升了SE-EE联合性能。


<details>
  <summary>Details</summary>
Motivation: 传统多天线系统在频谱效率和能量效率之间存在权衡，捏合天线系统通过优化天线位置和波束成形可以更好地平衡这一权衡，特别是在多用户和服务覆盖范围增加时。

Method: 单用户场景：两阶段联合波束成形设计，第一阶段提出迭代闭式细化方案对齐接收信号相位并确定PA位置，第二阶段推导最优发射波束成形器。多用户场景：基于交替优化的联合波束成形设计，考虑服务质量要求。

Result: 数值结果表明：1）所提算法显著提升SE-EE联合性能且收敛速度快；2）随着PA数量和服务覆盖范围增加，PASS与传统多天线系统在SE-EE权衡区间的差距扩大。

Conclusion: 捏合天线系统通过联合优化发射波束成形和天线位置，能够有效平衡频谱效率和能量效率的权衡，特别是在多用户和大覆盖场景下表现优于传统多天线系统。

Abstract: The joint transmit and pinching beamforming design for spectral efficiency
(SE) and energy efficiency (EE) tradeoff in pinching-antenna systems (PASS) is
proposed. Both PASS-enabled single- and multi-user communications are
considered. In the single-user scenario, it is proved that the optimal pinching
antenna (PA) positions are independent of the transmit beamforming. Based on
this insight, a two-stage joint beamforming design is proposed. Specifically,
in the first stage, an iterative closed-form refinement (ICR) scheme is
proposed to align the phases of the received signals, based on which a PA
placement framework is proposed. In the second stage, the closed-form solution
for the optimal transmit beamformer is derived given the optimal PA positions.
In the multi-user scenario, an alternating optimization (AO)-based joint
beamforming design is proposed to balance the SE-EE performance while taking
the quality-of-service (QoS) requirements into account. It is proved that the
proposed AO-based algorithm is guaranteed to converge when no constraints are
violated in PA placement subproblem. Numerical results demonstrate that: 1) the
proposed algorithms significantly improve joint SE-EE performance with fast
convergence speed; 2) the SE-EE tradeoff regime gap between PASS and
conventional multi-antenna system widens as the number of PAs and service
coverage increase.

</details>


### [17] [State Space and Self-Attention Collaborative Network with Feature Aggregation for DOA Estimation](https://arxiv.org/abs/2510.25193)
*Qi You,Qinghua Huang,Yi-Cheng Lin*

Main category: eess.SP

TL;DR: FA-Stateformer是一个用于声音源DOA估计的混合网络，结合状态空间模型和自注意力机制，通过特征聚合和轻量化设计实现高效的时间序列建模。


<details>
  <summary>Details</summary>
Motivation: 声音源的DOA估计面临声学特性随时间频率连续变化的挑战，需要有效聚合相关特征并建模时间依赖关系，同时平衡模型性能和计算效率。

Method: 提出FA-Stateformer网络：1）特征聚合模块增强时频维度信息特征；2）轻量化Conformer架构压缩前馈层减少冗余；3）时间移位机制扩展卷积感受野；4）双向Mamba模块高效建模双向时间依赖；5）自注意力层与Mamba块协同建模。

Result: 大量实验表明，FA-Stateformer相比传统架构实现了更优越的性能和效率。

Conclusion: FA-Stateformer通过状态空间和自注意力的协同设计，在表示能力和计算效率之间取得了良好平衡，为DOA估计提供了有效的解决方案。

Abstract: Accurate direction-of-arrival (DOA) estimation for sound sources is
challenging due to the continuous changes in acoustic characteristics across
time and frequency. In such scenarios, accurate localization relies on the
ability to aggregate relevant features and model temporal dependencies
effectively. In time series modeling, achieving a balance between model
performance and computational efficiency remains a significant challenge. To
address this, we propose FA-Stateformer, a state space and self-attention
collaborative network with feature aggregation. The proposed network first
employs a feature aggregation module to enhance informative features across
both temporal and spectral dimensions. This is followed by a lightweight
Conformer architecture inspired by the squeeze-and-excitation mechanism, where
the feedforward layers are compressed to reduce redundancy and parameter
overhead. Additionally, a temporal shift mechanism is incorporated to expand
the receptive field of convolutional layers while maintaining a compact kernel
size. To further enhance sequence modeling capabilities, a bidirectional Mamba
module is introduced, enabling efficient state-space-based representation of
temporal dependencies in both forward and backward directions. The remaining
self-attention layers are combined with the Mamba blocks, forming a
collaborative modeling framework that achieves a balance between representation
capacity and computational efficiency. Extensive experiments demonstrate that
FA-Stateformer achieves superior performance and efficiency compared to
conventional architectures.

</details>


### [18] [Cramér-Rao Bound Optimization for Movable Antenna-Empowered Integrated Sensing and Uplink Communication System](https://arxiv.org/abs/2510.25246)
*Yuan Guo,Wen Chen,Qingqing Wu,Yang Liu,Qiong Wu*

Main category: eess.SP

TL;DR: 提出一种基于可移动天线技术的集成感知与通信系统，通过联合优化波束成形、功率分配、接收滤波器和天线位置配置，最小化目标角度估计的CRB界，同时保证通信性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线的ISAC系统无法充分利用空间自由度，导致雷达感知和通信性能受限。可移动天线技术能够通过优化天线位置来改善信道条件，提升系统性能。

Method: 使用主优化-最小化(MM)和惩罚-对偶分解(PDD)方法，开发低复杂度算法求解包含分数项和四次项的波束成形配置问题，通过迭代优化所有变量而不依赖数值求解器。

Result: 数值仿真结果表明所提算法的有效性和高效性，采用可移动天线的ISAC系统实现了显著的性能提升。

Conclusion: 可移动天线技术能够显著提升ISAC系统的感知和通信性能，所提出的算法为6G系统中ISAC应用提供了有效的解决方案。

Abstract: Integrated sensing and communication (ISAC) is a promising solution for the
future sixth-generation (6G) system. However, classical fixed-position antenna
(FPA) ISAC systems fail to fully utilize spatial degrees of freedom (DoFs),
resulting in limited gains for both radar sensing and communication
functionalities. This challenge can be addressed by the emerging novel movable
antenna (MA) technology, which can pursue better channel conditions and improve
sensing and communication performances. In this paper, we aim to minimize the
Cram\'er-Rao bound (CRB) for estimating the target's angle while guaranteeing
communication performance. This involves jointly optimizing active beamforming,
power allocation, receiving filters, and MA position configurations, which is a
highly non-convex problem. To tackle this difficulty, we propose an efficient
iterative solution that analytically optimizes all variables without relying on
numerical solvers, i.e., CVX. Specifically, by leveraging cutting-edge
majorization-minimization (MM) and penalty-dual-decomposition (PDD) methods, we
develop a low-complexity algorithm to solve the beamformer configuration
problem containing the fractional and quartic terms. Numerical simulation
results demonstrate the effectiveness and efficiency of our proposed algorithm,
highlighting significant performance improvements achieved by employing MA in
the ISAC system.

</details>


### [19] [Fair Rate Maximization for Multi-user Multi-cell MISO Communication Systems via Novel Transmissive RIS Transceiver](https://arxiv.org/abs/2510.25290)
*Yuan Guo,Wen Chen,Qingqing Wu,Zhendong Li,Kunlun Wang,Hongying Tang,Jun Li*

Main category: eess.SP

TL;DR: 提出了一种基于透射式可重构智能表面收发器的多小区MISO下行通信系统，开发了低复杂度优化算法来最大化用户最小速率，显著降低了计算复杂度且无性能损失。


<details>
  <summary>Details</summary>
Motivation: 在多小区MISO下行通信系统中，传统的优化问题由于目标函数不可微而难以求解，需要开发高效的优化算法来解决这一挑战性问题。

Method: 采用分数规划方法将速率函数转化为可处理形式，利用平滑逼近理论近似最大最小目标函数，结合主最小化技术和最优性条件分析，提出无需数值求解器的解析更新算法。

Result: 数值结果表明所提算法具有良好的收敛性和有效性，能显著降低计算复杂度且无性能损失，TRTC部署方案明显优于基准方案。

Conclusion: 所提出的低复杂度优化算法成功解决了多小区MISO系统的最大最小速率优化问题，为透射式可重构智能表面收发器的实际应用提供了有效的解决方案。

Abstract: This paper explores a multi-cell multiple-input single-output (MISO) downlink
communication system enabled by a unique transmissive reconfigurable
intelligent surface (RIS) transceiver (TRTC) configuration. Within this system
framework, we formulate an optimization problem for the purpose of maximizing
the minimum rate of users for each cell via designing the transmit beamforming
of the TRTC, subject to the power constraints of each TRTC unit. Since the
objective function is non-differentiable, the max-min rate problem is difficult
to solve. In order to tackle this challenging optimization problem, an
efficient low-complexity optimization algorithm is developed. Specifically, the
log-form rate function is transformed into a tractable form by employing the
fractional programming (FP) methodology. Next, the max-min objective function
can be approximated using a differentiable function derived from smooth
approximation theory. Moreover, by applying the majorization-minimization (MM)
technique and examining the optimality conditions, a solution is proposed that
updates all variables analytically without relying on any numerical solvers.
Numerical results are presented to demonstrate the convergence and
effectiveness of the proposed low-complexity algorithm. Additionally, the
algorithm can significantly reduce the computational complexity without
performance loss. Furthermore, the simulation results illustrate the clear
superiority of the deployment of the TRTC over the benchmark schemes.

</details>


### [20] [Millimeter-Wave Radar Sensing of Wombat Respiration](https://arxiv.org/abs/2510.25293)
*Marina Murakami,Ryoko Iwase,Chiemi Iba,Daisuke Ogura,Takuya Sakamoto*

Main category: eess.SP

TL;DR: 使用79GHz毫米波雷达系统对袋熊进行非接触式呼吸监测，通过自相关函数谐波分量求和的方法估计呼吸间隔，测量误差分别为47.4毫秒(2.44%)和0.81次/分钟(2.21%)。


<details>
  <summary>Details</summary>
Motivation: 开发非接触式呼吸监测方法，用于袋熊的健康监测，避免传统接触式监测的干扰。

Method: 使用79GHz毫米波雷达系统，基于自相关函数谐波分量求和的方法来估计呼吸间隔，捕获由呼吸引起的体表准周期性位移。

Result: 呼吸间隔和呼吸率的测量误差分别为47.4毫秒(2.44%)和0.81次/分钟(2.21%)，并观察到两只袋熊之间以及6月和12月之间的呼吸率差异。

Conclusion: 该方法在袋熊非接触式健康监测方面具有应用潜力。

Abstract: This study demonstrates the feasibility of radar-based non-contact
respiratory monitoring for wombats. Two measurement experiments were conducted
in June and December 2024 using 79-GHz millimeter-wave radar systems to monitor
the respiration of two wombats. To estimate the respiratory interval, we used a
method based on summing harmonic components in the autocorrelation function,
capturing the quasi-periodic displacement of the body surface caused by
respiration. Estimation accuracy was evaluated through simultaneous
measurements from different angles using two radar units. The respiratory
interval and respiratory rate were measured with errors of 47.4 ms (2.44%) and
0.81 bpm (2.21%), respectively. We also discuss the differences in respiratory
rates between the two wombats, as well as seasonal variations between June and
December. The results support the potential application of this method to
non-contact health monitoring of wombats.

</details>


### [21] [Low-Overhead CSI Prediction via Gaussian Process Regression -- Part~I: Data-Driven Spatial Interpolation](https://arxiv.org/abs/2510.25390)
*Syed Luqman Shah,Nurul Huda Mahmood,Italo Atzeni*

Main category: eess.SP

TL;DR: 提出基于高斯过程回归(GPR)的CSI估计框架，通过少量观测值预测完整信道状态信息，可减少50%导频开销，同时保持92%以上的链路容量。


<details>
  <summary>Details</summary>
Motivation: 传统基于导频的CSI估计方法在天线数量增加时会产生过高开销，需要开发能减少导频开销的新方法。

Method: 使用高斯过程回归(GPR)框架，结合径向基函数、Matérn和有理二次核函数来建模天线阵列几何产生的空间相关性，通过少量观测值预测完整CSI。

Result: 在Kronecker和Weichselberger信道模型下，GPR方法在50%导频节省下实现了最低预测误差、最高95%置信区间覆盖率和最佳互信息保持性能。

Conclusion: GPR框架能有效减少50%导频开销，同时保持超过92%的链路容量，为多天线系统提供了高效的CSI估计解决方案。

Abstract: Accurate channel state information (CSI) is critical for current and
next-generation multi-antenna systems. Yet conventional pilot-based estimators
incur prohibitive overhead as antenna counts grow. In this paper, we address
this challenge by developing a novel framework based on Gaussian process
regression (GPR) that predicts full CSI from only a few observed entries,
thereby reducing pilot overhead. The correlation between data points in GPR is
defined by the covariance function, known as kernels. In the proposed GPR-based
CSI estimation framework, we incorporate three kernels, i.e., radial basis
function, Mat\'ern, and rational quadratic, to model smooth and multi-scale
spatial correlations derived from the antenna array geometry. The proposed
approach is evaluated across Kronecker and Weichselberger channel models with
three distinct pilot probing schemes. Results show that the proposed GPR with
50% pilot saving achieves the lowest prediction error, the highest empirical
95% credible-interval coverage, and the best preservation of mutual information
relative to benchmarks. This enables up to 50% pilot reduction while preserving
over 92% of the link capacity.

</details>


### [22] [Model-Free Robust Beamforming in Satellite Downlink using Reinforcement Learning](https://arxiv.org/abs/2510.25393)
*Alea Schröder,Steffen Gracla,Carsten Bockelmann,Dirk Wübben,Armin Dekorsy*

Main category: eess.SP

TL;DR: 使用强化学习为6G卫星通信系统开发鲁棒预编码算法，以应对不完美的信道状态信息，在多种场景下显著优于传统分析方法。


<details>
  <summary>Details</summary>
Motivation: 卫星通信中由于信道状态信息不完美（如位置估计错误），传统预编码方法性能会快速恶化，而推导鲁棒预编码在分析上通常难以处理。

Method: 采用Soft Actor-Critic强化学习算法，将其适配到卫星下行波束成形问题，通过数据驱动方式灵活推导鲁棒预编码算法。

Result: 学习到的算法在多种场景下（单卫星/多卫星协作、全局/局部信道信息、不同误差模型）均匹配或显著优于两种分析基线方法，能够适应所需的鲁棒性水平。

Conclusion: 强化学习能够有效解决卫星通信中不完美信道信息下的鲁棒预编码问题，学习到的算法具有良好的适应性和性能表现。

Abstract: Satellite-based communications are expected to be a substantial future market
in 6G networks. As satellite constellations grow denser and transmission
resources remain limited, frequency reuse plays an increasingly important role
in managing inter-user interference. In the multi-user downlink, precoding
enables the reuse of frequencies across spatially separated users, greatly
improving spectral efficiency. The analytical calculation of suitable
precodings for perfect channel information is well studied, however, their
performance can quickly deteriorate when faced with, e.g., outdated channel
state information or, as is particularly relevant for satellite channels, when
position estimates are erroneous. Deriving robust precoders under imperfect
channel state information is not only analytically intractable in general but
often requires substantial relaxations of the optimization problem or heuristic
constraints to obtain feasible solutions. Instead, in this paper we flexibly
derive robust precoding algorithms from given data using reinforcement
learning. We describe how we adapt the applied Soft Actor-Critic learning
algorithm to the problem of downlink satellite beamforming and show numerically
that the resulting precoding algorithm adjusts to all investigated scenarios.
The considered scenarios cover both single satellite and cooperative
multi-satellite beamforming, using either global or local channel state
information, and two error models that represent increasing levels of
uncertainty. We show that the learned algorithms match or markedly outperform
two analytical baselines in sum rate performance, adapting to the required
level of robustness. We also analyze the mechanisms that the learned algorithms
leverage to achieve robustness. The implementation is publicly available for
use and reproduction of the results.

</details>


### [23] [Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems](https://arxiv.org/abs/2510.25416)
*Jiaming Cheng,Wei Chen,Bo Ai*

Main category: eess.SP

TL;DR: 提出了一种用于无导频和无循环前缀无线系统的自适应端到端收发器架构，结合AI驱动的星座整形和神经网络接收器，通过轻量级信道适配器实现快速适应，支持多调制阶数统一模型，并解决OFDM的高峰均比问题。


<details>
  <summary>Details</summary>
Motivation: 传统OFDM系统严重依赖导频和循环前缀，导致显著开销和频谱效率降低。AI原生无线通信需要智能空中接口在高度动态环境中自适应高效运行。

Method: 结合AI驱动的星座整形和神经网络接收器的端到端联合训练架构，引入轻量级信道适配器模块实现快速适应，采用统一模型支持多调制阶数，通过约束端到端训练控制峰均比。

Result: 广泛的仿真表明，该框架在不同信道场景下实现了优越的误码率、吞吐量和鲁棒性性能。

Conclusion: 该框架展示了AI原生下一代无线通信系统的潜力，能够显著提高频谱效率并降低系统开销。

Abstract: The advent of artificial intelligence (AI)-native wireless communication is
fundamentally reshaping the design paradigm of next-generation (NextG) systems,
where intelligent air interfaces are expected to operate adaptively and
efficiently in highly dynamic environments. Conventional orthogonal frequency
division multiplexing (OFDM) systems rely heavily on pilots and the cyclic
prefix (CP), resulting in significant overhead and reduced spectral efficiency.
To address these limitations, we propose an adaptive end-to-end (E2E)
transceiver architecture tailored for pilot-free and CP-free wireless systems.
The architecture combines AI-driven constellation shaping and a neural receiver
through joint training. To enhance robustness against mismatched or
time-varying channel conditions, we introduce a lightweight channel adapter
(CA) module, which enables rapid adaptation with minimal computational overhead
by updating only the CA parameters. Additionally, we present a framework that
is scalable to multiple modulation orders within a unified model, significantly
reducing model storage requirements. Moreover, to tackle the high
peak-to-average power ratio (PAPR) inherent to OFDM, we incorporate constrained
E2E training, achieving compliance with PAPR targets without additional
transmission overhead. Extensive simulations demonstrate that the proposed
framework delivers superior bit error rate (BER), throughput, and resilience
across diverse channel scenarios, highlighting its potential for AI-native
NextG.

</details>


### [24] [Learning-Based Blockage-Resilient Beam Training in Near-Field Terahertz Communications](https://arxiv.org/abs/2510.25433)
*Caihao Weng,Yuqing Guo,Bowen Zhao,Ying Wang,Wen Chen,Zhendong Li*

Main category: eess.SP

TL;DR: 本文提出基于自加速艾里光束的阻塞弹性近场波束训练方案，通过曲线传播绕过障碍物，并使用轻量级注意力网络联合预测最优波束参数。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信在6G中具有高吞吐量潜力，但高频段穿透损耗高，阻塞问题严重，特别是在近场室内通信中。需要解决障碍物导致的通信中断问题。

Method: 分析艾里光束轨迹和波束模式，揭示波束模式隐含接收器与障碍物的空间关系，提出基于多任务学习的轻量级注意力网络AMPBT-Net，联合预测角度、距离和曲率参数。

Result: 仿真结果表明艾里光束有效缓解阻塞效应，所提方案在显著降低训练开销的同时，性能可与穷举波束扫描相媲美。

Conclusion: 基于艾里光束的阻塞弹性波束训练方案为太赫兹近场通信提供了一种有效的障碍物规避方法，具有实际应用价值。

Abstract: Terahertz (THz) band is considered a promising candidate to meet the
high-throughput requirement for future sixth-generation (6G) wireless
communications due to its ultrawide bandwidth. However, due to the high
penetration loss at high-frequencies, blockage becomes a serious problem in THz
communications, especially in near-field indoor communications with numerous
obstacles. To address this issue, this paper investigates blockage-resilient
near-field beam training based on self-accelerating Airy beam, which can
propagate along a curved trajectory to circumvent obstacles. Specifically, we
first analyze the trajectory of the Airy beam and the beam pattern at the
receiver using a discrete Fourier transform (DFT) codebook in the presence of
obstacles. Interestingly, we reveal that the beam pattern not only captures the
receiver's location information but also implicitly encodes the spatial
relationship between the receiver and obstacle, which facilitates identifying
the optimal Airy beam configuration. Based on this insight, we formulate the
blockage-resilient beam training task as a multitask learning problem and
propose a lightweight attention-based multi-parameter beam training network
(AMPBT-Net) to jointly predict the angle, distance, and curvature parameters of
the optimal Airy beam based on the beam pattern. Finally, simulation results
demonstrate that the Airy beam effectively mitigates blockage effects and the
proposed scheme achieves comparable performance to exhaustive beam sweeping
while significantly reducing training overhead.

</details>


### [25] [Echo-Conditioned Denoising Diffusion Probabilistic Models for Multi-Target Tracking in RF Sensing](https://arxiv.org/abs/2510.25464)
*Amirhossein Azarbahram,Onel L. A. López*

Main category: eess.SP

TL;DR: 提出了一种基于条件去噪扩散概率模型(C-DDPM)的动态射频感知系统，用于多目标空间跟踪，通过变分自编码器压缩回波信号，利用分类器自由引导增强条件去噪性能。


<details>
  <summary>Details</summary>
Motivation: 开发能够学习目标参数时间演化的动态射频感知系统，利用噪声回波观测作为条件特征，提高多目标跟踪的准确性。

Method: 集成变分自编码器(VAE)进行回波压缩，使用条件去噪扩散概率模型(C-DDPM)预测未来目标状态，结合分类器自由引导技术增强条件去噪效果。

Result: 仿真结果显示，该方法在角度和距离跟踪方面显著优于传统信号处理、滤波和深度学习基准方法，实现了更低的估计误差。

Conclusion: C-DDPM辅助框架展示了生成模型在集成感知与通信中的潜力，为动态多目标跟踪提供了有效解决方案。

Abstract: In this paper, we consider a dynamic radio frequency sensing system aiming to
spatially track multiple targets over time. We develop a conditional denoising
diffusion probabilistic model (C-DDPM)-assisted framework that learns the
temporal evolution of target parameters by leveraging the noisy echo
observations as conditioning features. The proposed framework integrates a
variational autoencoder (VAE) for echo compression and utilizes classifier-free
guidance to enhance conditional denoising. In each transmission block, VAE
encodes the received echo into a latent representation that conditions DDPM to
predict future target states, which are then used for codebook beam selection.
Simulation results show that the proposed approach outperforms classical signal
processing, filtering, and deep learning benchmarks. The C-DDPM-assisted
framework achieves significantly lower estimation errors in both angle and
distance tracking, demonstrating the potential of generative models for
integrated sensing and communications.

</details>


### [26] [Adaptive Channel Estimation and Quantized Feedback for RIS Assisted Optical Wireless Communication Systems](https://arxiv.org/abs/2510.25467)
*Muhammad Khalil,Ke Wang,Jinho Choi*

Main category: eess.SP

TL;DR: 提出了一个用于可重构智能表面辅助光无线链路的统一建模、估计和反馈框架，包含长曝光像素增益模型、信道估计器和量化相位反馈，通过分析得出具体设计规则。


<details>
  <summary>Details</summary>
Motivation: 为RIS辅助光无线链路提供物理光学建模与估计反馈设计的统一框架，解决可扩展链路预算问题。

Method: 使用长曝光像素增益模型扩展经典衍射限制响应，结合自由空间路径损耗、大气消光、像素级衍射和光学效率，采用单位导频最小二乘信道估计器和量化相位反馈。

Result: 分析结果与蒙特卡洛模拟高度匹配：在N=64像素、M=2N导频长度、SNR=20dB条件下，归一化均方误差为0.005，有效SNR损失约0.5，容量损失0.007比特/秒；6位相位量化无额外损失。

Conclusion: 该框架将物理光学建模与估计反馈设计相协调，为RIS辅助光网络中的可扩展链路预算提供了原则性基础。

Abstract: This paper presents a unified modeling, estimation, and feedback framework
for reconfigurable intelligent surface RIS-assisted optical wireless links. The
key modeling element is a long-exposure pixel gain that extends the classical
diffraction-limited response by statistically averaging angular jitter and
mispointing; it admits an exact real-integral form and captures boresight
attenuation and progressive sidelobe filling. The end-to-end system couples
free-space path loss, Beer--Lambert atmospheric extinction, pixel-level
diffraction, and optical efficiency with a unitary-pilot least-squares channel
estimator and quantized phase feedback. Analysis closely matches Monte Carlo
simulations and yields concrete design rules: with a surface of N=64 pixels,
pilot length $M=2N$, and pilot SNR=20 dB, the normalized mean-squared error
is0.005, implying an effective-SNR loss of about 0.5 and a capacity penalty of
0.007bits-s. Six-bit phase quantization introduces no measurable additional
penalty at these operating points, setting a practical benchmark for feedback
resolution. Training overhead scales strongly with pixel geometry: halving
pixel width (quartering pixel area) increases the pilot length required to
maintain the same NMSE by roughly fourfold. The framework reconciles
physical-optics modeling with estimation-and-feedback design and provides a
principled basis for scalable link budgeting in RIS-assisted optical networks.

</details>


### [27] [Dynamic Beamforming and Power Allocation in ISAC via Deep Reinforcement Learning](https://arxiv.org/abs/2510.25496)
*Duc Nguyen Dao,André B. J. Kokkeler,Haibin Zhang,Yang Miao*

Main category: eess.SP

TL;DR: 提出基于深度强化学习的ISAC系统动态波束成形和功率分配方法，在2000次训练后收敛，达到SDR基准80%的频谱效率，但决策时间从4500ms大幅缩短到20ms，相比DQN基准提升30%总速率。


<details>
  <summary>Details</summary>
Motivation: 6G网络中集成感知与通信面临动态环境下实时资源分配的计算挑战，需要高效的自适应解决方案。

Method: 使用深度强化学习代理与环境交互，通过试错学习最优策略，采用预定义奖励函数指导训练过程。

Result: DRL方法在2000次训练后收敛，达到SDR基准80%频谱效率，决策时间20ms（相比SDR的4500ms），相比DQN基准提升30%总速率。

Conclusion: 深度强化学习在动态ISAC场景中具有实现实时高性能的潜力，显著提升计算效率。

Abstract: Integrated Sensing and Communication (ISAC) is a key enabler in 6G networks,
where sensing and communication capabilities are designed to complement and
enhance each other. One of the main challenges in ISAC lies in resource
allocation, which becomes computationally demanding in dynamic environments
requiring real-time adaptation. In this paper, we propose a Deep Reinforcement
Learning (DRL)-based approach for dynamic beamforming and power allocation in
ISAC systems. The DRL agent interacts with the environment and learns optimal
strategies through trial and error, guided by predefined rewards. Simulation
results show that the DRL-based solution converges within 2000 episodes and
achieves up to 80\% of the spectral efficiency of a semidefinite relaxation
(SDR) benchmark. More importantly, it offers a significant improvement in
runtime performance, achieving decision times of around 20 ms compared to 4500
ms for the SDR method. Furthermore, compared with a Deep Q-Network (DQN)
benchmark employing discrete beamforming, the proposed approach achieves
approximately 30\% higher sum-rate with comparable runtime. These results
highlight the potential of DRL for enabling real-time, high-performance ISAC in
dynamic scenarios.

</details>


### [28] [Quickest Change Point Detection with Measurements over a Lossy Link](https://arxiv.org/abs/2510.25604)
*Krishna Chaythanya KV,Saqib Abbas Baba,Anurag Kumar,Arpan Chattopadhyay,Rajesh Sundaresan*

Main category: eess.SP

TL;DR: 提出了一种用于无线传感器网络中快速变化检测的CUSUM算法，通过马尔可夫过程建模和队列管理策略来优化检测延迟。


<details>
  <summary>Details</summary>
Motivation: 受工业4.0应用驱动，研究在无线链路传输测量数据时如何快速检测过程变化，考虑数据包丢失和队列管理的影响。

Method: 使用伯努利采样和重传策略，将问题建模为马尔可夫过程，提出CUSUM算法，并探索LCFS队列策略和多传感器调度算法。

Result: 证明了算法在虚警率趋于零时的渐近最优性，通过数值分析展示了非渐近情况下的性能权衡。

Conclusion: 所提出的CUSUM算法和队列管理策略能有效降低检测延迟，适用于无线传感器网络中的快速变化检测问题。

Abstract: Motivated by Industry 4.0 applications, we consider quickest change detection
(QCD) of an abrupt change in a process when its measurements are transmitted by
a sensor over a lossy wireless link to a decision maker (DM). The sensor node
samples measurements using a Bernoulli sampling process, and places the
measurement samples in the transmit queue of its transmitter. The transmitter
uses a retransmit-until-success transmission strategy to deliver packets to the
DM over the lossy link, in which the packet losses are modeled as a Bernoulli
process, with different loss probabilities before and after the change. We pose
the QCD problem in the non-Bayesian setting under Lorden's framework, and
propose a CUSUM algorithm. By defining a suitable Markov process, involving the
DM measurements and the queue length process, we show that the problem reduces
to QCD in a Markov process. Characterizing the information measure per
measurement sample at the DM, we establish the asymptotic optimality of our
algorithm when the false alarm rate tends to zero. Further, when the DM
receives incomplete data due to channel loss, we present asymptotically optimal
QCD algorithms by suitably modifying the CUSUM algorithm. We then explore the
last-come-first-served (LCFS) queuing discipline at the sensor transmit queue
to lower detection delay in the non-asymptotic case. Next, we consider the case
of multiple sensors, each with its own wireless transmitter queue, and show
that our analysis extends to the case of multiple homogeneous sensors. When the
sensors are heterogeneous, we present a sensor scheduling algorithm that
minimizes detection delay by balancing the trade-off between the age of the
observations and their information content. Numerical analysis demonstrate
trade-offs that can be used to optimize system design parameters in the
non-asymptotic regime.

</details>


### [29] [Continuous subsurface property retrieval from sparse radar observations using physics informed neural networks](https://arxiv.org/abs/2510.25648)
*Ishfaq Aziz,Mohamad Alipour*

Main category: eess.SP

TL;DR: 提出了一种物理信息机器学习框架，用于重建地下介电常数作为深度的连续函数，通过结合测量数据和麦克斯韦方程进行训练。


<details>
  <summary>Details</summary>
Motivation: 传统波反演方法假设离散均匀层，需要密集测量或强先验知识，限制了在连续变化属性场景下的可扩展性和准确性。

Method: 使用物理信息机器学习框架，将地下介电常数重建为深度的连续神经网络函数，同时满足测量数据和麦克斯韦方程。

Result: 在多层天然材料上验证，与现场介电常数测量结果高度一致（R^2=0.93），对细微变化敏感（Δε_r=2），在两层系统中仅需三个战略布置传感器即可恢复准确剖面。

Conclusion: 该方法将地下反演从边界驱动重构为连续属性估计，能够准确表征平滑介电常数变化，推动低成本雷达系统的电磁成像应用。

Abstract: Estimating subsurface dielectric properties is essential for applications
ranging from environmental surveys of soils to nondestructive evaluation of
concrete in infrastructure. Conventional wave inversion methods typically
assume few discrete homogeneous layers and require dense measurements or strong
prior knowledge of material boundaries, limiting scalability and accuracy in
realistic settings where properties vary continuously. We present a physics
informed machine learning framework that reconstructs subsurface permittivity
as a fully neural, continuous function of depth, trained to satisfy both
measurement data and Maxwells equations. We validate the framework with both
simulations and custom built radar experiments on multilayered natural
materials. Results show close agreement with in-situ permittivity measurements
(R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2).
Parametric analysis reveals that accurate profiles can be recovered with as few
as three strategically placed sensors in two layer systems. This approach
reframes subsurface inversion from boundary-driven to continuous property
estimation, enabling accurate characterization of smooth permittivity
variations and advancing electromagnetic imaging using low cost radar systems.

</details>


### [30] [PyDPF: A Python Package for Differentiable Particle Filtering](https://arxiv.org/abs/2510.25693)
*John-Joseph Brady,Benjamin Cox,Víctor Elvira,Yunpeng Li*

Main category: eess.SP

TL;DR: 本文提出了一个基于PyTorch的统一API实现，用于多种可微分粒子滤波器(DPFs)，使这些算法更易于访问并促进算法间的比较。


<details>
  <summary>Details</summary>
Motivation: 粒子滤波器(PF)在状态空间模型中被广泛使用，但标准粒子滤波器不可微分，无法直接应用梯度优化技术。最近提出的可微分粒子滤波器通过修改重采样步骤解决了这个问题。

Method: 在PyTorch框架上构建统一API，实现多种可微分粒子滤波器算法，并通过复现现有研究实验来验证框架。

Result: 成功复现了多个现有研究的实验，展示了DPFs如何应用于解决状态空间建模中的常见挑战。

Conclusion: 该实现使可微分粒子滤波器算法更容易被广泛研究社区访问，并促进了算法间的直接比较。

Abstract: State-space models (SSMs) are a widely used tool in time series analysis. In
the complex systems that arise from real-world data, it is common to employ
particle filtering (PF), an efficient Monte Carlo method for estimating the
hidden state corresponding to a sequence of observations. Applying particle
filtering requires specifying both the parametric form and the parameters of
the system, which are often unknown and must be estimated. Gradient-based
optimisation techniques cannot be applied directly to standard particle
filters, as the filters themselves are not differentiable. However, several
recently proposed methods modify the resampling step to make particle filtering
differentiable. In this paper, we present an implementation of several such
differentiable particle filters (DPFs) with a unified API built on the popular
PyTorch framework. Our implementation makes these algorithms easily accessible
to a broader research community and facilitates straightforward comparison
between them. We validate our framework by reproducing experiments from several
existing studies and demonstrate how DPFs can be applied to address several
common challenges with state space modelling.

</details>


### [31] [Low Probability of Detection Communication Using Noncoherent Grassmannian Signaling](https://arxiv.org/abs/2510.25751)
*Diego Cuevas,Mikel Gutiérrez,Jesús Ibáñez,Ignacio Santamaria*

Main category: eess.SP

TL;DR: 提出基于直接序列扩频和Grassmannian信号的非相干低检测概率通信系统，通过噪声样分布增强隐蔽性，在低信噪比下提供竞争性误码率。


<details>
  <summary>Details</summary>
Motivation: 开发更隐蔽的通信系统，避免使用需要导频进行信道估计的相干方案，提高抗检测能力。

Method: 结合直接序列扩频和Grassmannian星座设计，利用其噪声样分布特性实现非相干通信。

Result: 仿真显示Grassmannian信号在低信噪比下提供竞争性误码率，同时保持对非目标接收器的低检测概率。

Conclusion: 非相干Grassmannian信号在LPD通信中具有实用性和安全性优势，改善了隐蔽性和性能。

Abstract: This paper proposes a noncoherent low probability of detection (LPD)
communication system based on direct sequence spread spectrum (DSSS) and
Grassmannian signaling. Grassmannian constellations enhance covertness because
they tend to follow a noise-like distribution. Simulations showed that
Grassmannian signaling provides competitive bit error rates (BER) at low
signal-to-noise ratio (SNR) regimes with low probability of detection at the
unintended receiver compared to coherent schemes that use QPSK or QAM
modulation formats and need pilots to perform channel estimation. The results
suggest the practicality and security benefits of noncoherent Grassmannian
signaling for LPD communications due to their improved covertness and
performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo协议利用群体智能和分布式成对排名共识，在AI推理中实现卓越性能，显著优于多数投票方法，并在多个基准测试中表现出更高的准确性和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 随着集中式AI面临计算瓶颈和训练规模收益递减，需要一种能够在容量和能力上水平扩展的推理层。

Method: 采用群体推理方法：基于同行排名、声誉加权的异构模型共识机制，使用成对排名和Bradley-Terry风格聚合模型，结合链上声誉系统和能力证明来抵抗女巫攻击。

Result: 在GPQA Diamond上达到85.90%准确率，比多数投票高出17.21个百分点（相对提升25.1%），在六个挑战性基准测试中均表现出更高准确性，对抗性提示注入仅降低0.12%。

Conclusion: 为去中心化AI系统奠定了基础，通过集体智能实现高质量推理的民主化访问，同时不牺牲可靠性或安全性。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [33] [From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning](https://arxiv.org/abs/2510.24812)
*Junsoo Oh,Jerry Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文分析了从弱CNN到强CNN的弱到强泛化现象，识别了数据稀缺和数据丰富两种机制，并揭示了泛化通过良性过拟合或标签修正实现，但过度训练可能降低性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究对弱到强泛化的理论解释多限于抽象框架或简单模型，缺乏对结构化数据和实际CNN架构的深入分析。

Method: 使用线性CNN作为弱模型，两层ReLU CNN作为强模型，分析梯度下降动态，考虑包含不同难度信号和噪声的结构化数据。

Result: 识别了两种机制：数据稀缺时泛化通过良性过拟合实现，数据丰富时通过早期标签修正实现，但过度训练会降低性能。

Conclusion: 弱到强泛化的机制取决于数据特征，数据稀缺时依赖过拟合，数据丰富时依赖标签修正，但需避免过度训练。

Abstract: Weak-to-strong generalization refers to the phenomenon where a stronger model
trained under supervision from a weaker one can outperform its teacher. While
prior studies aim to explain this effect, most theoretical insights are limited
to abstract frameworks or linear/random feature models. In this paper, we
provide a formal analysis of weak-to-strong generalization from a linear CNN
(weak) to a two-layer ReLU CNN (strong). We consider structured data composed
of label-dependent signals of varying difficulty and label-independent noise,
and analyze gradient descent dynamics when the strong model is trained on data
labeled by the pretrained weak model. Our analysis identifies two regimes --
data-scarce and data-abundant -- based on the signal-to-noise characteristics
of the dataset, and reveals distinct mechanisms of weak-to-strong
generalization. In the data-scarce regime, generalization occurs via benign
overfitting or fails via harmful overfitting, depending on the amount of data,
and we characterize the transition boundary. In the data-abundant regime,
generalization emerges in the early phase through label correction, but we
observe that overtraining can subsequently degrade performance.

</details>


### [34] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA是一个Python框架，用于从突变数据构建和分析适应性景观，计算20个生物学相关特征，帮助解释和比较适应性预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试缺乏关于底层适应性景观的地形信息，这限制了模型性能的解释和比较。

Method: GraphFLA框架从DNA、RNA、蛋白质等多种模式的突变数据构建适应性景观，计算20个特征来表征景观地形的4个基本方面。

Result: 应用GraphFLA分析了5,300多个景观，揭示了影响模型准确性的因素和不同模型的各自优势，并发布了155个组合完整的经验适应性景观。

Conclusion: GraphFLA提供了一个强大的工具来表征适应性景观地形，有助于更好地理解和比较适应性预测模型。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [35] [Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT](https://arxiv.org/abs/2510.24829)
*Benjamin Karic,Nina Herrmann,Jan Stenkamp,Paula Scharf,Fabian Gieseke,Angela Schwering*

Main category: cs.LG

TL;DR: 该论文评估了在ESP32-S3微控制器上使用低功耗广域网和压缩CNN进行环境监测，通过设备端推理仅传输结果可将能耗降低高达5倍。


<details>
  <summary>Details</summary>
Motivation: 环境监测需要能在偏远地区长期运行的节能物联网设备，数据传输能耗高是主要挑战。

Method: 在ESP32-S3微控制器上使用低功耗广域网和经过后训练量化的压缩CNN，进行设备端推理并仅传输结果。

Result: 设备端执行CNN推理并仅传输结果，相比发送原始图像数据可将整体能耗降低高达5倍，量化模型精度损失仅几个百分点。

Conclusion: 嵌入式机器学习可开发碳足迹更小、能在环境监测场景中自主运行的物联网应用。

Abstract: The integration of the Internet of Things (IoT) and Artificial Intelligence
offers significant opportunities to enhance our ability to monitor and address
ecological changes. As environmental challenges become increasingly pressing,
the need for effective remote monitoring solutions is more critical than ever.
A major challenge in designing IoT applications for environmental monitoring -
particularly those involving image data - is to create energy-efficient IoT
devices capable of long-term operation in remote areas with limited power
availability. Advancements in the field of Tiny Machine Learning allow the use
of Convolutional Neural Networks (CNNs) on resource-constrained,
battery-operated microcontrollers. Since data transfer is energy-intensive,
performing inference directly on microcontrollers to reduce the message size
can extend the operational lifespan of IoT nodes. This work evaluates the use
of common Low Power Wide Area Networks and compressed CNNs trained on domain
specific datasets on an ESP32-S3. Our experiments demonstrate, among other
things, that executing CNN inference on-device and transmitting only the
results reduces the overall energy consumption by a factor of up to five
compared to sending raw image data. %The compression of the model using Post
Training Quantization is accompanied by an acceptable reduction in accuracy of
only a few percentage points compared to a non-quantized model. These findings
advocate the development of IoT applications with reduced carbon footprint and
capable of operating autonomously in environmental monitoring scenarios by
incorporating Embedded Machine Learning.

</details>


### [36] [Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers](https://arxiv.org/abs/2510.25176)
*Mohammadreza Doostmohammadian,Zulfiya R. Gabidullina,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出了一种分布式机器学习的计算资源优化算法，通过协同优化数据分配和CPU资源分配，在时变网络中实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 随着AI研究的快速发展，对快速、计算高效且可扩展解决方案的需求日益增长，需要优化分布式机器学习的计算资源分配。

Method: 采用协同优化方法，在保证计算资源需求平衡约束的同时，训练每个计算节点的本地数据，支持对数尺度量化的信息交换。

Result: 与现有CPU调度方案相比，该算法将成本最优性差距改善了50%以上。

Conclusion: 该算法在时变网络中实现了收敛，并通过李雅普诺夫稳定性和特征谱分析证明了向最优情况的收敛性。

Abstract: In the rapidly evolving research on artificial intelligence (AI) the demand
for fast, computationally efficient, and scalable solutions has increased in
recent years. The problem of optimizing the computing resources for distributed
machine learning (ML) and optimization is considered in this paper. Given a set
of data distributed over a network of computing-nodes/servers, the idea is to
optimally assign the CPU (central processing unit) usage while simultaneously
training each computing node locally via its own share of data. This formulates
the problem as a co-optimization setup to (i) optimize the data processing and
(ii) optimally allocate the computing resources. The information-sharing
network among the nodes might be time-varying, but with balanced weights to
ensure consensus-type convergence of the algorithm. The algorithm is all-time
feasible, which implies that the computing resource-demand balance constraint
holds at all iterations of the proposed solution. Moreover, the solution allows
addressing possible log-scale quantization over the information-sharing
channels to exchange log-quantized data. For some example applications,
distributed support-vector-machine (SVM) and regression are considered as the
ML training models. Results from perturbation theory, along with Lyapunov
stability and eigen-spectrum analysis, are used to prove the convergence
towards the optimal case. As compared to existing CPU scheduling solutions, the
proposed algorithm improves the cost optimality gap by more than $50\%$.

</details>


### [37] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [38] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应多任务EEG分类器，使用GRU-TCN网络结合深度Q网络实时调整决策阈值，用于卒中快速分诊，在卒中类型分类上达到98.0%准确率。


<details>
  <summary>Details</summary>
Motivation: 卒中快速分诊需要准确且可在床边部署的工具，EEG有潜力但在首次接触时使用不足。

Method: 将32通道EEG信号转换为功率谱密度特征，使用GRU-TCN网络预测卒中类型、半球偏侧化和严重程度，并应用深度Q网络实时调整决策阈值。

Result: 基线GRU-TCN在卒中类型分类上达到89.3%准确率，严重程度96.9%，偏侧化96.7%；加入DQN阈值自适应后，卒中类型准确率提升至98.0%。

Conclusion: 自适应阈值调整可将操作点转向临床偏好的敏感度-特异度权衡，同时集成的头皮图谱和频谱可视化支持可解释性。

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [39] [Topic Analysis with Side Information: A Neural-Augmented LDA Approach](https://arxiv.org/abs/2510.24918)
*Biyi Fang,Kripa Rajshekhar,Truong Vo,Diego Klabjan*

Main category: cs.LG

TL;DR: 提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，在主题一致性、困惑度和下游分类任务上优于传统LDA和Dirichlet-Multinomial回归。


<details>
  <summary>Details</summary>
Motivation: 传统主题模型如LDA难以整合元数据、用户属性或文档标签等辅助信息，限制了其表达能力、个性化和可解释性。

Method: nnLDA使用神经先验机制，通过神经网络基于辅助特征生成主题比例的先验分布，捕捉辅助信息与主题分布间的复杂非线性交互。采用随机变分EM算法联合优化神经和概率组件。

Result: 在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类任务上持续优于LDA和Dirichlet-Multinomial回归。

Conclusion: 结合神经表示学习和概率主题建模在存在辅助信息的情况下具有显著优势。

Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been
widely used to uncover latent structures in text corpora, but they often
struggle to integrate auxiliary information such as metadata, user attributes,
or document labels. These limitations restrict their expressiveness,
personalization, and interpretability. To address this, we propose nnLDA, a
neural-augmented probabilistic topic model that dynamically incorporates side
information through a neural prior mechanism. nnLDA models each document as a
mixture of latent topics, where the prior over topic proportions is generated
by a neural network conditioned on auxiliary features. This design allows the
model to capture complex nonlinear interactions between side information and
topic distributions that static Dirichlet priors cannot represent. We develop a
stochastic variational Expectation-Maximization algorithm to jointly optimize
the neural and probabilistic components. Across multiple benchmark datasets,
nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in
topic coherence, perplexity, and downstream classification. These results
highlight the benefits of combining neural representation learning with
probabilistic topic modeling in settings where side information is available.

</details>


### [40] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: KAN-GCN是一个用于冰盖建模的快速准确模拟器，将Kolmogorov-Arnold网络作为特征校准器置于图卷积网络之前，通过可学习的一维扭曲和线性混合步骤改善特征条件和非线性编码，在保持精度的同时提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 提高冰盖数值模型模拟器的性能，通过改进特征条件和非线性编码来优化精度与效率的权衡，适用于大规模瞬态场景扫描。

Method: 使用KAN作为GCN的前端特征校准器，应用可学习的一维扭曲和线性混合步骤，在不增加消息传递深度的情况下改善特征条件。在Pine Island Glacier的36个融化率模拟数据集上进行训练和测试。

Result: 在2到5层架构中，KAN-GCN匹配或超越了纯GCN和MLP-GCN基线的准确性。在较粗网格上通过用节点级变换替换边级消息传递层提高了推理吞吐量，仅在最细网格上有适度成本增加。

Conclusion: KAN优先设计为大规模瞬态场景扫描提供了有利的精度与效率权衡，是冰盖建模模拟器的有效架构选择。

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [41] [WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning](https://arxiv.org/abs/2510.24927)
*Joel Frank Huarayo Quispe,Lilian Berton,Didier Vega-Oliveros*

Main category: cs.LG

TL;DR: 提出WBT-BGRL框架，通过加权三元组损失增强自举学习，用于二分图的归纳链接预测


<details>
  <summary>Details</summary>
Motivation: 二分图链接预测在推荐系统和故障检测中很重要，但现有方法在归纳、加权和二分场景下的有效性未经测试，且对比方法存在负采样效率低和偏差问题

Method: 使用双GCN编码器的二分架构，通过新颖的加权机制增强三元组损失的自举学习

Result: 在真实数据集（工业和电商）上表现出竞争力，特别是在预训练期间应用加权时效果显著

Conclusion: 加权非对比学习对二分图的归纳链接预测具有重要价值

Abstract: Link prediction in bipartite graphs is crucial for applications like
recommendation systems and failure detection, yet it is less studied than in
monopartite graphs. Contrastive methods struggle with inefficient and biased
negative sampling, while non-contrastive approaches rely solely on positive
samples. Existing models perform well in transductive settings, but their
effectiveness in inductive, weighted, and bipartite scenarios remains untested.
To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph
Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped
learning with a novel weighting mechanism in the triplet loss. Using a
bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against
adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on
real-world datasets (Industry and E-commerce) show competitive performance,
especially when weighting is applied during pretraining-highlighting the value
of weighted, non-contrastive learning for inductive link prediction in
bipartite graphs.

</details>


### [42] [Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought](https://arxiv.org/abs/2510.24941)
*Jiachen Zhao,Yiyou Sun,Weiyan Shi,Dawn Song*

Main category: cs.LG

TL;DR: 研究发现大语言模型生成的思维链中许多推理步骤对最终预测没有实际因果影响，仅少数步骤真正驱动模型预测。作者提出真实思维评分来识别这些装饰性思维步骤，并发现可以通过潜在空间操控强制模型执行或忽略特定推理步骤。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的思维链推理步骤常被视为模型内部思考过程的忠实反映，用于监控不安全意图。但作者发现许多推理步骤实际上并不真正贡献于模型预测，这影响了LLM推理的效率和思维链的可信度。

Method: 提出真实思维评分(TTS)来测量每个推理步骤对模型最终预测的因果影响，识别真实思维步骤和装饰性思维步骤。在LLM潜在空间中识别真实思维方向，通过操控该方向强制模型执行或忽略特定推理步骤。

Result: 在AIME数据集上，Qwen-2.5模型中只有平均2.3%的思维链推理步骤TTS≥0.7。自我验证步骤也可能是装饰性的，通过沿真实思维方向操控可以改变最终结果。

Conclusion: 大语言模型常常口头表达推理步骤但内部并未真正执行，这既降低了LLM推理效率，也损害了思维链的可信度。需要更可靠的方法来确保模型推理过程的真实性。

Abstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT)
at test time, enabling them to solve complex tasks. These reasoning steps in
CoT are often assumed as a faithful reflection of the model's internal thinking
process, and used to monitor unsafe intentions. However, we find many reasoning
steps don't truly contribute to LLMs' prediction. We measure the step-wise
causal influence of each reasoning step on the model's final prediction with a
proposed True Thinking Score (TTS). We reveal that LLMs often interleave
between true-thinking steps (which are genuinely used to produce the final
output) and decorative-thinking steps (which only give the appearance of
reasoning but have minimal causal impact). Notably, only a small subset of the
total reasoning steps have a high TTS that causally drive the model's
prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning
steps in CoT have a TTS >= 0.7 (range: 0-1) under the Qwen-2.5 model.
Furthermore, we identify a TrueThinking direction in the latent space of LLMs.
By steering along or against this direction, we can force the model to perform
or disregard certain CoT steps when computing the final result. Finally, we
highlight that self-verification steps in CoT (i.e., aha moments) can also be
decorative, where LLMs do not truly verify their solution. Steering along the
TrueThinking direction can force internal reasoning over these steps, resulting
in a change in the final results. Overall, our work reveals that LLMs often
verbalize reasoning steps without actually performing them internally, which
undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.

</details>


### [43] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 该论文研究了视觉语言模型中文化敏感神经元的存在及其对文化多样性视觉问答的影响，提出了一种新的对比激活选择方法来识别这些神经元，并发现它们倾向于聚集在特定的解码器层中。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型表现出色，但在处理文化相关输入时仍有困难。研究旨在了解模型如何处理文化背景信息，特别是是否存在对特定文化背景敏感的神经元。

Method: 使用CVQA基准测试，识别文化选择性神经元，并通过停用不同识别方法标记的神经元进行因果测试。提出新的基于边际的选择器——对比激活选择(CAS)，并在三个VLMs上对25个文化群体进行实验。

Result: 实验证明存在这样的神经元：停用它们会显著损害对应文化问题的性能，而对其他文化影响最小。CAS方法在识别文化敏感神经元方面优于现有的基于概率和熵的方法。层分析显示这些神经元倾向于聚集在某些解码器层。

Conclusion: 研究揭示了多模态表征的内部组织方式，为理解视觉语言模型如何处理文化信息提供了新视角。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [44] [Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms](https://arxiv.org/abs/2510.24951)
*Bernhard Klein*

Main category: cs.LG

TL;DR: 该论文提出通过算法-硬件协同设计的方法，同时提升机器学习的效率和可靠性，包括模型压缩、近似贝叶斯推理、数字/模拟硬件优化等技术。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习在嵌入式等资源受限平台上的计算需求不断增长，限制了可扩展性和效率。神经网络不仅需要高效运行，还需要在分布偏移或未见数据下提供可靠预测。

Method: 采用算法和硬件效率的联合优化：1) Galen系统通过敏感性分析和硬件反馈进行自动分层压缩；2) 模拟加速器建模设备缺陷，扩展噪声训练；3) 开发解析和集成近似方法替代昂贵采样；4) 概率光子计算利用模拟噪声作为熵源。

Result: 实现了资源高效且鲁棒的推理，为传统和贝叶斯神经网络提供了效率与可靠性的共同提升。

Conclusion: 通过算法-硬件协同设计，效率和可靠性可以共同推进，为下一代可信赖、高能效的机器学习系统奠定基础。

Abstract: While modern machine learning has transformed numerous application domains,
its growing computational demands increasingly constrain scalability and
efficiency, particularly on embedded and resource-limited platforms. In
practice, neural networks must not only operate efficiently but also provide
reliable predictions under distributional shifts or unseen data. Bayesian
neural networks offer a principled framework for quantifying uncertainty, yet
their computational overhead further compounds these challenges.
  This work advances resource-efficient and robust inference for both
conventional and Bayesian neural networks through the joint pursuit of
algorithmic and hardware efficiency. The former reduces computation through
model compression and approximate Bayesian inference, while the latter
optimizes deployment on digital accelerators and explores analog hardware,
bridging algorithmic design and physical realization. The first contribution,
Galen, performs automatic layer-specific compression guided by sensitivity
analysis and hardware-in-the-loop feedback. Analog accelerators offer
efficiency gains at the cost of noise; this work models device imperfections
and extends noisy training to nonstationary conditions, improving robustness
and stability. A second line of work advances probabilistic inference,
developing analytic and ensemble approximations that replace costly sampling,
integrate into a compiler stack, and optimize embedded inference. Finally,
probabilistic photonic computing introduces a paradigm where controlled analog
noise acts as an intrinsic entropy source, enabling fast, energy-efficient
probabilistic inference directly in hardware.
  Together, these studies demonstrate how efficiency and reliability can be
advanced jointly through algorithm-hardware co-design, laying the foundation
for the next generation of trustworthy, energy-efficient machine-learning
systems.

</details>


### [45] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 该论文发现大型语言模型存在固有的低维结构，表现为模型logits矩阵具有低近似秩，并证明可以利用这种结构通过无关甚至无意义提示的线性组合来生成目标提示的响应。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型的内在低维结构，理解其作为序列概率模型的基本特性。

Method: 通过实证分析展示多种现代语言模型的logits矩阵具有低秩特性，并利用这种低秩结构进行生成实验，同时建立理论抽象模型来分析表示能力和学习保证。

Result: 实验证实语言模型确实存在低秩结构，能够通过线性组合无关提示的输出来生成目标响应，理论分析也支持这一发现。

Conclusion: 语言模型具有固有的低维结构，这种结构可以用于改进生成方法，并为理解模型内部机制提供了新的理论框架。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [46] [Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection](https://arxiv.org/abs/2510.24043)
*Akira Tamamori*

Main category: cs.LG

TL;DR: 提出了一种名为Two-Stage LKPLO的新型多阶段异常检测框架，通过结合广义损失函数、全局核PCA和局部聚类，解决了传统投影方法依赖固定统计指标和单一数据结构的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于投影的异常检测方法存在两个共存限制：依赖固定统计指标和假设单一数据结构。这些限制导致在处理复杂数据时性能不佳。

Method: 采用三阶段方法：(1) 使用广义损失函数PLO替代固定指标；(2) 全局核PCA阶段线性化非线性数据结构；(3) 局部聚类阶段处理多模态分布。

Result: 在10个基准数据集上的5折交叉验证实验显示，Two-Stage LKPLO实现了最先进的性能，特别是在多簇数据(Optdigits)和复杂高维数据(Arrhythmia)上显著优于现有方法。

Conclusion: 该工作为重要类别的异常检测问题提供了强大新工具，并强调了混合多阶段架构的重要性，消融研究证实核化和局部化阶段的协同组合对其优越性能不可或缺。

Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection
framework that overcomes the coexisting limitations of conventional
projection-based methods: their reliance on a fixed statistical metric and
their assumption of a single data structure. Our framework uniquely synthesizes
three key concepts: (1) a generalized loss-based outlyingness measure (PLO)
that replaces the fixed metric with flexible, adaptive loss functions like our
proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear
data structures; and (3) a subsequent local clustering stage to handle
multi-modal distributions. Comprehensive 5-fold cross-validation experiments on
10 benchmark datasets, with automated hyperparameter optimization, demonstrate
that Two-Stage LKPLO achieves state-of-the-art performance. It significantly
outperforms strong baselines on datasets with challenging structures where
existing methods fail, most notably on multi-cluster data (Optdigits) and
complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study
empirically confirms that the synergistic combination of both the kernelization
and localization stages is indispensable for its superior performance. This
work contributes a powerful new tool for a significant class of outlier
detection problems and underscores the importance of hybrid, multi-stage
architectures.

</details>


### [47] [Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution](https://arxiv.org/abs/2510.24974)
*Mia Adler,Carrie Liang,Brian Peng,Oleg Presnyakov,Justin M. Baker,Jannelle Lauffer,Himani Sharma,Barry Merriman*

Main category: cs.LG

TL;DR: 提出了一种基于排名条件的委员会框架，利用排名构象为每个排名分配深度神经网络委员会，从而从原理上分离认知不确定性和构象不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的结构感知机器学习辅助定向进化方法通常依赖单一构象或单一委员会，无法有效分离构象不确定性和认知不确定性。

Method: 引入排名条件委员会框架，利用排名构象为每个排名分配深度神经网络委员会，实现认知不确定性和构象不确定性的分离。

Result: 在SARS-CoV-2抗体对接任务上验证，相比基线策略有显著改进。

Conclusion: 该方法为治疗性抗体发现提供了可扩展的途径，同时直接解决了构象不确定性建模的挑战。

Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for
efficiently navigating antibody fitness landscapes. Many structure-aware MLDE
pipelines rely on a single conformation or a single committee across all
conformations, limiting their ability to separate conformational uncertainty
from epistemic uncertainty. Here, we introduce a rank -conditioned committee
(RCC) framework that leverages ranked conformations to assign a deep neural
network committee per rank. This design enables a principled separation between
epistemic uncertainty and conformational uncertainty. We validate our approach
on SARS-CoV-2 antibody docking, demonstrating significant improvements over
baseline strategies. Our results offer a scalable route for therapeutic
antibody discovery while directly addressing the challenge of modeling
conformational uncertainty.

</details>


### [48] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 提出基于博弈论的端到端特征选择框架，通过评估特征的协同作用和边际贡献来确定特征重要性，在保持预测性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 数据量指数增长导致机器学习模型训练计算成本急剧上升，许多特征对模型性能没有正面贡献却消耗大量计算资源。

Method: 基于合作博弈论构建特征选择框架，将特征建模为玩家，通过评估协同交互和边际贡献确定特征重要性。包含四个核心组件：样本选择、博弈论特征重要性评估、冗余特征消除和优化模型训练。

Result: 实验结果表明该方法在保持预测性能的同时实现了显著的计算减少，为解决大规模机器学习计算挑战提供了高效解决方案。

Conclusion: 基于博弈论的特征选择框架能够有效识别和消除冗余特征，在降低计算成本的同时维持模型性能，为大规模机器学习应用提供了实用解决方案。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [49] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion是一种风险感知的采样方法，通过将每个去噪步骤视为无条件先验和状态条件策略头之间的顺序假设检验，为扩散策略添加了统计风险控制。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散策略在离线强化学习中具有竞争力，但采样时通常使用缺乏统计风险概念的启发式方法指导。需要一种具有用户可解释风险预算的风险感知采样规则。

Method: 累积对数似然比，并使用逻辑控制器对条件均值进行门控，其阈值τ在H0下校准一次以满足用户指定的Type-I水平α。训练保持标准DDPM结构，LRT指导与Q梯度自然组合。

Result: 在D4RL MuJoCo任务上，LRT-Diffusion在实现中优于强Q指导基线，改善了回报-OOD权衡，同时满足期望的α水平。

Conclusion: LRT-Diffusion是一种即插即用的推理时方法，为离线RL的扩散策略添加了原则性、校准的风险控制。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [50] [Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation](https://arxiv.org/abs/2510.24986)
*Ria Jayanti,Tanish Jain*

Main category: cs.LG

TL;DR: 该研究提出了一种结合实时癫痫检测和预测的新方法，使用多种机器学习算法在CHB-MIT头皮EEG数据库上进行评估，旨在实现从被动管理到主动预防的转变。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅在癫痫发作后检测，限制了早期干预机会。本研究旨在开发能够预测癫痫发作的系统，实现更主动的治疗策略。

Method: 使用K近邻、逻辑回归、随机森林和支持向量机进行癫痫检测；使用长短期记忆网络进行癫痫预测，利用其处理时间序列数据的能力。

Result: 逻辑回归检测准确率90.9%，召回率89.6%；随机森林和SVM准确率94.0%但召回率为0%；LSTM预测准确率达到89.26%。

Conclusion: 研究表明开发可访问的实时监测工具具有潜力，能够从被动检测转向主动预测，帮助患者采取预防措施降低风险。

Abstract: In recent years, machine learning has become an increasingly powerful tool
for supporting seizure detection and monitoring in epilepsy care. Traditional
approaches focus on identifying seizures only after they begin, which limits
the opportunity for early intervention and proactive treatment. In this study,
we propose a novel approach that integrates both real-time seizure detection
and prediction, aiming to capture subtle temporal patterns in EEG data that may
indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT
Scalp EEG Database, which includes 969 hours of recordings and 173 seizures
collected from 23 pediatric and young adult patients with drug-resistant
epilepsy. To support seizure detection, we implemented a range of supervised
machine learning algorithms, including K-Nearest Neighbors, Logistic
Regression, Random Forest, and Support Vector Machine. The Logistic Regression
achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced
performance suitable for clinical screening. Random Forest and Support Vector
Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to
detect any seizures, illustrating that accuracy alone is insufficient for
evaluating medical ML models with class imbalance. For seizure prediction, we
employed Long Short-Term Memory (LSTM) networks, which use deep learning to
model temporal dependencies in EEG data. The LSTM model achieved 89.26%
prediction accuracy. These results highlight the potential of developing
accessible, real-time monitoring tools that not only detect seizures as
traditionally done, but also predict them before they occur. This ability to
predict seizures marks a significant shift from reactive seizure management to
a more proactive approach, allowing patients to anticipate seizures and take
precautionary measures to reduce the risk of injury or other complications.

</details>


### [51] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 论文研究了训练和测试数据分布比例不匹配的情况，发现在许多场景下这种分布偏移可能有益，甚至能提升测试性能，即使各组件之间没有关联和迁移。


<details>
  <summary>Details</summary>
Motivation: 探讨训练和测试数据分布比例不匹配的影响，挑战传统认为分布偏移总是有害的观点，研究在什么条件下分布偏移反而能带来性能提升。

Method: 通过理论分析和多种场景的建模，识别最优训练比例，分析分布偏移的潜在益处，并扩展到组合性场景中不同组件"技能"的分布变化。

Result: 发现在许多设置中，分布偏移确实可以是有益的，测试性能可能因训练比例不匹配而改善，即使各组件之间没有关联。确定了最优训练比例和分布偏移的益处程度。

Conclusion: 分布偏移在某些条件下可以是有益的，而非总是有害，这为机器学习中的训练策略提供了新的视角，特别是在处理混合分布和组合性任务时。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [52] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 该论文提出了一种将自监督Transformer变化点检测模块集成到Option-Critic框架中的新架构，通过自适应分割状态轨迹来发现选项，解决了分层强化学习中子目标发现和终止边界学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 分层强化学习在长时程任务中通过时间抽象提高决策可扩展性，但实际应用中面临自主发现语义上有意义的子目标和学习最优选项终止边界的挑战。

Method: 集成自监督Transformer变化点检测模块到Option-Critic框架，使用启发式伪标签训练CPD模块推断环境动态的潜在变化，利用推断的变化点稳定终止函数梯度、预训练内部选项策略，并通过选项间差异惩罚强制功能专业化。

Result: 在Four-Rooms和Pinball任务上的实验表明，CPD引导的智能体表现出加速收敛、更高累积回报和显著改进的选项专业化。

Conclusion: 通过变化点分割整合结构先验可以在复杂环境中产生更可解释、样本效率更高和更稳健的分层策略。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [53] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，将数据增强与因果推断结合，论证了数据增强不仅可用于i.i.d.设置下的正则化，还能通过将其视为对治疗生成机制的干预来改善因果效应估计，特别是在存在未观测混杂因素的情况下。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强主要用于i.i.d.设置下的正则化，但作者认为当结果生成机制对数据增强选择不变时，数据增强可被视为对治疗生成机制的干预，这有助于减少由隐藏混杂因素引起的因果效应估计偏差。由于工具变量在实际应用中可能不易获得，而数据增强更易实施，因此探索数据增强在因果推断中的潜力具有重要价值。

Method: 作者引入了IV-like回归概念，通过适当正则化基于工具变量的估计器来缓解混杂偏差，即使某些工具变量属性被放宽。此外，将参数化数据增强建模为IV-like回归问题，并通过组合使用模拟数据增强的最坏情况应用。

Result: 理论分析和模拟实验表明，该方法在因果估计和泛化任务上的表现优于简单数据增强。在总体情况和有限样本情况下均得到验证，并通过真实数据实验进一步支持了该方法的有效性。

Conclusion: 数据增强可以超越传统的i.i.d.正则化应用，在因果推断中发挥重要作用，特别是在工具变量不可得的情况下，通过将数据增强视为干预并采用IV-like回归方法，能够有效改善因果效应估计和跨干预的泛化性能。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [54] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 本文系统解构了矩阵白化优化器，发现性能提升主要来自方差适应而非谱归一化，方差适应的优化器始终优于符号下降版本。


<details>
  <summary>Details</summary>
Motivation: 研究各种近似矩阵白化变换的优化器，旨在分离解释性能的关键组件，理解为什么矩阵白化方法优于元素级优化器如Adam。

Method: 通过系统解构矩阵白化优化器，比较不同变体的性能，包括SOAP和Muon等，分析谱归一化和方差适应各自的作用。

Result: 所有矩阵白化方法都可靠地优于元素级优化器；性能提升主要来自方差适应而非准确的谱归一化；低秩方差估计器能有效降低内存成本且无性能损失。

Conclusion: 矩阵白化的性能优势主要源于方差适应组件，而非谱归一化；方差适应的优化器设计是提升性能的关键因素。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [55] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出效用校准框架，通过特定效用函数评估多类分类器的校准误差，统一并重新解释现有校准指标。


<details>
  <summary>Details</summary>
Motivation: 现有多类校准评估方法要么关注预测的特定方面（如top-class置信度、类级校准），要么使用计算复杂的变分公式，需要可扩展的评估框架。

Method: 提出效用校准框架，通过定义特定效用函数来衡量校准误差，该函数封装了与最终用户相关的目标或决策标准。

Result: 该框架能够统一和重新解释多个现有校准指标，特别是允许更鲁棒的top-class和类级校准指标，并扩展到评估更丰富的下游效用类别。

Conclusion: 效用校准提供了一个通用且可扩展的框架来评估多类校准，超越了传统的二值化方法，能够适应多样化的下游应用需求。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [56] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE是一种深度多编码器自编码器，能够将多区域神经记录分解为共享和私有潜在子空间，通过新颖的对齐和解缠损失来分离网络级动态和区域特定活动。


<details>
  <summary>Details</summary>
Motivation: 在多区域神经数据建模中，分离共享网络级动态与区域特定活动是一个核心挑战，现有方法难以有效处理非线性扭曲和时间错位。

Method: SPIRE使用深度多编码器自编码器架构，引入对齐和解缠损失来分解神经记录，仅使用基线数据进行训练，能够恢复跨区域结构并揭示外部扰动如何重组这些结构。

Result: 在具有真实潜在变量的合成基准测试中，SPIRE在非线性扭曲和时间错位下优于经典概率模型。在颅内深部脑刺激记录应用中，SPIRE显示共享潜在变量可靠地编码了刺激特异性特征，这些特征在不同部位和频率间具有泛化性。

Conclusion: SPIRE为分析刺激下多区域神经动态提供了一个实用、可复现的工具，能够可靠地识别跨区域共享的刺激特异性神经特征。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [57] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN是一个基于线性RNN的单变量时间序列基础模型，仅使用合成数据进行预训练，在零样本预测中表现出色，超越了现有仅使用合成数据的方法和大多数使用真实数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 现有零样本时间序列预测的基础模型在高效长时预测和可复现性方面面临挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。

Method: 采用GatedDeltaProduct架构和状态编织技术，基于线性RNN构建模型，使用统一的合成数据流水线生成多样化数据，包括随机微分方程、高斯过程和音频合成等。

Result: 在Gift-Eval基准测试的零样本评估中，TempoPFN取得了顶级竞争性能，超越了所有现有的仅使用合成数据的方法和大多数使用真实数据训练的模型。

Conclusion: TempoPFN提供了一个可复现的时间序列预测基础模型，通过完全可并行化的训练和推理实现了高效性，开源了完整的数据生成流水线和训练代码。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [58] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 该研究系统评估了基于放射组学的机器学习模型在MRI序列分布偏移下的鲁棒性，发现使用协议不变特征训练的模型在分布偏移下保持高性能，而使用全部特征的模型性能下降40%。数据增强显著改善了不确定性估计质量。


<details>
  <summary>Details</summary>
Motivation: 放射组学模型在临床决策支持中具有潜力，但容易受到成像协议、定位和分割变化引起的分布偏移影响，需要系统评估其鲁棒性。

Method: 使用16个水果的体模，评估了：(1)5种MRI序列的协议变化；(2)分割变化（完整、部分、旋转）；(3)观察者间变异性。训练XGBoost分类器，比较协议不变特征与序列特定特征的表现。

Result: 使用协议不变特征训练的模型在分布偏移下F1分数>0.85，而使用全部特征的模型在协议变化下性能下降40%。数据增强使预期校准误差降低35%，且不牺牲准确性。

Conclusion: 协议感知特征选择和受控体模研究能有效预测模型在分布偏移下的行为，为开发对真实世界协议变化具有鲁棒性的放射组学模型提供了框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [59] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Orlicz几何结构的广义Sobolev IPM方法，通过Musielak正则化将复杂的优化问题简化为单变量优化，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有Sobolev IPM方法受限于L^p几何结构，无法融入其他结构先验。为了克服这一限制，需要开发能够容纳多样化几何先验的广义框架。

Method: 通过Orlicz几何结构推广Sobolev IPM，建立Orlicz-Sobolev范数与Musielak范数的理论联系，并利用图结构将GSI-M简化为单变量优化问题。

Result: GSI-M在计算上比流行的OW快几个数量级，在文档分类和拓扑数据分析任务中表现出实际优势。

Conclusion: 提出的GSI-M框架不仅包含经典Sobolev IPM作为特例，还能容纳超越传统L^p结构的多样化几何先验，同时实现了显著的计算效率提升。

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [60] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出了一种基于因果效应估计任务的ADMG图距离度量方法，用于评估因果发现方法在潜在混杂下的性能


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法难以在潜在混杂下进行有效评估，需要一种基于实际因果推断任务的图距离度量

Method: 使用固定识别方法和符号验证器，量化图差异对因果效应估计量的扭曲程度

Result: 分析了该度量在不同图扰动下的行为，并与现有距离度量进行了比较

Conclusion: 该方法为因果发现方法在潜在混杂下的评估提供了更实用的度量标准

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [61] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了神经随机流（NSFs）及其潜在变体，通过条件归一化流直接学习SDE转移规律，实现任意状态间的一次性采样，相比传统数值方法获得高达两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统SDE建模需要昂贵的数值求解器在任意时间点间采样，计算成本高。

Method: 使用带有架构约束的条件归一化流直接学习SDE转移规律，保持随机流的数学特性。

Result: 在大时间间隔下获得高达两个数量级的加速，同时在合成SDE模拟和真实世界跟踪、视频数据上保持与数值方法相当的分布准确性。

Conclusion: NSFs在保持分布准确性的同时，显著减少了任意时间点采样的计算成本。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


### [62] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出DWMGrad优化算法，通过动态引导机制自适应调整动量和学习率，解决传统优化器在复杂模型和非凸优化中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统优化算法如SGD和Adam在处理复杂数据结构和模型时存在不足，包括学习率选择困难、容易陷入局部最优、高维空间导航等问题。

Method: 基于传统方法，引入依赖历史数据的动态引导机制，动态更新动量和学习率，使优化器能灵活调整对历史信息的依赖。

Result: 实验验证表明DWMGrad在多种场景下能够实现更快的收敛速度和更高的准确率。

Conclusion: DWMGrad算法通过动态自适应机制显著提升了优化器在复杂训练环境下的性能表现。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [63] [Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs](https://arxiv.org/abs/2510.25074)
*Andrew Clark,Jack Moursounidis,Osmaan Rasouli,William Gan,Cooper Doyle,Anna Leontjeva*

Main category: cs.LG

TL;DR: BOND是一种扰动方法，用于估计无法访问计算图的网络结构中的偏导数，比现有扰动方法具有更高准确性和可扩展性，支持集成黑盒函数的新架构探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在无法访问计算图的网络结构中估计偏导数，限制了集成黑盒函数的可训练架构的发展。

Method: 提出了Bounded Numerical Differentiation (BOND)方法，这是一种扰动方法，通过数值微分技术估计网络结构中的偏导数。

Result: BOND方法在准确性和可扩展性上优于现有扰动方法，实验表明固定、未训练的黑盒函数网络可以提升模型性能而不增加可训练参数数量。

Conclusion: 固定不可训练模块有潜力扩展模型容量，为结合模拟和数字设备扩展网络提供了可行路径。

Abstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method
for estimating partial derivatives across network structures with inaccessible
computational graphs. BOND demonstrates improved accuracy and scalability from
existing perturbative methods, enabling new explorations of trainable
architectures that integrate black-box functions. We observe that these
black-box functions, realized in our experiments as fixed, untrained networks,
can enhance model performance without increasing the number of trainable
parameters. This improvement is achieved without extensive optimization of the
architecture or properties of the black-box function itself. Our findings
highlight the potential of leveraging fixed, non-trainable modules to expand
model capacity, suggesting a path toward combining analogue and digital devices
as a mechanism for scaling networks.

</details>


### [64] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO是一种用于推荐系统中LoRA持续学习的新方法，通过近端正则化来平衡适应性和保留性，专注于捕捉近期用户行为而非保留过时偏好。


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的持续学习方法主要关注保留先前任务的性能，但推荐系统的目标是预测当前偏好而非过去偏好，过时的偏好甚至会在用户兴趣显著变化时损害性能。

Method: 提出PESO方法，引入近端正则化器将当前适配器锚定到最近的冻结状态，使模型能够灵活平衡适应性和保留性，更好地捕捉近期用户行为。

Result: 理论上证明这种近端设计在LoRA子空间中提供数据感知、方向性指导；实证上PESO持续优于现有的基于LoRA的持续学习方法。

Conclusion: PESO通过近端正则化有效解决了推荐系统中持续学习的独特挑战，能够更好地适应不断变化的用户偏好。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [65] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: FairMIB是一个多视图信息瓶颈框架，通过分解图的特征、结构和扩散视图来减轻GNN中的复杂性偏见，使用对比学习和条件信息瓶颈目标来平衡任务效用和公平性。


<details>
  <summary>Details</summary>
Motivation: GNN在处理关系数据时可能放大训练数据偏见，传播歧视性属性和结构不平衡，导致不公平结果。现有公平方法通常将偏见视为单一来源，忽略了属性和结构效应的区别。

Method: 提出FairMIB框架：1）将图分解为特征、结构和扩散三个视图；2）使用对比学习最大化跨视图互信息进行无偏见表示学习；3）集成多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性；4）在扩散视图中引入逆概率加权邻接校正以减少偏见传播。

Result: 在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。

Conclusion: FairMIB通过多视图分解和信息瓶颈方法有效解决了GNN中的公平性问题，在保持任务效用的同时显著提升了公平性表现。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [66] [The Neural Differential Manifold: An Architecture with Explicit Geometric Structure](https://arxiv.org/abs/2510.25113)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出神经微分流形(NDM)架构，将神经网络重新概念化为可微分流形，通过几何结构增强泛化能力和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在欧几里得参数空间中运作，缺乏几何结构。NDM旨在将几何结构显式融入神经网络设计，提供内在正则化和更好的优化特性

Method: 三层架构：坐标层实现平滑图表转换，几何层通过辅助子网络动态生成流形度量，演化层通过双目标损失函数优化任务性能和几何简洁性

Result: 该框架支持与学习流形几何对齐的自然梯度下降优化，提供前所未有的可解释性，赋予内部表示清晰的几何意义

Conclusion: NDM代表了向几何结构化、可解释且高效深度学习系统的根本性转变，尽管仍存在显著计算挑战

Abstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural
network architecture that explicitly incorporates geometric structure into its
fundamental design. Departing from conventional Euclidean parameter spaces, the
NDM re-conceptualizes a neural network as a differentiable manifold where each
layer functions as a local coordinate chart, and the network parameters
directly parameterize a Riemannian metric tensor at every point. The
architecture is organized into three synergistic layers: a Coordinate Layer
implementing smooth chart transitions via invertible transformations inspired
by normalizing flows, a Geometric Layer that dynamically generates the
manifold's metric through auxiliary sub-networks, and an Evolution Layer that
optimizes both task performance and geometric simplicity through a
dual-objective loss function. This geometric regularization penalizes excessive
curvature and volume distortion, providing intrinsic regularization that
enhances generalization and robustness. The framework enables natural gradient
descent optimization aligned with the learned manifold geometry and offers
unprecedented interpretability by endowing internal representations with clear
geometric meaning. We analyze the theoretical advantages of this approach,
including its potential for more efficient optimization, enhanced continual
learning, and applications in scientific discovery and controllable generative
modeling. While significant computational challenges remain, the Neural
Differential Manifold represents a fundamental shift towards geometrically
structured, interpretable, and efficient deep learning systems.

</details>


### [67] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的双层对抗学习模型，从数据扰动角度解释聚类模型中的对抗攻击机制，并分析了δ-度量的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型结构复杂，对抗攻击机制尚未得到很好解释，如何衡量攻击效果仍不清楚。

Method: 提出统一的双层对抗学习模型，从数据扰动角度研究聚类模型中的对抗攻击，分析δ-度量的良好定义性。

Result: 发现当数据扰动较小时聚类模型具有鲁棒性，当扰动较大时聚类结果会改变从而导致攻击。

Conclusion: δ-度量可用于所提出的聚类模型对抗学习双层模型中，为衡量对抗攻击效果提供了有效工具。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [68] [Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data](https://arxiv.org/abs/2510.25123)
*Woojin Cho,Kookjin Lee,Noseong Park,Donsub Rim,Gerrit Welper*

Main category: cs.LG

TL;DR: 提出一种基于数据驱动的降维方法，专门用于处理双曲波传播的物理数据，使用低秩神经网络表示(LRNR)架构，能够从数据中学习高效的波传播低维表示。


<details>
  <summary>Details</summary>
Motivation: 针对物理基础的双曲波传播数据，需要找到高效的降维表示方法，理论证明这类波存在高效表示的可能性。

Method: 在超网络框架中使用专门的LRNR神经网络架构，结合深度学习技术直接从数据中学习波传播的低维表示。

Result: 训练后的LRNR自然产生低秩张量表示，揭示了波传播的新分解方式，每个分解模式对应可解释的物理特征，并支持通过压缩方案实现高效推理。

Conclusion: LRNR架构能够有效学习波传播的低维表示，提供物理可解释的分解，并在性能要求高的场景中实现高效推理。

Abstract: We present a data-driven dimensionality reduction method that is well-suited
for physics-based data representing hyperbolic wave propagation. The method
utilizes a specialized neural network architecture called low rank neural
representation (LRNR) inside a hypernetwork framework. The architecture is
motivated by theoretical results that rigorously prove the existence of
efficient representations for this wave class. We illustrate through archetypal
examples that such an efficient low-dimensional representation of propagating
waves can be learned directly from data through a combination of deep learning
techniques. We observe that a low rank tensor representation arises naturally
in the trained LRNRs, and that this reveals a new decomposition of wave
propagation where each decomposed mode corresponds to interpretable physical
features. Furthermore, we demonstrate that the LRNR architecture enables
efficient inference via a compression scheme, which is a potentially important
feature when deploying LRNRs in demanding performance regimes.

</details>


### [69] [Bridging the Divide: End-to-End Sequence-Graph Learning](https://arxiv.org/abs/2510.25126)
*Yuen Chen,Yulun Wu,Samuel Sharpe,Igor Melnyk,Nam H. Nguyen,Furong Huang,C. Bayan Bruss,Rizal Fathony*

Main category: cs.LG

TL;DR: BRIDGE是一个统一的端到端架构，将序列编码器与图神经网络（GNN）结合在单一目标下，通过token级跨注意力层实现邻居间细粒度的消息传递，在友谊预测和欺诈检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常是序列性和关系性的结合，现有方法往往忽视其中一种模态。作者认为序列和图不是分离的问题，而是同一数据集的互补方面，应该联合学习。

Method: 提出BRIDGE架构，将序列编码器与GNN耦合在单一目标下，允许梯度在两者间流动。引入TOKENXATTN层，在邻居序列的事件间进行token级消息传递。

Result: 在友谊预测（Brightkite）和欺诈检测（Amazon）两个场景中，BRIDGE在排序和分类指标上持续优于静态GNN、时序图方法和仅序列基线。

Conclusion: 序列和图应该联合建模，BRIDGE通过端到端耦合序列编码器和GNN，实现了更有效的表示学习，在多个任务中表现出色。

Abstract: Many real-world datasets are both sequential and relational: each node
carries an event sequence while edges encode interactions. Existing methods in
sequence modeling and graph modeling often neglect one modality or the other.
We argue that sequences and graphs are not separate problems but complementary
facets of the same dataset, and should be learned jointly. We introduce BRIDGE,
a unified end-to-end architecture that couples a sequence encoder with a GNN
under a single objective, allowing gradients to flow across both modules and
learning task-aligned representations. To enable fine-grained token-level
message passing among neighbors, we add TOKENXATTN, a token-level
cross-attention layer that passes messages between events in neighboring
sequences. Across two settings, friendship prediction (Brightkite) and fraud
detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph
methods, and sequence-only baselines on ranking and classification metrics.

</details>


### [70] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 本文提出了一种基于线性嫁接的Lipschitz常数优化方法，通过将非线性激活函数替换为线性函数来消除近似误差，从而获得更紧的局部Lipschitz常数，提高认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在认证鲁棒性中，较小的Lipschitz常数意味着模型对对抗样本具有更好的鲁棒性。然而，现有的过近似方法存在显著的近似误差，这阻碍了获得紧致的局部Lipschitz常数。线性嫁接可以消除近似误差，但之前缺乏理论分析。

Method: 提出Lipschitz感知的线性嫁接方法，将非线性激活函数替换为线性函数，从而消除主导的近似误差源。通过理论分析证明线性嫁接如何改善局部Lipschitz常数的紧致性。

Result: 实验表明，在线性嫁接影响较大的激活函数上进行嫁接，能够收紧l∞局部Lipschitz常数，并增强认证鲁棒性，即使没有进行认证训练。

Conclusion: 线性嫁接通过消除非线性激活函数带来的近似误差，能够有效收紧局部Lipschitz常数，从而提升模型的认证鲁棒性，为神经网络验证提供了新的理论视角和方法。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [71] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习引导的框架，用于快速解决最优断电问题，通过利用电力系统实例间的共享模式，在管理野火风险的同时减少负荷削减。


<details>
  <summary>Details</summary>
Motivation: 为了缓解野火点火风险，电力公司需要在高风险区域对电力线路进行断电。最优断电问题是计算复杂的混合整数线性规划问题，需要在操作环境中快速频繁求解。由于特定电力系统的OPS实例具有共享结构但参数不同，这促使使用机器学习方法来利用实例间的共享模式。

Method: 开发了一种机器学习引导的框架，扩展了现有的ML引导MILP求解方法，同时整合了关于通电和断电线路数量的领域知识。

Result: 在基于加利福尼亚的大规模现实合成测试系统上的结果显示，所提出的ML引导方法比传统优化方法更快地产生高质量解决方案。

Conclusion: 机器学习引导的方法能够有效解决最优断电问题，在保证解质量的同时显著提高求解速度，适用于需要快速决策的电力系统操作环境。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [72] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种用于深度时间序列预测的选择性学习策略，通过双重掩码机制筛选可泛化的时间步，避免模型对不确定和异常时间步的过拟合。


<details>
  <summary>Details</summary>
Motivation: 深度模型在时间序列预测中容易受到噪声和异常的影响而严重过拟合，传统方法对所有时间步进行统一优化会导致模型学习不确定和异常的时间步。

Method: 采用选择性学习策略，通过不确定性掩码（利用残差熵）和异常掩码（利用残差下界估计）双重机制筛选时间步子集来计算MSE损失。

Result: 在8个真实世界数据集上的实验表明，该方法显著提升了主流深度模型的预测性能，其中Informer的MSE降低了37.4%，TimesNet降低了8.4%，iTransformer降低了6.5%。

Conclusion: 选择性学习策略能有效缓解深度时间序列预测模型的过拟合问题，通过关注可泛化的时间步来提升预测性能。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [73] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: BOLT-GAN是基于贝叶斯最优学习阈值（BOLT）原理改进的WGAN框架，通过使用Lipschitz连续判别器，在四个标准图像生成基准测试中比WGAN获得10-60%更低的FID分数。


<details>
  <summary>Details</summary>
Motivation: 改进WGAN框架，利用BOLT原理来提升GAN训练的稳定性和性能。

Method: 在WGAN框架中引入BOLT原理，使用Lipschitz连续判别器，隐式最小化不同于Earth Mover距离的度量距离。

Result: 在CIFAR-10、CelebA-64、LSUN Bedroom-64和LSUN Church-64四个基准测试中，BOLT-GAN相比WGAN获得10-60%更低的FID分数，训练稳定性更好。

Conclusion: BOLT是一个广泛适用于增强GAN训练的原则，BOLT-GAN在图像生成任务中表现出色。

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [74] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出了一种基于自适应损失加权的成本敏感多类正未标记学习方法，通过为正向和推断负向损失分配不同权重，实现无偏风险估计，在多个数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中标注可靠负样本困难且成本高，而现有多类PU学习方法大多无法保证无偏风险估计，限制了性能和稳定性。

Method: 在经验风险最小化框架下，为正向和从未标记混合数据中推断的负向损失分量分配不同的数据依赖权重，使经验目标成为目标风险的无偏估计器。

Result: 在八个公共数据集上的广泛实验表明，该方法在不同类别先验和类别数量下，在准确性和稳定性方面均优于强基线方法。

Conclusion: 提出的自适应损失加权方法为多类PU学习提供了有效的无偏风险估计框架，在多个基准测试中表现出优越性能。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [75] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: BSFA框架通过区分处理损失Hessian矩阵的特征子空间，在主导子空间抑制更新、在主体子空间放大更新，从而加速深度学习训练并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现深度学习优化中存在根本性二分：沿损失Hessian顶部特征方向的参数更新幅度大但对损失减少贡献小，而正交方向更新幅度小但驱动大部分学习进展。

Method: 提出Bulk-Space-Filtration-Accelerator (BSFA)框架，通过差异缩放不同子空间的更新分量；使用PCA对历史更新进行高效子空间估计；采用分块策略在参数块级别应用估计。

Result: 在各种任务中实现加速，特别是在WikiText-103上预训练LLaMA-72M和在OpenWebText上预训练LLaMA-134M时，相比标准AdamW获得约2倍加速。

Conclusion: BSFA是一个实用且可扩展的即插即用框架，能有效加速深度学习训练，同时增强稳定性。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [76] [Scaling Up Bayesian DAG Sampling](https://arxiv.org/abs/2510.25254)
*Daniele Nikzad,Alexander Zhilkin,Juha Harviainen,Jack Kuipers,Giusi Moffa,Mikko Koivisto*

Main category: cs.LG

TL;DR: 提出了两种改进贝叶斯网络结构采样的技术：高效实现基本移动操作和通过预处理修剪可能父集来加速求和计算


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络结构推断通常通过马尔可夫链采样进行，但现有方法效率有待提升

Method: 1. 高效实现添加、删除或反转单条弧的基本移动操作；2. 设计预处理方法修剪可能父集以近似保持求和结果

Result: 实证研究表明，相比之前的方法，这些技术能带来显著的效率提升

Conclusion: 所提出的技术能有效提高贝叶斯网络结构采样的效率

Abstract: Bayesian inference of Bayesian network structures is often performed by
sampling directed acyclic graphs along an appropriately constructed Markov
chain. We present two techniques to improve sampling. First, we give an
efficient implementation of basic moves, which add, delete, or reverse a single
arc. Second, we expedite summing over parent sets, an expensive task required
for more sophisticated moves: we devise a preprocessing method to prune
possible parent sets so as to approximately preserve the sums. Our empirical
study shows that our techniques can yield substantial efficiency gains compared
to previous methods.

</details>


### [77] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: 提出了基于信息瓶颈原理的IB-Inspired Normalization (IBNorm)，通过有界压缩操作在保持预测信息的同时抑制无关变异性，优于传统的方差中心化归一化方法。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法如BatchNorm、LayerNorm和RMSNorm都是方差中心化的，强制零均值和单位方差来稳定训练，但没有控制表示如何捕获任务相关信息。

Method: 基于信息瓶颈原理，引入有界压缩操作，鼓励嵌入保持预测信息同时抑制无关变异性，保持标准归一化的稳定性和兼容性。

Result: 在大型语言模型(LLaMA、GPT-2)和视觉模型(ResNet、ViT)上一致优于BatchNorm、LayerNorm和RMSNorm，互信息分析证实了更优的信息瓶颈行为。

Conclusion: IBNorm在理论和实证上都优于传统方差中心化归一化方法，实现了更高的信息瓶颈值和更紧的泛化边界。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [78] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: 该论文通过敏感性分析的统一视角，研究神经网络对输入和参数扰动的响应，提出Lipschitz网络、曲率正则化和随机平滑等方法来解决模型不稳定性和脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型存在不稳定性和脆弱性问题：输入微小变化可能严重影响预测结果，优化过程受尖锐损失函数影响。需要系统性解决模型对输入和参数扰动的敏感性。

Method: 1) 使用Lipschitz网络约束输入扰动敏感性；2) 基于损失函数曲率的正则化技术；3) 随机平滑增强决策边界鲁棒性；4) 结合三种方法的统一框架。

Result: 开发了高效谱范数计算、新型Lipschitz约束层和改进的认证程序，提高了模型的泛化能力、对抗鲁棒性和训练稳定性。

Conclusion: 通过Lipschitz连续性、随机平滑和曲率正则化的协同作用，为解决深度学习稳定性挑战提供了理论分析和实用方法。

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [79] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: 提出了一种分层物理嵌入学习框架，用于从稀疏噪声数据中进行时空预测和物理定律发现。该框架采用两级架构，第一级学习PDE的基本符号组件，第二级学习其控制组合，从而降低学习复杂度并实现先验知识的结构化整合。


<details>
  <summary>Details</summary>
Motivation: 建模远离平衡系统的复杂时空动力学是科学中的重大挑战。传统方法存在局限：纯数据驱动模型物理不一致且数据密集，现有物理信息方法缺乏表示复杂算子的结构能力。需要一种能系统整合部分物理知识的方法。

Method: 基于自适应傅里叶神经算子的分层架构，第一级学习基本符号组件，第二级学习控制组合。将已知物理定律直接嵌入计算图，保证物理一致性。通过结构化解耦已知和未知项，实现可解释的方程发现。

Result: 该框架能够有效捕捉动力系统的非局部依赖和高阶算子特征，提高数据效率，并实现物理一致的预测。通过符号回归可解释地发现控制方程，无需预设函数形式。

Conclusion: 分层物理嵌入学习框架在时空预测和物理定律发现方面取得根本性进展，通过结构化整合先验知识和分层分解，解决了复杂系统建模中的关键挑战。

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [80] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: 提出了一种多目标强化学习算法，在最大化期望回报的同时，确保策略在目标状态上具有分散的边缘状态分布。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法主要关注最大化期望回报，可能导致策略只利用少数奖励源。但在许多自然场景中，需要在达到目标状态的同时均匀访问所有目标状态，这一方面尚未得到充分探索。

Method: 提出基于离线强化学习的新算法，通过优化自定义奖励函数来学习高回报的策略混合，使边缘状态分布在目标状态上分散。算法在每次迭代中基于当前策略混合计算奖励，并使用采样轨迹更新策略。

Result: 算法在合成MDP和标准RL环境中进行了实验验证，证明了其有效性。

Conclusion: 该工作形式化了多目标强化学习问题，提出了具有性能保证的算法，能够同时优化期望回报和目标状态上的分布分散度。

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [81] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: 提出了一种基于循环矩阵和对角矩阵乘积的新型可逆线性层，显著降低了参数复杂度和计算复杂度，并在此基础上构建了CDFlow模型，在自然图像数据集上取得了良好的密度估计效果。


<details>
  <summary>Details</summary>
Motivation: 设计能够增强表达能力同时保持雅可比行列式和逆矩阵高效计算的可逆线性层是标准化流模型的关键挑战。

Method: 使用循环矩阵和对角矩阵的乘积分解来构建可逆线性层，利用快速傅里叶变换将矩阵求逆的时间复杂度从O(n³)降低到O(mn log n)，对数行列式计算从O(n³)降低到O(mn)。

Result: CDFlow在自然图像数据集上实现了强大的密度估计，并能有效建模具有周期性结构的数据，显著加速了标准化流中的关键操作。

Conclusion: 提出的循环-对角分解方法在保持表达能力的同时大幅降低了计算复杂度，为可扩展生成建模提供了实用优势。

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [82] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: 该论文提出了解决信息级联流行度预测中三个关键问题的方法：时间泄露、特征贫乏数据集和计算效率低下。通过时间有序分割策略、大规模电商数据集Taoke和轻量级框架CasTemp，实现了无泄露评估下的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前信息级联流行度预测存在三个关键局限：时间泄露导致评估不真实、特征贫乏数据集缺乏下游转换信号、图方法计算效率低下。这些问题限制了实际应用价值。

Method: 1) 时间有序分割策略按时间顺序划分数据；2) 构建Taoke电商数据集，包含丰富的推广者/产品属性和真实购买转换；3) 开发CasTemp轻量级框架，使用时间游走、Jaccard邻居选择和GRU编码与时序注意力。

Result: 在无泄露评估下，CasTemp在四个数据集上达到最先进性能，计算速度提升数个数量级，特别擅长预测第二阶段流行度转换这一实际关键任务。

Conclusion: 通过系统解决任务设置、数据集构建和模型设计三个方面的挑战，该研究为信息级联预测提供了更真实、实用和高效的解决方案，特别在电商场景下具有重要应用价值。

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [83] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文分析了随机几何超图上的变分学习，提出了高阶超图学习方法，通过骨架图的拉普拉斯幂实现多尺度平滑正则化。


<details>
  <summary>Details</summary>
Motivation: 超图为建模高阶交互提供了自然框架，但在半监督学习中的理论基础仍然有限，需要分析超图学习的渐近一致性。

Method: 提出高阶超图学习方法，通过骨架图的拉普拉斯幂进行正则化，实现多尺度平滑。

Result: 理论分析表明该方法收敛到高阶Sobolev半范数，在标准基准测试中表现优异。

Conclusion: 高阶超图学习为超图上的半监督学习提供了理论保证和有效的实用方法。

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [84] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 提出在知识图谱嵌入模型中使用模型合并（特别是加权平均）方法替代传统集成学习，通过维护训练过程中的参数运行平均值来提升链接预测性能，同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习方法需要训练多个模型，导致计算开销大、延迟高和内存占用多。模型合并方法提供了不需要训练多个模型的替代方案。

Method: 采用加权平均方法，从训练周期开始维护模型参数的运行平均值用于预测。还提出选择性更新策略，仅在验证集上泛化性能提升时才更新集成模型参数。

Result: 在链接预测任务中，加权平均方法相比最先进的基准集成方法表现更优。在字面增强KGE模型和多跳查询回答任务中也获得了一致的性能提升。

Conclusion: 提出的加权平均方法在各种评估设置中都能持续提升性能，为知识图谱嵌入模型提供了一种高效且有效的集成学习替代方案。

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [85] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: 提出一个基于损失函数从初始非凸性向最优解附近凸性转换假设的两阶段优化算法，通过检测梯度范数与损失的关系来切换使用Adam和共轭梯度方法，显著提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 现实任务中的损失函数经常具有非凸区域，导致广泛采用Adam等非凸优化方法。但局部最小值周围环境通常是凸的，可以利用二阶方法获得超线性收敛保证。

Method: 设计两阶段优化算法：通过观察梯度范数与损失的关系检测凸性转换点，在非凸区域使用Adam，在凸区域使用共轭梯度方法。

Result: 计算实验证实这种简单的凸性结构在现实任务中足够常见，可以实际利用来显著改善收敛性和精度。

Conclusion: 损失函数的凸性转换特性可以有效地被利用来设计混合优化算法，结合非凸和凸优化方法的优势，获得更好的收敛性能。

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [86] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: 该论文提出生物学启发机器学习（BIML），作为物理启发机器学习（PIML）的扩展，旨在应对生物建模中的独特挑战，如不确定先验知识、异构数据和复杂网络。


<details>
  <summary>Details</summary>
Motivation: 将PIML成功应用于生物领域，但生物建模面临多面性先验知识、异构噪声数据、部分可观测性和高维网络等挑战，需要新的方法。

Method: 提出BIML框架，基于四个基础支柱：不确定性量化、情境化、约束潜在结构推断和可扩展性，利用基础模型和大型语言模型作为关键推动者。

Result: BIML保留了PIML的结构基础，同时适应生物学的实际需求，通过概率形式的先验知识进行建模。

Conclusion: BIML为PIML在生物学应用提供了路线图，建议构建BIML生态系统，将PIML创新导向具有高科学和社会相关性的挑战。

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [87] [A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory](https://arxiv.org/abs/2510.25379)
*Adrien Weihs,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer*

Main category: cs.LG

TL;DR: 本文研究了学习函数空间之间映射（即算子）的问题，提出了多算子学习和独立学习多个算子的两种方法，并开发了新的网络架构和理论框架。


<details>
  <summary>Details</summary>
Motivation: 科学应用需要近似函数空间之间的映射（算子），而传统机器学习主要关注有限维空间之间的映射。本文旨在为多算子学习建立统一的理论和实践基础。

Method: 提出了两种新架构MNO和MONet用于多算子学习，建立了在连续、可积和Lipschitz算子情况下的通用逼近理论，并为学习多个独立算子开发了平衡子网络复杂度的框架。

Result: 理论分析显示网络规模与逼近精度之间存在明确的缩放规律，实验验证了所提架构在参数化PDE基准测试中的强大表达能力和效率。

Conclusion: 这项工作为跨多个算子的可扩展神经算子学习建立了统一的理论和实践基础，为科学计算中的算子学习问题提供了系统解决方案。

Abstract: While many problems in machine learning focus on learning mappings between
finite-dimensional spaces, scientific applications require approximating
mappings between function spaces, i.e., operators. We study the problem of
learning collections of operators and provide both theoretical and empirical
advances. We distinguish between two regimes: (i) multiple operator learning,
where a single network represents a continuum of operators parameterized by a
parametric function, and (ii) learning several distinct single operators, where
each operator is learned independently. For the multiple operator case, we
introduce two new architectures, $\mathrm{MNO}$ and $\mathrm{MONet}$, and
establish universal approximation results in three settings: continuous,
integrable, or Lipschitz operators. For the latter, we further derive explicit
scaling laws that quantify how the network size must grow to achieve a target
approximation accuracy. For learning several single operators, we develop a
framework for balancing architectural complexity across subnetworks and show
how approximation order determines computational efficiency. Empirical
experiments on parametric PDE benchmarks confirm the strong expressive power
and efficiency of the proposed architectures. Overall, this work establishes a
unified theoretical and practical foundation for scalable neural operator
learning across multiple operators.

</details>


### [88] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: GPTOpt是一种基于大语言模型的优化方法，通过在多样化贝叶斯优化参数化生成的合成数据集上微调LLM，使其具备连续黑盒优化能力，无需参数调优即可在各种基准测试中超越传统优化器。


<details>
  <summary>Details</summary>
Motivation: 解决昂贵、无导数黑盒函数全局优化需要极高样本效率的问题。传统贝叶斯优化方法虽然有效但需要针对每个应用领域进行精细参数调优，而现有大语言模型在连续黑盒优化任务上能力有限。

Method: 通过从多样化贝叶斯优化参数化生成的大规模合成数据集微调大语言模型，利用LLM的预训练能力实现跨优化任务的泛化。

Result: 在各种黑盒优化基准测试中，GPTOpt超越了传统优化器，展示了LLM在高级数值推理方面的能力。

Conclusion: GPTOpt为无需参数调优的全局优化提供了一个灵活框架，证明了LLM在连续黑盒优化任务中的潜力。

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [89] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: 提出梯度权重对齐（GWA）作为深度学习中的鲁棒验证指标，通过量化每个样本梯度与模型权重之间的一致性来跟踪泛化性能，无需验证集即可实现早期停止、模型比较和识别有影响力的训练样本。


<details>
  <summary>Details</summary>
Motivation: 在深度学习领域，需要鲁棒的验证指标来检测过拟合、监控训练动态和评估泛化性能。现有的方法通常依赖验证集，本研究旨在开发一种直接从训练数据中获取的验证指标。

Method: 引入梯度权重对齐（GWA）方法，该方法量化每个样本梯度与模型权重之间的一致性。有效学习对应一致的对齐，而失配则表明泛化性能下降。该方法可在训练过程中高效计算。

Result: 大量实验表明，GWA能够准确预测最优早期停止点，实现有原则的模型比较，并识别有影响力的训练样本，为模型分析提供了无需验证集的方法。

Conclusion: GWA作为一种直接从训练数据中获取的验证指标，能够有效跟踪泛化性能，为深度学习模型的分析和优化提供了新的工具。

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [90] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: 该论文提出原型神经符号架构来解决神经符号AI中的推理捷径问题，通过原型学习理论确保模型基于正确概念而非伪相关进行推理，在极低数据量下仍能有效学习正确概念。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号AI模型容易学习到伪相关的神经谓词，形成推理捷径，导致模型基于错误概念满足符号约束。

Method: 引入原型神经符号架构，基于原型学习理论，训练模型在满足背景知识的同时考虑输入与少量标注数据的相似性。

Result: 在rsbench基准测试中，无论是合成任务（MNIST-EvenOdd和Kand-Logic）还是真实高风险任务（BDD-OIA），都显著改善了正确概念的学习效果。

Conclusion: 原型接地是一种有效且标注效率高的策略，为安全可靠的神经符号学习开辟了新途径。

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [91] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: 该研究使用机器学习方法预测员工倦怠风险，在三种监督学习算法中，支持向量机(SVM)表现最佳(R²=0.84)，并开发了交互界面供非技术人员使用。


<details>
  <summary>Details</summary>
Motivation: 倦怠对个人福祉和组织绩效有显著影响，需要开发有效的早期检测方法来支持基于数据的心理健康策略。

Method: 使用HackerEarth员工倦怠挑战数据集，评估了最近邻(KNN)、随机森林和支持向量机(SVM)三种监督学习算法，采用30折交叉验证和决定系数(R²)评估模型性能。

Result: SVM模型取得了最高的预测性能(R²=0.84)，基于配对t检验统计上优于KNN和随机森林。开发了基于Streamlit的交互界面。

Conclusion: 机器学习在组织环境中支持倦怠早期检测和促进数据驱动的心理健康策略方面具有潜力。

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [92] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出了一种具有模型内在机制概念解释的新模型，强调概念解释的忠实性，概念在类别间共享且可忠实追溯其对logit的贡献和输入可视化。


<details>
  <summary>Details</summary>
Motivation: 现有基于概念的后验解释方法不够忠实于模型，且对模型学习的概念做出了限制性假设（如类别特异性、小空间范围或与人类期望对齐）。

Method: 开发了具有模型内在机制概念解释的新模型，概念在类别间共享，可从任何层忠实追溯其对logit的贡献和输入可视化，并利用基础模型提出了新的概念一致性度量C²-Score。

Result: 与先前工作相比，提出的概念在定量上更一致，用户认为概念更具可解释性，同时在ImageNet上保持了有竞争力的性能。

Conclusion: 该方法提供了一种更忠实、更一致的概念解释框架，在保持模型性能的同时提高了概念解释的质量和可解释性。

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [93] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 本文提出了一种基于核引导互信息(KG-MI)的新目标函数，结合多头注意力机制，能够在多项式时间内收敛到全局最优解，并准确恢复有向无环图(DAG)的底层结构。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力机制的Transformer模型在图结构学习方面主要局限于树状图，每个节点只有一个父节点。扩展到更一般的DAG（每个节点有多个父节点）面临挑战，因为难以设计训练目标让不同注意力头分别学习多个不同的父子关系。

Method: 引入基于f-散度的核引导互信息(KG-MI)度量，结合多头注意力框架，每个头关联不同的边际转移核来有效建模多样化的父子依赖关系。

Result: 理论证明：对于K-父DAG生成的序列，通过梯度上升训练单层多头Transformer能在多项式时间内收敛到全局最优；当f-散度特化为KL散度时，学习到的注意力分数能准确反映真实邻接矩阵，从而可证明地恢复底层图结构。实验验证了理论结果。

Conclusion: 该方法成功解决了多头注意力机制学习多父DAG结构的理论挑战，为Transformer在图结构学习中的应用提供了理论保证。

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [94] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典循环神经网络架构，其中整个循环核心由参数化量子电路实现，由经典前馈网络控制。该模型在多个序列学习任务中表现出与强经典基线竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 将量子计算的优势与循环神经网络结合，利用量子态的高维希尔伯特空间作为高容量记忆，同时保持物理一致性，探索量子机器学习在序列学习任务中的潜力。

Method: 使用参数化量子电路作为循环核心，隐藏状态是n量子比特的量子态。通过中间电路测量获取部分观测值，结合输入嵌入由经典前馈网络处理，提供显式经典非线性。输出参数化PQC，通过幺正动力学更新隐藏状态。

Result: 在情感分析、MNIST、置换MNIST、复制记忆和语言建模等任务上进行了评估，使用最多14个量子比特。还设计了基于中间电路读数的软注意力机制，在机器翻译中表现出有效性。

Conclusion: 这是第一个基于量子操作的模型，在广泛的序列学习任务类别中实现了与强经典基线竞争的性能，为量子机器学习开辟了新方向。

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [95] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: 将Aurora深度学习模型从大气预测迁移到海洋预测，在加那利上升流系统预测海表温度，实现了低误差和高相关性，但沿海区域细节捕捉仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数值海洋预测模型计算成本高、可扩展性有限，需要更高效的预测方法。

Method: 采用预训练的Aurora深度学习模型，通过分阶段微调、纬度加权误差指标和超参数优化，使用高分辨率海洋再分析数据进行训练。

Result: 模型达到0.119K的低RMSE，异常相关系数约0.997，能再现大尺度海表温度结构，但在沿海区域细节捕捉方面存在困难。

Conclusion: 证明了深度学习模型在不同领域间迁移学习的可行性，为数据驱动的海洋预测开辟了新途径，未来可通过整合更多变量、提高分辨率和使用物理信息神经网络来改进。

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [96] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: 提出了一个统一框架，能够从随机PAC-Bayesian保证中提取适用于单个假设的保证，解决了传统PAC-Bayes只能提供随机采样假设期望风险保证的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统PAC-Bayes框架只能为随机采样假设提供期望风险保证，需要随机预测，这在实际部署单个确定性假设时不可用。

Method: 提出了一个统一框架，包括通用oracle边界、数值边界和针对多数投票的专门化方法。

Result: 经验表明，该方法在确定性分类器的泛化边界方面始终优于流行基线（最多可达2倍）。

Conclusion: 该框架成功地将随机PAC-Bayesian保证转化为适用于单个确定性假设的保证，解决了实际部署中的关键问题。

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [97] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 该论文系统研究了对称矩阵低秩伪逆在噪声环境下的谱范数鲁棒性，提出了改进的扰动边界，比经典方法提升高达√n倍。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的矩阵常受到采样、素描和量化等噪声影响，但低秩逆近似的谱范数鲁棒性研究不足，需要系统分析噪声对低秩逆近似的影响。

Method: 在温和的噪声假设下，使用轮廓积分技术分析非全纯函数f(z)=1/z，推导出考虑特征间隙、谱衰减和噪声对齐的尖锐非渐近扰动边界。

Result: 提出的边界比经典全逆边界改进高达√n倍，经验验证显示新边界能准确跟踪真实扰动误差，而基于经典结果的估计往往过度预测。

Conclusion: 研究结果为噪声计算环境中低秩逆近似提供了实用的、频谱感知的保证，改进了现有理论边界。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [98] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 提出了一种基于SVD分解的结构化局部学习框架，在低秩流形上训练深度神经网络，减少可训练参数数量，同时保持与反向传播相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 反向传播需要全局误差传播和全参数化，内存和计算开销大；直接反馈对齐虽然支持局部并行更新，但反馈结构无组织且在深层网络中扩展性差。

Method: 在SVD分解的权重矩阵低秩流形上训练，对SVD分量应用复合损失函数（交叉熵、子空间对齐和正交正则化），构建匹配SVD结构的反馈矩阵。

Result: 在CIFAR-10、CIFAR-100和ImageNet上达到与反向传播相当的准确率，消融研究证实了低秩设置中各损失项的重要性。

Conclusion: 低秩流形上的局部学习是完整秩梯度训练的一个原理性、可扩展的替代方案。

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [99] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 提出基于核评分规则的不确定性度量框架，统一了多种现有方法，为回归任务中的不确定性量化提供原则性设计指南


<details>
  <summary>Details</summary>
Motivation: 回归任务特别是安全关键领域需要合适的不确定性量化，但现有文献主要关注分类问题

Method: 基于适当评分规则（特别是核评分）构建总不确定性、偶然不确定性和认知不确定性的度量族

Result: 实验证明这些度量在下游任务中有效，并揭示了不同实例化之间的权衡（如鲁棒性和分布外检测性能）

Conclusion: 该框架为任务特定的不确定性度量提供了理论指导和实用设计原则

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [100] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: 本文系统比较了浮点(FP)和整数(INT)量化格式在不同粒度下的性能，发现FP在粗粒度量化中表现优异，但在细粒度(如块大小为32)8位量化中，MXINT8在算法精度和硬件效率上均优于FP格式。对于4位格式，FP通常具有精度优势，但通过异常值缓解技术，NVINT4可以超越NVFP4。


<details>
  <summary>Details</summary>
Motivation: 现代AI硬件(如Nvidia Blackwell架构)越来越多地采用低精度浮点格式来处理LLM中的激活异常值，但缺乏FP和INT量化在不同粒度下的统一比较，导致算法和硬件协同设计缺乏明确指导。

Method: 系统研究FP和INT格式之间的权衡，分析不同粒度的量化性能，引入对称裁剪方法解决细粒度低比特INT训练中的梯度偏差问题，并应用Hadamard旋转等异常值缓解技术。

Result: 发现关键性能交叉点：FP在粗粒度量化中表现优异，但在细粒度8位量化中MXINT8在精度和硬件效率上均优于FP；对于4位格式，FP通常有精度优势，但NVINT4结合异常值缓解技术可以超越NVFP4；对称裁剪方法实现了MXINT8训练几乎无损的性能。

Conclusion: 挑战当前硬件发展轨迹，证明一刀切的FP方法并非最优，主张细粒度INT格式(特别是MXINT8)为未来AI加速器提供了更好的精度、功耗和效率平衡。

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [101] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文系统研究了VLA模型在动作微调过程中的表征保留问题，发现简单的动作微调会导致视觉表征退化，并提出了一种简单有效的方法来缓解这种退化并改善OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型的发展，预训练的VLM被认为能为智能体提供可迁移的世界知识和视觉语言基础。但在适应动作模态时，这些VLM原有的视觉语言表征和知识保留程度尚不明确。

Method: 通过探测VLA的隐藏表征和注意力图，设计对比任务来隔离动作微调引起的VL能力变化，评估多种视觉表征对齐策略，并提出一种简单有效的缓解方法。

Result: 研究表明动作微调会导致视觉表征退化，提出的方法能有效缓解这种退化并提高OOD场景的泛化能力。

Conclusion: 本文阐明了动作微调与VL表征退化之间的权衡关系，并强调了恢复继承VL能力的实用方法。

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [102] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出FedLap框架，用于处理联邦学习中图结构数据的隐私保护问题，通过拉普拉斯平滑在谱域捕获节点间依赖关系，同时保证隐私和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦学习图数据处理方法存在隐私风险或计算复杂度高的问题，特别是在子图互连场景下，客户端间的连接关系对学习过程有重要影响。

Method: 使用拉普拉斯平滑在谱域捕获全局结构信息，避免交换敏感节点嵌入，同时提供形式化的隐私分析。

Result: 在基准数据集上的实验表明，FedLap相比现有技术实现了竞争性或更优的效用。

Conclusion: FedLap是首个具有强隐私保证的子图联邦学习方案，有效解决了隐私保护和可扩展性问题。

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [103] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 本文建立了对称矩阵谱范数扰动的新边界，改进了经典的Eckart-Young-Mirsky定理，并应用于差分隐私PCA，解决了文献中的开放问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习中需要理解噪声或测量误差如何影响低秩近似，特别是在谱范数下。差分隐私低秩近似中，需要在保护隐私的同时保留数据的top-p结构。现有工作主要分析Frobenius范数误差，但可能高估或低估真实的子空间失真。

Method: 采用复分析中的新颖轮廓自举方法，并将其扩展到包括多项式和矩阵指数在内的广泛谱泛函类。在温和的特征值间隙和范数条件下，建立了对称矩阵的高概率谱范数扰动边界。

Result: 新边界对∥(A+E)_p - A_p∥给出了尖锐估计，改进因子可达√n。经验结果证实边界在不同扰动机制下能紧密跟踪实际谱误差。

Conclusion: 本文的谱范数扰动边界为差分隐私PCA提供了改进的效用保证，解决了文献中的开放问题，并为更广泛的谱泛函分析提供了新方法。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [104] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: 该研究表明循环神经网络可以通过噪声维持的轨道动力学来模拟隐马尔可夫模型的离散状态转换，揭示了RNN实现概率计算的新机制。


<details>
  <summary>Details</summary>
Motivation: 过去研究主要关注简单、确定性行为，但自然行为往往更丰富、自发且具有随机性。需要理解RNN如何生成这种复杂行为，特别是与HMM揭示的离散状态转换机制的关系。

Method: 首先训练RNN复制HMM的发射统计特性，然后对训练好的网络进行逆向工程分析其实现机制。研究多种HMM架构（全连接、循环、线性链）下的通用性。

Result: 训练后的RNN在没有输入时活动会坍缩到单个固定点；在随机输入驱动下，轨迹沿闭合轨道呈现噪声维持的动力学。网络发展出高度结构化的连接，少量"踢神经元"负责在慢速噪声驱动区域间进行快速确定性转换。

Conclusion: RNN通过进入随机共振机制来实现概率计算，这种解决方案在不同HMM架构中通过模块化重用相同的动力学模式而通用化，表明RNN可以通过组合原则模拟复杂的离散潜在动力学。

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [105] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: GNSS是一个基于图神经网络的动态结构模拟器，通过局部坐标系、符号感知损失函数和波长感知连接半径等创新设计，在动态结构问题中实现了准确且高效的模拟。


<details>
  <summary>Details</summary>
Motivation: 目前图神经网络在计算流体力学中已有应用，但在结构问题特别是动态案例中研究较少，需要开发专门针对动态结构模拟的GNN框架。

Method: 采用编码-处理-解码范式，关键创新包括：节点固定局部坐标系避免速度计算误差、符号感知回归损失减少相位误差、波长感知连接半径优化图构建。

Result: 在50kHz脉冲激励的梁结构案例中，GNSS能够准确再现物理过程数百个时间步，并能泛化到未见载荷条件，相比有限元方法实现了显著推理加速。

Conclusion: 具有物理一致性更新规则的局部保持GNN是动态波主导结构模拟的有竞争力替代方案，在保持时空保真度的同时实现高效计算。

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [106] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了卷积脉冲GRU（CS-GRU）单元，通过卷积操作保留局部结构，结合脉冲神经元的时间精度和GRU的门控机制，在时序和时空数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在处理长序列时会丢失局部细节，现有方法如SpikGRU无法捕捉事件型时空数据中的细粒度局部依赖关系。

Method: 设计CS-GRU单元，集成卷积操作来保持局部结构依赖，同时结合脉冲神经元的时间精度和GRU的高效门控机制。

Result: 在时序数据集（NTIDIGITS、SHD）和时空基准测试（MNIST、DVSGesture、CIFAR10DVS）上表现优异，平均比最先进GRU变体提升4.35%，在MNIST上达到99.31%准确率，比SpikGRU效率高69%。

Conclusion: CS-GRU是一个多功能架构，在时序和时空任务上都表现出色，显著优于现有方法。

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [107] [LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries](https://arxiv.org/abs/2510.25731)
*René P. Klausen,Ivan Timofeev,Johannes Frank,Jonas Naujoks,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了一种利用李群对称性精确求解初边值问题的方法，通过对称变换构建满足PDE的模型，相比PINNs更快更准确。


<details>
  <summary>Details</summary>
Motivation: 传统方法如PINNs在求解PDE约束问题时存在收敛慢和精度不足的问题，需要一种能精确满足PDE且计算高效的方法。

Method: 利用李群对称性构建模型，通过对称变换使模型自动满足PDE，仅需初始和边界数据训练，损失函数直接反映模型精度。

Result: LieSolver在线性齐次PDE上表现优于PINNs，计算速度更快、精度更高，并能进行严格的误差估计。

Conclusion: 该方法显著提高了PDE约束问题的计算效率和预测可靠性，为物理约束学习提供了更优的解决方案。

Abstract: We introduce a method for efficiently solving initial-boundary value problems
(IBVPs) that uses Lie symmetries to enforce the associated partial differential
equation (PDE) exactly by construction. By leveraging symmetry transformations,
the model inherently incorporates the physical laws and learns solutions from
initial and boundary data. As a result, the loss directly measures the model's
accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our
method enables rigorous error estimation. The approach yields compact models,
facilitating an efficient optimization. We implement LieSolver and demonstrate
its application to linear homogeneous PDEs with a range of initial conditions,
showing that it is faster and more accurate than physics-informed neural
networks (PINNs). Overall, our method improves both computational efficiency
and the reliability of predictions for PDE-constrained problems.

</details>


### [108] [MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction](https://arxiv.org/abs/2510.25755)
*David S Maxwell,Michael Darkoh,Sidharth R Samudrala,Caroline Chung,Stephanie T Schmidt,Bissan Al-Lazikani*

Main category: cs.LG

TL;DR: MLPrE是一个基于SparkDataFrames的机器学习预处理和探索性数据分析工具，通过JSON配置文件实现可扩展的数据处理流程，包含69个处理阶段，能够处理多种数据格式并支持图数据库准备。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习需求的增长，现有数据处理工具在可扩展性和集成性方面存在限制，无法满足大规模数据处理管道（如Apache Airflow）的需求，需要开发一个轻量级、可扩展的预处理工具。

Method: 利用SparkDataFrames存储数据确保可扩展性，采用通用JSON输入文件格式描述对DataFrame的逐步更改，实现了输入输出、过滤、基础统计、特征工程和探索性数据分析等处理阶段。

Result: 开发了包含69个处理阶段的MLPrE工具，在六个不同数据集上验证了关键阶段的功能，展示了处理平面文件中多个字段并重新组合的能力，以及数据聚类和图数据库准备功能。

Conclusion: MLPrE提供了一个通用且可扩展的预处理和早期数据分析工具，填补了机器学习应用中对这类工具的关键需求，能够加速和简化大型工作流中的早期开发阶段。

Abstract: With the recent growth of Deep Learning for AI, there is a need for tools to
meet the demand of data flowing into those models. In some cases, source data
may exist in multiple formats, and therefore the source data must be
investigated and properly engineered for a Machine Learning model or graph
database. Overhead and lack of scalability with existing workflows limit
integration within a larger processing pipeline such as Apache Airflow, driving
the need for a robust, extensible, and lightweight tool to preprocess arbitrary
datasets that scales with data type and size. To address this, we present
Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which
SparkDataFrames were utilized to hold data during processing and ensure
scalability. A generalizable JSON input file format was utilized to describe
stepwise changes to that DataFrame. Stages were implemented for input and
output, filtering, basic statistics, feature engineering, and exploratory data
analysis. A total of 69 stages were implemented into MLPrE, of which we
highlight and demonstrate key stages using six diverse datasets. We further
highlight MLPrE's ability to independently process multiple fields in flat
files and recombine them, otherwise requiring an additional pipeline, using a
UniProt glossary term dataset. Building on this advantage, we demonstrated the
clustering stage with available wine quality data. Lastly, we demonstrate the
preparation of data for a graph database in the final stages of MLPrE using
phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and
scalable tool for preprocessing and early data analysis, filling a critical
need for such a tool given the ever expanding use of machine learning. This
tool serves to accelerate and simplify early stage development in larger
workflows.

</details>


### [109] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: 该论文设计了一个合成分类任务，证明传统多示例学习方法忽略实例间上下文关系的局限性，并展示了相关MIL方法在泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统多示例学习方法在处理医学图像时，将实例（如切片或补丁）单独处理，忽略了相邻实例之间的上下文关系，而这些关系在实际应用中可能至关重要。

Method: 设计了一个合成分类任务，其中考虑相邻实例特征对准确预测至关重要，并将现成MIL方法与最优贝叶斯估计器进行性能量化比较。

Result: 实证研究表明，即使是最新的相关MIL方法，在从数万个实例从头开始训练时，仍难以达到最优的泛化能力。

Conclusion: 当前的多示例学习方法在处理实例间上下文关系方面存在局限性，需要开发能够更好捕捉这些关系的新方法。

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [110] [Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees](https://arxiv.org/abs/2510.24754)
*Yuqicheng Zhu,Jingcheng Wu,Yizhen Wang,Hongkuan Zhou,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: stat.ML

TL;DR: 提出了UnKGCP框架，为不确定性知识图谱嵌入方法生成具有理论保证的预测区间，量化预测不确定性


<details>
  <summary>Details</summary>
Motivation: 现有不确定性知识图谱嵌入方法仅提供点估计，无法量化预测不确定性，限制了在高风险应用中的可靠性

Method: 基于共形预测框架，引入针对UnKGE方法的非共形性度量，以及高效的区间构建过程

Result: 实验验证了区间的理论保证，并在多个标准基准测试中证明区间是尖锐的且能有效捕捉预测不确定性

Conclusion: UnKGCP框架能够为UnKGE方法提供具有理论保证的预测区间，显著提升预测可靠性和不确定性量化能力

Abstract: Uncertain knowledge graph embedding (UnKGE) methods learn vector
representations that capture both structural and uncertainty information to
predict scores of unseen triples. However, existing methods produce only point
estimates, without quantifying predictive uncertainty-limiting their
reliability in high-stakes applications where understanding confidence in
predictions is crucial. To address this limitation, we propose \textsc{UnKGCP},
a framework that generates prediction intervals guaranteed to contain the true
score with a user-specified level of confidence. The length of the intervals
reflects the model's predictive uncertainty. \textsc{UnKGCP} builds on the
conformal prediction framework but introduces a novel nonconformity measure
tailored to UnKGE methods and an efficient procedure for interval construction.
We provide theoretical guarantees for the intervals and empirically verify
these guarantees. Extensive experiments on standard benchmarks across diverse
UnKGE methods further demonstrate that the intervals are sharp and effectively
capture predictive uncertainty.

</details>


### [111] [Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm](https://arxiv.org/abs/2510.24815)
*Clément Bénard*

Main category: stat.ML

TL;DR: 提出了TreeHFD算法，用于从数据样本中估计树集合的Hoeffding分解，解决了输入变量依赖时的可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 树集合在表格数据预测中表现出色，但其黑盒特性限制了在关键决策应用中的使用。Hoeffding分解虽然强大，但在输入变量依赖时的实际估计仍是一个开放问题。

Method: 开发了TreeHFD算法，通过分层正交约束来估计树集合的Hoeffding分解，确保分解的唯一性和稀疏性。

Result: 实验证明TreeHFD具有收敛性、正交性、稀疏性和因果变量选择等特性，在模拟和真实数据上表现优异。

Conclusion: TreeHFD有效解决了树集合的可解释性问题，并且揭示了TreeSHAP方法与Hoeffding分解之间的强关联。

Abstract: Tree ensembles have demonstrated state-of-the-art predictive performance
across a wide range of problems involving tabular data. Nevertheless, the
black-box nature of tree ensembles is a strong limitation, especially for
applications with critical decisions at stake. The Hoeffding or ANOVA
functional decomposition is a powerful explainability method, as it breaks down
black-box models into a unique sum of lower-dimensional functions, provided
that input variables are independent. In standard learning settings, input
variables are often dependent, and the Hoeffding decomposition is generalized
through hierarchical orthogonality constraints. Such generalization leads to
unique and sparse decompositions with well-defined main effects and
interactions. However, the practical estimation of this decomposition from a
data sample is still an open problem. Therefore, we introduce the TreeHFD
algorithm to estimate the Hoeffding decomposition of a tree ensemble from a
data sample. We show the convergence of TreeHFD, along with the main properties
of orthogonality, sparsity, and causal variable selection. The high performance
of TreeHFD is demonstrated through experiments on both simulated and real data,
using our treehfd Python package (https://github.com/ThalesGroup/treehfd).
Besides, we empirically show that the widely used TreeSHAP method, based on
Shapley values, is strongly connected to the Hoeffding decomposition.

</details>


### [112] [Generative Bayesian Optimization: Generative Models as Acquisition Functions](https://arxiv.org/abs/2510.25240)
*Rafael Oliveira,Daniel M. Steinberg,Edwin V. Bonilla*

Main category: stat.ML

TL;DR: 提出一种将生成模型转化为批量贝叶斯优化候选解采样器的通用策略，利用直接偏好优化思想，通过观测值直接计算效用值来训练生成模型，形成与获取函数值成比例的提议分布。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法在处理大规模批量、非连续设计空间、高维和组合设计时面临挑战，需要一种能够有效扩展且避免构建代理模型的新方法。

Method: 受直接偏好优化启发，使用观测值直接计算噪声效用值来训练生成模型，形成与期望效用（获取函数值）成比例的提议分布，避免构建回归或分类代理模型。

Result: 理论分析表明该方法在特定条件下能渐进收敛到全局最优解，实验验证了该方法在高维大规模批量优化问题上的有效性。

Conclusion: 该方法为批量贝叶斯优化提供了一种通用且可扩展的生成模型框架，能够处理复杂优化问题并避免传统代理模型的构建。

Abstract: We present a general strategy for turning generative models into candidate
solution samplers for batch Bayesian optimization (BO). The use of generative
models for BO enables large batch scaling as generative sampling, optimization
of non-continuous design spaces, and high-dimensional and combinatorial design.
Inspired by the success of direct preference optimization (DPO), we show that
one can train a generative model with noisy, simple utility values directly
computed from observations to then form proposal distributions whose densities
are proportional to the expected utility, i.e., BO's acquisition function
values. Furthermore, this approach is generalizable beyond preference-based
feedback to general types of reward signals and loss functions. This
perspective avoids the construction of surrogate (regression or classification)
models, common in previous methods that have used generative models for
black-box optimization. Theoretically, we show that the generative models
within the BO process approximately follow a sequence of distributions which
asymptotically concentrate at the global optima under certain conditions. We
also demonstrate this effect through experiments on challenging optimization
problems involving large batches in high dimensions.

</details>


### [113] [Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains](https://arxiv.org/abs/2510.25514)
*Maik Overmars,Jasper Goseling,Richard Boucherie*

Main category: stat.ML

TL;DR: 本文研究了在可逆马尔可夫链下，带线性函数逼近的离策略TD(0)算法的收敛性，建立了明确的折扣因子上界条件，证明了算法以概率1收敛到投影贝尔曼误差为零。


<details>
  <summary>Details</summary>
Motivation: 离策略学习与函数逼近结合可能导致算法发散，现有方法通过重要性采样等修改算法来保证收敛但增加了复杂度。本文旨在分析标准算法，但限制在可逆马尔可夫链类中，利用领域知识假设链的可逆性来建立收敛保证。

Method: 将Tsitsiklis和Van Roy[1997]用于同策略情况的随机逼近框架适应到离策略情况，分析标准离策略TD(0)算法在可逆马尔可夫链下的收敛性。

Result: 在可逆马尔可夫链假设下，建立了折扣因子的明确上界条件（用同策略和离策略过程之间的差异表示），证明了算法以概率1收敛且达到投影贝尔曼误差为零。

Conclusion: 在可逆马尔可夫链的温和条件下，标准离策略TD(0)算法能够收敛，这为许多应用提供了实用的收敛保证，而无需修改算法或增加额外复杂度。

Abstract: We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.

</details>


### [114] [Using latent representations to link disjoint longitudinal data for mixed-effects regression](https://arxiv.org/abs/2510.25531)
*Clemens Schächter,Maren Hackenberg,Michelle Pfaffenlehner,Félix B. Tambe-Ndonfack,Thorsten Schmidt,Astrid Pechmann,Janbernd Kirschner,Jan Hasenauser,Harald Binder*

Main category: stat.ML

TL;DR: 提出了一种结合变分自编码器和混合效应回归的方法，用于分析罕见疾病治疗转换的影响，解决了测量工具变化导致的数据不连续问题。


<details>
  <summary>Details</summary>
Motivation: 罕见疾病治疗选择有限，患者常在新药出现时转换治疗。但由于样本量小且测量工具随时间变化，传统建模方法难以应用。

Method: 使用变分自编码器将不同测量工具的项目值映射到共享的潜在空间，然后应用混合效应回归模型在潜在表示中捕捉时间动态和治疗转换效应。

Result: 该方法成功应用于脊髓性肌萎缩症患者，能够对齐不同测量工具的运动表现项目，量化治疗转换效果。

Conclusion: 在联合潜在表示中建模为解决小数据挑战提供了潜力，能够进行模型选择和评估治疗转换效果。

Abstract: Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.

</details>


### [115] [Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations](https://arxiv.org/abs/2510.25544)
*Hugo Lavenant,Giacomo Zanella*

Main category: stat.ML

TL;DR: 本文研究了掩码扩散模型(MDMs)的计算精度权衡，提供了与数据维度无关的误差界限，并优化了非恒定调度大小以获得更好的生成效果。


<details>
  <summary>Details</summary>
Motivation: 研究离散数据生成模型中的计算成本与采样分布准确性之间的权衡，特别是掩码扩散模型相对于自回归模型的优势与局限。

Method: 使用条件独立近似来降低计算成本，分析不同调度大小(即生成过程中未掩码标记数量)的影响，并基于数据分布的信息特征优化调度策略。

Result: 建立了与数据维度无关的相对熵误差界限，证明了MDMs的经验成功，并确定了基于数据分布信息特征的最优调度大小。

Conclusion: 掩码扩散模型在计算效率与生成质量之间提供了良好的平衡，通过优化调度策略可以进一步提升性能，且证明方法简单透明。

Abstract: Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.

</details>


### [116] [Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification](https://arxiv.org/abs/2510.25573)
*Christopher T. Franck,Anne R. Driscoll,Zoe Szajnfarber,William H. Woodall*

Main category: stat.ML

TL;DR: 提出了一种基于累积和动态控制限的方法，用于持续监测机器学习模型的校准状态，能够检测传统过程监控和概念漂移应用中的校准失效问题。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习模型在图像分类方面取得了显著进展，但如何评估和维持这些模型预测的校准性是一个重要问题。现有方法主要关注校准评估和重新校准，而较少关注随时间推移持续监测模型校准状态的需求。

Method: 采用累积和控制图方法，结合动态控制限，能够监测概率预测和事件结果，无需访问机器学习模型的内部结构。

Result: 该方法能够在操作环境变化影响图像分类性能时实现早期检测，适用于任何需要随时间监测概率预测校准状态的情况。

Conclusion: 提出的累积和动态控制限方法为持续监测机器学习模型校准提供了有效工具，特别适用于检测概念漂移和操作环境变化导致的校准失效。

Abstract: Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.

</details>


### [117] [How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs](https://arxiv.org/abs/2510.25753)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 该论文研究了预训练Transformer中的上下文学习能力，分析了非线性MLP头在非线性任务中的作用，证明了非线性MLP能显著提升ICL性能，特别是在非线性任务中，并揭示了数据混合效应和特征学习的关键条件。


<details>
  <summary>Details</summary>
Motivation: 现有理论研究通常使用简化的架构和数据模型，限制了其对现实场景的适用性。本文旨在研究更接近实际情况的预训练Transformer，包括非线性MLP头、多数据源和异质分布。

Method: 分析包含两层MLP的模型，其中第一层通过单步梯度训练，第二层完全优化。在高维渐近条件下，证明此类模型在ICL误差上等价于结构化多项式预测器，利用高斯普适性和正交多项式理论。

Result: 非线性MLP显著提升了ICL性能，特别是在非线性任务中。识别了高质量数据源的关键属性（低噪声、结构化协方差），并发现只有当任务协方差具有足够结构时才会出现特征学习。

Conclusion: 该工作推进了Transformer中ICL的理论基础，为架构和数据在ICL中的作用提供了可操作的见解，并通过多语言情感分析等真实场景验证了研究结果。

Abstract: Pretrained Transformers demonstrate remarkable in-context learning (ICL)
capabilities, enabling them to adapt to new tasks from demonstrations without
parameter updates. However, theoretical studies often rely on simplified
architectures (e.g., omitting MLPs), data models (e.g., linear regression with
isotropic inputs), and single-source training, limiting their relevance to
realistic settings. In this work, we study ICL in pretrained Transformers with
nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with
heterogeneous input, task, and noise distributions. We analyze a model where
the MLP comprises two layers, with the first layer trained via a single
gradient step and the second layer fully optimized. Under high-dimensional
asymptotics, we prove that such models are equivalent in ICL error to
structured polynomial predictors, leveraging results from the theory of
Gaussian universality and orthogonal polynomials. This equivalence reveals that
nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear
tasks, compared to linear baselines. It also enables a precise analysis of data
mixing effects: we identify key properties of high-quality data sources (low
noise, structured covariances) and show that feature learning emerges only when
the task covariance exhibits sufficient structure. These results are validated
empirically across various activation functions, model sizes, and data
distributions. Finally, we experiment with a real-world scenario involving
multilingual sentiment analysis where each language is treated as a different
source. Our experimental results for this case exemplify how our findings
extend to real-world cases. Overall, our work advances the theoretical
foundations of ICL in Transformers and provides actionable insight into the
role of architecture and data in ICL.

</details>


### [118] [E-Scores for (In)Correctness Assessment of Generative Model Outputs](https://arxiv.org/abs/2510.25770)
*Guneet S. Dhillon,Javier González,Teodora Pandeva,Alicia Curth*

Main category: stat.ML

TL;DR: 该论文提出使用e值来评估生成模型输出的错误性，解决了传统p值方法容易受到p-hacking影响的问题，允许用户在观察e值后灵活选择容错水平。


<details>
  <summary>Details</summary>
Motivation: 当前评估生成模型（特别是大语言模型）正确性的机制有限，传统基于p值的方法容易受到p-hacking的影响，无法保证统计保证的有效性。

Method: 利用符合预测框架，采用e值来补充生成模型输出，通过e-score衡量错误性，实现与之前相同的统计保证，同时允许用户自适应选择容错水平。

Result: 实验证明该方法在评估LLM输出的数学事实性和属性约束满足等正确性类型方面具有有效性。

Conclusion: e-score方法不仅保持了统计保证，还提供了更大的灵活性，能够有效评估生成模型的正确性，解决了p-hacking问题。

Abstract: While generative models, especially large language models (LLMs), are
ubiquitous in today's world, principled mechanisms to assess their
(in)correctness are limited. Using the conformal prediction framework, previous
works construct sets of LLM responses where the probability of including an
incorrect response, or error, is capped at a desired user-defined tolerance
level. However, since these methods are based on p-values, they are susceptible
to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the
guarantees. We therefore leverage e-values to complement generative model
outputs with e-scores as a measure of incorrectness. In addition to achieving
the same statistical guarantees as before, e-scores provide users flexibility
in adaptively choosing tolerance levels after observing the e-scores
themselves, by upper bounding a post-hoc notion of error called size
distortion. We experimentally demonstrate their efficacy in assessing LLM
outputs for different correctness types: mathematical factuality and property
constraints satisfaction.

</details>
