{"id": "2507.18627", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18627", "abs": "https://arxiv.org/abs/2507.18627", "authors": ["Jiahang Zhang", "Mingtong Chen", "Zhengbao Yang"], "title": "Gait Recognition Based on Tiny ML and IMU Sensors", "comment": null, "summary": "This project presents the development of a gait recognition system using Tiny\nMachine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The\nsystem leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU\nsensor to capture motion data, including acceleration and angular velocity,\nfrom four distinct activities: walking, stationary, going upstairs, and going\ndownstairs. The data collected is processed through Edge Impulse, an edge AI\nplatform, which enables the training of machine learning models that can be\ndeployed directly onto the microcontroller for real-time activity\nclassification.The data preprocessing step involves extracting relevant\nfeatures from the raw sensor data using techniques such as sliding windows and\ndata normalization, followed by training a Deep Neural Network (DNN) classifier\nfor activity recognition. The model achieves over 80% accuracy on a test\ndataset, demonstrating its ability to classify the four activities effectively.\nAdditionally, the platform enables anomaly detection, further enhancing the\nrobustness of the system. The integration of Tiny ML ensures low-power\noperation, making it suitable for battery-powered or energy-harvesting devices.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eTiny ML\u548cIMU\u4f20\u611f\u5668\u7684\u6b65\u6001\u8bc6\u522b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u65f6\u5206\u7c7b\u56db\u79cd\u6d3b\u52a8\uff0c\u51c6\u786e\u7387\u8d85\u8fc780%\u3002", "motivation": "\u5229\u7528\u5fae\u578b\u8bbe\u5907\u548c\u4f4e\u529f\u8017\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u6b65\u6001\u8bc6\u522b\uff0c\u9002\u7528\u4e8e\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u3002", "method": "\u4f7f\u7528XIAO-nRF52840 Sense\u548cLSM6DS3\u4f20\u611f\u5668\u91c7\u96c6\u6570\u636e\uff0c\u901a\u8fc7Edge Impulse\u5e73\u53f0\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548cDNN\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523080%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "conclusion": "\u7cfb\u7edf\u5c55\u793a\u4e86Tiny ML\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e2d\u5b9e\u73b0\u9ad8\u6548\u6b65\u6001\u8bc6\u522b\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17921", "categories": ["stat.ML", "cs.LG", "eess.IV", "math.ST", "stat.CO", "stat.ME", "stat.TH", "62H20, 62H25 (Primary) 62J10, 62L10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.17921", "abs": "https://arxiv.org/abs/2507.17921", "authors": ["Arvind Prasadan"], "title": "Sliding Window Informative Canonical Correlation Analysis", "comment": "22 pages, submitted", "summary": "Canonical correlation analysis (CCA) is a technique for finding correlated\nsets of features between two datasets. In this paper, we propose a novel\nextension of CCA to the online, streaming data setting: Sliding Window\nInformative Canonical Correlation Analysis (SWICCA). Our method uses a\nstreaming principal component analysis (PCA) algorithm as a backend and uses\nthese outputs combined with a small sliding window of samples to estimate the\nCCA components in real time. We motivate and describe our algorithm, provide\nnumerical simulations to characterize its performance, and provide a\ntheoretical performance guarantee. The SWICCA method is applicable and scalable\nto extremely high dimensions, and we provide a real-data example that\ndemonstrates this capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u6d41\u6570\u636e\u73af\u5883\u4e0b\u7684CCA\u6269\u5c55\u65b9\u6cd5SWICCA\uff0c\u7ed3\u5408\u6d41\u5f0fPCA\u548c\u6ed1\u52a8\u7a97\u53e3\u5b9e\u65f6\u4f30\u8ba1CCA\u7ec4\u4ef6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCCA\u65e0\u6cd5\u5904\u7406\u6d41\u5f0f\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5b9e\u65f6\u5206\u6790\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6d41\u5f0fPCA\u4f5c\u4e3a\u540e\u7aef\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6837\u672c\u5b9e\u65f6\u4f30\u8ba1CCA\u7ec4\u4ef6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u6027\u80fd\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "conclusion": "SWICCA\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6d41\u5f0f\u6570\u636eCCA\u65b9\u6cd5\u3002"}}
{"id": "2507.18118", "categories": ["stat.ML", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.18118", "abs": "https://arxiv.org/abs/2507.18118", "authors": ["Jinjuan Wang", "Qianglin Wen", "Yu Zhang", "Xiaodong Yan", "Chengchun Shi"], "title": "A Two-armed Bandit Framework for A/B Testing", "comment": null, "summary": "A/B testing is widely used in modern technology companies for policy\nevaluation and product deployment, with the goal of comparing the outcomes\nunder a newly-developed policy against a standard control. Various causal\ninference and reinforcement learning methods developed in the literature are\napplicable to A/B testing. This paper introduces a two-armed bandit framework\ndesigned to improve the power of existing approaches. The proposed procedure\nconsists of three main steps: (i) employing doubly robust estimation to\ngenerate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to\nconstruct the test statistic, and (iii) applying a permutation-based method to\ncompute the $p$-value. We demonstrate the efficacy of the proposed method\nthrough asymptotic theories, numerical experiments and real-world data from a\nridesharing company, showing its superior performance in comparison to existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e24\u81c2\u8001\u864e\u673a\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u3001\u6784\u5efa\u6d4b\u8bd5\u7edf\u8ba1\u91cf\u548c\u7f6e\u6362\u6cd5\u8ba1\u7b97p\u503c\uff0c\u663e\u8457\u63d0\u5347\u4e86A/B\u6d4b\u8bd5\u7684\u6548\u80fd\u3002", "motivation": "A/B\u6d4b\u8bd5\u5728\u79d1\u6280\u516c\u53f8\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u65b9\u6cd5\u63d0\u5347\u6d4b\u8bd5\u7684\u7edf\u8ba1\u529f\u6548\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u6b65\uff1a(i)\u4f7f\u7528\u53cc\u91cd\u7a33\u5065\u4f30\u8ba1\u751f\u6210\u4f2a\u7ed3\u679c\uff0c(ii)\u5728\u4e24\u81c2\u8001\u864e\u673a\u6846\u67b6\u4e0b\u6784\u5efa\u6d4b\u8bd5\u7edf\u8ba1\u91cf\uff0c(iii)\u901a\u8fc7\u7f6e\u6362\u6cd5\u8ba1\u7b97p\u503c\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u3001\u6570\u503c\u5b9e\u9a8c\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86A/B\u6d4b\u8bd5\u7684\u6548\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.18147", "categories": ["stat.ML"], "pdf": "https://arxiv.org/pdf/2507.18147", "abs": "https://arxiv.org/abs/2507.18147", "authors": ["Stefan Klus", "Jason J. Bramburger"], "title": "Learning graphons from data: Random walks, transfer operators, and spectral clustering", "comment": null, "summary": "Many signals evolve in time as a stochastic process, randomly switching\nbetween states over discretely sampled time points. Here we make an explicit\nlink between the underlying stochastic process of a signal that can take on a\nbounded continuum of values and a random walk process on a graphon. Graphons\nare infinite-dimensional objects that represent the limit of convergent\nsequences of graphs whose size tends to infinity. We introduce transfer\noperators, such as the Koopman and Perron--Frobenius operators, associated with\nrandom walk processes on graphons and then illustrate how these operators can\nbe estimated from signal data and how their eigenvalues and eigenfunctions can\nbe used for detecting clusters, thereby extending conventional spectral\nclustering methods from graphs to graphons. Furthermore, we show that it is\nalso possible to reconstruct transition probability densities and, if the\nrandom walk process is reversible, the graphon itself using only the signal.\nThe resulting data-driven methods are applied to a variety of synthetic and\nreal-world signals, including daily average temperatures and stock index\nvalues.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4fe1\u53f7\u7684\u968f\u673a\u8fc7\u7a0b\u4e0e\u56fe\u4e0a\u7684\u968f\u673a\u6e38\u8d70\u8054\u7cfb\u8d77\u6765\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u8bba\u4e2d\u7684\u56fe\u5b50\uff08graphon\uff09\u6982\u5ff5\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u8c31\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u4ece\u4fe1\u53f7\u6570\u636e\u4e2d\u91cd\u6784\u8f6c\u79fb\u6982\u7387\u5bc6\u5ea6\u548c\u56fe\u5b50\u3002", "motivation": "\u7814\u7a76\u4fe1\u53f7\u5728\u65f6\u95f4\u4e0a\u7684\u968f\u673a\u6f14\u5316\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u5982\u4f55\u5728\u79bb\u6563\u91c7\u6837\u70b9\u95f4\u968f\u673a\u5207\u6362\u72b6\u6001\uff0c\u5e76\u63a2\u7d22\u5176\u4e0e\u56fe\u4e0a\u7684\u968f\u673a\u6e38\u8d70\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u5f15\u5165\u4e0e\u56fe\u5b50\u4e0a\u968f\u673a\u6e38\u8d70\u76f8\u5173\u7684\u8f6c\u79fb\u7b97\u5b50\uff08\u5982Koopman\u548cPerron-Frobenius\u7b97\u5b50\uff09\uff0c\u5e76\u901a\u8fc7\u4fe1\u53f7\u6570\u636e\u4f30\u8ba1\u8fd9\u4e9b\u7b97\u5b50\u53ca\u5176\u7279\u5f81\u503c\u548c\u7279\u5f81\u51fd\u6570\uff0c\u7528\u4e8e\u805a\u7c7b\u5206\u6790\u3002", "result": "\u6210\u529f\u5c06\u8c31\u805a\u7c7b\u65b9\u6cd5\u4ece\u56fe\u6269\u5c55\u5230\u56fe\u5b50\uff0c\u5e76\u80fd\u4ece\u4fe1\u53f7\u4e2d\u91cd\u6784\u8f6c\u79fb\u6982\u7387\u5bc6\u5ea6\u548c\u53ef\u9006\u968f\u673a\u6e38\u8d70\u7684\u56fe\u5b50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u4fe1\u53f7\uff08\u5982\u6e29\u5ea6\u548c\u80a1\u7968\u6307\u6570\uff09\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u4e3a\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.17966", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17966", "abs": "https://arxiv.org/abs/2507.17966", "authors": ["Mohsen Bayat", "Sanoopkumar P. S.", "Arman Farhang"], "title": "Time and Frequency Synchronization for Multiuser OTFS in Uplink", "comment": null, "summary": "In this paper, we propose time and frequency synchronization techniques for\nuplink multiuser OTFS (MU-OTFS) systems in high-mobility scenarios. This work\nfocuses on accurately estimating and correcting timing offsets (TOs) and\ncarrier frequency offsets (CFOs). Specifically, TO estimation is essential for\nlocating users' pilots on the delay-time plane, while CFO estimation enhances\nchannel estimation accuracy. First, we propose a TO estimation technique for an\nexisting multiuser pilot structure in MU-OTFS. We replace the impulse pilot\n(IMP) in this pilot structure with a more practical pilot with a cyclic prefix\n(PCP), referred to as single-user-inspired PCP (SU-PCP). This structure employs\ndifferent Zadoff-Chu (ZC) sequences, which enables pilot separation via\ncorrelation at the receiver side. Consequently, we introduce a\ncorrelation-based TO estimation technique for uplink MU-OTFS using this pilot\nstructure. Next, a spectrally efficient and practical pilot pattern is\nproposed, where each user transmits a PCP within a shared pilot region on the\ndelay-Doppler plane, referred to as MU-PCP. At the receiver, the second TO\nestimation technique utilizes a bank of filters to separate different users'\nsignals and accurately estimate their TOs. Then, we derive a mathematical\nthreshold range to enhance TO estimation accuracy by finding the first major\npeak in the correlation function rather than relying solely on the highest\npeak. After locating the received users' pilot signals using one of the\nproposed TO estimation techniques, our proposed CFO estimation technique\nreduces the multi-dimensional maximum likelihood (ML) search problem into\nmultiple one-dimensional search problems. In this technique, we apply the\nChebyshev polynomials of the first kind basis expansion model (CPF-BEM) to\neffectively handle the time-variations of the channel in obtaining the CFO\nestimates for all the users.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u4e0a\u884c\u591a\u7528\u6237OTFS\u7cfb\u7edf\u7684\u65f6\u95f4\u548c\u9891\u7387\u540c\u6b65\u6280\u672f\uff0c\u91cd\u70b9\u89e3\u51b3\u4e86\u5b9a\u65f6\u504f\u79fb\u548c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\u7684\u4f30\u8ba1\u4e0e\u6821\u6b63\u95ee\u9898\u3002", "motivation": "\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u4f30\u8ba1\u548c\u6821\u6b63\u5b9a\u65f6\u504f\u79fb\uff08TOs\uff09\u548c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\uff08CFOs\uff09\u5bf9\u591a\u7528\u6237OTFS\u7cfb\u7edf\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e24\u79cdTO\u4f30\u8ba1\u6280\u672f\uff1a\u57fa\u4e8eSU-PCP\u7684TO\u4f30\u8ba1\u548c\u57fa\u4e8eMU-PCP\u7684TO\u4f30\u8ba1\uff0c\u5e76\u5229\u7528CPF-BEM\u6a21\u578b\u7b80\u5316CFO\u4f30\u8ba1\u7684\u591a\u7ef4\u641c\u7d22\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6539\u8fdb\u7684\u5bfc\u9891\u7ed3\u6784\u548c\u76f8\u5173\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684TO\u548cCFO\u4f30\u8ba1\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u540c\u6b65\u6280\u672f\u5728\u9ad8\u79fb\u52a8\u6027\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u591a\u7528\u6237OTFS\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.18372", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.18372", "abs": "https://arxiv.org/abs/2507.18372", "authors": ["George Wynne"], "title": "On Reconstructing Training Data From Bayesian Posteriors and Trained Models", "comment": null, "summary": "Publicly releasing the specification of a model with its trained parameters\nmeans an adversary can attempt to reconstruct information about the training\ndata via training data reconstruction attacks, a major vulnerability of modern\nmachine learning methods. This paper makes three primary contributions:\nestablishing a mathematical framework to express the problem, characterising\nthe features of the training data that are vulnerable via a maximum mean\ndiscrepancy equivalance and outlining a score matching framework for\nreconstructing data in both Bayesian and non-Bayesian models, the former is a\nfirst in the literature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u6846\u67b6\u6765\u5206\u6790\u8bad\u7ec3\u6570\u636e\u91cd\u6784\u653b\u51fb\uff0c\u9996\u6b21\u5728\u6587\u732e\u4e2d\u5b9e\u73b0\u4e86\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u5206\u6570\u5339\u914d\u91cd\u6784\u65b9\u6cd5\u3002", "motivation": "\u516c\u5f00\u6a21\u578b\u53c2\u6570\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u88ab\u91cd\u6784\u653b\u51fb\uff0c\u8fd9\u662f\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u7684\u4e3b\u8981\u6f0f\u6d1e\u3002", "method": "\u5efa\u7acb\u6570\u5b66\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5747\u503c\u5dee\u5f02\u7b49\u4ef7\u6027\u5206\u6790\u6613\u53d7\u653b\u51fb\u7684\u6570\u636e\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u5206\u6570\u5339\u914d\u6846\u67b6\u7528\u4e8e\u8d1d\u53f6\u65af\u548c\u975e\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6570\u636e\u91cd\u6784\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u91cd\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u9632\u5fa1\u8bad\u7ec3\u6570\u636e\u91cd\u6784\u653b\u51fb\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2507.17982", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.17982", "abs": "https://arxiv.org/abs/2507.17982", "authors": ["Pablo Ram\u00edrez-Espinosa", "Cleof\u00e1s Segura-G\u00f3mez", "\u00c1ngel Palomares-Caballero", "F. Javier L\u00f3pez-Mart\u00ednez", "David Morales-Jim\u00e9nez"], "title": "Metasurface-based Fluid Antennas: from Electromagnetics to Communications Model", "comment": null, "summary": "Fluid antenna systems (FASs) have become a popular topic in the wireless\ncommunity as an effective yet simple means of exploiting spatial diversity. Due\nto the limitations of physically moving radiating elements, electronically\nreconfigurable antennas are emerging as practical implementations of FASs,\nsince changing the radiation pattern is functionally equivalent to physically\nmoving the device. However, electronically reconfigurable antennas pose a\nchallenge in terms of analytical modeling, often requiring full-wave\nsimulations or measurements for their characterization; this severely limits\nthe extraction of theoretical insights useful for system design. Motivated by\nthese difficulties and the growing interest in FASs, we propose in this paper a\ncomplete analytical model for metasurface-based embodiments of FASs.\nSpecifically, we advocate for the implementation of the FAS concept through\ndynamic metasurface antennas (DMAs), hitherto proposed as array replacements in\nmultiple-input multiple-output (MIMO) systems. We leverage circuit theory to\nrewrite the conventional signal model of FASs in terms of admittance matrices\naccounting for the electromagnetic effects inherent to metasurfaces. The model\nis validated with full-wave simulations, showing good agreement. We further\nillustrate how to apply the model for standard performance analysis, and\nprovide closed-form expressions for key metrics, including the resulting signal\ncovariance matrix. Results confirm that practical DMA-based FASs can achieve\nsimilar performance to that of idealized implementations of position-flexible\nantennas.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u8868\u9762\u7684\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u7684\u5b8c\u6574\u5206\u6790\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8d85\u8868\u9762\u5929\u7ebf\uff08DMA\uff09\u5b9e\u73b0\uff0c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u5316\u5b9e\u73b0\u3002", "motivation": "\u7531\u4e8e\u7535\u5b50\u53ef\u91cd\u6784\u5929\u7ebf\u5728\u5206\u6790\u5efa\u6a21\u4e0a\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u6d41\u4f53\u5929\u7ebf\u7cfb\u7edf\uff08FAS\uff09\u5728\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u6f5c\u529b\uff0c\u4f5c\u8005\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u5b9e\u7528\u7684\u5206\u6790\u6a21\u578b\u3002", "method": "\u5229\u7528\u7535\u8def\u7406\u8bba\u5c06FAS\u7684\u4fe1\u53f7\u6a21\u578b\u6539\u5199\u4e3a\u8003\u8651\u8d85\u8868\u9762\u7535\u78c1\u6548\u5e94\u7684\u5bfc\u7eb3\u77e9\u9635\uff0c\u5e76\u901a\u8fc7\u5168\u6ce2\u4eff\u771f\u9a8c\u8bc1\u6a21\u578b\u3002", "result": "\u6a21\u578b\u9a8c\u8bc1\u663e\u793a\u4e0e\u4eff\u771f\u7ed3\u679c\u4e00\u81f4\uff0cDMA\u5b9e\u73b0\u7684FAS\u6027\u80fd\u63a5\u8fd1\u7406\u60f3\u5316\u5929\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u6790\u6a21\u578b\u4e3aFAS\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0cDMA\u5b9e\u73b0\u5c55\u793a\u4e86\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.17768", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17768", "abs": "https://arxiv.org/abs/2507.17768", "authors": ["Yujia Tong", "Jingling Yuan", "Chuang Hu"], "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction", "comment": null, "summary": "With the development of mobile and edge computing, the demand for low-bit\nquantized models on edge devices is increasing to achieve efficient deployment.\nTo enhance the performance, it is often necessary to retrain the quantized\nmodels using edge data. However, due to privacy concerns, certain sensitive\ndata can only be processed on edge devices. Therefore, employing\nQuantization-Aware Training (QAT) on edge devices has become an effective\nsolution. Nevertheless, traditional QAT relies on the complete dataset for\ntraining, which incurs a huge computational cost. Coreset selection techniques\ncan mitigate this issue by training on the most representative subsets.\nHowever, existing methods struggle to eliminate quantization errors in the\nmodel when using small-scale datasets (e.g., only 10% of the data), leading to\nsignificant performance degradation. To address these issues, we propose QuaRC,\na QAT framework with coresets on edge devices, which consists of two main\nphases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy\nScore\" to identify the subsets that most effectively capture the model's\nquantization errors. During the training phase, QuaRC employs the Cascaded\nLayer Correction strategy to align the intermediate layer outputs of the\nquantized model with those of the full-precision model, thereby effectively\nreducing the quantization errors in the intermediate layers. Experimental\nresults demonstrate the effectiveness of our approach. For instance, when\nquantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%\nimprovement in Top-1 accuracy on the ImageNet-1K dataset compared to\nstate-of-the-art techniques.", "AI": {"tldr": "QuaRC\u662f\u4e00\u4e2a\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7ed3\u5408\u6838\u5fc3\u96c6\u9009\u62e9\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u5bf9\u71b5\u5206\u6570\u548c\u7ea7\u8054\u5c42\u6821\u6b63\u7b56\u7565\u663e\u8457\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u4f20\u7edfQAT\u4f9d\u8d56\u5b8c\u6574\u6570\u636e\u96c6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u96be\u4ee5\u6d88\u9664\u91cf\u5316\u8bef\u5dee\u3002", "method": "QuaRC\u5206\u4e24\u9636\u6bb5\uff1a\u6838\u5fc3\u96c6\u9009\u62e9\u9636\u6bb5\u4f7f\u7528\u76f8\u5bf9\u71b5\u5206\u6570\u9009\u53d6\u4ee3\u8868\u6027\u5b50\u96c6\uff1b\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u7ea7\u8054\u5c42\u6821\u6b63\u7b56\u7565\u5bf9\u9f50\u91cf\u5316\u6a21\u578b\u548c\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u8f93\u51fa\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75281%\u6570\u636e\u5b50\u96c6\u91cf\u5316ResNet-18\u81f32\u6bd4\u7279\u65f6\uff0cQuaRC\u7684Top-1\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u53475.72%\u3002", "conclusion": "QuaRC\u901a\u8fc7\u521b\u65b0\u7684\u6838\u5fc3\u96c6\u9009\u62e9\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u91cf\u5316\u8bef\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.18464", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18464", "abs": "https://arxiv.org/abs/2507.18464", "authors": ["Miguel Aspis", "Sebasti\u00e1n A. Cajas Ord\u00f3nez", "Andr\u00e9s L. Su\u00e1rez-Cetrulo", "Ricardo Sim\u00f3n Carbajo"], "title": "DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts", "comment": "Accepted at the SYNDAiTE@ECMLPKDD 2025 workshop", "summary": "Learning from non-stationary data streams subject to concept drift requires\nmodels that can adapt on-the-fly while remaining resource-efficient. Existing\nadaptive ensemble methods often rely on coarse-grained adaptation mechanisms or\nsimple voting schemes that fail to optimally leverage specialized knowledge.\nThis paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture\nthat addresses these limitations through a novel co-training framework.\nDriftMoE features a compact neural router that is co-trained alongside a pool\nof incremental Hoeffding tree experts. The key innovation lies in a symbiotic\nlearning loop that enables expert specialization: the router selects the most\nsuitable expert for prediction, the relevant experts update incrementally with\nthe true label, and the router refines its parameters using a multi-hot\ncorrectness mask that reinforces every accurate expert. This feedback loop\nprovides the router with a clear training signal while accelerating expert\nspecialization. We evaluate DriftMoE's performance across nine state-of-the-art\ndata stream learning benchmarks spanning abrupt, gradual, and real-world drifts\ntesting two distinct configurations: one where experts specialize on data\nregimes (multi-class variant), and another where they focus on single-class\nspecialization (task-based variant). Our results demonstrate that DriftMoE\nachieves competitive results with state-of-the-art stream learning adaptive\nensembles, offering a principled and efficient approach to concept drift\nadaptation. All code, data pipelines, and reproducibility scripts are available\nin our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.", "AI": {"tldr": "DriftMoE\u662f\u4e00\u79cd\u5728\u7ebf\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u534f\u540c\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u73b0\u6709\u81ea\u9002\u5e94\u96c6\u6210\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6982\u5ff5\u6f02\u79fb\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u81ea\u9002\u5e94\u96c6\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7c97\u7c92\u5ea6\u9002\u5e94\u673a\u5236\u6216\u7b80\u5355\u6295\u7968\u65b9\u6848\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4e13\u5bb6\u77e5\u8bc6\u3002", "method": "DriftMoE\u91c7\u7528\u7d27\u51d1\u7684\u795e\u7ecf\u7f51\u7edc\u8def\u7531\u5668\u4e0e\u589e\u91cfHoeffding\u6811\u4e13\u5bb6\u6c60\u534f\u540c\u8bad\u7ec3\uff0c\u901a\u8fc7\u5171\u751f\u5b66\u4e60\u5faa\u73af\u5b9e\u73b0\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u4e5d\u79cd\u6570\u636e\u6d41\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDriftMoE\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u81ea\u9002\u5e94\u96c6\u6210\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "DriftMoE\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u539f\u5219\u6027\u7684\u6982\u5ff5\u6f02\u79fb\u9002\u5e94\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.18035", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18035", "abs": "https://arxiv.org/abs/2507.18035", "authors": ["Hyeonho Noh", "Hyeonsu Lyu", "Hyun Jong Yang"], "title": "Multiple Active STAR-RIS-Assisted Secure Integrated Sensing and Communication via Cooperative Beamforming", "comment": null, "summary": "This paper explores an integrated sensing and communication (ISAC) network\nempowered by multiple active simultaneously transmitting and reflecting\nreconfigurable intelligent surfaces (STAR-RISs). A base station (BS) furnishes\ndownlink communication to multiple users while concurrently interrogating a\nsensing target. We jointly optimize the BS transmit beamformer and the\nreflection/transmission coefficients of every active STAR-RIS in order to\nmaximize the aggregate communication sum-rate, subject to (i) a stringent\nsensing signal-to-interference-plus-noise ratio (SINR) requirement, (ii) an\nupper bound on the leakage of confidential information, and (iii) individual\nhardware and total power constraints at both the BS and the STAR-RISs. The\nresulting highly non-convex program is tackled with an efficient alternating\noptimization (AO) framework. First, the original formulation is reformulated\ninto an equivalent yet more tractable representation and partitioned into\nsubproblems. The BS beamformer is updated in closed form via the\nKarush-Kuhn-Tucker (KKT) conditions, whereas the STAR-RIS reflection and\ntransmission vectors are refined through successive convex approximation (SCA),\nyielding a semidefinite program that is then solved via semidefinite\nrelaxation. Comprehensive simulations demonstrate that the proposed algorithm\ndelivers substantial sum-rate gains over passive-RIS and single STAR-RIS\nbaselines, all the while rigorously meeting the prescribed sensing and security\nconstraints.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7531\u591a\u4e2a\u4e3b\u52a8\u540c\u65f6\u4f20\u8f93\u548c\u53cd\u5c04\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08STAR-RISs\uff09\u652f\u6301\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7f51\u7edc\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cSTAR-RIS\u7cfb\u6570\uff0c\u6700\u5927\u5316\u901a\u4fe1\u603b\u901f\u7387\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u3001\u5b89\u5168\u548c\u529f\u7387\u7ea6\u675f\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528STAR-RISs\u63d0\u5347ISAC\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u901a\u4fe1\u901f\u7387\u3001\u611f\u77e5\u8981\u6c42\u548c\u4fe1\u606f\u5b89\u5168\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u5206\u522b\u901a\u8fc7KKT\u6761\u4ef6\u548cSCA\u65b9\u6cd5\u4f18\u5316\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548cSTAR-RIS\u7cfb\u6570\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u901a\u4fe1\u603b\u901f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u88ab\u52a8RIS\u548c\u5355STAR-RIS\u57fa\u51c6\uff0c\u540c\u65f6\u6ee1\u8db3\u611f\u77e5\u548c\u5b89\u5168\u7ea6\u675f\u3002", "conclusion": "\u4e3b\u52a8STAR-RISs\u5728ISAC\u7f51\u7edc\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u8054\u5408\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.17784", "categories": ["cs.LG", "68", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.17784", "abs": "https://arxiv.org/abs/2507.17784", "authors": ["Minh-Duong Nguyen", "Quoc-Viet Pham", "Nguyen H. Tran", "Hoang-Khoi Do", "Duy T. Ngo", "Won-Joo Hwang"], "title": "Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach", "comment": "13 pages, 12 figures, 4 tables", "summary": "In this study, we design a low-complexity and generalized AI model that can\ncapture common knowledge to improve data reconstruction of the channel decoder\nfor semantic communication. Specifically, we propose a generative adversarial\nnetwork that leverages causality-invariant learning to extract causal and\nnon-causal representations from the data. Causal representations are invariant\nand encompass crucial information to identify the data's label. They can\nencapsulate semantic knowledge and facilitate effective data reconstruction at\nthe receiver. Moreover, the causal mechanism ensures that learned\nrepresentations remain consistent across different domains, making the system\nreliable even with users collecting data from diverse domains. As\nuser-collected data evolves over time causing knowledge divergence among users,\nwe design sparse update protocols to improve the invariant properties of the\nknowledge while minimizing communication overheads. Three key observations were\ndrawn from our empirical evaluations. Firstly, causality-invariant knowledge\nensures consistency across different devices despite the diverse training data.\nSecondly, invariant knowledge has promising performance in classification\ntasks, which is pivotal for goal-oriented semantic communications. Thirdly, our\nknowledge-based data reconstruction highlights the robustness of our decoder,\nwhich surpasses other state-of-the-art data reconstruction and semantic\ncompression methods in terms of Peak Signal-to-Noise Ratio (PSNR).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u3001\u901a\u7528\u7684AI\u6a21\u578b\uff0c\u5229\u7528\u56e0\u679c\u4e0d\u53d8\u5b66\u4e60\u6539\u8fdb\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u4fe1\u9053\u89e3\u7801\u5668\u6570\u636e\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u6570\u636e\u591a\u6837\u5316\u548c\u77e5\u8bc6\u5206\u6563\u5bfc\u81f4\u7684\u8bed\u4e49\u901a\u4fe1\u4e2d\u6570\u636e\u91cd\u5efa\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u548c\u56e0\u679c\u4e0d\u53d8\u5b66\u4e60\uff0c\u63d0\u53d6\u6570\u636e\u7684\u56e0\u679c\u548c\u975e\u56e0\u679c\u8868\u793a\uff0c\u8bbe\u8ba1\u7a00\u758f\u66f4\u65b0\u534f\u8bae\u3002", "result": "\u56e0\u679c\u4e0d\u53d8\u77e5\u8bc6\u786e\u4fdd\u8de8\u8bbe\u5907\u4e00\u81f4\u6027\uff0c\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u91cd\u5efa\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2507.18520", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH", "62R07, 62G"], "pdf": "https://arxiv.org/pdf/2507.18520", "abs": "https://arxiv.org/abs/2507.18520", "authors": ["Keyi Li", "Yuval Kluger", "Boris Landa"], "title": "Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise", "comment": null, "summary": "Pairwise Euclidean distance calculation is a fundamental step in many machine\nlearning and data analysis algorithms. In real-world applications, however,\nthese distances are frequently distorted by heteroskedastic\nnoise$\\unicode{x2014}$a prevalent form of inhomogeneous corruption\ncharacterized by variable noise magnitudes across data observations. Such noise\ninflates the computed distances in a nontrivial way, leading to\nmisrepresentations of the underlying data geometry. In this work, we address\nthe tasks of estimating the noise magnitudes per observation and correcting the\npairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly,\nwe show that in general high-dimensional settings and without assuming prior\nknowledge on the clean data structure or noise distribution, both tasks can be\nperformed reliably, even when the noise levels vary considerably. Specifically,\nwe develop a principled, hyperparameter-free approach that jointly estimates\nthe noise magnitudes and corrects the distances. We provide theoretical\nguarantees for our approach, establishing probabilistic bounds on the\nestimation errors of both noise magnitudes and distances. These bounds,\nmeasured in the normalized $\\ell_1$ norm, converge to zero at polynomial rates\nas both feature dimension and dataset size increase. Experiments on synthetic\ndatasets demonstrate that our method accurately estimates distances in\nchallenging regimes, significantly improving the robustness of subsequent\ndistance-based computations. Notably, when applied to single-cell RNA\nsequencing data, our method yields noise magnitude estimates consistent with an\nestablished prototypical model, enabling accurate nearest neighbor\nidentification that is fundamental to many downstream analyses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5f02\u65b9\u5dee\u566a\u58f0\u4e0b\u7684\u566a\u58f0\u5e45\u5ea6\u5e76\u6821\u6b63\u6b27\u6c0f\u8ddd\u79bb\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "motivation": "\u5f02\u65b9\u5dee\u566a\u58f0\u4f1a\u626d\u66f2\u6b27\u6c0f\u8ddd\u79bb\uff0c\u5f71\u54cd\u6570\u636e\u51e0\u4f55\u8868\u793a\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u65b9\u6cd5\u6765\u6821\u6b63\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8054\u5408\u4f30\u8ba1\u566a\u58f0\u5e45\u5ea6\u548c\u6821\u6b63\u8ddd\u79bb\u7684\u7b97\u6cd5\uff0c\u65e0\u9700\u5047\u8bbe\u6e05\u6d01\u6570\u636e\u7ed3\u6784\u6216\u566a\u58f0\u5206\u5e03\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u566a\u58f0\u5e45\u5ea6\u548c\u8ddd\u79bb\u7684\u4f30\u8ba1\u8bef\u5dee\u4ee5\u591a\u9879\u5f0f\u901f\u7387\u6536\u655b\u4e3a\u96f6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\uff08\u5982\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\uff09\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddd\u79bb\u8ba1\u7b97\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.18096", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18096", "abs": "https://arxiv.org/abs/2507.18096", "authors": ["Jihong Huang", "Rong Yang", "Wei Gao", "Xingqun Zhan", "Zheng Yao"], "title": "Geometrical portrait of Multipath error propagation in GNSS Direct Position Estimation", "comment": null, "summary": "Direct Position Estimation (DPE) is a method that directly estimate position,\nvelocity, and time (PVT) information from cross ambiguity function (CAF) of the\nGNSS signals, significantly enhancing receiver robustness in urban\nenvironments. However, there is still a lack of theoretical characterization on\nmultipath errors in the context of DPE theory. Geometric observations highlight\nthe unique characteristics of DPE errors stemming from multipath and thermal\nnoise as estimation bias and variance respectively. Expanding upon the\ntheoretical framework of DPE noise variance through geometric analysis, this\npaper focuses on a geometric representation of multipath errors by quantifying\nthe deviations in CAF and PVT solutions caused by off-centering bias relative\nto the azimuth and elevation angles. A satellite circular multipath bias (SCMB)\nmodel is introduced, amalgamating CAF and PVT errors from multiple satellite\nchannels. The boundaries for maximum or minimum PVT bias are established\nthrough discussions encompassing various multipath conditions. The correctness\nof the multipath geometrical portrait is confirmed through both Monte Carlo\nsimulations and urban canyon tests. The findings indicate that the maximum PVT\nbias depends on the largest multipath errors observed across various satellite\nchannels. Additionally, the PVT bias increases with satellite elevation angles,\ninfluenced by the CAF multipath bias projection. This serves as a reference for\nselecting DPE satellites from a geometric standpoint, underscoring the\nimportance of choosing a balanced combination of high and low elevation angles\nto achieve an optimal satellite geometry configuration.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u91cf\u5316\u4e86DPE\u4e2d\u591a\u5f84\u8bef\u5dee\u5bf9CAF\u548cPVT\u89e3\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86SCMB\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u591a\u5f84\u8bef\u5dee\u7684\u51e0\u4f55\u7279\u6027\u3002", "motivation": "\u73b0\u6709DPE\u7406\u8bba\u7f3a\u4e4f\u5bf9\u591a\u5f84\u8bef\u5dee\u7684\u7cfb\u7edf\u6027\u7406\u8bba\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u591a\u5f84\u6548\u5e94\u5bf9PVT\u89e3\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u91cf\u5316\u591a\u5f84\u8bef\u5dee\u5bf9CAF\u548cPVT\u89e3\u7684\u5f71\u54cd\uff0c\u63d0\u51faSCMB\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u57ce\u5e02\u5ce1\u8c37\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6700\u5927PVT\u504f\u5dee\u53d6\u51b3\u4e8e\u5404\u536b\u661f\u901a\u9053\u4e2d\u7684\u6700\u5927\u591a\u5f84\u8bef\u5dee\uff0c\u4e14PVT\u504f\u5dee\u968f\u536b\u661f\u4ef0\u89d2\u589e\u52a0\u800c\u589e\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4ece\u51e0\u4f55\u89d2\u5ea6\u9009\u62e9DPE\u536b\u661f\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u9ad8\u4f4e\u4ef0\u89d2\u536b\u661f\u7684\u5e73\u8861\u7ec4\u5408\u5bf9\u4f18\u5316\u51e0\u4f55\u914d\u7f6e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.17785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17785", "abs": "https://arxiv.org/abs/2507.17785", "authors": ["Jingyi Ding", "Chengwen Qi", "Hongfei Wang", "Jianshe Wu", "Licheng Jiao", "Yuwei Guo", "Jian Gao"], "title": "Self-similarity Analysis in Deep Neural Networks", "comment": null, "summary": "Current research has found that some deep neural networks exhibit strong\nhierarchical self-similarity in feature representation or parameter\ndistribution. However, aside from preliminary studies on how the power-law\ndistribution of weights across different training stages affects model\nperformance,there has been no quantitative analysis on how the self-similarity\nof hidden space geometry influences model weight optimization, nor is there a\nclear understanding of the dynamic behavior of internal neurons. Therefore,\nthis paper proposes a complex network modeling method based on the output\nfeatures of hidden-layer neurons to investigate the self-similarity of feature\nnetworks constructed at different hidden layers, and analyzes how adjusting the\ndegree of self-similarity in feature networks can enhance the classification\nperformance of deep neural networks. Validated on three types of networks MLP\narchitectures, convolutional networks, and attention architectures this study\nreveals that the degree of self-similarity exhibited by feature networks varies\nacross different model architectures. Furthermore, embedding constraints on the\nself-similarity of feature networks during the training process can improve the\nperformance of self-similar deep neural networks (MLP architectures and\nattention architectures) by up to 6 percentage points.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u8f93\u51fa\u7279\u5f81\u7684\u590d\u6742\u7f51\u7edc\u5efa\u6a21\u65b9\u6cd5\uff0c\u7814\u7a76\u4e0d\u540c\u9690\u85cf\u5c42\u6784\u5efa\u7684\u7279\u5f81\u7f51\u7edc\u7684\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u5206\u6790\u5982\u4f55\u901a\u8fc7\u8c03\u6574\u81ea\u76f8\u4f3c\u6027\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u53d1\u73b0\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5f81\u8868\u793a\u6216\u53c2\u6570\u5206\u5e03\u4e0a\u5177\u6709\u5c42\u6b21\u81ea\u76f8\u4f3c\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9690\u85cf\u7a7a\u95f4\u51e0\u4f55\u81ea\u76f8\u4f3c\u6027\u5982\u4f55\u5f71\u54cd\u6743\u91cd\u4f18\u5316\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u4ee5\u53ca\u5bf9\u5185\u90e8\u795e\u7ecf\u5143\u52a8\u6001\u884c\u4e3a\u7684\u6e05\u6670\u7406\u89e3\u3002", "method": "\u91c7\u7528\u590d\u6742\u7f51\u7edc\u5efa\u6a21\u65b9\u6cd5\uff0c\u57fa\u4e8e\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u8f93\u51fa\u7279\u5f81\u6784\u5efa\u7279\u5f81\u7f51\u7edc\uff0c\u5206\u6790\u5176\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u81ea\u76f8\u4f3c\u6027\u7ea6\u675f\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u7279\u5f81\u7f51\u7edc\u81ea\u76f8\u4f3c\u6027\u7a0b\u5ea6\u5404\u5f02\uff0c\u5d4c\u5165\u81ea\u76f8\u4f3c\u6027\u7ea6\u675f\u53ef\u63d0\u5347\u81ea\u76f8\u4f3c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08MLP\u548c\u6ce8\u610f\u529b\u67b6\u6784\uff09\u6027\u80fd\u8fbe6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u7279\u5f81\u7f51\u7edc\u7684\u81ea\u76f8\u4f3c\u6027\u53ef\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.17792", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.17792", "abs": "https://arxiv.org/abs/2507.17792", "authors": ["Jingyi Yu", "Tim Pychynski", "Marco F. Huber"], "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains", "comment": null, "summary": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.", "AI": {"tldr": "CICME\u662f\u4e00\u79cd\u4e09\u6b65\u9aa4\u65b9\u6cd5\uff0c\u901a\u8fc7\u56e0\u679c\u8fc1\u79fb\u5b66\u4e60\u4ece\u591a\u9886\u57df\u5f02\u6784\u6570\u636e\u4e2d\u63a8\u65ad\u56e0\u679c\u673a\u5236\uff0c\u5e76\u5728\u5236\u9020\u8fc7\u7a0b\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u4ece\u590d\u6742\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u901a\u8fc7\u56e0\u679c\u6027\u83b7\u5f97\u66f4\u6df1\u5c42\u6b21\u7684\u89c1\u89e3\u3002", "method": "\u63d0\u51faCICME\u65b9\u6cd5\uff0c\u7ed3\u5408\u56e0\u679c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5206\u4e09\u6b65\u63a8\u65ad\u57df\u4e0d\u53d8\u548c\u4e2a\u4f53\u56e0\u679c\u673a\u5236\u3002", "result": "\u5728\u5236\u9020\u8fc7\u7a0b\u7684\u7ebf\u6027\u9ad8\u65af\u6a21\u578b\u4e2d\uff0cCICME\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CICME\u5728\u591a\u9886\u57df\u6570\u636e\u4e2d\u80fd\u6709\u6548\u8bc6\u522b\u56e0\u679c\u673a\u5236\uff0c\u5e76\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.18149", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18149", "abs": "https://arxiv.org/abs/2507.18149", "authors": ["Dongdong Zou", "Wei Wang", "Jiawen Yao", "Zhongxing Tian", "Zeyu Feng", "Huan Huang", "Fan Li", "Gordon Ning Liu", "Gangxiang Shen", "Yi Cai"], "title": "Envelope Control Enabled Probabilistic Shaping for Peak Power Constrained IM DD Systems", "comment": null, "summary": "Probabilistic shaping (PS) has attracted significant attention in\nintensity-modulation and direct-detection (IM-DD) systems. However, due to the\nunique system model and inherent constraints, the effective application of the\nPS technique is still an open question in IM-DD systems, particularly in\nsystems with memory effects. In this paper, a novel indirect PS scheme tailored\nfor peak power constrained (PPC) IM-DD systems is proposed. The key idea lies\nin strategically controlling the signal envelope to mitigate memory-induced\nimpairments, such as nonlinearity, overshoot, peak-to-average power ratio\nenhancement, etc. The proposed scheme incorporates a dynamic selective mapping\n(DSLM) mechanism at the transmitter, enabling an untypical bit-to-symbol\nmapping in which the current symbol is not only determined by the current bits\npattern but also by previously generated symbols within a specified memory\nlength. At the receiver side, a turbo equalizer with a modified M-BCJR\nalgorithm is proposed to achieve the recovery of ambiguous bits induced by\nDSLM. Experimental verification in a 56GBaud PAM8 system demonstrates that the\nproposed scheme exhibits 1dB receiver sensitivity improvement over 2km\nsingle-mode fiber transmission. In addition, the proposed scheme has also been\ndemonstrated to be compatible with the typical probabilistic amplitude shaping\narchitecture, enabling a simple and fine-granularity rate adaptation\ncapability. To the best of our knowledge, this work opens a new sight for the\napplication of the PS technique in PPC IM-DD systems with memory effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5cf0\u503c\u529f\u7387\u53d7\u9650IM-DD\u7cfb\u7edf\u7684\u95f4\u63a5\u6982\u7387\u6574\u5f62\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6027\u6620\u5c04\u548c\u4fee\u6539\u7684M-BCJR\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u7531\u4e8eIM-DD\u7cfb\u7edf\u7684\u72ec\u7279\u6a21\u578b\u548c\u56fa\u6709\u7ea6\u675f\uff0c\u6982\u7387\u6574\u5f62\u6280\u672f\u7684\u6709\u6548\u5e94\u7528\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u8bb0\u5fc6\u6548\u5e94\u7684\u7cfb\u7edf\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u6027\u6620\u5c04\u673a\u5236\uff08DSLM\uff09\uff0c\u7ed3\u5408\u4fee\u6539\u7684M-BCJR\u7b97\u6cd5\u7684turbo\u5747\u8861\u5668\uff0c\u4ee5\u89e3\u51b3\u8bb0\u5fc6\u6548\u5e94\u5f15\u8d77\u7684\u4fe1\u53f7\u635f\u4f24\u3002", "result": "\u572856GBaud PAM8\u7cfb\u7edf\u4e2d\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63a5\u6536\u7075\u654f\u5ea6\u63d0\u5347\u4e861dB\uff0c\u4e14\u517c\u5bb9\u5178\u578b\u6982\u7387\u5e45\u5ea6\u6574\u5f62\u67b6\u6784\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6982\u7387\u6574\u5f62\u6280\u672f\u5728\u5177\u6709\u8bb0\u5fc6\u6548\u5e94\u7684IM-DD\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.17786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17786", "abs": "https://arxiv.org/abs/2507.17786", "authors": ["Florian Sobieczky", "Alfredo Lopez", "Erika Dudkin", "Christopher Lackner", "Matthias Hochsteger", "Bernhard Scheichl", "Helmut Sobieczky"], "title": "Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation", "comment": null, "summary": "We introduce a reinforcement learning (RL) based adaptive optimization\nalgorithm for aerodynamic shape optimization focused on dimensionality\nreduction. The form in which RL is applied here is that of a surrogate-based,\nactor-critic policy evaluation MCMC approach allowing for temporal 'freezing'\nof some of the parameters to be optimized. The goals are to minimize\ncomputational effort, and to use the observed optimization results for\ninterpretation of the discovered extrema in terms of their role in achieving\nthe desired flow-field.\n  By a sequence of local optimized parameter changes around intermediate CFD\nsimulations acting as ground truth, it is possible to speed up the global\noptimization if (a) the local neighbourhoods of the parameters in which the\nchanged parameters must reside are sufficiently large to compete with the\ngrid-sized steps and its large number of simulations, and (b) the estimates of\nthe rewards and costs on these neighbourhoods necessary for a good step-wise\nparameter adaption are sufficiently accurate. We give an example of a simple\nfluid-dynamical problem on which the method allows interpretation in the sense\nof a feature importance scoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u964d\u7ef4\u7684\u6c14\u52a8\u5f62\u72b6\u4f18\u5316\uff0c\u901a\u8fc7\u4ee3\u7406\u6a21\u578b\u548cMCMC\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u89e3\u91ca\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u76ee\u6807\u662f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u89e3\u91ca\u4f18\u5316\u7ed3\u679c\u5728\u6d41\u573a\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684actor-critic\u7b56\u7565\u8bc4\u4f30MCMC\u65b9\u6cd5\uff0c\u5141\u8bb8\u90e8\u5206\u53c2\u6570\u4e34\u65f6\u51bb\u7ed3\uff0c\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u53c2\u6570\u53d8\u5316\u52a0\u901f\u5168\u5c40\u4f18\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u7b80\u5355\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\u5c55\u793a\u4e86\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u7684\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u548c\u51c6\u786e\u7684\u5956\u52b1\u4f30\u8ba1\uff0c\u6709\u6548\u52a0\u901f\u4e86\u5168\u5c40\u4f18\u5316\u5e76\u63d0\u4f9b\u4e86\u7ed3\u679c\u89e3\u91ca\u3002"}}
{"id": "2507.17796", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.17796", "abs": "https://arxiv.org/abs/2507.17796", "authors": ["Nicholas A. Pearson", "Francesca Zanello", "Davide Russo", "Luca Bortolussi", "Francesca Cairoli"], "title": "CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series", "comment": "Accepted for Presentation at Runtime Verification 25", "summary": "We propose a novel framework that harnesses the power of generative\nartificial intelligence and copula-based modeling to address two critical\nchallenges in multivariate time-series analysis: delivering accurate\npredictions and enabling robust anomaly detection. Our method, Copula-based\nConformal Anomaly Identification for Multivariate Time-Series (CoCAI),\nleverages a diffusion-based model to capture complex dependencies within the\ndata, enabling high quality forecasting. The model's outputs are further\ncalibrated using a conformal prediction technique, yielding predictive regions\nwhich are statistically valid, i.e., cover the true target values with a\ndesired confidence level. Starting from these calibrated forecasts, robust\noutlier detection is performed by combining dimensionality reduction techniques\nwith copula-based modeling, providing a statistically grounded anomaly score.\nCoCAI benefits from an offline calibration phase that allows for minimal\noverhead during deployment and delivers actionable results rooted in\nestablished theoretical foundations. Empirical tests conducted on real\noperational data derived from water distribution and sewerage systems confirm\nCoCAI's effectiveness in accurately forecasting target sequences of data and in\nidentifying anomalous segments within them.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u5f0fAI\u548ccopula\u6a21\u578b\u7684\u65b0\u6846\u67b6CoCAI\uff0c\u7528\u4e8e\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u51c6\u786e\u9884\u6d4b\u548c\u9c81\u68d2\u5f02\u5e38\u68c0\u6d4b\u4e24\u5927\u6311\u6218\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u6355\u6349\u6570\u636e\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u7ed3\u5408\u4fdd\u5f62\u9884\u6d4b\u6280\u672f\u6821\u51c6\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u964d\u7ef4\u548ccopula\u6a21\u578b\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u6c34\u52a1\u7cfb\u7edf\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86CoCAI\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "CoCAI\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2507.18166", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.18166", "abs": "https://arxiv.org/abs/2507.18166", "authors": ["Jonas Elmiger", "Gian Marti", "Christoph Studer"], "title": "GNSS Jammer and Spoofer Mitigation via Multi-Antenna Processing", "comment": null, "summary": "Modern positioning relies on radio signals from global navigation satellite\nsystems (GNSS). Their low receive power renders these radio signals susceptible\nto jamming attacks, in which malicious transmitters emit strong interference to\ndisrupt signal acquisition. Moreover, GNSS are vulnerable to spoofing attacks,\nin which malicious transmitters mimic legitimate satellites by transmitting\nspurious GNSS signals. We propose SCHIEBER, a novel method for multi-antenna\nGNSS receivers that mitigates jammers as well as spoofers without requiring any\nprior knowledge of the receiver position or attack type: Jammers are mitigated\nduring signal acquisition using a recently developed adaptive spatial filtering\ntechnique. Spoofers are identified and rejected after signal acquisition using\na novel approach that tests the consistency of acquired signals by comparing\ntheir respective direction of arrival (DoA) and pseudorange estimates in a test\nthat is invariant with respect to the unknown receiver position. We demonstrate\nthe efficacy of our method using extensive simulations of a GPS L1 C/A system\nunder spoofing and jamming attacks.", "AI": {"tldr": "SCHIEBER\u662f\u4e00\u79cd\u9488\u5bf9\u591a\u5929\u7ebfGNSS\u63a5\u6536\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u7f13\u89e3\u5e72\u6270\u548c\u6b3a\u9a97\u653b\u51fb\u3002", "motivation": "\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf\uff08GNSS\uff09\u4fe1\u53f7\u6613\u53d7\u5e72\u6270\u548c\u6b3a\u9a97\u653b\u51fb\uff0c\u5bfc\u81f4\u5b9a\u4f4d\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u7a7a\u95f4\u6ee4\u6ce2\u6280\u672f\u7f13\u89e3\u5e72\u6270\uff0c\u5229\u7528\u4fe1\u53f7\u5230\u8fbe\u65b9\u5411\uff08DoA\uff09\u548c\u4f2a\u8ddd\u4f30\u8ba1\u7684\u4e00\u81f4\u6027\u68c0\u6d4b\u6b3a\u9a97\u4fe1\u53f7\u3002", "result": "\u5728GPS L1 C/A\u7cfb\u7edf\u7684\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5e72\u6270\u548c\u6b3a\u9a97\u653b\u51fb\u3002", "conclusion": "SCHIEBER\u4e3aGNSS\u63a5\u6536\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u6297\u5e72\u6270\u548c\u6297\u6b3a\u9a97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17787", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17787", "abs": "https://arxiv.org/abs/2507.17787", "authors": ["Neil He", "Hiren Madhu", "Ngoc Bui", "Menglin Yang", "Rex Ying"], "title": "Hyperbolic Deep Learning for Foundation Models: A Survey", "comment": "11 Pages, SIGKDD 2025", "summary": "Foundation models pre-trained on massive datasets, including large language\nmodels (LLMs), vision-language models (VLMs), and large multimodal models, have\ndemonstrated remarkable success in diverse downstream tasks. However, recent\nstudies have shown fundamental limitations of these models: (1) limited\nrepresentational capacity, (2) lower adaptability, and (3) diminishing\nscalability. These shortcomings raise a critical question: is Euclidean\ngeometry truly the optimal inductive bias for all foundation models, or could\nincorporating alternative geometric spaces enable models to better align with\nthe intrinsic structure of real-world data and improve reasoning processes?\nHyperbolic spaces, a class of non-Euclidean manifolds characterized by\nexponential volume growth with respect to distance, offer a mathematically\ngrounded solution. These spaces enable low-distortion embeddings of\nhierarchical structures (e.g., trees, taxonomies) and power-law distributions\nwith substantially fewer dimensions compared to Euclidean counterparts. Recent\nadvances have leveraged these properties to enhance foundation models,\nincluding improving LLMs' complex reasoning ability, VLMs' zero-shot\ngeneralization, and cross-modal semantic alignment, while maintaining parameter\nefficiency. This paper provides a comprehensive review of hyperbolic neural\nnetworks and their recent development for foundation models. We further outline\nkey challenges and research directions to advance the field.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u53cc\u66f2\u7a7a\u95f4\u4f5c\u4e3a\u6539\u8fdb\u51e0\u4f55\u5f52\u7eb3\u504f\u7f6e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7efc\u8ff0\u4e86\u53cc\u66f2\u795e\u7ecf\u7f51\u7edc\u53ca\u5176\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8868\u793a\u80fd\u529b\u6709\u9650\u3001\u9002\u5e94\u6027\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7814\u7a76\u53cc\u66f2\u7a7a\u95f4\u662f\u5426\u80fd\u66f4\u597d\u5730\u5bf9\u9f50\u73b0\u5b9e\u6570\u636e\u7684\u56fa\u6709\u7ed3\u6784\u3002", "method": "\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u7684\u6570\u5b66\u7279\u6027\uff08\u5982\u6307\u6570\u4f53\u79ef\u589e\u957f\uff09\uff0c\u6539\u8fdb\u57fa\u7840\u6a21\u578b\u7684\u5d4c\u5165\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u53cc\u66f2\u7a7a\u95f4\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3001\u96f6\u6837\u672c\u6cdb\u5316\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u3002", "conclusion": "\u53cc\u66f2\u7a7a\u95f4\u4e3a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5173\u952e\u6311\u6218\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.18366", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.18366", "abs": "https://arxiv.org/abs/2507.18366", "authors": ["Lakshmana Sri Harsha Nemani", "P. K. Srijith", "Tomasz Ku\u015bmierczyk"], "title": "Efficient Uncertainty in LLMs through Evidential Knowledge Distillation", "comment": null, "summary": "Accurate uncertainty quantification remains a key challenge for standard\nLLMs, prompting the adoption of Bayesian and ensemble-based methods. However,\nsuch methods typically necessitate computationally expensive sampling,\ninvolving multiple forward passes to effectively estimate predictive\nuncertainty.\n  In this paper, we introduce a novel approach enabling efficient and effective\nuncertainty estimation in LLMs without sacrificing performance. Specifically,\nwe distill uncertainty-aware teacher models - originally requiring multiple\nforward passes - into compact student models sharing the same architecture but\nfine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct\ndistillation strategies: one in which the student employs traditional\nsoftmax-based outputs, and another in which the student leverages\nDirichlet-distributed outputs to explicitly model epistemic uncertainty via\nevidential learning.\n  Empirical evaluations on classification datasets demonstrate that such\nstudents can achieve comparable or superior predictive and uncertainty\nquantification performance relative to their teacher models, while critically\nrequiring only a single forward pass. To our knowledge, this is the first\ndemonstration that immediate and robust uncertainty quantification can be\nachieved in LLMs through evidential distillation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8d1d\u53f6\u65af\u548c\u96c6\u6210\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u6807\u51c6LLMs\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u901a\u8fc7\u84b8\u998f\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6559\u5e08\u6a21\u578b\u5230\u5b66\u751f\u6a21\u578b\uff0c\u91c7\u7528LoRA\u5fae\u8c03\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8esoftmax\u548cDirichlet\u5206\u5e03\u7684\u4e24\u79cd\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u751f\u6a21\u578b\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e0b\uff0c\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6027\u80fd\u4e0e\u6559\u5e08\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u901a\u8fc7\u8bc1\u636e\u84b8\u998f\u53ef\u4ee5\u5728LLMs\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002"}}
{"id": "2507.18167", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18167", "abs": "https://arxiv.org/abs/2507.18167", "authors": ["Yuxuan Wen", "Xiaoming Chen", "Maojun Zhang", "Zhaoyang Zhang"], "title": "ICWLM: A Multi-Task Wireless Large Model via In-Context Learning", "comment": null, "summary": "The rapid evolution of wireless communication technologies, particularly\nmassive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),\nintroduces significant network complexity and computational demands.\nSignificant research efforts have been made to improve physical layer\nperformance by resorting to deep learning (DL) methods, which, however, are\nusually task-specific and struggle with data scarcity and generalization. To\naddress these challenges, we propose a novel In-Context Wireless Large Model\n(ICWLM), a wireless-native foundation model designed for simultaneous\nmulti-task learning at the physical layer. Unlike conventional methods that\nadapt wireless data to pre-trained large language models (LLMs), ICWLM is\ntrained directly on large-scale, mixed wireless datasets from scratch. It\njointly solves multiple classical physical layer problems, including multi-user\nprecoding (sum-rate maximization and max-min SINR) and channel prediction. A\nkey innovation of ICWLM is its utilization of in-context learning (ICL),\nenabling the model to adapt to varying system configurations and channel\nconditions with minimal demonstration pairs, eliminating the need for extensive\nretraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm\nto dynamically balance the individual task losses during multi-task training,\nensuring efficient and stable learning across diverse objectives. Extensive\nsimulation results demonstrate that ICWLM achieves competitive performance\ncompared to task-specific methods while exhibiting remarkable generalization\ncapabilities to unseen system configurations. This work offers a promising\nparadigm for developing unified and adaptive AI models for future wireless\nnetworks, potentially reducing deployment complexity and enhancing intelligent\nresource management.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u65e0\u7ebf\u539f\u751f\u57fa\u7840\u6a21\u578bICWLM\uff0c\u7528\u4e8e\u7269\u7406\u5c42\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff08\u5982mMIMO\u548cmmWave\uff09\u5e26\u6765\u4e86\u7f51\u7edc\u590d\u6742\u6027\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4efb\u52a1\u5355\u4e00\u4e14\u96be\u4ee5\u6cdb\u5316\uff0c\u4e9f\u9700\u4e00\u79cd\u7edf\u4e00\u4e14\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faICWLM\u6a21\u578b\uff0c\u76f4\u63a5\u5728\u5927\u89c4\u6a21\u6df7\u5408\u65e0\u7ebf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u9002\u5e94\u4e0d\u540c\u7cfb\u7edf\u914d\u7f6e\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u6743\u91cd\u5e73\u5747\uff08DWA\uff09\u7b97\u6cd5\u5e73\u8861\u591a\u4efb\u52a1\u635f\u5931\u3002", "result": "ICWLM\u5728\u591a\u9879\u7269\u7406\u5c42\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u9002\u7528\u4e8e\u672a\u89c1\u8fc7\u7684\u7cfb\u7edf\u914d\u7f6e\u3002", "conclusion": "ICWLM\u4e3a\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u81ea\u9002\u5e94\u7684AI\u6a21\u578b\u8303\u5f0f\uff0c\u6709\u671b\u964d\u4f4e\u90e8\u7f72\u590d\u6742\u6027\u5e76\u63d0\u5347\u8d44\u6e90\u7ba1\u7406\u667a\u80fd\u6027\u3002"}}
{"id": "2507.17788", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17788", "abs": "https://arxiv.org/abs/2507.17788", "authors": ["Ali Vardasbi", "Gustavo Penha", "Claudia Hauff", "Hugues Bouchard"], "title": "Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking", "comment": null, "summary": "When using LLMs to rank items based on given criteria, or evaluate answers,\nthe order of candidate items can influence the model's final decision. This\nsensitivity to item positioning in a LLM's prompt is known as position bias.\nPrior research shows that this bias exists even in large models, though its\nseverity varies across models and tasks. In addition to position bias, LLMs\nalso exhibit varying degrees of low repetition consistency, where repeating the\nLLM call with the same candidate ordering can lead to different rankings. To\naddress both inconsistencies, a common approach is to prompt the model multiple\ntimes with different candidate orderings and aggregate the results via majority\nvoting. However, this repetition strategy, significantly increases\ncomputational costs. Extending prior findings, we observe that both the\ndirection -- favoring either the earlier or later candidate in the prompt --\nand magnitude of position bias across instances vary substantially, even within\na single dataset. This observation highlights the need for a per-instance\nmitigation strategy. To this end, we introduce a dynamic early-stopping method\nthat adaptively determines the number of repetitions required for each\ninstance. Evaluating our approach across three LLMs of varying sizes and on two\ntasks, namely re-ranking and alignment, we demonstrate that transitioning to a\ndynamic repetition strategy reduces the number of LLM calls by an average of\n81%, while preserving the accuracy. Furthermore, we propose a confidence-based\nadaptation to our early-stopping method, reducing LLM calls by an average of\n87% compared to static repetition, with only a slight accuracy trade-off\nrelative to our original early-stopping method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u65e9\u505c\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11LLM\u5728\u6392\u5e8f\u548c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u91cd\u590d\u8c03\u7528\u6b21\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "LLM\u5728\u6392\u5e8f\u548c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u548c\u91cd\u590d\u4e00\u81f4\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u591a\u6b21\u91cd\u590d\u8c03\u7528\u548c\u591a\u6570\u6295\u7968\u89e3\u51b3\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u65e9\u505c\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6bcf\u4e2a\u5b9e\u4f8b\u6240\u9700\u7684\u91cd\u590d\u8c03\u7528\u6b21\u6570\uff0c\u5e76\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "\u52a8\u6001\u7b56\u7565\u5e73\u5747\u51cf\u5c1181%\u7684LLM\u8c03\u7528\uff0c\u7f6e\u4fe1\u5ea6\u4f18\u5316\u540e\u51cf\u5c1187%\uff0c\u51c6\u786e\u6027\u635f\u5931\u8f83\u5c0f\u3002", "conclusion": "\u52a8\u6001\u65e9\u505c\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.18555", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.18555", "abs": "https://arxiv.org/abs/2507.18555", "authors": ["Jun'ichi Takeuchia", "Yoshinari Takeishia", "Noboru Muratab", "Kazushi Mimurac", "Ka Long Keith Hod", "Hiroshi Nagaoka"], "title": "Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights", "comment": null, "summary": "Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU\nnetworks with random hidden weight are argued. We discuss the relation between\nboth notions as a linear transformation and show that spectral decomposition of\nNTK with concrete forms of eigenfunctions with major eigenvalues. We also\nobtain an approximation formula of the functions presented by the 2-layer\nneural networks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e862\u5c42ReLU\u7f51\u7edc\u7684Fisher\u4fe1\u606f\u77e9\u9635\u4e0e\u795e\u7ecf\u6b63\u5207\u6838\uff08NTK\uff09\u7684\u5173\u7cfb\uff0c\u5c55\u793a\u4e86NTK\u7684\u8c31\u5206\u89e3\u53ca\u5176\u4e3b\u8981\u7279\u5f81\u503c\u7684\u5177\u4f53\u5f62\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e862\u5c42\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u7684\u8fd1\u4f3c\u516c\u5f0f\u3002", "motivation": "\u7814\u7a76Fisher\u4fe1\u606f\u77e9\u9635\u4e0eNTK\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u6df1\u5165\u7406\u89e32\u5c42ReLU\u7f51\u7edc\u7684\u8868\u793a\u80fd\u529b\u53ca\u5176\u4f18\u5316\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u53d8\u6362\u5206\u6790Fisher\u4fe1\u606f\u77e9\u9635\u4e0eNTK\u7684\u5173\u7cfb\uff0c\u5e76\u63a8\u5bfcNTK\u7684\u8c31\u5206\u89e3\u53ca\u5176\u7279\u5f81\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\u3002", "result": "\u5c55\u793a\u4e86NTK\u7684\u4e3b\u8981\u7279\u5f81\u503c\u53ca\u5176\u7279\u5f81\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e862\u5c42\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u7684\u8fd1\u4f3c\u516c\u5f0f\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Fisher\u4fe1\u606f\u77e9\u9635\u4e0eNTK\u7684\u7d27\u5bc6\u8054\u7cfb\uff0c\u4e3a\u7406\u89e32\u5c42ReLU\u7f51\u7edc\u7684\u8868\u793a\u548c\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.18370", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18370", "abs": "https://arxiv.org/abs/2507.18370", "authors": ["Morriel Kasher", "Michael Tinston", "Predrag Spasojevic"], "title": "Quantized Signal Recovery with Interference via Parametrized Look-Up Tables", "comment": "13 pages, 18 figures", "summary": "Efficient all-digital post-correction of low-resolution analog-to-digital\nconverters can be achieved by using Look-Up Tables (LUTs). The performance of a\nLUT can be optimized by incorporating a parametric model for the expected input\nsignal, noise level, and interference signals. We evaluate three analytical\nestimators for integration with parametrized LUTs, especially with applications\nto low-resolution, non-linear, or wideband quantizers. We also propose several\napproximations to improve tractability of the estimation problem for\nPhase-Shift Keyed input signals and Linear Frequency Modulated interference\nsignals. Simulated results validate the ability of our estimator to recover the\ninstantaneous value of the desired input signal in real-time with a high degree\nof accuracy. This includes cancellation of harmonic distortion that aliases\ninto the desired signal bandwidth from front-end saturation due to high-power\nout-of-band interference. Our estimators are shown to achieve a significant\ngain over conventional linear-filtering techniques while also being robust to\nchanges in input parameters, non-linear quantizers, and time-variant\ninterference sources. For a tone input quantized to 3 bits and estimated with a\nfixed 12-tap model order we achieve $>$10 dB improvement in Mean Square Error\nand $>$20 dBc improvement in Spurious-Free Dynamic Range.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u67e5\u627e\u8868\uff08LUT\uff09\u7684\u6570\u5b57\u540e\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u4f18\u5316\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u3001\u975e\u7ebf\u6027\u6216\u5bbd\u5e26\u91cf\u5316\u5668\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u6a21\u6570\u8f6c\u6362\u5668\u7684\u5b9e\u65f6\u6821\u6b63\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5b58\u5728\u566a\u58f0\u548c\u5e72\u6270\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u5206\u6790\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408\u53c2\u6570\u5316LUT\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u4fe1\u53f7\u7c7b\u578b\uff08\u5982PSK\u548cLFM\uff09\u63d0\u51fa\u8fd1\u4f3c\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u6062\u590d\u8f93\u5165\u4fe1\u53f7\uff0c\u663e\u8457\u6539\u5584\u5747\u65b9\u8bef\u5dee\u548c\u65e0\u6742\u6563\u52a8\u6001\u8303\u56f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7ebf\u6027\u6ee4\u6ce2\u6280\u672f\uff0c\u5bf9\u8f93\u5165\u53c2\u6570\u53d8\u5316\u3001\u975e\u7ebf\u6027\u91cf\u5316\u5668\u548c\u65f6\u53d8\u5e72\u6270\u6e90\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.17791", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17791", "abs": "https://arxiv.org/abs/2507.17791", "authors": ["Eduardo Aguilar-Bejarano", "Daniel Lea", "Karthikeyan Sivakumar", "Jimiama M. Mase", "Reza Omidvar", "Ruizhe Li", "Troy Kettle", "James Mitchell-White", "Morgan R Alexander", "David A Winkler", "Grazziela Figueredo"], "title": "Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data", "comment": "17 pages", "summary": "Helix is an open-source, extensible, Python-based software framework to\nfacilitate reproducible and interpretable machine learning workflows for\ntabular data. It addresses the growing need for transparent experimental data\nanalytics provenance, ensuring that the entire analytical process -- including\ndecisions around data transformation and methodological choices -- is\ndocumented, accessible, reproducible, and comprehensible to relevant\nstakeholders. The platform comprises modules for standardised data\npreprocessing, visualisation, machine learning model training, evaluation,\ninterpretation, results inspection, and model prediction for unseen data. To\nfurther empower researchers without formal training in data science to derive\nmeaningful and actionable insights, Helix features a user-friendly interface\nthat enables the design of computational experiments, inspection of outcomes,\nincluding a novel interpretation approach to machine learning decisions using\nlinguistic terms all within an integrated environment. Released under the MIT\nlicence, Helix is accessible via GitHub and PyPI, supporting community-driven\ndevelopment and promoting adherence to the FAIR principles.", "AI": {"tldr": "Helix\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u8868\u683c\u6570\u636e\u63d0\u4f9b\u53ef\u91cd\u73b0\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u6570\u636e\u9884\u5904\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u89e3\u91ca\u7b49\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u900f\u660e\u6027\u548c\u53ef\u91cd\u73b0\u6027\u7684\u9700\u6c42\uff0c\u786e\u4fdd\u5206\u6790\u8fc7\u7a0b\u7684\u6bcf\u4e00\u6b65\u90fd\u88ab\u8bb0\u5f55\u548c\u7406\u89e3\u3002", "method": "\u63d0\u4f9b\u6807\u51c6\u5316\u6a21\u5757\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u53ef\u89c6\u5316\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u548c\u89e3\u91ca\uff0c\u5e76\u914d\u5907\u7528\u6237\u53cb\u597d\u754c\u9762\u3002", "result": "Helix\u652f\u6301\u793e\u533a\u9a71\u52a8\u5f00\u53d1\uff0c\u9075\u5faaFAIR\u539f\u5219\uff0c\u5e2e\u52a9\u975e\u6570\u636e\u79d1\u5b66\u80cc\u666f\u7684\u7814\u7a76\u8005\u83b7\u5f97\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "conclusion": "Helix\u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002"}}
{"id": "2507.18561", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.18561", "abs": "https://arxiv.org/abs/2507.18561", "authors": ["Varsha Ramineni", "Hossein A. Rahmani", "Emine Yilmaz", "David Barber"], "title": "Beyond Internal Data: Constructing Complete Datasets for Fairness Testing", "comment": "9 pages, 6 figures", "summary": "As AI becomes prevalent in high-risk domains and decision-making, it is\nessential to test for potential harms and biases. This urgency is reflected by\nthe global emergence of AI regulations that emphasise fairness and adequate\ntesting, with some mandating independent bias audits. However, procuring the\nnecessary data for fairness testing remains a significant challenge.\nParticularly in industry settings, legal and privacy concerns restrict the\ncollection of demographic data required to assess group disparities, and\nauditors face practical and cultural challenges in gaining access to data.\nFurther, internal historical datasets are often insufficiently representative\nto identify real-world biases. This work focuses on evaluating classifier\nfairness when complete datasets including demographics are inaccessible. We\npropose leveraging separate overlapping datasets to construct complete\nsynthetic data that includes demographic information and accurately reflects\nthe underlying relationships between protected attributes and model features.\nWe validate the fidelity of the synthetic data by comparing it to real data,\nand empirically demonstrate that fairness metrics derived from testing on such\nsynthetic data are consistent with those obtained from real data. This work,\ntherefore, offers a path to overcome real-world data scarcity for fairness\ntesting, enabling independent, model-agnostic evaluation of fairness, and\nserving as a viable substitute where real data is limited.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u91cd\u53e0\u6570\u636e\u96c6\u6784\u5efa\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u516c\u5e73\u6027\u6d4b\u8bd5\u4e2d\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u516c\u5e73\u6027\u6d4b\u8bd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u83b7\u53d6\u5305\u542b\u4eba\u53e3\u7edf\u8ba1\u6570\u636e\u7684\u771f\u5b9e\u6570\u636e\u5b58\u5728\u6cd5\u5f8b\u548c\u9690\u79c1\u6311\u6218\u3002", "method": "\u901a\u8fc7\u91cd\u53e0\u6570\u636e\u96c6\u6784\u5efa\u5305\u542b\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u7684\u5408\u6210\u6570\u636e\uff0c\u5e76\u9a8c\u8bc1\u5176\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5408\u6210\u6570\u636e\u5f97\u51fa\u7684\u516c\u5e73\u6027\u6307\u6807\u4e0e\u771f\u5b9e\u6570\u636e\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u516c\u5e73\u6027\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002"}}
{"id": "2507.18587", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18587", "abs": "https://arxiv.org/abs/2507.18587", "authors": ["J\u00e9r\u00f4me Emery", "Ali Hasanzadeh Karkan", "Jean-Fran\u00e7ois Frigon", "Fran\u00e7ois Leduc-Primeau"], "title": "A Foundation Model for Massive MIMO Precoding with an Adaptive per-User Rate-Power Tradeoff", "comment": "6 pages, 3 figures. Accepted to the IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "summary": "Deep learning (DL) has emerged as a solution for precoding in massive\nmultiple-input multiple-output (mMIMO) systems due to its capacity to learn the\ncharacteristics of the propagation environment. However, training such a model\nrequires high-quality, local datasets at the deployment site, which are often\ndifficult to collect. We propose a transformer-based foundation model for mMIMO\nprecoding that seeks to minimize the energy consumption of the transmitter\nwhile dynamically adapting to per-user rate requirements. At equal energy\nconsumption, zero-shot deployment of the proposed foundation model\nsignificantly outperforms zero forcing, and approaches weighted minimum mean\nsquared error performance with 8x less complexity. To address model adaptation\nin data-scarce settings, we introduce a data augmentation method that finds\ntraining samples similar to the target distribution by computing the cosine\nsimilarity between the outputs of the pre-trained feature extractor. Our work\nenables the implementation of DL-based solutions in practice by addressing\nchallenges of data availability and training complexity. Moreover, the ability\nto dynamically configure per-user rate requirements can be leveraged by higher\nlevel resource allocation and scheduling algorithms for greater control over\nenergy efficiency, spectral efficiency and fairness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u57fa\u7840\u6a21\u578b\u7528\u4e8emMIMO\u9884\u7f16\u7801\uff0c\u51cf\u5c11\u53d1\u5c04\u673a\u80fd\u8017\u5e76\u52a8\u6001\u9002\u5e94\u7528\u6237\u901f\u7387\u9700\u6c42\uff0c\u96f6\u6837\u672c\u90e8\u7f72\u6027\u80fd\u4f18\u4e8e\u96f6\u5f3a\u8feb\uff0c\u63a5\u8fd1\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u6027\u80fd\u4e14\u590d\u6742\u5ea6\u4f4e8\u500d\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728mMIMO\u7cfb\u7edf\u4e2d\u9884\u7f16\u7801\u65f6\u9700\u9ad8\u8d28\u91cf\u672c\u5730\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u3002", "method": "\u4f7f\u7528Transformer\u57fa\u7840\u6a21\u578b\uff0c\u5f15\u5165\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u8f93\u51fa\u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u6027\u627e\u5230\u7c7b\u4f3c\u76ee\u6807\u5206\u5e03\u7684\u6837\u672c\u3002", "result": "\u96f6\u6837\u672c\u90e8\u7f72\u6027\u80fd\u4f18\u4e8e\u96f6\u5f3a\u8feb\uff0c\u63a5\u8fd1\u52a0\u6743\u6700\u5c0f\u5747\u65b9\u8bef\u5dee\u6027\u80fd\u4e14\u590d\u6742\u5ea6\u4f4e8\u500d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u89e3\u51b3\u6570\u636e\u53ef\u7528\u6027\u548c\u8bad\u7ec3\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4f7f\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u89e3\u51b3\u65b9\u6848\u5728\u5b9e\u8df5\u4e2d\u53ef\u884c\uff0c\u540c\u65f6\u652f\u6301\u52a8\u6001\u914d\u7f6e\u7528\u6237\u901f\u7387\u9700\u6c42\uff0c\u63d0\u5347\u80fd\u6548\u3001\u9891\u8c31\u6548\u7387\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2507.17795", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17795", "abs": "https://arxiv.org/abs/2507.17795", "authors": ["Shiyuan Zhang", "Tong Li", "Zhu Xiao", "Hongyang Du", "Kaibin Huang"], "title": "LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction", "comment": "14 pages, 9 figures", "summary": "Service-level mobile traffic prediction for individual users is essential for\nnetwork efficiency and quality of service enhancement. However, current\nprediction methods are limited in their adaptability across different urban\nenvironments and produce inaccurate results due to the high uncertainty in\npersonal traffic patterns, the lack of detailed environmental context, and the\ncomplex dependencies among different network services. These challenges demand\nadvanced modeling techniques that can capture dynamic traffic distributions and\nrich environmental features. Inspired by the recent success of diffusion models\nin distribution modeling and Large Language Models (LLMs) in contextual\nunderstanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model\n(LSDM). LSDM integrates the generative power of diffusion models with the\nadaptive learning capabilities of transformers, augmented by the ability to\ncapture multimodal environmental information for modeling service-level\npatterns and dynamics. Extensive evaluations on real-world service-level\ndatasets demonstrate that the model excels in traffic usage predictions,\nshowing outstanding generalization and adaptability. After incorporating\ncontextual information via LLM, the performance improves by at least 2.83% in\nterms of the coefficient of determination. Compared to models of a similar\ntype, such as CSDI, the root mean squared error can be reduced by at least\n8.29%. The code and dataset will be available at:\nhttps://github.com/SoftYuaneR/LSDM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65f6\u7a7a\u6269\u6563\u6a21\u578b\uff08LSDM\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u4e2a\u4f53\u7528\u6237\u670d\u52a1\u7ea7\u79fb\u52a8\u6d41\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5728\u4e0d\u540c\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u4e14\u56e0\u4e2a\u4eba\u6d41\u91cf\u6a21\u5f0f\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u3001\u73af\u5883\u4e0a\u4e0b\u6587\u7f3a\u5931\u53ca\u7f51\u7edc\u670d\u52a1\u95f4\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u5bfc\u81f4\u9884\u6d4b\u4e0d\u51c6\u786e\u3002", "method": "LSDM\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u548cTransformer\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u80fd\u529b\uff0c\u5229\u7528LLM\u6355\u6349\u591a\u6a21\u6001\u73af\u5883\u4fe1\u606f\uff0c\u5efa\u6a21\u670d\u52a1\u7ea7\u6d41\u91cf\u6a21\u5f0f\u548c\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLSDM\u5728\u6d41\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\u3002\u52a0\u5165LLM\u540e\uff0c\u51b3\u5b9a\u7cfb\u6570\u81f3\u5c11\u63d0\u53472.83%\uff0c\u4e0e\u7c7b\u4f3c\u6a21\u578b\uff08\u5982CSDI\uff09\u76f8\u6bd4\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u81f3\u5c11\u964d\u4f4e8.29%\u3002", "conclusion": "LSDM\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548cLLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u7ea7\u6d41\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u7f51\u7edc\u6548\u7387\u548c\u670d\u52a1\u8d28\u91cf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.17797", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17797", "abs": "https://arxiv.org/abs/2507.17797", "authors": ["Shubham Toshniwal", "Ivan Sorokin", "Aleksander Ficek", "Ivan Moshkov", "Igor Gitman"], "title": "GenSelect: A Generative Approach to Best-of-N", "comment": "Presented at the 2nd AI for MATH Workshop @ ICML", "summary": "Generative reward models with parallel sampling have enabled effective\ntest-time scaling for reasoning tasks. Current approaches employ pointwise\nscoring of individual solutions or pairwise comparisons. However, pointwise\nmethods underutilize LLMs' comparative abilities, while pairwise methods scale\ninefficiently with larger sampling budgets. We introduce GenSelect, where the\nLLM uses long reasoning to select the best solution among N candidates. This\nleverages LLMs' comparative strengths while scaling efficiently across parallel\nsampling budgets. For math reasoning, we demonstrate that reasoning models,\nsuch as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing\nscoring approaches with simple prompting.", "AI": {"tldr": "GenSelect\u5229\u7528LLM\u7684\u957f\u63a8\u7406\u80fd\u529b\u4eceN\u4e2a\u5019\u9009\u65b9\u6848\u4e2d\u9009\u62e9\u6700\u4f73\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u70b9\u5bf9\u70b9\u6216\u6210\u5bf9\u8bc4\u5206\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u70b9\u5bf9\u70b9\u6216\u6210\u5bf9\u6bd4\u8f83\uff09\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u7684\u6bd4\u8f83\u80fd\u529b\u6216\u6269\u5c55\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faGenSelect\uff0c\u901a\u8fc7\u957f\u63a8\u7406\u4ece\u5e76\u884c\u91c7\u6837\u7684\u5019\u9009\u65b9\u6848\u4e2d\u9009\u62e9\u6700\u4f73\u89e3\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cGenSelect\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u8bc4\u5206\u65b9\u6cd5\u3002", "conclusion": "GenSelect\u7ed3\u5408\u4e86LLM\u7684\u6bd4\u8f83\u4f18\u52bf\u548c\u9ad8\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e76\u884c\u91c7\u6837\u4efb\u52a1\u3002"}}
{"id": "2507.17798", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17798", "abs": "https://arxiv.org/abs/2507.17798", "authors": ["Kenta Shiraishi", "Yuka Muto", "Atsushi Okazaki", "Shunji Kotsuki"], "title": "Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism", "comment": null, "summary": "High-resolution (HR) precipitation prediction is essential for reducing\ndamage from stationary and localized heavy rainfall; however, HR precipitation\nforecasts using process-driven numerical weather prediction models remains\nchallenging. This study proposes using Wasserstein Generative Adversarial\nNetwork (WGAN) to perform precipitation downscaling with an optimal transport\ncost. In contrast to a conventional neural network trained with mean squared\nerror, the WGAN generated visually realistic precipitation fields with\nfine-scale structures even though the WGAN exhibited slightly lower performance\non conventional evaluation metrics. The learned critic of WGAN correlated well\nwith human perceptual realism. Case-based analysis revealed that large\ndiscrepancies in critic scores can help identify both unrealistic WGAN outputs\nand potential artifacts in the reference data. These findings suggest that the\nWGAN framework not only improves perceptual realism in precipitation\ndownscaling but also offers a new perspective for evaluating and\nquality-controlling precipitation datasets.", "AI": {"tldr": "\u4f7f\u7528Wasserstein\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08WGAN\uff09\u8fdb\u884c\u964d\u6c34\u964d\u5c3a\u5ea6\uff0c\u63d0\u9ad8\u89c6\u89c9\u771f\u5b9e\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u964d\u6c34\u6570\u636e\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u964d\u6c34\u9884\u6d4b\u5bf9\u51cf\u5c11\u5c40\u90e8\u5f3a\u964d\u96e8\u7684\u635f\u5bb3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6570\u503c\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5728\u6b64\u65b9\u9762\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528WGAN\u8fdb\u884c\u964d\u6c34\u964d\u5c3a\u5ea6\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6210\u672c\u751f\u6210\u964d\u6c34\u573a\uff0c\u4e0e\u4f20\u7edf\u5747\u65b9\u8bef\u5dee\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5bf9\u6bd4\u3002", "result": "WGAN\u751f\u6210\u7684\u964d\u6c34\u573a\u89c6\u89c9\u4e0a\u66f4\u771f\u5b9e\uff0c\u5c3d\u7ba1\u5728\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u7565\u4f4e\uff1b\u5176\u5224\u522b\u5668\u8bc4\u5206\u4e0e\u4eba\u7c7b\u611f\u77e5\u771f\u5b9e\u6027\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "WGAN\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u964d\u6c34\u964d\u5c3a\u5ea6\u7684\u89c6\u89c9\u771f\u5b9e\u6027\uff0c\u8fd8\u4e3a\u964d\u6c34\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u548c\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.17848", "categories": ["cs.LG", "cs.AI", "cs.GT", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.17848", "abs": "https://arxiv.org/abs/2507.17848", "authors": ["Lijun Wu", "Dong Hao", "Zhiyi Fan"], "title": "Explainable Graph Neural Networks via Structural Externalities", "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved outstanding performance across a\nwide range of graph-related tasks. However, their \"black-box\" nature poses\nsignificant challenges to their explainability, and existing methods often fail\nto effectively capture the intricate interaction patterns among nodes within\nthe network. In this work, we propose a novel explainability framework,\nGraphEXT, which leverages cooperative game theory and the concept of social\nexternalities. GraphEXT partitions graph nodes into coalitions, decomposing the\noriginal graph into independent subgraphs. By integrating graph structure as an\nexternality and incorporating the Shapley value under externalities, GraphEXT\nquantifies node importance through their marginal contributions to GNN\npredictions as the nodes transition between coalitions. Unlike traditional\nShapley value-based methods that primarily focus on node attributes, our\nGraphEXT places greater emphasis on the interactions among nodes and the impact\nof structural changes on GNN predictions. Experimental studies on both\nsynthetic and real-world datasets show that GraphEXT outperforms existing\nbaseline methods in terms of fidelity across diverse GNN architectures ,\nsignificantly enhancing the explainability of GNN models.", "AI": {"tldr": "GraphEXT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5408\u4f5c\u535a\u5f08\u8bba\u548c\u793e\u4f1a\u5916\u90e8\u6027\u7684\u65b0\u578bGNN\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8282\u70b9\u5212\u5206\u4e3a\u8054\u76df\u5e76\u91cf\u5316\u5176\u8fb9\u9645\u8d21\u732e\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u6a21\u578b\u7684\u89e3\u91ca\u6027\u3002", "motivation": "GNN\u7684\u9ed1\u7bb1\u7279\u6027\u53ca\u5176\u96be\u4ee5\u6355\u6349\u8282\u70b9\u95f4\u590d\u6742\u4ea4\u4e92\u6a21\u5f0f\u7684\u95ee\u9898\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "method": "GraphEXT\u5229\u7528\u5408\u4f5c\u535a\u5f08\u8bba\u548c\u793e\u4f1a\u5916\u90e8\u6027\u6982\u5ff5\uff0c\u5c06\u8282\u70b9\u5212\u5206\u4e3a\u8054\u76df\uff0c\u5e76\u901a\u8fc7Shapley\u503c\u91cf\u5316\u8282\u70b9\u5728\u9884\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGraphEXT\u5728\u591a\u79cdGNN\u67b6\u6784\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "GraphEXT\u901a\u8fc7\u5f3a\u8c03\u8282\u70b9\u4ea4\u4e92\u548c\u7ed3\u6784\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u4e3aGNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u91ca\u6027\u6846\u67b6\u3002"}}
{"id": "2507.17876", "categories": ["cs.LG", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.17876", "abs": "https://arxiv.org/abs/2507.17876", "authors": ["R\u0131za \u00d6z\u00e7elik", "Sarah de Ruiter", "Francesca Grisoni"], "title": "Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic", "comment": null, "summary": "The scarcity of molecules with desirable properties (i.e., 'positive'\nmolecules) is an inherent bottleneck for generative molecule design. To\nsidestep such obstacle, here we propose molecular task arithmetic: training a\nmodel on diverse and abundant negative examples to learn 'property directions'\n$--$ without accessing any positively labeled data $--$ and moving models in\nthe opposite property directions to generate positive molecules. When analyzed\non 20 zero-shot design experiments, molecular task arithmetic generated more\ndiverse and successful designs than models trained on positive molecules.\nMoreover, we employed molecular task arithmetic in dual-objective and few-shot\ndesign tasks. We find that molecular task arithmetic can consistently increase\nthe diversity of designs while maintaining desirable design properties. With\nits simplicity, data efficiency, and performance, molecular task arithmetic\nbears the potential to become the $\\textit{de-facto}$ transfer learning\nstrategy for de novo molecule design.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5b50\u4efb\u52a1\u7b97\u672f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1f\u6837\u672c\u8bad\u7ec3\u751f\u6210\u6b63\u5206\u5b50\uff0c\u65e0\u9700\u6b63\u6837\u672c\u6570\u636e\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u591a\u6837\u6027\u548c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5206\u5b50\u8bbe\u8ba1\u4e2d\u6b63\u6837\u672c\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6b63\u6837\u672c\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8d1f\u6837\u672c\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u5c5e\u6027\u65b9\u5411\uff0c\u53cd\u5411\u79fb\u52a8\u6a21\u578b\u751f\u6210\u6b63\u5206\u5b50\u3002", "result": "\u572820\u4e2a\u96f6\u6837\u672c\u8bbe\u8ba1\u5b9e\u9a8c\u4e2d\uff0c\u751f\u6210\u66f4\u591a\u6837\u4e14\u6210\u529f\u7684\u5206\u5b50\uff1b\u5728\u53cc\u76ee\u6807\u548c\u5c11\u6837\u672c\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5206\u5b50\u4efb\u52a1\u7b97\u672f\u56e0\u5176\u7b80\u5355\u3001\u9ad8\u6548\u548c\u6027\u80fd\u4f18\u8d8a\uff0c\u6709\u671b\u6210\u4e3a\u5206\u5b50\u8bbe\u8ba1\u7684\u6807\u51c6\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002"}}
{"id": "2507.17887", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.17887", "abs": "https://arxiv.org/abs/2507.17887", "authors": ["Wonjae Lee", "Taeyoung Kim", "Hyungbin Park"], "title": "Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments", "comment": null, "summary": "This paper introduces an operator-based neural network, the mirror-padded\nFourier neural operator (MFNO), designed to learn the dynamics of stochastic\nsystems. MFNO extends the standard Fourier neural operator (FNO) by\nincorporating mirror padding, enabling it to handle non-periodic inputs. We\nrigorously prove that MFNOs can approximate solutions of path-dependent\nstochastic differential equations and Lipschitz transformations of fractional\nBrownian motions to an arbitrary degree of accuracy. Our theoretical analysis\nbuilds on Wong--Zakai type theorems and various approximation techniques.\nEmpirically, the MFNO exhibits strong resolution generalization--a property\nrarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.\nFurthermore, our model achieves performance that is comparable or superior to\nthese baselines while offering significantly faster sample path generation than\nclassical numerical schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b97\u5b50\u7684\u795e\u7ecf\u7f51\u7edcMFNO\uff0c\u7528\u4e8e\u5b66\u4e60\u968f\u673a\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u955c\u50cf\u586b\u5145\u5904\u7406\u975e\u5468\u671f\u6027\u8f93\u5165\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u975e\u5468\u671f\u6027\u8f93\u5165\u548c\u968f\u673a\u7cfb\u7edf\u52a8\u529b\u5b66\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0cMFNO\u901a\u8fc7\u955c\u50cf\u586b\u5145\u6269\u5c55FNO\uff0c\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MFNO\u7ed3\u5408\u955c\u50cf\u586b\u5145\u6280\u672f\uff0c\u6269\u5c55\u6807\u51c6FNO\uff0c\u5229\u7528Wong-Zakai\u5b9a\u7406\u548c\u8fd1\u4f3c\u6280\u672f\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "MFNO\u5728\u5206\u8fa8\u7387\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u4f18\u4e8eLSTM\u3001TCN\u548cDeepONet\uff0c\u4e14\u751f\u6210\u6837\u672c\u8def\u5f84\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "MFNO\u4e3a\u968f\u673a\u7cfb\u7edf\u52a8\u529b\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.17895", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.17895", "abs": "https://arxiv.org/abs/2507.17895", "authors": ["Amrith Setlur", "Pratiksha Thaker", "Jonathan Ullman"], "title": "Lower Bounds for Public-Private Learning under Distribution Shift", "comment": "Preprint", "summary": "The most effective differentially private machine learning algorithms in\npractice rely on an additional source of purportedly public data. This paradigm\nis most interesting when the two sources combine to be more than the sum of\ntheir parts. However, there are settings such as mean estimation where we have\nstrong lower bounds, showing that when the two data sources have the same\ndistribution, there is no complementary value to combining the two data\nsources. In this work we extend the known lower bounds for public-private\nlearning to setting where the two data sources exhibit significant distribution\nshift. Our results apply to both Gaussian mean estimation where the two\ndistributions have different means, and to Gaussian linear regression where the\ntwo distributions exhibit parameter shift. We find that when the shift is small\n(relative to the desired accuracy), either public or private data must be\nsufficiently abundant to estimate the private parameter. Conversely, when the\nshift is large, public data provides no benefit.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u516c\u5f00\u6570\u636e\u4e0e\u79c1\u6709\u6570\u636e\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u65f6\u7684\u4e92\u8865\u4ef7\u503c\uff0c\u53d1\u73b0\u5c0f\u5dee\u5f02\u65f6\u9700\u5145\u8db3\u6570\u636e\uff0c\u5927\u5dee\u5f02\u65f6\u516c\u5f00\u6570\u636e\u65e0\u76ca\u3002", "motivation": "\u63a2\u8ba8\u5728\u516c\u5f00\u6570\u636e\u4e0e\u79c1\u6709\u6570\u636e\u5206\u5e03\u5b58\u5728\u663e\u8457\u5dee\u5f02\u65f6\uff0c\u4e24\u8005\u7ed3\u5408\u7684\u4e92\u8865\u4ef7\u503c\u53ca\u5176\u5bf9\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u5f71\u54cd\u3002", "method": "\u6269\u5c55\u4e86\u516c\u5f00-\u79c1\u6709\u5b66\u4e60\u7684\u4e0b\u754c\u5206\u6790\uff0c\u5e94\u7528\u4e8e\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u548c\u7ebf\u6027\u56de\u5f52\u4e2d\u5206\u5e03\u5dee\u5f02\u7684\u60c5\u51b5\u3002", "result": "\u5c0f\u5dee\u5f02\u65f6\u9700\u5145\u8db3\u6570\u636e\u4f30\u8ba1\u79c1\u6709\u53c2\u6570\uff0c\u5927\u5dee\u5f02\u65f6\u516c\u5f00\u6570\u636e\u65e0\u76ca\u3002", "conclusion": "\u516c\u5f00\u6570\u636e\u4e0e\u79c1\u6709\u6570\u636e\u7684\u5206\u5e03\u5dee\u5f02\u5927\u5c0f\u51b3\u5b9a\u4e86\u5176\u4e92\u8865\u4ef7\u503c\uff0c\u5c0f\u5dee\u5f02\u9700\u66f4\u591a\u6570\u636e\uff0c\u5927\u5dee\u5f02\u5219\u516c\u5f00\u6570\u636e\u65e0\u6548\u3002"}}
{"id": "2507.17903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17903", "abs": "https://arxiv.org/abs/2507.17903", "authors": ["Obaidullah Zaland", "Chanh Nguyen", "Florian T. Pokorny", "Monowar Bhuyan"], "title": "Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges", "comment": "Accepted for Presentation at IEEE International Conference on Machine\n  Learning and Cybernetics (ICMLC) 2025", "summary": "Federated Learning (FL) is an emerging distributed machine learning paradigm,\nwhere the collaborative training of a model involves dynamic participation of\ndevices to achieve broad objectives. In contrast, classical machine learning\n(ML) typically requires data to be located on-premises for training, whereas FL\nleverages numerous user devices to train a shared global model without the need\nto share private data. Current robotic manipulation tasks are constrained by\nthe individual capabilities and speed of robots due to limited low-latency\ncomputing resources. Consequently, the concept of cloud robotics has emerged,\nallowing robotic applications to harness the flexibility and reliability of\ncomputing resources, effectively alleviating their computational demands across\nthe cloud-edge continuum. Undoubtedly, within this distributed computing\ncontext, as exemplified in cloud robotic manipulation scenarios, FL offers\nmanifold advantages while also presenting several challenges and opportunities.\nIn this paper, we present fundamental concepts of FL and their connection to\ncloud robotic manipulation. Additionally, we envision the opportunities and\nchallenges associated with realizing efficient and reliable cloud robotic\nmanipulation at scale through FL, where researchers adopt to design and verify\nFL models in either centralized or decentralized settings.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u662f\u4e00\u79cd\u65b0\u5174\u7684\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8bbe\u5907\u53c2\u4e0e\u534f\u4f5c\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u5171\u4eab\u79c1\u6709\u6570\u636e\u3002\u672c\u6587\u63a2\u8ba8\u4e86FL\u4e0e\u4e91\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8054\u7cfb\uff0c\u5e76\u5c55\u671b\u4e86\u5176\u5728\u5927\u89c4\u6a21\u9ad8\u6548\u53ef\u9760\u5e94\u7528\u4e2d\u7684\u673a\u9047\u4e0e\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u9700\u8981\u6570\u636e\u96c6\u4e2d\u5b58\u50a8\uff0c\u800cFL\u901a\u8fc7\u5206\u5e03\u5f0f\u8bbe\u5907\u8bad\u7ec3\u5171\u4eab\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002\u4e91\u673a\u5668\u4eba\u64cd\u4f5c\u53d7\u9650\u4e8e\u8ba1\u7b97\u8d44\u6e90\uff0cFL\u4e3a\u5176\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4ecb\u7ecd\u4e86FL\u7684\u57fa\u672c\u6982\u5ff5\u53ca\u5176\u4e0e\u4e91\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5173\u8054\uff0c\u63a2\u8ba8\u4e86\u5728\u96c6\u4e2d\u5f0f\u6216\u5206\u6563\u5f0f\u73af\u5883\u4e2d\u8bbe\u8ba1\u548c\u9a8c\u8bc1FL\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "result": "FL\u4e3a\u4e91\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u6548\u7387\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "FL\u5728\u4e91\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u5176\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.17907", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17907", "abs": "https://arxiv.org/abs/2507.17907", "authors": ["Phu Thien Nguyen", "Yousef Heider", "Dennis M. Kochmann", "Fadi Aldakheel"], "title": "Deep learning-aided inverse design of porous metamaterials", "comment": "31 pages, 29 figures", "summary": "The ultimate aim of the study is to explore the inverse design of porous\nmetamaterials using a deep learning-based generative framework. Specifically,\nwe develop a property-variational autoencoder (pVAE), a variational autoencoder\n(VAE) augmented with a regressor, to generate structured metamaterials with\ntailored hydraulic properties, such as porosity and permeability. While this\nwork uses the lattice Boltzmann method (LBM) to generate intrinsic permeability\ntensor data for limited porous microstructures, a convolutional neural network\n(CNN) is trained using a bottom-up approach to predict effective hydraulic\nproperties. This significantly reduces the computational cost compared to\ndirect LBM simulations. The pVAE framework is trained on two datasets: a\nsynthetic dataset of artificial porous microstructures and CT-scan images of\nvolume elements from real open-cell foams. The encoder-decoder architecture of\nthe VAE captures key microstructural features, mapping them into a compact and\ninterpretable latent space for efficient structure-property exploration. The\nstudy provides a detailed analysis and interpretation of the latent space,\ndemonstrating its role in structure-property mapping, interpolation, and\ninverse design. This approach facilitates the generation of new metamaterials\nwith desired properties. The datasets and codes used in this study will be made\nopen-access to support further research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u751f\u6210\u6846\u67b6\u63a2\u7d22\u591a\u5b54\u8d85\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08pVAE\uff09\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u5b9a\u5236\u6c34\u529b\u7279\u6027\u7684\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u591a\u5b54\u8d85\u6750\u6599\u7684\u9006\u5411\u8bbe\u8ba1\uff0c\u4ee5\u751f\u6210\u5177\u6709\u7279\u5b9a\u6c34\u529b\u7279\u6027\uff08\u5982\u5b54\u9699\u7387\u548c\u6e17\u900f\u7387\uff09\u7684\u6750\u6599\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7ed3\u5408\u56de\u5f52\u5668\uff08pVAE\uff09\uff0c\u901a\u8fc7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u9884\u6d4b\u6c34\u529b\u7279\u6027\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "pVAE\u6846\u67b6\u6210\u529f\u751f\u6210\u4e86\u5177\u6709\u5b9a\u5236\u7279\u6027\u7684\u65b0\u6750\u6599\uff0c\u5e76\u63d0\u4f9b\u4e86\u6f5c\u5728\u7a7a\u95f4\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u5177\u6709\u6240\u9700\u7279\u6027\u7684\u65b0\u6750\u6599\u63d0\u4f9b\u4e86\u9ad8\u6548\u9014\u5f84\uff0c\u76f8\u5173\u6570\u636e\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.17912", "categories": ["cs.LG", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.17912", "abs": "https://arxiv.org/abs/2507.17912", "authors": ["Charles H Martin", "Christopher Hinrichs"], "title": "SETOL: A Semi-Empirical Theory of (Deep) Learning", "comment": "139 pages, 28 figures. Code for experiments available at\n  https://github.com/charlesmartin14/SETOL_experiments", "summary": "We present a SemiEmpirical Theory of Learning (SETOL) that explains the\nremarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We\nprovide a formal explanation of the origin of the fundamental quantities in the\nphenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the\nheavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior\nwork, these metrics have been shown to predict trends in the test accuracies of\npretrained SOTA NN models, importantly, without needing access to either\ntesting or training data. Our SETOL uses techniques from statistical mechanics\nas well as advanced methods from random matrix theory and quantum chemistry.\nThe derivation suggests new mathematical preconditions for ideal learning,\nincluding a new metric, ERG, which is equivalent to applying a single step of\nthe Wilson Exact Renormalization Group. We test the assumptions and predictions\nof SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating\nexcellent agreement with the key theoretical assumptions. For SOTA NN models,\nwe show how to estimate the individual layer qualities of a trained NN by\nsimply computing the empirical spectral density (ESD) of the layer weight\nmatrices and plugging this ESD into our SETOL formulas. Notably, we examine the\nperformance of the HTSR alpha and the SETOL ERG layer quality metrics, and find\nthat they align remarkably well, both on our MLP and on SOTA NNs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u7ecf\u9a8c\u5b66\u4e60\u7406\u8bba\uff08SETOL\uff09\uff0c\u89e3\u91ca\u4e86\u6700\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\uff08NNs\uff09\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4e0e\u91cd\u5c3e\u81ea\u6b63\u5219\u5316\uff08HTSR\uff09\u7406\u8bba\u7684\u5173\u8054\u3002", "motivation": "\u89e3\u91ca\u6700\u5148\u8fdb\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u80cc\u540e\u7684\u7406\u8bba\u673a\u5236\uff0c\u5c24\u5176\u662f\u91cd\u5c3e\u81ea\u6b63\u5219\u5316\uff08HTSR\uff09\u4e2d\u7684\u5173\u952e\u6307\u6807\uff08alpha\u548calpha-hat\uff09\u7684\u8d77\u6e90\u3002", "method": "\u7ed3\u5408\u7edf\u8ba1\u529b\u5b66\u3001\u968f\u673a\u77e9\u9635\u7406\u8bba\u548c\u91cf\u5b50\u5316\u5b66\u65b9\u6cd5\uff0c\u63a8\u5bfc\u51fa\u7406\u60f3\u5b66\u4e60\u7684\u65b0\u6570\u5b66\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u65b0\u6307\u6807ERG\u3002\u901a\u8fc73\u5c42\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u9a8c\u8bc1\u7406\u8bba\u5047\u8bbe\u3002", "result": "SETOL\u4e0eHTSR\u7684\u5173\u952e\u6307\u6807\uff08alpha\u548cERG\uff09\u5728MLP\u548c\u6700\u5148\u8fdbNNs\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7684\u6709\u6548\u6027\u3002", "conclusion": "SETOL\u4e3a\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5b66\u4e60\u6307\u6807ERG\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.17922", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17922", "abs": "https://arxiv.org/abs/2507.17922", "authors": ["Jessica Quaye", "Charvi Rastogi", "Alicia Parrish", "Oana Inel", "Minsuk Kahng", "Lora Aroyo", "Vijay Janapa Reddi"], "title": "From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models", "comment": null, "summary": "Text-to-image (T2I) models have become prevalent across numerous\napplications, making their robust evaluation against adversarial attacks a\ncritical priority. Continuous access to new and challenging adversarial prompts\nacross diverse domains is essential for stress-testing these models for\nresilience against novel attacks from multiple vectors. Current techniques for\ngenerating such prompts are either entirely authored by humans or synthetically\ngenerated. On the one hand, datasets of human-crafted adversarial prompts are\noften too small in size and imbalanced in their cultural and contextual\nrepresentation. On the other hand, datasets of synthetically-generated prompts\nachieve scale, but typically lack the realistic nuances and creative\nadversarial strategies found in human-crafted prompts. To combine the strengths\nof both human and machine approaches, we propose Seed2Harvest, a hybrid\nred-teaming method for guided expansion of culturally diverse, human-crafted\nadversarial prompt seeds. The resulting prompts preserve the characteristics\nand attack patterns of human prompts while maintaining comparable average\nattack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded\ndataset achieves substantially higher diversity with 535 unique geographic\nlocations and a Shannon entropy of 7.48, compared to 58 locations and 5.28\nentropy in the original dataset. Our work demonstrates the importance of\nhuman-machine collaboration in leveraging human creativity and machine\ncomputational capacity to achieve comprehensive, scalable red-teaming for\ncontinuous T2I model safety evaluation.", "AI": {"tldr": "Seed2Harvest\u662f\u4e00\u79cd\u6df7\u5408\u7ea2\u961f\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u548c\u673a\u5668\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u7684\u4f18\u52bf\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4eba\u7c7b\u751f\u6210\u89c4\u6a21\u5c0f\u3001\u5408\u6210\u751f\u6210\u7f3a\u4e4f\u771f\u5b9e\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u4ee5\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "method": "\u63d0\u51faSeed2Harvest\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u4eba\u7c7b\u751f\u6210\u7684\u5bf9\u6297\u6027\u63d0\u793a\u79cd\u5b50\uff0c\u7ed3\u5408\u673a\u5668\u8ba1\u7b97\u80fd\u529b\uff0c\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "\u751f\u6210\u7684\u63d0\u793a\u96c6\u5728\u653b\u51fb\u6210\u529f\u7387\uff080.31-0.36\uff09\u548c\u591a\u6837\u6027\uff08535\u4e2a\u5730\u7406\u4f4d\u7f6e\uff0c\u71b57.48\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u96c6\u3002", "conclusion": "\u4eba\u673a\u534f\u4f5c\u5728\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u4e2d\u81f3\u5173\u91cd\u8981\uff0cSeed2Harvest\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17924", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17924", "abs": "https://arxiv.org/abs/2507.17924", "authors": ["Hongrong Yang", "Markus Schlaepfer"], "title": "UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction", "comment": null, "summary": "Accurate population flow prediction is essential for urban planning,\ntransportation management, and public health. Yet existing methods face key\nlimitations: traditional models rely on static spatial assumptions, deep\nlearning models struggle with cross-city generalization, and Large Language\nModels (LLMs) incur high computational costs while failing to capture spatial\nstructure. Moreover, many approaches sacrifice resolution by clustering Points\nof Interest (POIs) or restricting coverage to subregions, limiting their\nutility for city-wide analytics. We introduce UrbanPulse, a scalable deep\nlearning framework that delivers ultra-fine-grained, city-wide OD flow\npredictions by treating each POI as an individual node. It combines a temporal\ngraph convolutional encoder with a transformer-based decoder to model\nmulti-scale spatiotemporal dependencies. To ensure robust generalization across\nurban contexts, UrbanPulse employs a three-stage transfer learning strategy:\npretraining on large-scale urban graphs, cold-start adaptation, and\nreinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS\nrecords from three metropolitan areas in California, UrbanPulse achieves\nstate-of-the-art accuracy and scalability. Through efficient transfer learning,\nUrbanPulse takes a key step toward making high-resolution, AI-powered urban\nforecasting deployable in practice across diverse cities.", "AI": {"tldr": "UrbanPulse\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u7ec6\u7c92\u5ea6\u7684\u57ce\u5e02\u8303\u56f4OD\u6d41\u91cf\u9884\u6d4b\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2aPOI\u4f5c\u4e3a\u72ec\u7acb\u8282\u70b9\u5efa\u6a21\uff0c\u7ed3\u5408\u65f6\u7a7a\u56fe\u5377\u79ef\u7f16\u7801\u5668\u548c\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u8de8\u57ce\u5e02\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u53e3\u6d41\u52a8\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u9759\u6001\u7a7a\u95f4\u5047\u8bbe\u3001\u8de8\u57ce\u5e02\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4ee5\u53ca\u7a7a\u95f4\u7ed3\u6784\u6355\u6349\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "UrbanPulse\u91c7\u7528\u65f6\u7a7a\u56fe\u5377\u79ef\u7f16\u7801\u5668\u548cTransformer\u89e3\u7801\u5668\u5efa\u6a21\u591a\u5c3a\u5ea6\u65f6\u7a7a\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff08\u9884\u8bad\u7ec3\u3001\u51b7\u542f\u52a8\u9002\u5e94\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\u786e\u4fdd\u8de8\u57ce\u5e02\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u52a0\u5dde\u4e09\u4e2a\u5927\u90fd\u5e02\u533a\u76841.03\u4ebf\u6761GPS\u8bb0\u5f55\u4e0a\u6d4b\u8bd5\uff0cUrbanPulse\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "UrbanPulse\u901a\u8fc7\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387AI\u9a71\u52a8\u7684\u57ce\u5e02\u9884\u6d4b\u5728\u4e0d\u540c\u57ce\u5e02\u7684\u5b9e\u9645\u90e8\u7f72\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2507.17934", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17934", "abs": "https://arxiv.org/abs/2507.17934", "authors": ["Xiaoxu Guo", "Siyan Liang", "Yachao Cui", "Juxiang Zhou", "Lei Wang", "Han Cao"], "title": "Multimodal Fine-grained Reasoning for Post Quality Evaluation", "comment": "48 pages", "summary": "Accurately assessing post quality requires complex relational reasoning to\ncapture nuanced topic-post relationships. However, existing studies face three\nmajor limitations: (1) treating the task as unimodal categorization, which\nfails to leverage multimodal cues and fine-grained quality distinctions; (2)\nintroducing noise during deep multimodal fusion, leading to misleading signals;\nand (3) lacking the ability to capture complex semantic relationships like\nrelevance and comprehensiveness. To address these issues, we propose the\nMultimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,\nwhich mimics human cognitive processes. MFTRR reframes post-quality assessment\nas a ranking task and incorporates multimodal data to better capture quality\nvariations. It consists of two key modules: (1) the Local-Global Semantic\nCorrelation Reasoning Module, which models fine-grained semantic interactions\nbetween posts and topics at both local and global levels, enhanced by a maximum\ninformation fusion mechanism to suppress noise; and (2) the Multi-Level\nEvidential Relational Reasoning Module, which explores macro- and micro-level\nrelational cues to strengthen evidence-based reasoning. We evaluate MFTRR on\nthree newly constructed multimodal topic-post datasets and the public\nLazada-Home dataset. Experimental results demonstrate that MFTRR significantly\noutperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3\nimprovement over the best unimodal method on the Art History dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMFTRR\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7ec6\u7c92\u5ea6\u4e3b\u9898-\u5e16\u5b50\u5173\u7cfb\u63a8\u7406\u6539\u8fdb\u5e16\u5b50\u8d28\u91cf\u8bc4\u4f30\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5e16\u5b50\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u5355\u6a21\u6001\u5206\u7c7b\u3001\u591a\u6a21\u6001\u878d\u5408\u566a\u58f0\u548c\u590d\u6742\u8bed\u4e49\u5173\u7cfb\u6355\u6349\u4e0d\u8db3\u3002", "method": "MFTRR\u6846\u67b6\u5305\u542b\u5c40\u90e8-\u5168\u5c40\u8bed\u4e49\u5173\u8054\u63a8\u7406\u6a21\u5757\u548c\u591a\u5c42\u6b21\u8bc1\u636e\u5173\u7cfb\u63a8\u7406\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u65b0\u6784\u5efa\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cMFTRR\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0cNDCG@3\u63d0\u5347\u6700\u9ad8\u8fbe9.52%\u3002", "conclusion": "MFTRR\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e16\u5b50\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.17953", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17953", "abs": "https://arxiv.org/abs/2507.17953", "authors": ["Chang Eun Song", "Weihong Xu", "Keming Fan", "Soumil Jain", "Gopabandhu Hota", "Haichao Yang", "Leo Liu", "Kerem Akarvardar", "Meng-Fan Chang", "Carlos H. Diaz", "Gert Cauwenberghs", "Tajana Rosing", "Mingu Kang"], "title": "Clo-HDnn: A 4.66 TFLOPS/W and 3.78 TOPS/W Continual On-Device Learning Accelerator with Energy-efficient Hyperdimensional Computing via Progressive Search", "comment": "Published in 2025 Symposium on VLSI Technology and Circuits (VLSI\n  Technology and Circuits), Kyoto, Japan, 2025", "summary": "Clo-HDnn is an on-device learning (ODL) accelerator designed for emerging\ncontinual learning (CL) tasks. Clo-HDnn integrates hyperdimensional computing\n(HDC) along with low-cost Kronecker HD Encoder and weight clustering feature\nextraction (WCFE) to optimize accuracy and efficiency. Clo-HDnn adopts\ngradient-free CL to efficiently update and store the learned knowledge in the\nform of class hypervectors. Its dual-mode operation enables bypassing costly\nfeature extraction for simpler datasets, while progressive search reduces\ncomplexity by up to 61% by encoding and comparing only partial query\nhypervectors. Achieving 4.66 TFLOPS/W (FE) and 3.78 TOPS/W (classifier),\nClo-HDnn delivers 7.77x and 4.85x higher energy efficiency compared to SOTA ODL\naccelerators.", "AI": {"tldr": "Clo-HDnn\u662f\u4e00\u79cd\u7528\u4e8e\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u7684\u8bbe\u5907\u7aef\u5b66\u4e60\u52a0\u901f\u5668\uff0c\u7ed3\u5408\u8d85\u7ef4\u8ba1\u7b97\u548c\u4f4e\u6210\u672c\u6280\u672f\u4f18\u5316\u7cbe\u5ea6\u4e0e\u6548\u7387\u3002", "motivation": "\u4e3a\u65b0\u5174\u7684\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210\u8d85\u7ef4\u8ba1\u7b97\u3001\u4f4e\u6210\u672cKronecker HD\u7f16\u7801\u5668\u548c\u6743\u91cd\u805a\u7c7b\u7279\u5f81\u63d0\u53d6\uff0c\u91c7\u7528\u68af\u5ea6\u65e0\u5173\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u80fd\u6548\u6bd4\u8fbe\u52304.66 TFLOPS/W\uff08\u7279\u5f81\u63d0\u53d6\uff09\u548c3.78 TOPS/W\uff08\u5206\u7c7b\u5668\uff09\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u53477.77\u500d\u548c4.85\u500d\u3002", "conclusion": "Clo-HDnn\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u548c\u53cc\u6a21\u5f0f\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u7684\u80fd\u6548\u548c\u6027\u80fd\u3002"}}
{"id": "2507.17958", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17958", "abs": "https://arxiv.org/abs/2507.17958", "authors": ["Daniel Carlstrom Schad", "Shrey Dixit", "Janis Keck", "Viktor Studenyak", "Aleksandr Shpilevoi", "Andrej Bicanski"], "title": "VIBE: Video-Input Brain Encoder for fMRI Response Modeling", "comment": null, "summary": "We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,\nand text features to predict fMRI activity. Representations from open-source\nmodels (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a\nmodality-fusion transformer and temporally decoded by a prediction transformer\nwith rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod\ndataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson\ncorrelations of 32.25 on in-distribution Friends S07 and 21.25 on six\nout-of-distribution films. An earlier iteration of the same architecture\nobtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second\noverall in the Algonauts 2025 Challenge.", "AI": {"tldr": "VIBE\u662f\u4e00\u79cd\u4e24\u9636\u6bb5Transformer\u6a21\u578b\uff0c\u878d\u5408\u591a\u6a21\u6001\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u7279\u5f81\u6765\u9884\u6d4bfMRI\u6d3b\u52a8\uff0c\u5728CNeuroMod\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\uff08\u89c6\u9891\u3001\u97f3\u9891\u3001\u6587\u672c\uff09\u6765\u66f4\u51c6\u786e\u5730\u9884\u6d4bfMRI\u6d3b\u52a8\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u63d0\u53d6\u591a\u6a21\u6001\u7279\u5f81\uff0c\u901a\u8fc7\u6a21\u6001\u878d\u5408Transformer\u548c\u65f6\u95f4\u9884\u6d4bTransformer\uff08\u5e26\u65cb\u8f6c\u5d4c\u5165\uff09\u8fdb\u884c\u89e3\u7801\uff0c\u5e76\u572865\u5c0f\u65f6\u7535\u5f71\u6570\u636e\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728\u5206\u5e03\u5185\u6570\u636e\uff08Friends S07\uff09\u4e0a\u5e73\u5747Parcel-wise Pearson\u76f8\u5173\u7cfb\u6570\u4e3a32.25\uff0c\u5206\u5e03\u5916\u6570\u636e\u4e0a\u4e3a21.25\uff0c\u4f18\u4e8e\u65e9\u671f\u7248\u672c\u3002", "conclusion": "VIBE\u5728\u591a\u6a21\u6001fMRI\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728Algonauts 2025\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002"}}
{"id": "2507.17977", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17977", "abs": "https://arxiv.org/abs/2507.17977", "authors": ["Rui Deng", "Ziqi Li", "Mingshu Wang"], "title": "Improving the Computational Efficiency and Explainability of GeoAggregator", "comment": "4 pages, 3 figures", "summary": "Accurate modeling and explaining geospatial tabular data (GTD) are critical\nfor understanding geospatial phenomena and their underlying processes. Recent\nwork has proposed a novel transformer-based deep learning model named\nGeoAggregator (GA) for this purpose, and has demonstrated that it outperforms\nother statistical and machine learning approaches. In this short paper, we\nfurther improve GA by 1) developing an optimized pipeline that accelerates the\ndataloading process and streamlines the forward pass of GA to achieve better\ncomputational efficiency; and 2) incorporating a model ensembling strategy and\na post-hoc model explanation function based on the GeoShapley framework to\nenhance model explainability. We validate the functionality and efficiency of\nthe proposed strategies by applying the improved GA model to synthetic\ndatasets. Experimental results show that our implementation improves the\nprediction accuracy and inference speed of GA compared to the original\nimplementation. Moreover, explanation experiments indicate that GA can\neffectively captures the inherent spatial effects in the designed synthetic\ndataset. The complete pipeline has been made publicly available for community\nuse (https://github.com/ruid7181/GA-sklearn).", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86GeoAggregator\uff08GA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u6d41\u7a0b\u548c\u96c6\u6210\u6a21\u578b\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u6539\u8fdb\u540e\u7684GA\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u4f18\u4e8e\u539f\u7248\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u548c\u89e3\u91ca\u5730\u7406\u7a7a\u95f4\u8868\u683c\u6570\u636e\uff08GTD\uff09\u5bf9\u7406\u89e3\u5730\u7406\u73b0\u8c61\u53ca\u5176\u5e95\u5c42\u8fc7\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "method": "1\uff09\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u6d41\u7a0b\u548c\u6a21\u578b\u524d\u5411\u4f20\u64ad\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\uff1b2\uff09\u96c6\u6210\u6a21\u578b\u7b56\u7565\u5e76\u5f15\u5165\u57fa\u4e8eGeoShapley\u6846\u67b6\u7684\u540e\u89e3\u91ca\u529f\u80fd\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6539\u8fdb\u540e\u7684GA\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u5e76\u80fd\u6709\u6548\u6355\u6349\u7a7a\u95f4\u6548\u5e94\u3002", "conclusion": "\u6539\u8fdb\u7684GA\u6a21\u578b\u5728\u529f\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u539f\u7248\uff0c\u4e14\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2507.17979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17979", "abs": "https://arxiv.org/abs/2507.17979", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning", "comment": null, "summary": "Identifying the factors driving data shifts in tabular datasets is a\nsignificant challenge for analysis and decision support systems, especially\nthose focusing on healthcare. Privacy rules restrict data access, and noise\nfrom complex processes hinders analysis. To address this challenge, we propose\nSIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular\nLearning) that (i) extracts privacy-compliant data summary statistics, (ii)\nemploys twin XGBoost models to disentangle intervention signals from noise with\nassistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted\ndecision tree to identify interpretable segments responsible for the shift.\nUnlike existing analyses which may ignore noise or require full data access for\nLLM-based analysis, SIFOTL addresses both challenges using only privacy-safe\nsummary statistics. Demonstrating its real-world efficacy, for a MEPS panel\ndataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of\n0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and\nstatistical tests (F1=0.20) in identifying the segment receiving the subsidy.\nFurthermore, across 18 diverse EHR datasets generated based on Synthea ABM,\nSIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with\ninjected observational noise, whereas baseline average F1 scores range from\n0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,\nprivacy-conscious workflow that is empirically robust to observational noise.", "AI": {"tldr": "SIFOTL\u662f\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u8868\u683c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u6458\u8981\u548cXGBoost\u6a21\u578b\u8bc6\u522b\u6570\u636e\u504f\u79fb\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u9690\u79c1\u9650\u5236\u548c\u590d\u6742\u566a\u58f0\u4e0b\u8868\u683c\u6570\u636e\u504f\u79fb\u5206\u6790\u7684\u6311\u6218\u3002", "method": "\u63d0\u53d6\u9690\u79c1\u5408\u89c4\u7684\u7edf\u8ba1\u6458\u8981\uff0c\u4f7f\u7528XGBoost\u548cLLM\u5206\u79bb\u4fe1\u53f7\u4e0e\u566a\u58f0\uff0c\u901a\u8fc7Pareto\u52a0\u6743\u51b3\u7b56\u6811\u8bc6\u522b\u504f\u79fb\u6bb5\u3002", "result": "\u5728MEPS\u548cEHR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u6570\u663e\u8457\u9ad8\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SIFOTL\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u5de5\u4f5c\u6d41\uff0c\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.17984", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17984", "abs": "https://arxiv.org/abs/2507.17984", "authors": ["Xin Wang", "R. Tyrrell Rockafellar", "Xuegang", "Ban"], "title": "Machine Unlearning of Traffic State Estimation and Prediction", "comment": null, "summary": "Data-driven traffic state estimation and prediction (TSEP) relies heavily on\ndata sources that contain sensitive information. While the abundance of data\nhas fueled significant breakthroughs, particularly in machine learning-based\nmethods, it also raises concerns regarding privacy, cybersecurity, and data\nfreshness. These issues can erode public trust in intelligent transportation\nsystems. Recently, regulations have introduced the \"right to be forgotten\",\nallowing users to request the removal of their private data from models. As\nmachine learning models can remember old data, simply removing it from back-end\ndatabases is insufficient in such systems. To address these challenges, this\nstudy introduces a novel learning paradigm for TSEP-Machine Unlearning\nTSEP-which enables a trained TSEP model to selectively forget\nprivacy-sensitive, poisoned, or outdated data. By empowering models to\n\"unlearn,\" we aim to enhance the trustworthiness and reliability of data-driven\ntraffic TSEP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0fTSEP-Machine Unlearning\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u4e0e\u9884\u6d4b\u4e2d\u7684\u9690\u79c1\u3001\u5b89\u5168\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u4e0e\u9884\u6d4b\u4f9d\u8d56\u654f\u611f\u6570\u636e\uff0c\u4f46\u9690\u79c1\u3001\u7f51\u7edc\u5b89\u5168\u548c\u6570\u636e\u65b0\u9c9c\u5ea6\u95ee\u9898\u53ef\u80fd\u524a\u5f31\u516c\u4f17\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002", "method": "\u5f15\u5165TSEP-Machine Unlearning\u8303\u5f0f\uff0c\u4f7f\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u80fd\u591f\u9009\u62e9\u6027\u9057\u5fd8\u9690\u79c1\u654f\u611f\u3001\u6c61\u67d3\u6216\u8fc7\u65f6\u6570\u636e\u3002", "result": "\u901a\u8fc7\u8ba9\u6a21\u578b\u201c\u9057\u5fd8\u201d\u7279\u5b9a\u6570\u636e\uff0c\u63d0\u5347\u4e86\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u4e0e\u9884\u6d4b\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u3002", "conclusion": "TSEP-Machine Unlearning\u4e3a\u89e3\u51b3\u6570\u636e\u9690\u79c1\u548c\u4fe1\u4efb\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u589e\u5f3a\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.18014", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18014", "abs": "https://arxiv.org/abs/2507.18014", "authors": ["Datta Nimmaturi", "Vaishnavi Bhargava", "Rajat Ghosh", "Johnu George", "Debojyoti Dutta"], "title": "Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models", "comment": null, "summary": "Fine-tuning large language models (LLMs) for reasoning tasks using\nreinforcement learning methods like Group Relative Policy Optimization (GRPO)\nis computationally expensive. To address this, we propose a predictive\nframework that models training dynamics and helps optimize resource usage.\nThrough experiments on Llama and Qwen models (3B 8B), we derive an empirical\nscaling law based on model size, initial performance, and training progress.\nThis law predicts reward trajectories and identifies three consistent training\nphases: slow start, rapid improvement, and plateau. We find that training\nbeyond certain number of an epoch offers little gain, suggesting earlier\nstopping can significantly reduce compute without sacrificing performance. Our\napproach generalizes across model types, providing a practical guide for\nefficient GRPO-based fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8eGRPO\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u8d44\u6e90\u4f7f\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u603b\u7ed3\u51fa\u7ecf\u9a8c\u6027\u6269\u5c55\u89c4\u5f8b\uff0c\u5e76\u53d1\u73b0\u8bad\u7ec3\u53ef\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff0c\u63d0\u524d\u505c\u6b62\u53ef\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7531\u4e8e\u4f7f\u7528GRPO\u7b49\u65b9\u6cd5\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u4f18\u5316\u8d44\u6e90\u4f7f\u7528\u3002", "method": "\u63d0\u51fa\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5728Llama\u548cQwen\u6a21\u578b\u4e0a\u603b\u7ed3\u51fa\u57fa\u4e8e\u6a21\u578b\u5927\u5c0f\u3001\u521d\u59cb\u6027\u80fd\u548c\u8bad\u7ec3\u8fdb\u5ea6\u7684\u6269\u5c55\u89c4\u5f8b\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff08\u7f13\u6162\u542f\u52a8\u3001\u5feb\u901f\u63d0\u5347\u3001\u5e73\u53f0\u671f\uff09\uff0c\u5e76\u8bc1\u660e\u63d0\u524d\u505c\u6b62\u53ef\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8eGRPO\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u7c7b\u578b\u3002"}}
{"id": "2507.18067", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.18067", "abs": "https://arxiv.org/abs/2507.18067", "authors": ["Abdessamad El-Kabid", "Loubna Benabbou", "Redouane Lguensat", "Alex Hern\u00e1ndez-Garc\u00eda"], "title": "Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents", "comment": "Workshop @ ICML2025", "summary": "Accurate modeling of physical systems governed by partial differential\nequations is a central challenge in scientific computing. In oceanography,\nhigh-resolution current data are critical for coastal management, environmental\nmonitoring, and maritime safety. However, available satellite products, such as\nCopernicus data for sea water velocity at ~0.08 degrees spatial resolution and\nglobal ocean models, often lack the spatial granularity required for detailed\nlocal analyses. In this work, we (a) introduce a supervised deep learning\nframework based on neural operators for solving PDEs and providing arbitrary\nresolution solutions, and (b) propose downscaling models with an application to\nCopernicus ocean current data. Additionally, our method can model surrogate\nPDEs and predict solutions at arbitrary resolution, regardless of the input\nresolution. We evaluated our model on real-world Copernicus ocean current data\nand synthetic Navier-Stokes simulation datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7b97\u5b50\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\u5e76\u63d0\u4f9b\u4efb\u610f\u5206\u8fa8\u7387\u89e3\uff0c\u540c\u65f6\u5e94\u7528\u4e8eCopernicus\u6d77\u6d0b\u6570\u636e\u964d\u5c3a\u5ea6\u5efa\u6a21\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u6d77\u6d0b\u6570\u636e\u5bf9\u6cbf\u6d77\u7ba1\u7406\u3001\u73af\u5883\u76d1\u6d4b\u548c\u6d77\u4e0a\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\uff08\u5982Copernicus\uff09\u5206\u8fa8\u7387\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u548c\u795e\u7ecf\u7b97\u5b50\uff0c\u5efa\u6a21\u66ff\u4ee3PDE\u5e76\u9884\u6d4b\u4efb\u610f\u5206\u8fa8\u7387\u89e3\u3002", "result": "\u5728\u771f\u5b9eCopernicus\u6570\u636e\u548c\u5408\u6210Navier-Stokes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u89e3\uff0c\u9002\u7528\u4e8e\u6d77\u6d0b\u6570\u636e\u964d\u5c3a\u5ea6\u548cPDE\u6c42\u89e3\u3002"}}
{"id": "2507.18071", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18071", "abs": "https://arxiv.org/abs/2507.18071", "authors": ["Chujie Zheng", "Shixuan Liu", "Mingze Li", "Xiong-Hui Chen", "Bowen Yu", "Chang Gao", "Kai Dang", "Yuqiong Liu", "Rui Men", "An Yang", "Jingren Zhou", "Junyang Lin"], "title": "Group Sequence Policy Optimization", "comment": null, "summary": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels.", "AI": {"tldr": "GSPO\u662f\u4e00\u79cd\u7a33\u5b9a\u3001\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7b97\u6cd5\u5728\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u91c7\u7528\u8bcd\u7ea7\u91cd\u8981\u6027\u6bd4\u7387\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5e8f\u5217\u4f3c\u7136\u7684\u91cd\u8981\u6027\u6bd4\u7387\u5b9a\u4e49\u3002", "method": "GSPO\u901a\u8fc7\u5e8f\u5217\u7ea7\u88c1\u526a\u3001\u5956\u52b1\u548c\u4f18\u5316\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u8bcd\u7ea7\u65b9\u6cd5\u3002", "result": "GSPO\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8eGRPO\u7b97\u6cd5\uff0c\u663e\u8457\u7a33\u5b9a\u4e86MoE RL\u8bad\u7ec3\uff0c\u5e76\u7b80\u5316\u4e86RL\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u3002", "conclusion": "GSPO\u7684\u6210\u529f\u5e94\u7528\u4e3a\u6700\u65b0Qwen3\u6a21\u578b\u7684\u663e\u8457\u6539\u8fdb\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.18072", "categories": ["cs.LG", "cs.CR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18072", "abs": "https://arxiv.org/abs/2507.18072", "authors": ["Ryusei Fujimoto", "Yugo Nakamura", "Yutaka Arakawa"], "title": "C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams", "comment": null, "summary": "Wearable accelerometers and gyroscopes encode fine-grained behavioural\nsignatures that can be exploited to re-identify users, making privacy\nprotection essential for healthcare applications. We introduce C-AAE, a\ncompressive anonymizing autoencoder that marries an Anonymizing AutoEncoder\n(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first\nprojects raw sensor windows into a latent space that retains activity-relevant\nfeatures while suppressing identity cues. ADPCM then differentially encodes\nthis latent stream, further masking residual identity information and shrinking\nthe bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE\ncuts user re-identification F1 scores by 10-15 percentage points relative to\nAAE alone, while keeping activity-recognition F1 within 5 percentage points of\nthe unprotected baseline. ADPCM also reduces data volume by roughly 75 %,\neasing transmission and storage overheads. These results demonstrate that C-AAE\noffers a practical route to balancing privacy and utility in continuous,\nsensor-based activity recognition for healthcare.", "AI": {"tldr": "C-AAE\u7ed3\u5408\u533f\u540d\u81ea\u7f16\u7801\u5668\u548c\u81ea\u9002\u5e94\u5dee\u5206\u8109\u51b2\u7f16\u7801\u8c03\u5236\uff0c\u6709\u6548\u4fdd\u62a4\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u7684\u9690\u79c1\uff0c\u540c\u65f6\u4fdd\u6301\u6d3b\u52a8\u8bc6\u522b\u6027\u80fd\u5e76\u51cf\u5c11\u6570\u636e\u91cf\u3002", "motivation": "\u53ef\u7a7f\u6234\u8bbe\u5907\u7684\u52a0\u901f\u5ea6\u8ba1\u548c\u9640\u87ba\u4eea\u6570\u636e\u53ef\u80fd\u6cc4\u9732\u7528\u6237\u8eab\u4efd\uff0c\u9690\u79c1\u4fdd\u62a4\u5bf9\u533b\u7597\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "C-AAE\u7ed3\u5408\u533f\u540d\u81ea\u7f16\u7801\u5668\uff08AAE\uff09\u548c\u81ea\u9002\u5e94\u5dee\u5206\u8109\u51b2\u7f16\u7801\u8c03\u5236\uff08ADPCM\uff09\uff0c\u5148\u901a\u8fc7AAE\u6291\u5236\u8eab\u4efd\u7279\u5f81\uff0c\u518d\u901a\u8fc7ADPCM\u8fdb\u4e00\u6b65\u9690\u85cf\u8eab\u4efd\u4fe1\u606f\u5e76\u538b\u7f29\u6570\u636e\u3002", "result": "\u5728MotionSense\u548cPAMAP2\u6570\u636e\u96c6\u4e0a\uff0cC-AAE\u5c06\u7528\u6237\u91cd\u8bc6\u522bF1\u5206\u6570\u964d\u4f4e10-15%\uff0c\u6d3b\u52a8\u8bc6\u522bF1\u5206\u6570\u4ec5\u4e0b\u964d5%\uff0c\u6570\u636e\u91cf\u51cf\u5c1175%\u3002", "conclusion": "C-AAE\u5728\u533b\u7597\u4f20\u611f\u5668\u6570\u636e\u4e2d\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u5b9e\u7528\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2507.18073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18073", "abs": "https://arxiv.org/abs/2507.18073", "authors": ["Qingcheng Zhu", "Yangyang Ren", "Linlin Yang", "Mingbao Lin", "Yanjing Li", "Sheng Xu", "Zichao Feng", "Haodong Zhu", "Yuguang Yang", "Juan Zhang", "Runqi Wang", "Baochang Zhang"], "title": "Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method", "comment": null, "summary": "Deploying large language models (LLMs) is challenging due to their massive\nparameters and high computational costs. Ultra low-bit quantization can\nsignificantly reduce storage and accelerate inference, but extreme compression\n(i.e., mean bit-width <= 2) often leads to severe performance degradation. To\naddress this, we propose Squeeze10-LLM, effectively \"squeezing\" 16-bit LLMs'\nweights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision\npost-training quantization (PTQ) framework and achieves an average of 1.6 bits\nper weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We\nintroduce Squeeze10LLM with two key innovations: Post-Binarization Activation\nRobustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a\nrefined weight significance metric that accounts for the impact of quantization\non activations, improving accuracy in low-bit settings. FIAS is a strategy that\npreserves full activation information during quantization to mitigate\ncumulative error propagation across layers. Experiments on LLaMA and LLaMA2\nshow that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit\nweight-only quantization, improving average accuracy from 43% to 56% on six\nzero-shot classification tasks--a significant boost over existing PTQ methods.\nOur code will be released upon publication.", "AI": {"tldr": "Squeeze10-LLM\u662f\u4e00\u79cd\u5206\u9636\u6bb5\u6df7\u5408\u7cbe\u5ea6\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c0680%\u7684\u6743\u91cd\u91cf\u5316\u4e3a1\u4f4d\u300120%\u91cf\u5316\u4e3a4\u4f4d\uff0c\u5b9e\u73b0\u5e73\u57471.6\u4f4d/\u6743\u91cd\u7684\u538b\u7f29\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u56e0\u53c2\u6570\u5e9e\u5927\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u96be\u4ee5\u90e8\u7f72\uff0c\u6781\u4f4e\u4f4d\u91cf\u5316\u867d\u80fd\u51cf\u5c11\u5b58\u50a8\u548c\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u6781\u7aef\u538b\u7f29\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51faSqueeze10-LLM\u6846\u67b6\uff0c\u7ed3\u5408\u540e\u4e8c\u503c\u5316\u6fc0\u6d3b\u9c81\u68d2\u6027\uff08PBAR\uff09\u548c\u5168\u4fe1\u606f\u6fc0\u6d3b\u76d1\u7763\uff08FIAS\uff09\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\u3002", "result": "\u5728LLaMA\u548cLLaMA2\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSqueeze10-LLM\u5728\u4f4e\u4e8e2\u4f4d\u7684\u6743\u91cd\u91cf\u5316\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u96f6\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u4ece43%\u63d0\u5347\u81f356%\u3002", "conclusion": "Squeeze10-LLM\u901a\u8fc7\u521b\u65b0\u91cf\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6781\u4f4e\u4f4d\u91cf\u5316\u7684\u6027\u80fd\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18098", "categories": ["cs.LG", "68T01", "I.5"], "pdf": "https://arxiv.org/pdf/2507.18098", "abs": "https://arxiv.org/abs/2507.18098", "authors": ["Kosuke Sugiyama", "Masato Uchida"], "title": "Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes", "comment": "32 pages, 11 figures", "summary": "In scenarios where training data is limited due to observation costs or data\nscarcity, enriching the label information associated with each instance becomes\ncrucial for building high-accuracy classification models. In such contexts, it\nis often feasible to obtain not only hard labels but also {\\it additional\nsupervision}, such as the confidences for the hard labels. This setting\nnaturally raises fundamental questions: {\\it What kinds of additional\nsupervision are intrinsically beneficial?} And {\\it how do they contribute to\nimproved generalization performance?} To address these questions, we propose a\ntheoretical framework that treats both hard labels and additional supervision\nas probability distributions, and constructs soft labels through their affine\ncombination. Our theoretical analysis reveals that the essential component of\nadditional supervision is not the confidence score of the assigned hard label,\nbut rather the information of the distribution over the non-hard-labeled\nclasses. Moreover, we demonstrate that the additional supervision and the\nmixing coefficient contribute to the refinement of soft labels in complementary\nroles. Intuitively, in the probability simplex, the additional supervision\ndetermines the direction in which the deterministic distribution representing\nthe hard label should be adjusted toward the true label distribution, while the\nmixing coefficient controls the step size along that direction. Through\ngeneralization error analysis, we theoretically characterize how the additional\nsupervision and its mixing coefficient affect both the convergence rate and\nasymptotic value of the error bound. Finally, we experimentally demonstrate\nthat, based on our theory, designing additional supervision can lead to\nimproved classification accuracy, even when utilized in a simple manner.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u786c\u6807\u7b7e\u548c\u989d\u5916\u76d1\u7763\u4fe1\u606f\u6784\u5efa\u8f6f\u6807\u7b7e\uff0c\u4ee5\u63d0\u9ad8\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\u7684\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u6216\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u989d\u5916\u76d1\u7763\u4fe1\u606f\uff08\u5982\u7f6e\u4fe1\u5ea6\uff09\u63d0\u5347\u5206\u7c7b\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u5c06\u786c\u6807\u7b7e\u548c\u989d\u5916\u76d1\u7763\u89c6\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u901a\u8fc7\u5b83\u4eec\u7684\u4eff\u5c04\u7ec4\u5408\u6784\u5efa\u8f6f\u6807\u7b7e\uff0c\u5e76\u5206\u6790\u989d\u5916\u76d1\u7763\u5bf9\u6807\u7b7e\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u989d\u5916\u76d1\u7763\u7684\u6838\u5fc3\u4ef7\u503c\u5728\u4e8e\u975e\u786c\u6807\u7b7e\u7c7b\u7684\u5206\u5e03\u4fe1\u606f\uff0c\u800c\u975e\u786c\u6807\u7b7e\u7684\u7f6e\u4fe1\u5ea6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "\u989d\u5916\u76d1\u7763\u548c\u6df7\u5408\u7cfb\u6570\u5728\u4f18\u5316\u8f6f\u6807\u7b7e\u4e2d\u8d77\u4e92\u8865\u4f5c\u7528\uff0c\u7406\u8bba\u5206\u6790\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u76d1\u7763\u4fe1\u606f\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.18111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18111", "abs": "https://arxiv.org/abs/2507.18111", "authors": ["Peyman Tehrani", "Anas Alsoliman"], "title": "Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN", "comment": null, "summary": "In this paper, we tackle the challenge of radio access network (RAN) slicing\nwithin an open RAN (O-RAN) architecture. Our focus centers on a network that\nincludes multiple mobile virtual network operators (MVNOs) competing for\nphysical resource blocks (PRBs) with the goal of meeting probabilistic delay\nupper bound constraints for their clients while minimizing PRB utilization.\nInitially, we derive a reward function based on the law of large numbers (LLN),\nthen implement practical modifications to adapt it for real-world experimental\nscenarios. We then propose our solution, the Percentile-based Delay-Aware Deep\nReinforcement Learning (PDA-DRL), which demonstrates its superiority over\nseveral baselines, including DRL models optimized for average delay\nconstraints, by achieving a 38\\% reduction in resultant average delay.\nFurthermore, we delve into the issue of model weight sharing among multiple\nMVNOs to develop a robust personalized model. We introduce a reward-based\npersonalization method where each agent prioritizes other agents' model weights\nbased on their performance. This technique surpasses traditional aggregation\nmethods, such as federated averaging, and strategies reliant on traffic\npatterns and model weight distance similarities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684RAN\u5207\u7247\u65b9\u6cd5\uff08PDA-DRL\uff09\uff0c\u5728O-RAN\u67b6\u6784\u4e2d\u4f18\u5316PRB\u5229\u7528\u7387\u5e76\u6ee1\u8db3\u5ef6\u8fdf\u7ea6\u675f\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3O-RAN\u67b6\u6784\u4e2d\u591a\u4e2aMVNO\u7ade\u4e89PRB\u8d44\u6e90\u65f6\u5982\u4f55\u6ee1\u8db3\u5ef6\u8fdf\u7ea6\u675f\u5e76\u6700\u5c0f\u5316\u8d44\u6e90\u5229\u7528\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5927\u6570\u5b9a\u5f8b\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u63d0\u51faPDA-DRL\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5956\u52b1\u7684\u4e2a\u6027\u5316\u6a21\u578b\u6743\u91cd\u5171\u4eab\u65b9\u6cd5\u3002", "result": "PDA-DRL\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e38%\uff0c\u4e2a\u6027\u5316\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u805a\u5408\u7b56\u7565\u3002", "conclusion": "PDA-DRL\u548c\u4e2a\u6027\u5316\u6743\u91cd\u5171\u4eab\u65b9\u6cd5\u5728O-RAN\u4e2d\u6709\u6548\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u5ef6\u8fdf\u6027\u80fd\u3002"}}
{"id": "2507.18113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18113", "abs": "https://arxiv.org/abs/2507.18113", "authors": ["Junyong Jiang", "Buwei Tian", "Chenxing Xu", "Songze Li", "Lu Dong"], "title": "Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification", "comment": null, "summary": "Reinforcement learning (RL) has achieved remarkable success in fields like\nrobotics and autonomous driving, but adversarial attacks designed to mislead RL\nsystems remain challenging. Existing approaches often rely on modifying the\nenvironment or policy, limiting their practicality. This paper proposes an\nadversarial attack method in which existing agents in the environment guide the\ntarget policy to output suboptimal actions without altering the environment. We\npropose a reward iteration optimization framework that leverages large language\nmodels (LLMs) to generate adversarial rewards explicitly tailored to the\nvulnerabilities of the target agent, thereby enhancing the effectiveness of\ninducing the target agent toward suboptimal decision-making. Additionally, a\ncritical state identification algorithm is designed to pinpoint the target\nagent's most vulnerable states, where suboptimal behavior from the victim leads\nto significant degradation in overall performance. Experimental results in\ndiverse environments demonstrate the superiority of our method over existing\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5bf9\u6297\u6027\u5956\u52b1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u73b0\u6709\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u5f15\u5bfc\u76ee\u6807\u7b56\u7565\u8f93\u51fa\u6b21\u4f18\u52a8\u4f5c\uff0c\u65e0\u9700\u4fee\u6539\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4fee\u6539\u73af\u5883\u6216\u7b56\u7565\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u5956\u52b1\u8bf1\u5bfc\u76ee\u6807\u4ee3\u7406\u505a\u51fa\u6b21\u4f18\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u5956\u52b1\u8fed\u4ee3\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9488\u5bf9\u76ee\u6807\u4ee3\u7406\u6f0f\u6d1e\u7684\u5bf9\u6297\u6027\u5956\u52b1\uff1b\u8bbe\u8ba1\u5173\u952e\u72b6\u6001\u8bc6\u522b\u7b97\u6cd5\uff0c\u5b9a\u4f4d\u76ee\u6807\u4ee3\u7406\u6700\u8106\u5f31\u72b6\u6001\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6027\u5956\u52b1\u548c\u8bc6\u522b\u5173\u952e\u72b6\u6001\uff0c\u6709\u6548\u8bf1\u5bfc\u76ee\u6807\u4ee3\u7406\u505a\u51fa\u6b21\u4f18\u51b3\u7b56\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u73af\u5883\u3002"}}
{"id": "2507.18122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18122", "abs": "https://arxiv.org/abs/2507.18122", "authors": ["Matthias Otth", "Jonas H\u00fcbotter", "Ido Hakimi", "Andreas Krause"], "title": "Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning", "comment": null, "summary": "Recent work has shown that language models can self-improve by maximizing\ntheir own confidence in their predictions, without relying on external\nverifiers or reward signals. In this work, we study the test-time scaling of\nlanguage models for mathematical reasoning tasks, where the model's own\nconfidence is used to select the most promising attempts. Surprisingly, we find\nthat we can achieve significant performance gains by continuing only the most\npromising attempt, selected by the model's prefix-confidence. We systematically\nevaluate prefix-confidence scaling on five mathematical reasoning datasets: the\nschool-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and\nAIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens\nachieves a better accuracy-compute trade-off than majority voting. Moreover,\nprefix-confidence scaling appears less susceptible than BoN to length biases.\nFinally, we also evaluate test-time training with prefix-confidence and find\nthat, while outperforming the base model, it does not improve over\nprefix-confidence scaling.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u6700\u5927\u5316\u81ea\u8eab\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u4ec5\u7ee7\u7eed\u6700\u6709\u5e0c\u671b\u7684\u5c1d\u8bd5\uff08\u57fa\u4e8e\u6a21\u578b\u524d\u7f00\u7f6e\u4fe1\u5ea6\u9009\u62e9\uff09\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u524d\u7f00\u7f6e\u4fe1\u5ea6\u7f29\u653e\u4f18\u4e8e\u591a\u6570\u6295\u7968\uff0c\u4e14\u4e0d\u6613\u53d7\u957f\u5ea6\u504f\u5dee\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5982\u4f55\u901a\u8fc7\u81ea\u8eab\u7f6e\u4fe1\u5ea6\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u51cf\u5c11\u5bf9\u5916\u90e8\u9a8c\u8bc1\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u7684\u524d\u7f00\u7f6e\u4fe1\u5ea6\u9009\u62e9\u6700\u6709\u5e0c\u671b\u7684\u5c1d\u8bd5\uff0c\u5e76\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\uff08GSM8K\u3001MATH500\u3001AMC23\u3001AIME24\u3001AIME25\uff09\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u524d\u7f00\u7f6e\u4fe1\u5ea6\u7f29\u653e\uff08\u4ec532\u4e2a\u6807\u8bb0\u524d\u7f00\uff09\u5728\u51c6\u786e\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u591a\u6570\u6295\u7968\uff0c\u4e14\u5bf9\u957f\u5ea6\u504f\u5dee\u66f4\u4e0d\u654f\u611f\u3002\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u867d\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u672a\u8d85\u8d8a\u524d\u7f00\u7f6e\u4fe1\u5ea6\u7f29\u653e\u3002", "conclusion": "\u524d\u7f00\u7f6e\u4fe1\u5ea6\u7f29\u653e\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5065\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.18139", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18139", "abs": "https://arxiv.org/abs/2507.18139", "authors": ["Alberto Marchisio", "Muhammad Shafique"], "title": "Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS), Ischia, Italy, July 2025", "summary": "The growing need for intelligent, adaptive, and energy-efficient autonomous\nsystems across fields such as robotics, mobile agents (e.g., UAVs), and\nself-driving vehicles is driving interest in neuromorphic computing. By drawing\ninspiration from biological neural systems, neuromorphic approaches offer\npromising pathways to enhance the perception, decision-making, and\nresponsiveness of autonomous platforms. This paper surveys recent progress in\nneuromorphic algorithms, specialized hardware, and cross-layer optimization\nstrategies, with a focus on their deployment in real-world autonomous\nscenarios. Special attention is given to event-based dynamic vision sensors and\ntheir role in enabling fast, efficient perception. The discussion highlights\nnew methods that improve energy efficiency, robustness, adaptability, and\nreliability through the integration of spiking neural networks into autonomous\nsystem architectures. We integrate perspectives from machine learning,\nrobotics, neuroscience, and neuromorphic engineering to offer a comprehensive\nview of the state of the field. Finally, emerging trends and open challenges\nare explored, particularly in the areas of real-time decision-making, continual\nlearning, and the development of secure, resilient autonomous systems.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u7b97\u6cd5\u3001\u786c\u4ef6\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e8b\u4ef6\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u3001\u81ea\u9002\u5e94\u548c\u8282\u80fd\u81ea\u4e3b\u7cfb\u7edf\u9700\u6c42\u7684\u589e\u957f\uff0c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u56e0\u5176\u4eff\u751f\u7279\u6027\u6210\u4e3a\u63d0\u5347\u611f\u77e5\u3001\u51b3\u7b56\u548c\u54cd\u5e94\u80fd\u529b\u7684\u6709\u524d\u666f\u9014\u5f84\u3002", "method": "\u901a\u8fc7\u6574\u5408\u673a\u5668\u5b66\u4e60\u3001\u673a\u5668\u4eba\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u795e\u7ecf\u5f62\u6001\u5de5\u7a0b\u7684\u89c6\u89d2\uff0c\u7efc\u8ff0\u4e86\u795e\u7ecf\u5f62\u6001\u7b97\u6cd5\u3001\u786c\u4ef6\u548c\u8de8\u5c42\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u80fd\u6548\u3001\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u4e8b\u4ef6\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u65f6\u51b3\u7b56\u3001\u6301\u7eed\u5b66\u4e60\u4ee5\u53ca\u5b89\u5168\u5f39\u6027\u81ea\u4e3b\u7cfb\u7edf\u5f00\u53d1\u7b49\u672a\u6765\u6311\u6218\u548c\u8d8b\u52bf\u3002"}}
{"id": "2507.18153", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18153", "abs": "https://arxiv.org/abs/2507.18153", "authors": ["Riting Xia", "Rucong Wang", "Yulin Liu", "Anchen Li", "Xueyan Liu", "Yan Zhang"], "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label", "comment": null, "summary": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGraphALP\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u89e3\u51b3\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u56fe\u8282\u70b9\u5206\u7c7b\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u56fe\u4e2d\u6807\u7b7e\u5e38\u542b\u566a\u58f0\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6807\u7b7e\u5e72\u51c0\uff0c\u65e0\u6cd5\u5904\u7406\u566a\u58f0\u95ee\u9898\u3002", "method": "\u57fa\u4e8eLLM\u7684\u8fc7\u91c7\u6837\u751f\u6210\u5c11\u6570\u7c7b\u8282\u70b9\uff0c\u52a8\u6001\u52a0\u6743\u4f2a\u6807\u7b7e\u51cf\u5c11\u566a\u58f0\uff0c\u4e8c\u6b21LLM\u8fc7\u91c7\u6837\u7f13\u89e3\u5206\u5e03\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGraphALP\u5728\u5e26\u566a\u58f0\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u56fe\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GraphALP\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u6807\u7b7e\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.18183", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18183", "abs": "https://arxiv.org/abs/2507.18183", "authors": ["Jianchao Wang", "Qingfeng Li", "Pengcheng Zheng", "Xiaorong Pu", "Yazhou Ren"], "title": "ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory", "comment": null, "summary": "Training deep neural networks on real-world datasets is often hampered by the\npresence of noisy labels, which can be memorized by over-parameterized models,\nleading to significant degradation in generalization performance. While\nexisting methods for learning with noisy labels (LNL) have made considerable\nprogress, they fundamentally suffer from static snapshot evaluations and fail\nto leverage the rich temporal dynamics of learning evolution. In this paper, we\npropose ChronoSelect (chrono denoting its temporal nature), a novel framework\nfeaturing an innovative four-stage memory architecture that compresses\nprediction history into compact temporal distributions. Our unique sliding\nupdate mechanism with controlled decay maintains only four dynamic memory units\nper sample, progressively emphasizing recent patterns while retaining essential\nhistorical knowledge. This enables precise three-way sample partitioning into\nclean, boundary, and noisy subsets through temporal trajectory analysis and\ndual-branch consistency. Theoretical guarantees prove the mechanism's\nconvergence and stability under noisy conditions. Extensive experiments\ndemonstrate ChronoSelect's state-of-the-art performance across synthetic and\nreal-world benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChronoSelect\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u56db\u9636\u6bb5\u8bb0\u5fc6\u67b6\u6784\u548c\u6ed1\u52a8\u66f4\u65b0\u673a\u5236\uff0c\u901a\u8fc7\u65f6\u95f4\u8f68\u8ff9\u5206\u6790\u5b9e\u73b0\u6837\u672c\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u6807\u7b7e\u4e0b\u7684\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u96c6\u4e2d\u566a\u58f0\u6807\u7b7e\u4f1a\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u9636\u6bb5\u8bb0\u5fc6\u67b6\u6784\u548c\u6ed1\u52a8\u66f4\u65b0\u673a\u5236\uff0c\u538b\u7f29\u9884\u6d4b\u5386\u53f2\u4e3a\u7d27\u51d1\u65f6\u95f4\u5206\u5e03\uff0c\u901a\u8fc7\u65f6\u95f4\u8f68\u8ff9\u5206\u6790\u548c\u53cc\u5206\u652f\u4e00\u81f4\u6027\u5b9e\u73b0\u6837\u672c\u5206\u7c7b\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u9886\u5148\u6027\u80fd\u3002", "conclusion": "ChronoSelect\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u5206\u6790\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u6807\u7b7e\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.18196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18196", "abs": "https://arxiv.org/abs/2507.18196", "authors": ["Daniel Grimm", "Ahmed Abouelazm", "J. Marius Z\u00f6llner"], "title": "Goal-based Trajectory Prediction for improved Cross-Dataset Generalization", "comment": "Accepted on IEEE ITSC 2025", "summary": "To achieve full autonomous driving, a good understanding of the surrounding\nenvironment is necessary. Especially predicting the future states of other\ntraffic participants imposes a non-trivial challenge. Current SotA-models\nalready show promising results when trained on real datasets (e.g. Argoverse2,\nNuScenes). Problems arise when these models are deployed to new/unseen areas.\nTypically, performance drops significantly, indicating that the models lack\ngeneralization. In this work, we introduce a new Graph Neural Network (GNN)\nthat utilizes a heterogeneous graph consisting of traffic participants and\nvectorized road network. Latter, is used to classify goals, i.e. endpoints of\nthe predicted trajectories, in a multi-staged approach, leading to a better\ngeneralization to unseen scenarios. We show the effectiveness of the goal\nselection process via cross-dataset evaluation, i.e. training on Argoverse2 and\nevaluating on NuScenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u76ee\u6807\u5206\u7c7b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u51c6\u786e\u9884\u6d4b\u5468\u56f4\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u672a\u6765\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u65b0\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u5305\u542b\u4ea4\u901a\u53c2\u4e0e\u8005\u548c\u77e2\u91cf\u5316\u9053\u8def\u7f51\u7edc\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u591a\u9636\u6bb5\u5206\u7c7b\u76ee\u6807\u4ee5\u6539\u8fdb\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\uff08Argoverse2\u8bad\u7ec3\uff0cNuScenes\u6d4b\u8bd5\uff09\uff0c\u9a8c\u8bc1\u4e86\u76ee\u6807\u9009\u62e9\u8fc7\u7a0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2507.18219", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18219", "abs": "https://arxiv.org/abs/2507.18219", "authors": ["Zhongzheng Yuan", "Lianshuai Guo", "Xunkai Li", "Yinlin Zhu", "Wenyu Wang", "Meixia Qu"], "title": "FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting", "comment": null, "summary": "Federated Graph Learning (FGL) is a distributed learning paradigm that\nenables collaborative training over large-scale subgraphs located on multiple\nlocal systems. However, most existing FGL approaches rely on synchronous\ncommunication, which leads to inefficiencies and is often impractical in\nreal-world deployments. Meanwhile, current asynchronous federated learning\n(AFL) methods are primarily designed for conventional tasks such as image\nclassification and natural language processing, without accounting for the\nunique topological properties of graph data. Directly applying these methods to\ngraph learning can possibly result in semantic drift and representational\ninconsistency in the global model. To address these challenges, we propose\nFedSA-GCL, a semi-asynchronous federated framework that leverages both\ninter-client label distribution divergence and graph topological\ncharacteristics through a novel ClusterCast mechanism for efficient training.\nWe evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain\nand Metis split algorithms, and compare it against 9 baselines. Extensive\nexperiments demonstrate that our method achieves strong robustness and\noutstanding efficiency, outperforming the baselines by an average of 2.92% with\nthe Louvain and by 3.4% with the Metis.", "AI": {"tldr": "FedSA-GCL\u662f\u4e00\u79cd\u534a\u5f02\u6b65\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7ClusterCast\u673a\u5236\u89e3\u51b3\u540c\u6b65\u901a\u4fe1\u6548\u7387\u4f4e\u548c\u5f02\u6b65\u65b9\u6cd5\u5728\u56fe\u6570\u636e\u4e0a\u7684\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u56fe\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u540c\u6b65\u901a\u4fe1\u6548\u7387\u4f4e\uff0c\u5f02\u6b65\u65b9\u6cd5\u672a\u8003\u8651\u56fe\u6570\u636e\u7684\u62d3\u6251\u7279\u6027\uff0c\u5bfc\u81f4\u8bed\u4e49\u6f02\u79fb\u3002", "method": "\u63d0\u51faFedSA-GCL\u6846\u67b6\uff0c\u7ed3\u5408ClusterCast\u673a\u5236\u5229\u7528\u5ba2\u6237\u7aef\u6807\u7b7e\u5206\u5e03\u5dee\u5f02\u548c\u56fe\u62d3\u6251\u7279\u6027\u8fdb\u884c\u534a\u5f02\u6b65\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cFedSA-GCL\u5e73\u5747\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf2.92%\uff08Louvain\uff09\u548c3.4%\uff08Metis\uff09\u3002", "conclusion": "FedSA-GCL\u5728\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.18220", "categories": ["cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.18220", "abs": "https://arxiv.org/abs/2507.18220", "authors": ["Ansei Yonezawa", "Heisei Yonezawa", "Shuichi Yahagi", "Itsuro Kajiwara", "Shinya Kijimoto", "Hikaru Taniuchi", "Kentaro Murakami"], "title": "Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The sparse identification of nonlinear dynamics (SINDy) approach can discover\nthe governing equations of dynamical systems based on measurement data, where\nthe dynamical model is identified as the sparse linear combination of the given\nbasis functions. A major challenge in SINDy is the design of a library, which\nis a set of candidate basis functions, as the appropriate library is not\ntrivial for many dynamical systems. To overcome this difficulty, this study\nproposes SINDy with library optimization mechanism (SINDy-LOM), which is a\ncombination of the sparse regression technique and the novel learning strategy\nof the library. In the proposed approach, the basis functions are parametrized.\nThe SINDy-LOM approach involves a two-layer optimization architecture: the\ninner-layer, in which the data-driven model is extracted as the sparse linear\ncombination of the candidate basis functions, and the outer-layer, in which the\nbasis functions are optimized from the viewpoint of the recursive long-term\n(RLT) prediction accuracy; thus, the library design is reformulated as the\noptimization of the parametrized basis functions. The resulting SINDy-LOM model\nhas good interpretability and usability, as the proposed approach yields the\nparsimonious model. The library optimization mechanism significantly reduces\nuser burden. The RLT perspective improves the reliability of the resulting\nmodel compared with the traditional SINDy approach that can only ensure the\none-step-ahead prediction accuracy. The validity of the proposed approach is\ndemonstrated by applying it to a diesel engine airpath system, which is a\nwell-known complex industrial system.", "AI": {"tldr": "SINDy-LOM\u7ed3\u5408\u7a00\u758f\u56de\u5f52\u6280\u672f\u548c\u65b0\u578b\u5e93\u5b66\u4e60\u7b56\u7565\uff0c\u4f18\u5316\u53c2\u6570\u5316\u57fa\u51fd\u6570\uff0c\u63d0\u5347\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edfSINDy\u65b9\u6cd5\u5728\u5e93\u8bbe\u8ba1\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u957f\u671f\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u63d0\u51faSINDy-LOM\u4ee5\u4f18\u5316\u5e93\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u67b6\u6784\uff1a\u5185\u5c42\u901a\u8fc7\u7a00\u758f\u56de\u5f52\u63d0\u53d6\u6a21\u578b\uff0c\u5916\u5c42\u57fa\u4e8e\u9012\u5f52\u957f\u671f\u9884\u6d4b\u7cbe\u5ea6\u4f18\u5316\u53c2\u6570\u5316\u57fa\u51fd\u6570\u3002", "result": "SINDy-LOM\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u51cf\u5c11\u4e86\u7528\u6237\u8d1f\u62c5\uff0c\u5e76\u5728\u67f4\u6cb9\u53d1\u52a8\u673a\u7a7a\u6c14\u8def\u5f84\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "SINDy-LOM\u901a\u8fc7\u4f18\u5316\u5e93\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7cfb\u7edf\u5efa\u6a21\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.18242", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18242", "abs": "https://arxiv.org/abs/2507.18242", "authors": ["Fabian Akkerman", "Julien Ferry", "Christian Artigues", "Emmanuel Hebrard", "Thibaut Vidal"], "title": "Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods", "comment": null, "summary": "Despite their theoretical appeal, totally corrective boosting methods based\non linear programming have received limited empirical attention. In this paper,\nwe conduct the first large-scale experimental study of six LP-based boosting\nformulations, including two novel methods, NM-Boost and QRLP-Boost, across 20\ndiverse datasets. We evaluate the use of both heuristic and optimal base\nlearners within these formulations, and analyze not only accuracy, but also\nensemble sparsity, margin distribution, anytime performance, and hyperparameter\nsensitivity. We show that totally corrective methods can outperform or match\nstate-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,\nwhile producing significantly sparser ensembles. We further show that these\nmethods can thin pre-trained ensembles without sacrificing performance, and we\nhighlight both the strengths and limitations of using optimal decision trees in\nthis context.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u9a8c\u7814\u7a76\u4e86\u516d\u79cd\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u5b8c\u5168\u6821\u6b63\u63d0\u5347\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u65b0\u65b9\u6cd5NM-Boost\u548cQRLP-Boost\uff0c\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4f7f\u7528\u6d45\u5c42\u6811\u65f6\u80fd\u8d85\u8d8a\u6216\u5339\u914dXGBoost\u548cLightGBM\u7b49\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u751f\u6210\u66f4\u7a00\u758f\u7684\u96c6\u6210\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u5b8c\u5168\u6821\u6b63\u63d0\u5347\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u8bc1\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u516d\u79cd\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u63d0\u5347\u65b9\u6cd5\uff0c\u5305\u62ec\u4e24\u79cd\u65b0\u65b9\u6cd5NM-Boost\u548cQRLP-Boost\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u548c\u6700\u4f18\u57fa\u5b66\u4e60\u5668\uff0c\u8bc4\u4f30\u51c6\u786e\u6027\u3001\u96c6\u6210\u7a00\u758f\u6027\u3001\u8fb9\u9645\u5206\u5e03\u7b49\u3002", "result": "\u5b8c\u5168\u6821\u6b63\u65b9\u6cd5\u5728\u4f7f\u7528\u6d45\u5c42\u6811\u65f6\u80fd\u8d85\u8d8a\u6216\u5339\u914dXGBoost\u548cLightGBM\uff0c\u4e14\u96c6\u6210\u66f4\u7a00\u758f\u3002\u8fd8\u80fd\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u7cbe\u7b80\u9884\u8bad\u7ec3\u96c6\u6210\u3002", "conclusion": "\u5b8c\u5168\u6821\u6b63\u63d0\u5347\u65b9\u6cd5\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u5177\u6709\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u7a00\u758f\u6027\u548c\u6d45\u5c42\u6811\u8868\u73b0\uff0c\u4f46\u4f7f\u7528\u6700\u4f18\u51b3\u7b56\u6811\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.18293", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18293", "abs": "https://arxiv.org/abs/2507.18293", "authors": ["Sjoerd van Straten", "Alessandro Padella", "Marwan Hassani"], "title": "Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring", "comment": null, "summary": "Predictive Process Monitoring (PPM) enables forecasting future events or\noutcomes of ongoing business process instances based on event logs. However,\ndeep learning PPM approaches are often limited by the low variability and small\nsize of real-world event logs. To address this, we introduce SiamSA-PPM, a\nnovel self-supervised learning framework that combines Siamese learning with\nStatistical Augmentation for Predictive Process Monitoring. It employs three\nnovel statistically grounded transformation methods that leverage control-flow\nsemantics and frequent behavioral patterns to generate realistic, semantically\nvalid new trace variants. These augmented views are used within a Siamese\nlearning setup to learn generalizable representations of process prefixes\nwithout the need for labeled supervision. Extensive experiments on real-life\nevent logs demonstrate that SiamSA-PPM achieves competitive or superior\nperformance compared to the SOTA in both next activity and final outcome\nprediction tasks. Our results further show that statistical augmentation\nsignificantly outperforms random transformations and improves variability in\nthe data, highlighting SiamSA-PPM as a promising direction for training data\nenrichment in process prediction.", "AI": {"tldr": "SiamSA-PPM\u662f\u4e00\u79cd\u7ed3\u5408Siamese\u5b66\u4e60\u548c\u7edf\u8ba1\u589e\u5f3a\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6027\u8fc7\u7a0b\u76d1\u63a7\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u4e49\u6709\u6548\u7684\u8f68\u8ff9\u53d8\u4f53\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60PPM\u65b9\u6cd5\u56e0\u73b0\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u7684\u4f4e\u53d8\u5f02\u6027\u548c\u5c0f\u89c4\u6a21\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u57fa\u4e8e\u7edf\u8ba1\u7684\u8f6c\u6362\u65b9\u6cd5\u751f\u6210\u65b0\u8f68\u8ff9\u53d8\u4f53\uff0c\u5e76\u7ed3\u5408Siamese\u5b66\u4e60\u6846\u67b6\u5b66\u4e60\u8fc7\u7a0b\u524d\u7f00\u7684\u901a\u7528\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSiamSA-PPM\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\uff0c\u7edf\u8ba1\u589e\u5f3a\u663e\u8457\u4f18\u4e8e\u968f\u673a\u8f6c\u6362\u3002", "conclusion": "SiamSA-PPM\u4e3a\u8fc7\u7a0b\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.18297", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18297", "abs": "https://arxiv.org/abs/2507.18297", "authors": ["Sergei Shumilin", "Alexander Ryabov", "Nikolay Yavich", "Evgeny Burnaev", "Vladimir Vanovskiy"], "title": "Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation", "comment": null, "summary": "Due to the high computational load of modern numerical simulation, there is a\ndemand for approaches that would reduce the size of discrete problems while\nkeeping the accuracy reasonable. In this work, we present an original algorithm\nto coarsen an unstructured grid based on the concepts of differentiable\nphysics. We achieve this by employing k-means clustering, autodifferentiation\nand stochastic minimization algorithms. We demonstrate performance of the\ndesigned algorithm on two PDEs: a linear parabolic equation which governs\nslightly compressible fluid flow in porous media and the wave equation. Our\nresults show that in the considered scenarios, we reduced the number of grid\npoints up to 10 times while preserving the modeled variable dynamics in the\npoints of interest. The proposed approach can be applied to the simulation of\nan arbitrary system described by evolutionary partial differential equations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u7269\u7406\u7684\u65e0\u7ed3\u6784\u7f51\u683c\u7c97\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7k-means\u805a\u7c7b\u3001\u81ea\u52a8\u5fae\u5206\u548c\u968f\u673a\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u7f51\u683c\u70b9\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u70b9\u52a8\u6001\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u6570\u503c\u6a21\u62df\u8ba1\u7b97\u8d1f\u8f7d\u9ad8\uff0c\u9700\u51cf\u5c11\u79bb\u6563\u95ee\u9898\u89c4\u6a21\u540c\u65f6\u4fdd\u6301\u5408\u7406\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528k-means\u805a\u7c7b\u3001\u81ea\u52a8\u5fae\u5206\u548c\u968f\u673a\u6700\u5c0f\u5316\u7b97\u6cd5\u8bbe\u8ba1\u7f51\u683c\u7c97\u5316\u7b97\u6cd5\u3002", "result": "\u5728\u4e24\u79cdPDE\uff08\u7ebf\u6027\u629b\u7269\u65b9\u7a0b\u548c\u6ce2\u52a8\u65b9\u7a0b\uff09\u6d4b\u8bd5\u4e2d\uff0c\u7f51\u683c\u70b9\u6570\u51cf\u5c1110\u500d\uff0c\u5173\u952e\u70b9\u52a8\u6001\u7cbe\u5ea6\u5f97\u4ee5\u4fdd\u6301\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u4efb\u610f\u7531\u6f14\u5316\u504f\u5fae\u5206\u65b9\u7a0b\u63cf\u8ff0\u7684\u7cfb\u7edf\u6a21\u62df\u3002"}}
{"id": "2507.18313", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.18313", "abs": "https://arxiv.org/abs/2507.18313", "authors": ["Daniele Ghiani", "Daniele Angioni", "Giorgio Piras", "Angelo Sotgiu", "Luca Minnei", "Srishti Gupta", "Maura Pintor", "Fabio Roli", "Battista Biggio"], "title": "Regression-aware Continual Learning for Android Malware Detection", "comment": "Submitted to IEEE Transactions on Information Forensics and Security", "summary": "Malware evolves rapidly, forcing machine learning (ML)-based detectors to\nadapt continuously. With antivirus vendors processing hundreds of thousands of\nnew samples daily, datasets can grow to billions of examples, making full\nretraining impractical. Continual learning (CL) has emerged as a scalable\nalternative, enabling incremental updates without full data access while\nmitigating catastrophic forgetting. In this work, we analyze a critical yet\noverlooked issue in this context: security regression. Unlike forgetting, which\nmanifests as a general performance drop on previously seen data, security\nregression captures harmful prediction changes at the sample level, such as a\nmalware sample that was once correctly detected but evades detection after a\nmodel update. Although often overlooked, regressions pose serious risks in\nsecurity-critical applications, as the silent reintroduction of previously\ndetected threats in the system may undermine users' trust in the whole updating\nprocess. To address this issue, we formalize and quantify security regression\nin CL-based malware detectors and propose a regression-aware penalty to\nmitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL\nsetting, preserving prior predictive behavior in a model-agnostic manner.\nExperiments on the ELSA, Tesseract, and AZ-Class datasets show that our method\neffectively reduces regression across different CL scenarios while maintaining\nstrong detection performance over time.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u4e2d\u7684\u5b89\u5168\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u56de\u5f52\u611f\u77e5\u7684\u60e9\u7f5a\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u6709\u5bb3\u9884\u6d4b\u53d8\u5316\u3002", "motivation": "\u6076\u610f\u8f6f\u4ef6\u5feb\u901f\u6f14\u53d8\uff0c\u4f20\u7edf\u5168\u91cf\u91cd\u65b0\u8bad\u7ec3\u4e0d\u5207\u5b9e\u9645\uff0c\u6301\u7eed\u5b66\u4e60\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b89\u5168\u56de\u5f52\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u53ef\u80fd\u5bfc\u81f4\u5df2\u68c0\u6d4b\u5230\u7684\u5a01\u80c1\u91cd\u65b0\u51fa\u73b0\u3002", "method": "\u901a\u8fc7\u8c03\u6574Positive Congruent Training\uff08PCT\uff09\u5230\u6301\u7eed\u5b66\u4e60\u73af\u5883\uff0c\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u4fdd\u7559\u5148\u524d\u7684\u9884\u6d4b\u884c\u4e3a\u3002", "result": "\u5728ELSA\u3001Tesseract\u548cAZ-Class\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u56de\u5f52\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u56de\u5f52\u611f\u77e5\u60e9\u7f5a\u65b9\u6cd5\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u6709\u6548\u7f13\u89e3\u5b89\u5168\u56de\u5f52\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.18320", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18320", "abs": "https://arxiv.org/abs/2507.18320", "authors": ["Janak M. Patel", "Milad Ramezankhani", "Anirudh Deodhar", "Dagnachew Birru"], "title": "State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer", "comment": "11 pages, 3 figures", "summary": "The rapid adoption of battery-powered vehicles and energy storage systems\nover the past decade has made battery health monitoring increasingly critical.\nBatteries play a central role in the efficiency and safety of these systems,\nyet they inevitably degrade over time due to repeated charge-discharge cycles.\nThis degradation leads to reduced energy efficiency and potential overheating,\nposing significant safety concerns. Accurate estimation of a State of Health\n(SoH) of battery is therefore essential for ensuring operational reliability\nand safety. Several machine learning architectures, such as LSTMs,\ntransformers, and encoder-based models, have been proposed to estimate SoH from\ndischarge cycle data. However, these models struggle with the irregularities\ninherent in real-world measurements: discharge readings are often recorded at\nnon-uniform intervals, and the lengths of discharge cycles vary significantly.\nTo address this, most existing approaches extract features from the sequences\nrather than processing them in full, which introduces information loss and\ncompromises accuracy. To overcome these challenges, we propose a novel\narchitecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).\nTIDSIT incorporates continuous time embeddings to effectively represent\nirregularly sampled data and utilizes padded sequences with temporal attention\nmechanisms to manage variable-length inputs without discarding sequence\ninformation. Experimental results on the NASA battery degradation dataset show\nthat TIDSIT significantly outperforms existing models, achieving over 50%\nreduction in prediction error and maintaining an SoH prediction error below\n0.58%. Furthermore, the architecture is generalizable and holds promise for\nbroader applications in health monitoring tasks involving irregular time-series\ndata.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTIDSIT\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u6c60\u5065\u5eb7\u72b6\u6001\uff08SoH\uff09\u4f30\u8ba1\u4e2d\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7535\u6c60\u5065\u5eb7\u76d1\u6d4b\u5bf9\u5b89\u5168\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5904\u7406\u4e0d\u89c4\u5219\u653e\u7535\u6570\u636e\u65f6\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTIDSIT\u67b6\u6784\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u5d4c\u5165\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u5904\u7406\u975e\u5747\u5300\u91c7\u6837\u548c\u53d8\u957f\u5e8f\u5217\u3002", "result": "\u5728NASA\u7535\u6c60\u9000\u5316\u6570\u636e\u96c6\u4e0a\uff0cTIDSIT\u5c06\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e50%\u4ee5\u4e0a\uff0cSoH\u9884\u6d4b\u8bef\u5dee\u4f4e\u4e8e0.58%\u3002", "conclusion": "TIDSIT\u4e0d\u4ec5\u663e\u8457\u63d0\u5347\u4e86\u7535\u6c60SoH\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u5177\u6709\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u4e0d\u89c4\u5219\u65f6\u95f4\u5e8f\u5217\u7684\u5065\u5eb7\u76d1\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2507.18333", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.18333", "abs": "https://arxiv.org/abs/2507.18333", "authors": ["Kale-ab Abebe Tessera", "Leonard Hinckeldey", "Riccardo Zamboni", "David Abel", "Amos Storkey"], "title": "Remembering the Markov Property in Cooperative MARL", "comment": "RLC Finding the Frame Workshop Camera-Ready, 8 pages", "summary": "Cooperative multi-agent reinforcement learning (MARL) is typically formalised\nas a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),\nwhere agents must reason about the environment and other agents' behaviour. In\npractice, current model-free MARL algorithms use simple recurrent function\napproximators to address the challenge of reasoning about others using partial\ninformation. In this position paper, we argue that the empirical success of\nthese methods is not due to effective Markov signal recovery, but rather to\nlearning simple conventions that bypass environment observations and memory.\nThrough a targeted case study, we show that co-adapting agents can learn\nbrittle conventions, which then fail when partnered with non-adaptive agents.\nCrucially, the same models can learn grounded policies when the task design\nnecessitates it, revealing that the issue is not a fundamental limitation of\nthe learning models but a failure of the benchmark design. Our analysis also\nsuggests that modern MARL environments may not adequately test the core\nassumptions of Dec-POMDPs. We therefore advocate for new cooperative\nenvironments built upon two core principles: (1) behaviours grounded in\nobservations and (2) memory-based reasoning about other agents, ensuring\nsuccess requires genuine skill rather than fragile, co-adapted agreements.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u60ef\u4f8b\u800c\u975e\u6709\u6548\u9a6c\u5c14\u53ef\u592b\u4fe1\u53f7\u6062\u590d\uff0c\u5e76\u63d0\u51fa\u65b0\u73af\u5883\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524dMARL\u7b97\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u6210\u529f\u4f9d\u8d56\u7b80\u5355\u60ef\u4f8b\u800c\u975e\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790\u667a\u80fd\u4f53\u5b66\u4e60\u60ef\u4f8b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5bf9\u6bd4\u5176\u5728\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u540c\u65f6\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u4f53\u53ef\u5b66\u4e60\u8106\u5f31\u60ef\u4f8b\u6216\u771f\u5b9e\u7b56\u7565\uff0c\u95ee\u9898\u6e90\u4e8e\u73af\u5883\u8bbe\u8ba1\u800c\u975e\u6a21\u578b\u672c\u8eab\u3002", "conclusion": "\u547c\u5401\u8bbe\u8ba1\u65b0\u73af\u5883\uff0c\u5f3a\u8c03\u57fa\u4e8e\u89c2\u5bdf\u548c\u8bb0\u5fc6\u7684\u63a8\u7406\u80fd\u529b\uff0c\u907f\u514d\u4f9d\u8d56\u8106\u5f31\u60ef\u4f8b\u3002"}}
{"id": "2507.18346", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.18346", "abs": "https://arxiv.org/abs/2507.18346", "authors": ["Etienne Zeudong", "Elsa Cardoso-Bihlo", "Alex Bihlo"], "title": "Low-rank adaptive physics-informed HyperDeepONets for solving differential equations", "comment": "14 pages, 6 figures, 5 tables", "summary": "HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an\nalternative architecture for operator learning, in which a hypernetwork\ngenerates the weights for the trunk net of a DeepONet. While this improves\nexpressivity, it incurs high memory and computational costs due to the large\nnumber of output parameters required. In this work we introduce, in the\nphysics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,\nwhich leverage low-rank adaptation (LoRA) to reduce complexity by decomposing\nthe hypernetwork's output layer weight matrix into two smaller low-rank\nmatrices. This reduces the number of trainable parameters while introducing an\nextra regularization of the trunk networks' weights. Through extensive\nexperiments on both ordinary and partial differential equations we show that\nPI-LoRA-HyperDeepONets achieve up to 70\\% reduction in parameters and\nconsistently outperform regular HyperDeepONets in terms of predictive accuracy\nand generalization.", "AI": {"tldr": "PI-LoRA-HyperDeepONets\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u51cf\u5c11HyperDeepONet\u7684\u53c2\u6570\u91cf\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3HyperDeepONet\u56e0\u9ad8\u53c2\u6570\u91cf\u5bfc\u81f4\u7684\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528LoRA\u5c06\u8d85\u7f51\u7edc\u7684\u8f93\u51fa\u5c42\u6743\u91cd\u77e9\u9635\u5206\u89e3\u4e3a\u4e24\u4e2a\u4f4e\u79e9\u77e9\u9635\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "result": "\u5728\u5e38\u5fae\u5206\u548c\u504f\u5fae\u5206\u65b9\u7a0b\u5b9e\u9a8c\u4e2d\uff0c\u53c2\u6570\u91cf\u51cf\u5c1170%\uff0c\u4e14\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u5e38\u89c4HyperDeepONet\u3002", "conclusion": "PI-LoRA-HyperDeepONets\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.18376", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18376", "abs": "https://arxiv.org/abs/2507.18376", "authors": ["Xing Hua", "Haodong Chen", "Qianqian Duan", "Danfeng Hong", "Ruijiao Li", "Huiliang Shang", "Linghua Jiang", "Haima Yang", "Dawei Zhang"], "title": "A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges", "comment": null, "summary": "With the global population growing and arable land resources becoming\nincreasingly scarce,smart agriculture and precision agriculture have emerged as\nkey directions for the future ofagricultural development.Artificial\nintelligence (AI) technologies, particularly deep learning models, have found\nwidespread applications in areas such as crop monitoring and pest detection. As\nan emerging generative model, diffusion models have shown significant promise\nin tasks like agricultural image processing, data augmentation, and remote\nsensing. Compared to traditional generative adversarial networks (GANs),\ndiffusion models offer superior training stability and generation quality,\neffectively addressing challenges such as limited agricultural data and\nimbalanced image samples. This paper reviews the latest advancements in the\napplication of diffusion models in agriculture, focusing on their potential in\ncrop pest and disease detection, remote sensing image enhancement, crop growth\nprediction, and agricultural resource management. Experimental results\ndemonstrate that diffusion models significantly improve model accuracy and\nrobustness in data augmentation, image generation, and denoising, especially in\ncomplex environments. Despite challenges related to computational efficiency\nand generalization capabilities, diffusion models are expected to play an\nincreasingly important role in smart and precision agriculture as technology\nadvances, providing substantial support for the sustainable development of\nglobal agriculture.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u519c\u4e1a\u4e2d\u5e94\u7528\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u75c5\u866b\u5bb3\u68c0\u6d4b\u3001\u9065\u611f\u56fe\u50cf\u589e\u5f3a\u7b49\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u589e\u5f3a\u548c\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u5168\u7403\u4eba\u53e3\u589e\u957f\u548c\u8015\u5730\u8d44\u6e90\u7a00\u7f3a\u4fc3\u4f7f\u667a\u80fd\u519c\u4e1a\u548c\u7cbe\u51c6\u519c\u4e1a\u6210\u4e3a\u53d1\u5c55\u65b9\u5411\uff0c\u6269\u6563\u6a21\u578b\u56e0\u5176\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u751f\u6210\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edfGANs\u3002", "method": "\u7efc\u8ff0\u6269\u6563\u6a21\u578b\u5728\u519c\u4e1a\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u56fe\u50cf\u751f\u6210\u548c\u53bb\u566a\u7b49\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6269\u6563\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u7684\u6311\u6218\uff0c\u6269\u6563\u6a21\u578b\u6709\u671b\u5728\u667a\u80fd\u519c\u4e1a\u4e2d\u53d1\u6325\u66f4\u5927\u4f5c\u7528\uff0c\u652f\u6301\u5168\u7403\u519c\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2507.18423", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.18423", "abs": "https://arxiv.org/abs/2507.18423", "authors": ["Mizuki Funato", "Yohei Sawada"], "title": "Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins", "comment": null, "summary": "Despite the critical need for accurate flood prediction and water management,\nmany regions lack sufficient river discharge observations, limiting the skill\nof rainfall-runoff analyses. Although numerous physically based and machine\nlearning models exist, achieving high accuracy, interpretability, and\ncomputational efficiency under data-scarce conditions remains a major\nchallenge. We address this challenge with a novel method, HYdrological\nPrediction with multi-model Ensemble and Reservoir computing (HYPER) that\nleverages multi-model ensemble and reservoir computing (RC). Our approach first\napplies Bayesian model averaging (BMA) to 43 \"uncalibrated\" catchment-based\nconceptual hydrological models. An RC model is then trained via linear\nregression to correct errors in the BMA output, a non-iterative process that\nensures high computational efficiency. For ungauged basins, we infer the\nrequired BMA and RC weights by linking them to catchment attributes from gauged\nbasins, creating a generalizable framework. We evaluated HYPER using data from\n87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta\nEfficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)\nbut required only 5% of its computational time. In a data-scarce scenario (23%\nof basins gauged), HYPER maintained robust performance (KGE 0.55) and lower\nuncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).\nThese results reveal that individual conceptual hydrological models do not\nnecessarily need to be calibrated when an effectively large ensemble is\nassembled and combined with machine-learning-based bias correction. HYPER\nprovides a robust, efficient, and generalizable solution for discharge\nprediction, particularly in ungauged basins, making it applicable to a wide\nrange of regions.", "AI": {"tldr": "HYPER\u662f\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u578b\u96c6\u6210\u548c\u50a8\u5c42\u8ba1\u7b97\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u63d0\u9ad8\u6d2a\u6c34\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u5730\u533a\u6d2a\u6c34\u9884\u6d4b\u548c\u6c34\u8d44\u6e90\u7ba1\u7406\u4e2d\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u8d1d\u53f6\u65af\u6a21\u578b\u5e73\u5747\uff08BMA\uff09\u7ed3\u540843\u4e2a\u672a\u6821\u51c6\u7684\u6d41\u57df\u6c34\u6587\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\u8fdb\u884c\u8bef\u5dee\u6821\u6b63\u3002", "result": "\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0cHYPER\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u7a33\u5b9a\u3002", "conclusion": "HYPER\u4e3a\u65e0\u6d4b\u7ad9\u6d41\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18519", "abs": "https://arxiv.org/abs/2507.18519", "authors": ["Leiji Zhang", "Zeyu Wang", "Xin Li", "Yao-Hui Li"], "title": "Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning", "comment": null, "summary": "Bisimulation metric has long been regarded as an effective control-related\nrepresentation learning technique in various reinforcement learning tasks.\nHowever, in this paper, we identify two main issues with the conventional\nbisimulation metric: 1) an inability to represent certain distinctive\nscenarios, and 2) a reliance on predefined weights for differences in rewards\nand subsequent states during recursive updates. We find that the first issue\narises from an imprecise definition of the reward gap, whereas the second issue\nstems from overlooking the varying importance of reward difference and\nnext-state distinctions across different training stages and task settings. To\naddress these issues, by introducing a measure for state-action pairs, we\npropose a revised bisimulation metric that features a more precise definition\nof reward gap and novel update operators with adaptive coefficient. We also\noffer theoretical guarantees of convergence for our proposed metric and its\nimproved representation distinctiveness. In addition to our rigorous\ntheoretical analysis, we conduct extensive experiments on two representative\nbenchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of\nour approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u53cc\u6a21\u62df\u5ea6\u91cf\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8868\u793a\u7279\u5b9a\u573a\u666f\u548c\u4f9d\u8d56\u9884\u5b9a\u4e49\u6743\u91cd\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u53cc\u6a21\u62df\u5ea6\u91cf\u5b58\u5728\u65e0\u6cd5\u8868\u793a\u67d0\u4e9b\u72ec\u7279\u573a\u666f\u548c\u4f9d\u8d56\u9884\u5b9a\u4e49\u6743\u91cd\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u5ea6\u91cf\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u53cc\u6a21\u62df\u5ea6\u91cf\uff0c\u5305\u62ec\u66f4\u7cbe\u786e\u7684\u5956\u52b1\u5dee\u8ddd\u5b9a\u4e49\u548c\u81ea\u9002\u5e94\u7cfb\u6570\u7684\u66f4\u65b0\u7b97\u5b50\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u65b0\u5ea6\u91cf\u7684\u6536\u655b\u6027\u548c\u8868\u793a\u533a\u5206\u6027\uff0c\u5b9e\u9a8c\u5728DeepMind Control\u548cMeta-World\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u6539\u8fdb\u7684\u53cc\u6a21\u62df\u5ea6\u91cf\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u6280\u672f\u3002"}}
{"id": "2507.18521", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18521", "abs": "https://arxiv.org/abs/2507.18521", "authors": ["Zhongtian Sun", "Anoushka Harit", "Alexandra Cristea", "Christl A. Donnelly", "Pietro Li\u00f2"], "title": "GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated significant success in\nlearning from graph-structured data but often struggle on heterophilous graphs,\nwhere connected nodes differ in features or class labels. This limitation\narises from indiscriminate neighbor aggregation and insufficient incorporation\nof higher-order structural patterns. To address these challenges, we propose\nGLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel\nframework that integrates logic-guided reasoning, dynamic graph refinement, and\nadaptive clustering to enhance graph representation learning. GLANCE combines a\nlogic layer for interpretable and structured embeddings, multi-head\nattention-based edge pruning for denoising graph structures, and clustering\nmechanisms for capturing global patterns. Experimental results in benchmark\ndatasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE\nachieves competitive performance, offering robust and interpretable solutions\nfor heterophilous graph scenarios. The proposed framework is lightweight,\nadaptable, and uniquely suited to the challenges of heterophilous graphs.", "AI": {"tldr": "GLANCE\u6846\u67b6\u901a\u8fc7\u903b\u8f91\u5f15\u5bfc\u63a8\u7406\u3001\u52a8\u6001\u56fe\u4f18\u5316\u548c\u81ea\u9002\u5e94\u805a\u7c7b\uff0c\u63d0\u5347\u5f02\u8d28\u56fe\u8868\u793a\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfGNN\u5728\u5f02\u8d28\u56fe\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u90bb\u5c45\u805a\u5408\u4e0d\u533a\u5206\u4e14\u9ad8\u9636\u7ed3\u6784\u6a21\u5f0f\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u903b\u8f91\u5c42\u3001\u591a\u5934\u6ce8\u610f\u529b\u8fb9\u526a\u679d\u548c\u805a\u7c7b\u673a\u5236\uff0c\u4f18\u5316\u56fe\u7ed3\u6784\u548c\u5168\u5c40\u6a21\u5f0f\u6355\u6349\u3002", "result": "\u5728Cornell\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u8f7b\u91cf\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GLANCE\u9002\u7528\u4e8e\u5f02\u8d28\u56fe\u6311\u6218\uff0c\u6027\u80fd\u5f3a\u4e14\u53ef\u89e3\u91ca\u3002"}}
{"id": "2507.18533", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18533", "abs": "https://arxiv.org/abs/2507.18533", "authors": ["Magnus Bengtsson", "Kenneth \u00d6stberg"], "title": "C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation", "comment": "12 pages", "summary": "We introduce C2G-KD, a data-free knowledge distillation framework where a\nclass-conditional generator is trained to produce synthetic samples guided by a\nfrozen teacher model and geometric constraints derived from PCA. The generator\nnever observes real training data but instead learns to activate the teacher's\noutput through a combination of semantic and structural losses. By constraining\ngenerated samples to lie within class-specific PCA subspaces estimated from as\nfew as two real examples per class, we preserve topological consistency and\ndiversity. Experiments on MNIST show that even minimal class structure is\nsufficient to bootstrap useful synthetic training pipelines.", "AI": {"tldr": "C2G-KD\u662f\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u6570\u636e\u3001\u57fa\u4e8e\u6559\u5e08\u6a21\u578b\u548cPCA\u51e0\u4f55\u7ea6\u675f\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6837\u672c\u5b9e\u73b0\u8bad\u7ec3\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u65e0\u9700\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u6559\u5e08\u6a21\u578b\u548c\u51e0\u4f55\u7ea6\u675f\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u6837\u672c\uff0c\u4ee5\u652f\u6301\u77e5\u8bc6\u84b8\u998f\u3002", "method": "\u4f7f\u7528\u7c7b\u6761\u4ef6\u751f\u6210\u5668\uff0c\u7ed3\u5408\u6559\u5e08\u6a21\u578b\u7684\u8f93\u51fa\u548cPCA\u51e0\u4f55\u7ea6\u675f\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7ed3\u6784\u635f\u5931\u751f\u6210\u6837\u672c\u3002", "result": "\u5728MNIST\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u4ec5\u7528\u6bcf\u7c7b\u4e24\u4e2a\u771f\u5b9e\u6837\u672c\uff0c\u4e5f\u80fd\u751f\u6210\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6837\u672c\u3002", "conclusion": "C2G-KD\u6846\u67b6\u5c55\u793a\u4e86\u5728\u6781\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u652f\u6301\u4e0b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6837\u672c\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.18549", "categories": ["cs.LG", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2507.18549", "abs": "https://arxiv.org/abs/2507.18549", "authors": ["Steven A. Frank"], "title": "The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection", "comment": null, "summary": "Diverse learning algorithms, optimization methods, and natural selection\nshare a common mathematical structure, despite their apparent differences. Here\nI show that a simple notational partitioning of change by the Price equation\nreveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} =\n\\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$\ndrives improvement in parameters, $\\Delta\\mathbf{\\theta}$, through the\ncovariance between the parameters and performance. The metric $\\mathbf{M}$\nrescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or\nchanges in the frame of reference. The noise $\\mathbf{\\xi}$ enables\nexploration. This framework unifies natural selection, Bayesian updating,\nNewton's method, stochastic gradient descent, stochastic Langevin dynamics,\nAdam optimization, and most other algorithms as special cases of the same\nunderlying process. The Price equation also reveals why Fisher information,\nKullback-Leibler divergence, and d'Alembert's principle arise naturally in\nlearning dynamics. By exposing this common structure, the FMB law provides a\nprincipled foundation for understanding, comparing, and designing learning\nalgorithms across disciplines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u529b-\u5ea6\u91cf-\u504f\u7f6e\uff08FMB\uff09\u5b9a\u5f8b\uff0c\u901a\u8fc7Price\u65b9\u7a0b\u63ed\u793a\u4e86\u4e0d\u540c\u5b66\u4e60\u7b97\u6cd5\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u81ea\u7136\u9009\u62e9\u7684\u5171\u540c\u6570\u5b66\u7ed3\u6784\u3002", "motivation": "\u63ed\u793a\u770b\u4f3c\u4e0d\u540c\u7684\u5b66\u4e60\u7b97\u6cd5\u3001\u4f18\u5316\u65b9\u6cd5\u548c\u81ea\u7136\u9009\u62e9\u4e4b\u95f4\u7684\u5171\u540c\u6570\u5b66\u7ed3\u6784\uff0c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u8de8\u5b66\u79d1\u7684\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4f7f\u7528Price\u65b9\u7a0b\u5bf9\u53d8\u5316\u8fdb\u884c\u7b80\u5355\u7684\u7b26\u53f7\u5212\u5206\uff0c\u63a8\u5bfc\u51fa\u529b-\u5ea6\u91cf-\u504f\u7f6e\uff08FMB\uff09\u5b9a\u5f8b\uff1a\u0394\u03b8 = Mf + b + \u03be\u3002", "result": "FMB\u5b9a\u5f8b\u7edf\u4e00\u4e86\u81ea\u7136\u9009\u62e9\u3001\u8d1d\u53f6\u65af\u66f4\u65b0\u3001\u725b\u987f\u6cd5\u3001\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b49\u591a\u79cd\u7b97\u6cd5\uff0c\u5e76\u89e3\u91ca\u4e86Fisher\u4fe1\u606f\u3001KL\u6563\u5ea6\u7b49\u5728\u5b66\u4e60\u52a8\u529b\u5b66\u4e2d\u7684\u81ea\u7136\u51fa\u73b0\u3002", "conclusion": "FMB\u5b9a\u5f8b\u4e3a\u8de8\u5b66\u79d1\u7684\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.18553", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18553", "abs": "https://arxiv.org/abs/2507.18553", "authors": ["Jiale Chen", "Torsten Hoefler", "Dan Alistarh"], "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm", "comment": null, "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86GPTQ\u91cf\u5316\u65b9\u6cd5\u4e0eBabai\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u7684\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u4e3a\u91cf\u5316\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u7814\u7a76GPTQ\u91cf\u5316\u65b9\u6cd5\u7684\u5185\u5728\u51e0\u4f55\u610f\u4e49\u548c\u7406\u8bba\u4fdd\u8bc1\uff0c\u4ee5\u6539\u8fdb\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5316\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bba\u8bc1\uff0c\u8bc1\u660eGPTQ\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e0eBabai\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u7b49\u4ef7\uff0c\u5e76\u5206\u6790\u5176\u8bef\u5dee\u4f20\u64ad\u548c\u4e0a\u754c\u3002", "result": "GPTQ\u7684\u8bef\u5dee\u4f20\u64ad\u5177\u6709\u51e0\u4f55\u89e3\u91ca\uff0c\u4e14\u5728\u65e0\u88c1\u526a\u6761\u4ef6\u4e0b\u7ee7\u627f\u4e86Babai\u7b97\u6cd5\u7684\u8bef\u5dee\u4e0a\u754c\u3002", "conclusion": "\u7814\u7a76\u4e3aGPTQ\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u672a\u6765\u91cf\u5316\u7b97\u6cd5\u7684\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.18597", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18597", "abs": "https://arxiv.org/abs/2507.18597", "authors": ["Ethan Pronovost", "Neha Boloor", "Peter Schleede", "Noureldin Hendy", "Andres Morales", "Nicholas Roy"], "title": "Linear Memory SE(2) Invariant Attention", "comment": "Best paper award, Equivariant Systems Workshop at RSS", "summary": "Processing spatial data is a key component in many learning tasks for\nautonomous driving such as motion forecasting, multi-agent simulation, and\nplanning. Prior works have demonstrated the value in using SE(2) invariant\nnetwork architectures that consider only the relative poses between objects\n(e.g. other agents, scene features such as traffic lanes). However, these\nmethods compute the relative poses for all pairs of objects explicitly,\nrequiring quadratic memory. In this work, we propose a mechanism for SE(2)\ninvariant scaled dot-product attention that requires linear memory relative to\nthe number of objects in the scene. Our SE(2) invariant transformer\narchitecture enjoys the same scaling properties that have benefited large\nlanguage models in recent years. We demonstrate experimentally that our\napproach is practical to implement and improves performance compared to\ncomparable non-invariant architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u5185\u5b58\u6d88\u8017\u7684SE(2)\u4e0d\u53d8\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7a7a\u95f4\u6570\u636e\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SE(2)\u4e0d\u53d8\u6027\u7f51\u7edc\u67b6\u6784\u56e0\u663e\u5f0f\u8ba1\u7b97\u6240\u6709\u5bf9\u8c61\u5bf9\u7684\u76f8\u5bf9\u4f4d\u59ff\u800c\u5bfc\u81f4\u7684\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cdSE(2)\u4e0d\u53d8\u6027\u7684\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\u673a\u5236\uff0c\u57fa\u4e8e\u76f8\u5bf9\u4f4d\u59ff\u5b9e\u73b0\u7ebf\u6027\u5185\u5b58\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6613\u4e8e\u5b9e\u73b0\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u975e\u4e0d\u53d8\u6027\u67b6\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u7a7a\u95f4\u6570\u636e\u5904\u7406\u65b9\u6848\u3002"}}
{"id": "2507.18603", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18603", "abs": "https://arxiv.org/abs/2507.18603", "authors": ["Zinan Ling", "Yi Shi", "Da Yan", "Yang Zhou", "Bo Hui"], "title": "Demystify Protein Generation with Hierarchical Conditional Diffusion Models", "comment": null, "summary": "Generating novel and functional protein sequences is critical to a wide range\nof applications in biology. Recent advancements in conditional diffusion models\nhave shown impressive empirical performance in protein generation tasks.\nHowever, reliable generations of protein remain an open research question in de\nnovo protein design, especially when it comes to conditional diffusion models.\nConsidering the biological function of a protein is determined by multi-level\nstructures, we propose a novel multi-level conditional diffusion model that\nintegrates both sequence-based and structure-based information for efficient\nend-to-end protein design guided by specified functions. By generating\nrepresentations at different levels simultaneously, our framework can\neffectively model the inherent hierarchical relations between different levels,\nresulting in an informative and discriminative representation of the generated\nprotein. We also propose a Protein-MMD, a new reliable evaluation metric, to\nevaluate the quality of generated protein with conditional diffusion models.\nOur new metric is able to capture both distributional and functional\nsimilarities between real and generated protein sequences while ensuring\nconditional consistency. We experiment with the benchmark datasets, and the\nresults on conditional protein generation tasks demonstrate the efficacy of the\nproposed generation framework and evaluation metric.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5e8f\u5217\u548c\u7ed3\u6784\u4fe1\u606f\u8fdb\u884c\u7aef\u5230\u7aef\u86cb\u767d\u8d28\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165Protein-MMD\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u86cb\u767d\u8d28\u8bbe\u8ba1\u9700\u8981\u751f\u6210\u529f\u80fd\u6027\u548c\u65b0\u9896\u7684\u5e8f\u5217\uff0c\u4f46\u73b0\u6709\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u53ef\u9760\u751f\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u591a\u7ea7\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u6574\u5408\u5e8f\u5217\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u540c\u65f6\u751f\u6210\u591a\u7ea7\u8868\u793a\uff0c\u5e76\u63d0\u51faProtein-MMD\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u548c\u8bc4\u4f30\u6307\u6807\u5728\u6761\u4ef6\u86cb\u767d\u8d28\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u591a\u7ea7\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548cProtein-MMD\u4e3a\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18623", "categories": ["cs.LG", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.18623", "abs": "https://arxiv.org/abs/2507.18623", "authors": ["Xuhui Kang", "Sung-Wook Lee", "Haolin Liu", "Yuyan Wang", "Yen-Ling Kuo"], "title": "Moving Out: Physically-grounded Human-AI Collaboration", "comment": "24 pages, 8 figures", "summary": "The ability to adapt to physical actions and constraints in an environment is\ncrucial for embodied agents (e.g., robots) to effectively collaborate with\nhumans. Such physically grounded human-AI collaboration must account for the\nincreased complexity of the continuous state-action space and constrained\ndynamics caused by physical constraints. In this paper, we introduce\n\\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a\nwide range of collaboration modes affected by physical attributes and\nconstraints, such as moving heavy items together and maintaining consistent\nactions to move a big item around a corner. Using Moving Out, we designed two\ntasks and collected human-human interaction data to evaluate models' abilities\nto adapt to diverse human behaviors and unseen physical attributes. To address\nthe challenges in physical environments, we propose a novel method, BASS\n(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of\nagents and their understanding of the outcome of actions. Our experiments show\nthat BASS outperforms state-of-the-art models in AI-AI and human-AI\ncollaboration. The project page is available at\n\\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4eba\u673a\u534f\u4f5c\u57fa\u51c6\u6d4b\u8bd5Moving Out\uff0c\u5e76\u63d0\u51fa\u4e86BASS\u65b9\u6cd5\u6765\u63d0\u5347AI\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u4eba\u673a\u534f\u4f5c\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u548c\u53d7\u9650\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86BASS\u65b9\u6cd5\uff08\u884c\u4e3a\u589e\u5f3a\u3001\u6a21\u62df\u548c\u9009\u62e9\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u4ee3\u7406\u7684\u591a\u6837\u6027\u548c\u5bf9\u52a8\u4f5c\u7ed3\u679c\u7684\u7406\u89e3\u6765\u5e94\u5bf9\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBASS\u5728AI-AI\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "Moving Out\u57fa\u51c6\u548cBASS\u65b9\u6cd5\u4e3a\u7269\u7406\u73af\u5883\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
